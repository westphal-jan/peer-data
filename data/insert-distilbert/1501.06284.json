{"id": "1501.06284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "On a Family of Decomposable Kernels on Sequences", "abstract": "in many applications data is naturally presented in terms of orderings of some basic elements or symbols. reasoning about such data requires a concrete notion dependent of representation similarity capable of handling sequences of different lengths. in this paper we describe a family of mercer kernel coding functions for such sequentially structured strings data. the family is appropriately characterized by a decomposable structure in terms of symbol - level and structure - level similarities, representing a specific abstract combination of kernels which allows for efficient computation. we provide additionally an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to emerge a state of the art sequence element kernel problem called, the global alignment kernel which has been shown to weakly outperform dynamic time warping", "histories": [["v1", "Mon, 26 Jan 2015 08:30:55 GMT  (323kb,D)", "http://arxiv.org/abs/1501.06284v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrea baisero", "florian t pokorny", "carl henrik ek"], "accepted": false, "id": "1501.06284"}, "pdf": {"name": "1501.06284.pdf", "metadata": {"source": "CRF", "title": "On a Family of Decomposable Kernels on Sequences On a Family of Decomposable Kernels on Sequences", "authors": ["Andrea Baisero", "Florian T. Pokorny"], "emails": ["andrea.baisero@ipvs.uni-stuttgart.de", "fpokorny@csc.kth.se", "chek@csc.kth.se"], "sections": [{"heading": null, "text": "Keywords: Kernel, Sequences"}, {"heading": "1. Introduction", "text": "Many types of data have inherent sequential structure. Sequences of letters in computational linguistics, series of images in computer vision or cell structures in computational biology and arbitrary data sets depending on a parameter such as time provide familiar examples of such data. It is hence not surprising that there exists a significant amount of work focused on representing such data. In (Rieck, 2011) the author reviews and broadly categorizes sequential similarity measures into three main categories: bag-of-words, edit-distance and string-kernel based methods. Bag-of-words (Harris, 1970) based similarity measures translate the notion of a sequence to a distribution over certain sub-sequences (i.e. words in natural language processing) of the sequence itself, meaning that such measures only encode the sequential structure up to the length of the sub-sequence and disregard information about word order. As such, Bag-of-words methods require us to be able to identify significant sub-sequences (the words), which is not always obvious for sequences arising outside natural language. Nevertheless, this approach captures some structure and, as the sequential data is translated into a vector space whose basis consists of elementary subsequences, it allows us to interpret the data and enables us to use well-developed learning methods for such vectorial data. Techniques based on edit distances (Damerau, 1964; Levenshtein,\nar X\niv :1\n50 1.\n06 28\n4v 1\n[ cs\n.L G\n] 2\n1966) relate sequences by defining a transformation from one sequence to the other and associating a cost to the transformation. Edit distances can be very useful if the notion of cost with respect to different transformations is well grounded. The third category refers to (dis)similarity measures defined by implicitly specifying an inner-product space through a kernel function between sequences. String kernels (Lodhi et al., 2002; Rousu and ShaweTaylor, 2006) were proposed in Computational Linguistics, where data consists of sequences (text) of discrete symbols (letters). The (dis)similarity measure is defined in terms of \u201cgaps\u201d between symbols in the two sequences. String kernels are a specific instance of a larger class of kernel functions referred to as rational kernels (Cortes et al., 2004). Rational kernels are related to weighted automata (Mohri, 2009) and define inner products from the specific sequential structure described by the automata. In this paper, we will focus on a new family of kernel based (dis)similarity measures.\nThe contributions of this work are in particular: a) A generic approach for the construction of sequence kernels which scales O(nm) in the lengths n,m of the input strings. b) The kernel decomposes intuitively into structure-level and symbol-level similarities. Compared to previous approaches, the structure of the symbol space can be encoded by any Mercer kernel. c) We show that a recently proposed intuitive (dis)similarity measure on sequences (Baisero et al., 2013), is positive definite kernel and falls into our class. d) We compare and evaluate several kernels from our family which perform favourably against the state of the art Global Alignment kernel."}, {"heading": "2. Related work", "text": "The three main sequence similarity approaches discussed above are all based on the concept that sequence similarity is defined in terms of discrete unordered symbols, and the similarity between two symbols a, b,\u2208 \u03a3 is typically is defined by zero if a 6= b and one otherwise. However, for many types of data the symbol space \u03a3 might be continuous, and we might in fact have a natural similarity measure on \u03a3 itself. As an example, consider the problem of matching two discretized waveforms \u03b1 = [\u03b11, . . . , \u03b1n], \u03b2 = [\u03b21, . . . , \u03b2m] where \u03b1i, \u03b2i \u2208 R = \u03a3 and where there exists a natural distance \u2016a \u2212 b\u2016 for a, b \u2208 \u03a3 = R. A popular similarity measure closely related to edit distances is Dynamic Time Warping (Sakoe and Chiba, 1978; Mu\u0308ller, 2007). It provides a similarity measure based on the cost of aligning two sequences such that the sum of matching each element is minimized. This measure does not by itself correspond to a positive definite kernel function (Bahlmann et al., 2002) and hence lacks a geometrical interpretation. One approach has been to use the dynamic time warping distance inside a radial basis exponential kernel function (Lei and Sun, 2007; Bahlmann et al., 2002). However this still suffers from the drawback that dynamic time warping is not a kernel itself. Even though non-positive kernels have been shown to be useful (Haasdonk, 2005) in practice, they lack a geometrical interpretation and the mathematical justification which makes the use of kernel methods so appealing.\nMotivated by the intuition for the definition of dynamic time warping, (Cuturi et al., 2007) developed a related similarity measure which in fact corresponds to a valid kernel function for sequences. Here a (dis)similarity function is defined by summarizing all possible alignments between two sequences through a \u2018soft-min\u2019 rather than using only the minimal cost alignment as in dynamic time warping. Importantly, compared to previous kernels on\nsequences, this kernel is capable of incorporating a structured non-discrete symbol space \u03a3. The resulting kernel is referred to as the Global Alignment kernel and was shown to outperform Dynamic Time Warping for sequence classification. However, to be a valid Mercer kernel, the structure of the symbol space \u03a3 have to be induced by a specific class of kernel functions. Further, it strongly favors small sequence perturbations over larger perturbations which reduces the ability of the kernel to generalize example data. Some of these issues have been addressed in (Cuturi, 2010) where only a subset of the possible alignments contributes to the inner-product.\nAnother approach was taken in (Baisero et al., 2013), where we proposed a (dis)similarity measure called the Path kernel. Just like the Global Alignment kernel, this kernel is defined by reasoning about the (dis)similarity of all possible alignments of two sequences. In experiments, the Path kernel performed better than the Global Alignment kernel for a set of experiments both with respect to accuracy and computational cost. We will show that this kernel naturally falls into a class of kernels that we will define in this work, thus proving that it is positive semi-definite1."}, {"heading": "3. On the construction of sequence kernels", "text": "We are interested in finite sequences s = (s1, . . . , s|s|), with symbols si \u2208 \u03a3, belonging to a symbol space \u03a3 which can be discrete or continuous. We denote the set of such finite sequences by Seq(\u03a3) and are interested in studying combinations of Mercer kernel functions on symbols k\u03a3 : \u03a3\u00d7 \u03a3 \u2192 R that yield valid Mercer kernels on a sequential level Seq(\u03a3). We follow the convention of calling a kernel k : X \u00d7 X \u2192 R positive definite if\u2211n\ni,j=1 cik(xi, xj)cj > 0 for any finite subset {x1, . . . , xn} \u2282 X, n \u2208 N and any {c1, . . . , cn} \u2282 R. Let us now describe a novel general approach towards the construction of such kernels for sequences belonging to Seq(\u03a3):\nLemma 3.1 Let k\u03a3 : \u03a3\u00d7\u03a3\u2192 R be a continuous positive definite kernel on \u03a3, where \u03a3 is a separable metric space and let kS : N \u00d7 N \u2192 R be a positive definite kernel on integers. Then the kernel\nk(s, t) = |s|\u2211 i=1 |t|\u2211 j=1 k\u03a3(si, tj)kS(i, j), (1)\ndefined for any finite sequences s, t \u2208 Seq(\u03a3), s = (s1, . . . , s|s|) and t = (t1, . . . , t|t|) is also positive definite.\nProof Observe that both k\u03a3 and kS can be trivially extended to kernels on \u03a3 \u00d7 N by K1((s, i), (t, j)) = k\u03a3(si, tj), K2((s, i), (t, j)) = kS(i, j) for si, tj \u2208 \u03a3 and i, j \u2208 N. Now K((s, i), (t, j)) = K1((s, i), (t, j))K2((s, i), (t, j)) is a positive kernel on U = \u03a3 \u00d7 N. Let X,Y be finite subsets of U . According to Lemma 1, (Haussler, 1999), the kernel\nL(X,Y ) = \u2211\nx\u2208X,y\u2208Y K(x, y)\n1. Note that, the Path kernel defined in (Baisero et al., 2013) is not related to the special Rational kernel proposed in (Takimoto and Warmuth, 2003), which is also referred to as a Path kernel\nis then also positive definite. Note that a sequence s = (s1, s2, . . . , sn) corresponds to a subset X = {(s1, 1), (s2, 2), . . . , (sn, n)}, and thus the above kernel is positive definite.\nNote that any countable discrete space and any finite dimensional vector space can be given the structure of a separable metric space. If \u03a3 is discrete and countable, any kernel on \u03a3 is trivially continuous with respect to the discrete topology. Note also that, while the above result readily follows from the work on convolution kernels by (Haussler, 1999), the above natural class of kernels has \u2013 to the best of our knowledge \u2013 not been studied or formulated in this manner. This might be partially, because classical kernels coming from natural language processing often only consider similarity measures on the symbol space \u03a3 directly.\nWe observe that the family of kernels described above relates all pairs of the input sequences\u2019 symbols using k\u03a3 and adjusts these values according to similarity of positions of the symbols within the sequences, as measured by kS . An added benefit of the proposed family of kernels is their relative computational simplicity, since kernel evaluations scale like O(|s||t|) in the length of the input strings s, t. Noting that,\nk(s, t) = tr(K\u03a3(s, t) TKS(|s|, |t|)), (2)\nwhere [K\u03a3(s, t)]ij = k\u03a3(si, tj), [KS(|s|, |t|)]ij = kS(i, j), i = 1, . . . , |s|, j = 1, . . . , |t| and tr denotes the trace, we observe that the matrix KS can be pre-computed once the maximal length of any sequence in a data-set is known. The evaluation of the kernel is then just a trace of a matrix product which can be efficiently implemented.\nIn a typical scenario kS and k\u03a3 might also depend on parameters \u03b8S \u2208 Rn and \u03b8\u03a3 \u2208 Rm. These parameters can be set through cross-validation but they can also be learned if the gradients of the kernel functions with respect to these parameters can be computed. If we wish to use the kernel to represent a functional relationship f : s 7\u2192 y, where y \u2208 Rd, we can encode a preference over the mapping f by a Gaussian process (Rasmussen and Williams, 2006). If the co-variance in the output space is encoded by a sequence kernel k, the parameters, \u03b8\u03a3 and \u03b8S can then be learned by maximizing the marginal likelihood of the model. In order to accommodate classification in a Gaussian process framework, the regression noise is usually squashed (Rasmussen and Williams, 2006) rendering the integration required to reach the marginal likelihood infeasible. However, it has been observed that learning the parameters for a classification task with 1\u2212C encoding, where each class is encoded using a binary variable, and a Gaussian noise assumption works well in practice (Kapoor et al., 2009)."}, {"heading": "4. Examples of Sequence Kernels", "text": "In this paper we will focus on kernels from the family in Lemma 3.1. A straightforward approach to formulate such a sequence kernel would be to pick a familiar kernel kS , where |i\u2212 j| determines the impact on k. This can be implemented by a stationary kernel kS such as an exponential kernel:\nCorollary 4.1 For \u03b1 > 0, the function ke : Seq(\u03a3)\u00d7 Seq(\u03a3)\u2192 R given by,\nke(s, t) = |s|\u2211 i=1 |t|\u2211 j=1 k\u03a3(si, tj)e \u2212 \u2016i\u2212j\u2016 2 \u03b1 , (3)\nis a valid kernel on Seq(\u03a3) corresponding to the exponential structure kernel on N.\nSimilarly, we can now define kernels by defining a kernel kS on integers by restricting any known kernel on R to the integers. Examples include the polynomial kernels kS(i, j) = (ij+ c)d, the perceptron kernel, etc. While the above kernel follows readily from the definition of our class of sequence kernels, we would now like to focus on a secondary viewpoint which, as we will show, also leads to a sequence kernel as in Lemma 3.1. In (Baisero et al., 2013), the authors proposed a novel (dis)similarity measure kp : Seq(\u03a3)\u00d7 Seq(\u03a3)\u2192 R which can be defined most elegantly in a recursive fashion as,\nkp(s, t) =  k\u03a3(s1, t1) + Chvkp(s2:, t) + Chvkp(s, t2:) + Cdkp(s2:, t2:) |s| > 1|t| > 1 Cd > 0 Chv > 0\n0 otherwise,\n. (4)\nwhere t2: denotes the sequence obtained by removing the first symbol from t \u2208 Seq(\u03a3). The recursive formulation above can be interpreted as accumulating information from all possible alignments of two strings. An alignment of two strings s and t is defined by a path \u03b3 through a matrix M of size |s| \u00d7 |t| from element [M]11 to [M]|s||t|. Each path defines a different alignment in terms of \u201cstretches\u201d of a sequence see Figure 2. Each path is decomposed into series of simple operations which have a different effect, parametrized by Cd and Chv, on the final similarity measure. The cardinality of the set of paths, and therefore of the alignments, for two sequences s and t is the Delannoy number D(|s|, |t|). In addition to the recursive formulation above, (Baisero et al., 2013) also showed that the resulting function can be written in a similar form to Eq. 1 where kS = k\u0393 and,\nk\u0393(i, j) = min(i,j)\u22121\u2211 d=0 Ci+j\u22122\u22122dhv C d d (i+ j \u2212 2\u2212 d)! (i\u2212 1\u2212 d)!(j \u2212 1\u2212 d)!d! . (5)\nWhile the recursive definition of kp is natural, since it assigns a cost for diagonal and offdiagonal moves in a matrix, the above formula seems rather unintuitive. While (Baisero et al., 2013) did not provide a proof of positive definiteness, we will now show that the path kernel kp does in fact define a positive definite kernel and that kp naturally falls into the class of kernels considered here.\nIn order to prove that kp is a positive definite kernel, we now need to show that k\u0393 : N \u00d7 N \u2192 R is indeed a kernel on integers. First lets recall that the Gamma function \u0393 : C\u2192 R is defined by,\n\u0393(z) = \u222b \u221e 0 tz\u22121e\u2212tdt, (6)\nfor z \u2208 C, Re(z) > 0 and that we have \u0393(n) = (n\u2212 1)! for n \u2208 N. Let us now think of the factorial as a curious example of a positive Mercer kernel on integers:\nLemma 4.2 Let d \u2208 Z and Xd = {x \u2208 N : x > d2}. The function k : Xd \u00d7Xd \u2192 R defined by,\nk(x, x\u2032) = (x+ x\u2032 \u2212 d)!,\nis a positive definite kernel on Xd corresponding to the feature mapping \u03c8d : Xd \u2192 L1(R>0) mapping x \u2208 Xd to the function fx(t) = tx\u2212 d 2 e\u2212 t 2 . I.e., considering the standard inner\nproduct on L1(R>0) given by \u3008f, g\u3009 = \u222b\u221e\n0 f(t)g(t)dt for two integrable functions f, g \u2208 L1(R>0), we have k(x, x\u2032) = \u3008fx, fy\u3009.\nProof The result follows directly from the above integral formula for \u0393(x+ x\u2032 \u2212 d+ 1).\nNote that, if Cd > 0, we can use the idea of the above lemma and write k\u0393(i, j) =\n\u2211min(i,j)\u22121 d=0 \u3008\u03d5d(i), \u03d5d(j)\u3009, where \u03d5d : X2+2d \u2192 L\n1(R>0) is the feature map mapping integers to functions which is given by,\n(\u03d5d(i))(t) = C d 2 d C i\u22121\u2212d hv\n(i\u2212 1\u2212 d)! ti\u22121\u2212 d 2 e\u2212 t 2 ,\nand \u3008f, g\u3009 is again the inner-product of functions obtained by integration. kd(i, j) = \u3008\u03d5d(i), \u03d5d(j)\u3009 is a kernel on X2d+2 = {x \u2208 N : x > d}. Note that the condition d 6 min(i, j) \u2212 1, i, j \u2208 N in the summation appearing in the definition of k\u0393 is equivalent to d < i and d < j, i.e. i, j \u2208 X2d+2. Let us call the extension of kd to N \u00d7 N k\u0302d, so that k\u0302d(i, j) = 0 if i or j /\u2208 X2d+2 and k\u0302d(i, j) = kd(i, j) if i, j \u2208 X2d+2. Then k\u0302d : N\u00d7 N\u2192 R is a positive definite kernel by construction and we have,\nk\u0393(i, j) = min(i,j)\u22121\u2211 d=0 Ci+j\u22122\u22122dhv C d d (i+ j \u2212 2\u2212 d)! (i\u2212 1\u2212 d)!(j \u2212 1\u2212 d)!d!\n= min(i,j)\u22121\u2211 d=0 kd(i, j) = min(i,j)\u22121\u2211 d=0 \u03b4d<ikd(i, j)\u03b4d<j\n= min(i,j)\u22121\u2211 d=0 k\u0302d(i, j) = \u221e\u2211 d=0 k\u0302d(i, j),\nwhere \u03b4d<i = 1 if d < i and zero otherwise. For any finite set of integers, only finitely many terms in the sum above are non-zero and the kernel kS is clearly positive since it is a sum of positive kernels.\nCorollary 4.3 Let k\u03a3 : \u03a3\u00d7\u03a3\u2192 R be a continuous positive definite kernel on \u03a3, where \u03a3 is a separable metric space. Then the associated path kernel kp : Seq(\u03a3) \u00d7 Seq(\u03a3) \u2192 R is a positive definite kernel for any Cd > 0 and Chv \u2208 R."}, {"heading": "5. Experiments", "text": "In this section we will experimentally evaluate the performance of the path sequence kernel on a set of real sequential classification data-sets. However, to provide intution for the approach, we will first show how the proposed path sequence kernel represent two sets of toy-data. One of the motivations behind this work is to provide a vectorial embedding of sequences of different length. To evaluate this we, generate 10 noisy sine and cosine curves of different length as shown in Figure 3. We now wish to find an embedding that separates the two classes of curves. By formulating the classification task as a regression problem to a 1\u2212C encoding and placing a Gaussian Process prior (Rasmussen and Williams, 2006) over the mapping, we can learn the kernel parameters through a maximum-likelihood approach. In Figure 3, the resulting embedding is displayed, clearly showing how the kernel manages to separate the two classes. It is important to note that both the sine and the cosine curves are generated over a full period which means that the first order statistics for the curves are the same. The discriminating information in the data is hence contained in the sequential structure which the Path sequence kernel extracts.\nOne of the benefits of the proposed decomposable kernel is that, by learning its parameters, we can determine if the discriminating information is contained in the structure or symbol level of the sequences. To evaluate this we generated a second toy data set Figure 4. The data-set consists of 10 noisy sine and square waves. Half of the sequences from each class have been altered such that 5 symbols at random places take the value 4. We will conduct two experiments on this data. In the first we will learn the parameters of the kernel as to separate the sine from the square waveform, while for the second experiment, we want to separate the sequences containing the randomly positioned symbol 4 irrespective of waveform. In the first experiment the structure of the symbols are much more important while in the second the only distinguishing aspect is that the sequence contains the symbol 4. This is reflected by the learned parameters, for the first experiment the coefficent for making diagonal moves, CD, which reflects the importance of the sequences being aligned, is much higher compared to the second experiment where there is little difference between diagonal and horizontal move reflecting that the information is contained at the symbol level of the sequences. In Figure 4 the embedding and the kernel matrices are shown.\nWe will now proceed to evaluate the performance of the different kernels on a set of well known sequential classification data-sets from the UCI Machine Learning repository (Bache and Lichman, 2013) with varying length, dimension and number of classes, see Table 1. We compared three different kernels: the exponential, the path sequence kernels and the global alignment kernelCuturi et al. (2007). In each of the experiments the same exponential kernel is used to represent the symbol space such that the kernels only differ in how they represent the structural component of each sequence. Classification is performed by applying a support vector machine Chang and Lin (2011) to the space induced by the various kernel. The parameters of the kernels and the classifier are learned using nestedcross validation with 3 outer and inner folds and 3 and 20 repetitions respectively. The outer cross-validation iterates through different divisions of the data into training and test\nsets. The inner cross-validation uses the training set to perform parameter selection, using the established number of folds and repetitions. The chosen parameters are finally used to test the model on the test set and the results are measured and averaged over the previously described number of folds and repetitions.\nTo make sure that the discriminative information in the data resides in the structure and not only in the first order statistics of the symbol space we tried to classify the fixed length data by assuming each dimension to be independent. To do so we concatenated each symbol in a sequence and used an exponential kernel and a Euclidean distance as a similarity measure. The results are shown in Table 6. As can be seen, the performance for the Libras and the different PEMS data-sets are roughly random indicating that the structure is indeed important. Comparing the three different kernels, we can see that the path sequence kernel is consistently outperforming the other two kernels. It is interesting to see the significant difference in performance between the exponential and the path sequence kernels. We argue that the difference in performance is due to the stationary characteristics of the exponential kernel where the influence of each symbol match only depends on the difference in position. The path kernel has a more fine-grained characteristic where the actual position of a match incluences the similarity score. Both the path and the global alignment kernel take all possible alignments into account. As we see, the path kernel significantly outperforms the global alignment kernel. This can be explained by the dominating influence of the \u201cbest\u201d alignment in the global alignment kernel compared to the path kernel which more gracefully accumulates information from all possible alignments into the final kernel value. This behavior also means that there is a strong preference towards sequences of the same length for the global alignment kernel which is something that can explain the big difference\nin performance compared with the path kernel on the AUSLAN data-set. We believe this shows the value of a non-stationary structured kernel for representing sequences. The path perspective provides an intuitive, rigorous and interpretable formulation for designing such new kernels."}, {"heading": "6. Conclusion", "text": "In this paper we have presented an approach to combine kernels in a structured manner such that the resulting measurement represent a Mercer kernel. This leads to kernels that provides a (dis)similarity measure between sequences of different length. In particular we proved that a recently proposed (dis)similarity measure (Baisero et al., 2013) falls within this family which allows us to adopt the intuitive notion of alignments which we believe will be provide useful insights for designing new kernels. We showed experimentally how the path sequence kernel significantly outperforms previous state-of-the-art methods. In future work we aim to further establish the path perspective and discuss how new novel kernels which include higher-order paths which takes more than simple moves into account can be designed."}], "references": [{"title": "The Path Kernel", "author": ["Andrea Baisero", "Florian T Pokorny", "Danica Kragic", "Carl Henrik Ek"], "venue": "In International Conference on Pattern Recognition Applications and Methods,", "citeRegEx": "Baisero et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baisero et al\\.", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Rational kernels: Theory and algorithms", "author": ["Corinna Cortes", "Patrick Haffner", "Mehryar Mohri"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cortes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2004}, {"title": "Fast Global Alignment Kernels", "author": ["Marco Cuturi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cuturi.,? \\Q2010\\E", "shortCiteRegEx": "Cuturi.", "year": 2010}, {"title": "A Kernel for Time Series Based on Global Alighments", "author": ["Marco Cuturi", "Jean-Philippe Vert", "Oystein Birkenes", "Tomoko Matsui"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing, pages II\u2013413\u2013II\u2013416. IEEE,", "citeRegEx": "Cuturi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cuturi et al\\.", "year": 2007}, {"title": "A technique for computer detection and correction of spelling errors", "author": ["Fred J Damerau"], "venue": "Communications of the ACM,", "citeRegEx": "Damerau.,? \\Q1964\\E", "shortCiteRegEx": "Damerau.", "year": 1964}, {"title": "Hand movement recognition for Brazilian Sign Language: A study using distance-based neural networks", "author": ["D B Dias", "R C B Madeo", "T Rocha", "H H Biscaro", "S M Peres"], "venue": "In Neural Networks,", "citeRegEx": "Dias et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dias et al\\.", "year": 2009}, {"title": "Feature space interpretation of SVMs with indefinite kernels", "author": ["B Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Haasdonk.,? \\Q2005\\E", "shortCiteRegEx": "Haasdonk.", "year": 2005}, {"title": "Distributional structure. In Papers in Structural and Transformational Linguistics, pages 775\u2013794", "author": ["Zelig Harris"], "venue": "D. Reidel Publishing", "citeRegEx": "Harris.,? \\Q1970\\E", "shortCiteRegEx": "Harris.", "year": 1970}, {"title": "Convolution kernels on discrete structures", "author": ["David Haussler"], "venue": "Technical report, University of California at Santa Cruz,", "citeRegEx": "Haussler.,? \\Q1999\\E", "shortCiteRegEx": "Haussler.", "year": 1999}, {"title": "Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "author": ["M W Kadous"], "venue": "PhD thesis,", "citeRegEx": "Kadous.,? \\Q2002\\E", "shortCiteRegEx": "Kadous.", "year": 2002}, {"title": "Gaussian Processes for Object Categorization", "author": ["Ashish Kapoor", "Kirsten Grauman", "Raquel Urtasun", "Trevor Darrell"], "venue": "International journal of computer vision,", "citeRegEx": "Kapoor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2009}, {"title": "Multidimensional curve classification using passing-through regions", "author": ["Mineichi Kudo", "Jun Toyama", "Masaru Shimbo"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Kudo et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 1999}, {"title": "A Study on the Dynamic Time Warping in Kernel Machines", "author": ["Hansheng Lei", "Bingyu Sun"], "venue": "In Third International IEEE Conference on Signal-Image Technologies and InternetBased System SITIS,", "citeRegEx": "Lei and Sun.,? \\Q2007\\E", "shortCiteRegEx": "Lei and Sun.", "year": 2007}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "author": ["V I Levenshtein"], "venue": "Soviet physics doklady,", "citeRegEx": "Levenshtein.,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Text classification using string kernels", "author": ["H Lodhi", "C Saunders", "J Shawe-Taylor", "N Cristianini", "C Watkins"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Weighted Automata Algorithms. In Handbook of weighted automata, pages 213\u2013254", "author": ["Mehryar Mohri"], "venue": null, "citeRegEx": "Mohri.,? \\Q2009\\E", "shortCiteRegEx": "Mohri.", "year": 2009}, {"title": "Dynamic Time Warping", "author": ["Meinard M\u00fcller"], "venue": null, "citeRegEx": "M\u00fcller.,? \\Q2007\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2007}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K I Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Similarity measures for sequential data", "author": ["Konrad Rieck"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,", "citeRegEx": "Rieck.,? \\Q2011\\E", "shortCiteRegEx": "Rieck.", "year": 2011}, {"title": "Efficient Computation of gapped substring kernels for large alphabets", "author": ["J Rousu", "J Shawe-Taylor"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rousu and Shawe.Taylor.,? \\Q2006\\E", "shortCiteRegEx": "Rousu and Shawe.Taylor.", "year": 2006}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H Sakoe", "S Chiba"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sakoe and Chiba.,? \\Q1978\\E", "shortCiteRegEx": "Sakoe and Chiba.", "year": 1978}, {"title": "Path kernels and multiplicative updates", "author": ["Eiji Takimoto", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}, {"title": "Extracting Motion Primitives from Natural Handwriting Data", "author": ["Ben H Williams", "Marc Toussaint", "Amos J Storkey"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Williams et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "In (Rieck, 2011) the author reviews and broadly categorizes sequential similarity measures into three main categories: bag-of-words, edit-distance and string-kernel based methods.", "startOffset": 3, "endOffset": 16}, {"referenceID": 8, "context": "Bag-of-words (Harris, 1970) based similarity measures translate the notion of a sequence to a distribution over certain sub-sequences (i.", "startOffset": 13, "endOffset": 27}, {"referenceID": 15, "context": "String kernels (Lodhi et al., 2002; Rousu and ShaweTaylor, 2006) were proposed in Computational Linguistics, where data consists of sequences (text) of discrete symbols (letters).", "startOffset": 15, "endOffset": 64}, {"referenceID": 2, "context": "String kernels are a specific instance of a larger class of kernel functions referred to as rational kernels (Cortes et al., 2004).", "startOffset": 109, "endOffset": 130}, {"referenceID": 16, "context": "Rational kernels are related to weighted automata (Mohri, 2009) and define inner products from the specific sequential structure described by the automata.", "startOffset": 50, "endOffset": 63}, {"referenceID": 0, "context": "c) We show that a recently proposed intuitive (dis)similarity measure on sequences (Baisero et al., 2013), is positive definite kernel and falls into our class.", "startOffset": 83, "endOffset": 105}, {"referenceID": 21, "context": "A popular similarity measure closely related to edit distances is Dynamic Time Warping (Sakoe and Chiba, 1978; M\u00fcller, 2007).", "startOffset": 87, "endOffset": 124}, {"referenceID": 17, "context": "A popular similarity measure closely related to edit distances is Dynamic Time Warping (Sakoe and Chiba, 1978; M\u00fcller, 2007).", "startOffset": 87, "endOffset": 124}, {"referenceID": 13, "context": "One approach has been to use the dynamic time warping distance inside a radial basis exponential kernel function (Lei and Sun, 2007; Bahlmann et al., 2002).", "startOffset": 113, "endOffset": 155}, {"referenceID": 7, "context": "Even though non-positive kernels have been shown to be useful (Haasdonk, 2005) in practice, they lack a geometrical interpretation and the mathematical justification which makes the use of kernel methods so appealing.", "startOffset": 62, "endOffset": 78}, {"referenceID": 4, "context": "Motivated by the intuition for the definition of dynamic time warping, (Cuturi et al., 2007) developed a related similarity measure which in fact corresponds to a valid kernel function for sequences.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "Some of these issues have been addressed in (Cuturi, 2010) where only a subset of the possible alignments contributes to the inner-product.", "startOffset": 44, "endOffset": 58}, {"referenceID": 0, "context": "Another approach was taken in (Baisero et al., 2013), where we proposed a (dis)similarity measure called the Path kernel.", "startOffset": 30, "endOffset": 52}, {"referenceID": 9, "context": "According to Lemma 1, (Haussler, 1999), the kernel L(X,Y ) = \u2211 x\u2208X,y\u2208Y K(x, y)", "startOffset": 22, "endOffset": 38}, {"referenceID": 0, "context": "Note that, the Path kernel defined in (Baisero et al., 2013) is not related to the special Rational kernel proposed in (Takimoto and Warmuth, 2003), which is also referred to as a Path kernel", "startOffset": 38, "endOffset": 60}, {"referenceID": 22, "context": ", 2013) is not related to the special Rational kernel proposed in (Takimoto and Warmuth, 2003), which is also referred to as a Path kernel", "startOffset": 66, "endOffset": 94}, {"referenceID": 9, "context": "Note also that, while the above result readily follows from the work on convolution kernels by (Haussler, 1999), the above natural class of kernels has \u2013 to the best of our knowledge \u2013 not been studied or formulated in this manner.", "startOffset": 95, "endOffset": 111}, {"referenceID": 18, "context": "If we wish to use the kernel to represent a functional relationship f : s 7\u2192 y, where y \u2208 Rd, we can encode a preference over the mapping f by a Gaussian process (Rasmussen and Williams, 2006).", "startOffset": 162, "endOffset": 192}, {"referenceID": 18, "context": "In order to accommodate classification in a Gaussian process framework, the regression noise is usually squashed (Rasmussen and Williams, 2006) rendering the integration required to reach the marginal likelihood infeasible.", "startOffset": 113, "endOffset": 143}, {"referenceID": 11, "context": "However, it has been observed that learning the parameters for a classification task with 1\u2212C encoding, where each class is encoded using a binary variable, and a Gaussian noise assumption works well in practice (Kapoor et al., 2009).", "startOffset": 212, "endOffset": 233}, {"referenceID": 0, "context": "In (Baisero et al., 2013), the authors proposed a novel (dis)similarity measure kp : Seq(\u03a3)\u00d7 Seq(\u03a3)\u2192 R which can be defined most elegantly in a recursive fashion as,", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "In addition to the recursive formulation above, (Baisero et al., 2013) also showed that the resulting function can be written in a similar form to Eq.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "While (Baisero et al., 2013) did not provide a proof of positive definiteness, we will now show that the path kernel kp does in fact define a positive definite kernel and that kp naturally falls into the class of kernels considered here.", "startOffset": 6, "endOffset": 28}, {"referenceID": 18, "context": "By formulating the classification task as a regression problem to a 1\u2212C encoding and placing a Gaussian Process prior (Rasmussen and Williams, 2006) over the mapping, we can learn the kernel parameters through a maximum-likelihood approach.", "startOffset": 118, "endOffset": 148}, {"referenceID": 2, "context": "We compared three different kernels: the exponential, the path sequence kernels and the global alignment kernelCuturi et al. (2007). In each of the experiments the same exponential kernel is used to represent the symbol space such that the kernels only differ in how they represent the structural component of each sequence.", "startOffset": 111, "endOffset": 132}, {"referenceID": 1, "context": "Classification is performed by applying a support vector machine Chang and Lin (2011) to the space induced by the various kernel.", "startOffset": 65, "endOffset": 86}, {"referenceID": 10, "context": "From top to bottom the data sets where presented in (Kadous, 2002; Dias et al., 2009; Cuturi, 2010; Kudo et al., 1999; Williams et al., 2006).", "startOffset": 52, "endOffset": 141}, {"referenceID": 6, "context": "From top to bottom the data sets where presented in (Kadous, 2002; Dias et al., 2009; Cuturi, 2010; Kudo et al., 1999; Williams et al., 2006).", "startOffset": 52, "endOffset": 141}, {"referenceID": 3, "context": "From top to bottom the data sets where presented in (Kadous, 2002; Dias et al., 2009; Cuturi, 2010; Kudo et al., 1999; Williams et al., 2006).", "startOffset": 52, "endOffset": 141}, {"referenceID": 12, "context": "From top to bottom the data sets where presented in (Kadous, 2002; Dias et al., 2009; Cuturi, 2010; Kudo et al., 1999; Williams et al., 2006).", "startOffset": 52, "endOffset": 141}, {"referenceID": 23, "context": "From top to bottom the data sets where presented in (Kadous, 2002; Dias et al., 2009; Cuturi, 2010; Kudo et al., 1999; Williams et al., 2006).", "startOffset": 52, "endOffset": 141}], "year": 2015, "abstractText": "In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping.", "creator": "LaTeX with hyperref package"}}}