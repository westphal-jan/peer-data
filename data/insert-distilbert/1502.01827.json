{"id": "1502.01827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2015", "title": "Hierarchical Maximum-Margin Clustering", "abstract": "we do present a hierarchical maximum - margin integer clustering method for unsupervised data bandwidth analysis. our method extends beyond flat maximum - margin clustering, and performs clustering recursively efficiently in a particularly top - down manner. we propose an effective greedy splitting criteria for selecting nodes which cluster to become split next, and employ independent regularizers that instead enforce feature sharing / competition for capturing data semantics. experimental results obtained on four standard datasets show that our method constantly outperforms flat and hierarchical clustering index baselines, while forming clean and semantically meaningful cluster hierarchies.", "histories": [["v1", "Fri, 6 Feb 2015 08:37:55 GMT  (412kb,D)", "http://arxiv.org/abs/1502.01827v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["guang-tong zhou", "sung ju hwang", "mark schmidt", "leonid sigal", "greg mori"], "accepted": false, "id": "1502.01827"}, "pdf": {"name": "1502.01827.pdf", "metadata": {"source": "META", "title": "Hierarchical Maximum-Margin Clustering", "authors": ["Guang-Tong Zhou", "Sung Ju Hwang", "Mark Schmidt"], "emails": ["GZA11@CS.SFU.CA", "SJHWANG@UNIST.AC.KR", "SCHMIDTM@CS.UBC.CA", "LSIGAL@DISNEYRESEARCH.COM", "MORI@CS.SFU.CA"], "sections": [{"heading": "1. Introduction", "text": "Clustering is an important topic in machine learning that, after decades of research, remains a challenging and active topic of research. Clustering aims to group instances together based on their underlying similarity in an unsupervised manner. Clustering remains an active topic of research due to its widespread applicability in the areas of data analysis, visualization, computer vision, information retrieval, and natural language processing. Popular clustering methods include k-means clustering (Lloyd, 1982) and spectral clustering (Ng et al., 2001).\nRecent progress in maximum-margin methods has led to the development of maximum-margin clustering (MMC) techniques (Xu et al., 2004), which aim to learn both the separating hyperplanes that separate clusters of data, and the label assignments of instances to the clusters. MMC outperforms traditional clustering methods in many cases, largely due to the discriminative margin separation criterion imposed among clusters.\nHowever, MMC also has limitations. First, MMC is not particularly efficient. While efficient MMC methods have been proposed (Zhang et al., 2007; Zhao et al., 2008), even in such cases the time complexity is at least linear or quadratic with respect to the number of samples and clusters. This scalability issue is a significant problem when considering the scale of modern datasets. Second, MMC has difficulty identifying clusters with small margins, which are particularly useful for fine-grained data. Consider clustering images of commercial vehicles. In such data the major source of dissimilarity among samples is the viewpoint and this is what MMC is likely to focus on. The variations in the make of the vehicle, which are semantically more meaningful, would result in more local fine-grained differences and may be ignored by MMC\u2019s flat-clustering criterion.\nHierarchical clustering methods, which are typically based on a tree structure, have been extensively studied for their benefits over their flat clustering counterparts. These hierarchical clustering methods can discover hierarchical structures in data that better represent many real-world data distributions. Computationally, hierarchical clustering methods are also often more efficient, because one can reduce\nar X\niv :1\n50 2.\n01 82\n7v 1\n[ cs\n.L G\na single large clustering problem into a set of smaller subproblems to be recursively solved. Since within each subproblem the data only needs to be clustered into a small number of clusters, and for lower levels of the hierarchy only a small subset of the data participates in each clustering step, this procedure tends to be a lot more efficient.\nTo leverage such benefits, we propose a hierarchical extension to MMC that recursively performs k-way clustering in a top-down manner. However, instead of naively performing MMC at each clustering step, we further leverage the observation from human-defined taxonomies that each grouping/splitting decision typically focuses on different features of the data.\nSuppose, again, that we want to cluster different types of commercial vehicles. Assuming we can cluster the data hierarchically, it is sensible to assume that first we should cluster the data based on the vehicle type (e.g., truck, SUV, sedan). Once we know which sub-group each instance belongs to, we may want to employ other criteria to separate them, e.g., according to the price range or the make. We want to leverage a similar intuition to learn clusters that focus on maximizing the margin along different directions at different levels in the hierarchy. Here, directions are defined by subsets of features from the much larger feature vectors describing each instance. More specifically, we employ regularization that allows clusters to group and compete for the features at different levels. Such regularization has been made popular in semantic supervised learning in recent years (Xiao et al., 2011; Hwang et al., 2011), but here we apply the idea in an unsupervised hierarchical clustering framework.\nWe test our hierarchical maximum-margin clustering (HMMC) method on several image datasets, and show that HMMC is able to outperform flat clustering methods like MMC. More significantly, it is able to discover clean and semantically meaningful cluster hierarchies, outperforming other hierarchical clustering alternatives.\nOur contributions are threefold: (i) we present a novel hierarchical clustering algorithm based on maximum-margin clustering with an effective greedy splitting criterion for selecting which cluster to split next, (ii) we employ regularization that enforces feature sharing/competition to learn clusters that can focus on important features during clustering, and (iii) we empirically validate that our HMMC can learn semantically meaningful clusters without any human supervision."}, {"heading": "2. Related Work", "text": "Maximum-margin clustering: MMC was first proposed by Xu et al. (Xu et al., 2004). It is a maximum-margin method for clustering, analogous to support vector ma-\nchines (SVMs) for supervised learning problems, that learns both the maximum-margin hyperplane for each cluster and the clustering assignment of instances to clusters. Since this joint learning results in a non-convex formulation, unlike SVMs, it is often solved by a semidefinite relaxation (Xu et al., 2004; Valizadegan & Jin, 2006) or alternating optimization (Zhang et al., 2007). While most of the MMC methods focus on efficient optimization of the nonconvex problems, the MMC formulation was also extended to handle the case of multi-cluster clustering problems (Xu & Schuurmans, 2005; Zhao et al., 2008), and to include latent variables (Zhou et al., 2013).\nHierarchical clustering methods: Most hierarchical clustering methods employ either top-down clustering strategies that recursively split clusters into fine-grained clusters, or bottom-up clustering strategies that recursively group the smaller clusters into larger ones (Manning et al., 2008). Our method is a top-down clustering method, and the canonical example of such a method is hierarchical k-means clustering, which performs k-means recursively in a top-down manner (e.g., the bisecting k-means method (Steinbach et al., 2000)). Variations on this idea include hierarchical spectral clustering (e.g., PDDP (Boley, 1998)) which performs the hierarchical clustering on the graph Laplacian of the similarity matrix, and modelbased hierarchical clustering (Vaithyanathan & Dom, 2000; Castro et al., 2004; Goldberger & Roweis, 2004) which fits probabilistic models at each split. To the best of our knowledge, this is the first work using a maximum-margin approach for hierarchical clustering.\nSharing/competing for features: Regularization methods that promote certain structures in the parameter or feature spaces have been extensively studied in the context of regression, classification, and sparse coding. The group lasso (Meier et al., 2008) employs a mixed `1,2-norm to promote sparsity among groups of features, identifying the groups that are most important for the task. This has been applied to classification tasks like multi-task learning and multi-class classification, where it encourages the classifier(s) to share features across the tasks/classes. A generalization of the group lasso is the sparse group lasso (Friedman et al., 2010), that further encourages sparsity within each individual model.\nHowever, in some cases it makes more sense to have models fit to exclusive sets of features. The exclusive lasso (Zhou et al., 2010) encourages two models to use different features, by minimizing the `2-norm of their `1- norms. This discourages different models from having nonzero values along the same feature dimensions, encouraging each model to use features that are exclusive to their tasks. Orthogonal transfer (Xiao et al., 2011) focuses on such exclusiveness between parent and child models in a\ntaxonomy, and enforces the exclusivity through \u201corthogonal regularization\u201d where we minimize the inner product of the SVM weights for parent and child nodes. The tree of metrics approach (Hwang et al., 2011) employs similar intuition, but learns Mahalanobis metrics instead of SVM weights, and focuses on selecting sparse and disjoint features. Tree-guided group lasso (Kim & Xing, 2010) employ both sharing and exclusive regularizations, to promote sharing between the labels that belong to the same parent, while also enforcing exclusive fitting between them, guided by a predefined taxonomy. These methods consider supervised learning scenarios, while our method utilizes the grouping and exclusive regularizers for unsupervised clustering."}, {"heading": "3. Hierarchical Maximum-Margin Clustering", "text": "We propose a hierarchical clustering method based on the maximum-margin criterion. We aim to find groups of data points with a large separation between them, while forming a cluster hierarchy. The proposed method builds on the standard flat MMC clustering (Xu et al., 2004), but extends MMC in the following two aspects: (i) we introduce regularizers to encourage the different layers of the hierarchy to focus on the use of different feature subsets, and (ii) we build the hierarchy iteratively from coarse clusters to fine-grained clusters (rather than forming all clusters in one split) using a greedy top-down algorithm with a novel splitting criterion. We first introduce the HMMC formulation in this section, and then describe the optimization method in Sec. 4.\nSuppose there are T non-leaf nodes {nt}Tt=1 in the learned hierarchy. We use Dt to denote the data on nt, and HMMC splits Dt into Kt clusters by learning a linear model wtk for each cluster k. We collect the Kt cluster models in wt = {wtk}Ktk=1. We split the data Dt on node nt using the MMC idea \u2013 finding a clustering assignment such that the resultant margin between clusters is maximal over all possible assignments. By summing over all the non-leaf splits, our global HMMC objective is formulated as:\nmin w,y \u03be\u22650 T\u2211 t=1 ( \u03b1G(wt) + \u03b2E(wt) +\n1\n|Dt|Kt \u2211 xi\u2208Dt y 6=yti \u03be2tiy ) , (1)\ns.t. w>tytixi \u2212w > tyxi \u2265 1\u2212 \u03betiy, \u2200t,xi \u2208 Dt, y 6= yti\nyti \u2208 {1, . . . ,Kt}, \u2200t,xi \u2208 Dt Lt \u2264 \u2211 xi\u2208Dt \u2206(yti = y) \u2264 Ut, \u2200t, y \u2208 {1, . . . ,Kt}\nwhere w = {wt} are the cluster model parameters, yti denotes the cluster label of an instance xi on node nt, \u03be\u2019s are slack variables to allow margin violations, G(\u00b7) and E(\u00b7) are regularizers, and \u03b1 and \u03b2 are trade-off parame-\nters. Our algorithm uses MMC for each data split, where we enforce the maximum-margin criterion by constraining the score of fitting xi to its assigned cluster to be sufficiently larger than to any other cluster, using the squared hinge loss (whose smoothness simplifies the optimization). The last constraint enforces the clusters to be balanced, to avoid degenerate solutions with empty clusters and infinite margins. Here \u2206(\u00b7) is an indicator function, while Lt and Ut are the lower and upper bounds controlling the size of the clusters. As suggested in (Zhou et al., 2013), we set Lt and Ut to 0.9\n|Dt| Kt and 1.1 |Dt|Kt , respectively, to achieve roughly balanced clusters at each split. Note that HMMC jointly optimizes the model parameters w and clustering assignments y = {yti} over all splits.\nThe two regularizers G(wt) and E(wt) promote learning of a semantically meaningful cluster hierarchy. These regularizers encourage splitting on a sparse group of features at each node, but encoding a preference towards using different features at different levels of the hierarchy. While the grouping and competition among features have proved useful for encoding semantic taxonomies in supervised learning problems (Xiao et al., 2011; Hwang et al., 2011), we apply these ideas for discovering semantically meaningful cluster hierarchies in an entirely unsupervised setting.\nGroup sparsity: In the hierarchy, we would like different splits to focus on different subsets of features. Thus, in splitting a non-leaf node, we encourage the clustering process to only use a sparse set of relevant features. Considering that there are Kt cluster models at node nt, we enforce group sparsity over different feature dimensions so that the Kt models are using the same subset of features. Formally, we have the following regularizer on the split of nt:\nG(wt) = 1\nPKt P\u2211 p=1 \u221a\u221a\u221a\u221a Kt\u2211 k=1 w2tk,p, (2)\nwhere P is the feature dimension, and wtk,p is the p-th element in wtk. This term encodes that if a feature is irrelevant, then it is zero-weighted in all the Kt cluster models.\nExclusive sparsity: We also want the cluster hierarchy to use different subsets of features in different layers, so that we consider different factors when traversing the hierarchy. In other words, a split is expected to explore features that are different from its ancestors and descendants, and thus the splits compete for features at different layers. We will denote a node nt\u2019s ancestors by At, which formally is the set of nodes on the path from the root to nt. With this notation the exclusive regularizer for node nt is defined by:\nE(wt) = 1\nKt|At|P Kt\u2211 k=1 \u2211 na\u2208At P\u2211 p=1 |wtk,p| \u00b7 |waka,p|, (3)\nwhere ka indexes the child of na (na \u2208 At) on the path\nto nt. Thus, waka is the parameter vector for the ancestral cluster to which nt belongs. Eq. (3) penalizes cooperation (using the same features) and encourages competition (using different features) between a cluster model wtk and each of its ancestor models {waka}na\u2208At . The degree of competition is calculated as the element-wise multiplication of the absolute weight values. Intuitively, this means that there is no penalty if two models use different features, but using the same features results in a high penalty. Consequently, minimizing the exclusive sparsity as we split nodes will encourage nodes to use features different from those used by their ancestors and descendants. In (Xiao et al., 2011; Vervier et al., 2014), it is shown that Eq. (3) becomes convex when combined with a sufficiently large `2-regularizer."}, {"heading": "4. Optimization", "text": "The objective of Eq. (1) is non-convex due to the unknown hierarchical structure, and because we do not know the split on each node that jointly optimizes w and y. To solve the problem, we propose a greedy top-down algorithm to build the hierarchy (Sec. 4.1), and an alternating descent algorithm for splitting a node (Sec. 4.2)."}, {"heading": "4.1. Building the Hierarchy", "text": "We build the cluster hierarchy in a top-down manner, where the challenge is to iteratively find the next leaf to split. Algorithm 1 gives an overview of our greedy method. We start from the root node n1 containing all the data. Note that n1 starts as a leaf node since it has no children. Each iteration tries to split the data on each leaf node nt (Step 4), and we define the splitting score (Step 5) as:\nS(nt) =\n\u2211 xi\u2208Dt w > tyixi G(wt) + E(wt) . (4)\nThe splitting score measures how well, and how easily, the data on node nt can be clustered. The numerator of Eq. (4) summarizes the scores of fitting each instance to its assigned cluster. A high value in the numerator indicates compact clusters where the instances are well-fit by the assigned cluster models. The denominator of Eq. (4) is the regularization term indicating the complexity of the cluster models, where a small value implies a simple model. Thus, a higher splitting score means the node is a better candidate to be split.\nThe leaf node to split is choosen to greedily maximize the splitting score (Step 6). We fix the cluster models on this node, mark it as a non-leaf node, and move it to the hierarchy (Step 7). Moreover, since we are splitting this node, we generate its child nodes according to the clustering result and add the child nodes to the leaf node set for the next iteration (Steps 8 to 11). We iterate this process until the\nAlgorithm 1 HMMC: A greedy algorithm for building hierarchy Input: n1 and D . n1 is the root node carrying all data in D Output: H . the cluster hierarchy including all non-leaf nodes 1: Initialize: L \u2190 {n1}; . the current set of leaf nodes 2: while the stopping criterion is not met do 3: for nt \u2208 L do 4: cluster the data on nt; . cf. Sec. 4.2 5: compute the splitting score S(nt); . cf. Eq. (4) 6: n\u2217 \u2190 argmaxnt\u2208L S(nt); . greedily find the next split 7: L \u2190 L \\ n\u2217;H \u2190 H\u222a n\u2217; . move n\u2217 from L toH 8: for each cluster in n\u2217 do 9: create a leaf node nc carrying the data in that cluster; 10: link nc as a child of n\u2217; 11: L \u2190 L\u222anc; . add nc to the current set of leaf nodes\nstopping criterion is satisfied, which could test whether (i) a given number of leaf nodes are found, (ii) whether the sizes of all leaf nodes are sufficiently small, or (iii) whether the hierarchy reaches a height limit. To speed up this process, we cache the clustering result on each leaf node, so that we do not have to rerun the clustering once the leaf node is selected to grow the hierarchy."}, {"heading": "4.2. Splitting A Node", "text": "The clustering on a given node nt is formulated as:\nmin wt,yt \u03be\u22650\n\u03b1G(wt)+\u03b2E(wt)+ 1\n|Dt|Kt \u2211 xi\u2208Dt \u2211 y 6=yti \u03be2tiy, (5)\nwhere we omit the constraints (from Eq. (1)) for brevity. Note that the cluster models of the ancestors of nt have been fixed in the greedy top-down learning process. Thus, the exclusive regularizer E(wt) becomes a weighted `1- norm (sparsity-inducing) regularizer on wt, where the weight on each model parameter wtk,p is set based on the\nancestor nodes to \u2211 na\u2208At |waka,p|\nKt|At|P . Together with the group sparsity G(wt), this yields a weighted sparse group lasso regularizer, generalizing the sparse group lasso regularizer of Friedman et al. (Friedman et al., 2010).\nEq. (5) is still a non-convex problem due to the joint optimization over wt and yt. We use an alternating descent algorithm to reach a solution. In each iteration we fix the model parameters wt and optimize yt by solving a clustering assignment problem, and then we update wt while keeping yt fixed using a proximal quasi-Newton algorithm (Lee et al., 2012; Schmidt, 2010). The algorithm stops when the objective converges to a local optimum with respect to these steps.\nClustering assignment: With wt fixed, the problem in Eq. (5) turns out to be an assignment problem, which minimizes the total cost for labeling all instances while main-\ntaining balanced clusters:\nmin yt \u2211 xi\u2208Dt \u2206(yti = y) \u00b7 Ctiy\ufe37 \ufe38\ufe38 \ufe37\u2211 y\u2032 6=y [1\u2212w>tyxi + w>ty\u2032xi]2+, (6)\ns.t. yti \u2208 {1, . . . ,Kt}, \u2200xi \u2208 Dt Lt \u2264 \u2211 xi\u2208Dt \u2206(yti = y) \u2264 Ut, \u2200y \u2208 {1, . . . ,Kt}\nwhere Ctiy is the cost for assigning an instance xi into a cluster y. Following (Zhou et al., 2013), we could solve Eq. (6) by constructing an integer linear programming (ILP) problem withO(|Dt|\u00b7Kt) variables andO(|Dt|+Kt) constraints. However, this ILP is time-consuming since in the worst case the complexity of existing ILP solvers is exponential in the number of variables. To efficiently solve this problem, we formulate it as a minimum cost flow (MCF) problem.\nWe re-write the clustering assignment as the problem of sending an MCF through an appropriately designed network, illustrated in Fig. 1. The flow capacity of an edge from the starting node s to an instance node xi is set to 1 since we are assigning every instance into a cluster. This one unit of flow is sent from xi to a cluster node wty , to which the instance is assigned, at cost Ctiy . Finally, each cluster node sends its receiving flows to the end node e, where we limit the flow capacity in the range [Lt, Ut] to take the cluster balance constraints into account. It can be shown that clustering the |Dt| instances into Kt clusters (under the cluster balance constraints) is equivalent to sending |Dt| units of flow from s to e, and the optimal network flow corresponds to the minimum total cost of Eq. (6). To find this optimal flow, we apply the capacity scaling algorithm (Edmonds & Karp, 1972) implemented in the LEMON library (Dezs et al., 2011), which is an efficient dual solution method running in O ( |Dt| \u00b7 Kt \u00b7 log(|Dt| +\nKt) \u00b7 log(Ut \u00b7 |Dt| \u00b7Kt) )\ncomplexity. In practice, our MCF solver speeds up the ILP solver in (Zhou et al., 2013) by 10\nto 100 times.\nUpdating wt: With fixed yt, we solve for wt (a convex problem) using a proximal quasi-Newton method (Lee et al., 2012; Schmidt, 2010). This method is designed to efficiently minimize smooth losses with non-smooth but simple regularizers, and on each iteration it computes a new estimate wt by solving:\nmin wt\n\u03b1G(wt) + \u03b2E(wt) +H(woldt ) (7)\n+H \u2032(woldt ) >(wt \u2212 woldt ) +\n1\n2s \u2016wt \u2212 woldt \u20162B ,\nwhere s is a step-size set using a backtracking line-search, H(woldt ) is the squared hinge-loss (i.e., the last term of Eq. (5) after using the constraints to eliminate the slack variables) estimated with woldt from the fixed yt, H \u2032(woldt ) is the derivative ofH(woldt ) w.r.t. woldt , and \u2016z\u20162B = z>Bz is a divergence formed using the L-BFGS matrix B (Byrd et al., 1994; Nocedal, 1980).\nA spectral proximal-gradient method is used to compute an approximate minimizer of this objective. This algorithm requires the proximal operator. For our weighted sparse group lasso regularizer, we can show that solving this minimizing problem involves a two-step procedure. First, we incorporate the weighted `1-norm penalty by applying the soft-threshold operator wtk,p =\nwtk,p |wtk,p| [|wtk,p| \u2212 s\u03b2\u03bbE ]+\nto each model parameter individually, where \u03bbE =\u2211 na\u2208At\n|waka,p| Kt|At|P is the weights coming from the ancestor models in E(wt). This operator returns 0 if wtk,p = 0. Second, we incorporate the group sparsity using the groupwise soft-threshold operator wt:,p =\nwt:,p \u2016wt:,p\u20162 [\u2016wt:,p\u20162 \u2212\ns\u03b1\u03bbG]+, where wt:,p = [wt1,p, . . . ,wtKt,p] > is the grouping of Kt cluster models on a feature dimension p, and \u03bbG =\n1 PKt is the normalization term from G(wt). Note that this operator returns 0 if wt:,p = 0.\nConvergence analysis: We now show that this alternating descent algorithm converges to a local optimum. The optimization consists of two alternating steps: updating the discrete yt and the continuous wt. In the wt update, we fix the clustering yt and use a method that is guaranteed to find a global optimum (Lee et al., 2012; Schmidt, 2010). The yt update (with wt fixed) is NP-hard but we can find a solution that guarantees improvement using MCF. Since there is a finite number of possible assignments to yt, the procedure guarantees convergence to a local minimum with respect to updating wt or yt."}, {"heading": "5. Experiments", "text": "Datasets: We evaluate the performance of HMMC on four datasets from two public image collections: Animal With Attributes (AWA) (Lampert et al., 2009) and Ima-\ngeNet (Deng et al., 2009). Both collections have natural hierarchies consisting of fine-grained image classes that can be grouped into more general classes.\nAWA contains 30,475 images from 50 animal classes (e.g., bat and deer). We use two datasets following the practice of (Hwang et al., 2011). The first one, AWA-ATTR, has 85 features consisting of the outputs of 85 linear SVMs trained to predict the presence/absence of the 85 nameable properties annotated by (Lampert et al., 2009), like red and furry. The second dataset, AWA-PCA, uses the provided features (SIFT, rgSIFT, PHOG, SURF, LSS, RGB) after being concatenated, normalized, and PCA-reduced to 100 dimensions. The ground-truth hierarchy of AWA is shown in Fig. 2 of (Hwang et al., 2011).\nWe use two datasets collected from ImageNet: VEHICLE contains 20 vehicle classes (e.g., cab and canoe) and 26,624 images (Hwang et al., 2011), and IMAGENET consists of 28,957 images spanning 20 non-animal, nonvehicle classes (e.g., lamp and drum) (Hwang et al., 2012). The raw image features are the provided bag-of-words histograms obtained by SIFT (Deng et al., 2010; 2009). We also project them down to 100 dimensions with PCA. The semantic hierarchies of VEHICLE and IMAGENET are given in Fig. 3 of (Hwang et al., 2011) and Fig. 2(e) of (Hwang et al., 2012), respectively.\nBaselines: We compare HMMC with four sets of baselines. The first set is the flat clustering methods k-means (KM), spectral clustering (SC) (Ng et al., 2001), and an MMC approach implemented in (Zhou et al., 2013).\nThe second set is hierarchical bottom-up clustering (HBUC). We have tested a variety of methods including Single-Link (SL), Average-Link (AL) and Complete-Link (CL) (Manning et al., 2008). The pairwise dissimilarity between two images is measured by Euclidean distance.\nThe third set is hierarchical top-down clustering methods (HTDC). We derive variants of hierarchical k-means (HKM) and hierarchical spectral clustering (HSC) directly from our HMMC approach. HKM and HSC apply the same greedy top-down approach as HMMC, but split a given node using k-means and spectral clustering, respectively. Similar to HMMC, HKM and HSC first try splitting all the current leaf nodes, and then greedily grow the leaf with the best splitting. The splitting score on a leaf node is defined as the average within-cluster distance \u2013 minimizing this gives the most compact clusters. We also considered two other baselines, HKM-D and HSC-D. Instead of growing the leaf with the most compact clusters, HKM-D and HSC-D grow the leaf with the most scattered data, which is defined as the total distance of all instances to their center.\nThe fourth set of baselines are variants of HMMC. We change the regularization to derive HMMC-G (group spar-\nsity only), HMMC-E (exclusive sparsity only), HMMC-1 (basic `1-norm), and HMMC-2 (squared `2-norm).\nParameters: For a fair comparison of all the hierarchical top-down clustering methods, we apply the same stopping criterion: we test if the number of leaf nodes exceeds a fixed limit F . Empirically, we set F as 1, 1.5 and 2 times the number of ground-truth classes in each dataset. The number of splits on each node also has a great impact on the learned hierarchy. To compare different hierarchical clustering methods, we simply use K-nary branching for all splits in all hierarchies. We experiment with K set as 2, 3, 4 and 5, respectively. With a particular setting of F and K, we can fairly compare different hierarchical clustering methods since they perform the same number of splits and obtain the same number of leaf nodes. We use the same solver for learning HMMC and its variants, and report the best performance with both \u03b1 and \u03b2 selected from the range {10\u22124, 10\u22123, 10\u22122, 10\u22121, 100}.\nFor the HBUC baselines, we apply the same F parameter as above. However, all the HBUC methods use binary branching and there is no result for K larger than 2.\nFor the flat clustering methods (i.e., KM, SC and MMC), we set the number of clusters to F to fairly compare performance with hierarchical methods. For SC, we use a 5- nearest neighborhood graph and set the width of the Gaussian similarity function as the average distance over all the 5-nearest neighbors. This also applies in HSC and HSC-D which use SC for splitting a node.\nPerformance Measures: We evaluate all the methods by three performance measures. The first two are semantic measures focusing on how well the learned hierarchy captures the semantics in the ground-truth hierarchy. The motivation is that two semantically similar images should be grouped in the same or nearby clusters in the learned hierarchy, and two semantically dissimilar images should be split into clusters that are far away from each other.\nFor each pair of images, we compute their semantic similarity from the ground-truth hierarchy, using the following two metrics. The first shortest path metric (Harispe et al., 2013) finds the shortest path linking the two image classes in the ground-truth hierarchy, normalizes the path distance by the maximum distance, and subtracts the distance from 1 as the semantic similarity. The second path sharing metric (Fergus et al., 2010) counts the number of nodes shared by the parent branches of the two image classes, normalized by the length of the longest of the two branches. Note that we can similarly define the shortest path similarity and the path sharing similarity using the learned hierarchy, for any pair of images, by checking the leaf node(s) where the two images are clustered. For flat clustering with no hierarchy, we simply set the similarity as 1 if two images are\nfrom the same cluster, and 0 otherwise.\nTo measure the goodness of the learned hierarchy in capturing semantics, we compute the mean squared error of the learned similarity and the ground-truth semantic similarity over all pairs of images, and subtract the mean squared error from 1 as our semantic measure. Note that we have two semantic measures, the shortest path similarity (SP) and the path sharing similarity (PS). The higher the values, the better the performance.\nMoreover, we also report the Rand Index (RI) (Rand, 1971), which evaluates the percentage of true positives within clusters and true negatives between clusters. Note that RI is a commonly-used measure for flat clustering. For hierarchical clustering methods, we simply ignore the hierarchy and evaluate RI on the leaf node clustering (allowing\ndirect comparisons with flat clustering methods)."}, {"heading": "5.1. Results", "text": "Comparing flat and hierarchical methods: Due to space limitations, we only report the clustering results with F equal to the number of ground-truth classes and K = 2 (binary splitting). The results are listed in Table 1, which shows that HMMC achieves the best performance on AWAPCA, VEHICLE and IMAGENET, and competitive results on AWA-ATTR. Specifically, HMMC improves over the second best by 0.2% on AWA-ATTR, 2% on AWA-PCA, 6% on VEHICLE and 4% on IMAGENET, respectively, in terms of the semantic measure SP. This verifies that HMMC better captures the semantics in the clustered hierarchies.\nMoreover, HMMC outperforms other HTDC baselines in most cases, showing the effectiveness of our greedy topdown algorithm for hierarchy building and our alternating descent algorithm for splitting data on a given node. Note that the HBUC baselines tend to perform worse since they typically produced extremely unbalanced clusters at the top levels (e.g., a child contains only one sample). They also did not lead to semantically meaningful hierarchies.\nUsing different F and K: We also vary the parameters F (i.e., the number of leaf nodes) and K (i.e., the number of splits), and plot the SP performance in Fig. 2. Here we have omitted the poor results of hierarchical bottom-up methods for better visualizations. HMMC consistently outperforms the other baselines on AWA-PCA, VEHICLE and IMAGENET, and is comparable with HKM-D and HSC-D on AWA-ATTR. Note that the performance of HMMC is stable with regard to the different settings of F and K.\nComparing the variants of HMMC: Table 1 shows that HMMC gets slightly better performance over the four variants of HMMC. This is reasonable since HMMC produces sparse models that may better capture semantics. We also compare the model sparsity (i.e., the percentage of zeros in the learned models) in Fig. 4. Here we omit HMMC-2 since the model is always non-sparse. For a fair comparison, we fix the trade-off parameters to 1 in all models. Note that by combining the grouping and exclusive regularizers, HMMC is sparser than HMMC-G and HMMC-E. HMMC-\n1 sometimes has slightly better sparsity than HMMC, but the performance is limited due to the lack of semantics.\nRuntime comparison: Table 1 also reports the runtime results. Our implementation of HMMC is between 1.4 to 8 times faster than MMC, showing the efficiency of the hierarchical method. Note that HMMC is more expensive than other hierarchical and flat methods. This is reasonable since HMMC needs to solve a more expensive optimization problem during clustering.\nVisualizations: Fig. 3 visualizes the learned hierarchy on AWA-ATTR. See the caption for details. Our model captures semantically meaningful attributes in building the hierarchy \u2013 note how the attribute \u201cquadrapedal\u201d is used to separate \u201cwhales\u201d and \u201cpolar bears\u201d, and how \u201clongneck\u201d is used to divide \u201crhinos\u201d and \u201cgiraffes\u201d."}, {"heading": "6. Conclusion", "text": "We have presented a hierarchical clustering method for unsupervised construction of taxonomies. We develop a greedy top-down splitting criterion, and use the grouping and exclusive regularizers for building semantically meaningful hierarchies from unsupervised data. Our method makes use of maximum-margin learning, and we propose effective algorithms to solve the resultant non-convex objective. We test our method on four standard datasets, showing the efficacy of our method in clustering, and the ability to capture semantics via the hierarchies."}], "references": [{"title": "Principal direction divisive partitioning", "author": ["Boley", "Daniel"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Boley and Daniel.,? \\Q1998\\E", "shortCiteRegEx": "Boley and Daniel.", "year": 1998}, {"title": "Representations of quasi-Newton matrices and their use in limited memory methods", "author": ["Byrd", "Richard H", "Nocedal", "Jorge", "Schnabel", "Robert B"], "venue": "Mathematical Programming,", "citeRegEx": "Byrd et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 1994}, {"title": "Likelihood based hierarchical clustering", "author": ["Castro", "Rui M", "Coates", "Mark", "Nowak", "Robert D"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Castro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2004}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "What does classifying more than 10, 000 image categories tell us", "author": ["Deng", "Jia", "Berg", "Alexander C", "Li", "Kai", "Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "LEMON - an open source C++ graph template library", "author": ["Dezs", "Bal\u00e1zs", "J\u00fcttner", "Alp\u00e1r", "Kov\u00e1cs", "P\u00e9ter"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "Dezs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dezs et al\\.", "year": 2011}, {"title": "Theoretical improvements in algorithmic efficiency for network flow problems", "author": ["Edmonds", "Jack", "Karp", "Richard M"], "venue": "Journal of ACM,", "citeRegEx": "Edmonds et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Edmonds et al\\.", "year": 1972}, {"title": "Semantic label sharing for learning with many categories", "author": ["Fergus", "Robert", "Bernal", "Hector", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In ECCV,", "citeRegEx": "Fergus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2010}, {"title": "A note on the group lasso and a sparse group", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "lasso. CoRR,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Hierarchical clustering of a mixture model", "author": ["Goldberger", "Jacob", "Roweis", "Sam T"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Semantic measures for the comparison of units of language, concepts or entities from text and knowledge base analysis", "author": ["Harispe", "S\u00e9bastien", "Ranwez", "Sylvie", "Janaqi", "Stefan", "Montmain", "Jacky"], "venue": "CoRR, abs/1310.1285,", "citeRegEx": "Harispe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Harispe et al\\.", "year": 2013}, {"title": "Learning a tree of metrics with disjoint visual features", "author": ["Hwang", "Sung Ju", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In NIPS,", "citeRegEx": "Hwang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2011}, {"title": "Semantic kernel forests from multiple taxonomies", "author": ["Hwang", "Sung Ju", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In NIPS,", "citeRegEx": "Hwang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Lampert", "Christoph H", "Nickisch", "Hannes", "Harmeling", "Stefan"], "venue": "In CVPR,", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Proximal Newton-type methods for convex optimization", "author": ["Lee", "Jason D", "Sun", "Yuekai", "Saunders", "Michael A"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Least squares quantization in PCM", "author": ["Lloyd", "Stuart P"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd and P.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd and P.", "year": 1982}, {"title": "Introduction to Information Retrieval", "author": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "The group lasso for logistic regression", "author": ["Meier", "Lukas", "Geer", "Sara Van De", "Buhlmann", "Peter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["Nocedal", "Jorge"], "venue": "Mathematics of computation,", "citeRegEx": "Nocedal and Jorge.,? \\Q1980\\E", "shortCiteRegEx": "Nocedal and Jorge.", "year": 1980}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["Rand", "William M"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand and M.,? \\Q1971\\E", "shortCiteRegEx": "Rand and M.", "year": 1971}, {"title": "Graphical Model Structure Learning with `1Regularization", "author": ["Schmidt", "Mark"], "venue": "PhD thesis,", "citeRegEx": "Schmidt and Mark.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt and Mark.", "year": 2010}, {"title": "A comparison of document clustering techniques", "author": ["Steinbach", "Michael", "Karypis", "George", "Kumar", "Vipin"], "venue": "In KDD Workshop on Text Mining,", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Model-based hierarchical clustering", "author": ["Vaithyanathan", "Shivakumar", "Dom", "Byron"], "venue": "In UAI,", "citeRegEx": "Vaithyanathan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Vaithyanathan et al\\.", "year": 2000}, {"title": "Generalized maximum margin clustering and unsupervised kernel learning", "author": ["Valizadegan", "Hamed", "Jin", "Rong"], "venue": "In NIPS,", "citeRegEx": "Valizadegan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Valizadegan et al\\.", "year": 2006}, {"title": "On learning matrices with orthogonal columns or disjoint supports", "author": ["Vervier", "Kevin", "Mah\u00e9", "Pierre", "D\u2019Aspremont", "Alexandre", "Veyrieras", "Jean-Baptiste", "Vert", "Jean-Philippe"], "venue": "In ECML/PKDD,", "citeRegEx": "Vervier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vervier et al\\.", "year": 2014}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["Xiao", "Lin", "Zhou", "Dengyong", "Wu", "Mingrui"], "venue": "In ICML,", "citeRegEx": "Xiao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}, {"title": "Unsupervised and semisupervised multi-class support vector machines", "author": ["Xu", "Linli", "Schuurmans", "Dale"], "venue": "In AAAI,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Maximum margin clustering", "author": ["Xu", "Linli", "Neufeld", "James", "Larson", "Bryce", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2004}, {"title": "Maximum margin clustering made practical", "author": ["Zhang", "Kai", "Tsang", "Ivor W", "Kwok", "James T"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "Efficient multiclass maximum margin clustering", "author": ["Zhao", "Bin", "Wang", "Fei", "Zhang", "Changshui"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}, {"title": "Latent maximum margin clustering", "author": ["Zhou", "Guang-Tong", "Lan", "Tian", "Vahdat", "Arash", "Mori", "Greg"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Zhou", "Yang", "Jin", "Rong", "Hoi", "Steven C. H"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "Popular clustering methods include k-means clustering (Lloyd, 1982) and spectral clustering (Ng et al., 2001).", "startOffset": 92, "endOffset": 109}, {"referenceID": 28, "context": "Recent progress in maximum-margin methods has led to the development of maximum-margin clustering (MMC) techniques (Xu et al., 2004), which aim to learn both the separating hyperplanes that separate clusters of data, and the label assignments of instances to the clusters.", "startOffset": 115, "endOffset": 132}, {"referenceID": 29, "context": "While efficient MMC methods have been proposed (Zhang et al., 2007; Zhao et al., 2008), even in such cases the time complexity is at least linear or quadratic with respect to the number of samples and clusters.", "startOffset": 47, "endOffset": 86}, {"referenceID": 30, "context": "While efficient MMC methods have been proposed (Zhang et al., 2007; Zhao et al., 2008), even in such cases the time complexity is at least linear or quadratic with respect to the number of samples and clusters.", "startOffset": 47, "endOffset": 86}, {"referenceID": 26, "context": "Such regularization has been made popular in semantic supervised learning in recent years (Xiao et al., 2011; Hwang et al., 2011), but here we apply the idea in an unsupervised hierarchical clustering framework.", "startOffset": 90, "endOffset": 129}, {"referenceID": 11, "context": "Such regularization has been made popular in semantic supervised learning in recent years (Xiao et al., 2011; Hwang et al., 2011), but here we apply the idea in an unsupervised hierarchical clustering framework.", "startOffset": 90, "endOffset": 129}, {"referenceID": 28, "context": "(Xu et al., 2004).", "startOffset": 0, "endOffset": 17}, {"referenceID": 28, "context": "Since this joint learning results in a non-convex formulation, unlike SVMs, it is often solved by a semidefinite relaxation (Xu et al., 2004; Valizadegan & Jin, 2006) or alternating optimization (Zhang et al.", "startOffset": 124, "endOffset": 166}, {"referenceID": 29, "context": ", 2004; Valizadegan & Jin, 2006) or alternating optimization (Zhang et al., 2007).", "startOffset": 61, "endOffset": 81}, {"referenceID": 30, "context": "While most of the MMC methods focus on efficient optimization of the nonconvex problems, the MMC formulation was also extended to handle the case of multi-cluster clustering problems (Xu & Schuurmans, 2005; Zhao et al., 2008), and to include latent variables (Zhou et al.", "startOffset": 183, "endOffset": 225}, {"referenceID": 31, "context": ", 2008), and to include latent variables (Zhou et al., 2013).", "startOffset": 41, "endOffset": 60}, {"referenceID": 16, "context": "Hierarchical clustering methods: Most hierarchical clustering methods employ either top-down clustering strategies that recursively split clusters into fine-grained clusters, or bottom-up clustering strategies that recursively group the smaller clusters into larger ones (Manning et al., 2008).", "startOffset": 271, "endOffset": 293}, {"referenceID": 22, "context": ", the bisecting k-means method (Steinbach et al., 2000)).", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": ", PDDP (Boley, 1998)) which performs the hierarchical clustering on the graph Laplacian of the similarity matrix, and modelbased hierarchical clustering (Vaithyanathan & Dom, 2000; Castro et al., 2004; Goldberger & Roweis, 2004) which fits probabilistic models at each split.", "startOffset": 153, "endOffset": 228}, {"referenceID": 17, "context": "The group lasso (Meier et al., 2008) employs a mixed `1,2-norm to promote sparsity among groups of features, identifying the groups that are most important for the task.", "startOffset": 16, "endOffset": 36}, {"referenceID": 8, "context": "A generalization of the group lasso is the sparse group lasso (Friedman et al., 2010), that further encourages sparsity within each individual model.", "startOffset": 62, "endOffset": 85}, {"referenceID": 32, "context": "The exclusive lasso (Zhou et al., 2010) encourages two models to use different features, by minimizing the `2-norm of their `1norms.", "startOffset": 20, "endOffset": 39}, {"referenceID": 26, "context": "Orthogonal transfer (Xiao et al., 2011) focuses on such exclusiveness between parent and child models in a", "startOffset": 20, "endOffset": 39}, {"referenceID": 11, "context": "The tree of metrics approach (Hwang et al., 2011) employs similar intuition, but learns Mahalanobis metrics instead of SVM weights, and focuses on selecting sparse and disjoint features.", "startOffset": 29, "endOffset": 49}, {"referenceID": 28, "context": "The proposed method builds on the standard flat MMC clustering (Xu et al., 2004), but extends MMC in the following two aspects: (i) we introduce regularizers to encourage the different layers of the hierarchy to focus on the use of different feature subsets, and (ii) we build the hierarchy iteratively from coarse clusters to fine-grained clusters (rather than forming all clusters in one split) using a greedy top-down algorithm with a novel splitting criterion.", "startOffset": 63, "endOffset": 80}, {"referenceID": 31, "context": "As suggested in (Zhou et al., 2013), we set Lt and Ut to 0.", "startOffset": 16, "endOffset": 35}, {"referenceID": 26, "context": "While the grouping and competition among features have proved useful for encoding semantic taxonomies in supervised learning problems (Xiao et al., 2011; Hwang et al., 2011), we apply these ideas for discovering semantically meaningful cluster hierarchies in an entirely unsupervised setting.", "startOffset": 134, "endOffset": 173}, {"referenceID": 11, "context": "While the grouping and competition among features have proved useful for encoding semantic taxonomies in supervised learning problems (Xiao et al., 2011; Hwang et al., 2011), we apply these ideas for discovering semantically meaningful cluster hierarchies in an entirely unsupervised setting.", "startOffset": 134, "endOffset": 173}, {"referenceID": 26, "context": "In (Xiao et al., 2011; Vervier et al., 2014), it is shown that Eq.", "startOffset": 3, "endOffset": 44}, {"referenceID": 25, "context": "In (Xiao et al., 2011; Vervier et al., 2014), it is shown that Eq.", "startOffset": 3, "endOffset": 44}, {"referenceID": 8, "context": "(Friedman et al., 2010).", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "In each iteration we fix the model parameters wt and optimize yt by solving a clustering assignment problem, and then we update wt while keeping yt fixed using a proximal quasi-Newton algorithm (Lee et al., 2012; Schmidt, 2010).", "startOffset": 194, "endOffset": 227}, {"referenceID": 31, "context": "Following (Zhou et al., 2013), we could solve Eq.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "To find this optimal flow, we apply the capacity scaling algorithm (Edmonds & Karp, 1972) implemented in the LEMON library (Dezs et al., 2011), which is an efficient dual solution method running in O ( |Dt| \u00b7 Kt \u00b7 log(|Dt| + Kt) \u00b7 log(Ut \u00b7 |Dt| \u00b7Kt) ) complexity.", "startOffset": 123, "endOffset": 142}, {"referenceID": 31, "context": "In practice, our MCF solver speeds up the ILP solver in (Zhou et al., 2013) by 10 to 100 times.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Updating wt: With fixed yt, we solve for wt (a convex problem) using a proximal quasi-Newton method (Lee et al., 2012; Schmidt, 2010).", "startOffset": 100, "endOffset": 133}, {"referenceID": 1, "context": "w t , and \u2016z\u2016B = z>Bz is a divergence formed using the L-BFGS matrix B (Byrd et al., 1994; Nocedal, 1980).", "startOffset": 71, "endOffset": 105}, {"referenceID": 14, "context": "In the wt update, we fix the clustering yt and use a method that is guaranteed to find a global optimum (Lee et al., 2012; Schmidt, 2010).", "startOffset": 104, "endOffset": 137}, {"referenceID": 13, "context": "Datasets: We evaluate the performance of HMMC on four datasets from two public image collections: Animal With Attributes (AWA) (Lampert et al., 2009) and Ima-", "startOffset": 127, "endOffset": 149}, {"referenceID": 3, "context": "geNet (Deng et al., 2009).", "startOffset": 6, "endOffset": 25}, {"referenceID": 11, "context": "We use two datasets following the practice of (Hwang et al., 2011).", "startOffset": 46, "endOffset": 66}, {"referenceID": 13, "context": "The first one, AWA-ATTR, has 85 features consisting of the outputs of 85 linear SVMs trained to predict the presence/absence of the 85 nameable properties annotated by (Lampert et al., 2009), like red and furry.", "startOffset": 168, "endOffset": 190}, {"referenceID": 11, "context": "2 of (Hwang et al., 2011).", "startOffset": 5, "endOffset": 25}, {"referenceID": 11, "context": ", cab and canoe) and 26,624 images (Hwang et al., 2011), and IMAGENET consists of 28,957 images spanning 20 non-animal, nonvehicle classes (e.", "startOffset": 35, "endOffset": 55}, {"referenceID": 12, "context": ", lamp and drum) (Hwang et al., 2012).", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "The raw image features are the provided bag-of-words histograms obtained by SIFT (Deng et al., 2010; 2009).", "startOffset": 81, "endOffset": 106}, {"referenceID": 11, "context": "3 of (Hwang et al., 2011) and Fig.", "startOffset": 5, "endOffset": 25}, {"referenceID": 12, "context": "2(e) of (Hwang et al., 2012), respectively.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": "The first set is the flat clustering methods k-means (KM), spectral clustering (SC) (Ng et al., 2001), and an MMC approach implemented in (Zhou et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 31, "context": ", 2001), and an MMC approach implemented in (Zhou et al., 2013).", "startOffset": 44, "endOffset": 63}, {"referenceID": 16, "context": "We have tested a variety of methods including Single-Link (SL), Average-Link (AL) and Complete-Link (CL) (Manning et al., 2008).", "startOffset": 105, "endOffset": 127}, {"referenceID": 10, "context": "The first shortest path metric (Harispe et al., 2013) finds the shortest path linking the two image classes in the ground-truth hierarchy, normalizes the path distance by the maximum distance, and subtracts the distance from 1 as the semantic similarity.", "startOffset": 31, "endOffset": 53}, {"referenceID": 7, "context": "The second path sharing metric (Fergus et al., 2010) counts the number of nodes shared by the parent branches of the two image classes, normalized by the length of the longest of the two branches.", "startOffset": 31, "endOffset": 52}], "year": 2015, "abstractText": "We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximummargin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies.", "creator": "LaTeX with hyperref package"}}}