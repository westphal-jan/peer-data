{"id": "1502.02233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2015", "title": "Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature", "abstract": "... in this paper we primarily describe a novel framework for the discovery of the desired topical content of a diverse data corpus, and the tracking of its complex structural changes across the temporal dimension. in contrast to previous work our model does not impose a prior on the rate limit at which documents are added to the corpus network nor does it adopt the markovian assumption which overly restricts determining the type of changes that the process model can capture. our paramount key technical contribution is a framework based on ( i ) discretization of time into epochs, ( v ii ) epoch - wise topic / discovery using a hierarchical dirichlet process - map based model, and ( iii ) a collaborative temporal similarity graph which allows for the modelling analyses of complex topic changes : emergence persistence and disappearance, evolution, activation and splitting and merging. the crucial power construction of the proposed framework is demonstrated on the medical literature current corpus concerned mostly with the autism suffered spectrum disorder ( asd ) - an increasingly important research subject of significant social and healthcare humanities importance. in addition to the collected asd literature corpus which we will make freely available, our contributions also include two free online tools we built as aids to asd researchers. these can be used for semantically verified meaningful navigation and searching, as well as quantitative knowledge discovery from this large and rapidly growing corpus of literature.", "histories": [["v1", "Sun, 8 Feb 2015 10:25:20 GMT  (2343kb,D)", "http://arxiv.org/abs/1502.02233v1", "In Proc. Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), 2015"]], "COMMENTS": "In Proc. Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), 2015", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["adham beykikhoshk", "ognjen arandjelovic", "dinh phung", "svetha venkatesh"], "accepted": false, "id": "1502.02233"}, "pdf": {"name": "1502.02233.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Dirichlet Process for Tracking Complex Topical Structure Evolution and Its Application to Autism Research Literature", "authors": ["Adham Beykikhoshk", "Ognjen Arandjelovi\u0107", "Dinh Phung", "Svetha Venkatesh"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The autism spectrum disorder (ASD) is a life-long neurodevelopmental disorder with poorly understood causes on the one hand, and a wide range of potential treatments supported by little evidence on the other. The disorder is characterized by severe impairments in social interaction, communication, and in some cases cognitive abilities. Considering the social and economic burden of ASD it is unsurprising that it has been attracting an increasing amount of research attention which has resulted in a rapid growth of the relevant corpus of literature. Navigating this vast amount of data by conventional, manual means is difficult and limiting; yet the rapid rise in the diagnosis rate of ASD demands timely research on its aetiology and treatment. Consequently, the potential benefit of tools based on novel data-mining and machine learning techniques is immense [1]. More meaningful ways for visualising or searching for data could provide invaluable information in clinical and administrative decision making as well as aid research, while automatic knowledge discovery would in its own right ar X iv :1\n50 2.\n02 23\n3v 1\n[ cs\n.I R\n] 8\nF eb\n2 01\nadvance the understanding of the underlying phenomena (e.g. epidemiological patterns). We describe a novel method which contributes towards this goal.\nMore specifically, we describe a general framework for the analysis of medical literature capable of (i) discovering the underlying topical structure, (ii) inferring the relationships between different discovered topics, and (iii) tracking the evolution of topics over time. The proposed framework uses the hierarchical Dirichlet process (HDP) to extract topics automatically, and then constructs a similarity graph over them using an inter-topic similarity measure; topic evolution over time can be inferred from this graph. The effectiveness of our approach is demonstrated on the specific example of a large longitudinal data corpus of medical literature on ASD which we collected. This corpus includes more than 18,000 articles published over the course of 42 years. In addition to the aforementioned technical contributions, our further contribution is this corpus which will be made public following the publication of the present paper.\nThe results we report on the collected ASD literature corpus illustrate the usefulness of our method and its ability to extract and track over time abstract topical knowledge, inferring the point at which a certain topic comes into existence, how its evolves, splits into multiple new topics or merges with the existing ones, and lastly when it ceases to exist. This is demonstrated on examples of well-known research directions in the field, making out work the first to examine the medical literature on ASD using advanced topic modelling tools. Our additional contributions come in the form of two free online tools which allow researchers to (i) navigate and search the literature in a semantically meaningful manner (see http://www.undersdtanfigutism.tk), and (ii) understand the development and relationships between different ideas which permeate research in the domain of ASD (see http://www.goo.gl/Ws7V64)."}, {"heading": "2 Previous work", "text": "In this section we review the most relevant previous work on topic modelling. We focus our attention first on latent topic models which have dominated the field in the last decade, and then on biomedical text mining, given the application domain within which our framework is evaluated in Section 4."}, {"heading": "2.1 Latent topic models", "text": "An important early topic modelling approach is the latent semantic indexing (LSI) [2] which remains popular. Two notable limitations of LSI are its inability to deal effectively with polysemy and to produce an explicit description of the latent space. A probabilistic improvement overcomes these by explicitly characterizing the latent space with semantic topics, and by employing a probabilistic generative model that addresses the polysemy problem [3]. Nevertheless, probabilistic LSI is prone to parameter overfitting caused by an uncontrolled growth in the number of parameters as the document corpus is increased. In addition, the necessary assignment of probabilities to documents is a nontrivial task [4].\nThe recently proposed latent Dirichlet allocation (LDA) method [4] overcomes the overfitting problem by adopting a Bayesian framework and a generative process at the document level. While LDA has quickly become a standard tool for topic modelling, it too experiences challenges when applied on real-world data. In particular, being a parametric model the number of desired output topics has to be specified in advance. The HDP model as the nonparametric counterpart of LDA was introduced by Teh et al. [5] and addressed this limitation by using a Dirichlet process (DP) (as opposed to a Dirichlet distribution) as the prior on topics. Therefore, each document is modelled using an infinite mixture model, allowing the data to inform the complexity of the model and infer the number of resulting topics automatically. We discuss this model in further detail in Section 3.\nTemporal topic modelling A notable limitation of most models described in the existing literature lies in their assumption that the data corpus is static; this includes those based on LDA mentioned previously, or the hierarchical Dirichlet process described in detail in the next section. However, in many practical applications documents are added to the corpus in a temporal manner and their ordering has significance (non-exchangeability property). As a consequence, the topical structure of the corpus changes over time. The assumption made by all previous work, and indeed adopted by us, is that documents are not exchangeable at large temporal scales but are at short time scales, thus treating the corpus at temporally locally static.\nThe existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori. Methods of the first group discretize time into epochs, apply a static topic model to each epoch, and by making the Markovian assumption relate the parameters of each epoch\u2019s topic model to those of the epochs adjacent to it in time [6,7,9,10]. While the approach we propose in this paper adopts the idea of time discretization, it diverges in its other features from this group of methods thereafter. In particular, instead of employing the Markovian assumption we describe a novel structure in form of a temporal similarity graph, which gives our method greater flexibility, as described in detail in the next section. The second group of methods in the literature regard document time-stamps as observations of a continuous random variable [8,11]. This assumption severely limits the type of topic changes which can be described. For example, as opposed to our model, these models are not capable of describing the evolution of topics, or their splitting and merging, and are rather constrained to tracking simple topic popularity (rise/fall)."}, {"heading": "2.2 Biomedical text mining", "text": "Most previous work on text-based knowledge discovery has rather focused on (i) the tagging of names of entities such as genes, proteins, and diseases [12], (ii) the discovery of relationships between different entities e.g. functional associations\nbetween genes [13], or (iii) the extraction of information pertaining to events such as gene expression or protein binding [14].\nThe idea that the medical literature could be mined for new knowledge is typically attributed to Swanson [15]. For example by manually examining medical literature databases he hypothesised that dietary fish oil could be beneficial for Raynaud\u2019s syndrome patients, which was later confirmed by experimental evidence. Work that followed sought to develop statistical methods which would make this process automatic. Most approaches adopted the use of term frequencies and co-occurrences using dictionaries such as Medical Subject Headings (MeSH) [16].\nMost existing work on biomedical knowledge discovery is based on what may be described as traditional data mining techniques (neural networks, support vector machines etc); comprehensive surveys can be found in [17,14]. The application of state-of-the-art Bayesian methods in this domain is scarce. Amongst the notable exceptions is the work by Blei et al. who showed how latent Dirichlet allocation (LDA) can be used to facilitate the process of hypothesis generation in the context of genetics [18]. Arnold et al. used a similar approach to demonstrate that abstract topic space representation is effective in patient-specific case retrieval [19]. In their later work they introduced a temporal model which learns topic trends and showed that the inferred topics and their temporal patterns correlate with valid clinical events and their sequences [20]. Wu et al. used LDA for gene-drug relationship ranking [21]."}, {"heading": "3 Proposed framework", "text": "We begin this section by reviewing the relevant theory underlying HDP mixture modelling which plays the central rule in the proposed framework. Then we turn our attention to the main technical contribution of our work and explain how the HDP is employed to discover the topical content of a literature corpus and track its structural changes over time."}, {"heading": "3.1 Hierarchical Dirichlet process mixture models", "text": "The Dirichlet process is a useful prior for mixture modelling which allows a document collection to accommodate a potentially infinite number of topics. It is the building block of Bayesian nonparametric methods. A Dirichlet process [22] DP (\u03b3,H) is defined as a distribution of a random probability measure G over a measure space (\u0398,B, \u00b5), such that for any finite measurable partition (A1, A2, . . . , Ar) of \u0398 the random vector (G (A1) , . . . , G (Ar)) is a Dirichlet distribution with parameters (\u03b3H (A1) , . . . , \u03b3H (Ar)). An alternative view of the DP emerges from the so-called stick-breaking process which adopts a constructive approach using a sequence of discrete draws [23]. Specifically, if G \u223c DP (\u03b3,H) then G = \u2211\u221e k=1 \u03b2k\u03b4\u03c6k where \u03c6k\niid\u223c H and \u03b2 = (\u03b2k)\u221ek=1 is the vector of weights obtained as \u03b2k = vk \u220fk\u22121 l=1 (1\u2212 vl) and vl\niid\u223c Beta (1, \u03b3). Owing to the discrete nature and infinite dimensionality of its draws, the DP is a highly useful prior for Bayesian mixture models. By associating different\nmixture components with atoms \u03c6k of the stick-breaking process, and assuming xi|\u03c6k\niid\u223c F (xi|\u03c6k) where F (.) is the likelihood kernel of the mixing components, we can formulate the Dirichlet process mixture model (DPM). The DPM is suitable for nonparametric clustering of exchangeable data in a single group e.g. words in a document where the DPM models the underlying structure of the document with potentially an infinite number of topics. However, many realworld problems are more appropriately modelled as comprising multiple groups of exchangeable data (e.g. a collection of documents). In such cases it is usually desirable to model the observations of different groups jointly, allowing them to share their generative clusters. This idea is known as the sharing statistical strength and is achieved using a hierarchical structure.\nAmongst different ways of linking group-level DPMs, HDP [5] offers an interesting solution whereby base measures of document-level DPs are drawn from another DP. In this way the atoms of the corpus-level DP (i.e. topics in our case) are shared across the corpus. Formally, if x = {x1, . . . ,xJ} is a document collection where xj = { xj1, . . . , xjNj } is the j-th document comprising Nj words, each document is modelled with a DPM Gj |\u03b10, G0 iid\u223c DP (\u03b10, G0) where its DP prior is further endowed by another DP G0|\u03b3,H \u223c DP (\u03b3,H). This is illustrated schematically in Figure 1a. Since the base measure of Gj is drawn from G0, it takes the same support as G0. Also the parameters of the group-level mixture components, \u03b8ji, share their values with the corpus-level DP support on {\u03c61, \u03c62, . . .}. Therefore Gj can be equivalently expressed using the stick-breaking process as Gj = \u2211\u221e k=1 \u03c0jk\u03b4\u03c6k where \u03c0j |\u03b10, \u03b3 \u223c DP (\u03b10, \u03b3)[5]. The posterior for \u03b8ji has been shown to follow a Chinese restaurant franchise process which can be used to develop inference algorithms based on Gibbs sampling [5]."}, {"heading": "3.2 Modelling topic evolution over time", "text": "In this section we show how the described HDP-based model can be applied to the analysis of temporal topic changes in a longitudinal data corpus. We begin by dividing the literature corpus by time into multiple epochs. Each epoch is then modelled separately using an HDP. Different epochs\u2019 models are inferred using the same initial corpus-level base measure and hyperparameters. Hence if n is the number of epochs, we obtain n sets of topics \u03b8 = {\u03b8t1 , . . . ,\u03b8tn} where \u03b8t = {\u03b81,t, . . . , \u03b8Kt,t} is the set of topics that describe epoch t, and Kt their number (which is inferred automatically, as described previously). This is illustrated in Figure 1b. In the next section we describe how given an inter-topic similarity measure the evolution of different topics across epochs can be tracked."}, {"heading": "3.3 Measuring topic similarity", "text": "Our goal now is to track changes in the topical structure of a data corpus over time. The simplest changes of interest include the emergence of new topics, and the disappearance of others. More subtly, we are also interested in how a specific topic changes \u2013 how it evolves over time in terms of the contributions of different words it comprises, as well as how it splits into new topics or merges\nwith the existing ones. Clearly this information can provide valuable insight into the refinement of ideas and findings in the scientific community, effected by new research and accumulating evidence.\nThe key idea behind our approach stems from the observation that while topics may change significantly over time, by their very nature the change between successive epochs is limited. Therefore we infer the continuity of a topic in one epoch by relating it to all topics in the immediately subsequent epoch which are sufficiently similar to it under some similarity measure. This can be seen to lead naturally to a similarity graph representation whose nodes correspond to topics and whose edges link those topics in two epochs which are related. Formally, the weight of the directed edge that links \u03c6j,t , the j-th topic in epoch t, and \u03c6k,t+1 is set equal to \u03c1 (\u03c6j,t, \u03c6k,t+1) where \u03c1 is an appropriate similarity measure. Given that in our HDP-based model each topic is represented by a probability distribution, suitable similarity metrics include the Jaccard similarity, the Jenson-Shannon divergence, or the L2-norm for example.\nA conceptual illustration of a similarity graph is shown in Figure 2a. It shows three consecutive time epochs t \u2212 1, t, and t + 1 and a selection of topics in these epochs. Graph edge weight i.e. inter-topic similarity is encoded by varying the thickness of the corresponding line connecting two nodes \u2013 a thicker line signifies more similar topics. We use a threshold to eliminate automatically weak edges, retaining only the edges which correspond to sufficiently similar topics in adjacent epochs. It can be seen that this readily allows us to detect the disappearance of a particular topic, the emergence of new topics, as well as the splitting or merging of different topics:\nEmergence If a node does not have any edges incident to it, the corresponding topic is taken as having emerged in the associated epoch (e.g. \u03c6j+2 at time t in Figure 2a).\nDisappearance If no edges originate from a node, the corresponding topic is taken to vanish in the associated epoch (e.g. \u03c6j at time t in Figure 2a).\nSplitting If more than a single edge originates from a node, the corresponding topic is understood as being split into multiple topics in the next epoch (e.g. \u03c6i is split into \u03c6j and \u03c6j+1 in Figure 2a).\nMerging If more than a single edge is incident to a node, the topics of the nodes from which the edges originate are understood as having merged together to form a new topic (e.g. \u03c6i and \u03c6i+1 merge to form \u03c6j+1 in Figure 2a)."}, {"heading": "4 Experimental evaluation", "text": "Having introduced the main technical contribution of our work we now illustrate its usefulness on the example of ASD literature analysis, and describe additional contributions in the form of two free online tools that we developed to aid ASD researchers."}, {"heading": "4.1 Data collection", "text": "To the best of our knowledge there are no publicly available corpora of ASDrelated medical literature. Hence we collected a comprehensive dataset ourselves, which will be made public following the acceptance of the present paper. We describe our collection methodology and the pre-processing of data we performed to extract standard features used for text analysis.\nRaw data collection We used the PubMed search engine that allows users to access the US National Library of Medicine for abstracts and references of life science and biomedical scholarly articles. We assumed a paper is related to ASD if the term \u201cautism\u201d is present in its title or abstract, and collected only papers written in English. The earliest publication fitting our criteria is that by Kanner [24], and we collected all matching publications up to the final one indexed by PubMed on 24th July 2014, yielding a corpus of 20,138 publications. We discarded the 1,946 which do not have an abstract indexed, ending with the total of 18,192 papers in our dataset. We used the abstracts text to evaluate our method.\nData pre-processing Following the standard practice in text processing literature we applied soft lemmatization on the abstracts in our dataset, using the freely available WordNet tool [25]. No stemming was performed to avoid potential distortion of words which is sometimes effected by heuristic rules used by stemming algorithms. After lemmatization and the removal of so-called stop words, we obtained 1.9 million terms in the entire corpus when repetitions are counted, and 37,278 unique terms. We construct the vocabulary for our method by selecting the subset of the most frequent unique terms which explain 90% of the energy of the corpus, which resulted in a 3,738 term vocabulary."}, {"heading": "4.2 Proposed method implementation", "text": "We divided the 42 year timespan of our data corpus into overlapping five year epochs, with a two year lag between consecutive epochs, resulting in 18 epochs in total. The topics of each epoch were then extracted as described in Section 3.2 and their dynamics inferred as per Section 3.3. The number of latent topics of different epoch is plotted in Figure 2b. Notice the exponential rise in the number of topics which mirrors the exponential increase in the number of publications over time in our dataset. This increasing interest in ASD can be illustrated by the observation that in 2013 there are five times as many publications as in 2000. For our inter-topic similarity described in Section 3.3 we adopted the use of the well-known Jaccard similarity; this similarity measure was used to obtain all results reported in this section. Lastly, Gibbs sampling was used for HDP inference, implemented in Python 2.7, with hyperparameter resampling as described by Teh et al. [5]."}, {"heading": "4.3 Case study 1: ASD and genetics", "text": "While the exact aetiology of the ASD is still poorly understood, the existence of a significant genetic component is beyond doubt [26]. Work on understanding complex genetic factors affecting the development of autism, which possibly involve multiple genes which interact with each other and the environment, is a major theme of research and as such a good case study on which the usefulness of the proposed method can be illustrated.\nWe started by identifying the topic of interest as that with the highest probability of the terms \u201cgene\u201d or \u201cgenetic\u201d conditioned on the topic, and tracing it back in time to the epoch in which it originated. This led to the discovery of the relevant topic in the epoch spanning the period 1986\u20131991. Figure 4 shows the evolution of this topic from 1992 revealed by our method (due to space constraints only the most significant parts of the similarity graph are shown; minor\nchanges to the topic before 1992 are also omitted for clarity, as indicated by the dotted line in the figure). Each topic is labelled with its first few dominant terms. The following interpretation of our findings is readily apparent. Firstly, in the period 1992\u20131997, the topic is rather general in nature. Over time it evolves and splits into topics which concern more specific concepts (recall that such splitting of topics cannot be captured by any of the existing methods). For example by the epoch 2002\u20132007 the single original topic has evolved and split into four topics which concern:\n\u2013 the relationship between mutations in the gene mecp2 (essential for normal functioning of neurone), and mental disorders and epilepsy (it is estimated that one third of ASD individuals also have epilepsy),\n\u2013 gene alternations, for example the duplication of 15q11\u201313 and deletion of 16p11.2 both of which are associated with ASD,\n\u2013 genetic linkage association analysis and heritability of autism, and\n\u2013 observational work on autistic twins and probands with siblings on the spectrum.\nOur framework also allows us to look \u2018back\u2019 in time. For example, by examining the topics that the 1992 genetics topic originate from we discovered that the topic evolved from the early concept of \u201cinfantile ASD\u201d [24]."}, {"heading": "4.4 Case study 2: ASD and vaccination", "text": "For our second case study we chose to examine research on the relationship between ASD development and vaccination. This subject has attracted much attention both in the research community, as well as in the media and the general public. The controversy was created with the publication of the work by Wakefield [27] which reported epidemiological findings linking MMR vaccination and the development of autism and colitis. Despite the full retraction of the article following the discovery that it was fraudulent, and numerous subsequent studies who failed to show the claimed link, a significant portion of the general public remains concerned with the issue.\nAs in the previous example, we begun by identifying the topic with the highest probability of the terms \u201cvaccine\u201d and \u201cvaccination\u201d conditioned on the topic, and tracing it back to the epoch in which it first emerged. Again, a single topic was readily identified, in the epoch spanning the period 1996\u20132001. Notice that this is consistent with the publication date of the first relevant publication by Wakefield [27]. The evolution of the topic is illustrated in Figure 5 in the same way as in the previous section. It can be seen that the original topic concerned the subjects initially brought to attention such as \u201cmeasles\u201d, \u201cvaccine\u201d, and \u201cautism\u201d. In the subsequent epoch, when the original claim was still thought to have credibility, the topic evolves and splits into numerous others mirroring research directions taken by various researchers. Following this period and the revelations of its fraudulence, the topic assumes mainly single-threaded evolution, at times incorporating various originally separate ideas. For example observe the independent emergence of the term \u201cmercury\u201d. Though initially unrelated to it this topic merges with the topic that concerns vaccination which can be explained by the widely publicized thiomersal (vaccine preservative) controversy (again note that such merging of topics cannot be captured by the existing methods). Al-\nthough rejected by the medical community due to a lack of evidence, this topic can be seen as persisting to date."}, {"heading": "4.5 Topic browser", "text": "A topic model can be seen as a dimensionality reduction framework that reduces documents into a topic space. This transformation of data can provide powerful insight and allow for the browsing of documents in a more subject-specific, semantic manner. For example by describing documents in the topic space, documents most related to a particular topic of interest can be readily identified and retrieved. To provide this functionality to the research community interested in ASD we used the framework described in this paper to model the entire literature corpus we collected, and built a website to facilitate free and ready use of our model and data. Researchers can use our online tool to browse topics, annotate them, and navigate through publications by topic. The website is available at http://www.understandingautism.tk."}, {"heading": "5 Conclusions", "text": "We described a novel framework for temporal modelling of the topical structure of a longitudinal document corpus. Our approach consists of discretizing time into overlapping epochs, modelling the static topic structure within each epoch using an HDP, and tracking the evolution of topics over time using an intertopic similarity measure. The resultant similarity graph captures relationships between topics in different epochs and allows for the automatic inference of the time of emergence and disappearance of topics, their evolution over time, merging and splitting. The power of the proposed general framework was demonstrated on the example of ASD-related medical literature. On two case studies which concern two important research issues in ASD literature we demonstrated that our method extracts meaningful topics and their temporal changes. A novel data corpus and free online tools are made freely available to researchers."}], "references": [{"title": "Datamining Twitter and the autism spectrum disorder: a pilot study", "author": ["A. Beykikhoshk", "O. Arandjelovi\u0107", "D. Phung", "S. Venkatesh", "T. Caelli"], "venue": "ASONAM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JASIS 41", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR 3", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association 101", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Continuous time dynamic topic models", "author": ["C. Wang", "D. Blei", "D. Heckerman"], "venue": "UAI", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Topics over time: a non-Markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "SIGKDD", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "The dynamic hierarchical Dirichlet process", "author": ["L. Ren", "D.B. Dunson", "L. Carin"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolutionary hierarchical Dirichlet processes for multiple correlated time-varying corpora", "author": ["J. Zhang", "Y. Song", "C. Zhang", "S. Liu"], "venue": "SIGKDD", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A nonparametric mixture model for topic modeling over time", "author": ["A. Dubey", "A. Hefny", "S. Williamson", "E.P. Xing"], "venue": "SDM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "ABNER: an open Source tool for automatically tagging genes, proteins and other entity names in text", "author": ["B. Settles"], "venue": "Bioinformatics 21", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A cancer microarray database and integrated data-mining platform", "author": ["D.R. Rhodes", "J. Yu", "K. Shanker", "N. Deshpande", "R. Varambally", "D. Ghosh", "T. Barrette", "A. Pander", "A.M. Chinnaiyan"], "venue": "Neoplasia 6", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Biomedical text mining: a survey of recent progress", "author": ["M.S. Simpson", "D. Demner-Fushman"], "venue": "Mining text data.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Undiscovered public knowledge", "author": ["D.R. Swanson"], "venue": "Library Quarterly 56", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1986}, {"title": "Medical subject headings", "author": ["F.B. Rogers"], "venue": "Bull Med Libr Assoc 51", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1963}, {"title": "Biomedical Literature Mining", "author": ["V.D. Kumar", "H.J. Tipney"], "venue": "Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical modeling of biomedical corpora: mining the Caenorhabditis genetic center bibliography for genes related to life span", "author": ["D.M. Blei", "K. Franks", "M.I. Jordan", "I.S. Mian"], "venue": "BMC Bioinformatics 7", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Clinical case-based retrieval using latent topic analysis", "author": ["C.W. Arnold", "S.M. El-Saden", "A.A. Bui", "R. Taira"], "venue": "AMIA 2010", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A topic model of clinical reports", "author": ["C.W. Arnold", "W. Speier"], "venue": "SIGIR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Ranking gene-drug relationships in biomedical literature using latent Dirichlet allocation", "author": ["Y. Wu", "M. Liu", "W. Zheng", "Z. Zhao", "H. Xu"], "venue": "Pacific Symposium on Biocomputing", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1973}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Technical report, DTIC Document", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1991}, {"title": "Irrelevant and metaphorical language in early infantile autism", "author": ["L. Kanner"], "venue": "American Journal of Psychiatry 103", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1946}, {"title": "WordNet: An online lexical database", "author": ["G.A. Miller", "R. Beckwith", "C.D. Fellbaum", "D. Gross", "K. Miller"], "venue": "Int J Lexicograph 1", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Autism spectrum disorders \u2013 a genetics review", "author": ["J.H. Miles"], "venue": "Nature 13", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children", "author": ["A.J. Wakefield", "S.H. Murch", "A. Anthony"], "venue": "The Lancet", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Consequently, the potential benefit of tools based on novel data-mining and machine learning techniques is immense [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "An important early topic modelling approach is the latent semantic indexing (LSI) [2] which remains popular.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "A probabilistic improvement overcomes these by explicitly characterizing the latent space with semantic topics, and by employing a probabilistic generative model that addresses the polysemy problem [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "In addition, the necessary assignment of probabilities to documents is a nontrivial task [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "The recently proposed latent Dirichlet allocation (LDA) method [4] overcomes the overfitting problem by adopting a Bayesian framework and a generative process at the document level.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "[5] and addressed this limitation by using a Dirichlet process (DP) (as opposed to a Dirichlet distribution) as the prior on topics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "The existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori.", "startOffset": 132, "endOffset": 139}, {"referenceID": 6, "context": "The existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori.", "startOffset": 132, "endOffset": 139}, {"referenceID": 7, "context": "The existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori.", "startOffset": 132, "endOffset": 139}, {"referenceID": 8, "context": "The existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori.", "startOffset": 157, "endOffset": 163}, {"referenceID": 9, "context": "The existing work on temporal topic modelling can be divided into two groups of approaches both of which can be based on parametric [6,7,8] or nonparametric [9,10] techniques, the former suffering from the limitation that they contain free parameters which must be set a priori.", "startOffset": 157, "endOffset": 163}, {"referenceID": 5, "context": "Methods of the first group discretize time into epochs, apply a static topic model to each epoch, and by making the Markovian assumption relate the parameters of each epoch\u2019s topic model to those of the epochs adjacent to it in time [6,7,9,10].", "startOffset": 233, "endOffset": 243}, {"referenceID": 6, "context": "Methods of the first group discretize time into epochs, apply a static topic model to each epoch, and by making the Markovian assumption relate the parameters of each epoch\u2019s topic model to those of the epochs adjacent to it in time [6,7,9,10].", "startOffset": 233, "endOffset": 243}, {"referenceID": 8, "context": "Methods of the first group discretize time into epochs, apply a static topic model to each epoch, and by making the Markovian assumption relate the parameters of each epoch\u2019s topic model to those of the epochs adjacent to it in time [6,7,9,10].", "startOffset": 233, "endOffset": 243}, {"referenceID": 9, "context": "Methods of the first group discretize time into epochs, apply a static topic model to each epoch, and by making the Markovian assumption relate the parameters of each epoch\u2019s topic model to those of the epochs adjacent to it in time [6,7,9,10].", "startOffset": 233, "endOffset": 243}, {"referenceID": 7, "context": "The second group of methods in the literature regard document time-stamps as observations of a continuous random variable [8,11].", "startOffset": 122, "endOffset": 128}, {"referenceID": 10, "context": "The second group of methods in the literature regard document time-stamps as observations of a continuous random variable [8,11].", "startOffset": 122, "endOffset": 128}, {"referenceID": 11, "context": "Most previous work on text-based knowledge discovery has rather focused on (i) the tagging of names of entities such as genes, proteins, and diseases [12], (ii) the discovery of relationships between different entities e.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "between genes [13], or (iii) the extraction of information pertaining to events such as gene expression or protein binding [14].", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "between genes [13], or (iii) the extraction of information pertaining to events such as gene expression or protein binding [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "The idea that the medical literature could be mined for new knowledge is typically attributed to Swanson [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "Most approaches adopted the use of term frequencies and co-occurrences using dictionaries such as Medical Subject Headings (MeSH) [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Most existing work on biomedical knowledge discovery is based on what may be described as traditional data mining techniques (neural networks, support vector machines etc); comprehensive surveys can be found in [17,14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 13, "context": "Most existing work on biomedical knowledge discovery is based on what may be described as traditional data mining techniques (neural networks, support vector machines etc); comprehensive surveys can be found in [17,14].", "startOffset": 211, "endOffset": 218}, {"referenceID": 17, "context": "who showed how latent Dirichlet allocation (LDA) can be used to facilitate the process of hypothesis generation in the context of genetics [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "used a similar approach to demonstrate that abstract topic space representation is effective in patient-specific case retrieval [19].", "startOffset": 128, "endOffset": 132}, {"referenceID": 19, "context": "In their later work they introduced a temporal model which learns topic trends and showed that the inferred topics and their temporal patterns correlate with valid clinical events and their sequences [20].", "startOffset": 200, "endOffset": 204}, {"referenceID": 20, "context": "used LDA for gene-drug relationship ranking [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "A Dirichlet process [22] DP (\u03b3,H) is defined as a distribution of a random probability measure G over a measure space (\u0398,B, \u03bc), such that for any finite measurable partition (A1, A2, .", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "An alternative view of the DP emerges from the so-called stick-breaking process which adopts a constructive approach using a sequence of discrete draws [23].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "Amongst different ways of linking group-level DPMs, HDP [5] offers an interesting solution whereby base measures of document-level DPs are drawn from another DP.", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "Therefore Gj can be equivalently expressed using the stick-breaking process as Gj = \u2211\u221e k=1 \u03c0jk\u03b4\u03c6k where \u03c0j |\u03b10, \u03b3 \u223c DP (\u03b10, \u03b3)[5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The posterior for \u03b8ji has been shown to follow a Chinese restaurant franchise process which can be used to develop inference algorithms based on Gibbs sampling [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 23, "context": "The earliest publication fitting our criteria is that by Kanner [24], and we collected all matching publications up to the final one indexed by PubMed on 24th July 2014, yielding a corpus of 20,138 publications.", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Data pre-processing Following the standard practice in text processing literature we applied soft lemmatization on the abstracts in our dataset, using the freely available WordNet tool [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "While the exact aetiology of the ASD is still poorly understood, the existence of a significant genetic component is beyond doubt [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "For example, by examining the topics that the 1992 genetics topic originate from we discovered that the topic evolved from the early concept of \u201cinfantile ASD\u201d [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 26, "context": "The controversy was created with the publication of the work by Wakefield [27] which reported epidemiological findings linking MMR vaccination and the development of autism and colitis.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "Notice that this is consistent with the publication date of the first relevant publication by Wakefield [27].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, and splitting and merging. The power of the proposed framework is demonstrated on the medical literature corpus concerned with the autism spectrum disorder (ASD) \u2013 an increasingly important research subject of significant social and healthcare importance. In addition to the collected ASD literature corpus which we will make freely available, our contributions also include two free online tools we built as aids to ASD researchers. These can be used for semantically meaningful navigation and searching, as well as knowledge discovery from this large and rapidly growing corpus of literature.", "creator": "LaTeX with hyperref package"}}}