{"id": "1502.02277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2015", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches", "abstract": "term frequency normalization is a potentially serious issue since lengths of documents are various. generally, documents become shorter long due to two closely different reasons - verbosity and multi - topicality. first, verbosity means that broadly the same basic topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well - summarized one. second, multi - topicality indicates that a newspaper document has advocated a broad discussion of multi - topics, rather separately than single topic. currently although these document characteristics should be differently being handled, all previous methods of designing term frequency format normalization have ignored these differences and have now used a far simplified length - driven approach which usually decreases the term frequency by only the length of a document, causing an extended unreasonable penalization. to playfully attack this problem, we propose a novel tf normalization computation method which is a type of partially - axiomatic approach. we first consider formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi - topicality characteristic, respectively. then, we modify language frequency modeling approaches to better satisfy these two constraints, and derive entirely novel smoothing methods. experimental results show that the proposed method increases significantly twice the precision likelihood for keyword queries, and substantially improves map ( mean and average precision ) for verbose queries.", "histories": [["v1", "Sun, 8 Feb 2015 17:32:44 GMT  (34kb)", "http://arxiv.org/abs/1502.02277v1", "8 pages, conference paper, published in ECIR '08"]], "COMMENTS": "8 pages, conference paper, published in ECIR '08", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["seung-hoon na", "in-su kang", "jong-hyeok lee"], "accepted": false, "id": "1502.02277"}, "pdf": {"name": "1502.02277.pdf", "metadata": {"source": "CRF", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches", "authors": ["Seung-Hoon Na", "In-Su Kang", "Jong-Hyeok Lee"], "emails": ["Korea@postech.ac.kr", "nsh1979@postech.ac.kr", "jhlee@postech.ac.kr", "Korea@kisti.re.kr", "dbaisk@kisti.re.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n02 27\n7v 1\n[ cs\n.I R\n] 8\nF eb"}, {"heading": "1 Introduction", "text": "The highly-performed retrieval models rely on two different factors - TF (term frequency) and IDF (inverse document frequency). Among them, TF factor becomes a non-trivial, since long-length documents may increase term frequency, different to shortlength ones, so that the naive estimation of term frequency would not be successful. Thus, term frequency of long-length documents should be seriously considered. Regarding this, Singhal observed the following two different types of reasons for making the length of a document long [1] 3.\n1. High term frequency: The same term repeatedly occurs in a long-length document. As a result, the term frequency factors may be large for long documents, increasing the average contribution of its terms towards the query-document similarity.\n3 Robertson and Walker mentioned two types of reasons as scope hypothesis and verbosity hypothesis, respectively [2].\n2. More terms: Long-length document has large size of vocabulary. This increases the number of matches between a query and a long document, increasing the querydocument similarity, and the chances of retrieval of long documents in preference over shorter documents.\nWithout loss of meaning, we can conceptualize these two reasons as verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, making term frequencies high. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic, making more terms. Using these concepts, we divide long-length documents into two different ideal types - verbose documents and multi-topical documents. Verbose document is the document which becomes long mainly due to verbosity, rather than multi-topicality, while multi-topical document is the document which follows typical characteristics of multi-topicality, rather than verbosity.\nSinghal pre-assumed that long-length documents should be penalized regardless of whether or not their types are verbosity (or multi-topicality) [1]. Basically, their approach belongs to a simplified length-driven method which decreases the term frequency of all long-length documents according to documents\u2019 length factor only. However, we insist that this Singhal\u2019s pre-assumption would be failed. We argue that the penalization should be applied to verbose document only, not to multi-topical document. As a main reason, terms in a multi-topical document are less repeated than ones in a verbose document, since the length of the multi-topical document is increased due to its broad topics. However, Singhal missed this point that these types of documents should be differently handled. Therefore, the retrieval function adopting Singhal\u2019s penalization will make multi-topical documents unreasonably less-preferred, causing an unfair retrieval ranking.\nTo clearly support our argument for verbose document and multi-topical document, we will exemplify two different situations to discuss different tendencies of term frequencies in verbose document and multi-topical document. First, let us examine the situation by considering two different document samples of D1 and D2 which have the same term frequency ratio.\nD1: Language modeling approach D2: Language modeling approach Language modeling approach\nD2 is twice the concatenation of D1. Suppose that a query is given by \u201clanguage modeling approach\u201d. Then, a question arises as \u201cwhich one of D1 and D2 is more relevant?\u201d. By comparing the contained information, we know that two documents have the exactly same contents, although the length of D2 is twice than that of D1. Thus, D1 and D2 should have the same relevance score. However, the absolute term frequency of D2 is twice than that of D1, thus, the naive TF \u00b7 IDF prefers D2 to D1. To avoid this unfair comparison, we should introduce a TF normalization. To this end, suppose that l is the length of documents, and t f is the term frequency of a query term. Then, one reasonable strategy of TF normalization is to use t f n = t f/l, instead of t f . Then, the modified TF \u00b7 IDF produces the same score for D1 and D2. Note that Singhal\u2019s pivoted length normalization will also well-work since t f n can be well-reflected in Singhal\u2019s original formula. Remark that D2 is a verbose document, not a multi-topical document,\nwhich is the main reason for the success of the normalization. Now, we examine the second situation by considering a multi-topical document sample D3, which contains all topics of D1 and D2 as a subpart.\nD3: Information retrieval model Language modeling approach\nHere, D3 describes a broad topic - \u201cinformation retrieval model\u201d, and contain \u201clanguage modeling approach\u201d as a subtopic. Again, suppose that the same query of \u201clanguage modeling approach\u201d is given. Consider the question about \u201cwhat relevance score should assigned to D3 be, compared with D1 and D2?\u201d. D3 contains all contents of D1 and D2, although D3 is different from D1 and D2. In this case, if user sees D3, he or she would think that D3 is also relevant, because all relevant content - D1 - is embedded to D3. From this viewpoint, D3 should have the same score as D1 and D2 (due to a partial relevance). However, if we apply the previous version of TF-normalization (i.e. t f n = t f/l) to D3, then D3 is much-less preferred to D1 and D2, since its term frequency of a query term is the same as D1 but its length is twice than that of D1. Of course, Singhal\u2019s method will assign less-score to D3 than D1 and D2. The mean reason of this failure is that D3 is not a verbose document but a multi-topical document. This result means that TF normalization problem is more complex, at least requiring the different strategies according to types of long-length documents. To avoid the unreasonable penalization for multi-topical ones, TF normalization problem should be more deeply re-investigated by discriminating multi-topical documents from verbose documents.\nTo obtain a more accurate TF normalization, we propose a novel TF normalization method which is a type of axiomatic approach. We try to modify language modeling approach as a case study without the loss of its elegance and principle. To this end, we first formulate two constraints that the retrieval scoring functions should satisfy for verbose and multi-topical documents, respectively. Then, we present the analysis result that previous language modeling approaches do not sufficiently satisfy these constraints. After that, we modify the language modeling approaches such that better satisfy these two constraints, derive a novel smoothing methods, and evaluate the proposed ones."}, {"heading": "2 Formal Constraints of New TF Normalization, and Analysis of Previous Language Modeling Approaches", "text": ""}, {"heading": "2.1 Constraints", "text": "From now on, we assume that \u03c4(D) is a measurement for calculating the number of topics in document D. We define K-verbosity and N-topicality as follows.\nDefinition (K-verbosity): Suppose that D1 and D2 are given. Let t f1(w) and t f2(w) be the term frequency of term w in D1 and D2, respectively. For all term w, if t f2(w) = K \u00b7 t f1(w) and \u03c4(D1) = \u03c4(D2), then D2 has K-verbosity to D1 or D2 is K-verbose to D1.\nDefinition (N-topicality): Suppose that D1 and D2 are given as \u03c4(D2) = N \u00b7 \u03c4(D1). Let l1 and l2 be the length of D1 and D2, respectively. If for all term w in D1, t f2(w)/l2 = t f1(w)/l1/N, then D2 has N-topicality to D1 and D2 is N-topical to D1.\nIn our three samples from the introduction, D2 has 2-verbosity to D1, and D3 has 2- topicality to D1. Remind that we have argued that D1, D2 and D3 should have the same\nrelevance score. This argument can be re-formulated to following two constraints - VNC and TNC which the retrieval function should satisfy for two cases when one document has K-verbosity and N-topicality to another document, respectively. Let score(Q,D) be a similarity function between a document D and a query Q.\nVNC (Verbosity Normalization Constraint): Suppose a pair of D1 and D2. If D2 is K-verbose to D1, then score(Q,D1) = score(Q,D2).\nTNC (Topicality Normalization Constraint): Suppose a pair of D1 and D2. If D2 is N-topicality to D1, then score(Q,D1) = score(Q,D2).\nThese constraints can be directly utilized to derive a new class of retrieval function as Fang\u2019s exploration [3]. Originally, Fang formulated two constraints related to term frequency - LNC1 and LNC2 [3]. Among them, LNC2 is highly relevant to VNC, where VNC is a more specific constraint - VNC entails LNC2, not vice versa. TNC is a new constraint which is not connected to Fang\u2019s any constraint. Note that our exploration of a retrieval function is different from Fang\u2019s one. We focus on only few constraints related to our issue, without identifying all constraints. Then, we select as the backbone model one among a previous well-performed retrieval model, and modify it to better satisfy the focused few constraints, without losing the elegance and the principle of the original model. In this regard, our exploration method belongs to the partially-axiomatic approach - 1) using partial constraints rather than full constraints, 2) using the restricted functional space which the backbone retrieval model can allows, rather than relying on full functional space. In contrast, Fang\u2019s approach is the fully-axiomatic approach [3, 4]. In Fang\u2019s approach, full constraints are completely identified as well as the focused constraints. A new class of retrieval function is explored as one in separate functional space which is not related to previous retrieval models. However, the fully-axiomatic approach such as Fang\u2019s exploration approach requires un-principled heuristics which are not derived from a well-designed retrieval model. A partially-axiomatic approach doesn\u2019t need to discard the well-founded retrieval model such as language modeling approach, enabling us to pursue a more elaborated retrieval model, without losing its mathematical elegance and principles."}, {"heading": "2.2 Analysis of Language Modeling Approaches", "text": "We selected the language modeling approaches as the backbone retrieval model [5]. Our goal is to modify the language modeling approaches such that better satisfies the proposed two constraints - VNC and TNC. We investigate two popular smoothing methods - Jelinek-Mercer smoothing (JM) and Dirichlet-prior smoothing (Dir) [6]. Before modifying them, we begin by discussing whether or not each smoothing method satisfies VNC and TNC in this subsection. Notations used in this paper are summarized as follows:\nQ A given query t fD(w) Term frequency of w in document D lD Length of document D t fC(w) Term frequency of w of collection lC Total term frequency of collection \u03b8D Smoothed document language model of D \u03b8\u0302D Unsmoothed document language model of D (MLE) \u03b8C Collection language model (MLE)\nAnalysis of Jelinek-Mercer Smoothing In JM (Jeliner-Mercer Smoothing), a smoothed document model is obtained by the interpolation of MLE (Maximum Likelihood Estimation) of a document model and the collection model as follows [6]:\nP(w|\u03b8D) = (1\u2212\u03bb)P(w|\u03b8\u0302D)+\u03bbP(w|\u03b8C) (1)\nwhere \u03bb is a smoothing parameter. By using JM, score(Q,D), the similarity score of document D for query Q can be written by using only query-matching terms as follows:\nscore(Q,D) = \u2211 w\u2208Q log\n(\n1\u2212\u03bb \u03bb P(w|\u03b8\u0302D) P(w|\u03b8C) + 1\n)\n= \u2211 w\u2208Q log\n(\n1\u2212\u03bb \u03bb t fD(w) lD lC t fC(w) + 1\n)\n(2) Our analysis of whether or not JM satisfies VNC and TNC is given as follows:\n1. JM satisfies VNC: Suppose that D2 is K-verbose to D1. Then, MLEs of two document models are the same, resulting in the same scores. 2. JM does not satisfy TNC: Generally, JM prefers normal documents to multi-topical documents, regardless of our definition of topicality measurement \u03c4. This proof is skipped.\nAnalysis of Dirichlet-Prior Smoothing In Dir (Dirichlet-prior smoothing), a smoothed document model is estimated as posterior model when taking \u00b5P(w|\u03b8C) as a prior probability of term w as follows [6]:\nP(w|\u03b8D) = t fD(w)+ \u00b5P(w|\u03b8C)\nlD + \u00b5 (3)\nThe equation is rewritten by\nP(w|\u03b8D) = lD\nlD + \u00b5 P(w|\u03b8\u0302D)+ \u00b5 lD + \u00b5 P(w|\u03b8C) (4)\nIf we set \u03bbD by \u00b5/(lD+\u00b5), then Dir is equivalent to JM-style smoothing using documentspecific smoothing parameter \u03bbD. score(D,Q) based on Dir is formulated as follows:\nscore(D,Q) = \u2211 w\u2208Q log\n(\n(1\u2212\u03bbD) P(w|\u03b8\u0302D) P(w|\u03b8C) +\u03bbD\n)\nThe analysis on whether or not Dir satisfies VNC and TNC is somewhat complicated, due to its document-specific smoothing parameter. We can easily show that Dir does not satisfy VNC and TNC. The following lists up the analysis result.\n1. Dir doesn\u2019t satisfy VNC: Generally, Dir makes inconsistent preferences according to whether or not a query term is topical. For a topical query term, Dir assigns the more score for verbose documents than normal documents. For a non-topical query terms, Dir assigns the less score for verbose documents than normal documents. The detailed proof is skipped. 2. Dir doesn\u2019t satisfy TNC: The detailed proof is skipped."}, {"heading": "3 Modification of Previous Retrieval Models", "text": "In the previous section, we have shown that two different smoothing methods do not satisfy two constraints well. In this section, we introduce the measurement of the number of topics, and modify the previous retrieval model such that it better satisfies VNC and TNC."}, {"heading": "3.1 Measurement of The Number of Topics", "text": "To figure out which measurement \u03c4(D) is acceptable to calculate the number of topics in document D, we propose two simple measurements for \u03c4(D) - The first one is vocabulary size, and the second one is information quantity.\nVocabulary Size: Generally, as there are more terms, a given document has more topics. Based on this idea, we can use the vocabulary size - \u03bd(D) - which indicates the number of unique terms in a given document, as a measurement for the number of topics.\nInformation Quantity: Even though the vocabulary size is simple and reasonable, it cannot discriminate the mainly topical terms from the causally-occurred terms. When using the vocabulary size, the number of topics may be unreasonably increased due to causally occurred terms. As for an alternative measurement, we consider the entropydriven value. Remind that entropy means the uncertainty of a generated sample. Entropy has the following positive properties for resolving the limitation of the vocabulary size. 1) As the number of possible events increases, entropy becomes larger. Here, events correspond to terms, hence the more terms are, the larger the entropy is likely to be. Thus, when a document has more topics, the content of the document can be described in more various ways, resulting in a larger entropy value. 2) Term generative probability of a document is used as the weight for calculating entropy value. As a term has more large probability, it makes more contribution to the final-entropy value. This property allows us to differentiate the effects of mainly topical terms and causally occurred terms.\nThe information quantity - \u03b5(D) - is defined as an exponential function of entropy of a document as follows:\n\u03c4(D) = \u03b5(D) = exp (\n\u2212\u2211 w\nP(w|\u03b8D)logP(w|\u03b8D) )\nSome Useful Definitions: We define some useful notations. Let us define the normalized measurement of the number of topics - \u03c4\u2032(D) -, and define the informative verbosity - \u03c9(D) - as follows:\n\u03c4\u2032(D) = \u03c4(D)/\u03c4\u0303, \u03c9(D) = lD/\u03c4(D)\nwhere \u03c4\u0303 is the mean of \u03c4(D) for all documents in a given test collection. Note that the informative verbosity indicates the average term frequency per unit information."}, {"heading": "3.2 Modification of JM", "text": "First Modification of JM Since JM exactly satisfies VNC, we would try to modify JM to additionally support TNC. The core idea of the modification of JM smoothing is a pseudo document. The pseudo document mainly consists of relevant parts to a query, which is constructed by extracting relevant parts from non-relevant parts. Then, the score of a document is calculated by using the pseudo document model, instead of original document model.\nThus, the pseudo-document makes us take a dynamic viewpoint of document representation where a document is dynamically changed according to a query. Note that a pseudo document is an imaginary concept, which is not really constructed at real time. All we require is generative probabilities for query terms from the pseudo document model.\nTo estimate probability of query terms in a pseudo document, we simplify the estimation problem by using probability in original document. In other words, for terms in the pseudo document having non-zero probabilities, their probabilities are assumed to be proportional to the probabilities of terms in the original document. As a result, the estimation problem is completed only if we determine the length of the pseudo document from the original length lD.\nIntuitively, the length of the pseudo document will be smaller, as topics are more. This intuition makes the length of the pseudo document proportional to lD/\u03c4(D). Thus, if \u03b8Pseudo(D) is the language model of pseudo document, then the probability of pseudo document model is\nP(w|\u03b8Pseudo(D)) \u221d t fD(w)/lD/\u03c4(D) = t fD(w) \u00b7 \u03c4(D)/lD\nIt is rewritten by using \u03c4\u2032(D) instead of \u03c4(D), and the constant K as follows:\nP(w|\u03b8Pseudo(D)) = K \u00b7 t fD(w) \u00b7 \u03c4\u2032(D)/lD\nIf we assume that the constant K is independent to any document and query, then K is not a tuning parameter since it can be included in smoothing parameter \u03bb.\nLet us derive a modified JM by substituting the original document model to this pseudo document model in Eq. (2). Then, score(Q,D) is reformulated as follows:\nscore(Q,D) = \u2211 w\u2208Q log\n(\n1\u2212\u03bb0 \u03bb0 K \u00b7 \u03c4\u2032(D) \u00b7 t fD(w) lD lC t fC(w) + 1\n)\n(5)\nwhere \u03bb0 is another smoothing parameter for the pseudo document model. Since K is independent to any document and query, we can select \u03bb such that (1\u2212 \u03bb0)K : \u03bb0 is\n(1\u2212\u03bb) : \u03bb, in order to eliminate constant K. Then, Eq. (5) is re-written by\nscore(Q,D) = \u2211 w\u2208Q log\n(\n1\u2212\u03bb \u03bb \u03c4\u2032(D) \u00b7 t fD(w) lD lC t fC(w) + 1\n)\n(6)\nBy using MLE of the original document model P(w|\u03b8\u0302D), Eq. (6) is rewritten by\nscore(Q,D) = \u2211 w\u2208Q log\n(\n1\u2212\u03bb \u03bb \u03c4\u2032(D)P(w|\u03b8\u0302D) lC t fC(w) + 1\n)\n(7)\nEq. (7) is the final modified JM, which is called JMV. JMV satisfies both of VNC and TNC.\n1. JMV satisfies VNC: Let D2 be K-verbose to D1. Then, \u03c4(D1) = \u03c4(D2) and P(w|D1) = P(w|D2). Thus, score(Q,D1) = score(Q,D2). 2. JMV satisfies TNC: Let D3 be N-topical to D1. Then, \u03c4(D3) = N\u03c4(D2) and P(w|D1) = NP(w|D3). It makes that \u03c4(D3)P(w|D3) = \u03c4(D1)P(w|D1). Therefore, score(Q,D1) = score(Q,D3).\nSecond Modification of JM In our preliminary experiments, we found that JMV performs well for keyword queries (i.e. title query), but is not reliable for verbose queries (i.e. description query), by showing serious sensitivity according to smoothing parameter \u03bb. To discuss the reason of this result, we focus on the main differences of keyword query and verbose query. First, there are common terms in a verbose query. Different from topical terms, common terms can be shared by all topics. A common term always verbosely acts regardless of verbose documents and multi-topical documents. Thus, the previous TF normalization would prefer multi-topical documents for queries including common terms. Second, verbose queries often contain noise terms such as \u201crelevant\u201d, \u201cfind\u201d and \u201cdocuments\u201d. When a document has more topics, it will increase the chance of existence of such noise terms. However, when our previous TF normalization is applied, noise term becomes very serious, because the number of topics is further multiplied to the normalized term frequency. Thus, the previous TF normalization would increase the scores of multi-topical documents for noise queries. These two differences may be the reason why Singhal et. al. penalized even multi-topical documents, as well as verbose documents [1]. However, we already discussed that their approach is not acceptable to topical terms.\nTo handle the problems of verbose-type queries, our TF normalization should be restricted to only document-specific terms, not to noise terms or common terms. As a query term is more topical term in a given document, we hope to perform more TF normalization, and vice versa. To this end, we define s(w,D) as term specificity of w in document D. As for s(w,D) this paper uses a probabilistic metric P(D|w) which is defined as follows:\ns(w,D) = P(D|w) = \u03bbsP(w|\u03b8\u0302D)\n\u03bbsP(w|\u03b8\u0302D)+ (1\u2212\u03bbs)P(w|\u03b8C)\nwhere \u03bbs is an additional smoothing parameter, which has 0.25 as the default value. By using the term specificity s(w,D), we newly modify the pseudo document model as follows:\nP(w|\u03b8Pseudo(D)) = K \u00b7 t fD(w) \u00b7 \u03c4\u2032(D)P(D|w)/lD (8)\nSince P(D|w) is between 0 and 1, the normalization is perfectly reflected when P(D|w) is 1, while it is weaken as P(D|w) is close to 0. One problem arises when \u03c4\u2032(D) is smaller than 1. In this case, as P(D|w) is larger, the effect of normalization becomes weaker. To resolve this problem, we considered the exceptional TF normalization, making the normalization proportional to P(D|w) even when \u03c4\u2032(D) is smaller than 1. In preliminary experiments, we found that the final retrieval performance is almost not changed, even after the exceptional TF normalization is applied. Thus, we select Eq. (8) for second modification. We call it JMV2."}, {"heading": "4 Modification of Dir", "text": "Our goal for Dir modification is to provide VNC. We introduce the concept of pseudo document model to modify Dir. Different from the pseudo document for JM modification that consists of query-relevant parts only, the pseudo document for Dir modification consists of all topics in the original document, but has a different length from the original length. Note that the change of the length only makes different models, since the smoothed model - P(w|\u03b8D) - is different according to the document length. In fact, the length-dependence was the main reason why Dir does not satisfy VNC.\nWe assume that the pseudo document model is proportional to original MLE document model. In addition, we set the length of the pseudo document by \u03c4(D). Remind that informative verbosity - \u03c9(D) - is defined as lD/\u03c4(D). That is, the pseudo document with length of \u03c4(D) compacts the original document with length lD by \u03c9(D) time. Therefore, each term w of document D has the following term frequency in the pseudo document.\nt fPseudo(D)(w) = t fD(w)/\u03c9(D) (9)\nAs a result, the pseudo document model becomes length-independent model, even though MLE of pseudo document model is the same as the original document model. By using pseudo document model, Dir produces the following smoothed model.\nP(w|\u03b8Pseudo(D)) = t fPseudo(D)(w)+ \u00b5P(w|\u03b8C)\n\u03c4(D)+ \u00b5 (10)\nBy substituting Eq. (9) to Eq. (10), Eq. (10) becomes\nP(w|\u03b8Pseudo(D)) = \u03c4(D)\n\u03c4(D)+ \u00b5 P(w|\u03b8\u0302D)+ \u00b5 \u03c4(D)+ \u00b5 P(w|\u03b8C) (11)\nThis final modified model can be viewed as JM-style smoothing using document-specific smoothing paramter \u03bbD with \u00b5/(\u03c4(D)+ \u00b5), which is not dependent to the length any more. We call this modification DirV. We can easily prove that DirV additionally satisfies VNC.\n1. DirV satisfies VNC: Let D2 be K-verbose to D1. Then, two MLE models are equal (i.e P(w|\u03b8D1) = P(w|\u03b8D2 )). \u03bbD1 is \u03bbD2 since \u03c4(D1) and \u03c4(D2) are the same. Thus, DirV gives the same score for D1 and D2. 2. DirV does not satisfy TNC: For DirV, we do not have a special consideration for supporting TNC."}, {"heading": "5 Experimentation", "text": ""}, {"heading": "5.1 Experimental Setting", "text": "For evaluation, we used five TREC test collections. The standard method was applied to extract index terms; We first separated words based on space character, eliminated stopwords, and then applied Porter\u2019s stemming. Table 1 summarizes the basic information of each test collection. In columns, #Q, Topics, #R, #Doc, avglen, and #Terms are the number of topics, corresponding query topic IDs, the number of relevant documents, the number of documents, the average length of documents, and the number of terms, respectively.\nAccording to Zhai\u2019s work [6], we used the following three different types of queries: 1) Short keyword (SK): Using only the title of the topic description. 2) Short Verbose (SV): Using only the description field (usually one sentence). 3) Long Verbose (LV): Using the title, description and the narrative field (more than 50 words on average). As for retrieval evaluation, we used MAP (Mean Average Precision), Pr@5 (Precision at 5 documents), and Pr@10 (Precision at 10 documents)."}, {"heading": "5.2 Experimental Results", "text": "Table 2 shows the best performances (MAP, Pr@5, Pr@10) of DirV and JMV2, compared with Dir. As for topic measurement \u03c4(D), we selected the information quantity (\u03b5(D)) since JMV2 and DirV using the information quantity is better than those using vocabulary size. We used MLE (Maximum Likelihood Estimation) for P(w|\u03b8D) to calculate the information quantity without any smoothing. We selected Dir as the baseline due to its superiority over JM in all test collections. To obtain the best performance of each run, we searched 20 different values between 0.01 and 0.99 for \u03bb, and 22 values between 100 and 30,000 for \u00b5. To check whether or not the proposed method (DirV and\nJMV2) significantly improves the baseline, we performed the Wilcoxon sign ranked test to examine at 95% and 99% confidence levels. We attached \u2020 and \u2021 to the performance number of each cell in the table when the test passes at 95% and 99% confidence level, respectively. The results are summarized as follows:\n1. DirV significantly improves MAP of Dir for verbose type of query (SV and LV). Exceptionally, TREC10 did not show an improvement for verbose type of query.\n2. DirV does not significantly improve MAP of Dir for keyword type of query (SK), but improves precisions (Pr@5 or Pr@10). Especially, on TREC7 and TREC8, Pr@10 is significantly improved over Dir. Although other test collections do not statistically show a significant improvement, there is large portion of the numerical increase.\n3. DirV or JMV2 show improvement on a specific test collection even for keyword type of query. For DirV, TREC10 is such a collection by showing a significant improvement of MAP. For JMV2, WT2G is such a test collection.\n4. Overall, DirV is slightly better than JMV2 in most of test collections. WT2G is an exceptional collection to show that JMV2 significantly improves DirV."}, {"heading": "6 Conclusion", "text": "This paper introduced a new issue for TF normalization by considering two different types of long-length documents - verbose documents and multi-topical documents. We proposed a novel TF normalization method which uses a partially-axiomatic approach. To this end, we formulated two desirable constraints, which the retrieval function should satisfy, and showed that previous language modeling approaches do not satisfy these constraints well. Then, we derived novel smoothing methods for language modeling approaches, without losing basic principles, and showed that the proposed methods satisfies these constraints more effectively. Experimental results on five standard TREC collections show that the proposed methods are better than previous smoothing methods, especially for verbose type of query. JMV2 significantly improved JM for all type of queries, and DirV eliminated the limitation of Dir by providing the robustness of performances for verbose type of query, as well as improving precisions (Pr@5 or Pr@10) for keyword type of query. This is comparable to recent results using more complicated query-specific smoothing based on Poisson language model [7].\nTo handle long-length documents, passage-based retrieval could be applied [8]. However, passage-based retrieval has a burden of decreasing efficiency, since it requires additional process such as indexing of position information, pre-segmenting individual passages, and more importantly the additional overhead at online retrieval time. Contrast to the complicated method such as the passage retrieval, this paper handles multi-topical documents in a simplified manner by investigating a more accurate TF normalization without additional cost of efficiency.\nAcknowledgement. This work was supported by the Korea Science and Engineering Foundation (KOSEF) through the Advanced Information Technology Research Center (AITrc), also in part by the BK 21 Project and MIC & IITA through IT Leading R&D Support Project in 2007."}], "references": [{"title": "Pivoted document length normalization", "author": ["A. Singhal", "C. Buckley", "M. Mitra"], "venue": "SIGIR \u201996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["S.E. Robertson", "S. Walker"], "venue": "SIGIR \u201994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A formal study of information retrieval heuristics", "author": ["H. Fang", "T. Tao", "C. Zhai"], "venue": "SIGIR \u201904.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "An exploration of axiomatic approaches to information retrieval", "author": ["H. Fang", "C. Zhai"], "venue": "SIGIR \u201905.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "SIGIR \u201998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR \u201901.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "A study of poisson query generation model for information retrieval", "author": ["Q. Mei", "H. Fang", "C. Zhai"], "venue": "SIGIR \u201907.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Effective ranking with arbitrary passages", "author": ["M. Kaszkiel", "J. Zobel"], "venue": "Journal of the American Society for Information Science and Technology (JASIST) 52(4)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Regarding this, Singhal observed the following two different types of reasons for making the length of a document long [1] 3.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "3 Robertson and Walker mentioned two types of reasons as scope hypothesis and verbosity hypothesis, respectively [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "Singhal pre-assumed that long-length documents should be penalized regardless of whether or not their types are verbosity (or multi-topicality) [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "These constraints can be directly utilized to derive a new class of retrieval function as Fang\u2019s exploration [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Originally, Fang formulated two constraints related to term frequency - LNC1 and LNC2 [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "In contrast, Fang\u2019s approach is the fully-axiomatic approach [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "In contrast, Fang\u2019s approach is the fully-axiomatic approach [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 4, "context": "We selected the language modeling approaches as the backbone retrieval model [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "We investigate two popular smoothing methods - Jelinek-Mercer smoothing (JM) and Dirichlet-prior smoothing (Dir) [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Analysis of Jelinek-Mercer Smoothing In JM (Jeliner-Mercer Smoothing), a smoothed document model is obtained by the interpolation of MLE (Maximum Likelihood Estimation) of a document model and the collection model as follows [6]:", "startOffset": 225, "endOffset": 228}, {"referenceID": 5, "context": "Analysis of Dirichlet-Prior Smoothing In Dir (Dirichlet-prior smoothing), a smoothed document model is estimated as posterior model when taking \u03bcP(w|\u03b8C) as a prior probability of term w as follows [6]:", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "penalized even multi-topical documents, as well as verbose documents [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "According to Zhai\u2019s work [6], we used the following three different types of queries: 1) Short keyword (SK): Using only the title of the topic description.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "This is comparable to recent results using more complicated query-specific smoothing based on Poisson language model [7].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "To handle long-length documents, passage-based retrieval could be applied [8].", "startOffset": 74, "endOffset": 77}], "year": 2015, "abstractText": "Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi-topicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}