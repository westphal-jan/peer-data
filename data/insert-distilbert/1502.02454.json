{"id": "1502.02454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "A fast PC algorithm for high dimensional causal discovery with multi-core PCs", "abstract": "discovering causal relationships from observational data is a crucial problem and has applications in many research areas. pc algorithm is the state - of - the - art method in the constraint based hardware approach. however, unfortunately the typical pc algorithm is worst - case exponential to the number of nodes ( variables ), whereas and thus it is inefficient when applying to high dimensional data, e. g. gene expression datasets where the causal relationships between thousands of nodes ( genes ) are explored. in this paper, we propose a fast and continuous memory efficient artificial pc algorithm using the parallel computing protocol technique. we apply our method on a range of synthetic and real - world high dimensional datasets. experimental results on a dataset from dream 5 challenge show that the pc algorithm could not produce any results after running more than after 24 hours ; meanwhile, our parallel - pc algorithm with a 4 - core cpu disk computer managed to precisely finish within around 12. 5 hours, and less than 6 hours with a 8 - core cpu computer.", "histories": [["v1", "Mon, 9 Feb 2015 12:15:21 GMT  (293kb,D)", "https://arxiv.org/abs/1502.02454v1", null], ["v2", "Sat, 11 Jul 2015 03:03:16 GMT  (724kb,D)", "http://arxiv.org/abs/1502.02454v2", null], ["v3", "Thu, 10 Nov 2016 12:23:48 GMT  (1210kb,D)", "http://arxiv.org/abs/1502.02454v3", "Thuc Le, Tao Hoang, Jiuyong Li, Lin Liu, Huawen Liu, Shu Hu, \"A fast PC algorithm for high dimensional causal discovery with multi-core PCs\", IEEE/ACM Transactions on Computational Biology and Bioinformatics, doi:10.1109/TCBB.2016.2591526"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thuc duy le", "tao hoang", "jiuyong li", "lin liu", "huawen liu"], "accepted": false, "id": "1502.02454"}, "pdf": {"name": "1502.02454.pdf", "metadata": {"source": "CRF", "title": "A fast PC algorithm for high dimensional causal discovery with multi-core PCs", "authors": ["Thuc Duy Le", "Tao Hoang", "Jiuyong Li", "Lin Liu", "Huawen Liu", "Shu Hu"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Causal discovery, PC algorithm, Parallel computing, High dimensional data, Gene expression data, miRNA targets.\nF"}, {"heading": "1 INTRODUCTION", "text": "INvestigating the associations between variables has longbeen a main research topic in statistics, data mining and other research areas. For instance, to construct a gene regulatory network, we might want to find the associations between the expression levels of genes. We may use the observed association to explain that one gene regulates the expression of the other. However, the association does not tell us which gene is the regulator and which gene is the target, or indicate the chance of a third gene regulating the two genes. Hence, associations between genes may not imply the cause-effect nature of gene regulatory relationships.\nThere has been a long controversy over whether causality can be discovered from observational data. Many scientists have attempted to design algorithms of inferring causality from observational data over the last few decades [1], [2], [3], [4]. As a result, there has been more and more evidence demonstrating the possibility of discovering causal relationships from non-experimental data in different research areas [1], [5], [6]. Especially, two Nobel prizes were awarded for such causality methods in the field of Economics in 2003 and 2011 respectively [5], [6]. These works have strongly motivated the utilisation of observational data in exploring causal relationships in other important research areas, such as finding the genetic causes of cancers.\n\u2022 T.D. Le, T. Hoang, J. Li, L. Liu are with the school of Information Technology and Mathematical Sciences, University of South Australia, Mawson Lakes, SA, 5095. E-mail: Thuc.Le@unisa.edu.au \u2022 H. Liu is with Department of Computer Science, Zhejiang Normal University, China. \u2022 S. Hu is Department of Computer Science, USTC, China. Manuscript received 2015; revised 2015.\nThe standard method for discovering causality is a randomised controlled experiment. For example, to assess the effect of gene A on other genes, biologists use the gene knockdown experiment. In the experiment, they create two groups of samples, control and transfected groups. Gene A is knocked down in the control group whilst remains in the transfected group. The changes in the expression levels of the target genes between the two groups are the effects that gene A has on them. However, conducting such experiments would incur humongous costs given that thousands of genes are required to be tested. Moreover a randomised controlled experiment is generally impossible to conduct or restricted by ethical concerns in many cases. Therefore, discovering causality from observational data is a crucial problem.\nOne of the most advanced theories with widespread recognition in discovering causality is the Causal Bayesian Network (CBN). In this framework, causal relationships are represented with a Directed Acyclic Graph (DAG) [1]. Learning a DAG from data is highly challenging and complex as the number of possible DAGs is super-exponential to the number of nodes [7]. There are two main approaches for learning a DAG: the search and score approach, and the constraint based approach. While the search and score approach raises an NP-hard problem, the complexity of the constraint based approach is exponential to the number of nodes. The high computational complexity has hindered the applications of causal discovery approaches to high dimensional datasets, especially gene expression datasets of which the number of genes (variables) is large and the number of samples is normally small.\nar X\niv :1\n50 2.\n02 45\n4v 3\n[ cs\n.A I]\n1 0\nN ov\n2 01\n6\nA well-known constraint based algorithm is the Inductive Causation (IC) algorithm proposed in [8]. It is a conceptual algorithm and was implemented in the PC (named after its authors, Peter and Clark) algorithm [2]. The PC algorithm starts with a complete, undirected graph and deletes recursively edges based on conditional independence decisions. For example, the edge between A and B is removed if we can find a set S that does not include A and B, and when conditioning on S, A and B are independent. The PC algorithm has been implemented in various open-source sofware such as TETRAD [2], pcalg [9], and bnlearn [10], and has become a reliable tool for causal explorations.\nThere have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15]. Specifically, Zhang et al. [11] applied the PC algorithm which used mutual information for the conditional independence tests to learn the gene regulatory networks from gene expression data. Maathuis et al. [12] used the PC algorithm to learn the causal structure of the gene regulatory networks, then they applied do-calculus [1] to the learnt network to infer the causal effect that a gene has on the other. Le et al. [13] applied the approach in [12] to predict the targets of microRNAs (miRNAs, an important class of gene regulators at the post-transcriptional level). In a similar fashion, Zhang et al. [14] inferred the miRNA-mRNA interactions that had different causal effects between biological conditions, e.g. normal and cancer. These applications of the PC algorithm have been shown to outperform other computational methods that are based on correlation or regression analysis. Moreover, the PC algorithm is able to distinguish direct interactions from indirect interactions in gene regulatory networks [11], [15].\nHowever, there are two main limitations of the PC algorithm, especially when applying to high dimensional biological datasets: (i) the runtime of the PC algorithm, in the worst case, is exponential to the number of nodes (variables), and thus it is inefficient when applying to high dimensional datasets such as gene expression datasets, and (ii) the result from the PC algorithm is variable ordereddependent, i.e. when we change the order of the variables in the input dataset, the result may change.\nSeveral methods [16], [17], [18], [19] have been proposed to improve the efficiency of the PC algorithm. They aim to introduce efficiency-improved alternative methods, but they rather compromise the accuracy. These methods either search for only specific causal structures or use heuristic functions to improve the efficiency of the algorithm. However, the results from these heuristic methods are either incomplete or involving a high rate of false discoveries.\nAdditionally, the order-dependence of the PC algorithm is problematic in high dimensional datasets. Colombo et al. [20] has shown experimentally that around 40% of the edges (2000 edges) learnt from a real gene expression dataset are not stable, i.e. these edges only appear in less than half of the results obtained with all the different orderings of nodes. This problem makes the knowledge inferred by\nusing the PC algorithm less reliable. To overcome the orderdependence of the PC algorithm, researchers proposed several modifications to the procedure of the algorithm [2], [20], [21], [22]. For instance, Colombo et al. [20] proposed a modified version of the PC algorithm called stable-PC. The algorithm aims to query all the neighbours of each node and fix these neighbours while conducting conditional independence tests (CI tests henceforward) at each level (based on size of the conditioning sets) of the PC algorithm. However, this modification requires more CI tests and therefore results in even longer running time for this already inefficient algorithm. Our experiment on a dataset from the DREAM 5 challenge (http://dreamchallenges.org/) with 1643 variables and 805 samples shows that the order-independence version of the PC algorithm [20] takes more than 24 hours to run (unfinished). The inefficiency of the algorithm hinders its application in practice, as we would not estimate the runtime of a real world dataset up-front, and thus, prolonged running time would prevent us from applying the algorithm.\nIn another direction, parallel computing provides a straightforward approach to improving the efficiency of the algorithm, but it has been impractical for normal end users. The last decade has seen a fast growth of parallel computing frameworks, such as MapReduce framework [23]. However, such frameworks need suitable cluster facilities and require some solid technical understanding from users. Meanwhile, end users, e.g. biologists, normally use personal computers or small lab servers for their everyday research, and may not easily acquire the required technical knowledge for applying parallelised data mining algorithms. On the other hand, the fast development of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers. It is common for a modern computer to have a four-core CPU, and small servers to have eight-core CPUs. Therefore, it is of great interest to design parallel algorithms that are suitable for personal computers. Specifically, we aim to design the parallelised PC algorithm (called parallel-PC hereafter) for multi-core personal computers.\nIn this paper, we present a method to parallelise the order-independence PC algorithm in [20] for high dimensional datasets. We propose to parallelise the CI tests at each level of the algorithm. The CI tests at each level are grouped and distributed over different cores of the computer, and the results are integrated at the end of each level. Consequently, the runtime of our parallel-PC algorithm is much shorter than the original PC algorithm and the results of the parallel-PC algirhtm is independent of the variable ordering in a dataset. Importantly, it does not require any extra knowledge or installation effort from users. Our experiment results on both synthetic and real world datasets show that it is now practical to explore causal relationships with the PC algorithm in gene expression datasets using a multi-core personal computer. The proposed algorithm contributes to bridging the gap between computer science theory and its practicality in important scientific research areas. As an application, we modified the causal inference method in [13], by replacing the PC algorithm with the parallel-PC\nalgorithm, to infer miRNA-mRNA regulatory relationships in three different cancer datasets. The experimental results show that our method not only outperforms the original causal inference method that uses the original PC algorithm, but also faster.\nWe summarise the contributions of the paper in the following:\n1) Presenting a fast PC algorithm which is suitable for high dimensional datasets, and the results are independent of the order of the variables. To the best of our knowledge, this is the first work of utilising parallel computing for the PC algorithm. 2) Providing the software tool and proving that it is reliable and efficient using a wide range of real world gene expression datasets. 3) Modifying a causal inference method to infer miRNA-mRNA regulatory relationships with better performance.\nThe rest of the paper is organised as follows. Section 2 discusses related work, and Section 3 presents the PC algorithm and the stable-PC algorithm as well as the demonstrating examples. The proposed algorithm is presented in Section 4. Experiment results are shown in Section 5, Section 6 presents an application of parallel-PC in inferring miRNA-mRNA regulatory relationships, and finally Section 7 concludes the paper."}, {"heading": "2 RELATED WORK", "text": "The well-recognised methods for causality discovery are based on probabilistic graphical modeling [1], [24]. The structure of these graphical models is a DAG (directed acyclic graph), with its nodes representing random variables and edges representing dependencies between the variables [25]. There are two main approaches to learning the causal structure from data: 1) search and score and 2) constraint based methods. The search and score methods [24], [26], [27] search for all possible DAGs whilst using a scoring function to measure the fit of each DAG to the data. The DAG that best fits the data will be chosen. However, learning Bayesian networks using this approach is an NPhard problem [28], and the proposed methods following this approach were only able to cater for datasets with a limited number of variables, and therefore may not be suitable for high dimensional datasets. Meanwhile, the constraint based approach [1], [16], [29] uses CI tests to remove non-causal relationships between variables. These methods are suitable for sparse datasets in practice [30].\nDue to the significance of the PC algorithm in the constraint based causal discovery approach, there have been several methods aiming to modify the procedure of the PC algorithm directly to improve the efficiency and/or the accuracy of the algorithm. Steck and Tresp [31] used the necessary path condition to reduce the number of CI tests in the PC algorithm. They have proved that when testing the edge between X to Y , we do not need to condition on the nodes that are not in a path from X to Y . This modification\nmay help reduce the chance for making errors by conducting fewer tests. However, it requires more running time for finding the nodes that are not in the path. Other researchers [29], [32] proposed to replace the CI test, which is normally based on the Chi-square statistical test, with the Bayesian statistical tests. They aim to reduce the error rates made by the CI tests. However, there is still no clear evidence in real world datasets about the impacts of this replacement on the efficiency improvement of the PC algorithm.\nMeanwhile, the other constraint based methods [1] infer causal relationships by performing CI tests and searching for specific causal structures. For instance, Cooper [29] proposed the LCD method to learn only a small portion of edges rather than the complete network. Specifically, the LCD algorithm searches for the CCC structures which consist of three pairs of correlated variables: (A, B), (A, C) and (B, C). If A and C become independent when conditioned on B, we may infer that one of the following causal relations exists between A, B, and C : A \u2190 B \u2192 C ; A \u2192 B \u2192 C ; and A \u2190 B \u2190 C . Silverstein et al. [16] proposed the similar method that focuses on the CCU structures. The CCU method searches for two correlated pairs (A,B) and (A,C) and an independent pair (B,C). When conditioning on A, if (B,C) becomes dependent then the causal relationships B \u2192 A\u2190 C are concluded. Compared to the PC algorithm, such specific structure finding methods are more efficient, but the causal discoveries are incomplete.\nDue to the inefficiency of algorithms concerning causal relationships between all pairs of variables in a dataset, several methods have been proposed to discover only the local causal relationships from data, i.e. the relationships between a specific target and its neighbours. A popular approach is to identify the Markov blanket of a target variable. In a causal graph, Markov blanket has two equivalent definitions: 1) the Markov blanket of a node T is a set MB(T ) such that T is independent of other nodes when conditioning on MB(T ), or 2) the Markov blanket of a node T comprises its parents, children, and children\u2019s parents (spouses). Based on these two definitions, two main approaches have been proposed to discover the Markov blanket, the grow-andshrink approach and the divide-and-conquer approach. The grow-and-shrink approach is based on the first definition and it uses the heuristic function in the \u201cgrow\u201d step to find the superset of the true Markov blanket, then the false positives in this super set will be removed in the \u201cshrink\u201d step. Some well-known methods in this approach include GS (Grow and Shrink) [33], IAMB (Incremental Association Markov Blanket) [17], and IAMB variants [34]. The divide and conquer approach uses the topology constraints in the second definition of Markov blanket. This approach firstly finds the parents and children of the target node, then the spouses can be found by searching through the parents and children of the identified nodes in the first step. This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).\nIn another direction, Jin et al. [39] proposed an efficient method for testing the persistent associations between the\ntarget variable and its neighbours. The method is based on partial association tests and association rule mining framework to remove the spurious associations. In a similar fashion, Li et al. [18], [19] integrated the ideas of retrospective cohort study with the association rule mining framework. They proposed to divide the samples into two groups of individuals, who share common characteristics but differ in regard to a certain factor of interest. The newly-designed dataset is called \u201cfair dataset\u201d and is used to infer the level of influence that the factor of interest affects the target variable. These works were extended in [40], and the software tools were also provided. However, these methods are only applicable for binary datasets with a fixed target variable, and thus may not be suitable for high dimensional datasets, e.g. gene expression data.\nThe common characteristic of most of the abovementioned works is that they aim to propose efficiencyimproved alternative methods to the PC algorithm. However, the results from these heuristic methods may involve a high rate of false discoveries.\nScutari [41] utilised parallel computing technique for constraint based local causal discovery methods. Meanwhile, Chen et al. [42] proposed a method to parallelise the Bayesian network structure learning algorithm using search and score approach. However, there is still no existing work that parallelises the PC algorithm."}, {"heading": "3 THE ORIGINAL-PC AND STABLE-PC ALGORITHMS", "text": ""}, {"heading": "3.1 Notation, definitions, and assumptions", "text": "Let G = (V,E) be a graph consisting of a set of vertices V and a set of edges E \u2286 V \u00d7V. The set of vertices that are adjacent to A in graph G is defined as: adj(A,G) = {B : (A,B) \u2208 E or(B,A) \u2208 E}. B is called a collider if we have the v-structure: A\u2192 B \u2190 C .\nGraphG is a Directed Acyclic Graph (DAG) ifG contains only directed edges and has no directed cycles. The skeleton of a DAG G is the undirected graph obtained from G by ignoring the direction of the edges. An equivalence class of DAGs is the set of DAGs which have the same skeleton and the same v-structures. An equivalence class of DAGs can be uniquely described by a completed partially directed acyclic graph (CPDAG) which includes both directed and undirected edges.\nDefinition 1 (d-separation) [43]. In a DAG, a path p between vertices A and B is active (d-connecting, where \u201cd\u201d stands for dependence) relative to a set of vertices C (A,B /\u2208 C) if (i) every non-collider on p is not a member of C; (ii) every collider on p is an ancestor of some member of C. Two sets of variables A and B are said to be d-separated by C if there is no active path between any member of A and any member of B relative to C.\nLet P be the joint probability distribution of V. The following assumptions are set when applying the PC algorithm.\nAssumption 1. (Causal Markov Condition [2]) G and P satisfy the Causal Markov Condition if and only if given the set of all its parents, a node of G is probabilistically independent of all its non-descendents in G.\nAssumption 2. (Faithfulness Condition [2]) G and P satisfy the Faithfulness Condition if and only if no conditional independence holds unless entailed by the Causal Markov Condition.\nAssumption 3. (Causal sufficiency [2]) For every pair of variables which have their observed values in a given dataset, all their common causes also have observations in the dataset."}, {"heading": "3.2 The original-PC algorithm", "text": "The PC algorithm [2] (original-PC algorithm henceforward) has two main steps. In the first step, it learns from data a skeleton graph, which contains only undirected edges. In the second step, it orients the undirected edges to form an equivalence class of DAGs. As the first step of the PC algorithm contributes to most of the computational costs, we only focus on the modification of this skeleton learning step in this paper, and information about the edge orientation step can be found in [2].\nThe theoretical foundation of the original-PC algorithm [2] is that if there is no link (edge) between nodes X and Y , then there is a set of vertices Z that either are neighbours of X or Y such that X and Y are independent conditioning on Z. In other words, Z disconnects X and Y . This foundation is presented as Theorem 1 below.\nTheorem 1 [2]. If vertices X and Y are not adjacent in a DAG G, then there is a set of vertices Z which is either a subset of adj(X,G)\\Y or a subset of adj(Y,G)\\X such that Z dseparates X and Y in G.\nBased on Theorem 1, a naive approach for learning the skeleton is that we can search exhaustively for all possible neighbours of X and Y and perform conditional independence tests to justify if there is a set Z disconnecting X and Y . Obviously, this is an inefficient approach, as the neighbours of X and Y are unknown, and thus the search space is all variables other than X and Y . In the following we explain how the original-PC algorithm works and list the details of the skeleton learning step of the original-PC algorithm in Algorithm 1.\nIn the skeleton learning step (Algorithm 1), the orginalPC algorithm starts with the fully connected network and uses the conditional independence tests to decide if an edge is removed or retained. For each edge, the PC algorithm tests if the pair of variables connected by the edge, X and Y , are independent conditioning on a subset Z of all neighbours of X and Y . The CI tests are organised by levels (based on the size of the conditioning sets, e.g. the depth d). At the first level (d = 0), all pairs of vertices are tested conditioning on the empty set. Some of the edges would be deleted and the algorithm only tests the remaining edges in the next level (d = 1). The size of the conditioning set, d, is progressively increased (by one) at each new level until d is greater than the size of the adjacent sets of the testing vertices. The key feature that makes the PC algorithm efficient in sparse true\nAlgorithm 1: Step 1 of the original-PC algorithm: learning the skeleton\nInput: Dataset D with a set of variables V, and significant level \u03b1 Output: The undirected graph G with a set of edges E Assume all nodes are connected innitially Let depth d = 0 repeat\nfor each ordered pair of adjacent vertices X and Y in G do\nif (|adj(X,G)\\{Y }| >= d) then for each subset Z \u2286 adj(X,G)\\{Y } and |Z| = d do\nTest I(X,Y |Z) if I(X,Y |Z) then\nRemove edge between X and Y Save Z as the separating set of (X,Y ) Update G and E break\nend end\nend end Let d = d+ 1\nuntil |adj(X,G)\\{Y }| < d for every pair of adjacent vertices in G;\nunderlying graphs is that the neighbours of a particular node are dynamically updated when an edge is removed. Therefore, the number of conditional independence tests is small when the true graph is sparse [2]. If the conditional independence tests are truly correct, then the PC algorithm will return the true graph as stated in Theorem 2 [2].\nTheorem 2 [2]. Let the distribution P be faithful to a DAG G = (V;E), and assume that we are given perfect conditional independence information about all pairs of variables (X,Y ) in V given subsets S. Then the output of the PC-algorithm is the CPDAG that represents G.\nFigure 1 (top) shows an example of original-PC algorithm being applied to a dataset with four nodes, A,B,C, and D. The original-PC algorithm starts with the fully connected graph. At the first level, all edges are tested conditioning on the empty set. In the example, after level 1 tests, there are three edges left. At the next level, each remaining edge will be tested conditioning on each neighbour (size 1) of the testing variables. For example, with the edge between A and B we have at most two tests which are conditioning on C and conditioning on D. If the test returns independence (e.g. I(A,B|C)), we remove the edge from the graph and move to testing the other edge. The procedure will stop when there is no test to perform."}, {"heading": "3.3 The stable-PC algorithm", "text": "The CI tests in the original-PC algorithm are prone to mistakes as the number of samples is limited. Moreover, incorrectly removing or retaining an edge would result in the changes in the neighbour sets of other nodes, as the\ngraph is updated dynamically. Therefore, the output graph is dependent on the order in which we perform the conditional independence tests. In other words, given a lexical order of performing the tests, the order of the variables in the input data will affect the output graph. In Figure 1 (top), at the beginning of level 2, after we perform test #10, the edge between A and B is removed and the graph is updated. This also means that the adjacent set of A is also updated, i.e. adj(A,G) = {C,D}. Therefore, when we test the edge between A and C , we do not need to condition on B. The updates make the original-PC algorithm efficient, but they also cause problems when the CI tests are making mistakes. If test #10 wrongly removes the edge between A and B, then we miss the test I(A,C|B) which may remove the edge between A and C . Moreover, if we test I(A,C|B)\nfirst and remove the edge between A and C , we will end up with a different graph. In other words, when we change the order of the variables in the dataset, e.g. A,C,B, and D, the result of the original-PC algorithm may change.\nColombo et al. [20] proposed a modification to the original-PC algorithm to obtain a stable output skeleton which does not depend on how variables are ordered in the input dataset. In this method (called stable-PC algorithm), the neighbour (adjacent) sets of all nodes are queried and kept unchanged at each particular level. As a result, an edge deletion at one level does not affect the conditioning sets of the other nodes, and thus the output is independent with the variable ordering. For instance, at the beginning of level 2 in the above example, the adjacent set of node A is adj(A,G) = {B,C,D}. This adjacent set is kept unchanged at this level. After performing test #10, we remove the edge between A and B in the graph, but the adj(A,G) is not updated. Therefore, we still consider B as an adjacent node of A, and tests #11 and #12, still need to be performed as shown in Figure 1 (bottom). Test #11 removes the edge between A and C , and therefore generates different output graph compared to the original-PC algorithm.\nHowever, this modification requires performing more conditional independence tests in each level of the algorithm, and thus it increases further the runtime of the algorithm. In the following section, we adopt this modification to design a modified PC algorithm which is both efficient and order-independent."}, {"heading": "4 PARALLEL-PC ALGORITHM", "text": ""}, {"heading": "4.1 Parallel Computing: Potentials and Challenges", "text": "Computer hardware technology has been advanced significantly in the last decade. Nowadays, it is common to have personal desktop computers and even laptops have 4 cores in their CPUs. Likewise, the number of cores in today\u2019s servers has jumped to 8, 16, 32 or more. These resources provide a great potential of utilising parallel computing for important algorithms, including the PC algorithm, for personal computers. In fact, the majority of end users are using their personal computers for doing research. Therefore, parallel algorithms would have significant impact if they work well for personal computers and do not require users\u2019 technical knowledge about parallelism and highperformance clusters.\nThe idea of parallelism is that we break down a big task into several different smaller subtasks and distribute them over different cores of the computer\u2019s CPU to run in parallel. The results from all subtasks will then be integrated to form the result of the original task. Theoretically, this divideand-conquer approach can speed up an algorithm by the maximum number-of-core times. This approach, however, requires the subtasks to be unrelated. In other words, the result from each subtask must be independent of one another and no communication is allowed between them.\nThe independence requirement has been a great barrier for many sequential algorithms, including the stable-PC\nalgorithm, to be parallelised. In the stable-PC algorithm, the performance bottleneck is the huge number of CI tests, which can reach hundreds of millions in real-world datasets. The CI tests, however, are order-dependent across different levels. Since the stable-PC algorithm updates the adjacent sets of all nodes at each level after all the CI tests of the level are completed, the test results of a particular level will influence the results of the next level. Therefore, it seems infeasible to parallelise the stable-PC algorithm as a whole."}, {"heading": "4.2 The parallel-PC Algorithm", "text": "Our proposed strategy is to parallelise the CI tests, not across different levels, but inside each level of the stable PC-algorithm. This approach is feasible because the CI tests at a particular level are independent of each other. Since the graph is only updated at the end of each level, the result of one CI test does not affect the others. Therefore, the CI tests at a level can be executed in parallel without changing the final result. Furthermore, this approach also enjoys the advantage of knowing the number of CI tests of each level in advance. This allows the CI tests to be evenly distributed over different cores, so that the parallelised algorithm can achieve maximum possible speedup.\nWithin a level (depth) of Algorithm 2, we employ the parallel computing paradigm to parallelise their executions (the parallelisation step). At each level with the conditioning sets of size d, the sequential process in the stable-PC algorithm is replaced by a three-staged process: (1) the CI tests are distributed evenly among the cores, (2) each core performs its own sets of CI tests in parallel with the others, and (3) the test results in all cores are integrated into the global graph. This three-staged process is applied at all the levels (the depth d) of the algorithm.\nWe observed that it is not efficient to distribute the CI tests of the same edge to different cores. Given an edge between X and Y , we need to perform the CI tests conditioning on the variables in adj(X,G) and in adj(Y,G). However, if we firstly perform the CI tests conditioning on variables in adj(X,G) and the edge between X and Y is removed, then the CI tests conditioning on variables in adj(Y,G) are unnecessary. We resolve this dependency by grouping the CI tests of the same edge together, instead of separating them as in the stable-PC algorithm. In Algorithm 2, if the CI tests between X and Y return independence when conditioning on Z \u2282 adj(X,G)\\{Y }, the algorithm will not perform the CI tests conditioning on variables in adj(Y,G). Grouping those tests together, the algorithm ensures the CI tests of the same edge will not be distributed to different cores of the CPU, and thus reduce the number of unnecessary CI tests that need to be performed.\nIn Algorithm 2, we provide an optional input parameter, memory-efficient indicator s. If s is set to TRUE, the algorithm will detect the free memory of the running computer to estimate the number of edges inG that will be distributed evenly to the cores. This step is to ensure that each core of the computer will not hold off a big amount of memory while waiting for the synchronisation step. The memoryefficient procedure may consume a little bit more time.\nAlgorithm 2: The parallel-PC algorithm Input: Dataset D, significant level \u03b1, P cores,\nmemory-efficient indicator s, number of edges per batch tb\nOutput: The undirected graph G with a set of edges E Assume all nodes are connected in graph G Let depth d = 0 repeat\nQuery and fix the adjacent set adj(X,G) of each node X in G Compute the set J of unordered pairs of adjacent vertices (X,Y ) in G // Parallelisation Step for each batch of tb edges (tb = |J | if s = FALSE) do\nDistribute the edges in the batch evenly into P cores, each with Jp edges for each core p = 1 . . . P in parallel do\nfor each pair (X,Y ) \u2208 Jp do Let kpX,Y indicate if (X,Y ) is adjacent, initialize kpX,Y = TRUE // On X\u2019s neighbours if |adj(X,G)\\{Y }| \u2265 d then\nfor each subset ZX \u2286 adj(X,G)\\{Y } and |ZX | = d do\nif I(X,Y |ZX) then kpX,Y = FALSE break\nend end\nend // On Y \u2019s neighbours if |adj(Y,G)\\{X}| \u2265 d then\nfor each subset ZY \u2286 adj(Y,G)\\{X} and |ZY | = d do\nif I(X,Y |ZY ) then kpX,Y = FALSE break\nend end\nend end\nend // Synchronisation Step for each core p = 1 . . . P do\nfor each pair (X,Y ) \u2208 Jp do if kpX,Y = FALSE then\nRemove the edge between X and Y and update G and E\nend end\nend end Let d = d+ 1\nuntil |adj(X,G)\\{Y }| < d for every pair of adjacent vertices in G;\nHowever, this option is recommended for computers with limited memory resources or for big datasets."}, {"heading": "4.3 Implementation", "text": "We implement our parallel-PC algorithm using R [44]. Our implementation is based on the stable-PC algorithm in the R package pcalg [9] and the native R library parallel. The implementation has been tailored to work in Linux, MacOS, and Windows operating systems. The codes of parallel-PC with and without the memory efficient option together with the instructions of running the algorithm are available at: http://nugget.unisa.edu.au/ParallelPC"}, {"heading": "5 EXPERIMENTAL EVALUATION AND APPLICATIONS", "text": ""}, {"heading": "5.1 Datasets", "text": "In this section, we evaluate the efficiency of the proposed algorithm by applying it to five real world datasets and a synthetic dataset (see Table 1 for a summary of the datasets). The first three datasets are commonly used in research of inferring the regulatory relationships between microRNAs (a type of gene regulators) and genes. These datasets were downloaded from [45]. Meanwhile, the last three datasets are used for discovering the regulatory relationships between transcription factors (another type of gene regulators) and the target genes regulated by them. The S. cerevisiae dataset was provided by the authors of [12], and the last two datasets were downloaded from [46]. The number of samples and variables in each dataset together with the runtime of the stable-PC algorithm are summarised in Table 1. We choose these datasets for the experiments, as they require a wide range of runtime when applying the stable-PC algorithm. All of the datasets are available at: nugget.unisa.edu.au/ParallelPC"}, {"heading": "5.2 Efficiency evaluations", "text": "We apply the stable-PC algorithm, and the parallel-PC algorithm to all of the datasets and compare their performance in terms of the runtime. For each dataset, we run five versions of the parallel-PC algorithm using 4, 5, 6, 7, and 8 cores CPU respectively. Meanwhile, for the stable-PC algorithm, we used the implementation in the pcalg [9] R package. We choose the range from four to eight cores for the experiments, as a modern personal computer normally has four cores and a small server usually has eight cores. We run all the experiments on a Linux server with 2.7 GB memory and\n2.6 Ghz per core CPU. All experiments were run three times and the average value of the three runs was reported.\nAs shown in Figure 2, the runtime of the parallel-PC algorithm is much shorter compared to the stable-PC algorithm. Please note that the parallel-PC algorithm generates the results that are independent of the order of the variables in the datasets, and the output results are consistent with the stable-PC algorithm.\nIn the relatively small datasets, NCI-60, stable-PC took around 14 minutes to complete, but parallel-PC with 4-core CPU only spent around 5 minutes. For this small dataset, increasing the number of cores does not reduce the runtime significantly due to the cost of synchronisation between the cores.\nFor the BR51 and MCC datasets, it took more than one hour each for running the stable-PC algorithm. The 6-core parallel-PC algorithm only used less than one third (less than 20 minutes) of its counterpart\u2019s runtime. Meanwhile, there is a clear trend in the S. cerevisiae dataset that the more powerful CPU (up to 15 cores) we use, the shorter runtime the parallel-PC algorithm has.\nThe efficiency of the parallel-PC algorithm is even higher with big datasets. In the S.aureus dataset, the runtime of the stable-PC algorithm was around 8 hours. Meanwhile, using a 4-core version of the parallel-PC algorithm, it took only around two hours. This is a significant result for end users\nwho only use personal computers for doing their research experiments. The runtime is further reduced when using computer CPU with more cores. For instance, it took just more than one hour (76.1 minutes) for running this dataset with an 8-core CPU computer (server). With the DREAM5Insilico dataset, after more than 24 hours the stable-PC still could not produce any results, and we had to abort the job. This is a typical situation when applying the stable-PC algorithm to big datasets, as we may not be able to predict its running time on a real world dataset. However, our 4- core parallel-PC algorithm managed to finish within around 12 hours, and the 8-core version took less than 6 hours (341.2 minutes).\nThe experiment results suggest that our proposed algorithm has significantly shorter runtime than the stablePC algorithm. Given that our method generates the same output and requires no extra effort from users compared to that of the stable-PC algorithm, the parallel-PC is more useful in exploring causal relations in high dimensional data, especially gene expression data. Note that the provided software tool is not restricted to only personal PCs, and it is ready for clusters or super computers with CPUs of more cores. To provide more insight into the experiments in Figure 2, we run the experiments on a super computer (with a 48-core CPU) using up to 16 cores of the CPU and the results are in Figure 3. For most of the datasets, the runtime was not improved further when we used more than 14 cores of the CPU."}, {"heading": "5.3 Scalability", "text": "To compare the scalability of the parallel-PC algorithm and the stable-PC algorithm, we used the DREAM5-insilico dataset as a base and randomly sample from it four datasets with different numbers of samples (200, 400, 600, and 805) and five datasets with different numbers of variables (500, 750, 1000, 1250, and 1500).\nFigure 4 (top) shows that the runtime of the stable-PC algorithm increases significantly when the number of variables is increased in a dataset. The reason is that the number of CI tests is increased when the number of variables is increased. The parallel-PC algorithm suffers from the same problem, but the increasing trend of the runtime is less significant compared to that of the stable-PC algorithm.\nSimilarly, the runtime of both PC and parallel-PC goes up when we increase the number of samples in the dataset as shown in Figure 4 (bottom). However, the reason for runtime increase is not because of the increased number of CI tests, but due to the increase in the time taken to perform a test. When we increase the number of samples in the input dataset, the time for performing one CI test is increased, and thus resulting in the large increase in total runtime given that the number of CI tests is huge."}, {"heading": "5.4 Impact of the memory-efficient option", "text": "As mentioned in Section 4, the memory-efficient version of the algorithm uses less amount of memory at any time point while running the algorithm. We observed that peak memory usage in the algorithm with memory-efficient option (s=TRUE) is about half of that of the algorithm without\nusing the option across the six datasets in Table 1. However, there is a trade-off between memory efficiency and time efficiency as show in Table 2. The experiments were conducted on a MacBook Pro with 8GB RAM, 2.6 GHz. Running S. cervisiae with s = FALSE failed to finish due to memory exhaustion."}, {"heading": "6 APPLICATION IN INFERRING MIRNA-MRNA REGULATORY RELATIONSHIPS", "text": ""}, {"heading": "6.1 Computational methods for identifying miRNA targets", "text": "microRNAs (miRNAs) are important gene regulators at post-transcriptional level. They regulate gene expression by promoting messenger RNA (mRNA) degradation and repressing translation [47]. miRNAs control a wide range\nof biological processes and are involved in several types of cancers [47], [48]. In the last decade, predictions of miRNA functions through identifying miRNA-mRNA regulatory relationships by computational methods have increasingly achieved promising results. Computational approaches are proving to be effective in generating hypotheses to assist with the design of wet-lab experiments for confirming miRNA targets.\nSeveral methods have been proposed to identify miRNA-mRNA regulatory relationships. In the first stream, researchers identified miRNA-mRNA interactions using sequence data [49], [50], [51]. Although these methods can predict potential miRNA targets, the results may contain a high-rate of false positives and false negatives [52]. In the second stream of research, various computational methods have been devised to use expression profiles in the study of miRNA-mRNA regulatory relationships. The principle of these methods is to investigate if a change in the miRNA expression level would result in a change in the mRNA expression. Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].\nPlease refer to [45] for a survey of different miRNA target prediction methods."}, {"heading": "6.2 Inferring miRNA-mRNA regulatory relationships with efficient-IDA", "text": "In this section, we propose to use parallel-PC to modify the the causal inference method [13] for inferring miRNAmRNA regulatory relationships. The method is based on a causal inference approach called IDA (Intervention effect when the DAG is absent) [63]. IDA firstly learns the causal structure from data using the PC algorithm, it then uses docalculus to estimate the causal effect that a variable has on the other. The estimated causal effects simulate the effects of randomised controlled experiments. Although the method is proved to be effective in uncovering miRNA-mRNA regulatory relationships, it has high computational complexity and suffers from the order-dependent problem. In this section, we propose to tackle these problems by modifying the IDA-based method by using parallel-PC rather than PC as the first step of the IDA algorithm. The modified algorithm is called efficient-IDA and includes the following steps.\n\u2022 Step 1: Find miRNAs and mRNAs that are differentially expressed between different conditions, e.g. normal vs cancer. The expression profiles of those miRNAs and mRNAs will be combined into a dataset where rows are samples and columns are variables (miRNAs and mRNAs). In our experiments, we download from [45] three processed cancer datasets with differentially expressed profiles. \u2022 Step 2: Apply parallel-PC to learn the causal structure from data. parallel-PC will help tackle the efficiency and order-dependency problems of the original PC algorithm. \u2022 Step 3: Use do-calculus to estimate the causal effects that the miRNAs have on the mRNAs. For more information on this step, please refer to [13].\nThe running time of IDA is quite similar to the PC algorithm, as the time for the inference step (Step 3 as described above) is small. Therefore we do not compare the runtime of efficient-IDA and IDA again and please refer to Figures 2 and 3 for details. In the following sub-sections, we compare the performance of efficient-IDA and IDA in correctly identifying miRNA-mRNA regualtory relationships."}, {"heading": "6.3 efficient-IDA discovers more experimentally miRNA targets than IDA", "text": "We applied efficient-IDA and IDA to NCI-60, MCC, and BR51 and compare their performance on discovering the experimentally confirmed miRNA targets.\nTo validate the target predictions from the two methods, we used the combination of four different experimentally confirmed miRNA target databases as the ground truth. They are Tarbase v6.0 [64], miRecords v2013 [65], miRWalk v2.0 [66], and miRTarBase v4.5 [67]. Tarbase, miRecords, and miRTarBase include verified interactions that are manually curated from the literature, and miRWalk contains both the predicted and the experimentally validated interactions.\nRespectively for Tarbase, miRecords, miRWalk, and miRTarBase, we have 20095 interactions with 228 miRNAs, 21590 interactions with 195 miRNAs, 1710 interactions with 226 miRNAs, and 37372 interactions with 576 miRNAs. After removing the duplicates, we have in total 62858 unique interactions to be used in the validation.\nWe used two different ways to extract prediction results for the comparison. Firstly, we extracted the top 100 predicted targets of each of the miRNAs in a dataset and validate them against the experimentally confirmed miRNA targets. Secondly, we extracted the top 1000, 2000, 5000, and 10000 predicted miRNA-mRNA interactions of each method for validation.\nFigures 5 shows the comparison results of the number of confirmed miRNA targets of efficient-IDA and IDA using the two validation methods respectively. We can see from the figure that efficient-IDA discovers more confirmed miRNA targets than IDA in both of the cases in all datasets. The experiment results suggest that efficient-IDA is not just faster in runtime, but the order-independent property of efficient-IDA helps improve the accuracy of the causal inference method (IDA)."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "It is very important to explore causal relationships in real world high dimensional datasets. For instance, the discovered causal relationships can help elucidate the causes of fatal diseases from gene expression data. However, existing causal discovery method do not provide practical solutions to the problem. In this paper, we have developed an efficient algorithm for exploring causal relationships in high dimensional datasets based on the stable-PC algorithm and parallel computing framework. We modify the stable-PC algorithm and group the CI tests in the algorithm to enable the tests to be performed in parallel. Our method produces the causal relationships that are consistent with the stable-PC algorithm, but much more efficiently regarding the runtime. The experiment results from a wide range of commonly used real world datasets suggest the efficiency of the proposed method. parallel-PC also shows its usefulness by improving the efficiency and accuracy of IDA in inferring miRNA-mRNA regulatory relationships.\nThe effectiveness of the parallel-PC algorithm could be further improved with a job weighting and scheduling scheme. In the current version of parallel-PC, we divide the number of CI tests equally and distribute to different cores of the CPU. However, there is no guarantee that the execution time of different cores is the same. A smart job weighting and scheduling scheme would help speed up the algorithm.\nFinally, a straightforward but useful future work is to apply the parallel-PC algorithm to other PC related causal discovery and causal inference methods. Most of the constraint based methods are based on the idea of the PC algorithm directly or indirectly. The FCI [2] algorithm and RFCI [68] are based on the PC algorithm to learn causal graphs which allows latent variables. The CCD algorithm [69] for learning Markov equivalent class utilises the PC algorithm as the first step. The parallelised versions of these algorithms would help bridging the gap between computer science theory and their applications in practice."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work has been supported by Australian Research Council Discovery Project DP140103617."}], "references": [{"title": "Causality: models, reasoning, and inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Learning Bayesian networks", "author": ["R. Neapolitan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["C.W. Granger"], "venue": "Econometrica: Journal of the Econometric Society, 424\u2013438, 1969.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1969}, {"title": "Money, income, and causality", "author": ["C.A. Sims"], "venue": "The American Economic Review, vol. 62, no. 4, 540\u2013552, 1972.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "Counting labeled acyclic digraphs", "author": ["J. Robinson"], "venue": "New Directions in the Theory of Graphs: Proc. of the Third Ann Arbor Conf. on Graph Theory. New York: Academic Press, 239\u2013273, 1971.  JOURNAL OF  LTEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014  12", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1971}, {"title": "Equivalence and synthesis of causal models", "author": ["Verma", "J. Pearl"], "venue": "Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, 220\u2013227, 1990.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Causal inference using graphical models with the R package pcalg", "author": ["M. Kalisch", "M. M\u00e4chler", "D. Colombo", "M.H. Maathuis", "P. B\u00fchlmann"], "venue": "J STAT SOFW, vol. 47, no. 11, 1\u201326, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning bayesian networks with the bnlearn R package", "author": ["M. Scutari"], "venue": "J STAT SOFW, vol. 35, no. 3, 1\u201322, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Inferring gene regulatory networks from gene expression data by path consistency algorithm based on conditional mutual information", "author": ["X. Zhang", "X.M.Liu", "K. He"], "venue": "Bioinformatics, vol. 28, no. 1, 98-104, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting causal effects in large-scale systems from observational data", "author": ["H. M Maathuis", "D. Colombo", "M. Kalisch", "P. Buhlmann"], "venue": "Nature Methods, 7: 247\u2013249, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Inferring microRNA\u2013mRNA causal regulatory relationships from expression data", "author": ["T.D. Le", "L. Liu", "A. Tsykin", "G.J. Goodall", "B. Liu", "B.-Y. Sun", "J. Li"], "venue": "Bioinformatics, vol. 29, no. 6, 765\u2013771, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring conditionspecific miRNA activity from matched miRNA and mRNA expression data", "author": ["J. Zhang", "T.D. Le", "L. Liu", "B. Liu", "J. He"], "venue": "Bioinformatics, 30: 3070-7, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Identifying direct miRNA-mRNA causal regulatory relationships in heterogeneous data", "author": ["J. Zhang", "T.D. Le", "L. Liu", "B. Liu", "J. He"], "venue": "Journal of Biomedical Informatics, 52: 438-47, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable techniques for mining causal structures", "author": ["C. Silverstein", "S. Brin", "R. Motwani", "J. Ullman"], "venue": "Data Min. Knowl. Discov., vol. 4, no. 2, 163\u2013192, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Algorithms for large scale markov blanket discovery", "author": ["I. Tsamardinos", "C.F. Aliferis", "A. Statnikov", "E. Statnikov"], "venue": "Proceedings of the sixteenth international Florida artificial intelligence research society conference, 376\u2013381, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining causal association rules", "author": ["J. Li", "T.D. Le", "L. Liu", "J. Liu", "Z. Jin", "B. Sun"], "venue": "Proceedings of ICDM Workshop on Causal Discovery (CD), 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "From Observational Studies to Causal Rule Mining", "author": ["J. Li", "T.D. Le", "L. Liu", "J. Liu", "Z. Jin", "B. Sun", "S. Ma"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) , vol. 7, no. 3, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A modification of the PC algorithm yielding order-independent skeletons", "author": ["D. Colombo", "M.H. Maathuis"], "venue": "ArXiv e-prints, Nov. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A hybrid anytime algorithm for the construction of causal models from sparse data", "author": ["D. Dash", "M.J. Druzdzel"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 142\u2013149, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "A score based ranking of the edges for the pc algorithm", "author": ["A. Cano", "M. G\u00f3mez-Olmedo", "S. Moral"], "venue": "Proceedings of the Fourth European Workshop on Probabilistic Graphical Models, 41\u201348, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM, vol. 51, no. 1, 107\u2013 113, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine learning, vol. 20, no. 3, 197\u2013243, 1995.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Introduction to graphical modelling", "author": ["D. Edwards"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "A bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine learning, vol. 9, no. 4, pp. 309\u2013347, 1992.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "Causation, computation and discovery", "author": ["C. Glymour", "G. Cooper"], "venue": "1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning bayesian networks is np-hard", "author": ["D.M. Chickering", "D. Geiger", "D. Heckerman"], "venue": "Citeseer, Tech. Rep., 1994.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "A simple constraint-based algorithm for efficiently mining observational databases for causal relationships", "author": ["G.F. Cooper"], "venue": "Data Min. Knowl. Discov., vol. 1, no. 2, 203\u2013224, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Estimating high-dimensional directed acyclic graphs with the PC-algorithm", "author": ["M. Kalisch", "P. Buhlmann"], "venue": "JMLR, vol. 8, 613\u2013 636, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian belief networks for data mining", "author": ["H. Steck", "V. Tresp"], "venue": "Proceedings of the 2. Workshop on Data Mining und Data Warehousing als Grundlage moderner entscheidungsunterst\u00fctzender Systeme. Citeseer, 145\u2013154, 1999.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Some variations on the pc algorithm", "author": ["J. Abelln", "M. Gmez-olmedo", "S. Moral"], "venue": "Proceedings of the first European Workshop on Probabilistic Graphical Models, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian network induction via local neighborhoods", "author": ["D. Margaritis", "S. Thrun"], "venue": "Proceedings of Conference on Neural Information  Processing Systems (NIPS-12), S. Solla, T. Leen, and K. R. M\u00fcller, Eds. MIT Press, 1999.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Speculative markov blanket discovery for optimal feature selection", "author": ["S. Yaramakala", "D. Margaritis"], "venue": "Data Mining, Fifth IEEE International Conference on. IEEE, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Local causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation", "author": ["C.F. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X.D. Koutsoukos"], "venue": "JMLR, vol. 11, 171\u2013234, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Time and sample efficient discovery of markov blankets and direct causal relations", "author": ["I. Tsamardinos", "C.F. Aliferis", "A. Statnikov"], "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201903. New York, NY, USA: ACM, 673\u2013678, 2003.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards scalable and data efficient learning of markov boundaries", "author": ["J.M. Pe\u00f1a", "R. Nilsson", "J. Bj\u00f6rkegren", "J. Tegn\u00e9r"], "venue": "INT J APPROX REASON, vol. 45, no. 2, 211\u2013232, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast markov blanket discovery algorithm via local learning within single pass", "author": ["S. Fu", "M.C. Desmarais"], "venue": "Advances in Artificial Intelligence. Springer, 96\u2013107, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Discovery of causal rules using partial association", "author": ["Z. Jin", "J. Li", "L. Liu", "T.D. Le", "B. Sun", "R. Wang"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, 309\u2013318.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Practical approaches to causal relationship exploration", "author": ["J. Li", "L. Liu", "T.D. Le"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimised Implementations in the bnlearn R Package", "author": ["M. Scutari"], "venue": "arXiv 1406:7648, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks", "author": ["Y. Chen", "J. Tian", "O. Nikolova", "S. Aluru"], "venue": "http://arxiv.org/abs/1408.1664v1, 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Adjacency-faithfulness and conservative causal inference", "author": ["J. Ramsey", "J. Zhang", "P.L. Spirtes"], "venue": "arXiv preprint arXiv:1206.6843, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A Language and Environment for Statistical Computing, R Foundation for Statistical Computing", "author": ["R R Core Team"], "venue": "Vienna, Austria,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "From miRNA regulation to miRNA - TF co-regulation: computational approaches and challenges", "author": ["T. D Le", "L. Liu", "J. Zhang", "B. Liu", "J. Li"], "venue": "Briefings in Bioinformatics, 16: 475-96, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Wisdom of crowds for robust gene network inference", "author": ["D. Marbach", "J.C. Costello", "R. K\u00fcffner", "N.M. Vega", "R.J. Prill", "D.M. Camacho", "K.R. Allison", "M. Kellis", "J.J. Collins", "G. Stolovitzky"], "venue": "Nature Methods, vol. 9, 796\u2013804, 2012.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "MicroRNAs: genomics, biogenesis, mechanism, and function,", "author": ["D. P Bartel"], "venue": "Cell, 116:", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}, {"title": "Genomics of microRNA,", "author": ["V. N Kim", "J. W Nam"], "venue": "Trends Genetics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "microRNA targets in Drosophila,", "author": ["A. J Enright", "B. John", "U. Gaul", "T. Tuschl", "C. Sander C"], "venue": "Genome Biology,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}, {"title": "Conserved seed pairing, often flanked by adenosines, indicates that thousands of human genes are microRNA targets", "author": ["B. P Lewis", "C. B Burge", "D. P Bartel"], "venue": "Cell, 120: 15\u201320, 2005.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Combinatorial microRNA target predictions", "author": ["A. Krek", "D. Gr\u00fcn", "M. N Poy", "R. Wolf", "L. Rosenberg"], "venue": "Nature Genetics, 37: 495- 500, 2005.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "microRNA target predictions in animals", "author": ["N. Rajewsky"], "venue": "Nature Genetics, 38: S8-13, 2006.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Identifying mRNA targets of microRNA dysregulated in cancer: with application to clear cell Renal Cell Carcinoma", "author": ["H. Liu", "A. Brannon", "A. Reddy", "G. Alexe", "M. Seiler"], "venue": "BMC Systems Biology, 4: 51, 2010.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrated miRNA and mRNA expression profiling of the inflammatory breast cancer subtype", "author": ["I. Van der Auwera", "R. Limame", "P. Van Dam", "P. Vermeulen", "L. Dirix"], "venue": "British Journal of Cancer, 103: 532\u2013541, 2010.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2010}, {"title": "A Lasso regression model for the construction of microRNA-target regulatory networks", "author": ["Y. Lu", "Y. Zhou", "W. Qu", "M. Deng", "C. Zhang"], "venue": "Bioinformatics, 27: 2406\u20132413, 2011.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Quantification of miRNA-mRNA interactions", "author": ["A. Muniategui", "R. Nogales-Cadenas", "M. V\u00e1zquez", "X. L Aranguren", "X. Agirre"], "venue": "PloS One, 7: e30766, 2012.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovery of microRNA-mRNA modules via population-based probabilistic learning,", "author": ["J. G Joung", "K. B Hwang", "J.W Nam", "S.J Kim", "B. T Zhang"], "venue": "Bioinformatics, 23:", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Finding microRNA regulatory modules in human genome using rule induction.,", "author": ["D. H Tran", "K. Satou", "T. B Ho"], "venue": "BMC Bioinformatics,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2008}, {"title": "Using expression profiling data to identify human microRNA targets", "author": ["J. C Huang", "T. Babak", "T. W Corson", "G. Chua", "S. Khan"], "venue": "Nature Methods, 4: 1045\u20131050, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploring complex miRNA-mRNA regulatory networks by splitting-averaging strategy", "author": ["B. Liu", "J. Li", "A. Tsykin", "L. Liu", "A. B Gaur"], "venue": "BMC Bioinformatics, 19: 1\u201319, 2009.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Inferring microRNA and transcription factor regulatory networks in heterogeneous data", "author": ["T.D. Le", "L. Liu", "B. Liu", "A. Tsykin", "G.J. Goodall", "K. Satou", "J. Li"], "venue": "BMC Bioinformatics, vol. 14:92, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble Methods for MiRNA Target Prediction from Expression Data", "author": ["T.D. Le", "J. Zhang", "L. Liu", "J. Li"], "venue": "PloS one, 10(6), e0131627, 2015.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating highdimensional intervention effects from observational data", "author": ["H.M. Maathuis", "M. Kalisch", "P. Buhlmann"], "venue": "Annals of Statistics, vol. 37, no. 6, 3133\u20133164, 2009.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2009}, {"title": "Tarbase 6.0: capturing the exponential growth of miRNA targets with experimental support", "author": ["T. Vergoulis", "I.S. Vlachos", "P. Alexiou", "G. Georgakilas", "M. Maragkakis"], "venue": "Nucleic Acids Research, 40: D222\u2013D229, 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "mirecords: an integrated resource for microrna-target interactions", "author": ["F. Xiao", "Z. Zuo", "G. Cai", "S. Kang", "X. Gao"], "venue": "Nucleic Acids Research, 37: D105\u2013D110, 2009.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2009}, {"title": "mirwalk\u2013database: prediction of possible mirna binding sites by walking the genes of three genomes", "author": ["H. Dweep", "C. Sticht", "P. Pandey", "N. Gretz"], "venue": "Journal of Biomedical Informatics, 44: 839\u2013847, 2011.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "miR- TarBase update 2014: an information resource for experimentally validated miRNA-target interactions", "author": ["S. D Hsu", "Y. T Tseng", "S. Shrestha", "Y. L Lin", "A. Khaleel"], "venue": "Nucleic Acids Research, 42: D78-85, 2014.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning high-dimensional directed acyclic graphs with latent and selection variables", "author": ["D. Colombo", "M.H. Maathuis", "M. Kalisch", "T.S. Richardson"], "venue": "Annals of Statistics, vol. 40, no. 1, 294\u2013321, 2012.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Many scientists have attempted to design algorithms of inferring causality from observational data over the last few decades [1], [2], [3], [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Many scientists have attempted to design algorithms of inferring causality from observational data over the last few decades [1], [2], [3], [4].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "Many scientists have attempted to design algorithms of inferring causality from observational data over the last few decades [1], [2], [3], [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "As a result, there has been more and more evidence demonstrating the possibility of discovering causal relationships from non-experimental data in different research areas [1], [5], [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "As a result, there has been more and more evidence demonstrating the possibility of discovering causal relationships from non-experimental data in different research areas [1], [5], [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "As a result, there has been more and more evidence demonstrating the possibility of discovering causal relationships from non-experimental data in different research areas [1], [5], [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "Especially, two Nobel prizes were awarded for such causality methods in the field of Economics in 2003 and 2011 respectively [5], [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Especially, two Nobel prizes were awarded for such causality methods in the field of Economics in 2003 and 2011 respectively [5], [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "In this framework, causal relationships are represented with a Directed Acyclic Graph (DAG) [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "Learning a DAG from data is highly challenging and complex as the number of possible DAGs is super-exponential to the number of nodes [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "A well-known constraint based algorithm is the Inductive Causation (IC) algorithm proposed in [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "The PC algorithm has been implemented in various open-source sofware such as TETRAD [2], pcalg [9], and bnlearn [10], and has become a reliable tool for causal explorations.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "The PC algorithm has been implemented in various open-source sofware such as TETRAD [2], pcalg [9], and bnlearn [10], and has become a reliable tool for causal explorations.", "startOffset": 112, "endOffset": 116}, {"referenceID": 9, "context": "There have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "There have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "There have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "There have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "There have been several applications of the PC algorithm in Bioinformatics [11], [12], [13], [14], [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "[11] applied the PC algorithm which used mutual information for the conditional independence tests to learn the gene regulatory networks from gene expression data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] used the PC algorithm to learn the causal structure of the gene regulatory networks, then they applied do-calculus [1] to the learnt network to infer the causal effect that a gene has on the other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[12] used the PC algorithm to learn the causal structure of the gene regulatory networks, then they applied do-calculus [1] to the learnt network to infer the causal effect that a gene has on the other.", "startOffset": 120, "endOffset": 123}, {"referenceID": 11, "context": "[13] applied the approach in [12] to predict the targets of microRNAs (miRNAs, an important class of gene regulators at the post-transcriptional level).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] applied the approach in [12] to predict the targets of microRNAs (miRNAs, an important class of gene regulators at the post-transcriptional level).", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "[14] inferred the miRNA-mRNA interactions that had different causal effects between biological conditions, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Moreover, the PC algorithm is able to distinguish direct interactions from indirect interactions in gene regulatory networks [11], [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "Moreover, the PC algorithm is able to distinguish direct interactions from indirect interactions in gene regulatory networks [11], [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Several methods [16], [17], [18], [19] have been proposed to improve the efficiency of the PC algorithm.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Several methods [16], [17], [18], [19] have been proposed to improve the efficiency of the PC algorithm.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "Several methods [16], [17], [18], [19] have been proposed to improve the efficiency of the PC algorithm.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "Several methods [16], [17], [18], [19] have been proposed to improve the efficiency of the PC algorithm.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "[20] has shown experimentally that around 40% of the edges (2000 edges) learnt from a real gene expression dataset are not stable, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "To overcome the orderdependence of the PC algorithm, researchers proposed several modifications to the procedure of the algorithm [2], [20], [21], [22].", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": "To overcome the orderdependence of the PC algorithm, researchers proposed several modifications to the procedure of the algorithm [2], [20], [21], [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "To overcome the orderdependence of the PC algorithm, researchers proposed several modifications to the procedure of the algorithm [2], [20], [21], [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "[20] proposed a modified version of the PC algorithm called stable-PC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "org/) with 1643 variables and 805 samples shows that the order-independence version of the PC algorithm [20] takes more than 24 hours to run (unfinished).", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "The last decade has seen a fast growth of parallel computing frameworks, such as MapReduce framework [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "In this paper, we present a method to parallelise the order-independence PC algorithm in [20] for high dimensional datasets.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "As an application, we modified the causal inference method in [13], by replacing the PC algorithm with the parallel-PC", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The well-recognised methods for causality discovery are based on probabilistic graphical modeling [1], [24].", "startOffset": 98, "endOffset": 101}, {"referenceID": 22, "context": "The well-recognised methods for causality discovery are based on probabilistic graphical modeling [1], [24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "The structure of these graphical models is a DAG (directed acyclic graph), with its nodes representing random variables and edges representing dependencies between the variables [25].", "startOffset": 178, "endOffset": 182}, {"referenceID": 22, "context": "The search and score methods [24], [26], [27] search for all possible DAGs whilst using a scoring function to measure the fit of each DAG to the data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "The search and score methods [24], [26], [27] search for all possible DAGs whilst using a scoring function to measure the fit of each DAG to the data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "The search and score methods [24], [26], [27] search for all possible DAGs whilst using a scoring function to measure the fit of each DAG to the data.", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "However, learning Bayesian networks using this approach is an NPhard problem [28], and the proposed methods following this approach were only able to cater for datasets with a limited number of variables, and therefore may not be suitable for high dimensional datasets.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Meanwhile, the constraint based approach [1], [16], [29] uses CI tests to remove non-causal relationships between variables.", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "Meanwhile, the constraint based approach [1], [16], [29] uses CI tests to remove non-causal relationships between variables.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "Meanwhile, the constraint based approach [1], [16], [29] uses CI tests to remove non-causal relationships between variables.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "These methods are suitable for sparse datasets in practice [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Steck and Tresp [31] used the necessary path condition to reduce the number of CI tests in the PC algorithm.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Other researchers [29], [32] proposed to replace the CI test, which is normally based on the Chi-square statistical test, with the Bayesian statistical tests.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "Other researchers [29], [32] proposed to replace the CI test, which is normally based on the Chi-square statistical test, with the Bayesian statistical tests.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "Meanwhile, the other constraint based methods [1] infer causal relationships by performing CI tests and searching for specific causal structures.", "startOffset": 46, "endOffset": 49}, {"referenceID": 27, "context": "For instance, Cooper [29] proposed the LCD method to learn only a small portion of edges rather than the complete network.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "[16] proposed the similar method that focuses on the CCU structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Some well-known methods in this approach include GS (Grow and Shrink) [33], IAMB (Incremental Association Markov Blanket) [17], and IAMB variants [34].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Some well-known methods in this approach include GS (Grow and Shrink) [33], IAMB (Incremental Association Markov Blanket) [17], and IAMB variants [34].", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "Some well-known methods in this approach include GS (Grow and Shrink) [33], IAMB (Incremental Association Markov Blanket) [17], and IAMB variants [34].", "startOffset": 146, "endOffset": 150}, {"referenceID": 33, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 114, "endOffset": 118}, {"referenceID": 36, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 131, "endOffset": 135}, {"referenceID": 33, "context": "This approach attracts several methods namely HITONPC/MB [35], MMPC-MB [36], semi-Interleaved HITON-PC [35], PCMB [37], and IPC-MB [38] (see [35] for a review).", "startOffset": 141, "endOffset": 145}, {"referenceID": 37, "context": "[39] proposed an efficient method for testing the persistent associations between the", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18], [19] integrated the ideas of retrospective cohort study with the association rule mining framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], [19] integrated the ideas of retrospective cohort study with the association rule mining framework.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "These works were extended in [40], and the software tools were also provided.", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "Scutari [41] utilised parallel computing technique for constraint based local causal discovery methods.", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "[42] proposed a method to parallelise the Bayesian network structure learning algorithm using search and score approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Definition 1 (d-separation) [43].", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "[20] proposed a modification to the original-PC algorithm to obtain a stable output skeleton which does not depend on how variables are ordered in the input dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "We implement our parallel-PC algorithm using R [44].", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Our implementation is based on the stable-PC algorithm in the R package pcalg [9] and the native R library parallel.", "startOffset": 78, "endOffset": 81}, {"referenceID": 43, "context": "These datasets were downloaded from [45].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "cerevisiae dataset was provided by the authors of [12], and the last two datasets were downloaded from [46].", "startOffset": 50, "endOffset": 54}, {"referenceID": 44, "context": "cerevisiae dataset was provided by the authors of [12], and the last two datasets were downloaded from [46].", "startOffset": 103, "endOffset": 107}, {"referenceID": 7, "context": "Meanwhile, for the stable-PC algorithm, we used the implementation in the pcalg [9] R package.", "startOffset": 80, "endOffset": 83}, {"referenceID": 45, "context": "They regulate gene expression by promoting messenger RNA (mRNA) degradation and repressing translation [47].", "startOffset": 103, "endOffset": 107}, {"referenceID": 45, "context": "of biological processes and are involved in several types of cancers [47], [48].", "startOffset": 69, "endOffset": 73}, {"referenceID": 46, "context": "of biological processes and are involved in several types of cancers [47], [48].", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "In the first stream, researchers identified miRNA-mRNA interactions using sequence data [49], [50], [51].", "startOffset": 88, "endOffset": 92}, {"referenceID": 48, "context": "In the first stream, researchers identified miRNA-mRNA interactions using sequence data [49], [50], [51].", "startOffset": 94, "endOffset": 98}, {"referenceID": 49, "context": "In the first stream, researchers identified miRNA-mRNA interactions using sequence data [49], [50], [51].", "startOffset": 100, "endOffset": 104}, {"referenceID": 50, "context": "Although these methods can predict potential miRNA targets, the results may contain a high-rate of false positives and false negatives [52].", "startOffset": 135, "endOffset": 139}, {"referenceID": 51, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 56, "endOffset": 60}, {"referenceID": 52, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 62, "endOffset": 66}, {"referenceID": 53, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 86, "endOffset": 90}, {"referenceID": 54, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 92, "endOffset": 96}, {"referenceID": 55, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 143, "endOffset": 147}, {"referenceID": 56, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 168, "endOffset": 172}, {"referenceID": 57, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 200, "endOffset": 204}, {"referenceID": 58, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 206, "endOffset": 210}, {"referenceID": 59, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 212, "endOffset": 216}, {"referenceID": 11, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 252, "endOffset": 256}, {"referenceID": 13, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 258, "endOffset": 262}, {"referenceID": 60, "context": "Some highlights of the methods are correlation analysis [53], [54], regression models [55], [56], populationbased probabilistic learning model [57], rule based methods [58], Bayesian network learning [59], [60], [61], causal inference techniques [13], [14], [15], and ensemble methods [62].", "startOffset": 285, "endOffset": 289}, {"referenceID": 43, "context": "Please refer to [45] for a survey of different miRNA target prediction methods.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "In this section, we propose to use parallel-PC to modify the the causal inference method [13] for inferring miRNAmRNA regulatory relationships.", "startOffset": 89, "endOffset": 93}, {"referenceID": 61, "context": "The method is based on a causal inference approach called IDA (Intervention effect when the DAG is absent) [63].", "startOffset": 107, "endOffset": 111}, {"referenceID": 43, "context": "In our experiments, we download from [45] three processed cancer datasets with differentially expressed profiles.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "For more information on this step, please refer to [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 62, "context": "0 [64], miRecords v2013 [65], miRWalk v2.", "startOffset": 2, "endOffset": 6}, {"referenceID": 63, "context": "0 [64], miRecords v2013 [65], miRWalk v2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 64, "context": "0 [66], and miRTarBase v4.", "startOffset": 2, "endOffset": 6}, {"referenceID": 65, "context": "5 [67].", "startOffset": 2, "endOffset": 6}, {"referenceID": 66, "context": "The FCI [2] algorithm and RFCI [68] are based on the PC algorithm to learn causal graphs which allows latent variables.", "startOffset": 31, "endOffset": 35}], "year": 2016, "abstractText": "Discovering causal relationships from observational data is a crucial problem and it has applications in many research areas. The PC algorithm is the state-of-the-art constraint based method for causal discovery. However, runtime of the PC algorithm, in the worst-case, is exponential to the number of nodes (variables), and thus it is inefficient when being applied to high dimensional data, e.g. gene expression datasets. On another note, the advancement of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers. There is a significant motivation for designing a parallelised PC algorithm that is suitable for personal computers and does not require end users\u2019 parallel computing knowledge beyond their competency in using the PC algorithm. In this paper, we develop parallel-PC, a fast and memory efficient PC algorithm using the parallel computing technique. We apply our method to a range of synthetic and real-world high dimensional datasets. Experimental results on a dataset from the DREAM 5 challenge show that the original PC algorithm could not produce any results after running more than 24 hours; meanwhile, our parallel-PC algorithm managed to finish within around 12 hours with a 4-core CPU computer, and less than 6 hours with a 8-core CPU computer. Furthermore, we integrate parallel-PC into a causal inference method for inferring miRNA-mRNA regulatory relationships. The experimental results show that parallel-PC helps improve both the efficiency and accuracy of the causal inference algorithm.", "creator": "LaTeX with hyperref package"}}}