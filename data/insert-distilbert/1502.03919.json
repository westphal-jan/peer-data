{"id": "1502.03919", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Policy Gradient for Coherent Risk Measures", "abstract": "we provide comprehensive sampling - based algorithms for optimization under establishing a coherent - risk objective. the class of coherent - risk causal measures is widely accepted in finance and operations research, among other fields, and encompasses popular risk - measures such as the conditional value at absolute risk ( cvar ) and the mean - semi - deviation. our typical approach arguably is suitable for problems in spaces which the tunable parameters control by the distribution of the cost, such as in reinforcement learning with a parameterized policy ; such problems subsequently cannot be solved using previous approaches. we consider both static risk measures, and time - consistent dynamic risk measures. for static risk measures, our approach is in the spirit of temporal policy gradient algorithms, while for the dynamic risk measures our approach is actor - critic style.", "histories": [["v1", "Fri, 13 Feb 2015 09:16:24 GMT  (100kb,D)", "https://arxiv.org/abs/1502.03919v1", null], ["v2", "Mon, 8 Jun 2015 06:31:42 GMT  (250kb,D)", "http://arxiv.org/abs/1502.03919v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["aviv tamar", "yinlam chow", "mohammad ghavamzadeh", "shie mannor"], "accepted": true, "id": "1502.03919"}, "pdf": {"name": "1502.03919.pdf", "metadata": {"source": "CRF", "title": "Policy Gradient for Coherent Risk Measures", "authors": ["Aviv Tamar", "Yinlam Chow"], "emails": ["avivt@tx.technion.ac.il", "ychow@stanford.edu", "ghavamza@adobe.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Risk-sensitive optimization considers problems in which the objective involves a risk measure of the random cost, in contrast to the typical expected cost objective. Such problems are important when the decision-maker wishes to manage the variability of the cost, in addition to its expected outcome, and are standard in various applications of finance and operations research. In reinforcement learning (RL) [33], risk-sensitive objectives have gained popularity as a means to regularize the variability of the total (discounted) cost/reward in a Markov decision process (MDP).\nMany risk objectives have been investigated in the literature and applied to RL, such as the celebrated Markowitz mean-variance model [19], Value-at-Risk (VaR) and Conditional Value at Risk (CVaR) [22, 35, 26, 12, 10, 36]. The view taken in this paper is that the preference of one risk measure over another is problem-dependent and depends on factors such as the cost distribution, sensitivity to rare events, ease of estimation from data, and computational tractability of the optimization problem. However, the highly influential paper of Artzner et al. [2] identified a set of natural properties that are desirable for a risk measure to satisfy. Risk measures that satisfy these properties are termed coherent and have obtained widespread acceptance in financial applications, among others. We focus on such coherent measures of risk in this work.\nFor sequential decision problems, such as MDPs, another desirable property of a risk measure is time consistency. A time-consistent risk measure satisfies a \u201cdynamic programming\u201d style property: if a strategy is risk-optimal for an n-stage problem, then the component of the policy from the t-th\nar X\niv :1\n50 2.\n03 91\n9v 2\n[ cs\n.A I]\ntime until the end (where t < n) is also risk-optimal (see principle of optimality in [5]). The recently proposed class of dynamic Markov coherent risk measures [30] satisfies both the coherence and time consistency properties.\nIn this work, we present policy gradient algorithms for RL with a coherent risk objective. Our approach applies to the whole class of coherent risk measures, thereby generalizing and unifying previous approaches that have focused on individual risk measures. We consider both static coherent risk of the total discounted return from an MDP and time-consistent dynamic Markov coherent risk. Our main contribution is formulating the risk-sensitive policy-gradient under the coherent-risk framework. More specifically, we provide:\n\u2022 A new formula for the gradient of static coherent risk that is convenient for approximation using sampling.\n\u2022 An algorithm for the gradient of general static coherent risk that involves sampling with convex programming and a corresponding consistency result.\n\u2022 A new policy gradient theorem for Markov coherent risk, relating the gradient to a suitable value function and a corresponding actor-critic algorithm.\nSeveral previous results are special cases of the results presented here; our approach allows to rederive them in greater generality and simplicity.\nRelated Work Risk-sensitive optimization in RL for specific risk functions has been studied recently by several authors. [8] studied exponential utility functions, [22], [35], [26] studied meanvariance models, [10], [36] studied CVaR in the static setting, and [25], [11] studied dynamic coherent risk for systems with linear dynamics. Our paper presents a general method for the whole class of coherent risk measures (both static and dynamic) and is not limited to a specific choice within that class, nor to particular system dynamics.\nReference [24] showed that an MDP with a dynamic coherent risk objective is essentially a robust MDP. The planning for large scale MDPs was considered in [37], using an approximation of the value function. For many problems, approximation in the policy space is more suitable (see, e.g., [18]). Our sampling-based RL-style approach is suitable for approximations both in the policy and value function, and scales-up to large or continuous MDPs. We do, however, make use of a technique of [37] in a part of our method.\nOptimization of coherent risk measures was thoroughly investigated by Ruszczynski and Shapiro [31] (see also [32]) for the stochastic programming case in which the policy parameters do not affect the distribution of the stochastic system (i.e., the MDP trajectory), but only the reward function, and thus, this approach is not suitable for most RL problems. For the case of MDPs and dynamic risk, [30] proposed a dynamic programming approach. This approach does not scale-up to large MDPs, due to the \u201ccurse of dimensionality\u201d. For further motivation of risk-sensitive policy gradient methods, we refer the reader to [22, 35, 26, 10, 36]."}, {"heading": "2 Preliminaries", "text": "Consider a probability space (\u2126,F , P\u03b8), where \u2126 is the set of outcomes (sample space), F is a \u03c3-algebra over \u2126 representing the set of events we are interested in, and P\u03b8 \u2208 B, where B :={ \u03be : \u222b \u03c9\u2208\u2126 \u03be(\u03c9) = 1, \u03be \u2265 0 } is the set of probability distributions, is a probability measure over F parameterized by some tunable parameter \u03b8 \u2208 RK . In the following, we suppress the notation of \u03b8 in \u03b8-dependent quantities.\nTo ease the technical exposition, in this paper we restrict our attention to finite probability spaces, i.e., \u2126 has a finite number of elements. Our results can be extended to the Lp-normed spaces without loss of generality, but the details are omitted for brevity.\nDenote by Z the space of random variables Z : \u2126 7\u2192 (\u2212\u221e,\u221e) defined over the probability space (\u2126,F , P\u03b8). In this paper, a random variable Z \u2208 Z is interpreted as a cost, i.e., the smaller the realization of Z, the better. For Z,W \u2208 Z , we denote by Z \u2264 W the point-wise partial order, i.e., Z(\u03c9) \u2264 W (\u03c9) for all \u03c9 \u2208 \u2126. We denote by E\u03be[Z] . = \u2211 \u03c9\u2208\u2126 P\u03b8(\u03c9)\u03be(\u03c9)Z(\u03c9) a \u03be-weighted expectation of Z.\nAn MDP is a tuple M = (X ,A, C, P, \u03b3, x0), where X and A are the state and action spaces; C(x) \u2208 [\u2212Cmax, Cmax] is a bounded, deterministic, and state-dependent cost; P (\u00b7|x, a) is the transition probability distribution; \u03b3 is a discount factor; and x0 is the initial state.1 Actions are chosen according to a \u03b8-parameterized stationary Markov2 policy \u00b5\u03b8(\u00b7|x). We denote by x0, a0, . . . , xT , aT a trajectory of length T drawn by following the policy \u00b5\u03b8 in the MDP."}, {"heading": "2.1 Coherent Risk Measures", "text": "A risk measure is a function \u03c1 : Z \u2192 R that maps an uncertain outcome Z to the extended real line R \u222a {+\u221e,\u2212\u221e}, e.g., the expectation E [Z] or the conditional value-at-risk (CVaR) min\u03bd\u2208R { \u03bd +\n1 \u03b1E [ (Z \u2212 \u03bd)+ ]} . A risk measure is called coherent, if it satisfies the following conditions for all Z,W \u2208 Z [2]:\nA1 Convexity: \u2200\u03bb \u2208 [0, 1], \u03c1 ( \u03bbZ + (1\u2212 \u03bb)W ) \u2264 \u03bb\u03c1(Z) + (1\u2212 \u03bb)\u03c1(W );\nA2 Monotonicity: if Z \u2264W , then \u03c1(Z) \u2264 \u03c1(W ); A3 Translation invariance: \u2200a\u2208R, \u03c1(Z + a) = \u03c1(Z) + a; A4 Positive homogeneity: if \u03bb \u2265 0, then \u03c1(\u03bbZ) = \u03bb\u03c1(Z).\nIntuitively, these condition ensure the \u201crationality\u201d of single-period risk assessments: A1 ensures that diversifying an investment will reduce its risk; A2 guarantees that an asset with a higher cost for every possible scenario is indeed riskier; A3, also known as \u2018cash invariance\u2019, means that the deterministic part of an investment portfolio does not contribute to its risk; the intuition behind A4 is that doubling a position in an asset doubles its risk. We further refer the reader to [2] for a more detailed motivation of coherent risk.\nThe following representation theorem [32] shows an important property of coherent risk measures that is fundamental to our gradient-based approach.\nTheorem 2.1. A risk measure \u03c1 : Z \u2192 R is coherent if and only if there exists a convex bounded and closed set U \u2282 B such that3\n\u03c1(Z) = max \u03be : \u03beP\u03b8\u2208U(P\u03b8)\nE\u03be[Z]. (1)\nThe result essentially states that any coherent risk measure is an expectation w.r.t. a worst-case density function \u03beP\u03b8, chosen adversarially from a suitable set of test density functions U(P\u03b8), referred to as risk envelope. Moreover, it means that any coherent risk measure is uniquely represented by its risk envelope. Thus, in the sequel, we shall interchangeably refer to coherent risk-measures either by their explicit functional representation, or by their corresponding risk-envelope.\nIn this paper, we assume that the risk envelop U(P\u03b8) is given in a canonical convex programming formulation, and satisfies the following conditions.\nAssumption 2.2 (The General Form of Risk Envelope). For each given policy parameter \u03b8 \u2208 RK , the risk envelope U of a coherent risk measure can be written as\nU(P\u03b8) = { \u03beP\u03b8 : ge(\u03be, P\u03b8) = 0, \u2200e \u2208 E , fi(\u03be, P\u03b8) \u2264 0, \u2200i \u2208 I, \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8(\u03c9) = 1, \u03be(\u03c9) \u2265 0 } , (2) where each constraint ge(\u03be, P\u03b8) is an affine function in \u03be, each constraint fi(\u03be, P\u03b8) is a convex function in \u03be, and there exists a strictly feasible point \u03be. E and I here denote the sets of equality and inequality constraints, respectively. Furthermore, for any given \u03be \u2208 B, fi(\u03be, p) and ge(\u03be, p) are twice differentiable in p, and there exists a M > 0 such that\nmax { max i\u2208I \u2223\u2223\u2223\u2223dfi(\u03be, p)dp(\u03c9) \u2223\u2223\u2223\u2223 ,maxe\u2208E \u2223\u2223\u2223\u2223dge(\u03be, p)dp(\u03c9) \u2223\u2223\u2223\u2223} \u2264M, \u2200\u03c9 \u2208 \u2126.\n1Our results may easily be extended to random costs, state-action dependent costs, and random initial states. 2For the dynamic Markov risk we study, an optimal policy is stationary Markov, while this is not necessarily the case for the static risk. Our results can be extended to history-dependent policies or stationary Markov policies on a state space augmented with the accumulated cost. The latter has shown to be sufficient for optimizing the CVaR risk [4].\n3When we study risk in MDPs, the risk envelop U(P\u03b8) in Eq. 1 also depends on the state x.\nAssumption 2.2 implies that the risk envelope U(P\u03b8) is known in an explicit form. From Theorem 6.6 of [32], in the case of a finite probability space, \u03c1 is a coherent risk if and only if U(P\u03b8) is a convex and compact set. This justifies the affine assumption of ge and the convex assumption of fi. Moreover, the additional assumption on the smoothness of the constraints holds for many popular coherent risk measures, such as the CVaR, the mean-semi-deviation, and spectral risk measures [1]."}, {"heading": "2.2 Dynamic Risk Measures", "text": "The risk measures defined above do not take into account any temporal structure that the random variable might have, such as when it is associated with the return of a trajectory in the case of MDPs. In this sense, such risk measures are called static. Dynamic risk measures, on the other hand, explicitly take into account the temporal nature of the stochastic outcome. A primary motivation for considering such measures is the issue of time consistency, usually defined as follows [30]: if a certain outcome is considered less risky in all states of the world at stage t + 1, then it should also be considered less risky at stage t. Example 2.1 in [16] shows the importance of time consistency in the evaluation of risk in a dynamic setting. It illustrates that for multi-period decision-making, optimizing a static measure can lead to \u201ctime-inconsistent\u201d behavior. Similar paradoxical results could be obtained with other risk metrics; we refer the readers to [30] and [16] for further insights.\nMarkov Coherent Risk Measures. Markov risk measures were introduced in [30] and are a useful class of dynamic time-consistent risk measures that are particularly important for our study of risk in MDPs. For a T -length horizon and MDPM, the Markov coherent risk measure \u03c1T (M) is\n\u03c1T (M) = C(x0) + \u03b3\u03c1 ( C(x1) + . . .+ \u03b3\u03c1 ( C(xT\u22121) + \u03b3\u03c1 ( C(xT ) ))) , (3)\nwhere \u03c1 is a static coherent risk measure that satisfies Assumption 2.2 and x0, . . . , xT is a trajectory drawn from the MDPM under policy \u00b5\u03b8. It is important to note that in (3), each static coherent risk \u03c1 at state x \u2208 X is induced by the transition probability P\u03b8(\u00b7|x) = \u2211 a\u2208A P (x\n\u2032|x, a)\u00b5\u03b8(a|x). We also define \u03c1\u221e(M) . = limT\u2192\u221e \u03c1T (M), which is well-defined since \u03b3 < 1 and the cost is bounded. We further assume that \u03c1 in (3) is a Markov risk measure, i.e., the evaluation of each static coherent risk measure \u03c1 is not allowed to depend on the whole past."}, {"heading": "3 Problem Formulation", "text": "In this paper, we are interested in solving two risk-sensitive optimization problems. Given a random variable Z and a static coherent risk measure \u03c1 as defined in Section 2, the static risk problem (SRP) is given by\nmin \u03b8 \u03c1(Z). (4)\nFor example, in an RL setting, Z may correspond to the cumulative discounted cost Z = C(x0) + \u03b3C(x1) + \u00b7 \u00b7 \u00b7+ \u03b3TC(xT ) of a trajectory induced by an MDP with a policy parameterized by \u03b8. For an MDPM and a dynamic Markov coherent risk measure \u03c1T as defined by Eq. 3, the dynamic risk problem (DRP) is given by\nmin \u03b8\n\u03c1\u221e(M). (5)\nExcept for very limited cases, there is no reason to hope that neither the SRP in (4) nor the DRP in (5) should be tractable problems, since the dependence of the risk measure on \u03b8 may be complex and non-convex. In this work, we aim towards a more modest goal and search for a locally optimal \u03b8. Thus, the main problem that we are trying to solve in this paper is how to calculate the gradients of the SRP\u2019s and DRP\u2019s objective functions\n\u2207\u03b8\u03c1(Z) and \u2207\u03b8\u03c1\u221e(M).\nWe are interested in non-trivial cases in which the gradients cannot be calculated analytically. In the static case, this would correspond to a non-trivial dependence of Z on \u03b8. For dynamic risk, we also consider cases where the state space is too large for a tractable computation. Our approach for dealing with such difficult cases is through sampling. We assume that in the static case, we may obtain i.i.d. samples of the random variable Z. For the dynamic case, we assume that for each state and action (x, a) of the MDP, we may obtain i.i.d. samples of the next state x\u2032 \u223c P (\u00b7|x, a). We show that sampling may indeed be used in both cases to devise suitable estimators for the gradients.\nTo finally solve the SRP and DRP problems, a gradient estimate may be plugged into a standard stochastic gradient descent (SGD) algorithm for learning a locally optimal solution to (4) and (5). From the structure of the dynamic risk in Eq. 3, one may think that a gradient estimator for \u03c1(Z) may help us to estimate the gradient \u2207\u03b8\u03c1\u221e(M). Indeed, we follow this idea and begin with estimating the gradient in the static risk case."}, {"heading": "4 Gradient Formula for Static Risk", "text": "In this section, we consider a static coherent risk measure \u03c1(Z) and propose sampling-based estimators for \u2207\u03b8\u03c1(Z). We make the following assumption on the policy parametrization, which is standard in the policy gradient literature [18]. Assumption 4.1. The likelihood ratio\u2207\u03b8 logP (\u03c9) is well-defined and bounded for all \u03c9\u2208\u2126.\nMoreover, our approach implicitly assumes that given some \u03c9 \u2208 \u2126, \u2207\u03b8 logP (\u03c9) may be easily calculated. This is also a standard requirement for policy gradient algorithms [18] and is satisfied in various applications such as queueing systems, inventory management, and financial engineering (see, e.g., the survey by Fu [14]).\nUsing Theorem 2.1 and Assumption 2.2, for each \u03b8, we have that \u03c1(Z) is the solution to the convex optimization problem (1) (for that value of \u03b8). The Lagrangian function of (1), denoted by L\u03b8(\u03be, \u03bb P , \u03bbE , \u03bbI), may be written as\nL\u03b8(\u03be, \u03bb P, \u03bbE, \u03bbI)= \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8(\u03c9)Z(\u03c9)\u2212\u03bbP (\u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8(\u03c9)\u22121 ) \u2212 \u2211 e\u2208E \u03bbE(e)ge(\u03be,P\u03b8)\u2212 \u2211 i\u2208I \u03bbI(i)fi(\u03be,P\u03b8). (6) The convexity of (1) and its strict feasibility due to Assumption 2.2 implies that L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) has a non-empty set of saddle points S. The next theorem presents a formula for the gradient \u2207\u03b8\u03c1(Z). As we shall subsequently show, this formula is particularly convenient for devising sampling based estimators for\u2207\u03b8\u03c1(Z). Theorem 4.2. Let Assumptions 2.2 and 4.1 hold. For any saddle point (\u03be\u2217\u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) \u2208 S of (6), we have\n\u2207\u03b8\u03c1(Z) = E\u03be\u2217\u03b8 [ \u2207\u03b8 logP (\u03c9)(Z \u2212 \u03bb\u2217,P\u03b8 ) ] \u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8 (e)\u2207\u03b8ge(\u03be \u2217 \u03b8 ;P\u03b8)\u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8 (i)\u2207\u03b8fi(\u03be \u2217 \u03b8 ;P\u03b8).\nThe proof of this theorem, given in the supplementary material, involves an application of the Envelope theorem [21] and a standard \u2018likelihood-ratio\u2019 trick. We now demonstrate the utility of Theorem 4.2 with several examples in which we show that it generalizes previously known results, and also enables deriving new useful gradient formulas."}, {"heading": "4.1 Example 1: CVaR", "text": "The CVaR at level \u03b1 \u2208 [0, 1] of a random variable Z, denoted by \u03c1CVaR(Z;\u03b1), is a very popular coherent risk measure [28], defined as\n\u03c1CVaR(Z;\u03b1) . = inf t\u2208R\n{ t+ \u03b1\u22121E [(Z \u2212 t)+] } .\nWhen Z is continuous, \u03c1CVaR(Z;\u03b1) is well-known to be the mean of the \u03b1-tail distribution of Z, E [Z|Z > q\u03b1], where q\u03b1 is a (1\u2212 \u03b1)-quantile of Z. Thus, selecting a small \u03b1 makes CVaR particularly sensitive to rare, but very high costs.\nThe risk envelope for CVaR is known to be [32] U = { \u03beP\u03b8 : \u03be(\u03c9) \u2208\n[0, \u03b1\u22121], \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8(\u03c9) = 1 } . Furthermore, [32] show that the saddle points of (6) satisfy \u03be\u2217\u03b8 (\u03c9) = \u03b1 \u22121 when Z(\u03c9) > \u03bb\u2217,P\u03b8 , and \u03be \u2217 \u03b8 (\u03c9) = 0 when Z(\u03c9) < \u03bb \u2217,P \u03b8 , where \u03bb \u2217,P \u03b8 is any (1\u2212 \u03b1)quantile of Z. Plugging this result into Theorem 4.2, we can easily show that\n\u2207\u03b8\u03c1CVaR(Z;\u03b1) = E [\u2207\u03b8 logP (\u03c9)(Z \u2212 q\u03b1)|Z(\u03c9) > q\u03b1] . This formula was recently proved in [36] for the case of continuous distributions by an explicit calculation of the conditional expectation, and under several additional smoothness assumptions. Here we show that it holds regardless of these assumptions and in the discrete case as well. Our proof is also considerably simpler."}, {"heading": "4.2 Example 2: Mean-Semideviation", "text": "The semi-deviation of a random variable Z is defined as SD[Z] .= ( E [ (Z \u2212 E [Z])2+ ])1/2 . The semi-deviation captures the variation of the cost only above its mean, and is an appealing alternative to the standard deviation, which does not distinguish between the variability of upside and downside deviations. For some \u03b1 \u2208 [0, 1], the mean-semideviation risk measure is defined as \u03c1MSD(Z;\u03b1) . = E [Z] + \u03b1SD[Z], and is a coherent risk measure [32]. We have the following result: Proposition 4.3. Under Assumption 4.1, with \u2207\u03b8E [Z] = E [\u2207\u03b8 logP (\u03c9)Z], we have\n\u2207\u03b8\u03c1MSD(Z;\u03b1) = \u2207\u03b8E [Z] + \u03b1E [(Z\u2212E [Z])+(\u2207\u03b8 logP (\u03c9)(Z\u2212E [Z])\u2212\u2207\u03b8E [Z])]\nSD(Z) .\nThis proposition can be used to devise a sampling based estimator for \u2207\u03b8\u03c1MSD(Z;\u03b1) by replacing all the expectations with sample averages. The algorithm along with the proof of the proposition are in the supplementary material. In Section 6 we provide a numerical illustration of optimization with a mean-semideviation objective."}, {"heading": "4.3 General Gradient Estimation Algorithm", "text": "In the two previous examples, we obtained a gradient formula by analytically calculating the Lagrangian saddle point (6) and plugging it into the formula of Theorem 4.2. We now consider a general coherent risk \u03c1(Z) for which, in contrast to the CVaR and mean-semideviation cases, the Lagrangian saddle-point is not known analytically. We only assume that we know the structure of the risk-envelope as given by (2). We show that in this case, \u2207\u03b8\u03c1(Z) may be estimated using a sample average approximation (SAA; [32]) of the formula in Theorem 4.2.\nAssume that we are given N i.i.d. samples \u03c9i \u223c P\u03b8, i = 1, . . . , N , and let P\u03b8;N (\u03c9) . = 1 N \u2211N i=1 I {\u03c9i = \u03c9} denote the corresponding empirical distribution. Also, let the sample risk envelope U(P\u03b8;N ) be defined according to Eq. 2 with P\u03b8 replaced by P\u03b8;N . Consider the following SAA version of the optimization in Eq. 1:\n\u03c1N (Z) = max \u03be:\u03beP\u03b8;N\u2208U(P\u03b8;N ) \u2211 i\u22081,...,N P\u03b8;N (\u03c9i)\u03be(\u03c9i)Z(\u03c9i). (7)\nNote that (7) defines a convex optimization problem with O(N) variables and constraints. In the following, we assume that a solution to (7) may be computed efficiently using standard convex programming tools such as interior point methods [9]. Let \u03be\u2217\u03b8;N denote a solution to (7) and \u03bb\u2217,P\u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N denote the corresponding KKT multipliers, which can be obtained from the convex programming algorithm [9]. We propose the following estimator for the gradient-based on Theorem 4.2:\n\u2207\u03b8;N\u03c1(Z) = N\u2211 i=1 P\u03b8;N (\u03c9i)\u03be \u2217 \u03b8;N (\u03c9i)\u2207\u03b8 logP (\u03c9i)(Z(\u03c9i)\u2212 \u03bb \u2217,P \u03b8;N ) (8)\n\u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8;N (e)\u2207\u03b8ge(\u03be \u2217 \u03b8;N ;P\u03b8;N )\u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8;N (i)\u2207\u03b8fi(\u03be \u2217 \u03b8;N ;P\u03b8;N ).\nThus, our gradient estimation algorithm is a two-step procedure involving both sampling and convex programming. In the following, we show that under some conditions on the set U(P\u03b8), \u2207\u03b8;N\u03c1(Z) is a consistent estimator of\u2207\u03b8\u03c1(Z). The proof has been reported in the supplementary material. Proposition 4.4. Let Assumptions 2.2 and 4.1 hold. Suppose there exists a compact setC = C\u03be\u00d7C\u03bb such that: (I) The set of Lagrangian saddle points S \u2282 C is non-empty and bounded. (II) The functions fe(\u03be, P\u03b8) for all e \u2208 E and fi(\u03be, P\u03b8) for all i \u2208 I are finite-valued and continuous (in \u03be) on C\u03be. (III) For N large enough, the set SN is non-empty and SN \u2282 C w.p. 1. Further assume that: (IV) If \u03beNP\u03b8;N \u2208 U(P\u03b8;N ) and \u03beN converges w.p. 1 to a point \u03be, then \u03beP\u03b8 \u2208 U(P\u03b8). We then have that limN\u2192\u221e \u03c1N (Z) = \u03c1(Z) and limN\u2192\u221e\u2207\u03b8;N\u03c1(Z) = \u2207\u03b8\u03c1(Z) w.p. 1.\nThe set of assumptions for Proposition 4.4 is large, but rather mild. Note that (I) is implied by the Slater condition of Assumption 2.2. For satisfying (III), we need that the risk be well-defined for every empirical distribution, which is a natural requirement. Since P\u03b8;N always converges to P\u03b8 uniformly on \u2126, (IV) essentially requires smoothness of the constraints. We remark that in particular,\nconstraints (I) to (IV) are satisfied for the popular CVaR, mean-semideviation, and spectral risk measures.\nTo summarize this section, we have seen that by exploiting the special structure of coherent risk measures in Theorem 2.1 and by the envelope-theorem style result of Theorem 4.2, we were able to derive sampling-based, likelihood-ratio style algorithms for estimating the policy gradient \u2207\u03b8\u03c1(Z) of coherent static risk measures. The gradient estimation algorithms developed here for static risk measures will be used as a sub-routine in our subsequent treatment of dynamic risk measures."}, {"heading": "5 Gradient Formula for Dynamic Risk", "text": "In this section, we derive a new formula for the gradient of the Markov coherent dynamic risk measure, \u2207\u03b8\u03c1\u221e(M). Our approach is based on combining the static gradient formula of Theorem 4.2, with a dynamic-programming decomposition of \u03c1\u221e(M). The risk-sensitive value-function for an MDP M under the policy \u03b8 is defined as V\u03b8(x) = \u03c1\u221e(M|x0 = x), where with a slight abuse of notation, \u03c1\u221e(M|x0 = x) denotes the Markovcoherent dynamic risk in (3) when the initial state x0 is x. It is shown in [30] that due to the structure of the Markov dynamic risk \u03c1\u221e(M), the value function is the unique solution to the risk-sensitive Bellman equation\nV\u03b8(x) = C(x) + \u03b3 max \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x))\nE\u03be[V\u03b8(x\u2032)], (9)\nwhere the expectation is taken over the next state transition. Note that by definition, we have \u03c1\u221e(M) = V\u03b8(x0), and thus,\u2207\u03b8\u03c1\u221e(M) = \u2207\u03b8V\u03b8(x0). We now develop a formula for \u2207\u03b8V\u03b8(x); this formula extends the well-known \u201cpolicy gradient theorem\u201d [34, 17], developed for the expected return, to Markov-coherent dynamic risk measures. We make a standard assumption, analogous to Assumption 4.1 of the static case. Assumption 5.1. The likelihood ratio \u2207\u03b8 log\u00b5\u03b8(a|x) is well-defined and bounded for all x \u2208 X and a \u2208 A. For each state x \u2208 X , let (\u03be\u2217\u03b8,x, \u03bb \u2217,P \u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x) denote a saddle point of (6), corresponding to the state x, with P\u03b8(\u00b7|x) replacing P\u03b8 in (6) and V\u03b8 replacing Z. The next theorem presents a formula for \u2207\u03b8V\u03b8(x); the proof is in the supplementary material. Theorem 5.2. Under Assumptions 2.2 and 5.1, we have\n\u2207V\u03b8(x) = E\u03be\u2217\u03b8 [ \u221e\u2211 t=0 \u03b3t\u2207\u03b8 log\u00b5\u03b8(at|xt)h\u03b8(xt, at) \u2223\u2223\u2223\u2223\u2223x0 = x ] ,\nwhere E\u03be\u2217\u03b8 [\u00b7] denotes the expectation w.r.t. trajectories generated by the Markov chain with transition probabilities P\u03b8(\u00b7|x)\u03be\u2217\u03b8,x(\u00b7), and the stage-wise cost function h\u03b8(x, a) is defined as h\u03b8(x, a)=C(x)+ \u2211 x\u2032\u2208X P (x\u2032|x, a)\u03be\u2217\u03b8,x(x\u2032) [ \u03b3V\u03b8(x \u2032)\u2212\u03bb\u2217,P\u03b8,x \u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8,x(i) dfi(\u03be \u2217 \u03b8,x, p) dp(x\u2032) \u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8,x(e) dge(\u03be \u2217 \u03b8,x, p) dp(x\u2032) ] .\nTheorem 5.2 may be used to develop an actor-critic style [34, 17] sampling-based algorithm for solving the DRP problem (5), composed of two interleaved procedures:\nCritic: For a given policy \u03b8, calculate the risk-sensitive value function V\u03b8, and Actor: Using the critic\u2019s V\u03b8 and Theorem 5.2, estimate \u2207\u03b8\u03c1\u221e(M) and update \u03b8. Space limitation restricts us from specifying the full details of our actor-critic algorithm and its analysis. In the following, we highlight only the key ideas and results. For the full details, we refer the reader to the full paper version, provided in the supplementary material.\nFor the critic, the main challenge is calculating the value function when the state space X is large and dynamic programming cannot be applied due to the \u2018curse of dimensionality\u2019. To overcome this, we exploit the fact that V\u03b8 is equivalent to the value function in a robust MDP [24] and modify a recent algorithm in [37] to estimate it using function approximation.\nFor the actor, the main challenge is that in order to estimate the gradient using Thm. 5.2, we need to sample from an MDP with \u03be\u2217\u03b8 -weighted transitions. Also, h\u03b8(x, a) involves an expectation for each\ns and a. Therefore, we propose a two-phase sampling procedure to estimate \u2207V\u03b8 in which we first use the critic\u2019s estimate of V\u03b8 to derive \u03be\u2217\u03b8 , and sample a trajectory from an MDP with \u03be \u2217 \u03b8 -weighted transitions. For each state in the trajectory, we then sample several next states to estimate h\u03b8(x, a).\nThe convergence analysis of the actor-critic algorithm and the gradient error incurred from function approximation of V\u03b8 are reported in the supplementary material."}, {"heading": "6 Numerical Illustration", "text": "In this section, we illustrate our approach with a numerical example. The purpose of this illustration is to emphasize the importance of flexibility in designing risk criteria for selecting an appropriate risk-measure \u2013 such that suits both the user\u2019s risk preference and the problem-specific properties.\nWe consider a trading agent that can invest in one of three assets (see Figure 1 for their distributions). The returns of the first two assets, A1 and A2, are normally distributed: A1 \u223c N (1, 1) and A2 \u223c N (4, 6). The return of the third asset A3 has a Pareto distribution: f(z) = \u03b1z\u03b1+1 \u2200z > 1, with \u03b1 = 1.5. The mean of the return from A3 is 3 and its variance is infinite; such heavy-tailed distributions are widely used in financial modeling [27]. The agent selects an action randomly, with probability P (Ai) \u221d exp(\u03b8i), where \u03b8 \u2208 R3 is the policy parameter. We trained three different policies \u03c01, \u03c02, and \u03c03. Policy \u03c01 is risk-neutral, i.e., max\u03b8 E [Z], and it was trained using standard policy gradient [18]. Policy \u03c02 is risk-averse and had a mean-semideviation objective max\u03b8 E [Z] \u2212 SD[Z], and was trained using the algorithm in Section 4. Policy \u03c03 is also risk-averse, with a mean-standarddeviation objective, as proposed in [35, 26], max\u03b8 E [Z] \u2212 \u221a Var[Z], and was trained using the algorithm of [35]. For each of these policies, Figure 1 shows the probability of selecting each asset vs. training iterations. Although A2 has the highest mean return, the risk-averse policy \u03c02 chooses A3, since it has a lower downside, as expected. However, because of the heavy upper-tail of A3, policy \u03c03 opted to chooseA1 instead. This is counter-intuitive as a rational investor should not avert high returns. In fact, in this case A3 stochastically dominates A1 [15]."}, {"heading": "7 Conclusion", "text": "We presented algorithms for estimating the gradient of both static and dynamic coherent risk measures using two new policy gradient style formulas that combine sampling with convex programming. Thereby, our approach extends risk-sensitive RL to the whole class of coherent risk measures, and generalizes several recent studies that focused on specific risk measures.\nOn the technical side, an important future direction is to improve the convergence rate of gradient estimates using importance sampling methods. This is especially important for risk criteria that are sensitive to rare events, such as the CVaR [3].\nFrom a more conceptual point of view, the coherent-risk framework explored in this work provides the decision maker with flexibility in designing risk preference. As our numerical example shows, such flexibility is important for selecting appropriate problem-specific risk measures for managing the cost variability. However, we believe that our approach has much more potential than that.\nIn almost every real-world application, uncertainty emanates from stochastic dynamics, but also, and perhaps more importantly, from modeling errors (model uncertainty). A prudent policy should\nprotect against both types of uncertainties. The representation duality of coherent-risk (Theorem 2.1), naturally relates the risk to model uncertainty. In [24], a similar connection was made between model-uncertainty in MDPs and dynamic Markov coherent risk. We believe that by carefully shaping the risk-criterion, the decision maker may be able to take uncertainty into account in a broad sense. Designing a principled procedure for such risk-shaping is not trivial, and is beyond the scope of this paper. However, we believe that there is much potential to risk shaping as it may be the key for handling model misspecification in dynamic decision making."}, {"heading": "A Proof of Theorem 4.2", "text": "First note from Assumption 2.2 that\n(i) Slater\u2019s condition holds in the primal optimization problem (1), (ii) L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) is convex in \u03be and concave in (\u03bbP , \u03bbE , \u03bbI).\nThus by the duality result in convex optimization [9], the above conditions imply strong duality and we have \u03c1(Z) = max\u03be\u22650 min\u03bbP ,\u03bbI\u22650,\u03bbE L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) = min\u03bbP ,\u03bbI\u22650,\u03bbE max\u03be\u22650 L\u03b8(\u03be, \u03bb\nP , \u03bbE , \u03bbI). From Assumption 2.2, one can also see that the family of functions {L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)}(\u03be,\u03bbP ,\u03bbE ,\u03bbI)\u2208R|\u2126|\u00d7R\u00d7R|E|\u00d7R|I| is equi-differentiable in \u03b8, L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) is Lipschitz, as a result, an absolutely continuous function in \u03b8, and thus, \u2207\u03b8L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) is continuous and bounded at each (\u03be, \u03bbP , \u03bbE , \u03bbI). Then for every selection of saddle point (\u03be\u2217\u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) \u2208 S of (6), using the Envelop theorem for saddle-point problems (see Theorem 4 of [21]), we have\n\u2207\u03b8 max \u03be\u22650 min \u03bbP ,\u03bbI\u22650,\u03bbE L\u03b8(\u03be, \u03bb P , \u03bbE , \u03bbI) = \u2207\u03b8L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)|(\u03be\u2217\u03b8 ,\u03bb\u2217,P\u03b8 ,\u03bb\u2217,E\u03b8 ,\u03bb\u2217,I\u03b8 ) . (10)\nThe result follows by writing the gradient in (10) explicitly, and using the likelihood-ratio trick:\u2211 \u03c9\u2208\u2126 \u03be(\u03c9)\u2207\u03b8P\u03b8(\u03c9)Z(\u03c9)\u2212\u03bbP \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)\u2207\u03b8P\u03b8(\u03c9) = \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P (\u03c9)\u2207\u03b8 logP (\u03c9) ( Z(\u03c9)\u2212\u03bbP ) ,\nwhere the last equality is justified by Assumption 4.1."}, {"heading": "B Gradient Results for Static Mean-Semideviation", "text": "In this section we consider the mean-semideviation risk measure, defined as follows: \u03c1MSD(Z) = E [Z] + c ( E [ (Z \u2212 E [Z])2+ ])1/2 , (11)\nFollowing the derivation in [32], note that ( E [ |Z|2 ])1/2 = \u2016Z\u20162, where \u2016 \u00b7 \u20162 denotes the L2 norm of the space L2(\u2126,F , P\u03b8). The norm may also be written as: \u2016Z\u20162 = sup\n\u2016\u03be\u20162\u22641 \u3008\u03be, Z\u3009,\nand hence( E [ (Z \u2212 E [Z])2+ ])1/2 = sup \u2016\u03be\u20162\u22641 \u3008\u03be, (Z \u2212 E [Z])+\u3009 = sup \u2016\u03be\u20162\u22641,\u03be\u22650 \u3008\u03be, Z \u2212 E [Z]\u3009\n= sup \u2016\u03be\u20162\u22641,\u03be\u22650\n\u3008\u03be \u2212 E [\u03be], Z\u3009.\nIt follows that Eq. (1) holds with\nU = {\u03be\u2032 \u2208 Z\u2217 : \u03be\u2032 = 1 + c\u03be \u2212 cE [\u03be] , \u2016\u03be\u2016q \u2264 1, \u03be \u2265 0} . For this case it will be more convenient to write Eq. (1) in the following form\n\u03c1MSD(Z) = sup \u2016\u03be\u2016q\u22641,\u03be\u22650\n\u30081 + c\u03be \u2212 cE [\u03be], Z\u3009. (12)\nLet \u03be\u0304 denote an optimal solution for (12). In [32] it is shown that \u03be\u0304 is a contact point of (Z\u2212E [Z])+, that is \u03be\u0304 \u2208 arg max {\u3008\u03be, (Z \u2212 E [Z])+\u3009 : \u2016\u03be\u20162 \u2264 1} , and we have that\n\u03be\u0304 = (Z \u2212 E [Z])+ \u2016(Z \u2212 E [Z])+\u20162 = (Z \u2212 E [Z])+ SD(Z) . (13)\nNote that \u03be\u0304 is not necessarily a probability distribution, but for c \u2208 [0, 1], it can be shown [32] that 1 + c\u03be\u0304 \u2212 cE [ \u03be\u0304 ] always is.\nIn the following we show that \u03be\u0304 may be used to write the gradient \u2207\u03b8\u03c1MSD(Z) as an expectation, which will lead to a sampling algorithm for the gradient.\nProposition B.1. Under Assumption 4.1, we have that\n\u2207\u03b8\u03c1MSD(Z) = \u2207\u03b8E [Z] + c\nSD(Z) E [(Z \u2212 E [Z])+ (\u2207\u03b8 logP (\u03c9)(Z \u2212 E [Z])\u2212\u2207\u03b8E [Z])] ,\nand, according to the standard likelihood-ratio method,\n\u2207\u03b8E [Z] = E [\u2207\u03b8 logP (\u03c9)Z] .\nProof. Note that in Eq. (12) the constraints do not depend on \u03b8. Therefore, using the envelope theorem we obtain that\n\u2207\u03b8\u03c1(Z) = \u2207\u03b8\u30081 + c\u03be\u0304 \u2212 cE [ \u03be\u0304 ] , Z\u3009\n= \u2207\u03b8\u30081, Z\u3009+ c\u2207\u03b8\u3008\u03be\u0304, Z\u3009 \u2212 c\u2207\u03b8\u3008E [ \u03be\u0304 ] , Z\u3009.\n(14)\nWe now write each of the terms in Eq. (14) as an expectation. We start with the following standard likelihood-ratio result: \u2207\u03b8\u30081, Z\u3009 = \u2207\u03b8E [Z] = E [\u2207\u03b8 logP (\u03c9)Z] . Also, we have that\n\u3008E [ \u03be\u0304 ] , Z\u3009 = E [ \u03be\u0304 ] E [Z] ,\ntherefore, by the derivative of a product rule: \u2207\u03b8\u3008E [ \u03be\u0304 ] , Z\u3009 = \u2207\u03b8E [ \u03be\u0304 ] E [Z] + E [ \u03be\u0304 ] \u2207\u03b8E [Z].\nBy the likelihood-ratio trick and Eq. (13) we have that \u2207\u03b8E [ \u03be\u0304 ] = 1\nSD(Z) E [\u2207\u03b8 logP (\u03c9)(Z \u2212 E [Z])+] .\nAlso, by the likelihood-ratio trick \u2207\u03b8E [ \u03be\u0304Z ] = E [ \u2207\u03b8 logP (\u03c9)\u03be\u0304Z ] .\nPlugging these terms back in Eq. (14), we have that \u2207\u03b8\u03c1(Z) = \u2207\u03b8E [Z] + c\u2207\u03b8E [ \u03be\u0304Z ] \u2212 c\u2207\u03b8E [ \u03be\u0304 ] E [Z]\u2212 cE [ \u03be\u0304 ] \u2207\u03b8E [Z]\n= \u2207\u03b8E [Z] + cE [ \u03be\u0304 (\u2207\u03b8 logP (\u03c9)Z \u2212\u2207\u03b8E [Z]) ] \u2212 c\u2207\u03b8E [ \u03be\u0304 ] E [Z] = \u2207\u03b8E [Z] + c\nSD(Z) E [(Z \u2212 E [Z])+ (\u2207\u03b8 logP (\u03c9)Z \u2212\u2207\u03b8E [Z])]\u2212 c\u2207\u03b8E\n[ \u03be\u0304 ] E [Z]\n= \u2207\u03b8E [Z] + c\nSD(Z) E [(Z \u2212 E [Z])+ (\u2207\u03b8 logP (\u03c9)(Z \u2212 E [Z])\u2212\u2207\u03b8E [Z])] .\nProposition 4.3 naturally leads to a sampling-based gradient estimation algortihm, which we term GMSD (Gradient of Mean Semi-Deviation). The algorithm is described in Algorithm 1."}, {"heading": "C Consistency Proof", "text": "Let (\u2126SAA,FSAA, PSAA) denote the probability space of the SAA functions (i.e., the randomness due to sampling).\nLet L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI) denote the Lagrangian of the SAA problem\nL\u03b8;N (\u03be, \u03bb P , \u03bbE , \u03bbI) = \u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8;N (\u03c9)Z(\u03c9)\u2212\u03bbP (\u2211 \u03c9\u2208\u2126 \u03be(\u03c9)P\u03b8;N (\u03c9)\u22121 ) \u2212 \u2211 e\u2208E \u03bbE(e)fe(\u03be, P\u03b8;N )\u2212 \u2211 i\u2208I \u03bbI(i)fi(\u03be, P\u03b8;N ).\n(15)\nRecall that S \u2282 R|\u2126| \u00d7R\u00d7R|E| \u00d7R|I|+ denotes the set of saddle points of the true Lagrangian (6). Let SN \u2282 R|\u2126| \u00d7 R\u00d7 R|E| \u00d7 R|I|+ denote the set of SAA Lagrangian (15) saddle points.\nSuppose that there exists a compact set C \u2261 C\u03be \u00d7C\u03bb, where C\u03be \u2282 R|\u2126| and C\u03bb \u2282 R\u00d7R|E|\u00d7R|I|+ such that:\nAlgorithm 1 GMSD"}, {"heading": "1: Given:", "text": "\u2022 Risk level c \u2022 An i.i.d. sequence z1, . . . , zN \u223c P\u03b8.\n2: Set\nE\u0302 [Z] = 1\nN N\u2211 i=1 zi.\n3: Set\nS\u0302D(Z) =\n( 1\nN N\u2211 i=1 (zi \u2212 E\u0302 [Z])2+\n)1/2 .\n4: Set\n\u2207\u0302\u03b8E [Z] = 1\nN N\u2211 i=1 \u2207\u03b8 logP (zi)zi."}, {"heading": "5: Return:", "text": "\u02c6\u2207\u03b8\u03c1(Z) = \u2207\u0302\u03b8E [Z] + c\nS\u0302D(Z)\n1\nN N\u2211 i=1 (zi \u2212 E\u0302 [Z])+ ( \u2207\u03b8 logP (zi)(zi \u2212 E\u0302 [Z])\u2212 \u2207\u0302\u03b8E [Z] ) .\n(i) The set of Lagrangian saddle points S \u2282 C is non-empty and bounded. (ii) The functions fe(\u03be, P\u03b8) for all e \u2208 E and fi(\u03be, P\u03b8) for all i \u2208 I are finite valued and continuous\n(in \u03be) on C\u03be.\n(iii) For N large enough the set SN is non-empty and SN \u2282 C w.p. 1.\nRecall from Assumption 2.2 that for each fixed \u03be \u2208 B, both fi(\u03be, p) and ge(\u03be, p) are continuous in p. Furthermore, by the S.L.L.N. of Markov chains, for each policy parameter, we have P\u03b8,N \u2192 P\u03b8 w.p. 1. From the definition of the Lagrangian function and continuity of constraint functions, one can easily see that for each (\u03be, \u03bbP , \u03bbE , \u03bbI) \u2208 R|\u2126| \u00d7 R \u00d7 R|E| \u00d7 R|I|+ , L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI) \u2192 L\u03b8(\u03be, \u03bb\nP , \u03bbE , \u03bbI) w.p. 1. Denote with D {A,B} the deviation of setA from setB, i.e., D {A,B} = supx\u2208A infy\u2208B \u2016x\u2212 y\u2016. Further assume that:\n(iv) If \u03beN \u2208 U(P\u03b8;N ) and \u03beN converges w.p. 1 to a point \u03be, then \u03be \u2208 U(P\u03b8).\nAccording to the discussion in Page 161 of [32], the Slater condition of Assumption 2.2 guarantees the following condition:\n(v) For some point \u03be \u2208 P there exists a sequence \u03beN \u2208 U(P\u03b8;N ) such that \u03beN \u2192 \u03be w.p. 1,\nand from Theorem 6.6 in [32], we know that both sets U(P\u03b8;N ) and U(P\u03b8) are convex and compact. Furthermore, note that we have\n(vi) The objective function on (1) is linear, finite valued and continuous in \u03be on C\u03be (these conditions obviously hold for almost all \u03c9 \u2208 \u2126 in the integrand function \u03be(\u03c9)Z(\u03c9)).\n(vii) S.L.L.N. holds point-wise for any \u03be.\nFrom (i,iv,v,vi,vii), and under the same lines of proof as in Theorem 5.5 of [32], we have that\n\u03c1N (Z)\u2192 \u03c1(Z) w.p. 1 as N \u2192\u221e, (16)\nD {PN ,P} \u2192 0 w.p. 1 as N \u2192\u221e, (17)\nIn part 1 and part 2 of the following proof, we show, by following similar derivations as in Theorem 5.2, Theorem 5.3 and Theorem 5.4 of [32], that L\u03b8;N (\u03be\u2217\u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) \u2192\nL\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) w.p. 1 and D {SN ,S} \u2192 0 w.p. 1 as N \u2192 \u221e. Based on the definition of the deviation of sets, the limit point of any element in SN is also an element in S. Assumptions (i) and (iii) imply that we can restrict our attention to the set C.\nPart 1 We first show that L\u03b8;N (\u03be\u2217\u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) converges to L\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) w.p. 1 as N \u2192\u221e. For each fixed (\u03bbP , \u03bbE , \u03bbI) \u2208 C\u03bb, the function L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) is convex and continuous in \u03be. Together with the point-wise S.L.L.N. property, Theorem 7.49 of [32] implies that L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI) \u2212 L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)\ne\u2192 0, where e\u2192 denotes epi-convergence. Furthermore, since the objective and constraint functions are convex in \u03be and are finite valued on C\u03be, the set domL\u03b8(\u00b7, \u03bbP , \u03bbE , \u03bbI) has non-empty interior. It follows from Theorem 7.27 of [32] that epi-convergence of L\u03b8,N to L\u03b8 implies uniform convergence on C\u03be, i.e., sup\u03be\u2208C\u03be\n\u2223\u2223L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI)\u2212 L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)\u2223\u2223 \u2264 . On the other hand, for each fixed \u03be \u2208 C\u03be, the function L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI) is linear and thus continuous in (\u03bbP , \u03bbE , \u03bbI) and domL\u03b8(\u03be, \u00b7, \u00b7, \u00b7) = R\u00d7R|E|\u00d7R|I| has non-empty interior. It follows from analogous arguments that sup(\u03bbP ,\u03bbE ,\u03bbI)\u2208C\u03bb\n\u2223\u2223L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI)\u2212 L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)\u2223\u2223 \u2264 . Combining these results implies that for any > 0 and a.e. \u03c9SAA \u2208 \u2126SAA there is a N\u2217( , \u03c9SAA) such that\nsup (\u03be,\u03bbP ,\u03bbE ,\u03bbI)\u2208C \u2223\u2223L\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI)\u2212 L\u03b8(\u03be, \u03bbP , \u03bbE , \u03bbI)\u2223\u2223 \u2264 . (18) Now, assume by contradiction that for some N > N\u2217( , \u03c9SAA) we have L\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) \u2212 L\u03b8(\u03be\u2217\u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) > . Then by definition of the saddle points\nL\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) \u2265 L\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N )\n> L\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) + \u2265 L\u03b8(\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) + ,\ncontradicting (18).\nSimilarly, assuming by contradiction that L\u03b8(\u03be\u2217\u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) \u2212 L\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) > gives\nL\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) \u2265 L\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 )\n> L\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) + \u2265 L\u03b8;N (\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) + ,\nalso contradicting (18). It follows that \u2223\u2223\u2223L\u03b8;N (\u03be\u2217\u03b8;N , \u03bb\u2217,P\u03b8;N , \u03bb\u2217,E\u03b8;N , \u03bb\u2217,I\u03b8;N )\u2212 L\u03b8(\u03be\u2217\u03b8 , \u03bb\u2217,P\u03b8 , \u03bb\u2217,E\u03b8 , \u03bb\u2217,I\u03b8 )\u2223\u2223\u2223 \u2264 for all N > N\u2217( , \u03c9SAA), and therefore\nlim N\u2192\u221e\nL\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) = L\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ), (19)\nw.p. 1.\nPart 2 Let us now show that D {SN ,S} \u2192 0. We argue by a contradiction. Suppose that D {SN ,S} 9 0. Since C is compact, we can assume that there exists a sequence (\u03be\u2217\u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) \u2208 SN that converges to a point (\u03be\u0304\u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) \u2208 C and (\u03be\u0304\u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) 6\u2208 S. However, from (17) we must have that \u03be\u0304\u2217 \u2208 P . Therefore, we must have that\nL\u03b8(\u03be\u0304 \u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) > L\u03b8(\u03be\u0304 \u2217, \u03bb\u2217,P\u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ),\nby definition of the saddle point set.\nNow,\nL\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N )\u2212 L\u03b8(\u03be\u0304 \u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) = [ L\u03b8;N (\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N )\u2212 L\u03b8(\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) ] +\n+ [ L\u03b8(\u03be \u2217 \u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N )\u2212 L\u03b8(\u03be\u0304 \u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) ] .\n(20)\nThe first term in the r.h.s. of (20) tends to zero, using the argument from (18), and the second by continuity of L\u03b8 guaranteed by (ii). We thus obtain that L\u03b8;N (\u03be\u2217\u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N ) tends to L\u03b8(\u03be\u0304 \u2217, \u03bb\u0304\u2217,P , \u03bb\u0304\u2217,E , \u03bb\u0304\u2217,I) > L\u03b8(\u03be \u2217 \u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ), which is a contradiction to (19).\nPart 3 We now show the consistency of \u2207\u03b8;N\u03c1(Z). Consider Eq. (8). Since \u2207\u03b8 logP (\u00b7) is bounded by Assumption 4.1, and \u2207\u03b8fi(\u00b7;P\u03b8) and \u2207\u03b8ge(\u00b7;P\u03b8) are bounded by Assumption 2.2, and using our previous result D {SN ,S} \u2192 0, we have that for a.e. \u03c9SAA \u2208 \u2126SAA\nlim N\u2192\u221e \u2207\u03b8;N\u03c1(Z) = \u2211 \u03c9\u2208\u2126 P\u03b8(\u03c9)\u03be \u2217 \u03b8 (\u03c9)\u2207\u03b8 logP (\u03c9)(Z(\u03c9)\u2212 \u03bb \u2217,P \u03b8 )\n\u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8 (e)\u2207\u03b8ge(\u03be \u2217 \u03b8 ;P\u03b8)\n\u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8 (i)\u2207\u03b8fi(\u03be \u2217 \u03b8 ;P\u03b8)\n= \u2207\u03b8\u03c1(Z).\nwhere the first equality is obtained from the Envelop theorem (see Theorem 4.2) with (\u03be\u2217\u03b8 , \u03bb \u2217,P \u03b8 , \u03bb \u2217,E \u03b8 , \u03bb \u2217,I \u03b8 ) \u2208 SN \u2229 S is the limit point of the converging sequence {(\u03be\u2217\u03b8;N , \u03bb \u2217,P \u03b8;N , \u03bb \u2217,E \u03b8;N , \u03bb \u2217,I \u03b8;N )}N\u2208N."}, {"heading": "D Proof of Theorem 5.2", "text": "Similar to the proof of Theorem 4.2, recall the saddle point definition of (\u03be\u2217\u03b8,x, \u03bb \u2217,P \u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x) \u2208 S and strong duality result, i.e.,\nmax \u03be : \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|x)V\u03b8(x\u2032) = max \u03be\u22650 min \u03bbP ,\u03bbI\u22650,\u03bbE L\u03b8,x(\u03be, \u03bb P , \u03bbE , \u03bbI)\n= min \u03bbP ,\u03bbI\u22650,\u03bbE max \u03be\u22650\nL\u03b8,x(\u03be, \u03bb P , \u03bbE , \u03bbI).\nthe gradient formula in (10) can be written as \u2207\u03b8V\u03b8(x) = \u2207\u03b8 [ C\u03b8(x)+\u03b3 max\n\u03be : \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) E\u03be[V\u03b8] ] = \u03b3\n\u2211 x\u2032\u2208X \u03be\u2217\u03b8,x(x \u2032)P\u03b8(x \u2032|x)\u2207\u03b8V\u03b8(x\u2032) + \u2211 a\u2208A \u00b5\u03b8(a|x)\u2207\u03b8 log\u00b5\u03b8(a|x)h\u03b8(x, a),\nwhere the stage-wise cost function h\u03b8(x, a) is defined in (26). By defining h\u0302\u03b8(x) =\u2211 a\u2208A \u00b5\u03b8(a|x)\u2207\u03b8 log\u00b5\u03b8(a|x)h\u03b8(x, a) and unfolding the recursion, the above expression implies\n\u2207\u03b8V\u03b8(x0) =h\u0302\u03b8(x0) + \u03b3 \u2211 x1\u2208X P\u03b8(x1|x0)\u03be\u2217\u03b8 (x1)\n[ h\u0302\u03b8(x1) + \u03b3\n\u2211 x2\u2208X P\u03b8(x2|x1)\u03be\u2217\u03b8 (x2)\u2207\u03b8V\u03b8 (x2)\n] .\nNow since \u2207\u03b8V\u03b8 is continuously differentiable with bounded derivatives, when t \u2192 \u221e, one obtains \u03b3t\u2207\u03b8V\u03b8(x) \u2192 0 for any x \u2208 X . Therefore, by Bounded Convergence Theorem, limt\u2192\u221e \u03c1(\u03b3 tV\u03b8(xt)) = 0, when x0 = x the above expression implies the result of this theorem."}, {"heading": "E Gradient Formula for Dynamic Risk - Full Results", "text": "In this section, we first derive a new formula for the gradient of a general Markov-coherent dynamic risk measure\u2207\u03b8\u03c1\u221e(M) that involves the value function of the risk objective \u03c1\u221e(M) (e.g., the value function proposed by [30]). This formula extends the well-known \u201cpolicy gradient theorem\u201d [34, 17]\ndeveloped for the expected return to Markov-coherent dynamic risk measures. Using this formula, we suggest the following actor-critic style algorithm for estimating\u2207\u03b8\u03c1\u221e(M): Critic: For a given policy \u03b8, calculate the risk-sensitive value function of \u03c1\u221e(M) (see Section E.3), and Actor: Using the critic\u2019s value function, estimate\u2207\u03b8\u03c1\u221e(M) by sampling (see Section E.4). The value function proposed by [30] assigns to each state a particular value that encodes the longterm risk starting from that state. When the state space X is large, calculating the value function by dynamic programming (as suggested by [30]) becomes intractable due to the \u201ccurse of dimensionality\u201d. For the risk-neutral case, a standard solution to this problem is to approximate the value function by a set of state-dependent features, and use sampling to calculate the parameters of this approximation [6]. In particular, temporal difference (TD) learning methods [33] are popular for this purpose, which have been recently extended to robust MDPs by [37]. We use their (robust) TD algorithm and show how our critic use it to approximates the risk-sensitive value function. We then discuss how the error introduced by this approximation affects the gradient estimate of the actor.\nE.1 Dynamic Risk\nWe provide a multi-period generalization of the concepts presented in Section 2.1. Here we closely follow the discussion in [30].\nConsider a probability space (\u2126,F , P\u03b8), a filtration F0 \u2282 F1 \u2282 F2 \u00b7 \u00b7 \u00b7 \u2282 FT \u2282 F , and an adapted sequence of real-valued random variables Zt, t \u2208 {0, . . . , T}. We assume that F0 = {\u2126, \u2205}, i.e., Z0 is deterministic. For each t \u2208 {0, . . . , T}, we denote by Zt the space of random variables defined over the probability space (\u2126,Ft, P\u03b8), and also let Zt,T := Zt \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 ZT be a sequence of these spaces. The sequence of random variables Zt can be interpreted as the stage-wise costs observed along a trajectory generated by an MDP parameterized by a parameter \u03b8, i.e., Z0,T . = ( Z0 = \u03b30C(x0, a0), . . . , ZT = \u03b3 TC(xT , aT ) ) \u2208 Z0,T .\nIn particular, we are interested in the sequence of random variables induced by the trajectories from a Markov decision process (MDP) parameterized by parameter \u03b8.\nExplicitly, for any t \u2265 0 and state dependent random variable Z(xt+1) \u2208 Zt+1, the risk evaluation is given by\n\u03c1 ( Z(xt+1) ) = max \u03be : \u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) E\u03be [ Z(xt+1) ] , (21)\nwhere we let U(xt, P\u03b8(\u00b7|xt)) denote the risk-envelope (2) with P\u03b8 replaced with P\u03b8(\u00b7|xt). The Markovian assumption on the risk measure \u03c1T (M) allows us to optimize it using dynamic programming techniques.\nE.2 Risk-Sensitive Bellman Equation\nOur value-function estimation method is driven by a Bellman-style equation for Markov coherent risks. Let B(X ) denote the space of real-valued bounded functions on X and C\u03b8(x) =\u2211 a\u2208A C(x, a)\u00b5\u03b8(a|x) be the stage-wise cost function induced by policy \u00b5\u03b8. We now define the risk sensitive Bellman operator T\u03b8[V ] : B(X ) 7\u2192 B(X ) as\nT\u03b8[V ](x) := C\u03b8(x) + \u03b3 max \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x))\nE\u03be[V ]. (22)\nAccording to Theorem 1 in [30], the operator T\u03b8 has a unique fixed-point V\u03b8, i.e., T\u03b8[V\u03b8](x) = V\u03b8(x), \u2200x \u2208 X , that is equal to the risk objective function induced by \u03b8, i.e., V\u03b8(x0) = \u03c1\u221e(M). However, when the state space X is large, exact enumeration of the Bellman equation is intractable due to \u201ccurse of dimensionality\u201d. Next, we provide an iterative approach to approximate the risk sensitive value function.\nE.3 Value Function Approximation\nConsider the linear approximation of the risk-sensitive value function V\u03b8(x) \u2248 v>\u03c6(x), where \u03c6(\u00b7) \u2208 R\u03ba2 is the \u03ba2-dimensional state-dependent feature vector. Thus, the approximate value function belongs to the low dimensional sub-space V = {\u03a6v|v \u2208 R\u03ba2}, where \u03a6 : X \u2192 R\u03ba2 is a function mapping such that \u03a6(x) = \u03c6(x). The goal of our critric is to find a good approximation of V\u03b8 from simulated trajectories of the MDP. In order to have a well-defined approximation scheme, we first impose the following standard assumption [6]. Assumption E.1. The mapping \u03a6 has full column rank.\nFor a function y : X \u2192 R, we define its weighted (by d) `2-norm as \u2016y\u2016d = \u221a\u2211 x\u2032 d(x \u2032|x)y(x\u2032)2, where d is a distribution over X . Using this, we define \u03a0 : X \u2192 V , the orthogonal projection from R to V , w.r.t. a norm weighted by the stationary distribution of the policy, d\u03b8(x\u2032|x). Note that the TD methods approximate the value function V\u03b8 with the fixed-point of the joint operator \u03a0T\u03b8, i.e., V\u0303\u03b8(x) = v\u2217>\u03b8 \u03c6(x), such that\n\u2200x \u2208 X , V\u0303\u03b8(x) = \u03a0T\u03b8[V\u0303\u03b8](x). (23) From Eq. 21 that has been derived from Theorem 2.1 for dynamic risks, it is easy to see that the risksensitive Bellman equation (22) is a robust Bellman equation [23] with uncertainty set U(x, P\u03b8(\u00b7|x)). Thus, we may use the TD approximation of the robust Bellman equation proposed by [37] to find an approximation of V\u03b8. We will need the following assumption analogous to Assumption 2 in [37]. Assumption E.2. There exists \u03ba \u2208 (0, 1) such that \u03be(x\u2032) \u2264 \u03ba/\u03b3, for all \u03be(\u00b7)P\u03b8(\u00b7|x) \u2208 U(x, P\u03b8(\u00b7|x)) and all x, x\u2032 \u2208 X .\nGiven Assumption E.2, Proposition 3 in [37] guarantees that the projected risk-sensitive Bellman operator \u03a0T\u03b8 is a contraction w.r.t. d\u03b8-norm. Therefore, Eq. 23 has a unique fixed-point solution V\u0303\u03b8(x) = v \u2217> \u03b8 \u03c6(x). This means that v \u2217 \u03b8 \u2208 R\u03ba2 satisfies v\u2217\u03b8 \u2208 arg minv \u2016T\u03b8[\u03a6v] \u2212 \u03a6v\u20162d\u03b8 . By the projection theorem on Hilbert spaces, the orthogonality condition for v\u2217\u03b8 becomes\u2211 x\u2208X d\u03b8(x|x0)\u03c6(x)\u03c6(x)>v\u2217\u03b8 = \u2211 x\u2208X d\u03b8(x|x0)\u03c6(x)C\u03b8(x)\n+ \u03b3 \u2211 x\u2208X d\u03b8(x|x0)\u03c6(x) max \u03be : \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) E\u03be[\u03a6v\u2217\u03b8 ].\nAs a result, given a long enough trajectory x0, a0, x1, a1, . . ., xN\u22121, aN\u22121 generated by policy \u03b8, we may estimate the fixed-point solution v\u2217\u03b8 using the projected risk sensitive value iteration (PRSVI) algorithm with the update rule\nvk+1 =\n( 1\nN N\u22121\u2211 t=0 \u03c6(xt)\u03c6(xt) >\n)\u22121 [ 1\nN N\u22121\u2211 t=0 \u03c6(xt)C\u03b8(xt)\n+ \u03b3 1\nN N\u22121\u2211 t=0 \u03c6(xt) max \u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) E\u03be[\u03a6vk] ] . (24)\nNote that using the law of large numbers, as both N and k tend to infinity, vk converges w.p. 1 to v\u2217\u03b8 , the unique solution of the fixed point equation \u03a0T\u03b8[\u03a6v] = \u03a6v.\nIn order to implement the iterative algorithm (24), one must repeatedly solve the inner optimization problem max\u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) E\u03be[\u03a6v]. When the state space X is large, solving this optimization problem is often computationally expensive or even intractable. Similar to Section 3.4 of [37], we propose the following SAA approach to solve this problem. For the trajectory, x0, a0, x1, a1, . . ., xN\u22121, aN\u22121, we define the empirical transition probability PN (x\u2032|x, a)\n. =\u2211N\u22121\nt=0 1{xt=x, at=a, xt+1=x \u2032}\u2211N\u22121\nt=0 1{xt=x, at=a} 4 and P\u03b8;N (x\u2032|x) =\n\u2211 a\u2208A PN (x\n\u2032|x, a)\u00b5\u03b8(a|x). Consider the following `2-regularized empirical robust optimization problem5\n4In the case when the sizes of state and action spaces are huge or when these spaces are continuous, the empirical transition probability can be found by kernel density estimation.\n5In the SAA approach, we only sum over the elements for which P\u03b8;N (x\u2032|x) > 0, thus, the sum has at most N elements.\n\u03c1N (\u03a6v) = max \u03be:\u03beP\u03b8;N\u2208U(x,P\u03b8;N ) \u2211 x\u2032\u2208X P\u03b8;N (x \u2032|x)\u03be(x\u2032)\u03c6>(x\u2032)v\n+ 1\n2N\n[ P\u03b8;N (x \u2032|x)\u03be(x\u2032) ]2 . (25)\nAs in [20], the `2-regularization term in this optimization problem guarantees convergence of optimizers \u03be\u2217 and the corresponding KKT multipliers, whenN \u2192\u221e. Convergence of these parameters is crucial for the policy gradient analysis in the next sections. We denote by \u03be\u2217\u03b8,x;N , the solution of the above empirical optimization problem, and by \u03bb\u2217,P\u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N , the corresponding KKT multipliers.\nWe obtain the empirical PRSVI algorithm by replacing the inner optimization max\u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) E\u03be[\u03a6v\u2217\u03b8 ] in Eq. 24 with \u03c1N (\u03a6v) from Eq. 25. Similarly, as both N and k tend to infinity, vk converges w.p. 1 to v\u2217\u03b8 . More details can be found in the supplementary material.\nE.4 Gradient Estimation\nIn Section E.3, we showed that we may effectively approximate the value function of a fixed policy \u03b8 using the (empirical) PRSVI algorithm in Eq. 24. In this section, we first derive a formula for the gradient of the Markov-coherent dynamic risk measure \u03c1\u221e(M), and then propose a SAA algorithm for estimating this gradient, in which we use the SAA approximation of value function from Section E.3. As described in Section E.2, \u03c1\u221e(M) = V\u03b8(x0), and thus, we shall first derive a formula for \u2207\u03b8V\u03b8(x0).\nLet (\u03be\u2217\u03b8,x, \u03bb \u2217,P \u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x) be the saddle point of (6) corresponding to the state x \u2208 X . In many common coherent risk measures such as CVaR and mean semi-deviation, there are closed-form formulas for \u03be\u2217\u03b8,x and KKT multipliers (\u03bb \u2217,P \u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x). We will briefly discuss the case when the saddle point does not have an explicit solution later in this section. Before analyzing the gradient estimation, we have the following standard assumption in analogous to Assumption 4.1 of the static case.\nAssumption E.3. The likelihood ratio \u2207\u03b8 log\u00b5\u03b8(a|x) is well-defined and bounded for all x \u2208 X and a \u2208 A.\nAs in Theorem 4.2 for the static case, we may use the envelope theorem and the risk-sensitive Bellman equation, V\u03b8(x) = C\u03b8(x)+\u03b3max\u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) E\u03be[V\u03b8], to derive a formula for\u2207\u03b8V\u03b8(x). We report this result in Theorem E.4, which is analogous to the risk-neutral policy gradient theorem [34, 17, 7]. The proof is in the supplementary material.\nTheorem E.4. Under Assumptions 2.2, we have\n\u2207V\u03b8(x)=E\u03be\u2217\u03b8 [ \u221e\u2211 t=0 \u03b3t\u2207\u03b8 log\u00b5\u03b8(at|xt)h\u03b8(xt,at) |x0=x ] ,\nwhere E\u03be\u2217\u03b8 [\u00b7] denotes the expectation w.r.t. trajectories generated by a Markov chain with transition probabilities P\u03b8(\u00b7|x)\u03be\u2217\u03b8,x(\u00b7), and the stage-wise cost function h\u03b8(x, a) is defined as\nh\u03b8(x, a) = C(x, a) + \u2211 x\u2032\u2208X P (x\u2032|x, a)\u03be\u2217\u03b8,x(x\u2032) [ \u03b3V\u03b8(x \u2032)\u2212\u03bb\u2217,P\u03b8,x\n\u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8,x(i) dfi(\u03be \u2217 \u03b8,x, p) dp(x\u2032) \u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8,x(e) dge(\u03be \u2217 \u03b8,x, p) dp(x\u2032) ] . (26)\nTheorem E.4 indicates that the policy gradient of the Markov-coherent dynamic risk measure \u03c1\u221e(M), i.e., \u2207\u03b8\u03c1\u221e(M) = \u2207\u03b8V\u03b8, is equivalent to the risk-neutral value function of policy \u03b8 in a MDP with the stage-wise cost function \u2207\u03b8 log\u00b5\u03b8(a|x)h\u03b8(x, a) (which is well-defined and bounded), and transition probability P\u03b8(\u00b7|x)\u03be\u2217\u03b8,x(\u00b7). Thus, when the saddle points are known and the state space X is not too large, we can compute\u2207\u03b8V\u03b8 using a policy evaluation algorithm. However, when the state space is large, exact calculation of \u2207V\u03b8 by policy evaluation becomes impossible,\nand our goal would be to derive a sampling method to estimate \u2207V\u03b8. Unfortunately, since the risk envelop depends on the policy parameter \u03b8, unlike the risk-neutral case, the risk sensitive (or robust) Bellman equation T\u03b8[V\u03b8](x) in (22) is nonlinear in the stationary Markov policy \u00b5\u03b8. Therefore h\u03b8 cannot be considered using the action-value function (Q-function) of the robust MDP. Therefore, even if the exact formulation of the value function V\u03b8 is known, it is computationally intractable to enumerate the summation over x\u2032 to compute h\u03b8(x, a). On top of that in many applications the value function V\u03b8 is not known in advance, which further complicates gradient estimation. To estimate the policy gradient when the value function is unknown, we approximate it by the projected risk sensitive value function \u03a6v\u2217\u03b8 . To address the sampling issues, we propose the following two-phase sampling procedure for estimating\u2207V\u03b8.\n(1) Generate N trajectories {x(j)0 , a (j) 0 , x (j) 1 , a (j) 1 , . . .}Nj=1 from the Markov chain induced by policy \u03b8 and transition probabilities P \u03be\u03b8 (\u00b7|x) := \u03be\u2217\u03b8,x(\u00b7)P\u03b8(\u00b7|x).\n(2) For each state-action pair (x(j)t , a (j) t ) = (x, a), generate N samples {y(k)}Nk=1 using the transition probability P (\u00b7|x, a) and calculate the following empirical average estimate of h\u03b8(x, a)\nh\u03b8,N (x,a) := C(x, a) + 1\nN N\u2211 k=1 \u03be\u2217\u03b8,x(y (k)) [ \u03b3v\u2217\u03b8 > \u03c6(y(k))\u2212 \u03bb\u2217,P\u03b8,x\n\u2212 \u2211 i\u2208I \u03bb\u2217,I\u03b8,x(i) dfi(\u03be \u2217 \u03b8,x, p) dp(y(k)) \u2212 \u2211 e\u2208E \u03bb\u2217,E\u03b8,x(e) dge(\u03be \u2217 \u03b8,x, p) dp(y(k))\n]\n(3) Calculate an estimate of \u2207V\u03b8 using the following average over all the samples: 1 N \u2211N j=1 \u2211\u221e t=0 \u03b3 t\u2207\u03b8 log\u00b5\u03b8(a(j)t |x (j) t )h\u03b8,N (x (j) t , a (j) t ).\nIndeed, by the definition of empirical transition probability PN (x\u2032|x, a), h\u03b8,N (x, a) can be rewritten as in the same structure of h\u03b8(x, a), except by replacing the transition probability P (x\u2032|x, a) with PN (x\u2032|x, a).\nFurthermore, in the case that the saddle points (\u03be\u2217\u03b8,x, \u03bb \u2217,P \u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x) do not have a closed-form solution, we may follow the SAA procedure of Section E.3 and replace them and the transition probabilities P (x\u2032|x, a) with their sample estimates (\u03be\u2217\u03b8,x;N , \u03bb \u2217,P \u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N ) and PN (x\n\u2032|x, a) respectively.\nAt the end, we show the convergence of the above two-phase sampling procedure. Let dP \u03be\u03b8 (x|x0) and \u03c0P \u03be\u03b8 (x, a|x0) be the state and state-action occupancy measure induced by the transition probability function P \u03be\u03b8 (\u00b7|x), respectively. Similarly, let dP \u03be\u03b8;N (x|x0) and \u03c0P \u03be\u03b8;N (x, a|x0) be the state and state-action occupancy measure induced by the estimated transition probability function P \u03be\u03b8;N (\u00b7|x) := \u03be\u2217\u03b8,x;N (\u00b7)P\u03b8;N (\u00b7|x). From the two-phase sampling procedure for policy gradient estimation and by the strong law of large numbers, when N \u2192 \u221e, with probability 1, we have that 1N \u2211N j=1 \u2211\u221e t=0 \u03b3 t1{x(j)t = x, a (j) t = a} = \u03c0P \u03be\u03b8;N (x, a|x0). Based on the strongly convex property of the `2-regularized objective function in the inner robust optimization problem \u03c1N (\u03a6v), we can show that both the state-action occupancy measure \u03c0P \u03be\u03b8;N (x, a|x0) and the stage-wise cost h\u03b8;N (x, a) converge to the their true values within a value function approximation error bound \u2206 = \u2016\u03a6v\u2217\u03b8 \u2212 V\u03b8\u2016\u221e. We refer the readers to the supplementary materials for these technical results. These results together with Theorem E.4 imply the consistency of the policy gradient estimation.\nTheorem E.5. For any x0 \u2208 X , the following expression holds with probability 1:\u2223\u2223\u2223\u2223 limN\u2192\u221e 1N N\u2211 j=1 \u221e\u2211 t=0 \u03b3t \u2207 log\u00b5\u03b8(a(j)t |x (j) t ) h\u03b8,N (x (j) t , a (j) t )\n\u2212\u2207V\u03b8(x0) \u2223\u2223\u2223\u2223 = O(\u2206).\nThm. E.5 guarantees that as the value function approximation error decreases and the number of samples increases, the sampled gradient converges to the true gradient."}, {"heading": "F Convergence Analysis of Empirical PRSVI", "text": "Lemma F.1 (Technical Lemma). Let P (\u00b7|\u00b7) and P\u0303 (\u00b7|\u00b7) be two arbitrary transition probability matrices. At state x \u2208 X , for any \u03be : \u03beP (\u00b7|x) \u2208 U(x, P (\u00b7|x)), there exists a M\u03be > 0 such that for some \u03be\u0303 : \u03be\u0303P\u0303 (\u00b7|x) \u2208 U(x, P\u0303 (\u00b7|x)),\u2211\nx\u2032\u2208X |\u03be(x\u2032)\u2212 \u03be\u0303(x\u2032)| \u2264M\u03be \u2211 x\u2032\u2208X \u2223\u2223\u2223P (x\u2032|x)\u2212 P\u0303 (x\u2032|x)\u2223\u2223\u2223 . Proof. From Theorem 2.1, we know that U(x, P (\u00b7|x)) is a closed, bounded, convex set of probability distribution functions. Since any conditional probability mass function P is in the interior of dom(U) and the graph of U(x, P (\u00b7|x)) is closed, by Theorem 2.7 in [29], U(x, P (\u00b7|x)) is a Lipschitz set-valued mapping with respect to the Hausdorff distance. Thus, for any \u03be : \u03beP (\u00b7|x) \u2208 U(x, P (\u00b7|x)), the following expression holds for some M\u03be > 0:\ninf \u03be\u0302\u2208U(x,P\u0303 (\u00b7|x)) \u2211 x\u2032\u2208X |\u03be(x\u2032)\u2212 \u03be\u0302(x\u2032)| \u2264M\u03be \u2211 x\u2032\u2208X \u2223\u2223\u2223P (x\u2032|x)\u2212 P\u0303 (x\u2032|x)\u2223\u2223\u2223 . Next, we want to show that the infimum of the left side is attained. Since the objective function is convex, and U(x, P\u0303 (\u00b7|x)) is a convex compact set, there exists \u03be\u0303 : \u03be\u0303P\u0303 (\u00b7|x) \u2208 U(x, P\u0303 (\u00b7|x)) such that infimum is attained.\nLemma F.2 (Strong Law of Large Number). Consider the sampling based PRSVI algorithm with update sequence {v\u0302k}. Then as both N and k tend to\u221e, v\u0302k converges with probability 1 to v\u2217\u03b8 , the unique solution of projected risk sensitive fixed point equation \u03a0T\u00b5[\u03a6v] = \u03a6v.\nProof. By the strong law of large number of Markov process, the empirical visiting distribution and transition probability asymptotically converges to their statistical limits with probability 1, i.e.,\u2211N\u22121\nt=0 1{xt = x} N \u2192 d\u03b8(x|x0), and P\u0302 (x\u2032|x, a)\u2192 P (x\u2032|x, a), \u2200x, x\u2032 \u2208 X , a \u2208 A.\nTherefore with probability 1,\n1\nN N\u22121\u2211 t=0 \u03c6(xt)\u03c6(xt) > \u2192 \u2211 x d\u03b8(x|x0) \u00b7 \u03c6(x)\u03c6>(x),\n1\nN N\u22121\u2211 t=0 \u03c6(xt)C\u03b8(xt)\u2192 \u2211 x d\u03b8(x|x0) \u00b7 \u03c6(x)C\u03b8(x).\nNow we show that following expression holds with probability 1:\nmax \u03be : \u03beP\u03b8;N (\u00b7|xt)\u2208U(xt,P\u03b8;N (\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8;N (x \u2032|xt)v>\u03c6 (x\u2032) + 1 2N (\u03be(x\u2032)P\u03b8;N (x \u2032|xt))2\n\u2192 max \u03be : \u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|xt)v>\u03c6 (x\u2032) .\n(27)\nNotice that for {\u03be\u2217\u03b8,xt;N (x \u2032)}x\u2032\u2208X \u2208 arg max\u03be : \u03beP\u03b8;N (\u00b7|xt)\u2208U(xt,P\u03b8;N (\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x \u2032)P\u03b8;N (x \u2032|xt)v>\u03c6 (x\u2032), Lemma F.1 implies\nmax \u03be : \u03beP\u03b8;N (\u00b7|xt)\u2208U(xt,P\u03b8;N (\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8;N (x \u2032|xt)v>\u03c6 (x\u2032) + 1 2N (\u03be(x\u2032)P\u03b8;N (x \u2032|xt))2\n\u2212 max \u03be : \u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|xt)v>\u03c6 (x\u2032)\n\u2264\u2016\u03a6v\u2016\u221e ( M\u03be\u2217\u03b8,xt;N + max x\u2208X |\u03be\u2217\u03b8,xt;N (x)| ) \u2211 x\u2032\u2208X |P\u03b8(x\u2032|xt)\u2212 P\u03b8;N (x\u2032|xt)|+ 1 2N .\nThe quantity maxx\u2208X |\u03be\u2217\u03b8,xt;N (x)| is bounded because U(xt, P\u03b8;N (\u00b7|xt)) is a closed and bounded convex set from the definition of coherent risk measures. By repeating the above analysis by interchanging P\u03b8 and P\u03b8;N and combining previous arguments, one obtains\u2223\u2223\u2223\u2223\u2223 max\u03be : \u03beP\u03b8;N (\u00b7|xt)\u2208U(xt,P\u03b8;N (\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8;N (x \u2032|xt)v>\u03c6 (x\u2032) + 1 2N (\u03be(x\u2032)P\u03b8;N (x \u2032|xt))2\n\u2212 max \u03be : \u03beP\u03b8(\u00b7|xt)\u2208U(xt,P\u03b8(\u00b7|xt)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|xt)v>\u03c6 (x\u2032) \u2223\u2223\u2223\u2223\u2223 \u2264\u2016\u03a6v\u2016\u221emax {( M\u03be\u2217 + max\nx\u2208X |\u03be\u2217(x)|\n) , ( M\u03be\u2217\u03b8,xt;N + max x\u2208X |\u03be\u2217\u03b8,xt;N (x)| )} \u2211 x\u2032\u2208X |P\u03b8(x\u2032|xt)\u2212 P\u03b8;N (x\u2032|xt)|+ 1 2N .\nTherefore, the claim in expression (27) holds when N \u2192 \u221e and\u2211 x\u2032\u2208X |P\u03b8(x\u2032|xt)\u2212 P\u03b8;N (x\u2032|xt)| \u2192 0. On the other hand, the strong law of large numbers also implies that with probability 1,\n1\nN N\u22121\u2211 t=0 \u03c6(xt)\u03c1(\u03a6vt)\u2192 d\u03b8(x|x0)\u03c6(x) max \u03be : \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|x)v\u2217\u03b8 >\u03c6 (x\u2032) .\nCombining the above arguments implies\n1\nN N\u22121\u2211 t=0 \u03c6(xt)\u03c1N (\u03a6vt)\u2192 d\u03b8(x|x0)\u03c6(x) max \u03be : \u03beP\u03b8(\u00b7|x)\u2208U(x,P\u03b8(\u00b7|x)) \u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8(x \u2032|x)v\u2217\u03b8 >\u03c6 (x\u2032) .\nAs N \u2192 \u221e, the above arguments imply that vk \u2212 v\u0302k \u2192 0. On the other hand, Proposition 1 in [37] implies that the projected risk sensitive Bellman operator \u03a0T\u03b8[V ] is a contraction, it follows that from the analysis in Section 6.3 in [5] that the sequence {\u03a6v\u0302k} generated by projected value iteration converges to the unique fixed point \u03a6v\u2217\u03b8 . This in turns implies that the sequence {\u03a6vk} converges to \u03a6v\u2217\u03b8 ."}, {"heading": "G Technical Results", "text": "Since by convention \u03be\u2217\u03b8,x;N (x \u2032) = 0 whenever P\u03b8;N (x\u2032|x) = 0. In this section, we simplify the analysis by letting P\u03b8;N (x\u2032|x) > 0 for any x\u2032 \u2208 X without loss of generality. Consider the following empirical robust optimization problem:\nmax \u03be : \u03beP\u03b8;N (\u00b7|x)\u2208U(x,P\u03b8;N (\u00b7|x)) \u2211 x\u2032\u2208X P\u03b8;N (x \u2032|x)\u03be(x\u2032)V\u03b8(x\u2032), (28)\nwhere the solution of the above empirical problem is \u03be\u0304\u2217\u03b8,x;N and the corresponding KKT multipliers are (\u03bb\u0304\u2217,P\u03b8,x;N , \u03bb\u0304 \u2217,E \u03b8,x;N , \u03bb\u0304 \u2217,I \u03b8,x;N ). Comparing to the optimization problem for \u03c1N (\u03a6v), i.e.,\n\u03c1N (\u03a6v) = max \u03be : \u03beP\u03b8;N (\u00b7|x)\u2208U(x,P\u03b8;N (\u00b7|x)) \u2211 x\u2032\u2208X P\u03b8;N (x \u2032|x)\u03be(x\u2032)\u03c6>(x\u2032)v + 1 2N (\u03be(x\u2032)P\u03b8;N (x \u2032|x))2,\n(29) where the solution of the above empirical problem is \u03be\u2217\u03b8,x;N and the corresponding KKT multipliers are (\u03bb\u2217,P\u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N ), the optimization problem in (28) can be viewed as having a skewed objective function of the problem in (29), within the deviation of magnitude \u2206 + 1/2N where \u2206 = \u2016\u03a6v\u2217\u03b8 \u2212 V\u03b8\u2016\u221e. Before getting into the main analysis, we have the following observations.\n(i) Without loss of generality, we can also assume (\u03be\u2217\u03b8,x;N , (\u03bb \u2217,P \u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N )) follows the\nstrict complementary slackness condition6. 6The existence of strict complementary slackness solution follows from the KKT theorem and one can easily construct a strictly complementary pair using i.e. the Balinski-Tucker tableau with the linearized objective function and constraints, in finite time.\n(ii) Recall from Assumption 2.2 that the functions fi(\u03be, p) and ge(\u03be, p) are twice differentiable in \u03be at p = P\u03b8,N (\u00b7|x) for any x \u2208 X .\n(iii) The Slater\u2019s condition in Assumption 2.2 implies the linear independence constraint qualification (LICQ).\n(iv) Since optimization problem (29) has a convex objective function and convex/affine constraints in \u03be \u2208 R|X |, equipped with the Slater\u2019s condition we have that the first order KKT condition holds at \u03be\u2217\u03b8,x;N with the corresponding KKT multipliers are (\u03bb \u2217,P \u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N ).\nFurthermore, define the Lagrangian function\nL\u0302\u03b8;N (\u03be, \u03bb P , \u03bbE , \u03bbI) . = \u2211 x\u2032\u2208X P\u03b8;N (x \u2032|x)\u03be(x\u2032)\u03c6>(x\u2032)v + 1 2N (P\u03b8;N (x \u2032|x)\u03be(x\u2032))2\n\u2212\u03bbP (\u2211 x\u2032\u2208X \u03be(x\u2032)P\u03b8;N (x \u2032|x)\u22121 ) \u2212 \u2211 e\u2208E \u03bbE(e)fe(\u03be, P\u03b8;N (\u00b7|x))\u2212 \u2211 i\u2208I \u03bbI(i)fi(\u03be, P\u03b8;N (\u00b7|x)).\nOne can easily conclude that \u22072L\u0302\u03b8;N (\u03be, \u03bbP , \u03bbE , \u03bbI) = \u2212P\u03b8;N (\u00b7|x)>P\u03b8;N (\u00b7|x)/N \u2212\u2211 i\u2208I \u03bb I(i)\u22072\u03befi(\u03be, P\u03b8;N (\u00b7|x)) such that for any vector \u03bd 6= 0,\n\u03bd>\u22072L\u0302\u03b8;N (\u03be\u2217\u03b8,x;N , \u03bb \u2217,P \u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N )\u03bd < 0,\nwhich further implies that the second order sufficient condition (SOSC) holds at (\u03be\u2217\u03b8,x;N , \u03bb \u2217,P \u03b8,x;N , \u03bb \u2217,E \u03b8,x;N , \u03bb \u2217,I \u03b8,x;N ).\nBased on all the above analysis, we have the following sensitivity result from Corollary 3.2.4 in [13], derived based on Implicit Function Theorem.\nProposition G.1 (Basic Sensitivity Theorem). Under the Assumption 2.2, for any x \u2208 X there exists a bounded non-singular matrix K\u03b8,x and a bounded vector L\u03b8,x, such that the difference between the optimizers and KKT multipliers of optimization problem (28) and (29) are bounded as follows:\n\u03be\u0304\u2217\u03b8,x;N \u03bb\u0304\u2217,I\u03b8,x;N \u03bb\u0304\u2217,P\u03b8,x;N \u03bb\u0304\u2217,E\u03b8,x;N  =  \u03be\u2217\u03b8,x;N \u03bb\u2217,I\u03b8,x;N \u03bb\u2217,P\u03b8,x;N \u03bb\u2217,E\u03b8,x;N + \u03a6\u22121\u03b8,x\u03a8\u03b8,x ( \u2206 + 1 2N ) + o ( \u2206 + 1 2N ) .\nOn the other hand, we know from Proposition 4.4 that \u03be\u0304\u2217\u03b8,x;N \u2192 \u03be\u2217\u03b8,x and (\u03bb\u0304 \u2217,P \u03b8,x;N , \u03bb\u0304 \u2217,E \u03b8,x;N , \u03bb\u0304 \u2217,I \u03b8,x;N )\u2192 (\u03bb\u2217,P\u03b8,x , \u03bb \u2217,E \u03b8,x , \u03bb \u2217,I \u03b8,x) with probability 1 as N \u2192 \u221e. Also recall from the law of large numbers that the sampled approximation error maxx\u2208X ,a\u2208A \u2016P (\u00b7|x, a) \u2212 PN (\u00b7|x, a)\u20161 \u2192 0 almost surely as N \u2192\u221e. Then we have the following error bound in the stage-wise cost approximation h\u0302\u03b8;N (x, a) and \u03b3\u2212visiting distribution \u03c0N (x, a). Lemma G.2. There exists a constant Mh > 0 such that maxx\u2208X ,a\u2208A |h\u03b8(x, a) \u2212 limN\u2192\u221e h\u0302\u03b8;N (x, a)| \u2264Mh\u2206.\nProof. First we can easily see that for any state x \u2208 X and action a \u2208 A, |h\u0302\u03b8;N (x, a)\u2212 h\u03b8(x, a)| \u2264M \u2211 i\u2208I \u2223\u2223\u2223\u03bb\u2217,I\u03b8,x;N (i)\u2212 \u03bb\u2217,I\u03b8,x(i)\u2223\u2223\u2223+M\u2211 e\u2208E \u2223\u2223\u2223\u03bb\u2217,E\u03b8,x;N (e)\u2212 \u03bb\u2217,E\u03b8,x(e)\u2223\u2223\u2223+ \u2223\u2223\u2223\u03bb\u2217,P\u03b8,x;N \u2212 \u03bb\u2217,P\u03b8,x \u2223\u2223\u2223 + \u03b3\u2016V\u03b8\u2016\u221e\u2016\u03be\u2217\u03b8,x;N \u2212 \u03be\u2217\u03b8,x\u20161 + \u03b3\u2016V\u03b8 \u2212 \u03a6v\u2217\u03b8\u2016\u221e + \u03b3\u2016V\u03b8\u2016\u221emax{\u2016\u03be\u2217\u03b8,x;N\u2016\u221e, \u2016\u03be\u2217\u03b8,x\u2016\u221e}\u2016P (\u00b7|x, a)\u2212 PN (\u00b7|x, a)\u20161.\nNote that at N \u2192 \u221e, \u2016P (\u00b7|x, a) \u2212 PN (\u00b7|x, a)\u20161 \u2192 0 with probability 1. Both \u2016\u03be\u2217\u03b8;N\u2016\u221e and \u2016\u03be\u2217\u03b8,x\u2016\u221e are finite valued because U(P\u03b8) and U(P\u03b8;N ) are convex compact sets of real vectors.\nTherefore, by noting that \u2016V\u03b8\u2016\u221e \u2264 Cmax/(1\u2212 \u03b3) and applying Proposition 4.4 and G.1, the proof of this Lemma is completed by letting N \u2192\u221e and defining\nMh(x) = max{1,M, \u03b3Cmax 1\u2212 \u03b3 } \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  \u03be\u2217\u03b8,x;N \u2212 \u03be\u0304\u2217\u03b8,x;N \u03bb\u2217,I\u03b8,x;N \u2212 \u03bb\u0304 \u2217,I \u03b8,x;N \u03bb\u2217,P\u03b8,x;N \u2212 \u03bb\u0304 \u2217,P \u03b8,x;N\n\u03bb\u2217,E\u03b8,x;N \u2212 \u03bb\u0304 \u2217,E \u03b8,x;N\n+  \u03be\u0304\u2217\u03b8,x;N \u2212 \u03be\u2217\u03b8,x \u03bb\u0304\u2217,I\u03b8,x;N \u2212 \u03bb \u2217,I \u03b8,x \u03bb\u0304\u2217,P\u03b8,x;N \u2212 \u03bb \u2217,P \u03b8,x\n\u03bb\u0304\u2217,E\u03b8,x;N \u2212 \u03bb \u2217,E \u03b8,x\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\n1\n+ \u03b3\u2206\n\u2264 (\nmax{1,M, \u03b3Cmax 1\u2212 \u03b3\n}\u2016\u03a6\u22121\u03b8,x\u03a8\u03b8,x\u20161 + \u03b3 ) \u2206.\nLemma G.3. There exists a constant M\u03c0 > 0 such that \u2016\u03c0 \u2212 limN\u2192\u221e \u03c0N\u20161 \u2264M\u03c0\u2206.\nProof. First, recall that the \u03b3\u2212visiting distribution satisfies the following identity: \u03b3 \u2211 x\u2032\u2208X dP \u03be\u03b8 (x\u2032|x)P \u03be\u03b8 (x|x \u2032) = dP \u03be\u03b8 (x)\u2212 (1\u2212 \u03b3)1{x0 = x}, (30)\nFrom here one easily notice this expression can be rewritten as follows:( I \u2212 \u03b3P \u03be\u03b8 )> dP \u03be\u03b8 (\u00b7|x) = 1{x0 = x}, \u2200x \u2208 X .\nOn the other hand, by repeating the analysis with P\u03b8;N (\u00b7|x), we can also write( I \u2212 \u03b3P \u03be\u03b8;N )> dP \u03be\u03b8;N = {1{x0 = z}}z\u2208X .\nCombining the above expressions implies for any x \u2208 X ,\ndP \u03be\u03b8 \u2212 dP \u03be\u03b8;N \u2212 \u03b3\n(( P \u03be\u03b8 )> dP \u03be\u03b8 \u2212 (P \u03be\u03b8;N ) >dP \u03be\u03b8;N ) = 0,\nwhich further implies ( I \u2212 \u03b3P \u03be\u03b8 )> ( dP \u03be\u03b8 \u2212 dP \u03be\u03b8;N ) = \u03b3 ( P \u03be\u03b8 \u2212 P \u03be \u03b8;N )> dP \u03be\u03b8;N\n\u21d0\u21d2 ( dP \u03be\u03b8 \u2212 dP \u03be\u03b8;N ) = ( I \u2212 \u03b3P \u03be\u03b8 )\u2212> \u03b3 ( P \u03be\u03b8 \u2212 P \u03be \u03b8;N )> dP \u03be\u03b8;N .\nNotice that with transition probability matrix P \u03be\u03b8 (\u00b7|x), we have (I \u2212 \u03b3P \u03be \u03b8 ) \u22121 = \u2211\u221e t=0 ( \u03b3P \u03be\u03b8 )k <\n\u221e. The series is summable because by Perron-Frobenius theorem, the maximum eigenvalue of P \u03be\u03b8 is less than or equal to 1 and I \u2212 \u03b3P \u03be\u03b8 is invertible. On the other hand, for every given x0 \u2208 X ,{(\nP \u03be\u03b8 \u2212 P \u03be \u03b8;N )> dP \u03be\u03b8;N } (z\u2032) = \u2211 x\u2208X \u221e\u2211 k=0 \u03b3k(1\u2212 \u03b3)PP \u03be\u03b8;N (xk = x|x0) ( P \u03be\u03b8 (z \u2032|x)\u2212 P \u03be\u03b8;N (z \u2032|x) ) , \u2200z\u2032 \u2208 X\n=EP \u03be\u03b8;N ( \u221e\u2211 k=0 \u03b3k(1\u2212 \u03b3) ( P \u03be\u03b8 (z \u2032|xk)\u2212 P \u03be\u03b8;N (z \u2032|xk) ) |x0 ) , \u2200z\u2032 \u2208 X\n\u2264EP \u03be\u03b8;N ( \u221e\u2211 k=0 \u03b3k(1\u2212 \u03b3) \u2223\u2223\u2223P \u03be\u03b8 (z\u2032|xk)\u2212 P \u03be\u03b8;N (z\u2032|xk)\u2223\u2223\u2223 |x0 ) , \u2200z\u2032 \u2208 X . =Q(z\u2032), \u2200z\u2032 \u2208 X .\nNote that every element in matrix (I \u2212 \u03b3P \u03be\u03b8 )\u22121 = \u2211\u221e t=0 ( \u03b3P \u03be\u03b8 )k is non-negative. This implies for any z \u2208 X ,\u2223\u2223\u2223{dP \u03be\u03b8 \u2212 dP \u03be\u03b8;N} (z)\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223{(I \u2212 \u03b3P \u03be\u03b8 )\u2212> \u03b3 (P \u03be\u03b8 \u2212 P \u03be\u03b8;N)> dP \u03be\u03b8;N } (z)\n\u2223\u2223\u2223\u2223 , \u2264 \u2223\u2223\u2223\u2223{(I \u2212 \u03b3P \u03be\u03b8 )\u2212> \u03b3Q} (z)\u2223\u2223\u2223\u2223 = {(I \u2212 \u03b3P \u03be\u03b8 )\u2212> \u03b3Q} (z).\nThe last equality is due to the fact that every element in vector Q is non-negative. Combining the above results with Proposition 4.4 and G.1, and noting that\n(I \u2212 \u03b3P \u03be\u03b8 ) \u22121e = \u221e\u2211 t=0 ( \u03b3P \u03be\u03b8 )k e = 1 1\u2212 \u03b3 e,\nwe further have that\n\u2016\u03c0 \u2212 \u03c0N\u20161 =\u2016dP \u03be\u03b8 \u2212 dP \u03be\u03b8;N \u20161 \u2264e> ( I \u2212 \u03b3P \u03be\u03b8 )\u2212> \u03b3Q\n= \u03b3\n1\u2212 \u03b3 e>Q\n\u2264 \u03b3 1\u2212 \u03b3 max x\u2208X \u2225\u2225\u2225P \u03be\u03b8 (\u00b7|x)\u2212 P \u03be\u03b8;N (\u00b7|x)\u2225\u2225\u2225 1 \u2264 \u03b3 1\u2212 \u03b3 max x\u2208X ( \u2016\u03be\u2217\u03b8,x(\u00b7)\u2212 \u03be\u2217\u03b8,x;N (\u00b7)\u20161\u2016P\u03b8(\u00b7|x)\u2016\u221e + max{\u2016\u03be\u2217\u03b8,x;N\u2016\u221e, \u2016\u03be\u2217\u03b8,x\u2016\u221e}\u2016P (\u00b7|x, a)\u2212 PN (\u00b7|x, a)\u20161 ) ,\nAs in previous arguments, when N \u2192 \u221e, one obtains \u2016P (\u00b7|x, a)\u2212 PN (\u00b7|x, a)\u20161 \u2192 0 with probability 1 and \u2016\u03be\u2217\u03b8,x(\u00b7)\u2212\u03be\u2217\u03b8,x;N (\u00b7)\u20161 \u2192 0. We thus set the constantM\u03c0 as \u03b3\u2016\u03a6 \u22121 \u03b8,x\u03a8\u03b8,x\u20161/(1\u2212\u03b3)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Several authors have recently developed risk-sensitive policy gradient methods<lb>that augment the standard expected cost minimization problem with a measure of<lb>variability in cost. These studies have focused on specific risk-measures, such as<lb>the variance or conditional value at risk (CVaR). In this work, we extend the pol-<lb>icy gradient method to the whole class of coherent risk measures, which is widely<lb>accepted in finance and operations research, among other fields. We consider<lb>both static and time-consistent dynamic risk measures. For static risk measures,<lb>our approach is in the spirit of policy gradient algorithms and combines a standard<lb>sampling approach with convex programming. For dynamic risk measures, our ap-<lb>proach is actor-critic style and involves explicit approximation of value function.<lb>Most importantly, our contribution presents a unified approach to risk-sensitive<lb>reinforcement learning that generalizes and extends previous results.", "creator": "LaTeX with hyperref package"}}}