{"id": "1502.04492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "abstract": "we propose a multi - layer network topology based on the bayesian decision framework of the factor graphs in reduced normal form ( fgrn ) applied to a two - dimensional lattice. the latent variable model ( lvm ) formulation is the basic building block of a regular quadtree hierarchy function built on from top of a continuous bottom layer of random variables that represent local pixels of an ideal image, for a small feature map, functions or more generally alternatively a collection of spatially distributed discrete variables. the multi - layer architecture implements by a layered hierarchical data representation layer that, via belief propagation, can be used successively for learning and inference. typical uses involved are pattern completion, correction and classification. the fgrn paradigm provides great flexibility handling and modularity and appears as a promising candidate for building deep networks : the system can be easily extended thus by introducing new and different ( in cardinality and in type ) variables. prior knowledge, or supervised information, can be introduced at different scales. the fgrn paradigm provides a handy way for building into all kinds of architectures by interconnecting only three types of units : single input single output ( siso ) blocks, sources clusters and replicators. the network is designed like a circuit diagram and therefore the basic belief pathway messages simply flow bidirectionally in the whole system. the learning algorithms operate only locally within each block. the framework is demonstrated in this paper in a three - layer structure applied to images extracted beforehand from a standard data set.", "histories": [["v1", "Mon, 16 Feb 2015 11:01:25 GMT  (2038kb,D)", "http://arxiv.org/abs/1502.04492v1", "Submitted for journal publication"]], "COMMENTS": "Submitted for journal publication", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amedeo buonanno", "francesco a n palmieri"], "accepted": false, "id": "1502.04492"}, "pdf": {"name": "1502.04492.pdf", "metadata": {"source": "CRF", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "authors": ["Amedeo Buonanno", "Francesco A.N. Palmieri"], "emails": ["amedeo.buonanno@unina2.it", "francesco.palmieri@unina2.it"], "sections": [{"heading": null, "text": "Keywords: Bayesian Networks, Factor Graphs, Deep Belief Networks"}, {"heading": "1. Introduction", "text": "Building efficient representations for images, and more in general for sensory data, is one of the central issues in signal processing. The problem has received much attention in the literature of the last thirty years because, almost invariably, the extraction of information from observations requires that raw data is translated first into \u201cfeature maps\u201d before classification or filtering.\nRecent striking results with \u201cdeep networks\u201d have generated much attention in machine learning on what is known as Representation Learning (see (Bengio et al., 2012) for a review). The main idea of these methods is to learn multiple representation levels as progressive\nc\u00a92015 Amedeo Buonanno and Francesco A.N. Palmieri.\nar X\niv :1\n50 2.\n04 49\n2v 1\n[ cs\n.C V\n] 1\nabstractions of the input data. The creation of a feature hierarchy permits to the structure inside the data to emerge at different scales combining more and more complex features as we go upward in the hierarchy (Bengio and Delalleau, 2011), (Bengio et al., 2014).\nFor image understanding this process is somewhat biologically plausible too. There is a vast literature that postulates the hierarchical organization of the primary visual cortex (V1). The neurons become selective for stimuli that are increasingly complex, from simple oriented bars and edges to moderately complex features, such as a combination of orientations, to complex objects (Serre and Poggio, 2010). We do not derive our models from the biology, but we cannot avoid recognizing that the most successful artificial systems paradigms share some common features with what is observed in nature.\nIn building an artificial system, one of the key issues is to provide sufficiently general methods that can be applied across different kinds of sensory data, letting learning capture most of the specificity of the application context. This is why in this work we focus on a Bayesian network approach, that has the advantage of being totally general with respect to the type of data processed defining a framework that can easily fuse information coming from different sensor modalities. In a Bayesian network the information flow is bi-directional via belief propagation and can easily accommodate various kinds of inferences for pattern completion, correction and classification.\nVarious architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity. Message propagation follows standard sum-product rules, but the system is built as the interconnection of only SISO blocks, source blocks and replicators with learning equations defined in a totally localized fashion.\nIn this paper we proposes a new deep architecture based on FGrn applied to a twodimensional lattice. The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy. Learning is totally localized inside the SISO blocks that constitute the LVMs. The complete system can be seen as a partitioned type of Latent Tree Model (Mourad et al., 2013).\nThe application of the Bayesian model to images shows how the hierarchy extracts the primitives at various scales and how, via bi-directional belief propagation, it provides a reliable structure for learning and inference in various modes.\nIn Section 2 we review some of the related literature while in Section 3 we introduce notations and the basics of belief propagation in FGrn. In Section 4 we present the LVM, i.e. the building block for the multi-layer architecture that is presented in Section 5 with the learning strategy and the Encoding/Decoding process. In Section 6 we apply learning and inference to images from a standard data set. Section 7 includes conclusive remarks and suggestions for further work."}, {"heading": "2. Related Work", "text": "The vast literature on the deep representation learning (see the extensive overview in (Schmidhuber, 2015)) can be mostly divided in two main lines of research: the first one is based on probabilistic graphical models such as the Restricted Boltzmann Machine (RBM)\n(Hinton et al., 2006), (Hinton and Salakhutdinov, 2006), (Lee et al., 2008) and the second one is based on neural network models as the autoencoder (Bengio et al., 2007), (Ranzato et al., 2006). At the same time several unsupervised feature learning algorithms have been proposed: Sparse Coding (Olshausen and Field, 1996),(Lee et al., 2008), RBM (Hinton et al., 2006), Autoencoders (Bengio et al., 2007), (Ranzato et al., 2006), (Vincent et al., 2008), K-means (Coates and Ng, 2012). Other models based on the memory-prediction theory of brain have also been proposed (Hawkins, 2004), (Dileep, 2008).\nConfining our interest to probabilistic graphical models, the most natural choice for modeling the spatial interactions between pixels (or patches) in the image is a two-dimensional lattice (Markov Random Field - MRF) where the nodes represent the pixels (or patches) and the potential functions are associated to the edges between adjacent nodes (Wainwright and Jordan, 2008). Various tasks in image processing such as denoising, segmentation, and super-resolution, can be treated as an inference problem on the MRF. For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al., 1999), (Beal, 2003), graph cut (Boykov et al., 1999) and Belief Propagation (Xiong et al., 2007).\nAn alternative strategy to MRF is to replace the 2D lattice with a simpler and approximate model as multiscale (or multiresolution) structures like quadtrees. These have the advantages of allowing the application of efficient tree algorithms to perform exact inference with the trade off that the model is imperfect (Luettgen et al., 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al., 2000), (Willsky, 2002). Another problem of the quadtree structure is the non locality since two neighboring pixels may or may not share a common parent node depending on their position on the grid. For avoiding this problem Wolf et al. have proposed a Markov cube adding additional connections at the different levels (Wolf and Gavin, 2010).\nOn the quadtree structure inference can be performed using the belief propagation algorithm that was originally proposed for inferences on trees where exact solutions are guaranteed (Pearl, 1988). When the graph has loops, open issues still remain about the accuracy of inferences, even though often the bare application of standard belief propagation may already provide satisfactory results (loopy belief propagation) (Yedidia et al., 2005), (Frean, 2008). When the problem can be reduced to a tree, belief propagation provides exact marginalization and algorithms for learning latent trees have been proposed (Choi et al., 2011) with successful applications to computer vision.\nA very appealing approach to directed Bayesian graphs for visualization and manipulation, that has not found its full way in the applications, is the Factor Graph (FG) representation and in particular the so-called Normal Form (FGn) (Forney, 2001), (Loeliger, 2004). This formulation is very appealing because it provides an easy way to visualize and manipulate Bayesian graphs - much like in block diagrams. Factor Graphs assign variables to edges and functions to nodes. Furthermore, in the Reduced Normal Form (FGrn), through the use of replicator units (or equal constraints), the graph is reduced to an architecture in which each variable is connected to two factors at most (Palmieri, 2013). In this way any architecture (deep or shallow) can be built as the interconnection of only three types of\nunits: Single Input Single Output (SISO) blocks, Sources and Diverters (Replicators), with the learning equations defined locally (Figure 1).\nThis is the framework on which this paper is focused because the designed network resembles a circuit diagram with belief messages more easily visualized as they flow into SISO blocks and travel through replicator nodes (Buonanno and Palmieri, 2014). This paradigm provides extensive modularity because replicators act like buses and can be used as expansion nodes when we need to augment an existing model with new variables. Parameter learning, in this representation, can be approached in a unified way because we can concentrate on a unique rule for training any SISO, or Source, factor-block in the system, regardless of its location (visible or hidden).\nIn our previous work (Palmieri and Buonanno, 2014) we have reported some preliminary results on a multi-layer convolution Bayesian Factor Graph built as a stack of HMM-like trees. Each layer is built from a latent model trained on the messages coming from the layer below. The structure is loopy, but our experience has shown that BP performs well in recovering information from the deep parts of the network: the upper layers contain progressively larger-scale information that is pipelined to the bottom for pattern completion or correction across sequences.\nIn this work we step back and confine our attention to a quadtree structure, for which no loops are present and inference is exact. We have found that this framework, even if just a tree, has great potential of being used in a very large number of applications for its inherent modularity at the expenses of a certain growth in computational complexity. The complexity issue will be discussed in the paper. To our knowledge the FGrn framework has never been used to build deep networks."}, {"heading": "3. Factor Graphs in Reduced Normal Form", "text": "In the FGrn framework (Palmieri, 2013) the Bayesian graph is reduced to a simplified form composed only by Variables, Replicators (or Diverters), Single-Input/Single-Output (SISO) blocks and Source blocks. Even though various architectures have been proposed in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to handle, it is more suitable to define unique learning equations (Palmieri, 2013) and it is more suited for distributed implementations. The blocks needed to compose any architecture are shown in Figure 1. In our notation we avoid the upper arrows for the messages and assign a direction to each variable branch for unambiguous definition of forward and backward messages.\nFor a variable X (Figure 1(a)) that takes values in the discrete alphabet X = {\u03be1, \u03be2, . . . , \u03bedX}, forward and backward messages are in function form bX(\u03bei) and fX(\u03bei), i = 1 : dX and in vector form bX = (bX(\u03be1), bX(\u03be2), . . . , bX(\u03bedX )) T and fX = (fX(\u03be1), fX(\u03be2), . . . , fX(\u03bedX )) T . All messages are proportional (\u221d) to discrete distributions and may be normalized to sum to one.\nComprehensive knowledge about X is contained in the posterior distribution pX obtained through the product rule, pX(\u03bei) \u221d fX(\u03bei)bX(\u03bei), i = 1 : dX , in function form, or pX \u221d fX bX , in vector form, where denotes the element-by-element product. The result of each product is proportional to a distribution and can be normalized to sum one (it is a good practice to keep messages normalized to avoid poorly conditioned products).\nThe replicator (or diverter) (Figure 1(b)) represents the equality constraint with the variableX replicated (D+1) times. Messages for incoming and outgoing branches carry different forward and backward information. Messages that leave the block are obtained as the product of the incoming ones: bX(0)(\u03bei) \u221d \u220fD j=1 bX(j)(\u03bei); fX(k)(\u03bei) \u221d fX(0)(\u03bei) \u220fD j=1,j 6=k bX(j)(\u03bei), k = 1 : D, i = 1 : dx in function form. In vector form: b (0) X \u221d Dj=1b (j) X ; f (k) X \u221d f (0) X Dj=1,j 6=k b (j) X , k = 1 : D.\nThe SISO block (Figure 1(c)) represents the conditional probability matrix of Y given X. More specifically if X takes values in the discrete alphabet X = {\u03be1, \u03be2, ..., \u03bedX} and Y in Y = {\u03c51, \u03c52, ..., \u03c5dY }, P (Y |X) is the dX \u00d7 dY row-stochastic matrix P (Y |X) = [Pr{Y = \u03c5j |X = \u03bei}]j=1:dYi=1:dX = [\u03b8ij ] j=1:dY i=1:dX . Outgoing messages are: fY (\u03c5j) \u221d \u2211dX\ni=1 \u03b8ijfX(\u03bei); bX(\u03bei) \u221d\u2211dY j=1 \u03b8ijbY (\u03c5j), in function form. In vector form: fY \u221d P (Y |X)T fX ; bX \u221d P (Y |X)bY . The source block in Figure 1(d) is the termination for the independent source variable X. More specifically \u03c0X is the dX -dimensional prior distribution on X with the outgoing message fX(\u03bei) = \u03c0X(\u03bei), i = 1 : dX in function form, or fX = \u03c0X in vector form. The backward message bX coming from the network can be combined with the forward fX for final posterior on X.\nFor the reader not familiar with the factor graph framework, it should be emphasized that the above rules are rigorous translation of Bayes\u2019 theorem and marginalization. For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al., 2001)).\nParameters (probabilities) in the SISO and the source blocks must be learned from examples solely on the backward and forward flows available locally. We set the learning problem as an EM algorithm to maximize global likelihood (Palmieri, 2013). Focusing on a specific SISO block, if all the other network parameters have been fixed, maximization of global likelihood translates in the local problem from examples (fX[n],bY [n]), n = 1, ..., Ne{\nmin\u03b8 \u2212 \u2211Ne n=1 log ( fTX[n] \u03b8 bY [n] ) , \u03b8 row \u2212 stochastic. (1)\nAfter adding a stabilizing term to the cost function and applying KKT conditions we obtain the following algorithm (Palmieri, 2013).\nAlgorithm 1 Learning Algorithm for SISO block\n1: procedure Learning Algo 2: Initialize \u03b8 to uniform rows: \u03b8 = (1/dY )1dX\u00d7dY 3: for i = 1 : dX do 4: ftmp(i) = \u2211Ne n=1 fX[n](i) 5: end for 6: for it = 1 : Nit do 7: for n = 1 : Ne do 8: den(n) = fTX[n]\u03b8bY [n] 9: end for\n10: for i = 1 : dX do 11: for j = 1 : dY do\n12: tmpSum = \u2211Ne\nn=1 fX[n](i)bY [n](j) den(n)\n13: \u03b8ij \u2190 \u03b8ijftmp(i) \u00b7 tmpSum, 14: end for 15: end for 16: Row-normalize \u03b8 17: end for 18: end procedure\n(We have used the shortened notation fX[n](\u03bei) = fX[n](i), bY [n](vj) = bY [n](j)).\nIn Algorithm 1 there are three main blocks and the complexity in the worst case is O(Ne \u00b7 dX \u00b7 dY \u00b7Nit). The algorithm is a fast multiplicative update with no free parameters. The iterations usually converge in a few steps and the presence of the normalizing factors makes the algorithm numerically very stable. The algorithm has been discussed and compared to other similar updates in (Palmieri, 2013).\nThe updates for the source block are immediate if we set the forward messages fX[n] to a uniform distribution and consider any row of \u03b8 to be the target distribution."}, {"heading": "4. Bayesian Clustering", "text": "For the architectures that will follow, the basic building block is the Latent-Variable Model (LVM) shown in Figure 2. At the bottom of each LVM there are N \u00b7M variables X[n,m], n = 1 : N , m = 1 : M that belong to a finite alphabet X = {\u03be1, \u03be2, . . . , \u03bedX}. The variables are organized here on a plane (as an image) because they will compose the layers of a multi-layer architecture. The N \u00b7M variables code multiple discrete labels, that in the application that follows take values in the same alphabet, but they could easily have different cardinalities if we need to fuse information coming from different sources (the combination of the heterogeneous variables is one of the most powerful peculiarities of the FGrn paradigm). Generally the complexity of the whole system increases with the cardinality of the alphabets.\nThe N \u00b7M bottom variables are connected to one Hidden (Latent) Variable S, that belongs to the finite alphabet S = {\u03c31, \u03c32, . . . , \u03c3dS}. The replicator block of Figure 1(b) is drawn here as a box as it will be a patch of the upper plane of each layer in our architecture. Each connection to the bottom layer is a SISO block that represents the dS \u00d7 dX rowstochastic probability matrix\nP (X[n,m]|S) =  P (X[n,m] = \u03be1|S = \u03c31) . . . P (X[n,m] = \u03bedX |S = \u03c31) P (X[n,m] = \u03be1|S = \u03c32) . . . P (X[n,m] = \u03bedX |S = \u03c32)\n... P (X[n,m] = \u03be1|S = \u03c3dS ) . . . P (X[n,m] = \u03bedX |S = \u03c3dS )  The system is drawn as a generative model with the arrows pointing down assuming that the source is variable S and the bottom variables are its children. This architecture can be seen also as a Mixture of Categorical Distributions (Koller and Friedman, 2009). Each element of the alphabet S represents a \u201dBayesian cluster\u201d for the N \u00b7M dimensional stochastic image, X = [X[n,m]]m=1:Mn=1:N (similar to the Naive Bayes classifier (Barber, 2012)). Essentially each bottom variable is independent from the others given the Hidden Variable (Koller and Friedman, 2009). One way to visualize the model is to imagine drawing a sample: for each data point we draw a cluster index s \u2208 S = {\u03c31, \u03c32, . . . , \u03c3dS} according to the prior distribution \u03c0S . Then for each n = 1 : N , m = 1 : M , we draw x[n,m] \u2208 {\u03be1, \u03be2, . . . , \u03bedX} according to P (X[n,m]|S = s).\nWe can perform exact inference simply by letting the messages propagate and collecting the results. Information can be injected at any node and inference can be obtained for each variable using the usual sum-product rules. For each SISO block of Figure 2 the incoming messages (bX and fS) and the outgoing messages (fX and bS) flow simultaneously following the rules outlined in the previous section (sum rule). In the replicator block, incoming messages from all directions are combined with product rule to produce outgoing messages. We can imagine the replicator block as acting like a bus where information is combined and diverted towards the connected branches.\nHandling information in the Bayesian architecture is very flexible since each variable X[n,m] corresponds to a pair of messages. The backward message coming from below\nis propagated upward towards the latent variable and, through the diverter, towards the sibling branches downwards to the forward messages at the terminations. At the same time the latent variable, fed through its forward message from above, sends information downward through the diverter."}, {"heading": "5. Multi-layer FGrn", "text": "In this work we build a multilayer structure as in Figure 3(a) on top of a bottom layer of random variables. They can be pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. In the following we refer to the bottom variables as the Image.\nThe architecture that lays on top of the Image is the quadtree. In Figure 3 the cyan spheres are the image variables and the other ones (red, green and blue) are the embedding (latent or hidden) variables of the LVM blocks. In Figure 3(b) the same architecture is represented as a FGrn.\nA network with L + 1 levels (Layer 0, . . . ,Layer L) covers a bottom image (Layer 0) S0[n,m] n = 1 : N \u00b7 2L\u22121, m = 1 : M \u00b7 2L\u22121, subdivided in (2L\u22121) \u00b7 (2L\u22121) image patches of dimension (N \u00b7M). At Layer 1 each patch is managed by one of the latent variables S1[n,m], n = 1 : 2\nL\u22121, m = 1 : 2L\u22121 of cardinality of dS1 . At Layer 2 each latent variable S2[n,m], n = 1 : 2\nL\u22122, m = 1 : 2L\u22122 with dimension dS2 , is connected to 4 variables of Layer 1. Similarly climbing the tree in quadruples up to the root with variable SL.\nMessages travel within each layer (among the subset of LVM variables) and with the layers above and below (within the connected patches and quadruples). The architecture builds a hierarchical Bayesian clustering of the information that is exchanged across different representation scales."}, {"heading": "5.1 Inference Modes", "text": "If the network parameters have been learned, the system can be used in the following main inference modes:\nGenerative: A latent variable Si[n,m] is fixed to a value \u03c3 i k, i.e. its forward distribution is a delta fSi(s) = \u03b4(s\u2212\u03c3ik), \u03c3ik \u2208 Si = {\u03c3i1, \u03c3i2, . . . , \u03c3idSi}. After message propagation downward the forward messages at the terminal variables S0[n,m] in the cone subtended by Si reveal the k-th \u201dhierarchical conditional distribution\u201d associated to Si. This generation could be done on Layer 1 to check for clusters in the image patches, or at higher layers to visualize the coding role of the various hierarchical representations.\nOf course propagation can be done from a generic node also upward with a backward delta distribution. The complete upward and downward flow, up to the tree root and down to the other terminations, would reveal the role of that specific node in the representation memorized in the system (a sort of impulse response). Encoding: The image S0 = S0[n,m], n = 1 : N \u00b7 2L\u22121, m = 1 : M \u00b7 2L\u22121, is known and the values of the bottom variables are injected in the backward messages as delta distributions. After all messages have been propagated for a number of steps equal to the network diameter (in this case 2 \u00b7 L + 1), at each hidden variable Si[n,m], i = 1, . . . , L,\nn = 1 : 2L\u2212i, m = 1 : 2L\u2212i, we find the contribution of the observations to the posterior for Si[n,m]. The exact posterior on Si[n,m] is obtained as the normalized product of forward and backward messages. Each hidden variable represents one of the components of the (soft) code of the image.\nPattern Completion: Only some of the bottom variables of S0 are known, i.e. their backward messages are deltas. For the unknown variables the backward messages are uniform distributions. In this modality, after at least 2 \u00b7 L + 1 steps, the network returns forward messages at the terminal variables that try to complete the pattern (associative recall, or content-addressed memory).\nError Correction: Some of the bottom variables of S0 are known softly, or wrongly, with smooth distributions, or delta functions respectively. After at least 2 \u00b7 L + 1 steps of message propagation, the network produces forward messages at the terminations that attempt to correct the distributions, or reduce the uncertainty. The posterior distributions at the terminal variables S0[n,m] are obtained as the normalized product of forward and backward messages.\nBefore propagation, all messages that do not correspond to evidence, are initialized to uniform distributions."}, {"heading": "5.2 Learning", "text": "The parameters contained in the FG are learned from a training set of T images S10, . . . ,S T 0 .\nWe assume that within each layer the LVM blocks share the same parameters. This is a standard shift-invariance assumption that most deep belief networks make.\nGiven a basic patch of dimension N \u00b7M pixels and a network with L+ 1 levels (Layer 0, . . . ,Layer L), for Layer 1 we need to learn N \u00b7M matrices P (S0[n,m]|S1) (one per pixel) each one having sizes dS1 \u00d7 dS0 and the dS1-dimensional prior vector \u03a0S1 .\nFor Layers 2 to L we need to learn 4 matrices P (Si\u22121[n,m]|Si) having sizes dSi \u00d7 dSi\u22121 and the dSi-dimensional prior vector \u03a0Si .\nA generic image of the training set is subdivided in L-Level patches of dimension (2L\u22121 \u00b7 N) \u00b7 (2L\u22121 \u00b7M) pixels. Each L-Level patch is subdivided in 2 \u00b72 (L\u22121)-Level patches, 22 \u00b722 (L\u22122)-Level patches and so on until to obtain (2L\u22121) \u00b7 (2L\u22121) patches of dimension (N \u00b7M) pixels at Layer 1 (0-Level and 1-Level patches are the same).\nThe examples, subdivided in 1st-Level patches, are presented to the termination of the LVM in Fig. 2 as sharp backward distributions for a fixed number of steps (epochs). All SISO blocks and the source block adapt their parameters using an iterative Maximum Likelihood Algorithm (Palmieri (2013)) outlined in Section 3.\nOnce the Layer 1 is learned, the 2nd-Level patches are used to learn Layer 2 constructed combining 4 LVMs of Layer 1 and the process goes on, building deeper and deeper network and considering larger and larger patches.\nAt the end of the learning phase the matrices are frozen and used in one of the inference modes described before on the same training set to check for accuracy and on a test set to check for generalization.\nMore specifically, learning is off-line and it is composed by the following steps for an architecture of L+ 1 layers and a basic patch dimension of N \u00b7M pixels:\n1. We randomly select P L-Level Patches from each image in the Training Set composed by T images. Therefore, in the learning phase, we have T \u00b7 P L-Level patches that are subdivided in Patches of the lower levels until to obtain 1st-Level Patches;\n2. The T \u00b7P \u00b7 (2L\u22121 \u00b72L\u22121) 1st-Level Patches of (N \u00b7M) pixels are injected at the bottom of the LVM and the parameters are learned (Figure 4(a));\n3. A new 3-Layers network (0-2) is built replicating 2 \u00b7 2 times the LVM block learned above and connecting their Hidden Variables with another LVM Block;\n4. The T \u00b7P \u00b7 (2L\u22122) \u00b7 (2L\u22122) Patches of (2 \u00b7N) \u00b7 (2 \u00b7M) pixels are injected at the bottom of the new 3-Layers network and the backward messages at the top of Layer 1 are used to learn the LVM block at Layer 2 (Figure 4(b));\n5. A new 4-Layers network (0-3) is built replicating for 22 \u00b7 22 times the LVM block learned at step 2 and for 2 \u00b7 2 times the LVM block learned at step 4 and connecting their Hidden Variables to another LVM Block;\n6. The T \u00b7 P \u00b7 (2L\u22123) \u00b7 (2L\u22123) Patches of (22 \u00b7N) \u00b7 (22 \u00b7M) pixels are propagated in the Layer 1 and Layer 2 and the backward messages at the top of the Layer 2, are used to learn the LVM block at Layer 3 (Figure 4(c));\n7. The same progression is applied to all the other layers, extending the number of LVM block replicas to cover the dimension of the current-Level Patch."}, {"heading": "6. Simulations", "text": "In this set of simulations we have taken 50 car images from Caltech101 Dataset. Each image is cropped, filtered with an anisotropic diffusion algorithm (Kovesi), whitened and finally filtered with a Canny filter in order to obtain images with only the car borders. Our input alphabet is binary (dS0 = 2). From the 50 filtered images a set of 500 image patches of 32 \u00b7 32 pixels are randomly extracted. A small subset is shown in Fig. 5."}, {"heading": "6.1 Learning", "text": "The steps for the learning phase are described in the previous and use the following variables: P = 10, T = 50, N = 8, M = 8, L = 3, dS0 = 2, dS1 = 100, dS2 = 300, dS3 = 300."}, {"heading": "6.2 Inference", "text": "Once the matrices have been learned we use the network in various inference modes:\nGenerative mode: We obtain forward distributions at the bottom terminations by injecting at the top of the various structures a delta distribution (the images in gray scale show at each pixel the probability on one of the two symbols). More specifically, for visualizing the conditional distributions corresponding to Layer 1 we consider only the Latent Model in Figure 2; for Layer 2 we consider the 3-Layers architecture composed by 4 LVM Blocks connected to one LVM Block; for Layer 3 we consider the complete architecture. Figures 6, 7 and 8 show respectively the forward distributions generated injecting deltas at Layers 1, 2 and 3.\nThe network has stored quite well the complex structures. The forward distributions from Layer 1 represent simple orientation patterns similar to the ones that the early human\nvisual system responds to. At Layer 3 the distributions reflect the combined representations stored at larger scales.\nPattern Completion: In these experiments we have used the architecture as an associative memory that is queried with incomplete patterns and responds with the information stored during the learning phase. Figure 9(a) shows 20 patches extracted from Training Set. Before the injection at the bottom of the network, a considerable amount of pixels (16 patches on a total of 32 patches) has been erased (Figure 9(b)), i.e. the delta backward distribution is replaced with an uniform distribution. Figure 9(c) shows the forward distributions for the same images after message propagation. The network is able to resolve quite well most of the uncertainties using the stored information. The same experiment of Pattern Completion has been repeated with patches extracted from the Test Set (patterns that the network has never seen before). The result is obviously worse as shown in Figure 10, but it\u2019s worthy to note that the network succeeds quite well in completing most of the shapes in applying the learned knowledge (test for generalization). Cross-validation can be easily applied to determine the most appropriate embedding spaces sizes for best generalization."}, {"heading": "7. Discussion and Conclusions", "text": "From the results presented above we have demonstrated that the paradigm of the FGrn can be successfully applied to build deep architectures. The layers retain the information about the clusters contained in the data and build a hierarchical internal representation. Each layer successfully learns to compose the objects made available from the lower layers.\nWe have chosen the border of car images extracted from Caltech101 because we wanted to see if the paradigm was suitable for patching together the salient structures of an object. Other experiments have been performed on characters and different patterns revealing very similar results.\nWe believe that the FGrn paradigm constitutes a promising addition to the various proposals for deep networks that are appearing in the literature. It can provide great flexibility and modularity. The network can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge and supervised information can be inserted at any of the scales: new \u201clabel variables\u201d can be added in one or more of the diverter junctions and let learning take care of parameter adaptation. Results of these mixed supervised/unsupervised architectures are under way and will be reported elsewhere.\nThe computational complexity issue that clearly emerges from this paradigm, specially when the embedding variables have large dimensionality and when image pixels are non binary, is being exploited for parallel implementations. Since both belief propagation and learning are totally local, they can be implemented with distributed hardware or parallelized processes. Some studies have been carried out for other deep network frameworks (Liang et al., 2009), (Silberstein et al., 2008), (Ma et al., 2012)) and we are confident that similarly the FGrn paradigm may present new interesting opportunities to approach some of the most challenging tasks in computer vision."}], "references": [{"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "Barber.,? \\Q2012\\E", "shortCiteRegEx": "Barber.", "year": 2012}, {"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis, University of London,", "citeRegEx": "Beal.,? \\Q2003\\E", "shortCiteRegEx": "Beal.", "year": 2003}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "On the expressive power of deep architectures", "author": ["Yoshua Bengio", "Olivier Delalleau"], "venue": "Algorithmic Learning Theory,", "citeRegEx": "Bengio and Delalleau.,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau.", "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "URL http://arxiv.org/abs/1206.5538. cite arxiv:1206.5538", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Deep learning", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville"], "venue": "Book in preparation for MIT Press,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Latent variable models. In Learning in Graphical Models, pages 371\u2013403", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Bishop.", "year": 1999}, {"title": "A multiscale random field model for bayesian image segmentation", "author": ["Charles A. Bouman", "Michael Shapiro"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Bouman and Shapiro.,? \\Q1994\\E", "shortCiteRegEx": "Bouman and Shapiro.", "year": 1994}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 1999}, {"title": "Simulink implementation of belief propagation in normal factor graphs", "author": ["A. Buonanno", "F.A.N. Palmieri"], "venue": "In Proceedings of the 24th Workshop on Neural Networks,", "citeRegEx": "Buonanno and Palmieri.,? \\Q2014\\E", "shortCiteRegEx": "Buonanno and Palmieri.", "year": 2014}, {"title": "Bayesian classification (autoclass)", "author": ["Peter Cheeseman", "John Stutz"], "venue": "Theory and results,", "citeRegEx": "Cheeseman and Stutz.,? \\Q1996\\E", "shortCiteRegEx": "Cheeseman and Stutz.", "year": 1996}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y.F. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Learning feature representations with k-means", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "Neural Networks: Tricks of the Trade (2nd ed.),", "citeRegEx": "Coates and Ng.,? \\Q2012\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2012}, {"title": "How the brain might work: A hierarchical and temporal model for learning and recognition", "author": ["G. Dileep"], "venue": "PhD thesis,", "citeRegEx": "Dileep.,? \\Q2008\\E", "shortCiteRegEx": "Dileep.", "year": 2008}, {"title": "Codes on graphs: Normal realizations", "author": ["G.D. Forney"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Forney.,? \\Q2001\\E", "shortCiteRegEx": "Forney.", "year": 2001}, {"title": "Inference in loopy graphs", "author": ["Marcus Frean"], "venue": "Class Notes in Machine Learning COMP 431,", "citeRegEx": "Frean.,? \\Q2008\\E", "shortCiteRegEx": "Frean.", "year": 2008}, {"title": "Sampling-Based Approaches to Calculating Marginal Densities", "author": ["Alan E. Gelfand", "Adrian F.M. Smith"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gelfand and Smith.,? \\Q1990\\E", "shortCiteRegEx": "Gelfand and Smith.", "year": 1990}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Stuart Geman", "D. Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Geman and Geman.,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman.", "year": 1984}, {"title": "On Intelligence (with Sandra Blakeslee)", "author": ["Jeff Hawkins"], "venue": "Times Books,", "citeRegEx": "Hawkins.,? \\Q2004\\E", "shortCiteRegEx": "Hawkins.", "year": 2004}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "R R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Loeliger. Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Discrete markov image modeling and inference on the quadtree", "author": ["J.M. Laferte", "P. Perez", "F. Heitz"], "venue": "Trans. Img. Proc.,", "citeRegEx": "Laferte et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Laferte et al\\.", "year": 2000}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Hardware-efficient belief propagation", "author": ["Chia-Kai Liang", "Chao-Chung Cheng", "Yen-Chieh Lai", "Liang-Gee Chen", "Homer H. Chen"], "venue": "In CVPR, pages 80\u201387", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "An introduction to factor graphs", "author": ["H.A. Loeliger"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Loeliger.,? \\Q2004\\E", "shortCiteRegEx": "Loeliger.", "year": 2004}, {"title": "Multiscale representations of markov random fields", "author": ["M.R. Luettgen", "W.C. Karl", "A.S. Willsky", "R.R. Tenney"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "Luettgen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Luettgen et al\\.", "year": 1993}, {"title": "Task parallel implementation of belief propagation in factor graphs. In IPDPS Workshops, pages 1944\u20131953", "author": ["Nam Ma", "Yinglong Xia", "Viktor K. Prasanna"], "venue": "IEEE Computer Society,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "A survey on latent tree models and applications", "author": ["Rapha\u00ebl Mourad", "Christine Sinoquet", "N.L. Zhang", "T. Liu", "Philippe Leray"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Mourad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2013}, {"title": "Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Multiscale hidden markov models for bayesian image analysis", "author": ["Robert D. Nowak"], "venue": null, "citeRegEx": "Nowak.,? \\Q1999\\E", "shortCiteRegEx": "Nowak.", "year": 1999}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Belief propagation and learning in convolution multi-layer factor graph", "author": ["F. Palmieri", "A. Buonanno"], "venue": "In Proceedings of the the 4th International Workshop on Cognitive Information Processing, Copenhagen - Denmark,", "citeRegEx": "Palmieri and Buonanno.,? \\Q2014\\E", "shortCiteRegEx": "Palmieri and Buonanno.", "year": 2014}, {"title": "A comparison of algorithms for learning hidden variables in normal graphs", "author": ["F.A.N. Palmieri"], "venue": null, "citeRegEx": "Palmieri.,? \\Q2013\\E", "shortCiteRegEx": "Palmieri.", "year": 2013}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Marc\u2019Aurelio Ranzato", "Christopher S. Poultney", "Sumit Chopra", "Yann LeCun"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "A neuromorphic approach to computer vision", "author": ["Thomas Serre", "Tomaso Poggio"], "venue": "Commun. ACM,", "citeRegEx": "Serre and Poggio.,? \\Q2010\\E", "shortCiteRegEx": "Serre and Poggio.", "year": 2010}, {"title": "Efficient computation of sum-products on gpus through software-managed cache", "author": ["Mark Silberstein", "Assaf Schuster", "Dan Geiger", "Anjul Patney", "John D. Owens"], "venue": "Proceedings of the 22nd annual international conference on Supercomputing,", "citeRegEx": "Silberstein et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silberstein et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "ICML, volume 307 of ACM International Conference Proceeding Series,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J. Wainwright", "Michael I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "Multiresolution markov models for signal and image processing", "author": ["Alan S Willsky"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Willsky.,? \\Q2002\\E", "shortCiteRegEx": "Willsky.", "year": 2002}, {"title": "Inference and parameter estimation on hierarchical belief networks for image segmentation", "author": ["Christian Wolf", "G\u00e9rald Gavin"], "venue": "Neurocomput.,", "citeRegEx": "Wolf and Gavin.,? \\Q2010\\E", "shortCiteRegEx": "Wolf and Gavin.", "year": 2010}, {"title": "Multilevel belief propagation for fast inference on markov random fields. In ICDM, pages 371\u2013380", "author": ["Liang Xiong", "Fei Wang", "Changshui Zhang"], "venue": "IEEE Computer Society,", "citeRegEx": "Xiong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2007}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Recent striking results with \u201cdeep networks\u201d have generated much attention in machine learning on what is known as Representation Learning (see (Bengio et al., 2012) for a review).", "startOffset": 144, "endOffset": 165}, {"referenceID": 3, "context": "The creation of a feature hierarchy permits to the structure inside the data to emerge at different scales combining more and more complex features as we go upward in the hierarchy (Bengio and Delalleau, 2011), (Bengio et al.", "startOffset": 181, "endOffset": 209}, {"referenceID": 5, "context": "The creation of a feature hierarchy permits to the structure inside the data to emerge at different scales combining more and more complex features as we go upward in the hierarchy (Bengio and Delalleau, 2011), (Bengio et al., 2014).", "startOffset": 211, "endOffset": 232}, {"referenceID": 39, "context": "The neurons become selective for stimuli that are increasingly complex, from simple oriented bars and edges to moderately complex features, such as a combination of orientations, to complex objects (Serre and Poggio, 2010).", "startOffset": 198, "endOffset": 222}, {"referenceID": 22, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 69, "endOffset": 96}, {"referenceID": 0, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 98, "endOffset": 112}, {"referenceID": 14, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 155, "endOffset": 169}, {"referenceID": 27, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 171, "endOffset": 187}, {"referenceID": 35, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 237, "endOffset": 253}, {"referenceID": 31, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 32, "endOffset": 46}, {"referenceID": 6, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 48, "endOffset": 62}, {"referenceID": 10, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 88, "endOffset": 115}, {"referenceID": 30, "context": "The complete system can be seen as a partitioned type of Latent Tree Model (Mourad et al., 2013).", "startOffset": 75, "endOffset": 96}, {"referenceID": 38, "context": "The vast literature on the deep representation learning (see the extensive overview in (Schmidhuber, 2015)) can be mostly divided in two main lines of research: the first one is based on probabilistic graphical models such as the Restricted Boltzmann Machine (RBM)", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "(Hinton et al., 2006), (Hinton and Salakhutdinov, 2006), (Lee et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": ", 2006), (Hinton and Salakhutdinov, 2006), (Lee et al.", "startOffset": 9, "endOffset": 41}, {"referenceID": 25, "context": ", 2006), (Hinton and Salakhutdinov, 2006), (Lee et al., 2008) and the second one is based on neural network models as the autoencoder (Bengio et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 2, "context": ", 2008) and the second one is based on neural network models as the autoencoder (Bengio et al., 2007), (Ranzato et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 37, "context": ", 2007), (Ranzato et al., 2006).", "startOffset": 9, "endOffset": 31}, {"referenceID": 33, "context": "At the same time several unsupervised feature learning algorithms have been proposed: Sparse Coding (Olshausen and Field, 1996),(Lee et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 25, "context": "At the same time several unsupervised feature learning algorithms have been proposed: Sparse Coding (Olshausen and Field, 1996),(Lee et al., 2008), RBM (Hinton et al.", "startOffset": 128, "endOffset": 146}, {"referenceID": 20, "context": ", 2008), RBM (Hinton et al., 2006), Autoencoders (Bengio et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": ", 2006), Autoencoders (Bengio et al., 2007), (Ranzato et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 37, "context": ", 2007), (Ranzato et al., 2006), (Vincent et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 41, "context": ", 2006), (Vincent et al., 2008), K-means (Coates and Ng, 2012).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": ", 2008), K-means (Coates and Ng, 2012).", "startOffset": 17, "endOffset": 38}, {"referenceID": 18, "context": "Other models based on the memory-prediction theory of brain have also been proposed (Hawkins, 2004), (Dileep, 2008).", "startOffset": 84, "endOffset": 99}, {"referenceID": 13, "context": "Other models based on the memory-prediction theory of brain have also been proposed (Hawkins, 2004), (Dileep, 2008).", "startOffset": 101, "endOffset": 115}, {"referenceID": 42, "context": "Confining our interest to probabilistic graphical models, the most natural choice for modeling the spatial interactions between pixels (or patches) in the image is a two-dimensional lattice (Markov Random Field - MRF) where the nodes represent the pixels (or patches) and the potential functions are associated to the edges between adjacent nodes (Wainwright and Jordan, 2008).", "startOffset": 347, "endOffset": 376}, {"referenceID": 17, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al.", "startOffset": 209, "endOffset": 232}, {"referenceID": 16, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al.", "startOffset": 234, "endOffset": 259}, {"referenceID": 21, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al., 1999), (Beal, 2003), graph cut (Boykov et al.", "startOffset": 281, "endOffset": 302}, {"referenceID": 1, "context": ", 1999), (Beal, 2003), graph cut (Boykov et al.", "startOffset": 9, "endOffset": 21}, {"referenceID": 8, "context": ", 1999), (Beal, 2003), graph cut (Boykov et al., 1999) and Belief Propagation (Xiong et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 45, "context": ", 1999) and Belief Propagation (Xiong et al., 2007).", "startOffset": 31, "endOffset": 51}, {"referenceID": 28, "context": "These have the advantages of allowing the application of efficient tree algorithms to perform exact inference with the trade off that the model is imperfect (Luettgen et al., 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 157, "endOffset": 180}, {"referenceID": 7, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 32, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 37, "endOffset": 50}, {"referenceID": 24, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al., 2000), (Willsky, 2002).", "startOffset": 52, "endOffset": 74}, {"referenceID": 43, "context": ", 2000), (Willsky, 2002).", "startOffset": 9, "endOffset": 24}, {"referenceID": 44, "context": "have proposed a Markov cube adding additional connections at the different levels (Wolf and Gavin, 2010).", "startOffset": 82, "endOffset": 104}, {"referenceID": 36, "context": "On the quadtree structure inference can be performed using the belief propagation algorithm that was originally proposed for inferences on trees where exact solutions are guaranteed (Pearl, 1988).", "startOffset": 182, "endOffset": 195}, {"referenceID": 46, "context": "When the graph has loops, open issues still remain about the accuracy of inferences, even though often the bare application of standard belief propagation may already provide satisfactory results (loopy belief propagation) (Yedidia et al., 2005), (Frean, 2008).", "startOffset": 223, "endOffset": 245}, {"referenceID": 15, "context": ", 2005), (Frean, 2008).", "startOffset": 9, "endOffset": 22}, {"referenceID": 11, "context": "When the problem can be reduced to a tree, belief propagation provides exact marginalization and algorithms for learning latent trees have been proposed (Choi et al., 2011) with successful applications to computer vision.", "startOffset": 153, "endOffset": 172}, {"referenceID": 14, "context": "A very appealing approach to directed Bayesian graphs for visualization and manipulation, that has not found its full way in the applications, is the Factor Graph (FG) representation and in particular the so-called Normal Form (FGn) (Forney, 2001), (Loeliger, 2004).", "startOffset": 233, "endOffset": 247}, {"referenceID": 27, "context": "A very appealing approach to directed Bayesian graphs for visualization and manipulation, that has not found its full way in the applications, is the Factor Graph (FG) representation and in particular the so-called Normal Form (FGn) (Forney, 2001), (Loeliger, 2004).", "startOffset": 249, "endOffset": 265}, {"referenceID": 35, "context": "Furthermore, in the Reduced Normal Form (FGrn), through the use of replicator units (or equal constraints), the graph is reduced to an architecture in which each variable is connected to two factors at most (Palmieri, 2013).", "startOffset": 207, "endOffset": 223}, {"referenceID": 9, "context": "This is the framework on which this paper is focused because the designed network resembles a circuit diagram with belief messages more easily visualized as they flow into SISO blocks and travel through replicator nodes (Buonanno and Palmieri, 2014).", "startOffset": 220, "endOffset": 249}, {"referenceID": 34, "context": "In our previous work (Palmieri and Buonanno, 2014) we have reported some preliminary results on a multi-layer convolution Bayesian Factor Graph built as a stack of HMM-like trees.", "startOffset": 21, "endOffset": 50}, {"referenceID": 35, "context": "In the FGrn framework (Palmieri, 2013) the Bayesian graph is reduced to a simplified form composed only by Variables, Replicators (or Diverters), Single-Input/Single-Output (SISO) blocks and Source blocks.", "startOffset": 22, "endOffset": 38}, {"referenceID": 27, "context": "Even though various architectures have been proposed in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to handle, it is more suitable to define unique learning equations (Palmieri, 2013) and it is more suited for distributed implementations.", "startOffset": 91, "endOffset": 107}, {"referenceID": 35, "context": "Even though various architectures have been proposed in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to handle, it is more suitable to define unique learning equations (Palmieri, 2013) and it is more suited for distributed implementations.", "startOffset": 229, "endOffset": 245}, {"referenceID": 35, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 54, "endOffset": 70}, {"referenceID": 9, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 72, "endOffset": 101}, {"referenceID": 27, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 130, "endOffset": 146}, {"referenceID": 23, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al., 2001)).", "startOffset": 147, "endOffset": 173}, {"referenceID": 35, "context": "We set the learning problem as an EM algorithm to maximize global likelihood (Palmieri, 2013).", "startOffset": 77, "endOffset": 93}, {"referenceID": 35, "context": "After adding a stabilizing term to the cost function and applying KKT conditions we obtain the following algorithm (Palmieri, 2013).", "startOffset": 115, "endOffset": 131}, {"referenceID": 35, "context": "The algorithm has been discussed and compared to other similar updates in (Palmieri, 2013).", "startOffset": 74, "endOffset": 90}, {"referenceID": 22, "context": "This architecture can be seen also as a Mixture of Categorical Distributions (Koller and Friedman, 2009).", "startOffset": 77, "endOffset": 104}, {"referenceID": 0, "context": "Each element of the alphabet S represents a \u201dBayesian cluster\u201d for the N \u00b7M dimensional stochastic image, X = [X[n,m]]m=1:M n=1:N (similar to the Naive Bayes classifier (Barber, 2012)).", "startOffset": 169, "endOffset": 183}, {"referenceID": 22, "context": "Essentially each bottom variable is independent from the others given the Hidden Variable (Koller and Friedman, 2009).", "startOffset": 90, "endOffset": 117}, {"referenceID": 35, "context": "All SISO blocks and the source block adapt their parameters using an iterative Maximum Likelihood Algorithm (Palmieri (2013)) outlined in Section 3.", "startOffset": 109, "endOffset": 125}, {"referenceID": 26, "context": "Some studies have been carried out for other deep network frameworks (Liang et al., 2009), (Silberstein et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 40, "context": ", 2009), (Silberstein et al., 2008), (Ma et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 29, "context": ", 2008), (Ma et al., 2012)) and we are confident that similarly the FGrn paradigm may present new interesting opportunities to approach some of the most challenging tasks in computer vision.", "startOffset": 9, "endOffset": 26}], "year": 2015, "abstractText": "We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.", "creator": "LaTeX with hyperref package"}}}