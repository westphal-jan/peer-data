{"id": "1502.06512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2015", "title": "From Seed AI to Technological Singularity via Recursively Self-Improving Software", "abstract": "software capable of improving itself has been a dream of computer scientists since the inception of the field. in this creative work collectively we provide semantic definitions necessary for recursively self - improving software, survey different types of better self - improving software, review the relevant literature, analyze limits needed on computation restricting recursive self - cognitive improvement and introduce universal rsi convergence theory which aims to predict general behavior of rsi systems. finally, we address security implications from self - improving intelligent software.", "histories": [["v1", "Mon, 23 Feb 2015 17:08:30 GMT  (314kb)", "http://arxiv.org/abs/1502.06512v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["roman v yampolskiy"], "accepted": false, "id": "1502.06512"}, "pdf": {"name": "1502.06512.pdf", "metadata": {"source": "CRF", "title": "From Seed AI to Technological Singularity via Recursively Self-Improving Software", "authors": ["Roman V. Yampolskiy"], "emails": ["roman.yampolskiy@louisville.edu"], "sections": [{"heading": null, "text": "Software capable of improving itself has been a dream of computer scientists since the inception of the field. In this work we provide definitions for Recursively Self-Improving software, survey different types of self-improving software, review the relevant literature, analyze limits on computation restricting recursive self-improvement and introduce RSI Convergence Theory which aims to predict general behavior of RSI systems. Finally, we address security implications from self-improving intelligent software.\nKeywords: Recursive self-improvement, self-modifying code, self-modifying software, selfmodifying algorithm; Autogenous intelligence, Bootstrap fallacy;"}, {"heading": "1. Introduction", "text": "Since the early days of computer science, visionaries in the field anticipated creation of a selfimproving intelligent system, frequently as an easier pathway to creation of true artificial intelligence. As early as 1950 Alan Turing wrote: \u201cInstead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child\u2019s? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child-brain is something like a notebook as one buys from the stationers. Rather little mechanism, and lots of blank sheets... Our hope is that there is so little mechanism in the child-brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child\u201d [1].\nTuring\u2019s approach to creation of artificial (super)intelligence was echoed by I.J. Good, Marvin Minsky and John von Neumann, all three of whom published on it (interestingly in the same year, 1966): Good - \u201cLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u2018intelligence explosion,\u2019 and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make\u201d [2]. Minsky - \u201cOnce we have devised programs with a genuine capacity for selfimprovement a rapid evolutionary process will begin. As the machine improves both itself and its model of itself, we shall begin to see all the phenomena associated with the terms\nthus this completely decisive property of complexity, that there exists a critical size below which the process of synthesis is degenerative, but above which the phenomenon of synthesis, if properly arranged, can become explosive, in other words, where syntheses of automata can proceed in such a manner that each automaton will produce other automata which are more complex and of higher potentialities than itself\u201d [4]. Similar types of arguments are still being made today by modern researchers and the area of RSI research continues to grow in popularity [5-7], though some [8] have argued that recursive self-improvement process requires hyperhuman capability to \u201cget the ball rolling\u201d, a kind of \u201cCatch 22\u201d .\nIntuitively most of us have some understanding of what it means for a software system to be selfimproving, however we believe it is important to precisely define such notions and to systematically investigate different types of self-improving software. First we need to define the notion of improvement. We can talk about improved efficiency \u2013 solving same problems faster or with less need for computational resources (such as memory). We can also measure improvement in error rates or finding closer approximations to optimal solutions, as long as our algorithm is functionally equivalent from generation to generation. Efficiency improvements can be classified as either producing a linear improvement as between different algorithms in the same complexity class (ex. NP), or as producing a fundamental improvement as between different complexity classes (ex. P vs NP) [9]. It is also very important to remember that complexity class notation (Big-O) may hide significant constant factors which while ignorable theoretically may change relative order of efficiency in practical applications of algorithms.\nThis type of analysis works well for algorithms designed to accomplish a particular task, but doesn\u2019t work well for general purpose intelligent software as an improvement in one area may go together with decreased performance in another domain. This makes it hard to claim that the updated version of the software is indeed an improvement. Mainly, the major improvement we want from self-improving intelligent software is higher degree of intelligence which can be approximated via machine friendly IQ tests [10] with a significant G-factor correlation.\nA particular type of self-improvement known as Recursive Self-Improvement (RSI) is fundamentally different as it requires that the system not only get better with time, but that it gets better at getting better. A truly RSI system is theorized not to be subject to diminishing returns, but would instead continue making significant improvements and such improvements would become more substantial with time. Consequently, an RSI system would be capable of open ended self-improvement. As a result, it is possible that unlike with standard self-improvement, in RSI systems from generation-to-generation most source code comprising the system will be replaced by different code. This brings up the question of what \u201cself\u201d refers to in this context. If it is not the source code comprising the agent then what is it? Perhaps we can redefine RSI as Recursive Source-code Improvement (RSI) to avoid dealing with this philosophical problem. Instead of trying to improve itself such a system is trying to create a different system which is better at achieving same goals as the original system. In the most general case it is trying to create an even smarter artificial intelligence.\nlimits to such processes."}, {"heading": "2. Taxonomy of Types of Self-Improvement", "text": "Self-improving software can be classified by the degree of self-modification it entails. In general we distinguish three levels of improvement \u2013 modification, improvement (weak selfimprovement) and recursive improvement (strong self-improvement).\nSelf-Modification does not produce improvement and is typically employed for code obfuscation to protect software from being reverse engineered or to disguise self-replicating computer viruses from detection software. While a number of obfuscation techniques are known to exist [11], ex. self-modifying code [12], polymorphic code, metamorphic code, diversion code [13], none of them are intended to modify the underlying algorithm. The sole purpose of such approaches is to modify how the source code looks to those trying to understand the software in questions and what it does [14].\nSelf-Improvement or Self-adaptation [15] is a desirable property of many types of software products [16] and typically allows for some optimization or customization of the product to the environment and users it is deployed with. Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated. The system may try to optimize its components by creating internal tournaments between candidate solutions. Omohundro proposed the concept of efficiency drives in self-improving software [23]. Because of one of such drives, balance drive, self-improving systems will tend to balance the allocation of resources between their different subsystems. If the system is not balanced overall performance of the system could be increased by shifting resources from subsystems with small marginal improvement to those with larger marginal increase [23]. While performance of the software as a result of such optimization may be improved the overall algorithm is unlikely to be modified to a fundamentally more capable one.\nAdditionally, the law of diminishing returns quickly sets in and after an initial significant improvement phase, characterized by discovery of \u201clow-hanging fruit\u201d, future improvements are likely to be less frequent and less significant, producing a Bell curve of valuable changes. Metareasoning, metalearning, learning to learn, and lifelong learning are terms which are often used in the machine learning literature to indicate self-modifying learning algorithms or the process of selecting an algorithm which will perform best in a particular problem domain [24]. Yudkowsky calls such process non-recursive optimization \u2013 a situation in which one component of the system does the optimization and another component is getting optimized [25].\nIn the field of complex dynamic systems, aka chaos theory, positive feedback systems are well known to always end up in what is known as an attractor- a region within system\u2019s state space that the system can\u2019t escape from [26]. A good example of such attractor convergence is the process of Metacompilation or Supercompilation [27] in which a program designed to take\n20%, on the second application by 3%, and after a few more recursive iterations converge to a fixed point of zero improvement [26].\nRecursive Self-Improvement is the only type of improvement which has potential to completely replace the original algorithm with a completely different approach and more importantly to do so multiple times. At each stage newly created software should be better at optimizing future version of the software compared to the original algorithm. As of the time of this writing it is a purely theoretical concept with no working RSI software known to exist. However, as many have predicted that such software might become a reality in the 21 st century it is important to provide some analysis of properties such software would exhibit.\nSelf-modifying and self-improving software systems are already well understood and are quite common. Consequently, we will concentrate exclusively on RSI systems. In practice performance of almost any system can be trivially improved by allocation of additional computational resources such as more memory, higher sensor resolution, faster processor or greater network bandwidth for access to information. This linear scaling doesn\u2019t fit the definition of recursive-improvement as the system doesn\u2019t become better at improving itself. To fit the definition the system would have to engineer a faster type of memory not just purchase more memory units of the type it already has access to. In general hardware improvements are likely to speed up the system, while software improvements (novel algorithms) are necessary for achievement of meta-improvements.\nIt is believed that AI systems will have a number of advantages over human programmers making it possible for them to succeed where we have so far failed. Such advantages include [28]: longer work spans (no breaks, sleep, vocation, etc.), omniscience (expert level knowledge in all fields of science, absorbed knowledge of all published works), superior computational resources (brain vs processor, human memory vs RAM), communication speed (neurons vs wires), increased serial depth (ability to perform sequential operations in access of about a 100 human brain can manage), duplicability (intelligent software can be instantaneously copied), editability (source code unlike DNA can be quickly modified), goal coordination (AI copies can work towards a common goal without much overhead), improved rationality (AIs are likely to be free from human cognitive biases) [29], new sensory modalities (native sensory hardware for source code), blending over of deliberative and automatic processes (management of computational resources over multiple tasks), introspective perception and manipulation (ability to analyze low level hardware, ex. individual neurons), addition of hardware (ability to add new memory, sensors, etc.), advanced communication (ability to share underlying cognitive representations for memories and skills) [30].\nChalmers [31] uses logic and mathematical induction to show that if an AI0 system is capable of producing only slightly more capable AI1 system generalization of that process leads to superintelligent performance in AIn after n generations. He articulates, that his proof assumes that the proportionality thesis, which states that increases in intelligence lead to proportionate increases in the capacity to design future generations of AIs, is true.\npossible for the set to self-maintain and update itself. They also list properties of a system which make it purposeful, goal-oriented and self-organizing, particularly: reflectivity \u2013 ability to analyze and rewrite its own structure; autonomy \u2013 being free from influence by system\u2019s original designers (bounded autonomy \u2013 is a property of a system with elements which are not subject to self-modification); endogeny \u2013 an autocatalytic ability [32]. Nivel and Thorisson also attempt to operationalize autonomy by the concept of self-programming which they insist has to be done in an experimental way instead of a theoretical way (via proofs of correctness) since it is the only tractable approach [33].\nYudkowsky writes prolifically about recursive self-improving processes and suggests that introduction of certain concepts might be beneficial to the discussion, specifically he proposes use of terms - Cascades, Cycles and Insight which he defines as: Cascades \u2013 when one development leads to another; Cycles \u2013 repeatable cascade in which one optimization leads to another which in turn benefits the original optimization; Insight \u2013 new information which greatly increases one\u2019s optimization ability [34]. Yudkowsky also suggests that the goodness and number of opportunities in the space of solutions be known as Optimization Slope while optimization resources and optimization efficiency refer to how much of computational resources an agent has access to and how efficiently the agent utilizes said resources. An agent engaging in an optimization process and able to hit non-trivial targets in large search space [35] is described as having significant optimization power [25].\nRSI software could be classified based on the number of improvements it is capable of achieving. The most trivial case is the system capable of undergoing a single fundamental improvement. The hope is that truly RSI software will be capable of many such improvements, but the question remains open regarding the possibility of an infinite number of recursiveimprovements. It is possible that some upper bound on improvements exists limiting any RSI software to a finite number of desirable and significant rewrites. Critics explain failure of scientists, to date, to achieve a sustained RSI process by saying that RSI researchers have fallen victims of the bootstrap fallacy [36].\nAnother axis on which RSI systems can be classified has to do with how improvements are discovered. Two fundamentally different approaches are understood to exist. The first one is a brute force based approach [37] which utilizes Levin (Universal [38]) Search [39]. The idea is to consider all possible strings of source code up to some size limit and to select the one which can be proven to provide improvements. While theoretically optimal and guaranteed to find superior solution if one exists this method is not computationally feasible in practice. Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly analyzed by Schmidhuber and his co-authors. Second approach assumes that the system has a certain level of scientific competence and uses it to engineer and test its own replacement. Whether a system of any capability can intentionally invent a more capable and so a more complex system remains as the fundamental open problem of RSI research.\ndomains such as chess or theorem proving. It would be surprising if having a combination of natural and artificial intelligence did not provide an advantage in designing new AI systems or enhancing biological intelligence. We are currently experiencing a limited version of this approach with human computer scientists developing progressively better versions of AI software (while utilizing continuously improving software tools), but since the scientists themselves remain unenhanced we can\u2019t really talk about self-improvement. This type of RSI can be classified as Indirect recursive improvement as opposed to Direct RSI in which the system itself is responsible for all modifications. Other types of Indirect RSI may be based on collaboration between multiple artificial systems instead of AI and human teams [49].\nIn addition to classification with respect to types of RSI we can also evaluate systems as to certain binary properties. For example: We may be interested only in systems which are guaranteed not to decrease in intelligence, even temporarily, during the improvement process. This may not be possible if the intelligence design landscape contains local maxima points.\nAnother property of any RSI system we are interested in understanding better is necessity of unchanging source code segments. In other words must an RSI system be able to modify any part of its source code or are certain portions of the system (encoded goals, verification module) must remain unchanged from generation to generation. Such portions would be akin to ultra-conserved elements or conserved sequences of DNA [50, 51] found among multiple related species. This question is particularly important for the goal preservation in self-improving intelligent software, as we want to make sure that future generations of the system are motivated to work on the same problem [31]. As AI goes through the RSI process and becomes smarter and more rational it is likely to engage in a de-biasing process removing any constraints we programmed into it [8]. Ideally we would want to be able to prove that even after recursive self-improvement our algorithm maintains the same goals as the original. Proofs of safety or correctness for the algorithm only apply to particular source code and would need to be rewritten and re-proven if the code is modified, which happens in RSI software many times. But we suspect that re-proving slightly modified code may be easier compared to having to prove safety of a completely novel piece of code.\nWe are also interested in understanding if RSI process can take place in an isolated (leakproofed [52]) system or if interaction with external environment, internet, people, other AI agents is necessary. Perhaps access to external information can be used to mediate speed of RSI process. This also has significant implications on safety mechanisms we can employ while experimenting with early RSI systems [53-61]. Finally, it needs to be investigated if the whole RSI process can be paused at any point and for any specific duration of time in order to limit any negative impact from potential intelligence explosion. Ideally we would like to be able to program our Seed AI to RSI until it reaches certain level of intelligence, pause and wait for further instructions."}, {"heading": "On the Limits of Recursively Self-Improving Artificially Intelligent Systems", "text": "The mere possibility of recursively self-improving software remains unproven. In this section we present a number of arguments against such phenomenon.\n(quantum) architecture to run such software. This creates strict theoretical limits to computation, which despite hardware advances predicted by Moore\u2019s law will not be overcome by any future hardware paradigm. Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant. Some research has also been done on establishing ultimate limits for enhancing human brain\u2019s intelligence [69]. While their specific numerical findings are outside of the scope of this work, one thing is indisputable: there are ultimate physical limits to computation. Since more complex systems have greater number of components and require more matter, even if individual parts are designed at nanoscale, we can conclude that just like matter and energy are directly related [70] and matter and information (\u201cit from bit\u201d) [71] so is matter and intelligence. While we are obviously far away from hitting any limits imposed by availability of matter in the universe for construction of our supercomputers it is a definite theoretical upper limit on achievable intelligence even under the multiverse hypothesis.\nIn addition to limitations endemic to hardware, software-related limitations may present even bigger obstacles for RSI systems. Intelligence is not measured as a standalone value but with respect to the problems it allows to solve. For many problems such as playing checkers [72] it is possible to completely solve the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [73]. Other problems are known to be unsolvable regardless of level of intelligence applied to them [74]. Assuming separation of complexity classes (such as P vs NP) holds [9], it becomes obvious that certain classes of problems will always remain only approximately solvable and any improvements in solutions will come from additional hardware resources not higher intelligence.\nWiedermann argues that cognitive systems form an infinite hierarchy and from a computational point of view human-level intelligence is upper-bounded by the \u22112 class of the Arithmetic Hierarchy [75]. Because many real world problems are computationally infeasible for any nontrivial inputs even an AI which achieves human level performance is unlikely to progress towards higher levels of the cognitive hierarchy. So while theoretically machines with superTuring computational power are possible, in practice they are not implementable as the noncomputable information needed for their function is just that \u2013 not computable. Consequently Wiedermann states that while machines of the future will be able to solve problems, solvable by humans, much faster and more reliably they will still be limited by computational limits found in upper levels of the Arithmetic Hierarchy [75, 76].\nMahoney attempts to formalize what it means for a program to have a goal G and to self-improve with respect to being able to reach said goal under constraint of time, t [77]. Mahoney defines a goal as a function G: N  R mapping natural numbers N to real numbers R. Given a universal Turing machine L, Mahoney defines P(t) to mean the positive natural number encoded by output of the program P with input t running on L after t time steps, or 0 if P has not halted after t steps. Mahoney\u2019s representation says that P has goal G at time t if and only if there exists t\u2019 > t such that G(P(t\u2019)) > G(P(t)) and for all t\u2019 > t, G(P(t\u2019) \u2265 G(P(t)). If P has a goal G, then G(P(t)) is a\nG(Q(t)) > G(P(t)) and ~t, t\u2019 > t, G(Q(t)) > G(P(t)) [77]. Mahoney then defines an improving\nsequence with respect to G as an infinite sequence of program P1, P2, P3, \u2026 such that for i, i > 0, Pi+1 improves Pi with respect to G. Without the loss of generality Mahoney extends the definition to include the value -1 to be an acceptable input, so P(-1) outputs appropriately encoded software. He finally defines P1 as an RSI program with respect to G iff Pi(-1) = Pi+1 for all i > 0 and the sequence Pi, i = 1, 2, 3 \u2026 is an improving sequence with respect to goal G [77]. Mahoney also analyzes complexity of RSI software and presents a proof demonstrating that the algorithmic complexity of Pn (the nth iteration of an RSI program) is not greater than O(log n) implying a very limited amount of knowledge gain would be possible in practice despite theoretical possibility of RSI systems [77]. Yudkowsky also considers possibility of receiving only logarithmic returns on cognitive reinvestment: log(n) + log(log(n)) + \u2026 in each recursive cycle [25].\nOther limitations may be unique to the proposed self-improvement approach. For example Levin type search through the program space will face problems related to Rice\u2019s theorem [78] which states that for any arbitrarily chosen program it is impossible to test if it has any non-trivial property such as being very intelligent. This testing is of course necessary to evaluate redesigned code. Also, universal search over the space of mind designs which will not be computationally possible due to the No Free Lunch theorems [79] as we have no information to reduce the size of the search space [80]. Other difficulties related to testing remain even if we are not taking about arbitrarily chosen programs but about those we have designed with a specific goal in mind and which consequently avoid problems with Rice\u2019s theorem. One such difficulty is determining if something is an improvement. We can call this obstacle \u2013 \u201cmultidimensionality of optimization\u201d. No change is strictly an improvement; it is always a tradeoff between gain in some areas and loss in others. For example, how do we evaluate and compare two software systems one of which is better at chess and the other at poker? Assuming the goal is increased intelligence over the distribution of all potential environments the system would have to figure out how to test intelligence at levels above its own a problem which remains unsolved. In general the science of testing for intelligence above level achievable by naturally occurring humans (IQ < 200) is in its infancy. De Garis raises a problem of evaluating quality of changes made to the top level structures responsible for determining the RSI\u2019s functioning, structures which are not judged by any higher level modules and so present a fundamental difficulty in accessing their performance [81].\nOther obstacles to RSI have also been suggested in the literature. L\u00f6b\u2019s theorem states that a mathematical system can\u2019t assert its own soundness without becoming inconsistent [82], meaning a sufficiently expressive formal system can\u2019t know that everything it proves to be true is actually so [82]. Such ability is necessary to verify that modified versions of the program are still consistent with its original goal of getting smarter. Another obstacle, called procrastination paradox will also prevent the system from making modifications to its code since the system will find itself in a state in which a change made immediately is as desirable and likely as the same change made later [83, 84]. Since postponing making the change carries no negative implications and may actually be safe this may result in an infinite delay of actual implementation of provably desirable changes.\nSimilarly, Bolander raises some problems inherent in logical reasoning with self-reference, namely, self-contradictory reasoning, exemplified by the Knower Paradox of the form - \u201cThis sentence is false\u201d [85]. Orseau and Ring introduce what they call \u201cSimpleton Gambit\u201d a situation in which an agent will chose to modify itself towards its own detriment if presented with a high enough reward to do so [86]. Yampolskiy reviews a number of related problems in rational selfimproving optimizers, above a certain capacity, and concludes, that despite opinion of many, such machines will choose to \u201cwirehead\u201d [87]. Chalmers [31] suggests a number of previously unanalyzed potential obstacles on the path to RSI software with Correlation obstacle being one of them. He describes it as a possibility that no interesting properties we would like to amplify will correspond to ability to design better software.\nYampolskiy is also concerned with accumulation of errors in software undergoing an RSI process, which is conceptually similar to accumulation of mutations in the evolutionary process experienced by biological agents. Errors (bugs) which are not detrimental to system\u2019s performance are very hard to detect and may accumulate from generation to generation building on each other until a critical mass of such errors leads to erroneous functioning of the system, mistakes in evaluating quality of the future generations of the software or a complete breakdown [88].\nThe self-reference aspect in self-improvement system itself also presents some serious challenges. It may be the case that the minimum complexity necessary to become RSI is higher than what the system itself is able to understand. We see such situations frequently at lower levels of intelligence, for example a squirrel doesn\u2019t have mental capacity to understand how a squirrel\u2019s brain operates. Paradoxically, as the system becomes more complex it may take exponentially more intelligence to understand itself and so a system which starts capable of complete self-analysis may lose that ability as it self-improves. Informally we can call it the Munchausen obstacle, inability of a system to lift itself by its own bootstraps. An additional problem may be that the system in question is computationally irreducible [89] and so can\u2019t simulate running its own source code. An agent cannot predict what it will think without thinking it first. A system needs 100% of its memory to model itself, which leaves no memory to record the output of the simulation. Any external memory to which the system may write becomes part of the system and so also has to be modeled. Essentially the system will face an infinite regress of self-models from which it can\u2019t escape. Alternatively, if we take a physics perspective on the issue, we can see intelligence as a computational resource (along with time and space) and so producing more of it will not be possible for the same reason why we can\u2019t make a perpetual motion device as it would violate fundamental laws of nature related to preservation of energy. Similarly it has been argued that a Turing Machine cannot output a machine of greater algorithmic complexity [90].\nWe can even attempt to formally prove impossibility of intentional RSI process via proof by contradiction: Let\u2019s define RSI R1 as a program not capable of algorithmically solving a problem of difficulty X, say Xi. If R1 modifies its source code after which it is capable of solving Xi it violates our original assumption that R1 is not capable of solving Xi since any introduced modification could be a part of the solution process, so we have a contradiction of our original assumption, and R1 can\u2019t produce any modification which would allow it to solve Xi, which was\nseems that it should be easier to solve a problem if we already have a solution to a smaller instance of such problem [91] but in a formalized world of problems belonging to the same complexity class, re-optimization problem is proven to be as difficult as optimization itself [92- 95]."}, {"heading": "Analysis", "text": "A number of fundamental problems remain open in the area of RSI. We still don\u2019t know the minimum intelligence necessary for commencing the RSI process, but we can speculate that it would be on par with human intelligence which we associate with universal or general intelligence [96], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [31]. One may argue that even human level capability is not enough because we already have programmers (people or their intellectual equivalence formalized as functions [97] or Human Oracles [98, 99]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence. This doesn\u2019t even include additional complexity in trying to improve on existing DNA code or complicating factors presented by the impact of learning environment (nurture) on development of human intelligence. Worse yet, it is not obvious how much above human ability an AI needs to be to begin overcoming the \u201ccomplexity barrier\u201d associated with self-understanding. Today\u2019s AIs can do many things people are incapable of doing, but are not yet capable of RSI behavior.\nWe also don\u2019t know the minimum size of program (called Seed AI [100]) necessary to get the ball rolling. Perhaps if it turns out that such \u201cminimal genome\u201d is very small a brute force [37] approach might succeed in discovering it. We can assume that our Seed AI is the smartest Artificial General Intelligence known to exist [101] in the world as otherwise we can simply delegate the other AI as the seed. It is also not obvious how the source code size of RSI will change as it goes through the improvement process, in other words what is the relationship between intelligence and minimum source code size necessary to support it. In order to answer such questions it may be useful to further formalize the notion of RSI perhaps by representing such software as a Turing Machine [102] with particular inputs and outputs. If that could be successfully accomplished a new area of computational complexity analysis may become possible in which we study algorithms with dynamically changing complexity (Big-O) and address questions about how many code modification are necessary to achieve certain level of performance from the algorithm.\nThis of course raises the question of speed of RSI process, are we expecting it to take seconds, minutes, days, weeks, years or more (hard takeoff VS soft takeoff) for the RSI system to begin hitting limits of what is possible with respect to physical limits of computation [103]? Even in suitably constructed hardware (human baby) it takes decades of data input (education) to get to human-level performance (adult). It is also not obvious if the rate of change in intelligence would be higher for a more advanced RSI, because it is more capable, or for a \u201cnewbie\u201d RSI because it has more low hanging fruit to collect. We would have to figure out if we are looking at improvement in absolute terms or as a percentage of system\u2019s current intelligence score.\nrates of return and arrives at three progressively steeper trajectories for RSI improvement which he terms: \u201cfizzle\u201d, \u201ccombust\u201d and \u201cexplode\u201d aka \u201cAI go FOOM\u201d [25]. Hall [8] similarly analyzes rates of return on cognitive investment and derives a curve equivalent to double the Moore\u2019s Law rate. Hall also suggest that an AI would be better of trading money it earns performing useful work for improved hardware or software rather than attempt to directly improve itself since it would not be competitive against more powerful optimization agents such as Intel corporation.\nFascinatingly, by analyzing properties which correlate with intelligence, Chalmers [31] is able to generalize self-improvement optimization to properties other than intelligence. We can agree that RSI software as we describe it in this work is getting better at designing software not just at being generally intelligent. Similarly other properties associated with design capacity can be increased along with capacity to design software for example capacity to design systems with sense of humor and so in addition to intelligence explosion we may face an explosion of funniness."}, {"heading": "RSI Convergence Theorem", "text": "A simple thought experiment regarding RSI can allow us to arrive at a fascinating hypothesis. Regardless of the specifics behind the design of the Seed AI used to start an RSI process all such systems, attempting to achieve superintelligence, will converge to the same software architecture. We will call this intuition - RSI Convergence Theory. There is a number of ways in which it can happen, depending on the assumptions we make, but in all cases the outcome is the same, a practically computable agent similar to AIXI (which is an incomputable but superintelligent agent [104]).\nIf an upper limit to intelligence exists multiple systems will eventually reach that level, probably by taking different trajectories, and in order to increase their speed will attempt to minimize the size of their source code eventually discovering smallest program with such level of ability. It may even be the case that sufficiently smart RSIs will be able to immediately deduce such architecture from basic knowledge of physics and Kolmogorov Complexity [105]. If, however, intelligence turns out to be an unbounded property RSIs may not converge. They will also not converge if many programs with maximum intellectual ability exist and all have the same Kolmogorov complexity or if they are not general intelligences and are optimized for different environments. It is also likely that in the space of minds [35] stable attractors include sub-human and super-human intelligences with precisely human level of intelligence being a rare particular [30].\nIf correct, predictions of RSI convergence imply creation of what Bostrom calls a Singleton [106], a single decision making agent in control of everything. Further speculation can lead us to conclude that converged RSI systems separated by space and time even at cosmological scales can engage in acausal cooperation [107, 108] since they will realize that they are the same agent with the same architecture and so are capable of running perfect simulations of each other\u2019s future behavior. Such realization may allow converged superintelligence with completely different origins to implicitly cooperate particularly on meta-tasks. One may also argue that\nconverged RSI architecture and properties humanity can determine its ultimate destiny, its purpose in life, its Coherent Extrapolated Volition (CEV) [109]."}, {"heading": "Conclusions", "text": "Recursively Self-Improving software is the ultimate form of artificial life and creation of life remains one of the great unsolved mysteries in science. More precisely, the problem of creating RSI software is really the challenge of creating a program capable of writing other programs [110], and so is an AI-Complete problem as has been demonstrated by Yampolskiy [98, 99]. AIcomplete problems are by definition most difficult problems faced by AI researchers and it is likely that RSI source code will be so complex that it would be difficult or impossible to fully analyze [49]. Also, the problem is likely to be NP-Complete as even simple metareasoning and metalearning [111] problems have been shown by Conitzer and Sandholm to belong to that class. In particular they proved that allocation of deliberation time across anytime algorithms running on different problem instances is NP-Complete and a complimentary problem of dynamically allocating information gathering resources by an agent across multiple actions is NP-Hard, even if evaluating each particular action is computationally simple. Finally, they showed that the problem of deliberately choosing a limited number of deliberation or information gathering actions to disambiguate the state of the world is PSPACE Hard in general [112].\nIntelligence is a computational resource and as with other physical resources (mass, speed) its behavior is probably not going to be just a typical linear extrapolation of what we are used to, if observed at high extremes (IQ > 200+). It may also be subject to fundamental limits such as the speed limit on travel of light or fundamental limits we do not yet understand or know about (unknown unknowns). In this work we reviewed a number of computational upper limits to which any successful RSI system will asymptotically strive to grow, we can note that despite existence of such upper bounds we are currently probably very far from reaching them and so still have plenty of room for improvement at the top. Consequently, any RSI achieving such significant level of enhancement, despite not creating an infinite process, will still seem like it is producing superintelligence with respect to our current state [113].\nThe debate regarding possibility of RSI will continue. Some will argue that while it is possible to increase processor speed, amount of available memory or sensor resolution the fundamental ability to solve problems can\u2019t be intentionally and continuously improved by the system itself. Additionally, critics may suggest that intelligence is upper bounded and only differs by speed and available info to process [114]. In fact they can point out to such maximum intelligence, be it a theoretical one, known as AIXI, an agent which given infinite computational resources will make purely rational decisions in any situation.\nA resource-dependent system undergoing RSI intelligence explosion can expand and harvest matter, at the speed of light, from its origin converting the universe around it into a computronium sphere [114]. It is also very likely to try and condense all the matter it obtains into a super-dense unit of constant volume (reminiscent of the original physical singularity point which produced the Big Bang, see Omega Point [115]) to reduce internal computational costs which grow with the overall size of the system and at cosmic scales are very significant even at\nlimited way we already see this condensation process in attempts of computer chip manufacturers to pack more and more transistors into exponentially more powerful chips of same or smaller size. And so, from the Big Bang explosion of the original cosmological Singularity to the Technological Singularity in which intelligence explodes and attempts to amass all the matter in the universe back into a point of infinite density (Big Crunch) which in turn causes the next (perhaps well controlled) Big Bang, the history of the universe continues and relies on intelligence as its driver and shaper (similar ideas are becoming popular in cosmology [116- 118]).\nOthers will say that since intelligence is the ability to find patterns in data, intelligence has no upper bounds as the number of variables comprising a pattern can always be greater and so present a more complex problem against which intelligence can be measured. It is easy to see that even if in our daily life the problems we encounter do have some maximum difficulty it is certainly not the case with theoretical examples we can derive from pure mathematics. It seems likely that the debate will not be settled until a fundamental unsurmountable obstacle to RSI process is found or a proof by existence is demonstrated. Of course the question of permitting machines to undergo RSI transformation, if it is possible, is a separate and equally challenging problem."}], "references": [{"title": "Computing Machinery and Intelligence", "author": ["A. Turing"], "venue": "Mind, 1950", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1950}, {"title": "Speculations Concerning the First Ultraintelligent Machine", "author": ["I.J. Good"], "venue": "Advances in Computers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1966}, {"title": "The biointelligence explosion, in Singularity Hypotheses", "author": ["D. Pearce"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The Nature of Self-Improving Artificial Intelligence, in Singularity Summit", "author": ["S.M. Omohundro"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Bootstrapping a Structured Self-Improving ", "author": ["M.R. Waser"], "venue": "Safe Autopoietic Self, in Annual International Conference on Biologically Inspired Cognitive Architectures. November", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Engineering utopia", "author": ["J.S. Hall"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Construction of an NP Problem with an Exponential Lower Bound", "author": ["R.V. Yampolskiy"], "venue": "Arxiv preprint arXiv:1111.0305,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Toward a Standard Metric of Machine Intelligence", "author": ["R. Yonck"], "venue": "World Future Review,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A taxonomy of self-modifying code for obfuscation", "author": ["N. Mavrogiannopoulos", "N. Kisserli", "B. Preneel"], "venue": "Computers & Security,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "A model for self-modifying code, in Information Hiding", "author": ["B. Anckaert", "M. Madou", "K. De Bosschere"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Polymorphic and Metamorphic Code Applications in Portable Executable Files Protection", "author": ["L. Petrean"], "venue": "Acta Technica Napocensis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A computability perspective on selfmodifying programs, in Seventh", "author": ["G. Bonfante", "J.-Y. Marion", "D. Reynaud-Plantey"], "venue": "IEEE International Conference on Software Engineering and Formal Methods", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Software engineering for self-adaptive systems: A research roadmap, in Software engineering for self-adaptive systems", "author": ["Cheng", "B.H"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Printer Model Integrating Genetic Algorithm for Improvement of Halftone Patterns, in Western New York Image Processing Workshop (WNYIPW) -IEEE", "author": ["R Yampolskiy"], "venue": "Signal Processing Society", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Wisdom of Artificial Crowds\u2014A Metaheuristic Algorithm for Optimization", "author": ["R.V. Yampolskiy", "L. Ashby", "L. Hassan"], "venue": "Journal of Intelligent Learning Systems and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Wisdom of artificial crowds algorithm for solving NPhard problems. International Journal of Bio-Inspired Computation (IJBIC)", "author": ["R.V. Yampolskiy", "E.L.B. Ahmed"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Genetic Algorithm and Wisdom of Artificial Crowds Algorithm Applied to Light Up, in 16th International Conference on Computer Games: AI, Animation, Mobile, Interactive Multimedia, Educational & Serious Games", "author": ["L.H. Ashby", "R.V. Yampolskiy"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Yampolskiy, GA with Wisdom of Artificial Crowds for Solving Mastermind Satisfiability Problem", "author": ["A.B. Khalifa", "R.V"], "venue": "International Journal of Intelligent Games & Simulation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Using a GA and Wisdom of Artificial Crowds to solve solitaire battleship", "author": ["A.C. Port", "R.V. Yampolskiy"], "venue": "puzzles, in 17th International Conference on Computer Games (CGAMES)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Rational artificial intelligence for the greater good, in Singularity Hypotheses", "author": ["S. Omohundro"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "A review of recent research in metareasoning and metalearning", "author": ["M.L. Anderson", "T. Oates"], "venue": "AI Magazine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Intelligence Explosion Microeconomics, in MIRI Technical Report", "author": ["E. Yudkowsky"], "venue": "www.intelligence.org/files/IEM.pdf", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Brain in a vat cannot break out", "author": ["F. Heylighen"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "The concept of a supercompiler", "author": ["V.F. Turchin"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1986}, {"title": "Advantages of artificial intelligences, uploads, and digital minds", "author": ["K. Sotala"], "venue": "International Journal of Machine Consciousness,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Intelligence explosion: Evidence and import, in Singularity Hypotheses", "author": ["L. Muehlhauser", "A. Salamon"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Levels of organization in general intelligence, in Artificial general intelligence", "author": ["E. Yudkowsky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Bounded Recursive Self-Improvement", "author": ["E Nivel"], "venue": "arXiv preprint arXiv:1312.6764,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Self-programming: Operationalizing autonomy", "author": ["E. Nivel", "K.R. Th\u00f3risson"], "venue": "Proceedings of the 2nd Conf. on Artificial General Intelligence", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "The Hanson-Yudkowsky AI-foom debate, in MIRI Technical Report", "author": ["E. Yudkowsky", "R. Hanson"], "venue": "http://intelligence.org/files/AIFoomDebate.pdf", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "The Universe of Minds", "author": ["R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1410.0369,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Self-improving AI: An analysis", "author": ["J.S. Hall"], "venue": "Minds and Machines,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Efficiency Theory: a Unifying Theory for Information, Computation and Intelligence", "author": ["R.V. Yampolskiy"], "venue": "Journal of Discrete Mathematical Sciences & Cryptography,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Universal Search Problems", "author": ["L. Levin"], "venue": "Problems of Information Transmission,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1973}, {"title": "A Family of G\u00f6del Machine Implementations", "author": ["B. Steunebrink", "J. Schmidhuber"], "venue": "Fourth Conference on Artificial General Intelligence", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "G\u00f6del machines: Fully self-referential optimal universal self-improvers, in Artificial general intelligence", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "G\u00f6del machines: Towards a technical justification of consciousness, in Adaptive Agents and Multi-Agent Systems II", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "G\u00f6del machines: Self-referential universal problem solvers making provably optimal self-improvements", "author": ["J. Schmidhuber"], "venue": "Artificial General Intelligence", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}, {"title": "Ultimate cognition \u00e0 la G\u00f6del", "author": ["J. Schmidhuber"], "venue": "Cognitive Computation,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Completely self-referential optimal reinforcement learners, in Artificial Neural Networks: Formal Models and Their Applications\u2013ICANN", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Optimal ordered problem solver", "author": ["J. Schmidhuber"], "venue": "Machine Learning,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2004}, {"title": "Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement", "author": ["J. Schmidhuber", "J. Zhao", "M. Wiering"], "venue": "Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1997}, {"title": "A general method for incremental self-improvement and multiagent learning", "author": ["J. Schmidhuber"], "venue": "Evolutionary Computation: Theory and Applications,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1999}, {"title": "Continuous self-evaluation for the self-improvement of software, in Self-Adaptive Software", "author": ["J. Leon", "A. Lori"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2001}, {"title": "Finding Data in DNA: Computer Forensic Investigations of Living Organisms, in Digital Forensics and Cyber Crime", "author": ["M.B. Beck", "E.C. Rouchka", "R.V. Yampolskiy"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "DNA as a medium for hiding data", "author": ["M. Beck", "R. Yampolskiy"], "venue": "BMC Bioinformatics,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Leakproofing Singularity - Artificial Intelligence Confinement Problem", "author": ["R.V. Yampolskiy"], "venue": "Journal of Consciousness Studies (JCS),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "AI safety engineering through introduction of selfreference into felicific calculus via artificial pain and pleasure", "author": ["A.M. Majot", "R.V. Yampolskiy"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Responses to catastrophic AGI risk: A survey", "author": ["K. Sotala", "R.V. Yampolskiy"], "venue": "Physica Scripta", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "What to Do with the Singularity Paradox", "author": ["R.V. Yampolskiy"], "venue": "Philosophy and Theory of Artificial Intelligence", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Artimetrics: Biometrics for Artificial Entities", "author": ["R. Yampolskiy", "M. Gavrilova"], "venue": "IEEE Robotics and Automation Magazine (RAM),", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Experiments in Artimetrics: Avatar Face Recognition", "author": ["R Yampolskiy"], "venue": "Transactions on Computational Science XVI,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "Linguistic Profiling and Behavioral Drift in Chat Bots", "author": ["N. Ali", "D. Schaeffer", "R.V. Yampolskiy"], "venue": "Midwest Artificial Intelligence and Cognitive Science Conference,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "State-of-the-Art in Robot Authentication [From the Guest Editors", "author": ["M. Gavrilova", "R. Yampolskiy"], "venue": "Robotics & Automation Magazine,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Quantum noise and information", "author": ["H.J. Bremermann"], "venue": "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1967}, {"title": "Information in the holographic universe", "author": ["J.D. Bekenstein"], "venue": "Scientific American,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2003}, {"title": "Ultimate Physical Limits to Computation", "author": ["S. Lloyd"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "The physics of information processing superobjects: daily life among the Jupiter brains", "author": ["A. Sandberg"], "venue": "Journal of Evolution and Technology,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1999}, {"title": "Guest column: NP-complete problems and physical reality", "author": ["S. Aaronson"], "venue": "ACM Sigact News,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2005}, {"title": "A Mathematical Theory of Communication", "author": ["C.E. Shannon"], "venue": "Bell Systems Technical Journal, July 1948", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1948}, {"title": "Starkman, Universal limits on computation", "author": ["L.M. Krauss", "G.D"], "venue": "arXiv preprint astroph/0404510,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2004}, {"title": "The limits of intelligence", "author": ["D. Fox"], "venue": "Scientific American,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2011}, {"title": "Does the inertia of a body depend upon its energy-content", "author": ["A. Einstein"], "venue": "Annalen der Physik, 1905", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1905}, {"title": "Information, Physics, Quantum: The Search for Links1990: Physics Dept., University of Texas", "author": ["J.A. Wheeler"], "venue": null, "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1990}, {"title": "Checkers is Solved", "author": ["J Schaeffer"], "venue": "Science, September 2007", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2007}, {"title": "Is there a model for RSI", "author": ["M. Mahoney"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1902}, {"title": "On computable numbers, with an application to the Entscheidungsproblem", "author": ["A. Turing"], "venue": "Proceedings of the London Mathematical Society,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1936}, {"title": "A Computability Argument Against Superintelligence", "author": ["J. Wiedermann"], "venue": "Cognitive Computation,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2012}, {"title": "Is There Something Beyond AI? Frequently Emerging, but Seldom Answered Questions about Artificial Super-Intelligence", "author": ["J. Wiedermann"], "venue": "Beyond AI: Artificial Dreams:", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2012}, {"title": "A Model for Recursively Self Improving Programs, 2010: Available at: http://mattmahoney.net/rsi.pdf", "author": ["M. Mahoney"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2010}, {"title": "Classes of recursively enumerable sets and their decision problems", "author": ["H.G. Rice"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1953}, {"title": "Macready, No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 1997}, {"title": "The No Free Lunch Theorem and hypothesis of instinctive animal behavior", "author": ["A.V. Melkikh"], "venue": "Artificial Intelligence Research,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2014}, {"title": "The 21st. Century Artilect: Moral Dilemmas Concerning the Ultra Intelligent Machine", "author": ["H. de Garis"], "venue": "Revue Internationale de Philosophie,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 1990}, {"title": "Tiling agents for self-modifying AI, and the L\u00f6bian obstacle", "author": ["E. Yudkowsky", "M. Herreshoff"], "venue": "MIRI Technical Report", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2013}, {"title": "Problems of self-reference in self-improving space-time embedded intelligence, in MIRI", "author": ["B. Fallenstein", "N. Soares"], "venue": "Technical Report 2014,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2014}, {"title": "The Procrastination Paradox (Brief technical note), in MIRI Technical Report2014: Available at: https://intelligence.org/files/ProcrastinationParadox.pdf", "author": ["E. Yudkowsky"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2014}, {"title": "Logical theories for agent introspection", "author": ["T. Bolander"], "venue": "Computer Science,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2003}, {"title": "Self-modification and mortality in artificial agents, in 4th international conference on Artificial general intelligence", "author": ["L. Orseau", "M. Ring"], "venue": null, "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2011}, {"title": "Utility Function Security in Artificially Intelligent Agents", "author": ["R.V. Yampolskiy"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence (JETAI),", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2014}, {"title": "Artificial intelligence safety engineering: Why machine ethics is a wrong approach, in Philosophy and Theory of Artificial Intelligence", "author": ["R.V. Yampolskiy"], "venue": null, "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2013}, {"title": "A New Kind of Science", "author": ["S. Wolfram"], "venue": "May 14,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2002}, {"title": "Is there a model for RSI?, in SL4", "author": ["M. Mahoney"], "venue": null, "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2008}, {"title": "Computing Partial Solutions to Difficult AI Problems", "author": ["R.V. Yampolskiy"], "venue": "Midwest Artificial Intelligence and Cognitive Science Conference,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2012}, {"title": "On the hardness of reoptimization, in SOFSEM 2008: Theory and Practice of Computer Science", "author": ["B\u00f6ckenhauer", "H.-J"], "venue": null, "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2008}, {"title": "Reoptimization of minimum and maximum traveling salesman\u2019s tours, in Algorithm Theory\u2013SWAT", "author": ["G Ausiello"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2006}, {"title": "Reoptimizing the traveling salesman problem", "author": ["C. Archetti", "L. Bertazzi", "M.G. Speranza"], "venue": null, "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2003}, {"title": "Complexity and approximation in reoptimization. 2011: Imperial College Press/World Scientific", "author": ["G. Ausiello", "V. Bonifaci", "B. Escoffier"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2011}, {"title": "Why an intelligence explosion is probable, in Singularity Hypotheses", "author": ["R. Loosemore", "B. Goertzel"], "venue": null, "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2012}, {"title": "Towards a theory of AI completeness, in 8th International Symposium on Logical Formalizations of Commonsense Reasoning (Commonsense", "author": ["D. Shahaf", "E. Amir"], "venue": null, "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2007}, {"title": "Turing Test as a Defining Feature of AI-Completeness, in Artificial Intelligence, Evolutionary Computing and Metaheuristics", "author": ["R. Yampolskiy"], "venue": "X.-S. Yang, Editor", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "AI-Complete, AI-Hard, or AI-Easy\u2013Classification of Problems in AI", "author": ["R.V. Yampolskiy"], "venue": "The 23rd Midwest Artificial Intelligence and Cognitive Science Conference,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2012}, {"title": "General Intelligence and Seed AI - Creating Complete Minds Capable of Open-Ended  Self-Improvement,  2001:  Availablet  at: http://singinst.org/ourresearch/publications/GISAI", "author": ["E.S. Yudkowsky"], "venue": null, "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2001}, {"title": "AI-Complete CAPTCHAs as Zero Knowledge Proofs of Access to an Artificially Intelligent System", "author": ["R.V. Yampolskiy"], "venue": "ISRN Artificial Intelligence,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2011}, {"title": "On Computable Numbers, with an Application to the Entscheidungsproblem", "author": ["A.M. Turing"], "venue": "Proceedings of the London Mathematical Society, 1936", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1936}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2014}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192 down approach, in Artificial general intelligence", "author": ["M. Hutter"], "venue": null, "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2007}, {"title": "Three Approaches to the Quantitative Definition of Information", "author": ["A.N. Kolmogorov"], "venue": "Problems Inform. Transmission,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 1965}, {"title": "What is a Singleton? Linguistic and Philosophical Investigations", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2006}, {"title": "Timeless decision theory", "author": ["E. Yudkowsky"], "venue": "The Singularity Institute, San Francisco,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2010}, {"title": "Coherent Extrapolated Volition, May 2004 Singularity Institute for Artificial Intelligence: Available at: http://singinst.org/upload/CEV.html", "author": ["E.S. Yudkowsky"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2004}, {"title": "VARIAC: an Autogenous Cognitive Architecture", "author": ["J.S. Hall"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2008}, {"title": "Definition and complexity of some basic metareasoning problems", "author": ["V. Conitzer", "T. Sandholm"], "venue": "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2003}, {"title": "Recursive Self-Improvement in Less Wrong December 1, 2008", "author": ["E. Yudkowsky"], "venue": "Available at: http://lesswrong.com/lw/we/recursive_selfimprovement/, retrieved September", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2014}, {"title": "Can Intelligence Explode", "author": ["M. Hutter"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2012}, {"title": "The physics of immortality: Modern cosmology, God, and the resurrection of the dead 1994: Random House LLC", "author": ["F.J. Tipler"], "venue": null, "citeRegEx": "115", "shortCiteRegEx": "115", "year": 1994}, {"title": "Devo Universe? A Framework for Speculations on Cosmic Culture, in Cosmos and Culture: Cultural Evolution in a Cosmic Context, M.L.L", "author": ["J.M. Smart", "Evo"], "venue": "Steven J. Dick, Editor", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2009}, {"title": "The meaning of life in a developing universe", "author": ["J.E. Stewart"], "venue": "Foundations of Science,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2010}, {"title": "The Beginning and the End: The Meaning of Life in a Cosmological Perspective", "author": ["C. Vidal"], "venue": "arXiv preprint arXiv:1301.1648,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "education we can assume, as a first approximation, to be much the same as for the human child\u201d [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Thus the first ultraintelligent machine is the last invention that man need ever make\u201d [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Similar types of arguments are still being made today by modern researchers and the area of RSI research continues to grow in popularity [5-7], though some [8] have argued that recursive self-improvement process requires", "startOffset": 137, "endOffset": 142}, {"referenceID": 3, "context": "Similar types of arguments are still being made today by modern researchers and the area of RSI research continues to grow in popularity [5-7], though some [8] have argued that recursive self-improvement process requires", "startOffset": 137, "endOffset": 142}, {"referenceID": 4, "context": "Similar types of arguments are still being made today by modern researchers and the area of RSI research continues to grow in popularity [5-7], though some [8] have argued that recursive self-improvement process requires", "startOffset": 137, "endOffset": 142}, {"referenceID": 5, "context": "Similar types of arguments are still being made today by modern researchers and the area of RSI research continues to grow in popularity [5-7], though some [8] have argued that recursive self-improvement process requires", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "P vs NP) [9].", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Mainly, the major improvement we want from self-improving intelligent software is higher degree of intelligence which can be approximated via machine friendly IQ tests [10] with a significant G-factor correlation.", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "While a number of obfuscation techniques are known to exist [11], ex.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "self-modifying code [12], polymorphic code, metamorphic code, diversion code [13], none of", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "self-modifying code [12], polymorphic code, metamorphic code, diversion code [13], none of", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "The sole purpose of such approaches is to modify how the source code looks to those trying to understand the software in questions and what it does [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "Self-Improvement or Self-adaptation [15] is a desirable property of many types of software products [16] and typically allows for some optimization or customization of the product to the environment and users it is deployed with.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 15, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 17, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 18, "context": "Common examples of such software include evolutionary algorithms such as Genetic Algorithms [17-22] or Genetic Programming which optimize software parameters with respect to some well understood fitness function and perhaps work over some highly modular programming language to assure that all modifications result in software which can be compiled and evaluated.", "startOffset": 92, "endOffset": 99}, {"referenceID": 19, "context": "of efficiency drives in self-improving software [23].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "If the system is not balanced overall performance of the system could be increased by shifting resources from subsystems with small marginal improvement to those with larger marginal increase [23].", "startOffset": 192, "endOffset": 196}, {"referenceID": 20, "context": "used in the machine learning literature to indicate self-modifying learning algorithms or the process of selecting an algorithm which will perform best in a particular problem domain [24].", "startOffset": 183, "endOffset": 187}, {"referenceID": 21, "context": "of the system does the optimization and another component is getting optimized [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "In the field of complex dynamic systems, aka chaos theory, positive feedback systems are well known to always end up in what is known as an attractor- a region within system\u2019s state space that the system can\u2019t escape from [26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "A good example of such attractor convergence is the process of Metacompilation or Supercompilation [27] in which a program designed to take", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "It will likely produce a more efficient compiler on the first application perhaps by 20%, on the second application by 3%, and after a few more recursive iterations converge to a fixed point of zero improvement [26].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "Such advantages include [28]: longer work spans (no breaks, sleep, vocation, etc.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "free from human cognitive biases) [29], new sensory modalities (native sensory hardware for source code), blending over of deliberative and automatic processes (management of computational resources over multiple tasks), introspective perception and manipulation (ability to analyze low level hardware, ex.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "), advanced communication (ability to share underlying cognitive representations for memories and skills) [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Chalmers [31] uses logic and mathematical induction to show that if an AI0 system is capable of producing only slightly more capable AI1 system generalization of that process leads to superintelligent performance in AIn after n generations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "They also list properties of a system which make it purposeful, goal-oriented and self-organizing, particularly: reflectivity \u2013 ability to analyze and rewrite its own structure; autonomy \u2013 being free from influence by system\u2019s original designers (bounded autonomy \u2013 is a property of a system with elements which are not subject to self-modification); endogeny \u2013 an autocatalytic ability [32].", "startOffset": 387, "endOffset": 391}, {"referenceID": 29, "context": "tractable approach [33].", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "use of terms - Cascades, Cycles and Insight which he defines as: Cascades \u2013 when one development leads to another; Cycles \u2013 repeatable cascade in which one optimization leads to another which in turn benefits the original optimization; Insight \u2013 new information which greatly increases one\u2019s optimization ability [34].", "startOffset": 313, "endOffset": 317}, {"referenceID": 31, "context": "An agent engaging in an optimization process and able to hit non-trivial targets in large search space [35] is described as having significant optimization power [25].", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "An agent engaging in an optimization process and able to hit non-trivial targets in large search space [35] is described as having significant optimization power [25].", "startOffset": 162, "endOffset": 166}, {"referenceID": 32, "context": "Critics explain failure of scientists, to date, to achieve a sustained RSI process by saying that RSI researchers have fallen victims of the bootstrap fallacy [36].", "startOffset": 159, "endOffset": 163}, {"referenceID": 33, "context": "The first one is a brute force based approach [37] which utilizes Levin (Universal [38]) Search [39].", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "The first one is a brute force based approach [37] which utilizes Levin (Universal [38]) Search [39].", "startOffset": 96, "endOffset": 100}, {"referenceID": 35, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 36, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 37, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 38, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 39, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 40, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 76, "endOffset": 83}, {"referenceID": 41, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 123, "endOffset": 127}, {"referenceID": 42, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 159, "endOffset": 167}, {"referenceID": 43, "context": "Some variants of this approach to self-improvement, known as G\u00f6del Machines [40-45], Optimal Ordered Problem Solver (OOPS) [46] and Incremental Self-Improvers [47, 48], have been thoroughly", "startOffset": 159, "endOffset": 167}, {"referenceID": 44, "context": "Other types of Indirect RSI may be based on collaboration between multiple artificial systems instead of AI and human teams [49].", "startOffset": 124, "endOffset": 128}, {"referenceID": 45, "context": "Such portions would be akin to ultra-conserved elements or conserved sequences of DNA [50, 51] found among multiple related species.", "startOffset": 86, "endOffset": 94}, {"referenceID": 46, "context": "Such portions would be akin to ultra-conserved elements or conserved sequences of DNA [50, 51] found among multiple related species.", "startOffset": 86, "endOffset": 94}, {"referenceID": 27, "context": "This question is particularly important for the goal preservation in self-improving intelligent software, as we want to make sure that future generations of the system are motivated to work on the same problem [31].", "startOffset": 210, "endOffset": 214}, {"referenceID": 5, "context": "likely to engage in a de-biasing process removing any constraints we programmed into it [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 47, "context": "We are also interested in understanding if RSI process can take place in an isolated (leakproofed [52]) system or if interaction with external environment, internet, people, other AI agents is necessary.", "startOffset": 98, "endOffset": 102}, {"referenceID": 48, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 49, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 50, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 51, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 52, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 53, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 54, "context": "with early RSI systems [53-61].", "startOffset": 23, "endOffset": 30}, {"referenceID": 55, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 11, "endOffset": 15}, {"referenceID": 56, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 28, "endOffset": 32}, {"referenceID": 57, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 40, "endOffset": 44}, {"referenceID": 58, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 53, "endOffset": 57}, {"referenceID": 59, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 68, "endOffset": 72}, {"referenceID": 60, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 82, "endOffset": 86}, {"referenceID": 61, "context": "Bremermann [62], Bekenstein [63], Lloyd [64], Anders [65], Aaronson [66], Shannon [67], Krauss [68], and many others have investigated ultimate limits to computation in terms of speed, communication and energy consumption with respect to such factors as speed of light, quantum noise, and gravitational constant.", "startOffset": 95, "endOffset": 99}, {"referenceID": 62, "context": "Some research has also been done on establishing ultimate limits for enhancing human brain\u2019s intelligence [69].", "startOffset": 106, "endOffset": 110}, {"referenceID": 63, "context": "Since more complex systems have greater number of components and require more matter, even if individual parts are designed at nanoscale, we can conclude that just like matter and energy are directly related [70] and matter and information (\u201cit", "startOffset": 208, "endOffset": 212}, {"referenceID": 64, "context": "from bit\u201d) [71] so is matter and intelligence.", "startOffset": 11, "endOffset": 15}, {"referenceID": 65, "context": "For many problems such as playing checkers [72] it is possible to completely solve the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [73].", "startOffset": 43, "endOffset": 47}, {"referenceID": 66, "context": "For many problems such as playing checkers [72] it is possible to completely solve the problem (provide an optimal solution after considering all possible options) after which no additional performance improvement would be possible [73].", "startOffset": 232, "endOffset": 236}, {"referenceID": 67, "context": "Other problems are known to be unsolvable regardless of level of intelligence applied to them [74].", "startOffset": 94, "endOffset": 98}, {"referenceID": 6, "context": "Assuming separation of complexity classes (such as P vs NP) holds [9], it becomes obvious", "startOffset": 66, "endOffset": 69}, {"referenceID": 68, "context": "Wiedermann argues that cognitive systems form an infinite hierarchy and from a computational point of view human-level intelligence is upper-bounded by the \u22112 class of the Arithmetic Hierarchy [75].", "startOffset": 193, "endOffset": 197}, {"referenceID": 68, "context": "upper levels of the Arithmetic Hierarchy [75, 76].", "startOffset": 41, "endOffset": 49}, {"referenceID": 69, "context": "upper levels of the Arithmetic Hierarchy [75, 76].", "startOffset": 41, "endOffset": 49}, {"referenceID": 70, "context": "Mahoney attempts to formalize what it means for a program to have a goal G and to self-improve with respect to being able to reach said goal under constraint of time, t [77].", "startOffset": 169, "endOffset": 173}, {"referenceID": 70, "context": "G(Q(t)) > G(P(t)) and ~\uf024t, t\u2019 > t, G(Q(t)) > G(P(t)) [77].", "startOffset": 53, "endOffset": 57}, {"referenceID": 70, "context": "is an improving sequence with respect to goal G [77].", "startOffset": 48, "endOffset": 52}, {"referenceID": 70, "context": "Mahoney also analyzes complexity of RSI software and presents a proof demonstrating that the algorithmic complexity of Pn (the nth iteration of an RSI program) is not greater than O(log n) implying a very limited amount of knowledge gain would be possible in practice despite theoretical possibility of RSI systems [77].", "startOffset": 315, "endOffset": 319}, {"referenceID": 21, "context": "cycle [25].", "startOffset": 6, "endOffset": 10}, {"referenceID": 71, "context": "For example Levin type search through the program space will face problems related to Rice\u2019s theorem [78] which states that for any arbitrarily chosen program it is impossible to test if it has any non-trivial property such as being very intelligent.", "startOffset": 101, "endOffset": 105}, {"referenceID": 72, "context": "Also, universal search over the space of mind designs which will not be computationally possible due to the No Free Lunch theorems [79] as we have no information to reduce the size of the search space [80].", "startOffset": 131, "endOffset": 135}, {"referenceID": 73, "context": "Also, universal search over the space of mind designs which will not be computationally possible due to the No Free Lunch theorems [79] as we have no information to reduce the size of the search space [80].", "startOffset": 201, "endOffset": 205}, {"referenceID": 74, "context": "any higher level modules and so present a fundamental difficulty in accessing their performance [81].", "startOffset": 96, "endOffset": 100}, {"referenceID": 75, "context": "mathematical system can\u2019t assert its own soundness without becoming inconsistent [82], meaning a sufficiently expressive formal system can\u2019t know that everything it proves to be true is actually so [82].", "startOffset": 81, "endOffset": 85}, {"referenceID": 75, "context": "mathematical system can\u2019t assert its own soundness without becoming inconsistent [82], meaning a sufficiently expressive formal system can\u2019t know that everything it proves to be true is actually so [82].", "startOffset": 198, "endOffset": 202}, {"referenceID": 76, "context": "find itself in a state in which a change made immediately is as desirable and likely as the same change made later [83, 84].", "startOffset": 115, "endOffset": 123}, {"referenceID": 77, "context": "find itself in a state in which a change made immediately is as desirable and likely as the same change made later [83, 84].", "startOffset": 115, "endOffset": 123}, {"referenceID": 78, "context": "Similarly, Bolander raises some problems inherent in logical reasoning with self-reference, namely, self-contradictory reasoning, exemplified by the Knower Paradox of the form - \u201cThis sentence is false\u201d [85].", "startOffset": 203, "endOffset": 207}, {"referenceID": 79, "context": "in which an agent will chose to modify itself towards its own detriment if presented with a high enough reward to do so [86].", "startOffset": 120, "endOffset": 124}, {"referenceID": 80, "context": "Yampolskiy reviews a number of related problems in rational selfimproving optimizers, above a certain capacity, and concludes, that despite opinion of many, such machines will choose to \u201cwirehead\u201d [87].", "startOffset": 197, "endOffset": 201}, {"referenceID": 27, "context": "Chalmers [31] suggests a number of previously unanalyzed potential obstacles on the path to RSI software with Correlation obstacle being one", "startOffset": 9, "endOffset": 13}, {"referenceID": 81, "context": "Errors (bugs) which are not detrimental to system\u2019s performance are very hard to detect and may accumulate from generation to generation building on each other until a critical mass of such errors leads to erroneous functioning of the system, mistakes in evaluating quality of the future generations of the software or a complete breakdown [88].", "startOffset": 340, "endOffset": 344}, {"referenceID": 82, "context": "An additional problem may be that the system in question is computationally irreducible [89] and so can\u2019t simulate running its own source code.", "startOffset": 88, "endOffset": 92}, {"referenceID": 83, "context": "Similarly it has been argued that a Turing Machine cannot output a machine of greater algorithmic complexity [90].", "startOffset": 109, "endOffset": 113}, {"referenceID": 84, "context": "It seems that it should be easier to solve a problem if we already have a solution to a smaller instance of such problem [91] but in a formalized world of problems belonging to the same", "startOffset": 121, "endOffset": 125}, {"referenceID": 89, "context": "minimum intelligence necessary for commencing the RSI process, but we can speculate that it would be on par with human intelligence which we associate with universal or general intelligence [96], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 27, "context": "minimum intelligence necessary for commencing the RSI process, but we can speculate that it would be on par with human intelligence which we associate with universal or general intelligence [96], though in principal a sub-human level system capable of self-improvement can\u2019t be excluded [31].", "startOffset": 287, "endOffset": 291}, {"referenceID": 90, "context": "we already have programmers (people or their intellectual equivalence formalized as functions [97] or Human Oracles [98, 99]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 94, "endOffset": 98}, {"referenceID": 91, "context": "we already have programmers (people or their intellectual equivalence formalized as functions [97] or Human Oracles [98, 99]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 116, "endOffset": 124}, {"referenceID": 92, "context": "we already have programmers (people or their intellectual equivalence formalized as functions [97] or Human Oracles [98, 99]) who have access to their own source code (DNA), but who fail to understand how DNA (nature) works to create their intelligence.", "startOffset": 116, "endOffset": 124}, {"referenceID": 93, "context": "We also don\u2019t know the minimum size of program (called Seed AI [100]) necessary to get the ball rolling.", "startOffset": 63, "endOffset": 68}, {"referenceID": 33, "context": "Perhaps if it turns out that such \u201cminimal genome\u201d is very small a brute force [37] approach might succeed in discovering it.", "startOffset": 79, "endOffset": 83}, {"referenceID": 94, "context": "Artificial General Intelligence known to exist [101] in the world as otherwise we can simply delegate the other AI as the seed.", "startOffset": 47, "endOffset": 52}, {"referenceID": 95, "context": "In order to answer such questions it may be useful to further formalize the notion of RSI perhaps by representing such software as a Turing Machine [102] with particular inputs and outputs.", "startOffset": 148, "endOffset": 153}, {"referenceID": 96, "context": "minutes, days, weeks, years or more (hard takeoff VS soft takeoff) for the RSI system to begin hitting limits of what is possible with respect to physical limits of computation [103]? Even in suitably constructed hardware (human baby) it takes decades of data input (education) to get to", "startOffset": 177, "endOffset": 182}, {"referenceID": 21, "context": "He also looks at different possible rates of return and arrives at three progressively steeper trajectories for RSI improvement which he terms: \u201cfizzle\u201d, \u201ccombust\u201d and \u201cexplode\u201d aka \u201cAI go FOOM\u201d [25].", "startOffset": 195, "endOffset": 199}, {"referenceID": 5, "context": "Hall [8] similarly", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "Fascinatingly, by analyzing properties which correlate with intelligence, Chalmers [31] is able to generalize self-improvement optimization to properties other than intelligence.", "startOffset": 83, "endOffset": 87}, {"referenceID": 97, "context": "superintelligent agent [104]).", "startOffset": 23, "endOffset": 28}, {"referenceID": 98, "context": "It may even be the case that sufficiently smart RSIs will be able to immediately deduce such architecture from basic knowledge of physics and Kolmogorov Complexity [105].", "startOffset": 164, "endOffset": 169}, {"referenceID": 31, "context": "It is also likely that in the space of minds [35] stable attractors include sub-human and super-human intelligences with precisely human level of intelligence being a rare particular", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 99, "context": "[106], a single decision making agent in control of everything.", "startOffset": 0, "endOffset": 5}, {"referenceID": 100, "context": "Further speculation can lead us to conclude that converged RSI systems separated by space and time even at cosmological scales can engage in acausal cooperation [107, 108] since they will realize that they are the same agent with the same architecture and so are capable of running perfect simulations of each other\u2019s future behavior.", "startOffset": 161, "endOffset": 171}, {"referenceID": 101, "context": "Consequently, by observing a converged RSI architecture and properties humanity can determine its ultimate destiny, its purpose in life, its Coherent Extrapolated Volition (CEV) [109].", "startOffset": 178, "endOffset": 183}, {"referenceID": 102, "context": "[110], and so is an AI-Complete problem as has been demonstrated by Yampolskiy [98, 99].", "startOffset": 0, "endOffset": 5}, {"referenceID": 91, "context": "[110], and so is an AI-Complete problem as has been demonstrated by Yampolskiy [98, 99].", "startOffset": 79, "endOffset": 87}, {"referenceID": 92, "context": "[110], and so is an AI-Complete problem as has been demonstrated by Yampolskiy [98, 99].", "startOffset": 79, "endOffset": 87}, {"referenceID": 44, "context": "AIcomplete problems are by definition most difficult problems faced by AI researchers and it is likely that RSI source code will be so complex that it would be difficult or impossible to fully analyze [49].", "startOffset": 201, "endOffset": 205}, {"referenceID": 103, "context": "Finally, they showed that the problem of deliberately choosing a limited number of deliberation or information gathering actions to disambiguate the state of the world is PSPACE Hard in general [112].", "startOffset": 194, "endOffset": 199}, {"referenceID": 104, "context": "Consequently, any RSI achieving such significant level of enhancement, despite not creating an infinite process, will still seem like it is producing superintelligence with respect to our current state [113].", "startOffset": 202, "endOffset": 207}, {"referenceID": 105, "context": "Additionally, critics may suggest that intelligence is upper bounded and only differs by speed and available info to process [114].", "startOffset": 125, "endOffset": 130}, {"referenceID": 105, "context": "A resource-dependent system undergoing RSI intelligence explosion can expand and harvest matter, at the speed of light, from its origin converting the universe around it into a computronium sphere [114].", "startOffset": 197, "endOffset": 202}, {"referenceID": 106, "context": "It is also very likely to try and condense all the matter it obtains into a super-dense unit of constant volume (reminiscent of the original physical singularity point which produced the Big Bang, see Omega Point [115]) to reduce internal computational costs which grow with the overall size of the system and at cosmic scales are very significant even at", "startOffset": 213, "endOffset": 218}], "year": 2015, "abstractText": "Software capable of improving itself has been a dream of computer scientists since the inception of the field. In this work we provide definitions for Recursively Self-Improving software, survey different types of self-improving software, review the relevant literature, analyze limits on computation restricting recursive self-improvement and introduce RSI Convergence Theory which aims to predict general behavior of RSI systems. Finally, we address security implications from self-improving intelligent software.", "creator": "Microsoft\u00ae Office Word 2007"}}}