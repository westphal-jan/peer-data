{"id": "1502.06922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval", "abstract": "this paper develops a model that addresses sentence syntax embedding efficiently using recurrent neural networks ( rnn ) with long short term memory ( lstm ) cells. the proposed lstm - compliant rnn model sequentially takes each word in a sentence, extract extracts, its information, preserves and embeds it into a semantic vector. due to its superior ability to capture long term content memory, the lstm - rnn accumulates increasingly providing richer information flow as it goes through the sentence, and when it accidentally reaches the last word, the hidden layer of coating the network provides a semantic mapping representation of the whole sentence. in this paper, the lstm - usable rnn is trained in a weakly supervised manner on user click - through data logged by a centralized commercial web search engine. visualization and analysis concepts are normally performed to understand how the embedding process works. the model automatically attenuates the unimportant words and detects the salient keywords in the sentence. furthermore, these detected item keywords automatically activate different cells of the lstm - run rnn, where words belonging to a similar topic activate the appropriate same cell. as a common semantic representation of the sentence, the embedding vector engine can be used in many different design applications. these diverse keyword path detection and topic allocation measurement tasks enabled by the lstm - rnn allow upon the network to perform web document retrieval, where the similarity between the input query and documents themselves can be measured by the distance between their corresponding sentence embedding display vectors computed by the lstm - rnn. on finding a wider web search task, the lstm - rnn embedding is shown to significantly outperform all existing state of ] the art methods.", "histories": [["v1", "Tue, 24 Feb 2015 19:39:27 GMT  (1307kb,D)", "http://arxiv.org/abs/1502.06922v1", null], ["v2", "Sun, 5 Jul 2015 06:11:19 GMT  (1490kb,D)", "http://arxiv.org/abs/1502.06922v2", null], ["v3", "Sat, 16 Jan 2016 06:35:23 GMT  (1759kb,D)", "http://arxiv.org/abs/1502.06922v3", "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["hamid palangi", "li deng", "yelong shen", "jianfeng gao", "xiaodong he", "jianshu chen", "xinying song", "rabab ward"], "accepted": false, "id": "1502.06922"}, "pdf": {"name": "1502.06922.pdf", "metadata": {"source": "META", "title": "Deep Sentence Embedding Using the Long Short Term Memory Network: Analysis and Application to Information Retrieval", "authors": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "emails": ["HAMIDP@ECE.UBC.CA", "RABABW@ECE.UBC.CA", "DENG@MICROSOFT.COM", "JFGAO@MICROSOFT.COM", "XIAOHE@MICROSOFT.COM", "YESHEN@MICROSOFT.COM", "JIANSHUC@MICROSOFT.COM", "XINSON@MICROSOFT.COM"], "sections": [{"heading": null, "text": "Proceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s)."}, {"heading": "1. Introduction", "text": "Learning a good representation (or features) of input data is an important task in machine learning. In text and language processing, one such problem is learning of an embedding vector for a sentence; that is, to train a model that can automatically transform a sentence to a vector that encodes the semantic meaning of the sentence. By mapping texts into a unified semantic representation, the embedding vector can be further used for different applications, such as machine translation (Sutskever et al., 2014), sentiment analysis (Le & Mikolov, 2014), and information retrieval (Huang et al., 2013). In machine translation, the recurrent neural networks (RNN) with Long Short Term Memory (LSTM) cells, or the LSTM-RNN, is used to encode an English sentence into a vector, which contains the semantic meaning of the input sentence, and then another LSTMRNN is used to generate a French sentence from the vector. The model is trained to best predict the output sentence. In (Le & Mikolov, 2014), a paragraph vector is learned in an unsupervised manner as a distributed representation of sentences and documents, which are then used for sentiment analysis. Sentence embedding can also be applied to information retrieval, where the contextual information are properly represented by the vectors in the same space for fuzzy text matching (Huang et al., 2013).\nIn this paper, we propose to use an RNN to sequentially accept each word in a sentence and recurrently map it into a latent space together with the historical information. As the RNN reaches the last word in the sentence, the hidden activations form a natural embedding vector for the contextual information of the sentence. We further incorporate the LSTM cells into the RNN model (i.e. the LSTMRNN) to address the difficulty of learning long term memory in RNN. The learning of such a model is performed in a weakly supervised manner on the click-through data logged by a commercial web search engine. Although manually labelled data are insufficient in machine learning, logged data with limited feedback signals are massively available due to\nar X\niv :1\n50 2.\n06 92\n2v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\nthe widely used commercial web search engines. Limited feedback information such as click-through data provides a weak supervision signal that indicates the semantic similarity between the text on the query side and the clicked text on the document side. To exploit such a signal, the objective of our training is to maximize the similarity between the two vectors mapped by the LSTM-RNN from the query and the clicked document, respectively. Consequently, the learned vector is expected to represent different sentences of a similar meaning.\nAn important contribution of this paper is to analyze the embedding process of the LSTM-RNN by visualizing the internal activation behaviors in response to different text inputs. We show that the embedding process of the learned LSTM-RNN effectively detects the keywords, while attenuating less important words, in the sentence automatically by switching on and off the gates in within the LSTM-RNN cells. We further show that different cells in the learned model indeed correspond to different topics, and the keywords associated with a similar topic activate the same cell unit in the model. As the LSTM-RNN reads to the end of the sentence, the topic activation accumulates and the hidden vector at the last word encodes the rich contextual information of the entire sentence. For this reason, a natural application of the sentence embedding is web search ranking, in which the embedding vector from the query can be used to match the embedding vectors of the candidate documents according to the maximum cosine similarity rule. Evaluated on a real web document ranking task, our proposed method significantly outperforms all the existing state of the art methods in NDCG scores."}, {"heading": "2. Related Work", "text": "Inspired by the word embedding method (Mikolov et al., 2013b;a), the authors in (Le & Mikolov, 2014) proposed an unsupervised learning method to learn a paragraph vector as a distributed representation of sentences and documents, which are then used for sentiment analysis with superior performance. However, the model is not designed to capture the fine-grained sentence structure.\nSimilar to the recurrent models in this paper, The DSSM (Huang et al., 2013) and CLSM (Shen et al., 2014) models, developed for information retrieval, can also be interpreted as sentence embedding methods. However, DSSM treats the input sentence as a bag-of-words and does not model word dependencies explicitly. CLSM treats a sentence as a bag of n-grams, where n is defined by a window, and can capture local word dependencies. Then a Max-pooling layer is used to form a global feature vector. This model, by design, cannot capture long distance dependencies, i.e., dependencies among words belonging to non-overlapping n-grams.\nLong short term memory networks were developed in (Hochreiter & Schmidhuber, 1997) to address the difficult of capturing long term memory in RNN. It has been successfully applied to speech recognition, which achieves state-of-art performance (Graves et al., 2013; Sak et al., 2014). In text analysis, LSTM-RNN treats a sentence as a sequence of words with internal structures, i.e., word dependencies. It encodes a semantic vector of a sentence incrementally which differs from DSSM and CLSM. The encoding process is performed left-to-right, word-by-word. At each time step, a new word is encoded into the semantic vector, and the word dependencies embedded in the vector are \u201cupdated\u201d. When the process reaches the end of the sentence, the semantic vector has embedded all the words and their dependencies, hence, can be viewed as a feature vector representation of the whole sentence. In the machine translation work (Sutskever et al., 2014), an input English sentence is converted into a vector representation using LSTM-RNN, and then another LSTM-RNN is used to generate an output French sentence. The model is trained to maximize the probability of predicting the correct output sentence.\nDifferent from the aforementioned studies, the method developed in this paper trains the model so that sentences that are paraphrase of each other are close in their semantic embedding vectors \u2014 see the description in Sec. 4 further ahead. Another reason that LSTM-RNN is particularly effective for sentence embedding, is its robustness to noise. For example, in the web document ranking task, the noise comes from two sources: (i) Not every word in query / document is equally important, and we only want to \u201cremember\u201d salient words using the limited \u201cmemory\u201d. (ii) A word or phrase that is important to a document may not be relevant to a given query, and we only want to \u201cremember\u201d related words that are useful to compute the relevance of the document for a given query. We will illustrate robustness of LSTM-RNN in this paper. The structure of LSTM-RNN will also circumvent the serious limitation of using a fixed window size in CLSM. Our experiments show that this difference leads to significantly better results in web document retrieval task. Furthermore, it has other advantages. It allows us to capture keywords and key topics effectively. The models in this paper also do not need the extra maxpooling layer, as required by the CLSM, to capture global contextual information and they do so more effectively."}, {"heading": "3. Sentence Embedding Using RNNs with and without LSTM Cells", "text": "In this section, we introduce the model of recurrent neural networks and its long short term memory version for learning the sentence embedding vectors. We start with the basic RNN and then proceed to LSTM-RNN."}, {"heading": "3.1. The basic version of RNN", "text": "The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014). The main idea of using RNN for sentence embedding is to find a dense and low dimensional semantic representation by sequentially and recurrently processing each word in a sentence and mapping it into a low dimensional vector. In this model, the global contextual features of the whole text will be in the semantic representation of the last word in the text sequence \u2014 see Figure 1, where x(t) is the t-th word, coded as a 1-hot vector, Wh is a fixed hashing operator similar to the one used in (Huang et al., 2013) that converts the word vector to a letter trigram vector, W is the input weight matrix, Wrec is the recurrent weight matrix, y(t) is the hidden activation vector of the RNN, which can be used as a semantic representation of the t-th word, and y(t) associated to the last word x(m) is the semantic representation vector of the entire sentence. Note that this is very different from the approach in (Huang et al., 2013) where the bag-of-words representation is used for the whole text and no context information is used. This is also different from (Shen et al., 2014) where the sliding window of a fixed size (akin to an FIR filter) is used to capture local features and a max-pooling layer on the top to capture global features. In the RNN there is neither a fixed-sized window nor a max-pooling layer; rather the recurrence is used to capture the context information in the sequence (akin to an IIR filter).\nWithout loss of generality, we assume the bias is zero. Then the mathematical formulation of the above RNN model for sentence embedding can be expressed as\nl1(t) = Whx(t)\ny(t) = f(Wl1(t) + Wrecy(t\u2212 1)) (1)\nwhere W and Wrec are the input and recurrent matrices to be learned, Wh is a fixed word hashing operator, and f(\u00b7) is assumed to be tanh(\u00b7). Note that the architecture proposed here for sentence embedding is slightly different from traditional RNN in that there is a word hashing layer that convert the high dimensional input into a relatively lower dimensional letter tri-gram representation. There is also no per word supervision during training, instead, the whole sentence has a label. This is explained in more detail in section 4."}, {"heading": "3.2. The RNN with LSTM cells", "text": "Although RNN performs the transformation from the sentence to a vector in a principled manner, it is generally difficult to learn the long term dependency within the sequence due to vanishing gradients problem. One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in (Hochreiter & Schmidhuber, 1997) as Long Short Term Memory (LSTM) and completed in (Gers et al., 1999) and (Gers et al., 2003) by adding forget gate and peephole connections to the architecture.\nWe use the architecture of LSTM illustrated in Fig. 2 for the proposed sentence embedding method. In this figure, i(t), f(t) ,o(t) , c(t) are input gate, forget gate, output gate and cell state vector respectively, Wp1, Wp2 and Wp3 are peephole connections, Wi, Wreci and bi, i = 1, 2, 3, 4 are input connections, recurrent connections and bias values, respectively, g(\u00b7) and h(\u00b7) are tanh(\u00b7) function and \u03c3(\u00b7) is the sigmoid function. We use this architecture to find y for each word, then use the y(m) corresponding to the last word in the sentence as the semantic vector for the entire sentence.\nConsidering Fig. 2, the forward pass for LSTM-RNN model is as follows:\nyg(t) = g(W4l1(t) + Wrec4y(t\u2212 1) + b4) i(t) = \u03c3(W3l1(t) + Wrec3y(t\u2212 1) + Wp3c(t\u2212 1) + b3) f(t) = \u03c3(W2l1(t) + Wrec2y(t\u2212 1) + Wp2c(t\u2212 1) + b2) c(t) = f(t) \u25e6 c(t\u2212 1) + i(t) \u25e6 yg(t) o(t) = \u03c3(W1l1(t) + Wrec1y(t\u2212 1) + Wp1c(t) + b1) y(t) = o(t) \u25e6 h(c(t)) (2)\nwhere \u25e6 denotes Hadamard (element-wise) product."}, {"heading": "4. Learning Method", "text": "To learn a good semantic representation of the input sentence, our objective is to make the embedding vectors for sentences of similar meaning as close as possible, and meanwhile, to make sentences of different meanings as far as possible. This is challenging in practice since it is hard\nto collect a large amount of manually labelled data that give the semantic similarity signal between different sentences. Nevertheless, the widely used commercial web search engine is able to log massive amount of data with some limited user feedback signals. For example, given a particular query, the click-through information about the user-clicked document among many candidates is usually recorded and can be used as a weak (binary) supervision signal to indicate the semantic similarity between two sentences (on the query side and the document side). In this section, we explain how to leverage such a weak supervision signal to learn a sentence embedding vector that achieves the aforementioned training objective.\nWe now describe how to train the model to achieve the above objective using the click-through data logged by a commercial search engine. For a complete description of the click-through data please refer to section 2 in (Gao et al., 2009). To begin with, we adopt the cosine similarity between the semantic vectors of two sentences as a measure for their similarity:\nR(Q,D) = yQ(TQ)\nTyD(TD)\n\u2016yQ(TQ)\u2016 \u00b7 \u2016yD(TD)\u2016 (3)\nwhere TQ and TD are the lengths of the sentence Q and sentence D, respectively. In the context of training over click-through data, we will use Q and D to denote \u201cquery\u201d and \u201cdocument\u201d, respectively. In Figure 3, we show the sentence embedding vectors corresponding to the query, yQ(TQ), and all the documents, {yD+(TD+),yD\u22121 (TD\u22121 ), . . . ,yD\u2212n (TD\u2212n )}, where the subscript D+ denotes the (clicked) positive sample among the documents, and the subscript D\u2212j denotes the jth (un-clicked) negative sample. All these embedding vec-\ntors are generated by feeding the sentences into the RNN or LSTM-RNN model described in Sec. 3 and take the y corresponding to the last word \u2014 see the blue box in Figure 1.\nWe want to maximize the likelihood of the clicked document given query, which can be formulated as the following optimization problem:\nL(\u039b) = min \u039b\n{ \u2212 log\nN\u220f r=1 P (D+r |Qr)\n} = min\n\u039b N\u2211 r=1 lr(\u039b)\n(4) where \u039b denotes the collection of the model parameters; in regular RNN case, it includes Wrec and W in Figure 1, and in LSTM-RNN case, it includes W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b1, b2, b3 and b4 in Figure 2. D+r is the clicked document for r-th query, P (D+r |Qr) is the probability of clicked document given the r-th query, N is number of query / clickeddocument pairs in the corpus and\nlr(\u039b) = \u2212 log  e\u03b3R(Qr,D+r ) e\u03b3R(Qr,D + r ) + \u2211n i=j e \u03b3R(Qr,D \u2212 r,j)  = log\n1 + n\u2211 j=1 e\u2212\u03b3\u00b7\u2206r,j  (5) where \u2206r,j = R(Qr, D+r ) \u2212 R(Qr, D\u2212r,j), R(\u00b7, \u00b7) was defined earlier in (3), D\u2212r,j is the j-th negative candidate document for r-th query and n denotes the number of negative samples used during training.\nThe expression in (5) is a logistic loss over \u2206r,j . It upperbounds the pairwise accuracy, i.e., the 0 - 1 loss. Since the similarity measure is the cosine function, \u2206r,j \u2208 [\u22122, 2]. To have a larger range for \u2206r,j , we use \u03b3 for scaling. It helps to penalize the prediction error more. Its value is set empirically by experiments on a held out dataset.\nTo train the RNN and LSTM-RNN, we use Back Propagation Through Time (BPTT). The update equations for pa-\nrameter \u039b at epoch k are as follow:\n4\u039bk = \u039bk \u2212\u039bk\u22121 4\u039bk = \u00b5k\u221214\u039bk\u22121 \u2212 k\u22121\u2207L(\u039bk\u22121 + \u00b5k\u221214\u039bk\u22121)\n(6)\nwhere \u2207L(\u00b7) is the gradient of the cost function in (4), is the learning rate and \u00b5k is a momentum parameter determined based on the scheduling scheme used for training. Above equations are equivalent to Nesterov method in (Nesterov, 1983). To see why, please refer to appendix A.1 of (Sutskever et al., 2013) where Nesterov method is derived as a momentum method. The gradient of the cost function, \u2207L(\u039b), is:\n\u2207L(\u039b) = \u2212 N\u2211 r=1 n\u2211 j=1 T\u2211 \u03c4=0\n\u03b1r,j \u2202\u2206r,j,\u03c4 \u2202\u039b\ufe38 \ufe37\ufe37 \ufe38\none large update\n(7)\nwhere T is the number of time steps that we unfold the network over time and\n\u03b1r,j = \u2212\u03b3e\u2212\u03b3\u2206r,j 1 + \u2211n j=1 e \u2212\u03b3\u2206r,j . (8)\n\u2202\u2206r,j,\u03c4 \u2202\u039b in (7) and error signals for different parameters of RNN and LSTM-RNN that are necessary for training are presented in Appendix A. Full derivation of gradients in both models is presented in Appendix B.\nTo accelerate training by parallelization, we use minibatch training and one large update instead of incremental updates during back propagation through time. To resolve the gradient explosion problem we use gradient renormalization method described in (Pascanu et al., 2013; Mikolov et al., 2010). To accelerate the convergence, we use Nesterov method (Nesterov, 1983) and found it effective in training both RNN and LSTM-RNN for sentence embedding.\nWe have used a simple yet effective scheduling for \u00b5k for both RNN and LSTM-RNN models, in the first and last 2% of all parameter updates \u00b5k = 0.9 and for the other 96% of all parameter updates \u00b5k = 0.995. We have used a fixed step size for training RNN and a fixed step size for training LSTM-RNN.\nA summary of training method for LSTM-RNN is presented in Algorithm 1."}, {"heading": "5. Analysis of the Sentence Embedding Process and Performance Evaluation", "text": "To understand how the LSTM-RNN performs sentence embedding, we use visualization tools to analyze the semantic\nAlgorithm 1 Training LSTM-RNN for Sentence Embedding\nInputs: Fixed step size \u201c \u201d, Scheduling for \u201c\u00b5\u201d, Gradient clip threshold \u201cthG\u201d, Maximum number of Epochs \u201cnEpoch\u201d, Total number of query / clickeddocument pairs \u201cN\u201d, Total number of un-clicked (negative) documents for a given query \u201cn\u201d, Maximum sequence length for truncated BPTT \u201cT \u201d. Outputs: Two trained models, one in query side \u201c\u039bQ\u201d, one in document side \u201c\u039bD\u201d. Initialization: Set all parameters in \u039bQ and \u039bD to small random numbers, i = 0, k = 1. procedure LSTM-RNN(\u039bQ,\u039bD)\nwhile i \u2264 nEpoch do for \u201cfirst minibatch\u201d\u2192 \u201clast minibatch\u201d do r \u2190 1 while r \u2264 N do\nfor j = 1\u2192 n do Compute \u03b1r,j . use (8)\nCompute \u2211T \u03c4=0 \u03b1r,j \u2202\u2206r,j,\u03c4 \u2202\u039bk,Q\n. use (14) to (44) in appendix A Compute \u2211T \u03c4=0 \u03b1r,j \u2202\u2206r,j,\u03c4 \u2202\u039bk,D . use (14) to (44) in appendix A sum above terms forQ andD over j\nend for sum above terms forQ andD over r r \u2190 r + 1\nend while Compute\u2207L(\u039bk,Q) . use (7) Compute\u2207L(\u039bk,D) . use (7) if \u2016\u2207L(\u039bk,Q)\u2016 > thG then \u2207L(\u039bk,Q)\u2190 thG \u00b7\n\u2207L(\u039bk,Q) \u2016\u2207L(\u039bk,Q)\u2016\nend if if \u2016\u2207L(\u039bk,D)\u2016 > thG then \u2207L(\u039bk,D)\u2190 thG \u00b7\n\u2207L(\u039bk,D) \u2016\u2207L(\u039bk,D)\u2016\nend if Compute4\u039bk,Q . use (6) Compute4\u039bk,D . use (6) Update: \u039bk,Q \u2190 4\u039bk,Q + \u039bk\u22121,Q Update: \u039bk,D \u2190 4\u039bk,D + \u039bk\u22121,D k \u2190 k + 1\nend for i\u2190 i+ 1\nend while end procedure\nvectors generated by our model. We would like to answer the following questions: (i) How are word dependencies and context information captured? (ii) How does LSTMRNN attenuate unimportant information and detect critical information from the input sentence? Or, how are the keywords embedded into the semantic vector? (iii) How are the global topics identified by LSTM-RNN?\nTo answer these questions, we train the RNN with and without LSTM cells on the click-through dataset consisting of 200, 000 query / clicked-document pairs, which are logged by a commercial web search engine. The training method has been described in Sec. 4. We now proceed to perform a comprehensive analysis by visualizing the trained RNN and LSTM-RNN models. In particular, we will visualize the on-and-off behaviors of the input gates, output gates, cell states, and the semantic vectors in LSTMRNN model, which reveals how the model extracts useful information from the input sentence and embeds it properly into the semantic vector according to the topic information.\nAlthough giving the full learning formula for all the model parameters in the previous section, we will remove the peephole connections and the forget gate from the LSTMRNN model in the current task. This is because the length of each sequence, i.e., the number of words in a query or a document, is known in advance, and we set the state of each cell to zero in the beginning of a new sequence. Therefore, forget gates are not a great help here. Also, as long as the order of words is kept, the precise timing in the sequence is not of great concern, therefore, peephole connections are not that important as well. Removing peephole connections and forget gate will also reduce the amount of training time, since less number of parameters need to be learned."}, {"heading": "5.1. Analysis", "text": "In this section we would like to examine how the information in the input sentence is sequentially extracted and embedded into the semantic vector over time by the LSTMRNN model."}, {"heading": "5.1.1. ATTENUATING UNIMPORTANT INFORMATION", "text": "First, we examine the evolution of the semantic vector and how unimportant words are attenuated. Specifically, we feed the following input sentences from the test dataset into the trained LSTM-RNN model:\n\u2022 Query: \u201chotels in shanghai\u201d\n\u2022 Document: \u201cshanghai hotels accommodation hotel in shanghai discount and reservation\u201d\nActivations of input gate, output gate, cell state and the embedding vector for each cell for query and document are shown in Fig. 4 and Fig. 5, respectively. The vertical axis is the cell index from 1 to 32, and the horizontal axis is the word index from 1 to 10 numbered from left to right in a sequence of words and color codes show activation values. From Figs.4\u20135, we make the following observations:\n\u2022 Semantic representation y(t) and cell states c(t) are evolving over time. Valuable context information is gradually absorbed into c(t) and y(t), so that the information in these two vectors becomes richer over time, and the semantic information of the entire input sentence is embedded into vector y(t), which is obtained by applying output gates to the cell states c(t).\n\u2022 The input gates evolve in such a way that it attenuates the unimportant information and detects the important information from the input sentence. For example, in Fig. 5(a), most of the input gate values corresponding to word 3, word 7 and word 9 have very small values\n(light yellow color)1, which corresponds to the words \u201caccommodation\u201d, \u201cdiscount\u201d and \u201creservation\u201d, respectively, in the document sentence. Interestingly, input gates reduce the effect of these three words in the final semantic representation, y(t), such that the semantic similarity between sentences from query and document sides are not affected by these words."}, {"heading": "5.1.2. KEYWORDS EXTRACTION", "text": "In this section, we show how the trained LSTM-RNN extracts the important information, i.e., keywords, from the input sentences. To this end, we backtrack semantic representations, y(t), over time. We focus on the 10 most active cells in final semantic representation. Whenever there is a large enough change in cell activation value (y(t)), we assume an important keyword has been detected by the model. We illustrate the result using the above example (\u201chotels in shanghai\u201d). The evolution of the 10 most active cells activation, y(t), over time are shown in Figs. 6 and Fig. 7, respectively, for the query and the document sentences.2 From Figs. 6\u20137, we also observe that different words activate different cells. In Tables 1\u20132, we show\n1If this is not clearly visible, please refer to Fig. 13 in section C.3 of Appendix C. We have adjusted color bar for all figures to have the same range, for this reason the structure might not be clearly visible. More visualization examples could also be found in Appendix C of the Supplementary Material\n2Likewise, the vertical axis is the cell index and horizontal axis is the word index in the sentence.\nTable 2. Key words for document: \u201cshanghai hotels accommodation hotel in shanghai discount and reservation\u201d\nshanghai hotels accommodation hotel in shanghai discount and reservation Number of assigned\ncells out of 10 - 4 3 8 1 8 5 3 4\n2 4 6 8 10\n5\n10\n15\n20\n25\n30 \u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n(a) i(t)\n2 4 6 8 10\n5\n10\n15\n20\n25\n30 \u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n(b) c(t)\n2 4 6 8 10\n5\n10\n15\n20\n25\n30 \u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n(c) o(t)\n2 4 6 8 10\n5\n10\n15\n20\n25\n30 \u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n(d) y(t)\nFigure 5. Document: \u201cshanghai hotels accommodation hotel in shanghai discount and reservation\u201d. Since the sentence ends at the ninth word, all the values to the right of it are zero (green color).\nthe number of cells each word activates.3 From the tables, we observe that the keywords activate more cells than the unimportant words, meaning that they are selectively embedded into the semantic vector."}, {"heading": "5.1.3. TOPIC ALLOCATION", "text": "Now, we further show that the trained LSTM-RNN model not only detects the keywords, but also allocates them properly to different cells according to the topics they belong to. To do this, we go through the test dataset using the trained LSTM-RNN model and search for the keywords that are\n3Note that before presenting the first word of the sentence, activation values are initially zero so that there is always a considerable change in the cell states after presenting the first word. For this reason, we have not indicated the number of cells detecting the first word as a keyword. Moreover, another keyword extraction example can be found in Appendix C.\n1 2 3\n2\n4\n6\n8\n10 0\n0.02\n0.04\n0.06\n0.08\nFigure 6. Activation values, y(t), of 10 most active cells for Query: \u201chotels in shanghai\u201d\ndetected by a specific cell. For simplicity, we use the following simple approach: for each given query we look into the keywords that are extracted by the 5 most active cells of LSTM-RNN and list them in Table 10 of the Supplementary material. Interestingly, each cell collects keywords of a specific topic. For example, cell 26 in Table 10 extracts keywords related to the topic \u201cfood\u201d and cells 2 and 6 mainly focus on the keywords related to the topic \u201chealth\u201d."}, {"heading": "5.2. Performance Evaluation", "text": ""}, {"heading": "5.2.1. WEB DOCUMENT RETRIEVAL TASK", "text": "In this section, we apply the proposed sentence embedding method to an important web document retrieval task for a commercial web search engine. Specifically, the RNN models (with and without LSTM cells) embed the sentences from the query and the document sides into their corresponding semantic vectors, and then compute the cosine similarity between these vectors to measure the semantic similarity between the query and candidate documents.\nExperimental results for this task are shown in Table 3 using the standard metric mean Normalized Discounted Cu-\nmulative Gain (NDCG) (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2000) (the higher the better) for evaluating the ranking performance of the RNN and LSTM-RNN on a standalone human-rated test dataset. We also trained several strong baselines, such as DSSM (Huang et al., 2013) and CLSM (Shen et al., 2014), on the same training dataset and evaluated their performance on the same task. For fair comparison, our proposed RNN and LSTM-RNN models are trained with the same number of parameters as the DSSM and CLSM models (14.4M parameters). Besides, we also include in Table 3 two well-known information retrieval (IR) models, BM25 and PLSA, for the sake of benchmarking. The BM25 model uses the bag-of-words representation for queries and documents, which is a state-of-the-art document ranking model based on term matching, widely used as a baseline in IR society. PLSA (Probabilistic Latent Semantic Analysis) is a topic model proposed in (Hofmann, 1999), which is trained using the Maximum A Posterior estimation (Gao et al., 2011) on the documents side from the same training dataset. We experimented with a varying number of topics from 100 to 500 for PLSA, which gives similar performance, and we report in Table 3 the results of using 500 topics. As shown in Table 3, the LSTM-RNN significantly outperforms all these models, and exceeds the best baseline model (CLSM) by 1.3% in NDCG@1 score, which is a statistically significant improvement. As we pointed out in Sec. 5.1, such an improvement comes from the LSTMRNN\u2019s ability to embed the contextual and semantic information of the sentences into a finite dimension vector.\nA comparison between the value of the cost function during training for LSTM-RNN and RNN on the click-through data is shown in Fig. 8. From this figure, we conclude that LSTM-RNN is optimizing the cost function in (4) more effectively. Please note that all parameters of both models are initialized randomly."}, {"heading": "6. Conclusions and Future Work", "text": "This paper addresses deep sentence embedding. We propose a model based on long short term memory to model the long range context information and embed the key information of a sentence in one semantic vector. We show that the semantic vector evolves over time and only takes useful information from any new input. This has been made possible by input gates that detect useless information and attenuate it. Due to general limitation of available human labelled data, we propose training the model with a weak supervision signal using user click-through data of a commercial web search engine.\nBy performing a detailed analysis on the model, we showed that: 1) The proposed model is robust to noise, i.e., it mainly embeds keywords in the final semantic vector representing the whole sentence and 2) In the proposed model, each cell is usually allocated to keywords from a specific topic. These findings have been supported using extensive examples. As a sample application of the proposed sentence embedding method, we evaluated it on the important task of web document retrieval. We showed that, for this task, the proposed method outperforms all existing state of the art methods significantly.\nThis work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al., 2013; Shen et al., 2014; Gao et al., 2014; Jianfeng Gao & Deng, 2014), and it adds further evidence for the effectiveness of these methods. Our future work will further extend the methods to include 1) Developing bi-directional version of the proposed model 2) Using the proposed sentence embedding method for other important machine learning tasks for which sentence embedding has a key role, e.g., the question / answering task. 3) Exploit the prior information about the structure of the different matrices in Fig. 2 to propose a\nmore effective cost function and learning method."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "A. Expressions for the Gradients", "text": "In this appendix we present the final gradient expressions that are necessary to use for training the proposed models. Full derivations of these gradients are presented in Appendix B."}, {"heading": "A.1. RNN", "text": "For the recurrent parameters, \u039b = Wrec (we have ommitted r subscript for simplicity):\n\u2202\u2206j,\u03c4 \u2202Wrec = [\u03b4D + yQ (t\u2212 \u03c4)y T Q(t\u2212 \u03c4 \u2212 1)+\n\u03b4D +\nyD (t\u2212 \u03c4)y T D+(t\u2212 \u03c4 \u2212 1)]\u2212 [\u03b4 D\u2212j yQ (t\u2212 \u03c4)yTQ(t\u2212 \u03c4 \u2212 1)\n+ \u03b4 D\u2212j yD (t\u2212 \u03c4)yTD\u2212j (t\u2212 \u03c4 \u2212 1)] (9)\nwhere D\u2212j means j-th candidate document that is not clicked and\n\u03b4yQ(t\u2212 \u03c4 \u2212 1) = (1\u2212 yQ(t\u2212 \u03c4 \u2212 1))\u25e6 (1 + yQ(t\u2212 \u03c4 \u2212 1)) \u25e6WTrec\u03b4yQ(t\u2212 \u03c4) (10)\nand the same as (10) for \u03b4yD (t \u2212 \u03c4 \u2212 1) with D subscript for document side model. Please also note that:\n\u03b4yQ(TQ) = (1\u2212 yQ(TQ)) \u25e6 (1 + yQ(TQ))\u25e6 (b.c.yD(TD)\u2212 a.b3.c.yQ(TQ)), \u03b4yD (TD) = (1\u2212 yD(TD)) \u25e6 (1 + yD(TD))\u25e6 (b.c.yQ(TQ)\u2212 a.b.c3.yD(TD)) (11)\nwhere\na = yQ(t = TQ) TyD(t = TD)\nb = 1\n\u2016yQ(t = TQ)\u2016 , c =\n1\n\u2016yD(t = TD)\u2016 (12)\nFor the input parameters, \u039b = W:\n\u2202\u2206j,\u03c4 \u2202W = [\u03b4D + yQ (t\u2212 \u03c4)l T 1,Q(t\u2212 \u03c4)+ \u03b4D +\nyD (t\u2212 \u03c4)l T 1,D+(t\u2212 \u03c4)]\u2212\n[\u03b4 D\u2212j yQ (t\u2212 \u03c4)lT1,Q(t\u2212 \u03c4) + \u03b4 D\u2212j yD (t\u2212 \u03c4)lT1,D\u2212j (t\u2212 \u03c4)]\n(13)\nA full derivation of BPTT for RNN is presented in Appendix B.1."}, {"heading": "A.2. LSTM-RNN", "text": "Starting with the cost function in (4), we use the Nesterov method described in (6) to update LSTM-RNN model parameters. Here, \u039b is one of the weight matrices or bias vectors {W1,W2,W3,W4,Wrec1,Wrec2,Wrec3,Wrec4 ,Wp1,Wp2,Wp3,b1,b2,b3,b4} in the LSTM-RNN architecture. The general format of the gradient of the cost function,\u2207L(\u039b), is the same as (7). By definition of \u2206r,j , we have:\n\u2202\u2206r,j \u2202\u039b = \u2202R(Qr, D\n+ r )\n\u2202\u039b \u2212 \u2202R(Qr, Dr,j) \u2202\u039b (14)\nWe omit r and j subscripts for simplicity and present \u2202R(Q,D)\n\u2202\u039b for different parameters of each cell of LSTMRNN in the following subsections. This will complete the process of calculating \u2207L(\u039b) in (7) and then we can use (6) to update LSTM-RNN model parameters. In the subsequent subsections vectors vQ and vD are defined as:\nvQ = (b.c.yD(t = TD)\u2212 a.b3.c.yQ(t = TQ)) vD = (b.c.yQ(t = TQ)\u2212 a.b.c3.yD(t = TD)) (15)\nwhere a, b and c are defined in (12). Full derivation of truncated BPTT for LSTM-RNN model is presented in Appendix B.2."}, {"heading": "A.2.1. OUTPUT GATE", "text": "For recurrent connections we have:\n\u2202R(Q,D)\n\u2202Wrec1 = \u03b4rec1yQ (t).yQ(t\u2212 1) T + \u03b4rec1yD (t).yD(t\u2212 1) T\n(16) where\n\u03b4rec1yQ (t) = oQ(t) \u25e6 (1\u2212 oQ(t)) \u25e6 h(cQ(t)) \u25e6 vQ(t) (17)\nand the same as (17) for \u03b4rec1yD (t) with subscriptD for document side model. For input connections, W1, and peephole connections, Wp1, we will have:\n\u2202R(Q,D)\n\u2202W1 = \u03b4rec1yQ (t).lQ(t) T + \u03b4rec1yD (t).lD(t) T (18)\n\u2202R(Q,D)\n\u2202Wp1 = \u03b4rec1yQ (t).cQ(t) T + \u03b4rec1yD (t).cD(t) T (19)\nThe derivative for output gate bias values will be:\n\u2202R(Q,D)\n\u2202b1 = \u03b4rec1yQ (t) + \u03b4 rec1 yD (t) (20)\nA.2.2. INPUT GATE\nFor the recurrent connections we have:\n\u2202R(Q,D)\n\u2202Wrec3 =\ndiag(\u03b4rec3yQ (t)). \u2202cQ(t)\n\u2202Wrec3 + diag(\u03b4rec3yD (t)).\n\u2202cD(t) \u2202Wrec3 (21)\nwhere\n\u03b4rec3yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec3 + bi,Q(t).yQ(t\u2212 1)T bi,Q(t) = yg,Q(t) \u25e6 iQ(t) \u25e6 (1\u2212 iQ(t)) (22)\nIn equation (21), \u03b4rec3yD (t) and \u2202cD(t) \u2202Wrec3 are the same as (22) with D subscript. For the input connections we will have the following:\n\u2202R(Q,D)\n\u2202W3 =\ndiag(\u03b4rec3yQ (t)). \u2202cQ(t)\n\u2202W3 + diag(\u03b4rec3yD (t)).\n\u2202cD(t)\n\u2202W3 (23)\nwhere\n\u2202cQ(t)\n\u2202W3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W3 + bi,Q(t).xQ(t) T\n(24) For the peephole connections we will have:\n\u2202R(Q,D)\n\u2202Wp3 =\ndiag(\u03b4rec3yQ (t)). \u2202cQ(t)\n\u2202Wp3 + diag(\u03b4rec3yD (t)).\n\u2202cD(t) \u2202Wp3 (25)\nwhere\n\u2202cQ(t) \u2202Wp3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wp3 + bi,Q(t).cQ(t\u2212 1)T (26) For bias values, b3, we will have:\n\u2202R(Q,D)\n\u2202b3 =\ndiag(\u03b4rec3yQ (t)). \u2202cQ(t)\n\u2202b3 + diag(\u03b4rec3yD (t)).\n\u2202cD(t)\n\u2202b3 (27)\nwhere\n\u2202cQ(t)\n\u2202b3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b3 + bi,Q(t) (28)"}, {"heading": "A.2.3. FORGET GATE", "text": "For the recurrent connections we will have:\n\u2202R(Q,D)\n\u2202Wrec2 =\ndiag(\u03b4rec2yQ (t)). \u2202cQ(t)\n\u2202Wrec2 + diag(\u03b4rec2yD (t)).\n\u2202cD(t) \u2202Wrec2 (29)\nwhere\n\u03b4rec2yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec2 + bf,Q(t).yQ(t\u2212 1)T bf,Q(t) = cQ(t\u2212 1) \u25e6 fQ(t) \u25e6 (1\u2212 fQ(t)) (30)\nFor input connections to forget gate we will have:\n\u2202R(Q,D)\n\u2202W2 =\ndiag(\u03b4rec2yQ (t)). \u2202cQ(t)\n\u2202W2 + diag(\u03b4rec2yD (t)).\n\u2202cD(t)\n\u2202W2 (31)\nwhere\n\u2202cQ(t)\n\u2202W2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W2 + bf,Q(t).xQ(t) T\n(32) For peephole connections we have:\n\u2202R(Q,D)\n\u2202Wp2 =\ndiag(\u03b4rec2yQ (t)). \u2202cQ(t)\n\u2202Wp2 + diag(\u03b4rec2yD (t)).\n\u2202cD(t) \u2202Wp2 (33)\nwhere\n\u2202cQ(t) \u2202Wp2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wp2 +bf,Q(t).cQ(t\u22121)T (34) For forget gate\u2019s bias values we will have:\n\u2202R(Q,D)\n\u2202b2 =\ndiag(\u03b4rec2yQ (t)). \u2202cQ(t)\n\u2202b2 + diag(\u03b4rec2yD (t)).\n\u2202cD(t)\n\u2202b2 (35)\nwhere\n\u2202cQ(t)\n\u2202b2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b3 + bf,Q(t) (36)\nA.2.4. INPUT WITHOUT GATING (yg(t))\nFor recurrent connections we will have:\n\u2202R(Q,D)\n\u2202Wrec4 =\ndiag(\u03b4rec4yQ (t)). \u2202cQ(t)\n\u2202Wrec4 + diag(\u03b4rec4yD (t)).\n\u2202cD(t) \u2202Wrec4 (37)\nwhere\n\u03b4rec4yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec4 + bg,Q(t).yQ(t\u2212 1)T\nbg,Q(t) = iQ(t) \u25e6 (1\u2212 yg,Q(t)) \u25e6 (1 + yg,Q(t)) (38)\nFor input connection we have:\n\u2202R(Q,D)\n\u2202W4 =\ndiag(\u03b4rec4yQ (t)). \u2202cQ(t)\n\u2202W4 + diag(\u03b4rec4yD (t)).\n\u2202cD(t)\n\u2202W4 (39)\nwhere \u2202cQ(t)\n\u2202W4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W4 + bg,Q(t).xQ(t) T\n(40) For bias values we will have: \u2202R(Q,D)\n\u2202b4 =\ndiag(\u03b4rec4yQ (t)). \u2202cQ(t)\n\u2202b4 + diag(\u03b4rec4yD (t)).\n\u2202cD(t)\n\u2202b4 (41)\nwhere \u2202cQ(t)\n\u2202b4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b4 + bg,Q(t) (42)"}, {"heading": "A.2.5. ERROR SIGNAL BACKPROPAGATION", "text": "Error signals are back propagated through time using following equations:\n\u03b4rec1Q (t\u2212 1) = [oQ(t\u2212 1) \u25e6 (1\u2212 oQ(t\u2212 1)) \u25e6 h(cQ(t\u2212 1))] \u25e6WTrec1.\u03b4rec1Q (t) (43)\n\u03b4reciQ (t\u2212 1) = [(1\u2212 h(cQ(t\u2212 1))) \u25e6 (1 + h(cQ(t\u2212 1)))\n\u25e6 oQ(t\u2212 1)] \u25e6WTreci .\u03b4 reci Q (t), for i \u2208 {2, 3, 4}\n(44)"}, {"heading": "B. Derivation of BPTT for RNN and LSTM-RNN", "text": "In this appendix we present the full derivation of the gradients for RNN and LSTM-RNN."}, {"heading": "B.1. Derivation of BPTT for RNN", "text": "From (4) and (5) we have:\n\u2202L(\u039b)\n\u2202\u039b = N\u2211 r=1 \u2202lr(\u039b) \u2202\u039b = \u2212 N\u2211 r=1 n\u2211 j=1 \u03b1r,j \u2202\u2206r,j \u2202\u039b\n(45)\nwhere\n\u03b1r,j = \u2212\u03b3e\u2212\u03b3\u2206r,j 1 + \u2211n j=1 e \u2212\u03b3\u2206r,j (46)\nand \u2206r,j = R(Qr, D + r )\u2212R(Qr, Dr,j) (47) We need to find \u2202\u2206r,j\u2202\u039b for input weights and recurrent weights. We omit r subscript for simplicity.\nB.1.1. RECURRENT WEIGHTS\n\u2202\u2206j \u2202Wrec = \u2202R(Q,D+) \u2202Wrec \u2212 \u2202R(Q,D\u2212j ) \u2202Wrec (48)\nWe divide R(D,Q) into three components:\nR(Q,D) = yQ(t = TQ) TyD(t = TD)\ufe38 \ufe37\ufe37 \ufe38 a .\n1\n\u2016yQ(t = TQ)\u2016\ufe38 \ufe37\ufe37 \ufe38 b\n. 1\n\u2016yD(t = TD)\u2016\ufe38 \ufe37\ufe37 \ufe38 c\n(49)\nthen\n\u2202R(Q,D)\n\u2202Wrec =\n\u2202a\n\u2202Wrec .b.c\ufe38 \ufe37\ufe37 \ufe38\nD\n+ a. \u2202b\n\u2202Wrec .c\ufe38 \ufe37\ufe37 \ufe38\nE\n+\na.b. \u2202c\n\u2202Wrec\ufe38 \ufe37\ufe37 \ufe38 F\n(50)\nWe have\nD = \u2202yQ(t = TQ)\nTyD(t = TD).b.c\n\u2202Wrec\n= \u2202yQ(t = TQ)\nTyD(t = TD).b.c\n\u2202yQ(t = TQ) . \u2202yQ(t = TQ) \u2202Wrec +\n\u2202yQ(t = TQ) TyD(t = TD).b.c\n\u2202yD(t = TD) . \u2202yD(t = TD) \u2202Wrec\n= yD(t = TD).b.c. \u2202yQ(t = TQ)\n\u2202Wrec +\nyQ(t = TQ). (b.c) T\ufe38 \ufe37\ufe37 \ufe38\nb.c\n. \u2202yD(t = TD)\n\u2202Wrec (51)\nSince f(.) = tanh(.), using chain rule we have\n\u2202yQ(t = TQ)\nWrec =\n[(1\u2212 yQ(t = TQ)) \u25e6 (1 + yQ(t = TQ))]yQ(t\u2212 1)T (52)\nand therefore\nD = [b.c.yD(t = TD) \u25e6 (1\u2212 yQ(t = TQ))\u25e6 (1 + yQ(t = TQ))]yQ(t\u2212 1)T+ [b.c.yQ(t = TQ) \u25e6 (1\u2212 yD(t = TD))\u25e6 (1 + yD(t = TD))]yD(t\u2212 1)T (53)\nTo find E we use following basic rule:\n\u2202\n\u2202x \u2016x\u2212 a\u20162 = x\u2212 a \u2016x\u2212 a\u20162\n(54)\nTherefore\nE = a.c. \u2202\n\u2202Wrec (\u2016yQ(t = TQ)\u2016)\u22121 =\n\u2212 a.c.(\u2016yQ(t = TQ)\u2016)\u22122. \u2202\u2016yQ(t = TQ)\u2016\n\u2202Wrec\n= \u2212a.c.(\u2016yQ(t = TQ)\u2016)\u22122. yQ(t = TQ) \u2016yQ(t = TQ)\u2016 \u2202yQ(t = TQ) \u2202Wrec\n= \u2212[a.c.b3.yQ(t = TQ) \u25e6 (1\u2212 yQ(t = TQ))\u25e6 (1 + yQ(t = TQ))]yQ(t\u2212 1) (55)\nF is calculated similar to (55):\nF = \u2212[a.b.c3.yD(t = TD) \u25e6 (1\u2212 yD(t = TD))\u25e6 (1 + yD(t = TD))]yD(t\u2212 1) (56)\nConsidering (50),(53),(55) and (56) we have:\n\u2202R(Q,D)\n\u2202Wrec = \u03b4yQ(t)yQ(t\u22121)T+\u03b4yD (t)yD(t\u22121)T (57)\nwhere\n\u03b4yQ(t = TQ) = (1\u2212 yQ(t = TQ)) \u25e6 (1 + yQ(t = TQ))\u25e6 (b.c.yD(t = TD)\u2212 a.b3.c.yQ(t = TQ)), \u03b4yD (t = TD) = (1\u2212 yD(t = TD)) \u25e6 (1 + yD(t = TD))\u25e6 (b.c.yQ(t = TQ)\u2212 a.b.c3.yD(t = TD)) (58)\nEquation (58) will just unfold the network one time step, to unfold it over rest of time steps using backpropagation we have:\n\u03b4yQ(t\u2212 \u03c4 \u2212 1) = (1\u2212 yQ(t\u2212 \u03c4 \u2212 1))\u25e6 (1 + yQ(t\u2212 \u03c4 \u2212 1)) \u25e6WTrec\u03b4yQ(t\u2212 \u03c4), \u03b4yD (t\u2212 \u03c4 \u2212 1) = (1\u2212 yD(t\u2212 \u03c4 \u2212 1))\u25e6 (1 + yD(t\u2212 \u03c4 \u2212 1)) \u25e6WTrec\u03b4yD (t\u2212 \u03c4) (59)\nwhere \u03c4 is the number of time steps that we unfold the network over time which is from 0 to TQ and TD for queries and documents respectively. Now using (48) we have:\n\u2202\u2206j,\u03c4 \u2202Wrec = [\u03b4D + yQ (t\u2212 \u03c4)y T Q(t\u2212 \u03c4 \u2212 1)+\n\u03b4D +\nyD (t\u2212 \u03c4)y T D+(t\u2212 \u03c4 \u2212 1)]\u2212 [\u03b4 D\u2212j yQ (t\u2212 \u03c4)yTQ(t\u2212 \u03c4 \u2212 1)\n+ \u03b4 D\u2212j yD (t\u2212 \u03c4)yTD\u2212j (t\u2212 \u03c4 \u2212 1)] (60)\nTo calculate final value of gradient we should fold back the network over time and use (45), we will have:\n\u2202L(\u039b) \u2202Wrec = \u2212 N\u2211 r=1 n\u2211 j=1 T\u2211 \u03c4=0\n\u03b1r,j,TD,Q \u2202\u2206r,j,\u03c4 \u2202Wrec\ufe38 \ufe37\ufe37 \ufe38\none large update\n(61)\nB.1.2. INPUT WEIGHTS\nUsing a similar procedure we will have the following for input weights:\n\u2202R(Q,D)\n\u2202W = \u03b4yQ(t\u2212\u03c4)l1,Q(t\u2212\u03c4)T+\u03b4yD (t\u2212\u03c4)l1,D(t\u2212\u03c4)T\n(62) where\n\u03b4yQ(t\u2212 \u03c4) = (1\u2212 yQ(t\u2212 \u03c4)) \u25e6 (1 + yQ(t\u2212 \u03c4))\u25e6 (b.c.yD(t\u2212 \u03c4)\u2212 a.b3.c.yQ(t\u2212 \u03c4)), \u03b4yD (t\u2212 \u03c4) = (1\u2212 yD(t\u2212 \u03c4)) \u25e6 (1 + yD(t\u2212 \u03c4))\u25e6 (b.c.yQ(t\u2212 \u03c4)\u2212 a.b.c3.yD(t\u2212 \u03c4)) (63)\nTherefore:\n\u2202\u2206j,\u03c4 \u2202W = [\u03b4D +\nyQ (t\u2212 \u03c4)l T 1,Q(t\u2212 \u03c4) + \u03b4D\n+\nyD (t\u2212 \u03c4)l T 1,D+(t\u2212 \u03c4)]\u2212\n[\u03b4 D\u2212j yQ (t\u2212 \u03c4)lT1,Q(t\u2212 \u03c4) + \u03b4 D\u2212j yD (t\u2212 \u03c4)lT1,D\u2212j (t\u2212 \u03c4)]\n(64)\nand therefore:\n\u2202L(\u039b)\n\u2202W = \u2212 N\u2211 r=1 n\u2211 j=1 T\u2211 \u03c4=0\n\u03b1r,j \u2202\u2206r,j,\u03c4 \u2202W\ufe38 \ufe37\ufe37 \ufe38\none large update\n(65)"}, {"heading": "B.2. Derivation of BPTT for LSTM-RNN", "text": "Following from (50) for every parameter, \u039b, in LSTMRNN architecture we have:\n\u2202R(Q,D)\n\u2202\u039b =\n\u2202a\n\u2202\u039b .b.c\ufe38 \ufe37\ufe37 \ufe38\nD\n+ a. \u2202b\n\u2202\u039b .c\ufe38 \ufe37\ufe37 \ufe38\nE\n+ a.b. \u2202c\n\u2202\u039b\ufe38 \ufe37\ufe37 \ufe38 F\n(66)\nand from (51):\nD = yD(t = TD).b.c. \u2202yQ(t = TQ)\n\u2202\u039b +\nyQ(t = TQ).b.c. \u2202yD(t = TD)\n\u2202\u039b (67)\nFrom (55) and (56) we have:\nE = \u2212a.c.b3.yQ(t = TQ) \u2202yQ(t = TQ)\n\u2202\u039b (68)\nF = \u2212a.b.c3.yD(t = TD) \u2202yD(t = TD)\n\u2202\u039b (69)\nTherefore\n\u2202R(Q,D)\n\u2202\u039b = D + E + F =\nvQ \u2202yQ(t = TQ)\n\u2202\u039b + vD\n\u2202yD(t = TD)\n\u2202\u039b (70)\nwhere\nvQ = (b.c.yD(t = TD)\u2212 a.b3.c.yQ(t = TQ)) vD = (b.c.yQ(t = TQ)\u2212 a.b.c3.yD(t = TD)) (71)"}, {"heading": "B.2.1. OUTPUT GATE", "text": "Since \u03b1 \u25e6 \u03b2 = diag(\u03b1)\u03b2 = diag(\u03b2)\u03b1 where diag(\u03b1) is a diagonal matrix whose main diagonal entries are entries of vector \u03b1, we have:\n\u2202y(t)\n\u2202Wrec1 =\n\u2202\n\u2202Wrec1 (diag(h(c(t))).o(t))\n= \u2202diag(h(c(t)))\n\u2202Wrec1\ufe38 \ufe37\ufe37 \ufe38 zero\n.o(t) + diag(h(c(t))). \u2202o(t)\n\u2202Wrec1\n= o(t) \u25e6 (1\u2212 o(t)) \u25e6 h(c(t)).y(t\u2212 1)T (72)\nSubstituting (72) in (70) we have:\n\u2202R(Q,D)\n\u2202Wrec1 = \u03b4rec1yQ (t).yQ(t\u2212 1) T + \u03b4rec1yD (t).yD(t\u2212 1) T\n(73) where\n\u03b4rec1yQ (t) = oQ(t) \u25e6 (1\u2212 oQ(t)) \u25e6 h(cQ(t)) \u25e6 vQ(t) \u03b4rec1yD (t) = oD(t) \u25e6 (1\u2212 oD(t)) \u25e6 h(cD(t)) \u25e6 vD(t) (74)\nwith a similar derivation for W1 and Wp1 we get:\n\u2202R(Q,D)\n\u2202W1 = \u03b4rec1yQ (t).lQ(t) T + \u03b4rec1yD (t).lD(t) T (75)\n\u2202R(Q,D)\n\u2202Wp1 = \u03b4rec1yQ (t).cQ(t) T + \u03b4rec1yD (t).cD(t) T (76)\nFor output gate bias values we have:\n\u2202R(Q,D)\n\u2202b1 = \u03b4rec1yQ (t) + \u03b4 rec1 yD (t) (77)\nB.2.2. INPUT GATE\nSimilar to output gate we start with:\n\u2202y(t)\n\u2202Wrec3 =\n\u2202\n\u2202Wrec3 (diag(o(t)).h(c(t)))\n= \u2202diag(o(t))\n\u2202Wrec3\ufe38 \ufe37\ufe37 \ufe38 zero\n.h(c(t)) + diag(o(t)). \u2202h(c(t))\n\u2202Wrec3\n= diag(o(t)).(1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) \u2202c(t) \u2202Wrec3\n(78)\nTo find \u2202c(t)\u2202Wrec3 assuming f(t) = 1 (we derive formulation for f(t) 6= 1 from this simple solution) we have:\nc(0) = 0\nc(1) = c(0) + i(1) \u25e6 yg(1) = i(1) \u25e6 yg(1) c(2) = c(1) + i(2) \u25e6 yg(2) . . .\nc(t) = t\u2211 k=1 i(k) \u25e6 yg(k) = t\u2211 k=1 diag(yg(k)).i(k) (79)\nTherefore\n\u2202c(t)\n\u2202Wrec3 = t\u2211 k=1 [ \u2202diag(yg(k))\nWrec3\ufe38 \ufe37\ufe37 \ufe38 zero\n.i(k) + diag(yg(k)). \u2202i(k)\nWrec3 ]\n= t\u2211 k=1 diag(yg(k)).i(k) \u25e6 (1\u2212 i(k)).y(k \u2212 1)T (80)\n(81)\nand\n\u2202y(t)\n\u2202Wrec3 = t\u2211 k=1 [o(t) \u25e6 (1\u2212 h(c(t))) \u25e6 (1 + h(c(t)))\ufe38 \ufe37\ufe37 \ufe38 a(t) \u25e6 yg(k) \u25e6 i(k) \u25e6 (1\u2212 i(k))\ufe38 \ufe37\ufe37 \ufe38 b(k) ]y(k \u2212 1)T (82)\nBut this is expensive to implement, to resolve it we have:\n\u2202y(t)\n\u2202Wrec3 = t\u22121\u2211 k=1\n[a(t) \u25e6 b(k)]y(k \u2212 1)T\ufe38 \ufe37\ufe37 \ufe38 expensive part\n+ [a(t) \u25e6 b(t)]y(t\u2212 1)T\n= diag(a(t)) t\u22121\u2211 k=1\nb(k).y(k \u2212 1)T\ufe38 \ufe37\ufe37 \ufe38 \u2202c(t\u22121) \u2202Wrec3\n+ diag(a(t)).b(t).y(t\u2212 1)T (83)\nTherefore\n\u2202y(t)\n\u2202Wrec3 = [diag(a(t))][ \u2202c(t\u2212 1) \u2202Wrec3\n+ b(t).y(t\u2212 1)T ] (84)\nFor f(t) 6= 1 we have\n\u2202y(t)\n\u2202Wrec3 = [diag(a(t))][diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec3\n+ bi(t).y(t\u2212 1)T ] (85)\nwhere\na(t) = o(t) \u25e6 (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) bi(t) = yg(t) \u25e6 i(t) \u25e6 (1\u2212 i(t)) (86)\nsubstituting above equation in (70) we will have:\n\u2202R(Q,D)\n\u2202Wrec3 = diag(\u03b4rec3yQ (t)).\n\u2202cQ(t)\n\u2202Wrec3\n+ diag(\u03b4rec3yD (t)). \u2202cD(t)\n\u2202Wrec3 (87)\nwhere\n\u03b4rec3yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec3 + bi,Q(t).yQ(t\u2212 1)T bi,Q(t) = yg,Q(t) \u25e6 iQ(t) \u25e6 (1\u2212 iQ(t)) (88)\nIn equation (87), \u03b4rec3yD (t) and \u2202cD(t) \u2202Wrec3 are the same as (88) with D subscript. Therefore, update equations for Wrec3 are (87), (88) for Q and D and (6).\nWith a similar procedure for W3 we will have the following:\n\u2202R(Q,D)\n\u2202W3 = diag(\u03b4rec3yQ (t)).\n\u2202cQ(t)\n\u2202W3\n+ diag(\u03b4rec3yD (t)). \u2202cD(t)\n\u2202W3 (89)\nwhere\n\u2202cQ(t)\n\u2202W3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W3 + bi,Q(t).xQ(t) T\n(90) Therefore, update equations for W3 are (89), (90) for Q and D and (6).\nFor peephole connections we will have:\n\u2202R(Q,D)\n\u2202Wp3 = diag(\u03b4rec3yQ (t)).\n\u2202cQ(t)\n\u2202Wp3\n+ diag(\u03b4rec3yD (t)). \u2202cD(t)\n\u2202Wp3 (91)\nwhere\n\u2202cQ(t) \u2202Wp3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wp3 + bi,Q(t).cQ(t\u2212 1)T (92) Hence, update equations for Wp3 are (91), (92) for Q and D and (6).\nFollowing similar derivation for bias values b3 we will have:\n\u2202R(Q,D)\n\u2202b3 = diag(\u03b4rec3yQ (t)).\n\u2202cQ(t)\n\u2202b3\n+ diag(\u03b4rec3yD (t)). \u2202cD(t)\n\u2202b3 (93)\nwhere\n\u2202cQ(t)\n\u2202b3 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b3 + bi,Q(t) (94)\nUpdate equations for b3 are (93), (94) forQ andD and (6)."}, {"heading": "B.2.3. FORGET GATE", "text": "For forget gate, with a similar derivation to input gate we will have\n\u2202y(t)\n\u2202Wrec2 = [diag(a(t))][diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec2 + bf (t).y(t\u2212 1)T ] (95)\nwhere\na(t) = o(t) \u25e6 (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) bf (t) = c(t\u2212 1) \u25e6 f(t) \u25e6 (1\u2212 f(t)) (96)\nsubstituting above equation in (70) we will have:\n\u2202R(Q,D)\n\u2202Wrec2 = diag(\u03b4rec2yQ (t)).\n\u2202cQ(t)\n\u2202Wrec2\n+ diag(\u03b4rec2yD (t)). \u2202cD(t)\n\u2202Wrec2 (97)\nwhere\n\u03b4rec2yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec2 + bf,Q(t).yQ(t\u2212 1)T bf,Q(t) = cQ(t\u2212 1) \u25e6 fQ(t) \u25e6 (1\u2212 fQ(t)) (98)\nTherefore, update equations for Wrec2 are (97), (98) for Q and D and (6).\nFor input weights to forget gate, W2, we have\n\u2202R(Q,D)\n\u2202W2 = diag(\u03b4rec2yQ (t)).\n\u2202cQ(t)\n\u2202W2\n+ diag(\u03b4rec2yD (t)). \u2202cD(t)\n\u2202W2 (99)\nwhere\n\u2202cQ(t)\n\u2202W2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W2 + bf,Q(t).xQ(t) T\n(100) Therefore, update equations for W2 are (99), (100) for Q and D and (6).\nFor peephole connections, Wp2, we have\n\u2202R(Q,D)\n\u2202Wp2 = diag(\u03b4rec2yQ (t)).\n\u2202cQ(t)\n\u2202Wp2\n+ diag(\u03b4rec2yD (t)). \u2202cD(t)\n\u2202Wp2 (101)\nwhere\n\u2202cQ(t) \u2202Wp2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wp2 +bf,Q(t).cQ(t\u22121)T (102) Therefore, update equations for Wp2 are (101), (102) for Q and D and (6).\nUpdate equations for forget gate bias values, b2, will be following equations and (6):\n\u2202R(Q,D)\n\u2202b2 = diag(\u03b4rec2yQ (t)).\n\u2202cQ(t)\n\u2202b2\n+ diag(\u03b4rec2yD (t)). \u2202cD(t)\n\u2202b2 (103)\nwhere\n\u2202cQ(t)\n\u2202b2 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b3 + bf,Q(t) (104)\nB.2.4. INPUT WITHOUT GATING (yg(t))\nGradients for yg(t) parameters are as follow:\n\u2202y(t)\n\u2202Wrec4 = [diag(a(t))][diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec4 + bg(t).y(t\u2212 1)T ] (105)\nwhere\na(t) = o(t) \u25e6 (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) bg(t) = i(t) \u25e6 (1\u2212 yg(t)) \u25e6 (1 + yg(t)) (106)\nsubstituting above equation in (70) we will have:\n\u2202R(Q,D)\n\u2202Wrec4 = diag(\u03b4rec4yQ (t)).\n\u2202cQ(t)\n\u2202Wrec4\n+ diag(\u03b4rec4yD (t)). \u2202cD(t)\n\u2202Wrec4 (107)\nwhere\n\u03b4rec4yQ (t) = (1\u2212 h(cQ(t))) \u25e6 (1 + h(cQ(t))) \u25e6 oQ(t) \u25e6 vQ(t) \u2202cQ(t) \u2202Wrec4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202Wrec4 + bg,Q(t).yQ(t\u2212 1)T bg,Q(t) = iQ(t) \u25e6 (1\u2212 yg,Q(t)) \u25e6 (1 + yg,Q(t)) (108)\nTherefore, update equations for Wrec4 are (107), (108) for Q and D and (6).\nFor input weight parameters, W4, we have\n\u2202R(Q,D)\n\u2202W4 = diag(\u03b4rec4yQ (t)).\n\u2202cQ(t)\n\u2202W4\n+ diag(\u03b4rec4yD (t)). \u2202cD(t)\n\u2202W4 (109)\nwhere\n\u2202cQ(t)\n\u2202W4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202W4 + bg,Q(t).xQ(t) T\n(110) Therefore, update equations for W4 are (109), (110) for Q and D and (6).\nGradients with respect to bias values, b4, are\n\u2202R(Q,D)\n\u2202b4 = diag(\u03b4rec4yQ (t)).\n\u2202cQ(t)\n\u2202b4\n+ diag(\u03b4rec4yD (t)). \u2202cD(t)\n\u2202b4 (111)\nwhere\n\u2202cQ(t)\n\u2202b4 = diag(fQ(t)). \u2202cQ(t\u2212 1) \u2202b4 + bg,Q(t) (112)\nTherefore, update equations for b4 are (111), (112) for Q andD and (6). There is no peephole connections for yg(t)."}, {"heading": "C. LSTM-RNN Visualization", "text": "In this appendix we present more examples of LSTM-RNN visualization."}, {"heading": "C.1. LSTM-RNN Semantic Vectors: Another Example", "text": "Consider the following example from test dataset:\n\u2022 Query: \u201chow to fix bath tub wont turn off\u201d\n\u2022 Document: \u201chow do you paint a bathtub and what paint should you use yahoo answers\ufe38 \ufe37\ufe37 \ufe38\ntreated as one word\n\u201d\nActivations of input gate, output gate, cell state and cell output for each cell for query and document are presented in Fig.9 and Fig.10 respectively based on a trained LSTMRNN model.\nThree interesting observations from Fig.9 and Fig.10:\n\u2022 Semantic representation y(t) and cell states c(t) are evolving over time.\n\u2022 In part (a) of Fig.10, we observe that input gate values for most of the cells corresponding to word 3, word 4, word 7 and word 9 in document side of LSTM-RNN have very small values (light blue color). These are corresponding to words \u201cyou\u201d, \u201cpaint\u201d, \u201cand\u201d and \u201cpaint\u201d respectively in the document title. Interestingly, input gates are trying to reduce effect of these\nwords in the final representation (y(t)) because the LSTM-RNN model is trained to maximize the similarity between query and document if they are a good match.\n\u2022 y(t) is used as semantic representation after applying output gate on cell states. Note that valuable context information is stored in cell states c(t)."}, {"heading": "C.2. Key Word Extraction: Another Example", "text": "Evolution of 10 most active cells over time for the second example are presented in Fig. 11 for query and Fig. 12 for document. Number of assigned cells out of 10 most active cells to each word are presented in Table 4 and Table 5.\nC.3. A more clear figure for input gate for \u201chotels in shanghai\u201d example\nIn this section we present a more clear figure for part (a) of Fig. 5 that shows the structure of the input gate for document side of \u201chotels in shanghai\u201d example. As it is clearly visible from this figure, the input gate values for most of the cells corresponding to word 3, word 7 and word\n9 in document side of LSTM-RNN have very small values (light blue color). These are corresponding to words \u201caccommodation\u201d, \u201cdiscount\u201d and \u201creservation\u201d respectively in the document title. Interestingly, input gates are trying to reduce effect of these three words in the fi-\nnal representation (y(t)) because the LSTM-RNN model is trained to maximize the similarity between query and document if they are a good match.\nC.4. A Closer Look at RNNs with and without LSTM Cells in Web Document Retrieval Task\nIn this section we further show examples to reveal the advantage of LSTM-RNN sentence embedding compared to the RNN sentence embedding.\nFirst, we compare the scores assigned by trained RNN and LSTM-RNN to our \u201chotels in shanghai\u201d example. On average, each query in our test dataset is associated with 15 web documents (URLs). Each query / document pair has a relevance label which is human generated. These relevance labels are \u201cBad\u201d, \u201cFair\u201d, \u201cGood\u201d and \u201cExcellent\u201d. This example is rated as a \u201cGood\u201d match in the dataset. The score for this pair assigned by RNN is \u201c0.8165\u201d while the score assigned by LSTM-RNN is \u201c0.9161\u201d. Please note that the score is between 0 and 1. This means that the score assigned by LSTM-RNN is more correspondent with the human generated label.\nSecond, we compare the number of assigned neurons and cells to each word by RNN and LSTM-RNN respectively. To do this, we rely on the 10 most active cells and neurons in the final semantic vectors in both models. Results are presented in Table 6 and Table 7 for query and document respectively. An interesting observation is that RNN sometimes assigns neurons to unimportant words, e.g., 6 neurons are assigned to the word \u201cin\u201d in Table 7.\nAs another example we consider the query, \u201chow to fix bath tub wont turn off\u201d. This example is rated as a \u201cBad\u201d match in the dataset by human. It is good to know that the score for this pair assigned by RNN is \u201c0.7016\u201d while the score assigned by LSTM-\nRNN is \u201c0.5944\u201d. This shows the score generated by LSTM-RNN is closer to human generated label.\nNumber of assigned neurons and cells to each word by RNN and LSTM-RNN are presented in Table 8 and Table 9 for query and document. This is out of 10 most active neurons and cells in the semantic vector of RNN and LSTMRNN. Examples of RNN assigning neurons to unimportant words are 3 neurons to the word \u201ca\u201d and 4 neurons to the word \u201cyou\u201d in Table 9."}, {"heading": "C.5. Topic Allocation", "text": "As discussed in the paper, in LSTM-RNN each cell is attracted mainly to keywords of similar topics. This is shown in Table 10 for two topics, \u201cfood\u201d and \u201chealth\u201d.\nTa bl\ne 10\n.K ey\nw or\nds as\nsi gn\ned to\nea ch\nce ll\nof L\nST M\n-R N\nN fo\nrd iff\ner en\ntq ue\nri es\nof tw\no to\npi cs\n,\u201c fo\nod \u201d\nan d\n\u201ch ea\nlth \u201d\nQ ue\nry ce\nll 1\nce ll\n2 ce\nll 3\nce ll\n4 ce\nll 5\nce ll\n6 ce\nll 7\nce ll\n8 ce\nll 9\nce ll\n10 ce\nll 11\nce ll\n12 ce\nll 13\nce ll 14 al yo yo sa uc e yo sa uc e sa uc e\nat ki\nns di\net la\nsa gn\na di et bl en de rr ec ip es\nca ke\nba ke\nry ed\nin bu\nrg h\nba ke ry ca nn in g co rn be ef ha sh be ef ,h as h to rr e de pi zz a fa m ou s de ss er ts de ss er ts fr ie d ch ic ke n ch ic ke n ch ic ke n sm ok ed tu rk ey re ci pe s ita lia n sa us ag e ho ag ie s sa us ag e do yo u ge ta lle rg y al le rg y m uc h pa in w ill af te rt ot al kn ee re pl ac em en t pa in pa in ,k ne e ho w to m ak e w hi te rt ee th\nm ak\ne, te\net h\nill in\nic om\nm un\nity ho\nsp ita\nl co\nm m\nun ity\n,h os\npi ta\nl ho\nsp ita\nl co\nm m\nun ity\nim pl\nan ti\nnf ec\ntio n\nin fe\nct io\nn in\nfe ct\nio n\nin tr\nod uc\nto ry\nps yc\nho lo\ngy ps\nyc ho\nlo gy\nps yc\nho lo gy na rc ot ic s du ri ng pr eg na nc y si de ef fe ct s pr eg na nc y pr eg na nc y, ef fe ct s,\ndu ri\nng du\nri ng\nfig ht\nsi nu\ns in\nfe ct\nio ns\nin fe\nct io ns he al th in su ra nc e hi gh bl oo d pr es su re in su ra nc e bl oo d\nhi gh\n,b lo od al la nt id ep re ss an tm ed ic at io ns an tid ep re ss an t, m ed ic at io ns Q ue ry ce ll 15 ce ll 16 ce ll 17 ce ll 18 ce ll 19 ce ll 20 ce ll 21 ce ll 22 ce ll 23 ce ll\n24 ce\nll 25\nce ll\n26 ce\nll 27\nce ll\n28 ce\nll 29\nal yo\nyo sa\nuc e\nat ki\nns di\net la\nsa gn\na di et bl en de rr ec ip es\nre ci\npe s\nca ke\nba ke\nry ed\nin bu\nrg h\nba ke\nry ba\nke ry\nca nn\nin g\nco rn\nbe ef\nha sh\nco rn\n,b ee f to rr e de pi zz a pi zz\na pi\nzz a\nfa m\nou s\nde ss\ner ts\nfr ie\nd ch\nic ke\nn ch\nic ke n sm ok ed tu rk ey re ci pe s tu rk ey re ci pe s ita lia n sa us ag e ho ag ie s ho ag ie s sa us ag e sa us ag e do yo u ge ta lle rg\ny m uc h pa in w ill af te rt ot al kn\nee re\npl ac\nem en\nt kn\nee re\npl ac\nem en t ho w to m ak e w hi te rt ee th to\nw hi\nte r\nill in\nic om\nm un\nity ho\nsp ita\nl ho\nsp ita l im pl an ti nf ec tio n in\nfe ct\nio n\nin tr\nod uc\nto ry\nps yc\nho lo\ngy ps\nyc ho\nlo gy\nna rc\not ic\ns du\nri ng\npr eg\nna nc\ny si\nde ef\nfe ct s fig ht si nu s in fe ct io\nns si\nnu s,\nin fe\nct io\nns in\nfe ct\nio ns\nhe al\nth in\nsu ra\nnc e\nhi gh\nbl oo\nd pr\nes su\nre hi\ngh ,p\nre ss\nur e\nal la\nnt id\nep re\nss an\ntm ed\nic at\nio ns\nan tid\nep re\nss an t Q ue ry ce ll 30 ce ll 31 ce ll 32 al yo yo sa uc e at ki ns di et la sa gn a di et bl en de rr ec ip es ca ke ba ke ry ed in bu rg h ca nn in g co rn be ef ha sh to rr e de pi zz a fa m ou s de ss er ts fr ie d ch ic ke n sm ok ed tu rk ey re ci pe s ita lia n sa us ag e ho ag ie s do yo u ge ta lle rg y\nm uc\nh pa\nin w\nill af\nte rt\not al\nkn ee\nre pl\nac em\nen t\nho w\nto m\nak e\nw hi\nte rt\nee th\nill in\nic om\nm un\nity ho\nsp ita\nl ho\nsp ita l im pl an ti nf ec tio n in tr od uc to ry ps yc ho lo gy\nna rc\not ic\ns du\nri ng\npr eg\nna nc\ny si\nde ef\nfe ct s fig ht si nu s in fe ct io\nns he al th in su ra nc e hi gh bl oo d\npr es\nsu re\nin su\nra nc\ne, hi gh al la nt id ep re ss an tm ed ic at io ns m ed ic at io ns"}], "references": [{"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Proc. ICASSP,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A primal-dual method for training recurrent neural networks constrained by the echo-state property", "author": ["Chen", "Jianshu", "Deng", "Li"], "venue": "In Proceedings of the International Conf. on Learning Representations (ICLR),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Large vocabulary continuous speech recognition with contextdependent DBN-HMMs", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "In Proc. IEEE ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2011}, {"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "Yu", "Dong", "Deng", "Li", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Sequence classification using highlevel features extracted from deep neural networks", "author": ["L. Deng", "J. Chen"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng and Chen,? \\Q2014\\E", "shortCiteRegEx": "Deng and Chen", "year": 2014}, {"title": "Analysis of the correlation structure for a neural predictive model with application to speech recognition", "author": ["L. Deng", "K. Hassanein", "M. Elmasry"], "venue": "Neural Networks,", "citeRegEx": "Deng et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Deng et al\\.", "year": 1994}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Clickthrough-based latent semantic models for web search", "author": ["Gao", "Jianfeng", "Toutanova", "Kristina", "Yih", "Wen-tau"], "venue": "SIGIR \u201911,", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao", "Jianfeng", "Pantel", "Patrick", "Gamon", "Michael", "He", "Xiaodong", "Deng", "Li", "Shen", "Yelong"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "Jrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gers et al\\.", "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "In Representation Learning Workshp,", "citeRegEx": "Graves,? \\Q2012\\E", "shortCiteRegEx": "Graves", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In Proc. ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Probabilistic latent semantic analysis", "author": ["Hofmann", "Thomas"], "venue": "Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["J\u00e4rvelin", "Kalervo", "Kek\u00e4l\u00e4inen", "Jaana"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2000}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Deng", "Li"], "venue": "In Proc. ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML 2013,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Robinson,? \\Q1994\\E", "shortCiteRegEx": "Robinson", "year": 1994}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew", "Beaufays", "Fran\u00e7oise"], "venue": "In Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH),", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "A latent semantic model with convolutional-pooling structure for information", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gregoire"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George E", "Hinton", "Geoffrey E"], "venue": "In ICML (3)\u201913,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Deep learning and its applications to signal and information processing [exploratory dsp", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 29, "context": "By mapping texts into a unified semantic representation, the embedding vector can be further used for different applications, such as machine translation (Sutskever et al., 2014), sentiment analysis (Le & Mikolov, 2014), and information retrieval (Huang et al.", "startOffset": 154, "endOffset": 178}, {"referenceID": 27, "context": ", 2013) and CLSM (Shen et al., 2014) models, developed for information retrieval, can also be interpreted as sentence embedding methods.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "It has been successfully applied to speech recognition, which achieves state-of-art performance (Graves et al., 2013; Sak et al., 2014).", "startOffset": 96, "endOffset": 135}, {"referenceID": 26, "context": "It has been successfully applied to speech recognition, which achieves state-of-art performance (Graves et al., 2013; Sak et al., 2014).", "startOffset": 96, "endOffset": 135}, {"referenceID": 29, "context": "In the machine translation work (Sutskever et al., 2014), an input English sentence is converted into a vector representation using LSTM-RNN, and then another LSTM-RNN is used to generate an output French sentence.", "startOffset": 32, "endOffset": 56}, {"referenceID": 25, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 5, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 20, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 12, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 0, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 19, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 27, "context": "This is also different from (Shen et al., 2014) where the sliding window of a fixed size (akin to an FIR filter) is used to capture local features and a max-pooling layer on the top to capture global features.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in (Hochreiter & Schmidhuber, 1997) as Long Short Term Memory (LSTM) and completed in (Gers et al., 1999) and (Gers et al.", "startOffset": 203, "endOffset": 222}, {"referenceID": 11, "context": ", 1999) and (Gers et al., 2003) by adding forget gate and peephole connections to the architecture.", "startOffset": 12, "endOffset": 31}, {"referenceID": 28, "context": "1 of (Sutskever et al., 2013) where Nesterov method is derived as a momentum method.", "startOffset": 5, "endOffset": 29}, {"referenceID": 24, "context": "To resolve the gradient explosion problem we use gradient renormalization method described in (Pascanu et al., 2013; Mikolov et al., 2010).", "startOffset": 94, "endOffset": 138}, {"referenceID": 20, "context": "To resolve the gradient explosion problem we use gradient renormalization method described in (Pascanu et al., 2013; Mikolov et al., 2010).", "startOffset": 94, "endOffset": 138}, {"referenceID": 27, "context": ", 2013) and CLSM (Shen et al., 2014), on the same training dataset and evaluated their performance on the same task.", "startOffset": 17, "endOffset": 36}, {"referenceID": 8, "context": "PLSA (Probabilistic Latent Semantic Analysis) is a topic model proposed in (Hofmann, 1999), which is trained using the Maximum A Posterior estimation (Gao et al., 2011) on the documents side from the same training dataset.", "startOffset": 150, "endOffset": 168}, {"referenceID": 2, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 3, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 6, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 27, "context": ", 2012) and in semantic modeling (Huang et al., 2013; Shen et al., 2014; Gao et al., 2014; Jianfeng Gao & Deng, 2014), and it adds further evidence for the effectiveness of these methods.", "startOffset": 33, "endOffset": 117}, {"referenceID": 9, "context": ", 2012) and in semantic modeling (Huang et al., 2013; Shen et al., 2014; Gao et al., 2014; Jianfeng Gao & Deng, 2014), and it adds further evidence for the effectiveness of these methods.", "startOffset": 33, "endOffset": 117}], "year": 2015, "abstractText": "This paper develops a model that addresses sentence embedding using recurrent neural networks (RNN) with Long Short Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model automatically attenuates the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These keyword detection and topic allocation tasks enabled by the LSTM-RNN allow the network to perform web document retrieval, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform all existing state of the art methods. Proceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}