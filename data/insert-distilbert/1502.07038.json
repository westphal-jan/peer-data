{"id": "1502.07038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing", "abstract": "we develop novel first - and second - hand order features for dependency parsing based on the google syntactic ngrams corpus, a collection structured of subtree counts of parsed sentences from scanned older books. we also extend previous work on surface $ n $ - gram dictionary features from us web1t to the open google books corpus and from first - order to arbitrary second - amendment order, comparing and analysing performance over newswire and web treebanks.", "histories": [["v1", "Wed, 25 Feb 2015 03:27:38 GMT  (30kb,D)", "http://arxiv.org/abs/1502.07038v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominick ng", "mohit bansal", "james r curran"], "accepted": false, "id": "1502.07038"}, "pdf": {"name": "1502.07038.pdf", "metadata": {"source": "CRF", "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing", "authors": ["Dominick Ng", "Mohit Bansal", "James R. Curran"], "emails": ["@-lab,", "dominick.ng@sydney.edu.au", "james.r.curran@sydney.edu.au", "mbansal@ttic.edu"], "sections": [{"heading": null, "text": "We develop novel first- and second-order features for dependency parsing based on the Google Syntactic Ngrams corpus, a collection of subtree counts of parsed sentences from scanned books. We also extend previous work on surface n-gram features from Web1T to the Google Books corpus and from first-order to second-order, comparing and analysing performance over newswire and web treebanks.\nSurface and syntactic n-grams both produce substantial and complementary gains in parsing accuracy across domains. Our best system combines the two feature sets, achieving up to 0.8% absolute UAS improvements on newswire and 1.4% on web text."}, {"heading": "1 Introduction", "text": "Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (Brants and Franz, 2006) have proven to be useful proxies for syntax (Bansal and Klein, 2011; Pitler, 2012), but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences. Longer n-grams are also noisier and sparser, limiting the range of potential features.\nIn this paper we develop new features for the graph-based MSTParser (McDonald and Pereira, 2006) from the Google Syntactic Ngrams corpus (Goldberg and Orwant, 2013), a collection of Stanford dependency subtree counts. These features capture information collated across millions of subtrees\nproduced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task. We compare the performance of our syntactic n-gram features against the surface n-gram features of Bansal and Klein (2011) in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies. We also extend the first-order surface n-gram features to second-order, and compare the utility of Web1T and the Google Books Ngram corpus (Lin et al., 2012) as surface n-gram sources.\nWe find that surface and syntactic n-grams provide statistically significant and complementary accuracy improvements in- and out-of-domain. Our best LTH system combines the two feature sets to achieve 92.5% unlabeled attachment score (UAS) on newswire and 85.2% UAS averaged over web text on a baseline of 91.7% and 83.8%. Our analysis shows that the combined system is able to draw upon the strengths of both surface and syntactic features whilst avoiding their weaknesses.\n2 Syntactic n-gram Features\nThe Google Syntactic Ngrams English (2013) corpus1 contains counts of dependency tree fragments over a 345 billion word selection of the Google Books data, parsed with a beam-search shift-reduce parser and Stanford dependencies (Goldberg and Orwant, 2013). The parser is trained over substantially more annotated data than typically used in dependency parsing.\nUnlike surface n-grams, syntactic n-grams are not restricted to linear word order or affected by non-\n1 commondatastorage.googleapis.com/books/syntactic-ngrams\nar X\niv :1\n50 2.\n07 03\n8v 1\n[ cs\n.C L\n] 2\n5 Fe\nb 20\nsyntactic co-occurrence. Given a head-argument ambiguity, we extract different combinations of word, POS tag, and directionality, and search the Syntactic Ngrams corpus for matching subtrees. To reduce the impact of this search during run time, we extract all possible combinations in the training and test corpora ahead of time and total the frequencies of each configuration, storing these in a lookup table that is used by the parser at run-time to compute feature values. We did not use any features based on the dependency label as these are assigned in a separate pass in MSTParser.\nTable 1 summarizes the first-order features extracted from the dependency hold \u2192 hearing depicted in Figure 1. The final feature encodes the POS tags of the head and argument, directionality, the binned distance between the head and argument, and a bucketed frequency of the syntactic n-gram calculated as per Equation 1, creating bucket labels from 0 in increments of 5 (0, 5, 10, etc.).\nbucket = \u230a\nlog2(\u2211 frequency) 5\n\u230b \u00d75 (1)\nAdditional features for each bucket value up to the maximum are also encoded. We also develop paraphrase-style features like those of Bansal and Klein (2011) based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).\nFigure 1 depicts the potential context words available the hold \u2192 hearing dependency.\nWe experiment with a number of second-order features, mirroring those extracted for surface ngrams in Section 3.3. We extract all triple and sibling word and POS structures considered by the parser in the training and test corpora (following the factorization depicted in Figure 2), and counted their frequency in the Syntactic Ngrams corpus. Importantly, we require that matching subtrees in the Syntactic Ngrams corpus maintain the position of the parent relative to its children. We generate separate features encoding the word and POS tag variants of each triple and sibling structure.\nSimilar to the surface n-gram features (Section 3), counts for our syntactic n-gram features are precomputed to improve the run-time efficiency of the parser. Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors.\n3 Surface n-gram Features\nBansal and Klein (2011) demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain.\nWe extract Bansal and Klein (2011)\u2019s affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their\nperformance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend Bansal and Klein\u2019s affinity and paraphrase features to second-order.\n3.1 Surface n-gram Corpora\nThe Web1T corpus contains counts of 1 to 5-grams over 1 trillion words of web text (Brants and Franz, 2006). Unigrams must appear at least 200 times in the source text before being included in the corpus, while longer n-grams have a cutoff of 40. Pitler et al. (2010) has documented a number of sources of noise in the corpus, including duplicate sentences (such as legal disclaimers and boilerplate text), disproportionately short or long sentences, and primarily alphanumeric sentences.\nThe Google Books Ngrams English corpus (2012) contains counts of 1 to 5-grams over 468 billion words sourced from scanned books published across three centuries (Michel et al., 2011). A uniform cutoff of 40 applies to all n-grams in this corpus. This corpus is affected by the accuracy of OCR and digitization tools; the changing typography of books across time is one issue that may create spurious cooccurrences and counts (Lin et al., 2012).\n3.2 First-order surface n-gram features\nAffinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005; Bansal and Klein, 2011). N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser. When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five. The total count is then discretized as per Equation 1 previously.\nThe final parser features include the POS tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency. Additional cumulative features are gener-\nated using each bucket from the pre-calculated up to the maximum bucket size.\nParaphrase-style surface n-gram features attempt to infer attachments indirectly. Nakov and Hearst (2005) propose several static patterns to resolve a variety of nominal and prepositional attachment ambiguities. For example, they give the example of sentence (1) below, paraphrase it into sentence (2), and examine how frequent the paraphrase is. If it should happen sufficiently often, this serves as evidence for the nominal attachment to demands in sentence (1) rather than the verbal attachment to meet .\n1. meet demands from customers 2. meet the customers demands\nIn Bansal and Klein (2011), paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus. For each attachment ambiguity, 3-grams of the form (? q1 q2), (q1 ? q2), and (q1 q2 ?) are extracted, where q1 and q2 are the head and argument in their linear order of appearance in the original sentence, and ? is any single context word appearing before, in between, or after the query words. Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument2.\nGiven the arc hold \u2192 hearing in Figure 2, public is the most frequent word appearing in the n-gram (hold ? hearing) in Web1T. Thus, the final encoded feature is POS (hold) \u2227 POS (hearing) \u2227 public \u2227 mid. Further generalization is achieved by using a unigram POS tagger trained on the WSJ data to tag each context word, and encoding features using each unique tag of the most frequent context words.\n3.3 Second-order surface n-gram features\nWe extend the first-order surface n-gram features to new features over second-order structures. We experimented with triple and sibling features, reflecting the second-order factorization used in MSTParser (see Figure 2).\nAs with first-order features, we convert all triple and sibling structures from the training and test\n2The top 20 words in between and top 5 words before and after are used for all paraphrase-style features in this paper.\ndata into query n-grams, maintaining their linear order. In Figure 2, the corresponding n-grams are hold hearing Tuesday , and hearing Tuesday . We then scan the n-gram corpus for each query n-gram and sum the frequency of each. Frequencies are summed over each configuration (including intervening words) that the query n-gram may appear in, as depicted below.\n\u2022 (q1 q2 q3) \u2022 (q1 ? q2 q3) \u2022 (q1 q2 ? q3)\n\u2022 (q1 ? q2 ? q3) \u2022 (q1 ? ? q2 q3) \u2022 (q1 q2 ? ? q3)\nwhere q1, q2, and q3 are the words of the triple in their linear order, and ? is a single intervening word of any kind.\nWe encode the POS tags of the parent and children (or just the children for sibling features), along with the bucketed count, directionality, and the binned distance between the two children. We also extract paraphrase-style features for siblings in the same way as for first-order n-grams, and cumulative variants up to the maximum bucket size."}, {"heading": "4 Experimental Setup", "text": "As with Bansal and Klein (2011) and Pitler (2012), we convert the Penn Treebank to dependencies using pennconverter3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MXPOST (Ratnaparkhi, 1996). We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing. The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (Petrov and McDonald, 2012) were converted to LTH and used for out-of-domain evaluation. We used MSTParser (McDonald and Pereira, 2006), trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc. We omit labeled attachment\n3 http://nlp.cs.lth.se/software/treebank_converter/\nscores in this paper for brevity, but they are consistent with the reported UAS scores."}, {"heading": "5 Results", "text": "Table 2 summarizes our results over the WSJ development and test datasets, and the SANCL 2012 test datasets. All of our features perform very similarly to one another: each feature set in isolation provides a roughly 0.5% UAS improvement over the baseline parser on the WSJ development and test sections. On the out-of-domain web treebank, surface and syntactic features each improve over the baseline by an average of roughly 0.8 \u2013 1.0% on the test sets. All of our results are also statistically significant improvements over the baseline.\nWhile our syntactic n-gram counts are computed over Stanford dependencies and almost certainly include substantial parser and OCR errors, they still provide a significant performance improvement for LTH parsing. Additionally, the Syntactic Ngrams dataset is drawn from a wide variety of genres, but helps with both newswire and web text parsing.\nThe best results on LTH dependencies used second-order sibling features in addition to the firstorder features for both surface and syntactic ngrams. A combined system of Google Books surface n-gram features and syntactic n-gram features (which performed individually best on the development set) produces absolute UAS improvements of 0.8% over the baseline on the WSJ test set, and 1.4% over the baseline averaged across the three web tree-\n0 100 200 300 400 500 600\nVBG\nTO\nCC\nRB\nVBD\nCD\nJJ\nNNS\nDT\nIN\nNNP\nNN\n# errors\nBaseline Web1T Books Syntactic Combined\nFigure 3: Total LTH attachment errors by gold argument POS tag, sorted by the total tag frequency.\nbank testing domains. These results are significantly higher than any feature set in isolation, showing that surface and syntactic n-gram features are complementary and individually address different types of errors being made by the parser."}, {"heading": "6 Analysis", "text": "Figure 3 gives an error breakdown by highfrequency gold argument POS tag on LTH dependencies for the baseline, Web1T surface n-grams, syntactic n-grams, and combined systems reported in Table 2. For almost every POS tag, the combined system outperforms the baseline and makes equal or fewer errors than either the surface or syntactic n-gram features in isolation. Syntactic n-grams are worse relative to surface n-grams on noun, adjectival, and prepositional parts of speech \u2013 constructions which are known to be difficult to parse. Without NP-bracketed training data or the extra features\nTag Freq BASE COMB % NN 5725 5433 5470 12.0 NNP 4043 3810 3843 10.7 IN 4026 3457 3513 18.2 DT 3511 3425 3431 2.0 NNS 2504 2344 2379 11.4 JJ 2472 2314 2335 6.8 CD 1845 1736 1739 1.0 VBD 1705 1579 1606 8.8 RB 1308 1091 1106 4.9 CC 1000 848 850 0.7 VB 983 941 947 2.0 TO 868 766 784 5.8 VBN 850 783 792 2.9 VBZ 705 636 638 0.7 PRP 612 604 606 0.7 VBG 588 500 511 3.6 POS 428 422 422 0.0 $ 352 345 343 -0.7 MD 344 307 313 2.0 VBP 341 298 305 2.3 PRP$ 288 281 280 -0.3 Other 1010 868 883 4.9\nTable 3: Correct attachments by gold argument POS tag and the percentage of the overall error reduction over WSJ section 22 for the baseline and combined systems in Table 2.\nthat we have discussed as helping resolve these issues, it is unsurprising that syntactic n-gram features using the counts from the Goldberg and Orwant (2013) parser are less effective. In comparison, surface n-grams are worse on conjunctive and verbal parts of speech, suggesting that the localized nature of these features is less useful for the idiosyncrasies of coordination representations and longerrange subject/object relationships.\nWhilst Web1T and Google Books features perform similarly overall, Books n-grams are more effective for noun structures, and Web1T n-grams are slightly better in predicting PP attachment sites.\nTable 3 lists a complete breakdown of correct attachments corrected by the combined system. The most substantial gains come in nominal and prepositional phrases \u2013 known weaknesses for parsers, and the categories where syntactic n-gram features\nalone fare worst. However, the system finds less improvement in coordinators, determiners, and cardinal numbers, all of which are also components of noun phrases. This shows the difficulty of correctly identifying a head noun in a nominal to attach modifiers to, and the general difficulty of representing and parsing coordination.\nWeb1T contains approximately double the total number of n-grams as Google Books. Table 4 shows that 27% and 32.5% of the n-gram queries from the WSJ sections 2-23 and the entire English Web Treebank do not receive features from Web1T and Google Books respectively. The intersection of these queries is 24.7% of the total, showing that the two corpora have small but substantive differences in word distributions; this may partially explain why our combined feature experiments work so well. However, the similar performance of surface n-gram features extracted from these sources suggests Web1T contains substantial noise.\nWe had expected our syntactic n-gram features to perform better than they did since they address many of the shortcomings of using surface n-grams. Syntactic features are sensitive to the quality of the parser used to produce them, but in this case the parser is difficult to assess as the source corpus is enormous and extracted using OCR from scanned books. Even if the parser is state of the art, it is being used to parse diverse texts spanning multiple genres across a wide time period, compounded by potential scanning and digitization errors. Additionally, a post-hoc analysis of the types of errors present in the corpus is impossible due to the exclusion of the full parse trees, though Goldberg and Orwant (2013) note that this data would almost certainly be computationally prohibitive to process. Despite this, our work has shown that counts from this corpus provide useful features for parsing. Futhermore, these\nfeatures stack with surface n-gram features, providing substantial overall performance improvements."}, {"heading": "6.1 Future Work", "text": "A combination of features from all of the sources used in this work would be interesting avenues for further investigation, especially since these features seem strongly complementary. We could also explore more of the POS and head-modifier annotations available in the Google Books Ngram corpus to develop features which are a middle ground between surface and syntactic n-gram features.\nThe Google Books and Syntactic Ngrams corpora both provide frequencies by date, and it would be interesting to explore how well features extracted from different date ranges would perform \u2013 particularly on text from roughly the same eras. Resampling Web1T to reduce it to a comparable corpus that is the same size as Google Books would also provide better insights on how many n-grams are noise."}, {"heading": "7 Related Work", "text": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from Bansal and Klein (2011), other feature-based approaches to improving dependency parsing include Pitler (2012), who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors. Chen et al. (2013) describe a novel way of generating meta-features that work to emphasise important feature types used by the parser.\nChen et al. (2009) generate subtree-based features that are similar to ours. However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger. They also use the same underlying parser to generate the BLLIP subtree counts and as the final test-time parser, while Syntactic Ngrams is parsed with a simpler, shift-reduce parser compared to the graph-based MSTParser used during test time. They also evaluate only on newswire text, whilst our work systematically explores various configurations of surface and syntactic n-gram features in- and outof-domain."}, {"heading": "8 Conclusion", "text": "We developed features for dependency parsing using subtree counts from 345 billion parsed words of scanned English books. We extended existing work on surface n-grams from first to second-order, and investigated the utility of web text and scanned books as sources of surface n-grams.\nOur individual feature sets all perform similarly, providing significant improvements in parsing accuracy of about 0.5% on newswire and up to 1.0% averaged across the web treebank domains. They are also complementary, with our best system combining surface and syntactic n-gram features to achieve up to 1.3% UAS improvements on newswire and 1.6% on web text. We hope that our work will encourage further efforts to unify different sources of unlabeled and automatically parsed data for dependency parsing, addressing the relative strengths and weaknesses of each source."}], "references": [{"title": "Web-Scale Features for Full-Scale Parsing", "author": ["Mohit Bansal", "Dan Klein."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 693\u2013702.", "citeRegEx": "Bansal and Klein.,? 2011", "shortCiteRegEx": "Bansal and Klein.", "year": 2011}, {"title": "Improving Dependency Parsing with Subtrees from Auto-Parsed Data", "author": ["Wenliang Chen", "Jun\u2019ichi Kazama", "Kiyotaka Uchimoto", "Kentaro Torisawa"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "SemiSupervised Feature Transformation for Dependency Parsing", "author": ["Wenliang Chen", "Min Zhang", "Yue Zhang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303\u20131313.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books", "author": ["Yoav Goldberg", "Jon Orwant."], "venue": "Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 241\u2013247.", "citeRegEx": "Goldberg and Orwant.,? 2013", "shortCiteRegEx": "Goldberg and Orwant.", "year": 2013}, {"title": "Extended Constituent-to-dependency Conversion for English", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "Proceedings of the 16th Nordic Conference of Computational Linguistics, pages 105\u2013112.", "citeRegEx": "Johansson and Nugues.,? 2007", "shortCiteRegEx": "Johansson and Nugues.", "year": 2007}, {"title": "Online Learning of Approximate Dependency Parsing Algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 81\u201388.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Using the Web as an Implicit Training Set: Application to Structural Ambiguity Resolution", "author": ["Preslav Nakov", "Marti Hearst."], "venue": "Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Nakov and Hearst.,? 2005", "shortCiteRegEx": "Nakov and Hearst.", "year": 2005}, {"title": "Overview of the 2012 Shared Task on Parsing the Web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on the Syntactic Analysis of NonCanonical Language.", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations", "author": ["Emily Pitler."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 768\u2013776.", "citeRegEx": "Pitler.,? 2012", "shortCiteRegEx": "Pitler.", "year": 2012}, {"title": "Using Web-scale N-grams to Improve Base NP Parsing Performance", "author": ["Emily Pitler", "Shane Bergsma", "Dekang Lin", "Kenneth Church."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 886\u2013894.", "citeRegEx": "Pitler et al\\.,? 2010", "shortCiteRegEx": "Pitler et al\\.", "year": 2010}, {"title": "A Maximum Entropy Model for Part-of-Speech Tagging", "author": ["Adwait Ratnaparkhi."], "venue": "Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing, pages 133\u2013142.", "citeRegEx": "Ratnaparkhi.,? 1996", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Exploiting the WWW as a corpus to resolve PP attachment ambiguities", "author": ["Martin Volk."], "venue": "Proceedings of the Corpus Linguistics 2001 Conference, pages 601\u2013 606.", "citeRegEx": "Volk.,? 2001", "shortCiteRegEx": "Volk.", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Features from n-gram counts over resources like Web1T (Brants and Franz, 2006) have proven to be useful proxies for syntax (Bansal and Klein, 2011; Pitler, 2012), but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.", "startOffset": 123, "endOffset": 161}, {"referenceID": 8, "context": "Features from n-gram counts over resources like Web1T (Brants and Franz, 2006) have proven to be useful proxies for syntax (Bansal and Klein, 2011; Pitler, 2012), but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.", "startOffset": 123, "endOffset": 161}, {"referenceID": 5, "context": "In this paper we develop new features for the graph-based MSTParser (McDonald and Pereira, 2006) from the Google Syntactic Ngrams corpus", "startOffset": 68, "endOffset": 96}, {"referenceID": 3, "context": "(Goldberg and Orwant, 2013), a collection of Stanford dependency subtree counts.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "formance of our syntactic n-gram features against the surface n-gram features of Bansal and Klein (2011) in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies.", "startOffset": 173, "endOffset": 200}, {"referenceID": 0, "context": "formance of our syntactic n-gram features against the surface n-gram features of Bansal and Klein (2011) in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies.", "startOffset": 81, "endOffset": 105}, {"referenceID": 3, "context": "The Google Syntactic Ngrams English (2013) corpus1 contains counts of dependency tree fragments over a 345 billion word selection of the Google Books data, parsed with a beam-search shift-reduce parser and Stanford dependencies (Goldberg and Orwant, 2013).", "startOffset": 228, "endOffset": 255}, {"referenceID": 0, "context": "We also develop paraphrase-style features like those of Bansal and Klein (2011) based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.", "startOffset": 56, "endOffset": 80}, {"referenceID": 0, "context": "We extract Bansal and Klein (2011)\u2019s affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their", "startOffset": 11, "endOffset": 35}, {"referenceID": 8, "context": "Pitler et al. (2010) has documented a number of sources of noise in the corpus, including duplicate sentences (such as legal disclaimers and boilerplate text), dispropor-", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005; Bansal and Klein, 2011).", "startOffset": 156, "endOffset": 204}, {"referenceID": 0, "context": "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005; Bansal and Klein, 2011).", "startOffset": 156, "endOffset": 204}, {"referenceID": 6, "context": "Nakov and Hearst (2005) propose several static patterns to resolve a variety of nominal and prepositional attachment ambiguities.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "In Bansal and Klein (2011), paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus.", "startOffset": 3, "endOffset": 27}, {"referenceID": 4, "context": "As with Bansal and Klein (2011) and Pitler (2012), we convert the Penn Treebank to dependencies using pennconverter3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MXPOST (Ratnaparkhi, 1996).", "startOffset": 117, "endOffset": 145}, {"referenceID": 10, "context": "As with Bansal and Klein (2011) and Pitler (2012), we convert the Penn Treebank to dependencies using pennconverter3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MXPOST (Ratnaparkhi, 1996).", "startOffset": 197, "endOffset": 216}, {"referenceID": 7, "context": "The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (Petrov and McDonald, 2012) were converted to LTH and used for out-of-domain evaluation.", "startOffset": 129, "endOffset": 156}, {"referenceID": 5, "context": "We used MSTParser (McDonald and Pereira, 2006), trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc.", "startOffset": 18, "endOffset": 46}, {"referenceID": 0, "context": "As with Bansal and Klein (2011) and Pitler (2012), we convert the Penn Treebank to dependencies using pennconverter3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MXPOST (Ratnaparkhi, 1996).", "startOffset": 8, "endOffset": 32}, {"referenceID": 0, "context": "As with Bansal and Klein (2011) and Pitler (2012), we convert the Penn Treebank to dependencies using pennconverter3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MXPOST (Ratnaparkhi, 1996).", "startOffset": 8, "endOffset": 50}, {"referenceID": 3, "context": "that we have discussed as helping resolve these issues, it is unsurprising that syntactic n-gram features using the counts from the Goldberg and Orwant (2013) parser are less effective.", "startOffset": 132, "endOffset": 159}, {"referenceID": 3, "context": "Additionally, a post-hoc analysis of the types of errors present in the corpus is impossible due to the exclusion of the full parse trees, though Goldberg and Orwant (2013) note that this data would almost certainly be computationally prohibitive to process.", "startOffset": 146, "endOffset": 173}, {"referenceID": 11, "context": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from Bansal and Klein (2011), other feature-based approaches to improving dependency parsing in-", "startOffset": 99, "endOffset": 135}, {"referenceID": 6, "context": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from Bansal and Klein (2011), other feature-based approaches to improving dependency parsing in-", "startOffset": 99, "endOffset": 135}, {"referenceID": 0, "context": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from Bansal and Klein (2011), other feature-based approaches to improving dependency parsing in-", "startOffset": 147, "endOffset": 171}, {"referenceID": 6, "context": "clude Pitler (2012), who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.", "startOffset": 6, "endOffset": 20}, {"referenceID": 1, "context": "Chen et al. (2013) describe a novel way of generating meta-features that work to emphasise important feature types used by the parser.", "startOffset": 0, "endOffset": 19}], "year": 2015, "abstractText": "We develop novel firstand second-order features for dependency parsing based on the Google Syntactic Ngrams corpus, a collection of subtree counts of parsed sentences from scanned books. We also extend previous work on surface n-gram features from Web1T to the Google Books corpus and from first-order to second-order, comparing and analysing performance over newswire and web treebanks. Surface and syntactic n-grams both produce substantial and complementary gains in parsing accuracy across domains. Our best system combines the two feature sets, achieving up to 0.8% absolute UAS improvements on newswire and 1.4% on web text.", "creator": "LaTeX with hyperref package"}}}