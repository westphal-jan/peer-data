{"id": "1502.07058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval", "abstract": "this paper presents a new state - of - the - art for document image classification and retrieval, using features learned by deep convolutional neural activation networks ( cnns ). in object and scene analysis, deep neural nets are capable method of precisely learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. the current current work explores this capacity in the realm of document analysis, and confirms that this representation technique strategy is superior to a variety of popular hand - crafted alternatives. experiments also show that ( i ) features extracted from cnns are robust to compression, ( ii ) synthetic cnns trained especially on non - document images transfer resources well to document analysis tasks, and ( iii ) enforcing region - specific feature - learning coding is unnecessary given sufficient training data. this work also makes available a new labelled subset of the iit - cdip collection, containing 400, 000 document images across 16 categories, useful for training new customized cnns for document analysis.", "histories": [["v1", "Wed, 25 Feb 2015 05:58:43 GMT  (5256kb,D)", "http://arxiv.org/abs/1502.07058v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG cs.NE", "authors": ["adam w harley", "alex ufkes", "konstantinos g derpanis"], "accepted": false, "id": "1502.07058"}, "pdf": {"name": "1502.07058.pdf", "metadata": {"source": "CRF", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval", "authors": ["Adam W. Harley", "Alex Ufkes", "Konstantinos G. Derpanis"], "emails": ["kosta}@scs.ryerson.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMany document types have a distinct visual style. For example, \u201cletter\u201d documents are typically written in a standard format, which is recognizable even at scales where the text is unreadable. Motivated by this observation, this paper addresses the problem of document classification and retrieval, based on the visual structure and layout of document images.\nContent-based analysis of document images has a number of applications. In digital libraries, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means basic image analysis is the only available tool for initial indexing and classification [26]. As a pre-processing stage, document image analysis can facilitate and improve OCR by providing information about each document\u2019s visual layout [10]. Furthermore, document information that is lost in OCR, such as typeface, graphics, and layout, can only be stored and indexed using images or image descriptors. Therefore, image analysis is complementary to OCR at several stages of document analysis.\nThe challenge of document image analysis arises from the fact that within each document type, there exists a wide range of visual variability. For example, of the correspondence documents shown in Figure 1, no two documents share the exact same spatial arrangement of header, date, address, body, and signature; some of the documents even omit these components entirely. This level of intra-class variability renders spatial layout analysis difficult, and rigid template matching impossible [8]. Another issue is that documents of different categories often have substantial visual similarities. For instance, there exist advertisements that look like news articles, and questionnaires that look like forms, and so on. From\nthe perspective of \u201cvisual styles\u201d, some erroneous retrievals in such circumstances may be justifiable, but in general the task of document image analysis is to effectively classify and retrieve documents despite intra-class variability, and interclass similarity.\nSimilar challenges appear in other fields, such as object recognition and scene classification. In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29]. Inspired by the success of CNNs in other domains, this paper presents an extensive evaluation of CNNs for document classification and retrieval. In the end, it is determined that features extracted from deep CNNs exceed the performance of all popular alternative features on both classification and retrieval, by a large margin. Experiments are also presented on transfer learning, which demonstrate that CNNs trained on object recognition learn features that are surprisingly effective at describing documents. Furthermore, it is found that the deep net strategy is not significantly improved by additional guidance toward region-focused features, suggesting that a CNN trained on whole images may already be capable of learning some amount of the information that region-based analysis would add."}, {"heading": "A. Related Work", "text": "In the past twenty years of document image analysis, research has oscillated from region-based analysis to whole image analysis, and simultaneously, from handcrafted features to machine-learned ones.\nThe power of region-based analysis of document images\nar X\niv :1\n50 2.\n07 05\n8v 1\n[ cs\n.C V\n] 2\n5 Fe\nb 20\n15\nhas been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18]. In general, this approach assumes that many document types have a distinct and consistent configuration of visually-identifiable components. For example, formal business letters typically share a particular spatial configuration of letterhead, date and salutation. To some extent, the classification of perfectly rigid documents (e.g., forms) can be reduced to the problem of template matching [7], and less-rigid document types (e.g., letters) can similarly be classified by fitting the geometric configuration of the document\u2019s components to one of several template configurations, via geometric transformations [15]. The drawback of this approach is that it requires the manual definition of a template for each document type to be categorized. Furthermore, the approach is limited to documents for which a template definition is possible. For documents with more flexible structures, as considered herein, template-based approaches are inapplicable.\nAn alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \u201clandmark\u201d features that may appear anywhere in the document [32, 31]. This strategy is sometimes called a \u201cbag of visual words\u201d approach, since it describes images with a histogram over an orderless vocabulary of features [12]. For example, a landmark feature discriminating letters from most other document classes is the salutation: finding a salutation in a document (potentially through OCR) is a good cue that the document is a letter, regardless of that feature\u2019s exact spatial position [32]. The advantage of holistic analysis is that the resulting representation of documents is invariant to the geometric configuration of the features. This approach has therefore been successful in retrieving and classifying a broader range of documents than the templatebased approaches, although the approach is less discriminating in the domain of rigid-template documents.\nRecently, there have been attempts to bridge the gap between region-based and holistic analyses. By concatenating image features pooled at several stages, beginning with a whole-image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics [23]. This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind [22]. For document retrieval, this type of representation represents the current state-of-the-art.\nAt the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9]. A popular area of research in this domain concerns the task of learning document structure. This typically involves training a decision tree to navigate the various possible geometric configurations of fixed features (i.e. \u201clandmarks\u201d) within each document type, toward the goal of structure-based classification [10, 21]. Most recently, it was shown that the entire pipeline of supervised document image classification, from feature-building to decision making, can be learned by a convolutional neural network (CNN) [17]. In that work, the authors reported a remarkable 22% increase in classification accuracy compared to the previous best reported on the same\ndataset, which had used spatial pyramid matching. However, the CNN approach has not yet been applied to document retrieval.\nA shift toward machine-learned features has been taking place in other areas of computer vision as well. In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13]. The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill-suited, such as attribute detection, and fine-grained object recognition [29]. The success of CNNs in fine-grained object recognition is especially relevant to document image analysis, since the two fields share some significant challenges, e.g., (i) the items being distinguished are very similar to each other, and (ii) there do not exist problem-specific datasets large enough to train a powerful CNN without causing it to overfit. It makes sense, therefore, to draw inspiration from fine-grained object recognition research on how to overcome these challenges.\nTwo major points on the training and usage of CNNs can be gleaned from fine-grained classification research. First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6]. This regularization technique addresses the issue of overfitting, and allows large CNNs to be effectively applied to small problems. Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33]. More generally, this second point suggests that it is unnecessary to rely entirely on machine learning, especially when human knowledge can be easily implemented in the system. This paper seeks to investigate whether these insights are relevant to document image analysis.\nFinally, CNNs in other domains have recently been extended to the task of image retrieval. After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations [24]. Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2]. The present work is the first to apply this idea toward document retrieval."}, {"heading": "B. Contributions", "text": "In the light of previous work, this paper makes the following contributions. First, the paper thoroughly evaluates the power of deep CNN features for representing document images. Toward this end, the paper presents experiments in CNN design, training, feature processing, and compression. Results show that features extracted from CNNs are superior to all handcrafted competitors, and furthermore can be compressed to very short codes with negligible loss in performance. Second, this work demonstrates that CNNs trained on nondocument images transfer well to document-related tasks. Third, this paper explores a strategy of embedding human knowledge of document structure into CNN architectures, by guiding an ensemble of CNNs toward learning region-specific\nfeatures. Interestingly, results show little to no improvement in classification and retrieval after this augmentation, suggesting that a basic holistic CNN may be learning region-specific features (or perhaps better features) automatically. Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents [25], containing 400,000 document images across 16 categories."}, {"heading": "II. TECHNICAL APPROACH", "text": "In structured documents, the layout of text and graphics elements often reflects important information about genre. Therefore, documents of a category often share region-specific features. This paper attempts to learn these informative features by training either a single holistic CNN or an ensemble of region-based CNNs. Additionally, the paper explores two different initialization strategies: the first initializes the weights of the CNNs randomly, and relies entirely on the training process to find the features; the second transfers weights from a network trained on another task, and relies on training only to fine-tune the features to the domain of document analysis."}, {"heading": "A. Holistic convolutional neural networks", "text": "In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19]. A typical network of this type has approximately 60 million trainable parameters; this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network\u2019s power as a featurebuilder and classifier [24].\nConvolutional neural network activations are not geometrically invariant. In applications such as object detection, this is sometimes an inconvenient property. Much work has been done to add spatial invariance to CNNs, e.g., by \u201cjittering\u201d the training data to add geometric variants of each image in the dataset [24], or by altering the architecture of the CNN to process the input at multiple scales and positions [14]. For document analysis, however, spatial specificity in CNN activations may be beneficial. For example, it makes sense to treat the header region of a document differently than the footer region. By design, a holistic CNN trained on a dataset of well-aligned document images should be capable of learning region-specific features automatically.\nTypically, CNNs are trained to perform a classification task, but a CNN trained on classification can be exploited to perform retrieval also. It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors [29]. These feature vectors are high-dimensional (e.g., 4096 dimensions), but their dimensionality can be reduced significantly via principal component analysis (e.g., to 128 dimensions) without significantly affecting their discriminative power [3]. Retrieval involves computing the Euclidean distance between a query descriptor and every descriptor of the training set. The sorted distances are then used to rank the images of the training data, and return a sorted list of documents."}, {"heading": "B. Region-based guidance", "text": "Accounting for the possibility that a holistic CNN may not take advantage of region-specific information in document images, guiding CNNs to learn region-based features may aid fine-grained discrimination by isolating subtle region-specific appearance differences between document categories. Consider the example of discriminating letters and memos, as illustrated in Figure 2. These two categories only consistently differ at the \u201caddress\u201d section; memos have a short \u201cTo\u201d and \u201cFrom\u201d, and letters have full addresses. It is possible that a holistic CNN will learn this automatically, but training a CNN to classify documents using only this region increases the likelihood that this feature will be learned. The idea of this approach is to devote one CNN to each region of interest, and therefore force multiple CNNs to learn rich region dependent representations, from which features can be extracted and combined.\nAny number of region-specific CNNs can be used in this approach. In this work, a total of five CNNs are used. Four of these are region-tuned, placed at the header, left body, right body, and footer of the document images. The fifth is a holistic CNN, trained on the entire images. The final region-based representation of document images is built by combining and compressing features extracted from each region-tuned CNN. The final descriptor is represented by the concatenation of region specific features: [\u03c60, \u03c61, . . . , \u03c6n], where \u03c60 represents the PCA-compressed feature vector extracted from the holistic CNN, and \u03c61, . . . , \u03c6n represent the analogous vectors extracted from regions 1 through n. Figure 3 illustrates the full process of this vector\u2019s construction. For retrieval, this new vector is used directly. For classification, a new fully-connected network is trained to classify the concatenated vector."}, {"heading": "C. Transfer learning", "text": "The goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data [1]. In the context of CNNs, transfer learning can be implemented at the weight initialization step. The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 [24]. An alternative strategy is to pretrain the network network on a complementary task, which potentially has more training data than the target task. This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process [29]. A popular choice for pre-training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories [30]. Features extracted from an\nImageNet-trained network have been shown to be effective general-purpose features in a variety of other vision challenges, even without fine-tuning on the target problem [29].\nThis paper studies three questions about transfer learning for document analysis. First, the paper investigates whether the ImageNet features are general enough to be applied to documents. That is, with no fine-tuning on documents, are generic object-recognition features applicable to document analysis? Second, the paper addresses the question of whether the initialization provided by pre-training on the ILSVRC challenge provides better results than random initialization for documentclassifying CNNs. Third, the paper seeks to investigate the usefulness of transfer learning between document categories; if a CNN is trained with a small number of document categories, are the features learned in that process useful for discriminating between unseen document categories? These questions will be answered in the retrieval tasks to follow."}, {"heading": "III. EMPIRICAL EVALUATION", "text": ""}, {"heading": "A. Datasets", "text": "The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection [25]. This collection contains high resolution images of scanned documents, collected from public records of lawsuits against American tobacco companies. In total, the database has over seven million documents, hand-labelled with tags. Often, the first tag of a document image is indicative of the document\u2019s category, but many documents in the dataset have missing or erroneous tags.\nThe first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work [20]. This version of the dataset was used in a number of related papers [20, 22, 17]. Each image has one of ten labels. There are an uneven\nnumber of images per category, with the largest proportion of images in the \u201cletter\u201d category. The distribution of categories is representative of the distribution present in the full dataset.\nThe second version of the dataset, listed in the results as BigTobacco, is a new random sample of 25000 images from each of 16 categories in the IIT CDIP collection, for a total of 400000 labelled images. This sample was collected specifically for the present paper. The 16 categories are \u201cletter\u201d, \u201cmemo\u201d, \u201cemail\u201d, \u201cfilefolder\u201d, \u201cform\u201d, \u201chandwritten\u201d, \u201cinvoice\u201d, \u201cadvertisement\u201d, \u201cbudget\u201d, \u201cnews article\u201d, \u201cpresentation\u201d, \u201cscientific publication\u201d, \u201cquestionnaire\u201d, \u201cresume\u201d, \u201cscientific report\u201d, and \u201cspecification\u201d. The selection of categories was guided by earlier work on document categorization [27], and also by the range of categories present in the already-existing SmallTobacco sample from the same collection. Another factor was the knowledge that CNNs do well with large datasets (e.g., over a million images) [19], so selection was restricted to categories that were well represented in the dataset. A representative sample of the dataset is shown in Figure 4. The final categories are not perfectly distinct: many images were originally labelled with multiple tags, which potentially covered several of the categories eventually selected; in this version of the dataset each image is labelled with a single category.\nEach dataset was split into three subsets for the purposes of experimentation. The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing. Since this is a small dataset, 10 random splits in those proportions were created; results reflect the median performance from those splits. In the case of retrieval, the median was selected based on mean average precision at the 10th retrieval (mAP@10). The BigTobacco dataset was split in proportions similar to those of ImageNet [30]: 320000 images were used for training, 40000 images for validation, and 40000 images for testing. The validation sets were used to find plateaus in the CNN training process. All results are reported on the test sets.\nB. Implementation details\nThe CNNs were implemented in Caffe [16]. All networks computed an N -way softmax at the top layer, where N is the number of categories being learned.\nAll but two of the CNNs used Caffe\u2019s reference ImageNet architecture, which is based on the work of Krizhevsky et al. [19]. This network has five convolutional layers, and three fully-connected layers. The network takes images of size 227 \u00d7 227. The full architecture can be written as 227 \u00d7 227 \u2212 11 \u00d7 11 \u00d7 96 \u2212 5 \u00d7 5 \u00d7 256 \u2212 3 \u00d7 3 \u00d7 384 \u2212 3\u00d7 3\u00d7 384\u2212 3\u00d7 3\u00d7 256\u2212 4096\u2212 4096\u2212N . Features were extracted from these CNNs by taking the output of the first fully-connected layer, which has 4096 dimensions.\nThe first network with a different architecture is listed in the results as \u201cSmall holistic CNN\u201d, which uses hyperparameters established in another work on document image analysis [17]. This network has two convolutional layers and three fully-connected layers, with pooling, ReLU, and dropout employed at several stages in between. The network takes as input images of size 150\u00d7150. The full architecture can be written as 150\u00d7150\u221236\u00d736\u00d720\u22128\u00d750\u22121000\u22121000\u2212N .\nAs with the ImageNet networks, features were extracted from this network by taking the output of rhe first fully-connected network, which in this case has 1000 dimensions.\nThe second network with a different architecture is the \u201cEnsemble of CNNs\u201d network, which uses vectors extracted from the region-based CNNs to perform classification. Since a vector of length 4096 \u00b7 5 is too large to classify, the individual region-based vectors were compressed using principle component analysis (PCA) to 640 dimensions before they were concatenated for classification. The network architecture can be written as 3200 \u00d7 4096 \u00d7 N . For retrieval, features for this approach were created by individually compressing each region\u2019s feature vector to 128 dimensions, and then concatenating, resulting in a vector with 640 dimensions.\nTo test the effect of transfer learning between categories of documents, one holistic CNN was trained using only two categories of the BigTobacco dataset: letters and memos. This network was pre-trained on ImageNet. In the results, it is listed as \u201cLetterMemo CNN\u201d.\nTo extract regions from the images, all images were first resized to 780 \u00d7 600. The header region was defined by the first 256 rows of pixels in each image. The footer region was similarly defined by the last 256 rows of pixels in each image. The left body region was delineated by the intersection of the 400 central rows and the 300 left columns; the right body region was symmetrically defined. Every extracted region was resized to 227\u00d7 227 before being used as input.\nSeveral state-of-the-art bag of words (BoW) approaches to document representation were also implemented. As in previous work [22], the words were k-means clustered SURF features [4]. These features were pooled in a spatial pyramid [23], as well as in various combinations of horizontal and vertical partitions [22]. In the results, we denote these horizontalvertical partitioning schemes with HaVb, where a is the number of times the image was recursively split horizontally, and b is the number of times the image was recursively split vertically. For example, H0V3 has 15 bags: 1 for the original image, 2 for the first vertical split, 4 for the second vertical split, and 8 for the third. For the holistic bag of words, the resulting feature vector has 300 dimensions; H2V0 has 2100 dimensions; H0V3 has 4500 dimensions; H2V3 and L3 both have 6300 dimensions. For classification of the BoW features,\na random forest with 500 trees and \u221a D feature dimensions was trained, where D was the length of the feature vector of the complete (concatenated) bag of words.\nThree additional features were added as baselines to the featured approaches: the GIST descriptor [28], average brightness, and ensemble-of-regions average brightness. The GIST descriptor has been shown to perform well on image retrieval tasks [11], but has not yet been applied to document analysis. Average brightness acts as a baseline for minimum performance; images in this representation are represented with a single value. Ensemble-of-regions average brightness represents document images a vector of five elements, corresponding to the average brightness in each of the regions created for the ensemble of CNNs approach. This is intended to demonstrate on a small scale the basic benefit afforded by region-based analysis.\nRetrieval was performed by computing the Euclidean distance between the test set descriptors and every descriptor of the training set. The sorted distances were then used to rank the images of the training data, and return a sorted list of documents for each test query. For all approaches with feature vectors larger than 128 dimensions, the vectors were first compressed to 128 dimensions using PCA before they were used for retrieval. This is consistent with the related work [29, 14]; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits. As in the related work, the feature vectors were L2-normalized before and after PCA compression."}, {"heading": "C. Classification results", "text": "Table I shows the classification accuracies of the various BoW approaches, along the various CNNs-based appraoches, on both the SmallTobacco dataset and the BigTobacco dataset.\nOn SmallTobacco, the ensemble of region-based CNNs performed better than any other approach, achieving 79.9% classification accuracy. The previous best reported result on this dataset was 65.4% with a randomly initialized \u201cSmall\u201d CNN, which was approximately replicated here. The holistic network performed only slightly worse than the ensemble of CNNs, suggesting that the holistic CNN may be learning some amount of the information that region-based analysis\nwas expected to add. Interestingly, the \u201cSmall\u201d CNN compares similarly to the large-sized holistic CNN when both are initialized with random weights. This appears to indicate that the additional parameters in the large network are not necessarily beneficial. Initializing the larger networks with ImageNettrained weights improves performance substantially. Without this initialization, the CNNs perform similarly to (or worse than) the BoW approaches. Between the BoW approaches, the spatial-pyramid-pooled BoW performs best.\nOn BigTobacco, the holistic CNN finetuned from Imagenet performed better than any other approach, including the ensemble of CNNs. This suggests that given sufficient training data, the advantage gained by region-tuned analysis is eliminated by the learning power of the holistic CNN. In these results, the CNN approaches perform far better than the BoW approaches, likely due to the benefit of additional training data. As observed in SmallTobacco, finetuning improves results, although by a smaller margin here than in the small dataset. Comparing the performance of BoW approaches between the two datasets, it is interesting to observe that performance drops by nearly 20%, suggesting that (i) the larger dataset presents a more difficult classification task (likely because it has more categories), and perhaps also (ii) the additional training data does not help these approaches. The confusion matrix for the holistic CNN is shown in Figure 5.\nThe CNN trained to classify only letters and memos achieved 95% accuracy on that task."}, {"heading": "D. Retrieval results", "text": "Retrieval was measured using mean average precision (mAP). Average precision computes the average value of precision as a function of recall on some interval. Formally, the discrete version of this metric is given by\nAP = \u2211n\nk=1(P (k)\u00d7 rel(k)) number of relevant documents , (1)\nwhere k is the rank of the document being retrieved, and rel(k) equals 1 if the document is relevant and 0 otherwise. This metric is sensitive to ranking order, so the score is higher if relevant documents are retrieved before irrelevant documents. Mean average precision is simply the average precision summed over all queries, divided by the number of queries. Retrieved documents were determined to be \u201crelevant\u201d if they had the same class label as the query image. Mean average precision for the first 10 retrievals on both datasets are summarized in Figure 6.\nOn the SmallTobacco dataset, the ensemble of region-tuned CNNs performs best, followed by a holistic CNN fine-tuned from ImageNet. Interestingly, the generic ImageNet descriptor performs well also, exceeding the performance of most other descriptors. Between the BoW approaches, the spatialpyramid-pooled BoW performs best. The GIST descriptor performs approximately as well as the BoW approaches.\nOn the BigTobacco dataset, the holistic CNN performs best, exceeding the ensemble of region-tuned CNNs by a small margin, but exceeding most other approaches by a large margin. The confusion matrix for the finetuned holistic CNN, computed using the first 10 retrievals, is shown in Figure 5. The BoW approaches are outperformed by every CNN vector, including the generic ImageNet vector. The \u201cLetterMemo\u201d CNN slightly improves upon the generic ImageNet descriptor, suggesting that some of the knowledge learned from letters and memos transfers to all 16 categories, but the gain is only marginal. Between the BoW approaches, the spatial-pyramidpooled BoW performs best, as in SmallTobacco. Interestingly, the GIST descriptor exceeds the performance of the BoW descriptors by a large margin on this dataset.\nFigure 8 shows a representative sample of the retrieval output of the holistic CNN on the BigTobacco dataset. In that figure, it is interesting to notice that in the first row, in which the query image is a memo, the top seven retrievals are all different memos from the same author (with the same signature) as the memo in the query image. The final row is similarly impressive: every document in the top ten retrievals has the same letterhead as the query document, despite variations in the other content, and also despite differing typefaces of the letterhead. There may exist biases in the dataset that lead to such fortunate retrievals (e.g., only a few letterheads, and only a few memo authors), but the results are still remarkable.\nAn additional experiment was performed to measure the effect of PCA compression on mAP@10 performance on the BigTobacco dataset, the results of which are summarized in Figure 7. Remarkably, the CNN vectors show almost no loss in performance until they are reduced to 16 dimensions. At all levels of compression, the holistic CNN performs exceeds the performance of every other approach."}, {"heading": "IV. CONCLUSION", "text": "This paper established a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). Generic features extracted from a CNN trained on ImageNet exceeded the performance of the state-of-the-art alternatives, and fine-tuning these features on document images pushed results even higher. Interestingly, experiments also showed that given sufficient training data, enforcing region-specific feature-learning is unnecessary; a single CNN trained on entire images performed approximately as well as an ensemble of CNNs trained on specific subregions of document images. In all, this work showed that the CNN approach to document image representation exceeds the power of hand-crafted alternatives."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by NSERC Discovery and Engage grants (held by K.G.D.), and an NSERC USRA (awarded\nto A.W.H.). The authors thank Palomino Systems for helpful discussions. The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU used for this research."}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "ICML, pages 17\u201324.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "From generic to specific deep representations for visual recognition", "author": ["H. Azizpour", "A.S. Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "arXiv, 1406.5774,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V.S. Lempitsky"], "venue": "ECCV, 8689:584\u2013599,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "SURF: Speeded Up Robust Features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "A. Leonardis, H. Bischof, and A. Pinz, editors, Proceedings of ECCV, volume 3951, pages 404\u2013417,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep poselets for human detection", "author": ["L.D. Bourdev", "F. Yang", "R. Fergus"], "venue": "arXiv, 1407.0717,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Bird species categorization using pose normalized deep convolutional nets", "author": ["S. Branson", "G.V. Horn", "S. Belongie", "P. Perona"], "venue": "BMVC,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Form classification using DP matching", "author": ["Y. Byun", "Y. Lee"], "venue": "Proc. of the 2000 ACM Symp. on App. Comp., volume 1, pages 1\u20134,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of document image classification: Problem statement, classifier architecture and performance evaluation", "author": ["N. Chen", "D. Blostein"], "venue": "IJDAR, 10(1):1\u201316,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "A clustering-based algorithm for automatic document separation", "author": ["K. Collins-Thompson", "R. Nickolov"], "venue": "SIGIR, pages 1\u20138,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering and classification of document structure-a machine learning approach", "author": ["A. Dengel", "F. Dubiel"], "venue": "ICDAR, volume 2, pages 587\u2013591,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Evaluation of GIST descriptors for web-scale image search", "author": ["M. Douze", "H. J\u00e9gou", "H. Sandhawalia", "L. Amsaleg", "C. Schmid"], "venue": "Proceedings of the ACM Int. Conf. on Image and Video Retrieval, CIVR \u201909, pages 19:1\u201319:8, New York, NY, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, volume 2, pages 524\u2013531,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV, 8695:392\u2013 407,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison and classification of documents based on layout similarity", "author": ["J. Hu", "R. Kashi", "G. Wilfong"], "venue": "Information Retrieval, 2(2- 3):227\u2013243,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "arXiv, http://caffe.berkeleyvision.org/,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for document image classification", "author": ["L. Kang", "J. Kumar", "P. Ye", "Y. Li", "D. Doerman"], "venue": "ICPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "User-defined template for identifying document type and extracting information from documents", "author": ["T. Kochi", "T. Saitoh"], "venue": "ICDAR, pages 127\u2013 130,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised classification of structurally similar document images", "author": ["J. Kumar", "D. Doermann"], "venue": "ICDAR, pages 1225\u20131229,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning document structure for retrieval and classication", "author": ["J. Kumar", "P. Ye", "D. Doermann"], "venue": "ICPR, pages 1558\u20131561,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Structural similarity for document image classification and retrieval", "author": ["J. Kumar", "P. Ye", "D. Doermann"], "venue": "PRL, 43:119,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, volume 2, pages 2169\u20132178,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "PIEEE, 86(11):2278\u20132324,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Building a test collection for complex document information processing", "author": ["D. Lewis", "G. Agam", "S. Argamon", "O. Frieder", "D. Grossman", "J. Heard"], "venue": "SIGIR, pages 665\u2013666,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Digital libraries and document image retrieval techniques: A survey", "author": ["S. Marinai", "B. Miotti", "G. Soda"], "venue": "M. Biba and F. Xhafa, editors, Learning Structure and Schemas from Documents, volume 375, pages 181\u2013204. Springer Berlin Heidelberg,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Twenty years of document image analysis in PAMI", "author": ["G. Nagy"], "venue": "PAMI, 22(1):38\u201362,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet large scale visual recognition", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Classification of document pages using structure-based features", "author": ["C. Shin", "D. Doermann", "A. Rosenfeld"], "venue": "IJDAR, 3(4):232\u2013247,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and functional decomposition of business documents", "author": ["S. Taylor", "M. Lipshutz", "R.W. Nilson"], "venue": "ICDAR, volume 2, pages 563\u2013566,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "PANDA: Pose aligned networks for deep attribute modeling", "author": ["N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L. Bourdev"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "In digital libraries, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means basic image analysis is the only available tool for initial indexing and classification [26].", "startOffset": 235, "endOffset": 239}, {"referenceID": 9, "context": "As a pre-processing stage, document image analysis can facilitate and improve OCR by providing information about each document\u2019s visual layout [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "This level of intra-class variability renders spatial layout analysis difficult, and rigid template matching impossible [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 23, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 18, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 28, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 6, "context": "has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18].", "startOffset": 112, "endOffset": 119}, {"referenceID": 17, "context": "has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": ", forms) can be reduced to the problem of template matching [7], and less-rigid document types (e.", "startOffset": 60, "endOffset": 63}, {"referenceID": 14, "context": ", letters) can similarly be classified by fitting the geometric configuration of the document\u2019s components to one of several template configurations, via geometric transformations [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 31, "context": "An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \u201clandmark\u201d features that may appear anywhere in the document [32, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 30, "context": "An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \u201clandmark\u201d features that may appear anywhere in the document [32, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 11, "context": "This strategy is sometimes called a \u201cbag of visual words\u201d approach, since it describes images with a histogram over an orderless vocabulary of features [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "For example, a landmark feature discriminating letters from most other document classes is the salutation: finding a salutation in a document (potentially through OCR) is a good cue that the document is a letter, regardless of that feature\u2019s exact spatial position [32].", "startOffset": 265, "endOffset": 269}, {"referenceID": 22, "context": "By concatenating image features pooled at several stages, beginning with a whole-image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics [23].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind [22].", "startOffset": 239, "endOffset": 243}, {"referenceID": 9, "context": "At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 8, "context": "At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 9, "context": "\u201clandmarks\u201d) within each document type, toward the goal of structure-based classification [10, 21].", "startOffset": 90, "endOffset": 98}, {"referenceID": 20, "context": "\u201clandmarks\u201d) within each document type, toward the goal of structure-based classification [10, 21].", "startOffset": 90, "endOffset": 98}, {"referenceID": 16, "context": "Most recently, it was shown that the entire pipeline of supervised document image classification, from feature-building to decision making, can be learned by a convolutional neural network (CNN) [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 12, "context": "In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 28, "context": "The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill-suited, such as attribute detection, and fine-grained object recognition [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 12, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 4, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 32, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 23, "context": "After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations [24].", "startOffset": 255, "endOffset": 259}, {"referenceID": 28, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 2, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 13, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 1, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 24, "context": "Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents [25], containing 400,000 document images across 16 categories.", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19].", "startOffset": 295, "endOffset": 303}, {"referenceID": 18, "context": "In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19].", "startOffset": 295, "endOffset": 303}, {"referenceID": 23, "context": "A typical network of this type has approximately 60 million trainable parameters; this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network\u2019s power as a featurebuilder and classifier [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 23, "context": ", by \u201cjittering\u201d the training data to add geometric variants of each image in the dataset [24], or by altering the architecture of the CNN to process the input at multiple scales and positions [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": ", by \u201cjittering\u201d the training data to add geometric variants of each image in the dataset [24], or by altering the architecture of the CNN to process the input at multiple scales and positions [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 28, "context": "It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": ", to 128 dimensions) without significantly affecting their discriminative power [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "The goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 23, "context": "The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "A popular choice for pre-training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories [30].", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "ImageNet-trained network have been shown to be effective general-purpose features in a variety of other vision challenges, even without fine-tuning on the target problem [29].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "The first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work [20].", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 21, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 16, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 26, "context": "The selection of categories was guided by earlier work on document categorization [27], and also by the range of categories present in the already-existing SmallTobacco sample from the same collection.", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": ", over a million images) [19], so selection was restricted to categories that were well represented in the dataset.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 21, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 16, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 29, "context": "The BigTobacco dataset was split in proportions similar to those of ImageNet [30]: 320000 images were used for training, 40000 images for validation, and 40000 images for testing.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The CNNs were implemented in Caffe [16].", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The first network with a different architecture is listed in the results as \u201cSmall holistic CNN\u201d, which uses hyperparameters established in another work on document image analysis [17].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "As in previous work [22], the words were k-means clustered SURF features [4].", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "As in previous work [22], the words were k-means clustered SURF features [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "These features were pooled in a spatial pyramid [23], as well as in various combinations of horizontal and vertical partitions [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "These features were pooled in a spatial pyramid [23], as well as in various combinations of horizontal and vertical partitions [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "Three additional features were added as baselines to the featured approaches: the GIST descriptor [28], average brightness, and ensemble-of-regions average brightness.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "The GIST descriptor has been shown to perform well on image retrieval tasks [11], but has not yet been applied to document analysis.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "This is consistent with the related work [29, 14]; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits.", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "This is consistent with the related work [29, 14]; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits.", "startOffset": 41, "endOffset": 49}], "year": 2015, "abstractText": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.", "creator": "LaTeX with hyperref package"}}}