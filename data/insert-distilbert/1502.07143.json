{"id": "1502.07143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "The VC-Dimension of Similarity Hypotheses Spaces", "abstract": "given example a set $ x $ and a function $ h : x \\ r longrightarrow \\ { 0, return 1 \\ } $ which labels each element f of $ x $ with either $ q 0 $ or $ \u2013 1 $, we use may explicitly define a function $ h ^ { ( s ) } $ variable to measure the similarity of pairs of points in $ x $ according @ to $ h $. specifically, for $. h \\ in \\ { 0, where 1 \\ } ^ x $ we define $ h ^ { ( s ) } \\ in \\ { 0, 1 \\ } ^ { x \\ d times x } $ by $ h ^ { ( s ) } ( w, \" x ) : = \\ mathbb { 1 } [ h ( w ) = \" h ( x ) ] $. this idea can be loosely extended to a partial set of arithmetic functions, mathematical or hypothesis function space $ \\ mathcal { h } \\ subseteq \\ { 0, \u2264 1 \\ } ^ x $ rr by immediately defining a similarity hypothesis space $ \\ mathcal { h } ^ { ( s ) } : = \\ { h ^ { ( s ) } : h \\ equality in \\ mathcal { h } \\ } $. where we show that $ { { % vc - dimension } } ( \\ mathcal { h } ^ { ( s ) } ) \\ in \\ gamma theta ( { { vc - high dimension } } ( \\ mathcal { h } ) ) $.", "histories": [["v1", "Wed, 25 Feb 2015 12:14:04 GMT  (9kb)", "http://arxiv.org/abs/1502.07143v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mark herbster", "paul rubenstein", "james townsend"], "accepted": false, "id": "1502.07143"}, "pdf": {"name": "1502.07143.pdf", "metadata": {"source": "CRF", "title": "The VC-Dimension of Similarity Hypothesis Spaces", "authors": ["Mark Herbster", "Paul Rubenstein", "James Townsend"], "emails": ["james.townsend.14}@ucl.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n07 14\n3v 1\n[ cs\n.L G\n] 2\n5 Fe\nby h(s)(w, x) := 1[h(w) = h(x)]. This idea can be extended to a set of functions, or hypothesis space H \u2286 {0, 1}X by defining a similarity hypothesis space H(s) := {h(s) : h \u2208 H}. We show that vc-dimension(H(s)) \u2208 \u0398(vc-dimension(H))."}, {"heading": "1 Introduction", "text": "Consider the problem of learning from examples. We may learn by receiving class labels as feedback: \u2018this is a dog\u2019, \u2018that is a wolf\u2019 , \u2018there is a cat\u2019, etc. We may also learn by receiving similarity labels: \u2018these are the same\u2019, \u2018those are different\u2019 and so forth. In this note we study the problem of learning with similarity versus class labels. Our approach is to use the VC-dimension [VC71] to study the fundamental difficulty of this learning task.\nIn the supervised learning model we are given a training set of patterns and associated labels. The goal is then to find a hypothesis function that maps patterns to labels that will predict with few errors on future data (small generalization error). A classic approach to this problem is empirical risk minimisation. Here the procedure is to choose a hypothesis from a set of hypothesis functions (hypothesis space) that \u2018fits\u2019 the data as closely as possible. If the hypothesis is from a hypothesis space with small VC-dimension and fits the data well then we are likely to predict well on future data [VC71, BEHW89]. The number of examples required to have small generalisation error with high probability is called the sample complexity. In the uniform learnability model the VC-dimension gives a nearly matching upper and lower bound on the sample complexity [BEHW89, EHKV89]. In Theorem 1 we demonstrate that the VC-dimension of a hypothesis space with respect to similarity-labels is proportionally bounded by the VC-dimension with respect to class-labels indicating that the sample complexities within the two feedback\nsettings are comparable. That is, the fundamental difficulties of the two learning tasks are comparable.\nRelated work\nWe are motivated by the results of [GHP13]. Here the authors considered the problem of similarity prediction in the online mistake bound model [Lit88]. In [GHP13, Theorem 1] it was found that given a basic algorithm for class-label prediction with a mistake bound there exists an algorithm for similarity-label prediction with a mistake bound which was larger by no more than a constant factor. In this work we find an analogous result in terms of the VC-dimension."}, {"heading": "2 The VC-dimension of similarity hypothesis spaces", "text": "A hypothesis space H \u2286 {0, 1}X is a set of functions from some set of patterns X to the set of labels Y = {0, 1} in the two-class setting. The restriction of a function h \u2208 {0, 1}X to a subset X \u2032 \u2286 X is the function h|X\u2032 \u2208 {0, 1}\nX\u2032 with h|X\u2032(x) := h(x) for each x \u2208 X \u2032. Analogously, one can define the restriction of a hypothesis space as H|X\u2032 := {h|X\u2032 : h \u2208 H}.\nA subset X \u2032 \u2286 X is said to be shattered by H if H|X\u2032 = {0, 1} X\u2032 , that is if the restriction contains all possible functions from X \u2032 to {0, 1}. The VC-dimension [VC71] of a hypothesis space H \u2286 {0, 1}X , denoted d(H), is the size of the largest subset of X which is shattered by H, that is\nd(H) := max X\u2032\u2286X\n{|X \u2032| : H|X\u2032 = {0, 1} X\u2032} .\nSauer\u2019s lemma [VC71, Sau72, She72], which gives a lower bound for the VC-dimension of a hypothesis space, will be used for proving our main result. It states that for a hypothesis space H \u2286 {0, 1}X , if\n|H| >\nm\u22121 \u2211\nk=0\n(\n|X|\nk\n)\n(1)\nthen d(H) \u2265 m. Given a function h : X \u2212\u2192 {0, 1}, we may define a function h(s) to measure the similarity of pairs of points in X according to h. Specifically, for h \u2208 {0, 1}X we define h(s) \u2208 {0, 1}X\u00d7X by h(s)(w, x) := 1[h(w) = h(x)], where 1 is the indicator function. This idea can be extended to a hypothesis space H by defining the similarity hypothesis space H(s) := {h(s) : h \u2208 H}. We now give our central result,\nTheorem 1. Given a hypothesis space H \u2286 {0, 1}X ,\nd(H)\u2212 1 \u2264 d(H(s)) \u2264 \u03b4d(H) ,\nwith \u03b4 = 4.55.\nProof. For the left hand inequality, let n := d(H) and pick a set T = {x1, x2, . . . , xn} of size n which is shattered by H. Then let T \u2032 = {(x1, x2), (x2, x3), . . . , (xn\u22121, xn)}. To demonstrate that T \u2032 is shattered by H(s), let g \u2208 {0, 1}T \u2032\nbe any mapping from T \u2032 to {0, 1}. Then since T is shattered by H we may find a map h \u2208 H with h(x1) = 0 and\nh(xi+1) =\n{\nh(xi) if g(xi, xi+1) = 1 1\u2212 h(xi) if g(xi, xi+1) = 0\nfor i = 1, . . . , n \u2212 1. Observe that g = h(s)|T \u2032 . Since g was chosen arbitrarily, we may conclude that T \u2032 is indeed shattered by H(s), and therefore d(H(s)) \u2265 |T \u2032| = d(H)\u2212 1.\nFor the right hand inequality, first let M := d(H(s)) and then pick a set U = {(w1, x1), (w2, x2), . . . , (wM , xM )} of size M in X \u00d7X which is shattered by H\n(s). Let V = {w1, w2, . . . , wM , x1, x2, . . . , xM} and note that |H|V | \u2265 |H (s)|U | = 2 M . This is because any two maps h and g which agree on V will induce maps h(s) and g(s) which agree on U , so H(s)|U cannot possibly contain more maps than H|V . Using this fact, and applying Sauer\u2019s Lemma (see (1)) to H|V , we see that if\n2M >\nm\u22121 \u2211\nk=0\n(\n|V |\nk\n)\nthen d(H) \u2265 d(H|V ) \u2265 m. Now note the following inequality (see e.g., [FG06, Lemma 16.19]), which bounds a sum of binomial coefficients:\n\u230a\u01ebn\u230b \u2211\ni=0\n(\nn\ni\n)\n\u2264 2H(\u01eb)n (0 < \u01eb < 1/2) , (2)\nwhere H(\u01eb) := \u01eb log2 1 \u01eb + (1 \u2212 \u01eb) log2 1 1\u2212\u01eb denotes the binary entropy function. If we set m = 1 + \u230a2\u01ebM\u230b for some \u01eb < 12 such that H(\u01eb) < 1 2 , we have\nm\u22121 \u2211\nk=0\n(\n|V |\nk\n)\n=\n\u230a2\u01ebM\u230b \u2211\nk=0\n(\n|V |\nk\n)\n\u2264\n\u230a2\u01ebM\u230b \u2211\nk=0\n(\n2M\nk\n)\n\u2264 22MH(\u01eb) < 2M\nusing (2) and that |V | \u2264 2M from the definition of V . Thus Sauer\u2019s lemma can be applied with the above value of m and hence\nd(H) \u2265 1 + \u230a2\u01ebM\u230b \u2265 2\u01ebM = 2\u01ebd(H(s)) ,\nas long as H(\u01eb) < 1/2. Observe that \u01eb = .11 satisfies this condition and thus we have that\nd(H(s)) \u2264 4.55d(H) ."}, {"heading": "3 Discussion", "text": "In the following, we give a family of examples where the VC-dimension of the similarity hypothesis space is exactly twice that of the original space. We use the following notation for the set of the first n natural numbers [n] := {1, 2, . . . , n}.\nExample 2. For the hypothesis space of k-sparse vectors, Hk := {h \u2208 {0, 1} [n] : \u2211n i=1 h(i) \u2264 k},\nd(Hk) = k and d(H (s) k ) = 2k ,\nprovided that n \u2265 2k + 1.\nProof. Let X := [n]. Firstly note that d(Hk) \u2265 k, since any subset T \u2286 X with |T | \u2264 k is shattered by Hk. If T\n\u2032 \u2286 X with |T \u2032| > k then T \u2032 cannot possibly be shattered by Hk since there is no element in Hk that labels all elements of T \u2032 as 1. Therefore d(Hk) = k.\nTo see that d(H (s) k ) \u2265 2k, let U = {(x1, x2), (x2, x3), . . . , (x2k, x2k+1)} for any distinct elements x1, x2, . . . , x2k+1 \u2208 X and note that |U | = 2k. To show that U is shattered by H (s) k , let g \u2208 {0, 1}\nU be any function from U to {0, 1}. We need to find an h \u2208 Hk such that g = h(s)|U . Two functions in {0, 1}\nX which satisfy the condition g = h(s)|U are h0 and h1 defined by h0(x1) = 0, h1(x1) = 1 and\nhj(xi+1) =\n{\nhj(xi) if g(xi, xi+1) = 1 1\u2212 hj(xi) if g(xi, xi+1) = 0\nhj(x) = 0 \u2200x 6\u2208 {x1, x2, . . . , x2k+1}\nfor i = 1, . . . , 2k and j = 0, 1. Observe that by construction, h0(xi)+h1(xi) = 1 for each i = 1, . . . , 2k+1 and therefore\n\u22112k+1 i=1 h0(xi)+ \u22112k+1 i=1 h1(xi) = \u22112k+1 i=1 [h0(xi)+h1(xi)] =\n2k + 1. This means that we must have \u22112k+1\ni=1 hj(xi) \u2264 k for some j and hence hj \u2208 Hk\nwith h (s) j |U = g. This proves that d(H (s) k ) \u2265 2k.\nNow suppose, for a contradiction, that d(H (s) k ) > 2k. Then there is some set\nE = {(u1, v1), (u2, v2), . . . , (u2k+1, v2k+1)} \u2286 X \u00d7 X of size 2k + 1 which is shattered by H(s). Let V := {u1, u2, . . . , u2k+1, v1, v2, . . . , v2k+1} (note that in general we do not necessarily have that |V | = 4k + 2 since the ui and vi need not all be distinct).\nLet G be the graph with vertex set V and edge set E. Observe that elements of Hk correspond to {0, 1}-labellings of V and that elements of H (s) k correspond to {0, 1}- labellings of E. Since E is shattered by H (s) k , every labelling of E is realisable as the induced map h(s) of some h \u2208 Hk. Note that G cannot contain a cycle since there is no labelling of V which could induce a similarity labelling on a cycle in which exactly one edge is labelled 0 and the rest are labelled 1\u2217. So the graph is a union of trees, also known as a \u2018forest\u2019. Note that in\n\u2217Indeed, under any such labelling of E any two vertices in the cycle are connected by two paths, one path containing exactly zero edges labelled with a 0 (implying that the two vertices are labelled the same) and one path containing exactly one edge labelled with a 0 (implying that the two vertices are labelled differently).\ngeneral the number of vertices in a forest is |V | = |E| + r, where |E| is the number of edges and r is the number of trees in the forest. In this case we have |V | = 2k + 1 + r.\nNow choose a labelling g, which labels the vertices of each connected component (tree) in G according to the following rule: for each connected component C in G, label \u230a |C|2 \u230b vertices v \u2208 C with a 1 and the remaining \u2308 |C| 2 \u2309 with a 0. Note that g /\u2208 Hk since\n\u2211\nv\u2208V\ng(v) = \u2211\nC\n\u2211\nv\u2208C\ng(v) = \u2211\nC\n\u230a\n|C|\n2\n\u230b\n\u2265 \u2211\nC\n|C| \u2212 1\n2 =\n|V | \u2212 r\n2 = k +\n1 2 > k.\nConsider the edge labelling g(s)|E. Since E is shattered by H (s) k , there must be some h \u2208 Hk such that h (s)|E = g\n(s)|E . But this is not possible, for if it were, then in order for h(s) to agree with g(s) we would need h|C = g|C or h|C = 1\u2212 g|C for each connected component C in G. Swapping the labellings between 0 and 1 on one or more of the connected components can only increase the number of 1 labellings and thus\n\u2211\nv\u2208V\nh(v) \u2265 \u2211\nv\u2208V\ng(v) > k\nso h cannot be in Hk. Thus we have found a labelling of E, namely g (s)|E , which cannot be in H (s) k . But this is a contradiction of our initial assumption that E was shattered by H (s) k . So we have proved that our assumption must have been incorrect and therefore d(H (s) k ) = 2k.\nIn Theorem 1, the lower bound d(H) \u2212 1 \u2264 d(H(s))) is tight, for example when H = {0, 1}[n]. However, observe that in Example 2, the hypothesis space of k-sparse vectors, the similarity space \u201cexpands\u201d only by a factor of 2, which is less than the factor \u03b4 = 4.55 of Theorem 1. We leave as a conjecture that the upper bound in Theorem 1 can be improved to a factor of two.\nAcknowledgements. We would like to thank Shai Ben-David, Ruth Urner and Fabio Vitale for valuable discussions. In particular we would like thank Ruth Urner for proving an initial motivating upper bound of d(H(s)) \u2264 2d(H) log(2d(H))."}], "references": [{"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "J. ACM,", "citeRegEx": "Blumer et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Blumer et al\\.", "year": 1989}, {"title": "A general lower bound on the number of examples needed for learning", "author": ["A. Ehrenfeucht", "D. Haussler", "M. Kearns", "L.G. Valiant"], "venue": "Information and Computation,", "citeRegEx": "Ehrenfeucht et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Ehrenfeucht et al\\.", "year": 1989}, {"title": "Parameterized Complexity Theory (Texts in Theoretical Computer Science. An EATCS Series)", "author": ["J. Flum", "M. Grohe"], "venue": null, "citeRegEx": "Flum and Grohe.,? \\Q2006\\E", "shortCiteRegEx": "Flum and Grohe.", "year": 2006}, {"title": "Online similarity prediction of networked data from known and unknown graphs", "author": ["Claudio Gentile", "Mark Herbster", "Stephen Pasteris"], "venue": "COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Gentile et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gentile et al\\.", "year": 2013}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning,", "citeRegEx": "Littlestone.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone.", "year": 1988}, {"title": "On the density of families of sets", "author": ["N. Sauer"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "Sauer.,? \\Q1972\\E", "shortCiteRegEx": "Sauer.", "year": 1972}, {"title": "A combinatorial problem; stability and order for models and theories in infinitary languages", "author": ["Saharon Shelah"], "venue": "Pacific J. Math.,", "citeRegEx": "Shelah.,? \\Q1972\\E", "shortCiteRegEx": "Shelah.", "year": 1972}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probab. and its Applications,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1971}], "referenceMentions": [], "year": 2015, "abstractText": null, "creator": "LaTeX with hyperref package"}}}