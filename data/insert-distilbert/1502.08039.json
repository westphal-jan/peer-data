{"id": "1502.08039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2015", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "abstract": "in this paper we subsequently propose a linear non - metric invariant ranking - scale based representation of semantic similarity that allows natural aggregation search of semantic information from multiple heterogeneous sources. we apply using the ranking - based representation to handle zero - shot learning problems, and present deterministic and probabilistic zero - shot classifiers which can be built from simpler pre - trained classifiers without initial retraining. we demonstrate their the advantages on two large real - world image datasets. in particular, we show that aggregating different sources of semantic cluster information, including crowd - sourcing, leads to possible more accurate classification.", "histories": [["v1", "Fri, 27 Feb 2015 20:00:53 GMT  (51kb,D)", "http://arxiv.org/abs/1502.08039v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["jihun hamm", "mikhail belkin"], "accepted": false, "id": "1502.08039"}, "pdf": {"name": "1502.08039.pdf", "metadata": {"source": "META", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "authors": ["Jihun Hamm", "Mikhail Belkin"], "emails": ["HAMMJ@CSE.OHIO-STATE.EDU", "MBELKIN@CSE.OHIO-STATE.EDU"], "sections": [{"heading": "1. Introduction", "text": "In standard multiclass classification settings, classes are treated as a categorical set without any extra structure. When we have side-information on the structure of classes, such as semantic relatedness, we can use this information to improve the classification itself, or transfer any knowledge learned from the training domain to solve problems in a new domain.\nConsider a classification problem of the following 10 visual objects: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. There are many sources from which semantic information for those objects can be obtained. WordNet is a knowledge-base of semantic hierarchies developed manually by linguistic experts (Miller, 1995). In WordNet, objects form a hierarchical tree (Figure 1, Left), where a child object is \u2018a kind of\u2019 its parent object. Several similarity metrics can be defined from the hierarchy 1, one of which is shown in Figure 1 (Middle) as a twodimensional classical multidimensional scaling (MDS) embedding. Semantic relatedness can also be mined automatically from existing corpora, such as Wikipedia, Google N-\n1http://maraca.d.umn.edu/similarity/ measures.html\nGram corpus, or using search engines, where cosine angles of co-occurrence vectors can be used as a similarity of two words. More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014). Figure 1 (Right) is an example MDS embedding from the representation from (Huang et al., 2012). As can be seen from the figure, similarity of the same objects can look very different depending on which semantic source and measure is used.\nNon-metric representation of similarity. Multiple sources of semantic information have the potential to complement each other for an improved classification result. Still, how to best aggregate similarity from inhomogeneous sources remains an open problem. Similarity measures from different corpora or methods are not directly comparable, and therefore a simple averaging of the measures will not be optimal. The first key idea of our paper is that we use non-metric, ranking-based representation of semantic similarity, instead of numerical representation.\nTo illustrate our approach, consider the problem of distinguishing cat and truck. In Figure 1 (Middle), cat is closer to dog than automobile:\nd(cat , dog) < d(cat , automobile),\nand truck is closer to automobile than to dog:\nd(truck , automobile) < d(truck , dog)).\nIn other words, we may be able to distinguish cat and truck from their closeness to other reference objects without using any numerical similarity. As a special case, we can use the similarity of all the other objects to cat to form a semantic ranking of cat. For example, cat has a semantic ranking\n\u03c0cat = [horse, deer , \u00b7 \u00b7 \u00b7 , automobile, airplane], (1)\nand truck has a semantic ranking\n\u03c0truck = [automobile, ship, \u00b7 \u00b7 \u00b7 , deer , horse], (2)\nar X\niv :1\n50 2.\n08 03\n9v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20\n15\naccording to the distance in Figure 1 (Middle). Not only the ordinal similarity may be sufficient for distinguishing cat and truck, but it also seems a more natural representation, since the ordinal similarity is invariant under scaling and monotonic transformation of numerical values and therefore has a better chance of being consistent across different heterogeneous sources. Moreover, ordinal information can be obtained directly from non-numerical comparisons. In particular, when we ask human subjects to judge similarity of objects, it is easier for subjects to rank objects rather than to assign numerical scores of similarity.\nZero-shot classification without retraining. In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013). In zero-shot learning we have samples {(xi, yi)} from the domain X \u00d7 Y (e.g., Y is the set of 8 objects), but no samples from the test domain X \u00d7 Z (e.g., Z = {cat , truck}). The goal is to construct a classifier X \u2192 Z using the only training data {(xi, yi)} and semantic knowledge of the two domains Y and Z .\nA standard approach to classifying C classes is to use binary classifications in one-vs-rest or one-vs-one setting, or to use multiclass losses directly. If we already have pre-trained classifiers of the training domain classes Y using one of those settings, can we use those classifiers \u2018for free\u2019 to distinguish unseen classes cat and truck without retraining with training domain samples? Figure 2 provides an intuition on the problem. Consider multiple decision hyperplanes learned from the one-vs-one setting (others will be discussed in Section 2.) The C(C\u22121)2 hyperplanes partition the feature space into \u2018cells\u2019, each of which assigns a ranking of C objects to points inside its interior. To see this, note that all pairs of objects are compared in each cell (either i \u227a j or j \u227a i), and transitivity (see Section 2)\nfollows the metric triangle inequality. The ranking of an unseen test sample assigned by pre-trained classifiers can be compared with the semantic rankings of cat or truck for zero-shot classification, assuming feature and semantic similarities are strongly correlated (see (Deselaers & Ferrari, 2011) for a discussion).\nBuilding on this idea, we present novel zero-shot classification methods that are free of re-training and can aggregate semantic information from multiple sources. We start by proposing a simple deterministic ranking-based method, and further improve the method by introducing probability models of rankings. In the probabilistic approach, realvalued classification scores are mapped to posterior probabilities of rankings, and combined with prior probability of rankings learned from (multiple) semantic sources. The advantage of using probabilistic approach will be explained more in the method and the experiment sections. For both the posterior and the prior probabilities of rankings, we use classic probabilistic models of ranking including the Plackett-Luce, the Mallows, and the BabingtonSmith models.\nTo summarize the contributions of this paper, we present\n1. non-metric ranking-based representation of a semantic structure, alternative to numerical similarity representation\n2. methods of aggregating multiple semantic sources using probability models of rankings\n3. deterministic and probabilistic zero-shot classifiers built from pre-trained classifiers without retraining.\nIn the experiment section we demonstrate the advantages of our approach over a numerically-based approach and a deterministic approach using two well-known image databases Animals-with-attributes (Lampert et al., 2009) and CIFAR-10/100 (Krizhevsky, 2009). In particular, we demonstrate that aggregating different semantic sources, including crowd-sourcing, leads to more accurate zero-shot classification.\nThe remainder of the paper is organized as follows: In Section 2, we present deterministic and probabilistic rankingbased algorithms for zero-shot classification. In Section 3, we relate our work to others in the literature. In Section 4, we test our methods with real-world image databases, and conclude the paper in Section 5."}, {"heading": "2. Zero-shot learning with rankings", "text": "Notations. Let R denote the set of all rankings on C items/classes, and \u03c0 = [\u03c0(1), ..., \u03c0(C)] \u2208 R denote a ranking: \u03c0(i) is the position of item i and \u03c0-1(j) is the item number whose position is j. We write i \u227a j (\u2018i precedes j\u2019) when \u03c0(i) < \u03c0(j) (\u2018item i is ranked higher than item j\u2019.) A top-K ranking is a straightforward generalization of a ranking, in which the order of only the first K items \u03c0-1(1), ..., \u03c0-1(K) matter and the order of the remaining C \u2212 K items are ignored. With an abuse of notation, we use \u03c0 andR as a top-K ranking and the as the set of all topK rankings as well, since a full ranking is a special case (K = C.) A partial order is a further generalization of a ranking and a top-K ranking. In a (full) ranking, a pair of items (i, j), i 6= j has to satisfy either i \u227a j or j \u227a i, whereas it can be neither in a partial order. In addition, a partial order has to satisfy the transitivity: for any triple (i, j, k), i \u227a j and j \u227a k implies i \u227a k. Item positions \u03c0(\u00b7) are in general undefined for a partial order."}, {"heading": "2.1. Deterministic approach", "text": "A simple deterministic approach to zero-shot learning using semantic rankings was already outlined in Introduction. In one-vs-one setting, C(C\u22121)2 pre-trained classifiers assign a ranking \u03c0(x). In one-vs-rest, C binary classifiers assign C real-valued scores to a test point according to the point\u2019s distances to C decision hyperplanes. The scores can be\nsorted to provide a ranking \u03c0(x). Given this ranking \u03c0(x) of a test sample x, and prior knowledge of semantic rankings {\u03c0z | z \u2208 Z} of test-domain classes Z , we predict\nz(x) = argmin z\u2208Z d(\u03c0(x), \u03c0z), (3)\nwhere d(\u00b7, \u00b7) is a distance between two rankings. For example, let Z = {cat , dog} whose semantic rankings are (1) and (2), respectively. If an unseen image x has classification scores in the order \u03c0(x) = [dog , deer , \u00b7 \u00b7 \u00b7 , ship, airplane], so that d(\u03c0(x), \u03c0cat) < d(\u03c0(x), \u03c0truck ) for some d(\u00b7, \u00b7), then we classify x as a cat rather than a truck. We use the Kendall\u2019s ranking distance which is the number of mismatching orders:\nd(\u03c01, \u03c02) = |{(i, j) | \u03c01(i) > \u03c01(j) \u2227 \u03c02(i) < \u03c02(j)}|. (4)\nSometimes it may make more sense to compare only the closest items than to compare all the items. The top-K version of the Kendall\u2019s distance was proposed in (Critchlow, 1985), which can be computed as follows. Let A, B, and D be the sets"}, {"heading": "A = {i \u2208 1, ..., C | \u03c01(i) \u2264 K, \u03c02(i) \u2264 K}", "text": ""}, {"heading": "B = {i \u2208 1, ..., C | \u03c01(i) \u2264 K, \u03c02(i) > K}", "text": "D = {i \u2208 1, ..., C | \u03c01(i) > K, \u03c02(i) > K}.\nThen the Kendall\u2019s top-K distance can be computed by\ndK(\u03c01, \u03c02) = |{(i, j) \u2208 A\u00d7A|\u03c01(i)<\u03c01(j), \u03c02(i)>\u03c02(j)}|\n+ |B|(C+K\u2212|B|\u22121 2 )\u2212 \u2211 i\u2208B \u03c01(i)\u2212 \u2211 i\u2208D \u03c02(i).(5)\nZero-shot classification using the rule (3) will be called deterministic ranking-based (DR) method."}, {"heading": "2.2. Probabilistic approach", "text": "We can further refine ranking-based algorithms by considering a probabilistic approach. There are several causes of uncertainty in ranking-based representation. First, classifier outputs for a test-domain sample x can have low confidence, since the classifiers are trained only with trainingdomain samples. Second, prior knowledge of semantic rankings from multiple semantic sources may not be unanimous. Third, feature and semantic similarities do not always coincide. For these reasons, we consider probability models of (top-K) rankings. We discuss three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see (Critchlow, 1985; Critchlow et al., 1991; Marden, 1995) for more reviews.)\nIn our probabilistic zero-shot learning approach, we assume the following dependence:\nx // \u03c0 // z (6)\nthat is, the label z of a sample x is dependent only on the predicted ranking \u03c0, which in turn is dependent only on the sample x. The probability of a test-domain label given the sample P (z|x) is obtained by marginalizing out the latent ranking variable \u03c0: P (z|x) = \u2211 \u03c0\u2208R P (\u03c0|x)P (z|\u03c0) = \u2211 \u03c0\u2208R P (\u03c0|x) P (z)P (\u03c0|z)\u2211 z P (z)P (\u03c0|z) , (7) and the final prediction of the label z for a test sample is made by argmaxz P (z|x).\nThere are two terms in (7): a probabilistic ranker P (\u03c0|x) and a prior for semantic ranking P (\u03c0|z). First, we describe the prior for semantic ranking P (\u03c0|z) learned from one or more semantic sources (e.g. different corpora or crowdsourcing) in Section 2.3. Second, we describe probabilistic rankers P (\u03c0|x) based on standard classifiers trained with training-domain data in Section 2.4. The final zero-shot classifier for unseen samples bringing these two learned components is described in Section 2.5."}, {"heading": "2.3. Prior for semantic ranking", "text": "We encode the semantic similarity between training- and test-domain classes by probabilistic ranking models of training-domain classes P (\u03c0|z) for each test-domain class z. To learn P (\u03c0|z), we use multiple instances of rankings for each test-domain class. These rankings can come from multiple linguistic corpora or by human-rated rankings directly. Below we outline three popular models of rankings \u2013 the Plackett-Luce, the Mallows, and the Babington-Smith models.\nPlackett-Luce. The Plackett-Luce model for the probability of observing a top-K ranking \u03c0 is\nP (\u03c0; v) = K\u220f i=1 v\u03c0-1(i)\u2211C j=i v\u03c0-1(j) . (8)\nThe non-negative parameters v1, ..., vC indicate the relative chances of being ranked higher than the rest of the items, and are invariant under constant scaling of v\u2019s. One interpretation of the generative procedure of the PlackettLuce model is the Vase interpretation from (Silberberg, 1980). Suppose there is a vase with infinite number of balls marked 1 to C, whose numbers are proportional to v\u2019s. At the first stage, a ball is drawn and is recorded as \u03c0-1(1). At the second stage, another ball is drawn and is recorded as \u03c0-1(2) unless the ball is already selected before (\u03c0-1(1)), in which case the drawing is tried again. The procedure is continued until K distinct balls are drawn and recorded. This generative probability is captured by (8).\nGiven N samples (=semantic sources) of rankings \u03c01, ..., \u03c0N for a class, the parameters can be estimated by\nMLE. The log-likelihood of (8) is\nL(v) = N\u2211 n=1 [ K\u2211 i=1 log(v\u03c0-1n(i))\u2212 log( C\u2211 j=i v\u03c0-1n(j))], (9)\nwith possibly an additional regularization term \u03b7 \u2211 v2i . Hunter (Hunter, 2004) proposed an iterative method of estimation using the Minorization-Maximization procedure which generalizes the Expectation-Maximization procedure and converges to a global maximum solution under a certain condition on the data. From our experience, simple gradient-based or Newton-Raphson methods seem to work well with an appropriate choice of the regularization parameter.\nMallows. The Mallows model (Mallows, 1957) for full rankings is defined as P (\u03c0;\u03c00, \u03bb) \u221d e\u2212\u03bb d(\u03c0,\u03c00), where \u03c00 is the mode, \u03bb is the spread parameter, and d(\u00b7, \u00b7) is the Kendall\u2019s distance between two rankings. It may be viewed as a discrete analog of the Gaussian distribution for ranking. The distance can further be written as d(\u03c0, \u03c00) = d(\u03c0\u03c0 -1 0 , e) = \u2211C\u22121 j=1 Vj(\u03c0\u03c0 -1 0 ), where e is the identity ranking and the Vj\u2019s are defined as\nVj(\u03c3) = \u2211 i>j I[\u03c3-1(i) < \u03c3-1(j)]. (10)\nFligner et al. (Fligner & Verducci, 1986) proposed the Mallows model for top-K lists by marginalizing the Mallows model:\nP (\u03c0 ;\u03c00, \u03bb) = 1\n\u03c6(\u03bb) e\u2212\u03bb\n\u2211K j=1 Vj(\u03c0\u03c0 -1 0 ), (11)\nwhere the Vj\u2019s are defined in (10) and \u03c6 is the normalization constant which can be computed in closed form: \u03c6(\u03bb) = \u220fK j=1(1\u2212 e\u2212(C\u2212j+1)\u03bb)/(1\u2212 e\u2212\u03bb).\nGiven N samples of rankings \u03c01, \u03c02, ..., \u03c0N , the parameters of the Mallows model for total rankings can also be found by MLE (Feigin & Cohen, 1978). When the mode \u03c00 is known, the MLE of the spread parameter \u03bb can be found by convex optimization, owing to the fact that the log-likelihood is a concave function of \u03bb. The MLE of the centroid \u03c00 is the maximum of \u2211 i logP (\u03c0i;\u03c00, \u03bb) and is equivalent to\nargmin \u03c00 \u2211 i d(\u03c0i, \u03c00). (12)\nThe minimization (12) is also known as the Kemeny optimal consensus or aggregation problem (Kemeny, 1959) and is known to be NP-hard (Bartholdi et al., 1989). However, there are known heuristic methods such as sequential transposition of adjacent items (Critchlow, 1985) or other admissible heuristics (Meila et al., 2007). We use the former method. Starting from the average ranking as the initial value of \u03c00, and we search adjacent items \u03c0-1(i) and\n\u03c0-1(i + 1) whose swapping lowers the sum of distances the most. We stop if there is no such item or if the maximum number of iteration (1000 in our case) is exceeded. The MLE with (11) can be solved by using \u2211K j=1 Vj(\u03c0) in place of d.\nBabington-Smith. The Babington-Smith model (Joe & Verducci, 1993; Smith, 1950) is another probabilistic ranking model based on pairwise comparisons. Given two items i and j, let \u03b1ij be the probability that item i is ranked higher than item j. Given these preferences {\u03b1ij}, the probability of a ranking \u03c0 is P (\u03c0;\u03b1) \u221d \u220f i<j \u03b1 I[\u03c0(i)<\u03c0(j)] ij (1 \u2212 \u03b1ij)1\u2212I[\u03c0(i)<\u03c0(j)]. After introducing new parameters vij = \u03b1ij/\u03b1ji (Joe & Verducci, 1993), the probability of a top-K ranking can be written as 2\nP (\u03c0; v) = 1\n\u03c8(v) K\u220f i=1 C\u220f j=i+1 v\u03c0-1(i)\u03c0-1(j) (13)\nThe Babington-Smith model is similar to the Plackett-Luce model in that the probability is the product of v\u2019s. The larger vij is, the more likely it is that item i is ranked higher than item j. However, unlike the Plackett-Luce model, the normalizing constant \u03c8(v) does not have a known closed form. We do not use it for modeling the semantic prior, but use it for probabilistic ranker in the next section."}, {"heading": "2.4. Probabilistic ranker from classifiers", "text": "The probabilistic ranker P (\u03c0|x) takes a sample x as input and probabilistically ranks the similarity of x to trainingdomain classes Y . We propose to build rankers from standard settings of multiclass classifiers: one-vs-rest, one-vsone, or multiclass-loss as in (Crammer & Singer, 2002). Any classifier that output a real-valued confidence or score can be used for this purpose.\nOne-vs-rest binary classifiers. In this setting, there will be C such scores f1(x), ...., fC(x) for each training-domain class. We relate the real-valued scores {fi} and the nonnegative parameters {vi} of the Plackett-Luce model (8) by setting vi = efi(x), to get\nP (\u03c0|x) = K\u220f i=1 ef\u03c0-1(i)(x)\u2211C j=i e f\u03c0-1(j)(x) . (14)\nInstead of producing a single ranking \u03c0 as in the deterministic approach (3), this ranker evaluates the probability of any ranking given x taking into account the confidence ({fi(x)}) of C one-vs-rest classifiers.\nOne-vs-one binary classifiers. In this setting, there will be C(C \u2212 1)/2 scores f1,2(x), ...., fC\u22121,C(x) for each\n2We presents a slightly modified form.\npair of training-domain classes. We related these scores to the C(C \u2212 1)/2 parameters {vij} of the Babington-Smith model (13) by vij = efij(x), to get\nP (\u03c0|x) \u221d K\u220f i=1 C\u220f j=i+1 ef\u03c0-1(i),\u03c0-1(j)(x). (15)\nSimilar to (14), this ranker evaluates the probability of any ranking \u03c0 given x taking into account the confidence ({fij(x)}) of C(C \u2212 1)/2 one-vsone classifiers. Note that if the pre-trained classifiers are linear, that is, fij(x) = w\u2032ijx, then this ranker is quite similar to (14), since P (\u03c0|x) \u221d\u220fK i=1 \u220fC j=i+1 e w\u03c0-1(i),\u03c0-1(j)(x) = \u220fK i=1 e w\u0302\u2032 \u03c0-1(i) x, with\nw\u0302\u03c0-1(i) defined as \u2211C j=i+1 w\u03c0-1(i),\u03c0-1(j). However, it has a different normalization term from (14).\nMulticlass-loss classifiers. Other types of classifiers can be accommodated. When the pre-trained classifiers are multinomial logistic regression (=softmax) or SVMs with a multiclass loss (Crammer & Singer, 2002), we again have C scores f1(x), ...., fC(x) computed from C parameter vectors w1, ..., wC . Similar to the one-vs-rest case, we can use the relation vi = efi(x) = ew \u2032 ix with the Plackett-Luce model which gives us the same ranker as (14). Note that if the original classifier is a multinomial logistic regression, the (14) is in fact a direct generalization of logistic regression for K = 1, which is also observed in (Cheng et al., 2010). In this case, the trained parameters {wi} coincide with the optimal maximum likelihood parameters for (14) trained with top-1 rankings which are ground truth labels of the training domain.\nTo summarize, there exist natural interpretations of the Plackett-Luce and the Babington-Smith models that allow us to relate classification scores to their parameters and use them to produce posterior probability P (\u03c0|x) of rankings without any further training3."}, {"heading": "2.5. Zero-shot prediction", "text": "The probabilistic rankers P (\u03c0|x) constructed from pretrained classifiers and the priors for semantic rankings P (\u03c0|z) learned from semantic sources are plugged into (7) P (z|x) = \u2211 \u03c0\u2208R P (\u03c0|x)P (z|\u03c0) = \u2211 \u03c0\u2208R P (\u03c0|x) P (z)P (\u03c0|z)\u2211 z P (z)P (\u03c0|z) ,\nand the final prediction of the label z for a test sample x is made by argmaxz P (z|x). The sum over (topK) rankings \u2211 \u03c0\u2208R cannot be computed analytically for either of the Plackett-Luce and the Mallows models and requires approximations, e.g., by Markov chain Monte\n3It is not immediately clear whether the Mallows model can be adapted in this setting and is left for future work.\nCarlo (MCMC) sampling. Alternatively, we use P (\u03c0|z) = I[\u03c0 = \u03c0z0 ] and a uniform prior P (z), somewhat similar to (Rohrbach et al., 2010). In our preliminary experiments, MCMC-based summation showed inferior results to this simple version and therefore will be omitted from the report. The final zero-shot classifier is the MAP classifier\nargmax z K\u220f i=1\ne f(\u03c0z0) -1(i)(x)\u2211C j=i e f(\u03c0z0) -1(j)(x) , (16)\nfor pre-trained one-vs-rest/multiclass-loss classifiers,\nand argmax z K\u220f i=1 C\u220f j=i+1 e f(\u03c0z0) -1(i),(\u03c0z0) -1(j)(x), (17)\nfor pre-trained one-vs-one classifiers. We summarize the overall training and testing procedures below.\nTraining Step 1. Obtain pre-trained classifiers\n\u2022 Input: training-domain sample and label pairs {(x1, y1), ..., (xN , yN )}, regularization hyperparameter \u2022 Output: score functions f1, ..., fC or f1,2, ..., fC\u22121,C \u2022 Method: one-vs-rest/one-vs-one/multiclass with any\nclassifier\nTraining Step 2. Learn priors for semantic rankings\n\u2022 Input: ranking and test-domain label pairs {(\u03c01, z1), ..., (\u03c0M , zM )} collected from corpora or crowdsourcing \u2022 Output: consensus rankings \u03c0z0 for each test-domain class z = 1, ..., L from either the Plackett-Luce model (8) or the Mallows (11) \u2022 Method: MLE of (9) by BFGS or MLE of (12) by sequential transposition\nTesting. Zero-shot classification\n\u2022 Input: data x, parameter K for top-K list size \u2022 Output: prediction of test-domain label z \u2022 Method: MAP estimation (16) or (17), using f(x)\u2019s\nfrom Training Step 1 and \u03c0z0 \u2019s from Training Step 2"}, {"heading": "3. Related work", "text": "There are two major approaches to zero-shot learning explored in the literature: attribute-based and similaritybased. In attribute-based knowledge transfer (e.g., (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes. Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010). However, designing the attributes that are discriminative, common to multiple classes, and correlated with the original feature at the same time, can be a non-trivial task that typically requires human supervision. Similar arguments\ncan be found in (Rohrbach et al., 2010) or (Mensink et al., 2014).\nBy contrast, similarity-based zero-shot approaches use semantic similarity between training-domain classes Y and test-domain classes Z directly. The advantage of this approach is that similarity information can be mined automatically from the web, linguistic corpora or other sources. Similarity information has been used to build a probabilistic zero-shot classifier called direct similarity-based method (DS) (Rohrbach et al., 2010; 2011), which parallels the attribute-based approach from (Lampert et al., 2009). Direct similarity-based method also uses classification scores and probabilistic inference as ours, but it uses numerical similarity instead of non-metric ranking presentation in our method. More recently, similarity-based approaches using semantic embedding have been proposed (Frome et al., 2013; Socher et al., 2013). In these algorithms, training and test domain objects are simultaneously embedded into a semantic space using multilayer neural networks. While these two methods produce interesting results, they use specific metric similarity models and require retraining when the semantic model changes, unlike our method. Mensink et al. use a linear combination of pre-trained classifiers for classifying unseen data (Mensink et al., 2014). They use co-occurrence statistics as semantic information, whereas we do not assume a specific type of similarity information. Lastly, our method provides a means to aggregate multiple semantic sources that has not been addressed in the literature."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Datasets", "text": "We use two datasets 1) Animals with Attributes dataset (Lampert et al., 2009) and 2) CIFAR-100/10 (Torralba et al., 2008) collected by (Krizhevsky, 2009). Semantic similarity is obtained from WordNet distance, web searches (Rohrbach et al., 2010), word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and from Amazon Mechanical Turk. Table 1 summarizes the characteristics of the datasets and the types of available semantic information used in the experiments. More details on data processing are provided in Appendix."}, {"heading": "4.2. Methods", "text": "We perform comprehensive tests of the probabilistic ranking-based (PR) zero-shot model under 1) three learning settings (one-vs-rest, one-vs-one, multiclass), 2) two types of semantic sources (linguistic, crowd-sourcing), and 3) different prior models for semantic rankings (the Plackett-Luce and the Mallows models). We compare probabilistic ranking-based method (PR, Sec. 2.5) to de-\nterministic ranking-based method (DR, Sec. 2.1)and direct similarity-based method (DS, (Rohrbach et al., 2010)) which is the closest state-of-the-art to our methods that uses classifier scores. We also refer to other results in the literature for comparison.\nRegularization parameters for classifiers are determined from the validation set and partially manually to avoid exhaustive cross-validation. We test with different hyperametersK (in top-K list) and report the results withK = 4. For one-vs-rest and one-vs-one, we trained SVMs followed by Platt\u2019s probabilistic scaling (Platt, 1999). For multiclass, we used multinomial logistic regression."}, {"heading": "4.3. Result 1 \u2013 Discriminability of semantic rankings", "text": "We first compare the discriminability of classes with ranking vs numerical representations of similarity without using image data. Using all five linguistic sources for Animals, we compute pairwise distances of 5\u00d710=50 similarity vectors. Two types of distances are computed \u2013 the Euclidean distance of numerical similarity, with or without l1 normalization, and the Hausdorff distance for top-K lists using the Kendall\u2019s ranking distance (5). Note that the rankings are obtained by sorting the numerically similarity. For these different representations, the average accuracy of leave-one-out 1-Nearest Neighbor classification was 0.44 (Euclidean), 0.62 (Euclidean with normalization), 0.72 (Kendall\u2019s, K=2), 0.70 (Kendall\u2019s, K=5), and 0.64 (Kendall\u2019s, K=10), which shows that the ranking distances are better than the Euclidean distances for discriminating\ntest-domain classes when there are multiple heterogeneous sources. Figure 3 shows the embeddings from classical Multidimensional Scaling (MDS) using these distances. It shows qualitative differences of numerical similarity (a and b) and ranking (c). The embedding of rankings has better between-class separation and within-class clustering than the embeddings of numerical similarity, suggesting that the non-metric order information is more consistent than numerical similarity across different sources.\nWe also compute leave-one-out accuracy of Bayesian classification with the rankings collected directly from crowdsourcing for Animals and CIFAR datasets. Out of 500 rankings, one ranking is held out and the 499 remaining rankings are used to build 10 semantic ranking probabilities P (\u03c0|z) using both the Mallows and the Plackett-Luce models. Prediction of the class of the held-out rankings is made by the maximum of P (\u03c0|z) over all 10 classes. For Animals, the average accuracy was 0.91/0.99/0.99 (K=2/5/10) using the Mallows model, and 0.79/0.84/0.96 (K=2/5/10) using the Plackett-Luce model. For CIFAR, the average accuracy was 0.73/0.79/0.84 (K=2/5/10) using the Mallows model, and 0.72/0.77/0.74 (K=2/5/10) using the PlackettLuce model. These numbers show that the rankings obtained from crowd-sourcing have information to discriminate the test-domain classes with up to 0.8 \u223c 1.0 accuracy."}, {"heading": "4.4. Result 2 - Comparison of PR, DR, and DS", "text": "We compare probabilistic ranking (PR), deterministic ranking (DR) and direct similarity (DS) methods for zero-shot classification accuracy. All three methods share the same image features and the same linguistic sources of semantic information (except for the crowd-sourcing for PR), but use them in different ways. PR uses probabilistic models to combine multiple sources of semantic similarity. DR and DS inherently use a single source of semantic similarity, and therefore the multiple sources have to be combined heuristically. We first normalize individual similarity sources to be in the range from 0 to 1, and then compute arithmetic and geometric means over multiple sources. Note that the main difference between DR and DS, is that DR uses rankings whereas DS uses numeric values.\nDS vs DR. The results are shown in Table 2. For both DR and DS, using averaged semantic similarity (\u201cArithm\u201d\nand \u201cGeom\u201d) is better than using individual similarity (\u201cIndiv\u201d), for both Animals and CIFAR datasets. A plausible interpretation is that the aggregate similarity is more reliable than individual similarities despite using heuristic methods of aggregation. The highest accuracy from DS is 0.354 (for Animals) and 0.316 (for CIFAR), whereas the hight accuracy from DR is 0.359 (for Animals) and 0.281 (for CIFAR). DR performs slightly better than DS in Animals, but worse in CIFAR. Within DR, accuracy is not affected much by the pre-trained classifier type (one-vs-rest, one-vs-one, multiclass).\nPR vs others. Using the same linguistic sources, the highest accuracy from PR is 0.370 (for Animals) and 0.339 (for CIFAR) which are much higher than DS and DR regardless of whether a single (Indiv) or multiple (Arithm and Geom) sources are used. This suggests the advantage of using probabilistic models to aggregate multiple semantic sources. Within PR results, one-vs-rest and one-vs-one classifiers perform comparably, and multiclass logistic regression performs the best. PR performs even better with crowd-sourced semantic information (0.395) than with linguistic sources (0.370) in Animals, but the opposite is true in CIFAR, probably due to the less reliability of human subject ratings with CIFAR (sorting 100 categories correctly compared to 40 in Animals).\nIn the literature, the accuracy of attributes-based methods with Animals ranges from 0.36 to 0.44 (Tables 3 and 4, (Akata et al., 2013)), compared to 0.395 from our method which do not use attributes. We remind the reader that finding \u2018good\u2019 attributes is itself a non-trivial task. When both similarity and attributes are mined automatically from corpora, similarity-based methods perform much better than attributed-based methods (individual average of 0.22 from\nTable 1, (Rohrbach et al., 2010)).\nLastly, Figure 4 shows two-class classification accuracy of PR (PL+linguistic sources), DS, and an embedding-based method on select pairs of classes from CIFAR (Figure 3, (Socher et al., 2013)). Although the numbers may not be directly comparable due to different settings4, PR performs noticeably better than the two state-of-the-arts. In fact, we can distinguish auto vs deer, deer vs ship, or cat vs truck with \u223c 95% accuracy, without a single training image of these categories."}, {"heading": "5. Conclusion", "text": "In this paper, we propose ranking-based representation of semantic similarity, as an alternative to metric representation of similarity. Using rankings, semantic information from multiple sources can be aggregated naturally to produce a better representation than individual sources. Using this representation and probability models of rankings, we present new zero-shot classifiers which can be constructed\n4Socher et al. used the rest of classes from CIFAR-10 instead of CIFAR-100 for training, and also used different semantic information\nfrom pretrained classifiers without retraining, and demonstrate their potential for exploiting semantic structures of real-world visual objects."}, {"heading": "A. Datasets", "text": "The Animals with Attributes dataset (Animals) was collected and processed by (Lampert et al., 2009). The training domain consists of images of 40 types of animals, from which 21,847 and 2,448 images were used as training and validation sets. From each image, 10,942 dimensional features are extracted (Lampert et al., 2009). The test domain consists of 6,180 images of 10 types of animals which are non-overlapping with the training-domain classes. Semantic similarity of the animals are provided by (Rohrbach et al., 2010) 5, which are computed from five different linguistic sources: path distance from WordNet, co-occurrence from Wikipedia, Yahoo web search, Yahoo image Search, and Flickr image search.\nThe CIFAR-100 and CIFAR-10 are collected by (Krizhevsky, 2009). The training domain (CIFAR100) consists of 60,000 images of 100 types of objects including animals, plants, household objects, and scenery. We use 50,000 and 10,000 images from CIFAR-100 as training and validation sets. The test domain (CIFAR-10) consists of 60,000 images of 10 types of objects similar to CIFAR-100, without any overlap with the classes from CIFAR-100. We use 10,000 images as test data. To compute features, we use a deep-trained neural network 6, which is trained from ImageNet ILSVRC2010 dataset7 consisting of 1.2 million images of 1000 categories. We apply CIFAR-100 and CIFAR-10 training images to the network, and use the 4096-dimensional output from the last hidden layer of the network as features. For semantic similarity of CIFAR-100 and CIFAR-10, we compute the WordNet path distance, and also used word2vec tools (Mikolov et al., 2013) 8 and GloVe tools (Pennington et al., 2014)9.\nIn addition to using linguistic sources, we use Amazon Mechanical Turk to collect word similarity data by crowdsourcing. Each participant of the survey is shown a word from the test domain classes, and is asked to sort 10 words from the training domain according to their perceived similarity to the given word. The initial order of 10 words is randomized for each survey. We pre-select those 10 closest words for each test-domain word, because we found from preliminary trials that ordering all words (40 for Animal and 100 for CIFAR) is too demanding and time-consuming for participants. For Animal, 10 closest words are selected based on the average ranking of the words w.r.t. the test-\n5http://www.d2.mpi-inf.mpg.de/nlp4vision 6https://github.com/jetpacapp/\nDeepBeliefSDK 7http://www.image-net.org/challenges/ LSVRC 8https://code.google.com/p/word2vec/ 9http://nlp.stanford.edu/projects/glove/\ndomain word from the five linguistic sources. For CIFAR, we use the path distance from WordNet. Fifty surveys are collected for each test-domain class.\nB. Implementation The Direct Similarity-based method (DS) (Rohrbach et al., 2010) is implemented as follows. The probability P (yk|x) is modeled by a one-vs-rest binary SVM classifier followed by the Platt\u2019s probabilistic scaling (Platt, 1999), trained with training-domain feature and label pairs. In testing, the probability P (yk|x) is evaluated for a test image, and the prediction of the test-domain class is made by MAP estimation using\nP (z|x) \u221d K\u220f k=1 ( P (yk|x) P (yk) )yzk , yzk = w z yk / 5\u2211 i=1 wzyi (18)\nwhere wzyk is the similarity score. The sum above is limited to five most similar training-domain classes. We have tested different values of the prior P (yk), which did not have visible effects on the result."}], "references": [{"title": "Label-embedding for attribute-based classification", "author": ["Akata", "Zeynep", "Perronnin", "Florent", "Harchaoui", "Zaid", "Schmid", "Cordelia"], "venue": "In CVPR,", "citeRegEx": "Akata et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Akata et al\\.", "year": 2013}, {"title": "Voting schemes for which it can be difficult to tell who won", "author": ["J. Bartholdi", "C.A. Tovey", "M. Trick"], "venue": "Social Choice and Welfare,", "citeRegEx": "Bartholdi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bartholdi et al\\.", "year": 1989}, {"title": "Label ranking methods based on the Plackett-Luce model", "author": ["Cheng", "Weiwei", "Dembczynski", "Krzysztof", "Hllermeier", "Eyke"], "venue": "In ICML, pp", "citeRegEx": "Cheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2010}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Crammer", "Koby", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2002}, {"title": "Probability models on rankings", "author": ["Critchlow", "Douglas E", "Fligner", "Michael A", "Verducci", "Joseph S"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Critchlow et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Critchlow et al\\.", "year": 1991}, {"title": "Metric methods for analyzing partially ranked data. Number 34 in Lecture notes in statistics", "author": ["Critchlow", "Douglas Edward"], "venue": null, "citeRegEx": "Critchlow and Edward.,? \\Q1985\\E", "shortCiteRegEx": "Critchlow and Edward.", "year": 1985}, {"title": "Visual and semantic similarity in imagenet", "author": ["T. Deselaers", "V. Ferrari"], "venue": "In CVPR, pp", "citeRegEx": "Deselaers and Ferrari,? \\Q2011\\E", "shortCiteRegEx": "Deselaers and Ferrari", "year": 2011}, {"title": "On a model for concordance between judges", "author": ["Feigin", "Paul D", "Cohen", "Ayala"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Feigin et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Feigin et al\\.", "year": 1978}, {"title": "Distance based ranking models", "author": ["Fligner", "Michael A", "Verducci", "Joseph S"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Fligner et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fligner et al\\.", "year": 1986}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "MM algorithms for generalized Bradley-Terry models", "author": ["Hunter", "David R"], "venue": "The Annals of Statistics,", "citeRegEx": "Hunter and R.,? \\Q2004\\E", "shortCiteRegEx": "Hunter and R.", "year": 2004}, {"title": "On the Babington Smith class of models for rankings. In Probability models and statistical analyses for ranking", "author": ["Joe", "Harry", "Verducci", "Joseph S"], "venue": null, "citeRegEx": "Joe et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Joe et al\\.", "year": 1993}, {"title": "Mathematics without numbers", "author": ["Kemeny", "John G"], "venue": "Daedalus, pp", "citeRegEx": "Kemeny and G.,? \\Q1959\\E", "shortCiteRegEx": "Kemeny and G.", "year": 1959}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR, pp", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Individual Choice Behavior", "author": ["R.D. Luce"], "venue": null, "citeRegEx": "Luce,? \\Q1959\\E", "shortCiteRegEx": "Luce", "year": 1959}, {"title": "Non-null ranking models", "author": ["Mallows", "Colin L"], "venue": "i. Biometrika,", "citeRegEx": "Mallows and L.,? \\Q1957\\E", "shortCiteRegEx": "Mallows and L.", "year": 1957}, {"title": "Analyzing and modeling rank data, volume 64", "author": ["Marden", "John I"], "venue": "CRC Press,", "citeRegEx": "Marden and I.,? \\Q1995\\E", "shortCiteRegEx": "Marden and I.", "year": 1995}, {"title": "Consensus ranking under the exponential model", "author": ["Meila", "Marina", "Phadnis", "Kapil", "Patterson", "Arthur", "Bilmes", "Jeff A"], "venue": null, "citeRegEx": "Meila et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meila et al\\.", "year": 2007}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela"], "venue": "In ECCV,", "citeRegEx": "Mensink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2012}, {"title": "Costa: Co-occurrence statistics for zero-shot classification", "author": ["Mensink", "Thomas", "Gavves", "Efstratios", "Snoek", "Cees GM"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Mensink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["Miller", "George A"], "venue": "Commun. ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Zero-shot learning with semantic output codes", "author": ["Palatucci", "Mark", "Pomerleau", "Dean", "Hinton", "Geoffrey", "Mitchell", "Tom"], "venue": "In NIPS, pp", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The analysis of permutations", "author": ["Plackett", "Robin L"], "venue": "Applied Statistics, pp", "citeRegEx": "Plackett and L.,? \\Q1975\\E", "shortCiteRegEx": "Plackett and L.", "year": 1975}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["Platt", "John C"], "venue": "In ADVANCES IN LARGE MARGIN CLASSIFIERS,", "citeRegEx": "Platt and C.,? \\Q1999\\E", "shortCiteRegEx": "Platt and C.", "year": 1999}, {"title": "Towards cross-category knowledge propagation for learning visual concepts", "author": ["Qi", "Guo-Jun", "C. Aggarwal", "Rui", "Yong", "Tian", "Chang", "Shiyu", "T. Huang"], "venue": null, "citeRegEx": "Qi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2011}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "Rohrbach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2011}, {"title": "What helps where and why? semantic relatedness for knowledge transfer", "author": ["Rohrbach", "Marcus", "Stark", "Michael", "Szarvas", "Gyorgy", "Gurevych", "Iryna", "Schiele", "Bernt"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2010}, {"title": "Discussion of professor Ross\u2019s paper", "author": ["Smith", "B Babington"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Smith and Babington.,? \\Q1950\\E", "shortCiteRegEx": "Smith and Babington.", "year": 1950}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In NIPS, pp", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}], "referenceMentions": [{"referenceID": 10, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 22, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 25, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 10, "context": "Figure 1 (Right) is an example MDS embedding from the representation from (Huang et al., 2012).", "startOffset": 74, "endOffset": 94}, {"referenceID": 10, "context": "Right: MDS embedding from (Huang et al., 2012).", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 15, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 30, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 28, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 20, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 9, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 32, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 15, "context": "In the experiment section we demonstrate the advantages of our approach over a numerically-based approach and a deterministic approach using two well-known image databases Animals-with-attributes (Lampert et al., 2009) and CIFAR-10/100 (Krizhevsky, 2009).", "startOffset": 196, "endOffset": 218}, {"referenceID": 16, "context": "We discuss three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see (Critchlow, 1985; Critchlow et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 4, "context": "We discuss three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see (Critchlow, 1985; Critchlow et al., 1991; Marden, 1995) for more reviews.", "startOffset": 227, "endOffset": 282}, {"referenceID": 1, "context": "The minimization (12) is also known as the Kemeny optimal consensus or aggregation problem (Kemeny, 1959) and is known to be NP-hard (Bartholdi et al., 1989).", "startOffset": 133, "endOffset": 157}, {"referenceID": 19, "context": "However, there are known heuristic methods such as sequential transposition of adjacent items (Critchlow, 1985) or other admissible heuristics (Meila et al., 2007).", "startOffset": 143, "endOffset": 163}, {"referenceID": 2, "context": "Note that if the original classifier is a multinomial logistic regression, the (14) is in fact a direct generalization of logistic regression for K = 1, which is also observed in (Cheng et al., 2010).", "startOffset": 179, "endOffset": 199}, {"referenceID": 30, "context": "Alternatively, we use P (\u03c0|z) = I[\u03c0 = \u03c0 0 ] and a uniform prior P (z), somewhat similar to (Rohrbach et al., 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 24, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 15, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 0, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 24, "context": "Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010).", "startOffset": 70, "endOffset": 117}, {"referenceID": 30, "context": "Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010).", "startOffset": 70, "endOffset": 117}, {"referenceID": 30, "context": "Similar arguments can be found in (Rohrbach et al., 2010) or (Mensink et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 21, "context": ", 2010) or (Mensink et al., 2014).", "startOffset": 11, "endOffset": 33}, {"referenceID": 30, "context": "Similarity information has been used to build a probabilistic zero-shot classifier called direct similarity-based method (DS) (Rohrbach et al., 2010; 2011), which parallels the attribute-based approach from (Lampert et al.", "startOffset": 126, "endOffset": 155}, {"referenceID": 15, "context": ", 2010; 2011), which parallels the attribute-based approach from (Lampert et al., 2009).", "startOffset": 65, "endOffset": 87}, {"referenceID": 9, "context": "More recently, similarity-based approaches using semantic embedding have been proposed (Frome et al., 2013; Socher et al., 2013).", "startOffset": 87, "endOffset": 128}, {"referenceID": 32, "context": "More recently, similarity-based approaches using semantic embedding have been proposed (Frome et al., 2013; Socher et al., 2013).", "startOffset": 87, "endOffset": 128}, {"referenceID": 21, "context": "use a linear combination of pre-trained classifiers for classifying unseen data (Mensink et al., 2014).", "startOffset": 80, "endOffset": 102}, {"referenceID": 15, "context": "We use two datasets 1) Animals with Attributes dataset (Lampert et al., 2009) and 2) CIFAR-100/10 (Torralba et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 30, "context": "Semantic similarity is obtained from WordNet distance, web searches (Rohrbach et al., 2010), word2vec (Mikolov et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 22, "context": ", 2010), word2vec (Mikolov et al., 2013), GloVe (Pennington et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": ", 2013), GloVe (Pennington et al., 2014), and from Amazon Mechanical Turk.", "startOffset": 15, "endOffset": 40}, {"referenceID": 22, "context": "Animals CIFAR Feature dimension 8941 4000 Number of training/validation/test samples 21847 / 2448 / 6180 50000 / 50000 / 10000 Number of training/test classes 40 / 10 100 / 10 Linguistic sources WordNet, Wikipedia, Yahoo, YahooImage, Flicker WordNet, word2vec (Mikolov et al., 2013), GloVe (Pennington et al.", "startOffset": 260, "endOffset": 282}, {"referenceID": 25, "context": ", 2013), GloVe (Pennington et al., 2014) Number of surveys from crowd-sourcing 500 500", "startOffset": 15, "endOffset": 40}, {"referenceID": 30, "context": "1)and direct similarity-based method (DS, (Rohrbach et al., 2010)) which is the closest state-of-the-art to our methods that uses classifier scores.", "startOffset": 42, "endOffset": 65}, {"referenceID": 0, "context": "44 (Tables 3 and 4, (Akata et al., 2013)), compared to 0.", "startOffset": 20, "endOffset": 40}, {"referenceID": 30, "context": "Table 1, (Rohrbach et al., 2010)).", "startOffset": 9, "endOffset": 32}, {"referenceID": 32, "context": "Lastly, Figure 4 shows two-class classification accuracy of PR (PL+linguistic sources), DS, and an embedding-based method on select pairs of classes from CIFAR (Figure 3, (Socher et al., 2013)).", "startOffset": 171, "endOffset": 192}], "year": 2015, "abstractText": "In this paper we propose a non-metric rankingbased representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zeroshot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.", "creator": "LaTeX with hyperref package"}}}