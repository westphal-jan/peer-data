{"id": "1503.01002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Projection onto the capped simplex", "abstract": "we provide a simple expressive and efficient algorithm for computing the euclidean triangular projection of a point onto the capped infinite simplex - - - a simplex with an additional fairly uniform bound on both each coordinate - - - together with an absolute elementary proof. both the matlab and c + + implementations of the proposed algorithm can be downloaded at", "histories": [["v1", "Tue, 3 Mar 2015 16:40:17 GMT  (10kb)", "http://arxiv.org/abs/1503.01002v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "canyi lu"], "accepted": false, "id": "1503.01002"}, "pdf": {"name": "1503.01002.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu", "canyilu@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n01 00\n2v 1\n[ cs\n.L G\n] 3\nM ar\nWe provide a simple and efficient algorithm for computing the Euclidean projection of a point onto\nthe capped simplex, formally defined as\nmin x\u2208RD\n1 2 \u2016x\u2212 y\u20162 s.t. x\u22a41 = s, 0 \u2264 x \u2264 1,\ntogether with an elementary proof. Both the MATLAB and C++ implementations of the proposed algorithm can be downloaded at https://eng.ucmerced.edu/people/wwang5."}, {"heading": "1 The Problem", "text": "In this report, we consider the following optimization problem\nmin x\u2208RD\n1 2 \u2016x\u2212 y\u20162 (1a)\ns.t. x\u22a41 = s (1b)\n0 \u2264 x \u2264 1, (1c)\nwhere s \u2208 [0, D] is a parameter of the problem, 0 and 1 are vectors of 0\u2019s and 1\u2019s respectively, and \u2264 means elementwise comparison. The feasible set of this problem is the intersection of the unit cube and a hyperplane with normal 1. Alternatively, the feasible set is the simplex {x : x \u2265 0, x\u22a41 = s} with an additional capping constraints x \u2264 1, so we call it the capped simplex. Problem 1 is a quadratic program and the objective function is strictly convex, so there is a unique solution which we denote by x = [x1, . . . , xD] \u22a4 with a slight abuse of notation.\nRemark 1.1. This problem is a slight generalization of the projection onto the probability simplex (see Duchi et al., 2008; Wang and Carreira-Perpin\u0303a\u0301n, 2013 and the references therein), which is a special case of (1) by setting s = 1 and can be solved exactly with O(D logD) time complexity. An elementary proof of the corresponding algorithm can be found in Wang and Carreira-Perpin\u0303a\u0301n (2013) and the cost mainly comes from sorting the dimensions of y. Our solution to (1) in this report is derived using a similar idea.\nRemark 1.2. The constraint 0 \u2264 x \u2264 1 in (1) can be generalized to 0 \u2264 x \u2264 t1 where t is any positive number. We only need to solve the following instance of (1):\nmin x\u0302\n1 2 \u2016x\u0302\u2212 y/t\u2016 2 , s.t. x\u0302\u22a41 = s/t, 0 \u2264 x\u0302 \u2264 1,\nand then scale its solution x\u0302 by t to obtain the solution of the original problem."}, {"heading": "2 The algorithm", "text": "We provide an O(D2) algorithm for solving (1) in Algorithm 1.\nAlgorithm 1 Euclidean projection of a vector onto the section of cube.\nInput: y \u2208 RD is sorted in ascending order: y1 \u2264 y2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 yD. 1: Set y0 = \u2212\u221e and yD+1 = \u221e, compute partial sums T0 = 0, and Tk = \u2211k j=1 yk, k = 1, . . . , D.\n2: for a = 0, 1, . . . , D do 3: if (s == D \u2212 a) && (ya+1 \u2212 ya \u2265 1) then 4: Set b=a. 5: break 6: end if 7: for b = a+ 1, . . . , D do 8: Compute \u03b3 = s+b\u2212n+Ta\u2212Tb\nb\u2212a .\n9: if (ya + \u03b3 \u2264 0) && (ya+1 + \u03b3 > 0) && (yb + \u03b3 < 1) && (yb+1 \u2265 1) then 10: break 11: end if 12: end for 13: end for Output: x = [0, . . . , 0, ya+1 + \u03b3, . . . , yb + \u03b3, 1, . . . , 1]."}, {"heading": "3 The proof", "text": "As mentioned earlier, (1) has a unique solution which is characterized by its KKT system (Nocedal and Wright, 2006). The Lagrangian function of the problem is\nL(x,\u03b1,\u03b2, \u03b3) = 1\n2 \u2016x\u2212 y\u2016\n2 \u2212\u03b1\u22a4x\u2212 \u03b2\u22a4(1\u2212 x)\u2212 \u03b3(1\u22a4x\u2212 s)\nwhere \u03b1 = [\u03b11, . . . , \u03b1D] \u22a4 and \u03b2 = [\u03b21, . . . , \u03b2D] \u22a4 are the Lagrange multipliers for the inequality constraints x \u2265 0 and 1 \u2212 x \u2265 0 respectively, and \u03b3 is the Lagrange multiplier for the equality constraint. At the optimal solution x the following KKT conditions hold:\nxi \u2212 yi \u2212 \u03b1i + \u03b2i \u2212 \u03b3 = 0, i = 1, . . . , D (2a)\nxi \u2265 0, i = 1, . . . , D (2b)\nxi \u2264 1, i = 1, . . . , D (2c)\n\u03b1i \u2265 0, i = 1, . . . , D (2d)\n\u03b2i \u2265 0, i = 1, . . . , D (2e) \u2211D\ni=1 xi = s, (2f)\n\u03b1ixi = 0, i = 1, . . . , D (2g)\n\u03b2i(1\u2212 xi) = 0, i = 1, . . . , D, (2h)\nwhere (2g) and (2g) are complementary slackness (CS) conditions. Without loss of generality, we assume the components of the optimal solution x are in ascending order:\n0 = x1 = \u00b7 \u00b7 \u00b7 = xa < xa+1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 xb < xb+1 = . . . xD = 1, (3)\nwhere a is the number of 0\u2019s in the solution while D \u2212 b is the number of 1\u2019s in the solution. The valid ranges for (a, b) are 0 \u2264 a \u2264 D and a \u2264 b \u2264 D. The KKT conditions can be simplified for different set of dimensions of the solution:\n(i) For i = 1, . . . , a, the CS condition (2h) indicates \u03b2i = 0, and thus\n0 = xi = yi + \u03b1i + \u03b3 \u2265 yi + \u03b3, (4)\nwhere the last inequality uses the fact that \u03b1i \u2265 0.\n(ii) For j = b+ 1, . . . , D, the CS condition (2g) indicates \u03b1j = 0, and thus\n1 = xj = yj \u2212 \u03b2j + \u03b3 \u2264 yj + \u03b3, (5)\nwhere the last inequality uses the fact that \u03b2j \u2265 0.\n(iii) For k = a+ 1, . . . , b, the CS conditions indicate \u03b1k = \u03b2k = 0, and thus\n0 < xk = yk + \u03b3 < 1. (6)\nIt is then clear that for any 1 \u2264 i \u2264 a, b+ 1 \u2264 j \u2264 D and a+ 1 \u2264 k \u2264 b, we have\nyi \u2264 \u2212\u03b3 < yk < 1\u2212 \u03b3 \u2264 yj . (7)\nIn other words, if the dimensions of y are sorted in ascending order, the corresponding dimensions of the solution x is also in ascending order. Therefore, the first step of our algorithm is to sort dimensions of y into ascending order. And all that is left is to find (a, b), the partition of x into the three segments. The only KKT condition we have not used so far is the sum constraint (2f), which now reduces to\nD\u2211\ni=1\nxi = a \u00b7 0 + b\u2211\nk=a+1\n(yk + \u03b3) + (n\u2212 b) \u00b7 1 = s. (8)\nThis means that if we know (a, b) for the solution x, we must have\n\u03b3 = s+ b\u2212 n\u2212\n\u2211b k=a+1 yk\nb\u2212 a . (9)\nSince there are only (D+1)(D+2)2 possible combinations for the indices (a, b), we could test each combination and compute the hypothesized \u03b3 value using (9). With the hypothesized \u03b3, the tests we need for (a, b, \u03b3) to produce the optimal x are the following:\nya + \u03b3 \u2264 0, ya+1 + \u03b3 > 0, yb + \u03b3 < 1, yb+1 + \u03b3 \u2265 1. (10)\nIt is easy to verify that the (a, b, \u03b3) combination that passes the above test leads to (x,\u03b1,\u03b2, \u03b3) that satisfy all the KKT conditions, where {\u03b1i} a i=1 and {\u03b2j} D j=b+1 can be retrieved using (4) and (5) respectively.\nRemark 3.1. A special case is when a = b, for which (9) is ill-defined. In this case, the optimal solution consists of a 0\u2019s and n \u2212 a 1\u2019s. For this to happen, we must have s = D \u2212 a and then (10) reduces to ya+1 + \u03b3 \u2265 1 and ya + \u03b3 \u2264 0, and so ya+1 \u2212 ya \u2265 1. Remark 3.2. The reason why the computational complexity of our algorithm is O(D2) for (1) as opposed to O(D logD) for projection onto probability simplex is due to the constraint x \u2264 1. This constraint is automatically satisfied for the projection onto probability simplex problem where s = 1, in which case we only need to figure out a\u2014the number of zero dimensions in the solution.\nRemark 3.3. In view of the previous remark, another way of solving (1) is to alternatively project the estimate onto the (scaled) probability simplex {x : x \u2265 0, x\u22a41 = s} and the set {x : x \u2264 1} for which the projection is trivial to compute (we simply threshold the dimensions that are greater than 1 to 1). And yet another approach is to apply the Alternating Direction Method of Multipliers (Boyd et al., 2011), which introduces another copy of the variables x and alternately optimize each copy with simple steps while encouraging the two copies to agree. We note that these approaches are iterative and the number of iterations depends on the desired accuracy. On the contrary, our method finds the exact solution within a fixed number of steps."}, {"heading": "4 Experiment", "text": "In this section, we demonstrate the effectiveness of our proposed method in Algorithm 1 for solving (1). We compare our method with another two solvers. The first one is the CVX package (Grant and Boyd,\n2012), a general convex program solver which transforms the problem into a semi-definite program and then applies interior point method. And the second one is the MATLAB command lsqlin for solving constrained linear least squares. We implement Algorithm 1 in both MATLAB and C++ and conduct all experiments in MATLAB (the C++ code is compiled within MATLAB and the mex-file is used).\nAll experiments are run on a PC with an Intel Core 2 Quad CPU Q9550 of frequency 2.83GH and 8GB main memory, under Windows 7 and MATLAB version 8.0. We generate y and s using the MATLAB commands\ny=rand(D,1)-0.5;\ns=round(rand*D);\nand record the running time of each method using tic and toc. We choose the dimension D of y from {50, 100, 500, 1000, 2000, 5000, 10000, 20000, 100000}. For each choice of D, the experiments are repeated 20 times and the average running times are reported for comparison.\nThe results are shown in Table 1. It can be seen that our proposed method is always faster than CVX and lsqlin for different dimensions D of y. And as D increases, the improvement of our method over the others becomes more significant. If D is relatively large, the compared solvers may run out of memory (denoted as \u2018\u2212\u2019 in Table 1). These results confirm that it is beneficiary to explore the special structures of our problem rather than using general convex program solvers. Furthermore, the C++ version of our method is by two orders of magnitude faster than the MATLAB version; this improvement is important for projections in very high dimensions."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "Proc. of the 25th Int. Conf. Machine Learning", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.0 beta", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "Grant and Boyd.,? \\Q2012\\E", "shortCiteRegEx": "Grant and Boyd.", "year": 2012}, {"title": "Numerical Optimization. Springer Series in Operations Research and Financial Engineering", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application", "author": ["W. Wang", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": null, "citeRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Carreira.Perpi\u00f1\u00e1n.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "This problem is a slight generalization of the projection onto the probability simplex (see Duchi et al., 2008; Wang and Carreira-Perpi\u00f1\u00e1n, 2013 and the references therein), which is a special case of (1) by setting s = 1 and can be solved exactly with O(D logD) time complexity. An elementary proof of the corresponding algorithm can be found in Wang and Carreira-Perpi\u00f1\u00e1n (2013) and the cost mainly comes from sorting the dimensions of y.", "startOffset": 92, "endOffset": 381}, {"referenceID": 3, "context": "3 The proof As mentioned earlier, (1) has a unique solution which is characterized by its KKT system (Nocedal and Wright, 2006).", "startOffset": 101, "endOffset": 127}, {"referenceID": 0, "context": "And yet another approach is to apply the Alternating Direction Method of Multipliers (Boyd et al., 2011), which introduces another copy of the variables x and alternately optimize each copy with simple steps while encouraging the two copies to agree.", "startOffset": 85, "endOffset": 104}], "year": 2015, "abstractText": "We provide a simple and efficient algorithm for computing the Euclidean projection of a point onto the capped simplex, formally defined as min x\u2208RD 1 2 \u2016x\u2212 y\u20162 s.t. x1 = s, 0 \u2264 x \u2264 1, together with an elementary proof. Both the MATLAB and C++ implementations of the proposed algorithm can be downloaded at https://eng.ucmerced.edu/people/wwang5.", "creator": "LaTeX with hyperref package"}}}