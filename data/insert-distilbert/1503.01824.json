{"id": "1503.01824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Deep Clustered Convolutional Kernels", "abstract": "dynamic deep neural networks have finally recently achieved state of not the art performance thanks to new training routines algorithms for rapid parameter estimation and new regularization methods to reduce overfitting. however, in practice the network architecture has to be manually set away by domain experts, generally by a costly trial and error procedure, which accordingly often accounts for a fixed large desired portion of the final system performance. we view this as a inherent limitation and propose a novel simulated training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then uniformly eliminating model redundancy by selectively removing parameters at training time. for convolutional neural networks, our method relies on iterative split / merge clustering of convolutional kernels uniformly interleaved by stochastic gradient lateral descent. we present simply a training performance algorithm training and experimental results on three different vision tasks, showing improved prototype performance compared to similarly sized hand - crafted architectures.", "histories": [["v1", "Fri, 6 Mar 2015 00:53:40 GMT  (1058kb)", "http://arxiv.org/abs/1503.01824v1", "draft"]], "COMMENTS": "draft", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["minyoung kim", "luca rigazio"], "accepted": false, "id": "1503.01824"}, "pdf": {"name": "1503.01824.pdf", "metadata": {"source": "META", "title": "Deep Clustered Convolutional Kernels", "authors": ["Minyoung Kim", "Luca Rigazio"], "emails": ["MINYOUNG.KIM@US.PANASONIC.COM", "LUCA.RIGAZIO@US.PANASONIC.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n01 82\n4v 1\n[ cs\n.L G\n] 6\nM ar\n2 01"}, {"heading": "1. Introduction", "text": "Recently, deep neural networks (DNNs) have led to significant improvement in several machine learning domains, from speech recognition (Dahl et al., 2012) to computer vision (Krizhevsky et al., 2012; Taigman et al., 2013) and machine translation (Sutskever et al., 2014). DNNs have reached state of the art performance thanks to their theoretically proven modeling and generalization capabilities (Hornik et al., 1989; Hornik, 1991; Ku\u030arkova\u0301, 1992), and practically driven by improvements in training algorithms for rapid parameter estimation (Martens, 2010; Sutskever et al., 2013), novel regularization methods to reduce overfitting (Srivastava et al., 2014) as well as ever in-\nProceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ncreasing data-sets (Deng et al., 2009) and powerful new computing platforms (Chetlur et al., 2014). However, before parameter estimation (so called training) can begin the DNN\u2019s structure (also called model architecture) is usually manually defined by domain experts (Lin et al., 2013), and can often account for a substantial portion of the final system performance (Szegedy et al., 2014). We view this step as a bottleneck in the current deep-learning pipeline, one that relies on a trial and error human expert in the loop approach which is, to say the least, rather alchemic in nature. We want to address this basic scalability issue of the deep learning development pipeline with training methods that automatically search for DNN architectures while jointly estimating model parameters.\nWhile structural optimization is a notoriously difficult combinatorial task, successful strategies were adopted in the past for (shallow) models that motivated our approach. For instance, for Hidden Markov Models with Gaussian mixture kernels, split/merge algorithms were used to independently vary model complexity for each HMM state, resulting in improved accuracy for large vocabulary speech recognition (Sankar, 1998). Information theoretic methods, such as the minimum description length criterion, were also applied to the problem of structural optimization (Barron et al., 1998), resulting in improved performance in speech recognition (Shinoda & Watanabe, 2000) and as well as training algorithms for autoencoders (Hinton & Zemel, 1994). However, to the best of our knowledge, there is little published work on structural optimization in the deep learning community, with the notable exception of work based on empirical evaluation (Bergstra & Bengio, 2012) and random search strategies (Bergstra & Bengio, 2012). Although, recently Bayesian optimization of hyper-parameters have been introduced (Snoek et al., 2012).\nWhile these works are interesting, hyper-parameters are only one aspect of the DNN structure, albeit one which is closely related to the performance of the training algorithm. However, there are several other structural parameters that strongly affect DNN\u2019s performance which are usu-\nally set by experimental trial and error, such as network depth and for convolutional models the number of convolutional filters and kernel size for each layer. In our work, we aim to optimize model architecture, specifically targeting convolutional neural networks (CNNs), and optimizing complexity for each layer. Therefore, in our approach, the model architecture is not maintained constant during training, instead the model complexity is continuously optimized throughout the training step (parameter estimation by stochastic gradient descent), resulting, we believe, in a more scalable approach to the training of deep neural networks. In section 2, we describe the general approach we are taking for problem of structure optimization of convolutional neural networks. In section 2.1, we describe the theoretical foundations of our approach. In section 3, we discuss data-sets and experimental results and in section 4 we discuss about limitations and possible future improvements."}, {"heading": "2. Deep Clustered Convolutional Kernels", "text": "The basic idea for our Deep Clustered Convolutional Kernels (DCCKs) it a convolutional model architecture and associated structural training algorithm. We adopt a split/merge outer-loop to the training process that first increases model capacity to model new factors of variability seen in the data, then estimates new parameters for this larger model by stochastic gradient descent (SGD), and finally reduces model capacity to minimize model-space redundancy. Our approach takes inspiration by previous work in the area of Gaussian kernel HMMs (Sankar, 1998; Rigazio et al., 2000; Bocchieri & Mak, 2001; Lee et al., 2001), and is philosophically based on Occam\u2019s razor principle whereby a smaller model with similar performance on a given data-set is likely to have better generalization capabilities to new unseen data.\nAn alternative view of work may be in the context of recent developments in DNN\u2019s compression: (Ba & Caruana, 2014) shows that a (shallow) DNN can approach the performance of a substantially larger DNN when trained to mimic the logit output of the larger model. Similarly, (Hinton et al., 2014) shows that logit-mimic training (referred to as \u201cDark Knowledge\u201d) results in orders of magnitude smaller models, compared to the initial complex ensemble models, yet provides competitive performance when tested on both small tasks (MNIST) as well as large scale industrial tasks (large vocabulary speech recognition). It is important to notice that for both these works the authors acknowledge that, while such smaller high performance models can be obtained by logit mimic training from a more complex model set, thus showing that there is an optimal point in the parameter space with high performance, there is currently no known training procedure to directly\nAlgorithm 1 Deep Clustered Convolutional Kernels training algorithm\nInput: Initial network architecture net with parameters \u03bb, noise variance \u03c3n and jitter angle \u03c3\u03b1, stopping conditions \u03b40,1,2 and mini-batch size while \u2206 Validation Accuracy > \u03b40 do\nwhile \u2206 Validation Accuracy > \u03b41 do // SPLIT nk = gaussianNoise(\u03c3n) \u03b1k = gaussianNoise(\u03c3\u03b1) \u03bb1 = concat(\u03bb, \u03bb+ nk) \u03bb = concat(\u03bb1, rotate(kernel(\u03bb), \u03b1k)) // FINETUNE while \u2206 Validation Accuracy > \u03b42 do\nrunSGD(M minibatches) end while\nend while // MERGE centroid = Kmeans(kernels(\u03bb)) \u03bb = nearest(kernels(\u03bb), centroid) while \u2206 Validation Accuracy > \u03b42 do\nrunSGD(M minibatches) end while\nend while\nachieve such optimal point in the smaller model. In this view finding such an elusive point in parameter space by systematically optimizing DNN\u2019s structure to eliminate redundancy and minimizing number of parameters, while at the same time estimating the model parameters under the given loss function. The main contribution of our work is a training methodology to iteratively optimize the number of convolutional kernels while estimating the convolutional filter parameters."}, {"heading": "2.1. Training algorithm", "text": "Conceptually our training procedure is rather straightforward: starting from an initial network architecture, we first train the model by SGD until performance tops out on a validation set. Next, we increase the model complexity of selected convolutional layers by splitting the convolutional kernels. Splitting has the purpose of creating new plausible convolutional filters given the current set of filters and can be done by applying image pre-processing techniques to the kernels, as well as adding jittering and noise to create enough variation. After splitting, the model is again trained by SGD and possibly split again until performance tops out. At that point model is merged to reduce redundancy in the parameter space and again trained by SGD. Notice that the split/merge procedure can start at any layer but than has to propagate upwards to change the number of kernels of the connecting layers (fan-out). In our setup, given by input\ndata x, forward propagation f is done by:\nf(x) = g(Wx+B) (1)\nwhere g is ReLU activation function with g(x) = max(0, x), W is the weight parameters of the convolutional layer, and B is the biases, each with the following dimensions:\nWl = Nl \u00d7\nP \ufe37 \ufe38\ufe38 \ufe37 dl \u00d7 kl \u00d7 kl (2)\nBl = 1\u00d7 1\u00d7 1\u00d7Nl (3)\nwhere l is a convolutional layer with l \u2208 {1, ..., L}, Nl is the number of outputs of l, dl is number of channels of l, and kl is size of kernel used for l. We use square convolutional kernels, so kernel dimensions are kl \u00d7 kl. For simplicity, we define sub-dimension of W as P , shown in 2. For the first convolutional layer we have d1 = 3 for RGB images and d1 = 1 for gray-scale images. In the following convolutional layers, d is the output of the previous convolutional layer thus, P would be the size of the feature vectors. This implies that, when we perform the split/merge steps for level l, we need to update both Wl and Bl as well as Wl+1. Biases for the following convolutional layer are independent. An important caveat is that the order of the optimal split/merge operation depends on the specific data-set and the filter parameters. For instance, if the initial filters are sparse it is beneficial to do merge first. Otherwise, it is best to perform split first especially on smaller data-sets when the initial filters are already compact and discriminative."}, {"heading": "2.1.1. SPLITTING KERNELS", "text": "With splitting, we want to increase model complexity by creating new convolutional kernels from the set of existing well-trained kernels. Therefore, we create new kernels by selectively choosing from a fixed set of transformations. The possible set of transformations to play with is vast and includes the six isometries of the plane, angular rotation, change in contrast (negative \u201creversing\u201d) and many others. In our experiments, we focus on two transformations that seemed to provide a consistent improvement:\n\u2022 Rotation creates new kernels by rotating existing kernels in random directions.\n\u2022 Noise perturbation creates new kernels by adding Gaussian noise to the existing kernels.\nOne important aspect we verified in our experiments is that rotating kernels has a lower computational cost at training time than rotating training images to create augmented\ntraining set. Moreover, we observed that rotating the filters can help improve robustness for highly tilted objects outlets, which would be otherwise hard to correctly classify (see Figure 1). Adding random Gaussian noise, on the other hand, has the obvious benefit of creating diversity and helping with the SGD, like previously reported by (Srivastava et al., 2014). Regarding the splitting strategy, currently we took the simplest approach and split every kernel by a fixed amount. This is bound to be locally unoptimal, and surely a better splitting strategy that tries to maximize some diversity or discrimination criteria could be devised, instead of indiscriminately splitting every single kernel. However, for the most part, we observe that wasteful parameters created by this simple splitting strategy will be eliminated during the final merging step; therefore, aside from a potential sub-optimality in the CPU/Memory usage, we speculate the final model accuracy might not be very affected by this uniform splitting strategy."}, {"heading": "2.1.2. MERGING KERNELS", "text": "After the splitting step, the model might have too much capacity and thus part of the model might become overparameterized, possibly resulting in over-fitting and lower generalization power. Therefore the merging step has the\npurpose of removing model space redundancies and reducing model size, while maintaining the overall model accuracy. In our algorithm we use k-means clustering to merge kernels since, naturally, k-means cluster distortion under the defined distortion measure (we employ L2 norm to compute cluster distortion). We empirically observe that k-means clustering to merge filter maps is effective in reducing kernel\u2019s redundancy (see filters in 2.1.2). Then, we train the network and get weight and bias matrices from each convolutional layer to then choose the filters that are nearest to each centroid. We update Wl and Bl, with W \u2032l and B\u2032l using k-means clustering to get centroids C as:\nC = argmin P\nC\u2211\nj=1\n\u2211\np\u2208P\n||p\u2212 \u00b5j || 2 (4)\nW \u2032l =\n{ [P \u20321, ..., P \u2032 i , ..., P \u2032 C ], or\n[C1, ..., Ci, ..., CC ] (5)\nwhere\nP \u2032i = argmin P \u2032 ||P \u2032 \u2212 Ci || 2 , i = {1, ..., C} (6)\nand finally,\nB\u2032l =\n{ [B\u20321, ..., B \u2032 i, ..., B \u2032 C ], or\n[\u03b21, ..., \u03b2i, ..., \u03b2C ] (7)\nwhere B\u2032i is P \u2032 i \u2019s matched biases matrix, and \u03b2i is\n\u03b2\u2032i =\n\u2211\ni\nbn\n\u03b7i , n = {1, ...Nl} (8)\nwhere \u03b7i is number of p in group Ci.\nAs shown in 5 and 8, we explored two different methods to update W and B. The first method consists in choosing the Pi that is closer to each centroid Ci. In this case, we use the correspondent bias vector Bi to the corresponding Pi selected. The other way is to use the centroid Ci itself as filter parameters and update Bi with average bias from each cluster. An important detail to choose the right value of k: if we choose k too small then average cluster distortion will be too high to appropriately represent the model parameters, possibly resulting in ineffective features maps. On the other hand, if we choose k too big, not enough kernels will be merged. This, unfortunately, may very well be a hyper-parameter that will have to be manually tuned. Table 2 shows k selected for each experiment which gives the best results on our network models."}, {"heading": "3. Experimental results", "text": "Our experimental results are based on three different datasets: MNIST, German Traffic Sign Recognition Benchmark (GTSRB), and CIFAR-10. As much as possible, to make our experiments significant and to validate our approach, we started from hand-tuned model architectures that were as close as possible to the state of the art, in an effort to prove that our split/merge training procedure can still improve model architecture even when starting from a very highly tuned architecture. Baseline performance are reported in Table 1. For all experiments, we used the BVLC Caffe C++ package (Jia et al., 2014). We started our experiments from MNIST since the quick training time allowed to quickly determine reasonable range of hyper-parameters such as the number of centroids k, number of kernels for the split/merge procedure. Next, we move to a more realistic task such as GTSRB for which we started from an initial model, extremely close to the state of the art and finally confirm the portability of our findings on the harder CIFAR-10 data-set. We report the details of each data-set experiments in the following sections."}, {"heading": "3.1. MNIST results", "text": "The MNIST data-set contains 60,000 training images and 10,000 testing images of hand-written digits of size 28x28. The baseline model is composed of two convolutional layers and two fully-connected layers, as shown in Table 2, with ReLU and pooling following each convolutional layer. This baseline model achieves 0.82% Error rate with this simple network. The DCCKs training algorithm begins by splitting the first convolutional layer from 100 to 200 kernels; after the subsequent fine-tuning the model achieved 0.59% error rate, which is almost 30% relative improvement from the original model. This compared favorably to a 200 kernel models trained from scratch, which achieves 0.78%, and even a 300 kernels model trained from scratch, which achieves 0.75%. This verifies that splitting filters has the potential to help the following SGD based fine-tuning to achieve an optimal point which generalists better. Also, more importantly after following merging step, back to 100 kernels, the performance dropped only 0.01% to an error rate of 0.59%."}, {"heading": "3.2. GTSRB results", "text": "The GTSRB data-set contains 39,209 training images and 12630 testing images of various size, with 43 different classes consisting of standard traffic signs from Germany (Houben et al., 2013). First, we resized all images to 48x48 and then we applied pre-processing techniques such as histogram equalization, adaptive histogram equalization, and contrast normalization. For this task, we have two sets of initial networks: a single model baseline GTSRB1, consisting of three convolutional and two fully connected reaching 2.44% error rate, and larger state of the art ensemble model GTSRB-3DNN (Table 4), inspired by MCDNN(Ciresan et al., 2012), and reaching 1.24% error"}, {"heading": "1 ORIGINAL 100 50 0.82", "text": ""}, {"heading": "2 ORIGINAL 200 50 0.78", "text": "rate, which is within 0.2$ from the best published result. We remark the ensemble models use different input size of 48x48 pixels, 38x48 pixels and 28x48 pixels: because of this, we expected a high degree of redundancy on the GTSRB-3DNN kernels which may be successfully exploited by the DCCKs merging step. Indeed, by visually inspecting the lower convolutional layers we could easily identify an abundant amount of redundancy (see 2.1.2). Because of this highly redundant structure in the initial model, we inverted the sequence of our training procedure to first merge kernels instead of splitting, which maintains the accuracy and provides significantly faster training 8.\nFurthermore, the specific structure of the traffic signs provided for some peculiar behaviors on this database: for instance, kernel rotation especially helped improving performance. A detailed inspection of the recognition errors highlighted that several traffic signs were misclassified by the baseline model were highly tilted; such instances were mostly recovered and correctly recognized after DCCKs training (see Figure 1 for one example of such instance). We also remark that using centroids as new kernels resulted in better gains on this data set.\nTable 5 and Table 6 shows the experimental results. We remark that in all the experiments, in almost all cases, we either achieve significantly better performance or similar performance with significantly reduced model size. One exception worth noticing is [5] in Table 5 which shows the worst performance of all experiments: in this case we merged the last convolutional layer which is fully con-\nnected to the first fully connected layer of this network architecture. We speculate this issue is due to the fact that is notoriously hard to optimize parameters of fully connected layers, splitting a convolutional layer which fans-out into a fully connected layer has the potential to harm the parameter structure to a point where SGD cannot easily recover."}, {"heading": "3.3. CIFAR10 results", "text": "The CIFAR-10 data-set consists of 50,000 training and 10,000 testing images. Each image is 32x32 pixels and represent a class of natural occurring objects. To develop the CIFAR-10 baseline we used the same techniques discussed in (Goodfellow et al., 2013) and the Network-InNetwork (Lin et al., 2013) model which achieves a baseline 10.4% error rate, which is within reasonable distance from to the state of the art. When we apply DCCKs training on the CIFAR-10 data-set, the increased performance is not as large as on the previous data-sets but it is still signif-\nicant and consistent. We believe that this is because the highly successful highly (manually) optimized NetworkIn-Network architecture makes it harder for the automatically devised DCCKs training to provide a large improvement. Therefore these results should demonstrate that DCCKs may still provide some improvement even when applied on top of more complex highly tuned architectures, while keeping the number of parameters under control. Additionally we show that by splitting layers and doubling the number of parameters we could achieve an additional 0.5% average error rate improvement."}, {"heading": "4. Discussion", "text": "In this work, we introduced the concept of DCCKs and introduced a training procedure whereby convolutional kernels learned by SGD can be effectively split and merged. Experimental results confirmed this process results in gradually improving performance, while the training algorithm jointly optimizes structure as well as model\u2019s parameters. Results show that DCCKs can make parsimonious use of model capacity by converging towards the minimal number of parameters that gives the best performance, even when starting with highly manually optimizing network architecture. Figure 3 shows training and validation data loss over fine-tune epochs; the \u201coriginal\u201d and the \u201cmerge\u201d curves refer to training and generalization loss for models having the same number of parameters; notice how the \u201cmerge\u201d curve is consistently above the \u201coriginal\u201d curve, apparently providing an upper-bound to the loss, and thus empirically confirming that the DCCKs architecture was indeed an improved by the training algorithm. Moreover, in some experiments, DCCKs resulted in significantly higher performance with smaller number of parameters than the original model. On the other hand, DCCKs showed bigger gains on simpler databases, such as MNIST and GTSRB, than on more complex CIFAR-10 data-set and to the more complex Network-In-Network model architecture. This is however\nto be expected, especially because the NIN architecture is extremely well tuned and very high performance to begin with, so it is natural to expect smaller gains by our automatic structure optimization procedure. Beside the obvious advantage of automatic structure optimization, a side benefit of DCCKs training is that manipulating kernels takes less computations than pre-processing training data, which makes DCCKs optimization more efficient.\nTo conclude, we believe there are several aspects of DCCKs training algorithm that could be improved. As we mentioned in 2.1.1 currently all kernels are split by the same amount. However, one could argue that some kernels might be better than others and should be replicated first, possibly based on the ability provide new discriminative features. If we could determine such kernels we could potentially improve training speed, though, potentially final accuracy after the merge step might not be much impacted as much. Finding a more extensive set of kernel transformations to achieve a highly selective split step would also be an appropriate next step, as well as comparison and combination with logit-mimic training and model compression techniques (Ba & Caruana, 2014; Hinton et al., 2014). Ultimately, like for any new methodology in the deeplearning sector, it would be very important to test how well DCCKs scale higher dimensional larger problems, such as IMAGE-NET and to different non-vision tasks such as speech recognition or language modeling."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "The minimum description length principle in coding and modeling", "author": ["Barron", "Andrew", "Rissanen", "Jorma", "Yu", "Bin"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Barron et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Barron et al\\.", "year": 1998}, {"title": "Subspace distribution clustering hidden markov model", "author": ["Bocchieri", "Enrico", "Mak", "BK-W"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Bocchieri et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bocchieri et al\\.", "year": 2001}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Multicolumn deep neural networks for image classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Schmidhuber", "Jrgen"], "venue": "In IN PROCEEDINGS OF THE 25TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Hornik", "Kurt"], "venue": "Neural networks,", "citeRegEx": "Hornik and Kurt.,? \\Q1991\\E", "shortCiteRegEx": "Hornik and Kurt.", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark", "author": ["Houben", "Sebastian", "Stallkamp", "Johannes", "Salmen", "Jan", "Schlipsing", "Marc", "Igel", "Christian"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Houben et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Houben et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Sergio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sergio et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Kolmogorov\u2019s theorem and multilayer neural networks", "author": ["K\u016frkov\u00e1", "V\u011bra"], "venue": "Neural networks,", "citeRegEx": "K\u016frkov\u00e1 and V\u011bra.,? \\Q1992\\E", "shortCiteRegEx": "K\u016frkov\u00e1 and V\u011bra.", "year": 1992}, {"title": "Data-driven design of hmm topology for online handwriting recognition", "author": ["Lee", "Jay J", "Kim", "Jahwan", "Jin H"], "venue": "International journal of pattern recognition and artificial intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Experiments with a gaussian mergingsplitting algorithm for hmm training for speech recognition", "author": ["Sankar", "Ananth"], "venue": "In Proceedings of DARPA Speech Recognition Workshop,", "citeRegEx": "Sankar and Ananth.,? \\Q1998\\E", "shortCiteRegEx": "Sankar and Ananth.", "year": 1998}, {"title": "Mdl-based contextdependent subword modeling for speech recognition", "author": ["Shinoda", "Koichi", "Watanabe", "Takao"], "venue": "The Journal of the Acoustical Society of Japan (E),", "citeRegEx": "Shinoda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shinoda et al\\.", "year": 2000}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1958}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Taigman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Recently, deep neural networks (DNNs) have led to significant improvement in several machine learning domains, from speech recognition (Dahl et al., 2012) to computer vision (Krizhevsky et al.", "startOffset": 135, "endOffset": 154}, {"referenceID": 12, "context": ", 2012) to computer vision (Krizhevsky et al., 2012; Taigman et al., 2013) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 21, "context": ", 2012) to computer vision (Krizhevsky et al., 2012; Taigman et al., 2013) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 9, "context": "DNNs have reached state of the art performance thanks to their theoretically proven modeling and generalization capabilities (Hornik et al., 1989; Hornik, 1991; K\u016frkov\u00e1, 1992), and practically driven by improvements in training algorithms for rapid parameter estimation (Martens, 2010; Sutskever et al.", "startOffset": 125, "endOffset": 175}, {"referenceID": 20, "context": ", 1989; Hornik, 1991; K\u016frkov\u00e1, 1992), and practically driven by improvements in training algorithms for rapid parameter estimation (Martens, 2010; Sutskever et al., 2013), novel regularization methods to reduce overfitting (Srivastava et al.", "startOffset": 131, "endOffset": 170}, {"referenceID": 6, "context": "creasing data-sets (Deng et al., 2009) and powerful new computing platforms (Chetlur et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": ", 2009) and powerful new computing platforms (Chetlur et al., 2014).", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": "Information theoretic methods, such as the minimum description length criterion, were also applied to the problem of structural optimization (Barron et al., 1998), resulting in improved performance in speech recognition (Shinoda & Watanabe, 2000) and as well as training algorithms for autoencoders (Hinton & Zemel, 1994).", "startOffset": 141, "endOffset": 162}, {"referenceID": 18, "context": "Although, recently Bayesian optimization of hyper-parameters have been introduced (Snoek et al., 2012).", "startOffset": 82, "endOffset": 102}, {"referenceID": 14, "context": "Our approach takes inspiration by previous work in the area of Gaussian kernel HMMs (Sankar, 1998; Rigazio et al., 2000; Bocchieri & Mak, 2001; Lee et al., 2001), and is philosophically based on Occam\u2019s razor principle whereby a smaller model with similar performance on a given data-set is likely to have better generalization capabilities to new unseen data.", "startOffset": 84, "endOffset": 161}, {"referenceID": 10, "context": "The GTSRB data-set contains 39,209 training images and 12630 testing images of various size, with 43 different classes consisting of standard traffic signs from Germany (Houben et al., 2013).", "startOffset": 169, "endOffset": 190}, {"referenceID": 4, "context": "44% error rate, and larger state of the art ensemble model GTSRB-3DNN (Table 4), inspired by MCDNN(Ciresan et al., 2012), and reaching 1.", "startOffset": 98, "endOffset": 120}], "year": 2015, "abstractText": "Deep neural networks have recently achieved state of the art performance thanks to new training algorithms for rapid parameter estimation and new regularization methods to reduce overfitting. However, in practice the network architecture has to be manually set by domain experts, generally by a costly trial and error procedure, which often accounts for a large portion of the final system performance. We view this as a limitation and propose a novel training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then eliminating model redundancy by selectively removing parameters at training time. For convolutional neural networks, our method relies on iterative split/merge clustering of convolutional kernels interleaved by stochastic gradient descent. We present a training algorithm and experimental results on three different vision tasks, showing improved performance compared to similarly sized hand-crafted architectures.", "creator": "LaTeX with hyperref package"}}}