{"id": "1503.01910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Sequential Relevance Maximization with Binary Feedback", "abstract": "motivated by online settings algorithms where users can provide explicit feedback about the relevance of products that are merely sequentially presented to them, we now look at both the recommendation process approaches as anticipating a problem of intentionally dynamically optimizing this relevance feedback. such an algorithm optimizes the fine tradeoff between presenting the useful products that are most likely to be relevant, and learning the preferences of the user so that more relevant recommendations can be made in the future.", "histories": [["v1", "Fri, 6 Mar 2015 11:02:41 GMT  (102kb,D)", "http://arxiv.org/abs/1503.01910v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vijay kamble", "nadia fawaz", "fernando silveira"], "accepted": false, "id": "1503.01910"}, "pdf": {"name": "1503.01910.pdf", "metadata": {"source": "CRF", "title": "Sequential Relevance Maximization with Binary Feedback", "authors": ["Vijay Kamble", "Nadia Fawaz", "Fernando Silveira"], "emails": ["vjk@eecs.berkeley.edu", "nadia.fawaz@technicolor.com", "fernando.silveira@technicolor.com"], "sections": [{"heading": "1 Introduction", "text": "Predicting the preferences of users in order to present them with more relevant engagements is a fundamental component of any recommendation system [27, 25]. Over the years, a wide variety of approaches have been proposed for this problem (see [1] for a survey). These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8]. In this paper, motivated by several settings of interest in which explicit feedback about the relevance of the recommendations can be received from the user on small timescales, we pursue a less studied approach (see [28]) of modeling the recommendation process as a sequential optimization problem. Below are a few examples of such settings.\n\u2022 Online retail: A user enters an online shopping portal to purchase an accessory, e.g. a watch. She is sequentially presented with various design choices and based on her feedback to these designs, the system adaptively presents recommendations that are more likely to be liked by her. \u2022 Online media-on-demand services: A user using an online music-on-demand service would\nlike to find a new genre of music to listen to. Short sound-clips are played for her sequen-\nar X\niv :1\n50 3.\n01 91\n0v 1\n[ cs\n.L G\n] 6\nM ar\n2 01\ntially, and based on the feedback that she provides for these clips, the recommendation system seeks to adaptively find genres that are better suited to her tastes.\n\u2022 Advertising in online video: As video ads are inherently more disruptive of a user\u2019s attention, and thus potentially more valuable than sponsored search ads, there is a strong motivation for designing ad allocation mechanisms that take into account the relevance of these ads to the users. Services like YouTube and Hulu collect explicit feedback about the relevance of an ad after it is shown, and this feedback can be used to adaptively learn the preferences of the users and show more relevant ads.\nWe consider a model that is derived from cluster models for collaborative filtering (see [6]) in which the history of user behaviors is compressed into a predictive model, where users are classified into \u2018types\u2019 that capture the preference profile of the user. A typical recommendation generation algorithm dynamically observes user behavior and uses maximum-likelihood estimates based on this predictive model to choose products that are more likely to be relevant. Our approach replaces this maximum-likelihood estimation with a sophisticated optimization problem, in which the two conflicting goals of presenting the most relevant products based on current predictions of user preferences, and learning the underlying type of the user so that more relevant engagements can be shown later, are concurrently optimized in a precise and systematic way.\nOur model assumes that a user that enters the system is sampled from a probability distribution over a set of types that is a priori known to the system designer. Each type is associated with a string of \u2018relevance\u2019 ratings for the different categories of products. We focus on the simplest case in which this relevance rating is binary, i.e. the user considers a category of products either relevant or irrelevant. We assume that the number of recommendation opportunities available in a session is random, modeled as a geometric random variable arising from the assumption that the user stays for each additional opportunity with a fixed probability \u03b2 independent of the past. Under this setting we focus on the problem of adaptively maximizing the expected cumulative relevance of recommendations presented to the user during the session.\nOur main contribution in this paper is the analysis of this sequential relevance maximization problem. At first glance, one can see that the optimal policy can be determined using a naive recursive algorithm. But as is typical of such algorithms, it is highly inefficient due to repetition of redundant work. The standard tool to solve such problems is dynamic programming, which turns these inefficient recursive algorithms into efficient iterative solutions. But unfortunately in our case, the state space for this program grows exponentially in the number of types and categories. Further, an efficient enumeration of these states is difficult.\nWe first derive certain key properties of the structure of the optimal policy using probabilistic interchange arguments. Using these properties, we provide an algorithm that strikes a balance between recursion and dynamic programming to solve for the optimal policy. Unfortunately, this algorithm still remains computationally prohibitive. Motivated by our structural results, we then propose and analyze two heuristic policies: a \u2018farsighted\u2019 greedy policy that is easier to compute and a naive greedy policy that is analogous to the maximum-likelihood prediction performed by typical recommendation systems. We then prove that these policies are approximately optimal, i.e. they achieve a constant factor of the optimal payoff. We finally perform extensive simulations on random problem instances and we observe that these heuristic policies typically perform much better than that predicted by our worst case bounds."}, {"heading": "1.1 Related work", "text": "The idea of posing the recommendation process as an optimization problem is not new. To the best of our knowledge, its earliest appearance in literature can be traced back to [5], which proposed a decision-theoretic modeling of the problem of generating recommendations (on a palm-top) for a user navigating through an airport. [28] proposed a framework for modeling the sequential optimization problem in online recommendation systems as a Markov Decision Process (MDP) [23]. Their underlying formulation is quite general and their focus is on defining and establishing this paradigm. The model that we consider on the other hand is more structured and our focus is on the analysis of the resulting optimization problem.\nThe sequential relevance maximization problem is closely related to Bayesian multi-armed bandit problems. In a multi-armed bandit problem (MAB), first introduced by Thompson in [29], a decision-maker faces a set of arms whose reward characteristics are uncertain and seeks to optimize the sequence in which they are pulled so as to maximize some long-run reward. In such problems one faces the tradeoff between exploration, i.e. learning the reward characteristics of the arms, and exploitation, i.e. accumulating rewards by choosing good arms based on current estimates. These problems have been commonly studied under two distinct settings: Bayesian and stochastic, with a different set of analytical approaches used in each. Our model falls in the Bayesian setting [12, 30], in which an initial prior distribution is assumed over the parameters of a probabilistic reward generating model for each arm, and one performs Bayesian updates of these estimates as rewards are observed. One then solves the well-defined problem of maximizing either the long-term average or discounted cost. The standard solution tool in this case is dynamic programming. The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2]. The focus is on characterizing this optimal rate.\nMost of the literature in these settings has focused on the case where the rewards of different arms are statistically independent. In the Bayesian case, a seminal result by Gittins [12] shows that the optimal policy dynamically computes an index for each arm independently of all other arms, and picks the arm with the highest index at each step. But in our case the relevance of different products are correlated through the hidden user type and hence it is a type of a Bayesian MAB problem with correlated or dependent arms. It is well known that the decomposition result of Gittins does not hold for this case. Over the years there has been sporadic progress in tackling this problem, with most papers focusing on specific models. [11] and [15] analyze two-armed bandit problems in which reward characteristics of two arms are known, but which arm corresponds to which reward distribution is not known, which leads to a natural dependence between the arms. In this case, the general case of more than two arms still remains open. [21] studies another version of the problem in which the arms can be grouped into clusters of dependent arms, in which case the Gittins decomposition result can be partially extended. Recently, [19] considered a specific model of a MAB problem with dependent arms, where they analyzed the performance of a greedy policy and derived asymptotic optimality results. These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different. The broad conclusion from this body of work is that the correlation between arms can be exploited to achieve better regret rates.\nAnother important difference between MAB problems and our problem is that, since any product can be presented only once and since there is a finite number of products in any category, there is a bound on the number of times each \u2018arm\u2019 can be pulled. Thus one cannot \u2018exploit\u2019 an arm forever and is forced to experiment intermittently. In the special case when each category has a single product, our problem is also related to the active sequential hypothesis testing problem [9, 20]. In this problem, one seeks to speedily learn a hidden random variable by adaptively choosing a sequence of correlates to observe, with a cost for each observation. This formulation would have been appropriate if our objective was to quickly learn the user type without any concern for the relevance feedback. But since our goal is to optimize the latter, a different approach is necessary."}, {"heading": "1.2 Structure of the paper", "text": "The structure of the paper is as follows. In Section 2, we introduce our model and define the relevance optimization problem. Section 3 is devoted to the analysis of this problem, in which we derive key structural properties of the optimal policy and finally present an algorithm to compute it. In Section 4 we propose two policies which are easier to compute and prove that they are approximately optimal. In Section 5, we extensively simulate our two approximately optimal policies on randomly generated problem instances and compare their performance to the optimal policy. Finally Section 6 summarizes our work and discusses extensions to our model. The proofs of all our results can be found in the appendix."}, {"heading": "2 Model", "text": "We consider the setting of a user who enters an online system and is sequentially presented with products from different categories, with the goal of maximizing the number of relevant products presented to him before he eventually leaves the system. Assume that there are L total products. The products are divided into categories, with each category representing a set of similar products. Let these categories be labeled as j \u2208 {1, \u00b7 \u00b7 \u00b7 , H} = [H]. Each category j has Lj products. A given user considers some set of categories to be relevant to him and this set is not known a priori. The system designer elicits explicit feedback about the relevance of a product after it is presented. This feedback is obtained as an answer to an explicit question, is assumed to be binary, and takes value 1 (resp. 0) when the product is relevant (resp. irrelevant). We assume that this feedback is accurately provided by the user. A product cannot be presented more than once to the same user during the session. Hence the maximum number of products that can be shown is restricted to L.\nWe capture the uncertainty in the preferences of the user by assuming that the user is one of N possible types and the actual type of the user is a latent random variable that is not observed at the beginning of the session. Let X \u2208 [N ] denote this random variable. Let pX be the corresponding probability distribution. We assume that the system designer only knows this distribution pX . For each user type i and for each product category j, let qij \u2208 {0, 1} denote the fixed binary relevance feedback of the user of type i to that category. The type of the user is not known, and so for each category j, we introduce a random variable Yj \u2208 {0, 1} which represents the binary feedback of a user for any product in that category. pX induces a probability distribution on Yj :\nP (Yj = 1) = N\u2211 i=1 qijpX(i).\nIt is convenient to associate each user type i \u2208 [N ] with an H-length binary vector of the {qij}, j \u2208 [H] values for different categories. Hence we can define a N \u00d7H relevance matrix Q = {qij}, whose rows represent user types, and columns represent product categories1. Figure 1 is an example of a relevance matrix with four types of users labeled 1 to 4 and four product categories labeled A to D. Each category has some specified number of products. For instance, type 1 finds category A and C relevant and finds B and D irrelevant.\nThe number of display opportunities that are available before the user leaves the system is modeled as a random variable C \u2208 {1, 2, \u00b7 \u00b7 \u00b7 } with a geometric probability distribution pC where pC(m) =\n1Notice that type space is quite general. If for a user, there is a joint distribution over finding different categories relevant then we can think of a user as being a convex combination of the types corresponding to the realizations of binary relevance vectors with the associated probabilities. Also, if some variation is observed in the feedback received for products that belong to the same category, then each of the products can be declared as individual categories. Although this increases the problem size, our analysis remains applicable.\n\u03b2m\u22121(1\u2212\u03b2) form \u2265 1. In other words, the user dynamics in the system is modeled as a memoryless random process, in which a user stays for each additional opportunity with probability \u03b2 or exits with probability 1 \u2212 \u03b2, independently of the past. This assumes that at least one opportunity is always available. Finally, the random variable C is independent of the user type X . The feedback for a product can be obtained after every display opportunity, but since the feedback for a product is the same for every other product in its category, one can assume that the feedback is requested and obtained only when the product presented belongs to a category that has not been shown before."}, {"heading": "2.1 Relevance maximization", "text": "The primary objective of the system designer is to maximize the expected number of relevant products presented to a user in the session. Once a user enters the website, at each display opportunity, the system designer adaptively decides which product should be shown to the user, while taking all the user feedback obtained in the past into consideration. We define the objective formally. A policy \u03c8 for the designer is the sequence of maps \u03c8 = {\u03c81, \u00b7 \u00b7 \u00b7 , \u03c8L} where each map \u03c8t : Ht \u2192 At is a mapping from the set of possible observations of user feedback until time t, denoted by Ht, to the set of possible actions At, which is the set of choices of products. Let \u03a8 be the set of all feasible policies. The objective of the designer is to find a policy which maximizes the expected number of relevant ads shown in a session under the constraint that no product is shown more than once. Let lt denote the product chosen at time t. Once a policy \u03c8 \u2208 \u03a8 is chosen, lt is a well defined random variable. With some abuse of notation, let j(lt) be its category. Then the objective of the publisher is the following.\nmax \u03c8\u2208\u03a8 E\u03c8{Yj(lt)},C [ C\u2211 t=1 Yj(lt)]\nsubject to C\u2211 t=1 lt1{lt=l} \u2264 1 for each product l \u2208 [L]. (1)\nAssuming memoryless user dynamics, the optimization problem (1) takes the following form\nmax \u03c8\u2208\u03a8 \u221e\u2211 t=1 \u03b2t\u22121E\u03c8{Yj(lt)} [Yj(lt)]. (2)\nAs mentioned earlier, this problem is a type of a Bayesian multi-armed bandit problem with correlated rewards (see [12, 30]) with an additional constraint on the number of times each arm may be pulled. At a first look, one can solve this problem using the following recursive program in Algorithm 1. But it is well known that such recursive algorithms can be very inefficient. The usual problem is when recursion leads to repeating work. This happens when you have overlapping subproblems, which is unfortunately the case here. Turning these inefficient recursive algorithms into efficient iterative algorithms is the role of dynamic programming. This requires us to define a state space of possible \u2018information states\u2019 for each opportunity t, which encapsulate all the information that has been gained till time t. In our case, the information state corresponds to a smaller relevance matrix obtained after computing the posterior distribution on the types, by eliminating all the rows corresponding to user types that have conditional probability 0 and all the columns corresponding to categories that have been exhausted. The state space thus grows prohibitively large with time and its enumeration is cumbersome. In the next section, we prove some structural properties of the optimal policy and based on these we provide an efficient algorithm that strikes a balance between recursion and iteration in order to compute this policy. These structural results are motivated by the following examples.\nExample: A triangular relevance matrix : Consider the relevance matrix shown in Figure 2. A quick circumspection convinces us that the optimal policy is one which shows the categories in the order A, B, C and then D. If a positive feedback is obtained for a category then all the advertisers in that category are exhausted. To see this, observe that this policy attains the optimal payoff obtained in the case that the type of the user is known at arrival. Structurally, there is a partial order relation on the categories where one category \u2018dominates\u2019 the other if the set of types which find it relevant is a strict subset of the set of types which finds the other relevant. This example shows that if this partial order relation leads to a complete ordering of the categories then the optimal policy simply presents the categories according to this order. But what if that is not the case? In lemma 2, we prove\nAlgorithm 1 (Optimal) Function [V (Q, p, \u03b2), A(Q, p, \u03b2)] where Q is a relevance matrix and p is a probability distribution over user types.\n\u2022 If Q is empty, return V (Q, p, \u03b2) = 0. \u2022 For a category j, let Mj denote the set of user types which find j relevant and let P (Mj) = P (X \u2208Mj). Also let Qj be the matrix obtained after removing the column corresponding to category j and the rows corresponding to all the user types in M cj and let Q j res be\nthe matrix obtained after removing the column corresponding to category j and the rows corresponding to all the user types in Mj . Finally, let pj denote the distribution on the user types conditional on the event {X \u2208 Mj} and pjres be the distribution on the user types conditional on {X \u2208M cj }. \u2022 Then define\nV j = P (Mj) ( 1\u2212 \u03b2Lj 1\u2212 \u03b2 + \u03b2LjV (Qj , pj , \u03b2) ) +(1\u2212 P (Mj))\u03b2V (Qjres, pjres, \u03b2)\nan appropriate generalization of this property for arbitrary relevance matrices using a probabilistic interchange argument. We show that if a category dominates some other then in the optimal policy it is presented before the other.\nExample: A permutation relevance matrix : Consider the relevance matrix shown in Figure 3. One can argue that in this case the optimal policy is greedy: choose the category with the maximum expected number of relevant ads. In fact, if the relevance matrix is a permutation of smaller block matrices, with multiple categories in each, we can consider the relevance optimization problem for each of the smaller blocks separately and greedily choose the order in which these blocks are chosen."}, {"heading": "3 Characteristics of the optimal allocation policy", "text": "In this section we present some structural properties of the optimal allocation policy."}, {"heading": "3.1 Property 1: If category A is relevant, show it", "text": "We first present the following intuitive property.\nLemma 3.1. In the optimal allocation policy, at any opportunity, conditional on the past observations, if there exists a product category j that will generate a positive feedback with probability 1, i.e. P (Yj = 1 | Ht) = 1, then any product in j that has not been shown is allotted immediately. If there are multiple such products then they can be allotted in any order.\nThis property implies that if a positive feedback is received for a product belonging to a particular category j, then all Lj products of that category are scheduled to be presented in the immediately following opportunities2. The proof uses a simple probabilistic interchange argument."}, {"heading": "3.2 Property 2: If \u2018likes A\u2019 implies \u2018likes B\u2019, then show B before showing A", "text": "To describe this next property, we first formally define a few ideas. In the dynamic allocation of products to the opportunities, we call an opportunity t to be an experimentation opportunity if conditional on information obtained until time t \u2212 1, there is not a single category j such that Yj = 1 with probability 1. If there existed such a category, the previous lemma tells us to exhaust all the advertisers in that category. But since there is no such category, an experimentation opportunity brings to us the non-trivial problem of deciding which category to present to the user next. Thus all the non-trivial decisions in the optimal dynamic allocation policy are taken at the experimentation opportunities. Let S(t) = {i \u2208 [N ] : P (X = i | Ht) > 0} be the set of user types that have a non-zero probability conditional on the history. Then note that after observing the feedback from the allocation made at an experimentation opportunity S(t\u2212 1)\u2212 S(t) \u2265 1. Let E(t) be the set of categories available i.e. which have not been presented till opportunity t. Let Q(t) be the relevance matrix with rows corresponding to the types in S(t) and the columns corresponding to the categories in E(t). Finally, for each category j in E(t), let Mj(t) = {i \u2208 S(t) : qij = 1}, which is the set of user types in S(t) which find category j relevant.\nDefinition 3.1. We say that category j dominates category j\u2032 at opportunity t if Mj\u2032(t) \u2282 Mj(t). The categories that are not dominated by any other category are called non-dominated categories.\nFor instance in Figure 1, A, C and D are the only non-dominated categories since A dominates B. Then we show the following.\nLemma 3.2. In the optimal allocation policy, at any experimentation opportunity, the product presented must be of a non-dominated category.\n2In order to not bore the user, we can introduce a bound on the number of products of the same category that can be successively shown to the user.\nIn other words, this lemma says that if the set of user types which find category A relevant is contained in the set of user types which find category B relevant, then in the optimal policy, category B is presented before category A. The proof of this lemma also uses a probabilistic interchange argument. Observe that the claim in the lemma is not an intuitively obvious fact. One may argue that in some cases, presenting a category that is dominated may help us learn the true user type faster and thus perform a better allocation in the future opportunities. Indeed if the goal is to minimize the expected number of opportunities taken to learn the user type exactly, then this property clearly does not hold (e.g. presenting a category that every user type finds relevant gives no information about the true type).\nNow let U(t) be a generic class of non-dominated categories that satisfy the condition that Mj(t) = Mj\u2032(t) for all j, j\u2032 \u2208 U(t). This means that U(t) is a class of categories found relevant by exactly same set of types. U(t) will be called a non-dominated equivalence class of categories and MU(t) denotes the set of types which find the class U(t) relevant. We allow for a class to be singleton in the definition and so suppose there are K(t) such non-dominated equivalence classes {U1, \u00b7 \u00b7 \u00b7 , UK(t)} that partition the set of non-dominated categories in the relevance matrix. Let this set of non-dominated equivalence classes of categories be denoted by U(t). If furthermore the sets of types {MU1 , \u00b7 \u00b7 \u00b7 ,MUK(t)} are mutually disjoint, then we say that the set of non-dominated equivalence classes partition the type space. In this case, the relevance matrix can be represented as a block diagonal matrix composed of K(t) smaller block matrices (up to permutation of the K(t) blocks), with each block matrix corresponding to an equivalent non-dominated class. Such a small block is composed of columns of all 1s, one for each category in the class, and columns corresponding to the categories that the class dominates.\nAs products are presented and we recompute the relevance matrix after each feedback, we may lose non-dominated categories or new categories may become non-dominated. Thus the set of nondominated equivalence classes will change. But in the case where new categories are added to a class of non-dominated categories, we want to be able to identify the new class with the old class. This can be done since the categories in an equivalence class in the relevance matrix at the first display opportunity will continue to remain in the same class as long as they are non-dominated and they have not been presented. Thus a class U in subsequent display opportunities is identified by equivalence to the set of categories in U at the first display opportunity. For example, in the relevance matrix in Figure 1, as mentioned before U1 = {A}, U2 = {C} and U3 = {D} are the non-dominated categories at the first opportunity. Suppose C is presented and a negative feedback is received. Then in the new relevance matrix obtained after deleting rows corresponding to type 1 and type 4, and column corresponding to category C, the only remaining non-dominated class is {A,B}. In this case we identify {A,B} with U1, which was the class that contained A in the first opportunity. Similarly if you present A initially and get a negative feedback, then {C,D} is left as the only non-dominated equivalence class, which is a result of merging classes {C} and {D}. In this case the new class is identified with any of the original classes U2 or U3. This brings us to the following property of any relevance matrix that can be easily verified.\nLemma 3.3. Consider a relevance matrix with an initial set of non-dominated classes of categories U. Suppose that a category from a class U \u2208 U is presented. Suppose that a negative feedback is received for this category, and consider the new relevance matrix obtained after deleting the rows corresponding to user types that find the presented category not relevant and the column corresponding to the presented category. Then the new set of non-dominated equivalence classes of categories U\u2032 satisfies U\u2032 \u2282 U.\nIntuitively this is because, when a negative feedback is obtained at some opportunity t, the rows corresponding to the user types that provide positive feedback to the shown category get deleted and thus it cannot happen that a category that was dominated at opportunity t becomes non-dominated at t + 1. On the other hand, after a positive feedback, completely new non-dominated equivalence classes can appear in the new relevance matrix computed after the posterior update. For example if A is presented and a positive feedback is received, the new relevance matrix has positive probability only on types 1, 2 and 3. In that case, D is dominated by B and hence ({B},{C}) is the new set of non-dominated equivalence classes (they are not equivalent), where notice that {B} appears as a new (singleton) class."}, {"heading": "3.3 Structure of the optimal policy", "text": "The lemmas 3.1, 3.2 and 3.3 reveal the following structure of the optimal policy. Beginning from a set of non-dominated equivalence classes of categories, these classes are presented in a certain order as long as we keep getting a negative feedback. If any class obtains a positive feedback in the process, then we present all the products in that class, \u2018zoom in\u2019 to the next level (eliminating all the other types from the relevance matrix) and restart with a new set of non-dominated equivalence classes. Utilizing this structure, the following Algorithm 2 computes the optimal payoff.\nAlgorithm 2 (Optimal) Function V (Q, p, \u03b2) where Q is a relevance matrix and p is a probability distribution over user types.\n\u2022 If Q is empty, return V (Q, p, \u03b2) = 0. \u2022 If Q is non-empty, enumerate the non-dominated equivalence classes of Q. Let them\nbe (U1, \u00b7 \u00b7 \u00b7 , UK). Calculate the number of products in each class, denoted by Lk =\u2211 j\u2208Uk Lj .\n\u2022 For each k = 1, \u00b7 \u00b7 \u00b7 ,K, and each \u03c0 \u2282 {1, \u00b7 \u00b7 \u00b7 ,K} such that k /\u2208 \u03c0, let \u03c9(\u03c0, k) be the event {X \u2208 S(\u03c0, k)} where\nS(\u03c0, k)\n= {i \u2208 N : qij = 0\u2200 j \u2208 Us, s \u2208 \u03c0 and qij = 1\u2200 j \u2208 Uk}. S(\u03c0, k) is thus the set of user types that find all the classes with labels in \u03c0 irrelevant, but find the class Uk relevant. Let Q\u03c0k be the relevance matrix obtained from deleting all the rows corresponding to user types in S(\u03c0, k)c and all columns corresponding to the categories in the classes in \u03c0 and the categories in k. Finally, let p\u03c0k be the probability distribution on the user types conditional on the event \u03c9(\u03c0, k). Then define\nV \u03c0k = P (\u03c9(\u03c0, k))\n( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk\nV (Q\u03c0k , p \u03c0 k , \u03b2)\n) .\n\u2022 Return V (Q, p, \u03b2) = OPT ,\nmax k1,\u00b7\u00b7\u00b7 ,kK\u2208\u03c3(1,\u00b7\u00b7\u00b7 ,K)\nVk1 + \u03b2V k1 k2 + \u00b7 \u00b7 \u00b7+ \u03b2K\u22121V k1,\u00b7\u00b7\u00b7 ,kK\u22121kK . (3)\nHere \u03c3(1, \u00b7 \u00b7 \u00b7 ,K) is the set of permutations of the K non-dominated equivalence classes.\nThe optimization problem (3), is defined on the domain of all the possible orderings of the nondominated equivalence classes of categories in Q3. This problem can be solved more efficiently using dynamic programming as opposed to comparing all the possible K! orderings. One can define the state of the program at step r as the set of classes {k1, k2, \u00b7 \u00b7 \u00b7 , kr} that have been presented till step r. A substantial reduction in the state space comes from the fact that Q\u03c0rk for any k does not depend on the order in which the classes in \u03c0r were presented, and hence the state of the program at any step needs to only remember this set. Thus the size of the state space is ( K 1 ) + ( K 2 ) + \u00b7 \u00b7 \u00b7+ ( K K ) = 2K .\nAt each state at step r, in the worst case,K\u2212r number of sub-programs need to be called to evaluate the set of payoff-to-go corresponding to the classes that have not been presented, i.e. {V (Q\u03c0rk ) : k /\u2208 \u03c0r} Thus at each level in the recursive program, the number of sub-programs that are called is exponential in the number of non-dominated equivalence classes at that level in the worst case. Considering this, we turn to find good heuristic policies that are easier to compute.\n3Note that if for some order, conditional on a sequence of classes getting negative feedback, if some class that is next in the order is dominated, then that order will simply not be chosen as the optimal order."}, {"heading": "4 Approximately optimal policies", "text": ""}, {"heading": "4.1 Policy 1: Farsighted Greedy", "text": "Consider the optimization problem (3) assuming that we have been given V (Q\u03c0k ) for each k = 1, \u00b7 \u00b7 \u00b7 ,K, and each \u03c0 \u2282 {1, \u00b7 \u00b7 \u00b7 ,K} such that k /\u2208 \u03c0. Now suppose that instead of optimally solving (3), we adopt the following \u2018greedy\u2019 policy. We iteratively define\nk\u2217s = arg max{V k\u22171 ,\u00b7\u00b7\u00b7 ,k \u2217 s\u22121 i : i \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} \\ {k \u2217 1 , \u00b7 \u00b7 \u00b7 , k\u2217s\u22121}}\nfor s = 1, \u00b7 \u00b7 \u00b7 ,K. This policy assumes that the payoff-to-go from the \u2018next level\u2019 onwards is given. But since it is not, we recursively compute an approximation to this payoff-to-go by assuming that we will follow the same greedy strategy in all the subsequent levels of the optimization problem. Algorithm 3 computes the proposed policy and its payoff.\nAlgorithm 3 (Farsighted greedy): Function W (Q, p, \u03b2) where Q is a relevance matrix and p is a probability distribution over user types.\n\u2022 If Q is empty, return W (Q, p, \u03b2) = 0. \u2022 If Q is non-empty, enumerate the non-dominated equivalence classes of Q. Let them\nbe (U1, \u00b7 \u00b7 \u00b7 , UK). Calculate the number of products in each class, denoted by Lk =\u2211 j\u2208Uk Lj .\n\u2022 Let the event \u03c9(\u03c0, k) be as defined in Algorithm 2. Similarly define Q\u03c0k and p\u03c0k . \u2022 Iteratively compute\nk\u2217s = arg max{W k\u22171 ,\u00b7\u00b7\u00b7 ,k \u2217 s\u22121 i : i \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} \\ {k \u2217 1 , \u00b7 \u00b7 \u00b7 , k\u2217s\u22121}} (4)\nwhere\nW\u03c0k = P (\u03c9(\u03c0, k))\n( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk\nW (Q\u03c0k , p \u03c0 k , \u03b2)\n) .\n\u2022 Return W (Q, p, \u03b2) = Wk\u22171 + \u03b2W k\u22171 k\u22172 + \u00b7 \u00b7 \u00b7+ \u03b2K\u22121W k \u2217 1 ,\u00b7\u00b7\u00b7 ,k \u2217 K\u22121 k\u2217K .\nNote the computational savings as compared to the algorithm for computing the optimal policy. The comparison in equation (4) when s classes have been presented already is overK\u2212s possibilities in the worst case. Thus the number of times a sub-program is called isK+(K\u22121)+(K\u22122)+\u00b7 \u00b7 \u00b7+1 = K(K+1)\n2 . Thus at each level in this recursive program, the number of sub-programs that are called is quadratic in the number of equivalence classes at that level. We can then prove the following performance guarantee for this policy. Theorem 4.1. LetLmin = minj\u22081,\u00b7\u00b7\u00b7 ,H Lj be the minimum number of products in any category and let H be the total number of categories. The farsighted greedy algorithm achieves 1\u2212\u03b2 Lmin 1+\u03b2\u2212\u03b2H\u2212\u03b2Lmin factor of the optimal payoff.\nNote that the worst case is when H is large and Lmin = 1, in which case the adaptive greedy policy achieves a 1\u2212 \u03b2 factor of the optimal payoff. The key idea of the proof is as follows. The departure from optimality at any level has two sources: the fact that the payoff-to-go from the next level onwards is an approximation to the optimal payoff-to-go, and the order in which the non-dominated classes are presented in the current level is chosen greedily. If one assumes that the ratio of the approximation to the optimal payoff-to-go and the optimal payoff-to-go at the next level is some \u03b3, and if one can quantify the departure from optimality of the greedy policy at the current level, one can compute a bound for the worst case ratio of the current payoff-to-go and the optimal current payoff-to-go as some \u03b3\u2032 = f(\u03b3). One can show that this operator is a contraction. Thus one can recursively find a sequence of lower bounds that are uniformly bounded below by the fixed point of this sequence, which is the quantity in the theorem.\nNote that the description of Algorithm 3 can be simplified by fully exploiting its recursive structure; we presented it in the current form to show the correspondence to Algorithm 2 and also to facilitate\nthe argument in the proof of Theorem 4.1. The equivalent implementation can be found in the appendix."}, {"heading": "4.2 Policy 2: Naive Greedy", "text": "Another simple heuristic that we can use is the following greedy policy.\nPolicy (Naive Greedy) Let the set of non-dominated equivalence classes at an experimentation opportunity t be (U1(t), \u00b7 \u00b7 \u00b7 , UK(t)), and (L1(t), \u00b7 \u00b7 \u00b7 , LK(t)) be the number of products in each of these classes. Then choose a product from a class k\u2217 where\nk\u2217 \u2208 arg max k\u2208{1,\u00b7\u00b7\u00b7 ,K}\n1\u2212 \u03b2Lk(t)\n1\u2212 \u03b2 P (X \u2208MUk(t) | Ht).\nWe then have the following theorem.\nTheorem 4.2. The Naive Greedy policy achieves 1\u2212\u03b2 Lmin\n1+\u03b2\u2212\u03b2H factor of the optimal payoff.\nNote that in the worst case, when Lmin = 1 and H is large, the greedy algorithm achieves at least 1\u2212\u03b2 1+\u03b2 factor of the optimal payoff. The proof of this theorem is similar to that of Theorem 4.1."}, {"heading": "4.3 A lower bound for \u03b2 close to 1", "text": "We can also obtain a lower bound on the ratio of payoffs under either of the heuristic algorithms and the optimal algorithm for values of \u03b2 close to 1. Intuitively, this follows from the observation that if user stays for long enough so that the number of ad opportunities available is greater than L, then any policy obtains all the positive feedback that one can possibly obtain.\nTheorem 4.3. Any feasible policy attains \u03b2L\u22121 factor of the optimal payoff."}, {"heading": "5 Simulations", "text": "In this section we compare the performance of the the greedy with foresight policy and the naive greedy policy with the optimal policy. We generate 50 random samples each of 5 \u00d7 5 and 7 \u00d7 7 relevance matrices with associated randomly chosen priors. We compute the payoff under all the three policies, for \u03b2 ranging from 0 to 1. For each \u03b2, we then plot the average and the minimum across the 50 samples of the ratio of the payoff under a non-optimal policy and the optimal policy. Our results are shown in Figure 4.\nNote that both the policies perform very close to optimal even in the worst case across the samples. Also, observe that for \u03b2 close to 0 and for \u03b2 close to 1, the payoff under both the policies approach the optimal payoff, which corroborates our bounds in theorems 4.1, 4.2 and 4.3. The curve corresponding to the naive greedy policy is smooth because the policy does not depend on \u03b2 and hence the resulting payoff is continuous in \u03b2 (and so is the optimal payoff)."}, {"heading": "6 Discussion and Conclusions", "text": "Our main contribution in this paper is the introduction and analysis of the sequential relevance maximization problem with binary feedback. This problem naturally arises in several settings where a designer needs to adaptively make a sequence of suggestions to a user while learning his preferences from his feedback. This basic framework is amenable to extensions that adapt our approach to a more practical setting where some of our assumptions may not hold. For example, we assume that the number of display opportunities in a session is independent of both the type of the user and the relevance feedback, which may not hold in practice. For example, a user may be more likely to leave sooner if he is consecutively shown irrelevant products. Also, one of our central assumptions is that the user feedback is binary, but in practice one may benefit from a more fine-grained feedback\nfrom the user. For example, the user may convey a rating for the product which may be a number from 0 to 5. In this case, one would want to maximize the sum of ratings obtained for the products shown in a session. Another interesting extension is to incorporate the values of the products so that one maximizes the total value of relevant products shown to a user in a session. We leave these extensions for future work. User type and personalization: In the current era of personalization of web services, it is important that a recommendation system be sensitive to the transience in the preferences of the users. For example, a user\u2019s preference for music can change every day, depending on her mood, company etc. A sequential optimization approach to generating recommendations can proactively learn these changes in user preferences by freshly eliciting relevance feedback for carefully chosen products, each time the user enters the system.\nIn the model that we have considered in this paper, the type of a user captures her preferences for the session under consideration and the prior distribution over these types is assumed to be known to the system designer. One interpretation of this distribution is that it captures the preferences of a \u2018typical\u2019 user in the population, and it is estimated from the observed behavior of all the past users. In another interpretation aligned with the notion of personalization, one can think of this distribution as capturing the variation in the preferences of the same user over multiple sessions. For example, in a naive interpretation, one can imagine that the type captures the \u2018mood\u2019 of a person, which is sampled independently everyday, and her preferences for music on a particular day depends on her mood on that day. Even more generally, there could be cross-temporal dependencies in these types. If one desires to optimize the performance of the recommendation process over multiple sessions, one needs to also estimate this type evolution process. We leave these considerations for future work."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 Proof of lemma 3.1", "text": "Proof. The result follows from a simple interchange argument. Suppose at time t, the posterior distribution over the set of possible types is {P (X = i | Ht)} = (p1t , \u00b7 \u00b7 \u00b7 , pNt ) and the set of remaining products is A(t). Consider the event W with a fixed realization of the user type X = i and a fixed realization of the random variable C = c, which is the time at which the user leaves. Thus on this event, the string of binary feedback for the different categories is {qij : j \u2208 [H]}. Then for any fixed policy \u03c8\u2032, the sequence of allocations of the products from time t onwards till time c is dictated by the policy and is determinate. Let this sequence of allocations be {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc} and the corresponding sequence of feedback be {yj(lt), yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)}. Suppose there exists an advertiser l\u2217 \u2208 A(t) such that, conditional on observations till time t\u2212 1, Yj(l\u2217) = 1 w.p. 1. We now consider 2 cases: Case 1 : assume that on event W , for policy \u03c8\u2032, l\u2217 \u2208 {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc}. Say lt\u2032 = l\u2217 for t\u2032 \u2208 {t, \u00b7 \u00b7 \u00b7 , c}. Then if t\u2032 6= t, we will construct a policy which generates the sequence of allocations {l\u2217, lt, \u00b7 \u00b7 \u00b7 , lt\u2032\u22121, lt\u2032+1, \u00b7 \u00b7 \u00b7 , lc} and thus give the same payoff on event W . This policy \u03c8 is the following:\n1. Allot advertiser l\u2217 at time t.\n2. from time t+ 1 onwards follow policy \u03c8\u2032 assuming Ht+1 = Ht\n3. When \u03c8\u2032 prescribes allotting l\u2217 at time t\u2032 + 1, use the information yl\u2217 = 1 to update the history toHt\u2032+1 and remove l\u2217 from the set of available advertisers. Follow the prescription of \u03c8\u2032 for allotting advertisers from t\u2032 + 1 and onwards.\nClearly, this policy gives the same payoff on event W since only the time at which l\u2217 is allotted has been interchanged. Case 2 : assume that on eventW , for policy \u03c8\u2032, l\u2217 /\u2208 {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc}. Then observe that the policy \u03c8 described above generates sequence of allocations {l\u2217, lt, lt+1, \u00b7 \u00b7 \u00b7 , lc\u22121}. Thus the difference in payoff is given by\nYj(l\u2217) \u2212 Yj(lc) = 1\u2212 Yj(lc) \u2265 0\nThus the number of relevant ads shown under policy \u03c8 is at least as high as that under policy \u03c8 for every such event W from a set of disjoint events whose union is the entire probability space. Thus the expected number of relevant ads shown is also at least as high."}, {"heading": "7.2 Proof of lemma 3.2", "text": "Proof. The proof uses a probabilistic interchange argument. Suppose that at opportunity t, the optimal policy \u03c8\u2032 allots a product l\u2032 of category j\u2032 while there exists a category j such that Mj\u2032(t) \u2282 Mj(t). Let l be any generic product of category j. Now consider a policy \u03c8 which allots category j before category j\u2032 by allotting product l at opportunity t. To further describe this policy, we consider two cases:\n(A) If the user finds j relevant, it exhausts all the products in that category and then moves on to allotting j\u2032. After it allots j\u2032 it behaves as if j was never allotted until \u03c8\u2032 prescribes allotting j, upon which the designer updates the information that j is relevant and moves on to allot the next category prescribed by \u03c8\u2032 and so on.\n(B) If the user finds j not relevant, then from time t+1 onwards it acts as if it allotted j\u2032 at time t and found that j\u2032 is not relevant. Then when \u03c8\u2032 prescribes allotting j, the designer updates the information that j is not relevant and moves on to allot the next category prescribed by \u03c8\u2032 and so on.\nWe will show that on every disjoint event of the underlying probability space, the system designer shows at least as many relevant products by following policy \u03c8 instead of \u03c8\u2032. Consider an event W on which the user leaves after opportunity c \u2265 t and on which the realization of the user type is X = i. Then for the policy \u03c8\u2032, the sequence of allocations of the products from time t onwards until time c is dictated by the policy and is determinate. Let this sequence of allocations be {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc} and the corresponding sequence of feedback be {yj(lt), yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)}. These allocations and feedback depend on the type i that was realized on W . For this, we consider 3 mutually exclusive and exhaustive cases:\nCase 1: We first consider the case where on the event W , X = i \u2208 Mj\u2032(t). Then observe that yj(lt) = yj(l\u2032) = 1 and immediately the designer deduces that Yj = Yj\u2032 = 1. Thus since \u03c8 is optimal, by the previous lemma w.l.o.g. the first Lj\u2032 + Lj allocations in the sequence {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc} can be assumed to be all the advertisers belonging to categories j\u2032 and j and thus the feedback is a sequence of Lj\u2032 + Lj 1s. Note that policy \u03c8 will be operating under case (A) and thus it will also generate a sequence of allocations in which the first Lj\u2032 + Lj allocations in the sequence {lt, lt+1, \u00b7 \u00b7 \u00b7 , lc} will be all the products belonging to categories j\u2032 and j (in a different order) and after that the rest of the sequence of allocations is identical to that under \u03c8\u2032. Thus on such an event W both the policies \u03c8\u2032 and \u03c8 generate the same sequence of relevance feedback.\nCase 2: Let us now consider the case where on the event W , X = i \u2208 Mj(t) \u2212 Mj\u2032(t). In this case yj(lt) = yj(l\u2032) = 0 but Yj = 1. In this case the policy \u03c8\n\u2032 generates the sequence of allocations {l\u2032, lt+1, \u00b7 \u00b7 \u00b7 , lc} and gets the feedback {0, yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)}. Where as observe that the policy \u03c8 operates under case (1) and the designer discovers that Yj = 1 by allotting l and then continues to exhaust all the products in j before switching to the prescriptions of \u03c8\u2032. Thus \u03c8 generates a sequence of allocations in which all the products in j are allotted first and then the prescription of \u03c8\u2032 is followed as described in case (A). In the case where \u03c8\u2032 prescribed allotting j at some opportunity and was able to allot all the products in j until the final opportunity c, this leads to a sequence of allocations which is just a different ordering of the elements of the sequence {l\u2032, lt+1, \u00b7 \u00b7 \u00b7 , lc} and thus generates the same number of relevant products shown until time c. In the case where \u03c8\u2032 allotted 0 \u2264 r < Lj products of category j up until the final opportunity c, then under policy \u03c8, the last Lj \u2212 r products in the sequence {0, yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)} are dropped out in lieu of the same number of products in category j in the beginning. But since all products in j are relevant, this number of relevant ads under policy \u03c8 is still at least as high as that under \u03c8\u2032.\nCase 3: Now consider the case where on the event W , X = i \u2208 S(t) \u2212 Mj(t). In this case yj(lt) = yj(l\u2032) = 0 and also Yj = 0. Thus the policy \u03c8\n\u2032 generates the sequence of allocations {l\u2032, lt+1, \u00b7 \u00b7 \u00b7 , lc} and gets the feedback {0, yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)}. In this case \u03c8 operates under case (B). Now in the case where l 6\u2208 {l\u2032, lt+1, \u00b7 \u00b7 \u00b7 , lc} for any product l in category j, \u03c8 generates the same sequence of feedback {0, yj(lt+1), \u00b7 \u00b7 \u00b7 , yj(lc)}. In the case where l \u2208 {l\u2032, lt+1, \u00b7 \u00b7 \u00b7 , lc} for some product l in category j, then under \u03c8\u2032, since j has already been tested in the beginning, the\nnegative feedback of category j is not repeated by re-allotting it. In lieu of that the policy moves on and a new feedback is obtained at the end which lay be 1 or 0. Thus \u03c8 generates at least as many relevant recommendations as \u03c8\u2032.\nThus the number of relevant products shown under policy \u03c8 is at least as high as that under policy \u03c8\u2032 for every such event W from a set of disjoint events whose union is the entire probability space. Thus the expected number of relevant products shown is also at least as high."}, {"heading": "7.3 Proof of Theorem 4.1", "text": "Proof. First note that if the relevance matrix Q is such that all the categories form a single nondominated equivalence class, then the farsighted greedy policy is the same as the optimal policy and so W (Q, p, \u03b2) = V (Q, p, \u03b2). Now consider an experimentation opportunity with an associated relevance matrix Q that has K non-dominated equivalence classes (U1, \u00b7 \u00b7 \u00b7 , UK). Further assume that there is some factor 0 < \u03b3 < 1 such that\nW (Q\u03c0k , p \u03c0 k , \u03b2) V (Q\u03c0k , p \u03c0 k , \u03b2) \u2265 \u03b3\nfor each k = 1, \u00b7 \u00b7 \u00b7 ,K, and each \u03c0 \u2282 {1, \u00b7 \u00b7 \u00b7 ,K} such that k /\u2208 \u03c0. Now we have\nV \u03c0k = P (\u03c9(\u03c0, k))( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk\nV (Q\u03c0k , p \u03c0 k , \u03b2))\nand\nW\u03c0k = P (\u03c9(\u03c0, k))( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk\nW (Q\u03c0k , p \u03c0 k , \u03b2))\n\u2265 P (\u03c9(\u03c0, k))(1\u2212 \u03b2 Lk\n1\u2212 \u03b2 + \u03b3\u03b2L\nk\nV (Q\u03c0k , p \u03c0 k , \u03b2)). (5)\nWe thus have\nW\u03c0k V \u03c0k \u2265\n1\u2212\u03b2L k\n1\u2212\u03b2 + \u03b3\u03b2 LkV (Q\u03c0k , p \u03c0 k , \u03b2) 1\u2212\u03b2Lk 1\u2212\u03b2 + \u03b2 LkV (Q\u03c0k , p \u03c0 k , \u03b2)\n(6)\nNow it can be easily verified that for a positive constant c, the function f(u) = c+\u03b3uc+u is strictly decreasing in u. Thus, since V (Q\u03c0k , p \u03c0 k , \u03b2) \u2264 11\u2212\u03b2 , we have that\nW\u03c0k V \u03c0k \u2265\n1\u2212\u03b2L k\n1\u2212\u03b2 + \u03b3\u03b2L\nk\n1\u2212\u03b2 1\u2212\u03b2Lk\n1\u2212\u03b2 + \u03b2Lk 1\u2212\u03b2\n= 1\u2212 (1\u2212 \u03b3)\u03b2L k \u2265 1\u2212 (1\u2212 \u03b3)\u03b2L min\n(7)\nwhere Lmin = mink Lk. This bound holds uniformly for each k = 1, \u00b7 \u00b7 \u00b7 ,K, and each \u03c0 \u2282 {1, \u00b7 \u00b7 \u00b7 ,K} such that k /\u2208 \u03c0. Now if we define\nOPT \u2032 ,\nmax k1,\u00b7\u00b7\u00b7 ,kK\u2208\u03c3(1,\u00b7\u00b7\u00b7 ,K)\nWk1 + \u03b2W k1 k2 + \u00b7 \u00b7 \u00b7+ \u03b2K\u22121W k1,\u00b7\u00b7\u00b7 ,kK\u22121kK , (8)\nthen from (7), one can easily show that OPT \u2032 V (Q,p,\u03b2) \u2265 1\u2212 (1\u2212 \u03b3)\u03b2 Lmin . Now we will show that the farsighted greedy algorithm attains 1 1+\u03b2\u2212\u03b2K factor of OPT\n\u2032. To show this we will use induction in the dynamic programming problem that solves (8). Let \u03b1i be the lower bound on the ratio of the payoff to go under the greedy policy and that under the optimal policy when the number of classes left is i where i varies from 1 to K in the problem (8). We are interested in proving that \u03b1K \u2265 11+\u03b2\u2212\u03b2K . Now if k1, \u00b7 \u00b7 \u00b7 , kK\u22121 is decided then there is only one option left for kK and hence the greedy policy gives the same payoff as the optimal payoff to go. Thus \u03b11 = 1. Now fix an i \u2265 2 and consider the payoff to go under the optimal policy when K \u2212 i classes in the order\nhave been selected. Let the set of these classes already selected be denoted by labels in \u03c0 and denote this optimal payoff to go by G\u03c0OPT \u2032 . Denote the payoff to go under the greedy policy by G \u03c0 g . Let the class selected by the greedy policy next be Uk for k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} \\ \u03c0. Then we have by the definition of \u03b1i\u22121:\nG\u03c0g = W \u03c0 k + \u03b2G \u03c0\u222ak g \u2265W\u03c0k + \u03b2\u03b1i\u22121G\u03c0\u222akOPT \u2032 (9)\nNow first we have G\u03c0OPT \u2032 = max\nj\u22081,\u00b7\u00b7\u00b7 ,K\\\u03c0 W\u03c0j + \u03b2G \u03c0\u222aj OPT \u2032 . (10)\nSuppose now that a genie reveals the feedback for a class Uk for free at this point. Then the optimal payoff under this new information is higher than the optimal payoff if this information is not available, i.e. G\u03c0OPT \u2032 (because one can always choose to ignore the genie). Denote this optimal payoff under the new information structure as G\u0304\u03c0OPT \u2032 . Then under this new information, clearly if it is revealed that the feedback for Uk is positive then one exhausts all the advertisers in Uk, where as if the feedback is negative then one removes Uk from the set of classes and moves on without wasting any opportunity on testing Uk. Thus\nG\u0304\u03c0OPT \u2032 = W \u03c0 k +G \u03c0\u222ak OPT \u2032 \u2265 G\u03c0OPT \u2032 . (11)\nAnd we thus have G\u03c0\u222akOPT \u2032 \u2265 G\u03c0OPT \u2032 \u2212W\u03c0k . (12) Substituting (12) in (9) we have\nG\u03c0g \u2265 W\u03c0k + \u03b2\u03b1i\u22121(G\u03c0OPT \u2032 \u2212W\u03c0k ) = W\u03c0k (1\u2212 \u03b2\u03b1i\u22121) + \u03b2\u03b1i\u22121G\u03c0OPT \u2032 (13)\nFurther observe that since the greedy policy chooses k, we have G\u03c0OPT \u2032 \u2264 W\u03c0k 1\u2212\u03b2i 1\u2212\u03b2 or W \u03c0 k \u2265 G\u03c0OPT \u2032 1\u2212\u03b2 1\u2212\u03b2i and thus we have\n\u03b1i = G\u03c0g G\u03c0OPT \u2032 \u2265 (1\u2212 \u03b2)(1\u2212 \u03b2\u03b1i\u22121) 1\u2212 \u03b2i + \u03b2\u03b1i\u22121\n\u2265 (1\u2212 \u03b2)(1\u2212 \u03b2\u03b1i\u22121) 1\u2212 \u03b2K + \u03b2\u03b1i\u22121.\nHere the second inequality follows since i \u2264 K. Now consider the recurrence equation\n\u03b1i = (1\u2212 \u03b2)(1\u2212 \u03b2\u03b1i\u22121)\n1\u2212 \u03b2K + \u03b2\u03b1i\u22121. (14)\nWe have that \u03b1i \u2264 \u03b1i\u22121 for \u03b1i\u22121 \u2265 11+\u03b2\u2212\u03b2K and hence the sequence {\u03b1i} generated by the recurrence relation, with \u03b11 = 1 is decreasing as long as \u03b1i \u2265 11+\u03b2\u2212\u03b2K . Further, we can verify that for \u03b1i\u22121 \u2265 11+\u03b2\u2212\u03b2K , we have\n\u03b1i\u22121 \u2212 \u03b1i = \u03b1i\u22121(1\u2212 \u03b2)\u2212 (1\u2212 \u03b2)(1\u2212 \u03b2\u03b1i\u22121)\n1\u2212 \u03b2K\n\u2264 \u03b1i\u22121 \u2212 1\n1 + \u03b2 \u2212 \u03b2K .\nThus we can conclude that the sequence {\u03b1i} is uniformly bounded below by \u03b1\u2217 = 11+\u03b2\u2212\u03b2K which is the fixed point of the recurrence equation. Thus we have that \u03b1K \u2265 11+\u03b2\u2212\u03b2K which is what we desired to prove.\nThus after combining the bounds, we have that\nW (Q, p, \u03b2) V (Q, p, \u03b2) \u2265 1\u2212 (1\u2212 \u03b3)\u03b2\nLmin\n1 + \u03b2 \u2212 \u03b2K . (15)\nLet Lmin be the minimum number of products in any category in L, i.e. Lmin = minj=1,\u00b7\u00b7\u00b7 ,H Lj . Now since Lmin \u2264 Lmin and K \u2264 H , which is the total number of categories, we have the following bound that holds irrespective of the relevance matrix Q at any given level:\nW (Q, p, \u03b2) V (Q, p, \u03b2) \u2265 1\u2212 (1\u2212 \u03b3)\u03b2 Lmin 1 + \u03b2 \u2212 \u03b2H . (16)\nNow let \u03b31 = 1 and for i \u2265 2 consider the recurrence equation\n\u03b3i = 1\u2212 (1\u2212 \u03b3i\u22121)\u03b2Lmin\n1 + \u03b2 \u2212 \u03b2H . (17)\nNow \u03b3i \u2264 \u03b3i\u22121 as long as \u03b3i\u22121 \u2265 1\u2212\u03b2 Lmin\n1+\u03b2\u2212\u03b2H\u2212\u03b2Lmin . Further we can verify that for \u03b3i\u22121 \u2265 1\u2212\u03b2Lmin\n1+\u03b2\u2212\u03b2H\u2212\u03b2Lmin ,\n\u03b3i\u22121 \u2212 \u03b3i = \u03b3i\u22121 \u2212 1\u2212 (1\u2212 \u03b3i\u22121)\u03b2Lmin\n1 + \u03b2 \u2212 \u03b2H\n\u2264 \u03b3i\u22121 \u2212 1\u2212 \u03b2Lmin\n1 + \u03b2 \u2212 \u03b2H \u2212 \u03b2Lmin\nWe can thus conclude that the sequence {\u03b3i} is uniformly bounded below by \u03b3\u2217 = 1\u2212\u03b2 Lmin 1+\u03b2\u2212\u03b2H\u2212\u03b2Lmin which is the fixed point of the recurrence relation. Thus for any (Q, p, \u03b2),\nW (Q, p, \u03b2) V (Q, p, \u03b2) \u2265 1\u2212 \u03b2 Lmin 1 + \u03b2 \u2212 \u03b2H \u2212 \u03b2Lmin . (18)"}, {"heading": "7.4 Proof of Theorem 4.2", "text": "Proof. Consider the set (U1, \u00b7 \u00b7 \u00b7 , UK) of the non-dominated classes of ad categories at the first experimentation opportunity. From the dynamic programming equation (3) we have\nV \u03c0k = P (\u03c9(\u03c0, k))( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk V \u03c0\nk ).\nHere Lk as defined before are the number of ads in class k and V \u03c0\nk is the optimal payoff-to-go conditional on the event E given that class k is also used up. We will approximate this payoff by \u00b5\u03c0k defined as\n\u00b5\u03c0k = P (\u03c9(\u03c0, k))( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 ).\nNote that under the greedy policy, k is chosen to maximize \u00b5\u03c0k . The ratio of the two quantities is\n\u00b5\u03c0k V \u03c0k =\n1\u2212\u03b2L k\n1\u2212\u03b2 1\u2212\u03b2Lk\n1\u2212\u03b2 + \u03b2 LkV\n\u03c0 k\n\u2265 1\u2212 \u03b2L k \u2265 1\u2212 \u03b2L min . (19)\nWhere the first inequality follows since \u03b2L k V \u03c0 k \u2264 \u03b2L\nk\n1\u2212\u03b2 and second follows from the definition of Lmin, since lemma 3.3 says that the number of ads in a class can only grow. We will later show in an example that for our greedy policy, this bound is tight. Now the optimal policy finds the best order in which to present the non-dominated equivalence classes which solves the following optimization problem.\nOPT , max k1,\u00b7\u00b7\u00b7 ,kK\nVk1 + \u03b2V k1 k2 + \u03b22V k1,k2k3 + \u00b7 \u00b7 \u00b7+ \u03b2 K\u22121V k1,\u00b7\u00b7\u00b7 ,kK\u22121 kK . (20)\nConsider instead\nOPT \u2032 , max k1,\u00b7\u00b7\u00b7 ,kK \u00b5k1 + \u03b2\u00b5 k1 k2 + \u03b22\u00b5k1,k2k3 + \u00b7 \u00b7 \u00b7+ \u03b2 K\u22121\u00b5 k1,\u00b7\u00b7\u00b7 ,kK\u22121 kK . (21)\nClearly (19) implies that OPT \u2032 OPT \u2265 1 \u2212 \u03b2 Lmin . Now using the same arguments as that used in the proof of Theorem 4.1 for the optimization problem in definition (8), we can show that the greedy algorithm attains OPT \u2032\n1+\u03b2\u2212\u03b2K in (21). Since K \u2264 H , the result follows."}, {"heading": "7.5 Proof of Theorem 4.3", "text": "Proof. For a user of type i, the total number of products with positive feedback is given by ri =\u2211H j=1 q i jLj . Thus the expected total number of products with positive feedback is\nR = N\u2211 i=1 riPX(i)\nOn the event W that the number of display opportunities is greater than L, any policy obtains the full payoff of R. Thus its expected payoff is bounded by\nVG \u2265 P (W )R = P (C \u2265 L)R = \u03b2L\u22121R.\nFurther the optimal policy cannot attain a payoff greater than R. Thus the ratio of the payoff under the any policy and that under the optimal policy is at least \u03b2L\u22121."}, {"heading": "7.6 Recursive computation of Farsighted Greedy", "text": "Algorithm 4 (Farsighted greedy): Function [W (Q, p, \u03b2), A(Q, p, \u03b2)] where Q is a relevance matrix and p is a probability distribution over user types.\n\u2022 If Q is empty, return W (Q, p, \u03b2) = 0. \u2022 If Q is non-empty, let the non-dominated equivalence classes be (U1, \u00b7 \u00b7 \u00b7 , UK) and the\nnumber of products in each class be denoted by Lk = \u2211 j\u2208Uk Lj . Let N be the number of\nrows in Q corresponding to the user types. \u2022 Let the event \u03c9(k) be the event {X \u2208 S(k)} where\nS(k) = {i \u2208 N : qij = 1\u2200 j \u2208 Uk}.\nLet Qk be the matrix obtained after removing all the columns corresponding to categories in Uk and the rows corresponding to all the user types in S(k)c and let Qkres be the matrix obtained after removing all the columns corresponding to categories in Uk and the rows corresponding to all the user types in S(k). Finally, let pk denote the distribution on the user types conditional on the event {X \u2208 S(k)} and pkres be the distribution on the user types conditional on {X \u2208 S(k)c}. \u2022 Then define\nWk = P (S(k))\n( 1\u2212 \u03b2Lk\n1\u2212 \u03b2 + \u03b2L\nk\nW (Qk, pk, \u03b2) ) \u2022 Let W \u2217 = maxkWk and let k\u2217 \u2208 arg maxkWk\nReturn W (Q, p, \u03b2) = W \u2217 + \u03b2(1\u2212 P (S(k\u2217))W (Qk \u2217 res, p k\u2217 res, \u03b2).\nA(Q, p, \u03b2) = k\u2217."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Fab: content-based, collaborative recommendation", "author": ["Marko Balabanovi\u0107", "Yoav Shoham"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "When policies are better than plans: Decisiontheoretic planning of recommendation sequences", "author": ["Thorsten Bohnenberger", "Anthony Jameson"], "venue": "In Proceedings of the 6th international conference on Intelligent user interfaces,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["John S Breese", "David Heckerman", "Carl Kadie"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Hybrid recommender systems: Survey and experiments", "author": ["Robin Burke"], "venue": "User modeling and user-adapted interaction,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Sequential design of experiments", "author": ["Herman Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1959}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Contributions to the", "author": ["Dorian Feldman"], "venue": "two-armed bandit\u201d problem. The Annals of Mathematical Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1962}, {"title": "Multi-armed bandit allocation indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": "Wiley Online Library,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["Jonathan L Herlocker", "Joseph A Konstan", "Loren G Terveen", "John T Riedl"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Parametrized stochastic multi-armed bandits with binary rewards", "author": ["Chong Jiang", "R Srikant"], "venue": "In American Control Conference (ACC),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Further contributions to the", "author": ["Robert Keener"], "venue": "two-armed bandit\u201d problem. The Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["Tze Leung Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1985}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["Pasquale Lops", "Marco De Gemmis", "Giovanni Semeraro"], "venue": "In Recommender systems handbook,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J Mersereau", "Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Active sequential hypothesis testing", "author": ["Mohammad Naghshvar", "Tara Javidi"], "venue": "The Annals of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Multi-armed bandit problems with dependent arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Content-based recommendation systems. In The adaptive web, pages 325\u2013341", "author": ["Michael J Pazzani", "Daniel Billsus"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Markov decision processes: discrete stochastic dynamic programming, volume 414", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Paul Resnick", "Neophytos Iacovou", "Mitesh Suchak", "Peter Bergstrom", "John Riedl"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Recommender systems in e-commerce", "author": ["J Ben Schafer", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "An mdp-based recommender system", "author": ["Guy Shani", "Ronen I Brafman", "David Heckerman"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1933}, {"title": "Multi-armed bandits and the gittins index", "author": ["Peter Whittle"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1980}], "referenceMentions": [{"referenceID": 25, "context": "Predicting the preferences of users in order to present them with more relevant engagements is a fundamental component of any recommendation system [27, 25].", "startOffset": 148, "endOffset": 156}, {"referenceID": 0, "context": "Over the years, a wide variety of approaches have been proposed for this problem (see [1] for a survey).", "startOffset": 86, "endOffset": 89}, {"referenceID": 21, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 110, "endOffset": 118}, {"referenceID": 17, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 110, "endOffset": 118}, {"referenceID": 23, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 155, "endOffset": 163}, {"referenceID": 12, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 155, "endOffset": 163}, {"referenceID": 3, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 268, "endOffset": 274}, {"referenceID": 7, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 268, "endOffset": 274}, {"referenceID": 26, "context": "In this paper, motivated by several settings of interest in which explicit feedback about the relevance of the recommendations can be received from the user on small timescales, we pursue a less studied approach (see [28]) of modeling the recommendation process as a sequential optimization problem.", "startOffset": 217, "endOffset": 221}, {"referenceID": 5, "context": "We consider a model that is derived from cluster models for collaborative filtering (see [6]) in which the history of user behaviors is compressed into a predictive model, where users are classified into \u2018types\u2019 that capture the preference profile of the user.", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "To the best of our knowledge, its earliest appearance in literature can be traced back to [5], which proposed a decision-theoretic modeling of the problem of generating recommendations (on a palm-top) for a user navigating through an airport.", "startOffset": 90, "endOffset": 93}, {"referenceID": 26, "context": "[28] proposed a framework for modeling the sequential optimization problem in online recommendation systems as a Markov Decision Process (MDP) [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[28] proposed a framework for modeling the sequential optimization problem in online recommendation systems as a Markov Decision Process (MDP) [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "In a multi-armed bandit problem (MAB), first introduced by Thompson in [29], a decision-maker faces a set of arms whose reward characteristics are uncertain and seeks to optimize the sequence in which they are pulled so as to maximize some long-run reward.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "Our model falls in the Bayesian setting [12, 30], in which an initial prior distribution is assumed over the parameters of a probabilistic reward generating model for each arm, and one performs Bayesian updates of these estimates as rewards are observed.", "startOffset": 40, "endOffset": 48}, {"referenceID": 28, "context": "Our model falls in the Bayesian setting [12, 30], in which an initial prior distribution is assumed over the parameters of a probabilistic reward generating model for each arm, and one performs Bayesian updates of these estimates as rewards are observed.", "startOffset": 40, "endOffset": 48}, {"referenceID": 16, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 23, "endOffset": 31}, {"referenceID": 15, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 23, "endOffset": 31}, {"referenceID": 6, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 293, "endOffset": 299}, {"referenceID": 1, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 293, "endOffset": 299}, {"referenceID": 11, "context": "In the Bayesian case, a seminal result by Gittins [12] shows that the optimal policy dynamically computes an index for each arm independently of all other arms, and picks the arm with the highest index at each step.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "[11] and [15] analyze two-armed bandit problems in which reward characteristics of two arms are known, but which arm corresponds to which reward distribution is not known, which leads to a natural dependence between the arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[11] and [15] analyze two-armed bandit problems in which reward characteristics of two arms are known, but which arm corresponds to which reward distribution is not known, which leads to a natural dependence between the arms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "[21] studies another version of the problem in which the arms can be grouped into clusters of dependent arms, in which case the Gittins decomposition result can be partially extended.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Recently, [19] considered a specific model of a MAB problem with dependent arms, where they analyzed the performance of a greedy policy and derived asymptotic optimality results.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 9, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 24, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 13, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "In the special case when each category has a single product, our problem is also related to the active sequential hypothesis testing problem [9, 20].", "startOffset": 141, "endOffset": 148}, {"referenceID": 19, "context": "In the special case when each category has a single product, our problem is also related to the active sequential hypothesis testing problem [9, 20].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "As mentioned earlier, this problem is a type of a Bayesian multi-armed bandit problem with correlated rewards (see [12, 30]) with an additional constraint on the number of times each arm may be pulled.", "startOffset": 115, "endOffset": 123}, {"referenceID": 28, "context": "As mentioned earlier, this problem is a type of a Bayesian multi-armed bandit problem with correlated rewards (see [12, 30]) with an additional constraint on the number of times each arm may be pulled.", "startOffset": 115, "endOffset": 123}], "year": 2015, "abstractText": "Motivated by online settings where users can provide explicit feedback about the relevance of products that are sequentially presented to them, we look at the recommendation process as a problem of dynamically optimizing this relevance feedback. Such an algorithm optimizes the fine tradeoff between presenting the products that are most likely to be relevant, and learning the preferences of the user so that more relevant recommendations can be made in the future. We assume a standard predictive model inspired by collaborative filtering, in which a user is sampled from a distribution over a set of possible types. For every product category, each type has an associated relevance feedback that is assumed to be binary: the category is either relevant or irrelevant. Assuming that the user stays for each additional recommendation opportunity with probability \u03b2 independent of the past, the problem is to find a policy that maximizes the expected number of recommendations that are deemed relevant in a session. We analyze this problem and prove key structural properties of the optimal policy. Based on these properties, we first present an algorithm that strikes a balance between recursion and dynamic programming to compute this policy. We further propose and analyze two heuristic policies: a \u2018farsighted\u2019 greedy policy that attains at least 1 \u2212 \u03b2 factor of the optimal payoff, and a naive greedy policy that attains at least 1\u2212\u03b2 1+\u03b2 factor of the optimal payoff in the worst case. Extensive simulations show that these heuristics are very close to optimal in practice.", "creator": "LaTeX with hyperref package"}}}