{"id": "1503.02143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2015", "title": "Model selection of polynomial kernel regression", "abstract": "polynomial kernel regression is one of the standard and state - of - the - art learning strategies. however, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. actually the first aim worthy of repeating this introductory paper is to develop a decision strategy to select correct these parameters. on relatively one hand, based all on the worst - case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. in other words, then the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel ratio is suitable tuned. on the like other hand, taking account of the implementation of the algorithm, the regularization of term is required. summarily, changing the effect of satisfying the regularization term error in polynomial kernel regression is only to circumvent the \" ill - condition \" of the kernel linear matrix. based on this, the second purpose of this paper is to propose a new simple model selection error strategy, choose and then design out an efficient learning search algorithm. both theoretical and experimental analysis show that the new strategy outperforms the previous estimate one. theoretically, we prove again that the new learning strategy is almost optimal correctly if the regression objective function is smooth. experimentally, it is shown that the new strategy can significantly reduce the computational burden without risking loss of generalization capability.", "histories": [["v1", "Sat, 7 Mar 2015 08:39:15 GMT  (123kb)", "http://arxiv.org/abs/1503.02143v1", "29 pages, 4 figures"]], "COMMENTS": "29 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shaobo lin", "xingping sun", "zongben xu", "jinshan zeng"], "accepted": false, "id": "1503.02143"}, "pdf": {"name": "1503.02143.pdf", "metadata": {"source": "CRF", "title": "Model selection of polynomial kernel regression", "authors": ["Shaobo Lin", "Xingping Sun", "Zongben Xu", "Jinshan Zeng"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n02 14\n3v 1\n[ cs\n.L G\n] 7\nM ar\nPolynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the \u201c ill-condition\u201d of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability.\nIndex Terms\nModel selection, regression, polynomial kernel, learning rate.\nS. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China, Z. Xu and J. Zeng are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an 710049, P R China, X. Sun is with the Department of Mathematics, Missouri State University, Springfield, MO 65897, USA\nMarch 10, 2015 DRAFT\n2 I. INTRODUCTION\nIn many scientific fields, large amount of data (xi, yi)mi=1 arise from sampling unknown functions. Scientists train data and then synthesize a function f such that f(x) is an efficient estimate of the output y when a new input x is given. The training process is usually divided into two steps. The one is to select a suitable model and the other focuses on designing an efficient learning algorithm based on the selected model. Generally speaking, the model selection strategy comprises choosing a hypothesis space, a family of parameterized functions that regulate the forms and properties of the estimator to be found, and selecting an optimization criterion, the sense in which the estimator is defined. The learning algorithm is an inference process to yield the objective estimator from a finite set of data. The central question of learning theory is how to select a feasible model and then develop an efficient algorithm such that the synthesized function can approximate the original unknown but definite function.\nIf the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter. After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection. The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19]. The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36]. The topic of the current paper falls into the second category. We study the parameter selection problem in polynomial kernel regression.\nDifferent from other widely used kernels [9], the reproducing kernel Hilbert space Hs of the polynomial kernel Ks = (1 + x \u00b7 y)s is a finite-dimensional vector space, and its dimension depends only on s. Therefore, one can tune s directly to control the capacity of Hs. Using this fact, [41] found that the regularization parameter in polynomial kernel regression should decrease exponentially fast with the sample size for appropriately selected s. They then attributed it as an essential feature of the polynomial kernel learning. The first purpose of this paper is to continue the study of [41]. Surprisingly, after the rigorous proof, we find that, as far as the learning rate is concerned, the regularization parameter can decrease arbitrarily fast for a suitable s. An extreme case is that in the framework of model selection, the regularization term can be omitted. This\nMarch 10, 2015 DRAFT\n3 automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression?\nTo answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points. However, what factor causes overfitting in the learning process is usually neglected by numerous programmers. Therefore, the essential role of the regularization term can not be captured. To the best of our knowledge, there are two main reasons cause the overfitting phenomenon. The one is the algorithm-based factor such as ill-condition of the kernel matrix and the other is the modelbased factor like too large capacity of the hypothesis space. We find that there is only one job the regularization term in polynomial kernel regression doing: to assure that a simple matrixinverse technique can finish the learning task. This phenomenon is quite different from the other kernel-based methods. For example, since the Gaussian-RKHS is an infinite dimensional vector space, the introducing of regularization term in Gaussian kernel regression is to control both the condition number of the kernel matrix and capacity of the hypothesis space [15].\nBased on the above assertions, the second purpose of this paper is to propose a new model selection method. By the well known representation theorem [7] in learning theory, the essential hypothesis space of polynomial kernel regression is the linear space H := span{(1+x1\u00b7x)s, \u00b7 \u00b7 \u00b7 , (1+ xm \u00b7x)s}. Since the algorithm-based factor is the only reason of over-fitting in polynomial kernel regression. We can choose n points {ti}ni=1 such that the matrix ((1 + tj \u00b7 xi)s)m,ni=1,j=1 is nonsingular. The set {ti}ni=1 can be easily obtained. For example, we can draw {ti}ni=1 identically and independently according to the uniform distribution. Then the pseudo-inverse technique [27] can conduct the estimator easily. In the new model, it can be found in Section 4 that there is only one parameter, s, need tuning. We also give an efficient strategy to select s based on the theoretical analysis. Surprisingly, we find that the difficulty of model selection in our setting depends heavily on the dimension of input space. It is shown that the higher the dimension, the easier the model selection.\nBoth theoretical analysis and experimental simulations are provided to illustrate the performance of the new model selection strategy. Theoretically, the new method is proved to be the almost optimal strategy if the so-called regression function is smooth. Furthermore, it is also shown that the pseudo-inverse technique can realize the almost optimality. Experimentally, both\nMarch 10, 2015 DRAFT\n4 toy simulations and UCI standard data experiments imply that the new method is more efficient than the previous model selection strategy. More concretely, the new method can significantly reduce the computational burden without loss of generalization capability. The most highlight of the proposed model is that there is only a parameter (or almost no parameter of high-dimensional case) need to be tuned in the learning process.\nThe rest of paper is organized as follows. In the next section, we give a fast review of statistical learning theory and kernel method. In Section 3, we study the model selection problem of the classical polynomial kernel regression. Section 4 describes a new model selection strategy and provide its theoretical properties. In Section 5, both toy and real world simulation results are reported to verify the theoretical results. Section 6 is devoted to proving the main results, and Section 7 draw a simple conclusion."}, {"heading": "II. A FAST REVIEW OF STATISTICAL LEARNING THEORY AND KERNEL METHODS", "text": "Let X \u2286 Rd be the input space and Y \u2286 R be the output space. Suppose that the unknown probability measure \u03c1 on Z := X \u00d7 Y admits the decomposition\n\u03c1(x, y) = \u03c1X(x)\u03c1(y|x).\nLet z = (xi, yi)mi=1 be a finite random sample of size m, m \u2208 N, drawn independently and identically according to \u03c1. Suppose further that f : X \u2192 Y is a function that one uses to model the correspondence between X and Y, as induced by \u03c1. One natural measurement of the error incurred by using f of this purpose is the generalization error, defined by\nE(f) := \u222b\nZ (f(x)\u2212 y)2d\u03c1,\nwhich is minimized by the regression function [7], defined by\nf\u03c1(x) := \u222b\nY yd\u03c1(y|x).\nWe do not know this ideal minimizer f\u03c1, since \u03c1 is unknown, but have access to random examples from X \u00d7 Y sampled according to \u03c1. Let L2\u03c1\nX be the Hilbert space of \u03c1X square integrable function on X , with norm denoted by\n\u2016 \u00b7 \u2016\u03c1. With the assumption that f\u03c1 \u2208 L2\u03c1 X , it is known that, for every f \u2208 L2\u03c1X , there holds\nE(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u20162\u03c1. (1)\nMarch 10, 2015 DRAFT\n5 So, the goal of learning is to find the best approximation of the regression function f\u03c1 within a hypothesis space H. Let fH \u2208 H be the best approximation of f\u03c1, i.e., fH := argming\u2208H \u2016f\u2212g\u2016\u03c1. If there is an estimator fz \u2208 H based on the samples z in hand, then we have\nE(fz)\u2212 E(f\u03c1) = \u2016f\u03c1 \u2212 fH\u20162\u03c1 + E(fz)\u2212 E(fH). (2)\nIt is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u20162\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH). The best hypothesis space H\u2217 is obtained when the best comprise between the conflicting requirements of small bias and small variance is achieved. This is the well known \u201cbias-variance\u201d dilemma of model selection.\nLet K : X \u00d7 X \u2192 R be continuous, symmetric and positive semidefinite, i.e., for any finite set of distinct points {x1, x2, . . . , xm} \u2282 X , the kernel matrix (K(xi, xj))mi,j=1 is positive semidefinite. If HK is the reproducing kernel Hilbert space associated with the kernel K. Then HK (see [2]) is the closure of the linear span of the set of functions {Kx = K(x, \u00b7) : x \u2208 X} with the inner product \u3008\u00b7, \u00b7\u3009K satisfying \u3008Kx, Ky\u3009K = K(x, y) and\n\u3008Kx, f\u3009K = f(x), \u2200x \u2208 X, f \u2208 HK .\nThe following Aronszajn Theorem (see [2]) describes an essential relationship between the RKHS and reproducing kernel.\nLemma 1: Let H be a separable Hilbert space of functions over X with orthonormal basis {\u03c6k}\u221ek=0. H is a reproducing kernel Hilbert space if and only if\n\u221e \u2211\nk=0\n|\u03c6k(x)|2 < \u221e\nfor all x \u2208 X . The unique reproducing kernel K is defined by\nK(x, y) := \u221e \u2211\nk=0\n\u03c6k(x)\u03c6k(y).\nThe regularized least square algorithm in HK is defined by\nfz,\u03bb := arg min f\u2208HK\n{\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 + \u03bb\u2016f\u20162K } .\nHere \u03bb \u2265 0 is a constant called the regularization parameter. Usually, it depends on the sample number m. If the empirical error is defined by\nEz(f) := 1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2,\nMarch 10, 2015 DRAFT\n6 then the corresponding problem can be represented as\nfz,\u03bb = arg min f\u2208HK\n{ Ez(f) + \u03bb\u2016f\u20162K } ."}, {"heading": "III. MODEL SELECTION IN POLYNOMIAL KERNEL REGRESSION", "text": "Let X = Bd, Y = [\u2212M,M ], where Bd denotes the unit ball in Rd, and M < \u221e. We employ the polynomial kernel Ks(x, y) = (1 + x \u00b7 y)s to tackle regression problem. From Lemma 1, it is easy to check that the RKHS of Ks, Hs, coincides with the space (Pds , \u3008\u00b7, \u00b7\u3009s), where \u3008\u00b7, \u00b7\u3009s be the inner product deduced from Ks according to \u3008Ks(x, \u00b7), Ks(\u00b7, y)\u3009s = Ks(x, y), and Pds be the set of algebraic polynomials of degree at most s.\nWe study the parameter selection for the following model\nfz,\u03bb,s := arg min f\u2208Hs\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 + \u03bb\u2016f\u20162s. (3)\nFrom (1, the main purpose of model selection is to yield an optimal estimate for\nE(fz,\u03bb,s)\u2212 E(f\u03c1). (4)\nThe error (4) clearly depends on z and therefore has a stochastic nature. As a result, it is impossible to say something about (4) in general for a fixed z. Instead, we can look at its behavior in expectation as measured by the expectation error\nE\u03c1m(\u2016fz \u2212 f\u03c1\u2016\u03c1) := \u222b\nZm \u2016fz \u2212 f\u03c1\u2016d\u03c1m, (5)\nwhere the expectation is taken over all realizations z obtained for a fixed m, and \u03c1m is the m fold tensor product of \u03c1. Obviously, the error (5) depends on m, s, \u03bb and f\u03c1.\nRecall that we do not know \u03c1 so that the best we can say about it is that it lies in M(\u0398), where M(\u0398) is the class of all Borel measures \u03c1 on Z such that f\u03c1 \u2208 \u0398 \u2282 L2\u03c1X . We enter into a competition over all estimators Am : z \u2192 fz and define\nem(\u0398) := inf Am sup \u03c1\u2208M(\u0398)\nE\u03c1m(\u2016f\u03c1 \u2212 fz\u20162\u03c1).\nIt is easy to see that em(\u0398) quantitively measures the quality of fz.\nNow, we are in a position to discuss the model selection of polynomial kernel regression. Let\nk = (k1, k2, . . . , kd), ki \u2208 N, and define the derivative\nDkf(x) := \u2202|k|f\n\u2202k1x1 \u00b7 \u00b7 \u00b7\u2202kdxd ,\nMarch 10, 2015 DRAFT\n7 where |k| := k1 + \u00b7 \u00b7 \u00b7+ kd. The classical Sobolev class is then defined for any r \u2208 N by\nW rp :=\n{\nf : max 0\u2264|k|\u2264r\n\u2016Dkf\u2016Lp(X) < \u221e, r \u2208 N } .\nLet \u03a0M t denote the clipped value of t at \u00b1M , that is, \u03a0M t := min{M, |t|}sgnt. Then it is obvious that [41] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds\nE(\u03a0Mfz,\u03bb,s)\u2212 E(f\u03c1) \u2264 E(fz,\u03bb,s)\u2212 E(f\u03c1).\nFor arbitrary C > 0, \u2308C\u2309 denotes the smallest integer not larger than C. The following Theorem 1 shows the actions of the parameters in deducing the learning rate and how to select optimal parameters.\nTheorem 1: Let r \u2208 N and fz,\u03bb,s be defined as in (3). Then, for arbitrary f\u03c1 \u2208 L\u221e(Bd), there holds\nE\u03c1m{\u2016\u03a0Mfz,\u03bb,s \u2212 f\u03c1\u20162\u03c1} \u2264 C   sd logm\nm + inf\nP\u2208Pd [s/2]\n\u2016f\u03c1 \u2212 P\u20162\u221e + \u03bb(4d)2s   . (6)\nFurthermore, if f\u03c1 \u2208 W r\u221e and s = \u2308 m 1 d+2r \u2309 , then for all 0 \u2264 \u03bb \u2264 m\u2212 2r2r+d (4d) \u22121 (d+2r) , there exist constants C1 and C2 depending only on d, r,M and p such that,\nC1m \u2212 2r d+2r \u2264 em(W r\u221e) \u2264 sup \u03c1\u2208M(W r \u221e ) E\u03c1m(\u2016f\u03c1 \u2212 \u03a0Mfz,\u03bb,s\u20162\u03c1) \u2264 C2m\u2212 2r d+2r logm. (7)\nAt first, we introduce some related work and compare them with Theorem 1. The first result, to the best of our knowledge, concerning selection of the optimal regularization parameter in the framework of learning theory belongs to [8]. As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter. They declared that there is an optimal regularization parameter \u03bb > 0 which makes the generalization error the smallest. This leads a prevailing conception that the error estimate (6) should have more terms containing \u03bb, besides the term \u03bb(4d)2s. However, it is not what our result has witnessed, which seems a contradiction at the first glance. After checking the proof of [8] carefully, we find that there is nothing to surprise. On one hand, the optimal parameter mentioned in [8] aims to the generalization error, containing learning rate and the constant C2, while our result only concerns the learning rate. On the other hand, [8]\u2019s result is more suitable to describe the performance of K(\u00b7, \u00b7) satisfying \u2016f\u2016\u221e \u2264 C\u2016f\u2016K , where C is a constant independent of m. However, this property doesn\u2019t hold for the polynomial kernel since the only thing we can confirm is that \u2016f\u2016\u221e \u2264 2s/2\u2016f\u2016s, where s depends on m.\nMarch 10, 2015 DRAFT\n8 After [8], we have witnessed the multiple emergence of the selection strategies of regularization parameter. The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41]. The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36]. The most different job we done is that we find the regularization parameter for polynomial kernel learning can decrease arbitrarily fast, an extreme case of which is that non-regularization leastsquare can also conduct the almost optimal learning rate. In other words, Theorem (1) shows that as far as the model selection is concerned, the choice of s is much more important than the choice of \u03bb.\nFor polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter. It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m\u2212 2r2r+d+1 ), which is improved by Theorem 1 in the following three directions. Firstly, the learning rate analysis in Theorem 1 is based on distribution-free theory: we do not impose any assumptions to the marginal distribution \u03c1X . Secondly, the optimal estimate is established for arbitrary W rp (0 < r < \u221e) rather than 0 < r \u2264 2. Thirdly, Theorem 1 states that the learning rate can be improved into the almost optimal one. Therefore, as far as the learning rate is concerned, polynomial kernel is almost optimal choice if the smoothness information of the regression function is known.\nEberts and Steinwart [15] have already built a similar learning rate analysis for Gaussian kernel regression. It is valuable to compare the performance between Gaussian kernel regression and polynomial kernel regression. In the former one, there are two parameters need tuning. The one is the width of the Gaussian kernel and the other is the regularization parameter. Both the width and the regularization parameter are real number in some intervals. Thus, a wisdom method is to use the so-called cross-validation strategy [14, Chpater 8] to fix them, which causes tremendous computation if the size of samples is large. Differently, the kernel parameter of polynomial kernel is a discrete quantity and our result shows that s = \u2308 m 1 d+2r \u2309 is almost optimal choice for arbitrary f\u03c1 \u2208 W rp . Although, the smoothness parameter r is usually unknown in practice, Theorem 1 gives us a criterion to chose s. Since s is discrete, and s may be smaller than \u2308m1/d\u2309, there are only [m1/d] possible value of s. Noting that if d is large, no matter how large m is,\nMarch 10, 2015 DRAFT\n9 [m1/d] can not be large than 10 (the most possible case is s \u2264 2 or s \u2264 3 (see Section 5)). Therefore, it is very easy to fix the kernel parameter through the cross-validation method. Under this circumstance, the computational burden of polynomial kernel regression can be reduced and much less than that of Gaussian kernel regression (See Table 4 in Section 5).\nBy using the well known plug-in rules, which define a classier gz,\u03bb,s has the form\ngz,\u03bb,s :=\n  \n  1, if fz,\u03bb,s \u2265 12 , 0, if fz,\u03bb,s < 12 ,\n(8)\nTheorem 1 and [39] imply that the classier defined as in (8) is also almost optimal if the well known Bayes decision function satisfies a certain smoothness assumption. Therefore, Ks is also one of the best choice to deal with pattern recognition problem under this setting.\nAt last, we discuss the effect of the regularization term playing in polynomial kernel regression. The purpose of introducing regularization term in kernel learning is to overcome the overfitting phenomenon. However, the factor causing overfitting is a little sophisticated. It may attribute to the high capacity of the hypothesis, the ill-condition of the kernel matrix, or both of them. In short, there are two main factors leading to overfitting in kernel learning. The one is the modelbased factor, i.e., a large capacity of the hypothesis space and the other is the algorithm-based factor, i.e., ill-condition of the kernel matrix. In the polynomial kernel regression, Theorem 1 shows that arbitrary small \u03bb (an extreme case is \u03bb = 0) can deduce almost optimal learning rate if s is appropriately tuned. Thus, the overfitting phenomenon in polynomial kernel learning is not caused by the model-based factor for suitable s. Recall that the kernel matrix, A := ((1+xi \u00b7 xj)) m i,j=1, is singular if m > ( s+d s ) , which is most possible in the learning process. This makes the simple matrix-inverse technique can not deduce the estimator directly. Thus, a regularization term is required to guarantee the non-singularity of the kernel matrix A. A small \u03bb leads to the ill-condition of the matrix A + \u03bbI and a large \u03bb conducts large approximation error. This reflects the known tug of war between variance and bias. In short, the overfitting phenomenon of polynomial kernel is caused by the algorithm-based factor rather than model selection. Thus, we can design a more efficient algorithm directly instead of imposing regularization term in the model to reduce the computational burden. This will be the main topic of the next section.\nMarch 10, 2015 DRAFT\n10"}, {"heading": "IV. AN EFFICIENT MODEL SELECTION FOR POLYNOMIAL KERNEL REGRESSION", "text": "In this section, we propose a feasible model selection method for polynomial kernel regression based on the theoretical analysis proposed in Section 3 and design a learning algorithm with low computational complexity. It is analyzed that the regularization term in the model\narg min f\u2208Hs\n{\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 + \u03bb\u2016f\u20162s }\nis to overcome the ill-condition of the kernel matrix A. And the model\narg min f\u2208Hs\n{\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 }\ncan realize the almost optimality of regression. Both of these make it possible to select a new and more efficient model for polynomial kernel regression. Noting that for arbitrary s, Hs = Pds , we can rewrite the above optimization problem as\narg min f\u2208Pds\n{\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 } .\nRecalling that the dimension of Pds is n = ( s+d s ) , we can find {\u03b7i}ni=1 \u2282 Bd such that {(1 + \u03b7i \u00b7 x)s}ni=1 is a linear independent system. Then,\nPds = { n \u2211\ni=1\nci(1 + \u03b7i \u00b7 x)s : ci \u2208 R } =: H\u03b7,n. (9)\nHence, the above optimization problem can be converted to {\narg min f\u2208H\u03b7,n\n1\nm\nm \u2211\ni=1\n(f(xi)\u2212 yi)2 } . (10)\nThus, there are two things we should do. The one is to give a selection strategy of {\u03b7i}ni=1 and the other is to guarantee the non-singularity of the matrix Am,n := ((1 + xi \u00b7 \u03b7j)s)m,ni,j=1.\nTo this end, we should introduce the conceptions of Haar space and fundamental system [34]. Let V \u2208 C(Bd) be an N-dimensional linear space. V is called a Haar space of dimension N if for arbitrary distinct points x1, . . . , xN \u2208 Bd and arbitrary f1, . . . , fN \u2208 R there exists exactly one function s \u2208 V with s(xi) = fi, 1 \u2264 i \u2264 N . The following Lemma 2 [34, Theorem 2.2] shows some important properties of Haar space.\nLemma 2: The following statements are equivalent. (1) V is N-dimensional Haar space. (2) Every u \u2208 V /{0} has at most N \u2212 1 zeros. (3) For any distinct points x1, . . . , xN \u2208 Bd and any basis u1, . . . , uN of V , the matrix (uj(xi))Ni,j=1 is non-singular.\nMarch 10, 2015 DRAFT\n11\nOf course, if we can find a set of points in Bd, {\u03b7i}ni=1, such that H\u03b7,n is the Haar space of dimension n + 1, then it follows from Lemma 2 that all above problems can be resolved. However, for d \u2265 2, this conjecture does not hold [34, Theorem 2.3]. Lemma 3: Suppose d \u2265 2. Then there does not exist Haar space on Bd of dimension N \u2265 2. Based on this, we introduce the conception of fundamental system with respect to the polynomial kernel Ks\nDefinition 1: Let \u03b6 := {\u03b6i}ni=1 \u2282 Bd. \u03b6 is called a Ks-fundamental system if\ndimH\u03b6,n = ( s+d s ) .\nFrom the above definition, it is easy to see that arbitrary Ks-fundamental system implies (9). The following Proposition 1 reveals that almost all n = (n+ss ) points set is the Ks-fundamental system.\nProposition 1: Let s, n \u2208 N and n = (n+ss ) . Then the set\n{\u03b6 = (\u03b6i)ni=1 : dimH\u03b6,n < n}\nhas Lebesgue measure 0.\nBased on Proposition 1, we can design a simple strategy to choose the centers {\u03b7j}nj=1. Since the uniform distribution is continuous with respect to Lebesgue measure [4], we can draw {\u03b7j}nj=1 independently and identically according to the uniform distribution. Then with probability 1, there holds\nPds = { n \u2211\ni=1\nci(1 + \u03b7i \u00b7 x)s : ci \u2208 R } .\nNow we turn to prove the non-singularity of the matrix Am,n, which can be implied from the\nfollowing Proposition 2.\nProposition 2: Let s,m, n \u2208 N. If {xi}mi=1 are i.i.d. random variables drawn according to arbitrary distribution \u00b5, and {\u03b7j}nj=1 is a Ks-fundamental system. Then for arbitrary vector c = (c1, . . . , cn),\n\u222b\nBd\n\n\nn \u2211\nj=1\ncjKs(\u03b7j , x)\n\n\n2 dx\n\u221a 1\u2212 |x|2 \u2264 1 m (Am,nc) TAm,nc \u2264 3 \u222b Bd\n\n\nn \u2211\nj=1\ncjKs(\u03b7j , x)\n\n\n2 dx\n\u221a 1\u2212 |x|2 .\nholds with probability at least 1\u2212 Cnd m , where C is a constant depending only on d.\nMarch 10, 2015 DRAFT\n12\nIt can be easily deduced from Proposition 2 that with probability at least 1\u2212 Cnd m , the matrix Am,n is non-singular. Indeed, if Am,n is non-singular, then it follows from Proposition 2 that there exists a nontrivial set {cj}nj=1 such that \u222b\nSd\n\n\nn \u2211\nj=1\ncjKs(\u03b7j, x)\n\n\n2 dx\n\u221a 1\u2212 |x|2 = 0.\nThis implies n \u2211\nj=1\ncjKs(\u03b7j , x) = 0, x \u2208 Bd,\nwhich is impossible since {\u03b7j} is a Ks-fundamental system. In the help of the above two propositions, we give an efficient algorithm, called efficient polynomial kernel regression (EPKR), based on the model selection strategy (10).\nAlgorithm 1 Efficient polynomial kernel regression (EPKR) Input: Let (xi, yi)mi=1 be m samples, s \u2208 N be the degree of polynomial kernel and Ks(x, x\u2032) = (1 + x \u00b7 x\u2032)s be the polynomial kernel Step 1: Let n = (\ns+d s\n)\nbe the number of centers and {\u03b7j}nj=1 be the set of centers, which is a Ks fundamental system. {\u03b7j}nj=1 can be drawn independently and identically according to the uniform distribution. Set Am,n := (Ks(xi, \u03b7j)) m,n i,j=1, y = (y1, . . . , ym) T . Step 2: Set c = pinv(Am,n)y = (c1, . . . , cn)T , where pinv(Am,n) denotes the pseudo-inverse operator in matlab. Output: fz,s(x) = \u2211n j=1 cjKs(\u03b7j, x).\nIt can be found that there is only one parameter s in EPKR. To fix s, we can use the socalled \u201ccross-validation\u201d method [14, Chapter 8] or \u201chold out\u201d method [14, Chapter 7]. To be precise, we explain the latter one. There are three steps to implement the \u201chold out \u201d strategy: (i) Splitting the sample set into two independent subsets z1 and z2, (ii) using z1 to build the sequence {fz,s}\u2308m 1/d\u2309 s=1 and (iii) using z2 to select a proper value of s and thus yield the final estimator fz. Noting that the choice of s is from 1 to \u2308m1/d\u2309, if d is large, then then \u2308m1/d\u2309 is always a small value. The following Theorem 2 illustrates the generalization capability of EPKR.\nMarch 10, 2015 DRAFT\n13\nTheorem 2: Let r \u2208 N, f\u03c1 \u2208 W r\u221e, and fz be the EPKR estimator. Then,\nC1m \u2212 2r d+2r \u2264 em(W r\u221e) \u2264 sup \u03c1\u2208M(W r \u221e ) E\u03c1m(\u2016f\u03c1 \u2212\u03a0Mfz\u20162\u03c1) \u2264 C2m\u2212 2r d+2r logm.\nTheorem 2 shows that the selected model (10) is almost optimal choice if the smoothness information of the regression function is known. Furthermore, the pseudo-inverse technique is sufficient to realize the almost optimality of the model (10). Furthermore, it can be easily deduced that the computational complexity of EPKR is very small compared to the classical polynomial kernel regression method (3). Indeed, for fixed s and n = (\ns+d d\n)\n, the computational complexity\nis mn2, while that of (3) is m3."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section, we give both toy and UCI standard data simulations of the model section strategy for polynomial kernel regression and the EPKR algorithm. All the numerical simulations are carried out in Matlab R2011b environment running Windows 7, Intel(R) Core(TM) i7-3770K CPU@ 3.50GHz 3.50 GHz."}, {"heading": "A. Toy simulation", "text": "1) Experimental setting: In this part, we introduce the simulation setting of the toy experiment. Method choices: In the toy simulation, there are four methods being employed. The first one is Gaussian kernel regression; the second one is the classical polynomial kernel regression; the third one is the efficient polynomial kernel regression (EPKR) whose centers {\u03b7j}nj=1 are drawn independently and identically to the uniform distribution; the last one is the EPKR whose centers {\u03b7j}nj=1 be the first n points in the sample data x. Samples: In the simulation, the training samples are generated as follows. Let f(t) = (1 \u2212 2t)5+(32t 2 + 10t + 1), where t \u2208 [0, 1] and a+ = max{a, 0}. Then it is easy to see that f \u2208 W 4\u221e([0, 1]) and f /\u2208 W 5\u221e([0, 1]). Let x = {xi}mi=1 be drawn independently and identically according to the uniform distribution with m = 1000 and y = {yi}mi=1 = f(xi) + \u03b4i, where the noise {\u03b4i}mi=1 are drawn independently and identically according to the Gaussian distribution N(0, \u03c32) with \u03c32 = 0.1. The test samples are generated as follows. x\u2032 = {x\u2032i}m \u2032 i=1 are drawn independently and identically according to the uniform distribution with m\u2032 = 1000 and yi = f(xi). In the numerical experiment, TestRMSE is the mean square root error (RMSE)\nMarch 10, 2015 DRAFT\n14\nof the testing data via 10 times simulation. TrainRMSE is the mean RMSE of the training data via 10 times simulation. TrainMT denotes the mean training time via 10 times simulation. And TestMT denotes the mean testing time via 10 times simulation.\n2) Simulation results: In the first simulation, we study the action of the regularization term in the classical polynomial kernel regression (3). In the left figure of Fig.1, it can be found that there exists an optimal \u03bb minimizing the TestRMSE for optimal selected s. This only means that introducing the penalty in (3) can avoid overfitting. Recalling Theorem 1, the action of regularization term in (3) is to avoid the ill-condition of the kernel matrix. Thus, more simulations are required. To this end, we introduce the coefficient-based regularization EPKR (CBR EPKR). The CBR EPKR is the algorithm which using\nc = pinv(Am,n + \u03bbIm,n)y\ninstead of Step 2 in the EPKR algorithm, where Im,n = (ai,j) m,n i,j=1 is the matrix with ai,i = 1, and ai,j = 0, i 6= j. For \u03bb = 0, the coefficient-based regularization EPKR algorithm coincides with EPKR. If the overfitting phenomenon is caused by the model-based factor, i.e., the hypothesis space of (3) or (10) is too large, then there may exist an optimal \u03bb > 0 in the middle figure of Fig. 1, which has not witnessed in Fig. 1. Indeed, the TestRMSE is a monotonously increasing function with respect to the regularization parameter \u03bb. This means that the capacity of hypothesis space of (3) is not large and suitable for the learning task. Thus, we can draw a conclusion form Fig.1 that the essential effect of the penalty in (10) is to overcome the ill-condition of the kernel matrix.\nReaders can find an interesting phenomenon in Fig.1. There is a \u03bb1 in the middle and right figures of Fig.1 such that for 0 \u2264 \u03bb \u2264 \u03bb1, the TestRMSE is a linear function with respect to \u03bb, while for \u03bb > \u03bb1, the slope decreases. We give a simple explanation of this phenomenon. The generalization error can be decomposed into approximation error and sample error. It is obvious that the approximation error is a linear function with respect to the regularization parameter \u03bb. However, the relation between the sample error and \u03bb is more sophisticated. It is easy to see that the hypothesis space belongs to the set {\nP \u2208 Hn,\u03b7 : n \u2211\ni=1\n|ai|2 \u2264 M2/\u03bb, P = n \u2211\ni=1\naiKs(\u03b7i, x)\n}\n.\nRoughly speaking, if \u03bb < \u03bb1, the covering number of the hypothesis space is larger than the quantity appearing in Lemma 5 for a fixed \u03b5. When \u03bb increase to \u03bb1, the covering number of\nMarch 10, 2015 DRAFT\n15\nthe hypothesis space decreases. Once the covering number of the hypothesis space is strictly smaller than the mentioned quantity, the sample error decreases with respect to \u03bb. Thus, the plus of approximation and sample errors is not a linear function with respect to \u03bb and the slope decreases according to \u03bb.\nIn the next simulation, we study the importance of s in both model (3) and model (10). Based on the results in Fig. 1, in the upper left figure of Fig. 2, we study the relation between TestRMSE and s for model (3), where \u03bb is the optimal value of 50 candidates drawn equally spaced in [10\u22125, 1]. It can be found that there exists an optimal s minimizing TestRMSE. Since f \u2208 W 2\u221e([0, 1]), it follows from Theorem 1 that the optimal s may close to the value \u2308m1/(2r+d)\u2309 = 4. It is shown in the upper right figure that the optimal s of (3) is 7 in our setting. The lower figures depict the relation between TestRMSE and s for EPKR. It can found in both of the lower figures of Fig. 2 that there is an optimal s minimizing TestRMSE and the optimal value of s is 5, which also coincides with the theoretical analysis in Theorem 2.\nIn the third simulation, we study the action of the choice of \u03b7 in EPKR. We compare the following four methods of choosing \u03b7 in (10). EPKR denotes that \u03b7 = {\u03b7i}ni=1 are drawn i.i.d according to the uniform distribution. EPKR1 denotes that {\u03b7i}ni=1 are selected as the first n elements of samples. EPKRF denotes that {\u03b7i}ni=1 are chosen as the n equally spaced points in [0, 1]. EPKRG denotes that {\u03b7i}ni=1 are generated i.i.d. according to the Gaussian distribution N (1/2, 1). It can be found in Fig. 3 that for for suitable s, the choice of \u03b7 doesn\u2019t effect the\nMarch 10, 2015 DRAFT\n16\nlearning capability. This verifies the theoretical result of Proposition 1.\nIn the last simulation, we study the learning capabilities of four methods: classical polynomial kernel regression (3), Gaussian kernel regression [15, eqs.(4)], EPKR and EPKR1. It can be seen from Fig. 4 that the learned functions of all the mentioned methods are almost the same. Since both the Gaussian kernel and polynomial kernel are infinitely smooth function and the regression function is at most 2-th smoothness, all of them cannot approximate the regression function within a very small tolerance. This coincides with the lower bound of Theorem 1 and Theorem 2.\nTable 1 shows a quantative comparison among the aforementioned methods. It can be found that all of them possess the similar TestRMSE. However, since there are two parameters in Gaussian kernel regression and classical polynomial kernel regression, large amount of computations are required to select a suitable model, i.e., to tune \u03bbG, \u03b4 in Gaussian kernel regression\nMarch 10, 2015 DRAFT\n17\nand \u03bbP , s in classical polynomial kernel regression. In this simulation, we use the three-fold cross-validation [14, Chapter 8] to choose these parameters. We choose 50 candidates of \u03bbG and \u03bbP as {10\u22125, 10\u22125 + 10\u22122, . . . , 10\u22125 + 49\u00d7 10\u22122}, 50 candidates of s as {1, 2, . . . , 50}, 40 candidates of \u03b4 as {0.01, 0.01 + 0.025, . . . , 0.01 + 39\u00d7 0.025}. There are only one parameter s of EPKR and EPKR1. We also use the three-fold cross-validation method to choose the optimal s from {1, . . . , 50}. It can be found in Table 1 that the training time of EPKR and EPKR1 are much less than that of the other two methods. The main reason of this phenomenon is based on the following assertions. On one hand, there is only one parameter need tuning in EPKR. On the other hand, the computational complexity of EPKR is O(mn2), which is smaller than O(m3) for small s. Noting that the deduced EPKR (or EPKR1) estimator is a linear combination of n = 5 basis function, while those of Gaussian kernel regression and polynomial kernel regression are 1000, the test time of EPKR and EPKR1 is much less than that of the other two methods.\nMarch 10, 2015 DRAFT\n18"}, {"heading": "B. UCI data", "text": "1) Experimental setting: In this part, we introduce the simulation setting of the UCI data ex-\nperiment. All the data are cited from http://www.niaad.liacc.up.pt/\u223cltorgo/Regression/ds menu.html. Method choices: In the UCI data experiment, we compare four methods containing support vector machine (SVM) [35], Gaussian kernel regression (GKR) [15, Eqs.(4)], classical polynomial kernel regression (3) (PKR) and EPKR on 9 real-world benchmark data sets covering various\nMarch 10, 2015 DRAFT\n19\nfields. We use three-fold cross-validation to select parameters of the aforementioned methods among 40 candidates of the width of Gaussian kernel and 50 candidates of the regularization parameter \u03bb. However, due to the theoretical analysis proposed in Theorem 1, there are only {1, . . . , \u2308m1/d\u2309} candidates of polynomial kernel parameter s. The centers of EPKR are drawn i.i.d according to the uniform distribution on [0, 1].\nSamples: The training and testing samples are drawn according to the following Table 2.\n2) Experimental results: As shown in Table 3, the TrainRMSE and TestRMSE of all the mentioned methods are similar. But as far as the TrainMT is concerned, it can be found in\nTable 4 that EPKR outperforms the others. It can also be found in Table 4 that the TrainMT of PKR is smaller than GKR and SVM. This is because we use the theoretical result in Theorem 1 to select the kernel parameter s. It is shown in Theorem 1 that it suffices to select s in the set {1, . . . , \u2308m1/d\u2309}. This degrades the difficulty of model selection of PKR. Since the TestMT depends heavily on the sparsity of the estimator, we give a comparison of the sparsity of the mentioned methods in Table 4, too. In short, as far as the generalization capability is concerned, all of these methods are of high quality. However, as far as the computational burden is concerned, EPKR is superior to the others. Furthermore, different from the classical polynomial kernel regression, EPKR can deduce sparse estimators. In addition, by our theoretical analysis, the computational burden of PKR can be heavily reduced.\nMarch 10, 2015 DRAFT\n20"}, {"heading": "VI. PROOFS", "text": "tration inequality can be found in [3, Lemma 3.2].\nLemma 4: Let F be a class of functions that are all bounded by M . For all m and \u03b1, \u03b2 > 0, we have\nP\u03c1m { \u2203f \u2208 F : \u2016f \u2212 f\u03c1\u20162\u03c1 \u2265 2(\u2016y \u2212 f\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m) + \u03b1 + \u03b2 }\nMarch 10, 2015 DRAFT\n21\n\u2264 14 sup x\nN ( \u03b2\n40M ,F , L1(\u03bdx)\n)\nexp ( \u2212 \u03b1n 2568M4 ) ,\nwhere x = (x1, . . . , xm) \u2208 Xm and N (t,F , L1(\u03bdx)) is the covering number for the class F by balls of radius t in L1(\u03bdx), with \u03bdx = 1m \u2211m i=1 \u03b4xi the empirical discrete measure.\nThe second one focusing on covering number estimation is deduced from [14, Chapter 9]. Lemma 5: Let \u03a0MHs := {\u03a0Mf : f \u2208 Hs}. Then,\nN (\u01eb, \u03c0MHs, L1(\u03bdx)) \u2264 3 ( 2eBp\n\u01ebp log\n3eBp\n\u01ebp\n)(s+dd ) .\nThe third one presents the minimal eigenvalue estimator of a matrix generated by the poly-\nnomial kernel, which can be found in [20, Theorem 20].\nLemma 6: Let s \u2208 N, n = (\ns+d d\n)\n, Sd\u22121 be the unit sphere in Rd, and {\u03bei}ni=1 \u2282 Sd\u22121. Then the minimal eigenvalue of the matrix A\u03be := (1 + \u03bei\u03bej)ni,j=1, \u00b5min(A\u03be), satisfies\n\u00b5min(A\u03be) \u2265 s!\u0393(d/2)\n2s\u0393(s+ d/2) .\nTo provide the last lemma, we should introduce the best approximation operator. A function\n\u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and\nsupp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].\nSuch a function can be easily constructed out of an orthogonal wavelet mask [10]. Let\nhk := \u03c01/2\u0393(d+ k)\u0393((d+ 1)/2)\n(k + d/2)k!\u0393(d/2) \u0393(d).\nDefine\nUk := (hk) \u22121/2G d/2 k , k = 0, 1, . . . , (11)\nwhere G\u00b5k is the well known Gegenbauer polynomial with order \u00b5 [26]. The best approximation kernel is defined by\nLs(x, y) := \u221e \u2211\nk=0\n\u03b7\n(\nk\n2s\n)\nv2k\n\u222b\nSd\u22121 Uk(x \u00b7 \u03be)Uk(y \u00b7 \u03be)d\u03c9d\u22121(\u03be),\nwhere d\u03c9d\u22121 stands for the aero element of Sd\u22121. It can easily deduced from [26] that\n|Ls(x, y)| \u2264 Csd, x, y \u2208 Bd. (12)\nLet\nEs(f)p := inf P\u2208Pds\n\u2016f \u2212 P\u2016Lp(Bd)\nMarch 10, 2015 DRAFT\n22\nbe the best approximation error of Pds . Define\nLsf(x) := \u222b\nBd Ls(x, y)f(y)dy. (13)\nIt is obvious that Lsf \u2208 Pds . The following Lemma 7 which can be found [?, Section 3] shows the best approximation property of (Lsf)(x).\nLemma 7: Let 1 \u2264 p \u2264 \u221e, and Ls be defined in (13), then for arbitrary f \u2208 Lp(Bd), there exists a constant C depending only on d and p such that\n\u2016f \u2212 Lsf\u2016Lp(Bd) \u2264 CE[s/2](f)p.\nNow we give the proof of Theorem 1.\nProof of Theorem 1: We write\n\u2016\u03a0Mfz,\u03bb,s \u2212 f\u03c1\u20162\u03c1 := T1(z, \u03bb, s) + T2(z, \u03bb, s),\nwhere\nT1(z, \u03bb, s) := \u2016\u03a0Mfz,\u03bb,s \u2212 f\u03c1\u20162\u03c1 \u2212 2(\u2016y \u2212\u03a0Mfz,\u03bb,s\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m)\nand\nT2(z, \u03bb, s) := 2(\u2016y \u2212 \u03a0Mfz,\u03bb,s\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m).\nTo bound T1(z, \u03bb, s), we use Lemma 4 and Lemma 5 and obtain\nP\u03c1m{T1(z, \u03bb, s) > u}\n= P\u03c1m { \u2016\u03a0Mfz,\u03bb,s \u2212 f\u03c1\u20162\u03c1 \u2265 2(\u2016y \u2212\u03a0Mfz,\u03bb,s\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m) + u\n2 +\nu 2\n}\n\u2264 P\u03c1m { \u2203f \u2208 \u03a0MHs : \u2016f \u2212 f\u03c1\u20162\u03c1 \u2265 2(\u2016y \u2212 f\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m) + u\n2 +\nu 2\n}\n\u2264 14 sup x\nN ( u\n80M ,\u03a0MHs, L1(\u03bdx)\n) exp ( \u2212 um 5136M4 )\n\u2264 42 ( 160eM2\nu log\n240eM2\nu\n)(s+dd ) exp (\n\u2212 um 5136M4\n)\nFor arbitrary u > 160eL4/m, we then obtain\nP\u03c1m{T1(z, \u03bb, s) > u} \u2264 42 ( mM2 )2(s+dd ) exp ( \u2212 um 5136M4 ) .\nMarch 10, 2015 DRAFT\n23\nThen, we get, for any v > 160eL4/m,\nE\u03c1m{T1(z, \u03bb, s)} \u2264 v + \u222b \u221e\nv P\u03c1m{T1(z, \u03bb, s) > t}dt\n\u2264 v + \u222b \u221e\nv 42\n( mM2 )2(s+dd ) exp ( \u2212 tm 5136M4 ) dt\n\u2264 v + 42 ( mM2 )2(s+dd ) 5136M\n4\nm exp\n(\n\u2212 vm 5136M4\n)\n.\nSetting\nv = 5136M4\nm log\n(\n42 ( mM2 )2(s+dd )\n)\n,\nwe have\nE\u03c1m{T1(z, \u03bb, s)} \u2264 C sd logm\nm . (14)\nNow we turn to bound T2(z, \u03bb, s). It follows from the definition of the truncation operator\n\u03a0M and fz,\u03bb,s that\nT2(z, \u03bb, s) = 2(\u2016y \u2212\u03a0Mfz,\u03bb,s\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m)\n\u2264 2(\u2016y \u2212 fz,\u03bb,s\u20162m \u2212 \u2016y \u2212 f\u03c1\u20162m) \u2264 2(\u2016y \u2212 fz,\u03bb,s\u20162m + \u03bb\u2016fz,\u03bb,s\u20162 \u2212 \u2016y \u2212 f\u03c1\u20162m) \u2264 2(\u2016y \u2212Lsf\u03c1\u20162m + \u03bb\u2016Lsf\u03c1\u20162 \u2212 \u2016y \u2212 f\u03c1\u20162m).\nTherefore, the definition of f\u03c1 yields that\nE\u03c1m{T2(z, \u03bb, s)} \u2264 2(E(Lsf\u03c1)\u2212 E(f\u03c1) + \u03bb\u2016Lsf\u03c1\u20162).\nThen, (1) yields that\nE\u03c1m{T2(z, \u03bb, s)} \u2264 2\u2016Lsf\u03c1 \u2212 f\u03c1\u20162\u03c1 + 2\u03bb\u2016Lsf\u03c1\u20162\u221e.\nSince f\u03c1 \u2208 L\u221e(Bd), Lemma 7 implies\nE\u03c1m{T2(z, \u03bb, s)} \u2264 C(E[s/2](f\u03c1)\u221e)2 + 2\u03bb\u2016Lsf\u03c1\u20162\u221e, (15)\nwhere C is a constant depending only on d. The only thing remainder is to bound \u2016Lsf\u03c1\u20162\u221e. To this end, let x0 \u2208 Bd satisfying\n\u2016Ls(x0, \u00b7)\u20162\u221e := sup x\u2208Bd \u2016Ls(x, \u00b7)\u20162\u221e.\nMarch 10, 2015 DRAFT\n24\nThen it follows from \u2016f\u03c1\u2016\u221e \u2264 M almost surely that\n\u2016Lsf\u03c1\u20162\u221e = \u2225 \u2225 \u2225 \u2225 \u222b\nBd Ls(x, \u00b7)f\u03c1(x)dx\n\u2225 \u2225 \u2225 \u2225 2\ns \u2264 CM2\u2016Ls(x0, \u00b7)\u20162s.\nAs Ls(x0, \u00b7) \u2208 Pds , for arbitrary {\u03bei}ni=1 \u2282 Sd\u22121 with n = ( s+d s ) , there holds\nLs(x0, x) = n \u2211\ni=1\nci(1 + \u03bei \u00b7 x)s.\nAnd c = (c1, . . . , cn)T satisfies\nc = A\u22121\u03be L,\nwhere A\u03be := ((1 + \u03bei\u03bej)s)ni,j=1 and L := (Ls(x0, \u03be1), . . . , Ls(x0, \u03ben)). Furthermore, it follows from Lemma 6 that n \u2211\ni=1\n|ci|2 \u2264 2s\u0393(s+ d/2)\ns!\u0393(d/2)\nn \u2211\ni=1\n|Ls(x0, \u03bei)|2.\nThen (12) together with simple computation implies n \u2211\ni=1\n|ci|2 \u2264 C2ss3d.\nTherefore,\n\u2016Lsf\u03c1\u2016s \u2264 CM2\u2016Ls(x0, \u00b7)\u2016s = CM2 \u2225 \u2225 \u2225 \u2225\n\u2225\nn \u2211\ni=1\nci(1 + \u03bei\u00b7)s \u2225 \u2225 \u2225 \u2225\n\u2225\n2\ns\n\u2264 CM2 n \u2211\ni=1\n|ci|\u2016(1 + \u03bei\u00b7)s\u2016s\n\u2264 C12s/2 \u221a n ( n \u2211\ni=1\n|ci|2 )1/2 \u2264 C22ss2d \u2264 C3(4d)s.\nThe above inequalities together with (15) and (14) yield\nE\u03c1m{\u2016\u03a0Mfz,\u03bb,s \u2212 f\u03c1\u20162\u03c1} \u2264 C ( sd logm\nm + (E[s/2](f\u03c1)\u221e)\n2 + \u03bb(4d)2s ) .\nThe proof of (6) is finished.\nTo prove (7) noting that the middle inequality can be deduced directly from the definition of em(W r\u221e) and the left inequality is proved in [13, Chapter 3], it suffices to prove the right inequality. Since f\u03c1 \u2208 W r\u221e, the well known Jackson inequality [12] shows that\n(E[s/2](f\u03c1)\u221e) 2 \u2264 Cs\u22122r.\nThus, let s = \u2308m1/(2r+d)\u2309, (7) holds obviously for any 0 \u2264 \u03bb \u2264 m\u2212 2r2r+d (4d)\u2212 12r+d . This completes the proof of Theorem 1.\nMarch 10, 2015 DRAFT\n25\nProof of Proposition 1: Let \u03b6 := {\u03b6j}nj=1 \u2282 Bd. Suppose that there exists a non-trivial set {ai}nj=1 such that\nn \u2211\nj=1\naj(1 + \u03b6j \u00b7 x)s = 0, x \u2208 Bd.\nThen the system of equations n \u2211\nj=1\naj(1 + \u03b6j \u00b7 \u03b6k)s = 0, k = 1, . . . , n\nis solvable. Noting that\n(1 + \u03b6j \u00b7 \u03b6i)s = s \u2211\nk=0\n(sk) (\u03b6j \u00b7 \u03b6i)k = s \u2211\nk=0\n(sk) \u2211\n|\u03b1|=k\nCk\u03b1\u03b6 \u03b1 j \u03b6 \u03b1 i ,\nwe obtain n \u2211\ni,j=1\naiaj(1 + \u03b6i \u00b7 \u03b6j)s = s \u2211\nk=0\n(sk) \u2211\n|\u03b1|=k\nCk\u03b1\n(\nn \u2211\ni=1\nai\u03b6 \u03b1 i\n)2\n,\nwhere\nCk\u03b1 = d!\n\u03b11! \u00b7 \u00b7 \u00b7\u03b1d! , \u03b1 := (\u03b11, . . . , \u03b1d).\nLet\nP (x) := n \u2211\ni=1\naix \u03b1.\nIf\n{\u03b6 = (\u03b6i)ni=1 : dimH\u03b6,n < n} ,\nthen \u03b6i, i = 1, . . . , n are n distinct zero points of P . Noting that the degree of P is at most s, then it can be easily deduced from [4, Lemma 3.1] that the zero set of P ,\nZ(p) := {x \u2208 Bd : P (x) = 0}\nhas Lebesgue measure 0. This completes the proof of Proposition 1.\nTo prove Proposition 2, we need the following two lemmas. The first one establishes a relation between the d-dimension unit ball Bd and the d + 1 dimension unit sphere Sd, which can be found in [38, Lemma 2.1].\nLemma 8: For any continuous function f defined on Sd, there holds \u222b\nSd f(\u03be)d\u03c9d(\u03be) =\n\u222b\nBd\n[\nf(x, \u221a 1\u2212 |x|2) + f(x,\u2212 \u221a 1\u2212 |x|2) ] dx \u221a\n1\u2212 |x|2 .\nMarch 10, 2015 DRAFT\n26\nLet h\u039b be the mesh norm of a set of points \u039b = {\u03bei}mi=1 \u2282 Sd defined by\nh\u039b := max \u03be\u2208Sd min j d(\u03be, \u03bej),\nwhere d(\u03be, \u03be\u2032) is the geodesic (great circle) distance between the points \u03be and \u03be\u2032 on Sd. The second one is the well known cubature formula on the sphere, which can be found in [21].\nLemma 9: If there exists a constant c such that h\u039b \u2264 n\u2212c/d, then there exists a set of numbers {ai}mi=1 satisfying\nm \u2211\ni=1\n|ai|p \u2264 Cm1\u2212p.\nsuch that \u222b\nSd P (y)d\u03c9(y) =\nm \u2211\ni=1\naiP (xi) for any P \u2208 \u03a0d2n.\nProof of Proposition 2: Based on Lemma 8 and Lemma 9, it suffices to prove for arbitrary\n\u03b5 > 0, with confidence at least 1\u2212 c m\u03b5d , there holds h\u039b \u2264 \u03b5. At first, we present an upper bound of h\u039b. Let D(\u03be, r) be the spherical cap with center \u03be and radius r. Then for arbitrary \u03b5 > 0, due to the definition of the mesh norm, we obtain\nP{h\u039b > \u03b5} = P{max \u03be\u2208Sd min j d(\u03be, \u03bej) > \u03b5} \u2264 E{(1\u2212 \u00b5(D(\u03be, \u03b5)))m}.\nLet t1, . . . , tN be the quasi-uniform points [34] on the sphere. Then it is easy to deduce that there exists a constant c > 0 such that\nN \u2264 c \u03b5d , and Sd \u2282\nN \u22c3\nj=1\nD(tj , \u03b5/2).\nIf \u03be \u2208 D(tj , \u03b5/2), then D(tj, \u03b5/2) \u2282 D(\u03be, \u03b5). Therefore, we get\nE{(1\u2212 \u00b5(D(\u03be, \u03b5)))m} \u2264 N \u2211\nj=1\n\u222b\nD(tj ,\u03b5/2) (1\u2212 \u00b5(D(\u03be, \u03b5)))md\u00b5\n\u2264 N \u2211\nj=1\n\u222b\nD(tj ,\u03b5/2) (1\u2212 \u00b5(D(tj, \u03b5/2)))md\u00b5 =\nN \u2211\nj=1\n\u00b5(D(tj, \u03b5/2))(1\u2212 \u00b5(D(tj, \u03b5/2)))m\n\u2264 N \u2211\nj=1\nmax u\nu(1\u2212 u)m \u2264 N \u2211\nj=1\nmax u\nue\u2212mu = eN\nm\n\u2264 c m\u03b5d .\nThat is,\nP{h\u039b > \u03b5} \u2264 c\nm\u03b5d .\nMarch 10, 2015 DRAFT\n27\nThis finishes the proof of Proposition 2\nProof of Theorem 2: The proof of Theorem 2 is almost the same as that of Theorem 1.\nNoting s \u2208 [ 1, \u2308m1/d\u2309 ] and \u2308 m1/(2r+d) \u2309 \u2208 [ 1, \u2308m1/d\u2309 ]\nfor arbitrary r \u2265 0, Theorem 2 can be easily deduced from Theorem 1. For the sake of brevity, we omit the details."}, {"heading": "VII. CONCLUSION", "text": "The main contributions of the present paper can be summarized as follows. Firstly, we study the parameter selection problem in polynomial kernel regression. After our analysis, we find that the essential role of the regularization term is to overcome the ill-condition phenomenon of the kernel matrix. Indeed, as far as the model selection is concerned, arbitrarily small regularization parameter can yield the almost optimal learning rate. Secondly, we improve the existing results about polynomial kernel regression in the following directions: building a distribution-free theoretical analysis, extending the range of regression function and establishing the almost optimal learning rate. Thirdly, based on the aforementioned theoretical analysis, we propose a new model concerning polynomial kernel regression and design an efficient learning algorithm. Both theoretical and experimental results show that the new method is of high quality."}, {"heading": "ACKNOWLEDGEMENT", "text": "The research was supported by the National 973 Programming (2013CB329404) and the\nNational Natural Science Foundation of China (Grant No. 11401462)."}], "references": [{"title": "A survey of the state of the art in learning the kernels", "author": ["M. Abbasnejad", "D. Ramachandram", "R. Mandava"], "venue": "Knowl. Inf. Syst., 31:193-221", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Soc., 68: 337-404", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1950}, {"title": "Approximation and learning by greedy algorithms", "author": ["A.R. Barron", "A. Cohen", "W. Dahmen", "R.A. Devore"], "venue": "Ann. Statist., 36: 64-94", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sampling of multivariate trigonometric polynomials", "author": ["R. Bass", "K. Gr\u00f6chenig"], "venue": "SIAM. J. Math. Anal., 36: 773- 795", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "No. AI-MEMO-2005-013. Massachusetts inst of tech Cambridge computer science and artificial intelligence lab", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Optimal rates for the regularized least squares algorithm", "author": ["A. Caponnetto", "E. DeVito"], "venue": "Found. Comput. Math., 7: 331-368", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc., 39: 1-49", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Best choices for regularization parameters in learning theory: on the bias-variance problem", "author": ["F. Cucker", "S. Smale"], "venue": "Found. Comput. Math., 2: 413-428", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["F. Cucker", "D.X. Zhou"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Ten Lectures on Wavelets", "author": ["I. Daubechies"], "venue": "SIAM, Philadelphia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Model selection for regularized least-squares algorithm in learning theory", "author": ["E. De Vito", "A. Caponnetto", "L. Rosasco"], "venue": "Found. Comput. Math., 5: 59-85", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Constructive Approximation", "author": ["R.A. DeVore", "G.G. Lorentz"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Approximation methods for supervised learning", "author": ["R.A. Devore", "G. Kerkyacharian", "D. Picard", "V. Temlyakov"], "venue": "Found. Comput. Math., 6: 3-58", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["L. Gy\u00f6rfy", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer, Berlin", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal learning rates for least squares SVMs using Gaussian kernels", "author": ["M. Eberts", "I. Steinwart"], "venue": "Advances in Neural Information Processing Systems 24 : 1539-1547", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple Kernel Learning Algorithms", "author": ["M. G\u00f6nen", "E. Alpaydm"], "venue": "J. Mach. Learn. Res., 12: 2211-2268", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer, New York", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning the Kernel Function via Regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "J. Mach. Learn. Res., 6: 1099-1125", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal Kernels", "author": ["C.A. Micchelli", "Y. Xu", "H. Zhang"], "venue": "J. Mach. Learn. Res., 7: 2651-2667", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Reproducing kernel Hilbert spaces in learning theory", "author": ["H.Q. Minh"], "venue": "Ph. D. Thesis in Mathematics, Brown University", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Spherical Marcinkiewicz-Zygmund inequalities and positive quadrature", "author": ["H.N. Mhaskar", "F.J. Narcowich", "J.D. Ward"], "venue": "Math. Comput., 70: 1113-1130", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Localized tight frames on spheres", "author": ["F.J. Narcowich", "P. Petrushev", "J.D. Ward"], "venue": "Siam J. Math. Anal., 38: 574-594", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning using hyperkernels", "author": ["C.S. Ong", "A.J. Smola"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "A", "author": ["C.S. Ong"], "venue": "J. Smola and R. C.Williamson. Learning the kernel with hyperkernels. J. Mach. Learn. Res., 6: 1043-1071", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Localized polynomial frames on the ball", "author": ["P.P. Petrushev", "Y. Xu"], "venue": "Constr. Approx., 27: 121-148", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalized inverse of matrices and its application", "author": ["C.R. Rao", "S.K. Mitra"], "venue": "Wiley, New York", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1971}, {"title": "Learning with Kernel: Support Vector Machine", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). The MIT Press, Cambridge", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["S. Smale", "D.X. Zhou"], "venue": "Constr. Approx., 26: 153-172", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast rates for support vector machines using Gaussian kernels", "author": ["I. Steinwart", "C. Scovel"], "venue": "Ann. Statist., 35: 575-607", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal rates for regularized least squares regression", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "Proceedings of the 22nd Conference on Learning Theory", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Composite kernel learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Learning rates for regularized classifiers using multivariate polynomial kernels", "author": ["H.Z. Tong", "D.R. Chen", "Z.P. Li"], "venue": "J. Complex., 24: 619-631", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J.S. Taylor", "N. Cristianini"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning rates of least square regularized regression", "author": ["Q. Wu", "Y.M. Ying", "D.X. Zhou"], "venue": "Found. Comput. Math., 6: 171-192", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Orthogonal polynomials and cubature formulae on spheres and on balls", "author": ["Y. Xu"], "venue": "SIAM J. Math. Anal., 29: 779-793", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Minimax nonparametric classification", "author": ["Y. Yang"], "venue": "I. Rates of convergence. II. Model selection for adaptation. IEEE Trans. Inform. Theory, 45: 2271-2292", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1999}, {"title": "Estimation of learning rate of least square algorithm via Jackson operator", "author": ["Y.Q. Zhang", "F.L. Cao", "Z.B. Xu"], "venue": "Neurocomputing, 74: 516-521", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation with polynomial kernels and SVM classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., 25: 323-344", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.", "startOffset": 22, "endOffset": 25}, {"referenceID": 34, "context": "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.", "startOffset": 87, "endOffset": 90}, {"referenceID": 24, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 157, "endOffset": 160}, {"referenceID": 31, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 127, "endOffset": 131}, {"referenceID": 39, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 157, "endOffset": 161}, {"referenceID": 32, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 163, "endOffset": 167}, {"referenceID": 38, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 205, "endOffset": 208}, {"referenceID": 10, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 210, "endOffset": 214}, {"referenceID": 30, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 216, "endOffset": 220}, {"referenceID": 35, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 222, "endOffset": 226}, {"referenceID": 8, "context": "Different from other widely used kernels [9], the reproducing kernel Hilbert space Hs of the polynomial kernel Ks = (1 + x \u00b7 y) is a finite-dimensional vector space, and its dimension depends only on s.", "startOffset": 41, "endOffset": 44}, {"referenceID": 39, "context": "Using this fact, [41] found that the regularization parameter in polynomial kernel regression should decrease exponentially fast with the sample size for appropriately selected s.", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "The first purpose of this paper is to continue the study of [41].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.", "startOffset": 283, "endOffset": 286}, {"referenceID": 8, "context": "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.", "startOffset": 288, "endOffset": 291}, {"referenceID": 14, "context": "For example, since the Gaussian-RKHS is an infinite dimensional vector space, the introducing of regularization term in Gaussian kernel regression is to control both the condition number of the kernel matrix and capacity of the hypothesis space [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 6, "context": "By the well known representation theorem [7] in learning theory, the essential hypothesis space of polynomial kernel regression is the linear space H := span{(1+x1\u00b7x), \u00b7 \u00b7 \u00b7 , (1+ xm \u00b7x)s}.", "startOffset": 41, "endOffset": 44}, {"referenceID": 26, "context": "Then the pseudo-inverse technique [27] can conduct the estimator easily.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Z (f(x)\u2212 y)d\u03c1, which is minimized by the regression function [7], defined by f\u03c1(x) := \u222b", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 26, "endOffset": 29}, {"referenceID": 13, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 31, "endOffset": 35}, {"referenceID": 1, "context": "Then HK (see [2]) is the closure of the linear span of the set of functions {Kx = K(x, \u00b7) : x \u2208 X} with the inner product \u3008\u00b7, \u00b7\u3009K satisfying \u3008Kx, Ky\u3009K = K(x, y) and \u3008Kx, f\u3009K = f(x), \u2200x \u2208 X, f \u2208 HK .", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "The following Aronszajn Theorem (see [2]) describes an essential relationship between the RKHS and reproducing kernel.", "startOffset": 37, "endOffset": 40}, {"referenceID": 39, "context": "Then it is obvious that [41] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds E(\u03a0Mfz,\u03bb,s)\u2212 E(f\u03c1) \u2264 E(fz,\u03bb,s)\u2212 E(f\u03c1).", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "The first result, to the best of our knowledge, concerning selection of the optimal regularization parameter in the framework of learning theory belongs to [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "After checking the proof of [8] carefully, we find that there is nothing to surprise.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "On one hand, the optimal parameter mentioned in [8] aims to the generalization error, containing learning rate and the constant C2, while our result only concerns the learning rate.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "On the other hand, [8]\u2019s result is more suitable to describe the performance of K(\u00b7, \u00b7) satisfying \u2016f\u2016\u221e \u2264 C\u2016f\u2016K , where C is a constant independent of m.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "After [8], we have witnessed the multiple emergence of the selection strategies of regularization parameter.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 122, "endOffset": 125}, {"referenceID": 30, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 193, "endOffset": 197}, {"referenceID": 28, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 250, "endOffset": 254}, {"referenceID": 35, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 256, "endOffset": 260}, {"referenceID": 32, "context": "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Eberts and Steinwart [15] have already built a similar learning rate analysis for Gaussian kernel regression.", "startOffset": 21, "endOffset": 25}, {"referenceID": 37, "context": "\uf8f4 \uf8f3 1, if fz,\u03bb,s \u2265 12 , 0, if fz,\u03bb,s < 1 2 , (8) Theorem 1 and [39] imply that the classier defined as in (8) is also almost optimal if the well known Bayes decision function satisfies a certain smoothness assumption.", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "To this end, we should introduce the conceptions of Haar space and fundamental system [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "Since the uniform distribution is continuous with respect to Lebesgue measure [4], we can draw {\u03b7j}j=1 independently and identically according to the uniform distribution.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Let f(t) = (1 \u2212 2t)+(32t 2 + 10t + 1), where t \u2208 [0, 1] and a+ = max{a, 0}.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Then it is easy to see that f \u2208 W 4 \u221e([0, 1]) and f / \u2208 W 5 \u221e([0, 1]).", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "Then it is easy to see that f \u2208 W 4 \u221e([0, 1]) and f / \u2208 W 5 \u221e([0, 1]).", "startOffset": 62, "endOffset": 68}, {"referenceID": 9, "context": "2, we study the relation between TestRMSE and s for model (3), where \u03bb is the optimal value of 50 candidates drawn equally spaced in [10, 1].", "startOffset": 133, "endOffset": 140}, {"referenceID": 0, "context": "2, we study the relation between TestRMSE and s for model (3), where \u03bb is the optimal value of 50 candidates drawn equally spaced in [10, 1].", "startOffset": 133, "endOffset": 140}, {"referenceID": 0, "context": "Since f \u2208 W 2 \u221e([0, 1]), it follows from Theorem 1 that the optimal s may close to the value \u2308m1/(2r+d)\u2309 = 4.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "EPKRF denotes that {\u03b7i}i=1 are chosen as the n equally spaced points in [0, 1].", "startOffset": 72, "endOffset": 78}, {"referenceID": 34, "context": "Method choices: In the UCI data experiment, we compare four methods containing support vector machine (SVM) [35], Gaussian kernel regression (GKR) [15, Eqs.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "d according to the uniform distribution on [0, 1].", "startOffset": 43, "endOffset": 49}, {"referenceID": 25, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 129, "endOffset": 135}, {"referenceID": 1, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 129, "endOffset": 135}, {"referenceID": 9, "context": "Such a function can be easily constructed out of an orthogonal wavelet mask [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": ", (11) where G\u03bck is the well known Gegenbauer polynomial with order \u03bc [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "It can easily deduced from [26] that |Ls(x, y)| \u2264 Cs, x, y \u2208 B.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "Since f\u03c1 \u2208 W r \u221e, the well known Jackson inequality [12] shows that (E[s/2](f\u03c1)\u221e) 2 \u2264 Cs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "The second one is the well known cubature formula on the sphere, which can be found in [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": ", tN be the quasi-uniform points [34] on the sphere.", "startOffset": 33, "endOffset": 37}], "year": 2015, "abstractText": "Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the \u201c ill-condition\u201d of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability. Index Terms Model selection, regression, polynomial kernel, learning rate. S. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China, Z. Xu and J. Zeng are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an 710049, P R China, X. Sun is with the Department of Mathematics, Missouri State University, Springfield, MO 65897, USA March 10, 2015 DRAFT", "creator": "LaTeX with hyperref package"}}}