{"id": "1503.02364", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Neural Responding Machine for Short-Text Conversation", "abstract": "we propose neural responding machine ( nrm ), a new neural network - based response generator for probing short - text conversation. nrm machine takes the general encoder - interaction decoder coordination framework : it formalizes the generation transform of response as a decoding process based on the latent representation of the input corpus text, while both encoding and decoding are realized with recurrent neural networks ( rnn ). the nrm is usually trained with a large statistical amount of one - round conversation data collected from a regular microblogging service. empirical study shows that nrm processors can comfortably generate grammatically correct and content - quality wise appropriate responses to over 75 % of of the input text, outperforming state - of - the - arts in perhaps the same setting, including retrieval - based and smt - based models.", "histories": [["v1", "Mon, 9 Mar 2015 02:54:29 GMT  (652kb)", "https://arxiv.org/abs/1503.02364v1", null], ["v2", "Mon, 27 Apr 2015 02:28:58 GMT  (652kb)", "http://arxiv.org/abs/1503.02364v2", "accepted as a full paper at ACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lifeng shang", "zhengdong lu", "hang li"], "accepted": true, "id": "1503.02364"}, "pdf": {"name": "1503.02364.pdf", "metadata": {"source": "CRF", "title": "Neural Responding Machine for Short-Text Conversation", "authors": ["Lifeng Shang", "Zhengdong Lu Hang Li"], "emails": ["Shang.Lifeng@huawei.com", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n02 36\n4v 2\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 5"}, {"heading": "1 Introduction", "text": "Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system.\nRecently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation.\nPrevious methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (Ritter et al., 2011). The basic idea of retrieval-based method is to pick a suitable response by ranking the candidate responses with a linear or non-linear combination of various matching features (e.g. number of shared words). The main drawbacks of the retrieval-based method are the following\n\u2022 the responses are pre-existed and hard to be customized for the particular text or requirement from the task, e.g., style or attitude.\n\u2022 the use of matching features alone is usually not sufficient for distinguishing positive responses from negative ones, even after time consuming feature engineering. (e.g., a penalty due to mismatched named entities is difficult to be incorporated into the model)\nThe SMT-based method, on the other hand, is generative. Basically it treats the response generation as a translation problem, in which the model is trained on a parallel corpus of post-response pairs. Despite\n1https://twitter.com/. 2http://www.weibo.com/.\nits generative nature, the method is intrinsically unsuitable for response generation, because the responses are not semantically equivalent to the posts as in translation. Actually one post can receive responses with completely different content, as manifested through the example in the following figure:\nPost Having my fish sandwich right now\nUserA For god\u2019s sake, it is 11 in the morning UserB Enhhhh... sounds yummy UserC which restaurant exactly?"}, {"heading": "1.1 Overview", "text": "In this paper, we take a probabilistic model to address the response generation problem, and propose employing a neural encoder-decoder for this task, named Neural Responding Machine (NRM). The neural encoder-decoder model, as illustrated in Figure 1, first summarizes the post as a vector representation, then feeds this representation to decoder to generate responses. We further generalize this scheme to allow the post representation dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment.\nNRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine translation, the task is to estimate the probability of a target language sentence conditioned on the source language sentence with the same meaning, which is much easier than the task of STC which we are considering here. In this paper, we demonstrate that NRM, when equipped with a reasonable amount of data, can yield a satisfying estimator of responses (hence response generator) for STC, despite the difficulty of the task.\nOur main contributions are two-folds: 1) we propose to use an encoder-decoder-based neural network to generate a response in STC; 2) we have empirically verified that the proposed method, when trained with a reasonable amount of data, can yield performance better than traditional retrieval-based and translation-based methods."}, {"heading": "1.2 RoadMap", "text": "In the remainder of this paper, we start with introducing the dataset for STC in Section 2. Then we elaborate on the model of NRM in Section 3, followed by the details on training in Section 4. After that, we report the experimental results in Section 5. In Section 6 we conclude the paper."}, {"heading": "2 The Dataset for STC", "text": "Our models are trained on a corpus of roughly 4.4 million pairs of conversations from Weibo 3.\n3The dataset and its English translation (by machine translation system) will be released soon."}, {"heading": "2.1 Conversations on Sina Weibo", "text": "Weibo is a popular Twitter-like microblogging service in China, on which a user can post short messages (referred to as post in the reminder of this paper) visible to the public or a group of users following her/him. Other users make comment on a published post, which will be referred to as response. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on both posts and responses, making the post-response pair an ideal surrogate for short-text conversation."}, {"heading": "2.2 Dataset Description", "text": "To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial responses like \u201cwow\u201d, 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency. Table 1 shows some statistics of the dataset used in this work. It can be seen that each post have 20 different responses on average. In addition to the semantic gap between post and its responses, this is another key difference to a general parallel data set used for traditional translation."}, {"heading": "3 Neural Responding Machines for STC", "text": "The basic idea of NRM is to build a hidden representation of a post, and then generate the response based on it, as shown in Figure 2. In the particular illustration, the encoder converts the input sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ) into a set of high-dimensional hidden representations h = (h1, \u00b7 \u00b7 \u00b7 , hT ), which, along with the attention signal at time t (denoted as \u03b1t), are fed to the context-generator to build the context input to decoder at time t (denoted as ct). Then ct is linearly transformed by a matrix L (as part of the decoder) into a stimulus of generating RNN to produce the t-th word of response (denoted as yt).\nIn neural translation system, L converts the representation in source language to that of target language. In NRM, L plays a more difficult role: it needs to transform the representation of post (or some part of it) to the rich representation of many plausible responses. It is a bit surprising that this can be achieved to a reasonable level with a linear transformation in the \u201cspace of representation\u201d, as validated in Section 5.3, where we show that one post can actually invoke many different responses from NRM.\nThe role of attention signal is to determine which part of the hidden representation h should be emphasized during the generation process. It should be noted that \u03b1t could be fixed over time or changes dynamically during the generation of response sequence y. In the dynamic settings, \u03b1t can be function of historically generated subsequence (y1, \u00b7 \u00b7 \u00b7 , yt\u22121), input sequence x or their latent representations, more details will be shown later in Section 3.2.\nWe use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014)."}, {"heading": "3.1 The Computation in Decoder", "text": "Figure 3 gives the graphical model of the decoder, which is essentially a standard RNN language model except conditioned on the context input c. The generation probability of the t-th word is calculated by\np(yt|yt\u22121, \u00b7 \u00b7 \u00b7 , y1,x) = g(yt\u22121, st, ct), (1)\nwhere yt is a one-hot word representation, g(\u00b7) is a softmax activation function, and st is the hidden state of decoder at time t calculated by\nst = f(yt\u22121, st\u22121, ct), (2)\nand f(\u00b7) is a non-linear activation function and the transformation L is often assigned as parameters of f(\u00b7). Here f(\u00b7) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014). Compared to \u201cungated\u201d logistic function, LSTM and GRU are specially designed for its long term memory: it can store information over extended time steps without too much decay. We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014), but has less parameters and easier to train."}, {"heading": "3.2 The Computation in Encoder", "text": "We consider three types of encoding schemes, namely 1) the global scheme, 2) the local scheme, and the hybrid scheme which combines 1) and 2).\nGlobal Scheme: Figure 4 shows the graphical model of the RNN-encoder and related context generator for a global encoding scheme. The hidden state at time t is calculated by ht = f(xt, ht\u22121) (i.e. still\nGRU unit), and with a trivial context generation operation, we essentially use the final hidden state hT as the global representation of the sentence. The same strategy has been taken in (Cho et al., 2014) and (Sutskever et al., 2014) for building the intermediate representation for machine translation. This scheme however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough4. In the reminder of this paper, a NRM with this global encoding scheme is referred to as NRM-glo.\nLocal Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = \u2211T j=1 \u03b1tjhj , where weighting factors \u03b1tj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states \u03b1tj = q(hj , st\u22121), as pictorially shown in Figure 5. Basically, the attention mechanism \u03b1tj models the alignment between the inputs around position j and the output at position t, so it can be viewed as a local matching model. This local scheme is devised in (Bahdanau et al., 2014) for automatic alignment between the source sentence and the partial target sentence in machine translation. This scheme enjoys the advantage of adaptively focusing on some important words of the input text according to the generated words of response. A NRM with this local encoding scheme is referred to as NRM-loc.\n4Sutskever et al. (2014) has to use 4, 000 dimension for satisfying performance on machine translation, while (Cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence."}, {"heading": "3.3 Extensions: Local and Global Model", "text": "In the task of STC, NRM-glo has the summarization of the entire post, while NRM-loc can adaptively select the important words in post for various suitable responses. Since post-response pairs in STC are not strictly parallel and a word in different context can have different meanings, we conjecture that the global representation in NRM-glo may provide useful context for extracting the local context, therefore complementary to the scheme in NRM-loc. It is therefore a natural extension to combine the two models by concatenating their encoded hidden states to form an extended hidden representation for each time stamp, as illustrated in Figure 6. We can see the summarization hgT is incorporated into ct and \u03b1tj to provide a global context for local matching. With this hybrid method, we hope both the local and global information can be introduced into the generation of response. The model with this context generation mechanism is denoted as NRM-hyb.\nIt should be noticed that the context generator in NRM-hyb will evoke different encoding mechanisms in the global encoder and the local encoder, although they will be combined later in forming a unified representation. More specifically, the last hidden state of NRM-glo plays a role different from that of the last state of NRM-loc, since it has the responsibility to encode the entire input sentence. This role of NRM-glo, however, tends to be not adequately emphasized in training the hybrid encoder when the parameters of the two encoding RNNs are learned jointly from scratch. For this we use the following trick: we first initialize NRM-hyb with the parameters of NRM-loc and NRM-glo trained separately, then fine tune the parameters in encoder along with training the parameters of decoder.\nTo learn the parameters of the model, we maximize the likelihood of observing the original response conditioned on the post in the training set. For a new post, NRMs generate their responses by using a left-to-right beam search with beam size = 10."}, {"heading": "4 Experiments", "text": "We evaluate three different settings of NRM described in Section 3, namely NRM-glo, NRM-loc, and NRM-hyb, and compare them to retrieval-based and SMT-based methods."}, {"heading": "4.1 Implementation Details", "text": "We use Stanford Chinese word segmenter 5 to split the posts and responses into sequences of words. Although both posts and responses are written in the same language, the distributions on words for the two are different: the number of unique words in post text is 125,237, and that of response text is 679,958. We therefore construct two separate vocabularies for posts and responses by using 40,000 most frequent words on each side, covering 97.8% usage of words for post and 96.2% for response respectively. All the\n5http://nlp.stanford.edu/software/segmenter.shtml\nremaining words are replaced by a special token \u201cUNK\u201d. The dimensions of the hidden states of encoder and decoder are both 1,000, and the dimensions of the word-embedding for post and response are both 620. Model parameters are initialized by randomly sampling from a uniform distribution between -0.1 and 0.1. All our models were trained on a NVIDIA Tesla K40 GPU using stochastic gradient descent algorithm with mini-batch. The training stage of each model took about two weeks."}, {"heading": "4.2 Competitor Models", "text": "Retrieval-based: with retrieval-based models, for any given post p\u2217, the response r\u2217 is retrieved from a big post-response pairs (p, r) repository. Such models rely on three key components: a big repository, sets of feature functions \u03a6i(p\u2217, (p, r)), and a machine learning model to combine these features. In this work, the whole 4.4 million Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to some deep matching models (Ji et al., 2014) are used to determine the suitability of a post to a given post p\u2217 through the following linear model\nscore(p\u2217, (p, r)) = \u2211\ni\n\u03c9i\u03a6i(p \u2217, (p, r)). (3)\nFollowing the ranking strategy in (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters \u03c9i based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process.\nSMT-based: In SMT-based models, the post-response pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation modelMoses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the \u201cResponse\u201d of Twitter \u201cStimulus\u201d. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the possible phrasepairs in the training data are considered and their associated probabilities are estimated by the Fisher\u2019s Exact Test, which yields performance slightly better than default setting8. Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model."}, {"heading": "5 Results and Analysis", "text": "Automatic evaluation of response generation is still an open problem. The widely accepted evaluation methods in translation (e.g. BLUE score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage. It is also not reasonable to evaluate with Perplexity, a generally used measurement in statistical language modeling, because the naturalness of response and the relatedness to post can not be well evaluated. We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with important difference."}, {"heading": "5.1 Evaluation Methods", "text": "We adopt human annotation to compare the performance of different models. Five labelers with at least three-year experience of Sina Weibo are invited to do human evaluation. Responses obtained from the five evaluated models are pooled and randomly permuted for each labeler. The labelers are instructed to imagine that they were the authors of the original posts and judge whether a response (generated or retrieved) is appropriate and natural to a input post. Three levels are assigned to a response with scores from 0 to 2:\n6we use the default similarity function of Lucene 7 8Reported results showed that the new model outperformed the baseline SMT model 57.7% of the time.\n\u2022 Suitable (+2): the response is evidently an appropriate and natural response to the post;\n\u2022 Neutral (+1): the response can be a suitable response in a specific scenario;\n\u2022 Unsuitable (0): it is hard or impossible to find a scenario where response is suitable. To make the annotation task operable, the suitability of generated responses is judged from the following five criteria: (a) Grammar and Fluency: Responses should be natural language and free of any fluency or grammat-\nical errors;\n(b) Logic Consistency: Responses should be logically consistent with the test post;\n(c) Semantic Relevance: Responses should be semantically relevant to the test post;\n(d) Scenario Dependence: Responses can depend on a specific scenario but should not contradict the first three criteria;\n(e) Generality: Responses can be general but should not contradict the first three criteria; If any of the first three criteria (a), (b), and (c) is contradicted, the generated response should be labeled as \u201cUnsuitable\u201d. The responses that are general or suitable to post in a specific scenario should be labeled as \u201cNeutral\u201d. Figure 7 shows an example of the labeling results of a post and its responses. The first two responses are labeled as \u201cUnsuitable\u201d because of the logic consistency and semantic relevance errors. Response4 depends on the scenario (i.e., the current score is 0:0), and is therefore annotated as \u201cNeutral\u201d."}, {"heading": "5.2 Results", "text": "Our test set consists of 110 posts that do not appear in the training set, with length between 6 to 22 Chinese words and 12.5 words on average. The experimental results based on human annotation are summarized in Table 2, consisting of the ratio of three categories and the agreement among the five labelers for each model. The agreement is evaluated by Fleiss\u2019 kappa (Fleiss, 1971), as a statistical measure of inter-rater consistency. Except the SMT-based model, the value of agreement is in a range from 0.2 to 0.4 for all the other models, which should be interpreted as \u201cFair agreement\u201d. The SMT-based model has a relatively higher kappa value 0.448, which is larger than 0.4 and considered as \u201cModerate agreement\u201d, since the responses generated by the SMT often have the fluency and grammatical errors, making it easy to reach an agreement on such unsuitable cases.\nFrom Table 2, we can see the SMT method performs significantly worse than the retrieval-based and NRM models and 74.4% of the generated responses were labeled as unsuitable mainly due to fluency and relevance errors. This observation confirms with our intuition that the STC dataset, with one post potentially corresponding to many responses, can not be simply taken as parallel corpus in a SMT model. Surprisingly, more than 60% of responses generated by all the three NRM are labeled as \u201cSuitable\u201d or \u201cNeutral\u201d, which means that most generated responses are fluent and semantically relevant to post. Among all the NRM variants\n\u2022 NRM-loc outperforms NRM-glo, suggesting that a dynamically generated context might be more effective than a \u201cstatic\u201d fixed-length vector for the entire post, which is consistent with the observation made in (Bahdanau et al., 2014) for machine translation;\n\u2022 NRM-hyp outperforms NRM-loc and NRM-glo, suggesting that a global representation of post is complementary to dynamically generated local context.\nThe retrieval-based model has the similar mean score as NRM-glo, and its ratio on neutral cases outperforms all the other methods. This is because 1) the responses retrieved by retrieval-based method are actually wrote by human, so they do not suffer from grammatical and fluency problems, and 2) the combination of various feature functions potentially makes sure the picked responses semantically relevant to test posts. However the picked responses are not customized for new test posts, so the ratio of suitable cases is lower than the three neural generation models.\nTo test statistical significance, we use the Friedman test (Howell, 2010), which is a non-parametric test on the differences of several related samples, based on ranking. Table 3 shows the average rankings over all annotations and the corresponding p-values for comparisons between different pairs of methods. The comparison between retrieval-based and NRM-glo is not significant and their difference in ranking is tiny. This indicates that the retrieval-based method is comparable to the NRM-glo method. The NRMhyb outperforms all the other methods, and the difference is statistically significant (p < 0.05). The difference between NRM-loc and retrieval-based method is marginal (p = 0.062). SMT is significantly worse than retrieval-based and NRM-hyb methods."}, {"heading": "5.3 Case Study", "text": "Figure 8 shows some example responses generated by our NRMs (only the one with biggest likelihood is given) and the comparable retrieval-based model. It is intriguing to notice that three NRM variants give suitable but quite distinct responses, with different perspectives and choices of words. This, as we conjecture, is caused by both the architecture variations among models as well as the variations from random effects like the initialization of parameters. Another interesting observation is on the forth example, where the retrieval-based method returns a response with the a mismatched entity name \u201cWenShan\u201d, which is actually a quite common problem for retrieval-based model, where the inconsistency details (e.g., dates, named entities), which often render the response unsuitable, cannot be adequately considered in the matching function employed in retrieving the responses. In contrast, we observe that NRMs tend to make general response and barely generate those details.\nWe also use the NRM-hyb as an example to investigate the ability of NRM to generate multiple responses. Figure 9 lists 5 responses to the same post, which are gotten with beam search with beam size\n= 500, among which we keep only the best one (biggest likelihood) for each first word. It can be seen that the responses are fluent, relevant to the post, and still vastly different from each other, validating our initial conjecture that NRM, when fueled with large and rich training corpus, could work as a generator that can cover a lot of modes in its density estimation."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we explored using encoder-decoder-based neural network system, with coined name Neural Responding Machine, to generate responses to a post. Empirical studies confirm that the newly proposed NRMs, especially the hybrid encoding scheme, can outperform state-of-the-art retrieval-based and SMTbased methods. Our preliminary study also shows that NRM can generate multiple responses with great variety to a given post. In future work, we would consider adding the intention (or sentiment) of users as an external signal of decoder to generate responses with specific goals."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Fleiss.,? \\Q1971\\E", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Generating sequences with recurrent neural networks. preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Fundamental Statistics for the Behavioral Sciences", "author": ["David C. Howell"], "venue": "PSY", "citeRegEx": "Howell.,? \\Q2010\\E", "shortCiteRegEx": "Howell.", "year": 2010}, {"title": "An information retrieval approach to short text conversation", "author": ["Ji et al.2014] Zongcheng Ji", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1408.6988", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In SIGKDD,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Njfun: a reinforcement learning spoken dialogue system", "author": ["Litman et al.2000] Diane Litman", "Satinder Singh", "Michael Kearns", "Marilyn Walker"], "venue": "In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems,", "citeRegEx": "Litman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Litman et al\\.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Reinforcement learning of question-answering dialogue policies for virtual museum guides", "author": ["Misu et al.2012] Teruhisa Misu", "Kallirroi Georgila", "Anton Leuski", "David Traum"], "venue": "In SIGDIAL,", "citeRegEx": "Misu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Misu et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A dataset for research on shorttext conversations", "author": ["Wang et al.2013] Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen"], "venue": "In EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D Williams", "Steve Young"], "venue": "Computer Speech & Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 18, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 14, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 12, "context": "Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000).", "startOffset": 93, "endOffset": 184}, {"referenceID": 8, "context": "This makes a data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible.", "startOffset": 69, "endOffset": 107}, {"referenceID": 17, "context": "This makes a data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible.", "startOffset": 69, "endOffset": 107}, {"referenceID": 8, "context": "Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (Ritter et al.", "startOffset": 81, "endOffset": 98}, {"referenceID": 17, "context": ", 2014), and 2) the statistical machine translation (SMT) based method (Ritter et al., 2011).", "startOffset": 71, "endOffset": 92}, {"referenceID": 1, "context": "We further generalize this scheme to allow the post representation dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment.", "startOffset": 139, "endOffset": 162}, {"referenceID": 0, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 19, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 1, "context": "Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 82, "endOffset": 180}, {"referenceID": 20, "context": "2 Dataset Description To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial responses like \u201cwow\u201d, 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency.", "startOffset": 184, "endOffset": 203}, {"referenceID": 13, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 19, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 2, "context": "We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 153, "endOffset": 217}, {"referenceID": 3, "context": "Here f(\u00b7) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014).", "startOffset": 179, "endOffset": 217}, {"referenceID": 2, "context": "Here f(\u00b7) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014).", "startOffset": 179, "endOffset": 217}, {"referenceID": 3, "context": "We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014), but has less parameters and easier to train.", "startOffset": 82, "endOffset": 102}, {"referenceID": 2, "context": "The same strategy has been taken in (Cho et al., 2014) and (Sutskever et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 19, "context": ", 2014) and (Sutskever et al., 2014) for building the intermediate representation for machine translation.", "startOffset": 12, "endOffset": 36}, {"referenceID": 1, "context": "This local scheme is devised in (Bahdanau et al., 2014) for automatic alignment between the source sentence and the partial target sentence in machine translation.", "startOffset": 32, "endOffset": 55}, {"referenceID": 1, "context": "Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = \u2211T j=1 \u03b1tjhj , where weighting factors \u03b1tj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states \u03b1tj = q(hj , st\u22121), as pictorially shown in Figure 5.", "startOffset": 24, "endOffset": 47}, {"referenceID": 1, "context": "Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = \u2211T j=1 \u03b1tjhj , where weighting factors \u03b1tj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states \u03b1tj = q(hj , st\u22121), as pictorially shown in Figure 5.", "startOffset": 24, "endOffset": 65}, {"referenceID": 2, "context": "(2014) has to use 4, 000 dimension for satisfying performance on machine translation, while (Cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence.", "startOffset": 92, "endOffset": 110}, {"referenceID": 8, "context": "4 million Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to some deep matching models (Ji et al., 2014) are used to determine the suitability of a post to a given post p\u2217 through the following linear model", "startOffset": 130, "endOffset": 147}, {"referenceID": 8, "context": "Following the ranking strategy in (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.", "startOffset": 34, "endOffset": 51}, {"referenceID": 9, "context": "We use ranking SVM model (Joachims, 2006) for the parameters \u03c9i based on the labeled dataset.", "startOffset": 25, "endOffset": 41}, {"referenceID": 11, "context": "We use the most widely used open-source phrase-based translation modelMoses (Koehn et al., 2007).", "startOffset": 76, "endOffset": 96}, {"referenceID": 17, "context": "In (Ritter et al., 2011), the authors used a modified SMT model to obtain the \u201cResponse\u201d of Twitter \u201cStimulus\u201d.", "startOffset": 3, "endOffset": 24}, {"referenceID": 16, "context": "BLUE score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage.", "startOffset": 11, "endOffset": 34}, {"referenceID": 17, "context": "We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with important difference.", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "The agreement is evaluated by Fleiss\u2019 kappa (Fleiss, 1971), as a statistical measure of inter-rater consistency.", "startOffset": 44, "endOffset": 58}, {"referenceID": 1, "context": "\u2022 NRM-loc outperforms NRM-glo, suggesting that a dynamically generated context might be more effective than a \u201cstatic\u201d fixed-length vector for the entire post, which is consistent with the observation made in (Bahdanau et al., 2014) for machine translation; \u2022 NRM-hyp outperforms NRM-loc and NRM-glo, suggesting that a global representation of post is complementary to dynamically generated local context.", "startOffset": 209, "endOffset": 232}, {"referenceID": 7, "context": "To test statistical significance, we use the Friedman test (Howell, 2010), which is a non-parametric test on the differences of several related samples, based on ranking.", "startOffset": 59, "endOffset": 73}], "year": 2015, "abstractText": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.", "creator": "LaTeX with hyperref package"}}}