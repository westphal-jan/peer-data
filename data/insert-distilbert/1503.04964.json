{"id": "1503.04964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "Energy Sharing for Multiple Sensor Nodes with Finite Buffers", "abstract": "we consider the problem of finding optimal energy sharing policies that maximize the network performance of a system comprising clusters of multiple sensor nodes and a single energy harvesting ( eh ) source. sensor nodes periodically sense the random field and generate data, which is stored in the randomly corresponding data queues. the eh source harnesses energy from ambient energy sources and the generated energy is stored in an energy buffer. sensor nodes receive energy for data transmission from the eh queue source. the eh source has to efficiently share the stored energy among the nodes in order to minimize the long - run average delay in data chain transmission. we formulate the problem of energy sharing between the nodes in the framework of average cost infinite - horizon coherent markov decision processes ( mdps ). we develop efficient energy sharing algorithms, namely the q - learning algorithm with exploration mechanisms based on the $ \\ epsilon $ - greedy method as well as upper confidence bound ( ucb ). we extend these algorithms by incorporating state and action space aggregation to tackle state - action infinite space explosion in the mdp. we also develop a cross entropy based method that incorporates policy parameterization in order to find a near optimal energy gain sharing policies. through simulations, we show that our algorithms yield energy knowledge sharing policies that outperform the heuristic greedy method.", "histories": [["v1", "Tue, 17 Mar 2015 09:32:29 GMT  (2063kb,D)", "http://arxiv.org/abs/1503.04964v1", "38 pages, 10 figures"]], "COMMENTS": "38 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["sindhu padakandla", "prabuchandran k j", "shalabh bhatnagar"], "accepted": false, "id": "1503.04964"}, "pdf": {"name": "1503.04964.pdf", "metadata": {"source": "CRF", "title": "Energy Sharing for Multiple Sensor Nodes with Finite Buffers", "authors": ["Sindhu Padakandla"], "emails": ["shalabh}@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "Keywords: Energy harvesting sensor nodes, energy sharing, Markov decision process, Q-learning, state aggregation."}, {"heading": "1 Introduction", "text": "A sensor network is a group of independent sensor nodes, each of which senses the environment. Sensor networks find applications in weather and soil conditions monitoring, object tracking and structure monitoring. Each sensor node in the network senses the environment\nar X\niv :1\n50 3.\n04 96\n4v 1\n[ cs\n.N I]\n1 7\nM ar\nand transmits the sensed data to a fusion node. The fusion node obtains data from several sensor nodes and carries out further processing.\nIn order to sense the environment and transmit data to the fusion node, nodes require energy and most often the nodes are equipped with pre-charged batteries for this purpose. However, as the nodes exhaust their battery power and stop sensing, the network performance degrades. The lifetime of the network is linked to the lifetimes of the individual nodes. Hence, the network becomes inoperable when a large number of nodes stop sensing. Thus, in a network with battery operated sensor nodes, the primary intention is to enhance the lifetime of the network, which may often lead to a compromise in the network performance. Many techniques have been proposed, which focus on improving lifetime of networks of sensor nodes. One of the more recent techniques which deals with this problem is the usage of energy harvesting to provide a perpetual source of energy for the nodes.\nAn energy harvesting (EH) sensor node replenishes the energy it consumes by harvesting energy from the environment (e.g., solar, wind power etc.) or other sources (e.g., body movements, finger strokes etc.) and converting into electrical energy. This way an EH node can be constantly powered through energy replenishment. So when compared to networks consisting of battery operated nodes, the long-term network performance metrics become appropriate. Thus, the goal pertaining to an EH sensor network is to reduce the average delay in data transmission. Even though an EH sensor node potentially has infinite amount of energy, yet the energy harvested is infrequently available as it is usually location and time dependent. Moreover the amount of energy replenished might be lower than the required amount. Therefore it is important to match the energy consumption with the amount of energy harvested in order to prevent energy starvation. This underlines the need for intelligently managing harvested energy to achieve the goal of good network performance.\nA drawback associated with an EH sensor (node) is that it requires additional circuitry to harvest energy, which increases the cost of the node. A network which contains several such nodes is not economically viable. The cost of the network can be minimized if there exists a central EH source which harvests energy and shares the available energy among multiple sensor nodes in its vicinity. Such an architecture is incorporated in motes. A mote (Fig. 1) is a single unit on which sensors with different functionalities are arranged (see [13]). For instance, there could be pressure sensors, temperature sensors etc., in the same unit to make different sets of measurements simultaneously. Alternatively, the sensors could be of the same functionality but deployed together at different angles in order to have a 360\u25e6 view of the entire sensing region.\nEach of these sensors (within a unit) have their own data buffers and a common EH source feeds energy to each of the data queues. Usually, the EH source is a battery which is recharged by energy harvesting. The sensors in the mote are perpetually powered, but only if the energy harvested in the source is efficiently shared. Thus there is a need for a technique that dynamically allocates energy to each of the data buffers of individual sensors in order that the average queue lengths (or transmission delays) across the data buffers are minimized.\nIn this paper, we focus on the problem of developing algorithms that achieve efficient\nenergy allocation in a system comprising of multiple sensor nodes with their own data buffers and a common EH source. Another scenario (that however we do not consider here) where our techniques are applicable is the case of downlink transmissions [22], where a base station (BS) maintains a separate data queue for each individual sensor node. The BS in question would also typically be powered by a single EH source, and again the problem would be to dynamically allocate the available energy to each one of the data queues. As suggested by a reviewer of the journal version of this paper, the above is equivalent to a communication setup with with an energy harvesting transmitter and n receivers which are connected to the transmitter over orthogonal links and equal gain links. The transmitter employs n finite data buffers to store incoming data, intended for the n receivers and must optimally allocate its energy to transmit data intended for the n receivers.\nWe present learning algorithms for a controller which has to judiciously distribute the energy amongst the competing nodes. The controller decides on the amount of energy to be allocated to every node at every decision instant considering the amount of data waiting to be transmitted in each of the data queues. Thus the state of the system comprises of the\namount of data in each of the data queues along with the energy available in the source. Given the system state at an instant, the controller has to find out the best possible way to allocate energy to the individual nodes. The decided allocation has a bearing on the total amount of data transmitted at that instant as well as the amount of data that will be transmitted in the future. Our algorithms help the controller learn the optimal allocation for every state, one which reduces the buildup of data in the data buffers. In the algorithm we present, the controller systematically tries out several possible allocations feasible in a state, before learning the optimal allocation. This method is computationally efficient for small number of states. However it becomes computationally expensive when there are numerous states. We propose approximation algorithms to find the near-optimal allocation of energy in this scenario. In the following subsection, we survey literature on EH nodes and energy management policies employed in EH sensor networks."}, {"heading": "1.1 Related Work", "text": "Optimizing energy usage in battery-powered sensors is addressed in [33, 34]. The problem of designing appropriate sensor schedules of sensor data transmission is discussed in [33]. A schedule of data transmission indicates when the battery-powered sensor transmits data. Transmitting data uses up energy, while not transmitting data results in error in estimation of parameters dependent on the sensor data. The authors in [33] consider battery-powered sensor nodes, each of which needs to minimize the energy utilized for data transmission. The estimation of parameters dependent on the sensor data may however involve error if the sensor does not transmit data for long periods of time. The objective in [33] is to find optimal periodic sensor schedules which minimize the estimation error at the fusion node and optimize energy usage.\nIn [34], the authors consider battery-powered sensors with two transmission power levels. The transmission power levels have different packet drop rates with the higher transmission power level having a lower packet drop rate. The sensor can choose one of the power levels for data transmission. It is assumed that the fusion node sends an acknowledgment (ACK or NACK) to the sensor node which indicates whether the data packet has been received or not. The objective in [34] is to minimize the average expected error in state estimation under energy constraint. At time k, based on the communication feedback the sensor knows whether the previous packets have been received by the fusion node or not. The problem of choosing the transmission power level is modeled as a MDP and the optimal schedule is shown to be stationary. The works [33, 34] consider the problem of efficient energy usage in battery powered sensors. The aspect of network performance is not considered in these. Our work deals with optimizing energy sharing in EH nodes where maximizing a network performance objective is the primary goal.\nAn early work in rechargeable sensors is [18]. The authors of [18] present a framework for the sensor network to adaptively learn the spatio-temporal characteristics of energy availability and provide algorithms to use this information for task sharing among nodes. In [17], the irregular and spatio-temporal characteristics of harvested energy are considered. The authors discuss the conditions for ensuring energy-neutral operation, i.e., using the energy\nharvested at an appropriate rate such that the system continues to operate forever. Practical methods for a harvesting system to achieve energy-neutral operation are developed. Compared to [18, 17], we focus on minimizing the delay in data transmission from the nodes and also ensuring energy neutral operation.\nThe scenario of a single EH transmitter with limited battery capacity is considered in [41, 26]. In [26], the transmitter communicates in a fading channel, whereas in [41], no specific constraints on the channel are considered. The problem of finding the optimal transmission policy to maximize the short-term throughput of an EH transmitter is considered in [41]. Under the assumption of an increasing concave power-rate relationship, the shortterm throughput maximizing transmission policy is identified. In [26], the transmitter gets channel state information and the node has to adaptively control the transmission rate. The objective is to maximize the throughput by a deadline and minimize the transmission completion time of a communication session. The authors in [26] develop an online algorithm which determines the transmit power at every instant by taking into account the amount of energy available and channel state.\nThe efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46]. A channel and data queue aware sleep/active/listen mechanism in this direction is proposed in [25]. Listen mode turns off the transmitter, while sleep mode is activated if channel quality is bad. The node periodically enters the active mode. In the listen mode, the queue can build up resulting in packets being dropped. In the sleep mode, incoming packets are blocked. A bargaining game approach is used to balance the probabilities of packet drop and packets being blocked. The Nash equilibrium solution of the game controls the sleep/active mode duration and the amount of energy used.\nThe model proposed in [36, 30] considers a single EH sensor node with finite energy and data buffers. The authors assume that data sensed is independent across time instants and so is the energy harvested. The amount of data that can be transmitted using some specified energy is modeled using a conversion function. In [36], a linear conversion function is used and optimal energy management policies are provided for the same. These policies are throughput optimal and mean delay optimal in a low SNR regime. However, in the case of non-linear conversion function, [36] provides certain heuristic policies. In [30], a nonlinear conversion function is used. The authors therein provide simulation-based learning algorithms for the energy management problem. These algorithms are model-free, i.e., do not require an explicit model of the system and the conversion function. Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source. The maximization objective is the delay in data transmission from the nodes. However, channel constraints are not addressed in our work.\nData packet scheduling problems in EH sensor networks are considered in [46] and [45]. It is assumed in [46] that a single EH node has separate data and energy queues, while the data sensed and energy harvested are random. The same assumption is made for each sensor in a two-sensor communication system considered in [45]. For simplicity it is assumed that all data bits have arrived in the queue and are ready for transmission, while the energy harvesting times and harvested energy amounts are known before the transmission\nbegins. In [46]([45]) the objective is to minimize the time by which all data packets from the node(s) are transmitted (to the fusion node). It is proposed to optimize this by controlling the transmission rate. The authors develop an algorithm to find the transmission rate at every instant, which optimizes the time to transmit the data packets. A two-user Gaussian interference channel with two EH sensor nodes and receivers is considered in [42]. This paper focuses on short-term sum throughput maximization of data transmitted from the two nodes before a given deadline. The authors provide generalized water-filling algorithms for the same. In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source. The data and energy arrivals are uncertain and unknown. Moreover the problem we deal with has an infinite horizon, wherein the objective is to reduce the mean delay of data transmission from the nodes. We develop simulation based learning algorithms for this problem.\nCooperative wireless network settings are considered in [10, 15, 43]. Three different network settings with energy transfer between nodes are considered in [15]. Energy management policies which maximize the system throughput within a given duration are determined in all the three cases. A water-filling algorithm is developed which controls the flow of harvested energy over time and among the nodes. In [43], there exists an EH relay node and multiple other EH source nodes. The source nodes have infinite data buffer capacity. The relay node transfers data between the source and destination nodes. The source and relay nodes can transfer energy to one another. A sum rate maximization problem in this setting is solved. In [10], multiple pairs of sources and destinations communicate via an EH relay node. The EH relay node has a limited battery, which is recharged by wireless energy transfer from the source nodes. The EH relay node has to efficiently distribute the power obtained among the multiple users. The authors investigate four different power allocation strategies for outage performance (outage is an event in which data is lost due to lack of battery energy or transmission failures caused by channel fades). We do not consider energy cooperation between nodes in the sensor network. Moreover, we do not assume wireless energy transfer in our model.\nA multi-user additive white Gaussian noise (AWGN) broadcast channel comprising of a single EH transmitter and M receivers is considered in [28]. The EH transmitter harvests energy from the environment and stores in a queue. The transmitter has M data queues, each of which stores data packets intended for a specific receiver. The data queues have fixed number of bits to be delivered to the receiver. The objective in [28] is to find a transmission policy that minimizes the time by which all the bits are transmitted to the receivers. An optimization problem is formulated and structural properties of the optimal policy are derived. In our work, we model energy sharing in multiple nodes when there is a single power source. We assume uncertain data and energy arrival processes. The objective is to minimize the average delay in data transmission from the nodes, when there is data arrival at every instant."}, {"heading": "1.2 Our Contributions", "text": "\u2022 We consider the problem of efficient energy allocation in a system with multiple sensor nodes, each with its own data buffer, and a common EH source.\n\u2022 We model the above problem as an infinite-horizon average cost Markov decision process (MDP) [4],[32] with an appropriate single-stage cost function. Our objective in the MDP setting is to minimize the long-run average delay in data transmission.\n\u2022 We develop reinforcement learning algorithms which provide optimal energy sharing policies for the above problem. The learning procedure used does not need the system knowledge such as data and energy rates or cost structure and learns using the data obtained in an online manner.\n\u2022 In order to deal with the dimensionality of the state space of the MDP, we present approximation algorithms. These algorithms find near-optimal energy distribution profiles when the state-action space of the MDP becomes unmanageable.\n\u2022 We demonstrate through simulations that the policies obtained from our algorithm are better than the policies obtained from a heuristic greedy method and a combined nodes Q-learning algorithm (see Section 6)."}, {"heading": "1.3 Organization of the Paper", "text": "The rest of the paper is organized as follows. The next section describes the model, related notation and assumptions. Section 3 formulates the energy sharing problem as an MDP. Section 4 presents the RL algorithms used for solving the MDP. Section 5 highlights the need for approximate policies and gives a detailed explanation of the approximation algorithms we develop for the problem. Section 6 presents the simulation results of our algorithms. Section 7 provides the concluding remarks and possible future directions. Finally, an appendix at the end of the paper contains the proof of two results."}, {"heading": "2 Model and Notation", "text": "We consider the problem of sharing the energy available in an energy harvesting source among multiple sensor nodes. We present a slotted, discrete-time, model (Fig. 2) for this problem. A sensor node in the network senses a random field and stores the sensed data in a finite data buffer of size D\nMAX . In order to transmit the sensed data to a fusion (or\ncentral) node, the sensor node needs energy, which it obtains from an energy harvesting source. The energy harvesting source has an energy buffer of finite capacity E\nMAX . The\ncommon EH source is an abtract entity in the model. It is generally a rechargeable battery which is replenished by random energy harvests. We assume fragmentation of data packets (fluid model) as in [36] and hence these will be treated as bit strings.\nLet qik denote the data buffer level of node i and Ek be the energy buffer level at the beginning of slot k. Sensor node i generates X ik bits of data by sensing the random field. The source harvests Yk units of energy. Based on the data queue levels (q 1 k, . . . , q n k ) and the energy level Ek, the energy sharing controller decides upon the number of energy bits to be provided to every node. Let T ik units of energy be provided to node i in slot k. Using it, the node transmits g(T ik ) bits of data. We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]). Note that the Shannon Channel capacity for Gaussian channels gives such a conversion function and in particular,\ng(Tk) = 1\n2 log(1 + \u03b2Tk),\nwhere \u03b2 is a constant and \u03b2Tk gives the Signal-to-Noise (SNR) ratio. This is a non-decreasing concave function. We have assumed this form in the simulation experiments. However, our algorithms work regardless of the form of the conversion function and will learn the optimal energy sharing policy for any form of conversion function (see Remark 15).\nIt should be noted that we do not consider wireless energy transfer from the source node to the sensor nodes. Here we consider the source node to be a rechargeable battery which powers the nodes. The queue lengths in the data buffers evolve with time as follows:\nqik+1 = (q i k \u2212 g(T ik ))+ +X ik 1 \u2264 i \u2264 n, k \u2265 0, (1)\nwhere (qik\u2212g(T ik ))+ = max(qik\u2212g(T ik ), 0) and the energy buffer queue length evolves as given below:\nEk+1 = ( Ek \u2212 n\u2211 i=1 T ik ) + Yk, 1 \u2264 i \u2264 n, k \u2265 0, (2)\nwhere n\u2211 i=1 T ik \u2264 Ek.\nAssumption 1. The generated data rates at time k + 1, Xk+1 , (X1k+1, X 2 k+1, . . . , X n k+1) where n denotes the number of sensors in a node, evolves as a jointly Markov process, i.e.,\nXk+1 = f 1(Xk,Wk), k \u2265 0 (3)\nwhere f 1 is some arbitrary vector valued function with n components and {Wk, k \u2265 1} is a noise sequence with probability distribution P (Wk | Xk) depending on Xk. Thus, the generated data {Xk, k \u2265 0} is both spatially and temporally correlated. Moreover, the sequence X ik , k \u2265 0 satisfies sup\nk\u22650 E[X ik ] \u2264 r <\u221e. Further, the energy arrival process evolves as:\nYk+1 = f 2(Yk, Vk), k \u2265 0, (4)\nwhere f 2 is some scalar valued function and {Vk, k \u2265 1} is the noise sequence with probability distribution P (Vk | Yk) depending on Yk.\nRemark 1. Assumption 1 is general enough to cover most of the stochastic models for the data and energy arrivals. A special case of Assumption 1 is to consider that for any k \u2265 0 and 1 \u2264 i \u2264 n, X ik is independent of X ik\u22121, X ik\u22122, . . . , X i1 , X i0 and the given sequence {X ik}k\u22650 for a given i \u2208 {1, . . . , n} is identically distributed. Similarly, for any k \u2265 0, Yk is independent of Yk\u22121, Yk\u22122, . . . , Y1, Y0 and the sequence {Yk} is identically distributed. In Section 6 we show results of experiments where the above i.i.d setting as well as a more general setting as described earlier are shown."}, {"heading": "3 Energy Sharing Problem as an MDP", "text": "A Markov decision process (MDP) is a tuple of states, actions, transition probabilities and single-stage costs. Given that the MDP is in a certain state, and an action is chosen by the controller, the MDP moves to a \u2018next\u2019 state according to the prescribed transition probabilities. The objective of the controller is to select a sequence of actions as a function of the states in order to minimize a given long-term objective (cost). We formulate the energy sharing problem in the MDP setting using the long-run average cost criterion. The MDP formulation requires that we identify the states, actions and the cost structure for the problem, which is described next.\nThe state sk is a tuple comprising of the data buffer level of all sensor nodes, the level of the energy buffer in the source, the data and energy arrivals in the past. Note that for\n1 \u2264 i \u2264 n, qik \u2208 {0, 1, . . . , DMAX}. Similarly Ek \u2208 {0, 1, . . . , EMAX}. Thus in stage k, in the context of Assumption 1, state sk = (q 1 k, q 2 k, . . . , q n k , Ek, Xk\u22121, Yk\u22121). However, when we assume that for 1 \u2264 i \u2264 n, {X ik} and {Yk} are i.i.d (as in Remark 1), then the state tuple simplifies to sk = (q 1 k, q 2 k, . . . , q n k , Ek).\nThe set of all states is the state-space, which is denoted by S. Similarly A denotes the action-space, which is the set of all actions. The set of feasible actions in a state sk is denoted by A(sk). A deterministic policy \u03c0 = {Tk, k \u2265 0} is a sequence of maps such that at time k when state sk = (q 1 k, . . . , q n k , Ek, Xk\u22121, Yk\u22121), i.e., when there are q j k units of data at node j, 1 \u2264 j \u2264 n and Ek bits of energy in the source, Xk is the data arrival vector and YK is the energy harvested at time k\u2212 1, then Tk(sk) = (T 1k (sk), T 2k (sk), . . . , T nk (sk)) gives the number of energy bits to be given to each node at time k (i.e., it gives the energy split). Thus the action to be taken in state sk is given by Tk(sk) \u2208 A(sk). A deterministic policy which does not change with time is referred to as a stationary deterministic policy (SDP). We denote such a policy \u03c0 as \u03c0 = (T, T, . . .), where T (sk) is the action chosen in state sk. We set the single-stage cost c\u0303(sk, T (sk)) as a sum of the number of bits in the data buffers. Thus,\nc\u0303(sk, T (sk)) = n\u2211 i=1 qik. (5)\nRemark 2. In order to formulate the energy sharing problem in the framework of MDP, we require the state sequence {sk = (q1k, q2k, . . . , qnk , Ek)}k\u22650 under a given policy to be a Markov chain, i.e., P (sk+1 | sk, sk\u22121, . . . , s0, \u03c0) = P (sk+1 | sk, \u03c0). We have generalized the assumption on {X ik, 1 \u2264 i \u2264 n}k\u22650 and {Yk}k\u22650 and consider jointly Markov data arrival and Markovian energy arrival processes. Remark 1 applies to the i.i.d case. If we assume the data arrivals {X ik}k\u22650 for a fixed i \u2208 {1, 2, . . . , n} and the energy arrivals {Yk}k\u22650 are i.i.d, then the Markov assumption can be seen to be easily satisfied.\nThe Markov property for the state evolution {sk}k\u22650 is necessary as we can only search for policies based only on the present state of the system. Otherwise, the policies will be based on the entire history. The search for optimal policies in the space of history based policies is a computationally infeasible task.\nIn the general case where {Xk} is jointly Markov, note that the state sequence {sk}k\u22650 under a given policy will not be a Markov chain. Now consider the augmented state s\u0304k\n\u2206 = skXk\u22121\nYk\u22121 . Now, under a given policy \u03c0 = (T, . . . , T ), the state evolution can be described as  q1k+1 ...\nqnk+1 Ek+1 Xk Yk\n =  (q1k \u2212 g(T 1(sk)))+ +X1k ... (qnk \u2212 g(T n(sk)))+ +Xnk( Ek \u2212 \u2211n i=1 T i(sk) )\n+ Yk, f 1(Xk\u22121,Wk\u22121) f 2(Yk\u22121, Vk\u22121)\n . (6)\nThis can be written as s\u0304k+1 = h(s\u0304k, T (s\u0304k),Wk\u22121, Vk\u22121) for suitable vector valued function h. This is the standard description for the state evolution for an MDP (see Chapter 1 in [3]). Since the probability distribution of the noise Wk\u22121 (Vk\u22121) depends only on Xk\u22121 (Yk\u22121), the augmented state sequence s\u0304k = {(sk, Xk\u22121, Yk\u22121)}k\u22650 forms a Markov chain. This facilitates search for policies only based on the present augmented state.\nRemark 3. The sensor node may generate data as packets, but in the model we allow for arbitrary fragmentation of data during transmission. Hence packet boundaries are no longer relevant and we consider bit strings. This is the fluid model as described in [11]. The data is considered to be stored in the data buffers as bit strings and hence the data buffer levels are discrete. The fluid model assumption (data discretization) has been made in [36, 14, 46]. For energy harvesting we consider energy discretization. Energy discretization implies that we have assumed that discrete levels of energy are harvested and stored in the queue. Energy discretization has been considered in some previous works [2, 36]. Owing to these assumptions on data generation and energy harvesting, the state space is discrete and finite.\nThe long-run average cost of an SDP \u03c0 is given by\n\u03bb\u0303\u03c0 = lim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 c\u0303(sk, T (sk))\n] . (7)\nIn contrast, a stationary randomized policy (SRP) is a sequence of maps \u03d5 = {\u03c8, \u03c8, . . .} such that for a state sk, \u03c8(sk, \u00b7) is a probability distribution over the set of feasible actions in state sk. Such a policy does not change with time. The single-stage cost d\u0303(sk) of an SRP \u03d5 is given by\nd\u0303(sk) = \u2211\na\u2208A(sk)\n\u03c8(sk, a) c\u0303(sk, a), (8)\nwhere a gives the energy split in state sk. The long-run average cost of an SRP \u03d5 is\n\u03bb\u0303\u03d5 = lim m\u2192\u221e\n1\nm m\u22121\u2211 k=0 d\u0303(sk). (9)\nWe observe that the term qik in (5) does not include the effect of action explicitly. Hence we modify the cost function to include the effect of the action taken explicitly into the cost function. In order to enable reformulation of the average cost objective in the modified form, we prove the following lemma. Define\n\u03bb\u03c0 = lim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 n\u2211 i=1 ( qik \u2212 g(T i(sk))\n)+] . (10)\nLemma 1. Let qik, 1 \u2264 i \u2264 n, T i(sk), 1 \u2264 i \u2264 n and g be as before and let E[X i], 1 \u2264 i \u2264 n denote the mean of the i.i.d random variables X i, 1 \u2264 i \u2264 n. Then\n\u03bb\u03c0 = \u03bb\u0303\u03c0 \u2212 n\u2211 i=1 E [ X i ]\nfor all policies \u03c0.\nProof. Using state evolution equations (1)-(2),\nlim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 n\u2211 i=1 ( qik \u2212 g(T i(sk))\n)+]\n= lim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 n\u2211 i=1 ( qik+1 \u2212X ik\n)]\n= lim m\u2192\u221e E [ n\u2211 i=1 1 m m\u22121\u2211 k=0 ( qik+1 \u2212X ik )]\n= n\u2211 i=1 lim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 ( qik+1 \u2212X ik\n)]\n= n\u2211 i=1 { lim m\u2192\u221e E [ 1 m m\u22121\u2211 k=0 qik+1 ] \u2212 lim m\u2192\u221e E [ 1 m m\u22121\u2211 k=0 X ik ]}\n= n\u2211 i=1 { lim m\u2192\u221e E [ 1 m ( m\u22121\u2211 k=0 qik + q i m \u2212 qi0 )] \u2212 lim m\u2192\u221e E [ 1 m m\u22121\u2211 k=0 X ik ]}\n= n\u2211 i=1 { lim m\u2192\u221e E [ 1 m ( m\u22121\u2211 k=0 qik + q i m \u2212 qi0 )] \u2212 E [ X i ]}\n= n\u2211 i=1 { lim m\u2192\u221e E [ 1 m m\u22121\u2211 k=0 qik ] + lim m\u2192\u221e E [ 1 m ( qim \u2212 qi0 )] \u2212 E [ X i ]}\n= n\u2211 i=1 { lim m\u2192\u221e E [ 1 m m\u22121\u2211 k=0 qik ]} \u2212 n\u2211 i=1 E [ X i ]\n= \u03bb\u0303\u03c0 \u2212 n\u2211 i=1 E [ X i ] .\nThe second last equality above follows from the fact that lim m\u2192\u221e\nE [\n1 m (qim \u2212 qi0) ] = 0. The\nclaim follows.\nThe linear relationship between \u03bb\u0303\u03c0 and \u03bb\u03c0 enables us to define the new single-stage cost function as:\nc(sk, Tk) = n\u2211 i=1 (qik \u2212 g(T i(sk)))+. (11)\nWith this single-stage cost function, the long-run average cost of an SDP \u03c0 is given by\n\u03bb\u03c0 = lim m\u2192\u221e E\n[ 1\nm m\u22121\u2211 k=0 c(sk, T (sk))\n] . (12)\nThe single-stage cost d(sk) of an SRP \u03d5 is given by d(sk) = \u2211\na\u2208A(sk)\n\u03c8(sk, a) c(sk, a), (13)\nwhere a gives the energy split. The long-run average cost of an SRP \u03d5 is\n\u03bb\u03d5 = lim m\u2192\u221e\n1\nm m\u22121\u2211 k=0 d(sk). (14)\nIt can be inferred from Lemma 1 that a policy which minimizes the average cost in (11) (or (14)) will also minimize the average cost given by (7) (or (9)). In this paper we are interested in finding stationary policies (deterministic or randomized) which optimally share the energy among a set of nodes. Therefore our aim is to find policies which minimize the average cost per step, when the single-stage cost is given by (11).\nAny stationary optimal policy minimizes the average cost of the system over all policies. Let \u03c0\u2217 be an optimal policy and \u03a0 be the set of all policies. The average cost of policy \u03c0\u2217 is denoted \u03bb\u2217. Then \u03bb\u2217 = inf\n\u03c0\u2208\u03a0 \u03bb\u03c0.\nThe policy corresponding to the above average cost minimizes the sum of (data) queue lengths of all nodes. By Little\u2019s law, under stationarity, the average sum of data queue lengths at the sensor nodes is proportional to the average waiting time or delay of the arrivals (bits). Hence an average cost optimal policy minimizes the stationary mean delay as well.\nThe class of stationary deterministic policies is contained in the class of stationary randomized policies and in the system we consider, an optimal policy is known to exist in the class of stationary deterministic policies. We provide an algorithm which finds an optimal SDP. The algorithm is computationally efficient for small state and action spaces. However for large state-action spaces, the algorithm computations are expensive. To mitigate this problem, we provide approximation algorithms which find near-optimal stationary policies for the system. These algorithms are described in the following sections."}, {"heading": "4 Energy Sharing Algorithms", "text": ""}, {"heading": "4.1 Background", "text": "Consider an optimal SDP \u03c0\u2217 for the energy sharing MDP. Then \u03bb\u2217 corresponds to the average cost of the policy \u03c0\u2217. Suppose ir is a reference state in the MDP. For any state i \u2208 S, let h\u2217(i) be the relative (or the differential) cost defined as the minimum of the difference between the expected cost to reach state ir from i and the expected cost incurred if the cost per stage was \u03bb\u2217. The quantities \u03bb\u2217 and h\u2217(i), i \u2208 S satisfy the Bellman Equation:\n\u03bb\u2217 + h\u2217(i) = min a\u2208A(i)\n( c(i, a) +\n\u2211 j\u2208S p(i, a, j)h\u2217(j)\n) , (15)\nwhere p(i, a, j) is the probability that the system will move from state i to state j under action a. We denote by Q\u2217(i, a), the optimal differential cost of any feasible state-action tuple (i, a) as follows:\nQ\u2217(i, a) = c(i, a) + \u2211 j\u2208S p(i, a, j)h\u2217(j). (16)\nEquation (15) can now be rewritten as\n\u03bb\u2217 + h\u2217(i) = min a\u2208A(i) Q\u2217(i, a), \u2200i \u2208 S (17)\nor alternately h\u2217(i) = min\na\u2208A(i) Q\u2217(i, a)\u2212 \u03bb\u2217, \u2200i \u2208 S. (18)\nPlugging (18) into (16), one obtains\nQ\u2217(i, a) = c(i, a) + \u2211 j\u2208S p(i, a, j) [ min b\u2208A(j) Q\u2217(j, b)\u2212 \u03bb\u2217 ]\n(19)\nor \u03bb\u2217 +Q\u2217(i, a) = c(i, a) + \u2211 j\u2208S p(i, a, j) min b\u2208A(j) Q\u2217(j, b), \u2200i \u2208 S,\u2200a \u2208 A(i). (20)\nEquation (20) is also referred to as the Q-Bellman equation. The important thing to note is that whereas the Bellman equation (15) is not directly amenable to stochastic approximation, the Q-Bellman equation (20) is; because of the fact that the minimization operation in (20) is inside the conditional expectation unlike (15) (where it is outside of it). If the transition probabilities and the cost structure of the system model are known, then (20) can be solved using dynamic programming techniques [40]. When the system model is not known (as in the problem we study), the Q-learning algorithm can be used to obtain optimal policies. This learning algorithm solves (20) in an online manner using simulation to obtain an optimal policy. It is described in the following subsection."}, {"heading": "4.2 Relative Value Iteration based Q-Learning", "text": "Q-learning is a stochastic iterative, simulation-based algorithm that aims to find the Q\u2217(i, a) values for all feasible state-action pairs (i, a). It is a model-free learning algorithm and proceeds by assuming that the transition probabilities p(i, a, j) are unknown. Initially Qvalues for all state-action pairs are set to zero, i.e., Q0(i, a) = 0,\u2200i \u2208 S, a \u2208 A(i). Then \u2200k \u2265 0, the Q-learning update [1] for a state-action pair visited during simulation is carried out as follows:\nQk+1(i, a) = (1\u2212 \u03b1(k))Qk(i, a) + \u03b1(k) ( c(i, a) + min\nb\u2208A(j) Qk(j, b)\u2212 min u\u2208A(ir) Qk(ir, u)\n) , (21)\nwhere i is the current state at decision time k and ir is the reference state. The action in state i is selected using one of the exploration mechanisms described below. State j corresponds\nto the \u2018next\u2019 state that is obtained from simulation when the action a is selected in state i. Also, \u03b1(k), k \u2265 0 is a given step-size sequence such that \u03b1(k) > 0, \u2200k \u2265 0 and satisfies the following conditions: \u2211\nk \u03b1(k) =\u221e and \u2211 k \u03b12(k) <\u221e.\nLet t(k) = k\u22121\u2211 i=0 \u03b1(i), k \u2265 1, with t(0) = 0. Then, t(k), k \u2265 0 corresponds to the \u201ctimescale\u201d of the algorithm\u2019s updates. The first condition above ensures that t(k) \u2192 \u221e as k \u2192 \u221e. This ensures that the algorithm does not converge prematurely. The second condition makes sure that the noise asymptotically vanishes. These conditions on step sizes guarantee the convergence of Q-learning to the optimal state-action value function, see [1] for a proof of convergence of the algorithm. The update (21) is carried out for the state-action pairs visited during simulation. The exploration mechanisms we employ are as follows:\n1. -greedy: In the energy sharing problem, the number of actions feasible in every state is finite. Hence there exists an action am for state i such that Qk(i, am) \u2264 Qk(i, a \u2032 ), \u2200a\u2032 \u2208\nA(i), \u2200k \u2265 0. We choose \u2208 (0, 1). In state i, action am is picked with probability 1\u2212 , while any other action is picked with probability .\n2. UCB Exploration: Let Ni(k) be the number of times state i is visited until time k. Similarly let Ni,a(k) be the number of times action a is picked in state i upto time k. The Q-value of state-action pair (i, a) at time k is Qk(i, a). When the state i is encountered at time k, the action for this state is picked according to the following rule:\na\u2032 = arg max a\u2208A(i)\n( \u2212Qk(i, a) + \u03b2 \u221a lnNi(k)\nNi,a(k)\n) , (22)\nwhere \u03b2 is a constant. The first term on the right hand side gives preference to an action that has yielded good performance in the past visits to state i, while the second term gives preference to actions that have not been tried out many times so far, relative to lnNi(k).\nRemark 4. The convergence rates for the discounted Q-learning have been studied in [39, 19, 12]. The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12]. In the Q-learning algorithm, to explore the value of different states and actions, one needs to visit each state-action pair infinitely often. However, in practice, depending on the size of the state-action space, we need to simulate the Q-learning algorithm so that each state-action pair is visited a sufficient number of times. In our experiments for the case of two sensor nodes, the size of the state space is of the order of 105 and we ran our algorithm for 108 iterations.\nOnce we determine Q\u2217(i, a) for all state-action pairs, we can obtain the optimal action for a state i by choosing the action that minimizes Q\u2217(i, a). So\na\u2217 = arg min a\u2208A(i) Q\u2217(i, a). (23)\nIt should be noted that the Q-learning algorithm does not need knowledge of the cost structure and transition probabilities, and it learns an optimal policy by interacting with the system."}, {"heading": "5 Approximation Algorithms", "text": "The learning algorithm described in Section 4 is an iterative stochastic algorithm that learns the optimal energy split. This method requires that the Q(s, a) values be stored for all (s, a) tuples. The values of Q(s, a) for each (s, a) tuple are updated in (21) over a number of iterations using adequate exploration. These updations play a key role in finding the optimal control for a given state. Nevertheless for large state-action spaces these computations are expensive as every lookup operation and updation require memory access. For example, if there are two nodes sharing energy and buffer sizes are E\nMAX = D MAX = 30, then the\nnumber of (s, a) tuples would be of the order 106, which demands enormous amount of computation time and memory space. This condition is exacerbated when the number of nodes that share energy increases. For instance, in the case of four nodes sharing energy with E\nMAX = D MAX = 30, we have |S \u00d7 A| \u2248 309. Thus, we have a scenario where the\nstate-action space can be extremely large. To mitigate this problem, we propose two algorithms that are both based on certain threshold features. Both algorithms tackle the curse of dimensionality, by reducing the computational complexity. We describe below our threshold based features, following which we describe our algorithms."}, {"heading": "5.1 Threshold based Features", "text": "The fundamental idea of threshold based features is to cluster states in a particular manner, based on the properties of the differential value functions. The following proposition proves the monotonicity property of the differential value functions for the scenario where there is a single node and an EH source. This simple scenario is considered for the sake of clarity in the proof.\nProposition 1. Let H\u2217(q, E) be the differential value of state (q, E). Let q < qL \u2264 DMAX and EMAX \u2265 EL > E, respectively. Then,\nH\u2217(q, E) \u2264 H\u2217(qL, E), (24) H\u2217(q, E) \u2265 H\u2217(q, EL). (25)\nProof. Let J(s) be the total cost incurred when starting from state s. Define the Bellman operator L : Rn \u2192 Rn as\n(LJ)(s) = min T\u2208A(s)\n(c(s, T ) + E[J(s\u2032)]),\nwhere s\u2032 corresponds to the next state after s and T corresponds to the action taken in state s. As noted in Section 5.1, we show the proof for a single node and EH source. The proof\ncan be easily generalized to multiple nodes. Thus the state s corresponds to the tuple (q, E). Hence the above equation can be rewritten as\n(LJ)(q, E) = min T\u2208A(s)\n( c(q, E, T ) + E [ J(q \u2032 , E \u2032 ) ]) , \u2200(q, E) \u2208 S.\nWe consider the application of the operator L on the differential cost function H(\u00b7). We set out to prove this proposition using the relative value iteration scheme (see [5]). For this, we set a reference state r , (qr, Er) \u2208 S. The cost function in our case is (q \u2212 g(T ))+. Initially the differential value function has value zero for all states (q, E) \u2208 S, i.e., H(q, E) = 0, \u2200(q, E) \u2208 S. Then for some arbitrary (q, E) \u2208 S we have\nLH(q, E) = min T\u2208A(q,E)\n( (q \u2212 g(T ))+ + E [ H(q \u2032 , E \u2032 ) ]) \u2212 LH(qr, Er)\n= min T\u2208A(q,E)\n((q \u2212 g(T ))+)\u2212 LH(qr, Er)\nsince H(q \u2032 , E \u2032 ) = 0, \u2200(q\u2032 , E \u2032) \u2208 S. Let Tm be the value of T achieving the minimum in the first term of RHS. Then\nLH(q, E) = (q \u2212 g(Tm))+ \u2212 LH(qr, Er).\nNow consider the differential value of state qL where qL > q. Thus, consider\nLH(qL, E) = min T\u2208A(qL,E)\n( (qL \u2212 g(T ))+ + E [ H(q \u2032 , E \u2032 ) ]) \u2212 LH(qr, Er)\n= min T\u2208A(qL,E)\n((qL \u2212 g(T ))+)\u2212 LH(qr, Er)\n= (qL \u2212 g(TL))+ \u2212 LH(qr, Er),\nwhere TL is the value of T for which the minimum of the expression (q L \u2212 g(T ))+, in the above equations, is achieved. We have\nLH(q, E) = (q \u2212 g(Tm))\u2212 LH(qr, Er) \u2264 (q \u2212 g(TL))\u2212 LH(qr, Er) \u2264 (qL \u2212 g(TL))\u2212 LH(qr, Er) = LH(qL, E). (26)\nWe have H(qL, E) \u2265 H(q, E) since these values are initialized to zero and from (26), LH(qL, E) \u2265 LH(q, E). Now consider the differential value function of the state (q, EL) where EL > E.\nLH(q, EL) = min T\u2208A(q,EL)\n( (q \u2212 g(T ))+ + E [ H(q \u2032 , E \u2032 ) ]) \u2212 LH(qr, Er)\n= min T\u2208A(q,EL)\n( (q \u2212 g(T ))+ ) \u2212 LH(qr, Er)\n= (q \u2212 g(TE))+ \u2212 LH(qr, Er),\nwhere TE is the value of T for which the minimum of the expression (q\u2212g(T ))+, in the above equations, is achieved. We have,\nLH(q, EL) = (q \u2212 g(TE))+ \u2212 LH(qr, Er) \u2264 (q \u2212 g(Tm))+ \u2212 LH(qr, Er) \u2264 LH(q, E). (27)\nSince H(q, EL), H(q, E) are initialized to zero, we have H(q, EL) \u2264 H(q, E) and from (27), LH(q, EL) \u2264 LH(q, E). We prove the following statements using mathematical induction:\nLkH(q, E) \u2264 LkH(qL, E) \u2200k \u2265 0, LkH(q, E) \u2265 LkH(q, EL) \u2200k \u2265 0.\nWe have seen above that the two statements are true for both k = 0 and k = 1, respectively. Lets consider the first statement and assume that the statement holds for some k. We then prove that it holds for (k + 1). Consider\nLk+1H(q, E) = min T\u2208A(q,E)\n( (q \u2212 g(T ))+ + E [ LkH(q \u2032 , E \u2032 ) ]) \u2212 LkH(qr, Er).\nAssume Tm is the value of T at which the minimum of ((q \u2212 g(T ))+ + E [ LkH(q \u2032 , E \u2032 ) ] ) is attained. Then,\nLk+1H(q, E) = ((q \u2212 g(Tm))+ + E [ LkH(q \u2212 g(Tm) + x,E \u2212 Tm + y) ] )\u2212 LkH(qr, Er),\nwhere x, y are obtained from independent random distributions. Similarly, we get Lk+1H(qL, E) = ((qL \u2212 g(TL))+ + E [ LkH(qL \u2212 g(TL) + x,E \u2212 TL + y) ] )\u2212 LkH(qr, Er),\nwhere TL is the value of T for which the minimum in the expression ((q L \u2212 g(T ))+ + E [ LkH(q \u2032 , E \u2032 ) ] ) is achieved.\nLk+1H(q, E) = ((q \u2212 g(Tm))+ + E [ LkH(q \u2212 g(Tm) + x,E \u2212 Tm + y) ] )\u2212 LkH(qr, Er)\n\u2264 ((q \u2212 g(TL))+ + E [ LkH(q \u2212 g(TL) + x,E \u2212 TL + y) ] )\u2212 LkH(qr, Er)\n\u2264 ((qL \u2212 g(TL))+ + E [ LkH(qL \u2212 g(TL) + x,E \u2212 TL + y) ] )\u2212 LkH(qr, Er),\nsince the property holds true for LkH, i.e., LkH(q, E) \u2264 LkH(qL, E). Thus,\nLk+1H(q, E) \u2264 ((qL \u2212 g(TL))+ + E [ LkH(qL \u2212 g(TL) + x,E \u2212 TL + y) ] )\u2212 LkH(qr, Er)\n= Lk+1H(qL, E).\nHence, LkH(q, E) \u2264 LkH(qL, E) \u2200k \u2265 0. (28)\nSimilarly we get, Lk+1H(q, EL) = ((q \u2212 g(TE))+ + E [ LkH(q \u2212 g(TE) + x,EL \u2212 TE + y) ] )\u2212 LkH(qr, Er)\n\u2264 ((q \u2212 g(Tm))+ + E [ LkH(q \u2212 g(Tm) + x,EL \u2212 Tm + y) ] )\u2212 LkH(qr, Er)\n\u2264 ((q \u2212 g(Tm))+ + E [ LkH(q \u2212 g(Tm) + x,E \u2212 Tm + y) ] )\u2212 LkH(qr, Er) = Lk+1H(q, E),\nhence by mathematical induction on k we get,\nLkH(q, EL) \u2264 LkH(q, E) \u2200k \u2265 0. (29)\nAs a consequence of the relative value iteration scheme ([32]), when k \u2192 \u221e, LkH \u2192 H\u2217 with H\u2217(qr, Er) = \u03bb \u2217. Thus, from (28) and (29) as k \u2192\u221e, we obtain\nH\u2217(q, E) \u2264 H\u2217(qL, E)\nH\u2217(q, E) \u2265 H\u2217(q, EL). The claim now follows.\nProposition 1 can be easily generalized to multiple nodes in the following manner. Suppose there are n nodes and one EH source. Let s = (q1, . . . , qj, . . . , qn, E) and s\u2032 = (q1, . . . , qjL, . . . , q n, E), where qjL > q j. The states s and s\u2032 differ only in the data buffer queue lengths of node j, while the data buffer queue lengths of other nodes remain the same and so does the energy buffer level. Then it can be observed that H\u2217(q1, . . . , qj, . . . , qn, E) \u2264 H\u2217(q1, . . . , qjL, . . . , q\nn, E). In a similar manner, let state s\u2032\u2032 = (q1, q2, . . . , qn, EL) and EL > E. Then states s and s\u2032\u2032 differ only in the energy buffer levels. Hence H\u2217(q1, . . . , qn, E) \u2265 H\u2217(q1, . . . , qn, EL). This proposition provides us a method which is useful for clustering states.\nRemark 5. The monotonicity property of the differential value function H\u2217 provides a justification to group nearby states to form an aggregate state. The value function of the aggregated state will be the average of the value function of the states in a partition. If the difference between values of states in a cluster is not much, the value function of aggregated state will be close to the value function of the unaggregated state. Thus, the policy obtained from the aggregated value function is likely to be close to the policy obtained from unaggregated states. Without the monotonicity property, states may be grouped arbitrarily and consequently, state aggregation may not yield a good policy.\nRemark 6. In the case of MDP with large state-action space, one goes for function approximation based methods (see Chapter 8 in [37]). However, if one combines Q-learning with function approximation, we do not have convergence guarantees to the optimal policy unlike Q-learning without function approximation (Q-learning with tabular representation [37]). However, when Q-learning is combined with state-aggregation (QL-SA) we continue to have convergence guarantees (see Section 6.7 in [5]). Q-learning using state aggregation can produce good policies only when the value function has a monotonicity structure, which is proved in the previous remark."}, {"heading": "5.1.1 Clustering", "text": "The data and energy buffers are quantized and using this we formulate the aggregate stateaction space. The quantization of buffer space is described next. We predefine data buffer and energy buffer partitions (or quantization levels) d1, d2, . . . , ds and e1, e2, . . . , er respectively. The partition (or quantization level) di, (i \u2208 {1, . . . , s}) corresponds to a given range (xiL , x i U\n) and is fixed, where xi\nL and xi U represent the prescribed lower and upper data buffer level\nlimits. In a similar manner the quantization level ej, (j \u2208 {1, . . . , r}) (or energy buffer partition) corresponds to a given interval (yi\nL , yi U ), where yi L and yi U represent the prescribed\nlower and upper energy buffer level limits. As an example, suppose D MAX = E MAX = 10 and each of the buffers are quantized into three levels, i.e., s = r = 3. An instance of data and energy buffer partition ranges in this scenario can be y1\nL = x1 L = 0, y1 U = x1 U = 3, y2 L = x2 L =\n4, x2 U = y2 U = 7, x3 L = y3 L = 8, y3 U = x3 U = 10. Here Partition 1 corresponds to the number of data (energy) bits (units) in the range (0, 3), while Partition 3 corresponds to the number of data (energy) bits (units) in the range (8, 10). The following inequalities hold with respect to the partition limits:\n0 = x1 L < x1 U < x2 L < x2 U < . . . < xs L < xs U = D MAX and\nxi U + 1 = xi+1 L , 1 \u2264 i \u2264 s\u2212 1.\nSimilarly,\n0 = y1 L < y1 U < y2 L < y2 U < . . . < ys L < ys U = E MAX and\nyi U + 1 = yi+1 L , 1 \u2264 i \u2264 r \u2212 1."}, {"heading": "5.1.2 Aggregate States and Actions", "text": "We define an aggregate state as s\u2032 = {l1 , . . . , ln+1}, where for 1 \u2264 i \u2264 n, li is the data buffer level for the ith node and ln+1 is the energy buffer level. So l\ni \u2208 {1, . . . , s}, 1 \u2264 i \u2264 n and l n+1 \u2208 {1, . . . , r}. An aggregate action corresponding to the state s\u2032 is an n-tuple t\u2032 of the form t \u2032\n= (t1, . . . , tn), where ti \u2208 {1, . . . , ln+1}, 1 \u2264 i \u2264 n. Each component in t\u2032 indicates an energy level. By considering the data level in all the nodes, the controller decides on an energy level for each node. Thus the energy level indicates the energy partition which can be supplied to the node. For instance, if D\nMAX = E MAX = 15, s = r = 3 and there are two\nnodes in the system, then an example aggregate state is s\u2032 = (1, 1, 3). Suppose the controller selects the aggregate action t \u2032 = (2, 1), which means that the controller decides to give u number of energy bits to Node 1, and v number of energy bits to Node 2, with y2 L \u2264 u \u2264 y2 U and y1 L \u2264 v \u2264 y1 U , respectively."}, {"heading": "5.1.3 Cardinality Reduction", "text": "Note that s D MAX , r E MAX . Let the aggregated state and action spaces be denoted by S \u2032 and A \u2032 respectively. The aggregated state-action space has cardinality |S \u2032\u00d7A\u2032 |. Thus, the cardinality of the state-action space is reduced to a great extent by aggregation. For instance,\nin the case of four nodes sharing energy from one EH source and E MAX = D MAX = 30, the cardinality of the state-action space without state-aggregation is |S \u00d7 A| \u2248 309. However, with four partitions each for the data and energy buffers, the cardinality of the state-action space with aggregation is |S \u2032 \u00d7 A\u2032| \u2248 49."}, {"heading": "5.2 Approximate Learning Algorithm", "text": "We now explain our approximate learning algorithm for the energy sharing problem. It is based on Q-learning and state aggregation. Although the straightforward Q-learning algorithm described in Section 4 requires complete state information and is not computationally efficient with respect to large state-action spaces, its state-aggregation based counterpart requires significantly less computation and memory space. Also our experiments show that we do not compromise much on the policy obtained either (see Fig. 9b)."}, {"heading": "5.2.1 Method", "text": "Let s\u2032 = {l1k, . . . , l n+1 k } be the aggregate state at decision time k. The action taken in s\u2032 is t\u2032 = (t1k, . . . , t n k). The Q-value Q(s\n\u2032, t\u2032) indicates how good an aggregate state-action tuple is. The algorithm proceeds with the following update rule:\nQk+1(s \u2032, t\u2032) = (1\u2212\u03b1(k))Qk(s\u2032, t\u2032)+\u03b1(k) ( c(s\u2032, t\u2032) + min\nb\u2208A\u2032 (j\u2032) Qk(j \u2032, b)\u2212 min u\u2208A\u2032 (r\u2032) Qk(r \u2032, u)\n) , (30)\nwhere j\u2032 is the aggregate state obtained by simulating action t\u2032 in state s\u2032. Also, r\u2032 is a reference state and \u03b1(k), k \u2265 0 is a positive step-size schedule satisfying the conditions mentioned in Section 4.2. To facilitate exploration, we employ the mechanisms described in Section 4.2. Convergence of Q-learning with state aggregation is discussed in Section 6.7 of [5].\nRemark 7. The aggregate state in every step of the iteration (30) is computed by knowing the amount of data present in each sensor node. A viable implementation would just need a mapping of the buffer levels to these partitions, using which the controller can compute the aggregate state for any combination of buffer levels. Since this method requires storing of Q-value of the aggregate state-action pair and |S \u2032 \u00d7 A\u2032 | |S \u00d7 A|, the number of Q-values stored is much less compared to the unaggregated Q-learning algorithm. The computational complexity of the method described above is dependent on the size of the aggregate stateaction space and the number of iterations required to converge to an optimal policy (w.r.t the aggregate state-action space). For instance, in the case of four sensor nodes, the size of the state-action space grows to \u2248 309 with the data and energy buffer sizes being 30 each. The number of iterations that the above method requires to find a near-optimal policy is 109 with six partitions of the buffer size as compared to Q-learning without state aggregation (Section 4.2) which requires at least 1011 iterations.\nRemark 8. It must be observed that using (30), the controller decides the partition and not the number of energy bits to be distributed, i.e., it finds an optimal aggregate action for\nevery aggregate state. It follows from this that, in order to find the aggregate action for an aggregate state, the knowledge of the exact buffer levels is not required (since this is based on the Q-values of aggregate state-action pairs). In this manner (30) is beneficial. The optimal policy obtained using (30) would indicate only the energy levels. An added advantage of the above approximation algorithm is that the cost structure discussed in Section 3 holds good here as well."}, {"heading": "5.2.2 Energy distribution", "text": "Note that once an aggregate action is chosen for a state, the energy division is random adhering to the action levels chosen. For instance, lets assume that there are two sensor nodes in the system. Data and energy buffers have three partitions each and thus s = 3, r = 3. Here y1\nL = 0 and y3 U = E MAX . Suppose the number of energy bits in the energy buffer is z and\nthose bits belong to partition 3. Let the number of data bits at nodes 1 and 2 be x and y, respectively. Here x and y belong to partition 2. Hence the aggregate state is (2, 2, 3). The controller decides on the aggregate action (1, 2). Thus x1\nL bits of energy is provided to Node\n1, while Node 2 is given x2 L bits of energy. The remaining number of bits in the buffer will be r = z \u2212 (x1\nL + x2 L ). In order to distribute these bits, the proportions of data p1 = x x+y\nand 1 \u2212 p1 = yx+y are computed. Each of the r bits are supplied to Node 1 with probability p1 and to Node 2 with probability 1\u2212 p1. If u and v represent the total number of energy bits provided to Nodes 1 and 2 respectively, then u \u2264 x1\nU , v \u2264 x2 U and (u\u2212 x1 L ) + (v \u2212 x2 L ) \u2264 r. It\nmust be observed that even though an aggregate action chosen requires knowledge of only the aggregate state, the random distribution of energy (after a control is selected using (30)), is achieved by knowing the exact buffer levels.\nRemark 9. An advantage of using state-aggregation with Q-learning is that it has convergence guarantees (Chapter 6, Section 6.2 [5]). This overcomes the problem of basis selection for function approximation in the case of large state-action spaces. We have tried different partitoning schemes manually and all the schemes resulted in close policy performance. Also, we observed that increasing the number of partitions improves the policy performance (see Fig. 6 in Section 6) ."}, {"heading": "5.3 Cross Entropy using State Aggregation and Policy Parame-", "text": "terization\nThe cross-entropy method is an iterative approach ([35]) that we apply to find near-optimal stationary randomized policies for the energy sharing problem. The algorithm searches for a policy in the space of all stationary randomized policies in a systematic manner. We define a class of randomized stationary policies {\u03c0\u03b8,\u03b8 \u2208 RM}, parameterized by a vector \u03b8. For each pair (s, a) \u2208 S \u2032 \u00d7A\u2032 , \u03c0\u03b8(s, a) denotes the probability of taking action a when the state s is encountered under the policy corresponding to \u03b8. In order to follow the cross entropy approach and obtain the optimal \u03b8\u2217 \u2208 RM , we treat each component \u03b8i, i \u2208 {1, 2, . . . ,M} of \u03b8 as a normal random variable with mean \u00b5i and variance \u03c3i. We will refer to these two\nquantities (the parameters of the normal distribution) as meta-parameters. We will tune the meta-parameters using the cross-entropy update rule (32) to find the best values of \u00b5i and \u03c3i which will correspond to a mean of \u03b8 \u2217 i and a variance of zero. The cross entropy method works as follows: Multiple samples of \u03b8 namely \u03b81,\u03b82, . . . ,\u03b8N are generated according to the normal distribution with the current estimate of the meta-parameters. Each sampled \u03b8 will then correspond to a stationary randomized policy. We compute the average cost \u03bb(\u03b8) of an SRP determined by a sample \u03b8 by running a simulation trajectory with the policy parameter fixed with the sample \u03b8. We perform this average cost computation for all the sampled \u03b8i, i \u2208 {1, 2, . . . , N}, i.e., we compute \u03bb(\u03b81), \u03bb(\u03b82), . . . , \u03bb(\u03b8N). We then update the current estimates of the meta-parameters based on only those sampled \u03b8\u2019s (policies) whose average cost is lower than a threshold level (see (32)).\nRemark 10. The Cross Entropy method is an adaptive importance sampling [9] technique. The specific distribution from which the parameter \u03b8 is sampled is known as the importance sampling distribution. The Gaussian distribution used as the importance sampling distribution yields analytical updation formulas (32) for the mean and variance parameters (see [21]). For this reason, it is convenient to use the Gaussian vectors for the policy parameters."}, {"heading": "5.3.1 Policy Parameterization", "text": "Let \u03bb(\u03b8) be the average cost of the system when parameterized by \u03b8 = (\u03b81, . . . , \u03b8M) >. An optimal policy \u03b8\u2217 minimizes the average cost over all parameterizations. That is,\n\u03b8\u2217 = arg min \u03b8\u2208 RM \u03bb(\u03b8).\nAn example of parameterized randomized policies, which we use for the experiments (involving state aggregation) in this paper are the parameterized Boltzmann policies having the following form:\n\u03c0\u03b8(s, a) = e\u03b8 >\u03c6sa\u2211 b\u2208A(s) e\u03b8 >\u03c6sb\n\u2200s \u2208 S \u2032 , \u2200a \u2208 A\u2032(s), (31)\nwhere \u03c6sa is an M -dimensional feature vector for the aggregated state-action tuple (s, a) and \u03c6sa \u2208 RM . The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.\nRemark 11. The probability distribution over actions is parameterized by \u03b8 in the cross entropy method. Since actions in every state need to be explored, the distribution needs to assign a non-zero probability for every action feasible in a state. Hence the probability distribution must be chosen based on these requirements. The Boltzmann distribution for action selection fits these requirements and is a frequently used distribution in the literature (see [37, 38]) on policy learning and approximation algorithms.\nAs noted in the beginning of this subsection, the parameters \u03b81, . . . , \u03b8M are samples from the distributions N(\u00b5i, \u03c3i), 1 \u2264 i \u2264M , i.e., \u03b8i \u223c N(\u00b5i, \u03c3i), \u2200i."}, {"heading": "5.3.2 Method", "text": "Initially M parameter tuples {(\u00b51i , \u03c31i ), 1 \u2264 i \u2264 M} for the normal distribution are picked. The policy is approximated using the Boltzmann distribution. The method comprises of two phases. In the first phase trajectories corresponding to sample \u03b8s are simulated and the average cost of each policy is computed. The second phase inolves updation of the meta parameters. The algorithm proceeds as follows: Let iteration index t be set to 1. First Phase:\n1. Sample parameters \u03b81, . . . ,\u03b8N are drawn independently from the normal distributions {N(\u00b5ti, \u03c3ti), 1 \u2264 i \u2264M}. For 1 \u2264 j \u2264 N , \u03b8j \u2208 RM\u00d71 and \u03b8 j i is sampled from N(\u00b5 t i, \u03c3 t i).\n2. A trajectory is simulated using probability distribution \u03c0\u03b8 j (s, a), 1 \u2264 j \u2264 N . Hence at\nevery aggregate state s an aggregate action a is picked according to \u03c0\u03b8 j (s, .). Once an aggregate action is chosen for a state, the energy distribution is carried out as described in Section 5.2.2.\n3. The average cost per step of trajectory j is \u03bb(\u03b8j) and is computed for the trajectory simulated using \u03b8j. By abuse of notation we denote \u03bb(\u03b8j) as \u03bbj.\nSecond Phase:\n4. A quantile value \u03c1 \u2208 (0, 1) is selected.\n5. The average cost values are sorted in descending order. Let \u03bb1, . . . , \u03bbN be the sorted order. Hence \u03bb1 \u2265 . . . \u2265 \u03bbN .\n6. The d(1\u2212 \u03c1)eN th average cost is picked as the threshold level. So, let \u03bb\u0302c = \u03bbd(1\u2212\u03c1)eN .\n7. The meta-parameters {(\u00b5ti, \u03c3ti), 1 \u2264 i \u2264 M} are updated (refer [24]) in this phase. In iteration t, the parameters are updated in the second phase in the following manner:\n\u00b5 (t+1) i =\nN\u2211 j=1 I{\u03bbj\u2264\u03bb\u0302c}\u03b8 j i\nN\u2211 j=1 I{\u03bbj\u2264\u03bb\u0302c} ,\n\u03c32 (t+1)\ni =\nN\u2211 j=1 I{\u03bbj\u2264\u03bb\u0302c} ( \u03b8ji \u2212 \u00b5 (t+1) i )2 N\u2211 j=1 I{\u03bbj\u2264\u03bb\u0302c} .\n(32)\n8. Set t = t+ 1 .\nSteps 1-6 are repeated until the variances of the distributions converge to zero. Let \u00b5 = (\u00b51, . . . , \u00b5M)\n> be the vector of means of the converged distributions. The near-optimal SRP found by the algorithm is \u03c0\u0302 where\n\u03c0\u0302(s, a) = e\u00b5\u03c6(s,a)\u2211\nb\u2208A(s) e\u00b5\u03c6(s,b)\n, \u2200s \u2208 S \u2032 , a \u2208 A\u2032(s).\nRemark 12. The computational complexity of the cross entropy method is dependent on the number of updations required to arrive at the optimal parameter vector and the dimension of the vector. For instance in the case of four nodes, with data and energy buffer sizes being 30, the cross entropy method requires 103 sample trajectories for a hyperparameter (\u00b5, \u03c3) vector of dimension 50. The parameter \u03b8 is updated over 103 iterations to arrive at the optimal parameter vector.\nRemark 13. The heuristic cross-entropy algorithm solves hard optimization problems. It is an iterative scheme and requires multiple samples to arrive at the solution. In general one assumes that the parameter \u03b8 is unknown (non-random variable) and uses actor-critic architecture to obtain locally optimal policy. However, obtaining gradient estimates in actorcritic architecture is hard as it leads to large variance [20]. On the other hand, in our work, we let the parameter \u03b8 be a random variable and assume probability distibution over \u03b8 with hyperparameter (\u00b5, \u03c3) and use cross-entropy method to tune the hyperparameters. Cross entropy method is simple to implement, parallelizable and does not require gradient estimates. To the best of our knowledge, we are the first to combine the cross-entropy with state aggregation and apply it to a real world problem. In [23], the authors sampled from the entire transition probability matrix to calculate the score function and tested on problems with only small state-action space."}, {"heading": "6 Simulation Results", "text": "In this section we show simulation results for the energy sharing algorithms we described in Sections 4 and 5. For the sake of comparison we implement the greedy heuristic method in the case when the function g has a non-linear form. Also, we implement Q-learning to learn optimal policies for the case where we consider the sum of the data at all nodes and the available energy as the state. These methods are as follows:\n1. Greedy: This method takes as input the level of data qik at all nodes and supplies the energy based on the requirement. Since g(x) is the number of data bits that can be sent given x bits of allocated energy, g\u22121(y) gives the amount of energy required to send y bits of data. Suppose the energy available in the source is ek at stage k. The\ngreedy algorithm then provides tk units of energy, where tk = min ( ek, n\u2211 i=1 g\u22121(qik) ) . The energy bits are then shared between the nodes based on the proportion of the requirement of the nodes.\n2. Combined Nodes Q-learning: The state considered here is the sum of the data at all nodes and the available energy. Let the state space be Sc and action space be Ac. So\nstate sk = ( n\u2211 i=1 qik, Ek ) . The control specified is tk which is the total energy that needs to be distributed between the nodes. In contrast to the action space in Section 3, here the exact split is not decided upon. Instead, this method finds the total optimal energy to be supplied. The algorithm in Section 4.2 is then used to learn the optimal policies for the state-action space described here.\nIn the above described methods, after an action tk is selected, the proportion of data in the nodes is computed. Thus pi = qik\nn\u2211 j=1 qjk , 1 \u2264 i \u2264 n is computed at time k, where 0 \u2264 pi \u2264 1 and\nn\u2211 i=1 pi = 1. Each of the tk bits of energy is then shared based on these probabilities. Let ui be\nthe number of bits provided to node i. Then in the case of the greedy method, n\u2211 i=1 ui = tk,\nwhile in the combined nodes Q-learning method, n\u2211 i=1 ui \u2264 tk."}, {"heading": "6.1 Experimental Setup", "text": "\u2022 The algorithms described in Section 4 are simulated with two nodes and an energy source. We consider the following settings:\n1. For the case of jointly Markov data arrival and Markovian energy arrival processes, we consider energy buffer size of 20 and data buffer size of 10. The data arrivals evolve as: Xk = AXk\u22121 + \u03c9, where A is a fixed 2 \u00d7 2 matrix of coefficients and \u03c9 = (\u03c91, \u03c92) > is a 2\u00d71 random noise (or disturbance) vector. Here A = ( 0.2 0.3 0.3 0.2\n) The energy arrival evolves as Yk = bYk\u22121 + \u03c7, where \u03c7 is also random noise (or disturbance) variable and b = 0.5 is a fixed coefficient. The components in vector \u03c9 and \u03c7 are Poisson distributed. In the simulations, we vary the mean of the random noise variable \u03c91, while means of \u03c92, \u03c7 are kept constant.\n2. For the case of i.i.d data and energy arrivals the data and energy buffer sizes are fixed at 14. X1, X2 and Y are distributed according to the Poisson distribution. In the simulations, the mean data arrival at node two is fixed while that at node one is varied.\n\u2022 The algorithms described in Section 5 are simulated with four nodes and an energy source. We consider the following settings:\n1. For the case of jointly Markov data arrival and Markovian energy arrival processes, we consider energy buffer size of 25 and data buffer size of 10. The data arrivals evolve as: Xk = AXk\u22121 + \u03c9, where A is a fixed 4 \u00d7 4 matrix of coefficients and \u03c9 = (\u03c91, \u03c92, \u03c93, \u03c94) > is a 4\u00d7 1 random noise (or disturbance) vector. Here\nA = ( 0.1 0.1 0.1 0.2 0.1 0.1 0.2 0.1 0.1 0.2 0.1 0.1 0.2 0.1 0.1 0.1 ) The energy arrival evolves as Yk = bYk\u22121 + \u03c7, where \u03c7 is also random noise (or disturbance) variable and b = 0.5 is a fixed coefficient. The components in vector \u03c9 and \u03c7 are Poisson distributed. In the simulations, we vary the mean of the random noise variable \u03c91, while means of \u03c92-\u03c94, \u03c7 are kept constant. The energy buffer had 4 partitions while the data buffer had 2 partitions.\n2. For the case of i.i.d data and energy arrivals, the buffer sizes are taken to be 30 each. The data and energy buffers are clustered into 6 partitions. X1, X2, X3, X4, Y are Poisson distributed. In these experiments, the mean data arrivals at nodes 2, 3 and 4 are fixed while the same at node 1 is varied.\nFor all Q-learning algorithms ( -greedy, UCB based and Combined Nodes), stepsize \u03b1 = 0.1 is used in the updation scheme. For the -greedy method, = 0.1 is used for exploration. In the UCB exploration mechanism, the value of \u03b2 is set to 1. In our experimental simulations, we consider the function g(x) = ln(1 + x) for the i.i.d case and g(x) = 2 ln(1 + x) for the non-i.i.d case."}, {"heading": "6.2 Results", "text": "Figs. 3, 4a, 4b, 5 and 9a show the performance of the algorithms explained in Section 4. The simulations are carried out with two nodes and a single source. Similarly, Figs. 6, 7a and 7b show the performance comparisons of our algorithms explained in Section 5 with other algorithms. The simulations in this case are carried out with four nodes and a single source. In Figs. 3, 6 jointly Markov data arrival and Markovian energy arrival processes are considered and the noise in data and energy arrival at Node 1, i.e. E[\u03c91] is varied while that at the other nodes is kept constant. The i.i.d case of data and energy arrivals is considered in Figs. 4a,4b, 5, 7a and 7b. In these plots, the mean data arrival at Node 1 (E[X1]) is varied while keeping that at the other node(s) constant. Figs. 3-9b show the normalized long-run average cost of the policies determined by the algorithms along the y-axis. The mean energy arrival is also fixed.\nThe Q-learning algorithm is designed to learn optimal policies, hence it outperforms other algorithms, as shown in Figs. 3,4a, 4b, 5 and 9a. The policy learnt by our algorithm does better compared to the greedy policy and the policy obtained from the combined nodes Q-learning method. Note that Q-learning on combined nodes learns the total energy to be distributed and not the exact split. Hence its performance is poor compared to Q-learning on our problem MDP. Thus, sharing energy by considering the total amount of data in all the nodes is not optimal.\nFigs. 7a and 7b show the long-run normalized average costs of the policies obtained from the Greedy method and the algorithms described in Section 5. Since our algorithms are model-free, irrespective of the distributions of energy and data arrival (see Figs. 7a and 7b), our algorithms learn the optimal or near-optimal policies. These plots show that our approximation algorithms outperform the greedy and combined nodes Q-learning methods.\nIt can be observed that the gap between the average costs obtained from the combined nodes Q-learning method and the approximate learning algorithm (see Section 5.2) increases with an increase in the number of nodes. This is clear from Figs. 4a and 7a. This occurs because the combined nodes Q-learning method wastes energy and the amount of wastage increases with an increase in the number of nodes.\nFig. 8 shows the variation in average cost with different number of partitions of data and energy buffers used in state aggregation. As the number of partitions increase, the number of clusters also increase resulting in better policies.\nThe single-stage cost function as defined in (11), includes the effect of action in the conversion function g(\u00b7). The effect of the action taken can be explicitly included in the single-stage cost function of the following form:\nc(sk, T (sk)) = n\u2211 i=1 (r1 \u2217 (qik \u2212 g(T i(sk)))+ + r2 \u2217 T i(sk)), (33)\nwhere r1, r2 are the tradeoff parameters, r1 + r2 = 1 and r1, r2 \u2265 0. The above equation is a convex combination of the sum of data queue lengths and the collective energy supplied to the nodes. It can be observed that the single-stage cost function (11) used in our MDP model can be derived from (33) by taking r1 = 1 and r2 = 0. When r1 > 0 and r2 > 0, the cost structure (33) gives importance to the data queue length as well as the amount of energy supplied. The performance comparison of our algorithms (described in Section 4) with the greedy and combined nodes Q-learning methods using this single-stage cost function is shown in Fig. 9a. For the simulations, buffer sizes are fixed at 14 and X1, X2, Y are distributed according to the Poisson distribution with E[Y ] = 13 and E[X2] = 1.0.\nIn Fig. 9a, the x-axis indicates the change in data rate of Node 1. This setup is akin to that used in Fig. 4a. The y-axis indicates the normalized average queue length of all the nodes. We considered values r1 = 0.7 and r2 = 0.3. The plot indicates only the average queue length of all nodes, since our objective is to minimize the average delay of transmission of data (which is related to the data queue length). From Fig. 9a, it can be observed that all learning algorithms show an increase in the collective average queue length (referred to as the normalized average cost in Figs. 4a-8). This occurs because by using the cost function (33) the learning algorithms (Q-learning with UCB and -greedy exploration as well as combined nodes Q-learning) give less importance to the queue length component in the cost function. Thus the policies learnt by these algorithms minimize the energy usage albeit with an increase in data queue length. As the figure shows, the learning algorithms we described in Section 4 perform much better compared to the greedy and combined nodes methods.\nIn Fig. 9b, the performance comparison of Q-learning with and without state aggregation is shown for the case of two nodes and an EH source (i.i.d case) and compared with greedy and combined nodes Q-learning mathod. The -greedy exploration mechanism is used for both algorithms. The experimental setup is similar to that used in Fig. 4a. The x-axis indicates the variation in data rate of Node 1, while the y-axis indicates the normalized average cost of the nodes. The algorithm in Section 5.2 was simulated by partitioning the data and energy buffers into 3 partitions each. It can be observed in Fig. 9b that Q-learning with state aggregation performs better than the greedy and combined nodes methods. However since Q-learning with state aggregation algorithm finds near-optimal policy, its performance is not as good as the algorithm in Section 4.2 with the same exploration mechanism.\nRemark 14. The Greedy algorithm distributes the available energy among the sensor nodes\nbased on the proportion of data available in the nodes. It shares all the available energy at every decision instant without storing it for future use. We compare our algorithms with the Greedy algorithm in order to show that myopic strategy may not be optimal. Our results show that one has to devise the policy not only for the present requirement for energy but also for the future energy requirements as well. This idea is naturally incorporated in our RL algorithms. Moreover, Greedy policy is optimal when the conversion function g is linear. This has been derived in [36] for the case of single sensor. The performance of the algorithm proposed in [30] with non-linear g is compared with the performance of the greedy method. Thus, the comparison of the performance of our algorithms with the greedy method also follows naturally from the earlier cited works.\nThe Combined Nodes Q-learning method learns the policy which maps the total number of data bits available in all the nodes to the total amount of energy required. The energy sharing between the nodes is then based on the proportion of data available in the nodes. Under the Combined Nodes Q-learning algorithm, the state space is greatly reduced, i.e., instead of the cartesian product of states in each node (as in our Q-learning method with and without state aggregation), it is just the sum of the states of the sensor nodes. So, the learning is faster in combined nodes Q-learning algorithm. However, the policy learnt is suboptimal as was shown in Figs. 4a-9b and performs poorly in comparison with our algorithms. So, we compare our algorithms with Combined nodes Q-learning to illustrate the tradeoff of size of the state space with the nature of the obtained policy.\nNote that our RL algorithms learn the energy sharing policy not quantized to a single point but considers energy sharing among the sensor nodes. Learning an optimal energy sharing scheme is a difficult problem. Hence, we would like to understand how well our algorithms perform against a simple heuristic policy such as Greedy or a policy obtained from the Combined nodes Q-learning method.\nRemark 15. The function g(.) gives the number of bits that can be transmitted using certain units of energy. Our algorithms work regardless of the forms of g. RL algorithms use the simulation samples to learn the energy sharing policy by trying out various actions in each of the states. In our problem, at time k let us assume we are in state sk = (q 1 k, q 2 k, . . . , q n k , Ek, Xk\u22121, Yk\u22121), i.e., the data in the data buffer and energy in the energy buffer are fixed to some values. Based on the current Q-value, we share the energy available to the various sensor nodes by selecting action Tk = (T 1 k , T 2 k , . . . , T n k ). Depending on the action Tk, the state of the system evolves according to (1)-(2). In order to find the next state of the system ( (1) - (2)), it suffices to know the number of bits that got transmitted by chosing the action Tk in slot k in a real system, which is given by g(Tk). It must be noted that we do not need information on the functional form of g for finding the next state, but only the value of the function for action Tk. This value can be observed (in a real system) even if we do not have the precise model for the Gaussian function in terms of g(\u00b7). In other words, all we need is to observe the number of bits that got transmitted by supplying Tk units of energy.\nTo update the Q-value of state-action pair (sk, Tk) (see (21)), we need to know the cost c(sk, Tk) incurred by choosing action Tk in state sk, which is computed using (11), where again we only require information on g(T ik), i = 1, 2, . . . n, but not the exact form of g(\u00b7). Our proposed RL algorithms work by updating Q values, and such an updation essentially requires the cost information (computed using (11)). Similarly in the cross entropy method, to compute the average cost of the policy, we need to compute the single-stage cost (using (11)). In summary, our algorithms do not require the exact form of g(\u00b7).\nIn the case of the greedy algorithm, in order to decide the number of energy units Tk that need to be shared, the function g\u22121(\u00b7) and hence the functional form of g(\u00b7) must be known (see Section 6), i.e., one needs to obtain the mathematical model for the conversion function. In comparison, as stated before, our algorithms do not need such information.\nHowever, to simulate the environment, we need to know the functional form of the conversion function g. But, in a real physical system, our algorihms do not require the functional form of g. Figure 10 illustrates the performance of our algorithms and the Greedy and Combined nodes Q-learning methods for a different form of function g(\u00b7), i.e., g(\u00b7) = \u221a 3 log(1 + x). The setup is similar to that of Fig. 3. We observe from Fig. 10 that irrespective of the form of g(\u00b7), our algorithms find good policies, since they do not require this knowledge to do so."}, {"heading": "7 Conclusions and Future Work", "text": "We studied the problem of energy sharing in sensor networks and proposed a new technique to manage energy available through harvesting. Multiple nodes in the network sense random amounts of data and share the energy harvested by an energy harvesting source. We presented an MDP model for this problem and an algorithm that determines the optimal amount of energy to be supplied to every node at a decision instant. The algorithm minimizes the sum of (data) queue lengths in the data buffers, by finding the optimal energy split profile. In order to deal with the curse of dimensionality, we also proposed approximation algorithms that employ state aggregation effectively to reduce the computational complexity. Numerical experiments showed that our algorithms outperform the algorithms described in Section 6.\nOur future work would involve applying threshold tuning for state aggregation, gradient based approaches and basis adaptation methods for policy approximation. The partitions formed for clustering the state space (Section 5.1) can be improved by tuning the partition thresholds (see [31]). This method can be employed to obtain improved deterministic policies when state-action space is extremely large. Gradient based methods [6], [20], [8] approximate the policy using parameter \u03b8 and a set of given (fixed) basis functions {fk : 1 \u2264 k \u2264 n}. Typically a probability distribution over the actions corresponding to a state is defined using \u03b8 and {fk}. The parameter is updated using the gradient direction of the policy performance, which is usually the long-run average or discounted cost of the policy. In the approximation algorithm described in Section 5.3, the basis functions used in the policy parameterization are fixed. One could obtain better policies if the basis functions are also optimized. Basis adaptation methods [24], [7] start with a given set of basis functions. The random policy parameter \u03b8 is updated using simulated trajectories of the MDP on a faster timescale. The basis functions are tuned on a slower timescale. These methods can be employed to find better policies. We shall also develop prototype implementations for this model and test our algorithms."}, {"heading": "Acknowledgements", "text": "The authors would like to thank all the three reviewers of [29] for their detailed comments that significantly helped in improving the quality of this report and the manuscript [29]. This work was supported in part through projects from the Defence Research and Development Organisation (DRDO) and the Department of Science and Technology (DST), Government of India."}], "references": [{"title": "Learning algorithms for markov decision processes with average cost", "author": ["Jinane Abounadi", "D Bertsekas", "Vivek S Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Transmit power control policies for energy harvesting sensors with retransmissions", "author": ["Anup Aprem", "Chandra R Murthy", "Neelesh B Mehta"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Dynamic programming and optimal control, Vol. I", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Dynamic Programming and Optimal Control, Vol. II", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Stochastic Recursive Algorithms for Optimization, volume 434 of Lecture Notes in Control and Information Sciences", "author": ["S Bhatnagar", "H L Prasad", "L A Prashanth"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Feature search in the grassmanian in online reinforcement learning", "author": ["Shalabh Bhatnagar", "Vivek S Borkar", "K J Prabuchandran"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Introduction to rare event", "author": ["James Bucklew"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Power allocation strategies in energy harvesting wireless cooperative networks", "author": ["Zhiguo Ding", "S.M. Perlaza", "I Esnaola", "H.V. Poor"], "venue": "Wireless Communications, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Optimal throughput-delay scaling in wireless networks - part i: the fluid model", "author": ["A El Gamal", "J. Mammen", "B. Prabhakar", "D. Shah"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Learning rates for Q-learning", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Ultra-low-power rfid-based sensor mote", "author": ["Nicolas Gay", "W Fischer"], "venue": "In Sensors, 2010 IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Power constrained and delay optimal policies for scheduling transmission over a fading channel", "author": ["Munish Goyal", "Anurag Kumar", "Vinod Sharma"], "venue": "Twenty- Second Annual Joint Conference of the IEEE Computer and Communications. IEEE Societies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Energy cooperation in energy harvesting communications", "author": ["B. Gurakan", "O. Ozel", "Jing Yang", "S. Ulukus"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Optimal energy allocation for wireless communications powered by energy harvesters", "author": ["Chin Keong Ho", "Rui Zhang"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Power management in energy harvesting sensor networks", "author": ["Aman Kansal", "Jason Hsu", "Sadaf Zahedi", "Mani B Srivastava"], "venue": "ACM Transactions on Embedded Computing Systems (TECS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An environmental energy harvesting framework for sensor networks", "author": ["Aman Kansal", "Mani B Srivastava"], "venue": "In Low Power Electronics and Design,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "On actor-critic algorithms", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "The cross-entropy method for continuous multi-extremal optimization", "author": ["Dirk P Kroese", "Sergey Porotsky", "Reuven Y Rubinstein"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Power-optimal scheduling for a green base station with delay constraints", "author": ["Anusha Lalitha", "Santanu Mondal", "Vinod Sharma"], "venue": "In Communications (NCC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The cross entropy method for fast policy search", "author": ["Shie Mannor", "Reuven Y Rubinstein", "Yohai Gat"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["Ishai Menache", "Shie Mannor", "Nahum Shimkin"], "venue": "Annals of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Wireless sensor networks with energy harvesting technologies: a game-theoretic approach to optimal energy management", "author": ["Dusit Niyato", "Ekram Hossain", "Mohammad M Rashid", "Vijay K Bhargava"], "venue": "Wireless Communications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Transmission with energy harvesting nodes in fading wireless channels: Optimal policies", "author": ["O. Ozel", "K. Tutuncuoglu", "Jing Yang", "Sennur Ulukus", "A Yener"], "venue": "Selected Areas in Communications, IEEE Journal on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Adaptive transmission policies for energy harvesting wireless nodes in fading channels", "author": ["Omur Ozel", "Kaya Tutuncuoglu", "Jing Yang", "Sennur Ulukus", "Aylin Yener"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Optimal broadcast scheduling for an energy harvesting rechargeable transmitter with a finite capacity battery", "author": ["Omur Ozel", "Jing Yang", "Sennur Ulukus"], "venue": "Wireless Communications, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Energy sharing for multiple sensor nodes with finite buffers", "author": ["Sindhu Padakandla", "K J Prabuchandran", "Shalabh Bhatnagar"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Q-learning based energy management policies for a single sensor node with finite buffer", "author": ["K J Prabuchandran", "Sunil Kumar Meena", "Shalabh Bhatnagar"], "venue": "Wireless Communications Letters, IEEE,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Threshold tuning using stochastic optimization for graded signal control", "author": ["L.A. Prashanth", "S. Bhatnagar"], "venue": "Vehicular Technology, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "Optimal periodic sensor schedule for steady-state estimation under average transmission energy constraint", "author": ["Zhu Ren", "Peng Cheng", "Jiming Chen", "Ling Shi", "Youxian Sun"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Dynamic sensor transmission power scheduling for remote state estimation", "author": ["Zhu Ren", "Peng Cheng", "Jiming Chen", "Ling Shi", "Huanshui Zhang"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "The cross-entropy method for combinatorial and continuous optimization", "author": ["Reuven Rubinstein"], "venue": "Methodology and computing in applied probability,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Optimal energy management policies for energy harvesting sensor nodes", "author": ["Vinod Sharma", "Utpal Mukherji", "Vinay Joseph", "Shrey Gupta"], "venue": "IEEE Transactions on Wireless Communications,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Asynchronous stochastic approximation and q-learning", "author": ["John N Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1994}, {"title": "Short-term throughput maximization for battery limited energy harvesting nodes", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "In Communications (ICC),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Sum-rate optimal power policies for energy harvesting transmitters in an interference channel", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "Communications and Networks, Journal of,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Cooperative energy harvesting communications with relaying and energy sharing", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "In Information Theory Workshop (ITW),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Transmission completion time minimization in an energy harvesting system", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Optimal packet scheduling in a multiple access channel with energy harvesting transmitters", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "Communications and Networks, Journal of,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Optimal packet scheduling in an energy harvesting communication system", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "1) is a single unit on which sensors with different functionalities are arranged (see [13]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "Another scenario (that however we do not consider here) where our techniques are applicable is the case of downlink transmissions [22], where a base station (BS) maintains a separate data queue for each individual sensor node.", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Optimizing energy usage in battery-powered sensors is addressed in [33, 34].", "startOffset": 67, "endOffset": 75}, {"referenceID": 31, "context": "Optimizing energy usage in battery-powered sensors is addressed in [33, 34].", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "The problem of designing appropriate sensor schedules of sensor data transmission is discussed in [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "The authors in [33] consider battery-powered sensor nodes, each of which needs to minimize the energy utilized for data transmission.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "The objective in [33] is to find optimal periodic sensor schedules which minimize the estimation error at the fusion node and optimize energy usage.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "In [34], the authors consider battery-powered sensors with two transmission power levels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "The objective in [34] is to minimize the average expected error in state estimation under energy constraint.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "The works [33, 34] consider the problem of efficient energy usage in battery powered sensors.", "startOffset": 10, "endOffset": 18}, {"referenceID": 31, "context": "The works [33, 34] consider the problem of efficient energy usage in battery powered sensors.", "startOffset": 10, "endOffset": 18}, {"referenceID": 15, "context": "An early work in rechargeable sensors is [18].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "The authors of [18] present a framework for the sensor network to adaptively learn the spatio-temporal characteristics of energy availability and provide algorithms to use this information for task sharing among nodes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In [17], the irregular and spatio-temporal characteristics of harvested energy are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Compared to [18, 17], we focus on minimizing the delay in data transmission from the nodes and also ensuring energy neutral operation.", "startOffset": 12, "endOffset": 20}, {"referenceID": 14, "context": "Compared to [18, 17], we focus on minimizing the delay in data transmission from the nodes and also ensuring energy neutral operation.", "startOffset": 12, "endOffset": 20}, {"referenceID": 37, "context": "The scenario of a single EH transmitter with limited battery capacity is considered in [41, 26].", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "The scenario of a single EH transmitter with limited battery capacity is considered in [41, 26].", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "In [26], the transmitter communicates in a fading channel, whereas in [41], no specific constraints on the channel are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [26], the transmitter communicates in a fading channel, whereas in [41], no specific constraints on the channel are considered.", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "The problem of finding the optimal transmission policy to maximize the short-term throughput of an EH transmitter is considered in [41].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "In [26], the transmitter gets channel state information and the node has to adaptively control the transmission rate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "The authors in [26] develop an online algorithm which determines the transmit power at every instant by taking into account the amount of energy available and channel state.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 33, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 27, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 42, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 22, "context": "A channel and data queue aware sleep/active/listen mechanism in this direction is proposed in [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "The model proposed in [36, 30] considers a single EH sensor node with finite energy and data buffers.", "startOffset": 22, "endOffset": 30}, {"referenceID": 27, "context": "The model proposed in [36, 30] considers a single EH sensor node with finite energy and data buffers.", "startOffset": 22, "endOffset": 30}, {"referenceID": 33, "context": "In [36], a linear conversion function is used and optimal energy management policies are provided for the same.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "However, in the case of non-linear conversion function, [36] provides certain heuristic policies.", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "In [30], a nonlinear conversion function is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 23, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 33, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 22, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 27, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 42, "context": "Data packet scheduling problems in EH sensor networks are considered in [46] and [45].", "startOffset": 72, "endOffset": 76}, {"referenceID": 41, "context": "Data packet scheduling problems in EH sensor networks are considered in [46] and [45].", "startOffset": 81, "endOffset": 85}, {"referenceID": 42, "context": "It is assumed in [46] that a single EH node has separate data and energy queues, while the data sensed and energy harvested are random.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "The same assumption is made for each sensor in a two-sensor communication system considered in [45].", "startOffset": 95, "endOffset": 99}, {"referenceID": 42, "context": "In [46]([45]) the objective is to minimize the time by which all data packets from the node(s) are transmitted (to the fusion node).", "startOffset": 3, "endOffset": 7}, {"referenceID": 41, "context": "In [46]([45]) the objective is to minimize the time by which all data packets from the node(s) are transmitted (to the fusion node).", "startOffset": 8, "endOffset": 12}, {"referenceID": 38, "context": "A two-user Gaussian interference channel with two EH sensor nodes and receivers is considered in [42].", "startOffset": 97, "endOffset": 101}, {"referenceID": 42, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 41, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 38, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 7, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 12, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 39, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 12, "context": "Three different network settings with energy transfer between nodes are considered in [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "In [43], there exists an EH relay node and multiple other EH source nodes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [10], multiple pairs of sources and destinations communicate via an EH relay node.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "A multi-user additive white Gaussian noise (AWGN) broadcast channel comprising of a single EH transmitter and M receivers is considered in [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 25, "context": "The objective in [28] is to find a transmission policy that minimizes the time by which all the bits are transmitted to the receivers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "\u2022 We model the above problem as an infinite-horizon average cost Markov decision process (MDP) [4],[32] with an appropriate single-stage cost function.", "startOffset": 95, "endOffset": 98}, {"referenceID": 29, "context": "\u2022 We model the above problem as an infinite-horizon average cost Markov decision process (MDP) [4],[32] with an appropriate single-stage cost function.", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "We assume fragmentation of data packets (fluid model) as in [36] and hence these will be treated as bit strings.", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 40, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 13, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 24, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 11, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 2, "context": "This is the standard description for the state evolution for an MDP (see Chapter 1 in [3]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "This is the fluid model as described in [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 11, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 42, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 1, "context": "Energy discretization has been considered in some previous works [2, 36].", "startOffset": 65, "endOffset": 72}, {"referenceID": 33, "context": "Energy discretization has been considered in some previous works [2, 36].", "startOffset": 65, "endOffset": 72}, {"referenceID": 0, "context": "Then \u2200k \u2265 0, the Q-learning update [1] for a state-action pair visited during simulation is carried out as follows:", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "These conditions on step sizes guarantee the convergence of Q-learning to the optimal state-action value function, see [1] for a proof of convergence of the algorithm.", "startOffset": 119, "endOffset": 122}, {"referenceID": 36, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 16, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 9, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 16, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 9, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 29, "context": "(29) As a consequence of the relative value iteration scheme ([32]), when k \u2192 \u221e, LH \u2192 H\u2217 with H(qr, Er) = \u03bb \u2217.", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "In the case of MDP with large state-action space, one goes for function approximation based methods (see Chapter 8 in [37]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "However, if one combines Q-learning with function approximation, we do not have convergence guarantees to the optimal policy unlike Q-learning without function approximation (Q-learning with tabular representation [37]).", "startOffset": 214, "endOffset": 218}, {"referenceID": 32, "context": "The cross-entropy method is an iterative approach ([35]) that we apply to find near-optimal stationary randomized policies for the energy sharing problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "The Cross Entropy method is an adaptive importance sampling [9] technique.", "startOffset": 60, "endOffset": 63}, {"referenceID": 18, "context": "The Gaussian distribution used as the importance sampling distribution yields analytical updation formulas (32) for the mean and variance parameters (see [21]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 0, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 34, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 35, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 34, "context": "The Boltzmann distribution for action selection fits these requirements and is a frequently used distribution in the literature (see [37, 38]) on policy learning and approximation algorithms.", "startOffset": 133, "endOffset": 141}, {"referenceID": 35, "context": "The Boltzmann distribution for action selection fits these requirements and is a frequently used distribution in the literature (see [37, 38]) on policy learning and approximation algorithms.", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "The meta-parameters {(\u03bci, \u03c3 i), 1 \u2264 i \u2264 M} are updated (refer [24]) in this phase.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "However, obtaining gradient estimates in actorcritic architecture is hard as it leads to large variance [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In [23], the authors sampled from the entire transition probability matrix to calculate the score function and tested on problems with only small state-action space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "This has been derived in [36] for the case of single sensor.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "The performance of the algorithm proposed in [30] with non-linear g is compared with the performance of the greedy method.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "1) can be improved by tuning the partition thresholds (see [31]).", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Gradient based methods [6], [20], [8] approximate the policy using parameter \u03b8 and a set of given (fixed) basis functions {fk : 1 \u2264 k \u2264 n}.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "Gradient based methods [6], [20], [8] approximate the policy using parameter \u03b8 and a set of given (fixed) basis functions {fk : 1 \u2264 k \u2264 n}.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Basis adaptation methods [24], [7] start with a given set of basis functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "Basis adaptation methods [24], [7] start with a given set of basis functions.", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "The authors would like to thank all the three reviewers of [29] for their detailed comments that significantly helped in improving the quality of this report and the manuscript [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "The authors would like to thank all the three reviewers of [29] for their detailed comments that significantly helped in improving the quality of this report and the manuscript [29].", "startOffset": 177, "endOffset": 181}], "year": 2015, "abstractText": "We consider the problem of finding optimal energy sharing policies that maximize the network performance of a system comprising of multiple sensor nodes and a single energy harvesting (EH) source. Sensor nodes periodically sense the random field and generate data, which is stored in the corresponding data queues. The EH source harnesses energy from ambient energy sources and the generated energy is stored in an energy buffer. Sensor nodes receive energy for data transmission from the EH source. The EH source has to efficiently share the stored energy among the nodes in order to minimize the long-run average delay in data transmission. We formulate the problem of energy sharing between the nodes in the framework of average cost infinite-horizon Markov decision processes (MDPs). We develop efficient energy sharing algorithms, namely Q-learning algorithm with exploration mechanisms based on the -greedy method as well as upper confidence bound (UCB). We extend these algorithms by incorporating state and action space aggregation to tackle state-action space explosion in the MDP. We also develop a cross entropy based method that incorporates policy parameterization in order to find near optimal energy sharing policies. Through simulations, we show that our algorithms yield energy sharing policies that outperform the heuristic greedy method.", "creator": "LaTeX with hyperref package"}}}