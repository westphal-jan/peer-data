{"id": "1503.04996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications", "abstract": "random forest ( rf ) is widely an ensemble supervised machine learning technique that was developed by breiman over a century decade ago. compared with other hierarchical ensemble techniques, it has proved its accuracy promise and superiority. many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. this explains why, over the past decade, obviously there have been many extensions of rf where each extension employed a variety of techniques and strategies to improve certain aspect ( s ) of rf. since it has been proven empiricallthat specific ensembles these tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. first, it investigates how data clustering ( a well known diversity technique ) can be applied to identify groups of similar decision trees in an rf in order to eliminate redundant trees by selecting a representative from each group ( cluster ). second, these likely diverse representatives are then used to produce an extension of synthetic rf termed club - drf that is much smaller in size than rf, and yet performs at least as good as rf, and mostly exhibits higher performance in terms of accuracy. the terms latter refers to a known technique called ensemble pruning. experimental results on 15 real datasets from the uci repository prove the superiority of our proposed extension over the traditional rf. most of our experiments achieved at least 95 % or above pruning level while retaining flaws or outperforming the rf accuracy.", "histories": [["v1", "Tue, 17 Mar 2015 11:01:37 GMT  (244kb,D)", "http://arxiv.org/abs/1503.04996v1", "10 pages, 4 Figures"]], "COMMENTS": "10 pages, 4 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["khaled fawagreh", "mohamad medhat gaber", "eyad elyan"], "accepted": false, "id": "1503.04996"}, "pdf": {"name": "1503.04996.pdf", "metadata": {"source": "CRF", "title": "On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications", "authors": ["Khaled Fawagreh", "Mohamed Medhat Gaber", "Eyad Elyan"], "emails": ["(k.fawagreh@rgu.ac.uk,", "m.gaber1@rgu.ac.uk,", "e.elyan@rgu.ac.uk)"], "sections": [{"heading": null, "text": "Index Terms\u2014Random Forest, Ensemble Pruning, Clustering, Diversity\nI. INTRODUCTION\nENsemble classification is an application of ensemblelearning to boost the accuracy of classification. Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3]. Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2]. In such an ensemble, multiple classifiers are used. In its basic mechanism, majority voting is then used to determine the class label for unlabeled instances where each classifier in the ensemble is asked to predict the class label of the instance being considered. Once all the classifiers have been queried, the class that receives the greatest number of votes is returned as the final decision of the ensemble.\nThree widely used ensemble approaches could be identified, namely, boosting, bagging, and stacking. Boosting is an incremental process of building a sequence of classifiers, where each classifier works on the incorrectly classified instances of\nK. Fawagreh, M. M. Gaber, and E. Elyan are with IDEAS Research Institute, Robert Gordon University, Riverside East, Garthdee Road, Aberdeen AB10 7GJ, UK (k.fawagreh@rgu.ac.uk, m.gaber1@rgu.ac.uk, e.elyan@rgu.ac.uk)\nthe previous one in the sequence. AdaBoost [6] is the representative of this class of techniques. However, AdaBoost is prone to overfitting. The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7]. Bagging involves building each classifier in the ensemble using a randomly drawn sample of the data, having each classifier giving an equal vote when labeling unlabeled instances. Bagging is known to be more robust than boosting against model overfitting. Random Forest (RF) is the main representative of bagging [8]. Stacking (sometimes called stacked generalization) extends the crossvalidation technique that partitions the dataset into a held-in dataset and a held-out dataset; training the models on the heldin data; and then choosing whichever of those trained models performs best on the held-out data. Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9]. Stacking has been successfully used on both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].\nThe ensemble method that is relevant to our work in this paper is RF. RF has been proved to be the state-of-the-art ensemble classification technique. Since RF algorithms typically build between 100 and 500 trees [12], in real-time applications, it is of paramount importance to reduce the number of trees participating in majority voting and yet achieve performance that is at least as good as the original ensemble. In this paper, we propose a data clustering approach to prune RF ensembles where only a small subset of the ensemble is selected. We cluster trees according to their classification behavior on a subset of the dataset. Then we choose only one tree from each cluster, motivated by the fact that the tree is a representative of its cluster. At voting time, the number of voting trees is reduced significantly yielding classification accuracy at least as good as all voting trees. A thorough experimental study is conducted, with multiple of 5 clusters ranging from 5 through to 40, over an RF of 500 trees. Results show the potential of our technique for deployment in real-time systems, yielding higher predictive accuracy than traditional RF, with 17 to 100 times faster classification per instance resulting from pruning levels in the range of 94%\u201399%.\nThis paper is organized as follows. First we discuss related work in Section II. This is followed by Section III where preliminaries about the motivation and introduction to RF are covered. In Section IV, we present the clustering technique that will be utilized in our approach. Section V formalizes our approach and corresponding algorithm. Experiments, results and analysis demonstrating the superiority of the proposed pruned RF over the traditional RF are detailed in Section VI.\n3 The paper is finally concluded with a summary and pointers to future directions in Section VII."}, {"heading": "II. RELATED WORK", "text": "Several enhancements have been made in recent years in order to produce a subset of an ensemble that performs as well as, or better than, the original ensemble. The purpose of ensemble pruning is to search for such a good subset. This is particularly useful for large ensembles that require extra memory usage, computational costs, and occasional decreases in effectiveness. Grigorios et al. [13] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others. Clustering based methods, that are relevant to us in this paper, consist of two stages. In the first stage, a clustering algorithm is employed in order to discover groups of models that make similar predictions. Pruning each cluster then follows in the final stage. In this stage, several approaches have been used. One approach by [14] was to train a new model for each cluster, using the cluster centroids as values of the target variable. Another interesting approach was proposed by [15] that involved selecting from each cluster the classifier that is most distant to the rest of the clusters. A yet different approach by [16] that does not guarantee the selection of a single model from each cluster was by iteratively removing models from the least to the most accurate, until the accuracy of the entire ensemble starts to decrease. Selecting the most accurate model from each cluster was proposed by [17].\nIt is worth pointing out that there are several techniques available in the literature that use clustering-based approaches to reduce the number of trees in the ensemble (see [18] for a good review). Unlike such techniques, our clustering approach in this paper is different in many aspects. First of all, to the best of our knowledge, none of the clustering-based approaches was developed for RF ensembles. For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles. Furthermore, our approach differs from theirs in two other ways. For the selection of a representative from each cluster, we have selected the best performing representative on the out-of-bag (OOB) instances and this selection method was not used by anyone of them. As will be discussed in Section V, using OOB samples to evaluate a tree gives an unbiased estimate of its predictive accuracy since, unlike about 64% of the training data that was seen by the tree when it was built, OOB data was not seen and therefore, it is a more accurate measure of the tree\u2019s predictive accuracy. Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].\nRecent work in ensemble pruning just this year (2014) alone was reported. Without a significant loss of prediction accuracy, a combination of static and dynamic pruning techniques were applied on Adaboost ensembles in order to yield less memory consumption and improved classification speed [19]. A pruning scheme for high dimensional and large sized\nbenchmark datasets was developed by [20], In such a scheme, an extended feature selection technique was used to transform ensemble predictions into training samples, where classifiers were treated as features. Then a global heuristic harmony search was used to select a smaller subset of such artificial features."}, {"heading": "A. Diversity Creation Methods", "text": "Because of the vital role diversity plays on the performance of ensembles, it had received a lot of attention from the research community. G. Brown et al. [21] summarized the work done to date in this domain from two main perspectives. The first is a review of the various attempts that were made to provide a formal foundation of diversity. The second, which is more relevant to this paper, is a survey of the various techniques to produce diverse ensembles. For the latter, two types of diversity methods were identified: implicit and explicit. While implicit methods tend to use randomness to generate diverse trajectories in the hypothesis space, explicit methods, on the other hand, choose different paths in the space deterministically. In light of these definitions, bagging and boosting in the previous section are classified as implicit and explicit respectively.\nG. Brown et al. [21] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data. Methods in the first category use different starting points in the hypothesis space, therefore, influencing the convergence place within the space. Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5]. Methods in the second category vary the set of hypotheses that are available and accessible by the ensemble. For different ensembles, these methods vary either the training data used or the architecture employed. In the third category, the methods alter the way space is traversed. Occupying any point in the search space, gives a particular hypothesis. The type of the ensemble obtained will be determined by how the space of the possible hypotheses is traversed."}, {"heading": "B. Diversity Measures", "text": "Regardless of the diversity creation technique used, diversity measures were developed to measure the diversity of a certain technique or perhaps to compare the diversity of two techniques. Tang et al. [22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26]. The goal was not only to show the underlying relationships between them, but also to relate them to the concept of margin, which is one of the contributing factors to the success of ensemble learning algorithms.\nWe suffice to describe the first two measures as the others are outside the scope of this paper. The disagreement measure is used to measure the diversity between two base classifiers (in RF case, these are decision trees) hj and hk, and is calculated as follows:\n4 disj,k = N10 +N01\nN11 +N10 +N01 +N00\nwhere\n\u2022 N10: means number of training instances that were correctly classified by hj , but are incorrectly classified by hk \u2022 N01: means number of training instances that were incorrectly classified by hj , but are correctly classified by hk \u2022 N11: means number of training instances that were correctly classified by hj and hk \u2022 N00: means number of training instances that were incorrectly classified by hj and hk\nThe higher the disagreement measure, the more diverse the classifiers are. The double fault measure uses a slightly different approach where the diversity between two classifiers is calculated as:\nDFj,k = N00\nN11 +N10 +N01 +N00\nThe above two diversity measures work only for binary classification (AKA binomial) where there are only two possible values (like Yes/No) for the class label, hence, the objects are classified into exactly two groups. They do not work for multiclass (AKA multinomial) classification where the objects are classified into more than two groups. In Section V-B, we propose a simple diversity measure that works with both binary and multiclass classification."}, {"heading": "III. PRELIMINARIES", "text": ""}, {"heading": "A. Motivation", "text": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12]. Some empirical and theoretical studies have also clearly demonstrated that adding more trees to an RF beyond a certain number (i.e. 500) won\u2019t necessarily improve the RF accuracy [28]. Our research aims at pruning RF ensembles by producing a subset of the original ones that are significantly smaller in size and yet, have accuracy performance that is at least as good as that of the original RF from which they were derived. In other words, we are aiming at finding the optimial or near-optimal number of trees that will be used to generate an accurate RF."}, {"heading": "B. Random Forest", "text": "Random Forest is an ensemble supervised machine learning technique used for classification and regression. Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees with controlled variation. Using bagging, each decision tree in the ensemble is constructed using a sample with replacement from the training data. Statistically, the sample is likely to have about 64% of instances appearing at least once in the sample.\nInstances in the sample are referred to as in-bag-instances, and the remaining instances (about 36%), are referred to as out-of-bag instances. Each tree in the ensemble acts as a base classifier to determine the class label of an unlabeled instance. This is done via majority voting where each classifier casts one vote for its predicted class label, then the class label with the most votes is used to classify the instance. Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in dataset.\nAlgorithm 1 Random Forest Algorithm {User Settings} input N , S {Process} Create an empty vector \u2212\u2192 RF\nfor i = 1\u2192 N do Create an empty tree Ti repeat\nSample S out of all features F using Bootstrap sampling Create a vector of the S features \u2212\u2192 FS Find Best Split Feature B( \u2212\u2192 FS) Create A New Node using B( \u2212\u2192 FS) in Ti\nuntil No More Instances To Split On Add Ti to the \u2212\u2192 RF\nend for {Output} A vector of trees \u2212\u2192 RF\nTo achieve optimal results, the classifiers in the ensemble should both be accurate and diverse. An accurate classifier is one that has an error rate better than random guessing. Two classifiers are diverse if they make different errors on new data points. The more diverse the classifiers are, the better the results are. In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22]. This explains why many ensemble methods seek to promote diversity among the models they combine. In this paper, we will employ a data clustering technique to produce ensembles with diverse trees."}, {"heading": "IV. CLUSTERING", "text": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38]. Unlike classification, clustering is an unsupervised learning technique that attempts to organize objects into groups whose members are similar in some way. Each group is referred to as a cluster, hence, a cluster is a collection of objects which are similar between them and are dissimilar to the objects belonging to other clusters. Clustering is considered a data exploration method as it helps to unveil the natural grouping in a dataset without a prior knowledge of the groups to be produced. One of the earliest and most popular clustering algorithms is called K-means. It was developed by MacQueen [39] in the late sixties and despite its seniority, it is still considered as one of\n5 the most widely used algorithms, mainly due to its simplicity, efficiency, and empirical success [40].\nUnfortunately, however, this algorithm has a limitation that it only works with numerical data.To overcome this limitation, there have been some extensions of this algorithm to work with categorical data [41] [42] [43]. Huang [41] developed an extension of K-means called K-modes that uses modes instead of means, and can handle categorical data using the following simple matching dissimilarity measure:\nd1 = m\u2211 j=1 \u03b4(xj , yj) (1)\nwhere X, Y be two categorical objects with m categorical attributes, xj and yj (j=1..m) refer to the categorical attributes of X and Y respectively and\n\u03b4(xj , yj) = { 0, if xj = yj 1, otherwise\n(2)\nAccording to the above dissimilarity measure, the total number of mismatches of the corresponding attribute categories of the two objects is calculated. The similarity of the two objects is conversely proportional to the number of mismatches; the smaller it is, the more similar the two objects are. Algorithm 2 outlines the main steps involved in the K-modes algorithm.\nAlgorithm 2 K-modes Algorithm 1) Select k initial modes, one for each cluster. 2) Allocate an object to the cluster whose mode is the\nnearest to it according to (1). Update the mode of the cluster after each allocation. 3) After all objects have been allocated to clusters, retest the dissimilarity of objects against the current modes. If an object is found such that its nearest mode belongs to another cluster rather than its current one, reallocate the object to that cluster and update the modes of both clusters. 4) Repeat 3 until no object has changed clusters after a full cycle test of the whole data set.\nIn the experimental stage we will be using a popular machine learning software suite called Waikato Environment for Knowledge Analysis (WEKA) [44]. This suite comes with a clustering algorithm that integrates K-means and K-modes to work with both numerical and categorical data."}, {"heading": "V. PROPOSED EXTENSION", "text": "In this section, we propose an extension of RF called CLUBDRF that spawns a child RF that is 1) much smaller in size than the parent RF and 2) has an accuracy that is at least as good as that of the parent RF. In the remainder of this paper, we will refer to the parent/original traditional Random Forest as RF, and refer to the resulting child RF based on our method as CLUB-DRF.\nTraining Set\nRandom Forest Algorithm"}, {"heading": "A. CLUstering-Based Diverse Random Forest (CLUB-DRF)", "text": "The CLUB-DRF extension applies a clustering-based technique to produce diverse groups of trees in the RF. Assuming that the trees in the RF are denoted by the vector RF=< t1, t2,\u00b7 \u00b7 \u00b7 ,tn > (where n is number of trees in the RF), and the training set is denoted by T={r1,r2,\u00b7 \u00b7 \u00b7 ,rm}. Each tree in the RF will then be used to classify each record in the training set to determine the class label c. We use C( ti, T) (where ti \u2208 RF) to denote a vector of class labels obtained after having ti classify the training set T. That is C( ti, T)=< ci1, ci2,\u00b7 \u00b7 \u00b7 ,cim >. The result obtained of having each tree classify the training records will therefore be a super vector A\u030a containing class labels vectors as classified by each tree (that is, A\u030a is a vector of vectors): A\u030a = C( t1, T) \u222a C( t2, T) \u222a ... C( tn, T) This set will be fed as input to a clustering algorithm as shown in Figure 1. When clustering is completed, we will have a set of clusters where each cluster contains vectors that are similar and likely to have the least number of discrepancies. For example, using a training set of 5 records with the class label being a boolean (Y/N), the vectors < Y, Y, Y,N,N > and < Y, Y, Y, Y,N > are likely to fall in the same cluster. However, the vectors < Y, Y, Y,N,N > and < Y,N,N, Y, Y > are likely to appear in different clusters because there are many discrepancies in the class labels. As defined above, since a cluster contains objects which are similar between them but are dissimilar to other objects belonging to other clusters, a vector of class labels as classified by a tree in one cluster will be dissimilar (different) from another vector belonging to another cluster, and this means that the two vectors are diverse.\nWhen using a clustering algorithm that requires the number of clusters to be specified in advance (as in K-means), one interesting and challenging question that immediately comes to mind is how to determine this number so that it is not high and it is not low. Mardi [45] proposed a simple rule of thumb: number of clusters \u2248 \u221a n 2 , where n refers to the size of data points to be clustered. Based on Figure 1, we formalize the CLUB-DRF algorithm as shown in Algorithm 3 where T stands for the training set. The constant k refers to the number of clusters to be created\n6 which we define as a multiple of 5 in the range 5 to 40. This way and as we shall see in the experiments section, we can compare the performance of an CLUB-DRF of different sizes with that of the RF.\nIt is important to remember that the size of the resulting CLUB-DRF is determined by the number of clusters used. For example, if the number of the clusters is 5, then the resulting CLUB-DRF will have size 5, and so on.\nAlgorithm 3 CLUB-DRF Algorithm {User Settings} input T , k {Process} Create an empty vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 classLabels\nCreate an empty vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 CLUB \u2212DRF Using T, call Algorithm 1 above to create RF for i = 1\u2192 RF.getNumTrees() do\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 classLabels \u21d0 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 classLabels \u222a C(RF.tree(i), T) end for Cluster \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 classLabels into a set of k clusters: cluster1 ... clusterk From each cluster, select a representative tree RF.tree(j) that corresponds to the instance C(RF.tree(j), T) in the cluster Add RF.tree(j) to \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 CLUB \u2212DRF {Output} A vector of trees \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 CLUB \u2212DRF\nWhen selecting a representative from each cluster (refer to Algorithm 3 above), we will consider the best performing representative on the out-of-bag (OOB) instances. As mentioned in Section III-B, these are the instances that were not included in the sample with replacement that was used to build the tree, and they account for about 36% of the total number of instances. Using the OOB samples to evaluate a tree gives an unbiased estimate of its predictive accuracy since, unlike training data that was seen by the tree when it was built, OOB data was not seen and therefore, it is a more accurate measure of the tree\u2019s predictive accuracy."}, {"heading": "B. Diversity Measure", "text": "Here we propose a simple diversity measure to measure the diversity of two classifiers that works with binary and multiclass classification. Given two classifiers hj and hk and a training set T of size n. Let C(tl,si) denote the class label obtained after having tl classify the sample si in the training set T. Utilizing the dissimilarity measure defined in (1) above, the diversity between the two classifiers can be measured by:\ndiversityj,k =\nn\u2211 i=1 \u03b4(C(tj , vi), C(tk, vi))\nn (3)\nThe higher the number of discrepancies between the two classifiers, the higher the diversity is. For example, assume that we have a training set consisting of 10 training samples T={s1,s2,s3,s4,s5,s6,s7,s8,s9,s10}, and two classifiers t1 and t2. Assume also that there are 3 possible values for the class label {a,b,c}. Let C(t1,T)=<a,a,b,c,c,a,b,c,b,b> and\nC(t2,T)=<a,a,b,b,a,a,b,c,c,c>. According to (3) above, the diversity between the two classifiers is therefore 4/10 or 40%."}, {"heading": "VI. EXPERIMENTAL STUDY", "text": "For our experiments, we have used 15 real datasets with varying characteristics from the UCI repository [46]. To use the holdout testing method, each dataset was divided into 2 sets: training and testing. Two thirds (66%) were reserved for training and the rest (34%) for testing. Each dataset consists of input variables (features) and an output variable. The latter refers to the class label whose value will be predicted in each experiment. For the RF in Figure 1, the initial RF to produce the CLUB-DRF had a size of 500 trees, a typical upper limit setting for RF [12]. We chose the upper limit for two main reasons. First, the more trees we have, the more diverse ones we can get. Secondly, when we have many clusters, the more trees we have, the more unlikely that we wind up with empty clusters [47].\nThe CLUB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [44]. We ran the CLUB-DRF algorithm 10 times on each dataset where a new RF was created in each run. We then calculated the average of the 10 runs for each resulting CLUBDRF to produce the average for a variety of metrics including accuracy rate, minimum accuracy rate, maximum accuracy rate, standard deviation, FMeasure, and AUC as shown in Table V. For the RF, we just calculated the average accuracy rate, FMeasure, and AUC as shown in the last 3 columns of the table."}, {"heading": "A. Results", "text": "Table V compares the performance of CLUB-DRF and RF on the 15 datasets. To show the superiority of CLUB-DRF, we have highlighted in boldface the average accuracy rate of CLUB-DRF when it is greater than that of RF, and underlined the accuracy rate when it is equal to that of RF (only one occurrence found in the squash-unstored dataset). Taking a closer look at this table, we find that CLUB-DRF performed at least as good as RF on 13 datasets. Interestingly enough, of the 13 datasets, CLUB-DRF, regardless of its size, completely outperformed RF on 6 of the datasets, namely, breast-cancer, pasture, eucalyptus, glass, sonar, and vehicle. While CLUBDRF lost to RF on only 2 datasets (audit and vote), the difference was by a very small negligible fraction of less than 1%!"}, {"heading": "B. Analysis", "text": "By showing the number of datasets each was superior on, Figure 2 compares the accuracy rate of RF and CLUB-DRF using different sizes of CLUB-DRF. With one exception of size 35, the figure clearly shows that CLUB-DRF indeed performed at least as good as RF. Notably, the lesser the size of the ensemble, the better performance our method exhibits. This can be attributed to the higher diversity the ensemble has. Having 5 or 10 representatives guarantees a\n7\nfurther away trees chosen to form the ensemble. When we move to a higher number of clusters, and consequently larger ensemble sizes, CLUB-DRF moves towards the original RF. This can be especially true when tree behaviors are no longer distinguishable, and creating more clusters does not add any diversity to the ensemble."}, {"heading": "C. Pruning Level", "text": "By applying the above proposed clustering technique, we managed to achieve two objectives. First, CLUB-DRF ensembles with diverse trees were produced. Second and more importantly, we manged to significantly reduce the size of RF. The resulting pruned CLUB-DRF ensembles performed at least as good as the original RF, but mostly outperformed the original RF, as previously discussed. In ensemble pruning, a pruning level refers to the reduction ratio between the original ensemble and the pruned one. For example, if the size of the original ensemble is 500 trees and the pruned one is of size 50, then 100% \u2212 50500 \u00d7 100% = 90% is the pruning level that was achieved in the pruned ensemble. This means that the pruned ensemble is 90% smaller than the original one. Table I shows the pruning levels. The first column in this table shows the maximum possible pruning level for an CLUBDRF that has outperformed RF, and the second column shows the pruning level of the best performer CLUB-DRF. We can see that with extremely healthy pruning levels ranging from 94% to 99%, our technique outperformed RF. This makes CLUB-DRF a natural choice for real-time applications, where fast classification is an important desideratum. In most cases, 100 times faster classification can be achieved with the 99% pruning level, as shown in the table. In the worst case scenario, only 16.67 times faster classification with 94% pruning level in the white clover dataset. Such estimates are based on the fact that the number of trees traversed in the RF is the dominant factor in the classification response time. This is especially true, given that RF trees are unpruned bushy trees."}, {"heading": "D. Performance Comparison with Pruned Neural Network Ensemble", "text": "In a research by Lazarevic and Obradovic [16] where clustering was also used to prune neural network ensembles,\nthe researchers used diabetes and glass datasets, which we also used in our experiments. Table II depicts the accuracy of their entire and pruned ensembles with RF and our CLUBDRF. For both datasets, our CLUB-DRF was superior to their pruned ensemble. Notably with the glass dataset, CLUB-DRF has made a clear progression in predictive accuracy with a healthy 7.06% increase over the pruned neural network."}, {"heading": "E. Bias/Variance Analysis", "text": "Bias and variance are measures used to estimate the accuracy of a classifier [48]. The bias measures the difference between the classifier\u2019s predicted class value and the true value of the class label being predicted. The variance, on the other hand, measures the variability of the classifier\u2019s prediction as a result of sensitivity due to fluctuations in the training set. If the prediction is always the same regardless of the training set, it equals zero. However, as the prediction becomes more sensitive to the training set, the variance tends to increase. For a classifier to be accurate, it should maintain a low bias and variance.\nThere is a trade-off between a classifier\u2019s ability to minimize bias and variance. Understanding these two types of measures can help us diagnose classifier results and avoid the mistake of over- or under-fitting. Breiman et al. [49] provided an analysis of complexity and induction in terms of a trade-off between bias and variance. In this section, we will show that CLUBDRF can have a bias and variance comparable to and even better than RF. Starting with bias, the first column in Table III shows the pruning level of CLUB-DRF that performed the best relative to RF, and the second column shows the pruning level of the smallest CLUB-DRF that outperformed RF. As demonstrated in the table, CLUB-DRF has outperformed RF on all datasets. On the other hand, Table IV shows similar results but variance-wise. Once again, CLUB-DRF has outperformed RF on all datasets. Although looking at bias in isolation of variance (and vice versa) provides only half of the picture,\n8 our aim is to demonstrate that with a pruned ensemble, both bias and/or variance can be enhanced. We attribute this to the high diversity our ensemble exhibits.\nWe have also conducted experiments to compare the bias and variance between CLUB-DRFs and Random Forests of identical size. Figure 3 compares the bias and Figure 4 compares the variance. Both figures show that CLUB-DRF in most cases can have bias and variance equal to or better than Random Forest.\naudit breast\u2212cancer car\ncredit diabetes eucalyptus\nglass pasture sonar\nsoybean squash_stored squash_unstored\nvehicle vote white\u2212clover\n0.1\n0.2\n0.1\n0.2\n0.1\n0.2\n0.1\n0.2\n0.1\n0.2\n10 20 30 40 10 20 30 40 10 20 30 40 Size\nVa ria\nnc e V\nalu e\n\"CLUB\u2212DRF\"\nCLUB\u2212DRF RF\nFig. 4. Variance Comparison of CLUB-DRF and Random Forest"}, {"heading": "VII. CONCLUSION AND FUTURE DIRECTIONS", "text": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22]. We have used clustering to produce groups of similar trees and selected a representative tree from each group. These likely diverse trees are then used to produce a pruned ensemble of the original one which we called CLUB-DRF. As was demonstrated in the experiments section, CLUB-DRF, in most cases, performed at least as good as the original ensemble.\nAs a future research direction, we can consider using another method when selecting representative trees from the clusters. Instead of picking the best performing representative, a randomly selected representative from each cluster can be picked instead. It would be interesting to compare the results of both methods to determine the one that yields better results. We can also try different sizes for the initial RF and also different cluster increments.\nAnother interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53]. Perhaps the way clusters are formed by each algorithm may have an impact on the performance of CLUB-DRF. This can happen when representatives selected from the clusters of one algorithm are more/less diverse than others selected from clusters produced by another algorithm.\n9 TABLE V: Performance Metrics of CLUB-DRF & RF\nCLUB-DRF Size AVG MIN MAX SD Fmeasure AUC AVG FMeasure AUC breast-cancer 5 71.24 65.98 77.32 3.34 0.64 0.59 69.18 0.63 0.58 10 72.16 69.07 76.29 2.53 0.64 0.59 15 70.21 67.01 75.26 2.28 0.64 0.59 20 72.58 67.01 75.26 2.45 0.64 0.59 25 69.59 67.01 73.20 1.61 0.64 0.59 30 71.65 69.07 74.23 1.68 0.64 0.59 35 69.48 65.98 72.16 1.68 0.64 0.59 40 71.44 67.01 73.20 2.01 0.64 0.58 audit 5 95.86 93.22 97.05 1.19 0.92 0.88 96.53 0.92 0.88 10 96.18 94.99 96.76 0.49 0.91 0.87 15 96.06 95.43 96.76 0.50 0.91 0.88 20 96.47 95.58 97.05 0.41 0.91 0.87 25 96.30 95.87 96.61 0.31 0.91 0.87 30 96.42 96.17 96.61 0.15 0.91 0.87 35 96.39 95.72 96.61 0.25 0.91 0.87 40 96.42 96.17 96.76 0.18 0.91 0.87 credit 5 79.68 72.06 87.94 5.06 0.68 0.62 77.47 0.67 0.61 10 78.74 75.00 82.65 2.38 0.68 0.62 15 77.97 75.59 80.29 1.84 0.68 0.62 20 76.24 73.24 78.53 1.60 0.67 0.62 25 77.35 75.00 80.29 1.53 0.67 0.61 30 77.00 75.29 78.82 1.29 0.67 0.61 35 76.94 75.29 78.53 1.02 0.67 0.61 40 77.29 75.00 78.53 1.10 0.67 0.61 pasture 5 43.33 16.67 66.67 15.72 0.45 0.55 41.67 0.43 0.57 10 55.83 33.33 75.00 13.46 0.45 0.58 15 45.00 16.67 66.67 15.90 0.43 0.56 20 49.17 41.67 66.67 9.46 0.43 0.56 25 45.00 16.67 58.33 13.02 0.44 0.55 30 45.83 33.33 66.67 10.03 0.42 0.55 35 42.50 25.00 58.33 10.17 0.44 0.57 40 45.00 25.00 66.67 11.90 0.42 0.55 squash-unstored 5 76.67 50.00 100.00 12.25 0.63 0.72 72.50 0.58 0.70 10 80.00 75.00 83.33 4.08 0.64 0.73 15 70.83 58.33 83.33 10.03 0.62 0.72 20 79.17 66.67 91.67 7.68 0.60 0.72 25 75.83 66.67 83.33 4.49 0.61 0.72 30 72.50 50.00 83.33 12.94 0.59 0.70 35 75.00 58.33 91.67 9.13 0.59 0.70 40 70.00 50.00 83.33 12.47 0.58 0.70 squash-stored 5 63.33 50.00 91.67 13.02 0.57 0.64 55.83 0.50 0.58 10 65.00 50.00 83.33 9.72 0.55 0.62 15 54.17 41.67 66.67 8.54 0.51 0.59 20 65.00 58.33 83.33 7.26 0.54 0.62 25 53.33 41.67 58.33 6.67 0.52 0.61 30 55.00 50.00 66.67 6.67 0.52 0.60 35 52.50 41.67 66.67 6.51 0.50 0.59 40 56.67 50.00 66.67 6.24 0.52 0.60 white clover 5 75.00 57.14 92.86 10.23 0.62 0.58 77.14 0.60 0.55 10 75.71 50.00 85.71 11.16 0.62 0.57 15 67.86 57.14 85.71 10.23 0.61 0.55 20 72.14 57.14 78.57 7.46 0.60 0.55 25 72.14 57.14 85.71 10.81 0.62 0.57 30 79.29 57.14 92.86 9.82 0.62 0.57 35 73.57 64.29 85.71 6.43 0.62 0.57 40 73.57 64.29 78.57 4.57 0.62 0.56 eucalyptus 5 55.46 45.40 61.96 5.78 0.52 0.71 48.40 0.51 0.70 10 56.20 46.63 63.80 5.74 0.52 0.71 15 52.52 47.24 59.51 3.45 0.52 0.71 20 50.74 45.40 52.15 2.05 0.51 0.70 25 52.15 47.85 55.21 1.96 0.52 0.71 30 49.26 45.40 53.99 2.65 0.51 0.70\nContinued on next page\n10\n11"}], "references": [{"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and Systems Magazine, IEEE, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review, vol. 33, no. 1-2, pp. 1\u201339, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning, vol. 51, no. 2, pp. 181\u2013207, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Designing classifier ensembles with constrained performance requirements", "author": ["W. Yan", "K.F. Goebel"], "venue": "Defense and Security. International Society for Optics and Photonics, 2004, pp. 59\u201368.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Popular ensemble methods: An empirical study", "author": ["R. Maclin", "D. Opitz"], "venue": "Journal Of Artificial Intelligence Research, vol. 11, no. 1-2, pp. 169\u2013198, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, vol. 24, no. 2, pp. 123\u2013140, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["\u2014\u2014"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine learning, vol. 24, no. 1, pp. 49\u201364, 1996.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Machine Learning, vol. 36, no. 1-2, pp. 59\u201383, 1999.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Use R: Data Mining with Rattle and R: the Art of Excavating Data for Knowledge Discovery", "author": ["G. Williams"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "An ensemble pruning primer", "author": ["G. Tsoumakas", "I. Partalas", "I. Vlahavas"], "venue": "Applications of supervised and unsupervised ensemble methods. Springer, 2009, pp. 1\u201313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering ensembles of neural network models", "author": ["B. Bakker", "T. Heskes"], "venue": "Neural networks, vol. 16, no. 2, pp. 261\u2013269, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Design of effective multiple classifier systems by clustering of classifiers", "author": ["G. Giacinto", "F. Roli", "G. Fumera"], "venue": "Pattern Recognition, 2000. Proceedings. 15th International Conference on, vol. 2. IEEE, 2000, pp. 160\u2013163.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Effective pruning of neural network classifier ensembles", "author": ["A. Lazarevic", "Z. Obradovic"], "venue": "Neural Networks, 2001. Proceedings. IJCNN\u201901. International Joint Conference on, vol. 2. IEEE, 2001, pp. 796\u2013801.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Clustering-based selective neural network ensemble", "author": ["F. Qiang", "H. Shang-Xu", "Z. Sheng-Ying"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 5, pp. 387\u2013392, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Pruning of random forest classifiers: A survey and future directions", "author": ["V. Kulkarni", "P. Sinha"], "venue": "Data Science Engineering (ICDSE), 2012 International Conference on, July 2012, pp. 64\u201368.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A double pruning scheme for boosting ensembles", "author": ["V. Soto", "S. Garcia-Moratilla", "G. Martinez-Munoz", "D. Hern\u00e1ndez-Lobato", "A. Suarez"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 12, pp. 2682\u20132695, Dec 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection inspired classifier ensemble reduction", "author": ["R. Diao", "F. Chao", "T. Peng", "N. Snooke", "Q. Shen"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 8, pp. 1259\u20131268, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion, vol. 6, no. 1, pp. 5\u201320, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013271, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "The sources of increased accuracy for two proposed boosting algorithms", "author": ["D.B. Skalak"], "venue": "Proc. American Association for Artificial Intelligence, AAAI-96, Integrating Multiple Learned Models Workshop, vol. 1129. Citeseer, 1996, p. 1133.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing, vol. 19, no. 9, pp. 699\u2013707, 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Bias plus variance decomposition for zero-one loss functions", "author": ["R. Kohavi", "D.H. Wolpert"], "venue": "ICML, 1996, pp. 275\u2013283.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical methods for rates and proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Software diversity: practical statistics for its measurement and exploitation", "author": ["D. Partridge", "W. Krzanowski"], "venue": "Information and software technology, vol. 39, no. 10, pp. 707\u2013717, 1997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "On the selection of decision trees in random forests", "author": ["S. Bernard", "L. Heutte", "S. Adam"], "venue": "Neural Networks, 2009. IJCNN 2009. International Joint Conference on, June 2009, pp. 302\u2013307.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Random decision forests", "author": ["T.K. Ho"], "venue": "Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on, vol. 1. IEEE, 1995, pp. 278\u2013282.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "The random subspace method for constructing decision forests", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 20, no. 8, pp. 832\u2013844, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural computation, vol. 9, no. 7, pp. 1545\u20131588, 1997.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Accuracy and diversity in ensembles of text categorisers", "author": ["J.J.G. Adeva", "U. Beresi", "R. Calvo"], "venue": "CLEI Electronic Journal, vol. 9, no. 1, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Clustering with diversity", "author": ["J. Li", "K. Yi", "Q. Zhang"], "venue": "Automata, Languages and Programming. Springer, 2010, pp. 188\u2013200.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Using diversity in cluster ensembles", "author": ["L.I. Kuncheva", "S.T. Hadjitodorov"], "venue": "Systems, man and cybernetics, 2004 IEEE international conference on, vol. 2. IEEE, 2004, pp. 1214\u20131219.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "An evaluation of structural descriptors and clustering methods for use in diversity selection", "author": ["R. Brown", "Y. Martin"], "venue": "SAR and QSAR in Environmental Research, vol. 8, no. 1-2, pp. 23\u201339, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Enhancing the diversity of a corporate database using chemical database clustering and analysis", "author": ["N.E. Shemetulskis", "J.B. Dunbar Jr", "B.W. Dunbar", "D.W. Moreland", "C. Humblet"], "venue": "Journal of Computer-Aided Molecular Design, vol. 9, no. 5, pp. 407\u2013416, 1995.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Cluster-based transmit diversity scheme for mimo ofdm systems", "author": ["J. Lee", "Y. Sun", "R. Nabar", "H.-L. Lou"], "venue": "Vehicular Technology Conference, 2008. VTC 2008-Fall. IEEE 68th. IEEE, 2008, pp. 1\u2013 5.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Sifting through genomes with iterative-sequence clustering produces a large, phylogenetically diverse protein-family resource", "author": ["T. Sharpton", "G. Jospin", "D. Wu", "M. Langille", "K. Pollard", "J. Eisen"], "venue": "BMC bioinformatics, vol. 13, no. 1, p. 264, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, no. 281-297. California, USA, 1967, p. 14.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1967}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions to the k-means algorithm for clustering large data sets with categorical values", "author": ["Z. Huang"], "venue": "Data Mining and Knowledge Discovery, vol. 2, no. 3, pp. 283\u2013304, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "A fuzzy k-modes algorithm for clustering categorical data", "author": ["Z. Huang", "M.K. Ng"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 7, no. 4, pp. 446\u2013452, 1999.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1999}, {"title": "An alternative extension of the k-means algorithm for clustering categorical data", "author": ["O.M. San", "V.-N. Huynh", "Y. Nakamori"], "venue": "International Journal of Applied Mathematics and Computer Science, vol. 14, no. 2, pp. 241\u2013248, 2004.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "vol. 11,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Multivariate analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": "1980.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1980}, {"title": "Uci machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "A modified k-means algorithm to avoid empty clusters", "author": ["M.K. Pakhira"], "venue": "International Journal of Recent Trends in Engineering, vol. 1, no. 1, p. 1, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "IJCAI, vol. 14, no. 2, 1995, pp. 1137\u20131145.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1995}, {"title": "Classification and regression trees", "author": ["B. Leo", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group, 1984.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1984}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in KDD, vol", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Clarans: A method for clustering objects for spatial data mining", "author": ["R.T. Ng", "J. Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 14, no. 5, pp. 1003\u20131016, 2002.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2002}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "ACM SIGMOD Record, vol. 25, no. 2. ACM, 1996, pp. 103\u2013114.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1996}, {"title": "Cure: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "ACM SIGMOD Record, vol. 27, no. 2. ACM, 1998, pp. 73\u201384.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "AdaBoost [6] is the representative of this class of techniques.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Random Forest (RF) is the main representative of bagging [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Stacking has been successfully used on both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Stacking has been successfully used on both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Since RF algorithms typically build between 100 and 500 trees [12], in real-time applications, it is of paramount importance to reduce the number of trees participating in majority voting and yet achieve performance that is at least as good as the original ensemble.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "[13] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "One approach by [14] was to train a new model for each cluster, using the cluster centroids as values of the target variable.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Another interesting approach was proposed by [15] that involved selecting from each cluster the classifier that is most distant to the rest of the clusters.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "A yet different approach by [16] that does not guarantee the selection of a single model from each cluster was by iteratively removing models from the least to the most accurate, until the accuracy of the entire ensemble starts to decrease.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "Selecting the most accurate model from each cluster was proposed by [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "It is worth pointing out that there are several techniques available in the literature that use clustering-based approaches to reduce the number of trees in the ensemble (see [18] for a good review).", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "Without a significant loss of prediction accuracy, a combination of static and dynamic pruning techniques were applied on Adaboost ensembles in order to yield less memory consumption and improved classification speed [19].", "startOffset": 217, "endOffset": 221}, {"referenceID": 19, "context": "A pruning scheme for high dimensional and large sized benchmark datasets was developed by [20], In such a scheme, an extended feature selection technique was used to transform ensemble predictions into training samples, where classifiers were treated as features.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "[21] summarized the work done to date in this domain from two main perspectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 21, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 26, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 25, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 228, "endOffset": 232}, {"referenceID": 11, "context": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "500) won\u2019t necessarily improve the RF accuracy [28].", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 82, "endOffset": 85}, {"referenceID": 28, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 156, "endOffset": 160}, {"referenceID": 29, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 161, "endOffset": 165}, {"referenceID": 30, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in dataset.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 139, "endOffset": 142}, {"referenceID": 20, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 32, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 88, "endOffset": 92}, {"referenceID": 34, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 93, "endOffset": 97}, {"referenceID": 35, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 108, "endOffset": 112}, {"referenceID": 38, "context": "It was developed by MacQueen [39] in the late sixties and despite its seniority, it is still considered as one of", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "efficiency, and empirical success [40].", "startOffset": 34, "endOffset": 38}, {"referenceID": 40, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 85, "endOffset": 89}, {"referenceID": 42, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "Huang [41] developed an extension of K-means called K-modes that uses modes instead of means, and can handle categorical data using the following simple matching dissimilarity measure:", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": "In the experimental stage we will be using a popular machine learning software suite called Waikato Environment for Knowledge Analysis (WEKA) [44].", "startOffset": 142, "endOffset": 146}, {"referenceID": 44, "context": "Mardi [45] proposed a simple rule of thumb: number of clusters \u2248 \u221a n 2 , where n refers to the size of data points to be clustered.", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "varying characteristics from the UCI repository [46].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "For the RF in Figure 1, the initial RF to produce the CLUB-DRF had a size of 500 trees, a typical upper limit setting for RF [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 46, "context": "Secondly, when we have many clusters, the more trees we have, the more unlikely that we wind up with empty clusters [47].", "startOffset": 116, "endOffset": 120}, {"referenceID": 43, "context": "The CLUB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [44].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "In a research by Lazarevic and Obradovic [16] where clustering was also used to prune neural network ensembles, TABLE I MAXIMUM PRUNING LEVEL WITH BEST POSSIBLE PERFORMANCE", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "TABLE II PERFORMANCE COMPARISON BETWEEN ENTIRE AND PRUNED ENSEMBLE [16] WITH OUR RF AND CLUB-DRFS", "startOffset": 67, "endOffset": 71}, {"referenceID": 47, "context": "Bias and variance are measures used to estimate the accuracy of a classifier [48].", "startOffset": 77, "endOffset": 81}, {"referenceID": 48, "context": "[49] provided an analysis of complexity and induction in terms of a trade-off between bias and variance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 103, "endOffset": 106}, {"referenceID": 20, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 107, "endOffset": 111}, {"referenceID": 31, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 49, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 131, "endOffset": 135}, {"referenceID": 51, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 143, "endOffset": 147}, {"referenceID": 52, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 161, "endOffset": 165}], "year": 2015, "abstractText": "Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster). Second, these likely diverse representatives are then used to produce an extension of RF termed CLUB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, and mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 15 real datasets from the UCI repository prove the superiority of our proposed extension over the traditional RF. Most of our experiments achieved at least 95% or above pruning level while retaining or outperforming the RF accuracy.", "creator": "LaTeX with hyperref package"}}}