{"id": "1503.05296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Efficient Machine Learning for Big Data: A Review", "abstract": "with the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years, in fact, as much as 90 % of current data were created in the last couple of years, a trend that will continue ongoing for the foreseeable future. sustainable computing studies the process by ordering which computer engineer / scientist designs computers and associated technology subsystems efficiently and effectively with minimal impact on the environment. however, current intelligent machine - learning systems are performance monitoring driven, the focus is on the predictive / classification accuracy, based on known properties learned from the training samples. for instance, most machine - learning - based nonparametric models are known and to require high computational cost in order to find the global optima. with the learning task in a large dataset, the number of hidden nodes within the network will therefore surely increase significantly, which eventually leads to an exponential rise in computational complexity. this paper thus reviews the theoretical and experimental intelligent data - modeling literature, in large - scale data - intensive fields, relating to : ( 1 ) model efficiency, including computational requirements in learning, and data - intensive areas structure and cognitive design, and introduces ( 2 ) challenging new algorithmic approaches with setting the least memory requirements and processing to minimize computational cost, while maintaining / improving its predictive / classification accuracy and stability.", "histories": [["v1", "Wed, 18 Mar 2015 07:56:12 GMT  (435kb)", "http://arxiv.org/abs/1503.05296v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["o y al-jarrah", "p d yoo", "s muhaidat", "g k karagiannidis", "k taha"], "accepted": false, "id": "1503.05296"}, "pdf": {"name": "1503.05296.pdf", "metadata": {"source": "CRF", "title": "Efficient Machine Learning for Big Data: A Review", "authors": ["O. Y. Al-Jarrah", "P. D. Yoo", "G. K. Karagiannidis", "K. Taha"], "emails": [], "sections": [{"heading": null, "text": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years \u2013 in fact, as much as 90% of current data were created in the last couple of years \u2013 a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven \u2013 the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas\u2019 structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.\nKeywords: big data; green computing; efficient machine learning; computational modeling\n1. Introduction\nToday, it\u2019s no surprise that reducing energy costs is one of the top priorities for many energy-related businesses. The global information and communications technology (ICT) industry that pumps out around 830 Mt carbon dioxide (CO2) emission accounts for approximately 2 percent of the global CO2 emissions [1]. ICT giants are constantly installing more servers so as to expand their capacity. The number of server computers in data centers has increased sixfold to 30 million in the last decade, and each server draws far more electricity than its earlier models [2]. The aggregate electricity use for servers had doubled between the years 2000 and 2005 period, most of which came from businesses installing large numbers of new servers [3]. This increase in energy consumption consequently results in higher carbon dioxide emissions, and hence causing an impact on the environment. Furthermore, most of these businesses, especially in an uncertain economic climate are placed under the pressure to reduce their energy expenditure in order to remain competitive in the market [4].\nWith the emerging of new technologies and all associated devices, it is predicted that there will be as much data created as was created in the entire history of planet Earth [5]. Given the unprecedented amount of data that will be produced, collected and stored in the coming years, one of the technology industry\u2019s great challenges is how to benefit from it. During the past decade, mathematical intelligent machine-learning systems have been widely adopted in a number of massive and complex data-intensive fields such as astronomy, biology,\nclimatology, medicine, finance and economy. However, current intelligent machine-learning-based systems are not inherently efficient or scalable enough to deal with large volume of data. For example, for many years, it is known that most non-parametric and model-free approaches require high computational cost to find the global optima. With high-dimensional data, their good data fitting capacity not only makes them more susceptible to the generalization problem but leads to an exponential rise in computational complexity. Designing more accurate machine-learning systems so as to satisfy the market needs will hence lead to a higher likelihood of energy waste due to the increased computational cost.\nNowadays, there is a greater need to develop efficient intelligent models to cope with future demands that are in line with similar energy-related initiatives. Such energy-efficientoriented data modeling is important for a number of data-intensive areas, as they affect many related industries. Designers should focus on maximum performance and minimum energy use so as to break away from the traditional\u2019 performance vs. energy-use\u2019 tradeoff, and increase the number and diversity of options available for energy-efficient modeling. However, despite the fact that there is a demand for such efficient and sustainable data modeling methods for large and complex data-intensive fields, to our best knowledge, only a few of these literatures have been proposed in the field [6][7].\nThis paper provides a comprehensive review of state-of-\nthe-art sustainable/energy-efficient machine-learning literatures, including theoretical, empirical and experimental studies pertaining to the various needs and recommendations. Our objective is to introduce a new perspective for engineers, scientists, and researchers in the computer science, and green ICT domain, as well as to provide its roadmap for future research endeavors.\nThis paper is organized as follows. Section 2 introduces the different large-scale data-intensive areas and discusses their structure and nature, including the relation between data models and their characteristics. Section 3 discusses the issues in current intelligent data modeling for sustainability and gives recommendations. Section 4 concludes the paper."}, {"heading": "2. Big data challenge", "text": "e-Science areas are typically data-intensive in that the quality of their results improves with both quantity and quality of data available. However, current intelligent machinelearning systems are not inherently efficient enough which ends up, in many cases, a growing fraction of this quantity data unexplored and underexploited. It is no small problem when existing methods fail to capture such data immensity. When old concepts fail to keep up with change, traditions and past experience become inadequate guide for what to do next. Effective understanding and the use of this new wealth of raw information pose a great challenge to today\u2019s green engineers/researchers. It should be noted that the scope of the review is limited to the analytical aspects of science areas using immense datasets, and the methods for reducing computational complexity in distributed or grid-computing environment is excluded."}, {"heading": "2.1. Geo, climate and environment", "text": "There are many recent examples that can illustrate the tremendous growth in scientific data generation in the literature. It is estimated that there are thousands of wireless sensors currently in place, which generates about a gigabyte of data per sensor per day [8]. Such sensors measure and record sensory information about the natural environment at a joint spatial and temporal dimensions that has never previously been possible. This environmental information is gathered by sensors via its sensing devices that are attached to small, low-power computer systems with digital radio communications. The sensor nodes self-organize itself into a network to deliver, and perhaps process the collected data to a base station, where it can be made available to the users through the Internet. These sensors generate several petabytes of data per year and decisions need to be taken in real time as to how much data to analyze, how much to transmit for further analysis.\nBesides the environmentalists, a similar challenge facing the climatologists, meteorologists, and geologists today is also making sense of the vast and continually increasing amount of data generated by the earth observation satellites, radars, and high-throughput sensor networks. The World Data Centre for Climate (WDCC) is the world-largest climate data\nrepository, and is also known to have the largest database in the world [9]. The WDCC archives 340 terabytes of earth system model data and related observations, and 220 terabytes of data readily accessible on the web including information on climate research and anticipated climatic trends, as well as 110 terabytes (or 24,500 DVD\u2019s) worth of climate simulation data. The WDCC data is accessible by a standard web-interface (http://cera.wdc-climate.de). These data are increasingly available in many different formats and have to be incorporated correctly into the various climate change models. Timely and accurate interpretation of these data can provide advance warnings in times of severe weather changes, hence enabling corresponding action to be taken promptly so as to minimize its resulting catastrophic damage."}, {"heading": "2.2. Bio, medicine, and health", "text": "Biological data has been produced at a phenomenal rate due to the international research effort called the Human Genome Project. It is estimated that the human genome DNA contains around 3.2 billion base (3.2 gigabase) pairs distributed among twenty-three chromosomes, which is translated to about a gigabyte of information [10]. However, when we add the gene sequence data (data on the 100,000 or so translated proteins and the 32,000,000 amino acids), the relevant data volume can easily expand to an order of about 200 gigabyte [11]. Now, by including also the X-ray/NMR spectroscopy structure determination of these proteins, the data volume will increase dramatically to several petabytes, and that is assuming only one structure per protein.\nAs of December 2014, the GenBank repository of nucleic acid sequences contained above 178 million entries [12] and the SWISS-PROT database (inc. both UniProtKB/Swiss-Prot, UniProtKB/TrEMBL) of protein sequences contained about 18 million entries [13][14]. On average, these databases are doubling in size in every 15 months. This is further compounded by data generated from the myriad of related projects that study gene expression, that determines the protein structures encoded by the genes, and that details how these proteins interact with one another. From that, we can begin to imagine the enormous amount and variety of information that is being produced every month.\nOver the past decade, the health sector has also evolved significantly, from paper-based systems to largely paperless electronic systems. Many countries\u2019 public health systems are now providing electronic patient records with advanced medical imaging media. In fact, this has already been implemented by more than 200 American hospitals, and the days of squinting to decipher a doctor\u2019s untidy scrawl on a handwritten prescription will soon be a thing of the past in Canada and many other countries too [15].\nInSiteOne is one of the leading service providers in offering data archiving, storage, and disaster-recovery solutions to the healthcare industry. Its U.S. InSiteOne\u2019s archives include almost 4 billion medical images and 60 million clinical studies, in a coverage area of about 800 clinical sites [16]. The combined annual total of its radiological images exceeds 420 million and this number is still increasing at an approximate\nrate of about 12% per year. There are about 35,500 radiologists currently practicing in the U.S [17]. Each image will typically constitute several megabytes of digital data and is required to be archived for a minimum of five years. ESG (Enterprise Storage Group) forecasts medical image data in North America will grow to more than 35 percent per year and will reach nearly 2.6 million terabytes by 2014 [18]. It is also worthwhile to note that for the digital health data, its integrity and security issues are of critical importance in the field. For instance, for the former, data compression techniques may not be used, in many cases, as they may distort the data; and for the latter, the confidentiality of patient data is clearly cardinal in order to foster public confidence in such technologies."}, {"heading": "2.3. Stars, galaxies, and the universe", "text": "The digital data volume from the stars, galaxies and universe has multiplied over the past decade due to the rapid development of new technologies such as new satellites, telescopes and other observatory instruments. Recently, the Visible and Infrared Survey Telescope for Astronomy (VISTA) [19] and the Dark Energy Survey (DES) [20] \u2013 the largest universe survey projects initiated by two different consortiums of universities, from the U.K., and from the U.S., are expected to yield databases of 20\u201330 terabytes in size in the next decade.\nAccording to DES, its observatory field is so large that a single image will record data from an area of the sky 20 times the size of the moon as seen from the earth [20]. The survey will image 5000 degrees of the U.S. southern sky and will take about five years to complete. As for VISTA, its performance requirements were so challenging that it peaks at 55 megabytes/second data rate with a maximum of 1.4 terabytes of data per night [19]. But, these are now fairly commonplace. Many other astro-scientific databases, such as the Sloan Digital Sky Survey (SDSS) are already terabytes in size [21] and the Panoramic Survey Telescope-Rapid Response System (Pan-STARRS) is expected to produce a science database of more than 100 terabytes in size for the next five years [22]. Likewise, the Large Synoptic Survey Telescope (LSST) is producing 30 terabytes of data per night, yielding a total database of about 150 petabytes [23]. As the data produced by the new telescopes are expected to come to the Internet, this picture will change radically.\nMany believe that the massive data volume and the ever increasing computing power will dramatically change the way in how conventional science and technology are conducted. We believe that this surge in data will open up and challenge further research in each field, hence, instigating the search for new approaches. Likewise, such challenge needs to be addressed in the area of intelligent information science as well."}, {"heading": "3. Sustainable data modeling and efficient learning", "text": "With consideration of the large influx of data, it is definitely necessary to improve the way in how conventional\ncomputational/analytic data models are designed and developed. Sustainable data modeling can be defined as a form of data modeling technology, aimed to make sense of the large amount of data associated in its own field, by discovering patterns and correlations in an effective and efficient way. Sustainable data modeling specifically focuses on 1) maximum learning accuracy with minimum computational cost, and 2) rapid and efficient processing of large volumes of data. Sustainable data modeling seems to be ideal because of its ease in which large quantities of data are handled efficiently as well as its associated cost reduction observed in many cases. In a wider perspective, it entails a data-modeling revolution in e-sciences. In fact, these newly designed sustainable data models will effectively cope with the above data issues and, as a result, bring about benefits to the various e-science areas. Some of the excellent examples are well discussed in Patnaik et al., Sundaravaradan et al., and Marwah\u2019s article [24\u201327]. Hence, in this section, we will give a few recommendations to green engineers/researchers on a few key mechanics of the sustainable data modeling."}, {"heading": "3.1. Ensemble models", "text": "One of the key success elements of sustainable data modeling is to maintain or improve its performance while significantly reducing its computational cost. Recent data-modeling research has shown that ensemble methods have gained much popularity as they often perform better than individual models [28][29]. Ensemble method uses multiple models to obtain better performance than those that could be obtained from any of the constituent models [29][30]. However, it can result in significant increase in computational cost. If the model deals with large-scale data, model complexity and computational requirements will grow exponentially. An example of such ensemble model is the Bayes classifier [31]. In Bayes classifier, each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis was true. To facilitate the training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes classifier is expressed as follows:\n   Hh iiijCc\ni\nj hPhTPhcPy )()|()|(maxarg ,\nwhere y is the predicted class, C is the set of all possible classes, H is the hypothesis space, P refers to a probability, and T is the training data. As an ensemble, the Bayes classifier represents a hypothesis that is not necessarily in H. The hypothesis represented by the Bayes classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in H).\nConsidering the problem of numerical weather prediction, ensemble predictions are now commonly made at most of the major operational weather prediction facilities worldwide [32], including the National Centers for Environmental Prediction, U.S., the European Centre for Medium-Range\nWeather Forecasts (ECMWF), the United Kingdom Met Office, Metro France, Environment Canada, the Japanese Meteorological Agency, the Bureau of Meteorology, Australia, the China Meteorological Administration, the Korea Meteorological Administration, and CPTEC, Brazil."}, {"heading": "3.2. Model complexity problem", "text": "Bayes estimation techniques have been well-adopted in general intelligent data modeling because they provide a fundamental formalism for combining all the information available, with regards to the parameters to be estimated, with optimized time complexity [33].\nOne of the most serious problems in Bayes nonparametric learning models is its high-algorithmic complexity and extensive memory requirements, especially for the necessary quadratic programming in large-scale tasks. As a nonparametric Bayes classifier extracts worst-case example x and uses statistical analysis to build a classifying model, any learning algorithm that examines every attribute values of every training example must have at least the same or worse complexity [33].\nMany applications of machine learning deal with problems where both the number of features i as well as the number of examples xi is large. Linear Support Vector Machines are among the most prominent machine-learning techniques for such high-dimensional and sparse data. In this article, we use two machine-learning models as examples to be semiparameterized. In other words, the two models are to be modified to be more efficient and fast computationally. The time complexity of the Bayes and SVMs are well discussed in Elkan\u2019s and Joachims\u2019 article respectively [34][35]."}, {"heading": "3.3. Local learning strategy", "text": "Yoo et al. have proposed two different support-vectorbased efficient ensemble models that have shown to reduce its computational cost while maintaining its performance [36]. Their novel learning technique has proven to be successful by other similar studies [7]. With a nonparametric model, a unique model must be constructed for each test set, which will significantly increase its computational complexity and cost.\nTo reduce the computational cost, they have thus proposed to partition the training samples into clusters, with that, build a separate local model for each cluster \u2013 this method is called local learning. A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340]. If a local-learning method is adopted in the decision function of a nonparametric classifier (i.e., the general regression network), it will allow for the classifier to be semiparameterized. Its semiparametric approximation can be expressed as follows:\n \n   iZ\nj\nj\nT\nji\nT\ni i\nxxxxcxcx Z\n1 22 2\n)()( exp\n2\n)()( exp\n ,\nwhere xi is a training vector for class i in the input space,  is a single learning or smoothing parameter chosen during the\nnetwork training, and Zi is a number of input training vectors xi associated with its center ci. In nonparametric classification, many different types of radial basis functions can be chosen in place of the Gaussian function. The radial basis function, used in many cases, is actually a spherical kernel function, which is specifically used for nonparametric function estimation. If the number of training samples approaches infinity, the nonparametric function estimation hence becomes no longer dependent on the parameters of the radial basis function, however, for finite training samples, we can always observe some forms of dependency on the radial basis function parameters.\nThe local learning strategy provides more dependence on the radial basis function parameters than that of a nonparametric model because the local learning model is a semiparametric approximation of a nonparametric/global learning model. In other words, in semiparametric modeling, model assumptions gets stronger than those of nonparametric models, but are less restrictive than those of parametric model. In particular, this approximation avoids the practical disadvantages of nonparametric methods at the expense of increased risk of specification errors. Semiparametric models that are based on local learning help not only in reducing the model complexity but also in finding the optimal tradeoff between the parametric and nonparametric models \u2013 so as to achieve both low model bias and variance [41]. In short, it can therefore take on the inherent advantage of both the models while reducing its computational requirements effectively."}, {"heading": "3.4. Semiparametric approximation", "text": "The above examples can be seen as a spherical function mixture model with data-directed center vector allocation. That is because the relative widths of the spherical functions at each center are directly proportional to the relative number of training vectors associated with each center. Many different types of computational local models, and the diverse selection method of the yi and the grouping of the associated input vectors in each class i can be used for the global model semiparametric approximation.\nThe local learning strategy provides a reasonable approximation since xi are sufficiently close in the input vector space. In that case, they can be adequately represented by a single center vector ci in that local space. In the case of Support Vector Regression (SVR), the ci vectors can be derived from either the k-means or the codebook theory. In SVR, where the two classes are not separable, they map the input space into a high-dimensional feature space (where the classes are linearly separable), using a nonlinear kernel function. The kernel function calculates the scalar product of the images of two examples in the feature space.\nGiven a n-dimensional input vector, xi=(x1,x2,\u2026,xn) with two labels, yi{+1, \u20131} where i=1,2,...,N, the hyperplane decision function of the binary SVR with kernel method is:\n              \n\n11 ),(sgn)(),(sgn)( i iii i iii bxxkaybxxayxf\nand the quadratic program is given as:\nmaximize  \n \n1,1\n),( 2\n1 )(\nji\njijiji\ni\ni xxkyyaaaaW ,\nsubject to 0ia , ,,...,1 i and  \n \n1 ,0 i ii ya\nwhere  is the number of training patterns, ai is the parameters of SVR, K (.,.) is a spherical (nonparametric) kernel function, and b is the bias term. In the above case, the local model can be constructed from k-means clustering. The objective function of the k-means clustering can be expressed as follows:\nZC , min  \n   \n k\nj\nn\ni\nk\nj\nn\ni ijijiji yZRCXZ 1 1 1 1 ,\n2\n2, ,\nwhere Xi is the ith row of the similarity matrix , Cj is a 1  m row vector representing the centroid of the jth cluster, R is a non-negative scaling parameter, and \ud835\udc4d\ud835\udc56\ud835\udc57 \u2208 {0,1} is an element of the cluster membership matrix, whose value is equal to one if the ith source vector belongs to the jth cluster, and zero if otherwise. The first term in the objective function corresponds to a cluster cohesion measure. The minimization of the above equation would ensure that the training vectors in the same cluster have highly correlated similarity vectors. The second term measures the skewness of class distribution in each cluster. The minimization of this term would ensure that each cluster contains a balanced number of positive and negative estimation vectors. The cluster centroid C and cluster membership matrix Z are estimated iteratively as follows:\n We fix the cluster centroids and use them to deter-\nmine the cluster membership matrix.\n The revised cluster membership matrix is used to up-\ndate the centroids. \u2013 repeated until the algorithm converges to a local minimum.\nTo compute the cluster membership matrix Z, we transform the original optimization problem, using k slack variable tj, into:\ntZ , min  \n \n k\nj\nk\nj\nj\nn\ni jiji tRCXZ 1 11\n2\n2, ,\ns.t. , 1 , j\nn\ni ijij tyZt   \n,0jt ,10 ,  jiZ\n \n k\nj jiZ 1 , 1 ,\nif the cluster membership matrix is obtained, the cluster centroid Cj is updated based on the following:\nNj Z\nXZ CXQ\nn i ji\nn i iji\njmj ,...,2,1,)(\n1 ,\n1 , \n\n\n\n .\nTo construct a semiparametric model, we substituted Qi (X) for each training sample xi used in the SVR decision function. The new semiparametric model\u2019s approximation is therefore expressed as:\n          \n\n1 ),(sgn)(sgn)( i iii bcxkaybxwxf  ,\nand the quadratic program is given as:\nmaximize  \n \n1,1\n))(),(( 2\n1 )(\nji\njijiji\ni\ni xQxQkyyaaaaW ,\nsubject to 0ia , ,,...,1 i and  \n \n1 .0 i ii ya\nAs mentioned, the local model can also be constructed from the principle of codebook [42]. In this case, its basic idea is to replace key values from an original multidimensional vector space with values from a discrete subspace of lower dimension. The lower-dimension vector requires less storage space and the data is thus compressed.\nConsider a training sequence consisting of M source vectors, T={x1, x2, \u2026, xm}. M is assumed to be sufficiently large, such that all the statistical properties of the source are captured by the training sequence. We assume that the source vectors are k-dimensional, Xm=(xm,1, xm,2, \u2026, xm,k), m=1,2,\u2026,M. These vectors are compressed by choosing the nearest matching vectors, and form a codebook comprising of the entire set of codevectors. N is the number of codevectors, C={c1,c2,\u2026,cn} and each codevector is k-dimensional, cn=(cn,1,cn,2,\u2026,cn,k), n=1,2,\u2026,N. The representative codevector is determined to be the closest in Euclidean distance from the source vector. The Euclidean distance is defined by:\n \n k\nj ijji cxcxd 1\n2)(),( ,\nwhere xj is the jth component of the source vector, cij is the jth component of the codevector ci, Sn is the nearest-neighboring region associated with codevector cn, and the partitions of the whole region are denoted by P={S1,S2,\u2026,SN}. If the source vector Xm is in the region Sn, its approximation can be denoted by Q(Xm)=cn, if XmSn. The Voronoi region is defined by:\n,:{ ji k i cxcxRxV  for all }ij  ,\nthe training vectors falling into a particular region are approximated by a red dot associated with that region (Fig. 1.).\nTo find the optimal C and P, vector quantization uses a square-error distortion measure that specifies exactly how close the approximation is. The distortion measure is given as:\n  \n M\nm\nmmave XQX Mk D 1\n21\nIf C and P are solution parameters to the minimization problem, then it must satisfy two conditions: (1) nearest-neighbor and (2) centroid. The nearest-neighbor condition indicates that the subregion Sn should consist of all the vectors that are closer to cn than any of the other codevectors:\n NncxcxxS nnn ,...,2,1,: 2'2  , finally, the centroid condition indicates that the codevector cn can be derived from the average of all the training vectors in its Voronoi Region Sn:\nNn X c\nSnXm\nSnXm m\nn ,...,2,1, 1   \n\n .\nAs Elkan\u2019s discussed [34], the local learning techniques \u2013 use of cn vectors for building a local model \u2013 prove that any intelligent learning model that examines all the attribute values of every training example must have the same or worse complexity. In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].\nFig. 1. Two-dimensional (2D) vector quantization"}, {"heading": "3.5. Deep learning", "text": "Shallow learning models (e.g., SVM, MLP, and GMM) have been widely used in the literature to solve simple or well-constrained problems. However, their limited modeling and representational power do not support their use in solving more complex problem, such as natural language problems. In 2006, the so-called deep learning (a.k.a. Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.g., Feed-forward neural networks) [46]. The main advantage of deep learning is referred to the drastically increased chip processing abilities, the lowered cost of computing hardware, and the recent advances in ML.\nDeep neural networks (DNNs) are multilayer networks with many hidden layers, whose weights are fully connected and often initialized or pretrained using stacked Restricted Boltzmann Machine (RBM) or Deep Belief Networks (DBMs) [46]. DBM is a pretraining unsupervised step that utilizes large amount of unlabeled training data for extracting structures and regularities in input features [47]. DBN not only uses a huge amount of unlabeled training data but also provides good initialization weights for DNN. Moreover, overfitting and underfitting problems can be tackled by using the pretraining step of DBN. DNN has shown great performance in recognition and classification tasks, including natural language processing, image classification, and traffic flow detection [48]. However, DNN has high computational cost and difficult to scale [49]. DSN addresses the scalability problem of DNN, simple classifiers are stacked on top of each other in order to construct more complex classifier [50][51].\nNew techniques used in Sections 3.3 and 3.4 could fit to the problems of DNN naturally. The decision function of DNN is as follows:\n \nk k\nj\nj x\nx P\n)exp(\n)exp( ,\nwhere Pj represents the class probability and xj and xk represent the total input to units j and k respectively. The cross entropy is defined as follows:\n j jj pdC )log( ,\nwhere dj represents the target probability for output unit j, and Pj is the probability output for j after applying the activation function [52]. Now, the new semiparametric model\u2019s approximation is approximated as:\n \nk k\nj\nk k\nj\nx\nx\nc\nc\n)exp(\n)exp(\n)exp(\n)exp( ,\nthis approximation no longer extracts worst-case example x and is now able to reduce its complexity effectively. As in the local learning strategy, the model assumptions gets stronger than those of nonparametric models, but they are less restrictive than those of parametric model while reducing its computational complexity significantly."}, {"heading": "3.6. Big data computing", "text": "Big data computing systems fall into two major categories, based on how data is analyzed with regards to time constraint [53]. First, batch processing of large volumes of on-disk data with no time constraints (e.g., MapReduce and GraphLab). Second, streaming processing of in-memory data in real-time or short period of time (e.g., Storm, SAMOA) [54][55]. In [54], Huang and Li argued that next-generation computing systems for big data analytics need innovative designs in both hardware and software that would provide a good match between big data algorithms and the underlying computing and\nstorage resources.\nThere are several computing frameworks, e.g., Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning. The combination of deep learning and parallel training implementation techniques provides potential ways to process Big Data [61]. Quoc V. Le et al. [62] consider the problem of building high-level, class-specific feature detectors from only unlabeled data. Experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not.\nK. Zhang and X. Chen [63] presented a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce. The DBNs are trained in a distributed way by stacking a series of distributed RBMs for pretraining and a distributed backpropagation for fine-tuning. Experimental results demonstrate that the distributed RBMs and DBNs are amenable to large-scale data with a good performance in terms of accuracy and efficiency."}, {"heading": "4. Concluding Remarks", "text": "In this review, we provided an overview of the current state of research in sustainable data modeling. In particular, we discussed its theoretical and experimental aspects in large-scale data-intensive fields, relating to: (1) model energy efficiency, including computational requirements in learning, and possible approaches, and (2) data-intensive areas\u2019 structure and design, including the relation between data models and characteristics, With the surge in e-science data, sustainable data modeling has been shown to offer a way forward due to its ease in handling large quantities of data. It is also envisaged that such data-modeling revolution can be readily extended to various areas in e-science. These newly designed sustainable data models will not only be able to cope with the emerging large-scale data paradigm, but also provide a means in maximizing its return for the various e-science areas."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors are grateful to Dr. Jason W.P Ng at EBTIC, for his invaluable discussions and feedback, and special thanks to the British Telecom (BT) in London for their constructive criticism on this work."}], "references": [{"title": "Gartner estimates ict industry accounts for 2 percent of global co2 emissions.", "author": ["C. Pettey"], "venue": "http://www.gartner.com/newsroom/id/503867, Accessed on Aug", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "I.B.M. effort to focus on saving energy.", "author": ["S. Lohr"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Estimating total power consumption by servers in the u.s and the world", "author": ["J.G. Koomey"], "venue": "Stanford university, technical report, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Model-based energy management reduces energy costs.", "author": ["M. Strathman"], "venue": "E&P,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Managing data ood is industry challenge", "author": ["J. Nil"], "venue": "2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "An energy-efficient kernel framework for large-scale data modeling and classiffication", "author": ["P. Yoo", "J. Ng", "A. Zomaya"], "venue": "Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on, pp. 404\u2013408, May 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient algorithm for localized support vector machine", "author": ["H. Cheng", "P.-N. Tan", "R. Jin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 4, pp. 537\u2013549, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Energy, \u201cInsights learned from the human DNA sequence, what has been learned from analysis of the working draft sequence of the human genome? what is still unknown?.", "author": ["D. U"], "venue": "Online. http://www.ornl. gov/hgmis, Accessed on", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The data deluge: An e-science perspective", "author": ["A.J. Hey", "A.E. Trefethen"], "venue": "2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Electronic patient records will soon end doctors scrawl on paper, the globe and mail.", "author": ["T. Baluja"], "venue": "Online. http://www.theglobeandmail.com/news/national/toronto/electronic-patient-records-will-soon-end-doctors-scrawl-on-paper/article1982647, Accessed on", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "News, \u201cDell launches new cloud-based services for hospitals and physician practices.", "author": ["EMR E"], "venue": "Online. http://www.emrandhipaa.com/news/2011/02/21/ dell-launches-new-cloud-based-services-for-hospitals-and-physician-practiceAccessed on", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Vista: project status", "author": ["M A.M."], "venue": "vol. 6267, SPIE 6267, Ground-based and Airborne Telescopes, 626707 (23 June 2006); doi: 10.1117/12.671352, 2006. http://spie.org/Publications/ Proceedings/Paper/10.1117/12.671352.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The pan-starrs survey telescope project", "author": ["N. Kaiser"], "venue": "Bulletin of the American Astronomical Society, vol. 37, p. 1409, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Lsst receives $30 million fom charles simonyi and bill gates.", "author": ["S. Ref"], "venue": "Online. http://www.spaceref.com/news/viewpr.html?pid=24409, Accessed on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Using data mining to help design sustainable products", "author": ["M. Marwah", "A. Shah", "C. Bash", "C. Patel", "N. Ramakrishnan"], "venue": "IEEE Computer, vol. 44, no. 8, pp. 103\u2013106, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering life cycle assessment trees from impact factor databases", "author": ["N. Sundaravaradan", "D. Patnaik", "N. Ramakrishnan", "M. Marwah", "A. Shah"], "venue": "AAAI, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Data mining approaches for life cycle assessment", "author": ["N. Sundaravaradan", "M. Marwah", "A. Shah", "N. Ramakrishnan"], "venue": "IEEE ISSST, vol. 11, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Data mining for modeling chiller systems in data centers", "author": ["D. Patnaik", "M. Marwah", "R.K. Sharma", "N. Ramakrishnan"], "venue": "Advances in Intelligent Data Analysis IX, pp. 125\u2013136, Springer, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A review of ensemble methods in bioinformatics", "author": ["P. Yang", "Y. Hwa Yang", "B. B Zhou", "A. Y Zomaya"], "venue": "Current Bioinformatics, vol. 5, no. 4, pp. 296\u2013308, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Popular ensemble methods: An empirical study,\u201dJournal", "author": ["R. Maclin", "D. Opitz"], "venue": "Artifcial Intelligence Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and sytems magazine, IEEE, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards big data Bayesian network learning - an ensemble learning based approach", "author": ["Y. Tang", "Y. Wang", "K. Cooper", "L. Li"], "venue": "Big Data (BigData Congress), 2014 IEEE International Congress on, pp. 355\u2013357, June 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimation and Spectral Analysis", "author": ["J.A.R. Blais"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1988}, {"title": "Naive bayesian learning", "author": ["C. Elkan"], "venue": "Department of Computer Science and Engineering, University of California, San Diego, 1997. 8", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12 ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 217\u2013226, ACM, 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Siteseek: post-transltional modification analysis using adaptive locality-eff ective kernel methods and new profiles", "author": ["P.D. Yoo", "Y.S. Ho", "B.B. Zhou", "A.Y. Zomaya"], "venue": "BMC bioinformatics, vol. 9, no. 1, p. 272, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Local learning algorithms", "author": ["L. Bottou", "V. Vapnik"], "venue": "Neural computation, vol. 4, no. 6, pp. 888\u2013900, 1992.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1992}, {"title": "Local machine learning models for spatial data analysis", "author": ["N. Gilardi", "S. Bengio"], "venue": "Journal of Geographic Information and Decision Analysis, vol. 4, no. EPFL-ARTICLE-82651, pp. 11\u201328, 2000.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2000}, {"title": "Svm-knn: Discriminative nearest neighbor classiffication for visual category recognition", "author": ["H. Zhang", "A.C. Berg", "M. Maire", "J. Malik"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2, pp. 2126\u20132136, IEEE, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Local prediction of non-linear time series using support vector regression", "author": ["K. Lau", "Q. Wu"], "venue": "Pattern Recognition, vol. 41, no. 5, pp. 1539\u20131547, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Zomaya,\u201dDomnet: protein domain boundary prediction using enhanced general regression network and new profiles,", "author": ["P.D. Yoo", "A.R. Sikder", "J. Taheri", "B.B. Zhou", "A. Y"], "venue": "NanoBioscience, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Springer Science & Business Media,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1828}, {"title": "Big data deep learning: Challenges and perspectives", "author": ["X.-W. Chen", "X. Lin"], "venue": "Access, IEEE, vol. 2, pp. 514\u2013525, 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing, vol. 3, p. e2, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Traffic flow prediction with big data: A deep learning approach", "author": ["Y. Lv", "Y. Duan", "W. Kang", "Z. Li", "F.-Y. Wang"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20139, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "Proceedings of the 28 International Conference on Machine Learning (ICML-11), pp. 265\u2013272, 2011.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep convex net: A scalable architecture for speech pattern classiffication", "author": ["L. Deng", "D. Yu"], "venue": "Proceedings of the Interspeech, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1992}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "A scalable machine learning online service for big data real-time analysis", "author": ["A. Baldominos", "E. Albacete", "Y. Saez", "P. Isasi"], "venue": "Computational Intelligence in Big Data (CIBD), 2014 IEEE Symposium on, pp. 1\u20138, Dec 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data machine learning and graph analytics: Current state and future challenges", "author": ["H. Huang", "H. Liu"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 16\u201317, Oct 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data stream learning with samoa,\u201din", "author": ["A. Bifet", "G.D.F. Morales"], "venue": "Data Mining Workshop (ICDMW),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Shadoop: Improving mapreduce performance by optimizing job execution mechanism in hadoop clusters", "author": ["R. Gu", "X. Yang", "J. Yan", "Y. Sun", "B. Wang", "C. Yuan", "Y. Huang"], "venue": "Journal of Parallel and Distributed Computing, vol. 74, no. 3, pp. 2166 \u2013 2179, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Commapreduce: An improvement of mapreduce with lightweight communication mechanisms", "author": ["L. Ding", "J. Xin", "G. Wang", "S. Huang"], "venue": "Database Systems for Advanced Applications, pp. 150\u2013168, Springer, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Dryad: distributed dataparallel programs from sequential building blocks", "author": ["M. Isard", "M. Budiu", "Y. Yu", "A. Birrell", "D. Fetterly"], "venue": "ACM SIGOPS Operating Systems Review, vol. 41, pp. 59\u201372, ACM, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Piccolo: Building fast, distributed programs with partitioned tables", "author": ["R. Power", "J. Li"], "venue": "OSDI, vol. 10, pp. 1\u201314, 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Data-intensive applications, challenges, techniques and technologies: A survey on big data", "author": ["C.P. Chen", "C.Y. Zhang"], "venue": "Information Sciences, vol. 275, pp. 314\u2013347, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595\u20138598, IEEE, 2013.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale deep belief nets with mapreduce", "author": ["K. Zhang", "X. Chen"], "venue": "2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The global information and communications technology (ICT) industry that pumps out around 830 Mt carbon dioxide (CO2) emission accounts for approximately 2 percent of the global CO2 emissions [1].", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "The number of server computers in data centers has increased sixfold to 30 million in the last decade, and each server draws far more electricity than its earlier models [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "of new servers [3].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "Furthermore, most of these businesses, especially in an uncertain economic climate are placed under the pressure to reduce their energy expenditure in order to remain competitive in the market [4].", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "With the emerging of new technologies and all associated devices, it is predicted that there will be as much data created as was created in the entire history of planet Earth [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "However, despite the fact that there is a demand for such efficient and sustainable data modeling methods for large and complex data-intensive fields, to our best knowledge, only a few of these literatures have been proposed in the field [6][7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "However, despite the fact that there is a demand for such efficient and sustainable data modeling methods for large and complex data-intensive fields, to our best knowledge, only a few of these literatures have been proposed in the field [6][7].", "startOffset": 241, "endOffset": 244}, {"referenceID": 7, "context": "2 gigabase) pairs distributed among twenty-three chromosomes, which is translated to about a gigabyte of information [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "However, when we add the gene sequence data (data on the 100,000 or so translated proteins and the 32,000,000 amino acids), the relevant data volume can easily expand to an order of about 200 gigabyte [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 9, "context": "In fact, this has already been implemented by more than 200 American hospitals, and the days of squinting to decipher a doctor\u2019s untidy scrawl on a handwritten prescription will soon be a thing of the past in Canada and many other countries too [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 10, "context": "6 million terabytes by 2014 [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "Recently, the Visible and Infrared Survey Telescope for Astronomy (VISTA) [19] and the Dark Energy Survey (DES) [20] \u2013 the largest universe survey projects initiated by two different consortiums of universities, from the U.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "4 terabytes of data per night [19].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Many other astro-scientific databases, such as the Sloan Digital Sky Survey (SDSS) are already terabytes in size [21] and the Panoramic Survey Telescope-Rapid Response System (Pan-STARRS) is expected to produce a science database of more than 100 terabytes in size for the next five years [22].", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "Likewise, the Large Synoptic Survey Telescope (LSST) is producing 30 terabytes of data per night, yielding a total database of about 150 petabytes [23].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 17, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 18, "context": "Recent data-modeling research has shown that ensemble methods have gained much popularity as they often perform better than individual models [28][29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Recent data-modeling research has shown that ensemble methods have gained much popularity as they often perform better than individual models [28][29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "Ensemble method uses multiple models to obtain better performance than those that could be obtained from any of the constituent models [29][30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "Ensemble method uses multiple models to obtain better performance than those that could be obtained from any of the constituent models [29][30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "An example of such ensemble model is the Bayes classifier [31].", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Bayes estimation techniques have been well-adopted in general intelligent data modeling because they provide a fundamental formalism for combining all the information available, with regards to the parameters to be estimated, with optimized time complexity [33].", "startOffset": 257, "endOffset": 261}, {"referenceID": 22, "context": "As a nonparametric Bayes classifier extracts worst-case example x and uses statistical analysis to build a classifying model, any learning algorithm that examines every attribute values of every training example must have at least the same or worse complexity [33].", "startOffset": 260, "endOffset": 264}, {"referenceID": 23, "context": "The time complexity of the Bayes and SVMs are well discussed in Elkan\u2019s and Joachims\u2019 article respectively [34][35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 24, "context": "The time complexity of the Bayes and SVMs are well discussed in Elkan\u2019s and Joachims\u2019 article respectively [34][35].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "have proposed two different support-vectorbased efficient ensemble models that have shown to reduce its computational cost while maintaining its performance [36].", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "Their novel learning technique has proven to be successful by other similar studies [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 27, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 28, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 29, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 30, "context": "Semiparametric models that are based on local learning help not only in reducing the model complexity but also in finding the optimal tradeoff between the parametric and nonparametric models \u2013 so as to achieve both low model bias and variance [41].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "As mentioned, the local model can also be constructed from the principle of codebook [42].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "As Elkan\u2019s discussed [34], the local learning techniques \u2013 use of cn vectors for building a local model \u2013 prove that any intelligent learning model that examines all the attribute values of every training example must have the same or worse complexity.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 27, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 28, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 29, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 32, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 33, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 34, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 35, "context": ", Feed-forward neural networks) [46].", "startOffset": 32, "endOffset": 36}, {"referenceID": 35, "context": "with many hidden layers, whose weights are fully connected and often initialized or pretrained using stacked Restricted Boltzmann Machine (RBM) or Deep Belief Networks (DBMs) [46].", "startOffset": 175, "endOffset": 179}, {"referenceID": 36, "context": "DBM is a pretraining unsupervised step that utilizes large amount of unlabeled training data for extracting structures and regularities in input features [47].", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "DNN has shown great performance in recognition and classification tasks, including natural language processing, image classification, and traffic flow detection [48].", "startOffset": 161, "endOffset": 165}, {"referenceID": 38, "context": "However, DNN has high computational cost and difficult to scale [49].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "of DNN, simple classifiers are stacked on top of each other in order to construct more complex classifier [50][51].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "of DNN, simple classifiers are stacked on top of each other in order to construct more complex classifier [50][51].", "startOffset": 110, "endOffset": 114}, {"referenceID": 41, "context": "where dj represents the target probability for output unit j, and Pj is the probability output for j after applying the activation function [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 42, "context": "Big data computing systems fall into two major categories, based on how data is analyzed with regards to time constraint [53].", "startOffset": 121, "endOffset": 125}, {"referenceID": 43, "context": ", Storm, SAMOA) [54][55].", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": ", Storm, SAMOA) [54][55].", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": "In [54], Huang and Li argued that next-generation computing systems for big data analytics need innovative designs in both", "startOffset": 3, "endOffset": 7}, {"referenceID": 45, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 68, "endOffset": 72}, {"referenceID": 49, "context": "The combination of deep learning and parallel training implementation techniques provides potential ways to process Big Data [61].", "startOffset": 125, "endOffset": 129}, {"referenceID": 50, "context": "[62] consider the problem of building high-level, class-specific feature detectors from only unlabeled data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Chen [63] presented a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce.", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years \u2013 in fact, as much as 90% of current data were created in the last couple of years \u2013 a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven \u2013 the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas\u2019 structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.", "creator": "Microsoft\u00ae Word 2013"}}}