{"id": "1503.05571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "GSNs : Generative Stochastic Networks", "abstract": "we introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. the proposed generative stochastic networks ( gsn ) framework is based on learning the transition operator of a markov chain whose stationary distribution estimates dominate the data distribution. because the transition distribution is a conditional distribution generally involving a particularly small move, it has fewer dominant modes, being unimodal in the limit of small moves. thus, actually it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by back - propagation. the theorems initially provided here generalize recent work on the probabilistic interpretation of denoising auto - encoders and provide an interesting justification for dependency networks and generalized dependency pseudolikelihood ( along with defining an appropriate joint distribution and sampling mechanism, even when observed the conditionals are not consistent ). we already study how gsns can be intentionally used with missing inputs and can be used rather to sample subsets of variables given the rest. successful experiments are conducted, validating these theoretical results, on two image datasets with and with a particular architecture that mimics the deep boltzmann machine gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.", "histories": [["v1", "Wed, 18 Mar 2015 20:06:07 GMT  (11819kb,D)", "https://arxiv.org/abs/1503.05571v1", null], ["v2", "Mon, 23 Mar 2015 16:44:52 GMT  (5908kb,D)", "http://arxiv.org/abs/1503.05571v2", "arXiv admin note: substantial text overlap witharXiv:1306.1091"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guillaume alain", "yoshua bengio", "li yao", "jason yosinski", "eric thibodeau-laufer", "saizheng zhang", "pascal vincent"], "accepted": false, "id": "1503.05571"}, "pdf": {"name": "1503.05571.pdf", "metadata": {"source": "CRF", "title": "GSNs: Generative Stochastic Networks", "authors": ["Guillaume Alain", "Yoshua Bengio", "Li Yao", "Jason Yosinski", "\u00c9ric Thibodeau", "Saizheng Zhang", "Pascal Vincent"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Research in deep learning (see Bengio (2009) and Bengio et al. (2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoder variants (Bengio\n. +The first two authors had an equally important contribution\nar X\niv :1\n50 3.\n05 57\n1v 2\n[ cs\n.L G\n] 2\net al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007). However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al., 2012). The latest breakthrough in object recognition (Krizhevsky et al., 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al., 2012).\nIn all of these cases, the availability of large quantities of labeled data was critical.\nOn the other hand, progress with deep unsupervised architectures has been slower, with the established approaches with a probabilistic footing being the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009). Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon and Muller, 2012; Goodfellow et al., 2013). In contrast to recent progress toward joint supervised training of models with many layers, joint unsupervised training of deep models remains a difficult task.\nIn particular, the normalization constant involved in complex multimodal probabilistic models is often intractable and this is dealt with using various approximations (discussed below) whose limitations may be an important part of the difficulty for training and using deep unsupervised, semi-supervised or structured output models.\nThough the goal of training large unsupervised networks has turned out to be more elusive than its supervised counterpart, the vastly larger available volume of unlabeled data still beckons for efficient methods to model it. Recent progress in training supervised models raises the question: can we take advantage of this progress to improve our ability to train deep, generative, unsupervised, semi-supervised or structured output models?\nThis paper lays theoretical foundations for a move in this direction through the following main contributions:\n1 \u2013 Intuition: In Section 2 we discuss what we view as basic motivation for studying alternate ways of training unsupervised probabilistic models, i.e., avoiding the intractable sums or maximization involved in many approaches.\n2 \u2013 Training Framework: We start Section 3 by presenting our recent work on the generative view of denoising auto-encoders (Section 3.1). We present the walkback algorithm which addresses some of the training difficulties with denoising auto-encoders (Section 3.2).\nWe then generalize those results by introducing latent variables in the framework to define Generative Stochastic Networks (GSNs) (Section 3.4). GSNs aim to estimate the datagenerating distribution indirectly, by parametrizing the transition operator of a Markov chain rather than directly parametrizing a model P (X) of the observed random variable X. Most critically, this framework transforms the unsupervised density estimation problem into one which is more similar to supervised function approximation. This enables training by (possibly regularized) maximum likelihood and gradient descent computed via simple back-\npropagation, avoiding the need to compute intractable partition functions. Depending on the model, this may allow us to draw from any number of recently demonstrated supervised training tricks. For example, one could use a convolutional architecture with max-pooling for parametric parsimony and computational efficiency, or dropout (Hinton et al., 2012) to prevent co-adaptation of hidden representations.\n3 \u2013 General theory: Training the generative (decoding / denoising) component of a GSN P (X|h) with noisy representation h is often far easier than modeling P (X) explicitly (compare the blue and red distributions in Figure 1). We prove that if our estimated P (X|h) is consistent (e.g. through maximum likelihood), then the stationary distribution of the resulting Markov chain is a consistent estimator of the data-generating density, P (X) (Section 3.1 and Appendix 6).\n4 \u2013 Consequences of theory: We show that the model is general and extends to a wide range of architectures, including sampling procedures whose computation can be unrolled as a Markov Chain, i.e., architectures that add noise during intermediate computation in order to produce random samples of a desired distribution (Theorem 3). An exciting frontier in machine learning is the problem of modeling so-called structured outputs, i.e., modeling a conditional distribution where the output is high-dimensional and has a complex multimodal joint distribution (given the input variable). We show how GSNs can be used to support such structured output and missing values (Section 3.6).\n5 \u2013 Example application: In Section 4.2 we show an example application of the GSN theory to create a deep GSN whose computational graph resembles the one followed by Gibbs sampling in deep Boltzmann machines (with continuous latent variables), but that can be trained efficiently with back-propagated gradients and without layerwise pretraining. Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al. (2013). The experimental results show that such a model with latent states indeed mixes better than shallower models without them (Table 1).\n6 \u2013 Dependency networks: Finally, an unexpected result falls out of the GSN theory: it allows us to provide a novel justification for dependency networks (Heckerman et al., 2000) and for the first time define a proper joint distribution between all the visible variables that is learned by such models (Section 3.8)."}, {"heading": "2. Summing over too many major modes", "text": "The approach presented in this paper is motivated by a difficulty often encountered with probabilistic models, especially those containing anonymous latent variables. They are called anonymous because no a priori semantics are assigned to them, like in Boltzmann machines, and unlike in many knowledge-based graphical models. Whereas inference over non-anonymous latent variables is required to make sense of the model, anonymous variables\nare only a device to capture the structure of the distribution and need not have a clear human-readable meaning.\nHowever, graphical models with latent variables often require dealing with either or both of the following fundamentally difficult problems in the inner loop of training, or to actually use the model for making decisions: inference (estimating the posterior distribution over latent variables h given inputs x) and sampling (from the joint model of h and x). However, if the posterior P (h|x) has a huge number of modes that matter, then the approximations made may break down.\nMany of the computations involved in graphical models (inference, sampling, and learning) are made intractable and difficult to approximate because of the large number of non-negligible modes in the modeled distribution (either directly P (x) or a joint distribution P (x, h) involving latent variables h). In all of these cases, what is intractable is the computation or approximation of a sum (often weighted by probabilities), such as a marginalization or the estimation of the gradient of the normalization constant. If only a few terms in this sum dominate (corresponding to the dominant modes of the distribution), then many good approximate methods can be found, such as Monte-Carlo Markov chains (MCMC) methods.\nDeep Boltzmann machines (Salakhutdinov and Hinton, 2009) combine the difficulty of inference (for the positive phase where one tries to push the energies associated with the observed x down) and also that of sampling (for the negative phase where one tries to push up the energies associated with x\u2019s sampled from P (x)). Sampling for the negative phase is usually done by MCMC, although some unsupervised learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010; Bordes et al., 2013) involve \u201cnegative examples\u201d that are sampled through simpler procedures (like perturbations of the observed input, in a spirit reminiscent of the approach presented here). Unfortunately, using an MCMC method to sample from P (x, h) in order to estimate the gradient of the partition function may be seriously hurt by the presence of a large number of important modes, as argued below.\nTo evade the problem of highly multimodal joint or posterior distributions, the currently known approaches to dealing with the above intractable sums make very strong explicit assumptions (in the parametrization) or implicit assumptions (by the choice of approximation methods) on the form of the distribution of interest. In particular, MCMC methods are more likely to produce a good estimator if the number of non-negligible modes is small: otherwise the chains would require at least as many MCMC steps as the number of such important modes, times a factor that accounts for the mixing time between modes. Mixing time itself can be very problematic as a trained model becomes sharper, as it approaches a data-generating distribution that may have well-separated and sharp modes (i.e., manifolds) (Bengio et al., 2013b).\nWe propose to make another assumption that might suffice to bypass this multimodality problem: the effectiveness of function approximation. As is typical in machine learning, we postulate a rather large and flexible family of functions (such as deep neural nets) and then\nuse all manner of tricks to pick a member from that combinatorially large family (i.e. to train the neural net) that both fits observed data and generalizes to unseen data well.\nIn particular, the GSN approach presented in the next section relies on estimating the transition operator of a Markov chain, e.g. P (xt|xt\u22121) or P (xt, ht|xt\u22121, ht\u22121). Because each step of the Markov chain is generally local, these transition distributions will often include only a very small number of important modes (those in the neighborhood of the previous state). Hence the gradient of their partition function will be easy to approximate. For example consider the denoising transitions studied by Bengio et al. (2013c) and illustrated in Figure 1, where x\u0303t\u22121 is a stochastically corrupted version of xt\u22121 and we learn the denoising distribution P (x|x\u0303). In the extreme case (studied empirically here) where P (x|x\u0303) is approximated by a unimodal distribution, the only form of training that is required involves function approximation (predicting the clean x from the corrupted x\u0303).\nAlthough having the true P (x|x\u0303) turn out to be unimodal makes it easier to find an appropriate family of models for it, unimodality is by no means required by the GSN framework itself. One may construct a GSN using any multimodal model for output (e.g. mixture of Gaussians, RBMs, NADE, etc.), provided that gradients for the parameters of the model in question can be estimated (e.g. log-likelihood gradients).\nThe approach proposed here thus avoids the need for a poor approximation of the gradient of the partition function in the inner loop of training, but still has the potential of capturing very rich distributions by relying mostly on \u201cfunction approximation\u201d.\nBesides the approach discussed here, there may well be other very different ways of evading this problem of intractable marginalization, including approaches such as sumproduct networks (Poon and Domingos, 2011), which are based on learning a probability function that has a tractable form by construction and yet is from a flexible enough family of distributions. Another interesting direction of investigation that avoids the need for MCMC and intractable partition functions is the variational auto-encoder (Kingma and Welling, 2014; Gregor et al., 2014; Mnih and Gregor, 2014; Rezende et al., 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference."}, {"heading": "3. Generative Stochastic Networks", "text": "In this section we work our way from denoising auto-encoders (DAE) to generative stochastic networks (GSN). We illustrate the usefulness of denoising auto-encoders being applied iteratively as a way to generate samples (and model a distribution). We introduce the walkback training algorithm and show how it can facilitate the training.\nWe generalize the theory to GSNs, and provide a theorem that serves as a recipe as to how they can be trained. We also reference a classic result from matrix perturbation theory to analyze the behavior of GSNs in terms of their stationary distribution.\nWe then study how GSNs may be used to fill missing values and theoretical conditions for estimating associated conditional samples. Finally, we connect GSNs to dependency\nnets and show how the GSN framework fixes one of the main problems with the theoretical analysis of dependency nets and propose a particular way of sampling from them."}, {"heading": "3.1 Denoising auto-encoders to model probability distributions", "text": "Assume the problem we face is to construct a model for some unknown data-generating distribution P (X) given only examples of X drawn from that distribution. In many cases, the unknown distribution P (X) is complicated, and modeling it directly can be difficult.\nA recently proposed approach using denoising auto-encoders (DAE) transforms the difficult task of modeling P (X) into a supervised learning problem that may be much easier to solve. The basic approach is as follows: given a clean example data point X from P (X), we obtain a corrupted version X\u0303 by sampling from some corruption distribution C(X\u0303|X). For example, we might take a clean image, X, and add random white noise to produce X\u0303. We then use supervised learning methods to train a function to reconstruct, as accurately as possible, any X from the data set given only a noisy version X\u0303. As shown in Figure 1, the reconstruction distribution P (X|X\u0303) may often be much easier to learn than the data distribution P (X), because P (X|X\u0303) tends to be dominated by a single or few major modes (such as the roughly Gaussian shaped density in the figure). What we call a major mode is one that is surrounded by a substantial amount of probability mass. There may be a large number of minor modes that can be safely ignored in the context of approximating a distribution, but the major modes should not be missed.\nBut how does learning the reconstruction distribution help us solve our original problem of modeling P (X)? The two problems are clearly related, because if we knew everything about P (X), then our knowledge of the C(X\u0303|X) that we chose would allow us to precisely specify the optimal reconstruction function via Bayes rule: P (X|X\u0303) = 1zC(X\u0303|X)P (X), where z is a normalizing constant that does not depend on X. As one might hope, the relation is also true in the opposite direction: once we pick a method of adding noise, C(X\u0303|X), knowledge of the corresponding reconstruction distribution P (X|X\u0303) is sufficient to recover the density of the data P (X).\nIn a recent paper, Alain and Bengio (2013) showed that denoising auto-encoders with small Gaussian corruption and squared error loss estimated the score (derivative of the logdensity with respect to the input) of continuous observed random variables, thus implicitly estimating P (X). The following Proposition 1 generalizes this to arbitrary variables (discrete, continuous or both), arbitrary corruption (not necessarily asymptotically small), and arbitrary loss function (so long as they can be seen as a log-likelihood).\nProposition 1 Let P (X) be the training distribution for which we only have empirical samples. Let C(X\u0303|X) be the fixed corruption distribution and P\u03b8(X|X\u0303) be the trained reconstruction distribution (assumed to have sufficient capacity). We define a Markov chain that starts at some X0 \u223c P (X) and then iteratively samples pairs of values (Xk, X\u0303k) by alternatively sampling from C(X\u0303k|Xk) and from P\u03b8(Xk+1|X\u0303k).\nX X X X X0\n1\n1\n2\n2\n3\n3 k\nX X X X X~ k0 ~ ~ ~ ~\nLet \u03c0 be the stationary distribution of this Markov chain when we consider only the sequence of values of {Xk}\u221ek=0.\nIf we assume that this Markov chain is irreducible, that its stationary distribution exists, and if we assume that P\u03b8(X|X\u0303) is the distribution that minimizes optimally the following expected loss\nL = \u222b X\u0303 \u222b X P (X)C(X\u0303|X) logP\u03b8(X|X\u0303)dXdX\u0303,\nthen we have that the stationary distribution \u03c0 is the same as the training distribution P (X). Proof If we look at the density P (X\u0303) = \u222b P (X)C(X\u0303|X)dX\u0303 that we get for X\u0303 by applying C(X\u0303|X) to the training data from P (X), we can rewrite the loss as a KL divergence\u222b X\u0303 \u222b X P (X)C(X\u0303|X) logP\u03b8(X|X\u0303)dXdX\u0303 = \u2212KL ( P (X)C(X\u0303|X)\u2016P\u03b8(X|X\u0303)P (X\u0303) ) + cst\nwhere the constant is independent of P\u03b8(X|X\u0303). This expression is maximized when we have a P\u03b8(X|X\u0303) that satisfies\nP (X)C(X\u0303|X) = P\u03b8(X|X\u0303)P (X\u0303). (1)\nIn that case, we have that\nP\u03b8\u2217(X|X\u0303) = P (X)C(X\u0303|X)\nP (X\u0303 = P (X|X\u0303)\nwhere P (X|X\u0303) represents the true conditional that we get through the usual application of Bayes\u2019 rule.\nNow, when we sample iteratively between C(X\u0303k|Xk) and P\u03b8\u2217(Xk+1|X\u0303k) to get the Markov chain illustrated above, we are performing Gibbs sampling. We understand what Gibbs sampling does, and here we are sampling using the two possible ways of expressing the joint from equation (1). This means that the stationary distribution \u03c0 of the Markov chain will have P (X) as marginal density when we look only at the Xk component of the chain.\nBeyond proving that P (X|X\u0303) is sufficient to reconstruct the data density, Proposition 1 also demonstrates a method of sampling from a learned, parametrized model of the density, P\u03b8(X), by running a Markov chain that alternately adds noise using C(X\u0303|X) and denoises by sampling from the learned P\u03b8(X|X\u0303), which is trained to approximate the true P (X|X\u0303).\nBefore moving on, we should pause to make an important point clear. Alert readers may have noticed that P (X|X\u0303) and P (X) can each be used to reconstruct the other given knowledge of C(X\u0303|X). Further, if we assume that we have chosen a simple C(X\u0303|X) (say, a\nuniform Gaussian with a single width parameter), then P (X|X\u0303) and P (X) must both be of approximately the same complexity. Put another way, we can never hope to combine a simple C(X\u0303|X) and a simple P (X|X\u0303) to model a complex P (X). Nonetheless, it may still be the case that P (X|X\u0303) is easier to model than P (X) due to reduced computational complexity in computing or approximating the partition functions of the conditional distribution mapping corrupted input X\u0303 to the distribution of corresponding clean input X. Indeed, because that conditional is going to be mostly assigning probability to X locally around X\u0303, P (X|X\u0303) has only one or a few major modes, while P (X) can have a very large number of them.\nSo where did the complexity go? P (X|X\u0303) has fewer major modes than P (X), but the location of these modes depends on the value of X\u0303. It is precisely this mapping from X\u0303 \u2192 mode location that allows us to trade a difficult density modeling problem for a supervised function approximation problem that admits application of many of the usual supervised learning tricks.\nIn the Gaussian noise example, what happens is that the tails of the Gaussian are exponentially damping all but the modes that are near X, thus preserving the actual number of modes but considerably changing the number of major modes. In the Appendix we also present one alternative line of reasoning based on a corruption process C(X\u0303|X) that has finite local support, thus completely removing the modes that are not in the neighborhood of X. We argue that even with such a corruption process, the stationary distribution \u03c0 will match the original P (X), so long as one can still visit all the regions of interest through a sequence of such local jumps.\nTwo potential issues with Proposition 1 are that 1) we are learning distribution P\u03b8(X|X\u0303) based on experimental samples so it is only asymptotically minimizing the desired loss, and 2) we may not have enough capacity in our model to estimate P\u03b8(X|X\u0303) perfectly.\nThe issue is that, when running a Markov chain for infinitely long using a slightly imperfect P\u03b8(X|X\u0303), these small differences may affect the stationary distribution \u03c0 and compound over time. We are not allowed to \u201cadjust\u201d the P\u03b8(X|X\u0303) as the chain runs.\nThis is addressed by Theorem 4 cited in the later Section 3.4. That theorem gives us a result about continuity, so that, for \u201cwell-behaved\u201d cases, when P\u03b8(X|X\u0303) is close to P (X|X\u0303) we must have that the resulting stationary distribution \u03c0 is close to the original P (X)."}, {"heading": "3.2 Walkback algorithm for training denoising auto-encoders", "text": "In this section we describe the walkback algorithm which is very similar to the method from Proposition 1, but helps training to converge faster. It differs in the training samples that are used, and the fact that the solution is obtained through an iterative process. The parameter update changes the corruption function, which changes the X\u0303 in the training samples, which influences the next parameter update, and so on.\nSampling in high-dimensional spaces (like in experiments in Section 4.1) using a simple local corruption process (such as Gaussian or salt-and-pepper noise) suggests that if the corruption is too local, the DAE\u2019s behavior far from the training examples can create spurious modes in the regions insufficiently visited during training. More training iterations or increasing the amount of corruption noise helps to substantially alleviate that problem, but we discovered an even bigger boost by training the Markov chain to walk back towards the training examples (see Figure 2). We exploit knowledge of the currently learned model P\u03b8(X|X\u0303) to define the corruption, so as to pick values of X\u0303 that would be obtained by following the generative chain: wherever the model would go if we sampled using the generative Markov chain starting at a training example X, we consider to be a kind of \u201cnegative example\u201d X\u0303 from which the auto-encoder should move away (and towards X). The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).\nWe start by defining the modified corruption process Ck(X\u0303|X) that samples k times alternating between C(X\u0303|X) and the current P\u03b8(X|X\u0303).\nWe can express this recursively if we let C1(X\u0303|X) be our original C(X\u0303|X), and then define Ck+1(X\u0303|X) = \u222b X\u0303\u2032 \u222b X\u2032 C(X\u0303|X \u2032)P\u03b8(X \u2032|X\u0303 \u2032)Ck(X\u0303 \u2032|X)dX \u2032dX\u0303 \u2032 (2)\nNote that this corruption distribution Ck(X\u0303|X) now involves the distribution P\u03b8(X|X\u0303) that we are learning.\nWith the help of the above definition of Ck(X\u0303|X), we define the walkback corruption process Cwb(X\u0303|X). To sample from Cwb, we first draw a k distributed according to some distribution, e.g., a geometric distribution with parameter p = 0.5 and support on k \u2208 {1, 2, . . .}), and then we sample according to the corresponding Ck(X\u0303|X). Other values than p = 0.5 could be used, but we just want something convenient for that hyperparameter. Conceptually, the corruption process Cwb means that, from a starting point X we apply iteratively the original C and P\u03b8, and then we flip a coin to determine if we want to do it\nagain. We re-apply until we lose the coin flip, and then this gives us a final value for the sample X\u0303 based on X.\nThe walkback loss is given by\nLwb ' 1\nN N\u2211 i=1 logP\u03b8(X (i)|X\u0303(i)) (3)\nfor samples (X(i), k(i), X\u0303(i)) drawn from X \u223c P (X), k \u223c Geometric(0.5) and X\u0303 \u223c Ck(X\u0303|X). Minimizing this loss is an iterative process because the samples used in the empirical expression depend on the parameter \u03b8 to be learned. This iterated minimization is what we call the walkback algorithm. Samples are generated with the current parameter value \u03b8t, and then the parameters are modified to reduce the loss and yield \u03b8t+1. We repeat until the process stabilizes. In practical applications, we do not have infinite-capacity models and we do not have a guarantee that the walkback algorithm should converge to some \u03b8\u2217."}, {"heading": "3.2.1 Reparametrization Trick", "text": "Note that we do not need to analytically marginalize over the latent variables involved: we can back-propagate through the chain, considering it like a recurrent neural network with noise (the corruption) injected in it. This is an instance of the so-called reparametrization trick, already proposed in (Bengio, 2013; Kingma, 2013; Kingma and Welling, 2014). The idea is that we can consider sampling from a random variable conditionally on others (such as X\u0303 given X) as equivalent to applying a deterministic function taking as argument the conditioning variables as well as some i.i.d. noise sources. This view is particularly useful for the more general GSNs introduced later, in which we typically choose the latent variables to be continuous, i.e., allowing to backprop through their sampling steps when exploiting the reparametrization trick."}, {"heading": "3.2.2 Equivalence of the Walkback Procedure", "text": "With the walkback algorithm, one can also decide to include or not in the loss function all the intermediate reconstruction distributions through which the trajectories pass. That is, starting from some X0, we sample\nX0 \u223c P (X) X\u03030 \u223c C(X\u03030|X0), X1 \u223c P\u03b8(X1|X\u03030) X\u03031 \u223c C(X\u03031|X1) X2 \u223c P\u03b8(X2|X\u03031) X\u03032 \u223c C(X\u03032|X2)\n... ...\nXk\u22121 \u223c P\u03b8(Xk\u22121|X\u0303k\u22122) X\u0303k\u22121 \u223c C(X\u0303k\u22121|Xk\u22121)\nand we use all the pairs (X, X\u0303k) as training data for the walkback loss at equation (3).\nThe following proposition looks very similar to Proposition 1, but it uses the walkback corruption instead of the original corruption C(X\u0303|X). It is also an iterated process through\nwhich the current value of the parameter \u03b8t sets the loss function that will be minimized by the updated \u03b8t+1.\nProposition 2 Let P (X) be the training distribution for which we only have empirical samples. Let \u03c0(X) be the implicitly defined asymptotic distribution of the Markov chain alternating sampling from P\u03b8(X|X\u0303) and C(X\u0303|X), where C is the original local corruption process.\nIf we assume that P\u03b8(X|X\u0303) has sufficient capacity and that the walkback algorithm converges (in terms of being stable in the updates to P\u03b8(X|X\u0303)), then \u03c0(x) = P (X).\nThat is, the Markov chain defined by alternating P\u03b8(X|X\u0303) and C(X\u0303|X) gives us samples that are drawn from the same distribution as the training data.\nProof\nConsider that during training, we produce a sequence of estimators P\u03b8t(X|X\u0303) where P\u03b8t corresponds to the t-th training iteration (modifying the parameters after each iteration). With the walkback algorithm, P\u03b8t\u22121 is used to obtain the corrupted samples X\u0303 from which the next model P\u03b8t\u22121 is produced.\nIf training converges in terms of \u03b8t \u2192 \u03b8\u2217, it means that we have found a value of P\u03b8\u2217(X|X\u0303) such that\n\u03b8\u2217 = argmin\u03b8 1\nN N\u2211 i=1 logP\u03b8(X (i)|X\u0303(i))\nfor samples (X(i), X\u0303(i)) drawn from X \u223c P (X), X\u0303 \u223c Cwb(X\u0303|X).\nBy Proposition 1, we know that, regardless of the the corruption Cany(X\u0303|X) used, when we have a P\u03b8(X|X\u0303) that minimizes optimally the loss\u222b\nX\u0303 \u222b X P (X)Cany(X\u0303|X) logP\u03b8(X|X\u0303)dXdX\u0303\nthen we can recover P (X) by alternating between Cany(X\u0303|X) and P\u03b8(X|X\u0303).\nTherefore, once the model is trained with walkback, the stationary distribution \u03c0 of the Markov chain that it creates has the same distribution P (X) as the training data.\nHence if we alternate between the original corruption C(X\u0303|X) and the walkback solution P\u03b8\u2217(X|X\u0303), then the stationary distribution with respect to X is also P (X).\nNote that this proposition applies regardless of the value of geometric distribution used to determine how many steps of corruption will be used. It applies whether we keep all the samples along to the way, or only the one at the last step. It applies regardless of if we use a geometric distribution to determine which Ck to select, or any other type of distribution.\nA consequence is that the walkback training algorithm estimates the same distribution as the original denoising algorithm, but may do it more efficiently (as we observe in the\nexperiments), by exploring the space of corruptions in a way that spends more time where it most helps the model to kill off spurious modes.\nThe Markov chain that we get with walkback should also generally mix faster, be less susceptible to getting stuck in bad modes, but it will require a P\u03b8\u2217(X|X\u0303) with more capacity than originally. This is because P\u03b8\u2217(X|X\u0303) is now less local, covering the values of the initial X that could have given rise to the X\u0303 resulting from several steps of the Markov chain."}, {"heading": "3.3 Walkbacks with individual scaling factors to handle uncertainty", "text": "The use of the proposed walkback training procedure is effective in suppressing the spurious modes in the learned data distribution. Although the convergence is guaranteed asymptotically, in practice, given limited model capacity and training data, it has been observed that the more walkbacks in training, the more difficult it is to maximize P\u03b8(X|X\u0303). This is simply because more and more noise is added in this procedure, resulting in X\u0303 that is further away from X, therefore a potentially more complicated reconstruction distribution.\nIn other words, P\u03b8(X|X\u0303) needs to have the capacity to model increasingly complex reconstruction distributions. As a result of training, a simple, or usually unimodal P\u03b8(X|X\u0303) is most likely to learn a distribution with a larger uncertainty than the one learned without walkbacks in order to distribute some probability mass to the more complicated and multimodal distributions implied by the walkback training procedure. One possible solution to this problem is to use a multimodal reconstruction distribution such as in Ozair et al. (2014), Larochelle and Murray (2011), or Dinh et al. (2015). We propose here another solution, which can be combined with the above, that consists in allowing a different level of entropy for different steps of the walkback."}, {"heading": "3.3.1 Scaling trick in binary X", "text": "In the case of binary X, the most common choice of the reconstruction distribution is the factorized Multinoulli distribution where P\u03b8(X|X\u0303) = \u220fd i=1 P\u03b8(X\ni|X\u0303) and d is the dimensionality of X. Each factor P\u03b8(X\ni|X\u0303) is modeled by a Bernoulli distribution that has its parameter pi = sigmoid(fi(X\u0303)) where fi(\u00b7) is a general nonlinear transformation realized by a neural network. We propose to use a different scaling factor \u03b1k for different walkback steps, resulting in a new parameterization pki = sigmoid(\u03b1kfi(X\u0303)) for the k-th walkback step, with \u03b1k > 0 being learned. \u03b1k effectively scales the pre-activation of the sigmoid function according to the uncertainty or entropy associated with different walkback steps. Naturally, later reconstructions in the walkback sequence are less accurate because more noise has been injected. Hence, given the ki-th and kj-th walkback steps that satisfy ki < kj , the learning will tend to result in \u03b1ki > \u03b1kj because larger \u03b1k correspond to less entropy."}, {"heading": "3.3.2 Scaling trick in real-valued X", "text": "In the case of real-valued X, the most common choice of P\u03b8(X|X\u0303) is the factorized Gaussian. In particular, each factor P\u03b8(X i|X\u0303) is modeled by a Normal distribution with its parameters\n\u00b5i and \u03c3i. Using the same idea of learning separate scaling factors, we can parametrize it as P\u03b8(X\ni|X\u0303) = N (\u00b5i, \u03b1k\u03c32i ) for the k-th walkback step. \u03b1k is positive and also learned. However, Given the ki-th and kj-th walkback steps that satisfy ki < kj , the learning will result \u03b1ki < \u03b1kj , since in this case, larger \u03b1k indicates larger entropy."}, {"heading": "3.3.3 Sampling with the learned scaling factors", "text": "After learning the scaling factors \u03b1k for k different walkback steps, the sampling is straightforward. One noticeable difference is that we have learned k Markov transition operators. Although, asymptotically all k Markov chains generate the same distribution of X, in practice, they result in different distributions because of the different \u03b1k learned. In fact, using \u03b11 results the samples that are sharper and more faithful to the data distribution. We verify the effect of learning the scaling factor further in the experimental section."}, {"heading": "3.4 Extending the denoising auto-encoder to more general GSNs", "text": "The denoising auto-encoder Markov chain is defined by X\u0303t \u223c C(X\u0303|Xt) andXt+1 \u223c P\u03b8(X|X\u0303t), where Xt alone can serve as the state of the chain. The GSN framework generalizes the DAE in two ways:\n1. the \u201ccorruption\u201d function is not fixed anymore but a parametrized function that can be learned and corresponds to a \u201chidden\u201d state (so we write the output of this function H rather than X\u0303); and\n2. that intermediate variable H is now considered part of the state of the Markov chain, i.e., its value of Ht at step t of the chain depends not just on the previous visible Xt\u22121 but also on the previous state Ht\u22121.\nFor this purpose, we define the Markov chain associated with a GSN in terms of a visible Xt and a latent variable Ht as state variables, of the form\nHt+1 \u223c P\u03b81(H|Ht, Xt) Xt+1 \u223c P\u03b82(X|Ht+1).\nX2X0 X1\nH0 H1 H2\nThis definition makes denoising auto-encoders a special case of GSNs. Note that, given that the distribution of Ht+1 may depend on a previous value of Ht, we find ourselves with an extra H0 variable added at the beginning of the chain. This H0 complicates things when it comes to training, but when we are in a sampling regime we can simply wait a sufficient number of steps to burn in."}, {"heading": "3.4.1 Main result about GSNs", "text": "The next theoretical results give conditions for making the stationary distributions of the above Markov chain match a target data-generating distribution. It basically says that, in order to estimate the data-generating distribution P (X0), it is enough to achieve two conditions.\nThe first condition is similar to the one we obtain when minimizing denoising reconstruction error, i.e., we must make sure that the reconstruction distribution P (X1|H1) approaches the conditional distribution P (X0|H1), i.e., the X0\u2019s that could have given rise to H1.\nThe second condition is novel and regards the initial state H0 of the chain, which influences H1. It says that P (H0|X0) must match P (H1|X0). One way to achieve that is to initialize H0 associated with a training example X0 with the previous value of H1 that was sampled when example X0 was processed. In the graphical model in the statement of Theorem 3, note how the arc relating X0 and H0 goes in the X0 \u2192 H0 direction, which is different from the way we would sample from the GSN (graphical model above), where we have H0 \u2192 X0. Indeed, during training, X0 is given, forcing it to have the data-generating distribution.\nNote that Theorem 3 is there to provide us with a guarantee about what happens when those two conditions are satisfied. It is not originally meant to describe a training method.\nIn section 3.4.3 we explain how to these conditions could be approximately achieved.\nTheorem 3 Let (Ht, Xt) \u221e t=0 be the Markov chain defined by the following graphical model.\nX2X0 X1\nH0 H1 H2\nIf we assume that the chain has a stationary distribution \u03c0H,X , and that for every value of (x, h) we have that\n\u2022 all the P (Xt = x|Ht = h) = g(x|h) share the same density for t \u2265 1\n\u2022 all the P (Ht+1 = h|Ht = h\u2032, Xt = x) = f(h|h\u2032, x) shared the same density for t \u2265 0\n\u2022 P (H0 = h|X0 = x) = P (H1 = h|X0 = x)\n\u2022 P (X1 = x|H1 = h) = P (X0 = x|H1 = h)\nthen for every value of (x, h) we get that\n\u2022 P (X0 = x|H0 = h) = g(x|h) holds, which is something that was assumed only for t \u2265 1\n\u2022 P (Xt = x,Ht = h) = P (X0 = x,H0 = h) for all t \u2265 0\n\u2022 the stationary distribution \u03c0H,X has a marginal distribution \u03c0X such that \u03c0 (x) = P (X0 = x).\nThose conclusions show that our Markov chain has the property that its samples in X are drawn from the same distribution as X0.\nProof The proof hinges on a few manipulations done with the first variables to show that P (Xt = x|Ht = h) = g(x|h), which is assumed for t \u2265 1, also holds for t = 0.\nFor all h we have that\nP (H0 = h) = \u222b P (H0 = h|X0 = x)P (X0 = x)dx\n= \u222b P (H1 = h|X0 = x)P (X0 = x)dx (by hypothesis)\n= P (H1 = h).\nThe equality in distribution between (X1, H1) and (X0, H0) is obtained with\nP (X1 = x,H1 = h) = P (X1 = x|H1 = h)P (H1 = h) = P (X0 = x|H1 = h)P (H1 = h) (by hypothesis) = P (X0 = x,H1 = h)\n= P (H1 = h|X0 = x)P (X0 = x) = P (H0 = h|X0 = x)P (X0 = x) (by hypothesis) = P (X0 = x,H0 = h).\nThen we can use this to conclude that\nP (X0 = x,H0 = h) = P (X1 = x,H1 = h)\n=\u21d2 P (X0 = x|H0 = h) = P (X1 = x|H1 = h) = g(x|h)\nso, despite the arrow in the graphical model being turned the other way, we have that the density of P (X0 = x|H0 = h) is the same as for all other P (Xt = x|Ht = h) with t \u2265 1.\nNow, since the distribution of H1 is the same as the distribution of H0, and the transition probability P (H1 = h|H0 = h\u2032) is entirely defined by the (f, g) densities which are found at every step for all t \u2265 0, then we know that (X2, H2) will have the same distribution as (X1, H1). To make this point more explicitly,\nP (H1 = h|H0 = h\u2032) = \u222b P (H1 = h|H0 = h\u2032, X0 = x)P (X0 = x|H0 = h\u2032)dx\n= \u222b f(h|h\u2032, x)g(x|h\u2032)dx\n= \u222b P (H2 = h|H1 = h\u2032, X1 = x)P (X1 = x|H1 = h\u2032)dx\n= P (H2 = h|H1 = h\u2032)\nThis also holds for P (H3|H2) and for all subsequent P (Ht+1|Ht). This relies on the crucial step where we demonstrate that P (X0 = x|H0 = h) = g(x|h). Once this was shown, then we know that we are using the same transitions expressed in terms of (f, g) at every step.\nSince the distribution of H0 was shown above to be the same as the distribution of H1, this forms a recursive argument that shows that all the Ht are equal in distribution to H0. Because g(x|h) describes every P (Xt = x|Ht = h), we have that all the joints (Xt, Ht) are equal in distribution to (X0, H0).\nThis implies that the stationary distribution \u03c0X,H is the same as that of (X0, H0). Their marginals with respect to X are thus the same.\nIntuitively, the proof of Theorem 3 achieves its objective by forcing all the (Ht, Xt) pairs to share the same joint distribution, thus making the marginal over Xt as t\u2192\u221e (i.e. the stationary distribution of the chain \u03c0) be the same as P (X0), i.e., the data distribution. On the other hand, because it is a Markov chain, its stationary distribution does not depend on the initial conditions, making the model generate from an estimator of P (X0) for any initial condition.\nTo apply Theorem 3 in a context where we use experimental data to learn a model, we would like to have certain guarantees concerning the robustness of the stationary density \u03c0X . When a model lacks capacity, or when it has seen only a finite number of training examples, that model can be viewed as a perturbed version of the exact quantities found in the statement of Theorem 3."}, {"heading": "3.4.2 A note about consistency", "text": "A good overview of results from perturbation theory discussing stationary distributions in finite state Markov chains can be found in (Cho et al., 2000). We reference here only one of those results.\nTheorem 4 Adapted from (Schweitzer, 1968)\nLet K be the transition matrix of a finite state, irreducible, homogeneous Markov chain. Let \u03c0 be its stationary distribution vector so that K\u03c0 = \u03c0. Let A = I\u2212K and Z = (A+ C)\u22121 where C is the square matrix whose columns all contain \u03c0. Then, if K\u0303 is any transition matrix (that also satisfies the irreducible and homogeneous conditions) with stationary distribution \u03c0\u0303, we have that\n\u2016\u03c0 \u2212 \u03c0\u0303\u20161 \u2264 \u2016Z\u2016\u221e \u2225\u2225\u2225K \u2212 K\u0303\u2225\u2225\u2225\n\u221e .\nThis theorem covers the case of discrete data by showing how the stationary distribution is not disturbed by a great amount when the transition probabilities that we learn are close to their correct values. We are talking here about the transition between steps of the chain (X0, H0), (X1, H1), . . . , (Xt, Ht), which are defined in Theorem 3 through the (f, g) densities."}, {"heading": "3.4.3 Training criterion for GSNs", "text": "So far we avoided discussing the training criterion for a GSN. Various alternatives exist, but this analysis is for future work. Right now Theorem 3 suggests the following rules :\n\u2022 Define g(x|h) = P (X1 = x|H1 = h), i.e., the decoder, to be the estimator for P (X0 = x|H1 = h), e.g. by training an estimator of this conditional distribution from the samples (X0, H1), with reconstruction likelihood, logP (X1 = x0|H1), as this would asymptotically achieve the condition P (X0|H1) = P (X1|H1). To see that this is true, consider the following.\nWe sampleX0 from P (X0) (the data-generating distribution) andH1 from P (H1|H0, X0). Refer to one of the next bullet points for an explanation about how to get values for H0 to be used when sampling from P (H1|H0, X0) here. This creates a joint distribution over (X0, H1) that has P (X0|H1) as a derived conditional. Then we train the parameters of a model P\u03b8(X1|H1) to maximize the log-likelihood\nEx0\u223cP (X0),h1\u223cP (H1|x0)[logP\u03b8(X1 = x0|h1)]\n= \u222b x0,h1 P (x0, h1) logP\u03b8(X1 = x0|H1 = h1)dx0dh1\n= \u222b h1 P (h1) \u222b x0 P (X0 = x0|H1 = h1) logP\u03b8(X1 = x0|h1)dx0dh1 =\u2212 EH1 [KL(P (X0|H1)||P\u03b8(X1|H1))] + const. (4)\nwhere the constant does not depend on \u03b8, and thus the log-likelihood is maximized when\nP\u03b8(X1 = x|H1 = h) = P (X0 = x|H1 = h).\n\u2022 Pick the transition distribution f(h|h\u2032, x) to be useful, i.e., training it towards the same objective, i.e., sampling an h\u2032 that makes it easy to reconstruct x. One can think of f(h|h\u2032, x) as the encoder, except that it has a state which depends on its previous value in the chain.\n\u2022 To approach the condition P (H0|X0) = P (H1|X0), one interesting possibility is the following. For each X0 in the training set, iteratively sample H1|(H0, X0) and substitute the value of H1 as the updated value of H0. Repeat until you have achieved a kind of \u201cburn in\u201d. Note that, after the training is completed, when we use the chain for sampling, the samples that we get from its stationary distribution do not depend on H0. Another option is to store the value of H1 that was sampled for the particular training example x0, and re-use it as the initial H0 the next time that x0 is presented during training. These techniques of substituting H1 into H0 are only required during training. In our experiments, we actually found that a fixed H0 = 0 worked as well, so we have used this simpler approach in the reported experiments.\n\u2022 The rest of the chain for t \u2265 1 is defined in terms of (f, g)."}, {"heading": "3.5 Random variable as deterministic function of noise", "text": "There several equivalent ways of expressing a GSN. One of the interesting formulations is to use deterministic functions of random variables to express the densities (f, g) used in Theorem 3. With that approach, we define Ht+1 = \u03c6\u03b81(Xt, Zt, Ht) for some independent noise source Zt, and we insist that Xt cannot be recovered exactly from Ht+1, to avoid a situation in which the Markov chain would not be ergodic. The advantage of that formulation is that one can directly back-propagate the reconstruction log-likelihood logP (X1 = x0|H1 = f(X0, Z0, H0)) into all the parameters of f and g, using the reparametrization trick discussed above in Section 3.2.1. This method is described in (Williams, 1992).\nIn the setting described at the beginning of Section 3, the function playing the role of the \u201cencoder\u201d was fixed for the purpose of the theorem, and we showed that learning only the \u201cdecoder\u201d part (but a sufficiently expressive one) sufficed. In this setting we are learning both, which can cause certain broken behavior.\nOne problem would be if the created Markov chain failed to converge to a stationary distribution. Another such problem could be that the function \u03c6(Xt, Zt, Ht) learned would try to ignore the noise Zt, or not make the best use out of it. In that case, the reconstruction distribution would simply converge to a Dirac at the input X. This is the analogue of the constraint on auto-encoders that is needed to prevent them from learning the identity function. Here, we must design the family from which f and g are learned such that when the noise Z is injected, there are always several possible values of X that could have been the correct original input.\nAnother extreme case to think about is when \u03c6(X,Z,H) is overwhelmed by the noise and has lost all information about X. In that case the theorems are still applicable while giving uninteresting results: the learner must capture the full distribution of X in P\u03b82(X|H) because the latter is now equivalent to P\u03b82(X), since \u03c6(X,Z,H) no longer contains information about X. This illustrates that when the noise is large, the reconstruction distribution (parametrized by \u03b82) will need to have the expressive power to represent multiple modes. Otherwise, the reconstruction will tend to capture an average output, which would visually look like a fuzzy combination of actual modes. In the experiments performed here, we have only considered unimodal reconstruction distributions (with factorized outputs), because we expect that even if P (X|H) is not unimodal, it would be dominated by a single mode when the noise level is small. However, future work should investigate multimodal alternatives.\nA related element to keep in mind is that one should pick the family of conditional distributions P\u03b82(X|H) so that one can sample from them and one can easily train them when given (X,H) pairs, e.g., by maximum likelihood."}, {"heading": "3.6 Handling missing inputs or structured output", "text": "In general, a simple way to deal with missing inputs is to clamp the observed inputs and then run the Markov chain with the constraint that the observed inputs are fixed and\nnot resampled at each time step, whereas the unobserved inputs are resampled each time, conditioned on the clamped inputs.\nIn the context of the GSN described in Section 3.4 using the two distributions\nHt+1 \u223c P\u03b81(H|Ht, Xt) Xt+1 \u223c P\u03b82(X|Ht+1)\nwe need to make some adjustments to P\u03b82(X|Ht+1) to be able to sample X conditioned on some of its components being clamped. We also focus on the case where there are no connections between the Ht \u2192 Ht+1. That is, we study the more basic situation where we train an denoising auto-encoder instead of a GSN that has connections between the hidden units.\nLet S be a set of values that X can take. For example, S can be a subset of the units of X that are fixed to given values. We can talk about clamping X \u2208 S, or just \u201cclamping S\u201d when the meaning is clear. In order to sample from a distribution with clamped S, we need to be able to sample from\nHt+1 \u223c P\u03b81(H|Xt) Xt+1 \u223c P\u03b82(X|Ht+1, X \u2208 S).\nThis notation might be strange at first, but it\u2019s as legitimate as conditioning on 0 < X when sampling from any general distribution. It involves only a renormalization of the resulting distribution P\u03b82(X|Ht+1, X \u2208 S).\nIn a general scenario with two conditional distributions (P\u03b81 , P\u03b82) playing the roles of f(x|h) and g(h|x), i.e. the encoder and decoder, we can make certain basic assumptions so that the asymptotic distributions of (Xt, Ht) and (Xt, Ht+1) both exist. There is no reason to think that those two distributions are the same, and it is trivial to construct counter-examples where they differ greatly.\nHowever, when we train a DAE with infinite capacity, Proposition 1 shows that the optimal solution leads to those two joints being the same. That is, the two trained conditional distributions f(h|x) and g(x|h) are mutually compatible. They form a single joint distribution over (X,H). We can sample from it by the usual Gibbs sampling procedure. Moreover, the marginal distribution over X that we obtain will match that of the training data. This is the motivation for Proposition 1.\nKnowing that Gibbs sampling produces the desired joint distribution over (X,H), we can now see how it would be possible to sample from (X,H)|(X \u2208 S) if we are able to sample from f(h|x) and g(x|h, x \u2208 S). Note that it might be very hard to sample from g(x|h, x \u2208 S), depending on the particular model used. We are not making any assumption on the factorization of g(x|h), much like we are not making any assumption on the particular representation (or implementation) of g(x|h).\nIn section 3.4.2 we address a valid concern about the possibility that, in a practical setting, we might not train g(x|h) to achieve an exact match the density of X|H. That g(x|h) may be very close to the optimum, but it might not be able to achieve it due to\nits finite capacity or its particular parametrization. What does that imply about whether the asymptotic distribution of the Markov chain obtained experimentally compared to the exact joint (X,H) ?\nWe deal with this issue in the same way as we dealt with it when it arose in the context of Theorem 3. The best that we can do is to refer to Theorem 4 and rely on an argument made in the context of discrete states that would closely approximate our situation (which is in either discrete or continuous space).\nOur Markov chain is homogeneous because it does not change with time. It can be made irreducible my imposing very light constraints on f(h|x) so that f(h|x) > 0 for all (x, h). This happens automatically when we take f(h|x) to be additive Gaussian noise (with fixed parameters) and we train only g(x|h). In that case, the optimum g(x|h) will assign non-zero probability weight on all the values of x.\nWe cannot guarantee that a non-optimal g(x|h) will not be broken in some way, but we can often get g(x|h) to be non-zero by selecting a parametrized model that cannot assign a probability of exactly zero to an x. Finally, to use Theorem 4 we need to have that the constant \u2016Z\u2016\u221e from that Theorem 4 to be non-zero. This is a bit more complicated to enforce, but it is something that we will get if the transition matrix stays away from the identity matrix. That constant is zero when the chain is close to being degenerate.\nTheorem 4 says that, with those conditions verified, we have that an arbitrarily good g(x|h) will lead to an arbitrarily good approximation of the exact joint (X,H).\nNow that we know that this approach is grounded in sound theory, it is certainly reasonable to try it in experimental settings in which we are not satisfying all the requirements, and see if the results are useful or not. We would refer the reader to our experiment shown in Figure 6 where we clamp certain units and resample the rest.\nTo further understand the conditions for obtaining the appropriate conditional distributions on some of the visible inputs when others are clamped, we consider below sufficient and necessary conditions for making the stationary distribution of the clamped chain correspond to the normalized distribution (over the allowed values) of the unclamped chain.\nProposition 5 Let f(h|x) and g(x|h) be the encoder and decoder functions such that they are mutually compatible (i.e. they represent a single joint distribution for which we can sample using Gibbs sampling). Let \u03c0(X,H) denote that joint.\nNote that this happens when we minimize\nEX [ log \u222b g(x|h)f(h|x)dh ] or when we minimize the walkback loss (see Proposition 2).\nLet S \u2286 X be a set of values that X can take (e.g. some components of X can be assigned certain fixed values), and such that P(X \u2208 S) > 0. Let \u03c0(x|x \u2208 S) denote the conditional\ndistribution of \u03c0(X,H) on which we marginalize over H and condition on X \u2208 S. That is,\n\u03c0(x|x \u2208 S) \u221d \u03c0(x)I(x \u2208 S) = \u222b h \u03c0(x, h)I(x \u2208 S)dh\u222b\nx \u222b h \u03c0(x, h)I(x \u2208 S)dhdx .\nLet g(x|h, x \u2208 S) denote a restriction of the decoder function that puts probability weight only on the values of x \u2208 S. That is,\ng(x|h, x \u2208 S) \u221d g(x|h)I(x \u2208 S).\nIf we start from some x0 \u2208 S and we run a Markov chain by alternating between f(h|x) and g(x|h, x \u2208 S), then the asymptotic distribution of that chain with respect to X will be the same as \u03c0(x|x \u2208 S)."}, {"heading": "3.7 General conditions for clamping inputs", "text": "In the previous section we gave a sufficient condition for \u201cclamping S\u201d to work in the context of a Markov chain based on an encoder distribution with density f(h|x) and a decoder distribution with density g(x|h).\nIn this section, we will give a sufficient and necessary condition on the sufficient and necessary conditions for handling missing inputs by clamping observed inputs.\nProposition 6 Assume we have an ergodic Markov chain with transition operators having density f(h|x) and g(x|h). Its unique stationary distribution is \u03c0(x, h) over X \u00d7 H which satisfies: \u222b\nX\u00d7H \u03c0(x, h)f(h\u2032|x)g(x\u2032|h\u2032)dxdh = \u03c0(x\u2032, h\u2032).\nAssume that we start from (X0, H0) = (x0, h0) where x0 \u2208 S, S \u2286 X (S can be considered as a constraint over X) and we sample (Xt+1, Ht+1) by first sampling Ht+1 with encoder f(Ht+1|Xt) and then sampling Xt+1 with decoder g(Xt+1|Ht+1, Xt+1 \u2208 S), the new stationary distribution we reach is \u03c0S(x, h).\nThen a sufficient condition for\n\u03c0S(x) = \u03c0(x|x \u2208 S)\nis for \u03c0(x|x \u2208 S) to satisfy\u222b S \u03c0(x|x \u2208 S)f(h\u2032|x)dx = \u03c0(h\u2032|x \u2208 S) (5)\nwhere \u03c0(x|x \u2208 S) and \u03c0(h\u2032|x \u2208 S) are conditional distributions\n\u03c0(x|x \u2208 S) = \u03c0(x)\u222b S \u03c0(x \u2032)dx\u2032 , \u03c0(h\u2032|x \u2208 S) =\n\u222b S \u03c0(x, h\n\u2032)dx\u222b S\u00d7H \u03c0(x, h)dxdh .\nProof Based on the assumption that the chain is ergodic, we have that \u03c0S(X,H) is the unique distribution satisfying\u222b\nS\u00d7H \u03c0S(x, h)f(h\n\u2032|x)g(x\u2032|h\u2032, x\u2032 \u2208 S)dxdh = \u03c0S(x\u2032, h\u2032). (6)\nNow let us check if \u03c0(x, h|x \u2208 S) satisfies the equation above.\nThe Markov chain described in the statement of the Theorem is defined by looking at the slices (Xt, Ht). This means that, by construction, the conditional density \u03c0(x|h) is just given by g(x|h).\nThis relation still holds even if we put the S constraint on x\ng(x\u2032|h\u2032, x\u2032 \u2208 S) = \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S).\nNow if we substitute \u03c0S(x, h) by \u03c0(x, h|x \u2208 S) in Equation 6, the left side of Equation 6 becomes \u222b\nS\u00d7H \u03c0(x, h|x \u2208 S)f(h\u2032|x)\u03c0(x\u2032|h\u2032, x\u2032 \u2208 S)dxdh\n= \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S) \u222b S ( \u222b H \u03c0(x, h|x \u2208 S)dh)f(h\u2032|x)dx\n= \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S) \u222b S \u03c0(x|x \u2208 S)f(h\u2032|x)dx = \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S)\u03c0(h\u2032|x \u2208 S) (using Equation 5) = \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S)\u03c0(h\u2032|x\u2032 \u2208 S) = \u03c0(x\u2032, h\u2032|x\u2032 \u2208 S).\nThis shows that \u03c0(x, h|x \u2208 S) satisfies Equation 6. Due to the ergodicity of the chain, the distribution \u03c0S(x, h) that satisfies Equation 6 is unique, so we have \u03c0S(x, h) = \u03c0(x, h|x \u2208 S). By marginalizing over h we get\n\u03c0S(x) = \u03c0(x|x \u2208 S).\nProposition 6 gives a sufficient condition for dealing missing inputs by clamping observed inputs. Note that this condition is weaker than the mutually compatible condition discussed in Section 3.6. Furthermore, under certain circumstances, this sufficient condition becomes necessary, and we have the following proposition :\nProposition 7 Assume that the Markov chain in Proposition 6 has finite discrete state space for both X and H. The condition in Equation 5 in Proposition 6 becomes a necessary condition when all discrete conditional distributions g(x|h, x \u2208 S) are linear independent.\nProof We follow the same notions in Proposition 6 and now we have \u03c0S(x) = \u03c0(x|x \u2208 S). Because \u03c0S(x) is the marginal of the stationary distribution reached by alternatively\nsampling with encoder f(H|X) and decoder g(X|H,X \u2208 S), we have that \u03c0(x|x \u2208 S) satisfies \u222b\nS \u03c0(x|x \u2208 S)( \u222b H f(h\u2032|x)\u03c0(x\u2032|h\u2032, x\u2032 \u2208 S)dh\u2032)dx = \u03c0(x\u2032|x\u2032 \u2208 S)\nwhich is a direct conclusion from Equation 6 when considering the fact that \u03c0S(x) = \u03c0(x|x \u2208 S) and g(x\u2032|h\u2032, x\u2032 \u2208 S) = \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S). If we re-arrange the integral of equation above, we get: \u222b\nH \u03c0(x\u2032|h\u2032, x\u2032 \u2208 S)( \u222b S \u03c0(x|x \u2208 S)f(h\u2032|x)dx)dh\u2032 = \u03c0(x\u2032|x\u2032 \u2208 S). (7)\nNote that \u222b S \u03c0(x|x \u2208 S)f(h\n\u2032|x)dx is the same as the left side of Equation 5 in Proposition 6 and it can be seen as some function F (h\u2032) satisfying \u222b H F (h\n\u2032)dh\u2032 = 1. Because we have considered a GSN over a finite discrete state space X = {x1, \u00b7 \u00b7 \u00b7 , xN} and H = {h1, \u00b7 \u00b7 \u00b7 , hM}, the integral in Equation 7 becomes the linear matrix equation\nG \u00b7 F = Px,\nwhere G(i, j) = g(x\u2032i|h\u2032j , x\u2032 \u2208 S) = \u03c0(x\u2032i|h\u2032j , x\u2032 \u2208 S), F(i) = F (h\u2032i) and Px(i) = \u03c0(x\u2032i|x\u2032 \u2208 S). In other word, F is a solution of the linear matrix equation\nG \u00b7 Z = Px.\nFrom the definition of G and Px, it is obvious that Ph is also a solution of this linear matrix equation, if Ph(i) = \u03c0(h \u2032 i|x\u2032 \u2208 S). Because all discrete conditional distributions g(x|h, x \u2208 S) are linear independent, which means that all the column vectors of G are linear independent, then this linear matrix equation has no more than one solution. Since Ph is the solution, we have F = Ph, equivalently in integral form\nF (h\u2032) = \u222b S \u03c0(x|x \u2208 S)f(h\u2032|x)dx = \u03c0(h\u2032|x \u2208 S)\nwhich is the condition Equation 5 in Proposition 6.\nProposition 7 says that at least in discrete finite state space, if the g(x|h, x \u2208 S) satisfies some reasonable condition like linear independence, then along with Proposition 6, the condition in Equation 5 is the necessary and sufficient condition for handling missing inputs by clamping the observed part for at least one subset S. If we want this result to hold for any subset S, we have the following proposition:\nProposition 8 If the condition in Equation 5 in Proposition 6 holds for any subset of S that S \u2286 X , then we have\nf(h\u2032|x) = \u03c0(h\u2032|x)\nIn other words, f(h|x) and g(x|h) are two conditional distributions marginalized from a single joint distribution \u03c0(x, h).\nProof Because S can be any subset of X , of course that S can be a set which only has one element x0, i.e., S = {x0}. Now the condition in Equation 5 in Proposition 6 becomes\n1 \u00b7 f(h\u2032|x = x0) = \u03c0(h\u2032|x = x0).\nBecause x0 can be an arbitrary element in X , we have\nf(h\u2032|x) = \u03c0(h\u2032|x), or f(h|x) = \u03c0(h|x).\nSince from Proposition 6 we already know that g(x|h) is \u03c0(x|h), we have that f(h|x) and g(x|h) are mutually compatible, that is, they are two conditional distributions obtained by normalization from a single joint distribution \u03c0(x, h).\nAccording to Proposition 8, if condition in Equation 5 holds for any subset S, then f(h|x) and g(x|h) must be mutually compatible to the single joint distribution \u03c0(x, h)."}, {"heading": "3.8 Dependency Networks as GSNs", "text": "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x\u2212i), where x\u2212i denotes x \\ xi, i.e., the set of variables other than the i-th one, xi. Note that each Pi may be parametrized separately, thus not guaranteeing that there exists a joint of which they are the conditionals. Instead of the ordered pseudoGibbs sampler defined in Heckerman et al. (2000), which resamples each variable xi in the order x1, x2, . . ., we can view dependency networks in the GSN framework by defining a proper Markov chain in which at each step one randomly chooses which variable to resample. The corruption process therefore just consists of H = f(X,Z) = X\u2212s where X\u2212s is the complement of Xs, with s a randomly chosen subset of elements of X (possibly constrained to be of size 1). Furthermore, we parametrize the reconstruction distribution as P\u03b82(X = x|H) = \u03b4x\u2212s=X\u2212sP\u03b82,s(Xs = xs|x\u2212s) where the estimated conditionals P\u03b82,s(Xs = xs|x\u2212s) are not constrained to be consistent conditionals of some joint distribution over all of X.\nProposition 9 If the above GSN Markov chain has a stationary distribution, then the dependency network defines a joint distribution (which is that stationary distribution), which does not have to be known in closed form. Furthermore, if the conditionals P (Xs|X\u2212s) are consistent estimators of the ground truth conditionals, then that stationary distribution is a consistent estimator of the ground truth joint distribution.\nThe proposition can be proven by immediate application of Proposition 1 with the above particular GSN model definitions.\nThis joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent) conditionals gives rise to a stationary distribution \u03c0. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that \u03c0. Clearly, the conditionals derived from \u03c0 by Bayes rule are part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of \u03c0 introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X, being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudoGibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem.\nHowever, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyva\u0308rinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic. This result can be applied to show that the multi-prediction deep Boltzmann machine (MP-DBM) training procedure introduced by Goodfellow et al. (2013) also corresponds to a GSN. This has been exploited in order to obtain much better samples using the associated GSN Markov chain than by sampling from the corresponding DBM (Goodfellow et al., 2013). Another interesting conclusion that one can draw from that paper and its GSN interpretation is that state-of-the-art classification error can thereby be obtained: 0.91% on MNIST without fine-tuning (best comparable previous DBM results was well above 1%) and 10.6% on permutation-invariant NORB (best previous DBM results was 10.8%)."}, {"heading": "4. Experimental results", "text": "The theoretical results on Generative Stochastic Networks (GSNs) open for exploration a large class of possible parametrizations and training procedures which share the property that they can capture the underlying data distribution through the GSN Markov chain. What parametrizations will work well? Where and how should one inject noise to best balance fast mixing with making the implied conditional easy to model? We present results of preliminary experiments with specific selections for each of these choices, but the reader should keep in mind that the space of possibilities is vast.\nWe start in Section 4.1 with results involving GSNs without latent variables (denoising auto-encoders in Section 3.1 and the walkback algorithm presented in Section 3.2). Then in Section 4.2 we proceed with experiments related to GSNs with latent variables (model described in Section 3.4). Section 4.3 extends experiments of the walkback algorithm with the scaling factors discussed in Section 3.3. A Theano1 (Bergstra et al., 2010) implementation is available2, including the links of datasets."}, {"heading": "4.1 Experimental results regarding walkback in DAEs", "text": "We present here an experiment performed with a non-parametric estimator on two types of data and an experiment done with a parametric neural network on the MNIST dataset.\nNon-parametric case. The mathematical results presented here apply to any denoising training criterion where the reconstruction loss can be interpreted as a negative loglikelihood. This remains true whether or not the denoising machine P (X|X\u0303) is parametrized as the composition of an encoder and decoder. This is also true of the asymptotic estimation results in Alain and Bengio (2013). We experimentally validate the above theorems in a\n1. http://deeplearning.net/software/theano/ 2. https://github.com/yaoli/GSN\ncase where the asymptotic limit (of enough data and enough capacity) can be reached, i.e., in a low-dimensional non-parametric setting. Fig. 3 shows the distribution recovered by the Markov chain for discrete data with only 10 different values. The conditional P (X|X\u0303) was estimated by multinomial models and maximum likelihood (counting) from 5000 training examples. 5000 samples were generated from the chain to estimate the asymptotic distribution \u03c0n(X). For continuous data, Figure 3 also shows the result of 5000 generated samples and 500 original training examples with X \u2208 R10, with scatter plots of pairs of dimensions. The estimator is also non-parametric (Parzen density estimator of P (X|X\u0303)).\nMNIST digits. We trained a DAE on the binarized MNIST data (thresholding at 0.5). The 784-2000-784 auto-encoder is trained for 200 epochs with the 50000 training examples and salt-and-pepper noise (probability 0.5 of corrupting each bit, setting it to 1 or 0 with probability 0.5). It has 2000 tanh hidden units and is trained by minimizing cross-entropy loss, i.e., maximum likelihood on a factorized Bernoulli reconstruction distribution. With walkback training, a chain of 5 steps was used to generate 5 corrupted examples for each training example. Figure 4 shows samples generated with and without walkback. The quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a non-parametric density estimator P\u0302 (x) = meanX\u0303P (x|X\u0303) constructed\nfrom 10,000 consecutively generated samples (X\u0303 from the Markov chain). The expected value of E[P\u0302 (x)] over the samples can be shown (Bengio et al., 2013d) to be a lower bound (i.e. conservative estimate) of the true (implicit) model density P (x). The test set loglikelihood bound was not used to select among model architectures, but visual inspection of samples generated did guide the preliminary search reported here. Optimization hyperparameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the training objective. We compare against a state-of-the-art RBM (Cho et al., 2013) with an AIS log-likelihood estimate of -64.1 (AIS estimates tend to be optimistic). We also drew samples from the RBM and applied the same estimator (using the mean of the RBM\u2019s P (x|h) with h sampled from the Gibbs chain), and obtained a log-likelihood nonparametric bound of -233, skipping 100 MCMC steps between samples (otherwise numbers are very poor for the RBM, which mixes poorly). The DAE log-likelihood bound with and without walkback is respectively -116 and -142, confirming visual inspection suggesting that the walkback algorithm produces less spurious samples. However, the RBM samples can be improved by a spatial blur. By tuning the amount of blur (the spread of the Gaussian convolution), we obtained a bound of -112 for the RBM. Blurring did not help the autoencoder."}, {"heading": "4.2 Experimental results for GSNs with latent variables", "text": "We propose here to explore families of parametrizations which are similar to existing deep stochastic architectures such as the Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009). Basically, the idea is to construct a computational graph that is similar to the computational graph for Gibbs sampling or variational inference in Deep Boltzmann Machines. However, we have to diverge a bit from these architectures in order to accommodate the desirable property that it will be possible to back-propagate the gradient of reconstruction log-likelihood with respect to the parameters \u03b81 and \u03b82. Since the gradient\nof a binary stochastic unit is 0 almost everywhere, we have to consider related alternatives. An interesting source of inspiration regarding this question is a recent paper on estimating or propagating gradients through stochastic neurons (Bengio, 2013). Here we consider the following stochastic non-linearities: hi = \u03b7out + tanh(\u03b7in + ai) where ai is the linear activation for unit i (an affine transformation applied to the input of the unit, coming from the layer below, the layer above, or both) and \u03b7in and \u03b7out are zero-mean Gaussian noises.\nTo emulate a sampling procedure similar to Boltzmann machines in which the filled-in missing values can depend on the representations at the top level, the computational graph allows information to propagate both upwards (from input to higher levels) and downwards, giving rise to the computational graph structure illustrated in Figure 5, which is similar to that explored for deterministic recurrent auto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011). Downward weight matrices have been fixed to the transpose of corresponding upward weight matrices.\nWith the walkback algorithm, a different reconstruction distribution is obtained after each step of the short chain started at the training example X. It means that the computational graph from X to a reconstruction probability at step k actually involves generating intermediate samples as if we were running the Markov chain starting at X. In the experiments, the graph was unfolded so that 2D sampled reconstructions would be produced, where D is the depth (number of hidden layers). The training loss is the sum of the reconstruction negative log-likelihoods (of target X) over all 2D reconstructions.\nExperiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST dataset and the Toronto Face Database (TFD), following the setup in Bengio et al. (2013b).\nTheorem 3 requires H0 to have the same distribution as H1 (given X0) during training, and this may be achieved by initializing each training chain with H0 set to the previous value of H1 when the same example X0 was shown. However, it turned out that even with a dumb initialization of H0, good results were obtained in the experiments below.\nNetworks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders. The latter has just 1 hidden layer and no state to state transition, i.e., the computational graph can be split into separate graphs for each reconstruction step in the walkback algorithm. They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with saltand-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs, using stochastic gradient descent (minibatch size of one example). Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10,000 consecutively generated samples (using the realvalued mean-field reconstructions as the training data for the Parzen density estimator). This can be seen as a lower bound on the true log-likelihood, with the bound converging to the true likelihood as we consider more samples and appropriately set the smoothing parameter of the Parzen estimator.3\nResults are summarized in Table 1. As in Section 4.1, the test set Parzen log-likelihood bound was not used to select among model architectures, but visual inspection of generated samples guided this preliminary search. Optimization hyper-parameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the reconstruction log-likelihood training objective. The Parzen log-likelihood bound obtained with a two-layer model on MNIST is 214 (\u00b1 standard error of 1.1), while the log-likelihood bound obtained by a single-layer model (regular denoising auto-encoder, DAE in the table) is substantially worse, at -152\u00b12.2.\nIn comparison, Bengio et al. (2013b) report a log-likelihood bound of -244\u00b154 for RBMs and 138\u00b12 for a 2-hidden layer DBN, using the same setup. We have also evaluated a 3-hidden layer DBM (Salakhutdinov and Hinton, 2009), using the weights provided by the author, and obtained a Parzen log-likelihood bound of 32\u00b12. See http://www.utstat. toronto.edu/~rsalakhu/DBM.html for details.\nInterestingly, the GSN and the DBN-2 actually perform slightly better than when using samples directly coming from the MNIST training set, perhaps because the mean-field outputs we use are more \u201cprototypical\u201d samples.\n3. However, in this paper, to be consistent with the numbers given in Bengio et al. (2013b) we used a Gaussian Parzen density, which makes the numbers not comparable with the AIS log-likelihood upper bounds for binarized images reported in other papers for the same data.\nFigure 7 shows two runs of consecutive samples from this trained model, illustrating that it mixes quite well (faster than RBMs) and produces rather sharp digit images. The figure shows that it can also stochastically complete missing values: the left half of the image was initialized to random pixels and the right side was clamped to an MNIST image. The Markov chain explores plausible variations of the completion according to the trained conditional distribution."}, {"heading": "4.3 Experimental results for GSNs with the scaling factors for walkbacks", "text": "We present the experimental results regarding the discussion in Section 3.3. Experiments are done on both MNIST and TFD. For TFD, only the unsupervised part of the dataset\nis used, resulting 69,000 samples for train, 15,000 for validation, and 15,000 for test. The training examples are normalized to have a mean 0 and a standard deviation 1.\nFor MNIST the GSNs we used have 2 hidden layers with 1000 tanh units each. Salt-andpepper noise is used to corrupt inputs. We have performed extensive hyperparameter search on both the input noise level between 0.3 and 0.7, and the hidden noise level between 0.5 and 2.0. The number of walkback steps is also randomly sampled between 2 and 6. All the experiments are done with learning the scaling factors, following the parameterization in Section 3.3.1. Following previous experiments, the log-probability of the test set is estimated by the same Parzen density estimator on consecutive 10,000 samples generated from the trained model. The \u03c3 parameter in the Parzen estimator is cross-validated on the validation set. The sampling is performed with \u03b11, the learned scaling factor for the first walkback step. The best model achieves a log-likelihood LL=237.44 on MNIST test set, which can be compared with the best reported result LL=225 from Goodfellow et al. (2014).\nOn TFD, we follow a similar procedure as in MNIST, but with larger model capacity (GSNs with 2000-2000 tanh units) and a wider hyperparameter range on the input noise level (between 0.1 and 0.7), the hidden noise level (between 0.5 and 5.0), and the number of walkback steps (between 2 and 6). For comparison, two types of models are trained, one\nwith the scaling factor and one without. The evaluation metric is the same as the one used in MNIST experiments. We compute the Parzen density estimation on the first 10,000 test set examples. The best model without learning the scaling factor results in LL = 1044, and the best model with learning the scaling factor results in 1215 when the scaling factor from the first walkback step is used and 1189 when all the scaling factors are used together with their corresponding walkback steps. As two further comparisons, using the mean over training examples to train the Parzen density estimator results in LL = 632, and using the validation set examples to train the Parzen estimator obtains LL = 2029 (this can be considered as an upper bound when the generated samples are almost perfect). Figure 11 shows the consecutive samples generated with the best model, compared with Figure 9 that is trained without the scaling factor. In addition, Figure 10 shows the learned scaling factor for both datasets that confirms the hypothesis on the effect of the scaling factors made in Section 3.3."}, {"heading": "5. Conclusion", "text": "We have introduced a new approach to training generative models, called Generative Stochastic Networks (GSN), which includes generative denoising auto-encoders as a special case (with no latent variable). It is an alternative to directly performing maximum likelihood on an explicit P (X), with the objective of avoiding the intractable marginalizations and partition function that such direct likelihood methods often entail. The training procedure is more similar to function approximation than to unsupervised learning because the reconstruction distribution is simpler than the data distribution, often unimodal (provably so in the limit of very small noise). This makes it possible to train unsupervised models that capture the data-generating distribution simply using backprop and gradient descent in a computational graph that includes noise injection. The proposed theoretical results state that under mild conditions (in particular that the noise injected in the networks prevents perfect reconstruction), training a sufficient-capacity model to denoise and reconstruct its observations (through a powerful family of reconstruction distributions) suffices to capture the data-generating distribution through a simple Markov chain. Another view is that we are training the transition operator of a Markov chain whose stationary distribution esti-\nmates the data distribution, which has the potential of corresponding to an easier learning problem because the normalization constant for this conditional distribution is generally dominated by fewer modes. These theoretical results are extended to the case where the corruption is local but still allows the chain to mix and to the case where some inputs are missing or constrained (thus allowing to sample from a conditional distribution on a subset of the observed variables or to learned structured output models). The GSN framework is shown to lend to dependency networks a valid estimator of the joint distribution of the observed variables even when the learned conditionals are not consistent, also allowing to prove in a new way the consistency of generalized pseudolikelihood training, associated with the stationary distribution of a corresponding GSN (that randomly chooses a subset of variables and then resamples it). Experiments have been conducted to validate the theory, in the case where the GSN architecture is a simple denoising auto-encoder and in the case where the GSN emulates the Gibbs sampling process of a Deep Boltzmann Machine. A quantitative evaluation of the samples confirms that the training procedure works very well (in this case allowing us to train a deep generative model without layerwise pretraining) and can be used to perform conditional sampling of a subset of variables given the rest. After early versions of this work were published (Bengio et al., 2014), the GSN framework has been extended and applied to classification problems in several different ways (Goodfellow et al., 2013; Zhou and Troyanskaya, 2014; Zo\u0308hrer and Pernkopf, 2014) yielding very interesting results. In addition to providing a consistent generative interpretation to dependency networks, GSNs have been used to provide one to Multi-Prediction Deep Boltzmann Machines (Goodfellow et al., 2013) and to provide a fast sampling algorithm for deep NADE (Yao et al., 2014)."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the stimulating discussions and help from Vincent Dumoulin, Aaron Courville, Ian Goodfellow, and Hod Lipson, as well as funding from NSERC, CIFAR (YB is a CIFAR Senior Fellow), NASA (JY is a Space Technology Research Fellow), and the Canada Research Chairs and Compute Canada."}, {"heading": "6. Appendix: Argument for consistency based on local noise", "text": "This section presents one direction that we pursed initially to demonstrate that we had certain consistency properties in terms of recovering the correct stationary distribution when using a finite training sample. We discuss this issue when we cite Theorem 4 from the literature in section 3.4 and thought it would be a good idea to include our previous approach in this Appendix.\nThe main theorem in Bengio et al. (2013c) (stated in supplemental as Theorem S1) requires that the Markov chain be ergodic. A set of conditions guaranteeing ergodicity is given in the aforementioned paper, but these conditions are restrictive in requiring that C(X\u0303|X) > 0 everywhere that P (X) > 0. The effect of these restrictions is that P\u03b8(X|X\u0303) must have the capacity to model every mode of P (X), exactly the difficulty we were trying to avoid. We show here how we may also achieve the required ergodicity through other\nmeans, allowing us to choose a C(X\u0303|X) that only makes small jumps, which in turn only requires P\u03b8(X|X\u0303) to model a small part of the space around each X\u0303.\nLet P\u03b8n(X|X\u0303) be a denoising auto-encoder that has been trained on n training examples. P\u03b8n(X|X\u0303) assigns a probability to X, given X\u0303, when X\u0303 \u223c C(X\u0303|X). This estimator defines a Markov chain Tn obtained by sampling alternatively an X\u0303 from C(X\u0303|X) and an X from P\u03b8(X|X\u0303). Let \u03c0n be the asymptotic distribution of the chain defined by Tn, if it exists. The following theorem is proven by Bengio et al. (2013c).\nTheorem S1 If P\u03b8n(X|X\u0303) is a consistent estimator of the true conditional distribution P (X|X\u0303) and Tn defines an ergodic Markov chain, then as n\u2192\u221e, the asymptotic distribution \u03c0n(X) of the generated samples converges to the data-generating distribution P (X).\nIn order for Theorem S1 to apply, the chain must be ergodic. One set of conditions under which this occurs is given in the aforementioned paper. We slightly restate them here:\nCorollary 10 If the support for both the data-generating distribution and denoising model are contained in and non-zero in a finite-volume region V (i.e., \u2200X\u0303, \u2200X /\u2208 V, P (X) = 0, P\u03b8(X|X\u0303) = 0 and \u2200X\u0303, \u2200X \u2208 V, P (X) > 0, P\u03b8(X|X\u0303) > 0, C(X\u0303|X) > 0) and these statements remain true in the limit of n\u2192\u221e, then the chain defined by Tn will be ergodic.\nIf conditions in Corollary 10 apply, then the chain will be ergodic and Theorem S1 will apply. However, these conditions are sufficient, not necessary, and in many cases they may be artificially restrictive. In particular, Corollary 10 defines a large region V containing any possible X allowed by the model and requires that we maintain the probability of jumping between any two points in a single move to be greater than 0. While this generous condition helps us easily guarantee the ergodicity of the chain, it also has the unfortunate side effect of requiring that, in order for P\u03b8n(X|X\u0303) to converge to the conditional distribution P (X|X\u0303), it must have the capacity to model every mode of P (X), exactly the difficulty we were trying to avoid. The left two plots in Figure 12 show this difficulty: because C(X\u0303|X) > 0 everywhere in V , every mode of P (X) will leak, perhaps attenuated, into P (X|X\u0303).\nFortunately, we may seek ergodicity through other means. The following corollary allows us to choose a C(X\u0303|X) that only makes small jumps, which in turn only requires P\u03b8(X|X\u0303) to model a small part of the space V around each X\u0303.\nLet P\u03b8n(X|X\u0303) be a denoising auto-encoder that has been trained on n training examples and C(X\u0303|X) be some corruption distribution. P\u03b8n(X|X\u0303) assigns a probability to X, given X\u0303, when X\u0303 \u223c C(X\u0303|X) and X \u223c P(X). Define a Markov chain Tn by alternately sampling an X\u0303 from C(X\u0303|X) and an X from P\u03b8(X|X\u0303).\nCorollary 11 If the data-generating distribution is contained in and non-zero in a finitevolume region V (i.e., \u2200X /\u2208 V, P (X) = 0, and \u2200X \u2208 V, P (X) > 0) and all pairs of points in V can be connected by a finite-length path through V and for some > 0, \u2200X\u0303 \u2208 V,\u2200X \u2208 V within of each other, C(X\u0303|X) > 0 and P\u03b8(X|X\u0303) > 0 and these statements remain true in the limit of n\u2192\u221e, then the chain defined by Tn will be ergodic.\nProof Consider any two points Xa and Xb in V . By the assumptions of Corollary 11, there exists a finite length path between Xa and Xb through V . Pick one such finite length path P . Chose a finite series of points x = {x1, x2, . . . , xk} along P , with x1 = Xa and xk = Xb such that the distance between every pair of consecutive points (xi, xi+1) is less than as defined in Corollary 11. Then the probability of sampling X\u0303 = xi+1 from C(X\u0303|xi)) will be positive, because C(X\u0303|X)) > 0 for all X\u0303 within of X by the assumptions of Corollary 11. Further, the probability of sampling X = X\u0303 = xi+1 from P\u03b8(X|X\u0303) will be positive from the same assumption on P . Thus the probability of jumping along the path from xi to xi+1, Tn(Xt+1 = xi+1|Xt = xi), will be greater than zero for all jumps on the path. Because there is a positive probability finite length path between all pairs of points in V , all states commute, and the chain is irreducible. If we consider Xa = Xb \u2208 V , by the same arguments Tn(Xt = Xa|Xt\u22121 = Xa) > 0. Because there is a positive probability of remaining in the same state, the chain will be aperiodic. Because the chain is irreducible and over a finite state space, it will be positive recurrent as well. Thus, the chain defined by Tn is ergodic.\nAlthough this is a weaker condition that has the advantage of making the denoising distribution even easier to model (probably having less modes), we must be careful to choose the ball size large enough to guarantee that one can jump often enough between the major modes of P (X) when these are separated by zones of tiny probability. must be larger than half the largest distance one would have to travel across a desert of low probability separating two nearby modes (which if not connected in this way would make V not anymore have a single connected component). Practically, there is a trade-off between the difficulty of estimating P (X|X\u0303) and the ease of mixing between major modes separated by a very low density zone."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR\u20192013),", "citeRegEx": "Alain and Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Alain and Bengio.", "year": 2013}, {"title": "Learning iterative image reconstruction in the neural abstraction pyramid", "author": ["Sven Behnke"], "venue": "Int. J. Computational Intelligence and Applications,", "citeRegEx": "Behnke.,? \\Q2001\\E", "shortCiteRegEx": "Behnke.", "year": 2001}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS\u20192006,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Now Publishers,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Yoshua Bengio"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "In ICML\u201913,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In NIPS26. NIPS Foundation,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Bounding the test log-likelihood of generative models", "author": ["Yoshua Bengio", "Li Yao", "Kyunghyun Cho"], "venue": "Technical report, U. Montreal,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "Technical Report arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning: Special Issue on Learning Semantics,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Reweighted wake-sleep", "author": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "venue": "Technical report,", "citeRegEx": "Bornschein and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein and Bengio.", "year": 2014}, {"title": "Quickly generating representative samples from an RBM-derived process", "author": ["Olivier Breuleux", "Yoshua Bengio", "Pascal Vincent"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Comparison of perturbation bounds for the stationary distribution of a markov chain", "author": ["Grace E. Cho", "Carl D. Meyer", "Carl", "D. Meyer"], "venue": "Linear Algebra Appl,", "citeRegEx": "Cho et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2000}, {"title": "Enhanced gradient for training restricted boltzmann machines", "author": ["KyungHyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "Neural computation,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In ICML\u20192008,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["George E. Dahl", "Marc\u2019Aurelio Ranzato", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": "In NIPS\u20192010,", "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder", "author": ["L. Deng", "M. Seltzer", "D. Yu", "A. Acero", "A. Mohamed", "G. Hinton"], "venue": "In Interspeech", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "Nice: Non-linear independent components estimation", "author": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Multi-prediction deep Boltzmann machines. In NIPS26", "author": ["Ian J. Goodfellow", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "venue": "NIPS Foundation,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In International Conference on Machine Learning (ICML\u20192014),", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen"], "venue": "In AISTATS\u20192010,", "citeRegEx": "Gutmann and Hyvarinen.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["David Heckerman", "David Maxwell Chickering", "Christopher Meek", "Robert Rounthwaite", "Carl Kadie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Heckerman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 2000}, {"title": "Products of experts", "author": ["Geoffrey E. Hinton"], "venue": "In ICANN\u20191999,", "citeRegEx": "Hinton.,? \\Q1999\\E", "shortCiteRegEx": "Hinton.", "year": 1999}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Consistency of pseudolikelihood estimation of fully visible boltzmann machines", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2006\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2006}, {"title": "Fast gradient-based inference with continuous latent variable models in auxiliary form", "author": ["Diederik P. Kingma"], "venue": "Technical report,", "citeRegEx": "Kingma.,? \\Q2013\\E", "shortCiteRegEx": "Kingma.", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["Durk P. Kingma", "Max Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In AISTATS\u20192011,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Ng"], "venue": "In NIPS\u201906,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Texture modeling with convolutional spike-and-slab RBMs and deep extensions", "author": ["Heng Luo", "Pierre Luc Carrier", "Aaron Courville", "Yoshua Bengio"], "venue": "In AISTATS\u20192013,", "citeRegEx": "Luo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "In ICML\u20192014,", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Deep Boltzmann machines and the centering trick", "author": ["Gregoire Montavon", "Klaus-Robert Muller"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Montavon and Muller.,? \\Q2012\\E", "shortCiteRegEx": "Montavon and Muller.", "year": 2012}, {"title": "Deep directed generative autoencoders", "author": ["Sherjil Ozair", "Yoshua Bengio"], "venue": "Technical report, U. Montreal,", "citeRegEx": "Ozair and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Ozair and Bengio.", "year": 2014}, {"title": "Multimodal transitions for generative stochastic networks", "author": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "venue": "Technical report, U. Montreal,", "citeRegEx": "Ozair et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ozair et al\\.", "year": 2014}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In UAI\u20192011, Barcelona,", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS\u20192006,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "Technical report,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Salah Rifai", "Yoshua Bengio", "Yann Dauphin", "Pascal Vincent"], "venue": "In ICML\u201912,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Deep Boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "R\u00e9seaux de neurones \u00e0 relaxation entr\u00e2\u0131n\u00e9s par crit\u00e8re d\u2019autoencodeur d\u00e9bruitant", "author": ["Fran\u00e7ois Savard"], "venue": "Master\u2019s thesis, U. Montre\u0301al,", "citeRegEx": "Savard.,? \\Q2011\\E", "shortCiteRegEx": "Savard.", "year": 2011}, {"title": "Perturbation theory and finite markov chains", "author": ["Paul J Schweitzer"], "venue": "Journal of Applied Probability,", "citeRegEx": "Schweitzer.,? \\Q1968\\E", "shortCiteRegEx": "Schweitzer.", "year": 1968}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "In Interspeech", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Sebastian H. Seung"], "venue": "In NIPS\u201997,", "citeRegEx": "Seung.,? \\Q1998\\E", "shortCiteRegEx": "Seung.", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "ICML", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "On the equivalence between deep nade and generative stochastic networks", "author": ["Li Yao", "Sherjil Ozair", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Technical report, U. Montreal,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Deep supervised and convolutional generative stochastic network for protein secondary structure prediction", "author": ["Jian Zhou", "Olga G. Troyanskaya"], "venue": "In ICML\u20192014,", "citeRegEx": "Zhou and Troyanskaya.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Troyanskaya.", "year": 2014}, {"title": "General stochastic networks for classification", "author": ["Matthias Z\u00f6hrer", "Franz Pernkopf"], "venue": "In NIPS\u20192014,", "citeRegEx": "Z\u00f6hrer and Pernkopf.,? \\Q2014\\E", "shortCiteRegEx": "Z\u00f6hrer and Pernkopf.", "year": 2014}, {"title": "P\u03b8n(X|X\u0303) be a denoising auto-encoder that has been trained on n training examples. P\u03b8n(X|X\u0303) assigns a probability to X, given X\u0303, when X\u0303 \u223c C(X\u0303|X). This estimator defines a Markov chain Tn obtained by sampling alternatively an X\u0303 from C(X\u0303|X) and an X from P\u03b8(X|X\u0303). Let \u03c0n be the asymptotic distribution of the chain", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2013\\E", "shortCiteRegEx": "Bengio", "year": 2013}], "referenceMentions": [{"referenceID": 26, "context": "(2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoder variants (Bengio", "startOffset": 145, "endOffset": 166}, {"referenceID": 2, "context": "Research in deep learning (see Bengio (2009) and Bengio et al.", "startOffset": 31, "endOffset": 45}, {"referenceID": 2, "context": "Research in deep learning (see Bengio (2009) and Bengio et al. (2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 33, "context": ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).", "startOffset": 36, "endOffset": 76}, {"referenceID": 40, "context": ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).", "startOffset": 36, "endOffset": 76}, {"referenceID": 17, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 18, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 46, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 31, "context": ", 2011) and object recognition (Krizhevsky et al., 2012).", "startOffset": 31, "endOffset": 56}, {"referenceID": 31, "context": "The latest breakthrough in object recognition (Krizhevsky et al., 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 27, "context": ", 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al., 2012).", "startOffset": 155, "endOffset": 176}, {"referenceID": 26, "context": "On the other hand, progress with deep unsupervised architectures has been slower, with the established approaches with a probabilistic footing being the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009).", "startOffset": 179, "endOffset": 200}, {"referenceID": 43, "context": ", 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009).", "startOffset": 45, "endOffset": 77}, {"referenceID": 36, "context": "Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon and Muller, 2012; Goodfellow et al., 2013).", "startOffset": 268, "endOffset": 320}, {"referenceID": 21, "context": "Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon and Muller, 2012; Goodfellow et al., 2013).", "startOffset": 268, "endOffset": 320}, {"referenceID": 27, "context": "For example, one could use a convolutional architecture with max-pooling for parametric parsimony and computational efficiency, or dropout (Hinton et al., 2012) to prevent co-adaptation of hidden representations.", "startOffset": 139, "endOffset": 160}, {"referenceID": 2, "context": "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al.", "startOffset": 313, "endOffset": 335}, {"referenceID": 2, "context": "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al. (2013). The experimental results show that such a model with latent states indeed mixes better than shallower models without them (Table 1).", "startOffset": 313, "endOffset": 357}, {"referenceID": 24, "context": "6 \u2013 Dependency networks: Finally, an unexpected result falls out of the GSN theory: it allows us to provide a novel justification for dependency networks (Heckerman et al., 2000) and for the first time define a proper joint distribution between all the visible variables that is learned by such models (Section 3.", "startOffset": 154, "endOffset": 178}, {"referenceID": 43, "context": "Deep Boltzmann machines (Salakhutdinov and Hinton, 2009) combine the difficulty of inference (for the positive phase where one tries to push the energies associated with the observed x down) and also that of sampling (for the negative phase where one tries to push up the energies associated with x\u2019s sampled from P (x)).", "startOffset": 24, "endOffset": 56}, {"referenceID": 16, "context": "Sampling for the negative phase is usually done by MCMC, although some unsupervised learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010; Bordes et al., 2013) involve \u201cnegative examples\u201d that are sampled through simpler procedures (like perturbations of the observed input, in a spirit reminiscent of the approach presented here).", "startOffset": 104, "endOffset": 182}, {"referenceID": 23, "context": "Sampling for the negative phase is usually done by MCMC, although some unsupervised learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010; Bordes et al., 2013) involve \u201cnegative examples\u201d that are sampled through simpler procedures (like perturbations of the observed input, in a spirit reminiscent of the approach presented here).", "startOffset": 104, "endOffset": 182}, {"referenceID": 11, "context": "Sampling for the negative phase is usually done by MCMC, although some unsupervised learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010; Bordes et al., 2013) involve \u201cnegative examples\u201d that are sampled through simpler procedures (like perturbations of the observed input, in a spirit reminiscent of the approach presented here).", "startOffset": 104, "endOffset": 182}, {"referenceID": 2, "context": "For example consider the denoising transitions studied by Bengio et al. (2013c) and illustrated in Figure 1, where x\u0303t\u22121 is a stochastically corrupted version of xt\u22121 and we learn the denoising distribution P (x|x\u0303).", "startOffset": 58, "endOffset": 80}, {"referenceID": 39, "context": "Besides the approach discussed here, there may well be other very different ways of evading this problem of intractable marginalization, including approaches such as sumproduct networks (Poon and Domingos, 2011), which are based on learning a probability function that has a tractable form by construction and yet is from a flexible enough family of distributions.", "startOffset": 186, "endOffset": 211}, {"referenceID": 30, "context": "Another interesting direction of investigation that avoids the need for MCMC and intractable partition functions is the variational auto-encoder (Kingma and Welling, 2014; Gregor et al., 2014; Mnih and Gregor, 2014; Rezende et al., 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 145, "endOffset": 237}, {"referenceID": 22, "context": "Another interesting direction of investigation that avoids the need for MCMC and intractable partition functions is the variational auto-encoder (Kingma and Welling, 2014; Gregor et al., 2014; Mnih and Gregor, 2014; Rezende et al., 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 145, "endOffset": 237}, {"referenceID": 35, "context": "Another interesting direction of investigation that avoids the need for MCMC and intractable partition functions is the variational auto-encoder (Kingma and Welling, 2014; Gregor et al., 2014; Mnih and Gregor, 2014; Rezende et al., 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 145, "endOffset": 237}, {"referenceID": 41, "context": "Another interesting direction of investigation that avoids the need for MCMC and intractable partition functions is the variational auto-encoder (Kingma and Welling, 2014; Gregor et al., 2014; Mnih and Gregor, 2014; Rezende et al., 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 145, "endOffset": 237}, {"referenceID": 12, "context": ", 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 36, "endOffset": 89}, {"referenceID": 37, "context": ", 2014) and related directed models (Bornschein and Bengio, 2014; Ozair and Bengio, 2014), which rely on learned approximate inference.", "startOffset": 36, "endOffset": 89}, {"referenceID": 0, "context": "In a recent paper, Alain and Bengio (2013) showed that denoising auto-encoders with small Gaussian corruption and squared error loss estimated the score (derivative of the logdensity with respect to the input) of continuous observed random variables, thus implicitly estimating P (X).", "startOffset": 19, "endOffset": 43}, {"referenceID": 25, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 26, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 4, "context": "This is an instance of the so-called reparametrization trick, already proposed in (Bengio, 2013; Kingma, 2013; Kingma and Welling, 2014).", "startOffset": 82, "endOffset": 136}, {"referenceID": 29, "context": "This is an instance of the so-called reparametrization trick, already proposed in (Bengio, 2013; Kingma, 2013; Kingma and Welling, 2014).", "startOffset": 82, "endOffset": 136}, {"referenceID": 30, "context": "This is an instance of the so-called reparametrization trick, already proposed in (Bengio, 2013; Kingma, 2013; Kingma and Welling, 2014).", "startOffset": 82, "endOffset": 136}, {"referenceID": 36, "context": "One possible solution to this problem is to use a multimodal reconstruction distribution such as in Ozair et al. (2014), Larochelle and Murray (2011), or Dinh et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 31, "context": "(2014), Larochelle and Murray (2011), or Dinh et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 19, "context": "(2014), Larochelle and Murray (2011), or Dinh et al. (2015). We propose here another solution, which can be combined with the above, that consists in allowing a different level of entropy for different steps of the walkback.", "startOffset": 41, "endOffset": 60}, {"referenceID": 14, "context": "A good overview of results from perturbation theory discussing stationary distributions in finite state Markov chains can be found in (Cho et al., 2000).", "startOffset": 134, "endOffset": 152}, {"referenceID": 45, "context": "Theorem 4 Adapted from (Schweitzer, 1968)", "startOffset": 23, "endOffset": 41}, {"referenceID": 49, "context": "This method is described in (Williams, 1992).", "startOffset": 28, "endOffset": 44}, {"referenceID": 24, "context": "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x\u2212i), where x\u2212i denotes x \\ xi, i.", "startOffset": 20, "endOffset": 44}, {"referenceID": 24, "context": "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x\u2212i), where x\u2212i denotes x \\ xi, i.e., the set of variables other than the i-th one, xi. Note that each Pi may be parametrized separately, thus not guaranteeing that there exists a joint of which they are the conditionals. Instead of the ordered pseudoGibbs sampler defined in Heckerman et al. (2000), which resamples each variable xi in the order x1, x2, .", "startOffset": 21, "endOffset": 398}, {"referenceID": 21, "context": "This has been exploited in order to obtain much better samples using the associated GSN Markov chain than by sampling from the corresponding DBM (Goodfellow et al., 2013).", "startOffset": 145, "endOffset": 170}, {"referenceID": 26, "context": "Hyv\u00e4rinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "This result can be applied to show that the multi-prediction deep Boltzmann machine (MP-DBM) training procedure introduced by Goodfellow et al. (2013) also corresponds to a GSN.", "startOffset": 126, "endOffset": 151}, {"referenceID": 10, "context": "A Theano1 (Bergstra et al., 2010) implementation is available2, including the links of datasets.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "This is also true of the asymptotic estimation results in Alain and Bengio (2013). We experimentally validate the above theorems in a", "startOffset": 58, "endOffset": 82}, {"referenceID": 15, "context": "We compare against a state-of-the-art RBM (Cho et al., 2013) with an AIS log-likelihood estimate of -64.", "startOffset": 42, "endOffset": 60}, {"referenceID": 43, "context": "We propose here to explore families of parametrizations which are similar to existing deep stochastic architectures such as the Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009).", "startOffset": 157, "endOffset": 189}, {"referenceID": 4, "context": "An interesting source of inspiration regarding this question is a recent paper on estimating or propagating gradients through stochastic neurons (Bengio, 2013).", "startOffset": 145, "endOffset": 159}, {"referenceID": 47, "context": "To emulate a sampling procedure similar to Boltzmann machines in which the filled-in missing values can depend on the representations at the top level, the computational graph allows information to propagate both upwards (from input to higher levels) and downwards, giving rise to the computational graph structure illustrated in Figure 5, which is similar to that explored for deterministic recurrent auto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011).", "startOffset": 416, "endOffset": 457}, {"referenceID": 1, "context": "To emulate a sampling procedure similar to Boltzmann machines in which the filled-in missing values can depend on the representations at the top level, the computational graph allows information to propagate both upwards (from input to higher levels) and downwards, giving rise to the computational graph structure illustrated in Figure 5, which is similar to that explored for deterministic recurrent auto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011).", "startOffset": 416, "endOffset": 457}, {"referenceID": 44, "context": "To emulate a sampling procedure similar to Boltzmann machines in which the filled-in missing values can depend on the representations at the top level, the computational graph allows information to propagate both upwards (from input to higher levels) and downwards, giving rise to the computational graph structure illustrated in Figure 5, which is similar to that explored for deterministic recurrent auto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011).", "startOffset": 416, "endOffset": 457}, {"referenceID": 2, "context": "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST dataset and the Toronto Face Database (TFD), following the setup in Bengio et al. (2013b).", "startOffset": 174, "endOffset": 196}, {"referenceID": 13, "context": "Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10,000 consecutively generated samples (using the realvalued mean-field reconstructions as the training data for the Parzen density estimator).", "startOffset": 10, "endOffset": 33}, {"referenceID": 43, "context": "We have also evaluated a 3-hidden layer DBM (Salakhutdinov and Hinton, 2009), using the weights provided by the author, and obtained a Parzen log-likelihood bound of 32\u00b12.", "startOffset": 44, "endOffset": 76}, {"referenceID": 2, "context": "In comparison, Bengio et al. (2013b) report a log-likelihood bound of -244\u00b154 for RBMs and 138\u00b12 for a 2-hidden layer DBN, using the same setup.", "startOffset": 15, "endOffset": 37}, {"referenceID": 2, "context": "However, in this paper, to be consistent with the numbers given in Bengio et al. (2013b) we used a Gaussian Parzen density, which makes the numbers not comparable with the AIS log-likelihood upper bounds for binarized images reported in other papers for the same data.", "startOffset": 67, "endOffset": 89}, {"referenceID": 20, "context": "44 on MNIST test set, which can be compared with the best reported result LL=225 from Goodfellow et al. (2014).", "startOffset": 86, "endOffset": 111}, {"referenceID": 34, "context": "The LL is not directly comparable to AIS likelihood estimates because we use a Gaussian mixture rather than a Bernoulli mixture to compute the likelihood, but we can compare with Rifai et al. (2012); Bengio et al.", "startOffset": 179, "endOffset": 199}, {"referenceID": 9, "context": "After early versions of this work were published (Bengio et al., 2014), the GSN framework has been extended and applied to classification problems in several different ways (Goodfellow et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 21, "context": ", 2014), the GSN framework has been extended and applied to classification problems in several different ways (Goodfellow et al., 2013; Zhou and Troyanskaya, 2014; Z\u00f6hrer and Pernkopf, 2014) yielding very interesting results.", "startOffset": 110, "endOffset": 190}, {"referenceID": 51, "context": ", 2014), the GSN framework has been extended and applied to classification problems in several different ways (Goodfellow et al., 2013; Zhou and Troyanskaya, 2014; Z\u00f6hrer and Pernkopf, 2014) yielding very interesting results.", "startOffset": 110, "endOffset": 190}, {"referenceID": 52, "context": ", 2014), the GSN framework has been extended and applied to classification problems in several different ways (Goodfellow et al., 2013; Zhou and Troyanskaya, 2014; Z\u00f6hrer and Pernkopf, 2014) yielding very interesting results.", "startOffset": 110, "endOffset": 190}, {"referenceID": 21, "context": "In addition to providing a consistent generative interpretation to dependency networks, GSNs have been used to provide one to Multi-Prediction Deep Boltzmann Machines (Goodfellow et al., 2013) and to provide a fast sampling algorithm for deep NADE (Yao et al.", "startOffset": 167, "endOffset": 192}, {"referenceID": 50, "context": ", 2013) and to provide a fast sampling algorithm for deep NADE (Yao et al., 2014).", "startOffset": 63, "endOffset": 81}], "year": 2015, "abstractText": "We introduce a novel training principle for generative probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework generalizes Denoising Auto-Encoders (DAE) and is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution is a conditional distribution that generally involves a small move, so it has fewer dominant modes and is unimodal in the limit of small moves. This simplifies the learning problem, making it less like density estimation and more akin to supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here provide a probabilistic interpretation for denoising autoencoders and generalize them; seen in the context of this framework, auto-encoders that learn with injected noise are a special case of GSNs and can be interpreted as generative models. The theorems also provide an interesting justification for dependency networks and generalized pseudolikelihood and define an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Experiments validating these theoretical results are conducted on both synthetic datasets and image datasets. The experiments employ a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but that allows training to proceed with backprop through a recurrent neural network with noise injected inside and without the need for layerwise pretraining.", "creator": "LaTeX with hyperref package"}}}