{"id": "1503.05615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Learning to Search for Dependencies", "abstract": "simultaneously we create... a transition - based dependency parser using a general purpose learning to search system. the result is a fast implemented and accurate parser for many languages. compared to other transition - based dependency parsing approaches, our parser provides similar statistical reasoning and computational performance with best - known approaches while avoiding various adaptive downsides including randomization, extra feature requirements, and custom learning algorithms. we show that nowadays it is possible to implement a dependency functional parser consistent with an open - source learning to search library in about approximately 300 lines of c + + code, while existing systems design often requires several thousands composed of lines.", "histories": [["v1", "Wed, 18 Mar 2015 23:33:17 GMT  (42kb,D)", "http://arxiv.org/abs/1503.05615v1", null], ["v2", "Thu, 7 May 2015 22:12:11 GMT  (48kb,D)", "http://arxiv.org/abs/1503.05615v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["kai-wei chang", "he he", "hal daum\\'e iii", "john langford"], "accepted": false, "id": "1503.05615"}, "pdf": {"name": "1503.05615.pdf", "metadata": {"source": "META", "title": "Learning to Search for Dependencies", "authors": ["Kai-Wei Chang", "He He", "Hal Daum\u00e9 III", "John Langford"], "emails": ["kchang10@illinois.edu", "hhe@cs.umd.edu", "hal@cs.umd.edu", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014). Virtually all of these solutions use various problem-specific customizations of machine learning algorithms or problemspecific customizations of feature sets. In this paper, we aim to answer the following question: Is it possible to use a general purpose learning algorithm with only basic features to effectively solve dependency parsing?\nTo be general purpose, the system must solve a wide array of other problems as well. To approach this goal, we investigated using a learning to search system (L2S) (Daume\u0301 III et al., 2014) which has been used to solve about a half-dozen other structured prediction problems. The learning to search system in turn relies on a machine learning subsystem which is in broad use by 1000\u2019s of people.\nTo use a learning to search approach a search space must be defined. A natural choice for a search space is given by the sequence of actions of a transition-based parser. Several authors have investigated transition-based parser approaches (Goldberg and Nivre, 2013; Zhang and Nivre, 2011; Chen and Manning, 2014; Kuhlmann et al., 2011) achieving success with custom learning algorithms or custom features. Here, we show this is possible with standard learning algorithms. Aside from the overall transition-based dependency parsing structure, the L2S parser uses a dynamic oracle (Goldberg and Nivre, 2013) and a single-hidden-layer neural network \u201cfor free\u201d (enabled by a simple flag), as opposed to a system built from scratch to use neural networks (Chen and Manning, 2014).\nThe primary advantage of this approach is a combination of correctness and simplicity. The simplicity is evidenced several ways:\n1. It requires only\u223c300 lines of C++ code when implementing based on an open-source learning to search library1. Table 1 shows the number of lines of other popular dependency parsing systems.\n2. We do not randomize 5 times and take the best result as in (Goldberg and Nivre, 2013).\n1Available at: https://github.com/ JohnLangford/vowpal_wabbit\nar X\niv :1\n50 3.\n05 61\n5v 1\n[ cs\n.C L\n] 1\n8 M\nar 2\n01 5\n3. The implementation is future-proofed: future improvements in underlying machine learning algorithms and learning to search framework may yield a better parser.\nCorrectness is a more subtle issue. Regret analysis of learning to search (Daume\u0301 III et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Chang et al., 2015) suggests how to settle various details:\n1. Use a cost sensitive learning algorithm instead of a multiclass learning algorithm.\n2. Use rollin with the learned policy.\n3. Use rollouts with either a good reference policy or a mixture of reference and learned policy.\nMany of these details are correctly hand-crafted in individual implementations, but it is also common to neglect one of these details, with any neglect resulting in an inconsistent approach.2 Empirically an inconsistent approach has a lower ceiling performance. Having a system which gets these details correct everytime is a large part of the value of a learning to search system.\nExperiments evaluated on standard English Penn Treebank and 9 other languages from CoNLL-X show that our parser is competitive with recent published results (an average labeled accuracy of 81.70 over 10 languages, versus 80.33 and 75.34 for state of the art parsers published last year). We achieve this with this much simpler implementation and with the strong theoretical guarantees inherited from learning to search."}, {"heading": "2 Learning to Search", "text": "Learning to search is a family of approaches for solving structured prediction tasks. This family includes a number of specific algorithms including the incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), SEARN (Daume\u0301 III et al., 2009), DAGGER (Ross et al., 2011),\n2Inconsistency is a subtle issue that is often not dealt with properly even in machine learning. Inconsistency is important when the truth is noisy. A simple illustrative example of inconsistency occurs when reducing 3-class classification to binary classification via a one-against-all approach (1 vs {2, 3},2 vs {1, 3}, and 3 vs {2, 1}). When the class label is inherently uncertain, with probability 0.4 for label 1 and 0.3 for label 2 and 3, the learned binary classifier prefers {2, 3}, {1, 3}, {2, 1} since in each case the probability of the pair of labels exceeds the probability of the single missing label. However, these predictions give no preference for label 1 which is the most likely class.\nAlgorithm 1 RUNTAGGER(words) 1: output\u2190 [] 2: for n = 1 to LEN(words) do 3: ref \u2190 words[n].true label 4: output[n]\u2190 PREDICT(words[i], ref, output[:n-1]) 5: end for 6: LOSS(# parent[n] 6= parent[n].true label) 7: return output\nAGGREVATE (Ross and Bagnell, 2014), and others (Daume\u0301 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014). Learning to search approaches solve structured prediction problems by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses that control a policy that takes actions in this search space.\nIn this work we build on recent theoretical and implementational advances in learning to search that make development of novel structured prediction frameworks easy and efficient using \u201cimperative learning to search\u201d (Daume\u0301 III et al., 2014). In this framework, an application developer needs to write (a) a \u201cdecoder\u201d for the target structured prediction task (e.g., dependency parsing), (b) an annotation in the decoder that computes losses on the training data, and (c) a reference policy on the training data that returns at any prediction point a \u201csuggestion\u201d as to a good action to take at that state3.\nAlgorithm 1 shows the code one must write for a part of speech tagger (or generic sequence labeler) under Hamming loss. The only annotation in this code aside from the calls to the library function PREDICT are the computation of an reference (an oracle reference is trivial under Hamming loss) and the computation of the total sequence loss at the end of the function. Note that in this example, the prediction of the tag for the nth word depends explicitly on the predictions of all previous words!\nThe machine learning question that arises is how to learn a good PREDICT function given just this information. The \u201cimperative learning to search\u201d answer (Daume\u0301 III et al., 2014) is essentially to run the RUNTAGGER function many times, \u201ctrying out\u201d different versions of PREDICT in order to learn one that yields low LOSS. The\n3Some papers in the past make an implicit or explicit assumption that this reference policy is an oracle policy: for every state, it always chooses the best action (assuming it gets to make all future decisions as well).\nchallenge is how to do this efficiently. The general strategy is, for some number of epochs, and for each example (x, y) in the training data, to do the following:\n1. Execute RUNTAGGER on x with some rollin policy to obtain a search trajectory (sequence of action a) and loss `0 2. Many times: (a) Choose some time step t \u2264 |a| (b) Choose an alternative action a\u2032t 6= at (c) Execute RUNTAGGER on x, with PRE-\nDICT return a1:t\u22121 initially, then a\u2032t, then acting according to a rollout policy to obtain a new loss `t,a\u2032t (d) Compare the overall losses `0 and `t,a\u2032t to construct a classification/regression example that demonstrates how much better or worse a\u2032t is than at in this context 3. Update the learned policy Figure 1 shows a schematic of the search space implicitly defined by an imperative program. By executing this program three times (in this example), we are able to explore three different trajectories and compute their losses. These trajectories are defined by the rollin policy (what determines the initial trajectory), the position of one-step deviations (here, stateR), and the rollout policy (which completes the trajectory after a deviation).\nBy varying the rollin policy, the rollout policy and the manner in which classification/regression examples are created, this general framework can mimic algorithms like SEARN, DAGGER and AGGREVATE. For instance, DAGGER uses rollin=learned policy4 and rollout=reference, while SEARN uses rollin=rollout=stochastic mixture of learned and reference policies."}, {"heading": "3 Dependency Parsing by Learning to Search", "text": "Learning to search provides a natural framework for implementing a transition-based dependency parser. A transition-based dependency parser takes a sequence of actions and parses a sentence from left to right by maintaining a stack S, a buffer B, and a set of dependency arcs A. The stack maintains partial parses, the buffer stores the words to be parsed, and A keeps the arcs that have been\n4Technically, DAGGER rolls in with a mixture which is almost always instantiated to be \u201creference\u201d for the first epoch and \u201clearned\u201d for subsequent epochs.\ngenerated so far. The configuration of the parser at each stage can be defined by a triple (S,B,A). For the ease of notation, we usewp to represent the leftmost word in the buffer and use s1 and s2 to denote the top and the second top words in the stack. A dependency arc (wh, wm) is a directed edge that indicates wordwh is the parent of wordwm. When the parser terminates, the arcs in A form a projective dependency tree. We assume that each word only has one parent in the derived dependency parse tree, and use A[wm] to denote the parent of wordwm. For labeled dependency parsing, we further assign a tag to each arc representing the dependency type between the head and the modifier. For simplicity, we assume an unlabeled parser in the following description. The extension from an unlabeled parser to a labeled parser is straightforward, and is discussed at the end of this section.\nWe consider an arc-hybrid transition system (Kuhlmann et al., 2011)5. In the initial con-\n5The learning to search framework is also suitable for other transition-based dependency parsing systems, such as arc-eager (Nivre, 2003) or arc-standard (Nivre, 2004) transition systems.\nAlgorithm 2 TRANS(S, B, A, action) 1: Let wp be the leftmost element in B 2: if action = SHIFT then 3: S.push(wp) 4: remove wp from B 5: else if action= REDUCE-LEFT then 6: top\u2190 S.pop() 7: A\u2190 A\u222a (wp,top) 8: else if action = REDUCE-RIGHT then 9: top\u2190 S.pop()\n10: A\u2190 A\u222a (S.top(), top) 11: end if 12: return S,B,A\nfiguration, the buffer B contains all the words in the sentence, a dummy root node is pushed in the stack S, and the set of arcs A is empty. The root node cannot be popped out at anytime during parsing. The system then takes a sequence of actions until the buffer is empty and the stack contains only the root node (i.e., |B| = 0 and S = {Root}). When the process terminates, a parse tree is derived. At each state, the system can take one of the following actions:\n1. SHIFT: push wp to S and move p to the next word. (Valid when |B| > 0).\n2. REDUCE-LEFT: add an arc (wp, s1) to A and pop s1. (Valid when |B| > 0 and |S| > 1).\n3. REDUCE-RIGHT: add an arc (s2, s1) toA and pop s1. (Valid when |S| > 1).\nAlgorithm 2 shows the execution of these actions during parsing, and Figure 2 demonstrates an example of transition-based dependency parsing.\nAlgorithm 3 RUNPARSER(sentence) 1: stack S\u2190 {Root} 2: buffer B\u2190 [words in sentence] 3: arcs A\u2190 \u2205 4: while B 6= \u2205 or |S| > 1 do 5: ValidActs\u2190 GETVALIDACTIONS(S,B) 6: features\u2190 GETFEAT(S,B,A) 7: ref \u2190 GETGOLDACTION(S,B) 8: action\u2190 PREDICT(features, ref, ValidActs) 9: S,B,A\u2190 TRANS(S,B,A, action)\n10: end while 11: LOSS(A[w] 6= A\u2217[w], \u2200w \u2208 sentence) 12: return output\nWe can define a search space for dependency parser such that each state represents one configuration during the parsing. The start state is associated with the initial configuration, and the end states are associated with the configurations that |B| = 0 and S = {Root}. The loss of each end state is defined by the distance between the derived parse tree and the gold parse tree. The above transition actions define how to move from one search state to the other. In the following, we describe our implementation details. Implementation As mentioned in Section 2, to implement a parser using the learning to search framework, we need to provide a decoder, a loss function and reference policy. Thanks to recent work (Goldberg and Nivre, 2013), we know how to compute a \u201cdynamic oracle\u201d reference policy that is optimal. The loss can be measured by how many parents are different between the derived parse tree and the gold annotated parse tree. Algorithm 3 shows the pseudo-code of a decoder for\na unlabeled dependency parser. We discuss each subcomponent below.\n\u2022 GETVALIDACTION returns a set of valid actions that can be taken based on the current configuration.\n\u2022 GETFEAT extracts features based on the current configuration. The features depend on the top few words in the stack and leftmost few words in the buffer as well as their associated part-of-speech tags. We list our feature templates in Table 3. All features are generated dynamically because configuration changes during parsing.\n\u2022 GETGOLDACTION implements the dynamic oracle described in (Goldberg and Nivre, 2013). The dynamic oracle returns the optimal action at any state that leads to the reachable end state with the minimal loss.\n\u2022 PREDICT is a library call implemented in the learning to search system. Given training samples, the learning to search system can learn the policy automatically. Therefore, in the test phase, this function returns the predicted action leading to an end state with small structured loss.\n\u2022 TRANS function implements the hybrid-arc transition system. Based on the predicted action and labels, it updates the parser\u2019s configuration, and move the agent to the next search state.\n\u2022 LOSS function is used to measure the distance between the predicted output and the gold annotation. Here, we simply used the\nnumber of words for which the parent is wrong as the loss. The LOSS has no effect in the test phase.\nThe above decoder implements an unlabeled parser. To build a labeled parser, when the transition action is REDUCE-LEFT or REDUCE-RIGHT, we call the PREDICT function again to predict the dependency type of the arc. The loss in the labled dependnecy parser can be measured by\u2211\nwi loss(wi), where\nloss(wi) =  2 A[wi] 6= A\u2217[wi] 1 A[wi] = A\n\u2217[wi], L[wi] 6= L\u2217[wi] 0 Otherwise.\n(1)\nA[wi] and A\u2217[wi] are the parent of wi in the derived parse tree and gold parse tree, respectively, L[wi] is the label assign to the arc (A[wi], wi). We observe that this simple loss function performs well empericially.\nWe implemented our parser based on an opensource library supporting learning to search. The implementation requires about 300 lines of C++ code. Table 2 shows the number of code lines for each function. The reduction of implementation\neffort comes from two-folds. First, in the learning to search framework, there is no need to implement a learning algorithm. Once the decoding function is defined, the system is able to learn the best \u201cPREDICT\u201d function from training data. Second, L2S provides a unified framework, which allows the library to serve common functions for ease of implementation. For example, quadratic and cubic feature generating functions and a feature hashing mechanism are provided by the library. The unified framework also allows a user to experiment with different base learners and hyperparameters using command line arguments without modifying the code. Base Learner As mentioned in Section 2, the learning to search framework reduces structured prediction to cost-sensitive multi-class classification, which can be further reduced to regression. This reduction framework allows us to employ well-studied binary and multi-class classification methods as the base learner. We analyze the value of using more powerful base learners in the experiment section."}, {"heading": "4 Experimental Results", "text": "While most work compares with MaltParser or MSTParser, which are indeed weak baselines, we compare with two recent strong baselines: the greedy transition-based parser with dynamic oracle (Goldberg and Nivre, 2013) and the Stanford neural network parser (Chen and Manning, 2014). We evaluate on a wide range of different languages, and show that our parser achieves comparable or better results on all languages, with significantly less engineering."}, {"heading": "4.1 Datasets", "text": "We conduct experiments on the English Penn Treebank (PTB) (Marcus et al., 1993) and the CoNLL-X (Buchholz and Marsi, 2006) datasets for 9 other languages, including Arabic, Bulgarian, Chinese, Danish, Dutch, Japanese, Portuguese, Slovene and Swedish. For PTB, we convert the constituency trees to dependencies by the head rules of Yamada and Matsumoto (2006). We follow the standard split: sections 2 to 21 for training, section 22 for development and section 23 for testing. The POS tags in the evaluation data is assigned by the Stanford POS tagger (Toutanova et al., 2003), which has an accuracy of 97.2% on the PTB test set. For CoNLL-X, we use the given\ntrain/test splits and reserve the last 10% of training data for development if needed. The gold POS tags given in the CoNLL-X datasets are used."}, {"heading": "4.2 Setup and Parameters", "text": "For L2S, the rollin policy is a mixture of the current (learned) policy and the reference (dynamic oracle) policy. The probability of executing the reference policy decreases over each round. Specifically, we set it to be 1 \u2212 (1 \u2212 \u03b1)t, where t is the number of rounds and \u03b1 is set to 10\u22125 in all experiments. It has been shown (Ross and Bagnell, 2014; Chang et al., 2015) that when the reference policy is optimal, it is preferable to roll out with the reference. Therefore, we roll out with the dynamic oracle (Goldberg and Nivre, 2013).\nOur base learner is a simple neural network with one hidden layer. The hidden layer size is 5 and we do not use word or POS tag embeddings. We find the Follow-the-Regularized-Leader-Proximal (FTRL) online learning algorithm particularly effective with learning the neural network and simply use default hyperparameters.\nWe compare with the recent transition-based parser with dynamic oracles (DYNA) (Goldberg and Nivre, 2013), and the Stanford neural network parser (SNN) (Chen and Manning, 2014). Settings of the three parsers are shown in Table 5.\nFor DYNA, we use the software provided by the authors online6. Our initial experiments show that its performance is the best using the arc hybrid system with exploration parameters k = 1, p = 1, thus we use this setting for all experiments. The best model evaluated on the development set among 5 runs with different random seeds are chosen for testing.\nFor SNN, we use the latest Stanford parser.7 Since all other parsers do not use external resources, we do not provide pretrained word embeddings and initialize randomly. We use the same\n6Available at https://bitbucket.org/yoavgo/ tacl2013dynamicoracles\n7Available at http://nlp.stanford.edu/ software/nndep.shtml\nparameter values as suggested in (Chen and Manning, 2014), which are also the default settings of the software. The best model over 20000 iterations evaluated on the development set is used for testing.8\nIn addition, we compare with the RedShift9 parser on PTB. For fair comparison, we only use its basic features (excluding features based on the Brown cluster). We use the default parameters, which runs a beam search with width 8. In our experiments, the RedShift parser has UAS 92.10 and LAS 90.83 on the PTB test set."}, {"heading": "4.3 Results", "text": "We report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) in Table 4.10 Punctuation is excluded in all evaluations. Our parser achieves up to 4% improvement on both UAS and LAS. Compared with DYNA, our parser has the same transition system and oracle but more\n8Enabled by -saveIntermediate. 9Available at https://github.com/syllog1sm/ redshift 10 We notice that the Stanford neural network parser does particularly bad on Arabic, Portuguese and Slovene. One reason is that these languages do not have a \u201cROOT\u201d label in the dataset, and this is currently not handled well by the software.\npowerful base learners to choose from. Compared with SNN, we use much fewer hidden units and parameters to tune."}, {"heading": "5 The Value of Strong Base Learners", "text": "As mentioned in Section 3, a key advantage of L2S framework is that we can leverage from well studied binary and multi-class calcification methods in the literature. In this section, we show the empirical evidence for this merit.\nWe compare the performance of our parser when training with the following base learners\n\u2022 SGD: a learner with SGD update rules\n\u2022 Default: the default base classifier. This is an improved SGD-style update rule using an adaptive metric (Duchi et al., 2011; McMahan and Streeter, 2010), importance invariant updates (Karampatziakis and Langford, 2011), and normalized updates (Ross et al., 2013).\n\u2022 NN: a single-hidden-layer neural network with 5 hidden nodes.\n\u2022 NN + FTRL: a neural network learner with follow-the-regularized-leader regularization. This is the base learner we used in our parsing system.\nTable 6 shows the performance of our parser on PTB data with different base classifiers. Results show that using a strong base learner can improve the performance by around 3%.\nFinally, Figure 7 shows the performance of different feature templates. Using a comprehensive set of features leads to a better dependency parser."}, {"heading": "6 Related Work", "text": "Training a transition-based dependency parser can be viewed as an imitation learning problem. However, most early works focus on decoding or feature engineering instead of the core learning algorithm. For a long time, averaged perceptron is the default learner for dependency parsing. Goldberg and Nivre (2013) first proposed dynamic oracles under the framework of imitation learning. Their approach is essentially a special case of our algorithm: the base learner is a multi-class perceptron, and no rollout is executed to assign cost to actions. In this work, we combine dynamic oracles into learning and explore the search space in a more principled way by learning to search: by cost-sensitive classification, we evaluate the end result of each non-optimal action instead of treating them as equally bad.\nThere are a number of works that use the L2S approach to solve various other structured prediction problems, for example, sequence labeling (Doppa et al., 2014), coreference resolution (Ma et al., 204), graph-based dependency parsing (He et al., 2013). However, these works can be considered as a special setting under our unified learning framework, e.g., with a custom action set or different rollin/rollout methods.\nTo our knowledge, this is the first work that develops a general programming interface for dependency parsing, or more broadly, for structured prediction. Our system bears some resemblance to probabilistic programming language (e.g., (McCallum et al., 2009; Gordon et al., 2014)), however, instead of relying on a new programming language, ours is implemented in C++ and Python, thus is easily accessible."}, {"heading": "7 Conclusion and Discussion", "text": "We have described a simple transition-based dependency parser based on the learning to search framework. We show that it is now much easier to implement a high-performance dependency parser. Furthermore, we provide a wide range of advanced optimization methods to choose from during training. Experimental results show that we consistently achieve better performance across 10 languages. An interesting direction for future work is to extend the current system beyond greedy search. In addition, there is a large room for speeding up training time by smartly choosing where to rollout."}], "references": [{"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149\u2013164. Association for Computational Linguistics.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Learning to search better than your teacher", "author": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford."], "venue": "arXiv:1502.02206.", "citeRegEx": "Chang et al\\.,? 2015", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL).", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu."], "venue": "Machine Learning Journal.", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Efficient programmable learning to search", "author": ["Hal Daum\u00e9 III", "John Langford", "St\u00e9phane Ross."], "venue": "arXiv:1406.1837.", "citeRegEx": "III et al\\.,? 2014", "shortCiteRegEx": "III et al\\.", "year": 2014}, {"title": "Output space search for structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Doppa et al\\.,? 2012", "shortCiteRegEx": "Doppa et al\\.", "year": 2012}, {"title": "HC-Search: A learning framework for search-based structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli."], "venue": "Journal of Artificial Intelligence Research (JAIR), 50.", "citeRegEx": "Doppa et al\\.,? 2014", "shortCiteRegEx": "Doppa et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "author": ["Yoav Goldberg", "Michael Elhadad."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Goldberg and Elhadad.,? 2010", "shortCiteRegEx": "Goldberg and Elhadad.", "year": 2010}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the ACL, 1.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Yoav Goldberg", "Francesco Sartorio", "Giorgio Satta."], "venue": "Transactions of the ACL, 2.", "citeRegEx": "Goldberg et al\\.,? 2014", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Probabilistic programming", "author": ["Andrew D. Gordon", "Thomas A. Henzinger", "Aditya V. Nori", "Sriram K. Rajamani."], "venue": "Proceedings of the on Future of Software Engineering.", "citeRegEx": "Gordon et al\\.,? 2014", "shortCiteRegEx": "Gordon et al\\.", "year": 2014}, {"title": "Dynamic feature selection for dependency parsing", "author": ["He He", "Hal Daum\u00e9 III", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "He et al\\.,? 2013", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Online importance weight aware updates", "author": ["Nikos Karampatziakis", "John Langford."], "venue": "UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, pages 392\u2013399.", "citeRegEx": "Karampatziakis and Langford.,? 2011", "shortCiteRegEx": "Karampatziakis and Langford.", "year": 2011}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL).", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Dynamic programming algorithms for transition-based dependency parsers", "author": ["Marco Kuhlmann", "Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini."], "venue": "Computational linguistics, 19(2):330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "McCallum et al\\.,? 2009", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL).", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew J. Streeter."], "venue": "COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 244\u2013256.", "citeRegEx": "McMahan and Streeter.,? 2010", "shortCiteRegEx": "McMahan and Streeter.", "year": 2010}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "International Workshop on Parsing Technologies (IWPT), pages 149\u2013160.", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Boosting structured prediction for imitation learning", "author": ["Nathan Ratliff", "David Bradley", "J. Andrew Bagnell", "Joel Chestnutt."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Ratliff et al\\.,? 2007", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["St\u00e9phane Ross", "J. Andrew Bagnell."], "venue": "arXiv:1406.5979.", "citeRegEx": "Ross and Bagnell.,? 2014", "shortCiteRegEx": "Ross and Bagnell.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoff J. Gordon", "J. Andrew Bagnell."], "venue": "Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats).", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Normalized online learning", "author": ["St\u00e9phane Ross", "Paul Mineiro", "John Langford."], "venue": "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, Bellevue, WA, USA, August 11-15, 2013.", "citeRegEx": "Ross et al\\.,? 2013", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Umar Syed", "Robert E. Schapire."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Syed and Schapire.,? 2011", "shortCiteRegEx": "Syed and Schapire.", "year": 2011}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasbupta", "John Langford", "Alex Smola", "Josh Attenberg."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Weinberger et al\\.,? 2009", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "On learning linear ranking functions for beam search", "author": ["Yuehua Xu", "Alan Fern."], "venue": "ICML, pages 1047\u20131054.", "citeRegEx": "Xu and Fern.,? 2007", "shortCiteRegEx": "Xu and Fern.", "year": 2007}, {"title": "Discriminative learning of beam-search heuristics for planning", "author": ["Yuehua Xu", "Alan Fern", "Sung Wook Yoon."], "venue": "IJCAI, pages 2041\u20132046.", "citeRegEx": "Xu et al\\.,? 2007", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Statistical dependency analysis with support Vector Machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "International Workshop on Parsing Technologies (IWPT).", "citeRegEx": "Yamada and Matsumoto.,? 2006", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2006}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 188\u2013193. Association for Computational Linguistics.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014).", "startOffset": 106, "endOffset": 211}, {"referenceID": 23, "context": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014).", "startOffset": 106, "endOffset": 211}, {"referenceID": 17, "context": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014).", "startOffset": 106, "endOffset": 211}, {"referenceID": 10, "context": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014).", "startOffset": 106, "endOffset": 211}, {"referenceID": 12, "context": "Dependency parsing is a well-known problem with a long history of development resulting in many solutions (McDonald et al., 2005; Nivre, 2003; Koo et al., 2008; Goldberg and Elhadad, 2010; Goldberg et al., 2014).", "startOffset": 106, "endOffset": 211}, {"referenceID": 11, "context": "L2S (ours) \u223c300 Stanford \u223c3K RedShift \u223c2K (Goldberg and Nivre, 2013) \u223c4K Malt Parser \u223c10K", "startOffset": 42, "endOffset": 68}, {"referenceID": 11, "context": "Several authors have investigated transition-based parser approaches (Goldberg and Nivre, 2013; Zhang and Nivre, 2011; Chen and Manning, 2014; Kuhlmann et al., 2011) achieving success with custom learning algorithms or custom features.", "startOffset": 69, "endOffset": 165}, {"referenceID": 35, "context": "Several authors have investigated transition-based parser approaches (Goldberg and Nivre, 2013; Zhang and Nivre, 2011; Chen and Manning, 2014; Kuhlmann et al., 2011) achieving success with custom learning algorithms or custom features.", "startOffset": 69, "endOffset": 165}, {"referenceID": 2, "context": "Several authors have investigated transition-based parser approaches (Goldberg and Nivre, 2013; Zhang and Nivre, 2011; Chen and Manning, 2014; Kuhlmann et al., 2011) achieving success with custom learning algorithms or custom features.", "startOffset": 69, "endOffset": 165}, {"referenceID": 18, "context": "Several authors have investigated transition-based parser approaches (Goldberg and Nivre, 2013; Zhang and Nivre, 2011; Chen and Manning, 2014; Kuhlmann et al., 2011) achieving success with custom learning algorithms or custom features.", "startOffset": 69, "endOffset": 165}, {"referenceID": 11, "context": "Aside from the overall transition-based dependency parsing structure, the L2S parser uses a dynamic oracle (Goldberg and Nivre, 2013) and a single-hidden-layer neural network \u201cfor free\u201d (enabled by a simple flag), as opposed to a system built from scratch to use neural networks (Chen and Manning, 2014).", "startOffset": 107, "endOffset": 133}, {"referenceID": 2, "context": "Aside from the overall transition-based dependency parsing structure, the L2S parser uses a dynamic oracle (Goldberg and Nivre, 2013) and a single-hidden-layer neural network \u201cfor free\u201d (enabled by a simple flag), as opposed to a system built from scratch to use neural networks (Chen and Manning, 2014).", "startOffset": 279, "endOffset": 303}, {"referenceID": 11, "context": "We do not randomize 5 times and take the best result as in (Goldberg and Nivre, 2013).", "startOffset": 59, "endOffset": 85}, {"referenceID": 27, "context": "Regret analysis of learning to search (Daum\u00e9 III et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Chang et al., 2015) suggests how to settle various details:", "startOffset": 38, "endOffset": 125}, {"referenceID": 26, "context": "Regret analysis of learning to search (Daum\u00e9 III et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Chang et al., 2015) suggests how to settle various details:", "startOffset": 38, "endOffset": 125}, {"referenceID": 1, "context": "Regret analysis of learning to search (Daum\u00e9 III et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Chang et al., 2015) suggests how to settle various details:", "startOffset": 38, "endOffset": 125}, {"referenceID": 3, "context": "This family includes a number of specific algorithms including the incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), SEARN (Daum\u00e9 III et al.", "startOffset": 101, "endOffset": 146}, {"referenceID": 15, "context": "This family includes a number of specific algorithms including the incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), SEARN (Daum\u00e9 III et al.", "startOffset": 101, "endOffset": 146}, {"referenceID": 27, "context": ", 2009), DAGGER (Ross et al., 2011),", "startOffset": 16, "endOffset": 35}, {"referenceID": 26, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 32, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 33, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 25, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 29, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 7, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 8, "context": "AGGREVATE (Ross and Bagnell, 2014), and others (Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2011; Doppa et al., 2012; Doppa et al., 2014).", "startOffset": 47, "endOffset": 197}, {"referenceID": 18, "context": "We consider an arc-hybrid transition system (Kuhlmann et al., 2011)5.", "startOffset": 44, "endOffset": 67}, {"referenceID": 23, "context": "The learning to search framework is also suitable for other transition-based dependency parsing systems, such as arc-eager (Nivre, 2003) or arc-standard (Nivre, 2004) transition systems.", "startOffset": 123, "endOffset": 136}, {"referenceID": 24, "context": "The learning to search framework is also suitable for other transition-based dependency parsing systems, such as arc-eager (Nivre, 2003) or arc-standard (Nivre, 2004) transition systems.", "startOffset": 153, "endOffset": 166}, {"referenceID": 11, "context": "Thanks to recent work (Goldberg and Nivre, 2013), we know how to compute a \u201cdynamic oracle\u201d reference policy that is optimal.", "startOffset": 22, "endOffset": 48}, {"referenceID": 11, "context": "\u2022 GETGOLDACTION implements the dynamic oracle described in (Goldberg and Nivre, 2013).", "startOffset": 59, "endOffset": 85}, {"referenceID": 31, "context": "A feature hashing technique (Weinberger et al., 2009) is employed to provide a fast feature lookup.", "startOffset": 28, "endOffset": 53}, {"referenceID": 11, "context": "While most work compares with MaltParser or MSTParser, which are indeed weak baselines, we compare with two recent strong baselines: the greedy transition-based parser with dynamic oracle (Goldberg and Nivre, 2013) and the Stanford neural network parser (Chen and Manning, 2014).", "startOffset": 188, "endOffset": 214}, {"referenceID": 2, "context": "While most work compares with MaltParser or MSTParser, which are indeed weak baselines, we compare with two recent strong baselines: the greedy transition-based parser with dynamic oracle (Goldberg and Nivre, 2013) and the Stanford neural network parser (Chen and Manning, 2014).", "startOffset": 254, "endOffset": 278}, {"referenceID": 19, "context": "We conduct experiments on the English Penn Treebank (PTB) (Marcus et al., 1993) and the CoNLL-X (Buchholz and Marsi, 2006) datasets for 9 other languages, including Arabic, Bulgarian, Chinese, Danish, Dutch, Japanese, Portuguese, Slovene and Swedish.", "startOffset": 58, "endOffset": 79}, {"referenceID": 0, "context": ", 1993) and the CoNLL-X (Buchholz and Marsi, 2006) datasets for 9 other languages, including Arabic, Bulgarian, Chinese, Danish, Dutch, Japanese, Portuguese, Slovene and Swedish.", "startOffset": 24, "endOffset": 50}, {"referenceID": 30, "context": "The POS tags in the evaluation data is assigned by the Stanford POS tagger (Toutanova et al., 2003), which has an accuracy of 97.", "startOffset": 75, "endOffset": 99}, {"referenceID": 0, "context": ", 1993) and the CoNLL-X (Buchholz and Marsi, 2006) datasets for 9 other languages, including Arabic, Bulgarian, Chinese, Danish, Dutch, Japanese, Portuguese, Slovene and Swedish. For PTB, we convert the constituency trees to dependencies by the head rules of Yamada and Matsumoto (2006). We follow the standard split: sections 2 to 21 for training, section 22 for development and section 23 for testing.", "startOffset": 25, "endOffset": 287}, {"referenceID": 26, "context": "It has been shown (Ross and Bagnell, 2014; Chang et al., 2015) that when the reference policy is optimal, it is preferable to roll out with the reference.", "startOffset": 18, "endOffset": 62}, {"referenceID": 1, "context": "It has been shown (Ross and Bagnell, 2014; Chang et al., 2015) that when the reference policy is optimal, it is preferable to roll out with the reference.", "startOffset": 18, "endOffset": 62}, {"referenceID": 11, "context": "Therefore, we roll out with the dynamic oracle (Goldberg and Nivre, 2013).", "startOffset": 47, "endOffset": 73}, {"referenceID": 11, "context": "We compare with the recent transition-based parser with dynamic oracles (DYNA) (Goldberg and Nivre, 2013), and the Stanford neural network parser (SNN) (Chen and Manning, 2014).", "startOffset": 79, "endOffset": 105}, {"referenceID": 2, "context": "We compare with the recent transition-based parser with dynamic oracles (DYNA) (Goldberg and Nivre, 2013), and the Stanford neural network parser (SNN) (Chen and Manning, 2014).", "startOffset": 152, "endOffset": 176}, {"referenceID": 2, "context": "parameter values as suggested in (Chen and Manning, 2014), which are also the default settings of the software.", "startOffset": 33, "endOffset": 57}, {"referenceID": 9, "context": "This is an improved SGD-style update rule using an adaptive metric (Duchi et al., 2011; McMahan and Streeter, 2010), importance invariant updates (Karampatziakis and Langford, 2011), and normalized updates (Ross et al.", "startOffset": 67, "endOffset": 115}, {"referenceID": 22, "context": "This is an improved SGD-style update rule using an adaptive metric (Duchi et al., 2011; McMahan and Streeter, 2010), importance invariant updates (Karampatziakis and Langford, 2011), and normalized updates (Ross et al.", "startOffset": 67, "endOffset": 115}, {"referenceID": 16, "context": ", 2011; McMahan and Streeter, 2010), importance invariant updates (Karampatziakis and Langford, 2011), and normalized updates (Ross et al.", "startOffset": 66, "endOffset": 101}, {"referenceID": 28, "context": ", 2011; McMahan and Streeter, 2010), importance invariant updates (Karampatziakis and Langford, 2011), and normalized updates (Ross et al., 2013).", "startOffset": 126, "endOffset": 145}, {"referenceID": 11, "context": "Goldberg and Nivre (2013) first proposed dynamic oracles under the framework of imitation learning.", "startOffset": 0, "endOffset": 26}, {"referenceID": 8, "context": "There are a number of works that use the L2S approach to solve various other structured prediction problems, for example, sequence labeling (Doppa et al., 2014), coreference resolution (Ma et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 14, "context": ", 204), graph-based dependency parsing (He et al., 2013).", "startOffset": 39, "endOffset": 56}, {"referenceID": 20, "context": ", (McCallum et al., 2009; Gordon et al., 2014)), however, instead of relying on a new programming language, ours is implemented in C++ and Python, thus is easily accessible.", "startOffset": 2, "endOffset": 46}, {"referenceID": 13, "context": ", (McCallum et al., 2009; Gordon et al., 2014)), however, instead of relying on a new programming language, ours is implemented in C++ and Python, thus is easily accessible.", "startOffset": 2, "endOffset": 46}], "year": 2017, "abstractText": "We create a transition-based dependency parser using a general purpose learning to search system. The result is a fast and accurate parser for many languages. Compared to other transition-based dependency parsing approaches, our parser provides similar statistical and computational performance with best-known approaches while avoiding various downsides including randomization, extra feature requirements, and custom learning algorithms. We show that it is possible to implement a dependency parser with an open-source learning to search library in about 300 lines of C++ code, while existing systems often requires several thousands of lines.", "creator": "LaTeX with hyperref package"}}}