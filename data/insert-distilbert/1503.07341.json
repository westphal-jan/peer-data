{"id": "1503.07341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2015", "title": "An Experiment on Using Bayesian Networks for Process Mining", "abstract": "process capture mining is a technique that performs an automatic analysis simulations of business processes from a log of events with the promise principle of understanding how processes are executed in an organisation.", "histories": [["v1", "Wed, 25 Mar 2015 11:34:31 GMT  (1772kb,D)", "http://arxiv.org/abs/1503.07341v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["catarina moreira"], "accepted": false, "id": "1503.07341"}, "pdf": {"name": "1503.07341.pdf", "metadata": {"source": "CRF", "title": "An Experiment on Using Bayesian Networks for Process Mining", "authors": ["Catarina Moreira"], "emails": [], "sections": [{"heading": null, "text": "Process mining is a technique that performs an automatic analysis of business processes from a\nlog of events with the promise of understanding how processes are executed in an organisation.\nSeveral models have been proposed to address this problem, however, here we propose a different approach to deal with uncertainty. By uncertainty, we mean estimating the probability of some sequence of tasks occurring in a business process, given that only a subset of tasks may be observable.\nIn this sense, this work proposes a new approach to perform process mining using Bayesian Networks. These structures can take into account the probability of a task being present or absent in the business process. Moreover, Bayesian Networks are able to automatically learn these probabilities through mechanisms such as the maximum likelihood estimate and EM clustering.\nExperiments made over a Loan Application Case study suggest that Bayesian Networks are adequate structures for process mining and enable a deep analysis of the business process model that can be used to answer queries about that process."}, {"heading": "1 Introduction", "text": "Process mining is a technique that enables the automatic analysis of business processes based on event logs. Instead of designing a workflow, process mining consists in gathering the information of the tasks that take place during the workflow process and storing that data in structured formats called the event logs (van der 2011). While gathering this information, it is assumed that (1) each event refers to a task in the business process, (2) each event is associated to an instance of the workflow and (3) since the events are stored by their execution time, it is assumed that they are sorted (van der Aalst et al. 2004).\nDuring the last decade, process mining has been growing a lot of attention in the scientific community due to its promise to provide techniques for process discovery that will lead to an increase of productivity and to the reduction of costs (van der Aalst & de Medeiros 2005).\nar X\niv :1\n50 3.\n07 34\n1v 1\n[ cs\n.A I]\n2 5\nM ar\n2 01\n5\nProcess modelling can be seen as the techniques to graphically represent a business process. This graphical representation describes dependencies between activities that need to be executed together in order to fulfil a business target (Weske 2012).\nSince in process mining the order of the events is taken into consideration, there are already many models that can be directly applied to represent the workflow. Some of those models include Markov Chains (Ferreira et al. 2007, Rebuge & Ferreira 2012), Petri Nets (van der Aalst 1998), Neural Networks (Cook & Wolf 1998) and BPMN (van der 2011). However, Markov Chains and Petri Nets are the models that are most used in the literature of process mining (Tiwari et al. 2008).\nIn this work, it is proposed an alternative representation of business process by using Bayesian Networks. A Bayesian Networks can be defined as an acyclic directed graph in which each node represents a random variable and each edge represents a direct influence from the source node to the target node (conditional dependencies) (Spirtes et al. 2001). They differ from Markov Chains, because of their cycle-free and directed structure. Moreover, Bayesian Networks have the advantage of dealing with uncertainty differently from Markov Chains. In the latter, business processes are modelled as a chain of events that are observed to occur. Under a Bayesian Network perspective, this does not apply: each task can either be present or absent in the business process. Therefore, it is possible to perform special analysis that will enable the computation of the probability of some task of the business process occurring, given that we do not know which tasks have already been performed (Pearl 2009).\nWith this research work, we argue that the capabilities of Bayesian Networks provide a promising technique to model business processes, to perform analysis regarding risk management, cost reduction, finding irrelevant / repetitive tasks, etc.\nThe outline of this work is as follows. Section 2 presents a brief summary of Markov Chains. Section 3 makes an introduction to Bayesian Networks. It shows how to compute probabilistic inferences and presents some learning techniques that are used to automatically learn conditional probabilities in Bayesian Networks. Section 4 presents how Bayesian Networks can be applied in the realm of process mining. This section demonstrates how one can define the structure of a Bayesian Network and how one can perform automatic learning. Section 5 presents a case study in which we apply the proposed network. Finally, Section 6 summarises the current work, presents the main conclusions achieved and some directions for future work."}, {"heading": "2 Markov Chains", "text": "A Markov Chain is defined by a state space Val(X) and a model that defines, for every state x \u2208Val(X) a next-state distribution over Val(X). More precisely, the transition model \u03c4 specifies for each pair of states x,x\u2032 the probability \u03c4(x\u2192 x\u2032) of going from state x to x\u2032 (Koller & Friedman 2009).\nIn Markov Chains, the transition probability matrix must be stochastic, that is, each row of the matrix\nmust sum to one. Matrix 1 represents the transition matrix of the Markov Chain in Figure 1.\nPtransition =  0.9 0.075 0.025 0.15 0.8 0.05\n0.25 0.25 0.5\n (1)\nSuppose that one is in state B at time n. In order to compute the evolution of the system for n+ 1, one just needs to perform a matrix multiplication between the current state and the transition probability matrix. The current state B will be encoded as vector [0 1 0].\n[ 0 1 0 ] 0.9 0.075 0.025 0.15 0.8 0.05\n0.25 0.25 0.5\n= [ 0.15 0.8 0.05 ] (2)\nThe calculations in formula 2 show that the probability from transiting from state B\u2192 A is 0.15. The probability of transiting from B\u2192 B is 0.8. And the probability of transiting from state B\u2192C is 0.05.\nMoreover, if one wishes to compute the probability of the sequence A\u2192 B\u2192 B\u2192 C, one would\nneed to perform the following calculations:\nPr(A\u2192 B\u2192 B\u2192C) = Pr(A\u2192 B)Pr(B\u2192 B)Pr(B\u2192C) = 0.075\u00d70.8\u00d70.05 = 0.003 (3)"}, {"heading": "3 Bayesian Networks", "text": "Bayesian Networks are directed acyclic graphs in which each node represents a different random variable from a specific domain and each edge represents a direct influence from the source node to the target node (Pearl 1997). The graph represents independence relationships between variables and each node is associated with a conditional probability table (CPT) which specifies a distribution over the values of a node given each possible joint assignment of values of its parents. The full joint distribution of a Bayesian Network, where X is the list of variables, is given by (Russell & Norvig 2009):\nPrc(X1, . . . ,Xn) = n\n\u220f i=1 Pr(Xi|Parents(Xi)) (4)\nThe formula for computing classical exact inferences on Bayesian Networks is based on the full joint distribution (Equation 4). Let e be the list of observed variables and let Y be the remaining unobserved variables in the network. For some query X , the inference is given by:\nPrc(X |e) = \u03b1Prc(X ,e) = \u03b1 [ \u2211 y\u2208Y Prc(X ,e,y) ] (5)\nWhere \u03b1 = 1\n\u2211x\u2208X Prc(X = x,e)\nThe summation is over all possible y, i.e., all possible combinations of values of the unobserved variables y. The \u03b1 parameter, corresponds to the normalisation factor for the distribution Pr(X |e) (Russell & Norvig 2009)."}, {"heading": "3.1 Example of Application", "text": "Consider the Bayesian Network in Figure 2. Suppose that we want to determine the probability of raining given that we know that the grass is wet. In order to perform such inference on a Bayesian\nNetwork, one can use Equation 5 in the following way:\nPr( R = T |W = T ) = \u03b1 Pr( R = T )\u00d7\u2211 s\u2208S Pr( S = s | R = T )\u00d7Pr(W = T | S = s, R = T ) (6)\nPr( R = T |W = T ) = \u03b1 0.2\u00d7 [Pr(S = T |R = T )Pr(W = T |S = T,R = T )+\n+Pr(S = F |R = T )Pr(W = T |S = F,R = T )] (7)\nPr( R = T |W = T ) = \u03b1 0.2\u00d7 [0.01\u00d70.99+0.99\u00d70.8] = \u03b1 0.1604 = 0.3577 (8)\nGiven that Bayesian Networks are based on the Na\u0131\u0308ve Bayes rule, one needs to normalize the final probabilities by a factor \u03b1 . This normalisation factor \u03b1 corresponds to:\n\u03b1 = 1\nPr(R = T |W = T )+Pr(R = F |W = T ) (9)\nSo, in order to compute \u03b1 , one also needs to compute the probability of not raining given that the grass is wet, Pr( R = F |W = T ):\nPr( R = F |W = T ) = \u03b1 Pr( R = F )\u00d7\u2211 s\u2208S Pr( S = s | R = F )\u00d7Pr(W = T | S = s, R = F ) (10)\nPr( R = F |W = T ) = \u03b1 0.8\u00d7 [Pr(S = T |R = F)Pr(W = T |S = T,R = F)+\n+Pr(S = F |R = F)Pr(W = T |S = F, R = F )] (11)\nPr( R = F |W = T ) = \u03b1 0.8\u00d7 [0.4\u00d70.9+0.6\u00d70] = \u03b1 0.288 (12)\nGoing back to the normalisation factor in Equation 9, one can substitute Pr(R = T |W = T ) by the result in Equation 8 and Pr(R = F |W = T ) by the results in Equation 12.\n\u03b1 = 1\n0.1604+0.288 = 1 0.4484\n(13)\nNow that we have computed the normalisation factor, the final probabilities are:\nPr( R = T |W = T ) = \u03b1 0.1604 = 0.3577 (14)\nPr( R = F |W = T ) = \u03b1 0.288 = 0.6423 (15)"}, {"heading": "3.2 Learning in Bayesian Networks", "text": "There are two main approaches to build a Bayesian Network. One is to construct the network by hand and to use the knowledge of an expert to estimate the conditional probability tables. The second is to use statistical models to automatically learn these probabilities (Koller & Friedman 2009).\nEstimating the conditional probabilities by hand with the knowledge of an expert is problematic for several reasons. In some situations, the network is so big that it is almost impossible for the expert to make a reliable assignment of the probabilities to the random variables. Moreover, in many situations, the distribution of the data varies according to its application and through time. This makes it impossible for an expert to reliably estimate the probabilities associated to the random variables of the Bayesian Network.\nStatistical models, on the other hand, offer a mechanism to automatically learn a model that repre-\nsents the probability distribution of some population.\nAccording to the situation that one is modelling, one can have a fully observed dataset or have an incomplete dataset (or partially observed). For the scope of this work, we will only address the problem of learning in Bayesian Networks with a fully observed dataset and a known graphical structure. The data are considered fully observed if on each of the training instances there is a full instantiation to all the random variables of our sample space (Murphy 2012)."}, {"heading": "3.2.1 Maximum Likelihood Estimation in Bayesian Networks", "text": "The maximum likelihood estimation is a statistical method that assumes that data follows a Gaussian probability distribution. The mean and the variance of the probability distribution can be estimated by only knowing a partial sample of the dataset (Bishop 2007).\nSuppose that we have a Bayesian Network just like specified in Figure 3. This network is parameterized by a parameter vector \u03b8 which specifies the parameters for the conditional probability distribution of the network.\nThe training instances regarding Figure 3 consist in a tuple of the form \u3008x [m] ,y [m]\u3009, where x is an instance of the random variable X , y is an instance of the random variable Y and m is the mth training example from the training dataset D of size M.\nThe likelihood function is given by:\nL(\u03b8 : D) = M\n\u220f m=1 Pr(x [m] ,y [m] : \u03b8) (16)\nSince in a Bayesian Network we can specify a full joint probability distribution Pr(x [m] ,y [m] : \u03b8)\nby the chain rule, then, Equation 16 becomes:\nL(\u03b8 : D) = \u220f m Pr(x [m] : \u03b8X)Pr(y [m] |x [m] : \u03b8Y |X) (17)\nL(\u03b8 : D) = (\n\u220f m\nPr(x [m] : \u03b8X) )(\n\u220f m\nPr(y [m] |x [m] : \u03b8Y |X) )\n(18)\nEquation 18 shows that the likelihood function can be decomposed into two separate terms. If we had N random variables, then Equation 18 would also have N terms. Each of these terms is called a local likelihood function and can estimate how well a variable can predict its parents.\nMoreover, one can expand the second term of Equation 18 for each instance of x in the following\nway:\n\u220f m Pr(y [m] |x [m] : \u03b8Y |X) = \u220f m:x[m]=xtrue Pr(y [m] |x [m] : \u03b8Y |xtrue) \u220f m:x[m]=x f alse Pr(y [m] |x [m] : \u03b8Y |x f alse) (19)\nGoing back to the simple Bayesian Network in Figure 3, if we analyse the first term of Equation19, we can see that it refers to the number of instances of the training data in which x = true. This gives us two sets: x = true,y = true and x = true,y = f alse. Equation 20 discriminates these instances.\n\u220f m:x[m]=xtrue Pr(y [m] |x [m] : \u03b8Y |xtrue) = \u03b8y=true|x=true.\u03b8y= f alse|x=true (20)\nThen, Equation 20 becomes:\n\u03b8y= f alse|x=true = count(\u3008x = true,y = f alse\u3009)\ncount(\u3008x = true,y = f alse\u3009)+ count(\u3008x = true,y = true\u3009) (21)\n\u03b8y= f alse|x=true = count(\u3008x = true,y = f alse\u3009)\ncount(\u3008x = true\u3009) (22)\nFrom Equation 22, we can see that the maximum likelihood estimate for a Bayesian Network with a known structure and fully observed data consists in simply counting how many times each of the possible assignments of X and Y appear in the training data. In order to obtain a probability value, we normalize this score by counting the total number of instances that class X appears."}, {"heading": "3.3 SamIam", "text": "SamIam - Sensitivity, Analysis, Modeling, Inference and More - is a tool that enables the graphical modeling of Bayesian Networks. It was developed by the Automated Reasoning Group form the University of California1.\nSamIam is composed of a graphical interface and a reasoning engine. The graphical interface provides an easy way to model Bayesian Networks by specifying the random variables as nodes, causal connections as edges and the respective conditional probability tables. The reasoning engine, on the other hand, can perform classical inferences over the plotted Bayesian Network, make parameter estimations by learning mechanisms, sensitivity analysis, etc. For the scope of this work, only the classical\n1http://reasoning.cs.ucla.edu/samiam/\ninference and the learning mechanisms will be necessary.\nExamples of SamIam\u2019s graphical interface are given by Figures 4 to 9.\nFigure 4: SamIam representation of the Bayesian Network of Figure 2.\nFigure 5: Example of SamIam\u2019s inference engine: Pr(R|W = T ), Pr(S|W = T ).\nIn Figure 4, it is presented the Bayesian Network from Figure 2 under the SamIam graphical interface. The marginal probabilities for each node are automatically computed as soon as the user builds the Bayesian Network. Figure 4 shows that: Pr(R = T ) = 0.2, Pr(R = F) = 0.8, Pr(S = T ) = 0.3220, Pr(S = F) = 0.6780, Pr(W = T ) = 0.4484, Pr(W = F) = 0.5516.\nFigure 5 represents a graphical representation of the inference that was manually computed in Equations 8 and 12. The red markers represent variables which are observed. That is, variables, which have occurred. They can be seen as the conditions of probabilities. For instance, in the manually computed probability in Equation 6, the observed variable was the condition W = T , that is, we are asking the probability of Raining given that it was observed that the grass was wet.\nFor large Bayesian Networks, the inference process becomes very heavy and hard to be computed manually. Therefore, SamIam provides an easy interface that automatically performs such heavy operations.\nIn process mining, event logs are usually associated with a large amount of tasks, which can be\nmapped into nodes of a Bayesian Network. Consequently, for the scope of this work, we chose the capabilities of SamIam to automatically compute inferences related to the probability of certain sequences of tasks occurring. This mechanism will be more detailed in Section 4.2 of this work."}, {"heading": "4 Bayesian Networks for Process mining", "text": "Probabilistic graphical models, such as Bayesian Networks, are usually used for probabilistic inferences, that is, asking queries to the model and receiving answers in the form of probability values.\nUnder the realm of process mining, Bayesian Networks can represent activities as nodes (i.e. random variables) and the edges between activities can be seen as transitions between these tasks. From this structure, it is possible to automatically learn the conditional probability tables from a complete log of events using the Maximum Likelihood Estimations (Section 3.2.1). If the log is incomplete, then a Bayesian Network can also automatically learn and estimate the probability tables through the usage of EM Clustering, just like used in the work of Bobek et al. (2013), who developed a Bayesian Network to recommend business processes.\nIn the literature, business processes that are learnt from event logs are usually represented by either Markov Chains or Petri Nets (Weske 2012). In this work, however, we propose another approach to model business processes using Bayesian Networks. The reason why we do this is concerned with the fact that Bayesian Networks can deal with uncertainty more easily.\nBayesian Networks provide advantages in situations where we do not know if some task has occurred and we need to determine the probability of the process terminating or the probability of the process reaching some other task. Therefore, these structures provide more insights when there are high levels of uncertainty when compared to Markov Chains."}, {"heading": "4.1 Defining the Strucuture", "text": "Another advantage of Bayesian Networks is that they allow the direct representation of business process diagrams by capturing the direct dependencies between tasks. However, they do not allow an explicit representation of cycles, because Bayesian Networks are directed acyclic graphs. To represent a cycle, in a Bayesian Network, one would need to create many instances of the same node, which is intractable to perform inferences, since the inference problem is NP-Complete (Figures 10 and 11).\nIn this work, in order to eliminate cycles from the log of events, we used an heuristic that would\nchoose the most probable transitions between nodes. For instance, suppose that there is a transition from nodes A\u2192 B that occurred 900 times. Suppose also that there is a transition from nodes B\u2192 A that occurred 100 times. Following the proposed heuristic, we would only represent the Bayesian Network with the transition A\u2192 B. Figures 12 and 13 illustrates this example.\nAnother structure that Bayesian Networks cannot represent directly is concerned with mutual exclusion. Two events are mutually exclusive if they cannot occur at the same time. Bayesian Networks can capture mutually exclusive events through the notions of independence by manually adding new edges to the network. For instance, consider the business process represented by the Bayesian Network in Figure 14. Nodes B and C represent the end of the process, while node A represents a task that begins the process. In this situation, and following the semantics of the business process, it is required that nodes B and C become mutually exclusive. That is, the process flow can only end in one of these nodes and not on both of them at the same time.\nAs one can see in Figures 15 and 16, the Bayesian Network cannot represent this mutual exclusion. When computing Bayesian Inferences, all nodes depend on each other. Therefore, in order to semantically represent node B cannot occur at the same time as node C, one needs to add an extra edge between\nB\u2192 C. This additional edge will create a new dependency between these nodes. One can manually configure the conditional probability table of node C to represent this mutually exclusion: when node B is set to true, then the probability of occurring C is zero and vice-versa. The mutual exclusion of the Bayesian Network in Figure 14 is illustrated in Figures 17 to 19.\nNote that, in Figures 17 to 19, the probability of node C occurring when nothing is observed changed when compared to the Bayesian Network of Figure 14. This happened, because of the extra edge that was added in the later Bayesian Network, which ended up changing the configurations of the conditional probability tables and, consequently, final probability values."}, {"heading": "4.2 SamIam: Designing a Bayesian Network", "text": "SamIam provides an intuitive interface for constructing Bayesian Network. There are two modes in SamIam: the query mode (for learning and inferences) and the edit mode (for network structure and definition of conditional probabilities). When SamIam is started, the edit mode appears by default. Figure 20 describes the general edit mode interface.\nThe interface enables the creation/removal of nodes and the creation/removal of edges between nodes. For each node created, there will be a configuration window that can be accessed when the node is double-clicked. In this window, one must specify a unique identifier for the node and a name to be displayed in the SamIam interface. Additionally, one also needs to specify which states the node can have. For the scope of this work, we will only have binary random variables, so each node will have exactly two states: one representing the occurrence of the random variable and another representing its absence.\nThe conditional probability table can be accessed by clicking the tab Probailities. A windows,\nsimilar to the one presented in Figure 21, will appear.\nIn this window, a user can manually specify the conditional probabilities of the random variable. by default, Samiam fills these tables using a normal probability distribution, that is, each instance of each node has the same probability of occurring (Pr = 0.5 ).\nThe buttons Complement can be used to automatically assign the last probability value of the table. This takes into account the constraint that the probabilities of an event must sum to one. This way, the user can only manually specify n\u2212 1 entries of the table. SamIam computes the remaining probability by subtracting that value with 1: Pr(N = |n|) = 1\u2212\u2211|n|\u22121n=1 Pr(N = n).\nThe button Normalize normalizes all the entries of the conditional probability table."}, {"heading": "4.3 Learning", "text": "Given a log of events and a graphical structure, SamIam is able to find a statistical model that can automatically estimate the conditional probability tables of the given Bayesian Network. This learning process can be computed using the Maximum Likelihood Estimation (Section 3.2.1) if the log of events is complete or using the EM Clustering algorithm if the log of events is incomplete (Bishop 2007).\nIn the scope of this work, since we were given a complete event log, the process of filling the conditional probability tables was given by the maximum likelihood estimation, that is, by counting the\nnumber of times each instance of the log of events was present and then by normalizing to obtain a probability value.\nSamIam can automatically do this in the query mode. In the main SamIam interface, one can select the query mode just like presented in Figure 22. To go into the learning menu, one needs to find the option EM Learning (Figure 23).\nUnder the EM Learning menu, the user is presented with a window that asks for a training file, a probability threshold, the maximum number of iterations that the algorithm should perform and if the learning algorithm should ignore entries that lead to divisions by zero. Figure 24 illustrates these options.\nIn Figure 24, the field Max iterations corresponds to the total number of iterations that the EM Clustering should perform in case the algorithm does not converge. For the scope of this work, this entry is irrelevant since we are dealing with fully observed log of events. Consequently, the EM Clustering\nwill collapse to the Maximum Likelihood Estimate.\nThe field Log-likelihood threshold is also used in the scope of the EM Clustering. This threshold specifies that the algorithm will converge when the change in the log-likelihood function falls bellow a certain threshold. It is a common practice in the literature to set this value to 0.05 (Bishop 2007, Koller & Friedman 2009).\nThe option Use bias to prevent divisions by zero should always be used, otherwise the Maximum Likelihood Estimate formula will try to perform a division by zero when it tries to compute the probability of an instance that does not exist in the training set.\nIn process mining, a training set consists in a portion of the log of events that is used to fit (train) a model for prediction of values. In the scope of this work, a training set will consist of 70% randomized entries of the log of events. The format of the training file contains the names of all random variables (nodes) in the first line. The remaining lines of the file correspond to the instances of the nodes that are specified in the log of events. In this work, we modeled binary random variables with the instances present to represent the occurrence of a task in the business process and absent to represent the nonexistence of the task. Figure 25 shows the log of events (left) and the conversion of one instance of the log of events into a training file with the SamIan format (right).\nAfter SamIam learns the conditional probability tables, it is necessary to correct some semantics of the network. More specific the inclusion of mutually exclusive relationships. For instance, Figure 26 presents a conditional probability table that was automatically learned by SamIam. As one can see, when the node A PARTLYSUBMITTED is absent, SamIam did not update the normal probability distribution, so the probabilities 0.5 remained in the conditional probability table. This means that there were no events in the log that did not have an instance of the A PARTLYSUBMITTED node. This happens, because in process mining, the activities that are performed are usually mutually exclusive, unless a special structure is used to say the contrary. In order to correct these probabilities, such that the mutual exclusion is captured, one just needs to fill the conditional probability table just like illustrated in Figure 27. When the preceding node is absent, then the posterior nodes should also become absent."}, {"heading": "5 Case Study: Loan Application", "text": "The event log that we use in this work is taken from a Dutch Financial Institute2. The event log represents a loan application belonging to a global financial organization, in which a customer requests a certain amount of money. The process is composed of three different sub processes. The first letter of each task corresponds to an identifier of the sub process it belongs to. The tasks that start with letter A correspond to states of the application. The tasks that start with letter O correspond to offers belonging to the application. And the tasks that start with letter W correspond to the work item belonging to the application.\nThe general scenario is as follows. There is a webpage that enables the submission of loan applications. A customer selects a certain amount of money and then submits his request. Then, the application performs some automatic tasks and checks if an application is eligible. If it is eligible, then the customer is sent an offer by mail. After this offer is received, it will be evaluated. In case of any missing information, the offer goes back to the client and is again evaluated until all the required information is gathered. A final evaluation is done to the application. Finally, the application is approved and activated.\nThe log contains 262200 events and 13087 cases. The statistics of the log of events is summarised\nin Table 1."}, {"heading": "5.1 Converting the Log of Events into a SamIam Bayesian Nework", "text": "In this work, a Java program was made that received as input the log of events in csv format and returned a Bayesian Network in a special file format that can be readable by the SamIam toolkit. The program parsed every line of the log of events and grouped all activities that were complete and that belonged to the same instance (had the same caseId). The program automatically created a graph in a matrix form representation and computed the frequency of the connections between nodes.\nGiven this matrix form graph representation, another Java program was made in order to convert this\n2http://www.win.tue.nl/bpi/2012/challenge\nmatrix into a network file recognized by SamIam. Figures 28 and 29 show an example of a network file readable by SamIam. This example shows a network of the following form: C\u2190 A\u2192 B.\nIn a first attempt, we mapped the entire log of events into a Bayesian Network. However, the full log contained many tasks (about 24 random variables) and turned the process too big and complex to analyse. Figure 30 shows the network directly extracted from the log of events. The cycles that are present in this network were already expected, since the log of events contain many events that require cycles. Later in this work, we will specify an heuristic to remove such cyclic structures and turn any network into an acyclic directed graph.\nSince the network in Figure 30 was too complex, we decided to choose only the nodes concerned with the A tasks of the log of events, just like it was done in the works of (Adriansyah & Buijs 2012, Bautista et al. 2012, Bose & van der Aalst 2012, Kang et al. 2012).\nThe resulting Bayesian Network was smaller, containing only 10 random variables. We then altered the Bayesian Network in order to add mutually exclusive relationships between the nodes A DECLINED and A CANCELLED and between the nodes A APPROVED, A DECLINED and A CANCELLED.\nThe mutually exclusive relation between the nodes A DECLINED and A CANCELLED is straightforward. A loan application cannot be both declined and cancelled. Additionally, if an application is known to be declined, then the probability of being cancelled will be zero and vice-versa. Figure 31 presents this network and Figures 32 to 34 illustrate the mutual exclusion between nodes.\nIn order to compare our model with other works in the literature, we also created a Markov Chain from the same log of events (Section 2). We then computed the probability of each sequence of the test set occurring in the Bayesian Network and in the Markov Chain and then compared the results. Section 5.3 presents the main outcomes of these experiments."}, {"heading": "5.2 Converting the Log of Events into a Markov Chain", "text": "As already mentioned, we also developed a Markov Chain by a script in Python with the same training set used to generate the Bayesian Network. The transition probabilities of the Markov Chain were computed by simply counting the number of occurrences of each sequence of events and then by normalizing to obtain a probability value. Figure 35 shows the computed Markov Network."}, {"heading": "5.3 Results", "text": "After defining the structure of the Bayesian Network for the loan application, we generated a training set in which we randomly selected 70% of cases in the event log as training set and then, we used the remaining 30% as a test set to validate our model.\nThe training set was given as input to SamIam in order to learn the conditional probability tables. Then, to test the application, a MatLab program was developed in order to perform probabilistic inferences. Basically, the MatLab program received as input the SamIam\u2019s network file and returned a Bayesian Network structure. From this program, we were able to compute full joint probability distributions and marginal probabilities. Another Java program received as input the test set and was able to validate the model. The validation was performed as follows: we computed the probability of some events occurring in the test set and then we compared this value with the probability given in the trained Bayesian Network. Tables 2 to 8 show the results obtained for different queries both in the test set and the training set.\nThe overall results show that the Bayesian Network learned from the log of events is a good approach for process mining, since the errors obtained were very low. The most significant errors come associated with the node A CANCELLED. For instance, in Table 5, the probability Pr( A CANCELLED = present | A PREACCEPT ) achieved an error of 17%. One possible explanation can be given by the mutual exclusivities that were given to this node. Since in Bayesian Networks, all nodes depend of each other,"}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": ""}, {"heading": "Probability Test Set Training Set ERROR %", "text": "then by adding new relationships to the nodes, we are introducing some non-trivial effects in the model.\nAnother experiment made was to compare the proposed Bayesian Network with a Markov Chain.\nWe trained a Markov Chain in the same way we did for the Bayesian Network.\nIn order to validate both approaches, we leveraged on the test set and computed the probability of each sequence occurring in a Bayesian Network and in a Markov Chain. In the end, those probabilities were weighted with the number of occurrences of each sequence in the test set. The results obtained are discriminated in Table 10.\nTable 10 shows that the probabilities computed in a Bayesian Network are almost identical to the ones computed by the Markov Chain. Individually, the probabilities of computing the sequences in the test set did not have an error percentage superior to 4.13%, which is statistically insignificant given the total amount of data tested. Moreover, the overall error percentage between the proposed Bayesian Network and the Markov Chain was around 1.2674%, which is also statistically insignificant. This"}, {"heading": "Processes Process Encoding Processes Process Encoding", "text": ""}, {"heading": "Chain Occ. Test Set BN MC ERROR %", "text": "means that the Bayesian Networks have a similar performance as a Markov Chain. Consequently, one can conclude that Bayesian Networks are also good approaches to model business processes, with the advantage of being able to represent uncertainty (computing probabilities of tasks that we do not know if occurred)."}, {"heading": "5.4 Queries", "text": "As already mentioned, one of the capabilities of Bayesian Networks for process mining is their ability to deal with uncertainty. They enable the analysis of tasks that are not known to occur. For instance, for the Loan Application Bayesian Network, one can be interested in analyzing the probability of the business process ending successfully by only knowing that a couple of tasks were observed to occur. Combining this ability with SamIam\u2019s graphical capabilities will enable a fast analysis of business processes as well as risk management.\nFigure 36 shows the probabilities of some nodes of the Loan Application Bayesian Network, when it is only known that the application was declined, that is, the node A DECLINED was observed to occur. From this analysis, one can conclude that the majority of the applications that are declined have a high probability of reaching the state A PREACCEPT. Moreover, if an application is declined, then the nodes A ACTIVATED and A CANCELLED are never reached.\nAnother example is given by Figure 37. When it is known that the application ended up in a cancelled state, then one can estimate with a 100% probability that the process reached the task A PREACCEPT and never reached the tasks A DECLINED and A ACTIVATED. Moreover, there is a high probability that the application was cancelled during the tasks A ACCEPTED and A FINALIZED.\nThe maximum uncertainty in the loan application business process is given when one only knows\nthat the process was started, which happens when the task A SUBMITTED is observed to occur. In this situation, the proposed Bayesian Network estimates that there is a high probability of the process going to the task A PREACCEPT or being declined (A DECLINED). If one chooses task A PREACCEPT, then from Figure 39 one can conclude that there is a high probability that the process will be either accepted or finalised."}, {"heading": "6 Conclusion and Future Work", "text": "In this work, we propose the usage of Bayesian Networks as a new approach to represent business processes automatically extracted from event logs.\nIn a first step, we extracted the relationships between nodes from the log of events and then used this\nlog to train and validate the proposed Bayesian Network.\nExperiments made over a Loan Application Case study suggest that Bayesian Networks have the same performance as Markov Chains, so they are good models to make accurate predictions about sequences of events in the scope of process mining.\nMoreover, by modelling a business process through Bayesian Networks, one is able to take advantage of the ability of these structures to deal with uncertainty. More specifically, Bayesian Networks enable the reconstruction of a flow by only taking into account partial observations in the business process.\nAs for future work, it would be interesting to extend the capabilities of Bayesian Networks to learn from incomplete logs of events. One could train such network using the EM Clustering in order to find an approximate probability distribution for the occurrence of the tasks. Moreover, together with SamIam, one could try to estimate the most probable sequences of business processes using the probabilities learned from the incomplete log."}], "references": [{"title": "Mining process performance from event logs: The bpi challenge 2012 case study, in \u2018Proceedings of the 8th International Workshop on Business Process Intelligence", "author": ["A. Adriansyah", "J. Buijs"], "venue": null, "citeRegEx": "Adriansyah and Buijs,? \\Q2012\\E", "shortCiteRegEx": "Adriansyah and Buijs", "year": 2012}, {"title": "Process mining-driven optimization of a consumer loan approvals process: The bpic 2012 challenge, in \u2018Proceedings of the 8th International Workshop on Business Process Intelligence", "author": ["A.D. Bautista", "L. Wangikar", "S.M.K. Akbar"], "venue": null, "citeRegEx": "Bautista et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bautista et al\\.", "year": 2012}, {"title": "Pattern Recognition and Machine Learning, Springer", "author": ["C. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2007\\E", "shortCiteRegEx": "Bishop", "year": 2007}, {"title": "Application of bayesian networks to recommendations in business process modeling, in \u2018Proceedings of the Central Europe Workshop", "author": ["S. Bobek", "M. Baran", "K. Kluza", "G. Nalepa"], "venue": null, "citeRegEx": "Bobek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bobek et al\\.", "year": 2013}, {"title": "Process mining applied to the bpi challenge 2012: Divide and conquer while discerning resources, in \u2018Proceedings of the 8th International Workshop on Business Process Intelligence", "author": ["J.C. Bose", "W. van der Aalst"], "venue": null, "citeRegEx": "Bose and Aalst,? \\Q2012\\E", "shortCiteRegEx": "Bose and Aalst", "year": 2012}, {"title": "Discovering models of software processes from event-based data", "author": ["J. Cook", "A. Wolf"], "venue": "Journal of ACM Transactions on Software Engineering and Methodology", "citeRegEx": "Cook and Wolf,? \\Q1998\\E", "shortCiteRegEx": "Cook and Wolf", "year": 1998}, {"title": "Approaching process mining with sequence clustering: Experiments and findings, in \u2018In", "author": ["D. Ferreira", "M. Zacarias", "M. Malheiros", "P. Ferreira"], "venue": "Proceedings of the 5th International Conference on Business Process Management\u2019", "citeRegEx": "Ferreira et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ferreira et al\\.", "year": 2007}, {"title": "Analyzing application process for a personal loan or overdraft of dutch financial institute with process mining techniques, in \u2018Proceedings of the 8th International Workshop on Business Process Intelligence", "author": ["C.J. Kang", "C.K. Shin", "E.S. Lee", "J.H. Kim", "M.A. An"], "venue": null, "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2012\\E", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1997\\E", "shortCiteRegEx": "Pearl", "year": 1997}, {"title": "Causality: Models, Reasoning and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2009\\E", "shortCiteRegEx": "Pearl", "year": 2009}, {"title": "Business process analysis in healthcare environments: A methodology based on process mining", "author": ["A. Rebuge", "D. Ferreira"], "venue": "Journal of Information Systems: Management and Engineering of Process-Aware Information Systems", "citeRegEx": "Rebuge and Ferreira,? \\Q2012\\E", "shortCiteRegEx": "Rebuge and Ferreira", "year": 2012}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2009\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2009}, {"title": "Causation, Prediction and Search", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "Spirtes et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 2001}, {"title": "A review of business process mining: state-of-the-art and future trends", "author": ["A. Tiwari", "C. Turner", "B. Majeed"], "venue": "Journal of Business Process Management", "citeRegEx": "Tiwari et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tiwari et al\\.", "year": 2008}, {"title": "The application of petri nets to workflow management", "author": ["W. van der Aalst"], "venue": "Journal of Circuit Systems and Computers", "citeRegEx": "Aalst,? \\Q1998\\E", "shortCiteRegEx": "Aalst", "year": 1998}, {"title": "Process mining and security: Detecting anomalous process executions and checking process conformance", "author": ["W. van der Aalst", "A.K. de Medeiros"], "venue": "Journal of Electronic Notes in Theoretical Computer Science", "citeRegEx": "Aalst and Medeiros,? \\Q2005\\E", "shortCiteRegEx": "Aalst and Medeiros", "year": 2005}, {"title": "Workflow mining: Discovering process models from event logs", "author": ["W. van der Aalst", "T. Weijters", "L. Maruster"], "venue": "Journal of IEEE Transactions on Knowledge and Data Engineering 16,", "citeRegEx": "Aalst et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Aalst et al\\.", "year": 2004}, {"title": "Process Mining: Discovery, Conformance and Enhancement of Business Processes, Springer", "author": ["W. van der"], "venue": null, "citeRegEx": "der,? \\Q2011\\E", "shortCiteRegEx": "der", "year": 2011}, {"title": "Business Process Management: Concepts, Languages, Architectures, Springer", "author": ["M. Weske"], "venue": null, "citeRegEx": "Weske,? \\Q2012\\E", "shortCiteRegEx": "Weske", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "This graphical representation describes dependencies between activities that need to be executed together in order to fulfil a business target (Weske 2012).", "startOffset": 143, "endOffset": 155}, {"referenceID": 15, "context": "However, Markov Chains and Petri Nets are the models that are most used in the literature of process mining (Tiwari et al. 2008).", "startOffset": 108, "endOffset": 128}, {"referenceID": 14, "context": "A Bayesian Networks can be defined as an acyclic directed graph in which each node represents a random variable and each edge represents a direct influence from the source node to the target node (conditional dependencies) (Spirtes et al. 2001).", "startOffset": 223, "endOffset": 244}, {"referenceID": 11, "context": "Therefore, it is possible to perform special analysis that will enable the computation of the probability of some task of the business process occurring, given that we do not know which tasks have already been performed (Pearl 2009).", "startOffset": 220, "endOffset": 232}, {"referenceID": 10, "context": "Bayesian Networks are directed acyclic graphs in which each node represents a different random variable from a specific domain and each edge represents a direct influence from the source node to the target node (Pearl 1997).", "startOffset": 211, "endOffset": 223}, {"referenceID": 9, "context": "The data are considered fully observed if on each of the training instances there is a full instantiation to all the random variables of our sample space (Murphy 2012).", "startOffset": 154, "endOffset": 167}, {"referenceID": 2, "context": "The mean and the variance of the probability distribution can be estimated by only knowing a partial sample of the dataset (Bishop 2007).", "startOffset": 123, "endOffset": 136}, {"referenceID": 20, "context": "In the literature, business processes that are learnt from event logs are usually represented by either Markov Chains or Petri Nets (Weske 2012).", "startOffset": 132, "endOffset": 144}, {"referenceID": 3, "context": "If the log is incomplete, then a Bayesian Network can also automatically learn and estimate the probability tables through the usage of EM Clustering, just like used in the work of Bobek et al. (2013), who developed a Bayesian Network to recommend business processes.", "startOffset": 181, "endOffset": 201}, {"referenceID": 2, "context": "1) if the log of events is complete or using the EM Clustering algorithm if the log of events is incomplete (Bishop 2007).", "startOffset": 108, "endOffset": 121}], "year": 2015, "abstractText": "Process mining is a technique that performs an automatic analysis of business processes from a log of events with the promise of understanding how processes are executed in an organisation. Several models have been proposed to address this problem, however, here we propose a different approach to deal with uncertainty. By uncertainty, we mean estimating the probability of some sequence of tasks occurring in a business process, given that only a subset of tasks may be observable. In this sense, this work proposes a new approach to perform process mining using Bayesian Networks. These structures can take into account the probability of a task being present or absent in the business process. Moreover, Bayesian Networks are able to automatically learn these probabilities through mechanisms such as the maximum likelihood estimate and EM clustering. Experiments made over a Loan Application Case study suggest that Bayesian Networks are adequate structures for process mining and enable a deep analysis of the business process model that can be used to answer queries about that process.", "creator": "LaTeX with hyperref package"}}}