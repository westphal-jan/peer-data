{"id": "1503.07884", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2015", "title": "Transductive Multi-class and Multi-label Zero-shot Learning", "abstract": "beyond recently, zero - shot learning ( zsl ) schemes has received increasing interest. the commonly key idea underpinning existing zsl approaches is to exploit knowledge transfer via programming an intermediate - level semantic representation program which is usually assumed to be shared between the isolated auxiliary and target datasets, and is used to bridge access between separately these domains for knowledge transfer. the semantic representation used in existing approaches varies from ordinary visual attributes to semantic semantic word character vectors and semantic relatedness. however, the overall pipeline is similar : a projection mapping low - level features to the semantic representation is learned from the particular auxiliary dataset by either classification or regression fuzzy models and applied directly to subsequently map each given instance into the same semantic representation space where a zero - shot classifier is used to recognise the unseen target class instances confused with a plausible single known'prototype'specification of each target class. in this paper we discuss two related lines of work improving the conventional approach : exploiting transductive learning zsl, and generalising zsl to the multi - label case.", "histories": [["v1", "Thu, 26 Mar 2015 20:07:37 GMT  (3272kb,D)", "http://arxiv.org/abs/1503.07884v1", "4 pages, 4 figures, ECCV 2014 Workshop on Parts and Attributes"]], "COMMENTS": "4 pages, 4 figures, ECCV 2014 Workshop on Parts and Attributes", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yanwei fu", "yongxin yang", "timothy m hospedales", "tao xiang", "shaogang gong"], "accepted": false, "id": "1503.07884"}, "pdf": {"name": "1503.07884.pdf", "metadata": {"source": "CRF", "title": "Transductive Multi-class and Multi-label Zero-shot Learning", "authors": ["Yanwei Fu", "Yongxin Yang", "Timothy M. Hospedales", "Tao Xiang", "Shaogang Gong", "Y. Fu", "Y. Yang", "T. Hospedales", "T. Xiang", "S. Gong"], "emails": ["s.gong}@qmul.ac.uk"], "sections": [{"heading": null, "text": "Transductive Multi-class and Multi-label Zero-shot Learning\nYanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Shaogang Gong\nSchool of EECS, Queen Mary University of London, UK Email:{y.fu,yongxin.yang, t.hospedales, t.xiang, s.gong}@qmul.ac.uk\nThe same \u2018hasTail\u2019 attribute different visual appearance\n(a) visual space (c) multi-view embedding space\nPig Prototype\nPig\n(b) attribute space\nPrototype\nPrototype\nZebra\n\ufffd100 \ufffd80 \ufffd60 \ufffd40 \ufffd20 0 20 40 60 80 100 \ufffd150\n\ufffd100\n\ufffd50\n0\n50\n100\n(c) Word vector view\n-150 -100 -50 0 50 100 150 -150\n-100\n-50\n0\n50\n100\n150\npersian cat hippopotamus leopard humpback whale seal chimpanzee rat giant panda pig raccoon\nOther Representations\n(a) Overfeat view\n\u221260 \u221240 \u221220 0 20 40 60 80 \u221240\n\u221230 \u221220\n\u221210 0\n10 20\n30 40\n\ufffd150 \ufffd100 \ufffd50 0 50 100 \ufffd150\n\ufffd100\n\ufffd50\n0\n50\n100\n(b) Attribute view\n(d) Multi-view embedding space\n(A) zero-shot prototypes: red stars; (B) (a) overfeat view (b) attribute view (c) word vector view\nimage feature projections: blue (d) multi-view embedding space\nFigure 1. t-SNE visualisation of AwA: (A) Projection domain shift problem; (B)The instance distance measured by TMV-HLP in our embedding space.\nRecently, zero-shot learning (ZSL) has received increasing interest. The key idea underpinning existing ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation which is assumed to be shared between the auxiliary and target datasets, and is used to bridge between these domains for knowledge transfer. The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17]. However, the overall pipeline is similar: a projection mapping low-level features to the semantic representation is learned from the auxiliary dataset by either classification or regression models and applied directly to map each instance into the same semantic representation space where a zero-shot classifier is used to recognise the unseen target class instances with a single known \u2018prototype\u2019 of each target class. In this paper we discuss two related lines of work improving the conventional approach: exploiting transductive learning ZSL, and generalising ZSL to the multi-label case."}, {"heading": "1 Transductive multi-class zero-shot learning", "text": "Two inherent problems exist in the conventional ZSL formulation. (1) projection domain shift problem: Since the two datasets have different and potentially unrelated classes, the underlying data distributions of the classes differ, so do the\nar X\niv :1\n50 3.\n07 88\n4v 1\n[ cs\n.L G\n] 2\n6 M\nar 2\n01 5\n\u2018ideal\u2019 projection functions between the low-level feature space and the semantic spaces. Therefore, using the projection functions learned from the auxiliary dataset without any adaptation to the target dataset causes an unknown shift/bias. This is illustrated in Fig. 1(A), where both Zebra (auxiliary) and Pig (target) classes in AwA dataset share the same \u2018hasTail\u2019 semantic attribute, yet with different visual appearance of their tails. Similarly, many other attributes of Pig are visually different from the corresponding attributes in the auxiliary classes. Figure 1(A-b) illustrates the projection domain shift problem by plotting an 85D attribute space representation of image feature projections and class prototypes: a large discrepancy exists between the Pig prototype and the projections of its class member instances, but not for Zebra. This discrepancy inherently degrades the effectiveness of ZSL for class Pig. This problem has neither been identified nor addressed in the zero-shot learning literature. (2) Prototype sparsity problem: for each target class, we only have a single prototype which is insufficient to fully describe the class distribution. As shown in Figs. 1(B-b) and (B-c), there often exist large intra-class variations and inter-class similarities. Consequently, even if the single prototype is centred among its class members in the semantic representation space, existing ZSL classifiers still struggle to assign the correct class labels to these highly overlapped data points \u2013 one prototype per class simply is not enough to model the intra-class variability. This problem has never been explicitly identified although a partial solution exists [16].\nIn addition to these inherent problems, conventional approaches to ZSL are also limited in exploiting multiple intermediate semantic spaces/views, each of which may contain complementary information \u2013 they are useful in distinguishing different classes in different ways. In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited. This is challenging because they are often of very different dimensions and types and each suffers from different domain shift effects discussed above. Moreover, the exploitation has to be transductive for zero-shot learning as only unlabelled data are available for the target classes.\nIn our work [8,6], we propose to solve the projection domain shift problem using a transductive multi-view embedding framework. Under our framework, each unlabelled instance from the target dataset is represented by multiple views: its low-level feature view and its (biased) projections in multiple semantic spaces (visual attribute space and word space in this work). We introduce a multi-view semantic space alignment process to correlate different semantic views and the low-level feature view by projecting them onto a latent embedding space learned using multi-view Canonical Correlation Analysis (CCA) [10]. Learning this new embedding space is to transductively (using the unlabelled target data) aligns the semantic views with each other, and with the low-level feature view, thus rectifying the projection domain shift problem. Even with the proposed transductive multi-view embedding framework, the prototype sparsity problem remains \u2013 instead of one prototype per class, a handful are now available, but they are still sparse. Our solution to this problem is to explore the manifold structure of the\ndata distributions of different views projected onto the same embedding space via label propagation on a graph. To this end, we introduce novel transductive multi-view Bayesian label propagation (TMV-BLP) algorithm for recognition in [6] which combines multiple graphs by Bayesian model averaging in the embedding space. In our journal version [8], we further introduce a novel transductive multi-view hypergraph label propagation (TMV-HLP) algorithm for recognition. The core of our TMV-HLP algorithm is a new distributed representation of graph structure termed heterogeneous hypergraph. Instead of constructing hypergraphs independently in different views (i.e. homogeneous hypergraphs), data points in different views are combined to compute multi-view heterogeneous hypergraphs. This allows us to exploit the complementarity of different semantic and low-level feature views, as well as the manifold structure of the target data to compensate for the impoverished supervision available in the form of the sparse prototypes. Zero-shot learning is then performed by semi-supervised label propagation from the prototypes to the target data points within and across the graphs. Some results are shown in Tab. 1 and Fig. 1(B)."}, {"heading": "2 Transductive multi-label zero-shot learning", "text": "Many real-world data are intrinsically multi-label. For example, an image on Flickr often contains multiple objects with cluttered background, thus requiring more than one label to describe its content. And different labels are often correlated (e.g. cows often appear on grass). In order to better predict these labels given an image, the label correlation must be modelled: for n labels, there are 2n possible multi-label combinations and to collect sufficient training samples for each combination to learn the correlations of labels is infeasible. More fundamentally, existing multi-class ZSL algorithms cannot model any such correlation as no labeled examples are available in this setting.\nWe propose a novel framework for multi-label zero-shot learning [9]. Given an auxiliary dataset containing labelled images, and a target dataset multilabelled with unseen classes (i.e. none of the labels appear in the training set), we aim to learn a zero-shot model that performs multi-label classification on the test set with unseen labels. Zero-shot transfer is achieved using an intermediate semantic representation in the form of the skip-gram word vectors [15]\nwhich allows vector-oriented reasoning. For example, V ec(\u2018Moscow\u2032) is closer to V ec(\u2018Russia\u2032) + V ec(\u2018capital\u2032) than V ec(\u2018Russia\u2032) or V ec(\u2018capital\u2032) only. This property will enable zero-shot multi-label prediction by enabling synthesis of multi-label prototypes in the semantic word space.\nOur framework has two main components: multi-output deep regression (MulDR) and zero-shot multi-label prediction (ZS-MLP). Mul-DR is a 9 layer neural network that exploits convolutional neural network (CNN) layers, and includes two multi-output regression layers as the final layers. It learns from auxiliary data the mapping from raw image pixels to a linguistic representation defined by the skip-gram language model [15]. With Mul-DR, each test image is now projected into the semantic word space where the unseen labels and their combinations can be represented as data points without the need to collect any visual data. ZS-MLP addresses the multi-label ZSL problem in this semantic word space by exploiting the property that label combinations can be synthesised. We exhaustively synthesise the power set of all possible prototypes (i.e., combinations of multi-labels) to be treated as if they were a set of labelled instances in the space. With this synthetic dataset, we are able to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP). However, Mul-DR is learned using the auxiliary classes/labels, so it may not generalise well to the unseen classes/labels (projection domain shift problem, as discussed in the previous section). To overcome this problem, we further exploit self-training to adapt Mul-DR to the test classes to improve its generalisation capability. The experimental results on Natural Scene and IAPRTC-12 in Fig 2 show the efficacy of our framework for multi-label ZSL over a variety of baselines. For more details, please read our paper [9]."}], "references": [{"title": "Label-embedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Devise: A deep visual-semantic embedding model andrea", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Multi-view video summarization", "author": ["Y. Fu", "Y. Guo", "Y. Zhu", "F. Liu", "C. Song", "Z.-H. Zhou"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Attribute learning for understanding unstructured social activity", "author": ["Y. Fu", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Transductive multi-view embedding for zero-shot recognition and annotation", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "Z. Fu", "S. Gong"], "venue": "In ECCV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning multi-modal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Transductive multi-view zero-shot recognition and annotation", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "Submitted to IEEE TPAMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Transductive multi-label zero-shot learning", "author": ["Y. Fu", "Y. Yang", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "In BMVC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A multi-view embedding space for modeling internet", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE TPAMI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Recognizing human actions by attributes", "author": ["J. Liu", "B. Kuipers", "S. Savarese"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Efficient estimation of word representation in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of Workshop at ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Transfer learning in a transductive setting", "author": ["M. Rohrbach", "S. Ebert", "B. Schiele"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Evaluating knowledge transfer and zeroshot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "What helps where\u2013and why semantic relatedness for knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "A unified probabilistic approach modeling relationships between attributes and objects", "author": ["X. Wang", "Q. Ji"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Designing categorylevel attributes for discriminative visual recognition", "author": ["F.X. Yu", "L. Cao", "R.S. Feris", "J.R. Smith", "S.-F. Chang"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 86, "endOffset": 97}, {"referenceID": 1, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 86, "endOffset": 97}, {"referenceID": 12, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 86, "endOffset": 97}, {"referenceID": 2, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 123, "endOffset": 129}, {"referenceID": 18, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 123, "endOffset": 129}, {"referenceID": 16, "context": "The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "This problem has never been explicitly identified although a partial solution exists [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 44, "endOffset": 55}, {"referenceID": 1, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 44, "endOffset": 55}, {"referenceID": 12, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 44, "endOffset": 55}, {"referenceID": 6, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 44, "endOffset": 55}, {"referenceID": 13, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 117, "endOffset": 126}, {"referenceID": 2, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 117, "endOffset": 126}, {"referenceID": 18, "context": "In particular, while both visual attributes [11,2,13,7] and linguistic semantic representations such as word vectors [14,3,19] have been independently exploited successfully, multiple semantic \u2018views\u2019 have not been exploited.", "startOffset": 117, "endOffset": 126}, {"referenceID": 7, "context": "In our work [8,6], we propose to solve the projection domain shift problem using a transductive multi-view embedding framework.", "startOffset": 12, "endOffset": 17}, {"referenceID": 5, "context": "In our work [8,6], we propose to solve the projection domain shift problem using a transductive multi-view embedding framework.", "startOffset": 12, "endOffset": 17}, {"referenceID": 9, "context": "We introduce a multi-view semantic space alignment process to correlate different semantic views and the low-level feature view by projecting them onto a latent embedding space learned using multi-view Canonical Correlation Analysis (CCA) [10].", "startOffset": 239, "endOffset": 243}, {"referenceID": 5, "context": "To this end, we introduce novel transductive multi-view Bayesian label propagation (TMV-BLP) algorithm for recognition in [6] which combines multiple graphs by Bayesian model averaging in the embedding space.", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "In our journal version [8], we further introduce a novel transductive multi-view hypergraph label propagation (TMV-HLP) algorithm for recognition.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "Approach AwA (H [11]) AwA (O) AwA (O,D) USAA CUB (O) CUB (F) DAP 40.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "5([11]) / 41.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "4([12]) / 38.", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "2([7,5]) / 35.", "startOffset": 2, "endOffset": 7}, {"referenceID": 4, "context": "2([7,5]) / 35.", "startOffset": 2, "endOffset": 7}, {"referenceID": 10, "context": "8([11]) / 42.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "2([12]) \u2013 \u2013 \u2013 \u2013 \u2013 M2LATM [7] 41.", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "2([12]) \u2013 \u2013 \u2013 \u2013 \u2013 M2LATM [7] 41.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "9 \u2013 \u2013 ALE/HLE/AHLE [1] 37.", "startOffset": 19, "endOffset": 22}, {"referenceID": 17, "context": "0 Mo/Ma/O/D [18] 27.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "7 \u2013 \u2013 \u2013 \u2013 \u2013 PST [16] 42.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "2* [20] 43.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "4 \u2013 \u2013 \u2013 \u2013 \u2013 [21] 48.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "3** \u2013 \u2013 \u2013 \u2013 \u2013 TMV-BLP[6] 47.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "8 \u2013 \u2013 TMV-HLP [8] 49.", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "Mo, Ma, O and D represent the highest results in the mined object class-attribute associations, mined attributes, objectness as attributes and direct similarity methods used in [18] respectively.", "startOffset": 177, "endOffset": 181}, {"referenceID": 8, "context": "We propose a novel framework for multi-label zero-shot learning [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "Zero-shot transfer is achieved using an intermediate semantic representation in the form of the skip-gram word vectors [15]", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "It learns from auxiliary data the mapping from raw image pixels to a linguistic representation defined by the skip-gram language model [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 8, "context": "For more details, please read our paper [9].", "startOffset": 40, "endOffset": 43}], "year": 2015, "abstractText": "Recently, zero-shot learning (ZSL) has received increasing interest. The key idea underpinning existing ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation which is assumed to be shared between the auxiliary and target datasets, and is used to bridge between these domains for knowledge transfer. The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17]. However, the overall pipeline is similar: a projection mapping low-level features to the semantic representation is learned from the auxiliary dataset by either classification or regression models and applied directly to map each instance into the same semantic representation space where a zero-shot classifier is used to recognise the unseen target class instances with a single known \u2018prototype\u2019 of each target class. In this paper we discuss two related lines of work improving the conventional approach: exploiting transductive learning ZSL, and generalising ZSL to the multi-label case.", "creator": "LaTeX with hyperref package"}}}