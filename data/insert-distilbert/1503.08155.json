{"id": "1503.08155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2015", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories", "abstract": "this paper considers the challenging problem of knowledge inference on large - scale imperfect repositories with incomplete coverage by means of embedding ambiguous entities and relations at the first attempt. we propose iike ( imperfect and incomplete knowledge embedding ), a probabilistic model which measures the probability of each flawed belief, i. e. $ \\ langle h, r, t \\ boo rangle $, in large - scale knowledge bases such as nell and freebase, and our original objective is to effectively learn a better low - dimensional vector representation for each entity ( $ h $ and $ t $ ) and relation ( $ d r $ ) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning ( nell ) or crowdsouring ( freebase ), so that we can use $ | | { \\ bf h } + { \\ bf r } - { \\ bf : t } | | $ to assess the plausibility of a belief when conducting inference. we use logical subsets of those inexact knowledge bases to train our model and test the performances of statistical link prediction and triplet classification on ground truth beliefs, respectively. the results of extensive experiments show that iike achieves significant improvement compared with the baseline and state - of - the - art approaches.", "histories": [["v1", "Fri, 27 Mar 2015 17:13:03 GMT  (65kb,D)", "http://arxiv.org/abs/1503.08155v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["miao fan", "qiang zhou", "thomas fang zheng"], "accepted": false, "id": "1503.08155"}, "pdf": {"name": "1503.08155.pdf", "metadata": {"source": "CRF", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories", "authors": ["Miao Fan", "Qiang Zhou", "Thomas Fang Zheng"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The explosive growth in the number of web pages has drawn much attention to the study of information extraction [Sarawagi, 2008] in recent decades. The aim of this is to distill unstructured online texts, so that we can store and exploit the distilled information as structured knowledge. Thanks to the long-term efforts made by experts, crowdsouring and even machine learning techniques, several web-scale knowledge repositories have been built, such as Wordnet1, Freebase2 and NELL3, and most of them contain tens of millions of extracted beliefs which are commonly represented by triplets, i.e. \u3008head entity, relation, tail entity\u3009.\nAlthough we have gathered colossal quantities of beliefs, state of the art work in the literature [West et al., 2014] reported that in this field, our knowledge bases\n1http://wordnet.princeton.edu/ 2https://www.freebase.com/ 3http://rtw.ml.cmu.edu/rtw/\nare far from complete. For instance, nearly 97% persons in Freebase have unknown parents. To populate incomplete knowledge repositories, a large proportion of researchers follow the classical approach by extracting knowledge from texts [Zhou et al., 2005; Bach and Badaskar, 2007; Mintz et al., 2009]. For example, they explore ideal approaches that can automatically generate a precise belief like \u3008Madrid, capital city of, Spain\u3009 from the sentence \u201cMadrid is the capital and largest city of Spain.\u201d4 on the web. However, even cutting-edge research [Fan et al., 2014] could not satisfy the demand of web-scale deployment, due to the diversification of natural language expression. Moreover, many implicit relations between two entities which are not recorded by web texts still need to be mined.\nTherefore, some recent studies focus on inferring undiscovered beliefs based on the knowledge base itself without using extra web texts. One representative idea is to consider the whole repository as a graph where entities are nodes and relations are edges. The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph. An alternative paradigm aims to perform open-relation inference via embedding all the elements, including entities and relations, into low-dimensional vector spaces. The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.\nThis paper thus contributes a probabilistic knowledge embedding model called IIKE5 to measure the probability of each triplet, i.e. \u3008h, r, t\u3009, and our objective is to learn a better low-dimensional vector representation for each entity (h and t) and relation (r) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase). To the best of our knowledge, IIKE is the first approach that attempts to learn global connectivity patterns for open-relation inference on imperfect and incomplete knowledge bases. In order to prove the effectiveness of the model, we conduct exper-\n4http://en.wikipedia.org/wiki/Madrid 5short for Imperfect and Incomplete Knowledge Embedding.\nar X\niv :1\n50 3.\n08 15\n5v 1\n[ cs\n.A I]\n2 7\nM ar\n2 01\n5\niments on two tasks involved in knowledge inference, link prediction and triplet classification, using the two repositories mentioned above. Inexact beliefs are used to train our model, and we test the performance on ground truth beliefs. Results show that IIKE outperforms the other cutting-edge approaches on both different types of knowledge bases."}, {"heading": "2 Related Work", "text": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,\n\u2022 Symbolic representation v.s. Distributed representation: Graph-based models regard the entities and relations as atomic elements, and represent them in a symbolic framework. In contrast, embedding-based models explore distributed representations via learning a lowdimensional continuous vector representation for each entity and relation.\n\u2022 Relation-specific v.s. Open-relation: Graph-based models aim to induce rules or paths for a specific relation first, and then infer corresponding new beliefs. On the other hand, embedding-based models encode all relations into the same embedding space and conduct inference without any restriction on some specific relation."}, {"heading": "2.1 Graph-based Inference", "text": "Graph-based inference models generally learn the representation for specific relations from the knowledge graph.\nN-FOIL [Quinlan and Cameron-Jones, 1993] learns first order Horn clause rules to infer new beliefs from the known ones. So far, it has helped to learn approximately 600 such rules. However, its ability to perform inference over largescale knowledge repositories is currently still very limited.\nPRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation."}, {"heading": "2.2 Embedding-based Inference", "text": "Embedding-based inference models usually design various scoring functions fr(h, t) to measure the plausibility of a triplet \u3008h, r, t\u3009. The lower the dissimilarity of the scoring function fr(h, t) is, the higher the compatibility of the triplet will be.\nUnstructured [Bordes et al., 2013] is a naive model which exploits the occurrence information of the head and the tail entities without considering the relation between them. It defines a scoring function ||h \u2212 t||, and this model obviously\ncan not discriminate a pair of entities involving different relations. Therefore, Unstructured is commonly regarded as the baseline approach.\nDistance Model (SE) [Bordes et al., 2011] uses a pair of matrices (Wrh,Wrt), to characterize a relation r. The dissimilarity of a triplet is calculated by ||Wrhh \u2212Wrtt||1. As pointed out by Socher et al. [2013], the separating matrices Wrh and Wrt weaken the capability of capturing correlations between entities and corresponding relations, even though the model takes the relations into consideration.\nSingle Layer Model, proposed by Socher et al. [2013] thus aims to alleviate the shortcomings of the Distance Model by means of the nonlinearity of a single layer neural network g(Wrhh+Wrtt+br), in which g = tanh. The linear output layer then gives the scoring function: uTr g(Wrhh + Wrtt + br).\nBilinear Model [Sutskever et al., 2009; Jenatton et al., 2012] is another model that tries to fix the issue of weak interaction between the head and tail entities caused by Distance Model with a relation-specific bilinear form: fr(h, t) = hTWrt.\nNeural Tensor Network (NTN) [Socher et al., 2013] designs a general scoring function: fr(h, t) = uTr g(h\nTWrt + Wrhh+Wrtt+br), which combines the Single Layer Model and the Bilinear Model. This model is more expressive as the second-order correlations are also considered into the nonlinear transformation function, but the computational complexity is rather high.\nTransE [Bordes et al., 2013] is a canonical model different from all the other prior arts, which embeds relations into the same vector space of entities by regarding the relation r as a translation from h to t, i.e. h + r = t. It works well on the beliefs with ONE-TO-ONE mapping property but performs badly on multi-mapping beliefs. Given a series of facts associated with a ONE-TO-MANY relation r, e.g. (h, r, t1), (h, r, t2), ..., (h, r, tm), TransE tends to represent the embeddings of entities on MANY-side extremely the same with each other and hardly to be discriminated.\nTransH [Wang et al., 2014] is the state of the art approach as far as we know. It improves TransE by modeling a relation as a hyperplane, which makes it more flexible with regard to modeling beliefs with multi-mapping properties.\nEven though the prior arts of knowledge embedding are promising when conducting open-relation inference on largescale bases, the stage they stand on is made of ground-truth beliefs. The model IIKE that we have proposed belongs to the embedding-based community, but firstly tackles the problem with knowledge inference based on imperfect and incomplete repositories. Nevertheless, we compare our approach with the methods mentioned above, and assess the performance with both the dataset and the metrics they have used as part of the extensive experiments."}, {"heading": "3 Model", "text": "The plausibility of a belief \u3008h, r, t\u3009 can be regarded as the joint probability of the head entity h, the relation r and the tail entity t, namely Pr(h, r, t). Similarly, Pr(h|r, t) stands for the conditional probability of predict-\ning h given r and t. We assume that Pr(h, r, t) is collaboratively influenced by Pr(h|r, t), Pr(r|h, t) and Pr(t|h, r), and more specifically it equals to the geometric mean of Pr(h|r, t)Pr(r|h, t)Pr(t|h, r), which is shown in the subsequent equation,\nPr(h, r, t) = 3 \u221a Pr(h|r, t)Pr(r|h, t)Pr(t|h, r). (1)\nGiven r and t, there are multiple choices of h\u2032 which may appear as the head entity. Therefore, if we use Eh to denote the set of all the possible head entities given r and t, Pr(h|r, t) can be defined as\nPr(h|r, t) = exp D(h,r,t)\u2211\nh\u2032\u2208Eh exp D(h\u2032,r,t)\n, (2)\nThe other factors, i.e. Pr(r|h, t) and Pr(r|h, t), are defined accordingly by slightly revising the normalization terms as shown in Equation (3) and (4), in which R and Et represents the set of relations and tail entities, respectively.\nPr(r|h, t) = exp D(h,r,t)\u2211\nr\u2032\u2208R exp D(h,r\u2032,t)\n. (3)\nPr(t|h, r) = exp D(h,r,t)\u2211\nt\u2032\u2208Et exp D(h,r,t\u2032)\n. (4)\nThe last function that we do not explain in Equation (2), (3) and (4) is D(h, r, t). Inspired by somewhat surprising patterns learnt from word embeddings [Mikolov et al., 2013b] illustrated by Figure 1, the result of word vector calculation, for instance vMadrid \u2212 vSpain + vFrance, is closer to vParis than to any other words [Mikolov et al., 2013a]. If we study the example mentioned above, the most possible reason vSpain \u2212 vMadrid \u2248 vFrance \u2212 vParis, is that capital city of is the relation between Madrid and Spain , and so is Paris and France. In other words, hMadrid + rcapital city of \u2248 tSpain, if the belief is plausible. Therefore, we define D(h, r, t) as follows to calculate the dissimilarity between h + r and t using L1 or L2 norm, and set b as the bias parameter.\nD(h, r, t) = \u2212||h + r\u2212 t||+ b. (5)\nSo far, we have already modeled the probability of a belief, i.e. Pr(h, r, t). On the other hand, some imperfect repositories, such as NELL, which is automatically built by machine learning techniques [Carlson et al., 2010], assign a confidence score ([0.5 \u2212 1.0]) to evaluate the plausibility of the corresponding belief. Therefore, we define the cost function L shown in Equation (6), and our objective is to learn a better low-dimensional vector representation for each entity and relation while continuously minimizing the total loss of fitting each belief \u3008h, r, t, c\u3009 in the training set \u2206 to the corresponding confidence c.\narg min h,r,t\nL = \u2211\n\u3008h,r,t,c\u3009\u2208\u2206\n1 2 (logPr(h, r, t)\u2212 log c)2\n= \u2211\n\u3008h,r,t,c\u3009\u2208\u2206\n1 2 {1 3 [logPr(h|r, t) + logPr(r|h, t)\n+ logPr(t|h, r)]\u2212 log c)}2.\n(6)"}, {"heading": "4 Algorithm", "text": "To search for the optimal solution of Equation (6), we use Stochastic Gradient Descent (SGD) to update the embeddings of entities and relations in iterative fashion. However, it is cost intensive to directly compute the normalization terms in Pr(h|r, t), Pr(r|h, t) and Pr(t|h, r). Enlightened by the work of Mikolov et al. [2013a], we have found an efficient approach that adopts negative sampling to transform the conditional probability functions, i.e. Equation (2), (3) and (4), to the binary classification problem, as shown in the subsequent equations,\nlogPr(h|r, t) \u2248 logPr(1|h, r, t)\n+ k\u2211 i=1 Eh\u2032i\u223cPr(h\u2032\u2208Eh) logPr(0|h \u2032 i, r, t),\n(7)\nlogPr(r|h, t) \u2248 logPr(1|h, r, t)\n+ k\u2211 i=1 Er\u2032i\u223cPr(r\u2032\u2208R) logPr(0|h, r \u2032 i, t),\n(8)\nlogPr(t|h, r) \u2248 logPr(1|h, r, t)\n+ k\u2211 i=1 Et\u2032i\u223cPr(t\u2032\u2208Et) logPr(0|h, r, t \u2032 i),\n(9)\nin (7), (8), and (9), we sample k negative beliefs and discriminate them from the positive case. For the simple binary classification problem mentioned above, we choose the logistic function with the offset shown in Equation (10) to estimate the probability that the given belief \u3008h, r, t\u3009 is correct:\nPr(1|h, r, t) = 1 1 + exp\u2212D(h,r,t) + . (10)\nWe also display the framework of the learning algorithm of IIKE in pseudocode as follows,\nAlgorithm 1 The Learning Algorithm of IIKE"}, {"heading": "Input:", "text": "Training set \u2206 = {(h, r, t, c)}, entity set E, relation set R; dimension of embeddings d, number of negative samples k, learning rate \u03b1, convergence threshold \u03b7, maximum epoches n.\n1: /*Initialization*/ 2: foreach r \u2208 R do 3: r := Uniform(\u22126\u221a\nd , 6\u221a d )\n4: r := r|r| 5: end foreach 6: foreach e \u2208 E do 7: e := Uniform(\u22126\u221a\nd , 6\u221a d )\n8: e := e|e| 9: end foreach\n10: /*Training*/"}, {"heading": "11: i := 0", "text": "12: while Rel.loss > \u03b7 and i < n do 13: foreach \u3008h, r, t\u3009 \u2208 \u2206 do 14: foreach j \u2208 range(k) do 15: Negative sampling: \u3008h\u2032j , r, t\u3009 \u2208 \u2206\u2032h 16: /*\u2206\u2032h is the set of k negative beliefs replacing h*/\n17: Negative sampling: \u3008h, r\u2032j , t\u3009 \u2208 \u2206\u2032r 18: /*\u2206\u2032r is the set of k negative beliefs replacing r*/\n19: Negative sampling: \u3008h, r, t\u2032j\u3009 \u2208 \u2206\u2032t 20: /*\u2206\u2032t is the set of k negative beliefs replacing t*/ 21: end foreach 22: \u2211 h,r,t,h\u2032,r\u2032,t\u2032 \u2207 1 2 (logPr(h, r, t)\u2212 log c) 2 23: /*Updating embeddings of \u3008h, r, t\u3009 \u2208 \u2206, \u3008h\u2032, r, t\u3009 \u2208 \u2206\u2032h, \u3008h, r\u2032, t\u3009 \u2208 \u2206\u2032r, \u3008h, r, t\u2032\u3009 \u2208 \u2206\u2032t with \u03b1 and the batch gradients derived from Equation (7), (8), (9) and (10).*/ 24: end foreach 25: i++ 26: end while Output:\nAll the embeddings of h, t and r, where h, t \u2208 E and r \u2208 R."}, {"heading": "5 Experiments", "text": "Embedding the entities and relations into low-dimensional vector spaces facilitates several classical knowledge inference tasks, such as link prediction and triplet classification. More specifically, link prediction performs inference via predicting a ranked list of missing entities or relations given the other two elements of a triplet. For example, it can predict a series of t given h and r, or a bunch of h given r and t. And triplet classification is to discriminate whether a triplet \u3008h, r, t\u3009 is correct or wrong.\nSeveral recent research works [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they used subsets of Freebase (FB) data to evaluate their models and showed the performance on the above two tasks, respectively. In order to conduct solid experiments, we compare our model (IIKE) with many related studies including the baseline and cuttingedge approaches mentioned in Section 2.2. Moreover, we use a larger imperfect and incomplete dataset (NELL) to perform comparisons involving the same tasks to show the superior inference capability of IIKE, and have released this dataset for others to use.\nWe are also glad to share all the datasets, the source codes and the learnt embeddings for entities and relations, which can be freely downloaded from http://pan.baidu. com/s/1mgxGbg8."}, {"heading": "5.1 Link prediction", "text": "One of the benefits of knowledge embedding is that we can apply simple vector calculations to many reasoning tasks, and link prediction is a valuable task that contributes to completing the knowledge graph. With the help of knowledge embeddings, if we would like to tell whether the entity h has the relation r with the entity t, we just need to calculate the distance between h + r and t. The closer they are, the more possibility the triplet \u3008h, r, t\u3009 exists."}, {"heading": "Datasets", "text": "Bordes et al. [Bordes et al., 2013] released a large dataset (FB15K)6, extracted from Freebase and constructed by crowdsourcing, in which each belief is a triplet without a confidence score. Therefore, we assign 1.0 to each training triplet by default. We have also identified a larger repository on the web named NELL7 which is automatically built by\n6Related studies on this dataset can be looked up from the website https://www.hds.utc.fr/everest/doku. php?id=en:transe\n7The whole dataset of NELL can be downloaded from http: //rtw.ml.cmu.edu/rtw/resources\nmachine learning techniques, and each triplet is labeled with a probability estimated by synthetic algorithms [Carlson et al., 2010]. We reserve the beliefs with probability ranging (0.5 - 1.0], use the ground-truth (1.0) beliefs as the validating and testing examples, and train the models with the remains.\nTable 1 shows the statistics of these two datasets. The scale of NELL dataset is larger than FB15K with many more entities but fewer relations, which may lead to the differences of tuning parameters8."}, {"heading": "Evaluation Protocol", "text": "For each testing triplet, all the other entities that appear in the training set take turns to replace the head entity. Then we get a bunch of candidate triplets associated with the testing triplet. The dissimilarity of each candidate triplet is firstly computed by various scoring functions, such as ||h + r\u2212 t||, and then sorted in ascending order. Finally, we locate the ground-truth triplet and record its rank. This whole procedure runs in the same way when replacing the tail entity, so that we can gain the mean results. We use two metrics, i.e. Mean Rank and Mean Hit@10 (the proportion of ground truth triplets that rank in Top 10), to measure the performance. However, the results measured by those metrics are relatively inaccurate, as the procedure above tends to generate false negative triplets. In other words, some of the candidate triplets rank rather higher than the ground truth triplet just because they also appear in the training set. We thus filter out those triplets to report more reasonable results.\n8It turns out that embedding models prefer a larger dimension of vector representations for the dataset with more entities, and L1 norm for fewer relations."}, {"heading": "Experimental Results", "text": "We compared IIKE with the state-of-the-art TransH, TransE and other models mentioned in Section 2.2 evaluated on FB15K and NELL . We tuned the parameters of each previous model9 based on the validation set, and select the combination of parameters which leads to the best performance. The results of prior arts on FB15K are the same as those reported by Wang et al. [2014]. For IIKE, we tried several combinations of parameters: d = {20, 50, 100}, \u03b1 = {0.1, 0.05, 0.01, 0.005, 0.002}, b = {7.0, 10.0, 15.0} and norm = {L1, L2}, and finally chose d = 50, \u03b1 = 0.002, b = 7.0, norm = L2 for the FB15K dataset, and d = 100, \u03b1 = 0.001, b = 7.0, norm = L1 for the NELL dataset. Moreover, to make responsible comparisons between IIKE and the state-of-the-art approaches, we requested the authors of TransH to re-run their system on the NELL dataset and reported the best results. Table 2 demonstrates that IIKE outperforms all the prior arts, including the baseline model Unstructured [Bordes et al., 2014], RESCAL [Nickel et al., 2011], SE [Bordes et al., 2011], SME (LINEAR) [Bordes et al., 2014], SME (BILINEAR) [Bordes et al., 2014], LFM [Jenatton et al., 2012] and TransE [Bordes et al., 2013], and achieves significant improvements on the FB15K dataset, compared with the state-of-the-art TransH [Wang et al., 2014]. For the NELL dataset, IIKE performs stably on the evaluation metrics compared with TransH and TransE, as Table 3 shows that it improves by 28.9% in terms of Raw Mean Rank, and achieves comparable performance of Filter Mean Rank compared with TransH.\n9All the codes for the related models can be downloaded from https://github.com/glorotxa/SME"}, {"heading": "5.2 Triplet classification", "text": "Triplet classification is another inference related task proposed by Socher et al. [2013] which focuses on searching a relation-specific threshold \u03c3r to identify whether a triplet \u3008h, r, t\u3009 is plausible."}, {"heading": "Datasets", "text": "Wang et al. [2014] constructed a standard dataset FB15K sampled from Freebase. Moreover, we build another imperfect and incomplete dataset, i.e. NELL, following the same principle that the head or the tail entity can be randomly replaced with another one to produce a negative triplet, but in order to build much tough validation and testing datasets, the principle emphasizes that the picked entity should once appear at the same position. For example, (Pablo Picaso, nationality, American) is a potential negative example rather than the obvious irrational (Pablo Picaso, nationality, Van Gogh), given a positive triplet (Pablo Picaso, nationality, Spanish), as American and Spanish are more common as the tails of nationality. And the beliefs in the training sets are the same as those used in triplet classification. Table 4 shows the statistics of the standard datasets that we used for evaluating models on the triplet classification task."}, {"heading": "Evaluation Protocol", "text": "The decision strategy for binary classification is simple: if the dissimilarity of a testing triplet (h, r, t) computed by fr(h, t) is below the relation-specific threshold \u03c3r, it is predicted as positive, otherwise negative. The relation-specific threshold \u03c3r can be searched via maximizing the classification accuracy on the validation triplets which belong to the relation r."}, {"heading": "Experimental Results", "text": "We use the best combination of parameter settings in the link prediction task: d = 50, \u03b1 = 0.002, b = 7.0, norm = L2 for the FB15K dataset, and d = 100, \u03b1 = 0.001, b = 7.0, norm = L1 for the NELL dataset, to generate the entity and relation embeddings, and learn the best classification threshold \u03c3r for each relation r. Compared with several of the latest\napproaches, i.e. TransH [Wang et al., 2014], TransE [Bordes et al., 2013] and Neural Tensor Network (NTN)10 [Socher et al., 2013], the proposed IIKE approach still outperforms them, as shown in Table 5. We also drew the precision-recall curves which indicate the capability of global discrimination by ranking the distance of all the testing triplets, and Figure 2 shows that the AUC (Areas Under the Curve) of IIKE is much bigger than the other approaches."}, {"heading": "6 Conclusion", "text": "We challenge the problem of knowledge inference on imperfect and incomplete repositories in this paper, and have produced an elegant probabilistic embedding model to tackle this issue at the first attempt by measuring the probability of a given belief \u3008h, r, t\u3009. To efficiently learn the embeddings for each entity and relation, we also adopt the negative sampling technique to transform the original model and display the algorithm based on SGD to search the optimal solution. Extensive experiments on knowledge inference including link prediction and triplet classification show that our approach achieves significant improvement on two large-scale knowledge bases, compared with state-of-the-art and baseline methods.\nWe are pleased to see further improvements of the proposed model, which leaves open promising directions for the future work, such as taking advantage of the knowledge embeddings to enhance the studies of text summarization and open-domain question answering."}, {"heading": "7 Acknowledgments", "text": "This work is supported by National Program on Key Basic Research Project (973 Program) under Grant 2013CB329304,\n10Socher et al. reported higher classification accuracy in [Socher et al., 2013] with word embeddings. In order to conduct a fair comparison, the accuracy of NTN reported in Table 5 is same with the EV (entity vectors) results in Figure 4 of [Socher et al., 2013].\nNational Science Foundation of China (NSFC) under Grant No.61373075."}], "references": [{"title": "A review of relation extraction", "author": ["Nguyen Bach", "Sameer Badaskar"], "venue": "Literature review for Language and Statistics II,", "citeRegEx": "Bach and Badaskar. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "Learning structured embeddings of knowledge bases. In AAAI,", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell"], "venue": "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010),", "citeRegEx": "Carlson et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Distant supervision for relation extraction with matrix completion", "author": ["Fan et al", "2014] Miao Fan", "Deli Zhao", "Qiang Zhou", "Zhiyuan Liu", "Thomas Fang Zheng", "Edward Y. Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "pages 833\u2013838", "author": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell. Improving learning", "inference in a large knowledge-base using latent syntactic cues. In EMNLP"], "venue": "ACL,", "citeRegEx": "Gardner et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Rodolphe Jenatton", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "A latent factor model for highly multi-relational data. In NIPS, pages 3176\u20133184,", "citeRegEx": "Jenatton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning", "author": ["Ni Lao", "William W Cohen. Relational retrieval using a combination of path-constrained random walks"], "venue": "81(1):53\u201367,", "citeRegEx": "Lao and Cohen. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "author": ["Ni Lao", "Tom Mitchell", "William W. Cohen. Random walk inference", "learning in a large scale knowledge base"], "venue": "pages 529\u2013539, Edinburgh, Scotland, UK., July", "citeRegEx": "Lao et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013a] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "pages 746\u2013751", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig. Linguistic regularities in continuous space word representations. In HLT-NAACL"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al", "2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "In Proceedings of the 28th international conference on machine learning (ICML-11)", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data"], "venue": "pages 809\u2013816,", "citeRegEx": "Nickel et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Foil: A midterm report", "author": ["J. Ross Quinlan", "R. Mike Cameron-Jones"], "venue": "Proceedings of the European Conference on Machine Learning, ECML \u201993, pages 3\u201320, London, UK, UK,", "citeRegEx": "Quinlan and Cameron.Jones. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Foundations and trends in databases", "author": ["Sunita Sarawagi. Information extraction"], "venue": "1(3):261\u2013377,", "citeRegEx": "Sarawagi. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS", "author": ["Ilya Sutskever", "Ruslan Salakhutdinov", "Joshua B Tenenbaum. Modelling relational data using bayesian clustered tensor factorization"], "venue": "pages 1821\u20131828,", "citeRegEx": "Sutskever et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al", "2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Knowledge base completion via search-based question answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "WWW,", "citeRegEx": "West et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905)", "author": ["GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang. Exploring various knowledge in relation extraction"], "venue": "pages 427\u2013434, Ann Arbor, Michigan, June", "citeRegEx": "Zhou et al.. 2005", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "The explosive growth in the number of web pages has drawn much attention to the study of information extraction [Sarawagi, 2008] in recent decades.", "startOffset": 112, "endOffset": 128}, {"referenceID": 19, "context": "Although we have gathered colossal quantities of beliefs, state of the art work in the literature [West et al., 2014] reported that in this field, our knowledge bases", "startOffset": 98, "endOffset": 117}, {"referenceID": 20, "context": "To populate incomplete knowledge repositories, a large proportion of researchers follow the classical approach by extracting knowledge from texts [Zhou et al., 2005; Bach and Badaskar, 2007; Mintz et al., 2009].", "startOffset": 146, "endOffset": 210}, {"referenceID": 0, "context": "To populate incomplete knowledge repositories, a large proportion of researchers follow the classical approach by extracting knowledge from texts [Zhou et al., 2005; Bach and Badaskar, 2007; Mintz et al., 2009].", "startOffset": 146, "endOffset": 210}, {"referenceID": 14, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 8, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 9, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 6, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 17, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 7, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 1, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 2, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 16, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 14, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 8, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 9, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 6, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 17, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 7, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 1, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 2, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 16, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 14, "context": "N-FOIL [Quinlan and Cameron-Jones, 1993] learns first order Horn clause rules to infer new beliefs from the known ones.", "startOffset": 7, "endOffset": 40}, {"referenceID": 8, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 9, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 6, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 2, "context": "Unstructured [Bordes et al., 2013] is a naive model which exploits the occurrence information of the head and the tail entities without considering the relation between them.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "Distance Model (SE) [Bordes et al., 2011] uses a pair of matrices (Wrh,Wrt), to characterize a relation r.", "startOffset": 20, "endOffset": 41}, {"referenceID": 17, "context": "Bilinear Model [Sutskever et al., 2009; Jenatton et al., 2012] is another model that tries to fix the issue of weak interaction between the head and tail entities caused by Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.", "startOffset": 15, "endOffset": 62}, {"referenceID": 7, "context": "Bilinear Model [Sutskever et al., 2009; Jenatton et al., 2012] is another model that tries to fix the issue of weak interaction between the head and tail entities caused by Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.", "startOffset": 15, "endOffset": 62}, {"referenceID": 16, "context": "Neural Tensor Network (NTN) [Socher et al., 2013] designs a general scoring function: fr(h, t) = ur g(h Wrt + Wrhh+Wrtt+br), which combines the Single Layer Model and the Bilinear Model.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] is a canonical model different from all the other prior arts, which embeds relations into the same vector space of entities by regarding the relation r as a translation from h to t, i.", "startOffset": 7, "endOffset": 28}, {"referenceID": 11, "context": "Inspired by somewhat surprising patterns learnt from word embeddings [Mikolov et al., 2013b] illustrated by Figure 1, the result of word vector calculation, for instance vMadrid \u2212 vSpain + vFrance, is closer to vParis than to any other words [Mikolov et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 4, "context": "On the other hand, some imperfect repositories, such as NELL, which is automatically built by machine learning techniques [Carlson et al., 2010], assign a confidence score ([0.", "startOffset": 122, "endOffset": 144}, {"referenceID": 2, "context": "Several recent research works [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they used subsets of Freebase (FB) data to evaluate their models and showed the performance on the above two tasks, respectively.", "startOffset": 30, "endOffset": 91}, {"referenceID": 16, "context": "Several recent research works [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they used subsets of Freebase (FB) data to evaluate their models and showed the performance on the above two tasks, respectively.", "startOffset": 30, "endOffset": 91}, {"referenceID": 2, "context": "[Bordes et al., 2013] released a large dataset (FB15K)6, extracted from Freebase and constructed by crowdsourcing, in which each belief is a triplet without a confidence score.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Unstructured [Bordes et al., 2014] 1,074 / 14,951 979 / 14,951 4.", "startOffset": 13, "endOffset": 34}, {"referenceID": 13, "context": "3% RESCAL [Nickel et al., 2011] 828 / 14,951 683 / 14,951 28.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "1% SE [Bordes et al., 2011] 273 / 14,951 162 / 14,951 28.", "startOffset": 6, "endOffset": 27}, {"referenceID": 3, "context": "8% SME (LINEAR) [Bordes et al., 2014] 274 / 14,951 154 / 14,951 30.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "8% SME (BILINEAR) [Bordes et al., 2014] 284 / 14,951 158 / 14,951 31.", "startOffset": 18, "endOffset": 39}, {"referenceID": 7, "context": "3% LFM [Jenatton et al., 2012] 283 / 14,951 164 / 14,951 26.", "startOffset": 7, "endOffset": 30}, {"referenceID": 2, "context": "1% TransE [Bordes et al., 2013] 243 / 14,951 125 / 14,951 34.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] 4,254 / 74,037 4,218 / 74,037 11.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "machine learning techniques, and each triplet is labeled with a probability estimated by synthetic algorithms [Carlson et al., 2010].", "startOffset": 110, "endOffset": 132}, {"referenceID": 3, "context": "Table 2 demonstrates that IIKE outperforms all the prior arts, including the baseline model Unstructured [Bordes et al., 2014], RESCAL [Nickel et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 13, "context": ", 2014], RESCAL [Nickel et al., 2011], SE [Bordes et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": ", 2011], SE [Bordes et al., 2011], SME (LINEAR) [Bordes et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": ", 2011], SME (LINEAR) [Bordes et al., 2014], SME (BILINEAR) [Bordes et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": ", 2014], SME (BILINEAR) [Bordes et al., 2014], LFM [Jenatton et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": ", 2014], LFM [Jenatton et al., 2012] and TransE [Bordes et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 2, "context": ", 2012] and TransE [Bordes et al., 2013], and achieves significant improvements on the FB15K dataset, compared with the state-of-the-art TransH [Wang et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 16, "context": "NTN [Socher et al., 2013] 66.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "7% TransE [Bordes et al., 2013] 79.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": ", 2014], TransE [Bordes et al., 2013] and Neural Tensor Network (NTN)10 [Socher et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": ", 2013] and Neural Tensor Network (NTN)10 [Socher et al., 2013], the proposed IIKE approach still outperforms them, as shown in Table 5.", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "reported higher classification accuracy in [Socher et al., 2013] with word embeddings.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": "parison, the accuracy of NTN reported in Table 5 is same with the EV (entity vectors) results in Figure 4 of [Socher et al., 2013].", "startOffset": 109, "endOffset": 130}], "year": 2015, "abstractText": "This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. \u3008h, r, t\u3009, in largescale knowledge bases such as NELL and Freebase, and our objective is to learn a better lowdimensional vector representation for each entity (h and t) and relation (r) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use ||h + r \u2212 t|| to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}