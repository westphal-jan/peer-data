{"id": "1503.08316", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2015", "title": "A Variance Reduced Stochastic Newton Method", "abstract": "we present a new method to reduce unfortunately the variance of stochastic versions of the bfgs optimization method, applied to the optimization of a class of smooth strongly occurring convex functions. although stochastic gradient descent ( sgd ) is a popular method to solve this kind of problem, its convergence rate is sublinear as it is in fact limited by the noisy exponential approximation of knowing the true gradient. in order to recover a high convergence rate, one has to pick an appropriate step - size or explicitly reduce the variance of the predictable approximate gradients. another limiting factor of sgd is that it ignores the curvature of the objective function that can help greatly speed up convergence. stochastic variants algorithm of bfgs that include curvature have uniformly shown good empirical performance but suffer from the same noise effects as sgd. we here propose a new algorithm v ite that uses an existing technique to reduce this variance while allowing a constant step - size to be used. we merely show that the expected objective value converges to the optimum at a geometric rate. we experimentally demonstrate improved optimization convergence confidence rate on diverse stochastic optimization problems.", "histories": [["v1", "Sat, 28 Mar 2015 15:51:48 GMT  (1275kb)", "https://arxiv.org/abs/1503.08316v1", null], ["v2", "Wed, 1 Apr 2015 06:57:26 GMT  (1531kb)", "http://arxiv.org/abs/1503.08316v2", null], ["v3", "Mon, 20 Apr 2015 11:34:58 GMT  (1286kb)", "http://arxiv.org/abs/1503.08316v3", null], ["v4", "Tue, 9 Jun 2015 19:24:03 GMT  (1259kb)", "http://arxiv.org/abs/1503.08316v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aurelien lucchi", "brian mcwilliams", "thomas hofmann"], "accepted": false, "id": "1503.08316"}, "pdf": {"name": "1503.08316.pdf", "metadata": {"source": "CRF", "title": "A Variance Reduced Stochastic Newton Method", "authors": ["Aurelien Lucchi", "Brian McWilliams", "Thomas Hofmann"], "emails": ["@inf.ethz.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n08 31\n6v 4\n[ cs\n.L G\n] 9\nJ un\n2 01"}, {"heading": "1 Introduction", "text": "We consider the problem of optimizing a function expressed as an expectation over a set of datadependent functions. Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18]. The simplicity of SGD is both its greatest strength and weakness. Due to the effects of evaluating noisy approximation of the true gradient, SGD achieves a convergence rate which is only sub-linear in the number of steps. In an effort to deal with this randomness, two primary directions of focus have been developed. The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14]. If a decaying step-size is chosen, the variance is forced to zero asymptotically guaranteeing convergence. However, small steps also slow down progress and limit the rate of convergence in practise. The stepsize must be chosen carefully, which can require extensive experimentation possibly negating the computational speedup of SGD. Another approach is to use an improved, lower-variance estimate of the gradient. If this estimator is chosen correctly \u2013 such that its variance goes to zero asymptotically \u2013 convergence can be guaranteed with a constant learning rate. This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage. A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.\nWith variance reduction, first-order methods can obtain a linear convergence rate. In contrast, second-order methods have been shown to obtain super-linear convergence. However, this requires the computation and inversion of the Hessian matrix which is impractical for large-scale datasets. Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11]. Quasi-Newton methods such as BFGS do not require computing the Hessian matrix but instead construct a quadratic model of the objective function by successive measurements of the gradient. This also yields super-linear convergence when the quadratic model is accurate. Stochastic variants of BFGS have been proposed (oBFGS [17]), for which stochastic gradients replace their deterministic counterparts. A regularized version known as RES [12] achieves a sublinear convergence rate with a decreasing step-size by\nenforcing a bound on the eigenvalues of the approximate Hessian matrix. SQN [3], another related method also requires a decreasing step size to achieve sub-linear convergence. Although stochastic second order methods have not be shown to achieve super-linear convergence, they empirically outperform SGD for problems with a large condition number [12].\nA clear drawback to stochastic second order methods is that similarly to their first-order counterparts, they suffer from high variance in the approximation of the gradient. Additionally, this problem can be exaggerated due to the estimate of the Hessian magnifying the effect of this noise. Overall, this can lead to such algorithms taking large steps in poor descent directions.\nIn this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients. We call this method Variance-reduced Stochastic Newton (VITE). Under standard conditions on J\u0302 , we show that that variance reduction on the gradient estimate alone is sufficient for fast convergence. For smooth and strongly convex functions, VITE reaches the optimum at a geometric rate with a constant step-size. To our knowledge VITE is the first stochastic Quasi-Newton method with these properties.\nIn the following section, we briefly review the BFGS algorithm and its stochastic variants. We then introduce the VITE algorithm and analyze its convergence properties. Finally, we present experimental results on real-world datasets demonstrating its superior performance over a range of competitors."}, {"heading": "2 Stochastic second order optimization", "text": ""}, {"heading": "2.1 Problem setting", "text": "Given a dataset D = {(x1, y1), . . . , (xn, yn)} consisting of feature vectors xi \u2208 Rd and targets yi \u2208 [0, C], we consider the problem of minimizing the expected loss f(w) = E[fi(w)]. Each function fi(w) takes the form fi(w) = \u2113(h(w,xi), yi), where \u2113 is a loss function and h is a prediction model parametrized by w \u2208 Rd. The expectation is over the set of samples and we denote w\u2217 = argminw f(w).\nThis optimization problem can be solved exactly for convex functions using gradient descent, where the gradient of the loss function is expressed as \u2207wf(w) = E[\u2207wfi(w)]. When the size of the dataset n is large, the computation of the gradient is impractical and one has to resort to stochastic gradients. Similar to gradient descent, stochastic gradient descent updates the parameter vector wt by stepping in the opposite direction of the stochastic gradient \u2207wfi(wt) by an amount specified by a step size \u03b7t as follows:\nwt+1 = wt \u2212 \u03b7t\u2207wfi(wt). (1)\nIn general, a stochastic gradient can also be computed as an average over a sample of datapoints as f\u0302(wt) = r\u22121 \u2211r\ni=1 fi(wt). Given that the stochastic gradients are unbiased estimates of the gradient, Robbins and Monro [15] proved convergence of SGD to w\u2217 assuming a decreasing stepsize sequence. A common choice for the step size is [18, 12]\na) \u03b7t = \u03b70 t\nor b) \u03b7t = \u03b70T0 T0 + t\n(2)\nwhere \u03b70 is a constant initial step size and T0 controls the speed of decrease.\nAlthough the cost per iteration of SGD is low, it suffers from slow convergence for certain illconditioned problems [12]. An alternative is to use a second order method such as Newton\u2019s method that estimates the curvature of the objective function and can achieve quadratic convergence. In the following, we review Newton\u2019s method and its approximations known as quasi-Newton methods."}, {"heading": "2.2 Newton\u2019s method and BFGS", "text": "Newton\u2019s method is an iterative method that minimizes the Taylor expansion of f(w) around wt:\nf(w) =f(wt) + (w \u2212wt) \u22a4\u2207wf(wt) +\n1 2 (w \u2212wt) \u22a4H(w\u2212wt), (3)\nwhere H is the Hessian of the function f(w) and quantifies its curvature. Minimizing Eq. 3 leads to the following update rule:\nwt+1 = wt \u2212 \u03b7tH \u22121 t \u00b7 \u2207f(wt), (4)\nwhere \u03b7t is the step size chosen by backtracking line search.\nGiven that computing and inverting the Hessian matrix is an expensive operation, approximate variants of Newton\u2019s method have emerged, where H\u22121t is replaced by an approximate version H\u0303 \u22121 t selected to be positive definite and as close to H\u22121t as possible. The most popular member of this class of quasi-Newton methods is BFGS [13] that incrementally updates an estimate of the inverse Hessian, denoted Jt = H\u0303 \u22121 t . This estimate is computed by solving a weighted Frobenius norm minimization subject to the secant condition:\nwt+1 \u2212wt = Jt+1(\u2207f(wt+1)\u2212\u2207f(wt)). (5)\nThe solution can be obtained in closed form leading to the following explicit expression:\nJt+1 =\n(\nI \u2212 sy\u22a4\ny\u22a4s\n)\nJt\n(\nI \u2212 ys\u22a4\ny\u22a4s\n)\n+ ss\u22a4\ny\u22a4s , (6)\nwhere s = wt+1 \u2212 wt and y = \u2207f(wt+1) \u2212 \u2207f(wt). Eq. 6 is known to be positive definitive assuming that J0 is initialized to be a positive definite matrix."}, {"heading": "2.3 Stochastic BFGS", "text": "A stochastic version of BFGS (oBFGS) was proposed in [17] in which stochastic gradients are used for both the determination of the descent direction and the approximation of the inverse Hessian. The oBFGS approach described in Algorithm 1 uses the following update equation:\nwt+1 = wt \u2212 \u03b7tJ\u0302t \u00b7 \u2207f\u0302(wt), (7)\nwhere the matrix J\u0302t and the vector \u2207f\u0302(wt) are stochastic estimates computed as follows. Let A \u2282 {1 . . . n} and B \u2282 {1 . . . n} be sets containing two independent samples of datapoints. The variables y and \u2207f(w) defined in Eq. 6 are replaced by sampled variables computed as\ny\u0302 = 1\n|A|\n\u2211\nk\u2208A\n\u2207fk(wt+1)\u2212\u2207fk(wt) and \u2207f\u0302(wt) = \u2207fB(wt) = 1\n|B|\n\u2211\nk\u2208B\n\u2207fk(wt). (8)\nThe estimate of the inverse Hessian then becomes\nJ\u0302t+1 =\n(\nI \u2212 sy\u0302\u22a4\ny\u0302\u22a4s\n)\nJ\u0302t\n(\nI \u2212 y\u0302s\u22a4\ny\u0302\u22a4s\n)\n+ ss\u22a4\ny\u0302\u22a4s (9)\nUnlike Newton\u2019s method, oBFGS uses a fixed step size sequence instead of a line search. A common choice is to use a step size similar to the one used for SGD in Eq. 2.\nA regularized version of oBFGS (RES) was recently proposed in [12]. RES differs from oBFGS in the use of a regularizer to enforce a bound on the eigenvalues of J\u0302t such that\n\u03b3I J\u0302t \u03c1I =\n(\n\u03b3 + 1\n\u03b4\n)\nI, (10)\nwhere \u03b3 and \u03b4 are given positive constants and the notation A B means that B \u2212 A is a positive semi-definite matrix. Note that (10) also implies an upper and lower bound on E[J\u0302t] [12]. The update of RES is modified to incorporate an identity bias term \u03b3I as follows:\nwt+1 = wt \u2212 \u03b7t(J\u0302t + \u03b3I) \u00b7 \u2207f\u0302(wt). (11)\nThe convergence proof derived in [12] shows that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee convergence to the optimum. However, the analysis shows that RES will converge to the optimum at a rate O(1/t) and requires a decreasing step-size. Similar results were derived in [3] for the SQN algorithm.\nAlgorithm 1 oBFGS 1: INPUTS : 2: D : Training set of n examples. 3: w0 : Arbitrary initial values, e.g., 0. 4: {\u03b7t} : Step size sequence 5: OUTPUT : wt 6: J\u03020 \u2190 \u03b1I 7: for t = 0 . . . T do 8: Randomly pick two sets A and B 9: s \u2190 wt+1 \u2212wt 10: y\u0302 \u2190 \u2211\nk\u2208B \u2207fk(wt+1)\u2212\u2207fk(wt)\n11: \u2207f\u0302(wt) \u2190 \u2211 k\u2208A \u2207fk(wt)\n12: wt+1 \u2190 wt \u2212 \u03b7tJ\u0302t+1 \u00b7 \u2207f\u0302(wt) 13: J\u0302t+1 \u2190 ( I \u2212 sy\u0302 \u22a4\ny\u0302\u22a4s\n)\nJ\u0302t\n(\nI \u2212 y\u0302s \u22a4\ny\u0302\u22a4s\n)\n+ ss \u22a4\ny\u0302\u22a4s\n14: end for\n3 The VITE algorithm\nReducing the size of the sets A and B used to estimate the inverse Hessian approximation and the stochastic gradient is desirable for reasons of computational efficiency. However, doing so also increases the variance of the update step. Here we propose a new method called VITE that explicitly reduces this variance.\nIn order to simplify the analysis of VITE, we do not explicitly consider the randomness in the matrix J\u0302t. Instead, we assume that it is positive definite (which holds under weak conditions due to the BFGS update step) and that its variance can be kept under control, for example by using the regularization of the RES method.\nTo motivate VITE we first consider the standard oLBFGS step, (7) estimated with the sets A and B. The first and second moments simplify as\nE [J\u0302t\u2207fB(wt)] = J\u0302tEB[\u2207fB(wt)] (12)\nand\nE\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 J\u0302t\u2207fB(wt) \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 \u2264 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 J\u0302t \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 EB ||\u2207fB(wt)|| 2 , (13)\nrespectively. For |A| large enough, in order to reduce the variance of the estimate J\u0302t \u00b7 \u2207fB(wt), it is only required to reduce the variance of \u2207fB(wt) independently. We proceed using a technique similar to the one proposed in [8, 9].\nVITE differs from oBFGS and other stochastic Quasi-Newton methods in the use of a multi-stage scheme as shown in Algorithm 2. In the outer loop a variable w\u0303 is introduced. We periodically evaluate the gradient of the function with respect to w\u0303. This pivot point is inserted in the update equation to reduce the variance. Each inner loop runs for a a random number of steps tj \u2208 [1,m] whose distribution follows a geometric law with parameter \u03b2 = \u2211m\nt=1(1 \u2212 \u00b5\u03b3\u03b7) m\u2212t. Stochastic\ngradients at wt and w\u0303 are computed and the inverse Hessian approximation is updated in each iteration of the inner loop. J\u0302t can be updated using the same update as RES although we found in practice that using Eq. 9 did not affect the results significantly. The descent direction \u2207fB(w) is then replaced by\nvt = \u2207fB(wt)\u2212\u2207fB(w\u0303) + \u03bd\u0303.\nVITE then makes updates of the form\nwt+1 = wt \u2212 \u03b7J\u0302t \u00b7 vt. (14)\nClearly, \u03bd\u0303 = E[\u2207fB(w\u0303)] and E[vt] = E[\u2207fB(wt)] so in expectation the descent is in the same direction as Eq. (12). Following the analysis of [8], the variance of vt goes to zero when both w\u0303 and wt converge to the same parameter w\u2217. Therefore, convergence can be guaranteed with a constant step-size. The complexity of this approach depends on the number of epochs S and a constant m limiting the number of stochastic gradients computed in a single epoch, as well as other parameters that will be introduced in more detail in Section 4.\nAlgorithm 2 VITE 1: INPUTS : 2: D : Training set of n examples w\u03030 : Arbitrary initial values, e.g., 0 3: \u03b7 : Constant step size m: Arbitrary constant 4: OUTPUT : wt 5: J\u03020 \u2190 \u03b1I 6: for s = 0 . . . S do 7: w\u0303 = w\u0303s\u22121 8: \u03bd\u0303 = 1\nn \u2211n i=1 \u2207fi(w\u0303)\n9: w0 = w\u0303\n10: Let tj \u2190 t with probability (1\u2212\u00b5\u03c1\u03b7)m\u2212t\n\u03b2 for t = 1, . . . ,m\n11: for t = 0 . . . tj \u2212 1 do 12: Randomly pick independent sets A,B \u2282 {1 . . . n}"}, {"heading": "13: vt = \u2207fB(wt)\u2212\u2207fB(w\u0303) + \u03bd\u0303", "text": "14: wt+1 \u2190 wt \u2212 \u03b7J\u0302t \u00b7 vt 15: Update J\u0302t+1 16: end for 17: w\u0303s = wtj . 18: end for"}, {"heading": "4 Analysis", "text": "In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9]. Specifically, we show how variance reduction on the stochastic gradient direction is sufficient to establish geometric convergence rates, even when performing linear transformations with a matrix J\u0302t. Since we do not exploit the specific form of the stochastic evolution equations for J\u0302t, this analysis will not allow us to argue in favor of the specific choice of Eq. (9), yet it shows that variance reduction on the gradient estimate is sufficient for fast convergence as long as J\u0302t is sufficiently well behaved. Our analysis relies on the following standard assumptions:\nA1 Each function fi is differentiable and has a Lipschitz continuous gradient with constant L > 0, i.e. \u2200w,v \u2208 Rn,\nfi(w) \u2264 fi(v) + (w \u2212 v) \u22a4\u2207fi(v) +\nL 2 ||w \u2212 v||2 (15)\nA2 f is \u00b5-strongly convex, i.e. \u2200w,v \u2208 Rn,\nf(w) \u2265 f(v) + (w \u2212 v)\u22a4\u2207f(v) + \u00b5\n2 ||w \u2212 v||\n2 (16)\nwhich also implies ||\u2207f(w)|| 2 \u2265 2\u00b5(f(w)\u2212 f(w\u2217)) \u2200w \u2208 Rn (17)\nfor the minimizer w\u2217 of f .\nAssumptions A1 and A2 also implies that the eigenvalues of the Hessian are bounded as follows:\n\u00b5I Ht LI. (18)\nFinally we make the assumption that the inverse Hessian approximation is always well-behaved.\nA3 There exist positive constants \u03b3 and \u03c1 such that, \u2200w \u2208 Rn,\n\u03b3I J\u0302t \u03c1I. (19)\nAssumption A3 is equivalent to assuming that J\u0302t is bounded in expectation (see: e.g. [12]) but allows us to remove this complication, simplifying notation in the analysis which follows. We now introduce two lemmas required for the proof of convergence.\nLemma 1. The following identity holds:\nEf(w\u0303s+1) = 1\n\u03b2\nm\u22121 \u2211\nt=0\n\u03c4tEf(wt)\nwhere \u03c4t := (1\u2212 \u03b3\u03b7\u00b5)m\u2212t\u22121 and the weight vectors wt belong to epoch s.\nThis result follows directly from Lemma 3 in [9].\nLemma 2.\nE\u2016vt\u2016 2 \u2264 4L(f(wt)\u2212 f(w \u2217) + f(w\u0303)\u2212 f(w\u2217))\nThe proof is given in [8, 9] and reproduced for convenience in the Appendix. We are now ready to state our main result.\nTheorem 1. Let Assumptions A1-A3 be satisfied. Define the rescaled strong convexity \u00b5\u2032 := \u03b3\u00b5 \u2264 \u00b5 and Lipschitz L\u2032 := \u03c1L \u2265 L constants respectively. Choose 0 < \u03b7 \u2264 \u00b5 \u2032\n2L\u20322 and let m be sufficiently\nlarge so that \u03b1 = (1\u2212\u03b7\u00b5 \u2032)m\n\u03b2\u03b7(\u00b5\u2032\u22122L\u20322\u03b7) + 2L\n\u20322\u03b7\n\u00b5\u2032\u22122L\u20322\u03b7 < 1.\nThen the suboptimality of w\u0303s is bounded in expectation as follows:\nE(f(w\u0303s)\u2212 f(w \u2217) \u2264 \u03b1sE[f(w0)\u2212 f(w \u2217)]. (20)\nRemark 1. Observe that \u03b3 and \u03c1 are bounds on the inverse Hessian approximation. If J\u0302t is a good approximation to H , then by plugging in \u03b3 = L and \u03c1 = \u00b5, the upper bound on the learning rate reduces to \u03b7 \u2264 12\u00b5L .\nProof of Theorem 1. Our starting point is the basic inequality\nf(wt+1) = f(wt \u2212 \u03b7J\u0302t \u00b7 vt)\n\u2264 f(wt)\u2212 \u03b7\u3008\u2207f(wt), J\u0302t \u00b7 vt\u3009+ L 2 \u03b72 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 J\u0302tvt \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 . (21)\nWe first use the properties of vt and J\u0302t to reduce the dependence of (21) on J\u0302t to its largest and smallest eigenvalues given by (19). For the purpose of the analysis, we define Ft to be the sigmaalgebra measuring wt. By conditioning on Ft, and by A3, the remaining randomness is in the choice of the index set B in round t, which is tied to the stochasticity of vt. Taking expectations with respect to B gives us\nEB\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 J\u0302tvt \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 \u2264 \u2016J\u0302t\u2016 2 EB\u2016vt\u2016 2 \u2264 \u03c12EB\u2016vt\u2016 2 (22)\nand\nEB\u3008\u2207f(wt), J\u0302t \u00b7 vt\u3009 = \u3008\u2207f(wt), J\u0302t \u00b7 \u2207f(wt)\u3009 \u2265 \u03b3 ||\u2207f(wt)|| 2 (23)\nwhere (23) comes from the definition EBvt = \u2207f(wt). Therefore, taking the expectation of the inequality (21) and dropping the notational dependence on B results in\nEf(wt+1) \u2264 Ef(wt)\u2212 \u03b3\u03b7E ||\u2207f(wt)|| 2 +\nL 2 \u03b72\u03c12E ||vt|| 2 . (24)\nTo simplify the remainder of the proof we make the following substitution\n\u00b5\u2032 := \u03b3\u00b5 \u2264 \u00b5 and L\u2032 := \u03c1L \u2265 L.\nConsidering a fixed epoch s, we can further bound Ef(wt+1) using Lemma 2 and Eq. 17. By taking the expectation over Ft, adding and subtracting f(w\u2217), we get\nE[f(wt+1)\u2212 f(w \u2217)] \u2264E[f(wt)\u2212 f(w \u2217)] + 2\u03b72L\u2032 2( f(w\u0303s)\u2212 f(w \u2217) )\n(25)\n+ 2 ( \u03b72L\u2032 2 \u2212 \u03b7\u00b5\u2032 ) E[f(wt)\u2212 f(w \u2217)]\n=2\u03b72L\u2032 2( f(w\u0303s)\u2212 f(w \u2217) ) + ( 2\u03b72L\u2032 2 \u2212 2\u03b7\u00b5\u2032 + 1 ) E[f(wt)\u2212 f(w \u2217)].\nWriting \u2206f(wt) := f(wt)\u2212 f(w\u2217), we then have\n(\u03b7\u00b5\u2032 \u2212 2\u03b72L\u2032 2 )E\u2206f(wt) \u2264 2\u03b7 2L\u2032 2 \u2206f(w\u0303s) + (1\u2212 \u03b7\u00b5 \u2032)E\u2206f(wt)\u2212 E\u2206f(wt+1) (26)\nNow we sum all these inequalities at iterations t = 0, . . . ,m\u2212 1 performed in epoch s with weights \u03c4t = (1\u2212 \u03b7\u00b5 \u2032)m\u2212t\u22121. Applying Lemma 1 to the last summand to recover f(w\u0303s+1) we arrive at\n\u03b2E\u2206f(w\u0303s+1) \u2264 2\u03b2\u03b72L\u2032\n2\n\u03b7\u00b5\u2032 \u2212 2\u03b72L\u20322 E\u2206f(w\u0303s) +\nm\u22121 \u2211\nt=0\n\u03c4t (1 \u2212 \u03b7\u00b5\u2032)E\u2206f(wt)\u2212 E\u2206f(wt+1)\n\u03b7\u00b5\u2032 \u2212 2\u03b72L\u20322 .\nWe now need to bound the remaining sum (\u2217) in the numerator, which can be accomplished by re-grouping summands\n(\u2217) =(1\u2212 \u03b7\u00b5\u2032)mE\u25b3f(w\u0303s)\u2212 (1\u2212 \u03b7\u00b5 \u2032)E\u25b3f(w\u0303s+1)\nBy ignoring the negative term in (\u2217), we get the final bound\nE\u2206f(w\u0303s+1) \u2264 \u03b1E\u2206f(w\u0303s),\nwhere\n\u03b1 =\n(\n(1\u2212 \u03b7\u00b5\u2032)m\n\u03b2(\u03b7\u00b5\u2032 \u2212 2\u03b72L\u20322) +\n2\u03b72L\u2032 2\n\u03b7\u00b5\u2032 \u2212 2\u03b72L\u20322\n)\nTheorem 1 implies that VITE has a local geometric convergence rate with a constant learning rate. In order to satisfy E(f(w\u0303s)\u2212 f(w\u2217)) \u2264 \u01eb, the number of stages s needs to satisfy\ns \u2265 \u2212 log\u03b1\u22121 log E(f(w\u03030)\u2212 f(w\n\u2217))\n\u01eb .\nSince each stage requiresn+m(2|A|+2|B|) component gradient evaluations, the overall complexity is O((n+ 2m(|A|+ |B|)) log(1/\u01eb))."}, {"heading": "5 Experimental Results", "text": "This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information. We consider two commonly occurring problems in machine learning, namely least-square regression and regularized logistic regression.\nLinear Least Squares Regression. We apply least-square regression on the binary version of the COV dataset [4] that contains n = 581, 012 datapoints, each described by d = 54 input features. Logistic Regression. We apply logistic regression on the ADULT and IJCNN1 datasets obtained from the LIBSVM website 1. The ADULT dataset contains n = 32, 561 datapoints, each described by d = 123 input features. The IJCNN1 dataset contains n = 49, 990 datapoints, each described by d = 22 input features. We added an \u21132-regularizer with parameter \u03bb = 10\u22125 to ensure the objective is strongly convex.\nThe complexity of VITE depends on three quantities: the approximate Hessian J\u0302 , the pair of stochastic gradients (\u2207fB(w),\u2207fB(w\u0303)) and \u03bd\u0303, respectively computed over the sets A, B and D. Similarly to [12], we consider different choices for |A| and |B| and pick the best value in a limited interval {1, . . . , 0.05n}. These results are also reported for the RES method that also depends on both |A| and |B|. For SGD, we use |B| = 1 as we found this value to be the best performer on all datasets. Computing the average gradient, \u03bd\u0303 over the full dataset for SVRG and VITE is impractical. We therefore estimate \u03bd\u0303 over a small subset C \u2282 D. Although this introduces some bias, it did not seem to practically affect convergence for sufficiently large |C|. In our experiments, we selected |C| = 0.1n samples uniformly at random. Each experiment was averaged over 5 runs with different\n1http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets\ninitializations of w0 and a random selection of the samples in A, B and C. Given that the complexity per iteration of each method is different, we compare them as a function of the number of gradient evaluations.\nFig. 1 shows the empirical convergence properties of VITE against RES for least-square regression and logistic regression. The horizontal axis corresponds to the number of gradient evaluations while the vertical axis corresponds to the objective function value. The vertical bars in each plot show the variance over 5 runs. We show plots for different values of |B| and the best corresponding A. For small |B|, the variance of the stochastic gradients clearly hurts RES while the variance corrections of VITE lead to fast convergence. As we increase |B|, thus reducing the variance of the stochastic gradients, the convergence rate of RES and VITE becomes similar. However, VITE with small |B| is much faster to converge to a lower objective value. This clearly demonstrates how using small batches for the computation of the gradients while reducing their variance leads to a fast convergence rate. We also investigated the effect of |A| on the convergence of RES and VITE (see Appendix). In short, we find that a good-enough curvature estimate can be obtained for |A| = O(10\u22125n). Increasing this value incurs a penalty in terms of number of gradient evaluations required and so overall performance degrades.\nFinally, we compared VITE against SGD, RES and SVRG [8, 9]. A critical factor in the performance of SGD is the selection of the step-size. We use the step-size given in Eq. 2b and pick the parameters T0 and \u03b70 by performing cross-validation over T0 = {1, 10, 102, . . . , 104} and \u03b70 = {10\n\u22121, . . . , 10\u22125}. Although it is a quasi-Newton method, RES also requires a decaying stepsize and so the same selection process was performed. For SVRG and VITE, we used a constant step size chosen in the same interval as \u03b70. For SVRG and VITE we used the same size subset, C to compute \u03bd\u0303. Fig. 2 shows the objective value of each method in log scale. Although RES and SVRG are superior to SGD, neither clearly outperforms the other. On the other hand, we observe that VITE consistently converges faster than both RES and SVRG. This demonstrates that the combination of second order information and variance reduction is beneficial for fast convergence."}, {"heading": "6 Conclusion", "text": "We have shown that stochastic variants of BFGS can be made more robust to the effects of noisy stochastic gradients using variance reduction. We introduced VITE and showed that it obtains a geometric convergence rate for smooth convex functions \u2013 to our knowledge the first stochastic Quasi-Newton algorithm with this property. We have shown experimentally that VITE outperforms both variance reduced SGD and stochastic BFGS. The theoretical analysis we present is quite general and additionally only requires that the bound on the eigenvalues of the inverse Hessian matrix in (19) holds. Therefore, the variance reduced framework we propose can be extended to other quasi-Newton methods, including the widely used L-BFGS and ADAGRAD [7] algorithms. Finally, an important open question is how to bridge the gap between the theoretical and empirical results. Specifically, whether it is possible to obtain better convergence rates for stochastic BFGS algorithms which match the improvement we have demonstrated over SVRG."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 Proof of Lemma 2", "text": "E ||vt|| 2 = E ||\u2207fi(wt)\u2212\u2207fi(w\u0303) +\u2207f(w\u0303)|| 2\n\u2264 2E ||\u2207fi(wt)\u2212\u2207fi(w \u2217)||\n2\n+ 2E ||(\u2207fi(w\u0303)\u2212\u2207fi(w \u2217))\u2212\u2207f(w\u0303)||\n2\n= 2E ||\u2207fi(wt)\u2212\u2207fi(w \u2217)||2\n+ 2E ||(\u2207fi(w\u0303)\u2212\u2207fi(w \u2217))\u2212 (\u2207f(w\u0303)\u2212\u2207f(w\u2217))||\n2\n\u2264 2E ||\u2207fi(wt)\u2212\u2207fi(w \u2217)||\n2\n+ 2E ||\u2207fi(w\u0303)\u2212\u2207fi(w \u2217)||\n2\n\u2264 4L(f(wt)\u2212 f(w \u2217) + f(w\u0303)\u2212 f(w\u2217)) (27)\nThe second inequality uses E ||\u03be \u2212 E\u03be||2 = E ||\u03be||2 \u2212 ||E\u03be||2 \u2264 E ||\u03be||2 for any random vector \u03be.\nThe last inequality uses the following inequality derived from the fact that fi is a Lipschitz function:\nE ||\u2207fi(w \u2217)\u2212\u2207fi(wt)|| 2 \u2264 2L(f(wt)\u2212 f(w \u2217)).\n7.2 Selection of the parameter |A|.\nFigure 3 shows the effect of the set A, used to estimate the inverse Hessian, on the convergence of RES and VITE. We show results for |A| = {0.00001, 0.0001} \u00d7 n. Firstly we see that better performance is obtained for both methods for the smaller value of |A|. By increasing |A|, the penalty paid in terms of gradient evaluations outweighs the gain in terms of better curvature estimates and so convergence is slower. A similar observation was made in [12]. However, we also observe that VITE always outperforms RES for all combinations of |A|."}], "references": [{"title": "et al", "author": ["F. Bach", "E. Moulines"], "venue": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451\u2013459", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, pages 177\u2013186. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A stochastic quasi-newton method for large-scale optimization", "author": ["R.H. Byrd", "S. Hansen", "J. Nocedal", "Y. Singer"], "venue": "arXiv preprint arXiv:1401.7020", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A parallel mixture of svms for very large scale problems", "author": ["R. Collobert", "S. Bengio", "Y. Bengio"], "venue": "Neural computation, 14(5):1105\u20131114", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems, pages 1646\u20131654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Jr and J", "author": ["J.E. Dennis"], "venue": "J. Mor\u00e9. Quasi-newton methods, motivation and theory. SIAM review, 19(1):46\u201389", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1977}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, pages 315\u2013323", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method", "author": ["S. Lacoste-Julien", "M. Schmidt", "F. Bach"], "venue": "arXiv preprint arXiv:1212.2002", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, 45(1-3):503\u2013528", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1401.7625", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "volume 2. Springer New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "arXiv preprint arXiv:1109.5647", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pages 400\u2013407", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "Intl. Conf. Artificial Intelligence and Statistics (AIstats)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming, 127(1):3\u201330", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].", "startOffset": 166, "endOffset": 173}, {"referenceID": 17, "context": "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].", "startOffset": 166, "endOffset": 173}, {"referenceID": 0, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 9, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 13, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 4, "context": "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.", "startOffset": 23, "endOffset": 30}, {"referenceID": 7, "context": "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.", "startOffset": 31, "endOffset": 37}, {"referenceID": 8, "context": "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.", "startOffset": 31, "endOffset": 37}, {"referenceID": 5, "context": "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Stochastic variants of BFGS have been proposed (oBFGS [17]), for which stochastic gradients replace their deterministic counterparts.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "A regularized version known as RES [12] achieves a sublinear convergence rate with a decreasing step-size by", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "SQN [3], another related method also requires a decreasing step size to achieve sub-linear convergence.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "Although stochastic second order methods have not be shown to achieve super-linear convergence, they empirically outperform SGD for problems with a large condition number [12].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.", "startOffset": 109, "endOffset": 115}, {"referenceID": 8, "context": "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.", "startOffset": 109, "endOffset": 115}, {"referenceID": 14, "context": "Given that the stochastic gradients are unbiased estimates of the gradient, Robbins and Monro [15] proved convergence of SGD to w assuming a decreasing stepsize sequence.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "A common choice for the step size is [18, 12] a) \u03b7t = \u03b70 t or b) \u03b7t = \u03b70T0 T0 + t (2) where \u03b70 is a constant initial step size and T0 controls the speed of decrease.", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "A common choice for the step size is [18, 12] a) \u03b7t = \u03b70 t or b) \u03b7t = \u03b70T0 T0 + t (2) where \u03b70 is a constant initial step size and T0 controls the speed of decrease.", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "Although the cost per iteration of SGD is low, it suffers from slow convergence for certain illconditioned problems [12].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "The most popular member of this class of quasi-Newton methods is BFGS [13] that incrementally updates an estimate of the inverse Hessian, denoted Jt = H\u0303 \u22121 t .", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "3 Stochastic BFGS A stochastic version of BFGS (oBFGS) was proposed in [17] in which stochastic gradients are used for both the determination of the descent direction and the approximation of the inverse Hessian.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "A regularized version of oBFGS (RES) was recently proposed in [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Note that (10) also implies an upper and lower bound on E[\u0134t] [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "(11) The convergence proof derived in [12] shows that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee convergence to the optimum.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Similar results were derived in [3] for the SQN algorithm.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "We proceed using a technique similar to the one proposed in [8, 9].", "startOffset": 60, "endOffset": 66}, {"referenceID": 8, "context": "We proceed using a technique similar to the one proposed in [8, 9].", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "Following the analysis of [8], the variance of vt goes to zero when both w\u0303 and wt converge to the same parameter w.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].", "startOffset": 172, "endOffset": 178}, {"referenceID": 8, "context": "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].", "startOffset": 172, "endOffset": 178}, {"referenceID": 11, "context": "[12]) but allows us to remove this complication, simplifying notation in the analysis which follows.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This result follows directly from Lemma 3 in [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "E\u2016vt\u2016 2 \u2264 4L(f(wt)\u2212 f(w ) + f(w\u0303)\u2212 f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.", "startOffset": 63, "endOffset": 69}, {"referenceID": 8, "context": "E\u2016vt\u2016 2 \u2264 4L(f(wt)\u2212 f(w ) + f(w\u0303)\u2212 f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.", "startOffset": 63, "endOffset": 69}, {"referenceID": 7, "context": "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.", "startOffset": 116, "endOffset": 119}, {"referenceID": 11, "context": "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "We apply least-square regression on the binary version of the COV dataset [4] that contains n = 581, 012 datapoints, each described by d = 54 input features.", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Similarly to [12], we consider different choices for |A| and |B| and pick the best value in a limited interval {1, .", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "Finally, we compared VITE against SGD, RES and SVRG [8, 9].", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "Finally, we compared VITE against SGD, RES and SVRG [8, 9].", "startOffset": 52, "endOffset": 58}, {"referenceID": 6, "context": "Therefore, the variance reduced framework we propose can be extended to other quasi-Newton methods, including the widely used L-BFGS and ADAGRAD [7] algorithms.", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "A similar observation was made in [12].", "startOffset": 34, "endOffset": 38}], "year": 2015, "abstractText": "Quasi-Newton methods are widely used in practise for convex loss minimization problems. These methods exhibit good empirical performance on a wide variety of tasks and enjoy super-linear convergence to the optimal solution. For largescale learning problems, stochastic Quasi-Newton methods have been recently proposed. However, these typically only achieve sub-linear convergence rates and have not been shown to consistently perform well in practice since noisy Hessian approximations can exacerbate the effect of high-variance stochastic gradient estimates. In this work we propose VITE, a novel stochastic Quasi-Newton algorithm that uses an existing first-order technique to reduce this variance. Without exploiting the specific form of the approximate Hessian, we show that VITE reaches the optimum at a geometric rate with a constant step-size when dealing with smooth strongly convex functions. Empirically, we demonstrate improvements over existing stochastic Quasi-Newton and variance reduced stochastic gradient methods.", "creator": "LaTeX with hyperref package"}}}