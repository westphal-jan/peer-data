{"id": "1504.03991", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2015", "title": "Theory of Dual-sparse Regularized Randomized Reduction", "abstract": "in this paper, primarily we study simplified randomized reduction methods, approaches which reduce high - dimensional features into low - dimensional space by randomized methods ( e. g., random projection, random hashing ), for each large - scale high - dimensional classification. since previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e. g., low rank of the data complexity matrix or a large separable margin of random classification, which hinder their applications in broad domains. to address these limitations, we propose dual - sparse regularized simple randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. under a naturally mild condition that the original dual solution is a ( nearly ) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. in numerical experiments, we present such an empirical study to support the analysis and we also present a novel application of the dual - sparse regularized randomized reduction methods to reducing the communication cost of distributed learning from large - scale high - dimensional data.", "histories": [["v1", "Wed, 15 Apr 2015 19:16:54 GMT  (68kb)", "http://arxiv.org/abs/1504.03991v1", null], ["v2", "Mon, 18 May 2015 13:03:39 GMT  (84kb)", "http://arxiv.org/abs/1504.03991v2", null], ["v3", "Tue, 26 May 2015 21:44:02 GMT  (84kb)", "http://arxiv.org/abs/1504.03991v3", null], ["v4", "Sat, 18 Jul 2015 21:16:09 GMT  (84kb)", "http://arxiv.org/abs/1504.03991v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tianbao yang", "lijun zhang 0005", "rong jin", "shenghuo zhu"], "accepted": true, "id": "1504.03991"}, "pdf": {"name": "1504.03991.pdf", "metadata": {"source": "META", "title": "Theory of Dual-sparse Regularized Randomized Reduction ", "authors": ["Tianbao Yang", "Lijun Zhang", "Rong Jin", "Shenghuo Zhu"], "emails": ["TIANBAO-YANG@UIOWA.EDU", "ZHANGLJ@LAMDA.NJU.EDU.CN", "RONGJIN@CSE.MSU.EDU", "SHENGHUO@GMAIL.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n03 99\n1v 1\n[ cs\n.L G"}, {"heading": "1. Introduction", "text": "As the scale and dimensionality of data continue to grow in many applications (e.g., bioinformatics, finance, com-\nputer vision, medical informatics) (Sa\u0301nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems. Randomized reduction methods for large-scale or high-dimensional data analytics have received a great deal of attention in recent years (Mahoney & Drineas, 2009; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Mahoney, 2011). By either reducing the dimensionality (referred to as feature reduction) or reducing the number of training instances (referred to as instance reduction), the resulting problem has a smaller size of training data that is not only memoryefficient but also computation-efficient. While randomized instance reduction has been studied a lot for fast least square regression (Drineas et al., 2008; 2006; 2011; Ma et al., 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Shi et al., 2009a) (e.g., random hashing is a noticeable built-in tool in Vowpal Wabbit 1, a fast learning library, for solving highdimensional problems.). In this paper, we focus on the latter technique and refer to randomized feature reduction as randomized reduction for short.\nAlthough several theoretical properties have been examined for randomized reduction methods when applied to classification, e.g., generalization performance (Paul et al., 2013), preservation of margin (Blum, 2005; Balcan et al., 2006; Shi et al., 2012) and the recovery error of the model (Zhang et al., 2014), these previous results reply on strong assumptions about the data. For example, both (Paul et al., 2013) and (Zhang et al., 2014) assume the data\n1 http://hunch.net/\u02dcvw/\nmatrix is of low-rank, and (Blum, 2005; Balcan et al., 2006; Shi et al., 2012) make a assumption that all examples in the original space are separated with a positive margin (with a high probability). Another analysis in (Zhang et al., 2014) assumes the weight vector for classification is sparse. These assumptions are too strong to hold in many real applications.\nContributions. To address these limitations, we propose dual-sparse regularized randomized reduction methods referred to as DSRR by leveraging the (near) sparsity of dual solutions for large-scale high-dimensional (LSHD) classification problems (i.e., the number of (effective) support vectors is small compared to the total number of examples). In particular, we add a dual-sparse regularizer into the reduced dual problem. We present a novel theoretical analysis of the recovery error of the dual variables and the primal variable and study its implication for different randomized reduction methods (e.g., random projection, random hashing and random sampling).\nNovelties. Compared with previous works (Blum, 2005; Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013), our theoretical analysis demands a mild assumption about the data and directly provides guarantee on a small recovery error of the obtained model, which is critical for subsequent analysis, e.g., feature selection (Guyon et al., 2002; Brank et al., 2002) and model interpretation (Ra\u0308tsch et al., 2005; Sonnenburg & Franc, 2010; Rtsch et al., 2005; Sonnenburg et al., 2007; Ben-Hur et al., 2008). For example, when exploiting a linear model to classify people into sick or not sick based on genomic markers, the learned weight vector is important for understanding the effect of different genomic markers on the disease and for designing effective medicine (Jostins & Barrett, 2011; Kang & Cho, 2011). In addition, the recovery could also increase the predictive performance, in particular when there exists noise in the original features (Goldberger et al., 2005).\nCompared with (Zhang et al., 2014) that proposes to recover a linear model in the original feature space by dual recovery, i.e., constructing a weight vector using the dual variables learned from the reduced problem and the original feature vectors, our methods are better in that (i) we rely on a more realistic assumption of the sparsity of dual variables (e.g., in support vector machine (SVM)); (ii) we analyze both smooth loss functions and non-smooth loss functions (they focused on smooth functions); (iii) we study different randomized reduction methods in the same framework not just the random projection.\nIn numerical experiments, we present an empirical study on a real data set to support our analysis and we also demonstrate a novel application of the reduction and recovery framework in distributed learning from LSHD data, which combines the benefits of the two complementary\ntechniques for addressing big data problems. Distributed learning/optimization recently receives significant interest in solving big data problems (Jaggi et al., 2014; Li et al., 2014; Yang, 2013; Agarwal et al., 2011). However, it is notorious for high communication cost, especially when the dimensionality of data is very high. By solving a dimensionality reduced data problem and using the recovered solution as an initial solution to the distributed optimization on the original data, we can reduce the number of iterations and the communication cost. In practice, we employ the recently developed distributed stochastic dual coordinate ascent algorithm (Jaggi et al., 2014; Yang, 2013), and observe that using the recovered solution as an initial solution we are able to attain almost the same performance with only one or two communications of high dimensional vectors among multiple machines."}, {"heading": "2. Preliminaries", "text": "Let (xi, yi), i = 1, . . . , n denote a set of training examples, where xi \u2208 Rd, yi \u2208 {1,\u22121}. Assume both n and d are very large. The goal of classification is to solve the following optimization problem:\nw\u2217 = arg min w\u2208Rd\n1\nn\nn\u2211\ni=1\n\u2113(w\u22a4xiyi) + \u03bb\n2 \u2016w\u201622 (1)\nwhere \u2113(zy) is a convex loss function and \u03bb is a regularization parameter. Using the conjugate function, we can turn the problem into a dual problem:\n\u03b1\u2217 = arg max \u03b1\u2208Rn \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1TX\u22a4X\u03b1 (2)\nwhere X = (x1, . . . ,xn) is the data matrix and \u2113\u2217i (\u03b1) is the convex conjugate function of \u2113(zyi). Given the optimal dual solution \u03b1\u2217, the optimal primal solution can be computed by w\u2217 = \u2212 1\u03bbnX\u03b1\u2217. For LSHD problems, directly solving the primal problem (1) or the dual problem (2) could be very expensive. We aim to address the challenge by randomized reduction methods. Let A(\u00b7) : Rd \u2192 Rm denote a randomized reduction operator that reduces a ddimensional feature vector into m-dimensional feature vector. Let x\u0302 = A(x) denote the reduced feature vector. With the reduced feature vector x\u03021, . . . , x\u0302n of the training examples, a conventional approach is to solve the following reduced primal problem\nu\u2217 = arg min u\u2208Rd\n1\nn\nn\u2211\ni=1\n\u2113(u\u22a4x\u0302iyi) + \u03bb\n2 \u2016u\u201622 (3)\nor its the dual problem\n\u03b1\u0302\u2217 = arg max \u03b1\u2208Rn \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1T X\u0302\u22a4X\u0302\u03b1 (4)\nwhere X\u0302 = (x\u03021, . . . , x\u0302n) \u2208 Rm\u00d7n. Previous studies have analyzed the reduced problems for random projection methods and proved the preservation of margin (Blum,\n2005; Shi et al., 2012) and the preservation of minimum enclosing ball (Paul et al., 2013). Zhang et al. (2014) proposed a dual recovery approach that constructs a recovered solution by w\u0302\u2217 = \u2212 1\u03bbn \u2211n i=1[\u03b1\u0302\u2217]ixi and proved the recovery error for random projection under the assumption of low-rank data matrix or sparse w\u2217. In addition, they also showed that the naive recovery by A\u22a4u\u2217 whenA(x) = Ax has a large recovery error.\nOne limitation with the simple dual recovery approach is that due to the reduction in the feature space, many nonsupport vectors for the original optimization problem will become support vectors, which could result in the corruption in the recovery error. As a result, the original analysis of dual recovery method requires strong assumption of data (i.e. low rank assumption). In this work, we plan to address this limitation in a different way, which allows us to relax the assumption significantly."}, {"heading": "3. DSRR and its Guarantee", "text": "To reduce the number of or the contribution of training instances that are non-support vectors in the original optimization problem and are transformed into support vectors due to the reduction of the feature space, we employ a simple trick that adds a dual-sparse regularization to the reduced dual problem. In particular, we solve the following problem: \u03b1\u0303\u2217 = (5)\narg max \u03b1\u2208Rn \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1T X\u0302\u22a4X\u0302\u03b1\u2212 1 n R(\u03b1)\nwhere R(\u03b1) = \u03c4\u2016\u03b1\u20161, and \u03c4 > 0 is a regularization parameter, whose theoretical value will be revealed later.\nTo further understand the added dual-sparse regularizer, we consider SVM, where the loss function can be either the hinge loss (a non-smooth function) \u2113(zy) = max(0, 1\u2212zy) or the squared hinge loss (a smooth function) \u2113(zy) = max(0, 1 \u2212 zy)2. We first consider the hinge loss, where \u2113\u2217i (\u03b1i) = \u03b1iyi for \u03b1iyi \u2208 [\u22121, 0]. Then the new dual problem is equivalent to\nmax \u03b1\u25e6y\u2208[\u22121,0]n\n1\nn\nn\u2211\ni=1\n\u2212\u03b1iyi \u2212 1 2\u03bbn2 \u03b1T X\u0302\u22a4X\u0302\u03b1\u2212 \u03c4 n \u2016\u03b1\u20161\nUsing variable transformation \u2212\u03b1iyi \u2192 \u03b2i, the above problem is equivalent to\nmax \u03b2\u2208[0,1]n\n1\nn\nn\u2211\ni=1\n\u03b2i(1 \u2212 \u03c4)\u2212 1 2\u03bbn2 (\u03b2 \u25e6 y)T X\u0302\u22a4X\u0302(\u03b2 \u25e6 y)\nChanging into the primal form, we have\nmax u\u2208Rm\n1\nn\nn\u2211\ni=1\n\u21131\u2212\u03c4 (u \u22a4x\u0302iyi) +\n\u03bb 2 \u2016u\u201622 (6)\nwhere \u2113\u03b3(z) = max(0, \u03b3 \u2212 z) is a max-margin loss with margin given by \u03b3. It can be understood that adding the \u21131\nregularization in the reduced problem of SVM is equivalent to using a max-margin loss with a smaller margin, which is intuitive because examples become difficult to separate after dimensionality reduction and is consistent with several previous studies that the margin is reduced in the reduced feature space (Blum, 2005; Shi et al., 2012). Similarly for squared hinge loss, the equivalent primal problem is\nmax u\u2208Rm\n1\nn\nn\u2211\ni=1\n\u211321\u2212\u03c4 (u \u22a4x\u0302iyi) +\n\u03bb 2 \u2016u\u201622 (7)\nwhere \u21132\u03b3(z) = max(0, \u03b3 \u2212 z)2. Although adding a dual-sparse regularizer is intuitive and can be motivated from previous results, we emphasize that the proposed dual-sparse formulation provides a new perspective and bounding the dual recovery error \u2016\u03b1\u0303\u2217\u2212\u03b1\u2217\u2016 is a non-trivial task, which is a major contribution of this paper. We first state our main result in Theorem 1 for smooth loss functions.\nTheorem 1. Let \u03b1\u0303\u2217 be the optimal dual solution to (5). Assume \u03b1\u2217 is s-sparse with the support set given by S. If \u03c4 \u2265 2\u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e, then we have \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 3\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 (8) Furthermore, if \u2113(z) is a L-smooth loss function, we have\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 3\u03c4L \u221a s, \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20161 \u2264 12\u03c4Ls (9)\n\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2264 3\u03c4Ls, \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 9\u03c4Ls (10) where Sc is the complement of S, and [\u03b1]S is a vector that only contains the elements of \u03b1 in the set S. Remark 1: The inequality in (8) indicates that \u03b1\u0303\u2217 will concentrate on the optimal support set S. It can be seen that the dual recovery error is proportional to the value of \u03c4 which is dependent on \u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e, which we can bound without using any assumption about the data matrix or the optimal dual variable \u03b1\u2217. In contrast, previous bounds (Zhang et al., 2014; Paul et al., 2013) depend on \u2016X\u22a4X \u2212 X\u0302\u22a4X\u0302\u20162, which requires the low rank assumption on X . In next section, we provide an upper bound of 1\u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e that will allows us to understand how the reduced dimensionality m affects the recovery error. Essentially, the results indicate that for random projection, randomized Hadamard transform and random hashing, 1\u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e \u2264 O( \u221a\nlog(n/\u03b4) m )\u2016w\u2217\u20162 with a high probability 1\u2212\u03b4, and thus\nthe recovery error will be scaled as \u221a 1/m in terms of m - the same order of recovery error as in (Zhang et al., 2014) that assumes low rank of the data matrix.\nRemark 2: We would like make a connection with LASSO for sparse signal recovery (compressive sensing). In sparse signal recovery under noise measurements f = Uw\u2217 + e, where e denotes the noise in measurements, if a LASSO minw 1 2\u2016Uw\u2212f\u201622+\u03bb\u2016w\u20161 is solved for the solution, then the regularization parameter \u03bb is required to be larger than\nthe quantity \u2016U\u22a4e\u2016\u221e that depends on the noise in order to have an accurate recovery (Eldar & Kutyniok, 2012). Similarly in our formulation, the added \u21131 regularization \u03c4\u2016\u03b1\u20161 is to counteract the noise in X\u0302X\u0302\u22a4 as compared with XX\u22a4 and the value of \u03c4 is dependent on the noise.\nTo present the theoretical result on the non-smooth loss functions, we need to introduce restricted eigen-value conditions similar to those used in the sparse recovery analysis for LASSO (Bickel et al., 2009; Xiao & Zhang, 2013). In particular, we introduce the following definition of restricted eigen-value condition.\nDefinition 2. Given an integer s > 0, we define Kn,s = {\u03b1 \u2208 Rn : \u2016\u03b1\u20162 \u2264 1, \u2016\u03b1\u20161 \u2264 \u221a s}. We say that X satisfies the restricted eigenvalue condition at sparsity level s if there exists positive constants \u03c1+s and \u03c1\u2212s such that\n\u03c1+s = sup \u03b1\u2208Kn,s \u03b1\u22a4X\u22a4X\u03b1, \u03c1\u2212s = inf \u03b1\u2208Kn,s \u03b1\u22a4X\u22a4X\u03b1.\nWe also define another quantity that measures the restricted eigen-value of X\u22a4X \u2212 X\u0302\u22a4X\u0302 , namely\n\u03c3s = sup \u03b1\u2208Kn,s\n|\u03b1\u22a4(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1|. (11)\nTheorem 3. Let \u03b1\u0303\u2217 be the optimal dual solution to (5). Assume \u03b1\u2217 is s-sparse with the support set given by S. If \u03c4 \u2265 2\u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e, then we have \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 3\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 Assume the data matrix X satisfies the restricted eigenvalue condition at sparsity level 16s and \u03c316s < \u03c1 \u2212 16s, we have\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 3\u03c4\u03bbn\n\u221a s\n2(\u03c1\u221216s \u2212 \u03c316s)\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20161 \u2264 12\u03c4\u03bbns\n2(\u03c1\u221216s \u2212 \u03c316s)\nRemark 3: Compared to smooth loss functions, the conditions that guarantee a small recovery for non-smooth loss functions are more restricted. In next section, we will provide a bound on \u03c316s to further understand the condition of \u03c316s \u2264 \u03c1\u221216s, which essentially implies that m \u2265 \u2126 (( \u03c1+ 16s\n\u03c1\u2212 16s\n)2 s log(n/s) ) .\nRemark 4: With the recovery error bound for the dual solution, we can easily derive an error bound for the primal solution w\u0303\u2217 = \u2212 1\u03bbnX\u03b1\u0303\u2217, i.e., \u2016w\u0303\u2217\u2212w\u2217\u20162 \u2264 1\u03bbn\u2016X(\u03b1\u2217\u2212 \u03b1\u0303\u2217)\u20162 \u2264 \u221a \u03c1+ 16s\n\u03bbn \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162. Last but not least, we provide a theoretical result on the recovery error for the nearly sparse optimal dual variable \u03b1\u2217. We state the result for smooth loss functions. To quantify the near sparsity, we let \u03b1s\u2217 \u2208 Rn denote a vector that zeros all entries in \u03b1\u2217 except for the top-s elements in magnitude\nand assume \u03b1s\u2217 satisfies the following condition:\u2225\u2225\u2225\u2225\u2207\u2113\u2217(\u03b1s\u2217) + 1\n\u03bbn X\u22a4X\u03b1s\u2217 \u2225\u2225\u2225\u2225 \u221e \u2264 \u03be (12)\nwhere \u2207\u2113\u2217(\u03b1) = (\u2207\u2113\u22171(\u03b11), . . . ,\u2207\u2113\u2217n(\u03b1n))\u22a4. The above condition can be considered as a sub-optimality condition (Boyd & Vandenberghe, 2004) of \u03b1s\u2217 measured in the infinite norm. For the optimal solution \u03b1\u2217, we have \u2207\u2113\u2217(\u03b1\u2217) + 1\u03bbnX\u22a4X\u03b1\u2217 = 0. Theorem 4. Let \u03b1\u0303\u2217 be the optimal dual solution to (5). Assume \u03b1\u2217 is nearly s-sparse such that (12) holds with the support set of \u03b1s\u2217 given by S. If \u03c4 \u2265 2\u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e + 2\u03be, then we have \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 3\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 Furthermore, if \u2113(z) is a L-smooth loss function, we have\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1s\u2217\u20162 \u2264 3\u03c4L \u221a s, \u2016\u03b1\u0303\u2217 \u2212 \u03b1s\u2217\u20161 \u2264 12\u03c4Ls (13) \u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2264 3\u03c4Ls, \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 9\u03c4Ls (14)\nRemark 5: The proof is included in supplement. Compared to Theorem 1 for exactly sparse optimal dual solution, the dual recovery error bound for nearly sparse optimal dual solution is increased by 6L \u221a s\u03be for \u21132 norm and by 24Ls\u03be for \u21131 norm."}, {"heading": "4. Analysis", "text": "In this section, we first provide upper bound analysis of 2 \u03bbn\u2016(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1\u2217\u2016\u221e and \u03c3s, and then present the proofs of Theorem 1 and Theorem 3 for smooth loss functions and non-smooth loss functions, respectively. To facilitate our analysis, we define\n\u2206 = 1 \u03bbn (X\u0302\u22a4X\u0302 \u2212X\u22a4X)\u03b1\u2217"}, {"heading": "4.1. Bounding \u2016\u2206\u2016\u221e", "text": "A critical condition in both Theorem 1 and Theorem 3 is \u03c4 > \u2016\u2206\u2016\u221e. In order to reveal the theoretical value of \u03c4 and its implication for various randomized reduction methods, we need to bound \u2016\u2206\u2016\u221e. We first provide a general analysis and then study its implication for various randomized reduction methods separately. The analysis is based on the following assumption, which essentially is indicated by Johnson-Lindenstrauss (JL)-type lemmas.\nAssumption 1 (A1). Let A(x) = Ax be a linear projection operator where A \u2208 Rm\u00d7d such that for any given x \u2208 Rd with a high probability 1\u2212 \u03b4, we have\u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622\n\u2223\u2223 \u2264 \u01ebA,\u03b4\u2016x\u201622 where \u01ebA,\u03b4 depends on m, \u03b4 and possibly d.\nWith this assumption, we have the following theorem regarding the upper bound of \u2016\u2206\u2016\u221e. Theorem 5. Suppose A \u2208 Rm\u00d7d satisfies Assumption A,\nthen with a high probability 1\u2212 2\u03b4 we have \u2016\u2206\u2016\u221e \u2264 R\u2016w\u2217\u20162\u01ebA,\u03b4/n where R = maxi \u2016xi\u20162. Proof. 1\n\u03bbn (X\u0302\u22a4X\u0302 \u2212X\u22a4X)\u03b1\u2217 =\n1\n\u03bbn (X\u22a4A\u22a4AX \u2212X\u22a4X)\u03b1\u2217\n= 1\n\u03bbn X\u22a4(A\u22a4A\u2212 I)X\u03b1\u2217 = X\u22a4(I \u2212A\u22a4A)w\u2217\nwhere we use the fact w\u2217 = \u2212 1\u03bbnX\u03b1\u2217. Then 1\n\u03bbn [(X\u0302\u22a4X\u0302 \u2212X\u22a4X)\u03b1\u2217]i = x\u22a4i (I \u2212A\u22a4A)w\u2217\nTherefore in order to bound \u2016\u2206\u2016\u221e, we need to bound x\u22a4i (I \u2212 A\u22a4A)w\u2217 for all i \u2208 [n]. We first bound for individual i and then apply the union bound. Let x\u0303i and w\u0303\u2217 be normalized version of xi and w\u2217, i.e., x\u0303i = xi/\u2016xi\u20162 and w\u0303\u2217 = w\u2217/\u2016w\u2217\u20162. Suppose Assumption A is satisfied, then with a probability 1\u2212 \u03b4,\nx\u0303\u22a4i A \u22a4Aw\u0303\u2217 \u2212 x\u0303\u22a4i w\u0303\u2217 = \u2016A(x\u0303i + w\u0303i)\u201622 \u2212 \u2016A(x\u0303i \u2212 w\u0303\u2217)\u201622 4\n\u2212 x\u0303\u22a4i w\u0303\u2217 \u2264 \u01ebA,\u03b4 2\n(\u2016x\u0303i\u201622 + \u2016w\u0303\u2217\u201622) \u2264 \u01ebA,\u03b4 Similarly with a probability 1\u2212 \u03b4,\nx\u0303\u22a4i A \u22a4Aw\u0303 \u2212 x\u0303\u22a4i w\u0303\u2217 = \u2016A(x\u0303i + w\u0303\u2217)\u201622 \u2212 \u2016A(x\u0303i \u2212 w\u0303\u2217)\u201622 4\n\u2212 x\u0303\u22a4i w\u0303\u2217 \u2265 \u2212 \u01ebA,\u03b4 2\n(\u2016x\u0303\u2217\u201622 + \u2016w\u0303\u2217\u201622) \u2265 \u2212\u01ebA,\u03b4 Therefore with a probability 1\u2212 2\u03b4, we have |x\u22a4i A\u22a4Aw\u2217 \u2212 x\u22a4i w\u2217| \u2264 \u2016xi\u20162\u2016w\u2217\u20162|x\u0303\u22a4i A\u22a4Aw\u0303\u2217 \u2212 x\u0303\u22a4w\u0303\u2217| \u2264 \u2016xi\u20162\u2016w\u2217\u20162\u01ebA,\u03b4 Then applying union bound, we complete the proof.\nNext, we discuss four classes of randomized reduction operators, namely random projection, randomized Hadamard transform, random hashing and random sampling, and study the corresponding \u01ebA,\u03b4 and their implications for the recovery error.\nRandom Projection. Random projection has been employed widely for dimension reduction. The projection operator A is usually sampled from sub-Gaussian distributions with mean 0 and variance 1/m, e.g., (i) Gaussian distribution: Aij \u223c N (0, 1/m), (ii) Rademacher distribution: Pr(Aij = \u00b11/ \u221a m) = 0.5, (iii) discrete distribution:\nPr(Aij = \u00b1 \u221a 3/m) = 1/6 and Pr(Aij = 0) = 2/3. The last two distributions for dimensionality reduction were proposed and analyzed in (Achlioptas, 2003). The following lemma is the general JL-type lemma for A with subGaussian entries, which reveals the value of \u01ebA,\u03b4 in Assumption A.\nLemma 1. (Nelson) Let A \u2208 Rm\u00d7d be a random matrix with subGaussian entries of mean 0 and variance 1/m . For any given x with a probability 1\u2212 \u03b4, we have\n\u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622 \u2223\u2223 \u2264 c\n\u221a log(1/\u03b4)\nm \u2016x\u201622\nwhere c is some small universal constant.\nFollowing above lemma, we have with a probability 1\u2212 \u03b4, \u2016\u2206\u2016\u221e = max\ni |x\u22a4i (I \u2212A\u22a4A)w\u2217|\n\u2264 cR \u221a log(n/\u03b4)\nm \u2016w\u2217\u20162.\nwhich essentially indicates that \u03c4 \u2265 2cR \u221a\nlog(n/\u03b4) m \u2016w\u2217\u20162.\nRandomized Hadamard Transform. Randomized Hadamard transform was introduced to speed-up random projection, reducing the computational time 2 of random projection from O(dm) to O(d log d) or even O(d logm). The projection matrix A is of the form A = PHD, where\n\u2022 D \u2208 Rd\u00d7d is a diagonal matrix with Dii = \u00b11 with equal probabilities. \u2022 H is the d \u00d7 d Hadamard matrix (assuming d is a power of 2), scaled by 1/ \u221a d. \u2022 P \u2208 Rm\u00d7d is typically a sparse matrix that facilities computing Px. Several choices of P are possible (Nelson; Ailon & Chazelle, 2009; Tropp, 2011). Below we provide a JL-type lemma for a randomized Hadamard transform with P \u2208 Rm\u00d7d that samples m coordinates from \u221a d mHDx with replacement.\nLemma 2. (Nelson) Let A = \u221a\nd mPHD \u2208 Rm\u00d7d be a\nrandomized Hadamard transform with P being a random sampling matrix. For any given x with a probability 1\u2212 \u03b4, we have\n\u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622 \u2223\u2223 \u2264 c\n\u221a log(1/\u03b4) log(d/\u03b4)\nm \u2016x\u201622\nwhere c is some small universal constant.\nRemark 6: Compared to random projection, there is an additional \u221a log(d/\u03b4) factor in \u01ebA,\u03b4. However, it can be removed by applying an additional random projection. In particular, if we let A = \u221a\nd mP \u2032PHD \u2208 Rm\u00d7d, where P \u2208 Rt\u00d7d is a random sampling matrix with t = m log(d/\u03b4) and P \u2032 \u2208 Rm\u00d7t is a random projection matrix that satisfies Lemma 1, then we have the same order of \u01ebA,\u03b4. Please refer to (Nelson) for more details.\nRandom Hashing. Another line of work to speed-up random projection is random hashing which makes the projection matrix A much sparser and takes advantage of the sparsity of feature vectors. It was introduced in (Shi et al., 2009b) for dimensionality reduction and later was improved to an unbiased version by (Weinberger et al., 2009) with some theoretical analysis. Dasgupta et al. (2010) provided a rigorous analysis of the unbiased random hashing. Recently, Kane & Nelson (2014) proposed two new random hashing algorithms with a slightly sparser random matrix A. Here we provide a JL-type lemma for\n2refers to the running time of computing Ax.\nthe random hashing algorithm in (Weinberger et al., 2009; Dasgupta et al., 2010). Let h : N \u2192 [m] denote a random hashing function, and \u03be = (\u03be1, . . . , \u03bed) denote a Rademacher random variable, i.e., \u03bei, i = 1, . . . , d are independent and \u03bei \u2208 {1,\u22121} with equal probabilities. The projection matrix A can be written as A = HD, where D \u2208 Rd\u00d7d is a diagonal matrix with Djj = \u03bej , and H \u2208 Rm\u00d7d with Hij = \u03b4i,h(j) 3. Under the random matrix A, the feature vector x \u2208 Rd is reduced to x\u0302 \u2208 Rm, where [x\u0302]i = \u2211 j:h(j)=i[x]j\u03bej . The following JL-type Lemma is a basic result from (Dasgupta et al., 2010) with a rephrasing.\nLemma 3. Let A = HD \u2208 Rm\u00d7d be a random hashing matrix. For any given vectorx \u2208 Rd such that \u2016x\u2016\u221e\u2016x\u20162 \u2264 1\u221a c , for \u03b4 < 0.1, with a probability 1\u2212 3\u03b4, we have \u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622 \u2223\u2223 \u2264 \u221a 12 log(1/\u03b4)\nm \u2016x\u201622\nwhere c = 8 \u221a m/3 log1/2(1/\u03b4) log2(m/\u03b4).\nRemark 7: Compared to random projection, there is an additional condition on the feature vector \u2016x\u2016\u221e \u2264 \u2016x\u20162\u221ac . However, it can be removed by applying an extra preconditioner P to x before applying the projection matrix A, i.e., x\u0302 = HDPx. Two preconditioners were discussed in (Dasgupta et al., 2010), with one corresponding to duplicating x c times and scaling it by 1/ \u221a c and another one given by P \u2208 Rd\u00d7d which consists of d/b diagonal blocks of b \u00d7 b randomized Hadamard matrix, where b = 6c log(3c/\u03b4). The running time of the reduction using the later preconditioner is O(d log c log log c).\nRandom Sampling. Last we discuss random sampling and compare with the aforementioned randomized reduction methods. In fact, the JL-type lemma for random sampling is implicit in the proof of Lemma 2. We make it explicit in the following lemma.\nLemma 4. Let A = \u221a\nd mP \u2208 Rm\u00d7d be a scaled random\nsampling matrix where P \u2208 Rm\u00d7d samples m coordinates with replacement. Then with a probability 1\u2212 \u03b4, we have\n\u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622 \u2223\u2223 \u2264 \u2016x\u2016\u221e\u2016x\u20162\n\u221a 3d log(1/\u03b4)\nm \u2016x\u201622\nRemark 8: Compared with other three randomized reduction methods, there is an additional \u2016x\u2016\u221e\u2016x\u20162 \u221a d factor in \u01ebA,\u03b4, which could result in a much larger \u01ebA,\u03b4 and consequentially a larger recovery error. That is why the randomized Hadamard transform was introduced to make this additional factor close to a constant.\n3\u03b4ij = 1 if i = j, and 0 otherwise."}, {"heading": "4.2. Bounding \u03c3s for non-smooth case", "text": "Another condition in Theorem 3 is to require \u03c316s \u2264 \u03c1\u221216s. Since \u03c1\u221216s is dependent on the data, we provide an upper bound of \u03c316s to further understand the condition. In the\nfollowing analysis, we assume \u01ebA,\u03b4 = O( \u221a log(1/\u03b4) m ). Recall the definition of \u03c3s: \u03c3s = sup\n\u03b1\u2208Kn,s |\u03b1\u22a4(X\u22a4X \u2212 X\u0302\u22a4X\u0302)\u03b1|. (15)\nWe provide a bound of \u03c3s below. The key idea is to use the convex relaxation of Kn,s. Define Sn,s = {\u03b1 \u2208 Rn : \u2016\u03b1\u20162 \u2264 1, \u2016\u03b1\u20160 \u2264 s}. It was shown in (Plan & Vershynin, 2011) that conv(Sn,s) \u2282 Kn,s \u2282 2conv(Sn,s), where conv(S) is the convex hull of the set S. It is not difficult to show that (see the supplement)\nmax \u03b1\u2208Kn,s\n|(X\u03b1)\u22a4(I \u2212A\u22a4A)(X\u03b1)|\n\u2264 4 max \u03b11,\u03b12\u2208Sn,s |(X\u03b11)\u22a4(I \u2212A\u22a4A)(X\u03b12)| Let u1 = X\u03b11 and u2 = X\u03b12. For any fixed \u03b11, \u03b12 \u2208 Sn,s, with a probability 1\u2212 \u03b4 we can have |(X\u03b11)\u22a4(I \u2212A\u22a4A)(X\u03b12)| = O ( \u03c1+s \u221a log(1/\u03b4)\nm\n)\nwhere we use max\n\u03b1\u2208Sn,s \u2016X\u03b1\u201622 \u2264 max \u03b1\u2208Kn,s \u2016X\u03b1\u201622 = \u03c1+s\nThen by using Lemma 3.3 in (Plan & Vershynin, 2011) about the entropy of Sn,s and the union bound, we can arrive at the following upper bound for \u03c3s.\nTheorem 6. With a probability 1\u2212 \u03b4, we have\n\u03c3s \u2264 O ( \u03c1+s \u221a (log(1/\u03b4) + s log(n/s))\nm\n)\nRemark 9: With above result, we can further understand the condition \u03c316s \u2264 \u03c1\u221216s, which amounts to\nO ( \u03c1+16s \u221a (log(1/\u03b4) + s log(n/s))\nm\n) \u2264 \u03c1\u221216s,\ni.e., m \u2265 \u2126(\u03ba216s(log(1/\u03b4) + s log(n/s))) where \u03ba16s = \u03c1+16s/\u03c1 \u2212 16s is the restricted condition number of the data matrix."}, {"heading": "4.3. Proof of Theorem 1", "text": "Let F\u0302 (\u03b1) be defined as\nF\u0302 (\u03b1) = 1\nn\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i) + 1 2\u03bbn2 \u03b1T X\u0302\u22a4X\u0302\u03b1+ \u03c4 n \u2016\u03b1\u20161\nand F (\u03b1) be defined as\nF (\u03b1) = 1\nn\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i) + 1 2\u03bbn2 \u03b1TX\u22a4X\u03b1\nSince \u03b1\u0303\u2217 = argmin F\u0302 (\u03b1) therefore for any g\u2217 \u2208 \u2202\u2016\u03b1\u2217\u20161 0 \u2265F\u0302 (\u03b1\u0303\u2217)\u2212 F\u0302 (\u03b1\u2217)\n\u2265(\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4 ( 1\nn \u2207\u2113\u2217(\u03b1\u2217) +\n1\n\u03bbn2 X\u0302\u22a4X\u0302\u03b1\u2217\n)\n+ \u03c4\nn (\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4g\u2217 +\n1\n2nL \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622\nwhere we used the strong convexity of \u2113\u2217i and its strong convexity modulus 1/L. Since \u03b1\u2217 = argminL(\u03b1),\n0 \u2265 (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 ( 1\nn \u2207\u2113\u2217(\u03b1\u2217) +\n1\n\u03bbn2 X\u22a4X\u03b1\u2217\n) (16)\nCombining the above two inequalities we have\n0 \u2265(\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4 ( 1\n\u03bbn2 (X\u0302\u22a4X\u0302 \u2212X\u22a4X)\u03b1\u2217\n)\n+ \u03c4\nn (\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4g\u2217 +\n1\n2nL \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622\nSince the above inequality holds for any g\u2217 \u2208 \u2202\u2016\u03b1\u2217\u20161, if we choose [g\u2217]i = sign([\u03b1\u0303\u2217]i), i \u2208 Sc, then we have \u03c4\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2265\u2212 \u2016\u2206\u2016\u221e\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20161 + \u03c4\u2016[\u03b1\u0303\u2217]Sc\u20161\n+ 1\n2L \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622 (17)\nThus (\u03c4 + \u2016\u2206\u2016\u221e)\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2265(\u03c4 \u2212 \u2016\u2206\u2016\u221e)\u2016[\u03b1\u0303\u2217]Sc\u20161\n+ 1\n2L \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622\nAssuming \u03c4 \u2265 2\u2016\u2206\u2016\u221e, we have \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622 \u2264 3\u03c4L\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 3\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161\n(18)\nTherefore, \u2016[\u03b1\u0303\u2217 \u2212 \u03b1\u2217]S\u201621 \u2264 s\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622 \u2264 3\u03c4Ls\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 leading to the result \u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2264 3\u03c4Ls. Combing this inequality with inequalities in (18) we have\n\u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 9\u03c4Ls, \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 3\u03c4L \u221a s."}, {"heading": "4.4. Proof of Theorem 3", "text": "Following the same analysis, we first notice that inequality (17) holds for L = \u221e, i.e., \u03c4\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2265\u2212 \u2016\u2206\u2016\u221e\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20161 + \u03c4\u2016[\u03b1\u0303\u2217]Sc\u20161 Therefore if \u03c4 \u2265 2\u2016\u2206\u2016\u221e, we have \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2264 3\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 As a result,\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20161 \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 \u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 + \u2016[\u03b1\u0303\u2217]Sc\u20161\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 4\u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2264 4\u221as\nBy the definition of Kn,s, we have \u03b1\u0303\u2217 \u2212 \u03b1\u2217\n\u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u20162 \u2208 Kn,16s.\nTo proceed the proof, there exists g\u0303\u2217 \u2208 \u2202|\u03b1\u0303\u2217|1 such that\n0 \u2265(\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4 ( 1\nn \u2207\u2113\u2217(\u03b1\u0303\u2217) +\n1\n\u03bbn2 X\u0302\u22a4X\u0302\u03b1\u0303\u2217\n)\n+ \u03c4\nn (\u03b1\u0303\u2217 \u2212 \u03b1\u2217)\u22a4g\u0303\u2217\nAdding the above inequality with (16), we have\n0 \u2265 (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 ( 1\nn \u2207\u2113\u2217(\u03b1\u2217)\u2212\n1 n \u2207\u2113\u2217(\u03b1\u0303\u2217)\n)\n+ (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 ( 1\n\u03bbn2 X\u22a4X\u03b1\u2217 \u2212\n1\n\u03bbn2 X\u0302\u22a4X\u0302\u03b1\u0303\u2217\n)\n+ \u03c4\nn \u2016[\u03b1\u0303\u2217]Sc\u20161 \u2212\n\u03c4 n \u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161\nBy convexity of \u2113\u2217 we have\n(\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 [ 1\nn \u2207\u2113\u2217(\u03b1\u2217)\u2212\n1 n \u2207\u2113\u2217(\u03b1\u0303\u2217)\n] \u2265 0\nThus, we have \u03c4 \u2016[\u03b1\u0303\u2217]S \u2212 [\u03b1\u2217]S\u20161 \u2265 \u03c4 \u2016[\u03b1\u0303\u2217]Sc\u20161 + (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 ( 1\n\u03bbn X\u22a4X \u2212 1 \u03bbn X\u0302\u22a4X\u0302\n) \u03b1\u2217\n\u2212 (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4 ( 1\n\u03bbn X\u22a4X \u2212 1 \u03bbn X\u0302\u22a4X\u0302\n) (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\n+ 1\n\u03bbn (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4X\u22a4X(\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\nSince (\u03b1\u2217 \u2212 \u03b1\u0303\u2217)\u22a4\u2206 \u2265 \u2212\u2016\u2206\u2016\u221e\u2016\u03b1\u2217 \u2212 \u03b1\u0303\u2217\u20161, and \u03c4 \u2265 2\u2016\u2206\u2016\u221e and by the definition of \u03c1\u2212s , \u03c3s, we have 3\u03c4\n2 \u2016[\u03b1\u0303\u2217 \u2212 \u03b1\u2217]S\u20161 \u2265\n\u03c4 2 \u2016[\u03b1\u0303\u2217]Sc\u20161\n+ \u03c1\u221216s \u2212 \u03c316s\n\u03bbn \u2016\u03b1\u0303\u2217 \u2212 \u03b1\u2217\u201622\nThen the conclusion follows the same analysis as before."}, {"heading": "5. Numerical Experiments", "text": "In this section, we provide a case study in support of DSRR and the theoretical analysis, and a demonstration of the application of DSRR to distributed optimization.\nA case study on text classification. We use the RCV1binary data (Lewis et al., 2004) to conduct a case study. The data contains 697, 641 documents and 47, 236 features. We use a splitting 677, 399/20, 242 for training and testing. The feature vectors were normalized such that the \u21132 norm is equal to 1. We only report the results using random hashing since it is the most efficient, while other randomized reduction methods (except for random sampling) have similar performance. For the loss function, we use both the squared hinge loss (smooth) and the hinge loss (nonsmooth). We aim to examine two questions related to our analysis and motivation (i) how does the value of \u03c4 affect the recovery error? (ii) how does the number of samples m\naffect the recovery error?\nWe vary the value of \u03c4 among 0, 0.1, 0.2, . . . , 0.9, the value of m among 1024, 2048, 4096, 8192, and the value of \u03bb among 0.001, 0.00001. Note that \u03c4 = 0 corresponds to the randomized reduction approach without the sparse regularizer. The results averaged over 5 random trials are shown in Figure 1 for the squared hinge loss and in Figure 2 for the hinge loss. We first analyze the results in Figure 1. We can observe that when \u03c4 increases the ratio of \u2016[\u03b1\u0303\u2217]Sc\u20161\u2016[\u03b1\u0303\u2217]S\u2212[\u03b1\u2217]S\u20161 decreases indicating that the magnitude of dual variables for the original non-support vectors decreases. This is intuitive and consistent with our motivation. The recovery error of the dual solution (middle) first decreases and then increases. This can be partially explained by the theoretical result in Theorem 1. When the value of \u03c4 becomes larger than a certain threshold making \u03c4 > \u2016\u2206\u2016\u221e hold, then Theorem 1 implies that a larger \u03c4 will lead to a larger error. On the other hand, when \u03c4 is less than the threshold, the dual recovery error will decrease as \u03c4 increases. In addition, the figures exhibit that the thresholds for larger m are smaller which is consistent with our analysis of \u2016\u2206\u2016\u221e = O( \u221a 1/m). The difference between \u03bb = 0.001 and \u03bb = 0.00001 is because that smaller \u03bb will lead to larger \u2016w\u2217\u20162. In terms of the hinge loss, we observe similar trends, however, the recovery is much more difficult than that for squared hinge loss especially when the value of \u03bb is small.\nAn application to distributed learning. Although in some cases the solution learned in the reduced space can provide sufficiently good performance, it usually performs worse than the optimal solution that solves the original problem and sometimes the performance gap between them can not be ignored as seen in following experiments. To address this issue, we combine the benefits of distributed learning and the proposed randomized reduction methods for solving big data problems. When data is too large and sits on multiple machines, distributed learning can be employed to solve the optimization problem. In distributed learning, individual machines iteratively solve sub-problems associated with the subset of data on them and communicate some global variables (e.g., the primal solution w \u2208 Rd) among them. When the dimensionality d is very large, the total communication cost could be very high. To reduce the total communication cost, we propose to first solve the reduced data problem and then use the found solution as the initial solution to the distributed learning for the original data.\nBelow, we demonstrate the effectiveness of DSRR for the recently proposed distributed stochastic dual coordinate ascent (DisDCA) algorithm (Jaggi et al., 2014; Yang, 2013). The procedure is (1) reduce original high-dimensional data to very low dimensional space on individual machines; (2)\nvs \u03c4 , \u2016\u03b1\u0303\u2217\u2212\u03b1\u2217\u20162\nvs \u03c4 , and \u2016w\u0303\u2217\u2212w\u2217\u20162\nvs\nuse DisDCA to solve the reduced problem; (3) use the optimal dual solution to the reduce problem as an initial solution to DisDCA for solving the original problem. We record the running time for randomized reduction in step 1 and optimization of the reduced problem in step 2, and the optimization of the original problem in step 3. We compare the performance of four methods (i) the DSRR method that uses the model of the reduced problem solved by DisDCA to make predictions, (ii) the method that uses the recovered model in the original space, referred to as DSRR-Rec; (iii) the method that uses the dual solution to the reduced problem as an initial solution of DisDCA and runs it for the original problem with k = 1 or 2 communications (the number of updates before each communication is set to the number of examples in each machine), referred to as DSRR-DisDCA-k; and (iv) the distributed method that directly solves the original problem by DisDCA. For DisDCA to solve the original problem, we stop running when its performance on the testing data does not improve. Two data sets are used, namely RCV1-binary, KDD 2010 Cup data. For KDD 2010 Cup data, we use the one available on LibSVM data website. The statistics of the two data sets are summarized in Table 1. The results averaged over 5 trials are shown in Figure 3, which exhibit that the performance of DSRR-DisDCA-1/2 is remarkable in the sense that it achieves almost the same performance of directly\ntraining on the original data (DisDCA) and uses much less training time. In addition, DSRR-DisDCA performs much better than DSRR and has small computational overhead."}, {"heading": "6. Conclusions", "text": "In this paper, we have proposed dual-sparse regularized randomized reduction methods for classification. We presented rigorous theoretical analysis of the proposed dualsparse randomized reduction methods in terms of recovery error under a mild condition that the optimal dual variable is (nearly) sparse for both smooth and non-smooth loss functions, and for various randomized reduction approaches. The numerical experiments validate our theoretical analysis and also demonstrate that the proposed reduction and recovery framework can benefit distributed optimization by providing a good initial solution."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Achlioptas", "Dimitris"], "venue": "Journal of Computer and System Sciences.,", "citeRegEx": "Achlioptas and Dimitris.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas and Dimitris.", "year": 2003}, {"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dudk", "Miroslav", "Langford", "John"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "The fast johnson\u2013 lindenstrauss transform and approximate nearest neighbors", "author": ["Ailon", "Nir", "Chazelle", "Bernard"], "venue": "SIAM J. Comput.,", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Directional variance adjustment: improving covariance estimates for highdimensional portfolio optimization", "author": ["Bartz", "Daniel", "Hatrick", "Kerr", "Hesse", "Christian W", "M\u00fcller", "Klaus-Robert", "Lemm", "Steven"], "venue": null, "citeRegEx": "Bartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bartz et al\\.", "year": 2011}, {"title": "Support vector machines and kernels for computational biology", "author": ["Ben-Hur", "Asa", "Ong", "Cheng Soon", "Sonnenburg", "S\u00f6ren", "Sch\u00f6lkopf", "Bernhard", "R\u00e4tsch", "Gunnar"], "venue": "PLoS Comput Biology,", "citeRegEx": "Ben.Hur et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ben.Hur et al\\.", "year": 2008}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Ya\u2019acov", "Tsybakov", "Alexandre B"], "venue": "ANNALS OF STATISTICS,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["Blum", "Avrim"], "venue": "In Proceedings of the 2005 International Conference on Subspace, Latent Structure and Feature Selection,", "citeRegEx": "Blum and Avrim.,? \\Q2005\\E", "shortCiteRegEx": "Blum and Avrim.", "year": 2005}, {"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Feature selection using support vector machines", "author": ["Brank", "Janez", "Grobelnik", "Marko", "Mili\u0107-Frayling", "Natasa", "D. Mladeni\u0107"], "venue": "In Proceedings of the International Conference on Data Mining Methods and Databases for Engineering,", "citeRegEx": "Brank et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brank et al\\.", "year": 2002}, {"title": "A sparse johnson\u2013lindenstrauss transform", "author": ["Dasgupta", "Anirban", "Kumar", "Ravi", "Sarl\u00f3s", "Tam\u00e1s"], "venue": "In Proceedings of the 42nd ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2010}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error cur matrix decompositions", "author": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan"], "venue": "SIAM Journal Matrix Analysis Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Faster least squares approximation", "author": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan", "Sarl\u00f3s", "Tam\u00e0s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Compressed Sensing: Theory and Applications. Compressed Sensing: Theory and Applications", "author": ["Eldar", "Yonina C", "Kutyniok", "Gitta"], "venue": null, "citeRegEx": "Eldar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Eldar et al\\.", "year": 2012}, {"title": "Variable screening in high-dimensional feature space", "author": ["Fan", "Jianqing"], "venue": "In Proceedings of the International Conference on Computing and Mission (ICCM),", "citeRegEx": "Fan and Jianqing.,? \\Q2007\\E", "shortCiteRegEx": "Fan and Jianqing.", "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["Goldberger", "Jacob", "Roweis", "Sam", "Hinton", "Geoffrey", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goldberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2005}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Weston", "Jason", "Barnhill", "Stephen", "Vapnik", "Vladimir"], "venue": "Machine Learning (ML),", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Jaggi", "Martin", "Smith", "Virginia", "Tak\u00e1c", "Terhorst", "Jonathan", "Krishnan", "Sanjay", "Hofmann", "Thomas", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Genetic risk prediction in complex disease", "author": ["Jostins", "Luke", "Barrett", "Jeffrey C"], "venue": "Human Molecular Genetics,", "citeRegEx": "Jostins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jostins et al\\.", "year": 2011}, {"title": "Sparser johnsonlindenstrauss transforms", "author": ["Kane", "Daniel M", "Nelson", "Jelani"], "venue": "Journal of the ACM,", "citeRegEx": "Kane et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kane et al\\.", "year": 2014}, {"title": "Improved risk prediction for crohn\u2019s disease with a multi-locus approach", "author": ["J. Kang", "Kugathasan S. Georges M. Zhao H", "J.H. Cho"], "venue": "Human Molecular Genetics,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["Li", "Mu", "Andersen", "David G", "Smola", "Alex J", "Yu", "Kai"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A statistical perspective on algorithmic leveraging", "author": ["Ma", "Ping", "Mahoney", "Michael W", "Yu", "Bin"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Randomized algorithms for matrices and data", "author": ["Mahoney", "Michael W"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney and W.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney and W.", "year": 2011}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["Mahoney", "Michael W", "Drineas", "Petros"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney et al\\.", "year": 2009}, {"title": "Learning to decode cognitive states from brain", "author": ["Mitchell", "Tom M", "Hutchinson", "Rebecca", "Niculescu", "Radu S", "Pereira", "Francisco", "Wang", "Xuerui", "Just", "Marcel", "Newman", "Sharlene"], "venue": "images. Machine Learning,", "citeRegEx": "Mitchell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2004}, {"title": "Random projections for support vector machines", "author": ["Paul", "Saurabh", "Boutsidis", "Christos", "Magdon-Ismail", "Malik", "Drineas", "Petros"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "One-bit compressed sensing by linear programming", "author": ["Plan", "Yaniv", "Vershynin", "Roman"], "venue": "CoRR, abs/1109.4299,", "citeRegEx": "Plan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Plan et al\\.", "year": 2011}, {"title": "RASE: recognition of alternatively spliced exons in c.elegans", "author": ["G. R\u00e4tsch", "S. Sonnenburg", "B. Sch\u00f6lkopf"], "venue": "Bioinformatics, 21:i369\u2013i377,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2005}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["S\u00e1nchez", "Jorge", "Perronnin", "Florent", "Mensink", "Thomas", "Verbeek", "Jakob J"], "venue": "International Journal of Computer Vision,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2013}, {"title": "Hash kernels for structured data", "author": ["Shi", "Qinfeng", "Petterson", "James", "Dror", "Gideon", "Langford", "John", "Smola", "Alex", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Is margin preserved after random projection", "author": ["Shi", "Qinfeng", "Shen", "Chunhua", "Hill", "Rhys", "van den Hengel", "Anton"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt", "author": ["Simianer", "Patrick", "Riezler", "Stefan", "Dyer", "Chris"], "venue": "In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Simianer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Simianer et al\\.", "year": 2012}, {"title": "Coffin: A computational framework for linear svms", "author": ["Sonnenburg", "Sren", "Franc", "Vojtech"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Sonnenburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2010}, {"title": "Accurate splice site prediction using support vector machines", "author": ["Sonnenburg", "Sren", "Schweikert", "Gabriele", "Philips", "Petra", "Behr", "Jonas", "Rtsch", "Gunnar"], "venue": "BMC Bioinformatics,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2007}, {"title": "Improved analysis of the subsampled randomized hadamard transform", "author": ["Tropp", "Joel A"], "venue": "Advances in Adaptive Data Analysis,", "citeRegEx": "Tropp and A.,? \\Q2011\\E", "shortCiteRegEx": "Tropp and A.", "year": 2011}, {"title": "Finding important genes from high-dimensional data: An appraisal of statistical tests and machine-learning approaches", "author": ["Wang", "Chamont", "Gevertz", "Jana", "Chen", "Chaur-Chin", "Auslender", "Leonardo"], "venue": "CoRR, abs/1205.6523,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian Q", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alexander J", "Attenberg", "Josh"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "A proximal-gradient homotopy method for the sparse least-squares problem", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Yang", "Tianbao"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yang and Tianbao.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Tianbao.", "year": 2013}, {"title": "Random projections for classification: A recovery approach", "author": ["Zhang", "Lijun", "Mahdavi", "Mehrdad", "Jin", "Rong", "Yang", "Tianbao", "Zhu", "Shenghuo"], "venue": "IEEE Transactions on Information Theory (IEEE TIT),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": ", bioinformatics, finance, computer vision, medical informatics) (S\u00e1nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems.", "startOffset": 65, "endOffset": 183}, {"referenceID": 26, "context": ", bioinformatics, finance, computer vision, medical informatics) (S\u00e1nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems.", "startOffset": 65, "endOffset": 183}, {"referenceID": 37, "context": ", bioinformatics, finance, computer vision, medical informatics) (S\u00e1nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems.", "startOffset": 65, "endOffset": 183}, {"referenceID": 33, "context": ", bioinformatics, finance, computer vision, medical informatics) (S\u00e1nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems.", "startOffset": 65, "endOffset": 183}, {"referenceID": 3, "context": ", bioinformatics, finance, computer vision, medical informatics) (S\u00e1nchez et al., 2013; Mitchell et al., 2004; Wang et al., 2012; Simianer et al., 2012; Bartz et al., 2011; Fan, 2007), it becomes critical to develop efficient and effective algorithms to solve big data machine learning problems.", "startOffset": 65, "endOffset": 183}, {"referenceID": 32, "context": "Randomized reduction methods for large-scale or high-dimensional data analytics have received a great deal of attention in recent years (Mahoney & Drineas, 2009; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Mahoney, 2011).", "startOffset": 136, "endOffset": 238}, {"referenceID": 27, "context": "Randomized reduction methods for large-scale or high-dimensional data analytics have received a great deal of attention in recent years (Mahoney & Drineas, 2009; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Mahoney, 2011).", "startOffset": 136, "endOffset": 238}, {"referenceID": 38, "context": "Randomized reduction methods for large-scale or high-dimensional data analytics have received a great deal of attention in recent years (Mahoney & Drineas, 2009; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Mahoney, 2011).", "startOffset": 136, "endOffset": 238}, {"referenceID": 11, "context": "While randomized instance reduction has been studied a lot for fast least square regression (Drineas et al., 2008; 2006; 2011; Ma et al., 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al.", "startOffset": 92, "endOffset": 143}, {"referenceID": 23, "context": "While randomized instance reduction has been studied a lot for fast least square regression (Drineas et al., 2008; 2006; 2011; Ma et al., 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al.", "startOffset": 92, "endOffset": 143}, {"referenceID": 32, "context": ", 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Shi et al., 2009a) (e.", "startOffset": 80, "endOffset": 173}, {"referenceID": 27, "context": ", 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Shi et al., 2009a) (e.", "startOffset": 80, "endOffset": 173}, {"referenceID": 38, "context": ", 2014), randomized feature reduction is more popular for linear classification (Blum, 2005; Shi et al., 2012; Paul et al., 2013; Weinberger et al., 2009; Shi et al., 2009a) (e.", "startOffset": 80, "endOffset": 173}, {"referenceID": 27, "context": ", generalization performance (Paul et al., 2013), preservation of margin (Blum, 2005; Balcan et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 32, "context": ", 2013), preservation of margin (Blum, 2005; Balcan et al., 2006; Shi et al., 2012) and the recovery error of the model (Zhang et al.", "startOffset": 32, "endOffset": 83}, {"referenceID": 41, "context": ", 2012) and the recovery error of the model (Zhang et al., 2014), these previous results reply on strong assumptions about the data.", "startOffset": 44, "endOffset": 64}, {"referenceID": 27, "context": "For example, both (Paul et al., 2013) and (Zhang et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 41, "context": ", 2013) and (Zhang et al., 2014) assume the data", "startOffset": 12, "endOffset": 32}, {"referenceID": 32, "context": "matrix is of low-rank, and (Blum, 2005; Balcan et al., 2006; Shi et al., 2012) make a assumption that all examples in the original space are separated with a positive margin (with a high probability).", "startOffset": 27, "endOffset": 78}, {"referenceID": 41, "context": "Another analysis in (Zhang et al., 2014) assumes the weight vector for classification is sparse.", "startOffset": 20, "endOffset": 40}, {"referenceID": 32, "context": "Compared with previous works (Blum, 2005; Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013), our theoretical analysis demands a mild assumption about the data and directly provides guarantee on a small recovery error of the obtained model, which is critical for subsequent analysis, e.", "startOffset": 29, "endOffset": 99}, {"referenceID": 27, "context": "Compared with previous works (Blum, 2005; Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013), our theoretical analysis demands a mild assumption about the data and directly provides guarantee on a small recovery error of the obtained model, which is critical for subsequent analysis, e.", "startOffset": 29, "endOffset": 99}, {"referenceID": 16, "context": ", feature selection (Guyon et al., 2002; Brank et al., 2002) and model interpretation (R\u00e4tsch et al.", "startOffset": 20, "endOffset": 60}, {"referenceID": 8, "context": ", feature selection (Guyon et al., 2002; Brank et al., 2002) and model interpretation (R\u00e4tsch et al.", "startOffset": 20, "endOffset": 60}, {"referenceID": 29, "context": ", 2002) and model interpretation (R\u00e4tsch et al., 2005; Sonnenburg & Franc, 2010; Rtsch et al., 2005; Sonnenburg et al., 2007; Ben-Hur et al., 2008).", "startOffset": 33, "endOffset": 147}, {"referenceID": 35, "context": ", 2002) and model interpretation (R\u00e4tsch et al., 2005; Sonnenburg & Franc, 2010; Rtsch et al., 2005; Sonnenburg et al., 2007; Ben-Hur et al., 2008).", "startOffset": 33, "endOffset": 147}, {"referenceID": 4, "context": ", 2002) and model interpretation (R\u00e4tsch et al., 2005; Sonnenburg & Franc, 2010; Rtsch et al., 2005; Sonnenburg et al., 2007; Ben-Hur et al., 2008).", "startOffset": 33, "endOffset": 147}, {"referenceID": 15, "context": "In addition, the recovery could also increase the predictive performance, in particular when there exists noise in the original features (Goldberger et al., 2005).", "startOffset": 137, "endOffset": 162}, {"referenceID": 41, "context": "Compared with (Zhang et al., 2014) that proposes to recover a linear model in the original feature space by dual recovery, i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 17, "context": "Distributed learning/optimization recently receives significant interest in solving big data problems (Jaggi et al., 2014; Li et al., 2014; Yang, 2013; Agarwal et al., 2011).", "startOffset": 102, "endOffset": 173}, {"referenceID": 22, "context": "Distributed learning/optimization recently receives significant interest in solving big data problems (Jaggi et al., 2014; Li et al., 2014; Yang, 2013; Agarwal et al., 2011).", "startOffset": 102, "endOffset": 173}, {"referenceID": 1, "context": "Distributed learning/optimization recently receives significant interest in solving big data problems (Jaggi et al., 2014; Li et al., 2014; Yang, 2013; Agarwal et al., 2011).", "startOffset": 102, "endOffset": 173}, {"referenceID": 17, "context": "In practice, we employ the recently developed distributed stochastic dual coordinate ascent algorithm (Jaggi et al., 2014; Yang, 2013), and observe that using the recovered solution as an initial solution we are able to attain almost the same performance with only one or two communications of high dimensional vectors among multiple machines.", "startOffset": 102, "endOffset": 134}, {"referenceID": 27, "context": ", 2012) and the preservation of minimum enclosing ball (Paul et al., 2013).", "startOffset": 55, "endOffset": 74}, {"referenceID": 27, "context": ", 2012) and the preservation of minimum enclosing ball (Paul et al., 2013). Zhang et al. (2014) proposed a dual recovery approach that constructs a recovered solution by \u0175\u2217 = \u2212 1 \u03bbn \u2211n i=1[\u03b1\u0302\u2217]ixi and proved the recovery error for random projection under the assumption of low-rank data matrix or sparse w\u2217.", "startOffset": 56, "endOffset": 96}, {"referenceID": 32, "context": "It can be understood that adding the l1 regularization in the reduced problem of SVM is equivalent to using a max-margin loss with a smaller margin, which is intuitive because examples become difficult to separate after dimensionality reduction and is consistent with several previous studies that the margin is reduced in the reduced feature space (Blum, 2005; Shi et al., 2012).", "startOffset": 349, "endOffset": 379}, {"referenceID": 41, "context": "In contrast, previous bounds (Zhang et al., 2014; Paul et al., 2013) depend on \u2016X\u22a4X \u2212 X\u0302\u22a4X\u0302\u20162, which requires the low rank assumption on X .", "startOffset": 29, "endOffset": 68}, {"referenceID": 27, "context": "In contrast, previous bounds (Zhang et al., 2014; Paul et al., 2013) depend on \u2016X\u22a4X \u2212 X\u0302\u22a4X\u0302\u20162, which requires the low rank assumption on X .", "startOffset": 29, "endOffset": 68}, {"referenceID": 41, "context": "O( \u221a log(n/\u03b4) m )\u2016w\u2217\u20162 with a high probability 1\u2212\u03b4, and thus the recovery error will be scaled as \u221a 1/m in terms of m the same order of recovery error as in (Zhang et al., 2014) that assumes low rank of the data matrix.", "startOffset": 157, "endOffset": 177}, {"referenceID": 5, "context": "To present the theoretical result on the non-smooth loss functions, we need to introduce restricted eigen-value conditions similar to those used in the sparse recovery analysis for LASSO (Bickel et al., 2009; Xiao & Zhang, 2013).", "startOffset": 187, "endOffset": 228}, {"referenceID": 38, "context": ", 2009b) for dimensionality reduction and later was improved to an unbiased version by (Weinberger et al., 2009) with some theoretical analysis.", "startOffset": 87, "endOffset": 112}, {"referenceID": 9, "context": "Dasgupta et al. (2010) provided a rigorous analysis of the unbiased random hashing.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Dasgupta et al. (2010) provided a rigorous analysis of the unbiased random hashing. Recently, Kane & Nelson (2014) proposed two new random hashing algorithms with a slightly sparser random matrix A.", "startOffset": 0, "endOffset": 115}, {"referenceID": 38, "context": "the random hashing algorithm in (Weinberger et al., 2009; Dasgupta et al., 2010).", "startOffset": 32, "endOffset": 80}, {"referenceID": 9, "context": "the random hashing algorithm in (Weinberger et al., 2009; Dasgupta et al., 2010).", "startOffset": 32, "endOffset": 80}, {"referenceID": 9, "context": "The following JL-type Lemma is a basic result from (Dasgupta et al., 2010) with a rephrasing.", "startOffset": 51, "endOffset": 74}, {"referenceID": 9, "context": "Two preconditioners were discussed in (Dasgupta et al., 2010), with one corresponding to duplicating x c times and scaling it by 1/ \u221a c and another one given by P \u2208 Rd\u00d7d which consists of d/b diagonal blocks of b \u00d7 b randomized Hadamard matrix, where b = 6c log(3c/\u03b4).", "startOffset": 38, "endOffset": 61}, {"referenceID": 21, "context": "We use the RCV1binary data (Lewis et al., 2004) to conduct a case study.", "startOffset": 27, "endOffset": 47}, {"referenceID": 17, "context": "Below, we demonstrate the effectiveness of DSRR for the recently proposed distributed stochastic dual coordinate ascent (DisDCA) algorithm (Jaggi et al., 2014; Yang, 2013).", "startOffset": 139, "endOffset": 171}], "year": 2017, "abstractText": "In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their applications in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse regularized randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data.", "creator": "LaTeX with hyperref package"}}}