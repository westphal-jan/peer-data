{"id": "1504.05477", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2015", "title": "Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition", "abstract": "we re - analyze simultaneous exponential power conversion iteration and the block lanczos methods, two classical digital iterative algorithms for the singular value decomposition ( svd ). we are interested in convergence bounds that * don't depend * * on properties of the input matrix ( e. g. many singular value gaps ).", "histories": [["v1", "Tue, 21 Apr 2015 15:48:44 GMT  (34kb)", "http://arxiv.org/abs/1504.05477v1", null], ["v2", "Sat, 6 Jun 2015 23:43:50 GMT  (50kb)", "http://arxiv.org/abs/1504.05477v2", null], ["v3", "Wed, 1 Jul 2015 03:55:11 GMT  (50kb)", "http://arxiv.org/abs/1504.05477v3", null], ["v4", "Fri, 30 Oct 2015 19:35:08 GMT  (54kb)", "http://arxiv.org/abs/1504.05477v4", "Neural Information Processing Systems 2015"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA", "authors": ["cameron musco", "christopher musco"], "accepted": true, "id": "1504.05477"}, "pdf": {"name": "1504.05477.pdf", "metadata": {"source": "CRF", "title": "Stronger Approximate Singular Value Decomposition via the Block Lanczos and Power Methods", "authors": ["Cameron Musco"], "emails": ["cnmusco@mit.edu", "cpmusco@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n05 47\n7v 1\n[ cs\n.D S]\n2 1\nSimultaneous Iteration is known to give a low rank approximation within (1 + \u01eb) of optimal for spectral norm error in O\u0303(1/\u01eb) iterations. We strengthen this result, proving that it finds approximate principal components very close in quality to those given by an exact SVD. Our work bridges a divide between classical analysis, which can give similar bounds but depends critically on singular value gaps, and more recent work, which only focuses on low rank approximation\nFurthermore, we extend our bounds to the Block Lanczos method, which we show obtains the same approximation guarantees in just O\u0303(1/ \u221a \u01eb) iterations, giving the fastest known algorithm for spectral norm low rank approximation and principal component approximation. Despite their popularity, Krylov subspace methods like Block Lanczos previously seemed more difficult to analyze and did not come with rigorous gap-independent guarantees.\nFinally, we give insight beyond the worst case, justifying why Simultaneous Power Iteration and Block Lanczos can run much faster in practice than predicted. We clarify how simple techniques can potentially accelerate both algorithms significantly."}, {"heading": "1 Introduction", "text": "Any matrix A \u2208 Rn\u00d7d with rank r can be written using a singular value decomposition (SVD) as A = U\u03a3V\u22a4. U \u2208 Rn\u00d7r and V \u2208 Rd\u00d7r have orthonormal columns (A\u2019s left and right singular vectors) and \u03a3 \u2208 Rr\u00d7r is a positive diagonal matrix containing A\u2019s singular values: \u03c31 \u2265 . . . \u2265 \u03c3r.\nAmong countless applications, the SVD is often used in machine learning and dimensionality reduction to provide an optimal low rank approximation to A. Specifically, the Eckart-YoungMirsky theorem guarantees that A\u2019s partial SVD can be used to construct a rank k approximation Ak such that both \u2016A \u2212Ak\u2016F and \u2016A \u2212Ak\u20162 are as small as possible. Ak is simply equal to A projected onto the space spanned by its top k singular vectors. That is, Ak = UkU \u22a4 k A.\nThe SVD is also used for principal component analysis (PCA). A\u2019s top singular vector u1 provides a top principal component, which describes the direction of greatest variance within A. The ith singular vector ui provides the i\nth principal component, which is the direction of greatest variance orthogonal to all higher principal components. In other words,\nu\u22a4i AA \u22a4ui = \u03c3 2 i = max\nx:\u2016x\u20162=1 x\u22a5uj\u2200j<i\nx\u22a4AA\u22a4x,\nwhere AA\u22a4 is the covariance matrix of A and \u03c3i is its ith singular value. Since classical methods for computing a partial SVD are expensive, substantial research has focused on fast, randomized approximation algorithms that seek nearly optimal low rank approximation and PCA [Sar06, MRT06, RST09, HMT11, CW13]. These algorithms have proven to be very fast in practice, especially for large data problems [HMST11, SKT14, IBM14, Tul14].\nIdeally, an approximate partial SVD algorithm will provide good theoretical guarantees for both k rank approximation and PCA. For a specified error \u01eb, we hope to return a rank k matrix Z with orthonormal columns z1, . . . , zk satisfying:\nFrobenius Norm Error: \u2016A\u2212 ZZ\u22a4A\u20162F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162F (1) Spectral Norm Error: \u2016A\u2212 ZZ\u22a4A\u201622 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162 (2) Per Vector Error: \u2200i, \u2223 \u2223\n\u2223 u\u22a4i AA \u22a4ui \u2212 z\u22a4i AA\u22a4zi \u2223 \u2223 \u2223 \u2264 \u01eb\u03c32k+1 (3)\nFurthermore, we seek runtime bounds that do not depend on properties of A. While substantial literature exists on the convergence of iterative and approximate SVD algorithms, nearly all runtime guarantees depend on the gaps between A\u2019s singular values and become useless when these gaps are small. This limitation is due to a focus on how quickly approximate singular vectors converge to the actual singular vectors of A. When two singular vectors have nearly identical values they are difficult to distinguish, so convergence inherently depends on singular value gaps.\nOnly recently has a shift in approximation goal allowed for algorithms that avoid this dependence, and thus run provably fast for any matrix. For low rank approximation and PCA, we are only concerned with finding a subspace that captures nearly as much information in A as its top singular vectors \u2013 distinguishing between two close singular values is overkill."}, {"heading": "1.1 Comparing Guarantees", "text": "The Frobenius norm guarantee (1) is well studied and there now exist algorithms achieving (1 + \u01eb) error in O(nnz(A)) time, plus lower order terms depending on \u01eb, where nnz(A) is the number of nonzero entries inA [CW13]. However, as highlighted in prior work [RST09, HMT11, SKT14] Frobenius\nnorm error is often insufficient, especially in applications to data analysis and machine learning. WhenA has a \u201cheavy-tail\u201d of singular values, as is common for noisy data, \u2016A\u2212Ak\u20162F = \u2211r i=k+1 \u03c3 2 i can be huge, potentially larger than even A\u2019s largest singular value. This renders (1) meaningless since Z does not need to align with any large singular vectors to obtain good multiplicative error.\nTo address this shortcoming, a number of papers [Sar06, Woo14, SKT14] suggest targeting spectral norm error (2). When looking for a rank k approximation, A\u2019s top k singular vectors are usually considered data and the remaining tail is noise. Intuitively, a spectral norm guarantee ensures that ZZ\u22a4A recovers A up to this noise threshold. A series of work [RST09, HMT11, WC14, BDMI14, Woo14] shows that classical Simultaneous Power Iteration, implemented with randomized start vectors, achieves (1 + \u01eb) spectral error.\nHowever, while intuitively stronger for many matrices, (1 + \u01eb) spectral error does not imply (1 + \u01eb) Frobenius error. Even more concerning, we can construct matrices for which neither low rank approximation guarantee implies any sort of accuracy in Z. Consider an A with its top k+1 squared singular values all equal to 10 followed by a tail of smaller singular values (e.g. 1000k at 1). \u2016A\u2212Ak\u201622 = 10 but in fact \u2016A\u2212ZZ\u22a4A\u201622 = 10 for any rank k matrix Z, leaving the spectral norm bound useless. At the same time, \u2016A \u2212 Ak\u20162F is large, so (1 + \u01eb) multiplicative Frobenius error is meaningless as well. For example, any Z obtains \u2016A\u2212 ZZ\u22a4A\u20162F \u2264 (1.01)\u2016A \u2212Ak\u20162F .\nWe address this concern by introducing a per vector guarantee (3) which requires each approximate singular vector z1, . . . , zk to capture nearly as much variance as the corresponding true singular vector. The error bound is very strong in that it depends on \u01eb\u03c32k+1 instead of multiplicatively like \u2223\n\u2223u\u22a4i AA \u22a4ui \u2212 z\u22a4i AA\u22a4zi \u2223 \u2223 \u2264 \u01eb\u03c32i , which would be weaker for A\u2019s larger singular vectors. While (3) is reminiscent of the bounds sought in classical numerical analysis [Saa80], we stress that it does not require each zi to converge to ui in the presence of small singular values gaps."}, {"heading": "1.2 Our Contributions", "text": "In this paper we re-analyze the decades old Simultaneous Power Iteration and the Block Lanczos methods, combined with simple randomized initializations, for guarantees (1), (2), and (3).\nAlgorithm 1 Simultaneous Iteration input: A \u2208 Rn\u00d7d, error \u01eb \u2208 (0, 1), rank k \u2264 n, d output: Z \u2208 Rn\u00d7k. 1: q = \u0398(log d/\u01eb), \u03a0 \u223c N (0, 1)d\u00d7k 2: Set K = ( AA\u22a4 )q A\u03a0\n3: Orthonormalize the columns of K to obtain Q \u2208 Rn\u00d7k. 4: Compute M = Q\u22a4AA\u22a4Q \u2208 Rk\u00d7k. 5: Set U\u0304k to the top k singular vectors of M. 6: return Z = QU\u0304k.\nAlgorithm 2 Block Lanczos input: A \u2208 Rn\u00d7d, error \u01eb \u2208 (0, 1), rank k \u2264 n, d output: Z \u2208 Rn\u00d7k. 1: q = \u0398(log d/ \u221a \u01eb), \u03a0 \u223c N (0, 1)d\u00d7k\n2: Set K = [ A\u03a0, (AA\u22a4)A\u03a0, ..., (AA\u22a4)qA\u03a0 ] 3: Orthonormalize the columns of K to obtain Q \u2208 Rn\u00d7qk. 4: Compute M = Q\u22a4AA\u22a4Q \u2208 Rqk\u00d7qk. 5: Set U\u0304k to the top k singular vectors of M. 6: return Z = QU\u0304k.\nTheorem 1 (Main Theorem). With high probability, Algorithms 1 and 2 find approximate singular vectors Z = [z1, . . . , zk] satisfying low rank approximation guarantees (1) and (2), and PCA guarantee (3). For error \u01eb, Algorithm 1 requires q = O(log d/\u01eb) iterations while Algorithm 2 requires q = O(log d/ \u221a \u01eb) iterations. Excluding lower order terms, both algorithms run in time O(nnz(A)kq).\nOur proof appears in parts as Theorems 6 and 7 (runtime) and Theorems 10, 11, and 12 (accuracy).\nWhile Simultaneous Iteration was known to achieve (2) [Woo14], surprisingly no bounds comparable to (1) and (3) are known. In fact, our analysis is the first to show that an approximation algorithm can achieve per vector guarantees like (3) in runtime independent of singular value gaps.\nPerhaps of greater interest is the fact that our analysis naturally applies to Krylov subspace methods like Block Lanczos. The theory for these methods is more limited, even though they have been proposed, discussed, and tested as a potential improvements over randomized power methods [RST09, HMST11, Hal12]. As highlighted in [SKT14],\n\u201cDespite decades of research on Lanczos methods, the theory for the randomized algorithms is more complete and provides strong guarantees of excellent accuracy, whether or not there exist any gaps between the singular values.\u201d\nTheorem 1 addresses this issue by giving the first gap independent bound for a Krylov subspace method. For guarantees (2) and (3), randomized Block Lanczos gives the fastest known algorithm, improving on the \u01eb dependence of Simultaneous Iteration (substantially for small \u01eb).\nFinally, in Section 5.3 we use our results to give a very simple alternative analysis that does depend on singular value gaps and can offer significantly faster convergence when A has decaying singular values. It is possible to take further advantage of this result by running Algorithms 1 and 2 with a \u03a0 that has > k columns, a very simple modification for accelerating either method."}, {"heading": "2 Background and Intuition", "text": "The goal of this section is to 1) provide background on algorithms for approximate singular value decomposition and 2) give intuition for Simultaneous Power Iteration and the Block Lanczos method, justifying why they can give strong gap-independent error guarantees."}, {"heading": "2.1 Frobenius Norm Error", "text": "Progress on algorithms for Frobenius norm error low rank approximation (1) has been most considerable. Work in this direction dates back to the strong rank-revealing QR factorizations of Gu and Eisenstat [GE96]. They give deterministic algorithms that run in approximately O(ndk) time, vs. O(ndmin(n, d))1 for a full SVD, and roughly achieve constant factor Frobenius norm error.\nRecently, randomization has been applied to achieve even faster algorithms with (1 + \u01eb) error. The paradigm is to compute a linear sketch of A into very few dimensions using either a column sampling matrix or Johnson-Lindenstrauss random projection matrix \u03a0. Typically A\u03a0 has at most poly(k/\u01eb) columns and can be used to quickly find Z using a number of methods [Sar06, CEM+15].\nAn\u00d7d \u00d7\u03a0d\u00d7poly(k/\u01eb) = A\u03a0n\u00d7poly(k/\u01eb)\nThis approach was developed and refined in several pioneering results, including [FKV04, DFK+04, DKM06, DV06] for column sampling, [PTRV00, MRT06] for random projection, and definitive work by Sarlo\u0301s [Sar06]. Recent work on sparse Johnson-Lindenstrauss type matrices [CW13, MM13, NN13] has brought the cost of Frobenius error low rank approximation down to\n1 By the Abel-Ruffini Theorem, an exact SVD is incomputable even with exact arithmetic \u2013 see [TB97]. Accordingly, all SVD algorithm are inherently iterative. Nevertheless, classical methods such as the QR algorithm obtain superlinear convergence rates for the low rank approximation and PCA problems and in any reasonable computing environment, can be taken to run in O(ndmin(n, d)) time.\nO(nnz(A) + n poly(k/\u01eb)) time, where the first term is the number of non-zero entries in A and is considered to dominate since typically k \u226a n, d.\nThe sketch-and-solve method is very popular, largely because the the computation of A\u03a0 is easily parallelized and, regardless, pass-efficient in a single processor setting. Furthermore, once a small compression of A is obtained, it can be manipulated in fast memory. This is not typically true of A itself, making it difficult to directly process the original matrix at all. Fast implementations of random projection methods are available through [ME11], [IBM14], and [SKT14]."}, {"heading": "2.2 Spectral Norm Error", "text": "Unfortunately, as discussed, Frobenius norm error is often insufficient when A has a heavy singular value tail. Furthermore, it seems an inherent limitation of sampling or random projection methods. The noise from A\u2019s lower r \u2212 k singular values corrupts A\u03a0, making it impossible to extract a good partial SVD if the sum of these singular values (i.e. \u2016A\u2212Ak\u20162F ) is too large. In other words, any error inherently depends on the size of this tail.\nThis raises a natural question \u2013 is there any way to reduce this noise down to the scale of \u03c3k+1 = \u2016A\u2212Ak\u20162 and thus achieve a spectral norm bound like (2)? The answer is yes, and in fact this is exactly the intuition behind the famed power method.\nSimultaneous Power Iteration (Algorithm 1), also known as subspace iteration, or orthogonal iteration, denoises A by working with the powered matrix Aq [Bau57, Rut70]. By the spectral theorem, Aq has exactly the same singular vectors as A, but its singular values are equal to the singular values of A raised to the qth power2. Powering spreads the values apart and accordingly, Aq\u2019s lower singular values are relatively much smaller than its top singular values (see Figure 1a). This effectively reduces the noise in our problem \u2013 if we use a sketching method to find a good Z for approximating Aq, even up to Frobenius error, Z will have to align very well with Aq\u2019s large singular vectors.\nSpecifically, q = O\u0303(1/\u01eb) is sufficient to increase any singular value \u2265 (1 + \u01eb)\u03c3k+1 to be significantly (i.e. poly(d) times) larger than any value \u2264 \u03c3k+1. So \u2016Aq \u2212 Aqk\u20162F is extremely small compared to the top singular values of Aq. In order to achieve even rough multiplicative approximation to this error, Z must align extremely well with every singular vector with value \u2265 (1+ \u01eb)\u03c3k+1. It thus provides an accurate basis for approximating A up to small spectral norm error.\nComputing Aq directly is costly, so Aq\u03a0 is computed iteratively. We start with a random \u03a0 and repeatedly multiply by A on the left. Since even a rough Frobenius norm approximation for Aq suffices, \u03a0 is often chosen to have just k columns. Each each iteration thus takes O(nnz(A)k) time. After Aq\u03a0 is computed, Z can simply be set to a basis for its column span. Note that per vector guarantees will require a more careful choice of this basis.\nTo the best of our knowledge, this approach to analyzing Simultaneous Iteration without dependence on singular value gaps began with [RST09]. The technique was popularized in [HMT11] and its analysis improved in [WC14] and [BDMI14]. [Woo14] gives the first bound that directly achieves (2), showing that O(log d/\u01eb) power iterations is sufficient for (1 + \u01eb) error. All of these papers rely on an improved understanding of the benefits of starting with a randomized \u03a0, which has developed from work on the sketch-and-solve paradigm.\n2 For nonsymmetric matrices, we will work with (AA\u22a4)qA."}, {"heading": "2.3 Beating Simultaneous Iteration with Lanczos", "text": "Numerous papers hint at the possibility of beating Simultaneous Iteration with the Block Lanczos method [CD74, GU77, GLO81], a well studied variant of Lanczos iteration [Lan50], which is the canonical Krylov subspace method for large singular value problems. In particular, [RST09], [HMST11] and [Hal12] suggest and experimentally confirm the potential of randomized Block Lanczos (Algorithm 2) for beating Simultaneous Iteration for low rank approximation. [ME11] also notes the difficulty of beating state-of-the-art Lanczos implementations [Lar01, Lar05] with Simultaneous Iteration.\nThe intuition behind Block Lanczos matches that of many accelerated iterative methods. Simply put, there are better polynomials than Aq for denoising tail singular values. In particular, we can use lower degree polynomials, allowing us to compute fewer powers of A and thus leading to an algorithm with fewer iterations. For example, an appropriately shifted \u221a q degree Chebyshev polynomial can push the tail of A nearly as close to zero as Aq, even if the long run growth of the polynomial is much lower (see Figure 1b). Specifically, q = O\u0303(1/ \u221a \u01eb) will increase any singular value \u2265 (1 + \u01eb)\u03c3k+1 to be significantly larger than any singular value below \u03c3k+1 \u2013 enough to achieve near optimal spectral norm error using a sketching method.\nBlock Lanczos takes advantage of such polynomials by working with the block Krylov subspace,\nK = [ \u03a0 A\u03a0 A2\u03a0 A3\u03a0 . . . A \u221a q\u03a0 ] ,\nfrom which we can construct p\u221aq(A)\u03a0 for any polynomial p\u221aq(\u00b7) of degree \u221a q. Since an effective polynomial for denoising A must be scaled and shifted based on the value of \u03c3k+1, we cannot easily compute p\u221aq(A)\u03a0 directly. Instead, we argue that the best k rank approximation to A lying in the span of K at least matches the approximation achieved by projecting onto the span of p\u221aq(A)\u03a0. Finding this best approximation will therefore give a nearly optimal low rank approximation to A.\nUnfortunately, there\u2019s a catch. Perhaps surprisingly, it is not clear how to efficiently compute the best spectral norm error low rank approximation to A lying in a specific subspace (e.g. K\u2019s span)\n[BDMI14, SR10]. This challenge precludes an analysis of Krylov methods parallel to the recent work on simultaneous power iteration. Nevertheless, we show that computing the best Frobenius error low rank approximation in the span of K, exactly the post-processing step taken by classic Block Lanczos, will give a good enough spectral norm approximation for achieving (1 + \u01eb) error."}, {"heading": "2.4 Per Vector Error", "text": "Achieving the per vector guarantee of (3) requires a more nuanced understanding of how Simultaneous Iteration and Block Lanczos denoise the spectrum ofA. The analysis for spectral norm low rank approximation relies on the fact that Aq and p\u221aq(A) blow up any singular value \u2265 (1 + \u01eb)\u03c3k+1 to much larger than any singular value \u2264 \u03c3k+1. This ensures that the Z outputted by both algorithms aligns very well with the singular vectors corresponding to these large singular values.\nIf \u03c3k \u2265 (1 + \u01eb)\u03c3k+1, then Z aligns well with all top k singular vectors of A and we get good Frobenius norm error and the per vector guarantee (3). Unfortunately, when there is a small gap between \u03c3k and \u03c3k+1, Z could miss intermediate singular vectors whose values lie between \u03c3k+1 and (1+ \u01eb)\u03c3k+1. This is the case where gap dependent guarantees of classical analysis break down.\nNevertheless, we can argue that Aq or, for Block Lanczos, another \u221a q-degree polynomial in our Krylov subspace, significantly separates singular values > \u03c3k+1 from those < (1 \u2212 \u01eb)\u03c3k+1. Thus, each column of Z will align with A at least nearly as well as uk+1. There may be a large subspace of singular vectors with values in the intermediate range [(1\u2212 \u01eb)\u03c3k+1, (1+ \u01eb)\u03c3k+1]. Our polynomial cannot spread apart these values significantly, so we cannot characterize how Z aligns with this space. However, as long as it avoids singular values below this range, we can guarantee (3).\nFor Frobenius norm low rank approximation, we can argue that the degree to which Z falls outside of the space spanned by the top k singular vectors depends on the number of intermediate singular values between \u03c3k+1 and (1\u2212 \u01eb)\u03c3k+1. These are the singular values that may be \u2018swapped in\u2019 for the true top k singular values. Since their weight counts towards A\u2019s tail, we can show that the total loss compared to optimal is at worst \u01eb\u2016A\u2212Ak\u20162F ."}, {"heading": "3 Preliminaries", "text": "Before proceeding to the full technical analysis, we overview required results from linear algebra, polynomial approximation, and randomized low rank approximation."}, {"heading": "3.1 Singular Value Decomposition and Low Rank Approximation", "text": "As mentioned, the singular value decomposition can be used to write any A \u2208 Rn\u00d7d as A = U\u03a3V\u22a4, where U \u2208 Rn\u00d7r and V \u2208 Rd\u00d7r have orthonormal columns and \u03a3 \u2208 Rr\u00d7r is a positive diagonal matrix containing the singular values of A: \u03c31 \u2265 \u03c32 \u2265 . . . \u2265 \u03c3r. The singular value decomposition exists for all matrices. The pseudoinverse of A is given by A+ = V\u03a3\u22121U\u22a4. Additionally, for any polynomial p(x), we define p(A) = Up(\u03a3)V\u22a4. Note that, since singular values are always take to be non-negative, p(A)\u2019s singular values are given by |p(\u03a3)|.\nLet \u03a3k be \u03a3 with all but its largest k singular values zeroed out. Let Uk and Vk be U and V with all but their first k columns zeroed out. For any k, Ak = U\u03a3kV\n\u22a4 = Uk\u03a3kV\u22a4k is the closest rank k approximation to A for any unitarily invariant norm, including the Frobenius norm and spectral norm [Mir60]. The squared Frobenius norm is given by \u2016A\u20162F = \u2211 i,j A 2 i,j = tr(AA \u22a4) =\n\u2211 i \u03c3 2 i . The spectral norm is given by \u2016A\u20162 = \u03c31.\n\u2016A\u2212Ak\u2016F = min B|rank(B)=k \u2016A\u2212B\u2016F and \u2016A\u2212Ak\u20162 = min B|rank(B)=k \u2016A\u2212B\u20162.\nWe often work with the remainder matrixA\u2212Ak and label itAr\\k. Its singular value decomposition is given by Ar\\k = Ur\\k\u03a3r\\kV \u22a4 r\\k where Ur\\k, \u03a3r\\k, and V \u22a4 r\\k have their first k columns zeroed.\nWhile the SVD gives a globally optimal rank k approximation for A, both Simultaneous Iteration and Block Lanczos will return the best k rank approximation falling within some fixed subspace spanned by a basis Q (with rank \u2265 k). For the Frobenius norm, this simply requires projecting A to Q and taking the best rank k approximation of the resulting matrix using an SVD. Lemma 2 (Lemma 4.1 of [Woo14]). Given A \u2208 Rn\u00d7d and Q \u2208 Rm\u00d7n with orthonormal columns,\n\u2016A\u2212 ( QQ\u22a4A )\nk \u2016F = \u2016A\u2212Q\n( Q\u22a4A )\nk \u2016F = min C|rank(C)=k \u2016A\u2212QC\u2016F .\nThis low rank approximation can be obtained using an SVD (equivalently, eigendecomposition) of the m\u00d7m matrix M = Q\u22a4 ( AA\u22a4 )\nQ. Specifically, letting M = U\u0304\u03a3\u03042U\u0304\u22a4, then: (\nQU\u0304k ) ( QU\u0304k )\u22a4 A = Q ( Q\u22a4A )\nk .\nProof. The first fact is well known in the literature and is given as Lemma 4.1 of [Woo14]. The second fact just follows from noting that, if the SVD of Q\u22a4A is given by Q\u22a4A = U\u0304\u03a3\u0304V\u0304\u22a4 then M = Q\u22a4 ( AA\u22a4 ) Q = U\u0304\u03a3\u03042U\u0304\u22a4. So Q ( Q\u22a4A )\nk = QU\u0304k\u03a3\u0304kV\u0304 \u22a4 k = Q ( U\u0304kU\u0304 \u22a4 k ) U\u0304\u03a3\u0304V\u0304\u22a4 = QU\u0304kU\u0304\u22a4k Q \u22a4A.\nNote that QU\u0304k has orthonormal columns since U\u0304 \u22a4 k Q \u22a4QU\u0304k = U\u0304\u22a4k IU\u0304k = Ik.\nIn general, this rank k approximation does not give the best spectral norm approximation to A falling within Q [BDMI14]. A closed form solution for the best spectral norm approximation can be obtained using the results of [SR10], which are related to Parrott\u2019s theorem. However we do not know an efficient way to compute this solution without essentially performing an SVD of A. It is simple to show at least that, given a rank k basis, the optimal spectral norm approximation for A spanned by that basis is obtained by projecting A to the basis: Lemma 3 (Lemma 4.14 of [Woo14]). For A \u2208 Rn\u00d7d and Q \u2208 Rn\u00d7k with orthonormal columns, \u2016A\u2212QQ\u22a4A\u20162 = min\nC\n\u2016A\u2212QC\u20162."}, {"heading": "3.2 Other Linear Algebra Tools", "text": "Throughout this paper we will use span(M) to denote the column span of the matrix M and will say that a matrix Q is an orthonormal basis for the column span of M if Q has orthonormal columns and QQ\u22a4M = M. That is, projecting the columns of M to Q fully recovers those columns. QQ\u22a4 is the orthogonal projection matrix onto the span of Q. (QQ\u22a4)(QQ\u22a4) = QIQ\u22a4 = QQ\u22a4. If two matrices M and N have the same dimensions and MN\u22a4 = 0 then \u2016M + N\u20162F = \u2016M\u20162F + \u2016N\u20162F . This matrix Pythagorean theorem follows from the fact that \u2016M+N\u20162F = tr((M+N)(M+N)\u22a4). As an example, note that for any orthogonal projection QQ\u22a4A, A\u22a4(I \u2212 QQ\u22a4)QQ\u22a4A = 0 so:\n\u2016A\u2212QQ\u22a4A\u20162F = \u2016A\u20162F \u2212 \u2016QQ\u22a4A\u20162F . This implies for example, that since Ak = UkU \u22a4 k A minimizes \u2016A \u2212 Ak\u20162F over all rank k\nmatrices, UkU \u22a4 k A maximizes \u2016UkU\u22a4k A\u20162F over all rank k orthogonal projections."}, {"heading": "3.3 Randomized Low Rank Approximation", "text": "As mentioned, our proofs build on well known sketch-based algorithms for low rank approximation with Frobenius norm error. A short proof of the following Lemma is in Appendix A:\nLemma 4 (Frobenius Norm Low Rank Approximation). Take any A \u2208 Rn\u00d7d and \u03a0 \u2208 Rd\u00d7k where the entries of \u03a0 are independent Gaussians drawn from N (0, 1). If we let Z be an orthonormal basis for span (A\u03a0), then with probability at least 99/100, for some fixed constant c,\n\u2016A\u2212 ZZ\u22a4A\u20162F \u2264 c \u00b7 dk\u2016A\u2212Ak\u20162F ."}, {"heading": "3.4 Chebyshev Polynomials", "text": "As outlined in Section 2.3, our proof also requires polynomials to more effectively denoise the tail of A. As is standard for Krylov subspace methods, we use a variation on the Chebyshev polynomials. The proof of the following Lemma is relegated to Appendix A.\nLemma 5 (Chebyshev Minimizing Polynomial). Given a specified value \u03b1 > 0, gap \u03b3 \u2208 (0, 1], and degree q \u2265 1, there exists a degree q polynomial p(x) such that:\n1. p((1 + \u03b3)\u03b1) = (1 + \u03b3)\u03b1\n2. p(x) \u2265 x for all x \u2265 (1 + \u03b3)\u03b1\n3. p(x) \u2264 \u03b1 2q \u221a \u03b3\u22121 for all x \u2208 [0, \u03b1]"}, {"heading": "4 Implementation and Runtimes", "text": "In this section we briefly discuss runtime and implementation considerations for Algorithms 1 and 2, our randomized variants of Simultaneous Power Iteration and the Block Lanczos methods."}, {"heading": "4.1 Simulatenous Iteration", "text": "Algorithm 1 can be modified in a number of ways. \u03a0 can be replaced by a random sign matrix, or any matrix achieving the guarantee of Lemma 4. \u03a0 may also be chosen with p > k columns. We will discuss in detail how this approach can give improved accuracy in Section 5.3.\nIn our implementation we set Z = QU\u0304k. This ensures that, for all l \u2264 k, Zl gives the best rank l Frobenius norm approximation to A within the span of K (See Lemma 2). This is necessary for achieving per vector guarantees for approximate PCA. However, if we are only interested in computing a near optimal low rank approximation, we can simply set Z = Q. Projecting A to QU\u0304k is equivalent to projecting to Q as these two matrices have the same spans.\nTheorem 6 (Simultaneous Iteration Runtime). Algorithm 1 runs in time\nO(nnz(A)k log d/\u01eb+ nk2).\nProof. Computing K requires first multiplying A by \u03a0, which takes O(nnz(A)k) time. Computing (\nAA\u22a4 )i A\u03a0 given ( AA\u22a4 )i\u22121\nA\u03a0 then takes O(nnz(A)k) time to first multiply our (n\u00d7k) matrix by A\u22a4 and then by A. This gives a total runtime of O(nnz(A)kq) for computing K.\nFinding Q via Gram-Schmidt orthogonalization or Householder reflections takes O(nk2) time. Computing M by multiplying from left to right requires O(nnz(A)k + nk2) time. M\u2019s SVD then requires O(k3) time using classical techniques (e.g. the QR algorithm). Finally, multiplying U\u0304k by Q takes time O(nk2). Since we set q = \u0398(log d/\u01eb), our total runtime is O (\nnnz(A)k log d\u01eb + nk 2 ) ."}, {"heading": "4.2 Block Lanczos", "text": "As with Simultaneous Iteration, we can replace \u03a0 with any matrix achieving the guarantee of Lemma 4 and can use p > k columns to improve accuracy (see Section 5.3). Additionally, Q can be computed in a number of ways. In the traditional Block Lanczos algorithm, one starts by computing an orthonormal basis for A\u03a0, the first block in the Krylov subspace. Bases for subsequent blocks are computed from previous blocks using a three term recurrence that ensures Q\u22a4AA\u22a4Q is block tridiagonal, with k \u00d7 k sized blocks [GU77]. This technique can be useful if qk is large, since it is faster to compute the top singular vectors of a block tridiagonal matrix. However, computing Q using a recurrence can introduce a number of stability issues, and additional steps may be required to ensure that the matrix remains orthogonal [GVL96].\nAn alternative is to compute K explicitly and then compute Q using a QR decomposition. This method is used in [RST09] and [HMST11]. It does not guarantee that Q\u22a4AA\u22a4Q is block tridiagonal, but helps avoid a number of stability issues. Furthermore, if qk is small, taking the SVD of Q\u22a4AA\u22a4Q will still be fast and typically dominated by the cost of computing K.\nTheorem 7 (Block Lanczos Runtime). Algorithm 2 runs in time\nO\n(\nnnz(A) k log d\u221a\n\u01eb + n\nk2 log2 d\n\u01eb +\nk3 log3 d\n\u01eb3/2\n)\n.\nProof. Computing K requires O(nnz(A)kq) time just like computing K for Simultaneous Iteration (see Theorem 6). The remaining steps are analogous to those in Simultaneous Iteration except somewhat more costly as we work an k \u00b7q dimensional rather than k dimensional subspace. Finding Q takes O(n(kq)2) time. Computing M take O(nnz(A)(kq) + n(kq)2) time and its SVD then requires O((kq)3) time. Finally, multiplying U\u0304k by Q takes time O(nk(kq)). Plugging in q = \u0398(log d/ \u221a \u01eb) gives the claimed runtime."}, {"heading": "5 Error Bounds", "text": "We now prove that both Algorithms 1 and 2 return a basis Z that gives relative error Frobenius (1) and spectral norm (2) low rank approximation error as well as the per vector guarantees (3)."}, {"heading": "5.1 Main Approximation Lemma", "text": "We first prove a general approximation lemma, which gives three guarantees formalizing the intuition given in Section 2. All other proofs follow nearly immediately from this lemma.\nFor simplicity we assume throughout that k \u2264 r \u2264 n, d. However, if k is greater than r = rank(A) it can be seen that both algorithms still return a basis satisfying the proven guarantees. We start with a definition:\nDefinition 8. For a given matrix Z \u2208 Rn\u00d7k with orthonormal columns, letting Zl \u2208 Rn\u00d7l be the first l columns of Z, we define the error function:\nE(Zl,A) = \u2016Al\u20162F \u2212 \u2016ZlZ\u22a4l A\u20162F = \u2016A\u2212 ZlZ\u22a4l A\u20162F \u2212 \u2016A\u2212Al\u20162F\nRecall that Al is the best rank l approximation to A. This error function measures how well ZlZ \u22a4 l A approximates A in comparison to the optimal.\nLemma 9 (Main Approximation Lemma). Let m be the number of singular values \u03c3i of A with \u03c3i \u2265 (1+\u01eb/2)\u03c3k+1. Let w be the number of singular values with 11+\u01eb/2\u03c3k \u2264 \u03c3i < \u03c3k. With probability 99/100 Algorithms 1 and 2 return Z satisfying:\n1. \u2200l \u2264 m, E(Zl,A) \u2264 (\u01eb/2) \u00b7 \u03c32k+1.\n2. \u2200l \u2264 k, E(Zl,A) \u2264 E(Zl\u22121,A) + 3\u01eb \u00b7 \u03c32k+1.\n3. \u2200l \u2264 k, E(Zl,A) \u2264 (w + 1) \u00b7 3\u01eb \u00b7 \u03c32k+1.\nProperty 1 captures the intuition given in Section 2.2. Both algorithms return Z with Zl equal to the best Frobenius norm low rank approximation in span(K). Since \u03c31 \u2265 . . . \u2265 \u03c3m \u2265 (1+ \u01eb/2)\u03c3k+1 and our polynomials separate any values above this threshold from anything below \u03c3k+1, Z must align very well with A\u2019s top m singular vectors. Thus E(Zl,A) is very small for all l \u2264 m.\nProperty 2 captures the intuition of Section 2.4 \u2013 outside of the largest m singular values, Z still performs well. We may fail to distinguish between vectors with values between 11+\u01eb/2\u03c3k and (1 + \u01eb/2)\u03c3k+1. However, aligning with the smaller vectors in this range rather than the larger vectors can incur a cost of at most O(\u01eb)\u03c32k+1. Since every column of Z outside of the first m may incur such a cost, there is a linear accumulation as characterized by Property 2.\nFinally, Property 3 captures the intuition that the total error in Z is bounded by the number of singular values falling in the range 11+\u01eb/2\u03c3k \u2264 \u03c3i < \u03c3k. This is the total number of singular vectors that aren\u2019t necessarily separated from and can thus be \u2018swapped in\u2019 for any of the (k \u2212 m) true top vectors with singular value < (1 + \u01eb/2)\u03c3k+1. Property 3 is critical in achieving near optimal Frobenius norm low rank approximation.\nProof. Proof of Property 1\nAssumem \u2265 1. If m = 0 then Property 1 trivially holds. We will prove the statement for Algorithm 2, since this is the more complex case, and then explain how the proof extends to Algorithm 1.\nLet p1 be the polynomial from Lemma 5 with \u03b1 = \u03c3k+1, \u03b3 = \u01eb/2, and q \u2265 c log(d/\u01eb)/ \u221a \u01eb for\nsome fixed constant c. We can assume 1/\u01eb = O(poly d) and thus q = O(log d/ \u221a \u01eb). Otherwise our Krylov subspace would have as many columns as A and we may as well use a classical algorithm to compute A\u2019s partial SVD directly. Let Y1 \u2208 Rn\u00d7k be an orthonormal basis for the span of p1(A)\u03a0. Recall that we defined p1(A) = Up1(\u03a3)V\n\u22a4. As long as we choose q to be odd, by the recursive definition of the Chebyshev polynomials, p1(A) only contains odd powers of A. Any odd power i can be evaluted as ( AA\u22a4 )(i\u22121)/2\nA. Accordingly, p1(A)\u03a0 and thus Y1 have columns falling within the span of the Krylov subspace from Algorithm 2 (and hence its column basis Q).\nBy Lemma 4 we have with probability 99/100:\n\u2016p1(A)\u2212Y1Y\u22a41 p1(A)\u20162F \u2264 cdk\u2016p1(A)\u2212 p1(A)k\u20162F . (4)\nFurthermore, one possible rank k approximation of p1(A) is p1(Ak). By the optimality of p1(A)k,\n\u2016p1(A)\u2212 p1(A)k\u20162F \u2264 \u2016p1(A)\u2212 p1(Ak)\u20162F \u2264 d \u2211\ni=k+1\np1(\u03c3i) 2 \u2264 d \u00b7\n(\n\u03c32k+1\n22q \u221a \u01eb/2\u22122\n)\n\u2264 O ( \u01eb\n2d2 \u03c32k+1\n)\n.\nThe last inequalities follow from setting q = \u0398(log(d/\u01eb)/ \u221a \u01eb) and from the fact that \u03c3i \u2264 \u03c3k+1 =\n\u03b1 for all i \u2265 k + 1 and thus by property 3 of Lemma 5, p1(\u03c3i) \u2264 \u03c3k+1 2q \u221a \u01eb/2\u22121 . Noting that k \u2264 d, we can plug this bound into (4) to get\n\u2016p1(A)\u2212Y1Y\u22a41 p1(A)\u20162F \u2264 \u01eb\n2 \u03c32k+1. (5)\nApplying the Pythagorean theorem and the invariance of the Frobenius norm under rotation gives\n\u2016p1(\u03a3)\u20162F \u2212 \u01eb\u03c32k+1 2 \u2264 \u2016Y1Y\u22a41 Up1(\u03a3)\u20162F .\nY1 falls within A\u2019s column span, and therefore U\u2019s column span. So we can write Y1 = UC for some C \u2208 Rr\u00d7k. Since Y1 and U have orthonormal columns, so must C. We can now write\n\u2016p1(\u03a3)\u20162F \u2212 \u01eb\u03c32k+1 2 \u2264 \u2016UCC\u22a4U\u22a4Up1(\u03a3)\u20162F = \u2016UCC\u22a4p1(\u03a3)\u20162F = \u2016C\u22a4p1(\u03a3)\u20162F .\nLetting ci be the i th row of C, expanding out these norms gives\nr \u2211\ni=1\np1(\u03c3i) 2 \u2212 \u01eb\u03c3\n2 k+1 2 \u2264 r \u2211\ni=1\n\u2016ci\u201622p1(\u03c3i)2. (6)\nSince C\u2019s columns are orthonormal, its rows all have norms upper bounded by 1. So \u2016ci\u201622p1(\u03c3i)2 \u2264 p1(\u03c3i) 2 for all i. So for all l \u2264 r, (6) gives us\nl \u2211\ni=1\n(1\u2212 \u2016ci\u201622)p1(\u03c3i)2 \u2264 r \u2211\ni=1\n(1\u2212 \u2016ci\u201622)p1(\u03c3i)2 \u2264 \u01eb\u03c32k+1 2 .\nRecall that m is the number of singular values with \u03c3i \u2265 (1 + \u01eb/2)\u03c3k+1. By Property 2 of Lemma 5, for all i \u2264 m we have \u03c3i \u2264 p1(\u03c3i). This gives, for all l \u2264 m:\nl \u2211\ni=1\n(1\u2212 \u2016ci\u201622)\u03c32i \u2264 \u01eb\u03c32k+1 2 and so\nl \u2211\ni=1\n\u03c32i \u2212 \u01eb\u03c32k+1 2 \u2264 r \u2211\ni=1\n\u2016ci\u201622\u03c32i .\nConverting these sums back to norms yields \u2016\u03a3l\u20162F \u2212 \u01eb\u03c32k+1 2 \u2264 \u2016C\u22a4\u03a3l\u20162F and therefore \u2016Al\u20162F \u2212 \u01eb\u03c32k+1\n2 \u2264 \u2016Y1Y\u22a41 Al\u20162F and\n\u2016Al\u20162F \u2212 \u2016Y1Y\u22a41 Al\u20162F \u2264 \u01eb\u03c32k+1 2 . (7)\nNow Y1Y \u22a4 1 Al is a rank l approximation to A falling within the column span of Y and hence within the column span of Q. By Lemma 2, the best rank l Frobenius approximation to A within Q is given by QU\u0304l(QU\u0304l) \u22a4A. So we have\n\u2016Al\u20162F \u2212 \u2016QU\u0304l(QU\u0304l)\u22a4A\u20162F = E(Zl,A) \u2264 \u01eb\u03c32k+1 2 ,\ngiving Property 1.\nFor Algorithm 1, we instead choose p1(x) = (1+\u01eb/2)\u03c3k+1 \u00b7 (\nx (1+\u01eb/2)\u03c3k+1\n)2q+1 . For q = \u0398(log d/\u01eb),\nthis polynomial satisfies the necessary properties: for all i \u2265 k + 1, p1(\u03c3i) \u2264 O ( \u01eb 2d2\u03c3 2 k+1 )\nand for all i \u2264 m, \u03c3i \u2264 p1(\u03c3i). Further, up to a rescaling, p1(A)\u03a0 = K so Y1 spans the same space as K. Therefore since Algorithm 1 returns Z with Zl equal to the best rank l Frobenius norm approximation to A within the span of K, for all l we have:\n\u2016QU\u0304l(QU\u0304l)\u22a4A\u20162F \u2265 \u2016Y1Y\u22a41 Al\u20162F \u2265 \u2016Al\u20162F \u2212 \u01eb\u03c32k+1 2 ,\ngiving the proof.\nProof of Property 2\nProperty 1 and the fact that E(Zl,A) is always positive immediately gives Property 2 for l \u2264 m. So we need to show that it holds for m < l \u2264 k. Note that if w, the number of singular values with\n1 1+\u01eb/2\u03c3k \u2264 \u03c3i < \u03c3k is equal to 0, then \u03c3k+1 < 11+\u01eb/2\u03c3k, so m = k and we are done. So we assume w \u2265 1 henceforth. Again, we first prove the statement for Algorithm 2 and then explain how the proof extends to the simpler case of Algorithm 1.\nIntuitively, Property 1 follows from the guarantee that there is a rank m subspace of span(K) that aligns with A nearly as well as the space spanned by A\u2019s top m singular vectors. To prove Property 2 we must show that there is also some rank k subspace in span(K) whose components all align nearly as well with A as uk, the k\nth singular vector of A. The existence of such a subspace ensures that Z performs well, even on singular vectors in the intermediate range [\u03c3k, (1+ \u01eb/2)\u03c3k+1].\nLet p2 be the polynomial from Lemma 5 with \u03b1 = 1 1+\u01eb/2\u03c3k, \u03b3 = \u01eb/2, and q \u2265 c log(d/\u01eb)/ \u221a \u01eb for some fixed constant c. Let Y2 \u2208 Rn\u00d7k be an orthonormal basis for the span of p2(A)\u03a0. Again, as long as we choose q to be odd, p2(A) only contains odd powers of A and so Y2 falls within the span of the Krylov subspace from Algorithm 2. We wish to show that for every unit vector x in the column span of Y2, \u2016x\u22a4A\u20162 \u2265 11+\u01eb/2\u03c3k.\nLet Ainner = Ar\\k \u2212Ar\\(k+w). Ainner = U\u03a3innerV\u22a4 where \u03a3inner contains only the singular values \u03c3k+1, . . . , \u03c3k+w. These are the w intermediate singular values of A falling in the range [\n1 1+\u01eb/2\u03c3k, \u03c3k\n)\n. Let Aouter = A\u2212Ainner = U\u03a3outerV\u22a4. \u03a3outer contains all large singular values of A with \u03c3i \u2265 \u03c3k and all small singular values with \u03c3i < 11+\u01eb/2\u03c3k.\nLet Yinner \u2208 Rn\u00d7min{k,w} be an orthonormal basis for the columns of p2(Ainner)\u03a0. Similarly let Youter \u2208 Rn\u00d7k, be an orthonormal basis for the columns of p2(Aouter)\u03a0.\nEvery column of Yinner falls in the column span of Ainner and hence the column span of Uinner \u2208 Rn\u00d7w, which contains only the singular vectors of A corresponding to the inner singular values. Similarly, the columns of Youter fall within the span of Uouter \u2208 Rn\u00d7r\u2212w, which contains the remaining left singular vectors of A. So the columns of Yinner are orthogonal to those of Youter and [Yinner,Youter] forms an orthogonal basis. For any unit vector x \u2208 span(p2(A)\u03a0) = span(Y2) we can write x = xinner + xouter where xinner and xouter are orthogonal vectors in the spans of Yinner and Youter respectively. We have:\n\u2016x\u22a4A\u201622 = \u2016x\u22a4innerA\u201622 + \u2016x\u22a4outerA\u201622. (8)\nWe will lower bound \u2016x\u22a4A\u201622 by considering each contribution separately. First, any unit vector x\u2032 \u2208 Rn in the column span of Yinner can be written as x\u2032 = Uinnerz where z \u2208 Rw is a unit vector.\n\u2016x\u2032\u22a4A\u201622 = z\u22a4U\u22a4innerAA\u22a4Uinnerz = z\u22a4\u03a32innerz \u2265 (\n1\n1 + \u01eb/2 \u03c3k\n)2\n\u2265 (1\u2212 \u01eb)\u03c32k. (9)\nNote that we\u2019re abusing notation slightly, using \u03a3inner \u2208 Rw\u00d7w to represent the diagonal matrix containing all singular values of A with 11+\u01eb/2\u03c3k \u2264 \u03c3i \u2264 \u03c3k without diagonal entries of 0.\nWe next apply the argument used to prove Property 1 to p2(Aouter)\u03a0. The (k + 1) th singular\nvalue of Aouter is equal to \u03c3k+w+1 \u2264 11+\u01eb/2\u03c3k = \u03b1. So applying (7) we have for all l \u2264 k,\n\u2016Al\u20162F \u2212 \u2016 (Youter)l (Youter)\u22a4l Al\u20162F \u2264 \u01eb\u03c32k 2 . (10)\nNote that Aouter has the same top k singular vectors at A so (Aouter)l = Al. Let x \u2032 \u2208 Rn be any unit vector within the column space of Youter and let Youter = (I \u2212 x\u2032x\u2032\u22a4)Youter, i.e the matrix with x\u2032 projected off each column. We can use (10) and the optimality of the SVD for low rank approximation to obtain:\n\u2016Ak\u20162F \u2212 \u2016YouterY\u22a4outerAk\u20162F = \u2016Ak\u20162F \u2212 \u2016YouterY \u22a4 outerAk\u20162F \u2212 \u2016x\u2032x\u2032\u22a4Ak\u20162F \u2264 \u01eb\u03c32k 2\n\u2016Ak\u20162F \u2212 \u2016Ak\u22121\u20162F \u2212 \u01eb\u03c32k 2 \u2264 \u2016x\u2032x\u2032\u22a4Ak\u20162F (1\u2212 \u01eb/2)\u03c32k \u2264 \u2016x\u2032 \u22a4 A\u201622. (11)\nPlugging (9) and (11) into (8) yields that, for any x in span(Y2), i.e. span(p2(A)\u03a0),\n\u2016x\u22a4A\u201622 = \u2016x\u22a4innerA\u201622 + \u2016x\u22a4outerA\u201622 \u2265 ( \u2016xinner\u201622 + \u2016xouter\u201622 ) (1\u2212 \u01eb)\u03c32k \u2265 (1\u2212 \u01eb)\u03c32k. (12)\nSo, we have identified a rank k subspace Y2 within our Krylov subspace such that every vector in its span aligns at least as well with A as uk.\nNow, for any m \u2264 l \u2264 k, consider E(Zl,A). We know that given Zl\u22121, we can form a rank l matrix Zl in our Krylov subspace simply by appending a column x orthogonal to the l\u2212 1 columns of Zl\u22121 but falling in the span of Y2. Since Y2 has rank k, finding such a column is always\npossible. Since Zl is the optimal rank l Frobenius norm approximation to A falling within our Krylov subspace we have:\nE(Zl,A) \u2264 E(Zl,A) = \u2016Al\u20162F \u2212 \u2016ZlZ \u22a4 l A\u20162F\n= \u03c32l + \u2016Al\u22121\u20162F \u2212 \u2016Zl\u22121Z\u22a4l\u22121A\u20162F \u2212 \u2016xx\u22a4A\u20162F = E(Zl\u22121,A) + \u03c32l \u2212 \u2016xx\u22a4A\u20162F \u2264 E(Zl\u22121,A) + (1 + \u01eb/2)2\u03c32k+1 \u2212 (1\u2212 \u01eb)\u03c32k+1 \u2264 E(Zl\u22121,A) + 3\u01eb \u00b7 \u03c32k+1,\nwhich gives Property 2.\nAgain, a nearly identical proof applies for Algorithm 1. We just choose p2(x) = \u03c3k\n(\nx \u03c3k\n)2q+1 . For\nq = \u0398(log d/\u01eb) this polynomial satisfies the necessary properties: for all i \u2265 k, p1(\u03c3i) \u2264 O ( \u01eb 2d2 \u03c32k ) and for all i \u2264 k, \u03c3i \u2264 p2(\u03c3i).\nProof of Property 3\nBy Properties 1 and 2 we already have, for all l \u2264 k, E(Zl,A) \u2264 \u01eb\u03c32k+1 + (l \u2212 m) \u00b7 3\u01eb\u03c32k+1 \u2264 (1 + k \u2212m) \u00b7 3\u01eb \u00b7 \u03c32k+1. So if k \u2212m \u2264 w then we immediately have Property 3.\nOtherwise, w < k \u2212m so w < k and thus p2(Ainner)\u03a0 \u2208 Rn\u00d7k only has rank w. It has a null space of dimension k \u2212 w. Choose any z in this null space. Then p2(A)\u03a0z = p2(Ainner)\u03a0z + p2(Aouter)\u03a0z = p2(Aouter)\u03a0z. In other words, p2(A)\u03a0z falls entirely within the span of Youter. So, there is a k \u2212 w dimensional subspace of span(Y2) that is entirely contained in span(Youter).\nFor l \u2264 m+ w, then Properties 1 and 2 already give us E(Zl,A) \u2264 \u01eb\u03c32k+1 + (l \u2212m) \u00b7 3\u01eb\u03c32k+1 \u2264 (w+1) \u00b7 3\u01eb \u00b7 \u03c32k+1. So consider m+w \u2264 l \u2264 k. Given Zm, to form a rank l matrix Zl in our Krylov subspace we need to append l \u2212m orthonormal columns. We can choose min{k \u2212 w \u2212m, l \u2212m} columns, X1, from the k \u2212 w dimensional subspace within span(Y2) that is entirely contained in span(Youter). If necessary (i.e. k\u2212w\u2212m \u2264 l\u2212m), We can then choose the remaining l\u2212 (k\u2212w) columns X2 from the span of Y2.\nSimilar to our argument when considering a single vector in the span of Youter, letting Youter = (\nI\u2212X1X\u22a41 ) Youter, we have by (10):\n\u2016Ak\u20162F \u2212 \u2016YouterY\u22a4outerAk\u20162F \u2264 \u01eb\u03c32k 2\n\u2016Ak\u20162F \u2212 \u2016YouterY \u22a4 outerAk\u20162F \u2212 \u2016X1X\u22a41 Ak\u20162F \u2264 \u01eb\u03c32k 2\n\u2016Ak\u20162F \u2212 \u2016Ak\u2212min{k\u2212w\u2212m,l\u2212m}\u20162F \u2212 \u01eb\u03c32k 2\n\u2264 \u2016X1X\u22a41 Ak\u20162F k \u2211\ni=k\u2212min{k\u2212w\u2212m,l\u2212m}+1 \u03c32i \u2212\n\u01eb\u03c32k 2 \u2264 \u2016X1X\u22a41 A\u20162F .\nBy applying (12) directly to each column of X2 we also have:\n(l + w \u2212 k)\u03c32k \u2212 (l + w \u2212 k)\u01eb\u03c32k \u2264 \u2016X2X\u22a42 A\u20162F (l + w \u2212 k)\u03c32k+1 \u2212 (l + w \u2212 k)\u01eb\u03c32k+1 \u2264 \u2016X2X\u22a42 A\u20162F .\nAssume that min{k \u2212 w \u2212m, l \u2212m} = k \u2212 w \u2212m. Similar calculations show the same result when min{k \u2212 w \u2212m, l \u2212m} = l \u2212m. We can use the above two bounds to obtain:\nE(Zl,A) \u2264 E(Zl,A) = \u2016Al\u20162F \u2212 \u2016ZlZ \u22a4 l A\u20162F\n=\nl \u2211\ni=m+1\n\u03c32i + \u2016Am\u20162F \u2212 \u2016ZmZ\u22a4mA\u20162F \u2212 \u2016X1X\u22a41 A\u20162F \u2212 \u2016X2X\u22a42 A\u20162F\n\u2264 E(Zm,A) + l \u2211\ni=m+1\n\u03c32i \u2212 k \u2211\ni=w+m+1\n\u03c32i + \u01eb\u03c32k 2 \u2212 (l + w \u2212 k)\u03c32k+1 + (l + w \u2212 k)\u01eb\u03c32k+1\n\u2264 m+w \u2211\ni=m+1\n\u03c32i \u2212 w\u03c32k+1 + (l + w \u2212 k + 3/2)\u01eb\u03c32k+1\n\u2264 (l + 3w \u2212 k + 3/2)\u01eb\u03c32k+1 \u2264 (w + 1) \u00b7 3\u01eb \u00b7 \u03c32k+1,\ngiving Property 3 for all l \u2264 k."}, {"heading": "5.2 Error Bounds for Simultaneous Iteration and Block Lanczos", "text": "With Lemma 9 in place, we can easily prove that Simultaneous Iteration and Block Lanczos both achieve the low rank approximation and PCA guarantees (1), (2), and (3).\nTheorem 10 (Near Optimal Spectral Norm Error Approximation). With probability 99/100, Algorithms 1 and 2 return Z satisfying (2):\n\u2016A\u2212 ZZ\u22a4A\u20162 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162.\nProof. Let m be the number of singular values with \u03c3i \u2265 (1 + \u01eb/2)\u03c3k+1. If m = 0 then we are done since any Z will satisfy \u2016A \u2212 ZZ\u22a4A\u20162 \u2264 \u2016A\u20162 = \u03c31 \u2264 (1 + \u01eb/2)\u03c3k+1 \u2264 (1 + \u01eb)\u2016A \u2212Ak\u20162. Otherwise, by Property 1 of Lemma 9,\nE(Zm,A) \u2264 \u01eb\u03c32k+1 2\n\u2016A\u2212 ZmZ\u22a4mA\u20162F \u2264 \u2016A\u2212Am\u20162F + \u01eb\u03c32k+1 2 .\nAdditive error in Frobenius norm directly translates to additive spectral norm error. Specifically, applying Theorem 3.4 of [Gu14], which we also prove as Lemma 15 in Appendix A,\n\u2016A\u2212 ZmZ\u22a4mA\u201622 \u2264 \u2016A\u2212Am\u201622 + \u01eb\u03c32k+1 2 \u2264 \u03c32m+1 + \u01eb\u03c32k+1 2\n\u2264 (1 + \u01eb/2)\u03c32k+1 + \u01eb\u03c32k+1 2 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u201622. (13)\nFinally, ZmZ \u22a4 mA = ZZ \u22a4 mA and so by Lemma 3 we have \u2016A\u2212 ZZ\u22a4A\u201622 \u2264 \u2016A\u2212 ZmZ\u22a4mA\u201622, which combines with (13) to give the result.\nTheorem 11 (Near Optimal Frobenius Norm Error Approximation). With probability 99/100, Algorithms 1 and 2 return Z satisfying (1):\n\u2016A\u2212 ZZ\u22a4A\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016F .\nProof. By Property 3 of Lemma 9 we have:\nE(Zl,A) \u2264 (w + 1) \u00b7 3\u01eb \u00b7 \u03c32k+1 \u2016A\u2212 ZZ\u22a4A\u20162F \u2264 \u2016A\u2212Ak\u20162F + (w + 1) \u00b7 3\u01eb \u00b7 \u03c32k+1. (14)\nw is defined as the number of singular values with 11+\u01eb/2\u03c3k \u2264 \u03c3i < \u03c3k. So \u2016A \u2212 Ak\u20162F \u2265 w \u00b7 (\n1 1+\u01eb/2\u03c3k\n)2 . Plugging into (14) we have:\n\u2016A\u2212 ZZ\u22a4A\u20162F \u2264 \u2016A\u2212Ak\u20162F + (w + 1) \u00b7 3\u01eb \u00b7 \u03c32k+1 \u2264 (1 + 10\u01eb)\u2016A \u2212Ak\u20162F .\nAdjusting constants on the \u01eb gives us the result.\nTheorem 12 (Per Vector Quality Guarantee). With probability 99/100, Algorithms 1 and 2 return Z satisfying (3):\n\u2200i, \u2223 \u2223 \u2223 u\u22a4i AA \u22a4ui \u2212 z\u22a4i AA\u22a4zi \u2223 \u2223 \u2223 \u2264 \u01eb\u03c32k+1.\nProof. First note that z\u22a4i AA \u22a4zi \u2264 u\u22a4i AA\u22a4ui. This is because z\u22a4i AA\u22a4zi = z\u22a4i QQ\u22a4AA\u22a4QQ\u22a4zi = \u03c3i(QQ \u22a4A)2 by our choice of zi. \u03c3i(QQ\u22a4A)2 \u2264 \u03c3i(A)2 since applying a projection to A will decrease each of its singular values (which follows for example from the Courant-Fischer min-max principle). Then by Property 2 of Lemma 9 we have, for all i \u2264 k,\n\u2016Ai\u20162F \u2212 \u2016ZiZ\u22a4i \u20162F \u2264 \u2016Ai\u22121\u20162F \u2212 \u2016Zi\u22121Z\u22a4i\u22121\u20162F + 3\u01eb\u03c32k+1 \u03c32i \u2264 \u2016ziz\u22a4i A\u20162F + 3\u01eb\u03c32k+1 = z\u22a4i AA\u22a4zi + 3\u01eb\u03c32k+1.\n\u03c32i = u \u22a4 i AA \u22a4ui, so simply adjusting constants on \u01eb gives the result."}, {"heading": "5.3 Improved Convergence With Spectral Decay", "text": "In addition to the traditional Simultaneous Iteration and Block Lanczos methods (Algorithms 1 and 2), our analysis applies to the common modification of running the algorithms with \u03a0 \u2208 Rn\u00d7p for p \u2265 k [RST09, HMST11, HMT11]. This technique can significantly accelerate both methods for matrices with decaying singular values. For simplicity, we focus on Block Lanczos, although all arguments immediately extend to the simpler Simultaneous Iteration.\nIn order to avoid inverse dependence on the potentially small singular value gap \u03c3k\u03c3k+1 \u2212 1, the number of iterations of Block Lanczos inherently depends on 1/ \u221a \u01eb. This ensures that our matrix polynomial sufficiently separates small singular values from larger ones. However, when\n\u03c3k > (1 + \u01eb)\u03c3k+1 we can actually use q = \u0398\n(\nlog(d/\u01eb)/ \u221a min{1, \u03c3k\u03c3k+1 \u2212 1} ) iterations, which\nis sufficient for separating the top k singular values significantly from the lower values. Specifically, in the Block Lanczos case, if we set \u03b1 = \u03c3k+1 and \u03b3 =\n\u03c3k \u03c3k+1 \u2212 1, we know that with\nq = \u0398\n(\nlog(d/\u01eb)/ \u221a min{1, \u03c3k\u03c3k+1 \u2212 1} ) , (5) still holds. We can then just follow the proof of Lemma\n9 and show that Property 1 holds for all l \u2264 k (not just for l \u2264 m as originally proven). This gives Property 2 and Property 3 trivially.\nFurthermore, for p \u2265 k, the exact same analysis shows that q = \u0398 ( log(d/\u01eb)/ \u221a min{1, \u03c3k\u03c3p+1 \u2212 1} )\nsuffices. When A\u2019s spectrum decays rapidly, so \u03c3p+1 \u2264 c \u00b7 \u03c3k for some constant c < 1 and some p not much larger than k, we can obtain significantly faster runtimes. Our \u01eb dependence becomes logarithmic, rather than polynomial:\nTheorem 13 (Gap Dependent Convergence). With probability 99/100, for any p \u2265 k, Algorithm 1 or 2 initialized with \u03a0 \u223c N (0, 1)d\u00d7p returns Z satisfying guarantees (1), (2), and (3) as long as we set q = \u0398 ( log(d/\u01eb)/ (\nmin{1, \u03c3k\u03c3p+1 \u2212 1} )) or \u0398\n(\nlog(d/\u01eb)/ \u221a min{1, \u03c3k\u03c3p+1 \u2212 1} ) , respectively.\nThis theorem may prove especially useful in practice because, on many architectures, multiplying a large A by 2k or even 10k vectors is not much more expensive than multiplying by k vectors. Additionally, it should still be possible to perform all steps for post-processing K in memory, again limiting additional runtime costs due to its larger size.\nFinally, we note that while Theorem 13 is more reminiscent of classical gap-dependent bounds, it still takes substantial advantage of the fact that we\u2019re looking for nearly optimal low rank approximations and principal components instead of attempting to converge precisely to A\u2019s true singular values. This allows the result to avoid dependence on the gap between adjacent singular values, instead varying only with \u03c3k\u03c3p+1 , which should be much larger."}, {"heading": "6 Acknowledgements", "text": "We thank David Woodruff, Aaron Sidford, and Richard Peng for several valuable conversations. Additionally, Michael Cohen was very helpful in discussing many details of this project, including the ultimate form of Lemma 9. This work was partially supported by NSF Graduate Research Fellowship Grant No. 1122374, AFOSR grant FA9550-13-1-0042, DARPA grant FA8650-11-C-7192, and the NSF Center for Science of Information."}, {"heading": "A Appendix", "text": "Frobenius Norm Low Rank Approximation\nWe first give a deterministic Lemma, from which the main approximation result follows.\nLemma 14 (Special case of Lemma 4.4 of [Woo14], originally proven in [BDMI14]). Let A \u2208 Rn\u00d7d have SVD A = U\u03a3V\u22a4, let S \u2208 Rd\u00d7k be any matrix such that rank (\nV\u22a4k S ) = k, and let C \u2208 Rn\u00d7k be an orthonormal basis for the column span of AS. Then:\n\u2016A\u2212CC\u22a4A\u20162F \u2264 \u2016A\u2212Ak\u20162F + \u2016 (A\u2212Ak)S ( V\u22a4k S )+ \u20162F .\nLemma 4 (Frobenius Norm Low Rank Approximation). Take any A \u2208 Rn\u00d7d and \u03a0 \u2208 Rd\u00d7k where the entries of \u03a0 are independent Gaussians drawn from N (0, 1). If we let Z be an orthonormal basis for span (A\u03a0), then with probability at least 99/100, for some fixed constant c,\n\u2016A\u2212 ZZ\u22a4A\u20162F \u2264 c \u00b7 dk\u2016A\u2212Ak\u20162F .\nProof. We follow [Woo14]. Apply Lemma 14 with S = \u03a0. With probability 1, V\u22a4k S has full rank. So, to show the result we need to show that \u2016 (A\u2212Ak)S ( V\u22a4k S )+ \u20162F \u2264 c\u2016A\u2212Ak\u20162F for some fixed c. For any two matrices M and N, \u2016MN\u2016F \u2264 \u2016M\u2016F \u2016N\u20162. This property is known as spectral submultiplicativity. Noting that \u2016Ur\\k\u03a3r\\k\u20162F = \u2016A\u2212Ak\u20162F and applying submultiplicativity,\n\u2016 (A\u2212Ak)S ( V\u22a4k S )+ \u20162F \u2264 \u2016Ur\\k\u03a3r\\k\u20162F \u2016V\u22a4r\\kS\u201622\u2016 ( V\u22a4k S )+ \u201622.\nBy the rotational invariance of the Gaussian distribution, since the rows of V\u22a4 are orthonormal, the entries of V\u22a4k S and V \u22a4 r\\kS are independent Gaussians. By standard Gaussian matrix concentration results (Fact 6 of [Woo14], also in [RV10]), with probability at least 99/100, \u2016V\u22a4r\\kS\u201622 \u2264 c1 \u00b7max{k, r \u2212 k} \u2264 c1d\u0307 and \u2016 ( V\u22a4k S )+ \u201622 \u2264 c2k for some fixed constants c1, c2. So,\n\u2016Ur\\k\u03a3r\\k\u20162F \u2016V\u22a4r\\kS\u201622\u2016 ( V\u22a4k S )+ \u201622 \u2264 c \u00b7 dk\u2016A\u2212Ak\u20162F\nfor some fixed c, yielding the result. Note that we choose probability 99/100 for simplicity \u2013 we can obtain a result with higher probability by simply allowing for a higher constant c, which in our applications of Lemma 4 will only factor into logarithmic terms.\nChebyshev Polynomials\nLemma 5 (Chebyshev Minimizing Polynomial). Given a specified value \u03b1 > 0, gap \u03b3 \u2208 (0, 1], and degree q \u2265 1, there exists a degree q polynomial p(x) such that:\n1. p((1 + \u03b3)\u03b1) = (1 + \u03b3)\u03b1\n2. p(x) \u2265 x for all x \u2265 (1 + \u03b3)\u03b1\n3. p(x) \u2264 \u03b1 2q \u221a \u03b3\u22121 for all x \u2208 [0, \u03b1]\nProof. The required polynomial can be constructed using a standard Chebyshev polynomial of degree q, Tq(x), which is defined by the three term recurrence:\nT0(x) = 1\nT1(x) = x Tq(x) = 2xTq\u22121(x)\u2212 Tq\u22122(x)\nEach Chebyshev polynomial satisfies the well known property that Tq(x) \u2264 1 for all x \u2208 [\u22121, 1] and we can write the polynomials in closed form [MH02]:\nTq(x) = (x+\n\u221a x2 \u2212 1)q + (x\u2212 \u221a x2 \u2212 1)q\n2 . (15)\nFor Lemma 5, we simply set:\np(x) = (1 + \u03b3)\u03b1 Tq(x/\u03b1)\nTq(1 + \u03b3) , (16)\nwhich is clearly of degree q and well defined since, referring to (15), Tq(x) > 0 for all x > 1. Now,\np((1 + \u03b3)\u03b1) = (1 + \u03b3)\u03b1 Tq(1 + \u03b3)\nTq(1 + \u01eb) = (1 + \u03b3)\u03b1,\nso p(x) satisfies property 1. With property 1 in place, to prove that p(x) satisfies property 2, it suffices to show that p\u2032(x) \u2265 1 for all x \u2265 (1 + \u03b3)\u03b1. By chain rule,\np\u2032(x) = (1 + \u03b3) Tq(1 + \u03b3) T \u2032q(x/\u03b1).\nThus, it suffices to prove that, for all x \u2265 (1 + \u03b3),\n(1 + \u03b3)T \u2032q(x) \u2265 Tq(1 + \u03b3). (17)\nWe do this by showing that (1 + \u03b3)T \u2032q(1 + \u03b3) \u2265 Tq(1 + \u03b3) and then claim that T \u2032\u2032q (x) \u2265 0 for all x > (1 + \u03b3), so (17) holds for x > (1 + \u03b3) as well. A standard form for the derivative of the Chebyshev polynomial is\nT \u2032q =\n{\n2q (Tq\u22121 + Tq\u22123 + . . .+ T1) if q is even, 2q (Tq\u22121 + Tq\u22123 + . . .+ T2) + q if q is odd. (18)\n(18) can be verified via induction once noting that the Chebyshev recurrence gives T \u2032q = 2xT \u2032 q\u22121 + 2Tq\u22121 \u2212 T \u2032q\u22122. Since Ti(x) > 0 when x \u2265 1, we can conclude that T \u2032q(x) \u2265 2qTq\u22121(x). So proving (17) for x = (1 + \u03b3) reduces to proving that\n(1 + \u03b3)2qTq\u22121(1 + \u03b3) \u2265 Tq(1 + \u03b3). (19)\nNoting that, for x \u2265 1, (x+ \u221a x2 \u2212 1) > 0 and (x\u2212 \u221a x2 \u2212 1) > 0, it follows from (15) that\nTq\u22121(x) ( (x+ \u221a x2 \u2212 1) + (x\u2212 \u221a x2 \u2212 1) ) \u2265 Tq(x),\nand thus\nTq(x)\nTq\u22121(x) \u2264 2x.\nSo, to prove (19), it suffices to show that 2(1 + \u03b3) \u2264 (1 + \u03b3)2q, which is true whenever q \u2265 1. So (17) holds for all x = (1 + \u03b3).\nFinally, referring to (18), we know that T \u2032\u2032q must be some positive combination of lower degree Chebyshev polynomials. Again, since Ti(x) > 0 when x \u2265 1, we conclude that T \u2032\u2032q (x) \u2265 0 for all x \u2265 1. It follows that T \u2032q(x) does not decrease above x = (1+\u03b3), so (17) also holds for all x > (1+\u03b3) and we have proved property 2.\nTo prove property 3, we first note that, by the well known property that Ti(x) \u2264 1 for x \u2208 [\u22121, 1], Tq(x/\u03b1) \u2264 1 for x \u2208 [0, \u03b1]. So, to prove p(x) \u2264 \u03b12q\u221a\u03b3\u22121 , we just need to show that\n1 Tq(1 + \u03b3) \u2264 1 2q \u221a \u03b3\u22121 . (20)\nEquation (15) gives Tq(1+\u03b3) \u2265 12(1+\u03b3+ \u221a (1 + \u03b3)2 \u2212 1)q \u2265 12(1+ \u221a \u03b3)q. When \u03b3 \u2264 1, (1+\u221a\u03b3)1/ \u221a \u03b3 \u2265\n2. Thus, (1 + \u221a \u03b3)q \u2265 2q \u221a \u03b3 . Dividing by 2 gives Tq(1 + \u03b3) \u2265 2q \u221a \u03b3\u22121, which gives (20) and thus property 3.\nAdditive Frobenius Norm Error Implies Additive Spectral Norm Error\nLemma 15 (Theorem 3.4 of [Gu14]). For any A \u2208 Rn\u00d7d, let B \u2208 Rn\u00d7d be any rank k matrix satisfying \u2016A\u2212B\u20162F \u2264 \u2016A\u2212Ak\u20162F + \u03b7. Then\n\u2016A\u2212B\u201622 \u2264 \u2016A\u2212Ak\u201622 + \u03b7.\nProof. We follow the proof given in [Gu14] nearly exactly, including it for completeness. By Weyl\u2019s monotonicity theorem (Theorem 3.2 in [Gu14]), for any two matrices X,Y \u2208 Rn\u00d7d with n \u2265 d, for all i, j with i+ j \u2212 1 \u2264 n we have \u03c3i+j\u22121(X+Y) \u2264 \u03c3i(X) + \u03c3j(X). If we write A = (A\u2212B) +B and apply this theorem, then for all 1 \u2265 i \u2265 n\u2212 k,\n\u03c3i+k(A) \u2264 \u03c3i(A\u2212B) + \u03c3k+1(B).\nNote that if n < d, we can just work with A\u22a4 and B\u22a4. Now, \u03c3k+1(B) = 0 since B is rank k, so:\n\u2016A\u2212B\u20162F \u2264 \u2016A\u2212Ak\u20162F + \u03b7 n \u2211\ni=1\n\u03c32i (A\u2212B) \u2264 n \u2211\ni=k+1\n\u03c32i (A) + \u03b7\nn\u2212k \u2211\ni=1\n\u03c32i (A\u2212B) \u2264 n \u2211\ni=k+1\n\u03c32i (A) + \u03b7\n\u03c321(A\u2212B) + n\u2212k \u2211\ni=2\n\u03c32i (A) \u2264 n \u2211\ni=k+1\n\u03c32i (A) + \u03b7\n\u03c321(A\u2212B) \u2264 n \u2211\ni=k+1\n\u03c32i (A)\u2212 n\u2212k \u2211\ni=2\n\u03c32i (A) + \u03b7\n\u03c321(A\u2212B) \u2264 \u03c32k+1(A) + \u03b7."}], "references": [{"title": "Das verfahren der treppeniteration und verwandte verfahren zur l\u00f6sung algebraischer eigenwertprobleme", "author": ["Friedrich L. Bauer"], "venue": "Zeitschrift fu\u0308r angewandte Mathematik und Physik ZAMP,", "citeRegEx": "Bauer.,? \\Q1957\\E", "shortCiteRegEx": "Bauer.", "year": 1957}, {"title": "Near-optimal columnbased matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Boutsidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2014}, {"title": "A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices", "author": ["Jane Cullum", "W.E. Donath"], "venue": "In IEEE Conference on Decision and Control including the 13th Symposium on Adaptive Processes,", "citeRegEx": "Cullum and Donath.,? \\Q1974\\E", "shortCiteRegEx": "Cullum and Donath.", "year": 1974}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["Petros Drineas", "Alan Frieze", "Ravi Kannan", "Santosh Vempala", "V Vinay"], "venue": "Machine Learning,", "citeRegEx": "Drineas et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2004}, {"title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM J. Comput.,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Adaptive sampling and fast low-rank matrix approximation", "author": ["Amit Deshpande", "Santosh Vempala"], "venue": "In Proceedings of the 10th International Workshop on Randomization and Computation (RANDOM),", "citeRegEx": "Deshpande and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande and Vempala.", "year": 2006}, {"title": "Fast Monte Carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "Efficient algorithms for computing a strong rankrevealing qr factorization", "author": ["Ming Gu", "Stanley C. Eisenstat"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Gu and Eisenstat.,? \\Q1996\\E", "shortCiteRegEx": "Gu and Eisenstat.", "year": 1996}, {"title": "A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix", "author": ["Gene H. Golub", "Franklin T. Luk", "Michael L. Overton"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Golub et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1981}, {"title": "The block Lanczos method for computing eigenvalues", "author": ["Gene Golub", "Richard Underwood"], "venue": "Mathematical Software,", "citeRegEx": "Golub and Underwood.,? \\Q1977\\E", "shortCiteRegEx": "Golub and Underwood.", "year": 1977}, {"title": "Subspace iteration randomization and singular value problems", "author": ["Ming Gu"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "Gu.,? \\Q2014\\E", "shortCiteRegEx": "Gu.", "year": 2014}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Randomized methods for computing low-rank approximations of matrices", "author": ["Nathan P Halko"], "venue": "PhD thesis, University of Colorado,", "citeRegEx": "Halko.,? \\Q2012\\E", "shortCiteRegEx": "Halko.", "year": 2012}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Yoel Shkolnisky", "Mark Tygert"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "An iteration method for the solution of the eigenvalue problem of linear differential and integral operators1", "author": ["Cornelius Lanczos"], "venue": "Journal of Research of the National Bureau of Standards,", "citeRegEx": "Lanczos.,? \\Q1950\\E", "shortCiteRegEx": "Lanczos.", "year": 1950}, {"title": "PROPACK: Software for large and sparse SVD calculations", "author": ["Rasmus Munk Larsen"], "venue": "Stanford University,", "citeRegEx": "Larsen.,? \\Q2005\\E", "shortCiteRegEx": "Larsen.", "year": 2005}, {"title": "Fast algorithms for approximating the singular value decomposition", "author": ["Aditya Krishna Menon", "Charles Elkan"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Menon and Elkan.,? \\Q2011\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2011}, {"title": "Symmetric gauge functions and unitarily invariant norms", "author": ["L. Mirsky"], "venue": "The Quarterly Journal of Mathematics,", "citeRegEx": "Mirsky.,? \\Q1960\\E", "shortCiteRegEx": "Mirsky.", "year": 1960}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Michael W Mahoney", "Xiangrui Meng"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Mahoney and Meng.,? \\Q2013\\E", "shortCiteRegEx": "Mahoney and Meng.", "year": 2013}, {"title": "A randomized algorithm for the approximation of matrices", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Technical Report 1361,", "citeRegEx": "Martinsson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Martinsson et al\\.", "year": 2006}, {"title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L. Nguyen"], "venue": "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguyen.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguyen.", "year": 2013}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H. Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Papadimitriou et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 2000}, {"title": "A randomized algorithm for principal component analysis", "author": ["Vladimir Rokhlin", "Arthur Szlam", "Mark Tygert"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Rokhlin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Simultaneous iteration method for symmetric matrices", "author": ["H. Rutishauser"], "venue": "Numerische Mathematik,", "citeRegEx": "Rutishauser.,? \\Q1970\\E", "shortCiteRegEx": "Rutishauser.", "year": 1970}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "In Proceedings of the International Congress of Mathematicians 2010 (ICM),", "citeRegEx": "Rudelson and Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2010}, {"title": "On the rates of convergence of the Lanczos and the Block-Lanczos methods", "author": ["Y. Saad"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Saad.,? \\Q1980\\E", "shortCiteRegEx": "Saad.", "year": 1980}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T\u00e1mas Sarl\u00f3s"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "An implementation of a randomized algorithm for principal component analysis", "author": ["Arthur Szlam", "Yuval Kluger", "Mark Tygert"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "Szlam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szlam et al\\.", "year": 2014}, {"title": "On the minimum rank of a generalized matrix approximation problem in the maximum singular value norm", "author": ["Kin Cheong Sou", "Anders Rantzer"], "venue": "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems (MTNS),", "citeRegEx": "Sou and Rantzer.,? \\Q2010\\E", "shortCiteRegEx": "Sou and Rantzer.", "year": 2010}, {"title": "Numerical Linear Algebra", "author": ["Lloyd N. Trefethen", "David Bau"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Trefethen and Bau.,? \\Q1997\\E", "shortCiteRegEx": "Trefethen and Bau.", "year": 1997}, {"title": "Randomized algorithms for low-rank matrix factorizations: Sharp performance", "author": ["Rafi Witten", "Emmanuel J. Cand\u00e8s"], "venue": "bounds. Algorithmica,", "citeRegEx": "Witten and Cand\u00e8s.,? \\Q2014\\E", "shortCiteRegEx": "Witten and Cand\u00e8s.", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "We re-analyze Simultaneous Power Iteration and the Block Lanczos method, two classical iterative algorithms for the singular value decomposition (SVD). We are interested in convergence bounds that do not depend on properties of the input matrix (e.g. singular value gaps). Simultaneous Iteration is known to give a low rank approximation within (1 + \u01eb) of optimal for spectral norm error in \u00d5(1/\u01eb) iterations. We strengthen this result, proving that it finds approximate principal components very close in quality to those given by an exact SVD. Our work bridges a divide between classical analysis, which can give similar bounds but depends critically on singular value gaps, and more recent work, which only focuses on low rank approximation Furthermore, we extend our bounds to the Block Lanczos method, which we show obtains the same approximation guarantees in just \u00d5(1/ \u221a \u01eb) iterations, giving the fastest known algorithm for spectral norm low rank approximation and principal component approximation. Despite their popularity, Krylov subspace methods like Block Lanczos previously seemed more difficult to analyze and did not come with rigorous gap-independent guarantees. Finally, we give insight beyond the worst case, justifying why Simultaneous Power Iteration and Block Lanczos can run much faster in practice than predicted. We clarify how simple techniques can potentially accelerate both algorithms significantly.", "creator": "LaTeX with hyperref package"}}}