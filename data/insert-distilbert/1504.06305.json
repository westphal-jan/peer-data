{"id": "1504.06305", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2015", "title": "Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices", "abstract": "over the past few years, trace regression models'have steadily received considerable attention in covering the context of matrix completion, quantum state tomography, and compressed sensing. estimation considerations of comparing the underlying matrix from regularization - based approaches promoting low - rankedness, notably nuclear norm regularization, have enjoyed great popularity. in introducing the present paper, we argue that such regularization may no longer be necessary if the resultant underlying matrix is symmetric positive semidefinite ( \\ textsf { spd } ) and the design satisfies certain conditions. indeed in this situation, simple least squares estimation subject to an \\ textsf { o spd } constraint may perform as well possible as regularization - based approaches with a proper choice of the condition regularization parameter, which entails ample knowledge of the noise level and / or tuning. by contrast, solving constrained least squares estimation comes without achieving any minimal tuning parameters parameter and may hence be preferred alternatives due to its simplicity.", "histories": [["v1", "Thu, 23 Apr 2015 19:30:38 GMT  (191kb)", "http://arxiv.org/abs/1504.06305v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["martin slawski", "ping li 0001", "matthias hein 0001"], "accepted": true, "id": "1504.06305"}, "pdf": {"name": "1504.06305.pdf", "metadata": {"source": "CRF", "title": "Regularization-free estimation in trace regression with symmetric positive semidefinite matrices", "authors": ["Martin Slawski", "Ping Li", "Matthias Hein"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n06 30\n5v 1\n[ st\nat .M\nL ]\n2 3"}, {"heading": "1 Introduction", "text": "Trace regression models of the form\nyi = tr(X \u22a4 i \u03a3 \u2217) + \u03b5i, i = 1, . . . , n, (1)\nwhere \u03a3\u2217 \u2208 Rm1\u00d7m2 is the parameter of interest to be estimated given measurement matrices Xi \u2208 Rm1\u00d7m2 and observations yi contaminated by errors \u03b5i, i = 1, . . . , n, have attracted considerable interest in high-dimensional statistical inference, machine learning and signal processing over the past few years. Research in these areas has focused on a setting with few measurements n \u226a m1 \u00b7 m2 and \u03a3\u2217 being (at least approximately) of low rank r \u226a min{m1,m2}. Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9]. A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [22] in regularized estimation amenable to modern optimization techniques. This approach can be seen as natural generalization of \u21131-norm (aka lasso) regularization for the standard linear regression model [28] that arises as a special case of model (1) in which both \u03a3\u2217 and the measurement matrices {Xi}ni=1 are diagonal. It is inarguable that in general regularization is essential if n < m1 \u00b7 m2. However, the situation is less clear if \u03a3\u2217 is known to satisfy additional constraints that can be incorporated in estimation. Specifically, in the present paper we consider the case in which m1 = m2 = m and \u03a3\u2217 is known to be symmetric positive semidefinite (spd), written as \u03a3\u2217 \u2208 Sm+ with Sm+\ndenoting the positive semidefinite cone in the space of symmetric real-valued m \u00d7m matrices Sm. The set Sm+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning methods [24]. It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [33]. In the present paper, we focus on the usefulness of the spd constraint for estimation. We argue that if \u03a3\u2217 is spd and the measurement matrices {Xi}ni=1 obey certain conditions, constrained least squares estimation\nmin \u03a3\u2208Sm\n+\n1\n2n\nn\u2211\ni=1\n(yi \u2212 tr(X\u22a4i \u03a3))2 (2)\nmay perform similarly well in prediction and parameter estimation as approaches employing nuclear norm regularization with proper choice of the regularization parameter, including the interesting regime n < \u03b4m, where \u03b4m = dim(S\nm) = m(m + 1)/2. Note that the objective in (2) only consists of a data fitting term and is hence convenient to work with in practice since one does not need to choose any parameter. Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25]. In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to \u21131-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.\nRelated work. Model (1) with \u03a3\u2217 \u2208 Sm+ has been studied in several recent papers. A good deal of these papers consider the setup of compressed sensing according to which the matrices {Xi}ni=1 can be chosen by the user, with the goal to minimize the number of observations required to (approximately) recover \u03a3\u2217.\nIn [32], the problem of exactly recovering \u03a3\u2217 being low-rank from noiseless observations (\u03b5i = 0, i = 1, . . . , n) by solving a linear feasibility problem over the positive semidefinite cone is considered, which is equivalent to the proposed least squares problem (1) in a noiseless setting. Apart from the fact that we primarily study a noisy setting, we shall argue below that in the setup of compressed sensing the measurement matrices studied in [32] constitute an unfavourable choice relative to those recommended in the present paper.\nIn [10], recovery from rank-one measurements is considered, i.e., for {xi}ni=1 \u2282 Rm\nyi = x \u22a4 i \u03a3 \u2217xi + \u03b5i = tr(X \u22a4 i \u03a3 \u2217) + \u03b5i, with Xi = xix \u22a4 i , i = 1, . . . , n. (3)\nAs opposed to [10], where estimation based on nuclear norm regularization is proposed, the present work is devoted to regularization-free estimation. While rank-one measurements as in (3) are also in the center of interest herein, our framework is not limited to this specific case.\nIn [5], rank-one measurements are considered for general \u03a3\u2217 \u2208 Rm1\u00d7m2 . Specializing to \u03a3\u2217 \u2208 Sm+ , the authors discuss an application of (3) to covariance matrix estimation given only one-dimensional projections {x\u22a4i zi}ni=1 of the data points, where the {zi}ni=1 are i.i.d. from a distribution with zero mean and covariance matrix \u03a3\u2217. In fact, when using observations yi = (x \u22a4 i zi) 2, one obtains\n(x\u22a4i zi) 2 = x\u22a4i ziz \u22a4 i xi = x \u22a4 i \u03a3 \u2217xi + \u03b5i, with \u03b5i = x \u22a4 i {ziz\u22a4i \u2212 \u03a3\u2217}xi, i = 1, . . . , n. (4)\nOn the other hand, in [5], no specific attention is given to the spd constraint: the convex program proposed therein, which can be seen as a modification of the approach in [10], applies to general symmetric matrices and does not enforce positive semidefiniteness.\nSpecializing model (3) further to the case in which also \u03a3\u2217 = \u03c3\u2217(\u03c3\u2217)\u22a4 has rank one,\none obtains the quadratic model\nyi = |x\u22a4i \u03c3\u2217|2 + \u03b5i (5)\nwhich (with complex-valued \u03c3\u2217) is relevant to the problem of phase retrieval [17] that has received some attention recently. The approach of [9] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions. In followup work [6], the authors show a refined recovery result stating that imposing an spd constraint \u2212 without regularization \u2212 suffices. A similar result has been proven independently by [12]. However, the results in both [6] and [12] only concern model (5). In [18], \u03a3\u2217 is assumed to be a complex Hermitian positive semidefinite matrix of unit trace, which is the setting in quantum state tomography. While the setting as well as the measurement matrices under consideration are different from ours, a notable point of contact to our work can be seen in the fact that the negative von Neumann entropy1, which is the proposed regularizer in [18], does not promote low rankedness, but constitutes one possible way of enforcing positive definiteness. At the same time, adaptivity of the approach to low rankedness is established in [18].\nOutline and contributions of the paper. In Section 2, we study statistical properties of constrained least squares estimation (2) in small sample (n < \u03b4m) and low-rank settings. Specifically, we introduce certain geometric conditions associated with the measurements {Xi}ni=1 that allow us to derive non-asymptotic upper bounds on the prediction and estimation error indicating that (2) can achieve competitive performance while being regularization-free. On the other hand, we show that without extra conditions on the measurements {Xi}ni=1, the performance of (2) can be as poor as that of unconstrained least squares. Section 3 contains numerical results based on synthetic and real world data that support or complement our theoretical results. Our findings are briefly summarized in Section 4. The appendix contains the proofs.\nNotation. We here gather notation and terminology used throughout the paper. For an integer d \u2265 1, let Md denote the Euclidean vector space of real d \u00d7 d matrices with inner product \u3008M,M \u2032\u3009 := tr(M\u22a4M \u2032), M,M \u2032 \u2208 Md. The set of real symmetric d\u00d7 d matrices Sd is a subspace of Md of dimension \u03b4d := d(d+1)/2. Each element M of Sd has an eigen-decomposition M = U\u039bU\u22a4 = \u2211d j=1 \u03bbj(M)uju \u22a4 j , where \u03bb1(M) = \u03bbmax(M) \u2265 \u03bb2(M) \u2265 . . . \u2265 \u03bbd(M) = \u03bbmin(M) is the sequence of real eigenvalues with corresponding orthonormal eigenvectors {uj}dj=1, \u039b = diag(\u03bb1(M), . . . , \u03bbd(M)), and U = [u1 . . . ud]. For q \u2208 [1,\u221e], Sd can be endowed with a norm given by the mapping M 7\u2192 \u2016M\u2016q := (\u2211d j=1 |\u03bbj(M)|q )1/q called the Schatten-q-norm. In particular, for q = 1 we speak of the nuclear norm, while q = 2 yields the Frobenius norm \u2016\u00b7\u2016F . We set \u2016M\u2016\u221e := max1\u2264j\u2264d |\u03bbj(M)|, the spectral norm of M . We denote by S1(d) = {M \u2208 Sd : \u2016M\u2016q = 1} the Schatten-1-norm unit sphere and set S+1 (d) = S1(d) \u2229 Sd+, where Sd+ = {M \u2208 Sd : v\u22a4Mv \u2265 0 \u2200v \u2208 Rd} is the positive semidefinite cone in Sd. The symbols , ,\u227b,\u227a are understood with respect to the semidefinite ordering, e.g. M M \u2032 means that M \u2032 \u2212 M \u2208 Sd+. For v \u2208 Rd and q \u2208 [1,\u221e], \u2016v\u2016q denotes the usual q-norm. For set A,B and a real number \u03b1, \u03b1A := {\u03b1a, a \u2208 A}, A\u2212B = {a\u2212 b, a \u2208 A, b \u2208 B}, and for A,B \u2282 Rd, dist(A,B) := mina\u2208A,b\u2208B\u2016a\u2212 b\u20162.\nIt is convenient to re-write model (1) as\ny = X (\u03a3\u2217) + \u03b5,\nwhere y = (yi) n i=1, \u03b5 = (\u03b5i) n i=1 and X : Mm \u2192 Rn is a linear map defined by (X (M))i = tr(X\u22a4i M), i = 1, . . . , n, referred to as sampling operator. Its adjoint X \u2217 : Rn \u2192 Mm is given by the map v 7\u2192 \u2211n i=1 viXi.\n1The von Neumann entropy of a positive definite Hermitian matrix is given by the entropy of its eigenvalues"}, {"heading": "2 Analysis", "text": "Preliminaries. Throughout this section, we consider a special instance of model (1) in which\nyi = tr(Xi\u03a3 \u2217) + \u03b5i, where \u03a3 \u2217 \u2208 Sm+ , Xi \u2208 Sm, and \u03b5i i.i.d.\u223c N(0, \u03c32), i = 1, . . . , n.\n(6) The assumption that the errors {\u03b5i}ni=1 follow a Gaussian distribution is made for convenience as it simplifies the stochastic part of our analysis, which could be extended to cover error distributions with sub-Gaussian tails.\nNote that without loss of generality, we may assume that the {Xi}ni=1 are symmetric. In fact, any M \u2208 Mm can be decomposed as\nM = M sym +M skew, where M sym = M +M\u22a4\n2 and M skew = M \u2212M\u22a4 2\ndenote the Euclidean projections of M onto Sm and its orthogonal complement (the subspace of skew-symmetric matrices), respectively. Accordingly, since \u03a3\u2217 \u2208 Sm, we have tr(M\u03a3\u2217) = tr(M sym\u03a3\u2217).\nIn the sequel, we study the statistical performance of the constrained least squares estimator\n\u03a3\u0302 \u2208 argmin \u03a3\u2208Sm\n+\n1\n2n \u2016y \u2212X (\u03a3)\u201622 (7)\nunder model (6) with respect to prediction and estimation. More specifically, under certain conditions on X , we shall derive bounds on\n(a) 1\nn \u2016X (\u03a3\u2217)\u2212X (\u03a3\u0302)\u201622, and (b) \u2016\u03a3\u0302\u2212 \u03a3\u2217\u20161, (8)\nwhere (a) will be referred to as \u201cprediction error\u201d below.\nThe most basic method for estimating \u03a3\u2217 is ordinary least squares (ols) estimation\n\u03a3\u0302ols \u2208 argmin \u03a3\u2208Sm\n1\n2n \u2016y \u2212X (\u03a3)\u201622, (9)\nwhich is computationally much simpler than (7). While obtaining (7) requires techniques from convex programming, it is straightforward to compute (9) by solving a linear system of equations in \u03b4m = m(m + 1)/2 variables. On the other hand, the prediction error of ols scales as OP(dim(range(X ))/n), where dim(range(X )) can be as large as min{n, \u03b4m}, in which case the prediction error vanishes asymptotically only if \u03b4m/n \u2192 0 as n \u2192 \u221e. Moreover, the estimation error \u2016\u03a3\u0302ols\u2212\u03a3\u2217\u20161 is unbounded unless n \u2265 \u03b4m. Research conducted over the past few years has consequently focused on methods that deal successfully with the situation n < \u03b4m if the target \u03a3\n\u2217 possesses additional structure, notably low-rankedness. Indeed, if \u03a3\u2217 has rank r \u226a m, the intrinsic dimension of the problem becomes (roughly) mr \u226a \u03b4m. Rank-constrained estimation or regularized estimation with the matrix rank as regularizer yield computationally intractable optimization problems in general. In a large body of work, nuclear norm regularization, which can be seen as a convex surrogate of rank regularization, is considered as a computationally convenient alternative for which a series of adaptivity properties to underlying low-rankedness has been established, e.g. [7, 19, 21, 22, 23]. Complementing (9) with nuclear norm regularization gives rise to the estimator\n\u03a3\u03021 \u2208 argmin \u03a3\u2208Sm\n1\n2n \u2016y \u2212X (\u03a3)\u201622 + \u03bb\u2016\u03a3\u20161, (10)\nwhere \u03bb > 0 is a regularization parameter. In case an spd constraint is imposed (10) becomes\n\u03a3\u03021+ \u2208 argmin \u03a3\u2208Sm\n+\n1\n2n \u2016y \u2212X (\u03a3)\u201622 + \u03bb tr(\u03a3). (11)\nOur analysis aims at elucidating potential advantages of the spd constraint in the constrained least squares problem (7) from a statistical point of view. It turns out that depending on properties of X , the behaviour of \u03a3\u0302 can range from a performance similar to the least squares estimator \u03a3\u0302ols on the one hand to a performance similar to the nuclear norm regularized estimator \u03a3\u03021+ with properly chosen/tuned \u03bb on the\nother hand. The latter case appears to be remarkable inasmuch as \u03a3\u0302 may enjoy similar adaptivity properties as nuclear norm regularized estimators even though \u03a3\u0302 is obtained from a pure data fitting problem without any explicit form of regularization."}, {"heading": "2.1 Negative results", "text": "We first discuss examples of X for which the spd-constrained estimator \u03a3\u0302 does not improve (substantially) over the unconstrained estimator \u03a3\u0302ols. At the same time, these examples provide some clues on conditions that need to be imposed on X to achieve substantially better performance.\nExample 1: equivalence of constrained and unconstrained least squares Let m be even and consider measurement matrices of the form\nXi =\n[ X\u0303i 0\n0 \u2212X\u0303i\n]\nfor matrices X\u0303i \u2208 Sm/2, i = 1, . . . , n. For \u03a3 \u2208 Sm arbitrary, we can partition\n\u03a3 = [ \u03a311 \u03a312 \u03a312 \u03a322, ]\nwhere \u03a311 is the top m/2\u00d7m/2 block of \u03a3 etc. We have\ntr(Xi\u03a3) = tr(X\u0303i(\u03a311 \u2212 \u03a322)), i = 1, . . . , n.\nHence \u03a3 enters the least squares objective (2) via the difference of the top and bottom m/2\u00d7m/2 blocks. Since for any dimension d\n{\u03a3\u2212 \u03a3\u2032 : \u03a3 \u2208 Sd+, \u03a3\u2032 \u2208 Sd+} = Sd = {\u03a3\u2212 \u03a3\u2032 : \u03a3 \u2208 Sd, \u03a3\u2032 \u2208 Sd},\nthe spd constraint becomes vacuous and can be dropped from (7).\nExample 2: Orthonormal design The following statement indicates that for orthonormal design, the prediction error of \u03a3\u0302 cannot be expected to improve over that of \u03a3\u0302ols by substantially more than a constant factor 1/2.\nProposition 1. Let \u03a3\u2217 = 0 so that y = \u03b5, let n = \u03b4m and let {Xi}1\u2264i\u2264\u03b4m be an orthonormal basis of Sm. Then, \u2016X (\u03a3\u0302)\u201622/n \u2192 \u03c3 2 2 in probability as m,n \u2192 \u221e.\nBy contrast, it is desired that \u2016X (\u03a3\u0302)\u201622/n = oP(1) as m,n \u2192 \u221e.\nExample 3: Random Gaussian design Consider the Gaussian orthogonal ensemble (GOE) of random matrices\nGOE(m) = {X = (xjk)1\u2264j,k\u2264m, {xjj}mj=1 i.i.d.\u223c N(0, 1),\n{xjk = xkj}1\u2264j<k\u2264m i.i.d.\u223c N(0, 1/2)}.\nRandom Gaussian measurements are common in compressed sensing-type settings, see e.g. [7, 21]. It is hence of interest to study measurementsXi i.i.d.\u223c GOE(m), i = 1, . . . , n, in the context of the constrained least squares problem (7). The following statement, which follows from results in [2], points to a serious limitation associated with the use of such measurements.\nProposition 2. Consider measurements Xi i.i.d.\u223c GOE(m), i = 1, . . . , n. Then, for any \u03b5 > 0, if n \u2264 (1\u2212 \u03b5)\u03b4m/2, with probability at least 1\u2212 32 exp(\u2212\u03b52\u03b4m), there exists \u2206 \u2208 Sm+ , \u2206 6= 0 such that X (\u2206) = 0.\nProposition 2 has the following implications.\n\u2022 If the number of measurements drops below one half of the ambient dimension \u03b4m, estimating \u03a3\n\u2217 based on (7) becomes ill-posed; the estimation error \u2016\u03a3\u0302\u2212\u03a3\u2217\u20161 is unbounded, irrespective of the rank of \u03a3\u2217.\n\u2022 Geometrically, the consequence of Proposition 2 is that the convex cone CX = {z \u2208 Rn : z = X (\u2206), \u2206 \u2208 Sm+ } contains 0. Unless 0 is contained in the boundary of CX (we conjecture that this event has measure zero), this means that CX = Rn, i.e., the spd constraint becomes vacuous.\nRemarks.\n1. In [32], the following noiseless analog to the constrained least squares problem (7) is considered:\nfind \u03a3 \u2208 Sm+ such thatX (\u03a3) = y = X (\u03a3\u2217), (12)\nwhere Xi \u223c GOE(m), i = 1, . . . , n. The authors prove that for all \u03be \u2208 (0, 1), there exists \u03b1 \u2208 (0, 1) so that if n \u2265 \u03b1\u03b4m, \u03a3\u2217 is the unique solution of the feasibility problem (12) as long as rank(\u03a3\u2217) \u2264 \u03bem. While this implies that the spd constraint allows undersampling (i.e., n < \u03b4m), it is not clear to what extent undersampling is possible, i.e., how small \u03b1 could possibly be. Proposition 2 yields that \u03b1 cannot be smaller than 1/2.\n2. It is of interest to relate Proposition 2 to corresponding results on the vector case (equivalent to having diagonal {Xi}ni=1 and diagonal \u03a3\u2217) in [13]. Compared to Proposition 2, the corresponding result in [13] applies to a much wider class of random measurement matrices including all random matrices with i.i.d. entries from a symmetric distribution around zero. It is thus natural to ask whether Proposition 2 holds more generally for all Wigner matrices [27].\n3. The fact that the threshold 12\u03b4m for the number measurements in Proposition 2 equals (up to the scaling factor \u03c32) the asymptotic prediction error of Example 2 is not a coincidence; this is part of a wider phenomenon as pointed out in [2]. In the framework of [2], 12\u03b4m is the \u201cstatistical dimension\u201d of S m + ."}, {"heading": "2.2 Slow rate bound on the prediction error", "text": "We now turn to the first positive result on the spd-constrained least squares estimator \u03a3\u0302 under an additional condition on the sampling operator X . Specifically, the prediction error will be bounded as\n1 n \u2016X (\u03a3\u2217)\u2212X (\u03a3\u0302)\u201622 = O(\u03bb0\u2016\u03a3\u2217\u20161 + \u03bb20), where \u03bb0 = 1 n \u2016X \u2217(\u03b5)\u2016\u221e (13)\nwith \u03bb0 typically being of the order O( \u221a m/n) (up to logarithmic factors). The rate in (13) can be a significant improvement of what is achieved by \u03a3\u0302ols if \u2016\u03a3\u2217\u20161 = tr(\u03a3\u2217) is small. If \u03bb0 = o(\u2016\u03a3\u2217\u20161) that rate coincides with those of the nuclear norm regularized estimators (10), (11) with regularization parameter \u03bb \u2265 \u03bb0, cf. Theorem 1 in [23]. For nuclear norm regularized estimators, the rate O(\u03bb0\u2016\u03a3\u2217\u20161) is achieved for any choice of X and is hence slow in the sense that the squared prediction error only decays at the rate n\u22121/2 instead of n\u22121. Therefore, we refer to (13) as \u201cslow rate bound\u201d.\nCondition on X . In order to arrive at a suitable condition to be imposed on X so that (13) can be achieved, it makes sense to re-consider Example 3 to identify possible obstacles. Proposition 2 states that as long as n is bounded away from \u03b4m/2 from above, there is a non-trivial \u2206 \u2208 Sm+ such that X (\u2206) = 0. Equivalently,\ndist(PX , 0) = min \u2206\u2208S+\n1 (m)\n\u2016X (\u2206)\u20162 = 0, where\nPX := {z \u2208 Rn : z = X (\u2206), \u2206 \u2208 S+1 (m)}, and S+1 (m) := {\u2206 \u2208 Sm+ : tr(\u2206) = 1}.\nIn this situation, it is in general not possible to derive a non-trivial upper bound on the prediction error as dist(PX , 0) = 0 may imply that CX = Rn in which case \u2016X (\u03a3\u2217) \u2212 X (\u03a3\u0302)\u201622 = \u2016\u03b5\u201622. To rule this out, the condition dist(PX , 0) > 0 appears to be a natural requirement. More strongly, one may ask for the following:\nThere exists a constant \u03c4 > 0 such that \u03c420 (X ) := min \u2206\u2208S+\n1 (m)\n1 n \u2016X (\u2206)\u201622 \u2265 \u03c42. (14)\nThis condition is sufficient to obtain a slow rate bound in the vector case, cf. Theorem 1 in [25]. However, the condition required for the slow rate bound in Theorem 1 below is somewhat stronger than (14).\nCondition 1. There exist constants R\u2217 > 1 and \u03c4\u2217 > 0 such that \u03c4 2(X , R\u2217) \u2265 \u03c42\u2217 , where for R \u2208 R\n\u03c42(X , R) = dist2(RPX ,PX )/n = min A\u2208RS+\n1 (m)\nB\u2208S+ 1 (m)\n1 n \u2016X (A)\u2212X (B)\u201622.\nIt follows from\n\u03c42(X , R) = min A\u2208RS+\n1 (m)B\u2208S+ 1 (m)\n1 n \u2016X (A)\u2212X (B)\u201622\n\u2264 min A\u2208S+\n1 (m)\n1 n \u2016X (R \u00b7 A)\u2212X (A)\u201622\n= (R \u2212 1)2 min A\u2208S+\n1 (m)\n1 n \u2016X (A)\u201622 = (R \u2212 1)2\u03c420 (X )\n(15)\nthat Condition 1 is in fact stronger than (14). Below, we provide a sufficient condition on X that implies Condition 1. Proposition 3. Suppose that there exists a \u2208 Rn, \u2016a\u20162 \u2264 1, and constants 0 < \u03c6min \u2264 \u03c6max such that\n\u03bbmin(n \u22121/2X \u2217(a)) \u2265 \u03c6min, and \u03bbmax(n\u22121/2X \u2217(a)) \u2264 \u03c6max.\nThen for any \u03b6 > 1, X satisfies Condition 1 with R\u2217 = \u03b6 \u03c6max\u03c6min and \u03c4 2 \u2217 = (\u03b6 \u2212 1)2\u03c62max.\nThe condition of Proposition 3 can be phrased as having a positive definite matrix in the unit ball of the range of X \u2217, which, after scaling by 1/\u221an, has its smallest eigenvalues bounded away from zero and condition number bounded from above. As a simple example, suppose that X1 = \u221a nI. Invoking Proposition 3 with a = (1, 0, . . . , 0)\u22a4 and \u03b6 = 2, we find that Condition 1 is satisfied with R\u2217 = 2 and \u03c4 2 \u2217 = 1. A more interesting example is random design where the {Xi}ni=1 are (sample) covariance matrices, where the underlying random vectors satisfy appropriate tail or moment conditions.\nCorollary 1. Let \u03c0m be a probability distribution on R m with second moment matrix \u0393 := Ez\u223c\u03c0m [zz \u22a4] satisfying \u03bbmin(\u0393) > 0. Consider the random matrix ensemble\nM(\u03c0m, q) = { 1\nq\nq\u2211\nk=1\nzkz \u22a4 k , {zk}qk=1 i.i.d.\u223c \u03c0m } . (16)\nSuppose that {Xi}ni=1 i.i.d.\u223c M(\u03c0m, q) and let \u0393\u0302n := 1n \u2211n i=1 Xi and 0 < \u01ebn < \u03bbmin(\u0393). Under the event {\u2016\u0393\u2212 \u0393\u0302n\u2016\u221e \u2264 \u01ebn}, X satisfies Condition 1 with\nR\u2217 = 2(\u03bbmax(\u0393) + \u01ebn)\n\u03bbmin(\u0393)\u2212 \u01ebn and \u03c42\u2217 = (\u03bbmax(\u0393) + \u01ebn) 2.\nIt is instructive to spell out Corollary 1 with \u03c0m as the standard Gaussian distribution on Rm. The matrix \u0393\u0302n equals the sample covariance matrix computed from N = n \u00b7 q samples. It is well-known (see e.g. [11]) that for m,N large, \u03bbmax(\u0393\u0302n) and \u03bbmin(\u0393\u0302n) concentrate sharply around (1+ \u03b7n) 2 and (1\u2212 \u03b7n)2, respectively, where \u03b7n = \u221a m/N . Hence, for any \u03b3 > 0, there exists C\u03b3 > 1 so that if N \u2265 C\u03b3m, it holds that R\u2217 \u2264 2+\u03b3. Similar though weaker concentration results for \u2016\u0393\u2212 \u0393\u0302n\u2016\u221e are available for the broad class of distributions \u03c0m having finite fourth moments [30]. When specialized to q = 1, Corollary 1 yields a statement about X made up from random rank-one measurements Xi = zz\n\u22a4, i = 1, . . . , n, cf. (3). The preceding discussion indicates that Condition 1 tends to be satisfied in this case.\nMain result of this subsection. We are now in position to state the following theorem.\nTheorem 1. Suppose that model (6) holds with X satisfying Condition 1 with constants R\u2217 and \u03c4 2 \u2217 . We then have\n1 n \u2016X (\u03a3\u2217)\u2212X (\u03a3\u0302)\u201622 \u2264 max\n{ 2(1 +R\u2217)\u03bb0\u2016\u03a3\u2217\u20161, 2\u03bb0\u2016\u03a3\u2217\u20161 + 8 ( \u03bb0\nR\u2217 \u03c4\u2217\n)2}\nwhere, for any \u00b5 \u2265 0, with probability at least 1\u2212 (2m)\u2212\u00b5\n\u03bb0 \u2264 \u03c3 \u221a (1 + \u00b5)2 log(2m)\nV 2n n , where V 2n = \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nX2i \u2225\u2225\u2225\u2225\u2225 \u221e .\nRemarks.\n1. Under the scalings R\u2217 = O(1) and \u03c4 2 \u2217 = \u2126(1), the bound of Theorem 1 is of the\norder O(\u03bb0\u2016\u03a3\u2217\u20161 + \u03bb20) as announced in (13) at the beginning of this section.\n2. For given X , the quantity \u03c42(X , R) can be evaluated by solving a least squares problem with spd constraints. Hence it is feasible to check in practice whether Condition 1 holds. In fact, the bound of Theorem 1 can be replaced with\nmin R>1 max\n{ 2(1 +R)\u03bb0\u2016\u03a3\u2217\u20161, 2\u03bb0\u2016\u03a3\u2217\u20161 + 8 ( \u03bb0 R\n\u03c4(X , R)\n)2} .\n3. For later reference, it is of interest to evaluate the term V 2n for M(\u03c0m, q) with \u03c0m as the standard Gaussian distribution. It is proved in Appendix F that with high probability, it holds that\nV 2n \u2264 ( 1 + q\u22121/2 + \u221a m/(nq) )2 ( 1 + \u221a m/q + \u221a 4(m/q) logn )2 = O(m log n)\nas long as m = O(nq)."}, {"heading": "2.3 Bound on the estimation error", "text": "In the previous subsection, we did not make any assumptions about \u03a3\u2217 apart from \u03a3\u2217 \u2208 Sm+ . Henceforth, we suppose that \u03a3\u2217 is of low rank 1 \u2264 r \u226a m and study\nthe performance of the constrained least squares estimator (7) for prediction and estimation in such setting.\nPreliminaries. Let \u03a3\u2217 = U\u039bU\u22a4 be the eigenvalue decomposition of \u03a3\u2217, where\nU =\n[ U\u2016 U\u22a5\nm\u00d7 r m\u00d7 (m\u2212 r)\n] [ \u039br 0r\u00d7(m\u2212r)\n0(m\u2212r)\u00d7r 0(m\u2212r)\u00d7(m\u2212r)\n]\nwhere \u039br is diagonal with positive diagonal entries. Consider the linear subspace\nT \u22a5 = {M \u2208 Sm : M = U\u22a5AU\u22a4\u22a5 , A \u2208 Sm\u2212r}.\nFrom U\u22a4\u22a5\u03a3 \u2217U\u22a5 = 0, it follows that \u03a3 \u2217 is contained in the orthogonal complement\nT = {M \u2208 Sm : M = U\u2016B +B\u22a4U\u22a4\u2016 , B \u2208 Rr\u00d7m},\nwhich has dimension mr \u2212 r(r \u2212 1)/2 \u226a \u03b4m if r \u226a m. The image of T under X is denoted by T = {z \u2208 Rn : z = X (M), M \u2208 T}.\nConditions on X . We now introduce the key quantities the bound in this subsection depends on. Separability constant.\n\u03c42(T) = 1\nn dist2 (T ,PX ) , PX := {z \u2208 Rn : z = X (\u2206), \u2206 \u2208 T\u22a5 \u2229 S+1 (m)}\n= min \u0398\u2208T, \u039b\u2208S+\n1 (m)\u2229T\u22a5\n1 n \u2016X (\u0398)\u2212X (\u039b)\u201622\nRestricted eigenvalue.\n\u03c62(T) = min 06=\u2206\u2208T \u2016X (\u2206)\u201622/n \u2016\u2206\u201621 .\nAs indicated by the following statement concerning the noiseless case, for bounding \u2016\u03a3\u0302\u2212 \u03a3\u2217\u2016, it is inevitable to have lower bounds on the above two quantities.\nProposition 4. Consider the trace regression model (1) with \u03b5i = 0, i = 1, . . . , n. Then\nargmin \u03a3\u2208Sm\n+\n1\n2n \u2016X (\u03a3\u2217)\u2212X (\u03a3)\u201622 = {\u03a3\u2217} for all \u03a3\u2217 \u2208 T \u2229 Sm+\nif and only if it holds that \u03c42(T) > 0 and \u03c62(T) > 0.\nCorrelation constant. Moreover, we make use of the following the quantity. It is not yet clear to us whether control of this quantity is intrinsically required, or whether its appearance in our bound is for merely technical reasons.\n\u00b5(T) = max\n{ 1\nn \u3008X (\u2206),X (\u2206\u2032)\u3009 : \u2016\u2206\u20161 \u2264 1,\u2206 \u2208 T, \u2206\u2032 \u2208 S+1 (m) \u2229 T\u22a5\n} .\nWe are now in position to provide a bound on \u2016\u03a3\u0302\u2212 \u03a3\u2217\u20161.\nTheorem 2. Suppose that model (6) holds with \u03a3\u2217 as considered throughout this subsection and let \u03bb0 be defined as in Theorem 1. We then have\n\u2016\u03a3\u0302\u2212 \u03a3\u2217\u20161 \u2264 max { 8\u03bb0\n\u00b5(T)\n\u03c42(T)\u03c62(T)\n( 3\n2 +\n\u00b5(T)\n\u03c62(T)\n) + 4\u03bb0 ( 1\n\u03c62(T) +\n1\n\u03c42(T)\n) ,\n8\u03bb0 \u03c62(T)\n( 1 + \u00b5(T)\n\u03c62(T)\n) ,\n8\u03bb0 \u03c42(T)\n} .\nRemark. Given the above bound on \u2016\u03a3\u0302 \u2212 \u03a3\u2217\u20161, it is possible to obtain an improved bound on the prediction error scaling with \u03bb20 in place of \u03bb0, cf. (31) in Appendix E.\nThe quality of the bound of Theorem 2 depends on how the quantities \u03c42(T), \u03c62(T) and \u00b5(T) scale with n, m and r, which is highly design-dependent. Accordingly, the estimation error in nuclear norm can be non-finite in the worst case and O(\u03bb0r) in the best case.\n\u2022 The quantity \u03c42(T) is specific to the geometry of the constrained least squares problem (7) and hence of critical importance. For instance, it follows from Proposition 2 that for standard Gaussian measurements, \u03c42(T) = 0 with high probability once n < 12\u03b4m. The situation can be much better for random spd\nmeasurements (16) as exemplified for measurements Xi = ziz \u22a4 i with zi i.i.d.\u223c N(0, I) in the subsequent section. Specifically, it turns out that \u03c42(T) = \u2126(1/r) as long as n = \u2126(m \u00b7 r).\n\u2022 It is not restrictive to assume that the quantity \u03c62(T) is positive. Indeed, without that assumption, even an oracle estimator based on knowledge of the subspace T would fail. Reasonable sampling operators X have rank min{n, \u03b4m} so that the nullspace of X only has a trivial intersection with the subspace T as long as n \u2265 dim(T) = mr \u2212 r(r \u2212 1)/2.\n\u2022 For fixed T, computing \u00b5(T) entails solving a biconvex (albeit non-convex) optimization problem in the variables \u2206 \u2208 T and \u2206\u2032 \u2208 S+1 (m) \u2229 T\u22a5. Alternating optimization (also known as block coordinate descent) is a practical approach to such optimization problems for which a globally optimal solution is out of reach. In this manner we explore the scaling of \u00b5(T) numerically as done for \u03c42(T). We find that \u00b5(T) = O(\u03b4m/n) so that \u00b5(T) = O(1) apart from the regime n/\u03b4m \u2192 0, without ruling out the possibility of undersampling, i.e. n < \u03b4m."}, {"heading": "3 Numerical results", "text": "In this section, we provide a series of empirical results regarding properties of the estimator \u03a3\u0302. In particular, its performance relative to regularization-based methods is explored. We also present an application to spiked covariance estimation for the CBCL face image data set and stock prices from NASDAQ.\n3.1 Scaling of the constant \u03c4 2(T)\nFor X and T given, it is possible to evaluate \u03c42(T) by solving a convex optimization problem. This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.\nWe here consider sampling operators with random i.i.d. measurements Xi = ziz \u22a4 i , where zi \u223c N(0, I) is a standard Gaussian random vector in Rm (equivalently, Xi follows a Wishart distribution) , i = 1, . . . , n. We expect \u03c42(T) to behave similarly for random rank-one measurements of the same form as long as the underlying probability distribution has finite fourth moments, and thus for (a broad subclass of) the ensemble M(\u03c0m, q) (16).\nIn order to explore the scaling of \u03c42(T) with n, m and r, we fix m \u2208 {30, 50, 70, 100}. For each choice of m, we vary n = \u03b1\u03b4m, where a grid of 20 values ranging from 0.16 to 1.1 is considered \u03b1. For r, we consider the grid {1, 2, . . . ,m/5}. For each combination\nof m, n, and r, we use 50 replications. Within each replication, the subspace T is generated randomly from the eigenspace associated with the non-zero eigenvalues of a random matrix G\u22a4G, where the entries of the m\u00d7 r matrix G are i.i.d. N(0, 1). The results point to the existence of a phase transition as it is typical for problems related to that under study [2]. Specifically, it turns out that the scaling of \u03c42(T) can be well described by the relation\n\u03c42(T) \u2248 \u03c6m,n max{1/r \u2212 \u03b8m,n, 0}, (17)\nwhere \u03c6m,n, \u03b8m,n > 0 depend on m and n. In order to arrive at model (17), we first obtain the 5%-quantile as summary statistic of the 50 replications associated with each triple (n,m, r). At this point, note that the use of the mean as a summary statistic is not appropriate as it may mask the fact that the majority of the observations are zero. For each pair of (n,m), we then identify all values of r for which the corresponding 5%-quantile drops below 10\u22126, which serves as effective zero here. For the remaining values, we fit model (17) using nonlinear least squares (working on a log scale). Figure 1 shows that model (17) provides a rather accurate description of the given data. Concerning \u03c6m,n and \u03b8m,n, the scalings \u03c6m,n = \u03c60n/m and \u03b8m,n = \u03b80m/n for constants \u03c60, \u03b80 > 0 appear to be reasonable. This gives rise to the requirement n > \u03b80(mr) for exact recovery to be possible in the noiseless case (cf. Proposition 4) and yields that \u03c42(T) = \u2126(1/r) as long as n = \u2126(mr),"}, {"heading": "3.2 Comparison with regularization-based approaches", "text": "In this subsection, we empirically evaluate \u2016\u03a3\u0302\u2212\u03a3\u2217\u20161 relative to regularization-based methods proposed in the literature.\nSetup. We consider Wishart measurement matrices as in the previous subsection. Again, we expect a similar behaviour for (most) other random designs from ensemble M(\u03c0m, q). We fix m = 50 and let n \u2208 {0.24, 0.26, . . . , 0.36, 0.4, . . . , 0.56} \u00b7 m2 and r \u2208 {1, 2, . . . , 10} vary. For each configuration of n and r, we consider 50 replications. In each of these replications, we generate data\nyi = tr(Xi\u03a3 \u2217) + \u03c3\u03b5i, \u03c3 = 0.1, i = 1, . . . , n, (18)\nwhere \u03a3\u2217 is generated as the sum of rWishart matrices and the {\u03b5i}ni=1 are i.i.d.N(0, 1).\nRegularization-based approaches. We compare \u03a3\u0302 to the corresponding nuclear norm regularized estimator in (11). Regarding the choice of the regularization parameter \u03bb, we consider the grid \u03bb\u2217\u00b7{0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 4, 8, 16}, where \u03bb\u2217 = \u03c3 \u221a m/n as recommended in [21] and pick \u03bb so that the prediction error on a separate validation data set of size n generated from (18) is minimized. Note that in general, neither \u03c3 is known nor an extra validation data set is available. Our goal here is to ensure that the regularization parameter is properly tuned. In addition, we consider an oracular choice of \u03bb where \u03bb is picked from the above grid such that the performance measure of interest (the distance to the target in the nuclear norm) is minimized. We also compare to the constrained nuclear norm minimization approach of Chen et al. [10]\ngiven by min \u03a3 tr(\u03a3) subject to \u03a3 0, and \u2016y \u2212X (\u03a3)\u20161 \u2264 \u03bb. (19) For the parameter \u03bb, we consider the grid n\u03c3 \u221a\n2/\u03c0 \u00b7{0.2, 0.3, . . . , 1, 1.25}. This specific choice is motivated by the observation that E[\u2016y \u2212 X (\u03a3\u2217)\u20161] = E[\u2016\u03b5\u20161] = n\u03c3 \u221a 2/\u03c0. Apart from that, tuning of \u03bb is performed as for the nuclear norm regularized estimator. In addition, we have assessed the performance of the approach in [5], which does not impose an spd constraint but adds one more constraint to the formulation (19). That additional constraint significantly complicates optimization of the problem and yields a second tuning parameter. Therefore, instead of doing a grid search over a 2Dgrid, we use fixed values as specified in [5] given the knowledge of \u03c3. The results are similar or worse than those of (19) (note in particular that positive semidefiniteness is not taken advantage of in the approach of [5]) and are hence not reported here.\nDiscussion of the results. We can conclude from Figure 2 that in most cases, the performance of the constrained least squares estimator does not differ much from that of the regularization-based methods with careful parameter tuning, which are not too far from the oracle. However, for larger values of r, the constrained least squares estimator seems to require slightly more measurements to achieve competitive performance."}, {"heading": "3.3 Real data examples", "text": "We conclude this section by presenting an application to recovery of spiked covariance matrices, a notion due to [16]. Background. A spiked covariance matrix is of the form \u03a3\u2217 = \u2211r\nj=1 \u03bbjuju \u22a4 j + \u03c3 2I,\nwhere r \u226a m and \u03bbj \u226b \u03c32 > 0, j = 1, . . . , r. Note that for data {zi}ni=1 following the factor model\nzi = r\u2211\nj=1\n\u03b1ijfj + \u03c3\u03bei, \u03bei \u223c N(0, I), (20)\nfor orthogonal factors {fj}rj=1 and random coefficients \u03b1ij \u223c N(0, \u03bbj) independent from \u03bei, the population covariance matrix E[ziz \u22a4 i ], i = 1, . . . , n, is of the form given above. Model (20) is one possible way to motivate principal components analysis (PCA); this connection explains the relevance and the popularity of spiked covariance models.\nExtension to the spiked case. So far, we have assumed that the target \u03a3\u2217 is of low rank, but it is straightforward to extend the proposed approach to the case in which \u03a3\u2217 is spiked as long as \u03c32 is known or an estimate is available. A constrained least squares estimator of \u03a3\u2217 takes the form \u03a3\u0302 + \u03c32I, where\n\u03a3\u0302 \u2208 argmin \u03a3\u2208Sm\n+\n1\n2n \u2016y \u2212X (\u03a3 + \u03c32I)\u201622. (21)\nData sets. (1) The CBCL facial image data set [1] consist of N = 2429 images of 19 \u00d7 19 pixels (i.e., m = 361). We take \u03a3\u2217 as the sample covariance matrix of this data set. It turns out that \u03a3\u2217 can be well approximated by \u03a3r, r = 50, where \u03a3r is the best rank r approximation to \u03a3\u2217 obtained from computing its eigendecomposition and setting to zero all but the top r eigenvalues. (2) We construct a second data set from the daily end prices of m = 252 stocks from the technology sector in NASDAQ, starting from the beginning of the year 2000 to the end of the year 2014 (in total N = 3773 days, retrieved from finance.yahoo.com). We take \u03a3\u2217 as the resulting sampling correlation matrix and choose r = 100.\nExperimental setup. As in all preceding measurements, we consider n random Wishart measurements for the operator X , where n = C(mr), where C ranges from\n0.25 to 12. Since \u2016\u03a3r\u2212\u03a3\u2217\u2016F /\u2016\u03a3\u2217\u2016F \u2248 10\u22123 for both data sets, we work with \u03c32 = 0 in (21) for simplicity. To make the problem of recovering \u03a3\u2217 more difficult, we introduce additional noise to the problem by using observations\nyi = tr(XiSi), i = 1, . . . , n, (22)\nwhere Si is an approximation to \u03a3 \u2217 obtained from the sample covariance respectively sample correlation matrix of \u03b2N data points randomly sampled with replacement from the entire data set, i = 1, . . . , n, where \u03b2 ranges from 0.4 to 1/N (Si is computed from a single data point). For each choice of n and \u03b2, 20 replications are considered. The reported results are averages over these replications.\nResults. For the CBCL data set, it can be seen from Figure 3 and Table 1, that \u03a3\u0302 accurately approximates \u03a3\u2217 (within a factor of three of the best rank-r approximation \u03a3r) once the number of measurements crosses 2mr. Performance degrades once additional noise is introduced to the problem by using measurements (22) that are taken from a perturbed version of \u03a3\u2217. Even under significant perturbations (\u03b2 = 0.08), reasonable reconstruction of \u03a3\u2217 remains possible, albeit the number of required measurements increases accordingly. In the extreme case \u03b2 = 1/N , the error is still decreasing with n, but millions of samples seems to be required to achieve reasonable reconstruction error (for computational reasons, we stop at n = 12mr \u2248 216, 000). The general picture is similar for the NASDAQ data set, but the difference between using measurements based on the full sample correlation matrix on the one hand and approximations based on random subsampling (22) on the other hand are more pronounced. For \u03b2 = 1, the reduction in error with increasing n progresses visibly faster as for the first data set, and a smaller error relative to \u03a3r close to 1 is achieved."}, {"heading": "4 Conclusion", "text": "In this paper, we have investigated trace regression in the situation that the underlying matrix is symmetric positive semidefinite. We have shown that under certain restrictions on the design, the constrained least squares estimator enjoys excellent statistical properties similar to methods employing nuclear norm regularization. This may come as a surprise, as regularization is widely regarded as necessary in small sample settings. On the application side, we have pointed out the usefulness of our findings for recovering spiked covariance matrices from quadratic measurements."}, {"heading": "Acknowledgement", "text": "The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137."}, {"heading": "A Proof of Proposition 1", "text": "By rotational invariance of the Gaussian distribution of \u03b5, it suffices to consider the canonical orthonormal basis of Sm given by\nX1 = e1e \u22a4 1 , X2 = 1\u221a 2 (e1e \u22a4 2 + e2e \u22a4 1 ), . . . , Xm = 1\u221a 2 (e1e \u22a4 m + eme \u22a4 1 ), Xm+1 = e2e \u22a4 2 , Xm+2 = 1\u221a 2 (e2e \u22a4 3 + e3e \u22a4 2 ), . . . , X\u03b4m\u22121 = 1\u221a 2 (em\u22121e \u22a4 m + eme \u22a4 m\u22121), X\u03b4m = eme \u22a4 m,\nwhere {ej}mj=1 denote the canonical basis vectors of Rm. Equivalently, the corresponding map X : Sm \u2192 R\u03b4m equals the symmetric vectorization operator\n\u03a3 = (\u03c3jk) 7\u2192 (\u03c311, \u221a 2\u03c312, . . . , \u221a 2\u03c31m, \u03c322, \u221a 2\u03c323, . . . , \u221a 2\u03c3(m\u22121)m, \u03c3mm) \u22a4 (23)\nAccordingly, denote by {\u03b5jk}1\u2264j\u2264k\u2264m the error terms corresponding to the entries {\u03c3jk}1\u2264j\u2264k\u2264m. The minimization problem (7) can hence be expressed as\nmin \u03a3\u2208Sm\n+\n1\n2n    m\u2211\nj=1\n(\u03b5jj \u2212 \u03c3jj)2 + \u2211\nj<k\n(\u03b5jk \u2212 \u221a 2\u03c3jk) 2   \n= min \u03a3\u2208Sm\n+\n1\n2n    m\u2211\nj=1\n(\u03b5jj \u2212 \u03c3jj)2 + 2 \u2211\nj<k\n( \u03b5jk\u221a 2 \u2212 \u03c3jk )2   \n= min \u03a3\u2208Sm\n+\n\u2016E \u2212 \u03a3\u20162F , (24)\nwhere the matrix E = X \u2217(\u03b5) has entries Ejj = \u03b5jj , j = 1, . . . ,m, and Ejk = \u03b5jk/ \u221a 2, j, k = 1, . . . ,m, j 6= k. Now observe that the minimizer \u03a3\u0302 of (24) coincides with the Euclidean projection of E on Sm+ . It is well-known [3] that the projection of a symmetric matrix on the positive semidefinite cone is obtained by setting all its negative eigenvalues to zero, i.e., in terms of the eigendecomposition of E = \u2211p j=1 \u03bbj(E)u \u22a4 j u \u22a4 j , we have\n\u03a3\u0302 =\nm\u2211\nj=1\nmax{\u03bbj(E), 0}uju\u22a4j .\nAt this point, we note that E is a Wigner matrix, whose empirical distribution of its eigenvalues follows Wigner\u2019s semicircle law as m \u2192 \u221e (cf. [27]), which is symmetric\naround zero. Consequently, we have\n\u2016X (\u03a3\u0302)\u201622 = \u2016\u03a3\u0302\u20162F = m\u2211\nj=1\n{\u03bbj(E), 0}2 \u2192 1\n2 \u2016E\u20162F \u2192\n\u03c32\n2 \u03b4m in probability as m \u2192 \u221e."}, {"heading": "B Proof of Proposition 2", "text": "The proof of Proposition 2 follows from results in [2].\nDefinition B.1. Let C \u2286 Rd be a convex cone. The statistical dimension of C is defined as \u03b4(C) = E[\u2016\u03a0Cg\u201622], where \u03a0C denotes the Euclidean projection onto C and the entries of g are i.i.d. N(0, 1).\nTheorem B.1. [2] Let f : Rd \u2192 R\u222a{\u2212\u221e,+\u221e} be a proper convex function. Suppose that A \u2208 Rn\u00d7d has i.i.d. N(0, 1) entries, and let z0 = Ax0 for a fixed x0 \u2208 Rd. Consider the convex optimization problem\nminimize f(x) subject to Ax = z0. (25)\nand let D(f, x0) = \u22c3\nt>0{v \u2208 Rd : f(x0 + tv) \u2264 f(x0)} denote the descent cone of f at x0. Then, for any \u03b5 > 0, if n \u2264 (1 \u2212 \u03b5)\u03b4(D(f, x0)), with probability at least 1\u2212 32 exp(\u2212\u03b52\u03b4m), x0 fails to be the unique solution of (25).\nProof. (Proposition 2). Denote by svec : Sm \u2192 R\u03b4m the symmetric vectorization map (cf. (23)), which is an isometry with respect to the Euclidean inner product on Sm and R\u03b4m , and by svec\u22121 : R\u03b4m \u2192 Sm its inverse. We can then apply Theorem B.1 to the setting of Proposition 2 by using\nd = \u03b4m, x = svec(\u03a3), x0 = 0, f(x) = \u03b9Sm + (svec\u22121(x)), A =\n  svec(X1)\n... svec(Xn)\n  ,\nwhere \u03b9Sm + is the convex indicator function of Sm+ which takes the value 0 if its argument is contained in Sm+ and +\u221e otherwise. Observe that D(f, 0) = Sm+ . It is shown in [2], Proposition 3.2, that the statistical dimension \u03b4(Sm+ ) = \u03b4m/2. This concludes the proof."}, {"heading": "C Proof of Proposition 3", "text": "Proposition 3 follows from the dual problem of the convex optimization problem associated with \u03c42(X , R). Below, it will be shown that the Lagrangian dual of the optimization problem\nmin A,B\n1\nn1/2 \u2016X (A)\u2212X (B)\u20162\nsubject to A 0, B 0, tr(A) = R, tr(B) = 1. (26)\nis given by\nmax \u03b8,\u03b4,a\n\u03b8 \u00b7 R\u2212 \u03b4\nsubject to X \u2217(a)\u221a n \u03b8I, X \u2217(a)\u221a n \u03b4I, \u2016a\u20162 \u2264 1. (27)\nThe assertion of Proposition 3 follows immediately from (27) by identifying \u03b8 = \u03bbmin(n\n\u22121/2X \u2217(a)) and \u03b4 = \u03bbmax(n\u22121/2X \u2217(a)). In the remainder of the proof, duality of (26) and (27) is established. Using the shortcut X\u0303 = X/\u221an, the Lagrangian of the dual problem (27) is given by\nL(\u03b8, \u03b4, a;A,B, \u03ba) = \u03b8 \u00b7R\u2212 \u03b4 + \u2329 X\u0303 \u2217(a)\u2212 \u03b8I, A \u232a \u2212 \u2329 X\u0303 \u2217(a)\u2212 \u03b4I, B \u232a \u2212 \u03ba(\u2016a\u201622 \u2212 1).\nTaking derivatives w.r.t. \u03b8, \u03b4, r and the setting the result equal to zero, we obtain from the KKT conditions that a primal-dual optimal pair (\u03b8\u0302, \u03b4\u0302, a\u0302, A\u0302, B\u0302, \u03ba\u0302) obeys\ntr(A\u0302) = R, tr(B\u0302) = 1, X\u0303 (A\u0302)\u2212 X\u0303 (B\u0302)\u2212 \u03ba\u03022a\u0302 = 0. (28)\nTaking the inner product of the rightmost equation with a\u0302, we obtain\n\u2329 a\u0302, X\u0303 (A\u0302)\u2212 X\u0303 (B\u0302) \u232a \u2212 \u03ba\u03022\u2016a\u0302\u201622 = 0.\n\u21d4 \u2329 X\u0303 \u2217(a\u0302), A\u0302\u2212 B\u0302 \u232a \u2212 \u03ba\u03022\u2016a\u0302\u201622 = 0.\n\u21d4 \u03b8\u0302 tr(A\u0302)\u2212 \u03b4\u0302 tr(B\u0302)\u2212 \u03ba\u03022\u2016a\u0302\u201622 = 0. \u21d4 \u03b8\u0302R\u2212 \u03b4\u0302 = \u03ba\u03022\u2016a\u0302\u201622,\nwhere the second equivalence is by complementary slackness. Consider first the case \u03b8\u0302R\u2212 \u03b4\u0302 > 0. This entails \u03ba\u0302 > 0 and thus \u2016a\u0302\u201622 = 1, so that 2\u03ba\u0302 = \u03b8\u0302R\u2212 \u03b4\u0302. Substituting this result into the rightmost equation in (28) and taking norms, we obtain\n\u03b8\u0302R\u2212 \u03b4\u0302 = \u2016X\u0303 (A\u0302)\u2212 X\u0303 (B\u0302)\u20162 = 1\u221a n \u2016X (A\u0302)\u2212X (B\u0302)\u20162. (29)\nFor the second case, note that \u03b8\u0302R\u2212 \u03b4\u0302 cannot be negative as a = 0 is feasible for (27). Thus, \u03b8\u0302R \u2212 \u03b4\u0302 = 0 implies that a\u0302 = 0 and in turn also (29)."}, {"heading": "D Proof of Corollary 1", "text": "The corollary follows from Proposition 3 by choosing a = 1/ \u221a n so that n\u22121/2X \u2217(a) = 1 n \u2211n i=1 Xi, and using that \u2016\u0393 \u2212 \u0393\u0302n\u2016\u221e \u2264 \u01ebn implies that |\u03bbj(\u0393) \u2212 \u03bbj(\u0393\u0302n)| \u2264 \u01ebn, j = 1, . . . ,m ([15], \u00a74.3). The specific values of R\u2217 and \u03c42\u2217 are obtained by choosing \u03b6 = 2 in Proposition 3."}, {"heading": "E Proof of Theorem 1", "text": "The following lemma is a crucial ingredient in the proof. In the sequel, let \u2206\u0302 = \u03a3\u0302\u2212\u03a3\u2217. Let the eigendecomposition of \u2206\u0302 be given by\n\u2206\u0302 =\nm\u2211\nj=1\n\u03bbj(\u2206\u0302)uju \u22a4 j =\nm\u2211\nj=1 max{0, \u03bbj(\u2206\u0302)}uju\u22a4j \ufe38 \ufe37\ufe37 \ufe38\n=:\u2206\u0302+\n+\nm\u2211\nj=1 min{0, \u03bbj(\u2206\u0302)}uju\u22a4j \ufe38 \ufe37\ufe37 \ufe38\n=:\u2206\u0302\u2212\n= \u2206\u0302+ + \u2206\u0302\u2212\n(30)\nLemma E.1. Consider the decomposition (30). We have \u2016\u2206\u0302\u2212\u20161 \u2264 \u2016\u03a3\u2217\u20161.\nProof. Write \u2206\u0302+ = U+\u039b+U \u22a4 + and \u2206\u0302 \u2212 = U\u2212\u039b\u2212U \u22a4 \u2212 for the eigendecompositions of \u2206\u0302 +\nand \u2206\u0302\u2212, respectively. Since \u03a3\u0302 0, we must have tr(\u03a3\u0302U\u2212U\u22a4\u2212 ) \u2265 0 and thus\n0 \u2264 tr(\u03a3\u0302U\u2212U\u22a4\u2212 ) = tr(U\u22a4\u2212 \u03a3\u0302U\u2212) = tr(U\u22a4\u2212 (\u03a3 \u2217 + \u2206\u0302)U\u2212)\n= tr(U\u22a4\u2212 (\u03a3 \u2217 + U+\u039b+U \u22a4 + + U\u2212\u039b\u2212U \u22a4 \u2212 )U\u2212) = tr(\u03a3\u2217U\u2212U \u22a4 \u2212 ) + tr(\u039b\u2212),\nwhere for the last identity, we have used that U\u22a4+U\u2212 = 0. It follows that\n\u2016\u2206\u0302\u2212\u20161 = \u2016\u039b\u2212\u20161 = \u2212 tr(\u039b\u2212) \u2264 tr(\u03a3\u2217U\u2212U\u22a4\u2212 ) \u2264 \u2016\u03a3\u2217\u20161\u2016U\u2212U\u22a4\u2212 \u2016\u221e = \u2016\u03a3\u2217\u20161.\nEquipped with Lemma E.1, we turn to the proof of Theorem 1.\nProof. (Theorem 1) By definition of \u03a3\u0302, we have \u2016y \u2212X (\u03a3\u0302)\u201622 \u2264 \u2016y \u2212X (\u03a3\u2217)\u201622. Using (6) and the definition of \u2206\u0302, we obtain after re-arranging terms that\n1 n \u2016X (\u2206\u0302)\u201622 \u2264 2 n\n\u2329 \u03b5,X (\u2206\u0302) \u232a = 2\nn\n\u2329 X \u2217(\u03b5), \u2206\u0302 \u232a\n\u21d2 1 n \u2016X (\u2206\u0302)\u201622 \u2264 2\u2016X \u2217(\u03b5)/n\u2016\u221e\u2016\u2206\u0302\u20161 = 2\u03bb0(\u2016\u2206\u0302+\u20161 + \u2016\u2206\u0302\u2212\u20161), (31)\nwhere we have used Ho\u0308lder\u2019s inequality, the decomposition of \u2206\u0302 as in Lemma E.1 and \u03bb0 = \u2016X \u2217(\u03b5)/n\u2016\u221e. We now upper bound the l.h.s. of (31) by invoking Condition 1 and Lemma E.1, which yields \u2016\u2206\u0302\u2212\u20161 \u2264 \u2016\u03a3\u2217\u20161. If \u2016\u2206\u0302+\u20161 \u2264 R\u2217\u2016\u2206\u0302\u2212\u20161, we have\n1 n \u2016X (\u03a3\u0302)\u2212X (\u03a3\u2217)\u201622 = 1 n \u2016X (\u2206\u0302)\u201622 \u2264 2(R\u2217 + 1)\u03bb0\u2016\u03a3\u2217\u20161,\nwhich is the first part in the maximum of the bound to be established. In the opposite case, suppose first that \u2016\u2206\u0302\u2212\u20161 > 0 (the case \u2016\u2206\u0302\u2212\u20161 = 0 is discussed at the end of this proof) and we have \u2016\u2206\u0302+\u20161/\u2016\u2206\u0302\u2212\u20161 = R\u0302 > R\u2217 > 1. Consequently,\n1 n \u2016X (\u2206\u0302)\u201622 = 1 n \u2016X (\u2206\u0302+)\u2212X (\u2212\u2206\u0302\u2212)\u201622\n= \u2016\u2206\u0302\u2212\u201621 1\nn \u2225\u2225\u2225\u2225\u2225X ( \u2206\u0302+\n\u2016\u2206\u0302\u2212\u20161\n) \u2212X ( \u2212\u2206\u0302\u2212\n\u2016\u2206\u0302\u2212\u20161\n)\u2225\u2225\u2225\u2225\u2225 2\n2\n\u2265 \u2016\u2206\u0302\u2212\u201621 min A\u2208R\u0302S+\n1 (m)\nB\u2208S+ 1 (m)\n1 n \u2016X (A) \u2212X (B)\u201622\n= \u03c42(X , R\u0302)\u2016\u2206\u0302\u2212\u201621 = \u03c42(X , R\u0302) \u2016\u2206\u0302+\u201621 R\u03022\nInserting this into (31), we obtain the following upper bound on \u2016\u2206\u0302+\u20161.\n\u03c42(X , R\u0302) R\u03022 \u2016\u2206+\u201621 \u2264 2\u03bb0 R\u0302 + 1 R\u0302 \u2016\u2206\u0302+\u20161\n\u21d2 \u2016\u2206\u0302+\u20161 \u2264 2\u03bb0 R\u0302(R\u0302+ 1)\n\u03c42(X , R\u0302) \u2264 4\u03bb0\nR\u03022\n\u03c42(X , R\u0302) \u2264 4\u03bb0 R2\u2217 \u03c42\u2217 ,\nwhere the last inequality follows from the observation that for any R \u2265 R\u2217\n\u03c42(X , R) \u2265 (R/R\u2217)2\u03c42(X , R\u2217),\nwhich can be easily seen from the dual problem (27) associated with \u03c42(X , R). Substituting the above bound on \u2016\u2206\u0302+\u20161 into (31) and using the bound \u2016\u2206\u0302\u2212\u20161 \u2264 \u2016\u03a3\u2217\u20161 yields the second part in the maximum of the desired bound. To finish the proof, we still need to address the case \u2016\u2206\u0302\u2212\u20161 = 0. Recalling the definition of the quantity \u03c420 (X ) in (14), we bound\n1 n \u2016X\u0302(\u2206\u0302)\u201622 = 1 n \u2016X\u0302(\u2206\u0302+)\u201622 \u2265 \u03c420 (X )\u2016\u2206\u0302+\u201621.\nInserting this into (31), we obtain from (15)\n\u2016\u2206\u0302+\u20161 \u2264 2\u03bb0 \u03c420 (X ) \u2264 2\u03bb0(R\u2217 \u2212 1) 2 \u03c42\u2217 .\nBack-substitution into (31) yields a bound that is implied by that of Theorem 1. This concludes the proof.\nBound on \u03bb0. The bound on \u03bb0 is an application of Theorem 4.6.1 in [29]. Theorem E.1. [29] Consider a sequence {Xi}ni=1 of fixed matrices in Sm and let {\u03b5i}ni=1 i.i.d.\u223c N(0, \u03c32). Then for all t \u2265 0\nP (\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03b5iXi \u2225\u2225\u2225\u2225\u2225 \u221e \u2265 t ) \u2264 2m exp(\u2212t2/(2\u03c32V 2)), V 2 := \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 X2i \u2225\u2225\u2225\u2225\u2225 \u221e .\nChoosing t = \u03c3V \u221a (1 + \u00b5)2 log(2m) yields the desired bound."}, {"heading": "F Proof of Theorem 1, Remark 3", "text": "The bound hinges on the following concentration result for the extreme eigenvalues of the sample covariance of a Gaussian sample.\nTheorem F.1. [11] Let z1, . . . , zN be an i.i.d. sample from N(0, Im) and let \u0393N = 1 N \u2211N i=1 ziz \u22a4 i . We then have for any \u03b4 > 0\nP ( \u03bbmax ( 1\nN \u0393N\n) > ( 1 + \u03b4 + \u221a m\nN\n)2) \u2264 exp(\u2212N\u03b42/2).\nIn the proof, we also make use of the following fact.\nLemma F.1. Let {Xi}ni=1 \u2282 Sm+ . Then\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nX2i \u2225\u2225\u2225\u2225\u2225 \u221e \u2264 max 1\u2264i\u2264n \u2016Xi\u2016\u221e \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 Xi \u2225\u2225\u2225\u2225\u2225 \u221e .\nProof. First note that for any v \u2208 Rm and any M \u2208 Sm+ , we have that\nv\u22a4M2v =\nm\u2211\nj=1\n\u03bb2j(M)(u \u22a4 j v) 2 \u2264 \u03bbmax(M) m\u2211\nj=1\n\u03bbj(M)(u \u22a4 j v) 2 = \u2016M\u2016\u221ev\u22a4Xv,\nwhere {uj}mj=1 are the eigenvectors of X . Accordingly, we have \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nX2i \u2225\u2225\u2225\u2225\u2225 \u221e = max \u2016v\u20162=1 v\u22a4 n\u2211 i=1 X2i v \u2264 max 1\u2264i\u2264n \u2016Xi\u2016\u221e max \u2016v\u20162=1 v\u22a4 n\u2211 i=1 Xiv\n= max 1\u2264i\u2264n\n\u2016Xi\u2016\u221e \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 \u221e .\nWe now establish the bound to be shown. Each measurement matrix can be expanded as\nXi = 1\nq\nq\u2211\nk=1\nzikz \u22a4 ik, {zik}qk=1 i.i.d.\u223c N(0, Im), i = 1, . . . , n.\nAccordingly, we have\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nX2i \u2225\u2225\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225\u2225\u2225\u2225 1 n n\u2211 i=1 { 1 q q\u2211 k=1 zikz \u22a4 ik }2\u2225\u2225\u2225\u2225\u2225\u2225 \u221e\n\u2264 max 1\u2264i\u2264n {\u2225\u2225\u2225\u2225\u2225 { 1 q q\u2211\nk=1\nzikz \u22a4 ik }\u2225\u2225\u2225\u2225\u2225 \u221e }\u2225\u2225\u2225\u2225\u2225 1 nq n\u2211 i=1 q\u2211 k=1 zikz \u22a4 ik \u2225\u2225\u2225\u2225\u2225 \u221e\n\u2264 max 1\u2264i\u2264n\n{ \u03bbmax ( 1\nq\nq\u2211\nk=1\nzikz \u22a4 ik )} \u03bbmax(\u0393nq)\nwhere \u0393nq follows the distribution of \u0393N in Theorem F.1 with N = nq. For the first\nterm, applying Theorem F.1 with N = q and \u03b4 = \u221a 4m log(n)/q and using the union bound, we obtain that\nP ( \u03bbmax ( 1\nq\nq\u2211\nk=1\nzikz \u22a4 ik\n) > (\u221a q + \u221a m+ \u221a 4m logn\n\u221a q\n)2) \u2264 exp(\u2212(2m\u2212 1) logn).\nApplying Theorem F.1 to \u0393N with \u03b4 = 1/ \u221a q, we obtain that\nP ( \u03bbmax(\u0393nq) > ( 1 +\n1\u221a q +\n\u221a m\nnq\n)2) \u2264 exp(\u2212n/2).\nCombining the two previous bounds yields the assertion."}, {"heading": "G Proof of Proposition 4", "text": "In the sequel, we write \u03a0T and \u03a0T\u22a5 for the orthogonal projections on T and T \u22a5, respectively. Note first that since the {\u03b5i}ni=1 are zero, any minimizer \u03a3\u0302 satisfies\nX (\u03a3\u0302) = X (\u03a3\u2217) \u21d0\u21d2 X (\u2206\u0302) = 0 \u21d0\u21d2 X (\u2206\u0302T) + X (\u2206\u0302T\u22a5) = 0 (32)\nwhere \u2206\u0302T = \u03a0T\u2206\u0302 and \u2206\u0302T\u22a5 = \u03a0T\u22a5\u2206\u0302, where we recall that \u2206\u0302 = \u03a3\u0302 \u2212 \u03a3\u2217. Note that since \u03a3\u2217 = \u03a0T\u03a3 \u2217, for \u03a3\u0302 to be feasible, it is necessary that \u2206\u0302T\u22a5 0.\nSuppose first that \u03c42(T) = 0. Then there exist \u0398 \u2208 T and \u039b \u2208 S+1 (m) \u2229 T\u22a5 such that X (\u0398) + X (\u039b) = 0. Hence, for any \u03a3\u2217 \u2208 T with \u03a3\u2217 + \u0398 0, the choices \u2206\u0302T = \u0398 and \u2206\u0302T\u22a5 = \u039b ensure that \u03a3\u0302 is feasible and that (32) is satisfied. Since \u039b is contained in the Schatten 1-norm sphere of radius 1, it is necessarily non-zero and thus \u03a3\u0302 6= \u03a3\u2217. If \u03c62(T) = 0, there exists 0 6= \u0398 \u2208 T such that X (\u0398) = 0. Consequently, for any \u03a3\u2217 \u2208 T \u2229 Sm+ with \u03a3\u0302 = \u03a3\u2217 +\u0398 0, (32) is satisfied with \u03a3\u0302 6= \u03a3\u2217.\nConversely, if \u03c42(T) > 0, (32) cannot be satisfied for \u2206\u0302T\u22a5 0, \u2206\u0302T\u22a5 6= 0. Otherwise, we could divide by tr(\u2206\u0302T\u22a5), which would yield\nX (\u2206\u0302T / tr(\u2206\u0302T\u22a5)\ufe38 \ufe37\ufe37 \ufe38 \u2208T ) + X (\u2206\u0302T\u22a5 / tr(\u2206\u0302T\u22a5)\ufe38 \ufe37\ufe37 \ufe38\n\u2208S+ 1 (m)\u2229T\u22a5\n) = 0,\nwhich would imply \u03c42(T) = 0. Therefore, we must have \u2206\u0302T\u22a5 = 0 and X (\u2206\u0302T) = 0, which implies \u2206\u0302T = 0 as long as \u03c6 2(T) > 0."}, {"heading": "H Proof of Theorem 2", "text": "Let \u2206\u0302 = \u03a3\u0302 \u2212 \u03a3\u2217, \u2206\u0302T = \u03a0T\u2206\u0302 and \u2206\u0302T\u22a5 = \u03a0T\u22a5\u2206\u0302 0 as in the preceding proof. We start with the following analog to (31)\n1 n \u2016X (\u2206\u0302)\u201622 = 1 n \u2016X (\u2206\u0302T + \u2206\u0302T\u22a5)\u201622 \u2264 2\u03bb0(\u2016\u2206\u0302T\u20161 + \u2016\u2206\u0302T\u22a5\u20161) (33)\nSuppose that \u2206\u0302T\u22a5 6= 0. We then have\n\u2016\u2206\u0302T\u22a5\u201621    1 n \u2225\u2225\u2225\u2225\u2225X ( \u2206\u0302T\n\u2016\u2206\u0302T\u22a5\u20161\n) + X ( \u2206\u0302T\u22a5\n\u2016\u2206\u0302T\u22a5\u20161\n)\u2225\u2225\u2225\u2225\u2225 2\n2\n   \u2264 2\u03bb0(\u2016\u2206\u0302T\u20161 + \u2016\u2206\u0302T\u22a5\u20161)\nSince \u2206\u0302T/\u2016\u2206\u0302T\u22a5\u20161 \u2208 T and \u2206\u0302T\u22a5/\u2016\u2206\u0302T\u22a5\u20161 = \u2206\u0302T\u22a5/ tr(\u2206\u0302T\u22a5) \u2208 S+1 (m), we obtain that the term inside the curly brackets is lower bounded by \u03c42(T) and thus\n\u2016\u2206\u0302T\u22a5\u20161 \u2264 2\u03bb0 \u03c42(T)\n( 1 +\n\u2016\u2206\u0302T\u20161 \u2016\u2206\u0302T\u22a5\u20161\n) (34)\nOn the other hand, expanding the quadratic term in (33), we obtain that\n1 n \u2016X (\u2206\u0302T)\u201622 \u2212 2 n\n\u2329 X (\u2206\u0302T),X (\u2206\u0302T\u22a5 ) \u232a \u2264 1\nn \u2016X (\u2206\u0302)\u201622 \u2264 2\u03bb0(\u2016\u2206\u0302T\u20161 + \u2016\u2206\u0302T\u22a5\u20161)\n\u21d2 1 n \u2016X (\u2206\u0302T)\u201622 \u2264 2\u03bb0(\u2016\u2206\u0302T\u20161 + \u2016\u2206\u0302T\u22a5\u20161) + 2\u00b5(T)\u2016\u2206\u0302T\u20161\u2016\u2206\u0302T\u22a5\u20161 \u21d2 \u03c62(T)\u2016\u2206\u0302T\u201621 \u2264 2\u03bb0(\u2016\u2206\u0302T\u20161 + \u2016\u2206\u0302T\u22a5\u20161) + 2\u00b5(T)\u2016\u2206\u0302T\u20161\u2016\u2206\u0302T\u22a5\u20161\n\u21d2 \u2016\u2206\u0302T\u20161 \u2264 2\u03bb0\n( 1 + \u2016\u2206\u0302T\u22a5\u20161 / \u2016\u2206\u0302T\u20161 ) + 2\u00b5(T)\u2016\u2206\u0302T\u22a5\u20161\n\u03c62(T) (35)\nWe now distinguish several cases.\nCase 1: \u2016\u2206\u0302T\u20161 \u2264 \u2016\u2206\u0302T\u22a5\u20161. It then immediately follows from (34) that\n\u2016\u2206\u0302\u20161 \u2264 8\u03bb0 \u03c42(T) =: T3. (36)\nCase 2a: \u2016\u2206\u0302T\u20161 > \u2016\u2206\u0302T\u22a5\u20161 and \u2016\u2206\u0302T\u22a5\u20161 \u2264 4\u03bb0/\u03c62(T). From (35), we first get\n\u2016\u2206\u0302T\u20161 \u2264 4\u03bb0 + 2\u00b5(T)\u2016\u2206\u0302T\u22a5\u20161\n\u03c62(T) (37)\nand thus\n\u2016\u2206\u0302\u20161 \u2264 8\u03bb0 \u03c62(T)\n( 1 + \u00b5(T)\n\u03c62(T)\n) =: T2 (38)\nCase 2b: \u2016\u2206\u0302T\u20161 > \u2016\u2206\u0302T\u22a5\u20161 and \u2016\u2206\u0302T\u22a5\u20161 > 4\u03bb0/\u03c62(T). Plugging (37) into (34), we obtain that\n\u2016\u2206\u0302T\u22a5\u20161 \u2264 4\u03bb0 \u03c42(T) + 4\u03bb0\u00b5(T) \u03c42(T)\u03c62(T) .\nSubstituting this bound back into (37) yields\n\u2016\u2206\u0302T\u20161 \u2264 4\u03bb0 \u03c62(T) + 8\u03bb0\u00b5(T) \u03c42(T)\u03c62(T) +\n8\u03bb0\u00b5 2(T)\n\u03c64(T)\u03c42(T) .\nCollecting terms, we obtain altogether\n\u2016\u2206\u0302\u20161 \u2264 8\u03bb0 \u00b5(T)\n\u03c42(T)\u03c62(T)\n( 3\n2 +\n\u00b5(T)\n\u03c62(T)\n) + 4\u03bb0 ( 1\n\u03c62(T) +\n1\n\u03c42(T)\n) =: T1. (39)\nCombining (36), (38) and (39) yields the assertion."}], "references": [{"title": "Living on the edge: phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M. McCoy", "J. Tropp"], "venue": "Information and Inference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations", "author": ["A. Bruckstein", "M. Elad", "M. Zibulevsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Cai", "A. Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns", "author": ["E. Candes", "X. Li"], "venue": "Foundation of Computational Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements", "author": ["E. Candes", "Y. Plan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundation of Computational Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E. Candes", "T. Strohmer", "V. Voroninski"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317\u2013366", "author": ["K. Davidson", "S. Szarek"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Counting the faces of randomly-projected hypercubes and orthants, with applications", "author": ["D. Donoho", "J. Tanner"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Quantum State Tomography via Compressed Sensing", "author": ["D. Gross", "Y.-K. Liu", "S. Flammia", "S. Becker", "J. Eisert"], "venue": "Physical Review Letters,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Matrix Analysis", "author": ["R. Horn", "C. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["I. Johnstone"], "venue": "The Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "The phase retrieval problem", "author": ["M. Klibanov", "P. Sacks", "A. Tikhonarov"], "venue": "Inverse Problems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Von Neumann entropy penalization and low-rank matrix estimation", "author": ["V. Koltchinskii"], "venue": "The Annals of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sign-constrained least squares estimation for high-dimensional regression", "author": ["N. Meinshausen"], "venue": "The Electronic Journal of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear normminimization", "author": ["B. Recht", "M. Fazel", "P. Parillo"], "venue": "SIAM Review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["A. Rohde", "A. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization", "author": ["M. Slawski", "M. Hein"], "venue": "The Electronic Journal of Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Topics in Random Matrix Theory", "author": ["T. Tao"], "venue": "American Mathematical Society,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Regression shrinkage and variable selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["J. Tropp"], "venue": "http://users.cms.caltech.edu/~jtropp/", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "How close is the sample covariance matrix to the actual covariance matrix ", "author": ["R. Vershynin"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Conditions for a Unique Non-negative Solution to an Underdetermined System", "author": ["M. Wang", "A. Tang"], "venue": "In Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A unique \u2019nonnegative\u2019 solution to an underdetermined system: from vectors to matrices", "author": ["M. Wang", "W. Xu", "A. Tang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}], "referenceMentions": [{"referenceID": 6, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 78, "endOffset": 85}, {"referenceID": 22, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 78, "endOffset": 85}, {"referenceID": 5, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 106, "endOffset": 113}, {"referenceID": 17, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 106, "endOffset": 113}, {"referenceID": 10, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 18, "context": "A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [22] in regularized estimation amenable to modern optimization techniques.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "This approach can be seen as natural generalization of l1-norm (aka lasso) regularization for the standard linear regression model [28] that arises as a special case of model (1) in which both \u03a3 and the measurement matrices {Xi}i=1 are diagonal.", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "The set S+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning methods [24].", "startOffset": 124, "endOffset": 128}, {"referenceID": 29, "context": "It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [33].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].", "startOffset": 183, "endOffset": 191}, {"referenceID": 21, "context": "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].", "startOffset": 183, "endOffset": 191}, {"referenceID": 2, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 9, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 27, "context": "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.", "startOffset": 250, "endOffset": 261}, {"referenceID": 28, "context": "In [32], the problem of exactly recovering \u03a3 being low-rank from noiseless observations (\u03b5i = 0, i = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Apart from the fact that we primarily study a noisy setting, we shall argue below that in the setup of compressed sensing the measurement matrices studied in [32] constitute an unfavourable choice relative to those recommended in the present paper.", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "In [5], rank-one measurements are considered for general \u03a3 \u2208 R12 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "On the other hand, in [5], no specific attention is given to the spd constraint: the convex program proposed therein, which can be seen as a modification of the approach in [10], applies to general symmetric matrices and does not enforce positive semidefiniteness.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "yi = |xi \u03c3| + \u03b5i (5) which (with complex-valued \u03c3) is relevant to the problem of phase retrieval [17] that has received some attention recently.", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "The approach of [9] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "In followup work [6], the authors show a refined recovery result stating that imposing an spd constraint \u2212 without regularization \u2212 suffices.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "However, the results in both [6] and [12] only concern model (5).", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "In [18], \u03a3 is assumed to be a complex Hermitian positive semidefinite matrix of unit trace, which is the setting in quantum state tomography.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "While the setting as well as the measurement matrices under consideration are different from ours, a notable point of contact to our work can be seen in the fact that the negative von Neumann entropy, which is the proposed regularizer in [18], does not promote low rankedness, but constitutes one possible way of enforcing positive definiteness.", "startOffset": 238, "endOffset": 242}, {"referenceID": 14, "context": "At the same time, adaptivity of the approach to low rankedness is established in [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "[7, 19, 21, 22, 23].", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "[7, 21].", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[7, 21].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "The following statement, which follows from results in [2], points to a serious limitation associated with the use of such measurements.", "startOffset": 55, "endOffset": 58}, {"referenceID": 28, "context": "In [32], the following noiseless analog to the constrained least squares problem (7) is considered: find \u03a3 \u2208 S+ such thatX (\u03a3) = y = X (\u03a3), (12) where Xi \u223c GOE(m), i = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "It is of interest to relate Proposition 2 to corresponding results on the vector case (equivalent to having diagonal {Xi}i=1 and diagonal \u03a3) in [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "Compared to Proposition 2, the corresponding result in [13] applies to a much wider class of random measurement matrices including all random matrices with i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "It is thus natural to ask whether Proposition 2 holds more generally for all Wigner matrices [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The fact that the threshold 12\u03b4m for the number measurements in Proposition 2 equals (up to the scaling factor \u03c3) the asymptotic prediction error of Example 2 is not a coincidence; this is part of a wider phenomenon as pointed out in [2].", "startOffset": 234, "endOffset": 237}, {"referenceID": 0, "context": "In the framework of [2], 12\u03b4m is the \u201cstatistical dimension\u201d of S m + .", "startOffset": 20, "endOffset": 23}, {"referenceID": 19, "context": "Theorem 1 in [23].", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "Theorem 1 in [25].", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "[11]) that for m,N large, \u03bbmax(\u0393\u0302n) and \u03bbmin(\u0393\u0302n) concentrate sharply around (1+ \u03b7n) 2 and (1\u2212 \u03b7n), respectively, where \u03b7n = \u221a m/N .", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Similar though weaker concentration results for \u2016\u0393\u2212 \u0393\u0302n\u2016\u221e are available for the broad class of distributions \u03c0m having finite fourth moments [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "The results point to the existence of a phase transition as it is typical for problems related to that under study [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "5, 1, 2, 4, 8, 16}, where \u03bb\u2217 = \u03c3 \u221a m/n as recommended in [21] and pick \u03bb so that the prediction error on a separate validation data set of size n generated from (18) is minimized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "In addition, we have assessed the performance of the approach in [5], which does not impose an spd constraint but adds one more constraint to the formulation (19).", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Therefore, instead of doing a grid search over a 2Dgrid, we use fixed values as specified in [5] given the knowledge of \u03c3.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "The results are similar or worse than those of (19) (note in particular that positive semidefiniteness is not taken advantage of in the approach of [5]) and are hence not reported here.", "startOffset": 148, "endOffset": 151}, {"referenceID": 12, "context": "We conclude this section by presenting an application to recovery of spiked covariance matrices, a notion due to [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "It is well-known [3] that the projection of a symmetric matrix on the positive semidefinite cone is obtained by setting all its negative eigenvalues to zero, i.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[27]), which is symmetric", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The proof of Proposition 2 follows from results in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "[2] Let f : R \u2192 R\u222a{\u2212\u221e,+\u221e} be a proper convex function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "It is shown in [2], Proposition 3.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": ",m ([15], \u00a74.", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "1 in [29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "[29] Consider a sequence {Xi}i=1 of fixed matrices in S and let {\u03b5i}i=1 i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Let z1, .", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Over the past few years, trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularizationbased approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In the present paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of the regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.", "creator": "LaTeX with hyperref package"}}}