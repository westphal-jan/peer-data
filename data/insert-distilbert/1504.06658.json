{"id": "1504.06658", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods", "abstract": "most of previous work in knowledge base ( kb ) completion has focused on the problem of relation extraction. generally in this work, we focus on the task of inferring missing entity type instances in a kb, a fundamental task for kb competition yet receives usually little attention. due to the novelty of this task, we construct directly a hypothetical large - scale dataset and design an automatic evaluation methodology. our knowledge base completion evaluation method uses information within the existing kb and external information from wikipedia. we show that individual methods trained with a global objective that considers unobserved sensor cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. we also perform manual evaluation on a small subset of the data areas to verify the effectiveness of comparing our knowledge base completion methods and promote the consistency correctness of our proposed automatic evaluation method.", "histories": [["v1", "Fri, 24 Apr 2015 22:32:40 GMT  (349kb,D)", "http://arxiv.org/abs/1504.06658v1", "North American Chapter of the Association for Computational Linguistics- Human Language Technologies, 2015"]], "COMMENTS": "North American Chapter of the Association for Computational Linguistics- Human Language Technologies, 2015", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "ming-wei chang"], "accepted": true, "id": "1504.06658"}, "pdf": {"name": "1504.06658.pdf", "metadata": {"source": "CRF", "title": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods", "authors": ["Arvind Neelakantan", "Ming-Wei Chang"], "emails": ["arvind@cs.umass.edu", "minchang@microsoft.com"], "sections": [{"heading": null, "text": "Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method."}, {"heading": "1 Introduction", "text": "There is now increasing interest in the construction of knowledge bases like Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010) in the natural language processing community. KBs contain facts such as Tiger Woods is an athlete, and Barack Obama is the president of USA. However, one of the main drawbacks in existing KBs is that they are incomplete and are missing important facts (West et\n\u2217Most of the research conducted during summer internship at Microsoft.\nal., 2014), jeopardizing their usefulness in downstream tasks such as question answering. This has led to the task of completing the knowledge base entries, or Knowledge Base Completion (KBC) extremely important.\nIn this paper, we address an important subproblem of knowledge base completion\u2014 inferring missing entity type instances. Most of previous work in KB completion has only focused on the problem of relation extraction (Mintz et al., 2009; Nickel et al., 2011; Bordes et al., 2013; Riedel et al., 2013). Entity type information is crucial in KBs and is widely used in many NLP tasks such as relation extraction (Chang et al., 2014), coreference resolution (Ratinov and Roth, 2012; Hajishirzi et al., 2013), entity linking (Fang and Chang, 2014), semantic parsing (Kwiatkowski et al., 2013; Berant et al., 2013) and question answering (Bordes et al., 2014; Yao and Durme, 2014). For example, adding entity type information improves relation extraction by 3% (Chang et al., 2014) and entity linking by 4.2 F1 points (Guo et al., 2013). Despite their importance, there is surprisingly little previous work on this problem and, there are no datasets publicly available for evaluation.\nWe construct a large-scale dataset for the task of inferring missing entity type instances in a KB. Most of previous KBC datasets (Mintz et al., 2009; Riedel et al., 2013) are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training. Hence, the methods could be potentially evaluated by their ability to predict easy facts that the KB already contains. Moreover, the methods are not directly evaluated\nar X\niv :1\n50 4.\n06 65\n8v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\non their ability to predict missing facts. To overcome these drawbacks we construct the train and test data using two snapshots of the KB and evaluate the methods on predicting facts that are added to the more recent snapshot, enabling a more realistic and challenging evaluation.\nStandard evaluation metrics for KBC methods are generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type. This is not ideal because: (1) it treats every entity type equally not considering the distribution of types, (2) it does not measure the ability of the methods to rank predictions across types. Therefore, we additionally use a global evaluation metric, where the quality of predictions is measured within and across types, and also accounts for the high variance in type distribution. In our experiments, we show that models trained with negative examples from the entity side perform better on type-based metrics, while when trained with negative examples from the type side perform better on the global metric.\nIn order to design methods that can rank predictions both within and across entity (or relation) types, we propose a global objective to train the models. Our proposed method combines the advantages of previous approaches by using negative examples from both the entity and the type side. When considering the same number of negative examples, we find that the linear classifiers and the low-dimensional embedding models trained with the global objective produce better quality ranking within and across entity types when compared to training with negatives examples only from entity or type side. Additionally compared to prior methods, the model trained on the proposed global objective can more reliably suggest confident entity-type pair candidates that could be added into the given knowledge base.\nOur contributions are summarized as follows:\n\u2022 We develop an evaluation framework comprising of methods for dataset construction and evaluation metrics to evaluate KBC approaches for missing entity type instances. The dataset and evaluation scripts are publicly available at http://research. microsoft.com/en-US/downloads/\ndf481862-65cc-4b05-886c-acc181ad07bb/ default.aspx.\n\u2022 We propose a global training objective for KBC methods. The experimental results show that both linear classifiers and low-dimensional embedding models achieve best overall performance when trained with the global objective function.\n\u2022 We conduct extensive studies on models for inferring missing type instances studying the impact of using various features and models."}, {"heading": "2 Inferring Entity Types", "text": "We consider a KB \u039b containing entity type information of the form (e, t), where e \u2208 E (E is the set of all entities) is an entity in the KB with type t \u2208 T (T is the set of all types). For example, e could be Tiger Woods and t could be sports athlete. As a single entity can have multiple types, entities in Freebase often miss some of their types. The aim of this work is to infer missing entity type instances in the KB. Given an unobserved fact (an entity-type pair) in the training data (e, t) 6\u2208 \u039b where entity e \u2208 E and type t \u2208 T , the task is to infer whether the KB currently misses the fact, i.e., infer whether (e, t) \u2208 \u039b. We consider entities in the intersection of Freebase and Wikipedia in our experiments."}, {"heading": "2.1 Information Resources", "text": "Now, we describe the information sources used to construct the feature representation of an entity to\ninfer its types. We use information in Freebase and external information from Wikipedia to complete the KB.\n\u2022 Entity Type Features: The entity types observed in the training data can be a useful signal to infer missing entity type instances. For example, in our snapshot of Freebase, it is not uncommon to find an entity with the type /people/deceased person but missing the type /people/person.\n\u2022 Freebase Description: Almost all entities in Freebase have a short one paragraph description of the entity. Figure 1 shows the Freebase description of Jean Metellus that can be used to infer the type /book/author which Freebase does not contain as the date of writing this article.\n\u2022 Wikipedia: As external information, we include the Wikipedia full text article of an entity in its feature representation. We consider entities in Freebase that have a link to their Wikipedia article. The Wikipedia full text of an entity gives several clues to predict it\u2019s entity types. For example, Figure 2 shows a section of the Wikipedia article of Claire Martin which gives clues to infer the type /award/award winner that Freebase misses."}, {"heading": "3 Evaluation Framework", "text": "In this section, we propose an evaluation methodology for the task of inferring missing entity type instances in a KB. While we focus on recovering entity types, the proposed framework can be easily adapted to relation extraction as well.\nFirst, we discuss our two-snapshot dataset construction strategy. Then we motivate the importance of evaluating KBC algorithms globally and describe the evaluation metrics we employ."}, {"heading": "3.1 Two Snapshots Construction", "text": "In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training. However, given that the missing entries are usually selected randomly, the distribution\nof the selected unknown entries could be very different from the actual missing facts distribution. Also, since any fact could be potentially used for evaluation, the methods could be evaluated on their ability to predict easy facts that are already present in the KB.\nTo overcome this drawback, we construct our train and test set by considering two snapshots of the knowledge base. The train snapshot is taken from an earlier time without special treatment. The test snapshot is taken from a later period, and a KBC algorithm is evaluated by its ability of recovering newly added knowledge in the test snapshot. This enables the methods to be directly evaluated on facts that are missing in a KB snapshot. Note that the facts that are added to the test snapshot, in general, are more subtle than the facts that they already contain and predicting the newly added facts could be harder. Hence, our approach enables a more realistic and challenging evaluation setting than previous work.\nWe use manually constructed Freebase as the KB in our experiments. Notably, Chang et al. (2014) use a two-snapshot strategy for constructing a dataset for relation extraction using automatically constructed NELL as their KB. The new facts that are added to a KB by an automatic method may not have all the characteristics that make the two snapshot strategy more advantageous.\nWe construct our train snapshot \u039b0 by taking the Freebase snapshot on 3rd September, 2013 and consider entities that have a link to their Wikipedia page. KBC algorithms are evaluated by their ability to predict facts that were added to the 1st June, 2014 snapshot of Freebase \u039b. To get negative data, we make a closed world assumption treating any unobserved instance in Freebase as a negative example. Unobserved instances in the Freebase snapshot on 3rd September, 2013 and 1st June, 2014 are used as negative examples in training and testing respectively.1\nThe positive instances in the test data (\u039b\u2212\u039b0) are facts that are newly added to the test snapshot \u039b. Using the entire set of negative examples in the test data is impractical due to the large number of negative examples. To avoid this we only add the negative types\n1Note that some of the negative instances used in training could be positive instances in test but we do not remove them during training.\nof entities that have at least one new fact in the test data. Additionally, we add a portion of the negative examples for entities which do not have new fact in the test data and that were unused during training. This makes our dataset quite challenging since the number of negative instances is much larger than the number of positive instances in the test data.\nIt is important to note that the goal of this work is not to predict facts that emerged between the time period of the train and test snapshot2. For example, we do not aim to predict the type /award/award winner for an entity that won an award after 3rd September, 2013. Hence, we use the Freebase description in the training data snapshot and Wikipedia snapshot on 3rd September, 2013 to get the features for entities.\nOne might worry that the new snapshot might contain a significant amount of emerging facts so it could not be an effective way to evaluate the KBC algorithms. Therefore, we examine the difference between the training snapshot and test snapshot manually and found that this is likely not the case. For example, we randomly selected 25 /award/award winner instances that were added to the test snapshot and found that all of them had won at least one award before 3rd September, 2013.\nNote that while this automatic evaluation is closer to the real-world scenario, it is still not perfect as the new KB snapshot is still incomplete. Therefore, we also perform human evaluation on a small dataset to verify the effectiveness of our approach.\n2In this work, we also do not aim to correct existing false positive errors in Freebase"}, {"heading": "3.2 Global Evaluation Metric", "text": "Mean average precision (MAP) (Manning et al., 2008) is now commonly used to evaluate KB completion methods (Mintz et al., 2009; Riedel et al., 2013). MAP is defined as the mean of average precision over all entity (or relation) types. MAP treats each entity type equally (not explicitly accounting for their distribution). However, some types occur much more frequently than others. For example, in our large-scale experiment with 500 entity types, there are many entity types with only 5 instances in the test set while the most frequent entity type has tens of thousands of missing instances. Moreover, MAP only measures the ability of the methods to correctly rank predictions within a type.\nTo account for the high variance in the distribution of entity types and measure the ability of the methods to correctly rank predictions across types we use global average precision (GAP) (similarly to micro-F1) as an additional evaluation metric for KB completion. We convert the multi-label classification problem to a binary classification problem where the label of an entity and type pair is true if the entity has that type in Freebase and false otherwise. GAP is the average precision of this transformed problem which can measure the ability of the methods to rank predictions both within and across entity types.\nPrior to us, Bordes et al. (2013) use mean reciprocal rank as a global evaluation metric for a KBC task. We use average precision instead of mean reciprocal rank since MRR could be biased to the top predictions of the method (West et al., 2014)\nWhile GAP captures global ordering, it would be\nbeneficial to measure the quality of the top k predictions of the model for bootstrapping and active learning scenarios (Lewis and Gale, 1994; Cucerzan and Yarowsky, 1999). We report G@k, GAP measured on the top k predictions (similarly to Precision@k and Hits@k). This metric can be reliably used to measure the overall quality of the top k predictions."}, {"heading": "4 Global Objective for Knowledge Base Completion", "text": "We describe our approach for predicting missing entity types in a KB in this section. While we focus on recovering entity types in this paper, the methods we develop can be easily extended to other KB completion tasks."}, {"heading": "4.1 Global Objective Framework", "text": "During training, only positive examples are observed in KB completion tasks. Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative examples. Because the number of unobserved examples is much larger than the number of facts in the KB, we follow previous methods and sample few unobserved negative examples for every positive example.\nPrevious methods largely neglect the sampling methods on unobserved negative examples. The proposed global object framework allows us to systematically study the effect of the different sampling methods to get negative data, as the performance of the model for different evaluation metrics does depend on the sampling method.\nWe consider a training snapshot of the KB \u039b0, containing facts of the form (e, t) where e is an entity in the KB with type t. Given a fact (e, t) in the KB, we consider two types of negative examples constructed from the following two sets: NE(e, t) is the \u201cnegative entity set\u201d, and NT (e, t) is the \u201cnegative type set\u201d. More precisely,\nNE(e, t) \u2282 {e\u2032|e\u2032 \u2208 E, e\u2032 6= e, (e\u2032, t) /\u2208 \u039b0},\nand\nNT (e, t) \u2282 {t\u2032|t\u2032 \u2208 T, t\u2032 6= t, (e, t\u2032) /\u2208 \u039b0}.\nLet \u03b8 be the model parameters, m = |NE(e, t)| and n = |NT (e, t)| be the number of negative examples and types considered for training respectively. For each entity-type pair (e, t), we define the scoring function of our model as s(e, t|\u03b8).3 We define two loss functions one using negative entities and the other using negative types:\nLE(\u039b0, \u03b8) = \u2211\n(e,t)\u2208\u039b0,e\u2032\u2208NE(e,t)\n[s(e\u2032, t)\u2212 s(e, t) + 1]k+,\nand LT (\u039b0, \u03b8) = \u2211\n(e,t)\u2208\u039b0,t\u2032\u2208NT (e,t)\n[s(e, t\u2032)\u2212 s(e, t) + 1]k+,\nwhere k is the power of the loss function (k can be 1 or 2), and the function [\u00b7]+ is the hinge function.\nThe global objective function is defined as\nmin \u03b8 Reg(\u03b8) + CLT (\u039b0, \u03b8) + CLE(\u039b0, \u03b8), (1)\nwhere Reg(\u03b8) is the regularization term of the model, and C is the regularization parameter. Intuitively, the parameters \u03b8 are estimated to rank the observed facts above the negative examples with a margin. The total number of negative examples is controlled by the size of the sets NE and NT . We experiment by sampling only entities or only types or both by fixing the total number of negative examples in Section 5.\nThe rest of section is organized as follows: we propose three algorithms based on the global objective in Section 4.2. In Section 4.3, we discuss the relationship between the proposed algorithms and existing approaches. Let \u03a6(e) \u2192 Rde be the feature function that maps an entity to its feature representation, and \u03a8(t) \u2192 Rdt be the feature function that maps an entity type to its feature representation.4 de and dt represent the feature dimensionality of the entity features and the type features respectively. Feature representations of the entity types (\u03a8) is only used in the embedding model.\n3We often use s(e, t) as an abbreviation of s(e, t|\u03b8) in order to save space.\n4This gives the possibility of defining features for the labels in the output space but we use a simple one-hot representation for types right now since richer features did not give performance gains in our initial experiments.\nAlgorithm 1 The training algorithm for Linear.Adagrad. 1: Initialize wt = 0, \u2200t = 1 . . . |T | 2: for (e, t) \u2208 \u039b0 do 3: for e\u2032 \u2208 NE(e, t) do 4: if wTt \u03a6(e)\u2212wTt \u03a6(e\u2032)\u2212 1 < 0 then 5: AdaGradUpdate(wt,\u03a6(e\u2032)\u2212 \u03a6(e)) 6: end if 7: end for 8: for t\u2032 \u2208 NT (e, t) do 9: if wTt \u03a6(e)\u2212wTt\u2032\u03a6(e)\u2212 1 < 0 then 10: AdaGradUpdate(wt,\u2212\u03a6(e)) 11: AdaGradUpdate(wt\u2032 ,\u03a6(e)). 12: end if 13: end for 14: end for"}, {"heading": "4.2 Algorithms", "text": "We propose three different algorithms based on the global objective framework for predicting missing entity types. Two algorithms use the linear model and the other one uses the embedding model.\nLinear Model The scoring function in this model is given by s(e, t|\u03b8 = {wt}) = wTt \u03a6(e), where wt \u2208 Rde is the parameter vector for target type t. The regularization term in Eq. (1) is defined as follows: R(\u03b8) = 1/2 \u2211 t=1 w T t wt. We use k = 2 in our experiments. Our first algorithm is obtained by using the dual coordinate descent algorithm (Hsieh et al., 2008) to optimize Eq. (1), where we modified the original algorithm to handle multiple weight vectors. We refer to this algorithm as Linear.DCD.\nWhile DCD algorithm ensures convergence to the global optimum solution, its convergence can be slow in certain cases. Therefore, we adopt an online algorithm, Adagrad (Duchi et al., 2011). We use the hinge loss function (k = 1) with no regularization (Reg(\u03b8) = \u2205) since it gave best results in our initial experiments. We refer to this algorithm as Linear.Adagrad, which is described in Algorithm 1. Note that AdaGradUpdate(x, g) is a procedure which updates the vector x with the respect to the gradient g.\nEmbedding Model In this model, vector representations are constructed for entities and types using linear projection matrices. Recall \u03a8(t) \u2192 Rdt is the feature function that maps a type to its feature representation. The scoring function is given by\nAlgorithm 2 The training algorithm for the embedding model. 1: Initialize V,U randomly. 2: for (e, t) \u2208 \u039b0 do 3: for e\u2032 \u2208 NE(e, t) do 4: if s(e, t)\u2212 s(e\u2032, t)\u2212 1 < 0 then 5: \u00b5\u2190 VT \u03a8(t) 6: \u03b7 \u2190 UT (\u03a6(e\u2032)\u2212 \u03a6(e)) 7: for i \u2208 1 . . . d do 8: AdaGradUpdate(Ui, \u00b5[i](\u03a6(e\u2032)\u2212 \u03a6(e))) 9: AdaGradUpdate(Vi, \u03b7[i]\u03a8(t)) 10: end for 11: end if 12: end for 13: for t\u2032 \u2208 NT (e, t) do 14: if s(e, t)\u2212 s(e, t\u2032)\u2212 1 < 0 then 15: \u00b5\u2190 VT (\u03a8(t\u2032)\u2212\u03a8(t)) 16: \u03b7 \u2190 UT \u03a6(e) 17: for i \u2208 1 . . . d do 18: AdaGradUpdate(Ui, \u00b5[i]\u03a6(e)) 19: AdaGradUpdate(Vi, \u03b7[i](\u03a8(t\u2032)\u2212\u03a8(t))) 20: end for 21: end if 22: end for 23: end for\ns(e, t|\u03b8= (U,V)) = \u03a8(t)TVUT\u03a6(e), where U \u2208 Rde\u00d7d and V \u2208 Rdt\u00d7d are projection matrices that embed the entities and types in a ddimensional space. Similarly to the linear classifier model, we use the l1-hinge loss function (k = 1) with no regularization (Reg(\u03b8) = \u2205). Ui and Vi denote the i-th column vector of the matrix U and V, respectively. The algorithm is described in detail in Algorithm 2.\nThe embedding model has more expressive power than the linear model, but the training unlike in the linear model, converges only to a local optimum solution since the objective function is non-convex."}, {"heading": "4.3 Relationship to Existing Methods", "text": "Many existing methods for relation extraction and entity type prediction can be cast as a special case under the global objective framework. For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205. These models are trained only using negative entities which we refer to as Negative Entity (NE) objective. The entity type prediction model in Ling and Weld (2012) is a linear model with NE(e, t) = \u2205 which\nwe refer to as the Negative Type (NT) objective. The embedding model described in Weston et al. (2011) developed for image retrieval is also a special case of our model trained with the NT objective.\nWhile the NE or NT objective functions could be suitable for some classification tasks (Weston et al., 2011), the choice of objective functions for the KBC tasks has not been well motivated. Often the choice is made neither with theoretical foundation nor with empirical support. To the best of our knowledge, the global objective function, which includes bothNE(e, t) andNT (e, t), has not been considered previously by KBC methods."}, {"heading": "5 Experiments", "text": "In this section, we give details about our dataset and discuss our experimental results. Finally, we perform manual evaluation on a small subset of the data."}, {"heading": "5.1 Data", "text": "First, we evaluate our methods on 70 entity types with the most observed facts in the training data.5 We also perform large-scale evaluation by testing the methods on 500 types with the most observed facts in the training data.\nTable 1 shows statistics of our dataset. The number of positive examples is much larger in the training data compared to that in the test data since the test set contains only facts that were added to the more recent snapshot. An additional effect of this is\n5We removed few entity types that were trivial to predict in the test data.\nthat most of the facts in the test data are about entities that are not very well-known or famous. The high negative to positive examples ratio in the test data makes this dataset very challenging."}, {"heading": "5.2 Automatic Evaluation Results", "text": "Table 2 shows automatic evaluation results where we give results on 70 types and 500 types. We compare different aspects of the system on 70 types empirically.\nAdagrad Vs DCD We first study the linear models by comparing Linear.DCD and Linear.AdaGrad. Table 2a shows that Linear.AdaGrad consistently performs better for our task.\nImpact of Features We compare the effect of different features on the final performance using Linear.AdaGrad in Table 2b. Types are represented by boolean features while Freebase description and Wikipedia full text are represented using tfidf weighting. The best MAP results are obtained by using all the information (T+D+W) while best GAP results are obtained by using the Freebase description and Wikipedia article of the entity. Note that the features are simply concatenated when multiple resources are used. We tried to use idf weighting on type features and on all features, but they did not yield improvements.\nThe Importance of Global Objective Table 2c and 2d compares global training objective with NE and NT training objective. Note that all the three methods use the same number of negative examples. More precisely, for each (e, t) \u2208 \u039b0, |NE(e, t)| + |NT (e, t)| = m + n = 2. The results show that the global training objective achieves best scores on both MAP and GAP for classifiers and lowdimensional embedding models. Among NE and NT, NE performs better on the type-based metric while NT performs better on the global metric.\nLinear Model Vs Embedding Model Finally, we compare the linear classifier model with the embedding model in Table 2e. The linear classifier model performs better than the embedding model in both MAP and GAP.\nWe perform large-scale evaluation on 500 types with the description features (as experiments are expensive) and the results are shown in Table 2f.\nOne might expect that with the increased number of types, the embedding model would perform better than the classifier since they share parameters across types. However, despite the recent popularity of embedding models in NLP, linear model still performs better in our task."}, {"heading": "5.3 Human Evaluation", "text": "To verify the effectiveness of our KBC algorithms, and the correctness of our automatic evaluation method, we perform manual evaluation on the top 100 predictions of the output obtained from two dif-\nferent experimental setting and the results are shown in Table 3. Even though the automatic evaluation gives pessimistic results since the test KB is also incomplete6, the results indicate that the automatic evaluation is correlated with manual evaluation. More excitingly, among the 179 unique instances we manually evaluated, 17 of them are still7 missing in Freebase which emphasizes the effectiveness of our approach.\n6This is true even with existing automatic evaluation methods.\n7at submission time."}, {"heading": "5.4 Error Analysis", "text": "\u2022 Effect of training data: We find the perfor-\nmance of the models on a type is highly dependent on the number of training instances for that type. For example, the linear classifier model when evaluated on 70 types performs 24.86 % better on the most frequent 35 types compared to the least frequent 35 types. This indicates bootstrapping or active learning techniques can be profitably used to provide more supervision for the methods. In this case, G@k would be an useful metric to compare the effectiveness of the different methods.\n\u2022 Shallow Linguistic features: We found some of the false positive predictions are caused by the use of shallow linguistic features. For example, an entity who has acted in a movie and composes music only for television shows is wrongly tagged with the type /film/composer since words like \u201dmovie\u201d, \u201dcomposer\u201d and \u201dmusic\u201d occur frequently in the Wikipedia article of the entity (http://en.wikipedia. org/wiki/J._J._Abrams)."}, {"heading": "6 Related Work", "text": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method was still evaluated only at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity\u2019s Wikipedia article to perform named entity recognition on three entity types.\nKnowledge Base Completion Much of precious work in KB completion has focused on the problem of relation extraction. Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents. Riedel et al. (2013) use both information within and outside the KB to complete the KB.\nLinear Embedding Model Weston et al. (2011) is one of first work that developed a supervised linear embedding model and applied it to image retrieval. We apply this model to entity type prediction but we train using a different objective function which is more suited for our task."}, {"heading": "7 Conclusion and Future Work", "text": "We propose an evaluation framework comprising of methods for dataset construction and evaluation metrics to evaluate KBC approaches for inferring missing entity type instances. We verified that our automatic evaluation is correlated with human evaluation, and our dataset and evaluation scripts are publicly available.8 Experimental results show that models trained with our proposed global training objective produces higher quality ranking within and across types when compared to baseline methods.\nIn future work, we plan to use information from entity linked documents to improve performance and also explore active leaning, and other humanin-the-loop methods to get more training data."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Vivek Srikumar", "PeiChun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Christopher D. Manning"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": null, "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang et al.2014] Kai-Wei Chang", "Wen tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Language indep endent named entity recognition combining morphological and contextual evidence", "author": ["Cucerzan", "Yarowsky1999] Silviu Cucerzan", "David Yarowsky"], "venue": "In oint SIGDAT Conference on Empirical Methods in Natural Language", "citeRegEx": "Cucerzan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 1999}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Entity linking on microblogs with spatial and temporal signals", "author": ["Fang", "Chang2014] Yuan Fang", "Ming-Wei Chang"], "venue": "In Transactions of the Association for Computational Linguistics", "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "To link or not to link? a study on end-to-end tweet entity linking", "author": ["Guo et al.2013] Stephen Guo", "Ming-Wei Chang", "Emre Kiciman"], "venue": "In The North American Chapter of the Association for Computational Linguistics.,", "citeRegEx": "Guo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Joint coreference resolution and named-entity linking with multi-pass sieves", "author": ["Leila Zilles", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Hajishirzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hajishirzi et al\\.", "year": 2013}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["Hsieh et al.2008] Cho-Jui Hsieh", "Kai-Wei Chang", "ChihJen Lin", "S. Sathiya Keerthi", "S. Sundararajan"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["Kazama", "Torisawa2007] Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Kazama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kazama et al\\.", "year": 2007}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke. Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W. Cohen"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D. Lewis", "William A. Gale"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Introduction to information retrieval", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Mining entity types from query logs via user intent modeling", "author": ["Thomas Lin", "Michael Gamon"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Pantel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2012}, {"title": "Learning-based multi-sieve co-reference resolution with knowledge", "author": ["Ratinov", "Roth2012] Lev Ratinov", "Dan Roth"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In The North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A proposal to automatically", "author": ["Toral", "Munoz2006] Antonio Toral", "Rafael Munoz"], "venue": null, "citeRegEx": "Toral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toral et al\\.", "year": 2006}, {"title": "Knowledge base completion via search-based question answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd international conference on World wide web,", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "There is now increasing interest in the construction of knowledge bases like Freebase (Bollacker et al., 2008) and NELL (Carlson et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 4, "context": ", 2008) and NELL (Carlson et al., 2010) in the natural language processing community.", "startOffset": 17, "endOffset": 39}, {"referenceID": 5, "context": "Entity type information is crucial in KBs and is widely used in many NLP tasks such as relation extraction (Chang et al., 2014), coreference resolution (Ratinov and Roth, 2012; Hajishirzi et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 13, "context": "2013), entity linking (Fang and Chang, 2014), semantic parsing (Kwiatkowski et al., 2013; Berant et al., 2013) and question answering (Bordes et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 0, "context": "2013), entity linking (Fang and Chang, 2014), semantic parsing (Kwiatkowski et al., 2013; Berant et al., 2013) and question answering (Bordes et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 3, "context": ", 2013) and question answering (Bordes et al., 2014; Yao and Durme, 2014).", "startOffset": 31, "endOffset": 73}, {"referenceID": 5, "context": "For example, adding entity type information improves relation extraction by 3% (Chang et al., 2014) and entity linking by 4.", "startOffset": 79, "endOffset": 99}, {"referenceID": 9, "context": "2 F1 points (Guo et al., 2013).", "startOffset": 12, "endOffset": 30}, {"referenceID": 18, "context": "Most of previous KBC datasets (Mintz et al., 2009; Riedel et al., 2013) are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training.", "startOffset": 30, "endOffset": 71}, {"referenceID": 22, "context": "Most of previous KBC datasets (Mintz et al., 2009; Riedel et al., 2013) are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training.", "startOffset": 30, "endOffset": 71}, {"referenceID": 18, "context": "generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type.", "startOffset": 21, "endOffset": 62}, {"referenceID": 22, "context": "generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type.", "startOffset": 21, "endOffset": 62}, {"referenceID": 18, "context": "In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training.", "startOffset": 73, "endOffset": 114}, {"referenceID": 22, "context": "In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training.", "startOffset": 73, "endOffset": 114}, {"referenceID": 5, "context": "Notably, Chang et al. (2014) use a two-snapshot strategy for constructing a dataset for relation extraction using automatically constructed", "startOffset": 9, "endOffset": 29}, {"referenceID": 17, "context": "Mean average precision (MAP) (Manning et al., 2008) is now commonly used to evaluate KB completion methods (Mintz et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 18, "context": ", 2008) is now commonly used to evaluate KB completion methods (Mintz et al., 2009; Riedel et al., 2013).", "startOffset": 63, "endOffset": 104}, {"referenceID": 22, "context": ", 2008) is now commonly used to evaluate KB completion methods (Mintz et al., 2009; Riedel et al., 2013).", "startOffset": 63, "endOffset": 104}, {"referenceID": 25, "context": "We use average precision instead of mean reciprocal rank since MRR could be biased to the top predictions of the method (West et al., 2014)", "startOffset": 120, "endOffset": 139}, {"referenceID": 2, "context": "Prior to us, Bordes et al. (2013) use mean reciprocal rank as a global evaluation metric for a KBC task.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 2, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 22, "context": "Similar to previous work (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013), we get negative training examples by treating the unobserved data in the KB as negative", "startOffset": 25, "endOffset": 87}, {"referenceID": 11, "context": "Our first algorithm is obtained by using the dual coordinate descent algorithm (Hsieh et al., 2008) to optimize Eq.", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "Therefore, we adopt an online algorithm, Adagrad (Duchi et al., 2011).", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 2, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 22, "context": "For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205.", "startOffset": 61, "endOffset": 123}, {"referenceID": 2, "context": ", 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = \u2205. These models are trained only using negative entities which we refer to as Negative Entity (NE) objective. The entity type prediction model in Ling and Weld (2012) is a linear model with NE(e, t) = \u2205 which", "startOffset": 8, "endOffset": 252}, {"referenceID": 26, "context": "The embedding model described in Weston et al. (2011)", "startOffset": 33, "endOffset": 54}, {"referenceID": 26, "context": "While the NE or NT objective functions could be suitable for some classification tasks (Weston et al., 2011), the choice of objective functions for the KBC tasks has not been well motivated.", "startOffset": 87, "endOffset": 108}, {"referenceID": 20, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level.", "startOffset": 68, "endOffset": 110}, {"referenceID": 19, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 14, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 23, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 2, "context": "Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al.", "startOffset": 85, "endOffset": 166}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles.", "startOffset": 69, "endOffset": 229}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method was still evaluated only at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity\u2019s Wikipedia article to perform named entity recognition on three entity types.", "startOffset": 69, "endOffset": 463}, {"referenceID": 15, "context": "Entity Type Prediction and Wikipedia Features Much of previous work (Pantel et al., 2012; Ling and Weld, 2012) in entity type prediction has focused on the task of predicting entity types at the sentence level. Yao et al. (2013) develop a method based on matrix factorization for entity type prediction in a KB using information within the KB and New York Times articles. However, the method was still evaluated only at the sentence level. Toral and Munoz (2006), Kazama and Torisawa (2007) use the first line of an entity\u2019s Wikipedia article to perform named entity recognition on three entity types.", "startOffset": 69, "endOffset": 491}, {"referenceID": 2, "context": ", 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents.", "startOffset": 8, "endOffset": 71}, {"referenceID": 2, "context": ", 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents. Riedel et al. (2013) use both information within and outside the KB to complete the KB.", "startOffset": 8, "endOffset": 127}, {"referenceID": 26, "context": "Linear Embedding Model Weston et al. (2011) is one of first work that developed a supervised linear embedding model and applied it to image retrieval.", "startOffset": 23, "endOffset": 44}], "year": 2015, "abstractText": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.", "creator": "LaTeX with hyperref package"}}}