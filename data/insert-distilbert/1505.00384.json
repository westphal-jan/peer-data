{"id": "1505.00384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2015", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets", "abstract": "this paper proposes an alternate architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. the branches provide a channel for enforcing processing useful information in hidden layer which helps in attaining better accuracy, beneficial both for the final layer and hidden module layers. the shared layers modify considerably their weights using the gradients of all cost functions higher than the branching layer. this model provides a flexible inference system with very many levels per of targets which is modular and can usually be used efficiently in situations requiring different levels of results depending according to complexity. this paper applies the idea to a text classification task on 20 newsgroups data set with usually two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches.", "histories": [["v1", "Sun, 3 May 2015 00:58:38 GMT  (369kb,D)", "https://arxiv.org/abs/1505.00384v1", "11 pages, 6 figures"], ["v2", "Sat, 24 Sep 2016 07:31:47 GMT  (370kb,D)", "http://arxiv.org/abs/1505.00384v2", "Updated to add a note with commentary on original (v1) submission"]], "COMMENTS": "11 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["abhinav tushar"], "accepted": false, "id": "1505.00384"}, "pdf": {"name": "1505.00384.pdf", "metadata": {"source": "CRF", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets", "authors": ["Abhinav Tushar"], "emails": ["abhinav.tushar.vs@gmail.com"], "sections": [{"heading": null, "text": "Author\u2019s Note September, 2016\nThis document essentially was (May 2015) a hasty write up of a project for a course on artificial neural networks during my undergraduate studies. I am adding this note here to point out mistakes which are detrimental to writings. I have kept the original content intact, adding only this box.\nFirstly, the document doesn\u2019t really use the terms (information, deep networks) from the title well in the analysis. Talking about the idea itself, there is a similar concept of auxiliary classifier in literature which uses [same] targets at lower levels to improve performance (See arXiv:1409.4842v1 [cs.CV] for example). -1 to literature review. Furthermore, the comparison is not rigorous enough to back up the claims and needs more meaningful test.\nar X\niv :1\n50 5.\n00 38\n4v 2\n[ cs\n.N E\n] 2"}, {"heading": "1 Introduction", "text": "Deep neural networks aim at learning multiple level of features by using larger number of hidden layers as compared to shallow networks. Using many layers, higher order features can be automatically learned without the need of any domain specific feature engineering. This makes them more generalized inference systems. They are effective at learning features from raw data which would have required much efforts to pre process in case of shallow networks, for example, a recent work (Zhang and LeCun 2015) demonstrated deep temporal convolutional networks to learn abstract text concepts from character level inputs.\nHowever, having multiple layers, deep networks are not easy to train. Few of the problems are, getting stuck in local optima, problem of vanishing gradients etc. If the hyperparameters of networks are not engineered properly, deep networks also tend to overfit. The choice of activation functions (Glorot and Bengio 2010) as well as proper initialization of weights (Sutskever et al. 2013) plays important role in the performance of deep networks.\nSeveral methods have been proposed to improve the performance of deep networks. Layer by layer training of Deep Belief Networks (Hinton et al. 2006) uses unsupervised pre-training of the component Restricted Boltzmann Machines (RBMs) and further supervised fine tuning of the whole network. Similar models have been presented (Bengio and Lamblin 2007; Ranzato et al. 2007) that pre-train the network layer by layer and then fine tune using supervised techniques.\nUnsupervised pre-training is shown to effectively works as a regularizer (Erhan et al. 2009; Erhan, Courville, and Vincent 2010) and increase the performance as compared to network with randomly initialized weights.\nThis paper explores the idea of training a deep network to learn hierarchical targets in which lower level targets are learned from taps in lower hidden layers, while the highest level of target (which has highest details) is kept at the final layer of the network. The hypothesis is that this architecture should learn meaningful representation in hidden layers too, because of the branchings. This can be helpful since the same model can be used as an efficient inference system for any level of target, depending on the requirement. Also, the meaningful information content of hidden layer activations can be helpful in improving the overall performance of the network.\nThe following section presents the proposed deep network with hidden layer branchings. Section 3 provides the experimental results on 20 Newsgroups data set 1 along with the details of the network used in the experiment. Section 4 contains the concluding remarks and scope of future work is given in Section 5."}, {"heading": "2 Proposed Network Architecture", "text": "In the proposed network, apart from the final target layer, one (or more) target layer are branched from the hidden layers. A simple structure with one\n1The dataset can be downloaded here qwone.com/~jason/20Newsgroups/\nbranching is shown in Figure 1. The target layers are arranged in a hierarchical fashion with the most detailed targets being farthest form the input, while trivial targets closer to the input layer. The network will learn both the final layer outputs as well as hidden layer outputs. The following sub section explains the learning algorithm using the example network in Figure 1."}, {"heading": "2.1 Learning Algorithm", "text": "The network learns using Stochastic Gradient Descent. There are two costs to minimize, the first being that of final target and second of hidden target. For the network shown in the Figure 1, the network has a branch from the layer whose output is xB . Weights and biases from WB+1, bB+1 to WN+2, bN+2 are updated using the final target layer cost function only, while WH and bH are updated using only the hidden layer cost function.\nW i \u2190W i \u2212 \u03b7 \u2202C\n\u2202W i (1)\nbi \u2190 bi \u2212 \u03b7 \u2202C\n\u2202bi (2)\nHere, C is the hidden or final target cost function, depending on which weights are to be minimized. For the weights that are shared for both targets, i.e. weights and biases from W 1, b1 to WB , bB , the training uses both cost\nfunction and an averaged update is done for these parameters. If final target cost is CF and hidden target cost is CH , then the updates are:\nW i \u2190W i \u2212 \u03b7 ( \u03b1 \u2202CF \u2202W i + (1\u2212 \u03b1)\u2202CH \u2202W i ) (3)\nbi \u2190 bi \u2212 \u03b7 ( \u03b1 \u2202CF \u2202bi + (1\u2212 \u03b1)\u2202CH \u2202bi ) (4)\nA value of \u03b1 = 0.5 gives equal weights to both gradients. This value will be used in the experiment in this paper."}, {"heading": "2.2 Features of the network", "text": "\u2022 Performance Representation of meaningful data in hidden layers governed by the hidden layer branchings helps by providing features for higher layers and thus improves the overall performance of the network.\n\u2022 Hierarchical targets Different target branches, arranged in hierarchy of details, help in problems demanding scalability in level of details of targets.\n\u2022 Modularity The hidden layer targets lead to storage of meaningful content in hidden layers and thus, the network can be separated (recombined) from (with) the branch joints without loss of the learned knowledge."}, {"heading": "3 Experimental Results", "text": "Hidden layer taps can be exploited only if the problem has multiple and hierarchical targets. It can also work when it is possible to degrade the resolution (or any other parameter related to details) of output to create hidden layer outputs. This section explores the performance of the proposed model on 20 Newsgroups dataset."}, {"heading": "3.1 Data set", "text": "The data set has newsgroup posts from 20 newsgroups, thus resulting in a 20 class classification problem. According to the newsgroup topics, the 20 classes were partitioned in 5 primitive classes (details are in Table 1). The final layer of the network is made to learn the 20 class targets, while the hidden layer branching is made to learn the cruder, 5 class targets. The dataset has 18846 instances. Out of these, 14314 were selected for training, while the other 4532 instances were kept for testing."}, {"heading": "3.2 Word2Vec preprocessing", "text": "For representing text, a simple and popular model can be made using Bag of Words (BoW). In this, a vocabulary of words is built from the corpus, and each paragraph (or instance) is represented by a histogram of frequency of occurrence of words from the vocabulary. Although being intuitive and simple, this representation has a major disadvantage while working with neural networks. The vocabulary length is usually very large, of the order of tens of thousands, while each chunk of text in consideration has only few of the possible words, which results in a very sparse representation. Such sparse input representation can lead to poor learning and high inefficiency in neural networks. A new tool, Word2Vec 2 is used to represent words as dense vectors.\nWord2Vec is a tool for computing continuous distributed representation of\n2Python adaptation here https://radimrehurek.com/gensim/models/word2vec.html (R\u030ceh\u030aur\u030cek 2013)\nwords. It uses Continuous Bag of Words and Skip-gram methods to learn vector representations of words using a corpus (Mikolov et al. 2013b; Mikolov et al. 2013a). The representations provided by Word2Vec group similar words closer in latent space. These vectors have properties like (Mikolov, Yih, and Zweig 2013):\nv(\u2032king\u2032)\u2212 v(\u2032man\u2032) + v(\u2032woman\u2032) \u2248 v(\u2032queen\u2032)\nHere, v(\u2032word\u2032) represents the vector of \u201cword\u201d. For the problem in hand, a Word2Vec model with 1000 dimensional vector output was trained using the entire dataset (removing English language stop words). For making a vector for representing each newsgroup post, all the words\u2019 vectors in the post were averaged."}, {"heading": "3.3 Network Architecture", "text": "The network used had 4 hidden layers. The number of neurons in the layers were:\n1000(input)\u21d2 300\u21d2 200\u21d2 200\u21d2 130\u21d2 20(target) \u21d2 5(hiddentarget)\nFrom hidden layer 1 (with 300 neurons), a branch was created to learn hidden target. The weights and biases are:\nWN , bN for connections from layer N \u2212 1 to layer N . WH , bH for connections from hidden layer tap to hidden target. Rectified Linear Units (ReLUs) were chosen as the activation functions of neurons since they have less likelihood of vanishing gradient (Nair and Hinton 2010). ReLU activation function is given by:\nf(x) = max(x, 0) (5)\nThe output layers (both final and hidden branch) used softmax logistic regression while the cost function was log multinomial loss. For hidden output cost function, L2 regularization was also added for weights of hidden layer 1. The training was performed using simple stochastic gradient descent using the algorithm explained in Section 2.1 with mini batch size of 256 and momentum value of 0.9. Since, the aim is comparison, no attempts were made to achieve higher than the state-of-the-art accuracies.\nThe network was implemented using the Python library for Deep Neural Networks, kayak 3.\n3Harvard Intelligent Probabilistic Systems (HIPS), https://github.com/HIPS/Kayak"}, {"heading": "3.4 Performance", "text": "Three training experiments were performed, as elaborated below:\n1. With simultaneous updates for the shared layers (100 epochs) + fine tuning (20 epochs)\n2. Without simultaneous updates for shared layer by ignoring gradients coming from hidden layer target (100 epochs) + fine tuning (20 epochs)"}, {"heading": "3. Training only using the hidden layer target (100 epochs) + fine tuning (20 epochs)", "text": "The fine tuning step only updates the hidden tap to hidden target weights and biases, WH , bH . This was performed to see the state of the losses of the network with respect to the hidden layer targets. All the three training experiments were performed with the same set of hyper-parameters and were repeated 20 times to account for the random variations. Values of mean training losses throughout the course of training were plotted using all 20 repetitions.\nThe plot of training losses for final layer target in experiment 1 and 2 is shown in Figure 2. From the plot, simultaneous training is seemingly performing better than direct training involving only target cost function minimization.\nPlot of training losses for hidden layer target in all three experiments is given in Figure 3. Here, training with only minimization of final cost is not able to generate enough effective representation of data to help in minimization of\nhidden cost function, while simultaneous training and training involving only hidden cost minimization are giving almost similar performance. The situation is clearer in Figure 4, which is plot of losses for hidden target during the fine tuning process for all the three experiments. As this graph shows, training only with final target cost in consideration is not able minimize loss well as compared to other two methods. Also, curve of simultaneous training starts with lesser loss than curve of training with hidden cost only. This depicts better updates of weights in simultaneous training as compared to training with only hidden cost.\nFigure 5 and 6 show box plots of the accuracies over the 20 repeated experiments for hidden and final targets.\nTable 2 shows the mean classification accuracy on final and hidden target for both training and testing set. As clear from the table and box plots, the simultaneous training is providing better performance than other training methods."}, {"heading": "4 Conclusion", "text": "This paper presented a branching architecture for neural networks that, when applied to appropriate problem with multiple level of outputs, inherently cause the hidden layers to store meaningful representations and helps in improving performance. The training curves showed that during simultaneous training, the shared layers were learning a representation that minimized both cost functions as well as had better weights for hidden targets.\nThe branches helps in enforcing information in hidden layers and thus the auxiliary branches can be added or removed easily from the network, this provides flexibility in terms of modularity and scalability of network.\nHidden Target Accuracy\nTrain (%) Test (%)\nHidden Training 85.320316 (1.523254) 82.822154 (0.417403)\nFinal Training 70.205044 (4.088195) 77.763680 (1.602464)\nSimultaneous Training 84.051977 (1.182006) 83.052736 (0.356259)\nFinal Target Accuracy\nTrain (%) Test (%)\nHidden Training 4.998253 (1.453776) 5.015446 (1.446461)\nFinal Training 74.332472 (2.295639) 69.088703 (1.325522)\nSimultaneous Training 76.824787 (1.792208) 69.205649 (1.183573)\nTable 2: Mean accuracies for the experiments. The values in parentheses are standard deviations."}, {"heading": "5 Future Work", "text": "This key concept in the proposed architecture is to exploit the hidden layers by meaningful representations. Using a hierarchy of target, the proposed architecture can form meaningful hidden representations.\nAn extended experiment can be done with many branches. Convolutional networks working on computer vision problems are ideal candidates for these tests, as it is easy to visualize the weights to find connections with the desired representations. Also, vision problems can be broken in many level of details and thus a hierarchy of outputs can be generated from single output layer.\nWhereas this paper focused on a problem involving branches from the hidden layers, an exploration can be done in which few hidden neurons directly represent the hidden targets without any branching. Further, work can be done for construction of multiple level of outputs from single output. This can be useful for computer vision problems, where different level of outputs can be practically useful."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Pascal Lamblin"], "venue": "Advances in", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning ?", "author": ["Erhan", "Dumitru", "Aaron Courville", "Pascal Vincent"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan", "Dumitru"], "venue": "In: International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan and Dumitru,? \\Q2009\\E", "shortCiteRegEx": "Erhan and Dumitru", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Yoshua Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets.", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation", "citeRegEx": "Hinton and E,? \\Q2006\\E", "shortCiteRegEx": "Hinton and E", "year": 2006}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of NAACLHLT June,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas"], "venue": "In: NIPS,", "citeRegEx": "Mikolov and Tomas,? \\Q2013\\E", "shortCiteRegEx": "Mikolov and Tomas", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Mikolov", "Tomas"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR", "citeRegEx": "Mikolov and Tomas,? \\Q2013\\E", "shortCiteRegEx": "Mikolov and Tomas", "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Vinod", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "author": ["Ranzato", "Marc Aurelio"], "venue": "Advances In Neural Information Processing Systems", "citeRegEx": "Ranzato and Aurelio,? \\Q2007\\E", "shortCiteRegEx": "Ranzato and Aurelio", "year": 2007}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya"], "venue": "JMLR W&CP", "citeRegEx": "Sutskever and Ilya,? \\Q2013\\E", "shortCiteRegEx": "Sutskever and Ilya", "year": 2013}, {"title": "Optimizing word2vec in gensim", "author": ["\u0158eh\u030au\u0159ek", "Radim"], "venue": "url: http://radimrehurek", "citeRegEx": "\u0158eh\u030au\u0159ek and Radim,? \\Q2013\\E", "shortCiteRegEx": "\u0158eh\u030au\u0159ek and Radim", "year": 2013}, {"title": "Text Understanding from Scratch", "author": ["Zhang", "Xiang", "Yann LeCun (Feb"], "venue": "In: eprint: 1502.01710", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes an architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. The branches provide a channel for enforcing useful information in hidden layer which helps in attaining better accuracy, both for the final layer and hidden layers. The shared layers modify their weights using the gradients of all cost functions higher than the branching layer. This model provides a flexible inference system with many levels of targets which is modular and can be used efficiently in situations requiring different levels of results according to complexity. This paper applies the idea to a text classification task on 20 Newsgroups data set with two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches. Author\u2019s Note September, 2016 This document essentially was (May 2015) a hasty write up of a project for a course on artificial neural networks during my undergraduate studies. I am adding this note here to point out mistakes which are detrimental to writings. I have kept the original content intact, adding only this box. Firstly, the document doesn\u2019t really use the terms (information, deep networks) from the title well in the analysis. Talking about the idea itself, there is a similar concept of auxiliary classifier in literature which uses [same] targets at lower levels to improve performance (See arXiv:1409.4842v1 [cs.CV] for example). -1 to literature review. Furthermore, the comparison is not rigorous enough to back up the claims and needs more meaningful test. 1 ar X iv :1 50 5. 00 38 4v 2 [ cs .N E ] 2 4 Se p 20 16", "creator": "LaTeX with hyperref package"}}}