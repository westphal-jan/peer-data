{"id": "1505.00387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2015", "title": "Highway Networks", "abstract": "there is plenty of theoretical and empirical evidence that optimal depth of neural networks is a crucial ingredient for their success. however, network training becomes more difficult with increasing depth and training capacity of very deep networks more remains usually an open problem. in this extended abstract, we introduce a new architecture designed to ease gradient - based training complexity of very rich deep networks. we refer to networks with this architecture as highway networks, largely since they allow perfectly unimpeded information flow processes across several layers appearing on \" information highways \". the architecture is characterized by the use of gating sensor units which learn to regulate the specific flow of information through learning a network. highway networks with fewer hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, directly opening up the possibility of studying extremely deep and efficient architectures.", "histories": [["v1", "Sun, 3 May 2015 01:56:57 GMT  (311kb,D)", "http://arxiv.org/abs/1505.00387v1", "Extended Abstract. 6 pages, 2 figures"], ["v2", "Tue, 3 Nov 2015 18:15:15 GMT  (319kb,D)", "http://arxiv.org/abs/1505.00387v2", "6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is atarXiv:1507.06228"]], "COMMENTS": "Extended Abstract. 6 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["rupesh kumar srivastava", "klaus greff", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1505.00387"}, "pdf": {"name": "1505.00387.pdf", "metadata": {"source": "META", "title": "Highway Networks", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "emails": ["RUPESH@IDSIA.CH", "KLAUS@IDSIA.CH", "JUERGEN@IDSIA.CH"], "sections": [{"heading": "1. Introduction", "text": "Many recent empirical breakthroughs in supervised machine learning have been achieved through the application of deep neural networks. Network depth (referring to the number of successive computation layers) has played perhaps the most important role in these successes. For instance, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% (Krizhevsky et al., 2012) to \u223c95% (Szegedy et al., 2014; Simonyan & Zisserman, 2014) through the use of ensembles of deeper architectures and smaller receptive fields (Ciresan et al., 2011a;b; 2012) in just a few years.\nOn the theoretical side, it is well known that deep networks can represent certain function classes exponentially\nmore efficiently than shallow ones (e.g. the work of Ha\u030astad (1987); Ha\u030astad & Goldmann (1991) and recently of Montufar et al. (2014)). As argued by Bengio et al. (2013), the use of deep networks can offer both computational and statistical efficiency for complex tasks.\nHowever, training deeper networks is not as straightforward as simply adding layers. Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al., 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015).\nIn this extended abstract, we present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information flow which is inspired by Long Short Term Memory recurrent neural networks (Hochreiter & Schmidhuber, 1995). Due to this gating mechanism, a neural network can have paths along which information can flow across several layers without attenuation. We call such paths information highways, and such networks highway networks.\nIn preliminary experiments, we found that highway networks as deep as 900 layers can be optimized using simple Stochastic Gradient Descent (SGD) with momentum. For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015). We show that optimization of highway networks is virtually independent of depth, while for traditional networks it suffers significantly as the number of layers increases. We also show that architectures comparable to those recently presented by Romero et al. (2014) can be directly trained to obtain similar test\nar X\niv :1\n50 5.\n00 38\n7v 1\n[ cs\n.L G\n] 3\nM ay\nset accuracy on the CIFAR-10 dataset without the need for a pre-trained teacher network."}, {"heading": "1.1. Notation", "text": "We use boldface letters for vectors and matrices, and italicized capital letters to denote transformation functions. 0 and 1 denote vectors of zeros and ones respectively, and I denotes an identity matrix. The function \u03c3(x) is defined as \u03c3(x) = 11+e\u2212x , x \u2208 R."}, {"heading": "2. Highway Networks", "text": "A plain feedforward neural network typically consists of L layers where the lth layer (l \u2208 {1, 2, ..., L}) applies a nonlinear transform H (parameterized by WH,l) on its input xl to produce its output yl. Thus, x1 is the input to the network and yL is the network\u2019s output. Omitting the layer index and biases for clarity,\ny = H(x,WH). (1)\nH is usually an affine transform followed by a non-linear activation function, but in general it may take other forms.\nFor a highway network, we additionally define two nonlinear transforms T (x,WT) and C(x,WC) such that\ny = H(x,WH)\u00b7T (x,WT) + x \u00b7 C(x,WC). (2)\nWe refer to T as the transform gate andC as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1\u2212 T , giving\ny = H(x,WH)\u00b7T (x,WT) + x \u00b7 (1\u2212 T (x,WT)). (3)\nThe dimensionality of x,y, H(x,WH) and T (x,WT) must be the same for Equation (3) to be valid. Note that this re-parametrization of the layer transformation is much more flexible than Equation (1). In particular, observe that\ny = { x, if T (x,WT) = 0, H(x,WH), if T (x,WT) = 1.\n(4)\nSimilarly, for the Jacobian of the layer transform,\ndy dx = { I, if T (x,WT) = 0, H \u2032(x,WH), if T (x,WT) = 1.\n(5)\nThus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of a plain layer and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the ith unit computes yi = Hi(x), a highway network consists of multiple blocks such that the ith block computes a block state Hi(x) and transform gate output Ti(x). Finally, it produces the block output yi = Hi(x) \u2217 Ti(x) + xi \u2217 (1 \u2212 Ti(x)), which is connected to the next layer."}, {"heading": "2.1. Constructing Highway Networks", "text": "As mentioned earlier, Equation (3) requires that the dimensionality of x,y, H(x,WH) and T (x,WT) be the same. In cases when it is desirable to change the size of the representation, one can replace x with x\u0302 obtained by suitably sub-sampling or zero-padding x. Another alternative is to use a plain layer (without highways) to change dimensionality and then continue with stacking highway layers. This is the alternative we use in this study.\nConvolutional highway layers are constructed similar to fully connected layers. Weight-sharing and local receptive fields are utilized for both H and T transforms. We use zero-padding to ensure that the block state and transform gate feature maps are the same size as the input."}, {"heading": "2.2. Training Deep Highway Networks", "text": "For plain deep networks, training with SGD stalls at the beginning unless a specific weight initialization scheme is used such that the variance of the signals during forward and backward propagation is preserved initially (Glorot & Bengio, 2010; He et al., 2015). This initialization depends on the exact functional form of H .\nFor highway layers, we use the transform gate defined as T (x) = \u03c3(WT\nTx+bT), where WT is the weight matrix and bT the bias vector for the transform gates. This suggests a simple initialization scheme which is independent of the nature of H: bT can be initialized with a negative value (e.g. -1, -3 etc.) such that the network is initially biased towards carry behavior. This scheme is strongly inspired by the proposal of Gers et al. (1999) to initially bias the gates in a Long Short-Term Memory recurrent network to help bridge long-term temporal dependencies early in learning. Note that \u03c3(x) \u2208 (0, 1),\u2200x \u2208 R, so the conditions in Equation (4) can never be exactly true.\nIn our experiments, we found that a negative bias initialization was sufficient for learning to proceed in very deep networks for various zero-mean initial distributions of WH and different activation functions used by H . This is significant property since in general it may not be possible to find effective initialization schemes for many choices ofH ."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Optimization", "text": "Very deep plain networks become difficult to optimize even if using the variance-preserving initialization scheme form (He et al., 2015). To show that highway networks do not suffer from depth in the same way we train run a series of experiments on the MNIST digit classification dataset. We measure the cross entropy error on the training set, to investigate optimization, without conflating them with generalization issues.\nWe train both plain networks and highway networks with the same architecture and varying depth. The first layer is always a regular fully-connected layer followed by 9, 19, 49, or 99 fully-connected plain or highway layers and a single softmax output layer. The number of units in each layer is kept constant and it is 50 for highways and 71 for plain networks. That way the number of parameters is roughly the same for both. To make the comparison fair we run a random search of 40 runs for both plain and highway networks to find good settings for the hyperparameters. We optimized the initial learning rate, momentum, learning rate decay rate, activation function for H (either ReLU or tanh) and, for highway networks, the value for the transform gate bias (between -1 and -10). All other weights were initialized following the scheme introduced by (He et al., 2015).\nThe convergence plots for the best performing networks for each depth can be seen in Figure 1. While for 10 layers plain network show very good performance, their performance significantly degrades as depth increases. Highway networks on the other hand do not seem to suffer from an increase in depth at all. The final result of the 100 layer highway network is about 1 order of magnitude better than the 10 layer one, and is on par with the 10 layer plain network. In fact, we started training a similar 900 layer high-\nway network on CIFAR-100 which is only at 80 epochs as of now, but so far has shown no signs of optimization difficulties. It is also worth pointing out that the highway networks always converge significantly faster than the plain ones."}, {"heading": "3.2. Comparison to Fitnets", "text": "Deep highway networks are easy to optimize, but are they also beneficial for supervised learning where we are interested in generalization performance on a test set? To address this question, we compared highway networks to the thin and deep architectures termed Fitnets proposed recently by Romero et al. (2014) on the CIFAR-10 dataset augmented with random translations. Results are summarized in Table 1.\nRomero et al. (2014) reported that training using plain backpropogation was only possible for maxout networks with depth up to 5 layers when number of parameters was limited to\u223c250K and number of multiplications to\u223c30M. Training of deeper networks was only possible through the use of a two-stage training procedure and addition of soft targets produced from a pre-trained shallow teacher network (hint-based training). Similarly it was only possible to train 19-layer networks with a budget of 2.5M parameters using hint-based training.\nWe found that it was easy to train highway networks with number of parameters and operations comparable to fitnets directly using backpropagation. As shown in Table 1, Highway 1 and Highway 4, which are based on the architecture of Fitnet 1 and Fitnet 4 respectively obtain similar or higher accuracy on the test set. We were also able to train thinner and deeper networks: a 19-layer highway network with \u223c1.4M parameters and a 32-layer highway network with \u223c1.25M parameter both perform similar to the teacher network of Romero et al. (2014)."}, {"heading": "4. Analysis", "text": "In Figure 2 we show some inspections on the inner workings of the best1 50 hidden layer fully-connected highway networks trained on MNIST (top row) and CIFAR100 (bottom row). The first three columns show, for each transform gate, the bias, the mean activity over 10K random samples, and the activity for a single random sample respectively. The block outputs for the same single sample are displayed in the last column.\nThe transform gate biases of the two networks were initialized to -2 and -4 respectively. It is interesting to note that contrary to our expectations most biases actually decreased further during training. For the CIFAR-100 network the biases increase with depth forming a gradient. Curiously this gradient is inversely correlated with the average activity of the transform gates as seen in the second column. This indicates that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective. This behavior is also suggested by the fact that the transform gate activity for a single example (column 3) is very sparse. This effect is more pronounced for the CIFAR100 network, but can also be observed to a lesser extent in the MNIST network.\nThe last column of Figure 2 displays the block outputs and clearly visualizes the concept of \u201cinformation highways\u201d. Most of the outputs stay constant over many layers forming a pattern of stripes. Most of the change in outputs happens in the early layers (\u2248 10 for MNIST and \u2248 30 for CIFAR-100). We hypothesize that this difference is due to the higher complexity of the CIFAR-100 dataset.\n1obtained via random search over hyperparameters to minimize the best training set error achieved using each configuration\nIn summary it is clear that highway networks actually utilize the gating mechanism to pass information almost unchanged through many layers. This mechanism serves not just as a means for easier training, but is also heavily used to route information in a trained network. We observe very selective activity of the transform gates, varying strongly in reaction to the current input patterns."}, {"heading": "5. Conclusion", "text": "Learning to route information through neural networks has helped to scale up their application to challenging problems by improving credit assignment and making training easier (Srivastava et al., 2015). Even so, training very deep networks has remained difficult, especially without considerably increasing total network size.\nHighway networks are novel neural network architectures which enable the training of extremely deep networks using simple SGD. While the traditional plain neural architectures become increasingly difficult to train with increasing network depth (even with variance-preserving initialization), our experiments show that optimization of highway networks is not hampered even as network depth increases to a hundred layers.\nThe ability to train extremely deep networks opens up the possibility of studying the impact of depth on complex problems without restrictions. Various activation functions which may be more suitable for particular problems but for which robust initialization schemes are unavailable can be used in deep highway networks. Future work will also attempt to improve the understanding of learning in highway networks."}, {"heading": "Acknowledgments", "text": "This research was supported by the by EU project \u201cNASCENCE\u201d (FP7-ICT-317662). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A committee of neural networks for traffic sign classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Masci", "Jonathan", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gers et al\\.", "year": 1999}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Computational limitations of small-depth circuits", "author": ["H\u00e5stad", "Johan"], "venue": "MIT press,", "citeRegEx": "H\u00e5stad and Johan.,? \\Q1987\\E", "shortCiteRegEx": "H\u00e5stad and Johan.", "year": 1987}, {"title": "On the power of small-depth threshold circuits", "author": ["H\u00e5stad", "Johan", "Goldmann", "Mikael"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad et al\\.,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad et al\\.", "year": 1991}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "[cs],", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Technical Report FKI-207-95,", "citeRegEx": "Hochreiter et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "URL http://jmlr.org/ proceedings/papers/v38/lee15a.html", "author": ["Lee", "Chen-Yu", "Xie", "Saining", "Gallagher", "Patrick", "Zhang", "Zhengyou", "Tu", "Zhuowen"], "venue": "Deeply-supervised nets. pp", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "FitNets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "[cs],", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "URL http://arxiv.org/abs/1312", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "[cs],", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Understanding locally competitive networks", "author": ["Srivastava", "Rupesh Kumar", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "[cs],", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "For instance, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% (Krizhevsky et al., 2012) to \u223c95% (Szegedy et al.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": ", 2012) to \u223c95% (Szegedy et al., 2014; Simonyan & Zisserman, 2014) through the use of ensembles of deeper architectures and smaller receptive fields (Ciresan et al.", "startOffset": 16, "endOffset": 66}, {"referenceID": 0, "context": "As argued by Bengio et al. (2013), the use of deep networks can offer both computational and statistical efficiency for complex tasks.", "startOffset": 13, "endOffset": 34}, {"referenceID": 11, "context": "Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al.", "startOffset": 122, "endOffset": 181}, {"referenceID": 6, "context": "Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al.", "startOffset": 122, "endOffset": 181}, {"referenceID": 10, "context": ", 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al., 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al.", "startOffset": 60, "endOffset": 109}, {"referenceID": 14, "context": ", 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015).", "startOffset": 82, "endOffset": 122}, {"referenceID": 9, "context": ", 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015).", "startOffset": 82, "endOffset": 122}, {"referenceID": 6, "context": "For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015).", "startOffset": 119, "endOffset": 159}, {"referenceID": 6, "context": "For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015). We show that optimization of highway networks is virtually independent of depth, while for traditional networks it suffers significantly as the number of layers increases. We also show that architectures comparable to those recently presented by Romero et al. (2014) can be directly trained to obtain similar test ar X iv :1 50 5.", "startOffset": 143, "endOffset": 428}, {"referenceID": 6, "context": "For plain deep networks, training with SGD stalls at the beginning unless a specific weight initialization scheme is used such that the variance of the signals during forward and backward propagation is preserved initially (Glorot & Bengio, 2010; He et al., 2015).", "startOffset": 223, "endOffset": 263}, {"referenceID": 2, "context": "This scheme is strongly inspired by the proposal of Gers et al. (1999) to initially bias the gates in a Long Short-Term Memory recurrent network to help bridge long-term temporal dependencies early in learning.", "startOffset": 52, "endOffset": 71}, {"referenceID": 6, "context": "Very deep plain networks become difficult to optimize even if using the variance-preserving initialization scheme form (He et al., 2015).", "startOffset": 119, "endOffset": 136}, {"referenceID": 6, "context": "All other weights were initialized following the scheme introduced by (He et al., 2015).", "startOffset": 70, "endOffset": 87}, {"referenceID": 10, "context": "Deep highway networks are easy to optimize, but are they also beneficial for supervised learning where we are interested in generalization performance on a test set? To address this question, we compared highway networks to the thin and deep architectures termed Fitnets proposed recently by Romero et al. (2014) on the CIFAR-10 dataset augmented with random translations.", "startOffset": 292, "endOffset": 313}, {"referenceID": 10, "context": "25M parameter both perform similar to the teacher network of Romero et al. (2014).", "startOffset": 61, "endOffset": 82}, {"referenceID": 10, "context": "Network Number of Layers Number of Parameters Accuracy Fitnet Results reported by Romero et al. (2014) Teacher 5 \u223c9M 90.", "startOffset": 82, "endOffset": 103}, {"referenceID": 10, "context": "For comparison, results reported by Romero et al. (2014) using maxout networks are also shown.", "startOffset": 36, "endOffset": 57}, {"referenceID": 13, "context": "Learning to route information through neural networks has helped to scale up their application to challenging problems by improving credit assignment and making training easier (Srivastava et al., 2015).", "startOffset": 177, "endOffset": 202}], "year": 2015, "abstractText": "There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on information highways. The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.", "creator": "LaTeX with hyperref package"}}}