{"id": "1505.00863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions", "abstract": "2008 our frdc _ qa team participated in the qa - lab english subtask investigation of the ntcir - 11. in this paper, we describe our system mechanism for smoothly solving real - world university entrance exam questions, which are related to world history. wikipedia account is used as the main external resource for our system. since problems with choosing right / wrong sentence from multiple partial sentence choices account for answering about two - thirds of the total, we therefore individually design a classification based inference model formula for solving this singular type of questions. for other types of questions, we also design some simple methods.", "histories": [["v1", "Tue, 5 May 2015 02:06:23 GMT  (690kb)", "http://arxiv.org/abs/1505.00863v1", "5 pages, no figure"]], "COMMENTS": "5 pages, no figure", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["shuangyong song", "yao meng", "zhongguang zheng", "jun sun"], "accepted": false, "id": "1505.00863"}, "pdf": {"name": "1505.00863.pdf", "metadata": {"source": "CRF", "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions: FRDC_QA at NTCIR-11 QA-Lab Task", "authors": ["Shuangyong Song", "Yao Meng", "Zhongguang Zheng", "Jun Sun", "Huan Zhong"], "emails": ["sunjun}@cn.fujitsu.com"], "sections": [{"heading": null, "text": "subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.\nTeam Name FRDC_QA\nSubtask QA-Lab English Subtask\nKeywords Question Answering, The National Center Test for University Admissions, World History, Feature Extraction, Classification Model."}, {"heading": "1. INTRODUCTION", "text": "Question Answering (QA) is a specialized area in Information Retrieval. QA systems are concerned with providing relevant answers in response to questions proposed in natural language. QA is therefore composed of three distinct modules: question classification, information retrieval, and answer extraction, each of which has a core component beside other supplementary components [7]. Question classification plays an essential role in QA systems by classifying the submitted question according to its type.\nIn particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10]. NTCIR-11 QALab task aims to provide a module-based platform for system performance evaluations and comparisons for solving real-world university entrance exam questions, which are selected from The National Center Test for University Admissions and from secondary exams at 4 universities in Japan.\nFRDC_QA take part in the English subtask. We design a system and the details of it are given as follows: In section 2, we introduce the external resource and some convenient storage ways. The framework for questions with multiple sentence\nchoices is proposed in section 3. In section 4, the frameworks for other types of questions are described. The evaluation results of our system on world history exam B in 2007 Japan University Admissions are given in section 5. Finally we make a conclusion and discuss our plans for future work in section 6."}, {"heading": "2. HASH MAP & LUCENE INDEXES OF EXTERNAL RESOURCE", "text": ""}, {"heading": "2.1 External Resource", "text": "We utilize Wikipedia as external resource for our QA-Lab task. Wikipedia is a well-known free content, multilingual encyclopedia written collaboratively by contributors around the world [6]. In this paper, for the English subtask, we download the Wikipedia dataset with the version of \u2018enwiki dump progress on 20140502\u2019 from Wikimedia Downloads 1 . Downloaded dataset contains \u2018enwiki-20140502-all-titles\u2019 as the list of all the Wiki-items, and \u2018enwiki-20140502-pages-articles\u2019 with articles of all the Wiki-items. All those data will be processed to be more formal and then be stored in hash map or Lucene2 indexes, for realizing convenient and quick search in our QA system. The details are given below."}, {"heading": "2.2 Hash Map of Item Title", "text": "For quickly checking if a word or word group is a Wikipedia item, we put all Wikipedia item titles into a hash map. The title list dataset contains 32,877,103 titles of Wikipedia items in total, and we convert all characters of them to be lowercase. Word or word groups will also be converted to be characters in lowercase when they are checked, for realizing an exact matching. When we detect items contained in a sentence, we adopt a Maximum Matching Method. For example, for a sentence with N words, we first check if this whole sentence is a Wiki item, and then check all sub-sentences with N-1 continuous words, then subsentences with length of N-2, and so on. In particular, if a detected item consist of another detected item, the latter one will be removed and the longer one will be reserved."}, {"heading": "2.3 Lucene Index of Item Page", "text": "Each item has its related Wiki article, describing the details of this item. We put the title and page content into a Lucene index file as two word string fields, and then we can easily get the description of a Wiki item with simple Lucene search. This index file is used to search the relationship between items, since two related items will show in the Wiki article of each other, vice versa.\n1 http://download.wikipedia.com/enwiki/20140502 2 http://lucene.apache.org"}, {"heading": "2.4 Lucene Index of Item Redirection", "text": "Different Wiki items may have same meanings, such as \u2018AccessibleComputing\u2019 and \u2018Computer accessibility\u2019. For those items with same meanings, Wikipedia utilize \u2018redirection\u2019 tag to link one of them to another. Therefore, just one of them has a Wiki article with detailed description, and the Wiki article of another item just contains one sentence with redirection declaration. Take the \u2018AccessibleComputing\u2019 and \u2018Computer accessibility\u2019 for example, the Wiki article of \u2018Computer accessibility\u2019 contains detailed description of this item, but the Wiki article of \u2018AccessibleComputing\u2019 just contains one sentence \u201cRedirect page  Computer accessibility\u201d. We put those \u2018AccessibleComputing\u2019 like redirected Wiki item titles into a Lucene index file as one word string field, and take the related item titles as another word string field, then we can easily search the real description of those redirected Wiki items."}, {"heading": "2.5 Lucene Index of Item Time", "text": "For answering some questions about items\u2019 occurrence time, we extract the time information of each item from the Wiki articles and put those information into a Lucene index. There are two different types of time information we should extract, one is the exact time of this item, and another is the period of this item. Such as the time of \u2018Independence Day (United States)\u2019 is \u2018The Fourth of July\u2019, and the period of \u2018French and Indian War\u2019 is \u20181754\u20131763\u2019. For the period type, we respectively record the front part as \u2018start time\u2019 and latter part as \u2018end time\u2019 since lots of questions may ask them separately."}, {"heading": "3. FRAMEWORK FOR QUESTIONS WITH MULTIPLE SENTENCE CHOICES", "text": ""}, {"heading": "3.1 Brief Description of Framework", "text": "We first introduce the type of questions with multiple sentence choices, and an example is given below (italic characters):\nBackground text: \u2026 \u2026 (8) In India in the latter half of the 19th century, a large-scale rebellion against colonial rule took place; one of the things that triggered this was the fact that Muslim soldiers revolted due to a rumor that pork fat had been used on the cartridges in their guns. \u2026 \u2026\nQuestion: From 1-4 below, choose the most appropriate sentence concerning the underlined portion (8).\nChoices: 1. This rebellion is also called the Sipahi (Sepoy) Mutiny. 2. The Ever Victorious Army was actively involved in suppressing\nthis rebellion.\n3. The Ever Victorious Army was actively involved in suppressing\nthis rebellion.\n4. After this rebellion, Queen Victoria also became the empress of\nthe Mughal Empire.\nWe take this type of questions as accuracy probability ranking problem for all the choices, and we utilize classification models to handle this problem. By separating right choices and wrong choices in the training dataset, binary classification models can be trained. Seven features, such as semantic relationship between background text and choices, semantic relationship between question and choices, etc., are extracted for training classification models, and eleven classifiers are selected to calculate the accuracy probability of each choice. Then the results of those classifiers are combined together to get the final ranking of choices. Details of this framework are given in following subsections."}, {"heading": "3.2 Features", "text": "We extracted seven features of each choice in total for classifiers, and the details of them are given below:\n1) Internal Item Relativity:\nWe first detect all items contained in a choice sentence with the Maximum Matching Method described in section 2.2, and then detect relationships among those items. The method for judging if or not two items are related is detecting if an item shows in the Wiki article of another item. For an choice sentence consisting of N items, we can get N(N-1)/2 \u2018item couple\u2019, and each related \u2018item couple\u2019 will contribute 1 point to this feature. Therefore, the value of this feature \u2018Internal Item Relativity\u2019 will be from 0 to N(N-1)/2.\n2) Item Relativity between Text and Choice:\nAll the items contained in the \u2018text portion\u2019 and choice sentence need to be detected first, and relationships between Text items and Choice items will be detected. For a text sentence consisting of M items and a choice sentence consisting of N items, we can get M*N \u2018item couple\u2019, and each related \u2018item couple\u2019 will contribute 1 point to this feature. Therefore, the value of this feature \u2018Item Relativity between Text and Choice\u2019 will be from 0 to M*N.\n3) Item Relativity between Question and Choice:\nAll the items contained in the question sentence and choice sentence need to be detected first, and relationships between Question items and Choice items will be detected. For a question sentence consisting of Q items and a choice sentence consisting of N items, we can get Q*N \u2018item couple\u2019, and each related \u2018item couple\u2019 will contribute 1 point to this feature. Therefore, the value of this feature \u2018Item Relativity between Question and Choice\u2019 will be from 0 to Q*N.\n4) Minimum Distance with Negative Sentences:\nWe assume that one choice \u2018more similar with a negative sentence, more likely to be a wrong answer\u2019. For getting this feature, we firstly need to exact all the negative sentences in Wiki articles, which contains \u2018is not\u2019, \u2018are not\u2019, \u2018did not\u2019 or other negative expressions. After removing stop words in choice sentences and those negative sentences, all of them can be represented as word vectors. Distance between two word vectors V1 and V2 is calculated with the formula below:\n1 2( ) 2\n1 2 1 2\n1\n( , ) ( ) L V V\ni i\ni D V V w w \n  (1)\nin which, the D(V1,V2) means distance between vector V1 and vector V2, and the V1\u222aV2 means the union of V1 and V2, and the L(V1\u222aV2) means the length of V1\u222aV2, and the w1i means the value of V1 on the ith dimension of V1\u222aV2, and the w2i means the value of V2 on the ith dimension of V1\u222aV2. This formula is modified from the Euclidean Distance [2], without firstly creating the vector of words in all Wiki articles and choices, which is very time consuming and with low robustness.\n5) Number of Related Wiki Articles:\nWith the \u2018Lucene Index of Item Page\u2019 described in section 2.3, we take a choice as a query and search all the possible related Wiki articles from this index file. The number of returned Wiki articles will be taken as the value of this feature.\n6) Similarity with Top 1 Related Wiki Article:\nThe search method is same as the above feature, but the value of this feature is the value of the semantic similarity between the choice sentence and the top 1 returned Wiki article, which is very easy to get with a ready-made function in Lucene system.\n7) Similarity with Top 3 Related Wiki Articles:\nThe search method is same as the above feature, but the value of this feature is the average value of the semantic similarity between the choice sentence and the top 3 returned Wiki articles."}, {"heading": "3.3 Classifiers", "text": "In our system, we in total utilize eleven classifiers to training different classification models respectively. Simple description of them are given below:\nRandom Forest: Random forest is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees [3].\nLogitBoost: LogitBoost is a boosting algorithm that casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm [4].\nLogistic Model Trees: Logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning [11].\nAdaBoost M1: AdaBoost M1 is an improved version of traditional AdaBoost algorithm, which can be used to classify both binary and polynominal label with numerical, binominal and polynominal (and weighted) attributes [12].\nBagging: Bagging, also called bootstrap aggregating, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid over-fitting [13].\nMultiBoostAB: MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction [14].\nLocally Weighted Learning: Locally weighted learning uses an instance-based algorithm to assign instance weights which are then used by a specified Weighted Instances Handler. Can do classification (e.g. using naive Bayes) or regression (e.g. using linear regression) [15].\nLogistic Regression: Logistic regression is a probabilistic statistical classification model, which can be used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable based on one or more predictor variables. [16].\nSimple Na\u00efve Bayes: Naive Bayes classifier, in which the numeric attributes are modelled by a normal distribution [17].\nNa\u00efve Bayes: An improved Naive Bayes classifier using estimator classes. Numeric estimator precision values are chosen based on analysis of the training data [18].\nUpdateable Na\u00efve Bayes: An updateable version of Na\u00efve Bayes model. This classifier will use a default precision of 0.1 for numeric attributes when build Classifier is called with zero training instances [18].\nAll of the classification model training procedures are realized with WEKA [5], which is a collection of machine learning algorithms for data mining tasks and contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization."}, {"heading": "3.4 Choice Selection", "text": "Each classifier can get an accuracy probability for each choice, and the average value of the accuracy probability from all classifiers will be taken as the final accuracy probability of a choice. Then, if the question is asking us to choose the right choice with the keywords \u2018correct\u2019, \u2018correctly\u2019 or \u2018appropriate\u2019, we choose the choice with highest accuracy probability as the final answer. If the question is asking us to choose the wrong choice with the keywords \u2018incorrect\u2019, \u2018incorrectly\u2019 or \u2018mistake\u2019, we choose the choice with lowest accuracy probability as the final answer."}, {"heading": "4. FRAMEWORKS FOR OTHER TYPES", "text": ""}, {"heading": "OF QUESTIONS", "text": "In this section, we give some description of our frameworks for problems besides the type with multiple sentence choices, such as questions with chronological sequence choices, questions with term choices, etc."}, {"heading": "4.1 Framework for Questions with Chronological Sequence Choices (without images)", "text": "We first give an example of this type of question (italic characters):\nBackground text: \u2026 \u2026 (7) A Cold War began between the US and the Soviet Union, and the world faced another serious conflict. In relation to this \u2026 \u2026\nQuestion: In regard to the underlined portion (7), from (1)-(4) below, choose the correct chronological sequence of events relating to the Cold War.\nChoices: 1. Warsaw Treaty Organization formed - Berlin blockade - Cuban\nmissile crisis - Japan-US Security Treaty signed (1951)\n2. Berlin blockade - Japan-US Security Treaty signed (1951) -\nWarsaw Treaty Organization formed - Cuban missile crisis\n3. Japan-US Security Treaty signed (1951) - Cuban missile crisis -\nBerlin blockade - Warsaw Treaty Organization formed\n4. Berlin blockade - Warsaw Treaty Organization formed - Japan-\nUS Security Treaty signed (1951) - Cuban missile crisis\nFor this type of questions, we utilize the \u2018Lucene Index of Item Time\u2019 to search timestamp of each event in the choices, and rank them with the chronological order, then we can choose the right answer according to this order easily."}, {"heading": "4.2 Framework for Questions with Term Choices (without images)", "text": "An example of this type of question is given below (italic characters):\nBackground text: \u2026 \u2026 (1) Nomadic tribes on horseback emerged on the Eurasian continent. Their elusive character became a major threat to sedentary agricultural societies, so troops mounted on horseback were organized to counteract them. Rulers who sought good horses also emerged, such as \u2026 \u2026\nQuestion: In regard to the underlined portion (1), from 1-4 below, choose the one name that correctly describes the nomadic tribe on horseback that came to prominence in the 6th century and built up a nation.\nChoices: 1. Scythians 2. G\u00f6kt\u00fcrks 3. Yuezhi 4. Xiongnu\nWe detect items contained in the background text and the question with the Maximum Matching Method, then calculate the relativity between those items and the choice item, with using the same method described in section 3.2. Finally, the choice with highest relativity with the background text and the question will be chosen as the final answer."}, {"heading": "4.3 Framework for Questions with Judging True or False Sentences (without images)", "text": "An example of this type of question is given below (italic characters):\nBackground text: \u2026 \u2026 (3) founder of the kingdom - is believed to be the Chumo who appears in the \"Book of Wei (Weishu)\", which is a record of the Northern Wei dynasty\u2026 \u2026\nQuestion: In regard to the underlined portion (3), from 1-4 below, choose the correct combination of \"correct\" and \"incorrect\" in regard to the following sentences taa and b concerning the historic founder of the kingdom.\nQuestion text: a Liu Bang defeated Xiang Yu and made Chang'an the capital. b Yel\u00fc Dashi built the Kara-Khitan Khanate.\nChoices: 1. a - Correct b - Correct 2. a - Correct b - Incorrect 3. a - Incorrect b - Correct 4. a - Incorrect b - Incorrect\nWe use the same training data with same features as described in section 3.2 to train Support Vector Machine classification model (SVM) to handle this type of questions by directly output the \u2018true of false\u2019 result of each choice instead of the accuracy probability. Then we can easily choose the right choice according to the output of the SVM model."}, {"heading": "4.4 Framework for Other types of Questions", "text": "We choose the final answer with the random selection method for other types of questions, which usually need image analysis technology. In particular, we set a specified random seed to keep the stability of the results given by our system."}, {"heading": "5. EVALUATION RESULTS", "text": "Table 1 gives the evaluation results of our system on the phase 1 contest data - world history exam B in 2007 Japan University Admissions.\nFor types \u2018Questions with multiple sentence choices\u2019 and \u2018Questions with term choices (without images)\u2019, we achieve a precision of about 45% on both \u2018Number of correct answer\u2019 and \u2018Score of correct answer\u2019, which shows the much better effectiveness than random method, since we think the precision of random method should be 25% on four-choice questions. However, the real result of random method on \u2018other types of questions\u2019 is not as good as our thought. We got wrong answers on all the seven \u2018other types of questions\u2019 with the random method, which makes our total result getting a precision of 37%, far below the 45%."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In our work of NTCIR-11 QA-Lab task, we design a system for solving real-world university entrance exam questions, which are related to world history. We utilize Wikipedia as the main external resource for our system, since nearly all of world history knowledge can be found in Wikipedia. In addition, we design different solution frameworks for different types of questions, such as questions with multiple sentence choices, questions with temporal term choices, questions with nontemporal term choices, etc. Although our system performs much better than random methods, it is still far from meeting actual demand. Several attempts can be tried to improve the system performance in our future work, e.g., (1) more useful external resources can be utilized, such as query results from Google like search engines, electronic history books, etc. (2) more reasonable and intelligent combination way for different classification models should be tried; (3) different writing styles for timestamps, locations and personal names should be considered. Furthermore, a unified domain insensitive system for choosing wrong/right answer from multiple sentence choice will be a trial in our future work."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "We sincerely thank our colleagues in Fujitsu Laboratories Ltd., Takuya Makino, Hiroko Suzuki, Tomoya Iwakura and Tetsuro Takahashi, for their help on providing us external training dataset and valuable discussions with us about feature extraction and machine learning methods."}, {"heading": "8. REFERENCES", "text": "[1] Kano, Y. 2014. Solving History Problems of the National\nCenter Test for University Admissions. In Proceedings of\nthe 28th Annual Conference of the Japanese Society for Artificial Intelligence.\n[2] Song, S., Li, Q. and Bao, H. 2012. Detecting Dynamic\nAssociation among Twitter Topics. In Proceedings of the 21st International World Wide Web Conference (Lyon, France, April 16-20, 2012), pages 605-606.\n[3] Breiman, L. 2001. Random forests. Machine Learning.\n45(1): 5\u201332.\n[4] Friedman, J., Hastie, T. and Tibshirani, R. 2000. Additive\nlogistic regression: a statistical view of boosting. Annals of Statistics. 28(2): 337\u2013407.\n[5] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann,\nP., Witten, I. H. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations. Volume 11, Issue 1.\n[6] Bhole, A., Fortuna, B., Grobelnik, M. and Mladeni\u0107, D.\n2007. Extracting Named Entities and Relating Them over Time Based on Wikipedia. Informatica (Slovenia). 31(4): 463-468.\n[7] Allam, A. M. N., Haggag, M. H. 2012. The Question\nAnswering Systems: A Survey. International Journal of Research and Reviews in Information Sciences. Vol. 2, No. 3, September 2012.\n[8] Martin R, F., Sibyl, H., Veronika, K. 2005. Answering\nmultiple-choice questions in high-stakes medical examinations. Medical Education, Vol. 39, No. 9, September 2005, pp. 890-894.\n[9] Awadallah, R., Rauber, A. 2006. Web-Based multiple\nchoice question answering for english and arabic questions, In Proceeding of the 28th European conference on Advances in Information Retrieval, pp. 515-518.\n[10] Sorger, B., Dahmen, B., Reithler, J., Gosseries, O.,\nMaudoux, A., Laureys, S., Goebel, R. 2009. Another kind of \u2018BOLD Response\u2019: answering multiple-choice questions via online decoded single-trial brain signals. Progress in Brain Research, Vol. 177, 2009, pp. 275\u2013292.\n[11] Landwehr, N., Hall, M. A., Frank, E. 2003. Logistic Model\nTrees. In Proceeding of the 14th European Conference on Machine Learning. pp. 241-252.\n[12] Eibl, G., Pfeiffer, K. P. 2002. How to Make AdaBoost.M1\nWork for Weak Base Classifiers by Changing Only One Line of the Code. In Proceeding of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. pp. 72-83.\n[13] Breiman, L. 1996. Bagging predictors. Machine Learning,\nvol. 24, no. 2, pp. 123-140.\n[14] Webb, G. I. 2000. MultiBoosting: A Technique for\nCombining Boosting and Wagging. Machine Learning. Vol.40, no. 2, pp. 159-196.\n[15] Atkeson, C. G., Moore, A. W., Schaal, S. 1997. Locally\nWeighted Learning. Artificial Intelligence Review, vol. 11, no. 1, pp. 11-73.\n[16] Bishop, C. M., Nasrabadi, N. M. 2007. Pattern Recognition\nand Machine Learning. Journal of Electronic Imaging, vol. 16, no. 4.\n[17] Duda R., Hart, P. 1973. Pattern Classification and Scene\nAnalysis. Wiley, New York.\n[18] John, G. H., Langley, P. 1995. Estimating Continuous\nDistributions in Bayesian Classifiers. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. pp. 338-345."}], "references": [{"title": "Solving History Problems of the National Center Test for University Admissions", "author": ["Y. Kano"], "venue": "In Proceedings of  the 28th Annual Conference of the Japanese Society for Artificial Intelligence", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Detecting Dynamic Association among Twitter Topics", "author": ["S. Song", "Q. Li", "H. Bao"], "venue": "In Proceedings of the 21st International World Wide Web Conference", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations. Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Extracting Named Entities and Relating Them over Time Based on Wikipedia", "author": ["A. Bhole", "B. Fortuna", "M. Grobelnik", "D. Mladeni\u0107"], "venue": "Informatica (Slovenia)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The Question Answering Systems: A Survey", "author": ["A.M.N. Allam", "M.H. Haggag"], "venue": "International Journal of Research and Reviews in Information Sciences", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Answering multiple-choice questions in high-stakes medical examinations", "author": ["F. Martin R", "H. Sibyl", "K. Veronika"], "venue": "Medical Education,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Web-Based multiple choice question answering for english and arabic questions", "author": ["R. Awadallah", "A. Rauber"], "venue": "In Proceeding of the 28th European conference on Advances in Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Another kind of \u2018BOLD Response\u2019: answering multiple-choice questions via online decoded single-trial brain signals", "author": ["B. Sorger", "B. Dahmen", "J. Reithler", "O. Gosseries", "A. Maudoux", "S. Laureys", "R. Goebel"], "venue": "Progress in Brain Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Logistic Model Trees", "author": ["N. Landwehr", "M.A. Hall", "E. Frank"], "venue": "In Proceeding of the 14th European Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code", "author": ["G. Eibl", "K.P. Pfeiffer"], "venue": "In Proceeding of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "MultiBoosting: A Technique for Combining Boosting and Wagging", "author": ["G.I. Webb"], "venue": "Machine Learning. Vol.40,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Locally Weighted Learning", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Artificial Intelligence Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop", "N.M. Nasrabadi"], "venue": "Journal of Electronic Imaging,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Pattern Classification and Scene Analysis", "author": ["Duda R", "P. Hart"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}, {"title": "Estimating Continuous Distributions in Bayesian Classifiers", "author": ["G.H. John", "P. Langley"], "venue": "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}], "referenceMentions": [{"referenceID": 5, "context": "QA is therefore composed of three distinct modules: question classification, information retrieval, and answer extraction, each of which has a core component beside other supplementary components [7].", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 6, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 7, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 8, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 4, "context": "Wikipedia is a well-known free content, multilingual encyclopedia written collaboratively by contributors around the world [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "This formula is modified from the Euclidean Distance [2], without firstly creating the vector of words in all Wiki articles and choices, which is very time consuming and with low robustness.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "Logistic Model Trees: Logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning [11].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "AdaBoost M1: AdaBoost M1 is an improved version of traditional AdaBoost algorithm, which can be used to classify both binary and polynominal label with numerical, binominal and polynominal (and weighted) attributes [12].", "startOffset": 215, "endOffset": 219}, {"referenceID": 11, "context": "It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "using linear regression) [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Simple Na\u00efve Bayes: Naive Bayes classifier, in which the numeric attributes are modelled by a normal distribution [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Numeric estimator precision values are chosen based on analysis of the training data [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "1 for numeric attributes when build Classifier is called with zero training instances [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "All of the classification model training procedures are realized with WEKA [5], which is a collection of machine learning algorithms for data mining tasks and contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization.", "startOffset": 75, "endOffset": 78}], "year": 2015, "abstractText": "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.", "creator": "Microsoft\u00ae Word 2013"}}}