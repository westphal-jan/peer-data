{"id": "1505.04497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "A Definition of Happiness for Reinforcement Learning Agents", "abstract": "what is happiness for reinforcement learning agents? we seek a formal definition satisfying a general list of desiderata. our proposed definition exactly of happiness is the temporal difference error, i. e. the difference between the value of the obtained reward and observation and thus the agent's expectation sum of this value. this definition satisfies most of our term desiderata and is compatible with empirical research on intelligence humans. occasionally we state several simple implications and discuss examples.", "histories": [["v1", "Mon, 18 May 2015 03:14:39 GMT  (37kb,D)", "http://arxiv.org/abs/1505.04497v1", "AGI 2015"]], "COMMENTS": "AGI 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mayank daswani", "jan leike"], "accepted": false, "id": "1505.04497"}, "pdf": {"name": "1505.04497.pdf", "metadata": {"source": "CRF", "title": "A Definition of Happiness for Reinforcement Learning Agents\u2217", "authors": ["Mayank Daswani", "Jan Leike"], "emails": [], "sections": [{"heading": null, "text": "Keywords. Temporal difference error, reward prediction error, pleasure, wellbeing, optimism, machine ethics"}, {"heading": "1 Introduction", "text": "People are constantly in search of better ways to be happy. However, philosophers and psychologists have not yet agreed on a notion of human happiness. In this paper, we pursue the more general goal of defining happiness for intelligent agents. We focus on the reinforcement learning (RL) setting [SB98] because it is an intensively studied formal framework which makes it easier to make precise statements. Moreover, reinforcement learning has been used to model behaviour in both human and non-human animals [Niv09].\nHere, we decouple the discussion of happiness from the discussion of consciousness, experience, or qualia. We completely disregard whether happiness is actually consciously experienced or what this means. The problem of consciousness has to be solved separately; but its answer might matter insofar that it could tell us which agents\u2019 happiness we should care about.\nDesiderata. We can simply ask a human how happy they are. But artificial reinforcement learning agents cannot yet speak. Therefore we use our human \u201ccommon sense\u201d intuitions about happiness to come up with a definition. We arrive at the following desired properties.\n\u2217Research supported by the People for the Ethical Treatment of Reinforcement Learners http://petrl.org. This is the extended technical report. The final publication is available at http://link.springer.com. \u2020Both authors contributed equally.\nar X\niv :1\n50 5.\n04 49\n7v 1\n[ cs\n.A I]\n1 8\nM ay\n2 01\n\u2022 Scaling. Happiness should be invariant under scaling of the rewards. Replacing every reward rt by crt+d for some c, d \u2208 R with c > 0 (independent of t) does not change the reinforcement learning problem in any relevant way. Therefore we desire a happiness measure to be independent under rescaling of the rewards.\n\u2022 Subjectivity. Happiness is a subjective property of the agent depending only on information available to the agent. For example, it cannot depend on the true environment.\n\u2022 Commensurability. The happiness of different agents should be comparable. If at some time step an agent A has happiness x, and another agent B has happiness y, then it should be possible to tell whether A is happier than B by computing x\u2212 y. This could be relaxed by instead asking that A can calculate the happiness of B according to A\u2019s subjective beliefs.\n\u2022 Agreement. The happiness function should match experimental data about human happiness.\nIt has to be emphasised that in humans, happiness cannot be equated with pleasure [RSDD14]. In the reinforcement learning setting, pleasure corresponds to the reward. Therefore happiness and reward have to be distinguished. We crudely summarise this as follows; for a more detailed discussion see Section 3.\npleasure = reward 6= happiness\nThe happiness measure that we propose is the following. An agent\u2019s happiness in a time step t is the difference between the value of the obtained reward and observation and the agent\u2019s expectation of this value at time step t. In the Markov setting, this is also known as the temporal difference error (TD error) [SB90]. However, we do not limit ourselves to the Markov setting in this paper. In parts of the mammalian brain, the neuromodulator dopamine has a strong connection to the TD error [Niv09]. Note that while our definition of happiness is not equal to reward it remains highly correlated to the reward, especially if the expectation of the reward is close to 0.\nOur definition of happiness coincides with the definition for joy given by Jacobs et al. [JBJ14], except that the latter is weighted by 1 minus the (objective) probability of taking the transition which violates subjectivity. Schmidhuber\u2019s work on \u2018intrinsic motivation\u2019 adds a related component to the reward in order to motivate the agent to explore in interesting directions [Sch10].\nOur definition of happiness can be split into two parts. (1) The difference between the instantaneous reward and its expectation, which we call payout, and (2) how the latest observation and reward changes the agent\u2019s estimate of future rewards, which we call good news. Moreover, we identify two sources of happiness: luck, favourable chance outcomes (e.g. rolling a six on a fair die), and pessimism, having low expectations of the environment (e.g. expecting a fair die to be biased against you). We show that agents that know the world perfectly have zero expected happiness. Proofs can be found in Appendix A.\nIn the rest of the paper, we use our definition as a starting point to investigate the following questions. Is an off-policy agent happier than an on-policy one? Do monotonically increasing rewards necessarily imply a happy agent? How does value function initialisation affect the happiness of an agent? Can we construct an agent that maximises its own happiness?"}, {"heading": "2 Reinforcement Learning", "text": "In reinforcement learning (RL) an agent interacts with an environment in cycles: at time step t the agent chooses an action at \u2208 A and receives an observation ot \u2208 O and a real-valued reward rt \u2208 R; the cycle then repeats for time step t + 1 [SB98]. The list of interactions a1o1r1a2o2r2 . . . is called a history. We use ht to denote a history of length t, and we use the shorthand notation h := ht\u22121 and h\n\u2032 := ht\u22121atotrt. The agent\u2019s goal is to choose actions to maximise cumulative rewards. To avoid infinite sums, we use a discount factor \u03b3 with 0 < \u03b3 < 1 and maximise the discounted sum \u2211\u221e t=1 \u03b3\ntrt. A policy is a function \u03c0 mapping every history to the action taken after seeing this history, and an environment \u00b5 is a stochastic mapping from histories to observation-rewardtuples.\nA policy \u03c0 together with an environment \u00b5 yields a probability distribution over histories. Given a random variable X over histories, we write the \u03c0-\u00b5expectation of X conditional on the history h as E\u03c0\u00b5[X | h].\nThe (true) value function V \u03c0\u00b5 of a policy \u03c0 in environment \u00b5 maps a history ht to the expected total future reward when interacting with environment \u00b5 and taking actions according to the policy \u03c0:\nV \u03c0\u00b5 (ht) := E\u03c0\u00b5 [\u2211\u221e k=t+1 \u03b3 k\u2212t\u22121rk | ht ] . (1)\nIt is important to emphasise that E\u03c0\u00b5 denotes the objective expectation that can be calculated only by knowing the environment \u00b5. The optimal value function V \u2217\u00b5 is defined as the value function of the optimal policy, V \u2217 \u00b5 (h) := sup\u03c0 V \u03c0 \u00b5 (h).\nTypically, reinforcement learners do not know the environment and are trying to learn it. We model this by assuming that at every time step the agent has (explicitly or implicitly) an estimate V\u0302 of the value function V \u03c0\u00b5 . Formally, a value function estimator maps a history h to a value function estimate V\u0302 . Finally, we define an agent to be a policy together with a value function estimator. If the history is clear from context, we refer to the output of the value function estimator as the agent\u2019s estimated value.\nIf \u00b5 only depends on the last observation and action, \u00b5 is called Markov decision process (MDP). In this case, \u00b5(otrt | ht\u22121at) = \u00b5(otrt | ot\u22121at) and the observations are called states (st = ot). In MDPs we use the Q-value function, the value of a state-action pair, defined asQ\u03c0\u00b5(st, at) := E\u03c0\u00b5 [\u2211\u221e k=t+1 \u03b3 k\u2212t\u22121rk | stat ] . Assuming that the environment is an MDP is very common in the RL literature, but here we will not make this assumption."}, {"heading": "3 A Formal Definition of Happiness", "text": "The goal of a reinforcement learning agent is to maximise rewards, so it seems natural to suppose an agent is happier the more rewards it gets. But this does not conform to our intuition: sometimes enjoying pleasures just fails to provide happiness, and reversely, enduring suffering does not necessarily entail unhappiness (see Example 3 and Example 7). In fact, it has been shown empirically that rewards and happiness cannot be equated [RSDD14] (p-value < 0.0001).\nThere is also a formal problem with defining happiness in terms of reward: we can add a constant c \u2208 R to every reward. No matter how the agent-environment interaction plays out, the agent will have received additional cumulative rewards\nC := \u2211t i=1 c. However, this did not change the structure of the reinforcement learning problem in any way. Actions that were optimal before are still optimal and actions that are slightly suboptimal are still slightly suboptimal to the same degree. For the agent, no essential difference between the original reinforcement learning problem and the new problem can be detected: in a sense the two problems are isomorphic. If we were to define an agent\u2019s happiness as received reward, then an agent\u2019s happiness would vary wildly when we add a constant to the reward while the problem stays structurally exactly the same.\nWe propose the following definition of happiness.\nDefinition 1 (Happiness). The happiness of a reinforcement learning agent with estimated value V\u0302 at time step t with history hat while receiving observation ot and reward rt is\n,(hatotrt, V\u0302 ) := rt + \u03b3V\u0302 (hatotrt)\u2212 V\u0302 (h). (2)\nIf ,(h\u2032, V\u0302 ) is positive, we say the agent is happy, and if ,(h\u2032, V\u0302 ) is negative, we say the agent is unhappy.\nIt is important to emphasise that V\u0302 represents the agent\u2019s subjective estimate of the value function. If the agent is good at learning, this might converge to something close to the true value function V \u03c0\u00b5 . In an MDP (2) is also known as the temporal difference error [SB90]. This number is used used to update the value function, and thus plays an integral part in learning.\nIf there exists a probability distribution \u03c1 on histories such that the value function estimate V\u0302 is given by the expected future discounted rewards according to the probability distribution \u03c1,\nV\u0302 (h) = E\u03c0\u03c1 [\u2211\u221e k=t+1 \u03b3 k\u2212t\u22121rk | h ] , (3)\nthen we call E := E\u03c0\u03c1 the agent\u2019s subjective expectation. Note that we can always find such a probability distribution, but this notion only really makes sense for model-based agents (agents that learn a model of their environment). Using the agent\u2019s subjective expectation, we can rewrite Definition 1 as follows.\nProposition 2 (Happiness as Subjective Expectation). Let E denote an agent\u2019s subjective expectation. Then\n,(h\u2032, V\u0302 ) = rt \u2212 E[rt | h] + \u03b3 ( V\u0302 (h\u2032)\u2212 E[V\u0302 (haor) | h] ) . (4)\nProposition 2 states that happiness is given by the difference of how good the agent thought it was doing and what it learns about how well it actually does. We distinguish the following two components in (4):\n\u2022 Payout: the difference of the obtained reward rt and the agent\u2019s expectation of that reward E[rt | h].\n\u2022 Good News: the change in opinion of the expected future rewards after receiving the new information otrt.\n,(h\u2032, V\u0302 ) = rt \u2212 E[rt | h]\ufe38 \ufe37\ufe37 \ufe38 payout\n+\u03b3 ( V\u0302 (h\u2032)\u2212 E[V\u0302 (haor) | h]\ufe38 \ufe37\ufe37 \ufe38\ngood news\n)\nExample 3. Mary is travelling on an air plane. She knows that air planes crash very rarely, and so is completely at ease. Unfortunately she is flying on a budget airline, so she has to pay for her food and drink. A flight attendant comes to her seat and gives her a free beverage. Just as she starts drinking it, the intercom informs everyone that the engines have failed. Mary feels some happiness from the free drink (payout), but her expected future reward is much lower than in the state before learning the bad news. Thus overall, Mary is unhappy.\nFor each of the two components, payout and good news, we distinguish the following two sources of happiness.\n\u2022 Pessimism:1 the agent expects the environment to contain less rewards than it actually does.\n\u2022 Luck: the outcome of rt is unusually high due to randomness.\nrt \u2212 E[rt | h] = rt \u2212 E\u03c0\u00b5[rt | h]\ufe38 \ufe37\ufe37 \ufe38 luck +E\u03c0\u00b5[rt | h]\u2212 E[rt | h]\ufe38 \ufe37\ufe37 \ufe38 pessimism\nV\u0302 (h\u2032)\u2212 E[V\u0302 (haor) | h] = V\u0302 (h\u2032)\u2212 E\u03c0\u00b5[V\u0302 (haor) | h]\ufe38 \ufe37\ufe37 \ufe38 luck\n+ E\u03c0\u00b5[V\u0302 (haor) | h]\u2212 E[V\u0302 (haor) | h]\ufe38 \ufe37\ufe37 \ufe38 pessimism\nExample 4. Suppose Mary fears flying and expected the plane to crash (pessimism). On hearing that the engines failed (bad luck), Mary does not experience very much change in her future expected reward. Thus she is happy that she (at least) got a free drink.\nThe following proposition states that once an agent has learned the environment, its expected happiness is zero. In this case, underestimation cannot contribute to happiness and thus the only source of happiness is luck, which cancels out in expectation.\nProposition 5 (Happiness of Informed Agents). An agent that knows the world has an expected happiness of zero: for every policy \u03c0 and every history h,\nE\u03c0\u00b5[,(h\u2032, V \u03c0\u00b5 ) | h] = 0.\nAnalogously, if the environment is deterministic, then luck cannot be a source of happiness. In this case, happiness reduces to how much the agent underestimates the environment. By Proposition 5, having learned a deterministic environment perfectly, the agent\u2019s happiness is equal to zero."}, {"heading": "4 Matching the Desiderata", "text": "Here we discuss in which sense our definition of happiness satisfies the desiderata from Section 1.\n1Optimism is a standard term in the RL literature to denote the opposite phenomenon. However, this notion is somewhat in discord with optimism in humans.\nScaling. If we transform the rewards to r\u2032t = crt + d with c > 0, d \u2208 R for each time step t without changing the value function, the value of , will be completely different. However, a sensible learning algorithm should be able to adapt to the new reinforcement learning problem with the scaled rewards without too much problem. At that point, the value function gets scaled as well, Vnew(h) = cV (h) + d/(1\u2212 \u03b3). In this case we get\n,(hatotr\u2032, Vnew) = r\u2032t + \u03b3Vnew(hatotr \u2032 t)\u2212 Vnew(h)\n= crt + d+ \u03b3cV (hatotr \u2032 t) + \u03b3\nd 1\u2212 \u03b3 \u2212 cV (h)\u2212 d\n1\u2212 \u03b3 = c ( rt + \u03b3V (hatotr \u2032 t)\u2212 V (h) ) ,\nhence happiness gets scaled by a positive factor and thus its sign remains the same, which would not hold if we defined happiness just in terms of rewards.\nSubjectivity. The definition (4) of , depends only on the current reward and the agent\u2019s current estimation of the value function, both of which are available to the agent.\nCommensurability. The scaling property as described above means that the exact value of the happiness is not useful in comparing two agents, but the sign of the total happiness can at least tell us whether a given agent is happy or unhappy. Arguably, failing this desideratum is not surprising; in utility theory the utilities/rewards of different agents are typically not commensurable either.\nHowever, given two agents A and B, A can still calculate the A-subjective happiness of a history experienced by B as ,(haorB , V\u0302 A). This corresponds to the human intuition of \u201cputting yourself in someone else\u2019s shoes\u201d. If both agents are acting in the same environment, the resulting numbers should be commensurable, since the calculation is done using the same value function. It is entirely possible that A believes B to be happier, i.e. ,(haorB , V\u0302 A) > ,(haorA, V\u0302 A), but also that B believes A to be happier ,(haorA, V\u0302 B) > ,(haorB , V\u0302 B), because they have different expectations of the environment.\nAgreement. Rutledge et al. measure subjective well-being on a smartphonebased experiment with 18,420 participants [RSDD14]. In the experiment, a subject goes through 30 trials in each of which they can choose between a sure reward and a gamble that is resolved within a short delay. Every two to three trials the subjects are asked to indicate their momentary happiness.\nOur model based on Proposition 2 with a very simple learning algorithm and no loss aversion correlates fairly well with reported happiness (mean r = 0.56, median r2 = 0.41, median R2 = 0.27) while fitting individual discount factors, comparative to Rutledge et al.\u2019s model (mean r = 0.60, median r2 = 0.47, median R2 = 0.36) and a happiness=cumulative reward model (mean r = 0.59, median r2 = 0.46, median R2 = 0.35). This analysis is inconclusive, but unsurprisingly so: the expected reward is close to 0 and thus our happiness model correlates well with rewards. See Appendix B for the details of our data analysis.\nThe hedonic treadmill [BC71] refers to the idea that humans return to a baseline level of happiness after significant negative or positive events. Stud-\nies have looked at lottery winners and accident victims [BCJB78], and people dealing with paralysis, marriage, divorce, having children and other life changes [DLS06]. In most cases these studies have observed a return to baseline happiness after some period of time has passed; people learn to make correct reward predictions again. Hence their expected happiness returns to zero (Proposition 5). Our definition unfortunately does not explain why people have different baseline levels of happiness (or hedonic set points), but these may be perhaps explained by biological means (different humans have different levels of neuromodulators, neurotransmitters, hormones, etc.) which may move their baseline happiness. Alternatively, people might simply learn to associate different levels of happiness with \u201cfeeling happy\u201d according to their environment."}, {"heading": "5 Discussion and Examples", "text": ""}, {"heading": "5.1 Off-policy Agents", "text": "In reinforcement learning, we are mostly interested in learning the value function of the optimal policy. A common difference between RL algorithms is whether they learn off-policy or on-policy. An on-policy agent evaluates the value of the policy it is currently following. For example, the policy that the agent is made to follow could be an \u03b5-greedy policy, where the agent picks arg maxaQ\n\u03c0(h, a) a fraction (1\u2212 \u03b5) of the time, and a random action otherwise. If \u03b5 is decreased to zero over time, then the agent\u2019s learned policy tends to the optimal policy in MDPs. Alternatively, an agent can learn off-policy, that is it can learn about one policy (say, the optimal one) while following a different behaviour policy.\nThe behaviour policy (\u03c0b) determines how the agent acts while it is learning the optimal policy. Once an off-policy learning agent has learned the optimal value function V \u2217\u00b5 , then it is not happy if it still acts according to some other (possibly suboptimal) policy.\nProposition 6 (Happiness of Off-Policy Learning). Let \u03c0 be some policy and \u00b5 be some environment. Then for any history h\nE\u03c0\u00b5[,(h\u2032, V \u2217\u00b5 ) | h] \u2264 0.\nQ-learning is an example of an off-policy algorithm in the MDP setting. If Q-learning converges, and the agent is still following the sub-optimal behaviour policy then Proposition 6 tells us that the agent will be unhappy. Moreover, this means that SARSA (an on-policy RL algorithm) will be happier than Q-learning on average and in expectation."}, {"heading": "5.2 Increasing and Decreasing Rewards", "text": "Intuitively, it seems that if things are constantly getting better, this should increase happiness. However, this is not generally the case: even an agent that obtains monotonically increasing rewards can be unhappy if it thinks that these rewards mean even higher negative rewards in the future.\nExample 7. Alice has signed up for a questionable drug trial which examines the effects of a potentially harmful drug. This drug causes temporary pleasure to the user every time it is used, and increased usage results in increased pleasure.\nHowever, the drug reduces quality of life in the long term. Alice has been informed of the potential side-effects of the drug. She can be either part of a placebo group or the group given the drug. Every morning Alice is given an injection of an unknown liquid. She finds herself feeling temporary but intense feelings of pleasure. This is evidence that she is in the non-placebo group, and thus has a potentially reduced quality of life in the long term. Even though she experiences pleasure (increasing rewards) it is evidence of very bad news and thus she is unhappy.\nAnalogously, decreasing rewards do not generally imply unhappiness. For example, the pains of hard labour can mean happiness if one expects to harvest the fruits of this labour in the future."}, {"heading": "5.3 Value Function Initialisation", "text": "Example 8 (Increasing Pessimism Does Not Increase Happiness). Consider the deterministic MDP example in Figure 1. Assume that the agent has an initial value function Q\u03020(s0, \u03b1) = 0, Q\u03020(s0, \u03b2) = \u2212\u03b5, Q\u03020(s1, \u03b1) = \u03b5 and Q\u03020(s1, \u03b2) = 0. If no forced exploration is carried out by the agent, it has no incentive to visit s1. The happiness achieved by such an agent for some time step t is ,(s0\u03b1s00, V\u03020) = 0 where V\u03020(s0) := Q\u03020(s0, \u03b1) = 0. However, suppose the agent is (more optimistically) initialised with Q\u03020(s0, \u03b1) = 0, Q\u03020(s0, \u03b2) = \u03b5. In this case, the agent would take action \u03b2 and arrive in state s1. This transition would have happiness ,(s0\u03b2s1\u22121, V\u03020) = \u22121 + \u03b3Q\u03020(s1, \u03b1) \u2212 Q\u03020(s0, \u03b2) = \u22121 \u2212 0.5\u03b5. However, the next transition is s1\u03b1s12 which has happiness ,(s1\u03b1s12, V\u03020) = 2 + \u03b3Q\u03020(s1, \u03b1) \u2212 Q\u03020(s1, \u03b1) = 2 \u2212 0.5\u03b5. If Q\u03020 is not updated by some learning mechanism the agent will continue to accrue this positive happiness for all future time steps. If the agent does learn, it will still be some time steps before Q\u0302 converges to Q\u2217 and the positive happiness becomes zero (see Figure 2). It\nis arguable whether this agent which suffered one time step of unhappiness but potentially many time steps of happiness is overall a happier agent, but it is some evidence that absolute pessimism does not necessarily lead to the happiest agents."}, {"heading": "5.4 Maximising Happiness", "text": "How can an agent increase their own happiness? The first source of happiness, luck, depends entirely on the outcome of a random event that the agent has no control over. However, the agent could modify its learning algorithm to be systematically pessimistic about the environment. For example, when fixing the value function estimation below rmin/(1\u2212 \u03b3) for all histories, happiness is positive at every time step. But this agent would not actually take any sensible actions. Just as optimism is commonly used to artificially increase exploration, pessimism discourages exploration which leads to poor performance. As demonstrated in Example 8, a pessimistic agent may be less happy than a more optimistic one.\nAdditionally, an agent that explicitly tries to maximise its own happiness is no longer a reinforcement learner. So instead of asking how an agent can increase its own happiness, we should fix a reinforcement learning algorithm and ask for the environment that would make this algorithm happy."}, {"heading": "6 Conclusion", "text": "An artificial superintelligence might contain subroutines that are capable of suffering, a phenomenon that Bostrom calls mind crime [Bos14, Ch. 8]. More generally, Tomasik argues that even current reinforcement learning agents could have moral weight [Tom14]. If this is the case, then a general theory of happiness for reinforcement learners is essential; it would enable us to derive ethical standards in the treatment of algorithms. Our theory is very preliminary and should be thought of as a small step in this direction. Many questions are left unanswered, and we hope to see more research on the suffering of AI agents in the future.\nAcknowledgements. We thank Marcus Hutter and Brian Tomasik for careful reading and detailed feedback. The data from the smartphone experiment was kindly provided by Robb Rutledge. We are also grateful to many of our friends for encouragement and interesting discussions."}, {"heading": "A Omitted Proofs", "text": "Proof of Proposition 2.\n,(h\u2032, V\u0302 ) = rt + \u03b3V\u0302 (h\u2032)\u2212 V\u0302 (h)\n= rt + \u03b3V\u0302 (h \u2032)\u2212 E [ \u221e\u2211 k=t \u03b3k\u2212trk | h ]\n= rt \u2212 E[rt | h] + \u03b3V\u0302 (h\u2032)\u2212 E [ \u221e\u2211 k=t+1 \u03b3k\u2212trk | h ]\n= rt \u2212 E[rt | h] + \u03b3V\u0302 (h\u2032)\u2212 \u03b3E [ E [ \u221e\u2211 k=t+1 \u03b3k\u2212t\u22121rk | haor ] | h ] = rt \u2212 E[rt | h] + \u03b3V\u0302 (h\u2032)\u2212 \u03b3E [ V\u0302 (haor) | h ] ,\nwhere the second to last equality uses the tower property for conditional expectations.\nProof of Proposition 5. For the true value function V \u03c0\u00b5 , a subjective expectation exists by definition since (3) is the same as (1). Hence we can apply Proposition 2:\nE\u03c0\u00b5[,(h\u2032, V \u03c0\u00b5 ) | h] = E\u03c0\u00b5[rt \u2212 E\u03c0\u00b5[rt | h] + \u03b3V \u03c0\u00b5 (h\u2032)\u2212 \u03b3E\u03c0\u00b5[V \u03c0\u00b5 (haor) | h] | h] = E\u03c0\u00b5[rt | h]\u2212 E\u03c0\u00b5[E\u03c0\u00b5[rt | h] | h] + \u03b3E\u03c0\u00b5[V \u03c0\u00b5 (h\u2032) | h]\u2212 \u03b3E\u03c0\u00b5[E\u03c0\u00b5[V \u03c0\u00b5 (haor) | h] | h] = E\u03c0\u00b5[rt | h]\u2212 E\u03c0\u00b5[rt | h] + \u03b3E\u03c0\u00b5[V \u03c0\u00b5 (h\u2032) | h]\u2212 \u03b3E\u03c0\u00b5[V \u03c0\u00b5 (haor) | h] = 0\nProof of Proposition 6. Let h\u2032 be any history of length t, and let \u03c0\u2217 denote an optimal policy for environment \u00b5, i.e., V \u2217\u00b5 = V \u03c0\u2217\n\u00b5 . In this case, we have V \u2217\u00b5 (h) \u2265 V \u2217\u00b5 (h\u03c0(h)), and hence\nE\u03c0 \u2217\n\u00b5 [ \u221e\u2211 k=t \u03b3k\u2212trk | h ] \u2265 E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t \u03b3k\u2212trk | h\u03c0(h) ] . (5)\nWe use this in the following:\nE\u03c0\u00b5[,(h\u2032, V \u2217\u00b5 ) | h] = E\u03c0\u00b5[rt + \u03b3V \u2217\u00b5 (h\u2032)\u2212 V \u2217\u00b5 (h) | h]\n= E\u03c0\u00b5 [ rt + \u03b3E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t+1 \u03b3k\u2212t\u22121rk | h\u2032 ] \u2212 E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t \u03b3k\u2212trk | h ] | h ] (5) \u2264 E\u03c0\u00b5 [ rt + E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t+1 \u03b3k\u2212trk | h\u2032 ] \u2212 E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t \u03b3k\u2212trk | hat ] | h\n] = E\u03c0\u00b5 [ rt \u2212 E\u03c0 \u2217\n\u00b5 [rt | hat]\ufe38 \ufe37\ufe37 \ufe38 =0\n+ E\u03c0 \u2217\n\u00b5 [ \u221e\u2211 k=t+1 \u03b3k\u2212trk | h\u2032 ] \u2212 E\u03c0 \u2217 \u00b5 [ \u221e\u2211 k=t+1 \u03b3k\u2212trk | hat ] \ufe38 \ufe37\ufe37 \ufe38\n=0\n| h ]\nsince E\u03c0\u00b5[E\u03c0 \u2217 \u00b5 [X | hatotrt] | h] = E\u03c0\u00b5[E\u03c0 \u2217\n\u00b5 [X | hat] | h] because conditional on hat, the distribution of ot and rt is independent of the policy."}, {"heading": "B Data Analysis", "text": "In this section we describe the details of the data analysis on the Great Brain Experiment2 conducted by Rutledge et al. [RSDD14]. This experiment measures subjective well-being on a smartphone app and had 18,420 subjects. Each subject goes through 30 trials and starts with 500 points. At the start of a trial, the subject is given an option to choose between a certain reward (CR) and a 50\u201350 gamble between two outcomes. The gamble is pictorially shown to have an equal probability between the outcomes, thus making it easy for the subject to choose between the certain reward and the gamble.\nBefore the trials start the agent is asked to rate their current happiness on a scale of 0-100. The slider for the scale is initialised randomly. Every two to three trials and 12 times in each play the subject is asked the question \u201cHow happy are you at this moment?\u201d\nWe model this experiment as a reinforcement learning problem with 150 different states representing the different trials that the subject can see (the possible combinations of certain rewards and gamble outcomes), and two actions certain and gamble. For example, in a trial the subject could be presented with the choice between a certain reward of 30 and a 50\u201350 gamble between 0 and 72.\nThe expected reward after each state is uniquely determined by the state\u2019s description, but the agent has subjective uncertainty about the value of the state, since it does not know which states (types of trials) will follow the current one or how these states are distributed. (In the experiment, the a priori expected outcome of a trial (the average value of the states) was 5.5 points, and the maximum gain or loss in a single trial is 220 points.) Furthermore, the subject might incorrectly estimate the value of the gamble: although the visualisation correctly suggests that each of the two outcomes is equally likely, the subject might be uncertain about this, or simply compute the average incorrectly.\nRutledge et al. model happiness as an affine-linear combination of the certain reward (CR), the expected value of the gamble (EV) and the reward prediction error at the outcome of the gamble (RPE). The weights for this affine-linear combination were learned through linear regression on fMRI data from another similar experiment on humans (wCR = 0.52, wEV = 0.35, and wRPE = 0.8 for z-scored happiness ratings).\nThe data was kindly provided to us by Rutledge. We disregard the first happiness rating that occurs before the first trial. Moreover, we removed all 762 subjects whose happiness ratings (other than the first) had a standard deviation of 0.\n2http://www.thegreatbrainexperiment.com/\nTo test our happiness definition on the data, we have to model how humans estimate the value of a history. We chose a very simple model where a subject\u2019s expectation before each trial is the average of the previous outcomes. We use the empirical distribution function, that estimates all future rewards as the average outcome so far:\n\u03c1(rn | h) := #{k \u2264 t | rk = rn}\nt for n > t.\nWith Equation 3, this gives a value function estimate of V\u0302 (a1o1r1 . . . atotrt) = 1 t(1\u2212\u03b3) \u2211t i=1 ri.\nWe assume that the subject typically computes the expected value of the trial correctly, i.e. E[rt | h] := max{CR,EV}. We calculate happiness at time t with the formula from Proposition 2.\nSince subjects\u2019 happiness ratings are not given on every trial, we use geometric discounting with discount factor \u03b3 to aggregate happiness from previous trials:\npredicted happiness := t\u2211 k=1 \u03b3t\u2212k,(hk, V\u0302k),\nwhere hk is the history at time step k and V\u0302k is the value function estimate at time step k. For each subject, we optimise \u03b3 such that the Pearson correlation r is maximised. The result is that the majority of subjects use a \u03b3 of either very close to 0 or very close to 1 (see Figure 3). A possible explanation of this phenomenon could be that it is unclear to the human subjects whether they are to indicate their instantaneous or their cumulative happiness.\nWe use the sample Pearson correlation r, its square r2 and the coefficient of determination R2 to evaluate each model. For n data points (in our case n = 11,\nthe number of happiness ratings per subject), the sample Pearson correlation r is defined as\nr = rxy = \u2211n i=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\u221a\u2211n\ni=1(xi \u2212 x\u0304)2 \u2211n i=1(yi \u2212 y\u0304)2\nwhere x\u0304 = 1n \u2211n i=1 xi is the sample mean. The coefficient of determination R 2 on n data points with (training) data x \u2208 X and predictions y \u2208 Y is\nR2 = 1\u2212 \u2211n i=1(yi \u2212 xi)2\u2211n i=1(xi \u2212 x\u0304)2\nWe calculate R2 only on z-scored terms; z-scoring does not affect r. Our model\u2019s predicted happiness correlates fairly well with reported happiness (mean r = 0.56, median r2 = 0.41, median R2 = 0.27) while fitting individual discount factors, comparative to Rutledge et al.\u2019s model (mean r = 0.60, median r2 = 0.47, median R2 = 0.36) and a happiness=cumulative reward model (mean r = 0.59, median r2 = 0.46, median R2 = 0.35). So this analysis is inconclusive. Figure 4 and Figure 5 show the distribution of the correlation coefficients. We emphasise that our model was derived a priori and then tested on the data, while their model has three parameters (weights for CR, EV, and RPE) that were fitted on data from humans using linear regression.\nIt is well known that humans do not value rewards linearly. In particular, people appear to weigh losses more heavily than gains, an effect known as loss aversion. To model this, we set\nreward :=\n{ outcome\u03b1 if outcome > 0\n\u2212\u03bb \u00b7 (\u2212outcome)\u03b2 otherwise.\nUsing parameters found by Rutledge et al. (mean \u03bb = 1.7, mean \u03b1 = 1.05, mean \u03b2 = 1.01) we also get a slightly improved agreement (mean r = 0.60, median r2 = 0.48, median R2 = 0.39). Table 1 lists these results."}], "references": [{"title": "Hedonic relativism and planning the good society", "author": ["Philip Brickman", "Donald T Campbell"], "venue": "Adaptation-Level Theory, pages 287\u2013305,", "citeRegEx": "Brickman and Campbell.,? \\Q1971\\E", "shortCiteRegEx": "Brickman and Campbell.", "year": 1971}, {"title": "Lottery winners and accident victims: Is happiness relative", "author": ["Philip Brickman", "Dan Coates", "Ronnie Janoff-Bulman"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "Brickman et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Brickman et al\\.", "year": 1978}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Beyond the hedonic treadmill: Revising the adaptation theory of well-being", "author": ["Ed Diener", "Richard E Lucas", "Christie Napa Scollon"], "venue": "American Psychologist,", "citeRegEx": "Diener et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Diener et al\\.", "year": 2006}, {"title": "distress, hope, and fear in reinforcement learning", "author": ["Elmer Jacobs", "Joost Broekens", "Catholijn Jonker. Joy"], "venue": "In Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Jacobs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 2014}, {"title": "Reinforcement learning in the brain", "author": ["Yael Niv"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Niv.,? \\Q2009\\E", "shortCiteRegEx": "Niv.", "year": 2009}, {"title": "A computational and neural model of momentary subjective well-being", "author": ["Robb B Rutledge", "Nikolina Skandali", "Peter Dayan", "Raymond J Dolan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Rutledge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutledge et al\\.", "year": 2014}, {"title": "Time-derivative models of Pavlovian reinforcement", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1990\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1990}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Do artificial reinforcement-learning agents matter morally", "author": ["Brian Tomasik"], "venue": "Technical report,", "citeRegEx": "Tomasik.,? \\Q2014\\E", "shortCiteRegEx": "Tomasik.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent\u2019s expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.", "creator": "LaTeX with hyperref package"}}}