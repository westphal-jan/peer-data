{"id": "1505.04984", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Risk and Regret of Hierarchical Bayesian Learners", "abstract": "common statistical practice has shown that the full power of bayesian methods is not realized until hierarchical priors are used, as these allow for greater \" robustness \" and the ability intended to \" share statistical strength. \" yet it is simultaneously an ongoing challenge to provide a learning - theoretically sound formalism of such notions that : offers practical guidance concerning when and how best to utilize hierarchical models ; provides insights into what makes for a good enough hierarchical prior ; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. we present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. we provide regret bounds under log - loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. we also show how to naturally convert a bayesian log - loss regret bound into a bayesian risk bound values for any bounded loss, a result which may be of independent interest. risk and regret bounds for student's $ t $ and hierarchical gaussian priors allow us to formalize fully the concepts of \" robustness \" and \" sharing statistical strength. \" priors for feature selection are investigated efficiently as well. our results suggest that the learning - theoretic benefits of using hierarchical priors technologies can for often come without at little cost on practical problems.", "histories": [["v1", "Tue, 19 May 2015 13:12:41 GMT  (190kb,D)", "http://arxiv.org/abs/1505.04984v1", "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jonathan h huggins", "joshua b tenenbaum"], "accepted": true, "id": "1505.04984"}, "pdf": {"name": "1505.04984.pdf", "metadata": {"source": "META", "title": "Risk and Regret of Hierarchical Bayesian Learners", "authors": ["Jonathan H. Huggins", "Joshua B. Tenenbaum"], "emails": ["JHUGGINS@MIT.EDU", "JBT@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "There are two standard justifications for the use of hierarchical models. The first is that they allow for the representation of greater uncertainty by placing \u201chyperpriors\u201d on\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nthe hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences.\nWithin the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model. The motivation of Salakhutdinov et al. (2011) was that, for image classification tasks, some categories of objects (e.g., \u201ccar\u201d or \u201cdog\u201d) have many labeled positive and negative examples while other, visually related, categories (e.g., \u201cbus\u201d or \u201canteater\u201d) have only a few labeled examples. Fig. 1(right, a) shows the distribution of training examples for the 200 object categories used while Fig. 1(right, b) shows the same distribution, but now objects are grouped with those with similar appearances. In both cases, the distributions are fat-tailed: there are a few categories with many training examples and many categories with a few training examples. It was hypothesized that by using a hierarchical Bayesian model, the classes with large amounts of labeled data could be used to construct better classifiers for the classes with small amounts of labeled data.\nThe model used by Salakhutdinov et al. (2011), which\nar X\niv :1\n50 5.\n04 98\n4v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\n01 5\nRisk and Regret of Hierarchical Bayesian Learners\nwe will analyze in Section 4.2, consisted of a hierarchical Gaussian prior with a logistic regression likelihood. Twolevel, one-level, and flat priors were all tested. The purpose of using the two-level prior was that it was able to encode information about which object classes had visually similar objects (e.g., car and track, dog and horse). Fig. 1(left) compares the predictive performance of this two-level hierarchical prior with the two more impoverished priors. Observe that the one-level and two-level priors both improve performance on most object classes compared to the flat prior, but not all. Furthermore, the two-level prior always leads to greater improvement than the one-level prior on object classes where a hierarchical model helps, but also almost always leads to a greater degradation in performance on object classes where the hierarchical models decrease performance. Why the different performance characteristics for the two hierarchical models? Why do some categories have improved accuracy while others decreased accuracy? In a post-hoc analysis, Salakhutdinov et al. (2011) note that the \u201cobjects with the largest improvement...borrow visual appearance from other frequent objects\u201d while \u201cobjects with the largest decrease [such as \u2018umbrella\u2019 and \u2018merchandise\u2019] are abstract, and their visual appearance is very different from other object categories.\u201d\nThe results just described lead to numerous theoretical questions of practical consequence:\nQ1 Can we formalize why for some object classes there was a beneficial sharing of statistical strength, while for other classes the sharing was detrimental?\nQ2 Can we understand when a flat model should be preferred to a hierarchical one to avoid unfavorable sharing?\nQ3 More generally, can we obtain guidance on the best type of prior for the problem at hand? Perhaps a different hierarchical prior would have been better suited to learning the image classifiers. For example, could placing hyperpriors on the variance parameters lead to greater \u201crobustness\u201d for object categories such as \u2018umbrella\u2019 and \u2018merchandise,\u2019 whose visual appearance differs from other object categories?\nQ4 Once the form of the prior has been chosen, how should hyperparameters be set to maximize learning? The settings of the variance hyperparameters was left unspecified by Salakhutdinov et al. (2011), and it is not clear a priori how they should be set, or how much effect their choice will have on learning.\nWhile we have primarily framed these questions in terms of a single model from one paper, this focus was simply for concreteness. Similar results leading to the same types of questions can be found in the numerous articles that make\nLearning to Share Visual Appearance for Multiclass Object Detection\nRuslan Salakhutdinov, Antonio Torralba, Josh Tenenbaum Massachusetts Institute of Technology rsalakhu@mit.edu, torralba@csail.mit.edu, jbt@mit.edu\nAbstract\nWe present a hierarchical classification model that allows rare objects to borrow statistical strength from related objects that have many training examples. Unlike many of the existing object detection and recognition systems that treat different classes as unrelated entities, our model learns both a hierarchy for sharing visual appearance across 200 object categories and hierarchical parameters. Our experimental results on the challenging object localization and detection task demonstrate that the proposed model substantially improves the accuracy of the standard single object detectors that ignore hierarchical structure altogether.\n1. Introduction\nAs we move around the world, some objects are encountered very frequently (everyday, we see many different people, cars, trees, buildings, chairs, etc.), other objects are less frequent (encountering only a few different instances per day, such as televisions or mugs), other objects are quite rare (e.g., speakers, teapots, suitcases, docks), or extremely rare (seen only a few times each year or less, such as elephants, or aircraft carriers). This distribution of learning data is very different to the distributions generally used whe training object r cognition algorithms. Current work on learning from few examples generally creates a setting in which there are N object classes, with M training examples available per class, with M being small. This setting is artificial as it is becoming increasingly easy to collect large amounts of training data [23, 18], at least for a subset of the object classes. In addition, this artificial distribution of training data is likely to be quite different to the distribution of data encountered by humans, or by the mobile agents, moving around the world.\nIn this work we focus on the more realistic setting in which we have some classes containing lots of training data and many classes containing little data. Our goal is to use frequent classes to help to learn rare classes for which it is harder to collect the training data. This biased distribution is quite frequent and emerges in most natural training domains. One of the most common examples is when look-\n50\n100\n150\n200\n250\n300\nN u\nm b\ne r\no f tr\na in\nin g\ne xa\nm p\nle s\n0\n50\n100\n150\n200\n250\n300\nw a ll tr e e b u il d in g s k y fl o o r w in d o w p la n t c e il in g m o u n ta in p e rs o n ro a d g ra s s c a b in e t c h a ir g ro u n d d o o r ta b le c u rt a in fe n c e s id e w a lk p a in ti n g c e il in g la m p s h e lv e s s e a b o o k s b o x w a te r fl o w e rs c a r c o lu m n s ta n d p a lm tr e e fi e ld s ig n b o x e s p o t c u s h io n ra il in g s to n e s e a ts d e s k\np e rs o n s it ti n g p ic tu re s te p s m ir ro r te x t s c re e n b e d d e s k la m p ro c k s s tr e e tl ig h t s a n d p a th b a g ru g h a n d ra il w o rk to p ro c k s ta ir c a s e a rm c h a ir b e n c h b u ll e ti n b o a rd b o o k c a s e b o o k s o fa b a s k e t b a lc o n y s id e ta b le ri v e r c o u n te r s in k s to n e s b a ll a w n in g s to o l fl a g to w e l s w iv e lc h a ir tr a y b a rs s h o w c a s e b re a d\nfl u o re s c e\nn tt u b e p la te s to v e v a s e p o s te r g la s s\ns c u lp tu re fa u c e t s n o w d o u b le d o o r c h a n d e li e r p o le p il lo w e m b a n k m e n t b o tt le s h o e s s h e lf b e a m b o tt le s te le v is io n s c o n c e p e o p le p ip e m e rc h a n d is e b o w l re fr ig e ra to r p a p e r s lo tm a c h in e c h a ir s b ri d g e w in d o w s b a r c o ff e e ta b le c lo th e s tr u c k g la s s w a ll s p o tl ig h t m o n it o r e n tr a n c e s ta ll to m b s to n e d ra w e r b o a t s h o p w in d o w g a te d e c k c h a ir m ic ro w a v e b u c k e t o v e n a ir p la n e s te p fi re p la c e s ta g e la n d fl o o rl a m p to il e t s e p a ra ti o n c lo c k d is h w a s h e r h a t b e n c h e s\nc e n tr a lr e s e\nrv a ti o n c a k e s b ic y c le e x tr a c to rh o o d w a te rf\na ll te n t c o u n te rt o p c a n u m b re ll a d o o rf ra m e a tt ic ja r c a n d le te le p h o n e n a p k in v id e o s fo o d ts h ir t h e a d s to n e to w e r c o w c h im n e y m u g to y b a th tu b v a n o u tl e t fi s h ro o f d ra w e rs b u s s ta tu e c a k e tr a ff ic li g h ts\nc o ff e e m\na k e r te a p o t fl u o re s c e n tt u b e s p a ll e t b ra n d n a m e a ir c o n d it io n in g s h u tt e r tr a s h c a n s p e a k e r d o c k k e y b o a rd d o m e s u it c a s e c p u m e a t c a rs c u p w a ll s p o tl ig h t d is h h a n d le fr u it s s w it c h c lo s e t\nw a ll w in d o w c u rt a in w in d o w s g la s s w a ll\ns h o p w in d o\nw tr e e p la n t fl o w e rs p a lm tr e e fr u it s b u il d in g s k y a w n in g g a te te n t a tt ic to w e r\nk e y b o a rd c lo s e t fl o o r c u s h io n b e d ru g s ta ir c a s e to w e l p il lo w s ta g e n a p k in b a th tu b b ra n d n a m e ro a d g ra s s s id e w a lk s e a w a te r p a th\ne m b a n k m\ne n t s ta ll c o w d o c k c a b in e t ta b le s ta n d d e s k b e n c h\ns id e ta b le s in k s to o l c o ff e e ta b le s e p a ra ti o n to y p a ll e t c e il in g c e il in g la m p c h a n d e li e r s h o e s b e a m s c o n c e p ip e p a p e r h a n d le g ro u n d fe n c e fi e ld s a n d ri v e r s h o w c a s e b ri d g e la n d c e n tr a lr e s e rv a ti o n w a te rf a ll p e rs o n c o lu m n p e rs o n s it ti n g s c u lp tu re p o le b o tt le b o tt le s p e o p le to m b s to n e b ic y c le s ta tu e m o u n ta in ro c k s s to n e s s n o w c a k e s u m b re ll a ro o f p a in ti n g p ic tu re m ir ro r te x t s c re e n p o s te r te le v is io n m o n it o r m ic ro w a v e o v e n s p e a k e r c h a ir s e a ts a rm c h a ir s o fa s w iv e lc h a ir c h a ir s d e c k c h a ir s h e lv e s b o o k s b o x e s b o o k c a s e b o o k s lo tm a c h in e b a r b e n c h e s te le p h o n e d o o r b a rs d o u b le d o o r e n tr a n c e d o o rf ra m e o u tl e t s h u tt e r ra il in g s te p s h a n d ra il b a lc o n y fl u o re s c e n tt u b e s te p fl u o re s c e n tt u b e s p o t p la te v a s e g la s s b o w l b o a t b u c k e t c a n ja r m u g c o ff e e m a k e r te a p o t tr a s h c a n c u p b o x b a g b a s k e t to il e t d ra w e rs s u it c a s e s ig n fl a g fa u c e t s h e lf c lo th e s ts h ir t c p u s w it c h s to n e ro c k b a ll c lo c k h e a d s to n e c a k e d o m e w o rk to p c o u n te r s to v e re fr ig e ra to r d ra w e r d is h w a s h e r e x tr a c to rh o o d c o u n te rt o p c a r tr u c k a ir p la n e h a t v a n b u s c a rs d e s k la m p s tr e e tl ig h t fl o o rl a m p c a n d le c h im n e y tr a ff ic li g h ts b u ll e ti n b o a rd tr a y s p o tl ig h t fi re p la c e a ir c o n d it io n in g w a ll s p o tl ig h t b re a d m e rc h a n d is e fo o d fi s h m e a t d is h v id e o s\nc a r\ntr u\nc k\na ir p\nla n\ne\nv a n\nc a\nrs\nN u\nm b\ne r\no f tr\na in\nin g\ne xa\nm p\nle s\nv a\nn b\nu s\na)\nb) Figure 1. a) Distribution of amount of training data available per object class. Objects are sorted by decreasing amount of data. The distribution is similar to the Zipf\u2019s law, with 9 objects out of 200 accounting for 50% of all the available training data. The two images show the output of a detector trained to detect vans using only the training data available for the van class. b) Objects are grouped into clusters of objects with similar visual appearances. In this plot clusters (denoted by different colors) are sorted by the cluster mass (sum of all the samples available) and, within each cluster, objects are sorted by decreasing amount of data. Rare objects are likely to be inside a cluster with some very frequent objects. The images show detections of vans on test images without (top) and with (bottom) sharing.\ning at the frequency of words. The distribution of words approximates the Zipf\u2019s law [32]. A distribution similar to Zipf\u2019s law also has been found in several large object databases (e.g., what and where [26], labelme [23]). Other datasets have a uniform distribution over available data per class (e.g. Caltech 101, ImageNet [18], by making a big effort during the collection process in order to keep a uniform distribution over the object samples).\nFig. 1.a shows the distribution of amount of annotated data available for 200 object categories from the database\n1481\nb)\na)\n0 50 100 150 200 Sorted Object Index\n4\n2\n0\n2\n4\n6\n8\n10\nIn cr\ne m\ne n t\nA P o\nv e r\nfl a t\np ri\no r Flat prior\nOne-level prior Two-level prior\nFigure 1. Righ : a) Distribution of training examples per object class. b) Same as a), but with objects grouped by visual appearance. Left: I provement in classification accuracy of hierarchical models compared to flat model. Object categories are sorted by improvement. Reproduced and reconstructed from Salakhutdinov et al. (2011).\nuse of ierarchical Bayesian methods. For example, one\nmight instead consider the hierarchical models have been\nused in political science for analyzing polling and census data to predict election outcomes (Ghitza & Gelman, 2013) and in demography for predicting population growth, life expectancy, and fertility rates (Raftery et al., 2013; 2012; Alkema et al., 2011).\nIn this paper we seek to answer the questions just posed in terms of two learning-theoretic quantities: regret (in online learning) and statistical risk (in batch learning). The online learning setting applies, for example, to the demography applications and election prediction while the batch setting is relevant to the image classification problem as well as election prediction (whether the online or batch analysis applies to election prediction depends on how the problem is formulated).\nIn the online learning framework (Dawid & Vovk, 1999; Cesa-Bianchi & Lugosi, 2006), no assumptions are made about the data-generating mechanism. Inputs are presented to the learner one by one. After receiving each input the learner predicts the output, then suffers a loss after observing the true output. The goal of the learner is to not do much worse (i.e., have large regret) compared to a fixed class of predictors. Online learning guarantees are attractive for the analysis of hierarchical Bayesian models because such models are so often used in exactly those circumstances when orthodox Bayesian justifications do not apply: typically the modeler does not think that her model reflects the true data generating process, but is instead employing hierarchical methods either to increase robustness against a poor choice of hyperparameters or to speed learning by allowing for the sharing of statistical strength between populations.\nRegret bounds, however, do not themselves give any generalization guarantees about how the learner will perform on future data. Statistical risk bounds provide guarantees\nabout the learner\u2019s expected loss on unseen examples by making assumptions about how the data are generated \u2014 for example, from an i.i.d. or strongly mixing process. Although there is a stochastic assumption, risk bounds also do not assume that the data is generated according to the model used. We derive a general result for transferring Bayesian regret bounds to risk bounds for bounded losses.\nRegret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gru\u0308nwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al. (2008), which applies to a large class of Bayesian generalized linear models (GLMs). We extend the technique to apply to certain non-GLM likelihoods as well, including to multi-class logistic regression. All proofs are deferred to the Appendices.\nWe answer Questions Q1-Q4 in some important cases by deriving regret bounds for three types of hierarchical priors. First, we consider the use of an inverse gamma hyperprior for the Gaussian prior\u2019s variance parameter and demonstrate that the hyperprior leads to greater robustness to data that is well-explained by setting the GLM parameter vector to have very large `2 norm. Next, we analyze hierarchical Gaussian models that allow for the sharing of statistical strength. Our results, which complement existing work on transfer and multitask learning theory (Baxter, 1997; Ben-David & Schuller, 2003; Pentina & Lampert, 2014), show that when the parameters with small regret for a collection of related tasks are either (a) similar or (b) not unexpected under the prior, then the hierarchical model has a smaller regret bound than assuming the tasks are independent. Finally, we show that spike-and-slab priors can exploit sparse parameters with small regret."}, {"heading": "2. Bayesian Online Learning", "text": "In online learning, the learner must predict (a distribution over) y \u2208 Y \u2286 R after observing x \u2208 X \u2286 Rn. In this paper, we assume the prediction is made according to a generalized linear model (GLM) p(y |x,\u03b8) = p(y |\u03b8 \u00b7 x), where \u03b8 \u2208 \u0398 \u2286 Rn is a parameter vector to be chosen. GLMs provide significant modeling flexibility, and the class of GLM models and priors we analyze include a range of models used in real-world scientific applications (Gelman & Hill, 2006; Gelman et al., 2013). Two widely\nused GLMs are the logistic regression likelihood\np(y |\u03b8,x) = 1 1 + exp(y\u03b8 \u00b7 x) , y \u2208 {\u22121, 1}, (1)\nand the Gaussian linear regression likelihood p(y |\u03b8,x) = N(y |\u03b8 \u00b7 x, \u03c32), y \u2208 R. Since we are taking a Bayesian approach, we place a prior density p0(\u03b8) on \u0398, with corresponding distribution P0.1 At time step t, the learner observes xt, outputs a distribution over Y , then observes yt \u2208 Y . The Bayesian (model average) learner predicts p(y |xt, Zt\u22121), where Zt , {(x1, y1), . . . , (xt, yt)}, and then suffers the log-loss \u2212 ln p(yt |xt, Zt\u22121). Hence, the cumulative loss incurred is\nLBayes(ZT ) , \u2211T t=1\u2212 ln p(yt |xt, Zt\u22121).\nIf Q is a distribution over \u03b8, then using Q for prediction leads to loss on example t of `t(Q) , EQ[\u2212 ln p(yt |xt,\u03b8)] and hence cumulative loss\nLQ(ZT ) , EQ [\u2211T t=1\u2212 ln p(yt |xt,\u03b8) ] .\nIf Q = \u03b4\u03b8, then we write L\u03b8 instead of LQ, so LQ(ZT ) = EQ[L\u03b8(ZT )]. Our objective is to derive regret bounds of the form\nR(ZT ,\u03b8) , LBayes(ZT )\u2212 L\u03b8(ZT ) \u2264 B(\u03b8) + C(T ), (2)\nwhere R(ZT ,\u03b8) is the regret and B(\u03b8) + C(T ) is a regret bound depending on the choice of prior P0. We aim for C(T ) = o(T ), so that for a fixed \u03b8, the average loss T\u22121LBayes(ZT ) is bounded by T\u22121L\u03b8(ZT ) + o(1).\nOur approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result:\nProposition 2.1 (Kakade & Ng (2004); Banerjee (2006)). The Bayesian cumulative loss is bounded as\nLBayes(ZT ) \u2264 LQ(ZT ) + KL(Q||P0). (3)\nFor the GLM model p(y |\u03b8 \u00b7 x), define fy(z) , \u2212 ln p(y |\u03b8 \u00b7x = z). We make two assumptions throughout the remainder of the paper (they will usually not be stated explicitly):\n|f \u2032\u2032y (z)| \u2264 c for all y, z (A1) \u2016xt\u20162 \u2264 1 for all t. (A2)\nThe first assumption can be understood as requiring the likelihood to be sufficiently smooth. The second assumption sets the scale of the problem, which is necessary since\n1Throughout, we use lowercase letters for densities and uppercase letters to denote the corresponding measures.\nscaling xt up requires scaling \u03b8 down, and vice-versa: p(y |C\u22121\u03b8 \u00b7 Cx) = p(y |\u03b8 \u00b7 x) for any C 6= 0. Note that for the Gaussian linear regression model with variance \u03c32 and the logistic regression model, (A1) holds with c = 1/\u03c32 and c = 1/2, respectively. Proposition 2.1 leads to the following theorem for obtaining regret bounds for the Bayesian model average learner.\nTheorem 2.2 (Bayesian regret meta-theorem). Let Q\u03b8\u2217,\u03c6 be a distribution with parameter \u03c6 \u2208 \u03a6 \u2286 Rd (written \u03c6 if d = 1) and mean \u03b8\u2217. If (A1) and (A2) hold, then for all \u03c6,\nR(Z,\u03b8\u2217) \u2264 Tc 2 \u2016VarQ\u03b8\u2217,\u03c6 [\u03b8]\u2016+ KL(Q\u03b8\u2217,\u03c6||P0),\nwhere \u2016 \u00b7 \u2016 is the spectral norm. In particular, if the components of \u03b8\u2217 are uncorrelated, then \u2016VarQ\u03b8\u2217,\u03c6 [\u03b8]\u2016 = supi VarQ\u03b8\u2217,\u03c6 [\u03b8i]\nTheorem 2.2 is our first main result and will be repeatedly applied in Section 4 by choosing an appropriate Q\u03b8\u2217,\u03c6 and then optimizing\u03c6. Although the bound appears to be linear in T , typically\u03c6 can be chosen such that \u2016VarQ\u03b8\u2217,\u03c6 [\u03b8]\u2016 = \u0398(T\u22121) and KL(Q\u03b8\u2217,\u03c6||P0) = \u0398(lnT ), leading to a logarithmic regret bound. Theorem 2.2 provides an attractive approach to deriving regret bounds because there is no need to work directly with the posterior, which is often analytically intractable. For example, there is no closed-form expression for the posterior of the Bayesian logistic regression model. The theorem generalizes the approach originally taken in Kakade & Ng (2004), in which a Gaussian prior for \u03b8 was considered:\nTheorem 2.3 (Gaussian regret (Kakade & Ng, 2004)). If \u03b8 \u223c N(0, \u03c32I), thenR(Z,\u03b8\u2217) is bounded by\nRGBayes(Z,\u03b8 \u2217) ,\n1 2\u03c32 \u2016\u03b8\u2217\u20162 + n 2 ln\n( 1 + Tc\u03c32\nn\n) ."}, {"heading": "2.1. Beyond GLMs", "text": "Theorem 2.2 follows from a more general result, Theorem 2.4, which allows for non-GLM likelihoods. Specifically, instead of the likelihood being a GLM, we assume the likelihood can be written in the form p(y |x, \u03be,\u03c8) = p(y | \u03bex,\u03c8), where \u03be \u2208 Rn\u2032\u00d7n is a matrix and \u03c8 \u2208 Rn\u2032\u2032 . The full parameter vector is \u03b8 = (\u03be,\u03c8) \u2208 RN , N , nn\u2032 + n\u2032\u2032 (implicitly flattening the matrix \u03be). Let fy(z) , \u2212 ln p(y | (\u03bex,\u03c8) = z). We require the following assumption in place of (A1):\n\u2016f \u2032\u2032y (z)\u2016 \u2264 c for all y,z, (A1\u2019)\nwhere f \u2032\u2032y (z) denotes the matrix of second partial derivatives (Hessian).\nTheorem 2.4 (Generalized Bayesian regret meta-theorem). Let Q\u03b8\u2217,\u03c6 be a distribution with parameter \u03c6 \u2208 \u03a6 \u2286 Rd\nand mean \u03b8\u2217. If (A1\u2019) and (A2) hold, then for all \u03c6,\nR(Z,\u03b8) \u2264 Tc(n \u2032 + n\u2032\u2032)\n2 \u2016VarQ\u03b8\u2217,\u03c6 [\u03b8]\u2016+ KL(Q\u03b8\u2217,\u03c6||P0),\nOf particular interest is that Theorem 2.4 can handle multiclass logistic regression (MLR). In multi-class regression, each example xt has one of K labels yt \u2208 {1, . . . ,K}, indicating which class the example belongs to. For MLR, each class k has an associated parameter \u03b8(k). The parameters are combined into a single likelihood:\np(yt |\u03b8,xt) = exp(\u03b8(yt) \u00b7 x)\u2211K k=1 exp(\u03b8 (k) \u00b7 xt) . (4)\nTheorem 2.5 (MLR Gaussian regret). If \u03b8(k) \u223c N(0, \u03c32I), k = 1, . . . ,K, then using the MLR likelihood guarantees thatR(Z,\u03b8\u2217) is bounded by\nRmlr\u2212GBayes (\u03b8 \u2217, Z) ,\n1 2\u03c32 \u2016\u03b8\u2217\u20162 + nK 2 ln\n( 1 + TKc\u03c32\nn\n) ."}, {"heading": "3. Risk Bounds", "text": "While online regret bounds are attractive because they make no assumptions about the data-generating process, it is also desirable to have risk bounds in the batch setting since risk bounds provide generalization guarantees for unseen data. We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007). Such bounds also have the benefit of applying to any bounded loss (e.g., the 0-1 loss for binary classification), which may be more task-relevant than the log-loss. In the batch setting, the data ZT are received all at once by the learner and are assumed to be distributed i.i.d. according to some distribution D over X \u00d7 Y: (xt, yt)\ni.i.d.\u223c D, t = 1, . . . , T . Let ` be a bounded loss function taking a probability distribution over Y and an element of Y as arguments. Without loss of generality assume ` \u2208 [0, 1]. Writing `\u03b8(x, y) , `(p(\u00b7 |x,\u03b8), y), for any distribution Q over \u0398, let\nL(Q) , E(x,y)\u223cDE\u03b8\u223cQ[`\u03b8(x, y)] L\u0302(Q,ZT ) , T\u22121 \u2211T t=1 E\u03b8\u223cQ[`\u03b8(xt, yt)]\nbe, respectively, the expected and empirical losses under Q. PAC-Bayesian analyses consider the risk of the Gibbs predictor for the distribution Q (i.e., sample \u03b8 \u223c Q, predict with p(\u00b7 |x,\u03b8)), not the model average over Q (i.e., predict with \u222b p(\u00b7 |x,\u03b8)Q(d\u03b8)). A typical bound (specialized to the Bayesian setting) is the following (here pT (\u03b8) , p(\u03b8 |ZT )): Theorem 3.1 (Audibert & Bousquet (2007)). Fix \u03ba > 1/2 and write \u03ba\u2032 , 2\u03ba/(2\u03ba \u2212 1). For any distribution D, with\nprobability at least 1\u2212 \u03b4 over samples (xt, yt) i.i.d.\u223c D,\n|L(PT )\u2212 L\u0302(PT , ZT )| \u2264 T\u22121/2 \u221a \u03ba \u221a KL(PT ||P0) + ln\u03ba\u2032/\u03b4 . (5)\nCombining the PAC-Bayesian risk bound with Bayesian regret bounds leads to our second main result: Theorem 3.2. Assume that (2) holds and fix \u03ba > 1/2. For any distributionD, with probability at least 1\u2212\u03b4 over samples (xt, yt) i.i.d.\u223c D,\n|L(PT )\u2212 L\u0302(PT , ZT )|\n\u2264 T\u22121/2 \u221a \u03ba \u221a B(\u03b8\u0302) + C(T ) + ln\u03ba\u2032/\u03b4 ,\n(6)\nwhere \u03b8\u0302 , arg min\u03b8 L\u03b8(ZT ).\nAn attractive feature of Theorem 3.2 is that the bound does not rely on understanding the posterior PT , as is required by a direct application of a PAC-Bayesian bound such as that given in Theorem 3.1, which requires calculating KL(PT ||P0). Yet the PAC-Bayesian regret bound remains data-dependent due to its dependence on the empirical risk minimizer (ERM) p(y |x, \u03b8\u0302).\nExamining the proof of Theorem 3.2, it is easily seen that in fact any \u03b8\u0303 such that L\u03b8\u0303(ZT ) < LPT (ZT ) can be chosen in place of \u03b8\u0302. Such alternative choices may lead to significantly tighter bounds and are particularly important, for example, in the application of the theorem to the spike-andslab prior (cf. Section 4.3), as the ERM parameter will in almost all circumstances satisfy \u2016\u03b8\u0302\u20160 = n, which would lead to a poor generalization bound when n is large.\nIn words, Theorem 3.2 can be understood as stating that if the Bayesian (model average) learner has small log-loss regret compared to the ERM, then with high probability the Bayesian Gibbs predictor will generalize well if the loss function is bounded. Or, as a slogan, the theorem shows that \u201csmall regret in the online learning setting implies good generalization bounds in the batch setting.\u201d The theorem thus connects PAC-Bayesian bounds, Bayesian regret bounds, and empirical risk minimization."}, {"heading": "4. Applications", "text": "We now use Theorem 2.2 to analyze hierarchical priors for robustness, sharing of statistical strength, and feature selection."}, {"heading": "4.1. Hierarchical Priors for Robustness", "text": "In this section we answer questions Q2-Q4 as they relate to hierarchical priors for robust inference, demonstrating how, with a proper choice of hyperparameters, a hierarchical prior can lead to increased robustness compared to a flat\nprior. Specifically, we analyze a canonical use of a hierarchical prior \u2014 to capture greater uncertainty in the value of a parameter by placing a hyperprior on the variance of the Gaussian prior on that parameter (Berger, 1985; Bishop, 2006; Gelman et al., 2013):\n\u03c320 |\u03b1, \u03b2 \u223c \u0393\u22121(\u03b1, \u03b2) and \u03b8i |\u00b50, \u03c320 \u223c N(\u00b50, \u03c320),\nwhere \u0393\u22121(\u03b1, \u03b2) is the inverse gamma distribution with shape \u03b1 and scale \u03b2. Let \u03bd , 2\u03b1 and \u03c32 , \u03b2/\u03b1. Then the marginal distribution of \u03b8 follows the multivariate tdistribution with location \u00b501, scale matrix \u03c32I , and \u03bd degrees of freedom:\n\u03b8 |\u00b50, \u03c32, \u03bd \u223c T\u03bd(\u00b501, \u03c32I),\nwhere 1 is the all-ones vector. The multivariate tdistribution density is\npT(\u03b8 |\u00b5,\u03a3, \u03bd)\n= \u0393(\u03bd+n2 )\n( 1 + 1\u03bd (\u03b8 \u2212 \u00b5) >\u03a3\u22121(\u03b8 \u2212 \u00b5) )\u2212 \u03bd+n2\n\u0393(\u03bd2 )\u03c0 n/2\u03bdn/2|\u03a3|1/2\n.\nWhen \u03bd is finite, the multivariate t-distribution is heavytailed: the probability of \u2016\u03b8\u2016 decreases at a polynomial rate as \u2016\u03b8\u2016 \u2192 \u221e, compared to the exponential rate for a multivariate Gaussian. For example \u03bd = \u03c32 = 1 and \u03a3 = \u03c32 gives the multivariate Cauchy distribution. A multivariate Gaussian with covariance matrix \u03a3 is recovered by taking \u03bd \u2192 \u221e. Placing a multivariate t-distribution prior on \u03b8 yields the following regret bound:\nTheorem 4.1 (Multivariate t-distribution regret). If \u03b8 \u223c T\u03bd(0, \u03c3 2I), thenR(Z,\u03b8\u2217) is bounded by\nRmvtBayes(Z,\u03b8 \u2217) ,\n\u03bd + n\n2 ln\n( 1 + \u2016\u03b8\u2217\u20162\n\u03bd\u03c32 ) + n\n2 ln\n( (\u03bd + 1)(\u03bd + n)\n\u03bd2 + Tc(\u03bd + 1)\u03c32 \u03bdn\n) .\n(7)\nTheorem 2.3 can be obtained as a special case of Theorem 4.1 by taking \u03bd \u2192\u221e.\nAssume \u03bd \u2265 1. If \u2016\u03b8 \u2217\u20162 \u03bd\u03c32 is small, then\nF (\u2016\u03b8\u2016) , \u03bd + n 2 ln\n( 1 + \u2016\u03b8\u2217\u20162\n\u03bd\u03c32\n) \u2248 n+ \u03bd\n\u03bd\n\u2016\u03b8\u2217\u20162\n\u03c32 ,\nso for \u201csmall\u201d values of \u2016\u03b8\u2217\u20162 (relative to \u03bd\u03c32) the regret bound behaves similarly to having a Gaussian prior on \u03b8. However, if \u2016\u03b8\n\u2217\u20162 \u03bd\u03c32 1, then the regret bound grows only\nlogarithmically with \u2016\u03b8\u2016, as we would expect given that the multivariate t-distribution has heavy tails. Roughly speaking, F (x) can be thought of as switching from quadratic to logarithmic behavior when x2 = \u03bd\u03c32, since this is the value at which F switches from being convex to concave.\nIn general, the regret bound is large when the choice of \u03b8\u2217 with small loss has large magnitude. If a Gaussian prior is used, the possibility of \u2016\u03b8\u2217\u2016 being large can be ameliorated by choosing \u03c32 large, since there is only a logarithmic regret penalty in \u03c3. However, without a priori knowledge of how large the optimal \u03b8\u2217 might be, choosing a multivariate t-distribution prior with a small value for \u03bd and a moderate value for \u03c32 allows for guaranteed logarithmic regret in the magnitude of \u03b8\u2217 no matter how large \u2016\u03b8\u2217\u2016 is. Hence, the use of the hierarchical (multivariate tdistribution) prior does in fact yield greater robustness than the non-hierarchical (Gaussian) prior.2\nWe can, in fact, develop more specific guidance on the choice of hyperparameters for the t-distribution. Our goal is to choose \u03bd such that we obtain a t-distribution regret bound that is essentially as good as the Gaussian prior regret bound RGBayes = \u2016\u03b8\u2217\u20162 2\u03c32 + n 2 ln ( 1 + Tc\u03c3 2 n ) for small \u2016\u03b8\u2217\u2016 and better when \u2016\u03b8\u2217\u2016 is large. If we choose \u03bd equal to a constant, then for n much larger than \u03bd, we have RmvtBayes \u2248 n2 ln ( 1 + \u2016\u03b8 \u2217\u20162 \u03bd\u03c32 ) + n2 ln ( n \u03bd + Tc\u03c32 n ) . In the case of \u2016\u03b8\u2217\u2016 small, we therefore have that the first term of RmvtBayes is approximately n \u03bd \u2016\u03b8\u2217\u20162 2\u03c32 , and thus larger than the first term of RGBayes by a factor of n/\u03bd. Furthermore, for small T nc\u03c32 and any \u03b8\n\u2217, the second term of RmvtBayes is approximately n2 ln(n/\u03bd) whereas the second term of RGBayes is approximately Tc\u03c3\n2 n. Thus, RmvtBayes with constant \u03bd is not competitive with RGBayes in the large n and small T regimes. Instead consider the choice \u03bd = Cn for constant C > 0, so\nRmvtBayes\n\u2248 (C + 1)n 2 ln\n( 1 + \u2016\u03b8\u2217\u20162\nCn\u03c32\n) + n\n2 ln\n( C + 1\nC + Tc\u03c32 n ) \u2264 C + 1\nC\n\u2016\u03b8\u2217\u20162\n2\u03c32 + n 2 ln\n( C + 1\nC + Tc\u03c32 n\n) .\nIn this case, by choosing a moderate value of C, we see that a multivariate t-distribution prior with \u03bd = Cn has a competitive regret bound with a Gaussian prior in the small \u2016\u03b8\u2217\u2016 regime, and exponentially smaller regret bound as \u2016\u03b8\u2217\u2016 becomes large. Furthermore, the t-distribution prior remains competitive with the Gaussian prior when T is small.\n2A more rigorous version of this statement can be obtained for the Gaussian regression likelihood by using the fact that there is a matching lower bound on the regret for the Gaussian prior/Gaussian regression model (Kakade & Ng, 2004)."}, {"heading": "4.2. Hierarchical Priors for Sharing Statistical Strength", "text": ""}, {"heading": "4.2.1. BACKGROUND", "text": "We next consider hierarchical priors that allow for the sharing of statistical strength, providing answers to Q1 and Q2: we specify some conditions under which sharing of statistical strength can be achieved and others in which a nonhierarchical prior is preferable.3 In the machine learning literature, the goal of \u201csharing statistical strength\u201d has been formalized via multitask learning (MTL) and \u201clearning-tolearn\u201d (LTL) frameworks. A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective.\nTypically, tasks are equated with probability distributions over examples (e.g., (x, y) pairs). It is assumed that the tasks are drawn i.i.d. from an unknown task distribution. The goal is to learn the individual tasks and learn about the task distribution. Alternatively, a notion of similarity can be used to relate the tasks: the more similar the tasks, the greater the advantage of learning them using multitask algorithms. In the online learning framework no assumptions are made about the distribution of examples, so we consider two MTL scenarios in line with the latter setting. In the first scenario, one example from one task is received at each time step. In the second, which is described in the Appendix E.1, at each time step an example for each task is received simultaneously."}, {"heading": "4.2.2. SEQUENTIAL OBSERVATIONS FROM MULTIPLE SOURCES", "text": "The sequential observation setting is relevant to the image classification example given in the introduction, in which there are many observations from some data sources and only a small number of observations from numerous other data sources. To model this situation, at time step t, an input xt from source zt is observed, where zt \u2208\n3For simplicity results are for Gaussian priors, though the extension to multivariate t-distribution priors is straightforward.\n{1, . . . ,K}. The learner predicts yt according to the posterior of \u03b8(zt) given Zt\u22121. An equivalent formulation is that the Bayesian learner observes xt = (0, . . . ,x (k) t ,0, . . . ) (if zt = k) at each time, then receives yt. Instead of using independent Gaussian priors on \u03b8(1), . . . ,\u03b8(K), place a prior over the means of the K priors. For each dimension j = 1, . . . , n, let \u00b5j |\u03c320 \u223c N(0, \u03c320) and \u03b8\n(k) j |\u00b5j , \u03c32 \u223c N(\u00b5j , \u03c32), k = 1, . . . ,K, and write \u03b8\n(1:K) j , (\u03b8 (1) j , . . . , \u03b8 (K) j ). Integrating out \u00b5j yields\n\u03b8 (1:K) j |\u03c3 2 0 , \u03c3 2 \u223c N(0,\u03a3), (8)\nwhere, with 1K denoting the K \u00d7K all-ones matrix,\n\u03a3 , s2\u03c11K + s 2(1\u2212 \u03c1)I (9)\ns2 , \u03c320 + \u03c3 2, \u03c1 , \u03c320/(\u03c3 2 0 + \u03c3 2), (10)\nThis prior corresponds to the one-level prior in Salakhutdinov et al. (2011). Similar results, which will be discussed qualitatively below, can be obtained for the two-level prior at the cost of a significantly more complicated bound. Define T (k) , \u2211T t=1 \u03b4k(zt), Z\n(k) , {(x, y) \u2208 Z|zt = k}, and \u03b32 , K\u03c320 + \u03c3 2\nTheorem 4.2 (Hierarchical Gaussian regret, sequential observations). If \u03b8(1:K)j \u223c N(0,\u03a3), j = 1, . . . , n, then R(Z,\u03b8\u2217) is bounded by\nRHG\u2212seqBayes (Z,\u03b8 \u2217) ,\n1 2\u03b32 \u2211K k=1 \u2016\u03b8 \u2217(k)\u20162\n+ \u03c320 \u03c32\u03b32 \u2211 k<` \u2016\u03b8 \u2217(k) \u2212 \u03b8\u2217(`)\u20162 + n2 ln ( 1 + K\u03c320 \u03c32 ) + n\n2\n\u2211K k=1 ln ( 1\u2212 \u03c3 2 0 \u03b32 + T (k)c\u03c32 n ) . (11)\nIt is instructive to compare the upper bound given in (11) to \u2211 k R G Bayes(Z(k),\u03b8\n\u2217(k)) with prior variance s2 = \u03c320 + \u03c32. Setting \u03c30 = \u03c3 yields a condition for the hierarchical model to have smaller regret bound than the nonhierarchical model:\n4\u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u20162 + 3s2n \u22112 k=1 ln ( 4 3n+T (i)cs2 n+T (k)cs2 ) \u2264 \u2016\u03b8\u2217(1)\u20162 + \u2016\u03b8\u2217(2)\u20162 + 0.863s2n. (12)\nOf particular interest is the \u201cone-shot learning\u201d scenario, in which only one observation (or a small number of observations) from a source are made while many observations are made from some other sources. This setting is exactly that of the image classification problem of Salakhutdinov et al. (2011). For concreteness, consider a \u201clarge data\u201d task with T (1) ncs2 and a \u201csmall data\u201d task T (2) = 2, so that\nln ( 4 3n+T (1)cs2\nn+T (1)cs2\n) \u2248 0 and (12) becomes (approximately)\n4\u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u20162 + 3s2n ln ( 4n+ 6cs2\n3n+ 6cs2 ) \u2264 \u2016\u03b8\u2217(1)\u20162 + \u2016\u03b8\u2217(2)\u20162 + 0.863s2n.\nBut even for n = 1, 3 ln ( 4n+6cs2\n3n+6cs2\n) < 0.863, so the\nhierarchical model has smaller regret bound as long as 4\u2016\u03b8\u2217(1)\u2212\u03b8\u2217(2)\u20162 \u2264 \u2016\u03b8\u2217(1)\u20162 +\u2016\u03b8\u2217(2)\u20162 +Cs2n for some constant 0 < C < 0.863.\nHierarchical models for one-shot learning are designed with the goal of providing good predictive power on the new problem (the second data source) even with a small number of examples from that problem. To see if this is in fact the case for the hierarchical prior considered here, we can investigate how much greater the regret bound is for T (2) > 0 than for T (2) = 0 with T (1) ncs2 fixed. With \u03c320 = \u03c3\n2, (11) is greater in the former (K = 2) than the latter (K = 1) scenario by at most\n\u2212 \u2016\u03b8 \u2217(1)\u20162 6s2 + \u2016\u03b8\u2217(2)\u20162 3s2 + 2\u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u20162 3s2 + 3T (2)cs2 8 .\nSo if \u2016\u03b8\u2217(2)\u2016 is small and \u03b8\u2217(2) and \u03b8\u2217(1) are close in `2 distance, the regret bound for the second source is small.\nThe regret bound for the two-level prior in Salakhutdinov et al. (2011) is quite similar to that for the one-level prior. Let sk \u2208 {1, . . . , S} denote the superclass of class k. In the case of image classification, object classes that have similar visual appearance would have a common superclass. The two-level prior consistes of an overall parameter prior \u03b2 \u223c N(0, \u03c320I), superclass parameter priors \u00b5\n(s) \u223c N(\u03b2, \u03c321I), and class parameter priors \u03b8(k) \u223c N(\u00b5(sk), \u03c322I). The regret bound for the two-level prior is\nc0 \u2211K k=1 \u2016\u03b8 \u2217(k)\u20162 + \u2211 k<` ck`\u2016\u03b8 \u2217(k) \u2212 \u03b8\u2217(`)\u20162\n+ n\n2\n\u2211K k=1O(ln(c1 + c2T (k))) +O(1), (13)\nwhere c0, c1, c2 > 0 are constants, ck` = c\u0303sk if sk = s` and ck` = c\u0303sks` if sk 6= s`. Furthermore, c\u0303s > c\u0303s\u2032s\u2032\u2032 for all s, s\u2032, s\u2032\u2032 \u2208 {1, . . . , S}. Hence, the regret bound\u2019s being small depends more on the parameter vectors in the same superclass being close to each other than on parameter vectors from different superclasses being close to each other. See Appendix E.2 for details. The two-level regret bound well-explains the results of Salakhutdinov et al. (2011). The poor performance on image classes with very different visual appearance from the other classes is unsurprising since the parameter vectors that predict these classes well are going to have large `2 distance from the parameter vectors of other object classes."}, {"heading": "4.3. Hierarchical Priors for Feature Selection", "text": "A shortcoming of the priors investigated so far is the poor dependence on the feature space dimension n. For example, the Gaussian prior regret bound is (approximately)\nLBayes(Z) \u2264 inf \u03b8\u2217 L\u03b8\u2217(Z) +\n1 2\u03c32 \u2016\u03b8\u2217\u20162 + Tc\u03c3\n2\n2 ,\nwhen Tc\u03c32 n, so the regret may grow linearly in this regime.4 In the infinite-dimensional case, Gaussian processes can be used while still obtaining meaningful regret bounds (Kakade et al., 2005; Seeger et al., 2008). However, methods that are applicable to high-dimensional problems for which n T but n is still finite are of great general interest. For example, in the image classification example from the introduction, the feature vector has n \u2248 5000, whereas most object classes have fewer than 200 training examples. In high-dimensional problems it is desirable to use feature selection or sparse methods to reduce the effective dimension of the problem, with the aim of achieving better generalization performance and increasing interpretability of the model. A popular non-Bayesian approach for inducing sparsity is `1 regularization, such as the lasso for linear regression (Tibshirani, 1996). A Bayesian approach is the Bayesian lasso: the `1 regularizer of the lasso is converted into a prior, which amounts to placing a Laplace prior on \u03b8 (Park & Casella, 2008). However, the Bayesian lasso still seems to lead to a linear dependence on the dimension because the model puts zero prior mass on a component being exactly zero. A regret bound for the Bayesian lasso can be found in the Appendix F.1 (we suspect that our bound is essentially tight, though we have been unable to obtain a matching lower bound).\nAnother common Bayesian approach to inducing sparsity is to use a hierarchical \u201cspike and slab\u201d prior, which places positive probability on a component being exactly zero (Ishwaran & Rao, 2005; Narisetty & He, 2014). One version of the spike and slab prior is\nzi | p \u223c Bern(p) and \u03b8i | zi \u223c zi\u03b40 + (1\u2212 zi)N(0, \u03c32).\nSo with probability p component i is zero and with probability 1 \u2212 p it is Gaussian-distributed. Integrating out zi yields prior density p0(\u03b8i) = p\u03b40(\u03b8i)+(1\u2212p)N(\u03b8i | 0, \u03c32). Let \u2016v\u20160 denote the `0 norm of the vector v. Theorem 4.3. For the spike-and-slab prior, if m = \u2016\u03b8\u2217\u20160, thenR(Z,\u03b8\u2217) is bounded by\nRSSBayes(Z,\u03b8 \u2217) ,\n1 2\u03c32 \u2016\u03b8\u2217\u20162 +m ln 1 1\u2212 p (14)\n+ (n\u2212m) ln 1 p + m 2 ln\n( 1 + Tc\u03c32\nm\n) .\n4Dimension-independent regret bounds for the priors already considered can be obtained, but at the price of a constant greater than one front of the L\u03b8\u2217(Z) term. See, e.g., Banerjee (2007).\nIn particular, if p , q1/n for some constant 0 < q < 1, then RSSBayes(Z,\u03b8 \u2217) is at most\n\u2016\u03b8\u2217\u20162\n2\u03c32 +m ln\nn\n1\u2212 q + ln\n1 q + m 2 ln ( 1 + Tc\u03c32 m ) . (15)\nThe theorem shows the importance of properly scaling p with the dimension of the problem. If p is kept fixed, then the regret has linear dependence on n. However, by scaling p to be q1/n, we increase the probability of a component being zero as the dimension increases and thus are able to ensure that the regret is only logarithmic in nwhile simultaneously maintaining the appropriate linear dependence on m. The constant q turns out to be the prior probability that all of the components are set to zero. More generally, ( n k ) q(n\u2212k)/n(1 \u2212 q1/n)k n\u2192\u221e\u2192 q ln k q\u22121\nk! , the limiting prior probability of choosing exactly k components to be non-zero. Hence, as n \u2192 \u221e, the prior over the number of non-zero components converges to a Poisson distribution with rate parameter ln q\u22121. So when n is large the expected number of non-zero components is\u2248 ln q\u22121. The choice of p close to 1 for large n is in notable contrast to the common practice of setting p = 1/2 or some other constant independent of n (Schneider & Corcoran, 2004; Ishwaran & Rao, 2005). Our results strongly recommend against this practice. See Narisetty & He (2014) for a discussion of purely statistical reasons to scale p with the dimension."}, {"heading": "5. Conclusion", "text": "In this paper we set out to understand and quantify the learning-theoretic benefits of Bayesian hierarchical modeling. In Section 4, we used first our main result, Theorem 2.2, to analyze three specific hierarchical priors that, particularly when combined with a logistic or Gaussian regression likelihood, are widely used in practice. Indeed, these prior-likelihood combinations have often been used with substantial success even in situations when they are known to be rather poor models for the data generating mechanism. Our analysis offers an explanation for this success. The priors we analyzed are representative of the variety of ways in which hierarchical models are employed: representing uncertainty in hyperparameters, tying together related groups of observations, and creating more complicated distributions from simpler ones. Thus, our results answer Questions Q1-Q4 in some important cases and exemplify a learning-theoretic analysis technique that can be applied to other hierarchical models. In addition, using our second main result, Theorem 3.2, all of the insights gained in Section 4 for the log-loss regret setting apply equally well to the batch setting of statistical risk with bounded loss, further extending the applicability of our conclusions."}, {"heading": "Acknowledgments", "text": "Thanks to Peter Gru\u0308nwald, Sham Kakade, Peter Krafft, and Daniel Roy for helpful discussions and comments. Thanks also to the anonymous reviewers whose constructive comments improved the presentation of the results. JHH was supported by the U.S. Government under FA9550-11-C0028 and awarded by the DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a."}, {"heading": "A. Regret Bounds for Non-GLM Likelihoods", "text": "Recall Proposition 2.1, restated here for convenience:\nProposition. The Bayesian cumulative loss is bounded as\nLBayes(ZT ) \u2264 LQ(ZT ) + KL(Q||P0). (A.1)\nProof of Theorem 2.4. Fix a choice of \u03b8\u2217 and \u03c6 and write Q = Q\u03b8\u2217,\u03c6. Take a second-order Taylor expansion of fy about z\u2217, yielding\nfy(z) = fy(z \u2217) + f \u2032y(z \u2217)>(z \u2212 z\u2217) + 1 2 (z \u2212 z\u2217)>f \u2032\u2032y (\u03b6(z))(z \u2212 z\u2217),\nfor some function \u03b6. Let z = (\u03bex,\u03c8) with \u03b8 \u223c Q and let z\u2217 = E[z] = (\u03be\u2217x,\u03c8\u2217). Hence,\nEz[fy(z)] = fy(z\u2217) + f \u2032y(z\u2217)>0 + 1 2 Ez [ (z \u2212 z\u2217)>f \u2032\u2032y (\u03b6(z))(z \u2212 z\u2217) ] \u2264 fy(z\u2217) + c 2 Ez [ (z \u2212 z\u2217)>(z \u2212 z\u2217) ] .\nDefining\n\u03c9 , (x, . . . ,x\ufe38 \ufe37\ufe37 \ufe38 n\u2032 times , 1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 n\u2032\u2032 times ),\nwe next observe that\n(z \u2212 z\u2217)>(z \u2212 z\u2217) = \u03c9>(\u03b8 \u2212 \u03b8\u2217)(\u03b8 \u2212 \u03b8\u2217)>\u03c9. (A.2)\nLetting \u03a3 = Var[\u03b8], we thus have\nEz [ (z \u2212 z\u2217)>(z \u2212 z\u2217) ] = \u03c9>E\u03b8[(\u03b8 \u2212 \u03b8\u2217)(\u03b8 \u2212 \u03b8\u2217)>]\u03c9 \u2264 \u2016\u03c9\u201622\u2016E\u03b8[(\u03b8 \u2212 \u03b8\n\u2217)(\u03b8 \u2212 \u03b8\u2217)>]\u2016 = (n\u2032\u2016x\u201622 + n\u2032\u2032)\u2016\u03a3\u2016 \u2264 (n\u2032 + n\u2032\u2032)\u2016\u03a3\u2016\nsince it is assumed that \u2016x\u20162 \u2264 1. Noting that LQ(ZT ) = \u2211 t EQ[fyt(\u03bext,\u03c8)] and L\u03b8\u2217(ZT ) = \u2211 t fyt(\u03be \u2217xt,\u03c8 \u2217), we have\nLQ(ZT ) \u2264 L\u03b8\u2217(ZT ) + Tc(n\u2032 + n\u2032\u2032)\u2016\u03a3\u2016\n2 . (A.3)\nCombining (A.1) and (A.3) yields the theorem.\nProof of Theorem 2.2. Follows as a special case of Theorem 2.4 by choosing n\u2032 = 1 and n\u2032\u2032 = 0."}, {"heading": "A.1. Application to Multi-class Logistic Regression", "text": "For multi-class logistic regression (MLR) y \u2208 {1, . . . ,K} is one of K classes, the parameters are \u03b8 = {\u03b8(k)}Kk=1, and the likelihood is\np(y |\u03b8,x) = exp(\u03b8 (y) \u00b7 x)\u2211K\nk=1 exp(\u03b8 (k) \u00b7 x)\n. (A.4)\nIn order to apply Theorem 2.4, we require the following result:\nProposition A.1. Assumption (A1\u2019) holds for the MLR likelihood with c = 1/2.\nProof. First note that\nfy(z) = \u2212zy + ln \u2211K k=1 e zi , (A.5)\nwhere zi = \u03b8(k) \u00b7 x, and hence the Hessian of fy(z) is independent of y:\nf \u2032\u2032y (z) = 1 ( \u2211K k=1 e zi)2\n \u2211 i 6=1 e z1+zi \u2212ez1+z2 . . . \u2212ez1+zK \u2212ez2+z1 \u2211 i 6=2 e\nz2+zi . . . \u2212ez2+zK ... . . .  (A.6) Applying Gershgorin\u2019s circle theorem, we find that\n\u2016f \u2032\u2032y (z)\u2016 \u2264 2ez1\n\u2211 i6=1 e zi\n( \u2211K k=1 e zk)2 , (A.7)\nwhere with loss of generality we have applied the theorem to the first row of the Hessian. Defining a , ez1 \u2265 0 and b , \u2211 i 6=1 e\nzi \u2265 0, we have \u2016f \u2032\u2032y (z)\u2016 \u2264 2ab(a+b)2 . Maximization over the positive orthant occurs at a = b > 0, so \u2016f \u2032\u2032y (z)\u2016 \u2264 1/2.\nReasoning similarly to Theorem E.1, one can easily prove:\nTheorem A.2 (Hierarchical Gaussian regret, multi-class regression). If \u03b8(1:K)j \u223c N(0,\u03a3), j = 1, . . . , n, then using the MLR likelihood guarantees thatR(Z,\u03b8\u2217) is bounded by\nRmlr\u2212HGBayes (Z,\u03b8 \u2217) ,\n1 2\u03b32 \u2211K k=1 \u2016\u03b8 \u2217(k)\u20162 + \u03c3 2 0 \u03c32\u03b32 \u2211 k<` \u2016\u03b8 \u2217(k) \u2212 \u03b8\u2217(`)\u20162\n+ n\n2 ln\n( 1 +\nK\u03c320 \u03c32\n) + nK\n2 ln\n( 1\u2212 \u03c3 2 0\n\u03b32 + T\u03c32 2n\n) ,\n(A.8)\nwhere \u03b32 , K\u03c320 + \u03c3 2.\nTheorem 2.5 follows as a special case of Theorem A.2 by taking \u03c320 = 0."}, {"heading": "B. Proof of Theorem 3.2", "text": "Since pT (\u03b8) = p(Y |X,\u03b8)p0(\u03b8) p(Y |X) ,\nKL(PT ||P0) = EPT [ ln pT (\u03b8)\np0(\u03b8) ] = EPT [ ln p(Y |X,\u03b8) p(Y |X)\n] = LBayes(ZT )\u2212 LPT (ZT ). (B.1)\nCombining (2) and (B.1) with Theorem 3.1 implies that with probability 1\u2212 \u03b4, for all \u03b8,\n|L(PT )\u2212 L\u0302(PT , ZT )| \u2264 \u221a \u03ba\n\u221a L\u03b8(ZT )\u2212 LPT (ZT ) +B(\u03b8) + C(T ) + ln\u03ba\u2032/\u03b4\nT .\nObserving that L\u03b8\u2217(ZT ) < LPT (ZT ), so L\u03b8\u2217(ZT )\u2212 LPT (ZT ) < 0, completes the proof."}, {"heading": "C. KL Divergence Derivations", "text": ""}, {"heading": "C.1. Multivariate Gaussians", "text": "Let Di = N(\u00b5i,\u03a3i), i = 1, 2, where dim(\u00b5i) = n. Then\nKL(D1||D2) = 1\n2 ED1 [ ln |\u03a32| |\u03a31| \u2212 (x\u2212 \u00b51)>\u03a3\u221211 (x\u2212 \u00b51) + (x\u2212 \u00b52)>\u03a3 \u22121 2 (x\u2212 \u00b52) ] = 1\n2 { ln |\u03a32| |\u03a31| + ED1 [ \u2212Tr(\u03a3\u221211 (x\u2212 \u00b51)>(x\u2212 \u00b51)) + Tr(\u03a3 \u22121 2 (x\u2212 \u00b52)>(x\u2212 \u00b52)) ]} = 1\n2 { ln |\u03a32| |\u03a31| \u2212 Tr(\u03a3\u221211 \u03a31) + ED1 [ Tr(\u03a3\u221212 (x >x\u2212 2x>\u00b52 + \u00b5>2 \u00b52)) ]}\n= 1\n2 { ln |\u03a32| |\u03a31| \u2212 n+ ED1 [ Tr(\u03a3\u221212 (x >x\u2212 2x>\u00b52 + \u00b5>2 \u00b52)) ]}\n= 1\n2 { ln |\u03a32| |\u03a31| \u2212 n+ Tr(\u03a3\u221212 (\u03a31 + \u00b5>1 \u00b51 \u2212 2\u00b5>1 \u00b52 + \u00b5>2 \u00b52)) }\n= 1\n2 { ln |\u03a32| |\u03a31| \u2212 n+ Tr(\u03a3\u221212 \u03a31) + (\u00b51 \u2212 \u00b52)>\u03a3 \u22121 2 (\u00b51 \u2212 \u00b52) } .\nC.2. Gaussian and t-Distribution\nLet D1 = N(\u00b51,\u03a31) and D2 = T\u03bd(\u00b52,\u03a32), where dim(\u00b5i) = k. Then\nKL(D1||D2) = ln\n( \u0393(\u03bd2 )\u03bd k/2\n\u0393(\u03bd+k2 )\n) + k\n2 ln\u03c0 +\n1 2 ln |\u03a32| \u2212 k 2 ln 2\u03c0e\u2212 1 2 ln |\u03a31|\n+ \u03bd + k\n2 ED1\n[ ln ( 1 + 1\n\u03bd (x\u2212 \u00b52)>\u03a3\u221212 (x\u2212 \u00b52) )] = ln ( \u0393(\u03bd2 )\u03bd k/2\n\u0393(\u03bd+k2 )\n) + 1\n2 ln |\u03a32| |\u03a31| \u2212 k 2 ln 2e\n+ \u03bd + k\n2 ED1\n[ ln ( 1 + 1\n\u03bd (x\u2212 \u00b52)>\u03a3\u221212 (x\u2212 \u00b52)\n)] .\nFor the first term, if k is even, then\n\u0393(\u03bd2 )\u03bd k/2\n\u0393(\u03bd+k2 ) =\n\u03bdk/2\n(\u03bd+k2 ) k/2\n,\nwhere yn = y(y \u2212 1) . . . (y \u2212 n + 1) is the descending factorial. Now assume k is odd. By Gautschi\u2019s inequality, \u0393(a) \u0393(a+1/2) \u2264 ( 2a+1 2a2 )1/2 . Choosing a = \u03bd/2 yields\n\u0393(\u03bd2 )\u03bd k/2\n\u0393(\u03bd+k2 ) =\n\u0393(\u03bd2 )\u03bd 1/2\u03bd(k\u22121)/2\n\u0393(\u03bd+12 )( \u03bd+k 2 ) (k\u22121)/2 \u2264\n(\u03bd + 1)1/2\u03bd(k\u22121)/2\n(\u03bd2 ) 1/2(\u03bd+k2 )\n(k\u22121)/2 .\nNow, bounding the expectation gives\nED1 [ ln ( 1 + 1\n\u03bd (x\u2212 \u00b52)>\u03a3\u221212 (x\u2212 \u00b52) )] \u2264 ln ( 1 + 1\n\u03bd ED1\n[ (x\u2212 \u00b52)>\u03a3\u221212 (x\u2212 \u00b52) ]) = ln ( 1 + 1\n\u03bd Tr(\u03a3\u221212 \u03a31) +\n1 \u03bd (\u00b51 \u2212 \u00b52)>\u03a3\u221212 (\u00b51 \u2212 \u00b52)\n)\n\u2264 ln ( 1 + 1\n\u03bd (\u00b51 \u2212 \u00b52)>\u03a3\u221212 (\u00b51 \u2212 \u00b52)\n) +\nTr(\u03a3\u221212 \u03a31)\n\u03bd + (\u00b51 \u2212 \u00b52)>\u03a3\u221212 (\u00b51 \u2212 \u00b52) \u2264 ln ( 1 + 1\n\u03bd (\u00b51 \u2212 \u00b52)>\u03a3\u221212 (\u00b51 \u2212 \u00b52)\n) + 1\n\u03bd Tr(\u03a3\u221212 \u03a31),\nwhere the second inequality follows from the fact that ln(a+ b) \u2264 ln(a) + b/a. Combining everything yields\nKL(D1||D2) \u2264 ln \u039b\u03bd,k + 1 2 ln |\u03a32| |\u03a31| \u2212 k 2 ln 2e+ \u03bd + k 2\u03bd Tr(\u03a3\u221212 \u03a31)\n+ \u03bd + k\n2 ln\n( 1 + 1\n\u03bd (\u00b51 \u2212 \u00b52)>\u03a3\u221212 (\u00b51 \u2212 \u00b52)\n) ,\nwhere\n\u039b\u03bd,k =  \u03bdk/2 ( \u03bd+k2 ) k/2 if k is even (\u03bd+1)1/2\u03bd(k\u22121)/2\n( \u03bd2 ) 1/2( \u03bd+k2 )\n(k\u22121)/2 if k is odd."}, {"heading": "C.3. Gaussian and Laplace", "text": "Let D1 = N(\u00b5, \u03c32) and D2 = Lap(\u03b2). Then\nKL(D1||D2) = ln(2\u03b2) + 1\n\u03b2 ED1 [|x|]\u2212\n1 2 ln(2\u03c0e\u03c32)\n= ln(2\u03b2) + 1\n2\u03b2\n[ \u00b5Erf ( \u00b5\u221a 2 \u03c3 ) + 2 \u221a 2 \u03c3\u221a \u03c0 exp { \u2212 \u00b5 2 2\u03c32 }] \u2212 1 2 ln(2\u03c0e\u03c32)\n\u2264 1 2 ln 2\u03b22 \u03c32 + 1 2\u03b2\n[ |\u00b5| \u221a 1\u2212 exp { \u2212 2\u00b5 2\n\u03c0\u03c32\n} + 2 \u221a\n2 \u03c3\u221a \u03c0 exp\n{ \u2212 \u00b5 2\n2\u03c32\n}] \u2212 1\n2 ln(\u03c0e)."}, {"heading": "D. Proof of Theorem 4.1", "text": "Choose Q\u03b8\u2217,\u03c6 = N(\u03b8\u2217, \u03c62I). With P0 = T\u03bd(0, \u03c32I), we have (Appendix C.2)\nKL(Q\u03b8\u2217,\u03c6||P0) \u2264 ln \u039b\u03bd,n + n 2 ln \u03c32 \u03c62 \u2212 n 2 ln 2e+ n(\u03bd + n) 2\u03bd \u03c62 \u03c32 + \u03bd + n 2 ln\n( 1 + 1\n\u03bd\u03c32 \u2016\u03b8\u2217\u20162\n) ,\nwhere\n\u039b\u03bd,n =  \u03bdn/2 ( \u03bd+n2 ) n/2 if n is even (\u03bd+1)1/2\u03bd(n\u22121)/2\n( \u03bd2 ) 1/2( \u03bd+n2 )\n(n\u22121)/2 if n is odd.\nNote that if n is even then \u039b\u03bd,n 2n/2 \u2264 1 and if n is odd then \u039b\u03bd,n 2n/2 \u2264 \u03bd+1\u03bd . Since VarQ\u03b8\u2217,\u03c6 [\u03b8i] = \u03c6 2, we have\nLBayes(Z) \u2264 inf \u03b8\u2217 L\u03b8\u2217(Z) +\nTc\u03c62\n2 + n 2 ln \u03bd + 1 \u03bd + n 2 ln \u03c32 \u03c62 \u2212 n 2 + n(\u03bd + n) 2\u03bd \u03c62 \u03c32 + \u03bd + n 2 ln\n( 1 + 1\n\u03bd\u03c32 \u2016\u03b8\u2217\u20162 ) Choosing \u03c62 = \u03bd\u03c3\n2n Tc\u03bd\u03c32+(\u03bd+n)n yields the theorem."}, {"heading": "E. More on Hierarchical Priors for Sharing Statistical Strength", "text": ""}, {"heading": "E.1. Multiple Simultaneous Observations", "text": "The Bayesian learner receives K input-output pairs {(x(k)t , y (k) t )}Kk=1 at each time step. Each output is predicted using a separate weight vector \u03b8(k), so the k-th likelihood is p(y |\u03b8(k) \u00b7 x), k = 1, . . . ,K. Write Z(k) , {(x(k)t , y (k) t )}Tt=1.\nInstead of using independent Gaussian priors on \u03b8(1), . . . ,\u03b8(K), place a prior over the means of the K priors. For each dimension j = 1, . . . , n, let\n\u00b5j |\u03c320 \u223c N(0, \u03c320) (E.1)\nand\n\u03b8 (k) j |\u00b5j , \u03c3 2 \u223c N(\u00b5j , \u03c32), k = 1, . . . ,K, (E.2)\nand write \u03b8(1:K)j , (\u03b8 (1) j , . . . , \u03b8 (K) j ). Integrating out \u00b5j yields\n\u03b8 (1:K) j |\u03c3 2 0 , \u03c3 2 \u223c N(0,\u03a3), (E.3)\nwhere, with 1K denoting the K \u00d7K all-ones matrix,\n\u03a3 , s2\u03c11K + s 2(1\u2212 \u03c1)I s2 , \u03c320 + \u03c32 \u03c1 , \u03c320 \u03c320 + \u03c3 2 , (E.4)\nThe Bayesian learner uses this hierarchical prior to simultaneously predict y(1)t , . . . , y (K) t . For the following theorem, we must replace (A2) with an appropriately modified assumption for the simultaneous prediction task:\n\u2016x(k)t \u20162 \u2264 1 for all t, k. (A2\u2019)\nTheorem E.1 (Hierarchical Gaussian regret, simultaneous observations). If \u03b8(1:K)j \u223c N(0,\u03a3), j = 1, . . . , n, and (A2\u2019) holds in lieu of (A2), thenR(Z,\u03b8\u2217) is bounded by\nRHG\u2212simBayes (Z,\u03b8 \u2217) ,\n1 2\u03b32 \u2211K k=1 \u2016\u03b8 \u2217(k)\u20162 + \u03c3 2 0 \u03c32\u03b32 \u2211 k<` \u2016\u03b8 \u2217(k) \u2212 \u03b8\u2217(`)\u20162\n+ n\n2 ln\n( 1 +\nK\u03c320 \u03c32\n) + nK\n2 ln\n( 1\u2212 \u03c3 2 0\n\u03b32 + Tc\u03c32 n\n) ,\n(E.5)\nwhere \u03b32 , K\u03c320 + \u03c3 2.\nIt is instructive to compare the upper bound given in (E.5) to \u2211 k R G Bayes(Z(k),\u03b8 \u2217(k)) with prior variance s2 = \u03c320 + \u03c3 2.\nTo do so, we find \u2206(\u03b8\u2217) , \u2211 k R G Bayes(Z(k),\u03b8 \u2217(k))\u2212RHGBayes(Z,\u03b8 \u2217):\n\u2206(\u03b8\u2217) = (K \u2212 1)\u03c320 2\u03b32s2 \u2211K k=1 \u2016\u03b8 \u2217(k)\u20162 \u2212 \u03c3 2 0 \u03c32\u03b32 \u2211 k<` \u2016\u03b8 \u2217(k) \u2212 \u03b8\u2217(`)\u20162\n\u2212 nK 2 ln n s2\u03c32 (1\u2212 \u03c320\u03b32 ) + Tcs2 n+ Tcs2 \u2212 n 2 ln ([ 1 + K\u03c320 \u03c32 ] \u03c32K s2K ) For example, setting \u03c30 = \u03c3, so the correlation \u03c1 is 1/2, and K = 2, we find that if\n4\u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u20162 + 6s2n ln ( 4 3n+ Tcs 2\nn+ Tcs2\n) \u2264 \u2016\u03b8\u2217(1)\u20162 + \u2016\u03b8\u2217(2)\u20162 + 0.863s2n,\nthen the hierarchical model has a smaller regret bound than the non-hierarchical model.5 As long as Tcs2 > 2n, the condition becomes 4\u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u20162 \u2264 \u2016\u03b8\u2217(1)\u20162 + \u2016\u03b8\u2217(2)\u20162 +Cs2n for some 0 < C < 0.863. In this case there are two important observations about the benefits of the hierarchical model. First, noting that the expected magnitude of \u2016\u03b8\u2217(1)\u20162 and \u2016\u03b8\u2217(2)\u20162 is \u03c32n, as long as \u2016\u03b8\u2217(1)\u20162 and \u2016\u03b8\u2217(2)\u20162 are only a constant fraction C/4 of their expected magnitudes, the hierarchical model will always have smaller regret bound. Second, even if the previous condition does not hold, the difference in \u2016\u03b8\u2217(1) \u2212 \u03b8\u2217(2)\u2016 must be significantly larger than the expected magnitudes of \u2016\u03b8\u2217(1)\u20162 and \u2016\u03b8\u2217(2)\u20162 for the hierarchical model to have a larger regret bound than the non-hierarchical model. Thus, the use of the hierarchical model has potentially significantly reduced regret compared to the non-hierarchical model.\n5 For clarity, we have replaced 3 ln(4/3) with the bound 0.863."}, {"heading": "E.2. Two-level Prior", "text": "In this section we derive bounds for the two-level prior in the case of sequential observations. Recall that the prior is\n\u03b2 \u223c N(0, \u03c320I) (E.6) \u00b5(s) \u223c N(\u03b2, \u03c321I) s = 1, . . . , S (E.7)\n\u03b8(k) \u223c N(\u00b5(sk), \u03c322I) k = 1, . . . ,K. (E.8)\nIntegrating out \u03b2, we immediately obtain:\n\u00b5 (1:S) i \u223c N(0,\u03a3\u00b5), (E.9)\nwhere \u03a3\u00b5 , \u03c3201S + \u03c3 2 1I . Writing \u00b5i = \u00b5 (1:S) i and \u03b8i = \u03b8 (1:K) i , we have(\n\u00b5i \u03b8i\n) \u223c N (0,\u03a3) , \u03a3 , ( \u03a3\u00b5 \u03a3\u00b5\u03b8 \u03a3>\u00b5\u03b8 \u03a3\u03b8 ) . (E.10)\nHence,\n\u03b8i |\u00b5i \u223c N(\u03a3>\u00b5\u03b8\u03a3\u22121\u00b5 \u00b5i,\u03a3\u03b8 \u2212 \u03a3>\u00b5\u03b8\u03a3\u22121\u00b5 \u03a3\u00b5\u03b8). (E.11)\nDefine the matrix P such that Pks = 1{s = sk}. We therefore have \u03a3>\u00b5\u03b8\u03a3\u22121\u00b5 \u00b5i = P\u00b5i and hence \u03a3>\u00b5\u03b8 = P\u03a3\u00b5, and furthermore \u03a3\u03b8 \u2212 \u03a3>\u00b5\u03b8\u03a3\u22121\u00b5 \u03a3\u00b5\u03b8 = \u03c322I and hence \u03a3\u03b8 = \u03c322I + P\u03a3\u00b5P>. Hence, the prior on \u03b8i is P0 = N(0,\u03a3\u03b8). Choose Q\u03b8\u2217i ,\u03c6 = N(\u03b8 \u2217 i ,diag\u03c6), yielding\nKL(Q\u03b8\u2217i ,\u03c6||P0) = 1\n2 { ln |\u03a3\u03b8|\u220f k \u03c6 2 k \u2212 k \u2212 Tr(\u03a3\u22121\u03b8 ) \u2211 k \u03c62k + (\u03b8 \u2217 i ) >\u03a3\u22121\u03b8 \u03b8 \u2217 i } . (E.12)\nStraightforward calculations show that the regret is bounded by n\u2211 i=1 (\u03b8\u2217i ) >\u03a3\u22121\u03b8 \u03b8 \u2217 i + K\u2211 k=1 n 2 ln ( 2 Tr(\u03a3\u22121\u03b8 ) + cT (k) n ) + n 2 ln |\u03a3\u03b8|. (E.13)"}, {"heading": "E.3. Proof of Theorem E.1", "text": "First take n = 1, which will later generalize to arbitrary n. Choose Q\u03b8\u2217(1:K),\u03c6 = N(\u03b8 \u2217(1:K), \u03c62I) and note that\n|\u03a3| = \u03c32K\u22122(K\u03c320 + \u03c32) = \u03c32K\u22122\u03b32 and \u03a3\u22121 = \u2212 \u03c320 \u03c32\u03b32 1K + 1 \u03c32 I.\nThus (Appendix C.1)\nKL(Q\u03b8\u2217(1:K),\u03c6||P0) = 1\n2 { ln |\u03a3| |\u03c62I| \u2212K + \u03c62 Tr(\u03a3\u22121) + (\u03b8\u2217(1:K))>\u03a3\u22121\u03b8\u2217(1:K) }\n= K 2 ln \u03c32\u03b32/K \u03c62\u03c32/K \u2212 K 2 + K(\u03b32 \u2212 \u03c320) 2\u03c32\u03b32 \u03c62\n+ 1\n2\u03b32 K\u2211 k=1 (\u03b8\u2217(k))2 + \u03c320 \u03c32\u03b32 \u2211 k<` (\u03b8\u2217(k) \u2212 \u03b8\u2217(`))2.\nMoving to the case of general n, since VarQ\u03b8\u2217,\u03c6 [ \u2211 k \u03b8 (k) j ] = K\u03c6 2 for all j = 1, . . . , n, applying Theorem 2.2 gives\nLBayes(Z) \u2264 K\u2211 k=1 L\u03b8\u2217(k)(Z (k)) + TKc\u03c62 2 + nK 2 ln \u03c32\u03b32/K \u03c62\u03c32/K \u2212 nK 2\nnK(\u03b32 \u2212 \u03c320) 2\u03c32\u03b32 \u03c62 + 1 2\u03b32 K\u2211 k=1 \u2016\u03b8\u2217(k)\u20162 + \u03c3 2 0 \u03c32\u03b32 \u2211 k<` \u2016\u03b8\u2217(k) \u2212 \u03b8\u2217(`)\u20162.\nChoosing \u03c62 = n\u03c3 2\u03b32\nn(\u03b32\u2212\u03c320)+Tc\u03c32\u03b32 yields the theorem."}, {"heading": "E.4. Proof of Theorem 4.2", "text": "The proof is similar to that for Theorem E.1. However, use separate variances for each source: Q\u03b8\u2217(1:K),\u03c6 = \u220f k Q\u03b8\u2217(k),\u03c6k = \u220f k N(\u03b8\u2217(k), \u03c62k).\nThe error term from the Taylor expansion used in Theorem 2.2 is \u2211 k T (k)c\u03c62k 2 , so\nLBayes(Z) \u2264 K\u2211 k=1 L\u03b8\u2217(k)(Z (k)) + \u2211 k T (k)c\u03c62k 2 + n 2 ln \u03c32K\u03b32 \u03c32 \u220f k \u03c6 2 k \u2212 nK 2\nn(\u03b32 \u2212 \u03c320) 2\u03c32\u03b32 \u2211 k \u03c62k + 1 2\u03b32 K\u2211 k=1 \u2016\u03b8\u2217(k)\u20162 + \u03c3 2 0 \u03c32\u03b32 \u2211 k<` \u2016\u03b8\u2217(k) \u2212 \u03b8\u2217(`)\u20162.\nChoosing \u03c62k = n\u03c32\u03b32\nn(\u03b32\u2212\u03c320)+T (k)c\u03c32\u03b32 yields the theorem."}, {"heading": "F. More on Feature Selection", "text": ""}, {"heading": "F.1. The Bayesian Lasso", "text": "For Bayesian model average learner we have: Theorem F.1 (GLM Bayesian lasso regret). If \u03b8i \u223c Lap(\u03b8i, \u03b2), i = 1, . . . , n, then\nR(Z,\u03b8\u2217) \u2264 1 2\u03b2 \u2211 i min {\u221a 2 \u03c0\u03c62 (\u03b8\u2217i ) 2, |\u03b8\u2217i | }\n+ n\n2 ln  2T 2c2\u03b24(\u221a 2n2 + Tcn\u03b22\u03c0 \u2212 \u221a 2n2 )2  . (F.1)\nIn the regime of Tc\u03b22 n, (F.1) becomes (approximately)\nR(Z,\u03b8\u2217) \u2264 1 2\u03b2 \u2211 i min {\u221a 2 \u03c0\u03c62 (\u03b8\u2217i ) 2, |\u03b8\u2217i | } + Cn\nfor some constant C independent of \u03b2 and c. Hence, even for sparse \u03b8\u2217, the regret bound is \u0398(n). The inequalities used to prove the regret bound are all quite tight, so we conjecture that, up to constant factors, there is a matching lower bound, as least in the Gaussian regression case."}, {"heading": "F.2. Proof of Theorem F.1", "text": "Apply Theorem 2.2 with Q\u03b8\u2217,\u03c6 = N(\u03b8\u2217, \u03c62I). Since p0(\u03b8) = \u220f i Lap(\u03b8i, \u03b2), we have (see Appendix C.3)\nKL(Q\u03b8\u2217,\u03c6||P0) \u2264 n\n2 ln\n2\u03b22\n\u03c62 \u2212 n 2 ln(\u03c0e) + 1 2\u03b2 \u2211 i [ |\u03b8\u2217i | \u221a 1\u2212 exp { \u22122(\u03b8 \u2217 i ) 2 \u03c0\u03c62 } + 2 \u221a 2 \u03c6\u221a \u03c0 exp { \u2212 (\u03b8 \u2217 i ) 2 2\u03c62 }]\n\u2264 n 2 ln 2\u03b22 \u03c62 \u2212 n 2 ln(\u03c0e) + \u221a 2 n\u03c6\u221a \u03c0 \u03b2 + 1 2\u03b2 \u2211 i min {\u221a 2 \u03c0\u03c62 (\u03b8\u2217i ) 2, |\u03b8\u2217i | } .\nSince VarQ\u03b8\u2217,\u03c6 [\u03b8i] = \u03c6 2,\nLBayes(Z) \u2264 inf \u03b8\u2217 L\u03b8\u2217(Z) +\nTc\u03c62\n2 \u2212 n 2 ln(\u03c0e) + \u221a 2 n\u03c6\u221a \u03c0 \u03b2 + n 2 ln 2\u03b22 \u03c62 + 1 2\u03b2 \u2211 i min {\u221a 2 \u03c0\u03c62 (\u03b8\u2217i ) 2, |\u03b8\u2217i | } .\nChoosing \u03c62 =\n(\u221a 2n2+Tcn\u03b22\u03c0 \u2212 \u221a 2n2 )2 T 2c2\u03b22\u03c0 gives the desired result."}, {"heading": "F.3. Proof of Theorem 4.3", "text": "Fix some \u03b8\u2217. If \u03b8\u2217i = 0, then let Q\u03b8\u2217i ,\u03c62 = \u03b40, so KL(Q\u03b8\u2217i ,\u03c62 ||P0) = ln 1 p . If \u03b8 \u2217 i = 0, then let Q\u03b8\u2217i ,\u03c62 = N(\u03b8 \u2217 i , \u03c6 2), so\nKL(Q\u03b8\u2217i ,\u03c62 ||P0) = KL(Q\u03b8\u2217i ,\u03c62 ||N(0, \u03c3 2)) + ln\n1\n1\u2212 p .\nThe rest of the proof of (14) then closely follows earlier ones. To obtain (15), we observe that if p = q1/n, then\nm ln 1\n1\u2212 p = m ln\n1 1\u2212 q1/n \u2264 m ln n 1\u2212 q\nand\n(n\u2212m) ln 1 p = n\u2212m n ln 1 q \u2264 ln 1 q ."}], "references": [{"title": "Probabilistic Projections of the Total Fertility Rate for All Countries", "author": ["L. Alkema", "A.E. Raftery", "P. Gerland", "S.J. Clark", "F. Pelletier", "T. Buettner", "G.K. Heilig"], "venue": "Demography,", "citeRegEx": "Alkema et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alkema et al\\.", "year": 2011}, {"title": "Combining PACBayesian and generic chaining bounds", "author": ["Audibert", "J.-Y", "O. Bousquet"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Audibert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2007}, {"title": "On Bayesian Bounds", "author": ["A. Banerjee"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Banerjee,? \\Q2006\\E", "shortCiteRegEx": "Banerjee", "year": 2006}, {"title": "An Analysis of Logistic Models: Exponential Family Connections and Online Performance", "author": ["A. Banerjee"], "venue": "In International Conference on Data Mining, pp", "citeRegEx": "Banerjee,? \\Q2007\\E", "shortCiteRegEx": "Banerjee", "year": 2007}, {"title": "A Bayesian/information theoretic model of learning to learn via multiple task sampling", "author": ["J. Baxter"], "venue": "Machine learning,", "citeRegEx": "Baxter,? \\Q1997\\E", "shortCiteRegEx": "Baxter", "year": 1997}, {"title": "A Model of Inductive Bias Learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter,? \\Q2000\\E", "shortCiteRegEx": "Baxter", "year": 2000}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Ben.David and Schuller,? \\Q2003\\E", "shortCiteRegEx": "Ben.David and Schuller", "year": 2003}, {"title": "Statistical Decision Theory and Bayesian Analysis", "author": ["J.O. Berger"], "venue": null, "citeRegEx": "Berger,? \\Q1985\\E", "shortCiteRegEx": "Berger", "year": 1985}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning, volume 56 of Lecture Notes ", "author": ["O. Catoni"], "venue": "Monograph Series. Institute of Mathematical Statistics,", "citeRegEx": "Catoni,? \\Q2007\\E", "shortCiteRegEx": "Catoni", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Data Analysis Using Regression and Multilevel/Hierarchical Models", "author": ["A. Gelman", "J. Hill"], "venue": null, "citeRegEx": "Gelman and Hill,? \\Q2006\\E", "shortCiteRegEx": "Gelman and Hill", "year": 2006}, {"title": "Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups", "author": ["Y. Ghitza", "A. Gelman"], "venue": "American Journal of Political Science,", "citeRegEx": "Ghitza and Gelman,? \\Q2013\\E", "shortCiteRegEx": "Ghitza and Gelman", "year": 2013}, {"title": "The Minimum Description Length Principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald", "year": 2007}, {"title": "On universal transfer learning", "author": ["M.M. Hassan Mahmud"], "venue": "Theoretical Computer Science,", "citeRegEx": "Mahmud,? \\Q2009\\E", "shortCiteRegEx": "Mahmud", "year": 2009}, {"title": "Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations", "author": ["M.M. Hassan Mahmud", "S.R. Ray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahmud and Ray,? \\Q2007\\E", "shortCiteRegEx": "Mahmud and Ray", "year": 2007}, {"title": "Spike and slab variable selection: Frequentist and Bayesian strategies", "author": ["H. Ishwaran", "J.S. Rao"], "venue": "The Annals of Statistics,", "citeRegEx": "Ishwaran and Rao,? \\Q2005\\E", "shortCiteRegEx": "Ishwaran and Rao", "year": 2005}, {"title": "Estimating Relatedness via Data Compression", "author": ["B. Juba"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Juba,? \\Q2006\\E", "shortCiteRegEx": "Juba", "year": 2006}, {"title": "Online Bounds for Bayesian Algorithms", "author": ["S.M. Kakade", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade and Ng,? \\Q2004\\E", "shortCiteRegEx": "Kakade and Ng", "year": 2004}, {"title": "Worst-case bounds for Gaussian process models", "author": ["S.M. Kakade", "M. Seeger", "D.P. Foster"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2005}, {"title": "Simplified PAC-Bayesian Margin Bounds", "author": ["D.A. McAllester"], "venue": "In Conference on Learning Theory,", "citeRegEx": "McAllester,? \\Q2003\\E", "shortCiteRegEx": "McAllester", "year": 2003}, {"title": "Big Bayes Stories\u2014 Foreword", "author": ["K.L. Mengersen", "C.P. Robert"], "venue": "Statistical Science,", "citeRegEx": "Mengersen and Robert,? \\Q2014\\E", "shortCiteRegEx": "Mengersen and Robert", "year": 2014}, {"title": "Bayesian variable selection with shrinking and diffusing priors", "author": ["N.N. Narisetty", "X. He"], "venue": "The Annals of Statistics,", "citeRegEx": "Narisetty and He,? \\Q2014\\E", "shortCiteRegEx": "Narisetty and He", "year": 2014}, {"title": "The Bayesian Lasso", "author": ["T. Park", "G. Casella"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Park and Casella,? \\Q2008\\E", "shortCiteRegEx": "Park and Casella", "year": 2008}, {"title": "A PAC-Bayesian bound for Lifelong Learning", "author": ["A. Pentina", "C.H. Lampert"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Pentina and Lampert,? \\Q2014\\E", "shortCiteRegEx": "Pentina and Lampert", "year": 2014}, {"title": "Bayesian probabilistic population projections for all countries", "author": ["A.E. Raftery", "N. Li", "H. \u0160ev\u010d\u0131\u0301kov\u00e1", "P. Gerland", "G.K. Heilig"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Raftery et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raftery et al\\.", "year": 2012}, {"title": "Bayesian Probabilistic Projections of Life Expectancy for All Countries", "author": ["A.E. Raftery", "J.L. Chunn", "P. Gerland", "H. \u0160ev\u010d\u0131\u0301kov\u00e1"], "venue": "Demography,", "citeRegEx": "Raftery et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Raftery et al\\.", "year": 2013}, {"title": "Learning to share visual appearance for multiclass object detection", "author": ["R. Salakhutdinov", "A. Torralba", "J.B. Tenenbaum"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2011}, {"title": "Perfect sampling for Bayesian variable selection in a linear regression model", "author": ["U. Schneider", "J.N. Corcoran"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Schneider and Corcoran,? \\Q2004\\E", "shortCiteRegEx": "Schneider and Corcoran", "year": 2004}, {"title": "Information Consistency of Nonparametric Gaussian Process Methods", "author": ["M.W. Seeger", "S.M. Kakade", "D.P. Foster"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Seeger et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Seeger et al\\.", "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Competitive Online Statistics", "author": ["V. Vovk"], "venue": "International Statistical Review,", "citeRegEx": "Vovk,? \\Q2001\\E", "shortCiteRegEx": "Vovk", "year": 2001}], "referenceMentions": [{"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013).", "startOffset": 46, "endOffset": 105}, {"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al.", "startOffset": 47, "endOffset": 937}, {"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences.", "startOffset": 47, "endOffset": 962}, {"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model.", "startOffset": 47, "endOffset": 1164}, {"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model. The motivation of Salakhutdinov et al. (2011) was that, for image classification tasks, some categories of objects (e.", "startOffset": 47, "endOffset": 1312}, {"referenceID": 7, "context": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model. The motivation of Salakhutdinov et al. (2011) was that, for image classification tasks, some categories of objects (e.g., \u201ccar\u201d or \u201cdog\u201d) have many labeled positive and negative examples while other, visually related, categories (e.g., \u201cbus\u201d or \u201canteater\u201d) have only a few labeled examples. Fig. 1(right, a) shows the distribution of training examples for the 200 object categories used while Fig. 1(right, b) shows the same distribution, but now objects are grouped with those with similar appearances. In both cases, the distributions are fat-tailed: there are a few categories with many training examples and many categories with a few training examples. It was hypothesized that by using a hierarchical Bayesian model, the classes with large amounts of labeled data could be used to construct better classifiers for the classes with small amounts of labeled data. The model used by Salakhutdinov et al. (2011), which ar X iv :1 50 5.", "startOffset": 47, "endOffset": 2180}, {"referenceID": 27, "context": "Why the different performance characteristics for the two hierarchical models? Why do some categories have improved accuracy while others decreased accuracy? In a post-hoc analysis, Salakhutdinov et al. (2011) note that the \u201cobjects with the largest improvement.", "startOffset": 182, "endOffset": 210}, {"referenceID": 27, "context": "Why the different performance characteristics for the two hierarchical models? Why do some categories have improved accuracy while others decreased accuracy? In a post-hoc analysis, Salakhutdinov et al. (2011) note that the \u201cobjects with the largest improvement...borrow visual appearance from other frequent objects\u201d while \u201cobjects with the largest decrease [such as \u2018umbrella\u2019 and \u2018merchandise\u2019] are abstract, and their visual appearance is very different from other object categories.\u201d The results just described lead to numerous theoretical questions of practical consequence: Q1 Can we formalize why for some object classes there was a beneficial sharing of statistical strength, while for other classes the sharing was detrimental? Q2 Can we understand when a flat model should be preferred to a hierarchical one to avoid unfavorable sharing? Q3 More generally, can we obtain guidance on the best type of prior for the problem at hand? Perhaps a different hierarchical prior would have been better suited to learning the image classifiers. For example, could placing hyperpriors on the variance parameters lead to greater \u201crobustness\u201d for object categories such as \u2018umbrella\u2019 and \u2018merchandise,\u2019 whose visual appearance differs from other object categories? Q4 Once the form of the prior has been chosen, how should hyperparameters be set to maximize learning? The settings of the variance hyperparameters was left unspecified by Salakhutdinov et al. (2011), and it is not clear a priori how they should be set, or how much effect their choice will have on learning.", "startOffset": 182, "endOffset": 1463}, {"referenceID": 27, "context": "Reproduced and reconstructed from Salakhutdinov et al. (2011).", "startOffset": 34, "endOffset": 62}, {"referenceID": 26, "context": "For example, one might instead consider the hierarchical models have been used in political science for analyzing polling and census data to predict election outcomes (Ghitza & Gelman, 2013) and in demography for predicting population growth, life expectancy, and fertility rates (Raftery et al., 2013; 2012; Alkema et al., 2011).", "startOffset": 280, "endOffset": 329}, {"referenceID": 0, "context": "For example, one might instead consider the hierarchical models have been used in political science for analyzing polling and census data to predict election outcomes (Ghitza & Gelman, 2013) and in demography for predicting population growth, life expectancy, and fertility rates (Raftery et al., 2013; 2012; Alkema et al., 2011).", "startOffset": 280, "endOffset": 329}, {"referenceID": 31, "context": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient.", "startOffset": 76, "endOffset": 171}, {"referenceID": 19, "context": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient.", "startOffset": 76, "endOffset": 171}, {"referenceID": 2, "context": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient.", "startOffset": 76, "endOffset": 171}, {"referenceID": 29, "context": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient.", "startOffset": 76, "endOffset": 171}, {"referenceID": 4, "context": "Our results, which complement existing work on transfer and multitask learning theory (Baxter, 1997; Ben-David & Schuller, 2003; Pentina & Lampert, 2014), show that when the parameters with small regret for a collection of related tasks are either (a) similar or (b) not unexpected under the prior, then the hierarchical model has a smaller regret bound than assuming the tasks are independent.", "startOffset": 86, "endOffset": 153}, {"referenceID": 2, "context": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al.", "startOffset": 8, "endOffset": 424}, {"referenceID": 2, "context": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al.", "startOffset": 8, "endOffset": 470}, {"referenceID": 2, "context": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al.", "startOffset": 8, "endOffset": 487}, {"referenceID": 2, "context": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al. (2008), which applies to a large class of Bayesian generalized linear models (GLMs).", "startOffset": 8, "endOffset": 513}, {"referenceID": 19, "context": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2.", "startOffset": 110, "endOffset": 171}, {"referenceID": 29, "context": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2.", "startOffset": 110, "endOffset": 171}, {"referenceID": 17, "context": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2.1 (Kakade & Ng (2004); Banerjee (2006)).", "startOffset": 130, "endOffset": 253}, {"referenceID": 2, "context": "1 (Kakade & Ng (2004); Banerjee (2006)).", "startOffset": 23, "endOffset": 39}, {"referenceID": 20, "context": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007).", "startOffset": 87, "endOffset": 146}, {"referenceID": 9, "context": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007).", "startOffset": 87, "endOffset": 146}, {"referenceID": 9, "context": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007). Such bounds also have the benefit of applying to any bounded loss (e.g., the 0-1 loss for binary classification), which may be more task-relevant than the log-loss. In the batch setting, the data ZT are received all at once by the learner and are assumed to be distributed i.i.d. according to some distribution D over X \u00d7 Y: (xt, yt) i.i.d. \u223c D, t = 1, . . . , T . Let ` be a bounded loss function taking a probability distribution over Y and an element of Y as arguments. Without loss of generality assume ` \u2208 [0, 1]. Writing `\u03b8(x, y) , `(p(\u00b7 |x,\u03b8), y), for any distribution Q over \u0398, let L(Q) , E(x,y)\u223cDE\u03b8\u223cQ[`\u03b8(x, y)] L\u0302(Q,ZT ) , T\u22121 \u2211T t=1 E\u03b8\u223cQ[`\u03b8(xt, yt)] be, respectively, the expected and empirical losses under Q. PAC-Bayesian analyses consider the risk of the Gibbs predictor for the distribution Q (i.e., sample \u03b8 \u223c Q, predict with p(\u00b7 |x,\u03b8)), not the model average over Q (i.e., predict with \u222b p(\u00b7 |x,\u03b8)Q(d\u03b8)). A typical bound (specialized to the Bayesian setting) is the following (here pT (\u03b8) , p(\u03b8 |ZT )): Theorem 3.1 (Audibert & Bousquet (2007)).", "startOffset": 133, "endOffset": 1207}, {"referenceID": 7, "context": "Specifically, we analyze a canonical use of a hierarchical prior \u2014 to capture greater uncertainty in the value of a parameter by placing a hyperprior on the variance of the Gaussian prior on that parameter (Berger, 1985; Bishop, 2006; Gelman et al., 2013): \u03c3 0 |\u03b1, \u03b2 \u223c \u0393\u22121(\u03b1, \u03b2) and \u03b8i |\u03bc0, \u03c3 0 \u223c N(\u03bc0, \u03c3 0), where \u0393\u22121(\u03b1, \u03b2) is the inverse gamma distribution with shape \u03b1 and scale \u03b2.", "startOffset": 206, "endOffset": 255}, {"referenceID": 8, "context": "Specifically, we analyze a canonical use of a hierarchical prior \u2014 to capture greater uncertainty in the value of a parameter by placing a hyperprior on the variance of the Gaussian prior on that parameter (Berger, 1985; Bishop, 2006; Gelman et al., 2013): \u03c3 0 |\u03b1, \u03b2 \u223c \u0393\u22121(\u03b1, \u03b2) and \u03b8i |\u03bc0, \u03c3 0 \u223c N(\u03bc0, \u03c3 0), where \u0393\u22121(\u03b1, \u03b2) is the inverse gamma distribution with shape \u03b1 and scale \u03b2.", "startOffset": 206, "endOffset": 255}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory.", "startOffset": 115, "endOffset": 411}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory.", "startOffset": 115, "endOffset": 443}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models.", "startOffset": 115, "endOffset": 554}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective.", "startOffset": 115, "endOffset": 656}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective.", "startOffset": 115, "endOffset": 718}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective.", "startOffset": 115, "endOffset": 740}, {"referenceID": 4, "context": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective.", "startOffset": 115, "endOffset": 757}, {"referenceID": 27, "context": "\u03a3 , s\u03c11K + s (1\u2212 \u03c1)I (9) s , \u03c3 0 + \u03c3 , \u03c1 , \u03c3 0/(\u03c3 2 0 + \u03c3 ), (10) This prior corresponds to the one-level prior in Salakhutdinov et al. (2011). Similar results, which will be discussed qualitatively below, can be obtained for the two-level prior at the cost of a significantly more complicated bound.", "startOffset": 115, "endOffset": 143}, {"referenceID": 27, "context": "This setting is exactly that of the image classification problem of Salakhutdinov et al. (2011). For concreteness, consider a \u201clarge data\u201d task with T (1) n cs2 and a \u201csmall data\u201d task T (2) = 2, so that ln ( 4 3n+T cs n+T (1)cs2 ) \u2248 0 and (12) becomes (approximately)", "startOffset": 68, "endOffset": 96}, {"referenceID": 27, "context": "The regret bound for the two-level prior in Salakhutdinov et al. (2011) is quite similar to that for the one-level prior.", "startOffset": 44, "endOffset": 72}, {"referenceID": 27, "context": "The two-level regret bound well-explains the results of Salakhutdinov et al. (2011). The poor performance on image classes with very different visual appearance from the other classes is unsurprising since the parameter vectors that predict these classes well are going to have large ` distance from the parameter vectors of other object classes.", "startOffset": 56, "endOffset": 84}, {"referenceID": 19, "context": "4 In the infinite-dimensional case, Gaussian processes can be used while still obtaining meaningful regret bounds (Kakade et al., 2005; Seeger et al., 2008).", "startOffset": 114, "endOffset": 156}, {"referenceID": 29, "context": "4 In the infinite-dimensional case, Gaussian processes can be used while still obtaining meaningful regret bounds (Kakade et al., 2005; Seeger et al., 2008).", "startOffset": 114, "endOffset": 156}, {"referenceID": 30, "context": "A popular non-Bayesian approach for inducing sparsity is `1 regularization, such as the lasso for linear regression (Tibshirani, 1996).", "startOffset": 116, "endOffset": 134}, {"referenceID": 2, "context": ", Banerjee (2007). In particular, if p , q for some constant 0 < q < 1, then R Bayes(Z,\u03b8 \u2217) is at most \u2016\u03b8\u2217\u20162 2\u03c32 +m ln n 1\u2212 q + ln 1 q + m 2 ln ( 1 + Tc\u03c3 m ) .", "startOffset": 2, "endOffset": 18}], "year": 2015, "abstractText": "Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater \u201crobustness\u201d and the ability to \u201cshare statistical strength.\u201d Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student\u2019s t and hierarchical Gaussian priors allow us to formalize the concepts of \u201crobustness\u201d and \u201csharing statistical strength.\u201d Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.", "creator": "LaTeX with hyperref package"}}}