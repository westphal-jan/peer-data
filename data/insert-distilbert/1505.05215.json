{"id": "1505.05215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Learning with a Drifting Target Concept", "abstract": "we study the conceptual problem of learning in the presence of a drifting target concept. specifically, we provide bounds always on the error rate induced at a broadly given reaction time, given a learner with access bound to knowledge a history of independent neural samples invariably labeled according to a target information concept that can change on each round. one of our main geometric contributions is a refinement of the best previous results observed for polynomial - time algorithms for the space of linear separators under a uniform distribution. we also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. some users of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated accuracy bounds on counting the error rates.", "histories": [["v1", "Wed, 20 May 2015 00:41:23 GMT  (41kb)", "http://arxiv.org/abs/1505.05215v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["steve hanneke", "varun kanade", "liu yang"], "accepted": false, "id": "1505.05215"}, "pdf": {"name": "1505.05215.pdf", "metadata": {"source": "CRF", "title": "Learning with a Drifting Target Concept", "authors": ["Steve Hanneke", "Varun Kanade"], "emails": ["steve.hanneke@gmail.com", "varun.kanade@ens.fr", "yangli@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n05 21\n5v 1\n[ cs\n.L G\n] 2\n0 M"}, {"heading": "1 Introduction", "text": "Much of the work on statistical learning has focused on learning settings in which the concept to be learned is static over time. However, there are many application areas where this is not the case. For instance, in the problem of face recognition, the concept to be learned actually changes over time as each individual\u2019s facial features evolve over time. In this work, we study the problem of learning with a drifting target concept. Specifically, we consider a statistical learning setting, in which data arrive i.i.d. in a stream, and for each data point, the learner is required to predict a label for the data point at that time. We are then interested in obtaining low error rates for these predictions. The target labels are generated from a function known to reside in a given concept space, and at each time t the target function is allowed to change by at most some distance \u2206t: that is, the probability the new target function disagrees with the previous target function on a random sample is at most \u2206t.\nThis framework has previously been studied in a number of articles. The classic works of [HL91,HL94,BH96,Lon99,BBDK00] and [BL97] together provide a general analysis of a very-much related setting. Though the objectives in these works are specified slightly differently, the results established there are easily translated into our present framework, and we summarize many of the relevant results from this literature in Section 3.\nWhile the results in these classic works are general, the best guarantees on the error rates are only known for methods having no guarantees of computational efficiency. In a more recent effort, the work of [CMEDV10] studies this problem in the specific context of learning a homogeneous linear separator, when all the \u2206t values are identical. They propose a polynomial-time algorithm (based on the modified Perceptron algorithm of [DKM09]), and prove a bound on the number of mistakes it makes as a function of the number of samples, when the data distribution satisfies a certain condition called \u201c\u03bb-good\u201d (which generalizes a useful property of the uniform distribution on the origin-centered unit sphere). However, their result is again worse than that obtainable by the known computationally-inefficient methods.\nThus, the natural question is whether there exists a polynomial-time algorithm achieving roughly the same guarantees on the error rates known for the inefficient methods. In the present work, we resolve this question in the case of learning homogeneous linear separators under the uniform distribution, by proposing a polynomial-time algorithm that indeed achieves roughly the same bounds on the error rates known for the inefficient methods in the literature. This represents the main technical contribution of this work.\nWe also study the interesting problem of adaptivity of an algorithm to the sequence of \u2206t values, in the setting where \u2206t may itself vary over time. Since the values \u2206t might typically not be accessible in practice, it seems important to have learning methods having no explicit dependence on the sequence \u2206t. We propose such a method below, and prove that it achieves roughly the same bounds on the error rates known for methods in the literature which require direct access to the \u2206t values. Also in the context of variable \u2206t sequences, we discuss conditions on the sequence \u2206t necessary and sufficient for there to exist a learning method guaranteeing a sublinear rate of growth of the number of mistakes.\nWe additionally study an active learning extension to this framework, in which, at each time, after making its prediction, the algorithm may decide whether or not to request access to the label assigned to the data point at that time. In addition to guarantees on the error rates (for all times, including those for which the label was not observed), we are also interested in bounding the number of labels we expect the algorithm to request, as a function of the number of samples encountered thus far."}, {"heading": "2 Definitions and Notation", "text": "Formally, in this setting, there is a fixed distribution P over the instance space X , and there is a sequence of independent P-distributed unlabeled data X1, X2, . . .. There is also a concept space C, and a sequence of target functions h\u2217 = {h\u22171, h\u22172, . . .} in C. Each t has an associated target label Yt = h\u2217t (Xt). In this context, a (passive) learning algorithm is required, on each round t, to pro-\nduce a classifier h\u0302t based on the observations (X1, Y1), . . . , (Xt\u22121, Yt\u22121), and we denote by Y\u0302t = h\u0302t(Xt) the corresponding prediction by the algorithm for\nthe label of Xt. For any classifier h, we define ert(h) = P(x : h(x) 6= h\u2217t (x)). We also say the algorithm makes a \u201cmistake\u201d on instance Xt if Y\u0302t 6= Yt; thus, ert(h\u0302t) = P(Y\u0302t 6= Yt|(X1, Y1), . . . , (Xt\u22121, Yt\u22121)).\nFor notational convenience, we will suppose the h\u2217t sequence is chosen independently from the Xt sequence (i.e., h \u2217 t is chosen prior to the \u201cdraw\u201d of X1, X2, . . . \u223c P), and is not random. In each of our results, we will suppose h\u2217 is chosen from some set S of sequences in C. In particular, we are interested in describing the sequence h\u2217 in terms of the magnitudes of changes in h\u2217t from one time to the next. Specifically, for any sequence \u2206 = {\u2206t}\u221et=2 in [0, 1], we denote by S\u2206 the set of all sequences h\u2217 in C such that, \u2200t \u2208 N, P(x : ht(x) 6= ht+1(x)) \u2264 \u2206t+1.\nThroughout this article, we denote by d the VC dimension of C [VC71], and we suppose C is such that 1 \u2264 d < \u221e. Also, for any x \u2208 R, define Log(x) = ln(max{x, e}).\n3 Background: (\u01eb, S)-Tracking Algorithms\nAs mentioned, the classic literature on learning with a drifting target concept is expressed in terms of a slightly different model. In order to relate those results to our present setting, we first introduce the classic setting. Specifically, we consider a model introduced by [HL94], presented here in a more-general form inspired by [BBDK00]. For a set S of sequences {ht}\u221et=1 in C, and a value \u01eb > 0, an algorithm A is said to be (\u01eb, S)-tracking if \u2203t\u01eb \u2208 N such that, for any choice of h\u2217 \u2208 S, \u2200T \u2265 t\u01eb, the prediction Y\u0302T produced by A at time T satisfies\nP\n( Y\u0302T 6= YT ) \u2264 \u01eb.\nNote that the value of the probability in the above expression may be influenced by {Xt}Tt=1, {h\u2217t }Tt=1, and any internal randomness of the algorithm A.\nThe focus of the results expressed in this classical model is determining sufficient conditions on the set S for there to exist an (\u01eb, S)-tracking algorithm, along with bounds on the sufficient size of t\u01eb. These conditions on S typically take the form of an assumption on the drift rate, expressed in terms of \u01eb. Below, we summarize several of the strongest known results for this setting."}, {"heading": "3.1 Bounded Drift Rate", "text": "The simplest, and perhaps most elegant, results for (\u01eb, S)-tracking algorithms is for the set S of sequences with a bounded drift rate. Specifically, for any \u2206 \u2208 [0, 1], define S\u2206 = S\u2206, where \u2206 is such that \u2206t+1 = \u2206 for every t \u2208 N. The study of this problem was initiated in the original work of [HL94]. The best known general results are due to [Lon99]: namely, that for some \u2206\u01eb = \u0398(\u01eb2/d), for every \u01eb \u2208 (0, 1], there exists an (\u01eb, S\u2206)-tracking algorithm for all values of \u2206 \u2264 \u2206\u01eb.4 This refined an earlier result of [HL94] by a logarithmic 4 In fact, [Lon99] also allowed the distribution P to vary gradually over time. For simplicity, we will only discuss the case of fixed P .\nfactor. [Lon99] further argued that this result can be achieved with t\u01eb = \u0398(d/\u01eb). The algorithm itself involves a beautiful modification of the one-inclusion graph prediction strategy of [HLW94]; since its specification is somewhat involved, we refer the interested reader to the original work of [Lon99] for the details."}, {"heading": "3.2 Varying Drift Rate: Nonadaptive Algorithm", "text": "In addition to the concrete bounds for the case h\u2217 \u2208 S\u2206, [HL94] additionally present an elegant general result. Specifically, they argue that, for any \u01eb > 0, and any m = \u2126 (\nd \u01ebLog 1 \u01eb\n) , if \u2211m i=1 P(x : h\u2217i (x) 6= h\u2217m+1(x)) \u2264 m\u01eb/24, then for h\u0302 = argminh\u2208C \u2211m i=1 1[h(Xi) 6= Yi], P(h\u0302(Xm+1) 6= h\u2217m+1(Xm+1)) \u2264 \u01eb.5 This result immediately inspires an algorithm A which, at every time t, chooses a value mt \u2264 t\u22121, and predicts Y\u0302t = h\u0302t(Xt), for h\u0302t = argminh\u2208C \u2211t\u22121 i=t\u2212mt 1[h(Xi) 6= Yi]. We are then interested in choosing mt to minimize the value of \u01eb obtainable via the result of [HL94]. However, that method is based on the values P(x : h\u2217i (x) 6= h\u2217t (x)), which would typically not be accessible to the algorithm. However, suppose instead we have access to a sequence \u2206 such that h\u2217 \u2208 S\u2206. In this case, we could approximate P(x : h\u2217i (x) 6= h\u2217t (x)) by its upper bound \u2211t j=i+1 \u2206j . In this case, we are interested choosing mt to minimize the smallest value of \u01eb such that\n\u2211t\u22121 i=t\u2212mt \u2211t j=i+1 \u2206j \u2264 mt\u01eb/24 and mt = \u2126 ( d \u01ebLog 1 \u01eb )\n. One can easily verify that this minimum is obtained at a value\nmt = \u0398\n\nargmin m\u2264t\u22121\n1\nm\nt\u22121 \u2211\ni=t\u2212m\nt \u2211\nj=i+1\n\u2206j + dLog(m/d)\nm\n\n ,\nand via the result of [HL94] (applied to the sequenceXt\u2212mt , . . . , Xt) the resulting algorithm has\nP\n( Y\u0302t 6= Yt ) \u2264 O\n\n min 1\u2264m\u2264t\u22121\n1\nm\nt\u22121 \u2211\ni=t\u2212m\nt \u2211\nj=i+1\n\u2206j + dLog(m/d)\nm\n\n . (1)\nAs a special case, if every t has \u2206t = \u2206 for a fixed value \u2206 \u2208 [0, 1], this result recovers the bound \u221a\nd\u2206Log(1/\u2206), which is only slightly larger than that obtainable from the best bound of [Lon99]. It also applies to far more general and more intersting sequences \u2206, including some that allow periodic large jumps (i.e.,\u2206t = 1 for some indices t), others where the sequence\u2206t converges to 0, and so on. Note, however, that the algorithm obtaining this bound directly depends on the sequence\u2206. One of the contributions of the present work is to remove this requirement, while maintaining essentially the same bound, though in a slightly different form.\n5 They in fact prove a more general result, which also applies to methods approximately minimizing the number of mistakes, but for simplicity we will only discuss this basic version of the result."}, {"heading": "3.3 Computational Efficiency", "text": "[HL94] also proposed a reduction-based approach, which sometimes yields computationally efficient methods, though the tolerable \u2206 value is smaller. Specifically, given any (randomized) polynomial-time algorithmA that produces a classifier h \u2208 C with \u2211mt=1 1[h(xt) 6= yt] = 0 for any sequence (x1, y1), . . . , (xm, ym) for which such a classifier h exists (called the consistency problem), they propose a polynomial-time algorithm that is (\u01eb, S\u2206)-tracking for all values of \u2206 \u2264 \u2206\u2032\u01eb, where \u2206\u2032\u01eb = \u0398 ( \u01eb2\nd2Log(1/\u01eb)\n)\n. This is slightly worse (by a factor of dLog(1/\u01eb))\nthan the drift rate tolerable by the (typically inefficient) algorithm mentioned above. However, it does sometimes yield computationally-efficient methods. For instance, there are known polynomial-time algorithms for the consistency problem for the classes of linear separators, conjunctions, and axis-aligned rectangles."}, {"heading": "3.4 Lower Bounds", "text": "[HL94] additionally prove lower bounds for specific concept spaces: namely, linear separators and axis-aligned rectangles. They specifically argue that, for C a concept space\nBASICn = {\u222ani=1[i/n, (i+ ai)/n) : a \u2208 [0, 1]n}\non [0, 1], under P the uniform distribution on [0, 1], for any \u01eb \u2208 [0, 1/e2] and \u2206\u01eb \u2265 e4\u01eb2/n, for any algorithm A, and any T \u2208 N, there exists a choice of h\u2217 \u2208 S\u2206\u01eb such that the prediction Y\u0302T produced by A at time T satisfies P ( Y\u0302T 6= YT ) > \u01eb. Based on this, they conclude that no (\u01eb, S\u2206\u01eb)-tracking algorithm exists. Furthermore, they observe that the space BASICn is embeddable in many commonly-studied concept spaces, including halfspaces and axis-aligned rectangles in Rn, so that for C equal to either of these spaces, there also is no (\u01eb, S\u2206\u01eb)-tracking algorithm."}, {"heading": "4 Adapting to Arbitrarily Varying Drift Rates", "text": "This section presents a general bound on the error rate at each time, expressed as a function of the rates of drift, which are allowed to be arbitrary. Mostimportantly, in contrast to the methods from the literature discussed above, the method achieving this general result is adaptive to the drift rates, so that it requires no information about the drift rates in advance. This is an appealing property, as it essentially allows the algorithm to learn under an arbitrary sequence h\u2217 of target concepts; the difficulty of the task is then simply reflected in the resulting bounds on the error rates: that is, faster-changing sequences of target functions result in larger bounds on the error rates, but do not require a change in the algorithm itself."}, {"heading": "4.1 Adapting to a Changing Drift Rate", "text": "Recall that the method yielding (1) (based on the work of [HL94]) required access to the sequence \u2206 of changes to achieve the stated guarantee on the expected number of mistakes. That method is based on choosing a classifier to predict Y\u0302t by minimizing the number of mistakes among the previous mt samples, where mt is a value chosen based on the \u2206 sequence. Thus, the key to modifying this algorithm to make it adaptive to the \u2206 sequence is to determine a suitable choice of mt without reference to the \u2206 sequence. The strategy we adopt here is to use the data to determine an appropriate value m\u0302t to use. Roughly (ignoring logarithmic factors for now), the insight that enables us to achieve this feat is that, for the mt used in the above strategy, one can show that \u2211t\u22121\ni=t\u2212mt 1[h \u2217 t (Xi) 6= Yi] is roughly O\u0303(d), and that making the prediction Y\u0302t with any h \u2208 C with roughly O\u0303(d) mistakes on these samples will suffice to obtain the stated bound on the error rate (up to logarithmic factors). Thus, if we replace mt with the largest value m for which minh\u2208C \u2211t\u22121\ni=t\u2212m 1[h(Xi) 6= Yi] is roughly O\u0303(d), then the above observation implies m \u2265 mt. This then implies that, for h\u0302 = argminh\u2208C \u2211t\u22121 i=t\u2212m 1[h(Xi) 6= Yi], we have that \u2211t\u22121 i=t\u2212mt 1[h\u0302(Xi) 6= Yi] is also roughly O\u0303(d), so that the stated bound on the error rate will be achieved\n(aside from logarithmic factors) by choosing h\u0302t as this classifier h\u0302. There are a few technical modifications to this argument needed to get the logarithmic factors to work out properly, and for this reason the actual algorithm and proof below are somewhat more involved. Specifically, consider the following algorithm (the value of the universal constant K \u2265 1 will be specified below).\n0. For T = 1, 2, . . .\n1. Let m\u0302T =max\n{\nm\u2208{1, . . . , T\u22121} : min h\u2208C max m\u2032\u2264m\n\u2211T\u22121 t=T\u2212m\u2032\n1[h(Xt) 6=Yt] dLog(m\u2032/d)+Log(1/\u03b4) < K\n}\n2. Let h\u0302T = argmin h\u2208C max m\u2032\u2264m\u0302T\n\u2211T\u22121 t=T\u2212m\u2032\n1[h(Xt) 6=Yt] dLog(m\u2032/d)+Log(1/\u03b4)\nNote that the classifiers h\u0302t chosen by this algorithm have no dependence on \u2206, or indeed anything other than the data {(Xi, Yi) : i < t}, and the concept space C.\nTheorem 1. Fix any \u03b4 \u2208 (0, 1), and let A be the above algorithm. For any sequence \u2206 in [0, 1], for any P and any choice of h\u2217 \u2208 S\u2206, for every T \u2208 N\\{1}, with probability at least 1\u2212 \u03b4,\nerT\n(\nh\u0302T\n)\n\u2264 O\n\n min 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\n .\nBefore presenting the proof of this result, we first state a crucial lemma, which follows immediately from a classic result of [Vap82,Vap98], combined with the fact (from [Vid03], Theorem 4.5) that the VC dimension of the collection of sets {{x : h(x) 6= g(x)} : h, g \u2208 C} is at most 10d.\nLemma 1. There exists a universal constant c \u2208 [1,\u221e) such that, for any class C of VC dimension d, \u2200m \u2208 N, \u2200\u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, every h, g \u2208 C have\n\u2223 \u2223 \u2223 \u2223 \u2223 P(x : h(x) 6= g(x))\u2212 1 m m \u2211\nt=1\n1[h(Xt) 6= g(Xt)] \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2264 c\n\u221a \u221a \u221a \u221a ( 1\nm\nm \u2211\nt=1\n1[h(Xt) 6= g(Xt)] ) dLog(m/d) + Log(1/\u03b4)\nm\n+ c dLog(m/d) + Log(1/\u03b4)\nm .\nWe are now ready for the proof of Theorem 1. For the constant K in the algorithm, we will choose K = 145c2, for c as in Lemma 1.\nProof (Proof of Theorem 1). Fix any T \u2208 N with T \u2265 2, and define\nm\u2217T = max\n{\nm \u2208 {1, . . . , T \u2212 1} : \u2200m\u2032 \u2264 m,\nT\u22121 \u2211\nt=T\u2212m\u2032 1[h\u2217T (Xt) 6= Yt] < K(dLog(m\u2032/d) + Log(1/\u03b4))\n}\n.\nNote that\nT\u22121 \u2211\nt=T\u2212m\u2217T\n1[h\u2217T (Xt) 6= Yt] \u2264 K(dLog(m\u2217T /d) + Log(1/\u03b4)), (2)\nand also note that (since h\u2217T \u2208 C) m\u0302T \u2265 m\u2217T , so that (by definition of m\u0302T and h\u0302T )\nT\u22121 \u2211\nt=T\u2212m\u2217T\n1[h\u0302T (Xt) 6= Yt] \u2264 K(dLog(m\u2217T /d) + Log(1/\u03b4))\nas well. Therefore,\nT\u22121 \u2211\nt=T\u2212m\u2217T\n1[h\u2217T (Xt) 6= h\u0302T (Xt)] \u2264 T\u22121 \u2211\nt=T\u2212m\u2217T\n1[h\u2217T (Xt) 6= Yt] + T\u22121 \u2211\nt=T\u2212m\u2217T\n1[Yt 6= h\u0302T (Xt)]\n\u2264 2K(dLog(m\u2217T /d) + Log(1/\u03b4)).\nThus, by Lemma 1, for each m \u2208 N, with probability at least 1 \u2212 \u03b4/(6m2), if m\u2217T = m, then\nP(x : h\u0302T (x) 6= h\u2217T (x)) \u2264 (2K + c \u221a 2K + c) dLog(m\u2217T /d) + Log(6(m \u2217 T ) 2/\u03b4)\nm\u2217T .\nFurthermore, since Log(6(m\u2217T ) 2) \u2264 \u221a 2KdLog(m\u2217T /d), this is at most\n2(K + c \u221a 2K) dLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T .\nBy a union bound (over values m \u2208 N), we have that with probability at least 1\u2212\u2211\u221em=1 \u03b4/(6m2) \u2265 1\u2212 \u03b4/3,\nP(x : h\u0302T (x) 6= h\u2217T (x)) \u2264 2(K + c \u221a 2K) dLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T .\nLet us denote\nm\u0303T = argmin m\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm .\nNote that, for any m\u2032 \u2208 {1, . . . , T \u2212 1} and \u03b4 \u2208 (0, 1), if m\u0303T \u2265 m\u2032, then\nmin m\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\u2265 min m\u2208{m\u2032,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j = 1\nm\u2032\nT\u22121 \u2211\ni=T\u2212m\u2032\nT \u2211\nj=i+1\n\u2206j ,\nwhile if m\u0303T \u2264 m\u2032, then\nmin m\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\u2265 min m\u2208{1,...,m\u2032}\ndLog(m/d) + Log(1/\u03b4)\nm =\ndLog(m\u2032/d) + Log(1/\u03b4)\nm\u2032 .\nEither way, we have that\nmin m\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\u2265 min\n\n\n\ndLog(m\u2032/d) + Log(1/\u03b4)\nm\u2032 , 1 m\u2032\nT\u22121 \u2211\ni=T\u2212m\u2032\nT \u2211\nj=i+1\n\u2206j\n\n\n\n. (3)\nFor any m \u2208 {1, . . . , T \u2212 1}, applying Bernstein\u2019s inequality (see [BLM13], equation 2.10) to the random variables 1[h\u2217T (Xi) 6= Yi]/d, i \u2208 {T\u2212m, . . . , T\u22121}, and again to the random variables \u22121[h\u2217T (Xi) 6= Yi]/d, i \u2208 {T \u2212m, . . . , T \u2212 1}, together with a union bound, we obtain that, for any \u03b4 \u2208 (0, 1), with probability\nat least 1\u2212 \u03b4/(3m2),\n1\nm\nT\u22121 \u2211\ni=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x))\n\u2212\n\u221a \u221a \u221a \u221a ( 1\nm\nT\u22121 \u2211\ni=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x))\n)\n2 ln(3m2/\u03b4)\nm\n< 1\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi]\n< 1\nm\nT\u22121 \u2211\ni=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x))\n+ max\n\n\n\n\u221a\n(\n1 m \u2211T\u22121 i=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x))\n)\n4 ln(3m2/\u03b4) m\n(4/3) ln(3m2/\u03b4) m\n. (4)\nThe left inequality implies that\n1\nm\nT\u22121 \u2211\ni=T\u2212m P(x :h\u2217T (x) 6= h\u2217i (x)) \u2264 max\n{\n2\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi],\n8 ln(3m2/\u03b4)\nm\n}\n.\nPlugging this into the right inequality in (4), we obtain that\n1\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi] <\n1\nm\nT\u22121 \u2211\ni=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x))\n+ max\n\n\n\n\u221a \u221a \u221a \u221a ( 1\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi]\n)\n8 ln(3m2/\u03b4)\nm ,\n\u221a 32 ln(3m2/\u03b4)\nm\n\n\n\n.\nBy a union bound, this holds simultaneously for all m \u2208 {1, . . . , T \u2212 1} with probability at least 1 \u2212\u2211T\u22121m=1 \u03b4/(3m2) > 1 \u2212 (2/3)\u03b4. Note that, on this event, we obtain\n1\nm\nT\u22121 \u2211\ni=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x)) >\n1\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi]\n\u2212max\n\n\n\n\u221a \u221a \u221a \u221a ( 1\nm\nT\u22121 \u2211\ni=T\u2212m 1[h\u2217T (Xi) 6= Yi]\n)\n8 ln(3m2/\u03b4)\nm ,\n\u221a 32 ln(3m2/\u03b4)\nm\n\n\n\n.\nIn particular, taking m = m\u2217T , and invoking maximality of m \u2217 T , if m \u2217 T < T \u2212 1, the right hand side is at least\n(K \u2212 6c \u221a K) dLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T .\nSince 1m \u2211T\u22121 i=T\u2212m \u2211T j=i+1 \u2206j \u2265 1m \u2211T\u22121 i=T\u2212m P(x : h\u2217T (x) 6= h\u2217i (x)), taking K = 145c2, we have that with probability at least 1\u2212 \u03b4, if m\u2217T < T \u2212 1, then\n10(K + c \u221a 2K) min\nm\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\u2265 10(K + c \u221a 2K)min\n\n\n\ndLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T ,\n1\nm\u2217T\nT\u22121 \u2211\ni=T\u2212m\u2217T\nT \u2211\nj=i+1\n\u2206j\n\n\n\n\u2265 10(K + c \u221a 2K) dLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T\n\u2265 P(x : h\u0302T (x) 6= h\u2217T (x)).\nFurthermore, if m\u2217T = T\u22121, then we trivially have (on the same 1\u2212\u03b4 probability event as above)\n10(K + c \u221a 2K) min\nm\u2208{1,...,T\u22121}\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4)\nm\n\u2265 10(K + c \u221a 2K) min\nm\u2208{1,...,T\u22121}\ndLog(m/d) + Log(1/\u03b4)\nm\n= 10(K + c \u221a 2K) dLog((T \u2212 1)/d) + Log(1/\u03b4) T \u2212 1 = 10(K + c \u221a 2K) dLog(m\u2217T /d) + Log(1/\u03b4)\nm\u2217T \u2265 P(x : h\u0302T (x) 6= h\u2217T (x)).\n\u2293\u2294"}, {"heading": "4.2 Conditions Guaranteeing a Sublinear Number of Mistakes", "text": "One immediate implication of Theorem 1 is that, if the sum of \u2206t values grows sublinearly, then there exists an algorithm achieving an expected number of mistakes growing sublinearly in the number of predictions. Formally, we have the following corollary.\nCorollary 1. If \u2211T t=1 \u2206t = o(T ), then there exists an algorithm A such that, for every P and every choice of h\u2217 \u2208 S\u2206,\nE\n[\nT \u2211\nt=1\n1\n[ Y\u0302t 6= Yt ]\n]\n= o(T ).\nProof. For every T \u2208 N with T \u2265 2, let\nm\u0303T = argmin 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4T )\nm ,\nand define \u03b4T = 1 m\u0303T . Then consider running the algorithm A from Theorem 1, except that in choosing m\u0302T and h\u0302T for each T , we use the above value \u03b4T in place of \u03b4. Then Theorem 1 implies that, for each T , with probability at least 1\u2212 \u03b4T ,\nerT (h\u0302T ) \u2264 O\n\n min 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4T )\nm\n\n .\nSince erT (h\u0302T ) \u2264 1, this implies that\nP\n( Y\u0302T 6= YT ) = E [ erT (h\u0302T ) ]\n\u2264 O\n\n min 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(1/\u03b4T )\nm\n\n+ \u03b4T\n= O\n\n min 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d) + Log(m)\nm\n\n ,\nand since x 7\u2192 xLog(m/x) is nondecreasing for x \u2265 1, Log(m) \u2264 dLog(m/d), so that this last expression is\nO\n\n min 1\u2264m\u2264T\u22121\n1\nm\nT\u22121 \u2211\ni=T\u2212m\nT \u2211\nj=i+1\n\u2206j + dLog(m/d)\nm\n\n .\nNow note that, for any t \u2208 N and m \u2208 {1, . . . , t\u2212 1},\n1\nm\nt\u22121 \u2211\ns=t\u2212m\nt \u2211\nr=s+1\n\u2206r \u2264 1\nm\nt\u22121 \u2211\ns=t\u2212m\nt \u2211\nr=t\u2212m+1 \u2206r =\nt \u2211\nr=t\u2212m+1 \u2206s. (5)\nLet \u03b2t(m) = max { \u2211t r=t\u2212m+1\u2206r, dLog(m/d) m } , and note that \u2211t\nr=t\u2212m+1\u2206r + dLog(m/d)\nm \u2264 2\u03b2t(m). Thus, combining the above with (5), linearity of expectations, and the fact that the probability of a mistake on a given round is at most 1, we obtain\nE\n[\nT \u2211\nt=1\n1\n[ Y\u0302t 6= Yt ]\n]\n= O\n(\nT \u2211\nt=1\nmin m\u2208{1,...,t\u22121}\n\u03b2t(m) \u2227 1 ) .\nFixing any M \u2208 N, we have that for any T > M , T \u2211\nt=1\nmin m\u2208{1,...,t\u22121}\n\u03b2t(m) \u2227 1 \u2264 M + T \u2211\nt=M+1\n\u03b2t(M) \u2227 1\n\u2264 M + T \u2211\nt=M+1\n1\n[\ndLog(M/d)\nM \u2265\nt \u2211\nr=t\u2212M+1 \u2206r\n]\ndLog(M/d)\nM\n+ T \u2211\nt=M+1\n1\n[\nt \u2211\nr=t\u2212M+1 \u2206r >\ndLog(M/d)\nM\n]\n\u2264 M + dLog(M/d) M T +\nT \u2211\nt=M+1\nM\ndLog(M/d)\nt \u2211\nr=t\u2212M+1 \u2206r\n= dLog(M/d)\nM T + gM (T ),\nwhere gM is a function satisfying gM (T ) = o(T ) (holding M fixed). Since this is true of any M \u2208 N, we have that\nlim T\u2192\u221e\n1\nT\nT \u2211\nt=1\nmin m\u2208{1,...,t\u22121} \u03b2t(m) \u2227 1 \u2264 lim M\u2192\u221e lim T\u2192\u221e\ndLog(M/d)\nM +\ngM (T )\nT\n= lim M\u2192\u221e\ndLog(M/d)\nM = 0,\nso that E [\n\u2211T t=1 1\n[ Y\u0302t 6= Yt ]] = o(T ), as claimed. \u2293\u2294\nFor many concept spaces of interest, the condition \u2211T\nt=1 \u2206t = o(T ) in Corollary 1 is also a necessary condition for any algorithm to guarantee a sublinear number of mistakes. For simplicity, we will establish this for the class of homogeneous linear separators on R2, with P the uniform distribution on the unit circle, in the following theorem. This can easily be extended to many other spaces, including higher-dimensional linear separators or axis-aligned rectangles in Rk, by embedding an analogous setup into those spaces.\nTheorem 2. If X = {x \u2208 R2 : \u2016x\u2016 = 1}, P is Uniform(X ), and C = {x 7\u2192 21[w \u00b7 x \u2265 0] \u2212 1 : w \u2208 R2, \u2016w\u2016 = 1} is the class of homogeneous linear separators, then for any sequence \u2206 in [0, 1], there exists an algorithm A such that E [\n\u2211T t=1 1\n[ Y\u0302t 6= Yt ]]\n= o(T ) for every choice of h\u2217 \u2208 S\u2206 if and only if \u2211T\nt=1 \u2206t = o(T ).\nProof. The \u201cif\u201d part follows immediately from Corollary 1. For the \u201conly if\u201d part, suppose \u2206 is such that\n\u2211T t=1 \u2206t 6= o(T ). It suffices to argue that for any\nalgorithm A, there exists a choice of h\u2217 \u2208 S\u2206 for which E [ \u2211T t=1 1 [ Y\u0302t 6= Yt ]]\n6= o(T ). Toward this end, fix any algorithm A. We proceed by the probabilistic\nmethod, constructing a random sequence h\u2217 \u2208 S\u2206. Let B1, B2, . . . be independent Bernoulli(1/2) random variables (also independent from the unlabeled data X1, X2, . . .). We define the sequence h\n\u2217 inductively. For simplicity, we will represent each classifier in polar coordinates, writing h\u03c6 (for \u03c6 \u2208 R) to denote the classifier that, for x = (x1, x2), classifies x as h\u03c6(x) = 21[x1 cos(\u03c6)+x2 sin(\u03c6) \u2265 0]\u2212 1; note that h\u03c6 = h\u03c6+2\u03c0 for every \u03c6 \u2208 R. As a base case, start by defining a function h\u22170 = h0, and letting \u03c60 = 0. Now for any t \u2208 N, supposing h\u2217t\u22121 is already defined to be h\u03c6t\u22121 , we define \u03c6t = \u03c6t\u22121 + min{\u2206t, 1/2}\u03c0Bt, and h\u2217t = h\u03c6t . Note that P(x : h\u2217t (x) 6= h\u2217t\u22121(x)) = min{\u2206t, 1/2} for every t \u2208 N, so that this inductively defines a (random) choice of h\u2217 \u2208 S\u2206.\nFor each t \u2208 N, let Yt = h\u2217t (Xt). Now fix any algorithm A, and consider the sequence Y\u0302t of predictions the algorithm makes for points Xt, when the target sequence h\u2217 is chosen as above. Then note that, for any t \u2208 N, since Y\u0302t and Bt are independent,\nP\n( Y\u0302t 6= Yt ) \u2265 E [ P ( Y\u0302t 6= Yt \u2223 \u2223 \u2223 Y\u0302t, \u03c6t\u22121 )]\n\u2265 E [ 1\n2 P ( h\u03c6t\u22121+min{\u2206t,1/2}\u03c0(Xt) 6= h\u03c6t\u22121\u2212min{\u2206t,1/2}\u03c0(Xt) \u2223 \u2223\u03c6t\u22121 )\n]\n.\nFurthermore, since min{\u2206t, 1/2}\u03c0 \u2264 \u03c0/2, the regions {x : h\u03c6t\u22121+min{\u2206t,1/2}\u03c0(x) 6= h\u03c6t\u22121(x)} and {x : h\u03c6t\u22121\u2212min{\u2206t,1/2}\u03c0(x) 6= h\u03c6t\u22121(x)} have zero-probability overlap (indeed, are disjoint if \u2206t < 1/2), the above equals min{\u2206t, 1/2}.\nBy Fatou\u2019s lemma, linearity of expectations, and the law of total expectation, we have that\nE\n[\nlim sup T\u2192\u221e\n1 T E\n[\nT \u2211\nt=1\n1[Y\u0302t 6= Yt] \u2223 \u2223 \u2223 \u2223\n\u2223\nh\u2217 ]]\n\u2265 lim sup T\u2192\u221e\n1\nT\nT \u2211\nt=1\nP\n( Y\u0302t 6= Yt )\n\u2265 lim sup T\u2192\u221e\n1\nT\nT \u2211\nt=1\nmin{\u2206t, 1/2}.\nSince \u2211T t=1 \u2206t 6= o(T ), the rightmost expression is strictly greater than zero. Thus, it must be that, with probility strictly greater than 0,\nlim sup T\u2192\u221e\n1 T E\n[\nT \u2211\nt=1\n1[Y\u0302t 6= Yt] \u2223 \u2223 \u2223 \u2223\n\u2223\nh\u2217 ] > 0.\nIn particular, this implies that there exists a (nonrandom) choice of the sequence h\u2217 \u2208 S\u2206 for which E [ \u2211T t=1 1 [ Y\u0302t 6= Yt ]]\n6= o(T ). Since this holds for any choice of the algorithm A, this completes the proof. \u2293\u2294"}, {"heading": "5 Polynomial-Time Algorithms for Linear Separators", "text": "In this section, we suppose \u2206t = \u2206 for every t \u2208 N, for a fixed constant \u2206 > 0, and we consider the special case of learning homogeneous linear separators in\nRk under a uniform distribution on the origin-centered unit sphere. In this case, the analysis of [HL94] mentioned in Section 3.3 implies that it is possible to achieve a bound on the error rate that is O\u0303(d \u221a \u2206), using an algorithm that runs in time poly(d, 1/\u2206, log(1/\u03b4)) (and independent of t) for each prediction. This also implies that it is possible to achieve expected number of mistakes among T predictions that is O\u0303(d \u221a \u2206)\u00d7T . [CMEDV10]6 have since proven that a variant of the Perceptron algorithm is capable of achieving an expected number of mistakes O\u0303((d\u2206)1/4)\u00d7 T .\nBelow, we improve on this result by showing that there exists an efficient algorithm that achieves a bound on the error rate that is O\u0303( \u221a d\u2206), as was possible with the inefficient algorithm of [HL94,Lon99] mentioned in Section 3.1. This leads to a bound on the expected number of mistakes that is O\u0303( \u221a d\u2206) \u00d7 T . Furthermore, our approach also allows us to present the method as an active learning algorithm, and to bound the expected number of queries, as a function of the number of samples T , by O\u0303( \u221a d\u2206) \u00d7 T . The technique is based on a modification of the algorithm of [HL94], replacing an empirical risk minimization step with (a modification of) the computationally-efficient algorithm of [ABL13].\nFormally, define the class of homogeneous linear separators as the set of classifiers hw : R\nd \u2192 {\u22121,+1}, for w \u2208 Rd with \u2016w\u2016 = 1, such that hw(x) = sign(w \u00b7 x) for every x \u2208 Rd."}, {"heading": "5.1 An Improved Guarantee for a Polynomial-Time Algorithm", "text": "We have the following result.\nTheorem 3. When C is the space of homogeneous linear separators (with d \u2265 4) and P is the uniform distribution on the surface of the origin-centered unit sphere in Rd, for any fixed \u2206 > 0, for any \u03b4 \u2208 (0, 1/e), there is an algorithm that runs in time poly(d, 1/\u2206, log(1/\u03b4)) for each time t, such that for any h\u2217 \u2208 S\u2206, for every sufficiently large t \u2208 N, with probability at least 1\u2212 \u03b4,\nert(h\u0302t) = O\n( \u221a\n\u2206d log\n(\n1\n\u03b4\n)\n)\n.\nAlso, running this algorithm with \u03b4 = \u221a \u2206d\u22271/e, the expected number of mistakes among the first T instances is O (\u221a \u2206d log (\n1 \u2206d\n) T ) . Furthermore, the algorithm\ncan be run as an active learning algorithm, in which case, for this choice of \u03b4, the expected number of labels requested by the algorithm among the first T instances is O (\u221a \u2206d log3/2 (\n1 \u2206d\n) T ) .\n6 This work in fact studies a much broader model of drift, which in fact allows the distribution P to vary with time as well. However, this O\u0303((d\u2206)1/4) \u00d7 T result can be obtained from their more-general theorem by calculating the various parameters for this particular setting.\nWe first state the algorithm used to obtain this result. It is primarily based on a margin-based learning strategy of [ABL13], combined with an initialization step based on a modified Perceptron rule from [DKM09,CMEDV10]. For \u03c4 > 0 and x \u2208 R, define \u2113\u03c4 (x) = max { 0, 1\u2212 x\u03c4 }\n. Consider the following algorithm and subroutine; parameters \u03b4k, mk, \u03c4k, rk, bk, \u03b1, and \u03ba will all be specified in the context of the proof; we suppose M = \u2211\u2308log2(1/\u03b1)\u2309\nk=0 mk.\nAlgorithm: DriftingHalfspaces 0. Let h\u03030 be an arbitrary classifier in C 1. For i = 1, 2, . . . 2. h\u0303i \u2190 ABL(M(i\u2212 1), h\u0303i\u22121)\nSubroutine: ModPerceptron(t, h\u0303) 0. Let wt be any element of R\nd with \u2016wt\u2016 = 1 1. For m = t+ 1, t+ 2, . . . , t+m0 2. Choose h\u0302m = h\u0303 (i.e., predict Y\u0302m = h\u0303(Xm) as the prediction for Ym) 3. Request the label Ym 4. If hwm\u22121(Xm) 6= Ym 5. wm \u2190 wm\u22121 \u2212 2(wm\u22121 \u00b7Xm)Xm 6. Else wm \u2190 wm\u22121 7. Return wt+m0\nSubroutine: ABL(t, h\u0303) 0. Let w0 be the return value of ModPerceptron(t, h\u0303) 1. For k = 1, 2, . . . , \u2308log2(1/\u03b1)\u2309 2. Wk \u2190 {} 3. For s = t+\n\u2211k\u22121 j=0 mj + 1, . . . , t+ \u2211k j=0 mj\n4. Choose h\u0302s = h\u0303 (i.e., predict Y\u0302s = h\u0303(Xs) as the prediction for Ys) 5. If |wk\u22121 \u00b7Xs| \u2264 bk\u22121, Request label Ys and let Wk \u2190 Wk \u222a{(Xs, Ys)} 6. Find vk \u2208 Rd with \u2016vk \u2212 wk\u22121\u2016 \u2264 rk, 0 < \u2016vk\u2016 \u2264 1, and\n\u2211\n(x,y)\u2208Wk \u2113\u03c4k(y(vk \u00b7 x)) \u2264 inf v:\u2016v\u2212wk\u22121\u2016\u2264rk\n\u2211\n(x,y)\u2208Wk \u2113\u03c4k(y(v \u00b7 x)) + \u03ba|Wk|\n7. Let wk = 1\n\u2016vk\u2016vk 8. Return hw\u2308log2(1/\u03b1)\u2309\u22121\nBefore stating the proof, we have a few additional lemmas that will be needed. The following result for ModPerceptron was proven by [CMEDV10].\nLemma 2. Suppose \u2206 < 1512 . Consider the values wm obtained during the execution of ModPerceptron(t, h\u0303). \u2200m \u2208 {t + 1, . . . , t + m0}, P(x : hwm(x) 6= h\u2217m(x)) \u2264 P(x : hwm\u22121(x) 6= h\u2217m(x)). Furthermore, letting c1 = \u03c0 2\nd\u00b7400\u00b7215 , if P(x : hwm\u22121(x) 6= h\u2217m(x)) \u2265 1/32, then with probability at least 1/64, P(x : hwm(x) 6= h\u2217m(x)) \u2264 (1\u2212 c1)P(x : hwm\u22121(x) 6= h\u2217m(x)).\nThis implies the following.\nLemma 3. Suppose \u2206 \u2264 \u03c02400\u00b7227(d+ln(4/\u03b4)) . For m0 = max{\u2308128(1/c1) ln(32)\u2309, \u2308512 ln(4\u03b4 )\u2309}, with probability at least 1 \u2212 \u03b4/4, ModPerceptron(t, h\u0303) returns a vector w with P(x : hw(x) 6= h\u2217t+m0+1(x)) \u2264 1/16. Proof. By Lemma 2 and a union bound, in general we have\nP(x : hwm(x) 6= h\u2217m+1(x)) \u2264 P(x : hwm\u22121(x) 6= h\u2217m(x)) +\u2206. (6)\nFurthermore, if P(x : hwm\u22121(x) 6= h\u2217m(x)) \u2265 1/32, then wth probability at least 1/64,\nP(x : hwm(x) 6= h\u2217m+1(x)) \u2264 (1\u2212 c1)P(x : hwm\u22121(x) 6= h\u2217m(x)) +\u2206. (7)\nIn particular, this implies that the number N of values m \u2208 {t+ 1, . . . , t+m0} with either P(x : hwm\u22121(x) 6= h\u2217m(x)) < 1/32 or P(x : hwm(x) 6= h\u2217m+1(x)) \u2264 (1\u2212 c1)P(x : hwm\u22121(x) 6= h\u2217m(x)) +\u2206 is lower-bounded by a Binomial(m, 1/64) random variable. Thus, a Chernoff bound implies that with probability at least 1\u2212 exp{\u2212m0/512} \u2265 1\u2212 \u03b4/4, we have N \u2265 m0/128. Suppose this happens.\nSince \u2206m0 \u2264 1/32, if any m \u2208 {t + 1, . . . , t + m0} has P(x : hwm\u22121(x) 6= h\u2217m(x)) < 1/32, then inductively applying (6) implies that P(x : hwt+m0 (x) 6= h\u2217t+m0+1(x)) \u2264 1/32+\u2206m0 \u2264 1/16. On the other hand, if all m \u2208 {t+1, . . . , t+ m0} have P(x : hwm\u22121(x) 6= h\u2217m(x)) \u2265 1/32, then in particular we have N values ofm \u2208 {t+1, . . . , t+m0} satisfying (7). Combining this fact with (6) inductively, we have that\nP(x : hwt+m0 (x) 6= h \u2217 t+m0+1(x)) \u2264 (1 \u2212 c1) NP(x : hwt(x) 6= h\u2217t+1(x)) +\u2206m0\n\u2264 (1\u2212 c1)(1/c1) ln(32)P(x : hwt(x) 6= h\u2217t+1(x)) +\u2206m0 \u2264 1\n32 +\u2206m0 \u2264\n1\n16 .\n\u2293\u2294 Next, we consider the execution of ABL(t, h\u0303), and let the sets Wk be as in that execution. We will denote by w\u2217 the weight vector with \u2016w\u2217\u2016 = 1 such that h\u2217t+m0+1 = hw\u2217 . Also denote by M1 = M \u2212m0.\nThe proof relies on a few results proven in the work of [ABL13], which we summarize in the following lemmas. Although the results were proven in a slightly different setting in that work (namely, agnostic learning under a fixed joint distribution), one can easily verify that their proofs remain valid in our present context as well.\nLemma 4. [ABL13] Fix any k \u2208 {1, . . . , \u2308log2(1/\u03b1)\u2309}. For a universal constant c7 > 0, suppose bk\u22121 = c721\u2212k/ \u221a d, and let zk = \u221a\nr2k/(d\u2212 1) + b2k\u22121. For a universal constant c1 > 0, if \u2016w\u2217 \u2212 wk\u22121\u2016 \u2264 rk, \u2223\n\u2223 \u2223 \u2223 \u2223 \u2223 E\n\n\n\u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(|w\u2217 \u00b7 x|) \u2223 \u2223 \u2223wk\u22121, |Wk|\n\n\u2212 E\n\n\n\u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(y(w \u2217 \u00b7 x))\n\u2223 \u2223 \u2223wk\u22121, |Wk|\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 c1|Wk| \u221a 2k\u2206M1 zk \u03c4k .\nLemma 5. [BL13] For any c > 0, there is a constant c\u2032 > 0 depending only on c (i.e., not depending on d) such that, for any u, v \u2208 Rd with \u2016u\u2016 = \u2016v\u2016 = 1, letting \u03c3 = P(x : hu(x) 6= hv(x)), if \u03c3 < 1/2, then\nP ( x : hu(x) 6= hv(x) and |v \u00b7 x| \u2265 c\u2032 \u03c3\u221a d ) \u2264 c\u03c3.\nThe following is a well-known lemma concerning concentration around the equator for the uniform distribution (see e.g., [DKM09,BBZ07,ABL13]); for instance, it easily follows from the formulas for the area in a spherical cap derived by [Li11].\nLemma 6. For any constant C > 0, there are constants c2, c3 > 0 depending only on C (i.e., independent of d) such that, for any w \u2208 Rd with \u2016w\u2016 = 1, \u2200\u03b3 \u2208 [0, C/ \u221a d],\nc2\u03b3 \u221a d \u2264 P (x : |w \u00b7 x| \u2264 \u03b3) \u2264 c3\u03b3 \u221a d.\nBased on this lemma, [ABL13] prove the following.\nLemma 7. [ABL13] For X \u223c P, for any w \u2208 Rd with \u2016w\u2016 = 1, for any C > 0 and \u03c4, b \u2208 [0, C/ \u221a d], for c2, c3 as in Lemma 6,\nE\n[ \u2113\u03c4 (|w\u2217 \u00b7X |) \u2223 \u2223 \u2223|w \u00b7X | \u2264 b ] \u2264 c3\u03c4 c2b .\nThe following is a slightly stronger version of a result of [ABL13] (specifically, the size of mk, and consequently the bound on |Wk|, are both improved by a factor of d compared to the original result).\nLemma 8. Fix any \u03b4 \u2208 (0, 1/e). For universal constants c4, c5, c6, c7, c8, c9, c10 \u2208 (0,\u221e), for an appropriate choice of \u03ba \u2208 (0, 1) (a universal constant), if \u03b1 = c9 \u221a \u2206d log ( 1 \u03ba\u03b4 ) , for every k \u2208 {1, . . . , \u2308log2(1/\u03b1)\u2309}, if bk\u22121 = c721\u2212k/ \u221a d, \u03c4k = c82 \u2212k/ \u221a d, rk = c102 \u2212k, \u03b4k = \u03b4/(\u2308log2(4/\u03b1)\u2309\u2212k)2, and mk = \u2308 c5 2k \u03ba2 d log ( 1 \u03ba\u03b4k )\u2309 , and if P(x : hwk\u22121(x) 6= hw\u2217(x)) \u2264 2\u2212k\u22123, then with probability at least 1 \u2212 (4/3)\u03b4k, |Wk| \u2264 c6 1\u03ba2 d log ( 1 \u03ba\u03b4k ) and P(x : hwk(x) 6= hw\u2217(x)) \u2264 2\u2212k\u22124.\nProof. By Lemma 6, and a Chernoff and union bound, for an appropriately large choice of c5 and any c7 > 0, letting c2, c3 be as in Lemma 6 (with C = c7\u2228(c8/2)), with probability at least 1\u2212 \u03b4k/3,\nc2c72 \u2212kmk \u2264 |Wk| \u2264 4c3c72\u2212kmk. (8)\nThe claimed upper bound on |Wk| follows from this second inequality. Next note that, if P(x : hwk\u22121(x) 6= hw\u2217(x)) \u2264 2\u2212k\u22123, then\nmax{\u2113\u03c4k(y(w\u2217 \u00b7 x)) : x \u2208 Rd, |wk\u22121 \u00b7 x| \u2264 bk\u22121, y \u2208 {\u22121,+1}} \u2264 c11 \u221a d\nfor some universal constant c11 > 0. Furthermore, since P(x : hwk\u22121(x) 6= hw\u2217(x)) \u2264 2\u2212k\u22123, we know that the angle between wk\u22121 and w\u2217 is at most 2\u2212k\u22123\u03c0, so that\n\u2016wk\u22121 \u2212 w\u2217\u2016 = \u221a 2\u2212 2wk\u22121 \u00b7 w\u2217 \u2264 \u221a 2\u2212 2 cos(2\u2212k\u22123\u03c0)\n\u2264 \u221a 2\u2212 2 cos2(2\u2212k\u22123\u03c0) = \u221a 2 sin(2\u2212k\u22123\u03c0) \u2264 2\u2212k\u22123\u03c0 \u221a 2.\nFor c10 = \u03c0 \u221a 22\u22123, this is rk. By Hoeffding\u2019s inequality (under the conditional distribution given |Wk|), the law of total probability, Lemma 4, and linearity of conditional expectations, with probability at least 1\u2212 \u03b4k/3, for X \u223c P ,\n\u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(y(w \u2217 \u00b7 x)) \u2264 |Wk|E\n[ \u2113\u03c4k(|w\u2217 \u00b7X |) \u2223 \u2223 \u2223wk\u22121, |wk\u22121 \u00b7X | \u2264 bk\u22121 ]\n+ c1|Wk| \u221a 2k\u2206M1 zk \u03c4k + \u221a |Wk|(1/2)c211d ln(3/\u03b4k). (9)\nWe bound each term on the right hand side separately. By Lemma 7, the first term is at most |Wk| c3\u03c4kc2bk\u22121 = |Wk| c3c8 2c2c7 . Next,\nzk \u03c4k =\n\u221a\nc2102 \u22122k/(d\u2212 1) + 4c272\u22122k/d\nc82\u2212k/ \u221a d\n\u2264 \u221a 2c210 + 4c 2 7\nc8 ,\nwhile 2k \u2264 2/\u03b1 so that the second term is at most\n\u221a 2c1 \u221a 2c210 + 4c 2 7\nc8 |Wk|\n\u221a\n\u2206m\n\u03b1 .\nNoting that\nM1 =\n\u2308log2(1/\u03b1)\u2309 \u2211\nk\u2032=1\nmk\u2032 \u2264 32c5 \u03ba2 1 \u03b1 d log\n(\n1\n\u03ba\u03b4\n)\n, (10)\nwe find that the second term on the right hand side of (9) is at most\n\u221a\nc5 c9 8c1 \u03ba\n\u221a\n2c210 + 4c 2 7\nc8 |Wk|\n\u221a\n\u2206d log ( 1 \u03ba\u03b4 )\n\u03b12 =\n8c1 \u221a c5\n\u03ba\n\u221a\n2c210 + 4c 2 7\nc8c9 |Wk|.\nFinally, since d ln(3/\u03b4k) \u2264 2d ln(1/\u03b4k) \u2264 2\u03ba 2 c5 2\u2212kmk, and (8) implies 2\u2212kmk \u2264\n1 c2c7 |Wk|, the third term on the right hand side of (9) is at most\n|Wk| c11\u03ba\u221a c2c5c7 .\nAltogether, we have\n\u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(y(w \u2217 \u00b7 x)) \u2264 |Wk|\n(\nc3c8 2c2c7 + 8c1\n\u221a c5\n\u03ba\n\u221a\n2c210 + 4c 2 7\nc8c9 + c11\u03ba\u221a c2c5c7\n)\n.\nTaking c9 = 1/\u03ba 3 and c8 = \u03ba, this is at most\n\u03ba|Wk| ( c3 2c2c7 + 8c1 \u221a c5 \u221a 2c210 + 4c 2 7 + c11\u221a c2c5c7 ) .\nNext, note that because hwk(x) 6= y \u21d2 \u2113\u03c4k(y(vk \u00b7 x)) \u2265 1, and because (as proven above) \u2016w\u2217 \u2212 wk\u22121\u2016 \u2264 rk,\n|Wk|erWk(hwk) \u2264 \u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(y(vk \u00b7 x)) \u2264 \u2211\n(x,y)\u2208Wk\n\u2113\u03c4k(y(w \u2217 \u00b7 x)) + \u03ba|Wk|.\nCombined with the above, we have\n|Wk|erWk(hwk) \u2264 \u03ba|Wk| ( 1 + c3\n2c2c7 + 8c1\n\u221a c5 \u221a 2c210 + 4c 2 7 + c11\u221a c2c5c7\n)\n.\nLet c12 = 1 + c3\n2c2c7 + 8c1\n\u221a c5 \u221a 2c210 + 4c 2 7 + c11\u221a c2c5c7 . Furthermore,\n|Wk|erWk(hwk) = \u2211\n(x,y)\u2208Wk\n1[hwk(x) 6= y]\n\u2265 \u2211\n(x,y)\u2208Wk\n1[hwk(x) 6= hw\u2217(x)] \u2212 \u2211\n(x,y)\u2208Wk\n1[hw\u2217(x) 6= y].\nFor an appropriately large value of c5, by a Chernoff bound, with probability at least 1\u2212 \u03b4k/3,\nt+ \u2211k\nj=0 mj \u2211\ns=t+ \u2211k\u22121\nj=0 mj+1\n1[hw\u2217(Xs) 6= Ys] \u2264 2e\u2206M1mk + log2(3/\u03b4k).\nIn particular, this implies\n\u2211\n(x,y)\u2208Wk\n1[hw\u2217(x) 6= y] \u2264 2e\u2206M1mk + log2(3/\u03b4k),\nso that \u2211\n(x,y)\u2208Wk\n1[hwk(x) 6= hw\u2217(x)] \u2264 |Wk|erWk(hwk) + 2e\u2206M1mk + log2(3/\u03b4k).\nNoting that (10) and (8) imply\n\u2206M1mk \u2264 \u2206 32c5 \u03ba2\nd log ( 1 \u03ba\u03b4 )\nc9\n\u221a\n\u2206d log ( 1 \u03ba\u03b4 )\n2k\nc2c7 |Wk| \u2264 32c5 c2c7c9\u03ba2\n\u221a\n\u2206d log\n(\n1\n\u03ba\u03b4\n)\n2k|Wk|\n= 32c5\nc2c7c29\u03ba 2 \u03b12k|Wk| =\n32c5\u03ba 4\nc2c7 \u03b12k|Wk| \u2264\n32c5\u03ba 4\nc2c7 |Wk|,\nand (8) implies log2(3/\u03b4k) \u2264 2\u03ba 2 c2c5c7 |Wk|, altogether we have\n\u2211\n(x,y)\u2208Wk\n1[hwk(x) 6= hw\u2217(x)] \u2264 |Wk|erWk(hwk) + 64ec5\u03ba\n4\nc2c7 |Wk|+\n2\u03ba2\nc2c5c7 |Wk|\n\u2264 \u03ba|Wk| ( c12 + 64ec5\u03ba 3\nc2c7 +\n2\u03ba\nc2c5c7\n)\n.\nLetting c13 = c12+ 64ec5 c2c7 + 2c2c5c7 , and noting \u03ba \u2264 1, we have \u2211 (x,y)\u2208Wk 1[hwk(x) 6= hw\u2217(x)] \u2264 c13\u03ba|Wk|.\nLemma 1 (applied under the conditional distribution given |Wk|) and the law of total probability imply that with probability at least 1\u2212 \u03b4k/3,\n|Wk|P ( x : hwk(x) 6= hw\u2217(x) \u2223 \u2223 \u2223|wk\u22121 \u00b7 x| \u2264 bk\u22121 )\n\u2264 \u2211\n(x,y)\u2208Wk\n1[hwk(x) 6= hw\u2217(x)] + c14 \u221a |Wk|(d log(|Wk|/d) + log(1/\u03b4k)),\nfor a universal constant c14 > 0. Combined with the above, and the fact that (8) implies log(1/\u03b4k) \u2264 \u03ba 2\nc2c5c7 |Wk| and\nd log(|Wk|/d) \u2264 d log\n\n\n8c3c5c7 log (\n1 \u03ba\u03b4k\n)\n\u03ba2\n\n\n\u2264 d log ( 8c3c5c7 \u03ba3\u03b4k ) \u2264 3 log(8max{c3, 1}c5)c5d log ( 1 \u03ba\u03b4k ) \u2264 3 log(8max{c3, 1})\u03ba22\u2212kmk \u2264 3 log(8max{c3, 1})\nc2c7 \u03ba2|Wk|,\nwe have\n|Wk|P ( x : hwk(x) 6= hw\u2217(x) \u2223 \u2223 \u2223|wk\u22121 \u00b7 x| \u2264 bk\u22121 )\n\u2264 c13\u03ba|Wk|+ c14\n\u221a\n|Wk| ( 3 log(8max{c3, 1}) c2c7 \u03ba2|Wk|+ \u03ba2 c2c5c7 |Wk| )\n= \u03ba|Wk|\n\nc13 + c14\n\u221a\n3 log(8max{c3, 1}) c2c7 + 1 c2c5c7\n\n .\nThus, letting c15 =\n(\nc13 + c14\n\u221a\n3 log(8max{c3,1}) c2c7 + 1c2c5c7\n)\n, we have\nP ( x : hwk(x) 6= hw\u2217(x) \u2223 \u2223 \u2223|wk\u22121 \u00b7 x| \u2264 bk\u22121 ) \u2264 c15\u03ba. (11)\nNext, note that \u2016vk \u2212 wk\u22121\u20162 = \u2016vk\u20162 + 1 \u2212 2\u2016vk\u2016 cos(\u03c0P(x : hwk(x) 6= hwk\u22121(x))). Thus, one implication of the fact that \u2016vk\u2212wk\u22121\u2016 \u2264 rk is that \u2016vk\u20162 +\n1\u2212r2k 2\u2016vk\u2016 \u2264 cos(\u03c0P(x : hwk(x) 6= hwk\u22121(x))); since the left hand side is positive, we have P(x : hwk(x) 6= hwk\u22121(x)) < 1/2. Additionally, by differentiating, one can easily verify that for \u03c6 \u2208 [0, \u03c0], x 7\u2192 \u221a\nx2 + 1\u2212 2x cos(\u03c6) is minimized at x = cos(\u03c6), in which case \u221a\nx2 + 1\u2212 2x cos(\u03c6) = sin(\u03c6). Thus, \u2016vk \u2212 wk\u22121\u2016 \u2265 sin(\u03c0P(x : hwk(x) 6= hwk\u22121(x))). Since \u2016vk \u2212 wk\u22121\u2016 \u2264 rk, we have sin(\u03c0P(x : hwk(x) 6= hwk\u22121(x))) \u2264 rk. Since sin(\u03c0x) \u2265 x for all x \u2208 [0, 1/2], combining this with the fact (proven above) that P(x : hwk(x) 6= hwk\u22121(x)) < 1/2 implies P(x : hwk(x) 6= hwk\u22121(x)) \u2264 rk.\nIn particular, we have that both P(x : hwk(x) 6= hwk\u22121(x)) \u2264 rk and P(x : hw\u2217(x) 6= hwk\u22121(x)) \u2264 2\u2212k\u22123 \u2264 rk. Now Lemma 5 implies that, for any universal constant c > 0, there exists a corresponding universal constant c\u2032 > 0 such that\nP ( x : hwk(x) 6= hwk\u22121(x) and |wk\u22121 \u00b7 x| \u2265 c\u2032 rk\u221a d ) \u2264 crk\nand\nP ( x : hw\u2217(x) 6= hwk\u22121(x) and |wk\u22121 \u00b7 x| \u2265 c\u2032 rk\u221a d ) \u2264 crk,\nso that (by a union bound)\nP ( x : hwk(x) 6= hw\u2217(x) and |wk\u22121 \u00b7 x| \u2265 c\u2032 rk\u221a d )\n\u2264 P ( x : hwk(x) 6= hwk\u22121(x) and |wk\u22121 \u00b7 x| \u2265 c\u2032 rk\u221a d ) + P (\nx : hw\u2217(x) 6= hwk\u22121(x) and |wk\u22121 \u00b7 x| \u2265 c\u2032 rk\u221a d\n)\n\u2264 2crk.\nIn particular, letting c7 = c \u2032c10/2, we have c\u2032 rk\u221a d = bk\u22121. Combining this with (11), Lemma 6, and a union bound, we have that\nP (x : hwk(x) 6= hw\u2217(x)) \u2264 P (x : hwk(x) 6= hw\u2217(x) and |wk\u22121 \u00b7 x| \u2265 bk\u22121) + P (x : hwk(x) 6= hw\u2217(x) and |wk\u22121 \u00b7 x| \u2264 bk\u22121)\n\u2264 2crk + P ( x : hwk(x) 6= hw\u2217(x) \u2223 \u2223 \u2223 |wk\u22121 \u00b7 x| \u2264 bk\u22121 ) P (x : |wk\u22121 \u00b7 x| \u2264 bk\u22121) \u2264 2crk + c15\u03bac3bk\u22121 \u221a d = ( 25cc10 + c15\u03bac3c72 5 ) 2\u2212k\u22124.\nTaking c = 126c10 and \u03ba = 1 26c3c7c15 , we have P(x : hwk(x) 6= hw\u2217(x)) \u2264 2\u2212k\u22124, as required. By a union bound, this occurs with probability at least 1\u2212 (4/3)\u03b4k. \u2293\u2294\nProof (Proof of Theorem 3). We begin with the bound on the error rate. If \u2206 > \u03c02\n400\u00b7227(d+ln(4/\u03b4)) , the result trivially holds, since then 1 \u2264 400\u00b72 27 \u03c02 \u221a \u2206(d+ ln(4/\u03b4)). Otherwise, suppose \u2206 \u2264 \u03c02400\u00b7227(d+ln(4/\u03b4)) .\nFix any i \u2208 N. Lemma 3 implies that, with probability at least 1 \u2212 \u03b4/4, the w0 returned in Step 0 of ABL(M(i \u2212 1), h\u0303i\u22121) satisfies P(x : hw0(x) 6= h\u2217M(i\u22121)+m0+1(x)) \u2264 1/16. Taking this as a base case, Lemma 8 then inductively implies that, with probability at least\n1\u2212 \u03b4 4 \u2212\n\u2308log2(1/\u03b1)\u2309 \u2211\nk=1\n(4/3) \u03b4 2(\u2308log2(4/\u03b1)\u2309 \u2212 k)2 \u2265 1\u2212 \u03b4 2\n(\n1 + (4/3)\n\u221e \u2211\n\u2113=2\n1\n\u21132\n)\n\u2265 1\u2212\u03b4,\nevery k \u2208 {0, 1, . . . , \u2308log2(1/\u03b1)\u2309} has\nP(x : hwk(x) 6= h\u2217M(i\u22121)+m0+1(x)) \u2264 2 \u2212k\u22124, (12)\nand furthermore the number of labels requested during ABL(M(i \u2212 1), h\u0303i\u22121) total to at most (for appropriate universal constants c\u03021, c\u03022)\nm0 +\n\u2308log2(1/\u03b1)\u2309 \u2211\nk=1\n|Wk| \u2264 c\u03021\n\nd+ ln\n(\n1\n\u03b4\n)\n+\n\u2308log2(1/\u03b1)\u2309 \u2211\nk=1\nd log\n(\n(\u2308log2(4/\u03b1)\u2309 \u2212 k)2 \u03b4\n)\n\n\n\u2264 c\u03022d log ( 1\n\u2206d\n)\nlog\n(\n1\n\u03b4\n)\n.\nIn particular, by a union bound, (12) implies that for every k \u2208 {1, . . . , \u2308log2(1/\u03b1)\u2309}, every\nm \u2208\n\n\n\nM(i\u2212 1) + k\u22121 \u2211\nj=0\nmj + 1, . . . ,M(i\u2212 1) + k \u2211\nj=0\nmj\n\n\n\nhas\nP(x : hwk\u22121(x) 6= h\u2217m(x)) \u2264 P(x : hwk\u22121(x) 6= h\u2217M(i\u22121)+m0+1(x)) + P(x : h \u2217 M(i\u22121)+m0+1(x) 6= h \u2217 m(x))\n\u2264 2\u2212k\u22123 +\u2206M.\nThus, noting that\nM =\n\u2308log2(1/\u03b1)\u2309 \u2211\nk=0\nmk = \u0398\n\nd+ log\n(\n1\n\u03b4\n)\n+\n\u2308log2(1/\u03b1)\u2309 \u2211\nk=1\n2kd log (\u2308log2(1/\u03b1)\u2309 \u2212 k \u03b4 )\n\n\n= \u0398\n(\n1 \u03b1 d log\n(\n1\n\u03b4\n))\n= \u0398\n( \u221a\nd \u2206 log\n(\n1\n\u03b4\n)\n)\n,\nwith probability at least 1\u2212 \u03b4,\nP(x : hw\u2308log2(1/\u03b1)\u2309\u22121(x) 6= h \u2217 Mi(x)) \u2264 O (\u03b1+\u2206M) = O\n( \u221a\n\u2206d log\n(\n1\n\u03b4\n)\n)\n.\nIn particular, this implies that, with probability at least 1\u2212 \u03b4, every t \u2208 {Mi+ 1, . . . ,M(i+ 1)\u2212 1} has\nert(h\u0302t) \u2264 P(x : hw\u2308log2(1/\u03b1)\u2309\u22121(x) 6= h \u2217 Mi(x)) + P(x : h\u2217Mi(x) 6= h\u2217t (x))\n\u2264 O ( \u221a \u2206d log ( 1\n\u03b4\n)\n)\n+\u2206M = O\n( \u221a\n\u2206d log\n(\n1\n\u03b4\n)\n)\n,\nwhich completes the proof of the bound on the error rate. Setting \u03b4 = \u221a \u2206d, and noting that 1[Y\u0302t 6= Yt] \u2264 1, we have that for any t > M ,\nP\n( Y\u0302t 6= Yt ) = E [ ert(h\u0302t) ] \u2264 O ( \u221a \u2206d log ( 1\n\u03b4\n)\n)\n+ \u03b4 = O\n( \u221a\n\u2206d log\n(\n1\n\u2206d\n)\n)\n.\nThus, by linearity of the expectation,\nE\n[\nT \u2211\nt=1\n1\n[ Y\u0302t 6= Yt ]\n] \u2264 M +O ( \u221a \u2206d log ( 1\n\u2206d\n)\nT\n)\n= O\n( \u221a\n\u2206d log\n(\n1\n\u2206d\n)\nT\n)\n.\nFurthermore, as mentioned, with probability at least 1\u2212 \u03b4, the number of labels requested during the execution of ABL(M(i\u2212 1), h\u0303i\u22121) is at most\nO\n(\nd log\n(\n1\n\u2206d\n)\nlog\n(\n1\n\u03b4\n))\n.\nThus, since the number of labels requested during the execution of ABL(M(i\u2212 1), h\u0303i\u22121) cannot exceed M , letting \u03b4 = \u221a \u2206d, the expected number of requested labels during this execution is at most\nO\n( d log2 ( 1\n\u2206d\n)) + \u221a \u2206dM \u2264 O ( d log2 ( 1\n\u2206d\n))\n+O\n(\nd\n\u221a\nlog\n(\n1\n\u2206d\n)\n)\n= O\n( d log2 ( 1\n\u2206d\n))\n.\nThus, by linearity of the expectation, the expected number of labels requested among the first T samples is at most\nO\n( d log2 ( 1\n\u2206d\n)\u2308\nT\nM\n\u2309)\n= O (\u221a \u2206d log3/2 ( 1\n\u2206d\n)\nT\n)\n,\nwhich completes the proof. \u2293\u2294\nRemark: The original work of [CMEDV10] additionally allowed for some number K of \u201cjumps\u201d: times t at which \u2206t = 1. Note that, in the above algorithm, since the influence of each sample is localized to the predictors trained within that\n\u201cbatch\u201d of M instances, the effect of allowing such jumps would only change\nthe bound on the number of mistakes to O\u0303 (\u221a d\u2206T + \u221a\nd \u2206K\n)\n. This compares\nfavorably to the result of [CMEDV10], which is roughly O ( (d\u2206)1/4T + d 1/4 \u22063/4 K ) . However, the result of [CMEDV10] was proven for a more general setting, allowing distributions P that are not uniform (though they do require a relation between the angle between any two separators and the probability mass they disagree on, similar to that holding for the uniform distribution, which seems to require that the distributions approximately retain some properties of the uniform distribution). It is not clear whether Theorem 3 can be generalized to this larger family of distributions."}, {"heading": "6 General Results for Active Learning", "text": "As mentioned, the above results on linear separators also provide results for the number of queries in active learning. One can also state quite general results on the expected number of queries and mistakes achievable by an active learning algorithm. This section provides such results, for an algorithm based on the the well-known strategy of disagreement-based active learning. Throughout this section, we suppose h\u2217 \u2208 S\u2206, for a given \u2206 \u2208 (0, 1]: that is, P(x : h\u2217t+1(x) 6= h\u2217t (x)) \u2264 \u2206 for all t \u2208 N.\nFirst, we introduce a few definitions. For any set H \u2286 C, define the region of disagreement\nDIS(H) = {x \u2208 X : \u2203h, g \u2208 H s.t. h(x) 6= g(x)}. The analysis in this section is centered around the following algorithm. The Active subroutine is from the work of [Han12] (slightly modified here), and is a variant of the A2 (Agnostic Acive) algorithm of [BBL06]; the appropriate values of M and T\u0302k(\u00b7) will be discussed below.\nAlgorithm: DriftingActive 0. For i = 1, 2, . . . 1. Active(M(i\u2212 1))\nSubroutine: Active(t) 0. Let h\u03020 be an arbitrary element of C, and let V0 \u2190 C 1. Predict Y\u0302t+1 = h\u03020(Xt+1) as the prediction for the value of Yt+1 2. For k = 0, 1, . . . , log2(M/2) 3. Qk \u2190 {} 4. For s = 2k + 1, . . . , 2k+1 5. Predict Y\u0302s = h\u0302k(Xs) as the prediction for the value of Ys 6. If Xs \u2208 DIS(Vk) 7. Request the label Ys and let Qk \u2190 Qk \u222a {(Xs, Ys)} 8. Let h\u0302k+1 = argminh\u2208Vk \u2211\n(x,y)\u2208Qk 1[h(x) 6= y] 9. Let Vk+1 \u2190 {h \u2208 Vk : \u2211\n(x,y)\u2208Qk 1[h(x) 6= y]\u2212 1[h\u0302k+1(x) 6= y] \u2264 T\u0302k}\nAs in the DriftingHalfspaces algorithm above, this DriftingActive algorithm proceeds in batches, and in each batch runs an active learning algorithm designed to be robust to classification noise. This robustness to classification noise translates into our setting as tolerance for the fact that there is no classifier in C that perfectly classifies all of the data. The specific algorithm employed here maintains a set Vk \u2286 C of candidate classifiers, and requests the labels of samples Xs for which there is some disagreement on the classification among classifiers in Vk. We maintain the invariant that there is a low-error classifier contained in Vk at all times, and thus the points we query provide some information to help us determine which among these remaining candidates has low error rate. Based on these queries, we periodically (in Step 9) remove from Vk those classifiers making a relatively excessive number of mistakes on the queried samples, relative to the minimum among classifiers in Vk. All predictions are made with an element of Vk. 7\nWe prove an abstract bound on the number of labels requested by this algorithm, expressed in terms of the disagreement coefficient [Han07], defined as follows. For any r \u2265 0 and any classifier h, define B(h, r) = {g \u2208 C : P(x : g(x) 6= h(x)) \u2264 r}. Then for r0 \u2265 0 and any classifier h, define the disagreement coefficient of h with respect to C under P :\n\u03b8h(r0) = sup r>r0 P(DIS(B(h, r))) r .\nUsually, the disagreement coefficient would be used with h equal the target concept; however, since the target concept is not fixed in our setting, we will make use of the worst-case value of the disagreement coefficient: \u03b8C(r0) = suph\u2208C \u03b8h(r0). This quantity has been bounded for a variety of spaces C and distributions P (see e.g., [Han07,EYW12,BL13]). It is useful in bounding how quickly the region DIS(Vk) collapses in the algorithm. Thus, since the probability the algorithm requests the label of the next instance is P(DIS(Vk)), the quantity \u03b8C(r0) naturally arises in characterizing the number of labels we expect this algorithm to request. Specifically, we have the following result.8\nTheorem 4. For an appropriate universal constant c1 \u2208 [1,\u221e), if h\u2217 \u2208 S\u2206 for some \u2206 \u2208 (0, 1], then taking M = \u2308\nc1\n\u221a\nd \u2206\n\u2309\n2\n, and T\u0302k = log2(1/ \u221a d\u2206)+22k+2e\u2206,\nand defining \u01eb\u2206 = \u221a d\u2206Log(1/(d\u2206)), the above DriftingActive algorithm makes an expected number of mistakes among the first T instances that is\nO (\u01eb\u2206Log(d/\u2206)T ) = O\u0303 (\u221a d\u2206 ) T\nand requests an expected number of labels among the first T instances that is\nO (\u03b8C(\u01eb\u2206)\u01eb\u2206Log(d/\u2206)T ) = O\u0303 ( \u03b8C( \u221a d\u2206) \u221a d\u2206 ) T."}, {"heading": "7 One could alternatively proceed as in DriftingHalfspaces, using the final classifier from the previous batch, which would also add a guarantee on the error rate achieved at all sufficiently large t.", "text": "8 Here, we define \u2308x\u23092 = 2 \u2308log2(x)\u2309, for x \u2265 1.\nThe proof of Theorem 4 relies on an analysis of the behavior of the Active subroutine, characterized in the following lemma.\nLemma 9. Fix any t \u2208 N, and consider the values obtained in the execution of Active(t). Under the conditions of Theorem 4, there is a universal constant c2 \u2208 [1,\u221e) such that, for any k \u2208 {0, 1, . . . , log2(M/2)}, with probability at least 1 \u2212 2 \u221a d\u2206, if h\u2217t+1 \u2208 Vk, then h\u2217t+1 \u2208 Vk+1 and suph\u2208Vk+1 P(x : h(x) 6=\nh\u2217t+1(x)) \u2264 c22\u2212kdLog(c1/ \u221a d\u2206).\nProof. By a Chernoff bound, with probability at least 1\u2212 \u221a d\u2206,\n2k+1 \u2211\ns=2k+1\n1[h\u2217t+1(Xs) 6= Ys] \u2264 log2(1/ \u221a d\u2206) + 22k+2e\u2206 = T\u0302k.\nTherefore, if h\u2217t+1 \u2208 Vk, then since every g \u2208 Vk agrees with h\u2217t+1 on those points Xs /\u2208 DIS(Vk), in the update in Step 9 defining Vk+1, we have\n\u2211\n(x,y)\u2208Qk\n1[h\u2217t+1(x) 6= y]\u2212 1[h\u0302k+1(x) 6= y]\n=\n2k+1 \u2211\ns=2k+1\n1[h\u2217t+1(Xs) 6= Ys]\u2212 min g\u2208Vk\n2k+1 \u2211\ns=2k+1\n1[g(Xs) 6= Ys]\n\u2264 2k+1 \u2211\ns=2k+1\n1[h\u2217t+1(Xs) 6= Ys] \u2264 T\u0302k,\nso that h\u2217t+1 \u2208 Vk+1 as well. Furthermore, if h\u2217t+1 \u2208 Vk, then by the definition of Vk+1, we know every h \u2208 Vk+1 has\n2k+1 \u2211\ns=2k+1\n1[h(Xs) 6= Ys] \u2264 T\u0302k + 2k+1 \u2211\ns=2k+1\n1[h\u2217t+1(Xs) 6= Ys],\nso that a triangle inequality implies\n2k+1 \u2211\ns=2k+1\n1[h(Xs) 6= h\u2217t+1(Xs)] \u2264 2k+1 \u2211\ns=2k+1\n1[h(Xs) 6= Ys] + 1[h\u2217t+1(Xs) 6= Ys]\n\u2264 T\u0302k + 2 2k+1 \u2211\ns=2k+1\n1[h\u2217t+1(Xs) 6= Ys] \u2264 3T\u0302k.\nLemma 1 then implies that, on an additional event of probability at least 1 \u2212\u221a d\u2206, every h \u2208 Vk+1 has\nP(x : h(x) 6= h\u2217t+1(x))\n\u2264 2\u2212k3T\u0302k + c2\u2212k \u221a 3T\u0302k(dLog(2k/d) + Log(1/ \u221a d\u2206))\n+ c2\u2212k(dLog(2k/d) + Log(1/ \u221a d\u2206))\n\u2264 2\u2212k3 log2(1/ \u221a d\u2206) + 2k12e\u2206+ c2\u2212k \u221a 6 log2(1/ \u221a d\u2206)dLog(c1/ \u221a d\u2206)\n+ c2\u2212k \u221a 22k24e\u2206dLog(c1/ \u221a d\u2206) + 2c2\u2212kdLog(c1/ \u221a d\u2206)\n\u2264 2\u2212k3 log2(1/ \u221a d\u2206) + 12ec1 \u221a d\u2206+ 3c2\u2212k \u221a dLog(c1/ \u221a d\u2206)\n+ \u221a 24ec \u221a d\u2206Log(c1/ \u221a d\u2206) + 2c2\u2212kdLog(c1/ \u221a d\u2206),\nwhere c is as in Lemma 1. Since \u221a d\u2206 \u2264 2c1d/M \u2264 c1d2\u2212k, this is at most\n( 5 + 12ec21 + 3c+ \u221a 24ecc1 + 2c ) 2\u2212kdLog(c1/ \u221a d\u2206).\nLetting c2 = 5+12ec 2 1+3c+ \u221a 24ecc1+2c, we have the result by a union bound.\n\u2293\u2294\nWe are now ready for the proof of Theorem 4.\nProof (Proof of Theorem 4). Fix any i \u2208 N, and consider running Active(M(i\u2212 1)). Since h\u2217M(i\u22121)+1 \u2208 C, by Lemma 9, a union bound, and induction, with probability at least 1 \u2212 2 \u221a d\u2206 log2(M/2) \u2265 1\u2212 2 \u221a d\u2206 log2(c1 \u221a\nd/\u2206), every k \u2208 {0, 1, . . . , log2(M/2)} has\nsup h\u2208Vk\nP(x : h(x) 6= h\u2217M(i\u22121)+1(x)) \u2264 c221\u2212kdLog(c1/ \u221a d\u2206). (13)\nThus, since h\u0302k \u2208 Vk for each k, the expected number of mistakes among the predictions Y\u0302M(i\u22121)+1, . . . , Y\u0302Mi is\n1 +\nlog2(M/2) \u2211\nk=0\n2k+1 \u2211\ns=2k+1\nP(h\u0302k(XM(i\u22121)+s) 6= YM(i\u22121)+s)\n\u2264 1 + log2(M/2) \u2211\nk=0\n2k+1 \u2211\ns=2k+1\nP(h\u2217M(i\u22121)+1(XM(i\u22121)+s) 6= YM(i\u22121)+s)\n+\nlog2(M/2) \u2211\nk=0\n2k+1 \u2211\ns=2k+1\nP(h\u0302k(XM(i\u22121)+s) 6= h\u2217M(i\u22121)+1(XM(i\u22121)+s))\n\u2264 1 +\u2206M2 + log2(M/2) \u2211\nk=0\n2k ( c22 1\u2212kdLog(c1/ \u221a d\u2206) + 2 \u221a d\u2206 log2(M/2) )\n\u2264 1 + 4c21d+ 2c2dLog(c1/ \u221a d\u2206) log2(2c1 \u221a d/\u2206) + 4c1d log2(c1 \u221a d/\u2206)\n= O (dLog(d/\u2206)Log(1/(d\u2206))) .\nFurthermore, (13) implies the algorithm only requests the label YM(i\u22121)+s for s \u2208 {2k+1, . . . , 2k+1} if XM(i\u22121)+s \u2208 DIS(B(h\u2217M(i\u22121)+1, c221\u2212kdLog(c1/ \u221a d\u2206))), so that the expected number of labels requested among YM(i\u22121)+1, . . . , YMi is at most\n1 +\nlog2(M/2) \u2211\nk=0\n2k ( E[P(DIS(B(h\u2217M(i\u22121)+1, c221\u2212kdLog(c1/ \u221a d\u2206))))]\n+2 \u221a d\u2206 log2(c1 \u221a d/\u2206) )\n\u2264 1 + \u03b8C ( 4c2dLog(c1/ \u221a d\u2206)/M ) 2c2dLog(c2/ \u221a d\u2206) log2(2c1 \u221a d/\u2206)\n+ 4c1d log2(c1 \u221a d/\u2206)\n= O (\n\u03b8C\n(\u221a d\u2206Log(1/(d\u2206)) ) dLog(d/\u2206)Log(1/(d\u2206)) ) .\nThus, the expected number of mistakes among indices 1, . . . , T is at most\nO\n(\ndLog(d/\u2206)Log(1/(d\u2206))\n\u2308\nT\nM\n\u2309) = O (\u221a d\u2206Log(d/\u2206)Log(1/(d\u2206))T ) ,\nand the expected number of labels requested among indices 1, . . . , T is at most\nO\n(\n\u03b8C\n(\u221a d\u2206Log(1/(d\u2206)) ) dLog(d/\u2206)Log(1/(d\u2206)) \u2308 T\nM\n\u2309)\n= O (\n\u03b8C\n(\u221a d\u2206Log(1/(d\u2206)) )\u221a d\u2206Log(d/\u2206)Log(1/(d\u2206))T ) .\n\u2293\u2294"}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["ABL13. P. Awasthi", "M.-F. Balcan", "P.M. Long"], "venue": null, "citeRegEx": "Awasthi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2013}, {"title": "Learning changing concepts by exploiting the structure of change", "author": ["BBDK00. P.L. Bartlett", "S. Ben-David", "S.R. Kulkarni"], "venue": "Machine Learning,", "citeRegEx": "Bartlett et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2000}, {"title": "Agnostic active learning", "author": ["BBL06. M.F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In Proc. of the 23rd International Conference on Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Margin based active learning", "author": ["BBZ07. M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In Proceedings of the 20 Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "On the complexity of learning from drifting distributions", "author": ["R.D. Barve", "P.M. Long"], "venue": "In Proceedings of the ninth annual conference on Computational learning theory,", "citeRegEx": "Barve and Long.,? \\Q1996\\E", "shortCiteRegEx": "Barve and Long.", "year": 1996}, {"title": "On the complexity of learning from drifting distributions", "author": ["BL97. R.D. Barve", "P.M. Long"], "venue": "Inf. Comput.,", "citeRegEx": "Barve and Long.,? \\Q1997\\E", "shortCiteRegEx": "Barve and Long.", "year": 1997}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["BL13. M.-F. Balcan", "P.M. Long"], "venue": "In Proceedings of the 26 Conference on Learning Theory,", "citeRegEx": "Balcan and Long.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Long.", "year": 2013}, {"title": "Concentration Inequalities", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Regret minimization with concept drift", "author": ["CMEDV10. K. Crammer", "Y. Mansour", "E. Even-Dar", "J. Wortman Vaughan"], "venue": "In COLT,", "citeRegEx": "Crammer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2010}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A. Kalai", "C. Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "Active learning via perfect selective classification", "author": ["EYW12. R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2012\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2012}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Han07. S. Hanneke"], "venue": "In Proceedings of the 24 International Conference on Machine Learning,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Activized learning: Transforming passive to active with improved label complexity", "author": ["Han12. S. Hanneke"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hanneke.,? \\Q2012\\E", "shortCiteRegEx": "Hanneke.", "year": 2012}, {"title": "Tracking drifting concepts using random examples", "author": ["HL91. D.P. Helmbold", "P.M. Long"], "venue": "In COLT, pages", "citeRegEx": "Helmbold and Long.,? \\Q1991\\E", "shortCiteRegEx": "Helmbold and Long.", "year": 1991}, {"title": "Tracking drifting concepts by minimizing disagreements", "author": ["HL94. D.P. Helmbold", "P.M. Long"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Long.,? \\Q1994\\E", "shortCiteRegEx": "Helmbold and Long.", "year": 1994}, {"title": "Predicting {0, 1}-functions on randomly drawn points", "author": ["HLW94. D. Haussler", "N. Littlestone", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Haussler et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1994}, {"title": "Concise formulas for the area and volume of a hyperspherical cap", "author": ["Li11. S. Li"], "venue": "Asian Journal of Mathematics and Statistics,", "citeRegEx": "Li.,? \\Q2011\\E", "shortCiteRegEx": "Li.", "year": 2011}, {"title": "The complexity of learning according to two models of a drifting environment", "author": ["Lon99. P.M. Long"], "venue": "Machine Learning,", "citeRegEx": "Long.,? \\Q1999\\E", "shortCiteRegEx": "Long.", "year": 1999}, {"title": "Estimation of Dependencies Based on Empirical Data", "author": ["Vap82. V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1982\\E", "shortCiteRegEx": "Vapnik.", "year": 1982}, {"title": "Statistical Learning Theory", "author": ["Vap98. V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [], "year": 2015, "abstractText": "We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates.", "creator": "LaTeX with hyperref package"}}}