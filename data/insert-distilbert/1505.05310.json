{"id": "1505.05310", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Supervised Learning for Dynamical System Learning", "abstract": "recently there has been substantial interest in predictive state methods for learning dynamical systems : these algorithms are popular since they often offer a good tradeoff between computational speed requirement and total statistical efficiency. despite their desirable properties, though, predictive state methods can sometimes be overly difficult to use in practice. e. g., in contrast to the rich literature on the supervised learning methods, which allows us to fully choose from an extensive menu of models and associated algorithms to suit the prior beliefs we have about properties of the function to be learned, similarly predictive state dynamical system learning methods are comparatively inflexible : it is as if we were restricted to use only linear regression instead of being allowed to choose decision trees, nonparametric regression, or the lasso. to address this problem, we additionally propose a new view of predictive state methods in terms of instrumental variable regression. this view allows us to construct a wide variety of dynamical system learners simply by swapping in different supervised learning methods. we demonstrate the effectiveness of our proposed methods by experimenting with non - linear regression to essentially learn a hidden markov model, showing that the resulting algorithm outperforms the correctness of this algorithm follows purely directly from our general analysis.", "histories": [["v1", "Wed, 20 May 2015 10:38:44 GMT  (521kb,D)", "http://arxiv.org/abs/1505.05310v1", null], ["v2", "Wed, 4 Nov 2015 16:16:04 GMT  (530kb,D)", "http://arxiv.org/abs/1505.05310v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ahmed hefny", "carlton downey", "geoffrey j gordon"], "accepted": true, "id": "1505.05310"}, "pdf": {"name": "1505.05310.pdf", "metadata": {"source": "CRF", "title": "A New View of Predictive State Methods for Dynamical System Learning", "authors": ["Ahmed Hefny", "Carlton Downey", "Geoffrey J. Gordon"], "emails": ["ahefny@cs.cmu.edu", "cmdowney@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": null, "text": "for learning dynamical systems: these algorithms are popular since they often offer a good tradeoff between computational speed and statistical efficiency. Despite their desirable properties, though, predictive state methods can sometimes be difficult to use in practice. E.g., in contrast to the rich literature on supervised learning methods, which allows us to choose from an extensive menu of models and algorithms to suit the prior beliefs we have about properties of the function to be learned, predictive state dynamical system learning methods are comparatively inflexible: it is as if we were restricted to use only linear regression instead of being allowed to choose decision trees, nonparametric regression, or the lasso. To address this problem, we propose a new view of predictive state methods in terms of instrumentalvariable regression. This view allows us to construct a wide variety of dynamical system learners simply by swapping in different supervised learning methods. We demonstrate the effectiveness of our proposed methods by experimenting with non-linear regression to learn a hidden Markov model, showing that the resulting algorithm outperforms its linear counterpart; the correctness of this algorithm follows directly from our general analysis."}, {"heading": "1 Introduction", "text": "Recently, there has been substantial interest in a new class of algorithms for learning dynamical systems. These algorithms combine several key intuitions, of which two are important to the current discussion.\nar X\niv :1\n50 5.\n05 31\n0v 1\n[ st\nat .M\nL ]\nFirst is the idea of predictive state: we can replace a belief about a latent variable S by the prediction of some observable variables X that depend on S. That is, on observing some evidence E about S, we could calculate the belief P(S | E). But, as long as the function f(S) = E(X | S) is sufficiently rich (e.g., for discreteX and S represented by one-hot encodings, invertible), it is equivalent to calculate E(X | E) directly: we could recover P(S | E) by inverting f , but in many cases we don\u2019t need to do so.\nSecond is the method of moments: for a dynamical system model with parameters \u03b8, it is often intractable to solve the maximum likelihood problem\nmax \u03b8\nlnP (observations | \u03b8)\nBut, we can sometimes find a statistic T such that the expectation of T is a simple, invertible function of \u03b8: E\u03b8(T ) = g(\u03b8). In this case, we can replace the expectation E\u03b8(T ) with the empirical average T\u0302 = 1N \u2211N i=1 Ti. (Here Ti is the value of our statistic for the ith of a set ofN observations.) We can then define an estimator \u03b8\u0302 as\n\u03b8\u0302 = g\u22121(T\u0302 ) (1)\nand seek an efficient algorithm for solving the inverse problem (1). The trick to designing a good method of moments algorithm is to discover a statistic T such that problem (1) is well-conditioned and efficiently solvable. One of the main tools that algorithm designers use for this purpose is to expand the class of models considered, thereby removing difficult constraints from (1): for example, instead of learning a hidden Markov model (HMM), we can expand the model class to include all observable operator models (OOMs) [14].\nFor brevity, we will call methods that use the above intuitions predictive state methods. Predictive state methods are popular for dynamical system learning because they often offer a good tradeoff between computational speed and statistical efficiency.\nHowever, there are also some important difficulties with these methods. One is that it can be hard for predictive state methods to take advantage of prior knowledge about the structure or parameters of a dynamical system: for example, expanding from HMMs to OOMs removes our ability to directly refer to the conditional probability distribution of observations given states, a common place to incorporate structure. Another is that deriving, analyzing, and implementing new predictive state methods can require substantial expertise: it can be difficult to discover an appropriate statistic T , accumulate its empirical average T\u0302 efficiently,\nand track how estimation errors in T\u0302 propagate through the inverse problem (1) to affect \u03b8\u0302.\nWe address both of these problems with a new view of predictive state methods for dynamical system learning. In this view, a dynamical system learning problem is reduced to a sequence of supervised learning problems. So, we can directly apply the rich literature on supervised learning methods to incorporate many types of prior knowledge about problem structure. We give a general convergence rate analysis that allows a high degree of flexibility in designing estimators. And finally, implementing a new estimator becomes as simple as rearranging our data and calling the appropriate supervised learning subroutines.\nOur new view is based on instrumental variable regression [18, 22]. Instrumental variable regression is a well-known technique to compensate for certain types of observation noise in a linear regression problem; it can let us recover regression coefficients accurately where ordinary regression would yield biased estimates. The connection between predictive state learning and linear instrumental variable regression has been noted before, e.g., in [4]. We propose a generalization of the linear two stage ordinary least squares procedure [22], give error bounds for this generalization, and formulate dynamical systems learning as an instance of this regression technique.\nMore specifically, our contribution is to show that we can use much-moregeneral supervised learning algorithms in place of linear regression, and still get a meaningful theoretical analysis. In more detail: (1) we point out that we can equally well use any well-behaved supervised learning algorithm in place of linear regression in the first stage of instrumental-variable regression; (2) for the second stage of instrumental-variable regression, we generalize ordinary linear regression to its RKHS counterpart; (3) we analyze the resulting combination, and show that we get convergence to the correct answer, with a rate that depends on how quickly the individual supervised learners converge.\nIn the remainder of the paper, we first describe how to use instrumentalvariable regression to learn a dynamical system (Sec. 2). We then provide theoretical guarantees for the two-stage instrumental-variable regression technique with non-linearity (Sec. 4). Finally, we give an example of how our techniques let us rapidly design enhanced predictive state methods for dynamical systems by encoding modeling assumptions (Sec. 5): we show that replacing linear regression steps in the spectral HMM learning algorithm with a non-linear regression with fewer parameters results in better prediction performance. The correctness of the resulting algorithm follows immediately from our general analysis."}, {"heading": "2 Instrumental Regression for Dynamical Systems", "text": "We consider a dynamical system of the form in Fig. 1: a sequence of observations ot \u2208 O explained by latent states st \u2208 S connected in a chain. A key question we need to solve in order be able to perform inference in the dynamical system is how to recursively update our belief about state: given a belief about st and a new observation ot+1, compute a belief about st+1. This is referred to as filtering. Another inference task is prediction: predicting an observation ot+k given our belief about the current state st. This involves computing a belief about future state state st+1 given our belief about st.\nIf st and ot have small, discrete ranges, the predictive state algorithm for learning a dynamical system is well known: see, e.g., [6, 3]. In fact, it is also known that we can interpret this algorithm as linear instrumental-variable regression [4]. Our proposed framework generalizes that direction by reducing dynamical system learning to solving three supervised learning problems, with the additional ability to incorporate arbitrary non-linear regression models in two of these problems.\nThe first step to formulate dynamical system learning as a supervised learning problem is to use an observable (predictive state) representation by replacing our belief about st with a predictive state: pick a statistic \u03c8t = \u03c8(ot:t+k\u22121) of a window of future observations ot+1:t+k, and instead of tracking our belief P(st | o1:t\u22121), track the predictive state E[\u03c8t | o1:t\u22121]. (The dimension of \u03c8 must be at least as high as the number of discrete latent states.) We will use Qt|t\u2212k to denote E[\u03c8t | o1:t\u2212k] and hence our predictive predictive state is denoted by Qt \u2261 Qt|t\u22121. We assume that the system is k-observable and hence latent states are distinguishable by the distribution of a window of k future observations1 [25].\nSecond, we formulate a statistic \u03bet = \u03be(ot:t+k) over extended future observations ot:t+k with conditional expectation Pt = E[\u03bet | o1:t\u22121] such that Qt+1|t\u22121 and Qt+1 (i.e. our belief about the shifted future ot+1:t+k) can be inferred from Pt and ot.\n1In principle, the statistics can depend on the entire future. The restriction to a window of observations simplifies the notation and is commonly used in practice.\nLearning a dynamical system then amounts to learning an operator W that satisfies the moment condition:2\nPt = WQt \u2200t (2)\nFiltering and prediction then correspond to inferring Qt+1 and Qt+1|t\u22121 respectively from Pt. We assume that W is a linear operator. Unfortunately, we do not observeQt or Pt but noisy versions thereof. Moreover, due to the overlap between observation windows, the noise terms on \u03c8t and \u03bet are correlated. This noise correlation means that na\u0131\u0308ve linear regression (using samples of \u03c8t and \u03bet) will give a biased estimate of the dependence between Qt and Pt.\nTo counteract this bias, we employ instrumental regression [18, 22]. Instrumental regression uses instrumental variables that are correlated with the input Qt but not with the noise t:t+k. This property provides a criterion to denoise the inputs and outputs of the original regression problem: we remove that part of the input/output that is not correlated with the instrumental variables. Since past observations o1:t\u22121 do not overlap with future or extended future windows, they are not correlated with the noise t:t+k+1. Therefore, we can use history features ht \u2261 h(o1:t\u22121) as instrumental variables.\nIn more detail, by taking the expectation of (2) over ht we obtain an instrumentbased moment condition\nE[Pt | ht] = E[WQt | ht] E[E[\u03bet | o1:t\u22121] | ht] = WE[E[\u03c8t | o1:t\u22121] | ht]\nE[\u03bet | ht] = WE[\u03c8t | ht] (3)\nAssuming that there are enough independent dimensions in ht that are correlated with Qt, we maintain the rank of the moment condition when moving from (2) to (3), and we can recover W by least squares if we can compute E[\u03c8t | ht] and E[\u03bet | ht] for sufficiently many examples t.\nIn summary, learning and inference of a dynamical system through instrumental regression can be described as follows:\n\u2022 Model Specifcation: Pick features of history ht = h(o1:t\u22121), future \u03c8t = \u03c8(ot:t+k\u22121) and extended future \u03bet = \u03be(ot:t+k). \u03c8t must be a sufficient statistic for P(ot:t+k\u22121 | o1:t\u22121). \u03bet must satisfy\n2Note that, similar to [16], Pt is a deterministic function of Qt and hence this condition has a unique solution if we observe sufficient examples of Pt and Qt.\n\u2013 E[\u03c8t+1 | o1:t\u22121] = fpredict(E[\u03bet | o1:t\u22121]) for a known function fpredict. \u2013 E[\u03c8t+1 | o1:t] = ffilter(E[\u03bet | o1:t\u22121], ot) for a known function ffilter.\n\u2022 S1A (Stage 1A) Regression: Learn a (possibly non-linear) regression model to estimate \u03c8\u0304t \u2261 E[\u03c8t | ht]. The training data for this model are (ht, \u03c8t) across time steps t.3\n\u2022 S1B Regression: Learn a (possibly non-linear) regression model to estimate \u03be\u0304t \u2261 E[\u03bet | ht]. The training data for this model are (ht, \u03bet) across time steps t.\n\u2022 S2 Regression: Use the feature expectations estimated in the previous two steps to train a model to predict \u03be\u0304t = W\u03c8\u0304t, where W is a linear operator. The training data for this model are estimates of (\u03c8\u0304t, \u03be\u0304t) across time steps t obtained from S1 steps.\n\u2022 Initial State Estimation: Estimate an initial stateQ1 = E[\u03c81] by averaging \u03c81 across several example realizations of our time series.4\n\u2022 Inference: Starting from the initial state Q1, we can maintain the belief state Qt \u2261 E[\u03c8t | o1:t\u22121] through filtering: given Qt we compute Pt \u2261 E[\u03bet | o1:t\u22121] = WQt. Then, given the observation ot, we can compute Qt+1 = ffilter(Pt, ot). Or, in the absence of ot, we can predict the next state Qt+1|t\u22121 = fpredict(Pt). Finally, by definition, the belief stateQt is sufficient to predict P(ot:t+k\u22121 : o1:t\u22121).\nThe process of learning and inference is depicted in Figure 2. Modeling assumptions are reflected in the choice of the statistics \u03c8, \u03be and h as well as the regression models in stages S1A and S1B. In the supplementary material we show that, with linear S1 models and certain choices of statistics, we can recover existing spectral algorithms for dynamical systems learning. The two stage framework not only provides a unifying view of some of the successful dynamical systems learning algorithms but also paves the way for extending them in a theoretically justified manner, as we demonstrate in the experiments.\n3Our bounds assume that the training time steps t are sufficiently spaced for the underlying process to mix, but in practice, the error will only get smaller if we consider all time steps t.\n4This is the only step that needs multiple realizations of our time series. If only a single long realization is available, we need additional assumptions to be able to estimate an initial state; for example, if we assume stationarity, we can set the initial state to be the empirical average vector of future features, 1T \u2211T t=1 \u03c8t."}, {"heading": "3 Related Work", "text": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].\nOne common aspect in all these models is that they exploit the covariance structure between future and past observation sequences to obtain an unbiased observable state representation. Indeed, many of these algorithms can be reformulated as a two-stage linear instrumental regression. Boots and Gordon [4] note the connection between the HSE-HMM and instrumental variables, which is manifested in the use of kernel SVD of a future-past covariance operator to identify the latent state space. We use this connection to build a general framework for dynamical system learning where the state-space can be identified using supervised learning methods, including non-linear ones.\nReducing dynamical systems learning to supervised learning dates back to auto-regressive models [17], where the state of the system is assumed to be fully determined by the previous k observations. Our aim is to use supervised learn-\ning methods to learn latent state models from observation sequences. This bears similarity to Langford et al.\u2019s sufficient posterior representation (SPR) [16], which encodes the state by the sufficient statistics of the conditional distribution of the next observation and represents system dynamics by three vector-valued functions that are estimated using supervised learning approaches. While SPR allows all of these functions to be non-linear, there are some advantages that distinguish our work. First, while SPR is limited to 1-step observable systems (where the distribution over the next observation uniquely determines the state), our framework can seamlessly handle k-step observable systems by choosing a large enough (or even unbounded) window size. The use of instrumental variables ensures that correlated noise on overlapping windows does not bias our estimates of the system parameters. Secondly, SPR involves a rather complicated training procedure, involving multiple iterations of model refinement and model averaging, whereas our framework only requires solving three regression problems in sequence. Finally, the theoretical analysis of [16] only establishes the consistency of SPR learning assuming that all regression steps are solved perfectly. Our work, on the other hand, establishes convergence rates based on the performance of S1 regression."}, {"heading": "4 Theoretical Analysis", "text": "In this section we present our main theoretical result: consistency and a convergence rate bound for two-stage instrumental regression, under the assumption that S1 predictions converge to the true conditional expectations at an appropriate rate, regardless of the functional form of the S1 regressors.\nWe assume we are given i.i.d. triplets (xt, yt, zt), where xt \u2208 X , yt \u2208 Y and zt \u2208 Z denote input, output and instrumental variables respectively. (As mentioned above, we can equally well use correlated samples, as would result from successive time steps of a time series; our convergence rates will then include a factor that depends on the mixing rate of the underlying dynamical system.)\nFor generality, we assume that X , Y and Z are reproducing kernel Hilbert spaces (RKHS) of possibly infinite dimension, and that the operator W is estimated through (kernel) ridge regression\nW\u0302\u03bb = ( T\u2211 t=1 y\u0302t \u2297 x\u0302t )( T\u2211 t=1 x\u0302t \u2297 x\u0302t + \u03bbIX )\u22121 (4)\nwhere \u2297 denotes tensor product and \u03bb > 0 is a regularization parameter that ensures the invertibility of the estimated covariance. \u03bb can be 0 in finite dimensional\ncases where we have an invertible covariance matrix. The RKHS view is useful when the future statistics are represented in terms of kernels\u2014for example, if they are kernel mean maps of the distribution of future observations, a case that is closely related to the HSE-HMM [20] and HSE-PSR [5] models.\nLet x\u0304t and y\u0304t denote E[xt|zt] and E[yt|zt]. Also let x\u0302t and y\u0302t denote E\u0302[xt|zt] and E\u0302[yt|zt], as estimated by the S1A and S1B regression steps. We assume that x\u0304t, x\u0302t \u2208 X and y\u0304t, y\u0302t \u2208 Y . Let \u03a3x\u0304x\u0304 \u2208 X \u2297 X and \u03a3y\u0304y\u0304 \u2208 Y \u2297 Y denote the (uncentered) covariance operators of the distributions of x\u0304 and y\u0304 respectively: that is,\n\u03a3x\u0304x\u0304 = E[x\u0304\u2297 x\u0304] \u03a3y\u0304y\u0304 = E[y\u0304 \u2297 y\u0304]\nBefore we state our main theorem we need to quantify the quality of S1 regressions in a way that is independent of the functional form that we assume in S1.\nDefinition 1 (S1 Regression Bound). For a given \u03b4 > 0 and N \u2208 N+, we define the S1 regression bound \u03b7\u03b4,N > 0 to be a number satisfying the condition that, with probability at least (1\u2212 \u03b4/2), the following holds for all 1 \u2264 t \u2264 N :\n\u2016x\u0302t \u2212 x\u0304t\u2016X < \u03b7\u03b4,N \u2016y\u0302t \u2212 y\u0304t\u2016Y < \u03b7\u03b4,N\nAs long as, for each fixed \u03b4,\nlim N\u2192\u221e \u03b7\u03b4,N = 0, (5)\nour results show that the two stage estimator is consistent:\nTheorem 2. Assume that \u2016x\u0304\u2016X , \u2016x\u0304\u2016Y < c < \u221e almost surely. Let \u03b7\u03b4,N be as defined in Definition 1 and assume it satisfies (5). Assume W is a Hilbert-Schmidt operator, let W\u0302\u03bb be as defined in (4), and let R(\u03a3x\u0304x\u0304) denote the closure of the range of \u03a3x\u0304x\u0304. Then the following statement holds with probability at least 1 \u2212 \u03b4\nfor each xtest \u2208 R(\u03a3x\u0304x\u0304) s.t. \u2016xtest\u2016X \u2264 1.\n\u03b3\u03b4,N \u2261 \u2016W\u0302\u03bbxtest \u2212Wxtest\u2016Y =\nO \u03b7\u03b4,N 1\u03bb + \u221a 1 + \u221a log(1/\u03b4) N \u03bb 3 2  \n+O ( log(1/\u03b4)\u221a\nN\n( 1\n\u03bb +\n1\n\u03bb 3 2 )) +O (\u221a \u03bb )\nTheorem 2 gives a generic error bound on S2 regression in terms of S1 regression performance. We defer the proof, as well as finite sample analysis, to the supplementary material. The main insight from the theorem is that the error in estimating the parameter W is the sum of three contributions: the first term captures the error in the S1 regressions. The second term captures the effect of estimating the covariance operators from finite data, assuming S1 regression is exact. Finally, the third term captures the effect of regularization assuming covariance estimates are exact. It can be shown that, if X and Y are finite dimensional, \u03a3x\u0304x\u0304 spans X and we use least squares to estimate W (i.e. \u03bb = 0), then the first and last terms will vanish, and \u03bb in the middle two terms will be replaced by \u03bbx,min, the minimum eigenvalue of \u03a3x\u0304x\u0304.\nFor completeness, the following propositions provide concrete examples of S1 regression bounds \u03b7\u03b4,N for practical regression models.\nProposition 3. Assume X \u2261 Rdx ,Rdy ,Rdz for some dx, dy, dz < \u221e and that x\u0304 and y\u0304 are linear vector functions of z where the parameters are estimated using ordinary least squares. Assume that \u2016x\u0304\u2016X , \u2016y\u0304\u2016Y < c <\u221e almost surely. Let \u03b7\u03b4,N be as defined in Definition 1. Then\n\u03b7\u03b4,N = O (\u221a dz N log((dx + dy)/\u03b4) )\nProof. (sketch) This is based on results that bound parameter estimation error in linear regression with univariate response (e.g. [11]). Note that if x\u0304ti = U>i zt for\nsome Ui \u2208 Z , then a bound on the error norm \u2016U\u0302i\u2212Ui\u2016 implies a uniform bound of the same rate on x\u0302i \u2212 x\u0304. The probability of exceeding the bound is scaled by 1/(dx + dy) to correct for multiple regressions.\nVariants of Proposition 3 can also be developed using bounds on non-linear regression models (e.g., generalized linear models).\nThe next proposition addresses a scenario where X and Y are infinite dimensional.\nProposition 4. Assume that x and y are kernel evaluation functionals, x\u0304 and y\u0304 are linear vector functions of z where the linear operator is estimated using conditional mean embedding [21] with regularization parameter \u03bb0 > 0 and that \u2016x\u0304\u2016X , \u2016y\u0304\u2016Y < c < \u221e almost surely. Let \u03b7\u03b4,N be as defined in Definition 1. It follows that\n\u03b7\u03b4,N = O \u221a\u03bb0 + \u221a log(N/\u03b4)\n\u03bb0N  Proof. (sketch) This bound is based on [21], which gives a bound on the error in estimating the conditional mean embedding. The error probability is adjusted by \u03b4/4N to accommodate the requirement that the bound holds for all training data.\nIn the following, we apply theorem 2 to the setting of learning dynamical systems, where Qt \u2208 X , Pt \u2208 Y and ht \u2208 Z (Qt, Pt and ht are as defined in Section 2). One issue to note is that theorem 2 assumes that the test input lies within R(\u03a3x\u0304x\u0304). In dynamical systems context, however, the test input is an estimated predictive state Q\u0302t. Since S1 regression can fail to identify the subspace of true states given finite data, Q\u0302t can have a non-zero component t inR\u22a5(\u03a3x\u0304x\u0304), the orthogonal complement of R(\u03a3x\u0304x\u0304). The following lemma states that, in a stable system, this component gets smaller as S1 regression performs better.\nLemma 5. For a test sequence o1:T , let Q\u0302t denote the estimated state given o1:t\u22121. Let Q\u0303t denote the projection of Q\u0302t onto R(\u03a3x\u0304x\u0304). Assume that ffilter is L-Lipchitz continuous on Pt and that ffilter(Pt, ot) \u2208 R(\u03a3x\u0304x\u0304) for any Pt \u2208 R(\u03a3y\u0304y\u0304). Given the assumptions in theorem 2 and assuming that \u2016Q\u0302t\u2016X \u2264 R for all 1 \u2264 t \u2264 T , the following holds for all 1 \u2264 t \u2264 T with probability at least 1\u2212 \u03b4/2.\n\u2016 t\u2016X = \u2016Q\u0302t \u2212 Q\u0303t\u2016X = O ( \u03b7\u03b4,N\u221a \u03bb )\nSince W\u0302\u03bb is bounded. The prediction error due to adding t to the input diminishes at the same rate of \u2016 t\u2016X ."}, {"heading": "5 Case Study: Learning A Knowledge Tracing Model", "text": "In this section we demonstrate that we can learn a hidden Markov model using the two stage regression framework. We also demonstrate that we can change the regression methods to gain advantage. Specifically, we consider a limited data scenario, where we have a conflict between using many history features (picking a long history window to reduce noise in our predictions, and rich features of that window to achieve a linear relationship between history and future) or using few history features (reducing the number of parameters we have to learn from limited data). We show that we can use non-linear S1 regression models to reduce the number of parameters we need to learn, resulting in better empirical prediction accuracy compared to linear models while still maintaining consistency.\nIn this experiment we attempt to model and predict the performance of students learning from an interactive computer-based tutor. We use the Bayesian knowledge tracing (BKT) model [8], which is essentially a 2-state HMM: the state st represents whether a student has learned a knowledge component (KC), and the observation ot represents the success/failure of solving the tth question in a sequence of question that cover the said KC. With high probability, the student remains in the same state (learned or unlearned) and with smaller probability, the student may transition from unlearned to learned (learning) or learned to unlearned (forgetting). In the learned state, the student is more likely to answer a question correctly than in the unlearned state. It is also possible for the student to answer a question correctly while in the unlearned state (guessing) or incorrectly while in the learned state (slipping). The possible transitions and observations are summarized in figure 3."}, {"heading": "5.1 Data Description", "text": "The data set we used to evaluate the model is a publicly available data set from DataShop [15] called \u201cGeometry Area (1996-97).\u201d This data was generated by students learning introductory geometry, and contains attempts by 59 students in 12 knowledge components. As is typical for BKT, we consider a student\u2019s attempt at a question to be correct iff the student entered the correct answer on the first try, without requesting any hints from the help system. The sequence of first attempts"}, {"heading": "Skill", "text": ""}, {"heading": "Skill", "text": "for a student/KC pair constitutes a training sequence. We discard sequences of length less than 5, resulting in a total of 325 sequences. We pad each observation sequence at the beginning with dummy observations, to handle the case where the history window extends before the beginning of the sequence. (This procedure allows us to use more data in our regressions, which is important because of our limited sample size.) Therefore a history observation which is used as training input for S1 regression can be in one of three states: \u201ccorrect\u201d, \u201cincorrect\u201d or \u201cbefore beginning of time.\u201d We restrict the regression output however to be binary (\u201ccorrect\u201d or \u201cincorrect\u201d)."}, {"heading": "5.2 Model Description", "text": "Under the (reasonable) assumption that the two states have distinct observation probabilities, this model is 1-observable. It is reasonable then to choose the predictive state to be the expected next observation, which results in the following statistics:\n\u03c8t = ot\n\u03bet = ot \u2297k ot+1,\nwhere ot is represented by a 2 dimensional indicator vector and \u2297k denotes the Kronecker product. Given these statistics, Pt = E[\u03bet|o1:t\u22121] is a joint probability table of ot:t+1 from which conditioning on ot (filtering) and marginalizing over ot (prediction) are simple operations. It thus remains to choose the history features ht and the S1 regression model. In the appendix, we show that if use ht = ot\u22121 and linear regression as S1 regression model, the resulting algorithm is equivalent to spectral HMM method of [10] and thus we use it as a baseline. In fact, if we had access to sufficient data, we could learn the HMM using this base line model. Not counting dummy observations, the model has to learn 7 parameters (7 free covariance entries). Under limited data, however, we can achieve faster learning by incorporating prior knowledge. Here, we will take advantage of the intuition that switching states (learning or forgetting) is a relatively unlikely event. Hence, aggregating observations over multiple previous time steps is a better predictor of the state, since aggregation will mitigate the effects of guessing and slipping. So we would like to use ht = ot\u2212b:t\u22121 for some b > 1. We then have a choice: if we represent ht by an indicator vector of dimension 2b, then the optimal predictor of ot from ht will be linear, but the number of parameters we must learn will increase exponentially with b. On the other hand, if we represent ht by a binary vector of\nlength b, then we will only need to learn b + 1 parameters, but the optimal predictor of ot from ht will no longer be linear leading to poor performance of linear regression. It is not obvious a priori which choice will result in better learning performance.5\nOur formulation makes the choice much easier: we can use a history window of any length, pick the more-concise length-b representation, and train a nonlinear predictor such as a logistic regression. By doing so we combine the advantages of both of the previous paragraph\u2019s approaches: we only need to learn O(b) parameters, but our class of predictors still includes a near-optimal choice. (Logistic regression becomes exactly optimal as the probabilities of learning and forgetting approach zero. Since these probabilities are typically small in practice, logistic regression will be close to optimal in practice.) As we will see below, the result is better learning from limited data."}, {"heading": "5.3 Evaluation Procedure and Results", "text": "We evaluated three variants of HMM learning via two-stage regression. They are summarized in Table 1. We evaluated the models using 1000 random splits of the 325 sequences into 200 training and 125 testing. For each split, we trained each model on the training sequences. Then for each test sequence, we filter through the first 3 observations then predict the rest of the sequence, reporting the root mean square error for each split. The results are depicted in figure 4. The results show that, in terms of accuracy, model 3 outperforms model 2, which in turn outperforms model 1. In other words, feature expansion does increase predictive accuracy. However, even more gain is achieved using non-linear S1 models that require fewer parameters.\n5The numbers above ignore the effect of padding observation sequences, but the conclusions are similar in either case."}, {"heading": "6 Conclusion", "text": "In this work we developed a general framework for dynamical system learning using supervised learning methods. The proposed framework is based on two-stage regression: in the first stage we use history features to train regression models that denoise future observation windows into state estimates. In the second stage we use these state estimates to train a linear model that represents system dynamics.\nThis framework encompasses and provides a unified view of some successful dynamical system learning algorithms. We demonstrated the proposed framework in learning a Hidden Markov Model, where we have shown that we can use nonlinear regression to incorporate more history features in identifying the latent state without an exponential increase in the number of parameters.\nAs future work, we would like to apply this framework to more scenarios where we can leverage additional techniques such as manifold embedding, sparse learning and transfer learning in stage 1 regression. We would also like to extend the framework to controlled processes."}, {"heading": "A Spectral and HSE Dynamical System Learning as", "text": "Regression\nIn this section we provide examples of mapping some of the successful dynamical system learning algorithms to our framework."}, {"heading": "A.1 HMM", "text": "In this section we show that we can use instrumental regression framework to reproduce the spectral learning algorithm for learning HMM [10]. We consider 1- observable models but the argument applies to k-observable models. In this case we use \u03c8t = eot and \u03bet = eot:t+1 = eot \u2297k eot+1 , where \u2297k denotes the kronecker product. Let Pi,j \u2261 E[eoi \u2297 eoj ] be the joint probability table of observations i and j and let P\u0302i,j be its estimate from the data. We start with the (very restrictive) case where P1,2 is invertible. Given samples of h2 = eo1 , \u03c82 = eo2 and \u03be2 = eo2:3 , in S1 regression we apply linear regression to learn two matrices W\u03022,1 and W\u03022:3,1 such that:\nE\u0302[\u03c82|h2] = \u03a3\u0302o2o1\u03a3\u0302\u22121o1 h2 = P\u03022,1P\u0302 \u22121 1,1 ht \u2261 W\u03022,1h2 (A.1)\nE\u0302[\u03be2|h2] = \u03a3\u0302o2:3o1\u03a3\u0302\u22121o1 h2 = P\u03022:3,1P\u0302 \u22121 1,1 h2 \u2261 W\u03022:3,1h2, (A.2)\nwhere P2:3,1 \u2261 E[eo2:3 \u2297 eo1 ] In S2 regression, we learn the matrix W\u0302 that gives the least squares solution\nto the system of equations\nE\u0302[\u03be2|h2] \u2261 W\u03022:3,1eo1 = W\u0302 (W\u03022,1eo1) \u2261 W\u0302 E\u0302[\u03c82|h2] , for given samples of h2\nwhich gives\nW\u0302 = W\u03022:3,1E\u0302[eo1e>o1 ]W\u0302 > 2,1 ( W\u03022,1E\u0302[eo1e>o1 ]W\u0302 > 2,1 )\u22121 = ( P\u03022:3,1P\u0302 \u22121 1,1 P\u0302 > 2,1 )( P\u03022,1P\u0302 \u22121 1,1 P\u0302 > 2,1\n)\u22121 = P\u03022:3,1 ( P\u03022,1 )\u22121 (A.3)\nHaving learned the matrix W\u0302 , we can estimate\nP\u0302t \u2261 W\u0302Qt\nstarting from a state Qt. Since Pt specifies a joint distribution over eot+1 and eot we can easily condition on (or marginalize ot) to obtain Qt+1. We will show that this is equivalent to learning and applying observable operators as in [10]:\nFor a given value x of o2, define\nBx = u > x W\u0302 = u > x P\u03022:3,1 ( P\u0302>2,1 )\u22121 , (A.4)\nwhere ux is an |O| \u00d7 |O|2 matrix which selects a block of rows in P\u03022:3,1 corresponding to o2 = x. Specifically, ux = \u03b4x \u2297k I|O|. 6.\nQt+1 = E\u0302[eot+1|o1:t] \u221d u>otE\u0302[eot:t+1|o1:t\u22121] = u>otE\u0302[\u03bet|o1:t\u22121] = u > otW\u0302E[\u03c8t|o1:t\u22121] = BotQt\nwith a normalization constant given by\n1\n1>BotQt (A.5)\nNow we move to a more realistic setting, where we have rank(P2,1) = m < |O|. Therefore we project the predictive state using a matrix U that preserves the dynamics, by requiring that U>O (i.e. U is an independent set of columns spanning the range of the HMM observation matrix O).\nIt can be shown [10] thatR(O) = R(P2,1) = R(P2,1P\u221211,1 ). Therefore, we can use the leading m left singular vectors of W\u03022,1 , which corresponds to replacing the linear regression in S1A with a reduced rank regression. However, for the sake of our discussion we will use the singular vectors of P2,1. In more detail, let [U, S, V ] be the rank-m SVD decomposition of P2,1. We use \u03c8t = U>eot and \u03bet = eot \u2297k U>eot+1 . S1 weights are then given by W\u0302 rr2,1 = U>W\u03022,1 and W\u0302 rr2:3,1 = (I|O| \u2297k U>)W\u03022:3,1 and S2 weights are given by\nW\u0302 rr = (I|O| \u2297k U>)W\u03022:3,1E\u0302[eo1e>o1 ]W\u0302 > 2,1U ( U>W\u03022,1E\u0302[eo1e>o1 ]W\u0302 > 2,1U )\u22121 = (I|O| \u2297k U>)P\u03022:3,1P\u0302\u221211,1 V S ( SV >P\u0302\u221211,1 V S\n)\u22121 = (I|O| \u2297k U>)P\u03022:3,1P\u0302\u221211,1 V ( V >P\u0302\u221211,1 V )\u22121 S\u22121 (A.6)\n6Following the notation used in [10], u>x P\u03022:3,1 \u2261 P\u03023,x,1\nIn the limit of infinite data, V spans range(O) = rowspace(P2:3,1) and hence P2:3,1 = P2:3,1V V >. Substituting in (A.6) gives\nW rr = (I|O| \u2297k U>)P2:3,1V S\u22121 = (I|O| \u2297k U>)P2:3,1 ( U>P2,1 )+ Similar to the full-rank case we define, for each observation x anm\u00d7|O|2 selector matrix ux = \u03b4x \u2297k Im and an observation operator\nBx = u > x W\u0302 rr \u2192 U>P3,x,1 ( U>P2,1 )+ (A.7)\nThis is exactly the observation operator obtained in [10]. However, instead of using A.6, they use A.7 with P3,x,1 and P2,1 replaced by their empirical estimates.\nNote that for a state bt = E[\u03c8t|o1:t\u22121], Bxbt = P (ot|o1:t\u22121)E[\u03c8t+1|o1:t] = P (ot|o1:t\u22121)bt+1. To get bt+1, the normalization constant becomes 1P (ot|o1:t\u22121) =\n1 b>\u221eBxbt , where b>\u221eb = 1 for any valid predictive state b. To estimate b\u221e we solve the aforementioned condition for states estimated from all possible values of history features ht. This gives,\nb>\u221eW\u0302 rr 2,1I|O| = b > \u221eU >P\u03022,1P\u0302 \u22121 1,1 I|O| = 1 > |O|,\nwhere the columns of I|O| represent all possible values of ht. This in turn gives\nb>\u221e = 1 > |O|P\u03021,1(U >P\u03022,1) +\n= P\u0302>1 (U >P\u03022,1) +,\nthe same estimator proposed in [10]."}, {"heading": "A.2 Stationary Kalman Filter", "text": "A Kalman filter is given by\nst = Ost\u22121 + \u03bdt\not = Tst + t \u03bdt \u223c N (0,\u03a3s) t \u223c N (0,\u03a3o)\nWe consider the case of a stationary filter where \u03a3t \u2261 E[sts>t ] is independent of t. We choose our statistics\nht = ot\u2212H:t\u22121\n\u03c8t = ot:t+F\u22121\n\u03bet = ot:t+F\nIt can be shown [2, 25] that\nE[st|ht] = \u03a3s,h\u03a3\u22121h,hht\nand it follows that\nE[\u03c8t|ht] = \u0393\u03a3s,h\u03a3\u22121h,hht = W1ht E[\u03bet|ht] = \u0393+\u03a3s,h\u03a3\u22121h,hht = W2ht\nwhere \u0393 is the extended observation operator\n\u0393 \u2261  O OT\n... OT F\n ,\u0393+ \u2261  O OT\n... OT F+1  It follows that F and H must be large enough to have rank(W ) = n. Let U \u2208 RmF\u00d7n be the matrix of left singular values of W1 corresponding to non-zero singular values. Then U>\u0393 is invertible and we can write\nE[\u03c8t|ht] = UU>\u0393\u03a3s,h\u03a3\u22121h,hht = W1ht E[\u03bet|ht] = \u0393+\u03a3s,h\u03a3\u22121h,hht = W2ht E[\u03bet|ht] = \u0393+(U>\u0393)\u22121U> ( UU>\u0393\u03a3s,h\u03a3 \u22121 h,hht\n) = WE[\u03c8t|ht]\nwhich matches the instrumental regression framework. For the steady-state case (constant Kalman gain), one can estimate \u03a3\u03be given the data and the parameter W by solving Riccati equation as described in [25]. E[\u03bet|o1:t\u22121] and \u03a3\u03be then specify a joint Gaussian distribution where marginalization and conditioning can be easily performed."}, {"heading": "A.3 HSE-PSR", "text": "We define a class of non-parametric two-stage instrumental regression models. By using conditional mean embedding [21] as S1 regression model, we recover a single-action variant of HSE-PSR [5]. Let X ,Y ,Z denote three reproducing kernel Hilbert spaces with reproducing kernels kX , kY and kZ respectively. Assume \u03c8t \u2208 X and that \u03bet \u2208 Y is defined as the tuple (ot \u2297 ot, \u03c8t+1 \u2297 ot). Let \u03a8 \u2208 X \u2297RN , \u039e \u2208 Y \u2297RN and H \u2208 Z \u2297RN be operators that represent training data. Specifically, \u03c8s, \u03bes, hs are the sth \u201dcolumns\u201d in \u03a8 and \u039e and H respectively. It is possible to implement S1 using a non-parametric regression method that takes the form of a linear smoother. In such case the training data for S2 regression take the form\nE\u0302[\u03c8t | ht] = N\u2211 s=1 \u03b2s|ht\u03c8s\nE\u0302[\u03bet | ht] = N\u2211 s=1 \u03b3s|ht\u03bes,\nwhere \u03b2s and \u03b3s depend on ht. This produces the following training operators for S2 regression:\n\u03a8\u0303 = \u03a8B\n\u039e\u0303 = \u039e\u0393,\nwhere Bst = \u03b2s|ht and \u0393st = \u03b3s|ht . With this data, S2 regression uses a Gram matrix formulation to estimate the operator\nW = \u039e\u0393(B>GX ,XB + \u03bbIN) \u22121B>\u03a8\u2217 (A.8)\nNote that we can use an arbitrary method to estimate B. Using conditional mean maps, the weight matrix B is computed using kernel ridge regression\nB = (GZ,Z + \u03bbIN) \u22121GZ,Z (A.9)\nHSE-PSR learning is similar to this setting, with \u03c8t being a conditional expectation operator of test observations given test actions. For this reason, kernel ridge regression is replaced by application of kernel Bayes rule [9].\nFor each t, S1 regression will produce a denoised prediction E\u0302[\u03bet | ht] as a linear combination of training feature maps\nE\u0302[\u03bet | ht] = \u039e\u03b1t = N\u2211 s=1 \u03b1t,s\u03bes\nThis corresponds to the covariance operators\n\u03a3\u0302\u03c8t+1ot|ht = N\u2211 s=1 \u03b1t,s\u03c8s+1 \u2297 os = \u03a8\u2032diag(\u03b1t)O\u2217\n\u03a3\u0302otot|ht = N\u2211 s=1 \u03b1t,sos \u2297 os = Odiag(\u03b1t)O\u2217\nWhere, \u03a8\u2032 is the shifted future training operator satisfying \u03a8\u2032et = \u03c8t+1 Given these two covariance operators, we can use kernel Bayes rule [9] to condition on ot which gives\nQt+1 = E\u0302[\u03c8t+1 | ht] = \u03a3\u0302\u03c8t+1ot|ht(\u03a3\u0302otot|ht + \u03bbI)\u22121ot. (A.10)\nReplacing ot in (A.10) with its conditional expectation \u2211N\ns=1 \u03b1sos corresponds to marginalizing over ot (i.e. prediction). A stable Gram matrix formulation for (A.10) is given by [9]\nQt+1\n= \u03a8\u2032diag(\u03b1t)GO,O((diag(\u03b1t)GO,O) 2 + \u03bbNI)\u22121\n.diag(\u03b1t)O \u2217ot+1\n= \u03a8\u2032\u03b1\u0303t+1, (A.11)\nwhich is the state update equation in HSE-PSR. Given \u03b1\u0303t+1 we perform S2 regression to estimate\nP\u0302t+1 = E\u0302[\u03bet+1 | o1:t+1] = \u039e\u03b1t+1 = W\u03a8\u2032\u03b1\u0303t+1,\nwhere W is defined in (A.8)."}, {"heading": "B Proofs", "text": ""}, {"heading": "B.1 Proof of Main Theorem", "text": "In this section we provide a proof for theorem 2. We provide finite sample analysis of the effects of S1 regression, covariance estimation and regularization. The asymptotic statement becomes a natural consequence.\nWe will make use of matrix Bernstein\u2019s inequality stated below:\nLemma B.1 (Matrix Bernstein\u2019s Inequality [12]). Let A be a random square symmetric matrix, and r > 0, v > 0 and k > 0 be such that, almost surely,\nE[A] = 0, \u03bbmax[A] \u2264 r, \u03bbmax[E[A2]] \u2264 v, tr(E[A2]) \u2264 k.\nIf A1, A2, . . . , AN are independent copies of A, then for any t > 0,\nPr [ \u03bbmax [ 1\nN N\u2211 t=1 At\n] > \u221a 2vt\nN +\nrt\n3N ] \u2264 kt\nv (et \u2212 t\u2212 1)\u22121. (B.1)\nIf t \u2265 2.6, then t(et \u2212 t\u2212 1)\u22121 \u2264 e\u2212t/2.\nRecall that, assuming xtest \u2208 R(\u03a3x\u0304x\u0304), we have three sources of error: first, the error in S1 regression causes the input to S2 regression procedure (x\u0302t, y\u0302t) to be a perturbed version of the true (x\u0304t, y\u0304t); second, the covariance operators are estimated from a finite sample of size N ; and third, there is the effect of regularization. In the proof, we characterize the effect of each source of error. To do so, we define the following intermediate quantities:\nW\u03bb = \u03a3y\u0304x\u0304 (\u03a3x\u0304x\u0304 + \u03bbI) \u22121 (B.2)\nW\u0304\u03bb = \u03a3\u0302y\u0304x\u0304 ( \u03a3\u0302x\u0304x\u0304 + \u03bbI )\u22121 , (B.3)\nwhere\n\u03a3\u0302y\u0304x\u0304 \u2261 1\nN N\u2211 t=1 y\u0304t \u2297 x\u0304t\nand \u03a3\u0302x\u0304x\u0304 is defined similarly. Basically, W\u03bb captures only the effect of regularization and W\u0304\u03bb captures in addition the effect of finite sample estimate of the covariance. W\u0304\u03bb is the result of S2 regression if x\u0304 and y\u0304 were perfectly recovered by S1 regression. It is important to note that \u03a3\u0302x\u0304y\u0304 and \u03a3\u0302x\u0304x\u0304 are not observable quantities since they depend on the true expectations x\u0304 and y\u0304. We will use \u03bbxi and \u03bbyi to denote the ith eigenvalue of \u03a3x\u0304x\u0304 and \u03a3y\u0304y\u0304 respectively in descending order and we will use \u2016.\u2016 to denote the operator norm.\nBefore we prove the main theorem, we define the quantities \u03b6 x\u0304x\u0304\u03b4,N and \u03b6 x\u0304y\u0304 \u03b4,N which we use to bound the effect of covariance estimation from finite data, as stated in the following lemma:\nLemma B.2 (Covariance error bound). Let N be a positive integer and \u03b4 \u2208 (0, 1) and assume that \u2016x\u0304\u2016, \u2016y\u0304\u2016 < c <\u221e almost surely. Let \u03b6 x\u0304y\u0304\u03b4,N be defined as:\n\u03b6 x\u0304y\u0304\u03b4,N =\n\u221a 2vt\nN +\nrt\n3N , (B.4)\nwhere\nt = max(2.6, 2 log(4k/\u03b4v))\nr = c2 + \u2016\u03a3y\u0304x\u0304\u2016 v = c2 max(\u03bby1, \u03bbx1) + \u2016\u03a3x\u0304y\u0304\u20162 k = c2(tr(\u03a3x\u0304x\u0304) + tr(\u03a3y\u0304y\u0304))\nIn addition, let \u03b6 x\u0304x\u0304\u03b4,N be defined as:\n\u03b6 x\u0304x\u0304\u03b4,N =\n\u221a 2v\u2032t\u2032\nN + r\u2032t\u2032 3N , (B.5)\nwhere\nt\u2032 = max(2.6, 2 log(4k\u2032/\u03b4v\u2032)) r\u2032 = c2 + \u03bbx1 v\u2032 = c2\u03bbx1 + \u03bb 2 x1 k\u2032 = c2tr(\u03a3x\u0304x\u0304)\nand define \u03b6 y\u0304y\u0304\u03b4,N similarly for \u03a3y\u0304y\u0304.\nIt follows that, with probability at least 1\u2212 \u03b4/2,\n\u2016\u03a3\u0302y\u0304x\u0304 \u2212 \u03a3y\u0304x\u0304\u2016 < \u03b6 x\u0304y\u0304\u03b4,N \u2016\u03a3\u0302x\u0304x\u0304 \u2212 \u03a3x\u0304x\u0304\u2016 < \u03b6 x\u0304x\u0304\u03b4,N \u2016\u03a3\u0302y\u0304y\u0304 \u2212 \u03a3y\u0304y\u0304\u2016 < \u03b6 y\u0304y\u0304\u03b4,N\nProof. We show that each statement holds with probability at least 1 \u2212 \u03b4/6. The claim then follows directly from the union bound. We start with \u03b6 x\u0304x\u0304\u03b4,N . By setting At = x\u0304t \u2297 x\u0304t \u2212 \u03a3x\u0304x\u0304 then we would like to obtain a high probability bound on \u2016 1 N \u2211N t=1At\u2016. Lemma B.1 shows that, in order to satisfy the bound with probability at least 1\u2212 \u03b4/6, it suffices to set t to max(2.6, 2k log(6/\u03b4v)). So, it remains to find suitable values for r, v and k:\n\u03bbmax[A] \u2264 \u2016x\u0304\u20162 + \u2016\u03a3x\u0304x\u0304\u2016 \u2264 c2 + \u03bbx1 = r\u2032\n\u03bbmax[E[A2]] = \u03bbmax[E[\u2016x\u0304\u20162(x\u0304\u2297 x\u0304)\u2212 (x\u0304\u2297 x\u0304)\u03a3x\u0304x\u0304 \u2212 \u03a3x\u0304x\u0304(x\u0304\u2297 x\u0304) + \u03a3x\u0304x\u03042] = \u03bbmax[E[\u2016x\u0304\u20162(x\u0304\u2297 x\u0304)\u2212 \u03a3x\u0304x\u03042]] \u2264 c2\u03bbx1 + \u03bb2x1 = v\u2032\ntr[E[A2]] = tr[E[\u2016x\u0304\u20162(x\u0304\u2297 x\u0304)\u2212 \u03a3x\u0304x\u03042]] \u2264 tr[E[\u2016x\u0304\u20162(x\u0304\u2297 x\u0304)]] \u2264 c2tr(\u03a3x\u0304x\u0304) = k\u2032\nThe case of \u03b6 y\u0304y\u0304\u03b4,N can be proven similarly. Now moving to \u03b6 x\u0304y\u0304 \u03b4,N , we have Bt = y\u0304t\u2297x\u0304t\u2212\u03a3y\u0304x\u0304. SinceBt is not square, we use the Hermitian dilation H (B) defined as follows[24]:\nA = H (B) = [ 0 B B\u2217 0 ] Note that\n\u03bbmax[A] = \u2016B\u2016, A2 = [ BB\u2217 0\n0 B\u2217B ] therefore suffices to bound \u2016 1\nN \u2211N t=1At\u2016 using an argument similar to that used\nin \u03b6 x\u0304x\u0304\u03b4,N case.\nTo prove theorem 2, we write\n\u2016W\u0302\u03bbxtest \u2212Wxtest\u2016Y \u2264 \u2016(W\u0302\u03bb \u2212 W\u0304\u03bb)x\u0304test\u2016Y + \u2016(W\u0304\u03bb \u2212W\u03bb)x\u0304test\u2016Y + \u2016(W\u03bb \u2212W )x\u0304test\u2016Y (B.6)\nWe will now present bounds on each term. We consider the case where x\u0304test \u2208 R(\u03a3x\u0304x\u0304). Extension to R(\u03a3x\u0304x\u0304) is a result of the assumed boundedness of W , which implies the boundedness of W\u0302\u03bb \u2212W .\nLemma B.3 (Error due to S1 Regression). Assume that \u2016x\u0304\u2016, \u2016y\u0304\u2016 < c < \u221e almost surely, and let \u03b7\u03b4,N be as defined in Definition 1. The following holds with probability at least 1\u2212 \u03b4\n\u2016W\u0302\u03bb \u2212 W\u0304\u03bb\u2016 \u2264 \u221a \u03bby1 + \u03b6 y\u0304y\u0304 \u03b4,N (2c\u03b7\u03b4,N + \u03b7\u03b4,N 2)\n\u03bb 3 2\n+ (2c\u03b7\u03b4,N + \u03b7\u03b4,N\n2)\n\u03bb\n= O \u03b7\u03b4,N 1 \u03bb + \u221a 1 + log(1/\u03b4)\u221a N \u03bb 3 2  ."}, {"heading": "The asymptotic statement assumes \u03b7\u03b4,N \u2192 0 as N \u2192\u221e.", "text": "Proof. Write \u03a3\u0302x\u0302x\u0302 = \u03a3\u0302x\u0304x\u0304 + \u2206x and \u03a3\u0302y\u0302x\u0302 = \u03a3\u0302y\u0304y\u0304x + \u2206yx. We know that, with probability at least 1 \u2212 \u03b4/2, the following is satisfied for all unit vectors \u03c6x \u2208 X and \u03c6y \u2208 Y\n\u3008\u03c6y,\u2206yx\u03c6x\u3009Y = 1\nN N\u2211 t=1 \u3008\u03c6y, y\u0302t\u3009Y\u3008\u03c6x, x\u0302t\u3009X\n\u2212 \u3008\u03c6y, y\u0302t\u3009Y\u3008\u03c6x, x\u0304t\u3009X + \u3008\u03c6y, y\u0302t\u3009Y\u3008\u03c6x, x\u0304t\u3009X \u2212 \u3008\u03c6y, y\u0304t\u3009Y\u3008\u03c6x, x\u0304t\u3009X = 1\nN \u2211 t \u3008\u03c6y, y\u0304t + (y\u0302t \u2212 y\u0304t)\u3009Y\u3008\u03c6x, x\u0302t \u2212 x\u0304t\u3009X\n+ \u3008\u03c6y, y\u0302t \u2212 y\u0304t\u3009Y\u3008\u03c6x, x\u0304t\u3009X \u2264 2c\u03b7\u03b4,N + \u03b72\u03b4,N\nTherefore,\n\u2016\u2206yx\u2016 = sup \u2016\u03c6x\u2016X\u22641,\u2016\u03c6y\u2016Y\u22641 \u3008\u03c6y,\u2206yx\u03c6x\u3009Y \u2264 2c\u03b7\u03b4,N + \u03b7 2 \u03b4,N ,\nand similarly\n\u2016\u2206x\u2016 \u2264 2c\u03b7\u03b4,N + \u03b7\u03b4,N 2,\nwith probability 1\u2212 \u03b4/2. We can write\nW\u0302\u03bb \u2212 W\u0304\u03bb = \u03a3\u0302y\u0304x\u0304 ( (\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI) \u22121 \u2212 (\u03a3\u0302x\u0304x\u0304 + \u03bbI)\u22121 ) + \u2206yx(\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\nUsing the fact that B\u22121 \u2212A\u22121 = B\u22121(A\u2212B)A\u22121 for invertible operators A and B we get\nW\u0302\u03bb \u2212 W\u0304\u03bb = \u2212\u03a3\u0302y\u0304x\u0304(\u03a3\u0302x\u0304x\u0304 + \u03bbI)\u22121\u2206x(\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI)\u22121\n+ \u2206yx(\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\nwe then use the decomposition \u03a3\u0302y\u0304x\u0304 = \u03a3\u0302 1 2 y\u0304y\u0304V \u03a3\u0302 1 2 x\u0304x\u0304, where V is a correlation operator satisfying \u2016V \u2016 \u2264 1. This gives\nW\u0302\u03bb \u2212 W\u0304\u03bb =\n\u2212 \u03a3\u0302 1 2 y\u0304y\u0304V \u03a3\u0302 1 2 x\u0304x\u0304(\u03a3\u0302x\u0304x\u0304 + \u03bbI) \u2212 1 2 (\u03a3\u0302x\u0304x\u0304 + \u03bbI) \u2212 1 2 \u2206x(\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI) \u22121 + \u2206yx(\u03a3\u0302x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\nNoting that \u2016\u03a3\u0302 1 2 x\u0304x\u0304(\u03a3\u0302x\u0304x\u0304 +\u03bbI) \u2212 1 2\u2016 \u2264 1, the rest of the proof follows from triangular inequality and the fact that \u2016AB\u2016 \u2264 \u2016A\u2016\u2016B\u2016\nLemma B.4 (Error due to Covariance). Assuming that \u2016x\u0304\u2016X , \u2016y\u0304\u2016Y < c < \u221e almost surely, the following holds with probability at least 1\u2212 \u03b4\n2\n\u2016W\u0304\u03bb \u2212W\u03bb\u2016 \u2264 \u221a \u03bby1\u03b6 x\u0304x\u0304 \u03b4,N\u03bb \u2212 3 2 + \u03b6 x\u0304y\u0304\u03b4,N \u03bb\n, where \u03b6 x\u0304x\u0304\u03b4,N and \u03b6 x\u0304y\u0304 \u03b4,N are as defined in Lemma B.2.\nProof. Write \u03a3\u0302x\u0304x\u0304 = \u03a3x\u0304x\u0304 + \u2206x and \u03a3\u0302y\u0304x\u0304 = \u03a3y\u0304x\u0304 + \u2206yx. Then we get W\u0304\u03bb \u2212W\u03bb = \u03a3y\u0304x\u0304 ( (\u03a3x\u0304x\u0304 + \u2206x + \u03bbI) \u22121 \u2212 (\u03a3x\u0304x\u0304 + \u03bbI)\u22121 ) + \u2206yx(\u03a3x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\nUsing the fact that B\u22121 \u2212A\u22121 = B\u22121(A\u2212B)A\u22121 for invertible operators A and B we get\nW\u0304\u03bb \u2212W\u03bb = \u2212\u03a3y\u0304x\u0304(\u03a3x\u0304x\u0304 + \u03bbI)\u22121\u2206x(\u03a3x\u0304x\u0304 + \u2206x + \u03bbI)\u22121 + \u2206yx(\u03a3x\u0304x\u0304 + \u2206x + \u03bbI)\u22121\nwe then use the decomposition \u03a3y\u0304x\u0304 = \u03a3y\u0304y\u0304 1 2V \u03a3x\u0304x\u0304 1 2 , where V is a correlation operator satisfying \u2016V \u2016 \u2264 1. This gives\nW\u0304\u03bb \u2212W\u03bb =\n\u2212 \u03a3y\u0304y\u0304 1 2V \u03a3x\u0304x\u0304 1 2 (\u03a3x\u0304x\u0304 + \u03bbI) \u2212 1 2 (\u03a3x\u0304x\u0304 + \u03bbI) \u2212 1 2\n.\u2206x(\u03a3x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\n+ \u2206yx(\u03a3x\u0304x\u0304 + \u2206x + \u03bbI) \u22121\nNoting that \u2016\u03a3x\u0304x\u0304 1 2 (\u03a3x\u0304x\u0304+\u03bbI) \u2212 1 2\u2016 \u2264 1, the rest of the proof follows from triangular inequality and the fact that \u2016AB\u2016 \u2264 \u2016A\u2016\u2016B\u2016\nLemma B.5 (Error due to Regularization on inputs withinR(\u03a3x\u0304x\u0304)). For any x \u2208 R(\u03a3x\u0304x\u0304) s.t. \u2016x\u2016X \u2264 1 and \u2016\u03a3x\u0304x\u0304\u2212 1 2x\u2016X \u2264 C. The following holds\n\u2016(W\u03bb \u2212W )x\u2016Y \u2264 1\n2\n\u221a \u03bb\u2016W\u2016HSC\nProof. Since x \u2208 R(\u03a3x\u0304x\u0304) \u2286 R(\u03a3x\u0304x\u0304 1 2 ), we can write x = \u03a3x\u0304x\u0304 1 2v for some v \u2208 X s.t. \u2016v\u2016X \u2264 C. Then\n(W\u03bb \u2212W )x = \u03a3y\u0304x\u0304((\u03a3x\u0304x\u0304 + \u03bbI)\u22121 \u2212 \u03a3x\u0304x\u0304\u22121)\u03a3x\u0304x\u0304 1 2v\nLetD = \u03a3y\u0304x\u0304((\u03a3x\u0304x\u0304+\u03bbI)\u22121\u2212\u03a3x\u0304x\u0304\u22121)\u03a3x\u0304x\u0304 1 2 . We will bound the Hilbert-Schmidt norm of D. Let \u03c8xi \u2208 X , \u03c8yi \u2208 Y denote the eigenvector corresponding to \u03bbxi and \u03bbyi respectively. Define sij = |\u3008\u03c8yj,\u03a3x\u0304y\u0304\u03c8xi\u3009Y |. Then we have\n|\u3008\u03c8yj, D\u03c8xi\u3009Y | = \u2223\u2223\u2223\u2223\u2223\u3008\u03c8yj,\u03a3y\u0304x\u0304 \u03bb(\u03bbxi + \u03bb)\u221a\u03bbxi\u03c8xi\u3009Y \u2223\u2223\u2223\u2223\u2223\n= \u03bbsij\n(\u03bbxi + \u03bb) \u221a \u03bbxi = sij\u221a \u03bbxi 1 1\n\u03bb/\u03bbxi + 1\n\u2264 sij\u221a \u03bbxi . 1 2\n\u221a \u03bb\n\u03bbxi =\n1\n2 \u221a \u03bb sij \u03bbxi\n= 1\n2\n\u221a \u03bb|\u3008\u03c8yj,W\u03c8xi\u3009Y |,\nwhere the inequality follows from the arithmetic-geometric-harmonic mean inequality. This gives the following bound\n\u2016D\u20162HS = \u2211 i,j \u3008\u03c8yj, D\u03c8xi\u30092Y \u2264 1 2 \u221a \u03bb\u2016W\u20162HS\nand hence\n\u2016(W\u03bb \u2212W )x\u2016Y \u2264 \u2016D\u2016\u2016v\u2016X \u2264 \u2016D\u2016HS\u2016v\u2016X\n\u2264 1 2\n\u221a \u03bb\u2016W\u2016HSC\nNote that the additional assumption that \u2016\u03a3x\u0304x\u0304\u2212 1 2x\u2016X \u2264 C is not required to\nobtain an asymptotic O( \u221a \u03bb) rate for a given x. This assumption, however, allows us to uniformly bound the constant. Theorem 2 is simply the result of plugging the bounds in Lemmata B.3, B.4, and B.5 into (B.6) and using the union bound."}, {"heading": "B.2 Proof of Lemma 5", "text": "for t = 1: Let I be an index set over training instances such that\nQ\u0302test1 = 1 |I| \u2211 i\u2208I Q\u0302i\nThen\n\u2016Q\u0302test1 \u2212 Q\u0303test1 \u2016X = 1 |I| \u2211 i\u2208I \u2016Q\u0302i \u2212 Q\u0303i\u2016X \u2264 1 |I| \u2211 i\u2208I \u2016Q\u0302i \u2212Qi\u2016X \u2264 \u03b7\u03b4,N\nfor t > 1: Let A denote a projection operator onR\u22a5(\u03a3y\u0304y\u0304)\n\u2016Q\u0302testt+1 \u2212 Q\u0303testt+1\u2016X \u2264 L\u2016P\u0302 testt \u2212 P\u0303 testt \u2016Y \u2264 L\u2016AW\u0302\u03bbQ\u0302testt \u2016Y\n\u2264 L \u2225\u2225\u2225\u2225\u2225\u2225 1N ( N\u2211 i=1 AP\u0302i \u2297 Q\u0302i )( 1 N N\u2211 i=1 Q\u0302i \u2297 Q\u0302i + \u03bbI )\u22121\u2225\u2225\u2225\u2225\u2225\u2225 \u2225\u2225\u2225Q\u0302testt \u2225\u2225\u2225X\n\u2264 L \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 AP\u0302i \u2297 AP\u0302i \u2225\u2225\u2225\u2225\u2225 1 2 1\u221a \u03bb \u2016Q\u0302testt \u2016X \u2264 L \u03b7\u03b4,N\u221a \u03bb \u2016Q\u0302testt \u2016X ,\nwhere the second to last inequality follows from the decomposition similar to \u03a3Y X = \u03a3 1 2 Y V \u03a3 1 2 X , and the last inequality follows from the fact that \u2016AP\u0302i\u2016Y \u2264 \u2016P\u0302i \u2212 P\u0304i\u2016Y ."}], "references": [{"title": "Directional tuning profiles of motor cortical cells", "author": ["Bagrat Amirikian", "Apostolos P Georgopulos"], "venue": "Neuroscience research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Spectral Approaches to Learning Predictive Representations", "author": ["Byron Boots"], "venue": "PhD thesis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["Byron Boots", "Geoffrey Gordon"], "venue": "In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI-2011),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Two-manifold problems with applications to nonlinear system identification", "author": ["Byron Boots", "Geoffrey Gordon"], "venue": "In Proc. 29th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Hilbert Space Embeddings of Predictive State Representations", "author": ["Byron Boots", "Arthur Gretton", "Geoffrey J. Gordon"], "venue": "In Proc. 29th Intl. Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Closing the learning planning loop with predictive state representations", "author": ["Byron Boots", "Sajid Siddiqi", "Geoffrey Gordon"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity", "author": ["M Yu Byron", "John P Cunningham", "Gopal Santhanam", "Stephen I Ryu", "Krishna V Shenoy", "Maneesh Sahani"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Knowledge tracing: Modelling the acquisition of procedural knowledge", "author": ["Albert T. Corbett", "John R. Anderson"], "venue": "User Model. User-Adapt. Interact.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Kernel bayes\u2019 rule: Bayesian inference with positive definite kernels", "author": ["Kenji Fukumizu", "Le Song", "Arthur Gretton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Random design analysis of ridge regression", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Tail inequalities for sums of random matrices that depend on the intrinsic dimension", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Electronic Communications in Probability,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["Alan Julian Izenman"], "venue": "Journal of multivariate analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1975}, {"title": "Observable Operator Models for Discrete Stochastic Time Series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "A data repository for the EDM community: The PSLC DataShop", "author": ["Kenneth R. Koedinger", "R.S.J. Baker", "K. Cunningham", "A. Skogsholm", "B. Leber", "John Stamper"], "venue": "Handbook of Educational Data Mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning nonlinear dynamic models", "author": ["John Langford", "Ruslan Salakhutdinov", "Tong Zhang"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Time series and system analysis, with applications", "author": ["S.M. Pandit", "S.M. Wu"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1983}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Reduced-rank hidden Markov models", "author": ["Sajid Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proc. 27th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["Le Song", "Jonathan Huang", "Alexander J. Smola", "Kenji Fukumizu"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Introduction to Econometrics", "author": ["J.H. Stock", "M.W. Watson"], "venue": "Addison- Wesley series in economics. Addison-Wesley,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Direct cortical control of 3d neuroprosthetic devices", "author": ["Dawn M. Taylor", "Stephen I. Helms Tillery", "Andrew B. Schwartz"], "venue": "Science, pages 1829\u20131832,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["Joel A. Tropp"], "venue": "NIPS Tutorial,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "One of the main tools that algorithm designers use for this purpose is to expand the class of models considered, thereby removing difficult constraints from (1): for example, instead of learning a hidden Markov model (HMM), we can expand the model class to include all observable operator models (OOMs) [14].", "startOffset": 303, "endOffset": 307}, {"referenceID": 17, "context": "Our new view is based on instrumental variable regression [18, 22].", "startOffset": 58, "endOffset": 66}, {"referenceID": 21, "context": "Our new view is based on instrumental variable regression [18, 22].", "startOffset": 58, "endOffset": 66}, {"referenceID": 3, "context": ", in [4].", "startOffset": 5, "endOffset": 8}, {"referenceID": 21, "context": "We propose a generalization of the linear two stage ordinary least squares procedure [22], give error bounds for this generalization, and formulate dynamical systems learning as an instance of this regression technique.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": ", [6, 3].", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [6, 3].", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "In fact, it is also known that we can interpret this algorithm as linear instrumental-variable regression [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 17, "context": "To counteract this bias, we employ instrumental regression [18, 22].", "startOffset": 59, "endOffset": 67}, {"referenceID": 21, "context": "To counteract this bias, we employ instrumental regression [18, 22].", "startOffset": 59, "endOffset": 67}, {"referenceID": 15, "context": "\u03bet must satisfy 2Note that, similar to [16], Pt is a deterministic function of Qt and hence this condition has a unique solution if we observe sufficient examples of Pt and Qt.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 157, "endOffset": 165}, {"referenceID": 5, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 210, "endOffset": 216}, {"referenceID": 2, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 210, "endOffset": 216}, {"referenceID": 19, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 327, "endOffset": 331}, {"referenceID": 4, "context": "This work extends predictive state learning algorithms for dynamical systems, which include spectral algorithms for Kalman filters [2], Hidden Markov Models [10, 19] and Predictive State Representations (PSRs) [6, 3] as well as infinitedimensional variants such as the Hilbert space embedding of hidden Markov models (HSE-HMM) [20] and predictive state representations (HSE-PSR) [5].", "startOffset": 379, "endOffset": 382}, {"referenceID": 3, "context": "Boots and Gordon [4] note the connection between the HSE-HMM and instrumental variables, which is manifested in the use of kernel SVD of a future-past covariance operator to identify the latent state space.", "startOffset": 17, "endOffset": 20}, {"referenceID": 16, "context": "Reducing dynamical systems learning to supervised learning dates back to auto-regressive models [17], where the state of the system is assumed to be fully determined by the previous k observations.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "\u2019s sufficient posterior representation (SPR) [16], which encodes the state by the sufficient statistics of the conditional distribution of the next observation and represents system dynamics by three vector-valued functions that are estimated using supervised learning approaches.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "Finally, the theoretical analysis of [16] only establishes the consistency of SPR learning assuming that all regression steps are solved perfectly.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "The RKHS view is useful when the future statistics are represented in terms of kernels\u2014for example, if they are kernel mean maps of the distribution of future observations, a case that is closely related to the HSE-HMM [20] and HSE-PSR [5] models.", "startOffset": 219, "endOffset": 223}, {"referenceID": 4, "context": "The RKHS view is useful when the future statistics are represented in terms of kernels\u2014for example, if they are kernel mean maps of the distribution of future observations, a case that is closely related to the HSE-HMM [20] and HSE-PSR [5] models.", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Assume that x and y are kernel evaluation functionals, x\u0304 and \u0233 are linear vector functions of z where the linear operator is estimated using conditional mean embedding [21] with regularization parameter \u03bb0 > 0 and that \u2016x\u0304\u2016X , \u2016\u0233\u2016Y < c < \u221e almost surely.", "startOffset": 169, "endOffset": 173}, {"referenceID": 20, "context": "(sketch) This bound is based on [21], which gives a bound on the error in estimating the conditional mean embedding.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "We use the Bayesian knowledge tracing (BKT) model [8], which is essentially a 2-state HMM: the state st represents whether a student has learned a knowledge component (KC), and the observation ot represents the success/failure of solving the t question in a sequence of question that cover the said KC.", "startOffset": 50, "endOffset": 53}, {"referenceID": 14, "context": "1 Data Description The data set we used to evaluate the model is a publicly available data set from DataShop [15] called \u201cGeometry Area (1996-97).", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "In the appendix, we show that if use ht = ot\u22121 and linear regression as S1 regression model, the resulting algorithm is equivalent to spectral HMM method of [10] and thus we use it as a baseline.", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "Recently there has been substantial interest in predictive state methods for learning dynamical systems: these algorithms are popular since they often offer a good tradeoff between computational speed and statistical efficiency. Despite their desirable properties, though, predictive state methods can sometimes be difficult to use in practice. E.g., in contrast to the rich literature on supervised learning methods, which allows us to choose from an extensive menu of models and algorithms to suit the prior beliefs we have about properties of the function to be learned, predictive state dynamical system learning methods are comparatively inflexible: it is as if we were restricted to use only linear regression instead of being allowed to choose decision trees, nonparametric regression, or the lasso. To address this problem, we propose a new view of predictive state methods in terms of instrumentalvariable regression. This view allows us to construct a wide variety of dynamical system learners simply by swapping in different supervised learning methods. We demonstrate the effectiveness of our proposed methods by experimenting with non-linear regression to learn a hidden Markov model, showing that the resulting algorithm outperforms its linear counterpart; the correctness of this algorithm follows directly from our general analysis.", "creator": "TeX"}}}