{"id": "1505.05312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "A New Oscillating-Error Technique for Classifiers", "abstract": "this paper describes a new method for reducing the error in a classifier. it uses a weight adjustment update, but possibly includes the very simple rule either of either adding or subtracting the adjustment, based on whether the data point is currently larger or smaller locally than the desired value, and on a point - by - point basis. combining this gives added flexibility to the iteration convergence procedure, which where through a series of transpositions, values far away can continue towards the desired value, whereas corresponding values that are originally much greater closer can oscillate from one side to the other. tests show that the method can successfully classify some known datasets. it can also work in a batch mode, with reduced training times and can be used as part of a neural network, or classifiers in general. there are also simply some updates on an earlier proportional wave shape paper.", "histories": [["v1", "Wed, 20 May 2015 10:43:21 GMT  (405kb)", "http://arxiv.org/abs/1505.05312v1", null], ["v2", "Mon, 6 Jul 2015 18:50:41 GMT  (485kb)", "http://arxiv.org/abs/1505.05312v2", "Related work updates"], ["v3", "Fri, 30 Oct 2015 15:54:16 GMT  (548kb)", "http://arxiv.org/abs/1505.05312v3", "Error margin values were incorrect and some new test results"], ["v4", "Thu, 7 Apr 2016 16:51:57 GMT  (568kb)", "http://arxiv.org/abs/1505.05312v4", null], ["v5", "Mon, 31 Oct 2016 14:42:06 GMT  (582kb)", "http://arxiv.org/abs/1505.05312v5", null], ["v6", "Sat, 4 Feb 2017 19:34:18 GMT  (847kb)", "http://arxiv.org/abs/1505.05312v6", null], ["v7", "Tue, 10 Oct 2017 07:47:39 GMT  (853kb)", "http://arxiv.org/abs/1505.05312v7", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kieran greer"], "accepted": false, "id": "1505.05312"}, "pdf": {"name": "1505.05312.pdf", "metadata": {"source": "CRF", "title": "A New Oscillating-Error Technique for Classifiers", "authors": ["Kieran Greer"], "emails": [], "sections": [{"heading": null, "text": "adjustment update, but includes the very simple rule of either adding or subtracting the adjustment, based on whether the data point is currently larger or smaller than the desired value, and on a pointby-point basis. This gives added flexibility to the convergence procedure, where through a series of transpositions, values far away can continue towards the desired value, whereas values that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some known datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network, or classifiers in general. There are also some updates on an earlier wave shape paper.\nKeywords: classifier, oscillating error, transposition, matrix, neural network."}, {"heading": "1 Introduction", "text": "This paper describes a new method for reducing the error in a classifier. It uses a weight adjustment update, but includes the very simple rule of either adding or subtracting the adjustment, based on whether the data point is currently larger or smaller than the desired value, and on a point-by-point basis. Basically, if the data point is less than the desired value, the weight adjustment is added to it and if it is larger than the desired value, the weight adjustment is subtracted from it. This means that the data points of the same type could be treated differently through the rule, which gives added flexibility to the convergence procedure. Through a series of transpositions, values that are far away from the desired value can continue towards that point, whereas values that are originally much closer can reach the desired value and then oscillate from one side to the other, around it. The method is implemented here in matrix form, but would work in neural networks or classifiers in general and also in a batch update type of procedure. Another novel feature is\nthe fact that the weight value is added or subtracted, not multiplied. It is also calculated by using only the input and desired output. Therefore, it is not necessary to fine-tune the classifier with sets of random weights, or increment/decrement amounts, for example. A stopping criterion might be added however and the dataset might require some preprocessing first.\nThe rest of this paper is organised as follows: section 2 describes some related work and section 3 describes the theory behind the classifier. Section 4 runs through a very simple test example. Section 5 gives the result of some tests on real datasets, while section 6 gives some conclusions to the work."}, {"heading": "2 Related Work", "text": "Related work would include neural networks [6][8] and maybe the resonance type in particular [1]. For the author, the wave shape paper is important [5] and was the basis for this research. Cellular automata possibly have some relation as well [9][2]."}, {"heading": "3 Theory Behind the Algorithm", "text": "The theory of the new mechanism started with looking at the wave shape paper [5] again, which is described first with some new details. After that, the new oscillating error mechanism is described."}, {"heading": "3.1 Wave Shape Algorithm", "text": "This was proposed in [5] as an alternative way of looking at the relative input and output value sets. The idea was that the value differences would describe a type of wave shape and as resonance is known to be part of real neural networks, similar shapes could be combined in the synapses, as they would produce the same type of resonance. The design also uses average values, where both the input and the output can be summed and averaged over each column (all data rows), to represent each variable field with an average value. Tests do in fact show a substantial reduction in the error of the average input to the average output\nusing this method and even on established datasets, such as the Wine dataset [4][7]. The problem so far is that while the error can be reduced, it is reduced to an average output value that is not very accurate for each specific instance. For example, if the output values are 1, 2 and 3, then the input dataset can be averaged to produce a value close to 2, but this is not very helpful when comparing to 1 or 3. It is also worth noting that the shape values are probably mostly useful for the synapses. A similar sort of value can be achieved if you sum and average the values themselves, instead of the differences and this would probably be more appropriate for the neuron evaluator, as it needs to measure an exact error with the desired output value. The synapses however probably do need to consider shape more than actual values, where differences between values can change things. So possibly, modelling the network can consider that neurons and synapses are measuring a different type of quantity and for a different purpose \u2013 one to reinforce a type of signal (synapse) and one to produce a more exact result (neuron). As stated however, the network is too general at the moment and future work might try to make the mapping from the averaged input to the desired output more accurate."}, {"heading": "3.2 Oscillating-Error Method", "text": "This is the new algorithm of the paper and resulted from trying to make the input to output mapping of the last section more accurate. The new mechanism has again been tried on batch value sets, or summed and averaged values, as for section 3.1, but the convergence procedure is very different. If we take the starting point to be the summed and averaged values for each column of the input dataset, then we want to map this to the desired output value. We do this by measuring the error over each point in the averaged data row and calculating a weight adjustment for that point. This paper considers that each input data row belongs to a single category that is represented by a single value. In the case of the Wine dataset [4], the output therefore can be either \u20181\u2019, \u20182\u2019 or \u20183\u2019. So these represent the three different categories and can be represented by a single neuron or output element in the classifier. As the wave shape method is not accurate enough this way, the new method sums and averages for each category separately. In effect, it divides the dataset into batches, representing the rows for each category and produces an averaged data row for each of those groups. There are therefore three sets of input data, represented as single\naveraged data rows, one for each category. These then update the classifier separately, but the weight values produced can still be combined into a single weight value set, to be used in a more general setting. Other ideas with the wave shape were to use a weight value to extend or compress it, so that it matches better with other similar shapes and also to adjust it vertically, so that similar shapes on a slightly different scale can also match. Extending or compressing probably requires some middle line first ((max \u2013 min) / 2), around which you can try to do that and this would probably require the shape value to be adjusted vertically first, so that it can sit evenly around the middle value. So this new method is a simplification of that and only requires the vertical adjustment. It works as follows:\n1. Sum and average all input points for an entire column of data, for the selected output\ncategory. Do this for each variable field in the data rows, to produce an averaged data row. 2. Determine the related weight value by subtracting each point in the data row from the\ndesired output value (the output category). Make the weight value absolute. This produces an array of weight values, one for each field in the data row. The weight values are produced from the averaged input and output only, with no other variables required. 3. Store this weight array in a matrix, or some other 2-D structure, as a new layer. The\nweight arrays produce a set of transpositions over the input data points, to move them closer to the desired output value. 4. A total error for all of the averaged dataset categories can also be saved as part of the\nstopping criteria.\n5. On the next iteration, take the last set of weight adjusted inputs. For each point: if the\nvalue is smaller than the desired output value, then add the last calculated weight value related to it. If it is larger than the desired output value then subtract the related weight value from it. Save the new input set for the next iteration. 6. Subtract each newly calculated input value from the desired output again to produce a\nnew array of weight values.\n7. Go to 3 to store the new weight layer in the structure, and repeat the process until a\nstopping criterion is met. For example, the error does not reduce anymore, or number of iterations. 8. If there is more than one output category then there is more than one set of weights.\nThese can be added together at the end of each iteration and averaged to produce a general set of weights for the next iteration.\nThe input values that are very far from the desired one can continue to move towards it, while ones that are closer can start to oscillate around it and do not need to move away from it, but this requires the inclusion of a very simple rule. The weight adjustment is not always the same, where the algorithm needs to measure a relative size and can either add or subtract from values based on that. So this is really a new idea, but it is a very simple one, with a minimal disturbance to the mechanical or automatic nature of the process."}, {"heading": "4 Example Scenario", "text": "The following scenario traces through the process for a dataset with 5 fields in the averaged input data row and one output field. The output field averages to the value \u20184\u2019. The following steps show how the error would reduce at each iterative stage. If there is more than one output or category value, then the weight values can conflict and the error should not automatically reduce to 0, but it can still generalise well enough.\n Initial values: sum the input and average, then subtract from the desired output.\nInput column: 3, 8, 5, 10, 2 Output column: 4 Average error = Abs(4 \u2013 3), Abs(4 \u2013 8) , Abs(4 \u2013 5) , Abs(4 \u2013 10) , Abs(4 \u2013 2) Average error = 1, 4, 1, 6, 2\n Iteration 1: take input values and adjust vertically, by adding or subtracting the error.  3 is less than 4, so add 1 to it. 8 is larger than 4, so subtract 4 from it.  Subtract from the desired output again to get the new weight set.\nInput plus-minus error: 4, 4, 4, 4, 4 Average error = Abs(4 \u2013 4), Abs(4 \u2013 4) , Abs(4 \u2013 4) , Abs(4 \u2013 4) , Abs(4 \u2013 4) Average error = 0, 0, 0, 0, 0\nContinue until the stopping criterion is met. In this case, the error is now 0."}, {"heading": "5 Test Results", "text": "A test program has been written in the C# .Net language. It can read in a data file, normalise it, generate the classifier from it and measure how many categories it subsequently evaluates correctly. The classifier was tested on 3 datasets from the UCI Machine Learning Repository [7]. Recent work has been on categorical data, which is why these datasets have been used again. They are the Wine Recognition database [4], Iris Plants database [3] and the Zoo database [10]. These are categorical datasets with discrete output categories or values. Wine Recognition and Iris Plants have 3 categories, while the Zoo database has 7. The classifier was designed with only one output node however, which had to recognise the value 1, 2 or 3 (\u2026), as appropriate. Therefore, it had to adjust the input to more than one value for the output node. Two types of result were measured. The first was an average error for each data row, calculated as the output that it produced compared to the desired value. The second measurement was the minimum error margin required for the classifier to correctly classify all of the categories. The dataset was normalised first and so, for example, a 20% error margin would mean that a range of 0.8 \u2013 1.2 would be classified as the category 1. If selecting the closest category, it would still be 1, for example. A stopping criterion of 10 iterations was actually used to terminate the tests. So the error could probably have reduced further, but these results are still very impressive. It was also the case with the Wine dataset for example, that removing one third of the data, to produce a previously unseen test dataset would give similar errors.\nThese results show that the error has reduced to practically zero, but a small margin of error is required for all of the categories to be correctly classified. Only the best classifiers are 100% accurate with this classification problem however, so the result is still very good."}, {"heading": "6 Conclusions", "text": "This paper describes a new type of weight adjustment method that can be used as part of a neural network, or a classifier in general. The inclusion of a very simple comparison rule gives the mechanism much more power over weight updates that always have to follow exactly the same procedure. This is for values in the same field, or data column, in the dataset. Another feature is the fact that the weight is added or subtracted, not multiplied. Another potential advantage is the fact that it is calculated using only the input and the output values. Therefore, it is not necessary to fine-tune the classifier with sets of random weights, or increment/decrement weight amounts, to start with. A stopping criterion might be added however. The dataset might also need to be processed slightly. Normalisation might be an idea and a batch processing mode would require some form of separation of the data rows. It is thought that the weight adjustment performs a type of damping on the error, through transposition stages, which is only possible with the inclusion of the simple rule. There are probably several examples of this type of phenomenon in nature. The classifier has been implemented as a matrix, or 2-D array in this paper, but each layer might represent a level of a neural network instead, with transpositions between each level."}, {"heading": "Acknowledgement", "text": "The author wishes to acknowledge an email discussion with Charles Sauerbier of the US Navy, mainly because of its timing. He pointed out a belief that neural networks were a form of cellular automata and several other points, which the author did not fully appreciate, but the simple rule of this paper would push a neural element in that direction. The research itself however derived from a different place, looking at wave shapes and possibly some earlier ideas."}], "references": [{"title": "Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system", "author": ["G. Carpenter", "S. Grossberg", "D. Rosen"], "venue": "Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1991}, {"title": "Cellular Automata are Generic, U", "author": ["N. Dershowitz", "E. Falkovich"], "venue": "Dal Lago and R. Harmer (Eds.): Developments in Computational Models", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The use of multiple measurements in taxonomic problems, Annual Eugenics, 7, Part II, 179-188, also in 'Contributions to Mathematical Statistics", "author": ["R.A. Fisher"], "venue": "(John Wiley,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1936}, {"title": "PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies", "author": ["M Forina"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Artificial Neuron Modelling Based on Wave Shape, BRAIN", "author": ["K. Greer"], "venue": "Broad Research in Artificial Intelligence and Neuroscience,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Neural Networks: A Systematic Introduction", "author": ["R. Rojas"], "venue": "Springer-Verlag, Berlin and online at books.google.com,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Cellular Automata, Los Alamos science", "author": ["S. Wolfram"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}], "referenceMentions": [{"referenceID": 5, "context": "Related work would include neural networks [6][8] and maybe the resonance type in particular [1].", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "Related work would include neural networks [6][8] and maybe the resonance type in particular [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "For the author, the wave shape paper is important [5] and was the basis for this research.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "Cellular automata possibly have some relation as well [9][2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Cellular automata possibly have some relation as well [9][2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "The theory of the new mechanism started with looking at the wave shape paper [5] again, which is described first with some new details.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "This was proposed in [5] as an alternative way of looking at the relative input and output value sets.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "3 using this method and even on established datasets, such as the Wine dataset [4][7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "In the case of the Wine dataset [4], the output therefore can be either \u20181\u2019, \u20182\u2019 or \u20183\u2019.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "They are the Wine Recognition database [4], Iris Plants database [3] and the Zoo database [10].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "They are the Wine Recognition database [4], Iris Plants database [3] and the Zoo database [10].", "startOffset": 65, "endOffset": 68}], "year": 2015, "abstractText": "This paper describes a new method for reducing the error in a classifier. It uses a weight adjustment update, but includes the very simple rule of either adding or subtracting the adjustment, based on whether the data point is currently larger or smaller than the desired value, and on a pointby-point basis. This gives added flexibility to the convergence procedure, where through a series of transpositions, values far away can continue towards the desired value, whereas values that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some known datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network, or classifiers in general. There are also some updates on an earlier wave shape paper.", "creator": "Microsoft\u00ae Word 2010"}}}