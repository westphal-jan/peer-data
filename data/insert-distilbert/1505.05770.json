{"id": "1505.05770", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Variational Inference with Normalizing Flows", "abstract": "the choice of approximate posterior cumulative distribution coefficients is arguably one - of five the key core problems in variational inference. most applications of variational inference employ simple lie families of posterior approximations in order to allow for efficient inference, focusing on mean - field or other simple structured approximations. this restriction has garnered a significant useful impact on the quality of inferences made using variational methods. we introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior smoothed distributions. our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into exactly a more complex one by applying a sequence of invertible transformations until theoretically a desired level of complexity is attained. we use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior smoothing approximations. we demonstrate that the theoretical advantages of having weak posteriors that seek better match the true posterior, combined with the scalability of historically amortized variational approaches, provides a clear improvement required in computing performance and applicability aspects of variational utility inference.", "histories": [["v1", "Thu, 21 May 2015 15:36:37 GMT  (5463kb,D)", "http://arxiv.org/abs/1505.05770v1", null], ["v2", "Fri, 22 May 2015 09:13:28 GMT  (5463kb,D)", "http://arxiv.org/abs/1505.05770v2", "Proceedings of the 32nd International Conference on Machine Learning"], ["v3", "Tue, 26 May 2015 15:46:33 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v3", "Proceedings of the 32nd International Conference on Machine Learning"], ["v4", "Mon, 22 Jun 2015 18:36:32 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v4", "Proceedings of the 32nd International Conference on Machine Learning"], ["v5", "Mon, 13 Jun 2016 08:46:44 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v5", "Proceedings of the 32nd International Conference on Machine Learning"], ["v6", "Tue, 14 Jun 2016 09:01:36 GMT  (5485kb,D)", "http://arxiv.org/abs/1505.05770v6", "Proceedings of the 32nd International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG stat.CO stat.ME", "authors": ["danilo jimenez rezende", "shakir mohamed"], "accepted": true, "id": "1505.05770"}, "pdf": {"name": "1505.05770.pdf", "metadata": {"source": "META", "title": "Variational Inference with Normalizing Flows", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "emails": ["DANILOR@GOOGLE.COM", "SHAKIR@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "There has been a great deal of renewed interest in variational inference as a means of scaling probabilistic modeling to increasingly complex problems on increasingly larger data sets. Variational inference now lies at the core of large-scale topic models of text (Hoffman et al., 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al., 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ntool for the understanding of many physical and chemical systems. Despite these successes and ongoing advances, there are a number of disadvantages of variational methods that limit their power and hamper their wider adoption as a default method for statistical inference. It is one of these limitations, the choice of posterior approximation, that we address in this paper.\nVariational inference requires that intractable posterior distributions be approximated by a class of known probability distributions, over which we search for the best approximation to the true posterior. The class of approximations used is often limited, e.g., mean-field approximations, implying that no solution is ever able to resemble the true posterior distribution. This is a widely raised objection to variational methods, in that unlike other inferential methods such as MCMC, even in the asymptotic regime we are unable recover the true posterior distribution.\nThere is much evidence that richer, more faithful posterior approximations do result in better performance. For example, when compared to sigmoid belief networks that make use of mean-field approximations, deep auto-regressive networks use a posterior approximation with an autoregressive dependency structure that provides a clear improvement in performance (Mnih & Gregor, 2014). There is also a large body of evidence that describes the detrimental effect of limited posterior approximations. Turner & Sahani (2011) provide an exposition of two commonly experienced problems. The first is the widely-observed problem of under-estimation of the variance of the posterior distribution, which can result in poor predictions and unreliable decisions based on the chosen posterior approximation. The second is that the limited capacity of the posterior approximation can also result in biases in the MAP estimates of any model parameters (and this is the case e.g., in time-series models).\nA number of proposals for rich posterior approximations have been explored, typically based on structured meanfield approximations that incorporate some basic form of dependency within the approximate posterior. Another potentially powerful alternative would be to specify the approximate posterior as a mixture model, such as those developed by Jaakkola & Jordan (1998); Jordan et al. (1999);\nar X\niv :1\n50 5.\n05 77\n0v 1\n[ st\nat .M\nL ]\n2 1\nM ay\n2 01\n5\nGershman et al. (2012). But the mixture approach limits the potential scalability of variational inference since it requires evaluation of the log-likelihood and its gradients for each mixture component per parameter update, which is typically computationally expensive.\nThis paper presents a new approach for specifying approximate posterior distributions for variational inference. We begin by reviewing the current best practice for inference in general directed graphical models, based on amortized variational inference and efficient Monte Carlo gradient estimation, in section 2. We then make the following contributions: \u2022 We propose the specification of approximate poste-\nrior distributions using normalizing flows, a tool for constructing complex distributions by transforming a probability density through a series of invertible mappings (sect. 3). Inference with normalizing flows provides a tighter, modified variational lower bound with additional terms that only add terms with linear time complexity (sect 4). \u2022 We show that normalizing flows admit infinitesimal flows that allow us to specify a class of posterior approximations that in the asymptotic regime is able to recover the true posterior distribution, overcoming one oft-quoted limitation of variational inference. \u2022 We present a unified view of related approaches for improved posterior approximation as the application of special types of normalizing flows (sect 5). \u2022 We show experimentally that the use of general normalizing flows systematically outperforms other competing approaches for posterior approximation."}, {"heading": "2. Amortized Variational Inference", "text": "To perform inference it is sufficient to reason using the marginal likelihood of a probabilistic model, and requires the marginalization of any missing or latent variables in the model. This integration is typically intractable, and instead, we optimize a lower bound on the marginal likelihood. Consider a general probabilistic model with observations x, latent variables z over which we must integrate, and model parameters \u03b8. We introduce an approximate posterior distribution for the latent variables q\u03c6(z|x) and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:\nlog p\u03b8(x) = log \u222b p\u03b8(x|z)p(z)dz (1)\n= log \u222b q\u03c6(z|x) q\u03c6(z|x) p\u03b8(x|z)p(z)dz (2)\n\u2265\u2212IDKL[q\u03c6(z|x)\u2016p(z)]+Eq [log p\u03b8(x|z)]=\u2212F(x), (3)\nwhere we used Jensen\u2019s inequality to obtain the final equation, p\u03b8(x|z) is a likelihood function and p(z) is a prior\nover the latent variables. We can easily extend this formulation to posterior inference over the parameters \u03b8, but we will focus on inference over the latent variables only. This bound is often referred to as the negative free energy F or as the evidence lower bound (ELBO). It consists of two terms: the first is the KL divergence between the approximate posterior and the prior distribution (which acts as a regularizer), and the second is a reconstruction error. This bound (3) provides a unified objective function for optimization of both the parameters \u03b8 and \u03c6 of the model and variational approximation, respectively.\nCurrent best practice in variational inference performs this optimization using mini-batches and stochastic gradient descent, which is what allows variational inference to be scaled to problems with very large data sets. There are two problems that must be addressed to successfully use the variational approach: 1) efficient computation of the derivatives of the expected loglikelihood \u2207\u03c6Eq\u03c6(z)[log p\u03b8(x|z)], and 2) choosing the richest, computationally-feasible approximate posterior distribution q(\u00b7). The second problem is the focus of this paper. To address the first problem, we make use of two tools: Monte Carlo gradient estimation and inference networks, which when used together is what we refer to as amortized variational inference."}, {"heading": "2.1. Stochastic Backpropagation", "text": "The bulk of research in variational inference over the years has been on ways in which to compute the gradient of the expected log-likelihood \u2207\u03c6Eq\u03c6(z)[log p(x|z)]. Whereas we would have previously resorted to local variational methods (Bishop, 2006), in general we now always compute such expectations using Monte Carlo approximations (including the KL term in the bound, if it is not analytically known). This forms what has been aptly named doublystochastic estimation (Titsias & Lazaro-Gredilla, 2014), since we have one source of stochasticity from the minibatch and a second from the Monte Carlo approximation of the expectation.\nWe focus on models with continuous latent variables, and the approach we take computes the required gradients using a non-centered reparameterization of the expectation (Papaspiliopoulos et al., 2003; Williams, 1992), combined with Monte Carlo approximation \u2014 referred to as stochastic backpropagation (Rezende et al., 2014). This approach has also been referred to or as stochastic gradient variational Bayes (SGVB) (Kingma & Welling, 2014) or as affine variational inference (Challis & Barber, 2012).\nStochastic backpropagation involves two steps:\n\u2022 Reparameterization. We reparameterize the latent variable in terms of a known base distribution and a differentiable transformation (such as a locationscale transformation or cumulative distribution func-\ntion). For example, if q\u03c6(z) is a Gaussian distribution N (z|\u00b5, \u03c32), with \u03c6 = {\u00b5, \u03c32}, then the location-scale transformation using the standard Normal as a base distribution allows us to reparameterize z as:\nz \u223c N (z|\u00b5, \u03c32)\u21d4 z = \u00b5+ \u03c3 , \u223c N (0, 1)\n\u2022 Backpropagation with Monte Carlo. We can now differentiate (backpropagation) w.r.t. the parameters \u03c6 of the variational distribution using a Monte Carlo approximation with draws from the base distribution:\n\u2207\u03c6Eq\u03c6(z)[f\u03b8(z)]\u21d4 EN ( |0,1)[\u2207\u03c6f\u03b8(\u00b5+ \u03c3 )] .\nA number of general purpose approaches based on Monte Carlo control variate (MCCV) estimators exist as an alternative to stochastic backpropagation, and allow for gradient computation with latent variables that may be continuous or discrete (Williams, 1992; Mnih & Gregor, 2014; Ranganath et al., 2013; Wingate & Weber, 2013). An important advantage of stochastic backpropagation is that, for models with continuous latent variables, it has the lowest variance among competing estimators."}, {"heading": "2.2. Inference Networks", "text": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlmu\u0308ller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014). An inference network is a model that learns an inverse map from observations to latent variables. Using an inference network, we avoid the need to compute per data point variational parameters, but can instead compute a set of global variational parameters \u03c6 valid for inference at both training and test time. This allows us to amortize the cost of inference by generalizing between the posterior estimates for all latent variables through the parameters of the inference network. The simplest inference models that we can use are diagonal Gaussian densities, q\u03c6(z|x) = N (z|\u00b5\u03c6(x),diag(\u03c32\u03c6(x))), where the mean function \u00b5\u03c6(x) and the standard-deviation function \u03c3\u03c6(x) are specified using deep neural networks."}, {"heading": "2.3. Deep Latent Gaussian Models", "text": "In this paper, we study deep latent Gaussian models (DLGM), which are a general class of deep directed graphical models that consist of a hierarchy of L layers of Gaussian latent variables zl for layer l. Each layer of latent variables is dependent on the layer above in a non-linear way, and for DLGMs, this non-linear dependency is specified by deep neural networks. The joint probability model is:\np(x, z1, . . . , zL) = p (x|f0(z1)) L\u220f l=1 p (zl|fl(zl+1)) (4)\nwhere the Lth Gaussian distribution is not dependent on any other random variables. The prior over latent variables is a unit Gaussian p(zl) = N (0, I) and the observation likelihood p\u03b8(x|z) is any appropriate distribution that is conditioned on z1 and is also parameterized by a deep neural network (figure 2). This model class is very general and includes other models such as factor analysis and PCA, non-linear factor analysis, and non-linear Gaussian belief networks as special cases (Rezende et al., 2014).\nDLGMs use continuous latent variables and is a model class perfectly suited to fast amortized variational inference using the lower bound (3) and stochastic backpropagation. The end-to-end system of DLGM and inference network can be viewed as an encoder-decoder architecture, and this is the perspective taken by Kingma & Welling (2014) who present this combination of model and inference strategy as a variational auto-encoder. The inference networks used in Kingma & Welling (2014); Rezende et al. (2014) are simple diagonal or diagonal-plus-low rank Gaussian distributions. The true posterior distribution will be more complex than this assumption allows for, and defining multimodal and constrained posterior approximations in a scalable manner remains a significant open problem in variational inference."}, {"heading": "3. Normalizing Flows", "text": "By examining the bound (3), we can see that the optimal variational distribution that allows IDKL[q\u2016p] = 0 is one for which q\u03c6(z|x) = p\u03b8(z|x), i.e. q matches the true posterior distribution. This possibility is obviously not realizable given the typically used q(\u00b7) distributions, such as independent Gaussians or other mean-field approximations. Indeed, one limitation of the variational methodology due to the available choices of approximating families, is that even in an asymptotic regime we can not obtain the true posterior. Thus, an ideal family of variational distributions q\u03c6(z|x) is one that is highly flexible, preferably flexible enough to contain the true posterior as one solution. One path towards this ideal is based on the principle of normalizing flows (Tabak & Turner, 2013; Tabak & VandenEijnden, 2010).\nA normalizing flow describes the transformation of a probability density through a sequence of invertible mappings. By repeatedly applying the rule for change of variables, the initial density \u2018flows\u2019 through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow."}, {"heading": "3.1. Finite Flows", "text": "The basic rule for transformation of densities considers an invertible, smooth mapping f : IRd \u2192 IRd with inverse\nf\u22121 = g, i.e. the composition g \u25e6 f(z) = z. If we use this mapping to transform a random variable z with distribution q(z), the resulting random variable z\u2032 = f(z) has a distribution :\nq(z\u2032) = q(z) \u2223\u2223\u2223\u2223det \u2202f\u22121\u2202z\u2032 \u2223\u2223\u2223\u2223 = q(z) \u2223\u2223\u2223\u2223det \u2202f\u2202z \u2223\u2223\u2223\u2223\u22121 , (5) where the last equality can be seen by applying the chain rule (inverse function theorem) and is a property of Jacobians of invertible functions. We can construct arbitrarily complex densities by composing several simple maps and successively applying (5). The density qK(z) obtained by successively transforming a random variable z0 with distribution q0 through a chain of K transformations fk is:\nzK = fK \u25e6 . . . \u25e6 f2 \u25e6 f1(z0) (6)\nln qK(zK) = ln q0(z0)\u2212 K\u2211 k=1 ln det \u2223\u2223\u2223\u2223\u2202fk\u2202zk \u2223\u2223\u2223\u2223 , (7)\nwhere equation (6) will be used throughout the paper as a shorthand for the composition fK(fK\u22121(. . . f1(x))). The path traversed by the random variables zk = fk(zk\u22121) with initial distribution q0(z0) is called the flow and the path formed by the successive distributions qk is a normalizing flow. A property of such transformations, often referred to as the law of the unconscious statistician (LOTUS), is that expectations w.r.t. the transformed density qK can be computed without explicitly knowing qK . Any expectation EqK [h(z)] can be written as an expectation under q0 as:\nEqK [h(z)] = Eq0 [h(fK \u25e6 fK\u22121 \u25e6 . . . \u25e6 f1(z0))], (8)\nwhich does not require computation of the the logdetJacobian terms when h(z) does not depend on qK .\nWe can understand the effect of invertible flows as a sequence of expansions or contractions on the initial density. For an expansion, the map z\u2032 = f(z) pulls the points z away from a region in IRd, reducing the density in that region while increasing the density outside the region. Conversely, for a contraction, the map pushes points towards the interior of a region, increasing the density in its interior while reducing the density outside.\nThe formalism of normalizing flows now gives us a systematic way of specifying the approximate posterior distributions q(z|x) required for variational inference. With an appropriate choice of transformations fK , we can initially use simple factorized distributions such as an independent Gaussian, and apply normalizing flows of different lengths to obtain increasingly complex and multi-modal distributions."}, {"heading": "3.2. Infinitesimal Flows", "text": "It is natural to consider the case in which the length of the normalizing flow tends to infinity. In this case, we obtain\nan infinitesimal flow, that is described not in terms of a finite sequence of transformations \u2014 a finite flow, but as a partial differential equation describing how the initial density q0(z) evolves over \u2018time\u2019: \u2202\u2202tqt(z) = Tt[qt(z)], where T describes the continuous-time dynamics.\nLangevin Flow. One important family of flows is given by the Langevin stochastic differential equation (SDE):\ndz(t) = F(z(t), t)dt+ G(z(t), t)d\u03be(t), (9)\nwhere d\u03be(t) is a Wiener process with E[\u03bei(t)] = 0 and E[\u03bei(t)\u03bej(t\u2032)] = \u03b4i,j\u03b4(t \u2212 t\u2032), F is the drift vector and D = GG> is the diffusion matrix. If we transform a random variable z with initial density q0(z) through the Langevin flow (9), then the rules for the transformation of densities is given by the Fokker-Planck equation (or Kolmogorov equations in probability theory). The density qt(z) of the transformed samples at time t will evolve as:\n\u2202 \u2202t qt(z)=\u2212 \u2211 i \u2202 \u2202zi [Fi(z, t)qt]+ 1 2 \u2211 i,j \u22022 \u2202zi\u2202zj [Dij(z, t)qt] .\nIn machine learning, we most often use the Langevin flow with F (z, t) = \u2212\u2207zL(z) and G(z, t) = \u221a 2\u03b4ij , where L(z) is an unnormalised log-density of our model.\nImportantly, in this case the stationary solution for qt(z) is given by the Boltzmann distribution: q\u221e(z) \u221d e\u2212L(z). That is, if we start from an initial density q0(z) and evolve its samples z0 through the Langevin SDE, the resulting points z\u221e will be distributed according to q\u221e(z) \u221d e\u2212L(z), i.e. the true posterior. This approach has been explored for sampling from complex densities by Welling & Teh (2011); Ahn et al. (2012); Suykens et al. (1998).\nHamiltonian Flow. Hamiltonian Monte Carlo can also be described in terms of a normalizing flow on an augmented space z\u0303 = (z,\u03c9) with dynamics resulting from the HamiltonianH(z,\u03c9) = \u2212L(z)\u2212 12\u03c9\n>M\u03c9; HMC is also widely used in machine learning, e.g., Neal (2011). We will use the Hamiltonian flow to make a connection to the recently introduced Hamiltonian variational approach from Salimans et al. (2015) in section 5."}, {"heading": "4. Inference with Normalizing Flows", "text": "To allow for scalable inference using finite normalizing flows, we must specify a class of invertible transformations that can be used and an efficient mechanism for computing the determinant of the Jacobian. While it is straightforward to build invertible parametric functions for use in equation (5), e.g., invertible neural networks (Baird et al., 2005; Rippel & Adams, 2013), such approaches typically have a complexity for computing the Jacobian determinant that scales as O(LD3), where D is the dimension of the hidden layers and L is the number of hidden layers used. Furthermore, computing the gradients of the Jacobian determinant\ninvolves several additional operations that are alsoO(LD3) and involve matrix inverses that can be numerically unstable. We therefore require normalizing flows that allow for low-cost computation of the determinant, or where the Jacobian is not needed at all."}, {"heading": "4.1. Invertible Linear-time Transformations", "text": "We consider a family of transformations of the form:\nf(z) = z + uh(w>z + b), (10)\nwhere \u03bb = {w \u2208 IRD,u \u2208 IRD, b \u2208 IR} are free parameters and h(\u00b7) is a smooth element-wise non-linearity, with derivative h\u2032(\u00b7). For this mapping we can compute the logdet-Jacobian term in O(D) time (using the matrix determinant lemma):\n\u03c8(z) = h\u2032(w>z + b)w (11) det \u2223\u2223\u2223 \u2202f\u2202z \u2223\u2223\u2223 = |det(I + u\u03c8(z)>)| = |1 + u>\u03c8(z)|. (12)\nFrom (7) we conclude that the density qK(z) obtained by transforming an arbitrary initial density q0(z) through the sequence of maps fk of the form (10) is implicitly given by:\nzK = fK \u25e6 fK\u22121 \u25e6 . . . \u25e6 f1(z)\nln qK(zK) = ln q0(z)\u2212 K\u2211 k=1 ln |1 + u>k \u03c8k(zk)|. (13)\nThe flow defined by the transformation (13) modifies the initial density q0 by applying a series of contractions and expansions in the direction perpendicular to the hyperplane w>z+b = 0, hence we refer to these maps as planar flows.\nAs an alternative, we can consider a family of transformations that modify an initial density q0 around a reference point z0. The transformation family is:\nf(z) = z + \u03b2h(\u03b1, r)(z\u2212 z0), (14)\ndet \u2223\u2223\u2223\u2223\u2202f\u2202z \u2223\u2223\u2223\u2223 = [1 + \u03b2h(\u03b1, r)]d\u22121 [1 + \u03b2h(\u03b1, r) + h\u2032(\u03b1, r)r)] ,\nwhere r = |z \u2212 z0|, h(\u03b1, r) = 1/(\u03b1 + r), and the parameters of the map are \u03bb = {z0 \u2208 IRD, \u03b1 \u2208 IR, \u03b2 \u2208 IR}. This family also allows for linear-time computation of the determinant. It applies radial contractions and expansions around the reference point and are thus referred to as radial flows. We show the effect of expansions and contractions on a uniform and Gaussian initial density using the flows (10) and (14) in figure 1. This visualization shows that we can transform a spherical Gaussian distribution into a bimodal distribution by applying two successive transformations.\nNot all functions of the form (10) or (14) will be invertible. We discuss the conditions for invertibility and how to satisfy them in a numerically stable way in the appendix."}, {"heading": "4.2. Flow-Based Free Energy Bound", "text": "If we parameterize the approximate posterior distribution with a flow of length K, q\u03c6(z|x) := qK(zK), the free energy (3) can be written as an expectation over the initial distribution q0(z):\nF(x) = Eq\u03c6(z|x)[log q\u03c6(z|x)\u2212 log p(x, z)] = Eq0(z0) [ln qK(zK)\u2212 log p(x, zK)] = Eq0(z0) [ln q0(z0)]\u2212 Eq0(z0) [log p(x, zK)]\n\u2212 Eq0(z0) [ K\u2211 k=1 ln |1 + u>k \u03c8k(zk)| ] . (15)\nNormalizing flows and this free energy bound can be used with any variational optimization scheme, including generalized variational EM. For amortized variational inference, we construct an inference model using a deep neural network to build a mapping from the observations x to the parameters of the initial density q0 = N (\u00b5, \u03c3) (\u00b5 \u2208 IRD and \u03c3 \u2208 IRD) as well as the parameters of the flow \u03bb."}, {"heading": "4.3. Algorithm Summary and Complexity", "text": "The resulting algorithm is a simple modification of the amortized inference algorithm for DLGMs described by (Kingma & Welling, 2014; Rezende et al., 2014), which we summarize in algorithm 1. By using an inference net-\nAlgorithm 1 Variational Inf. with Normalizing Flows Parameters: \u03c6 variational, \u03b8 generative while not converged do\nx\u2190 {Get mini-batch} z0 \u223c q0(\u2022|x) zK \u2190 fK \u25e6 fK\u22121 \u25e6 . . . \u25e6 f1(z0) F(x) \u2248 F(x, zK) \u2206\u03b8 \u221d \u2212\u2207\u03b8F(x) \u2206\u03c6 \u221d \u2212\u2207\u03c6F(x)\nend while\nwork we are able to form a single computational graph which allows for easy computation of all the gradients of the parameters of the inference network and the generative model. The estimated gradients are used in conjunction with preconditioned stochastic gradient-based optimization methods such as RMSprop or AdaGrad (Duchi et al., 2010), where we use parameter updates of the form: (\u03b8t+1,\u03c6t+1) \u2190 (\u03b8t,\u03c6t) + \u0393t(gt\u03b8,gt\u03c6), with \u0393 is a diagonal preconditioning matrix that adaptively scales the gradients for faster minimization.\nThe algorithmic complexity of jointly sampling and computing the log-det-Jacobian terms of the inference model scales as O(LN2) + O(KD), where L is the number of deterministic layers used to map the data to the parameters of the flow, N is the average hidden layer size, K is the flow-length and D is the dimension of the latent variables. Thus the overall algorithm is at most quadratic making the overall approach competitive with other large-scale systems used in practice."}, {"heading": "5. Alternative Flow-based Posteriors", "text": "Using the framework of normalizing flows, we can provide a unified view of recent proposals for designing more flexible posterior approximations. At the outset, we distinguish between two types of flow mechanisms that differ in how the Jacobian is handled. The work in this paper considers general normalizing flows and presents a method for lineartime computation of the Jacobian. In contrast, volumepreserving flows design the flow such that its Jacobiandeterminant is equal to one while still allowing for rich posterior distributions. Both these categories allow for flows that may be finite or infinitesimal.\nThe Non-linear Independent Components Estimation (NICE) developed by Dinh et al. (2014) is an instance of a finite volume-preserving flow. The transformations used are neural networks f(\u00b7) with easy to compute inverses g(\u00b7) of the form:\nf(z) = (zA, zB + h\u03bb(zA)), (16) g(z\u2032) = (z\u2032A, z \u2032 B \u2212 h\u03bb(z\u2032A)). (17)\nwhere z = (zA, zB) is an arbitrary partitioning of the vec-\ntor z and h\u03bb is a neural network with parameters \u03bb. This form results in a Jacobian that has a zero upper triangular part, resulting in a determinant of 1. In order to build a transformation capable of mixing all components of the initial random variable z0, such flows must alternate between different partitionings of zk. The resulting density using the forward and inverse transformations is given by :\nln qK(fK \u25e6 fK\u22121 \u25e6 . . . \u25e6 f1(z0)) = ln q0(z0), (18) ln qK(z \u2032) = q0(g1 \u25e6 g2 \u25e6 . . . \u25e6 gK(z\u2032)). (19)\nWe will compare NICE to the general transformation approach described in section 2.1. Dinh et al. (2014) assume the partitioning is of the form z = [zA = z1:d, zB = zd+1:D]. To enhance mixing of the components in the flow, we introduce two mechanisms for mixing the components of z before separating them in the disjoint subgroups zA and zB . The first mechanism applies a random permutation (NICE-perm) and the second applies a random orthogonal transformation (NICE-orth)1.\nThe Hamiltonian variational approximation (HVI) developed by Salimans et al. (2015) is an instance of an infinitesimal volume-preserving flow. For HVI, we consider posterior approximations q(z,\u03c9|x) that make use of additional auxiliary variables\u03c9. The latent variables z are independent of the auxiliary variables \u03c9 and using the change of variables rule, the resulting distribution is: q(z\u2032,\u03c9\u2032) = |J|q(z)q(\u03c9), where z\u2032,\u03c9\u2032 = f(z,\u03c9) using a transformation f . Salimans et al. (2015) obtain a volume-preserving invertible transformation by exploiting the use of such transition operators in the MCMC literature, in particular the methods of Langevin and Hybrid Monte Carlo. This is an extremely elegant approach, since we now know that as the number of iterations of the transition function tends to infinity, the distribution q(z\u2032) will tend to the true distribution p(z|x). This is an alternative way to make use of the Hamiltonian infinitesimal flow described in section 3.2. A disadvantage of using the Langevin or Hamiltonian flow is that they require one or more evaluations of the likelihood and its gradients (depending in the number of leapfrog steps) per iteration during both training and test time."}, {"heading": "6. Results", "text": "Throughout this section we evaluate the effect of using normalizing flow-based posterior approximations for inference in deep latent Gaussian models (DLGMs). Training was performed by following a Monte Carlo estimate of the gradient of an annealed version of the free energy (20), with respect the model parameters \u03b8 and the variational parameters \u03c6 using stochastic backpropoagation. The Monte\n1 Random orthogonal transformations can be generated by sampling a matrix with independent unit-Gaussian entries Ai,j \u223c N (0, I) and then performing a QR-factorization. The resulting Q-matrix will be a random orthogonal matrix (Genz, 1998).\nTable 1. Test energy functions.\nPotential U(z)\n1: 1 2\n( \u2016z\u2016\u22122\n0.4\n)2 \u2212 ln ( e \u2212 1 2 [z1\u22122 0.6 ]2 + e \u2212 1 2 [z1+2 0.6 ]2) 2: 1\n2\n[ z2\u2212w1(z)\n0.4 ]2 3: \u2212 ln ( e \u2212 1 2 [z2\u2212w1(z) 0.35 ]2 + e \u2212 1 2 [z2\u2212w1(z)+w2(z) 0.35\n]2) 4: \u2212 ln ( e \u2212 1 2 [z2\u2212w1(z) 0.4 ]2 + e \u2212 1 2 [z2\u2212w1(z)+w3(z) 0.35\n]2) with w1(z) = sin ( 2\u03c0z1\n4\n) , w2(z) = 3e \u2212 1 2 [ (z1\u22121) 0.6 ]2 ,\nw3(z) = 3\u03c3 (z1\u22121\n0.3\n) and \u03c3(x) = 1/(1 + e\u2212x).\nCarlo estimate is computed using a single sample of the latent variables per data-point per parameter update.\nA simple annealed version of the free energy is used since this was found to provide better results. The modified bound is:\nzK = fK \u25e6 fK\u22121 \u25e6 . . . \u25e6 f1(z) F\u03b2t(x) = Eq0(z0) [ ln pK(zK)\u2212 log p(x, zK) ] = Eq0(z0) [ln q0(z0)]\u2212 \u03b2tEq0(z0) [log p(x, zK)]\n\u2212 Eq0(z0) [ K\u2211 k=1 ln |1 + uTk \u03c8k(zk)| ] (20)\nwhere \u03b2t \u2208 [0, 1] is an inverse temperature that follows a schedule \u03b2t = min(1, 0.01 + t/10000), going from 0.01 to 1 after 10000 iterations.\nThe deep neural networks that form the conditional probability between random variables consist of deterministic layers with 400 hidden units using the Maxout nonlinearity on windows of 4 variables (Goodfellow et al., 2013) . Briefly, the Maxout non-linearity with windowsize \u2206 takes an input vector x \u2208 IRd and computes: Maxout(x)k = maxi\u2208{\u2206k,\u2206(k+1)} xi for k = 0 . . . d/\u2206.\nWe use mini-batches of 100 data points and RMSprop optimization (with learning rate = 1 \u00d7 10\u22125 and momentum = 0.9) (Kingma & Welling, 2014; Rezende et al., 2014). Results were collected after 500, 000 parameter updates. Each experiment was repeated 100 times with different random seeds and we report the averaged scores and standard errors. The true marginal likelihood is estimated by importance sampling using 200 samples from the inference network as in (Rezende et al., 2014, App. E)."}, {"heading": "6.1. Representative Power of Normalizing Flows", "text": "To provide an insight into the representative power of density approximations based on normalizing flows, we parameterize a set of unnormalized 2D densities p(z) \u221d exp[\u2212U(z)] which are listed in table 1.\nIn figure 3(a) we show the true distribution for four cases,\nwhich show distributions that have characteristics such as multi-modality and periodicity that cannot be captured with typically-used posterior approximations.\nFigure 3(b) shows the performance of normalizing flow approximations for these densities using flow lengths of 2, 8 and 32 transformations. The non-linearity h(z) = tanh(z) in equation (10) was used for the mapping and the initial distribution was a diagonal Gaussian, q0(z) = N (z|\u00b5, \u03c32I). We see a substantial improvement in the approximation quality as we increase the flow length. Figure 3(c) shows the same approximation using the volumepreserving transformation used in NICE (Dinh et al., 2014) for the same number of transformations. We show summary statistics for the planar flow (13), and NICE (18) for random orthogonal matrices and with random permutation matrices in 3(d). We found that NICE and the planar flow (13) may achieve the same asymptotic performance as we grow the flow-length, but the planar flow (13) requires far fewer parameters. Presumably because all parameters of the flow (13) are learned, in contrast to NICE which requires an extra mechanism for mixing the components that is not learned but randomly initialized. We did not observe a substantial difference between using random orthogonal matrices or random permutation matrices in NICE."}, {"heading": "6.2. MNIST and CIFAR-10 Images", "text": "The MNIST digit dataset (LeCun & Cortes, 1998) contains 60,000 training and 10,000 test images of ten handwritten\ndigits (0 to 9) that are 28 \u00d7 28 pixels in size. We used the binarized dataset as in (Uria et al., 2014). We trained different DLGMs with 40 latent variables for 500, 000 parameter updates.\nThe performance of a DLGM using the (planar) normalizing flow (DLGM+NF) approximation is compared to the volume-preserving approaches using NICE (DLGM+NICE) on exactly the same model for different flow-lengths K, and we summarize the performance in figure 4. This graph shows that an increase in the flow-length systematically improves the bound F , as shown in figure 4(a), and reduces the KL-divergence between the approximate posterior q(z|x) and the true posterior distribution p(z|x) (figure 4(b)). It also shows that the approach using general normalizing flows outperforms that of NICE. We also show a wider comparison in table 2. Results are included for the Hamiltonian variational approach as well, but the model specification is different and thus gives an indication of attainable performance for this approach on this data set.\nThe CIFAR-10 natural images dataset (Krizhevsky & Hinton, 2010) consists of 50,000 training and 10,000 test RGB images that are of size 3x32x32 pixels from which we extract 3x8x8 random patches. The color levels were converted to the range [ , 1 \u2212 ] with = 0.0001. Here we used similar DLGMs as used for the MNIST experiment,\nbut with 30 latent variables. Since this data is non-binary, we use a logit-normal observation likelihood, p(x|\u00b5,\u03b1) =\u220f i N (logit(xi)|\u00b5i,\u03b1i) xi(1\u2212xi) , where logit(x) = log x\n1\u2212x . We summarize the results in table 3 where we are again able to show that an increase in the flow length K systematically improves the test log-likelihoods, resulting in better posterior approximations."}, {"heading": "7. Conclusion and Discussion", "text": "In this work we developed a simple approach for learning highly non-Gaussian posterior densities by learning transformations of simple densities to more complex ones through a normalizing flow. When combined with an amortized approach for variational inference using inference networks and efficient Monte Carlo gradient estimation, we are able to show clear improvements over simple approximations on different problems. Using this view of normalizing flows, we are able to provide a unified perspective of other closely related methods for flexible posterior estimation that points to a wide spectrum of approaches for designing more powerful posterior approximations with different statistical and computational tradeoffs.\nAn important conclusion from the discussion in section 3 is that there exist classes of normalizing flows that allow us to create extremely rich posterior approximations for variational inference. With normalizing flows, we are able to show that in the asymptotic regime, the space of solutions is rich enough to contain the true posterior distribution. If we combine this with the local convergence and consistency results for maximum likelihood parameter estimation in certain classes of latent variables models (Wang & Titterington, 2004), we see that we are now able overcome the objections to using variational inference as a competitive and default approach for statistical inference. Making such statements rigorous is an important line of future research.\nNormalizing flows allow us to control the complexity of the posterior at run-time by simply increasing the flow length of the sequence. The approach we presented considered normalizing flows based on simple transformations of the form (10) and (14). These are just two of the many maps that can be used, and alternative transforms can be designed for posterior approximations that may require other constraints, e.g., a restricted support. An important avenue of future research lies in describing the classes of transformations that allow for different characteristics of the posterior and that still allow for efficient, linear-time computation.\nAckowledgements: We thank Charles Blundell, Theophane Weber and Daan Wierstra for helpful discussions."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "One-step neural network inversion with PDF learning and emulation", "author": ["L. Baird", "D. Smalenberger", "S. Ingkiriwang"], "venue": "In IJCNN,", "citeRegEx": "Baird et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baird et al\\.", "year": 2005}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer New York,", "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Affine independent variational inference", "author": ["E. Challis", "D. Barber"], "venue": "In NIPS,", "citeRegEx": "Challis and Barber,? \\Q2012\\E", "shortCiteRegEx": "Challis and Barber", "year": 2012}, {"title": "Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network", "author": ["P. Dayan"], "venue": null, "citeRegEx": "Dayan,? \\Q2000\\E", "shortCiteRegEx": "Dayan", "year": 2000}, {"title": "NICE: Non-linear independent components estimation", "author": ["L. Dinh", "D. Krueger", "Y. Bengio"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Methods for generating random orthogonal matrices", "author": ["A. Genz"], "venue": "Monte Carlo and Quasi-Monte Carlo Methods,", "citeRegEx": "Genz,? \\Q1998\\E", "shortCiteRegEx": "Genz", "year": 1998}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M. Hoffman", "D. Blei"], "venue": "In ICML,", "citeRegEx": "Gershman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2012}, {"title": "Amortized inference in probabilistic reasoning", "author": ["S.J. Gershman", "N.D. Goodman"], "venue": "In Annual Conference of the Cognitive Science Society,", "citeRegEx": "Gershman and Goodman,? \\Q2014\\E", "shortCiteRegEx": "Gershman and Goodman", "year": 2014}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "A. Mnih", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "Blei", "D. M", "C. Wang", "J. Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In Learning in graphical models,", "citeRegEx": "Jaakkola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1998}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Unpublished manuscript,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2010}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "MCMC using hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Noncentered parameterisations for hierarchical models and data augmentation", "author": ["O. Papaspiliopoulos", "G.O. Roberts", "M. Sk\u00f6ld"], "venue": "In Bayesian Statistics", "citeRegEx": "Papaspiliopoulos et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Papaspiliopoulos et al\\.", "year": 2003}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "High-dimensional probability estimation with deep density models", "author": ["O. Rippel", "R.P. Adams"], "venue": "arXiv preprint arXiv:1302.5125,", "citeRegEx": "Rippel and Adams,? \\Q2013\\E", "shortCiteRegEx": "Rippel and Adams", "year": 2013}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["T. Salimans", "D.P. Kingma", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning stochastic inverses", "author": ["A. Stuhlm\u00fcller", "J. Taylor", "N. Goodman"], "venue": "In NIPS, pp. 3048\u20133056,", "citeRegEx": "Stuhlm\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Stuhlm\u00fcller et al\\.", "year": 2013}, {"title": "Online learning Fokker-Planck machine", "author": ["J.A.K. Suykens", "H. Verrelst", "J. Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "Suykens et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Suykens et al\\.", "year": 1998}, {"title": "A family of nonparametric density estimation algorithms", "author": ["E.G. Tabak", "C.V. Turner"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Tabak and Turner,? \\Q2013\\E", "shortCiteRegEx": "Tabak and Turner", "year": 2013}, {"title": "Density estimation by dual ascent of the log-likelihood", "author": ["Tabak", "E. G", "E. Vanden-Eijnden"], "venue": "Communications in Mathematical Sciences,", "citeRegEx": "Tabak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tabak et al\\.", "year": 2010}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M. Titsias", "M. Lazaro-Gredilla"], "venue": "In ICML,", "citeRegEx": "Titsias and Lazaro.Gredilla,? \\Q2014\\E", "shortCiteRegEx": "Titsias and Lazaro.Gredilla", "year": 2014}, {"title": "Two problems with variational expectation maximisation for time-series models", "author": ["R.E. Turner", "M. Sahani"], "venue": "Bayesian Time series models,", "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "A deep and tractable density estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "In ICML,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Convergence and asymptotic normality of variational Bayesian approximations for exponential family models with missing values", "author": ["B. Wang", "D.M. Titterington"], "venue": "In UAI,", "citeRegEx": "Wang and Titterington,? \\Q2004\\E", "shortCiteRegEx": "Wang and Titterington", "year": 2004}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In ICML,", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Automated variational inference in probabilistic programming", "author": ["D. Wingate", "T. Weber"], "venue": "In NIPS Workshop on Probabilistic Programming,", "citeRegEx": "Wingate and Weber,? \\Q2013\\E", "shortCiteRegEx": "Wingate and Weber", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Variational inference now lies at the core of large-scale topic models of text (Hoffman et al., 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al.", "startOffset": 79, "endOffset": 101}, {"referenceID": 15, "context": ", 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al., 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 10, "context": ", 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default", "startOffset": 97, "endOffset": 140}, {"referenceID": 21, "context": ", 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default", "startOffset": 97, "endOffset": 140}, {"referenceID": 13, "context": "Another potentially powerful alternative would be to specify the approximate posterior as a mixture model, such as those developed by Jaakkola & Jordan (1998); Jordan et al. (1999); ar X iv :1 50 5.", "startOffset": 160, "endOffset": 181}, {"referenceID": 13, "context": "We introduce an approximate posterior distribution for the latent variables q\u03c6(z|x) and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:", "startOffset": 121, "endOffset": 142}, {"referenceID": 2, "context": "Whereas we would have previously resorted to local variational methods (Bishop, 2006), in general we now always compute such expectations using Monte Carlo approximations (including the KL term in the bound, if it is not analytically known).", "startOffset": 71, "endOffset": 85}, {"referenceID": 19, "context": "We focus on models with continuous latent variables, and the approach we take computes the required gradients using a non-centered reparameterization of the expectation (Papaspiliopoulos et al., 2003; Williams, 1992), combined with Monte Carlo approximation \u2014 referred to as stochastic backpropagation (Rezende et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 21, "context": ", 2003; Williams, 1992), combined with Monte Carlo approximation \u2014 referred to as stochastic backpropagation (Rezende et al., 2014).", "startOffset": 109, "endOffset": 131}, {"referenceID": 20, "context": "A number of general purpose approaches based on Monte Carlo control variate (MCCV) estimators exist as an alternative to stochastic backpropagation, and allow for gradient computation with latent variables that may be continuous or discrete (Williams, 1992; Mnih & Gregor, 2014; Ranganath et al., 2013; Wingate & Weber, 2013).", "startOffset": 241, "endOffset": 325}, {"referenceID": 24, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 21, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 4, "context": "A second important practice is that the approximate posterior distribution q\u03c6(\u00b7) is represented using a recognition model or inference network (Stuhlm\u00fcller et al., 2013; Rezende et al., 2014; Dayan, 2000; Gershman & Goodman, 2014; Kingma & Welling, 2014).", "startOffset": 143, "endOffset": 254}, {"referenceID": 21, "context": "This model class is very general and includes other models such as factor analysis and PCA, non-linear factor analysis, and non-linear Gaussian belief networks as special cases (Rezende et al., 2014).", "startOffset": 177, "endOffset": 199}, {"referenceID": 21, "context": "The inference networks used in Kingma & Welling (2014); Rezende et al. (2014) are simple diagonal or diagonal-plus-low rank Gaussian distributions.", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "This approach has been explored for sampling from complex densities by Welling & Teh (2011); Ahn et al. (2012); Suykens et al.", "startOffset": 93, "endOffset": 111}, {"referenceID": 0, "context": "This approach has been explored for sampling from complex densities by Welling & Teh (2011); Ahn et al. (2012); Suykens et al. (1998).", "startOffset": 93, "endOffset": 134}, {"referenceID": 18, "context": ", Neal (2011). We will use the Hamiltonian flow to make a connection to the recently introduced Hamiltonian variational approach from Salimans et al.", "startOffset": 2, "endOffset": 14}, {"referenceID": 18, "context": ", Neal (2011). We will use the Hamiltonian flow to make a connection to the recently introduced Hamiltonian variational approach from Salimans et al. (2015) in section 5.", "startOffset": 2, "endOffset": 157}, {"referenceID": 1, "context": ", invertible neural networks (Baird et al., 2005; Rippel & Adams, 2013), such approaches typically have a complexity for computing the Jacobian determinant that scales as O(LD), where D is the dimension of the hidden layers and L is the number of hidden layers used.", "startOffset": 29, "endOffset": 71}, {"referenceID": 21, "context": "The resulting algorithm is a simple modification of the amortized inference algorithm for DLGMs described by (Kingma & Welling, 2014; Rezende et al., 2014), which we summarize in algorithm 1.", "startOffset": 109, "endOffset": 155}, {"referenceID": 6, "context": "The estimated gradients are used in conjunction with preconditioned stochastic gradient-based optimization methods such as RMSprop or AdaGrad (Duchi et al., 2010), where we use parameter updates of the form: (\u03b8,\u03c6) \u2190 (\u03b8,\u03c6) + \u0393t(g\u03b8,g\u03c6), with \u0393 is a diagonal preconditioning matrix that adaptively scales the gradients for faster minimization.", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "The Non-linear Independent Components Estimation (NICE) developed by Dinh et al. (2014) is an instance of a finite volume-preserving flow.", "startOffset": 69, "endOffset": 88}, {"referenceID": 5, "context": "Dinh et al. (2014) assume the partitioning is of the form z = [zA = z1:d, zB = zd+1:D].", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "The Hamiltonian variational approximation (HVI) developed by Salimans et al. (2015) is an instance of an infinitesimal volume-preserving flow.", "startOffset": 61, "endOffset": 84}, {"referenceID": 23, "context": "The Hamiltonian variational approximation (HVI) developed by Salimans et al. (2015) is an instance of an infinitesimal volume-preserving flow. For HVI, we consider posterior approximations q(z,\u03c9|x) that make use of additional auxiliary variables\u03c9. The latent variables z are independent of the auxiliary variables \u03c9 and using the change of variables rule, the resulting distribution is: q(z\u2032,\u03c9\u2032) = |J|q(z)q(\u03c9), where z\u2032,\u03c9\u2032 = f(z,\u03c9) using a transformation f . Salimans et al. (2015) obtain a volume-preserving invertible transformation by exploiting the use of such transition operators in the MCMC literature, in particular the methods of Langevin and Hybrid Monte Carlo.", "startOffset": 61, "endOffset": 482}, {"referenceID": 7, "context": "The resulting Q-matrix will be a random orthogonal matrix (Genz, 1998).", "startOffset": 58, "endOffset": 70}, {"referenceID": 21, "context": "9) (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 3, "endOffset": 49}, {"referenceID": 5, "context": "Figure 3(c) shows the same approximation using the volumepreserving transformation used in NICE (Dinh et al., 2014) for the same number of transformations.", "startOffset": 96, "endOffset": 115}, {"referenceID": 23, "context": "Results below from (Salimans et al., 2015) DLGM + HVI (1 leapfrog step) 88.", "startOffset": 19, "endOffset": 42}, {"referenceID": 10, "context": "Results below from (Gregor et al., 2014) DARN nh = 500 84.", "startOffset": 19, "endOffset": 40}, {"referenceID": 30, "context": "We used the binarized dataset as in (Uria et al., 2014).", "startOffset": 36, "endOffset": 55}], "year": 2015, "abstractText": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "creator": "LaTeX with hyperref package"}}}