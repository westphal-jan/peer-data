{"id": "1505.06292", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2015", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "abstract": "we help propose and study a row - and - write column affine measurement scheme responsible for low - noise rank matrix recovery. each measurement chosen is a linear combination of elements in one particular row or one column of a binary matrix $ x $. ( this setting arises usually naturally easily in applications from different domains. however, current algorithms developed mostly for standard matrix recovery problems don't perform well in our case, hence the need especially for substantially developing new algorithms and theory updates for our problem. we propose a simple algorithm for the above problem based on singular value decomposition ( $ svd $ ) and least - squares ( $ ls $ ), which we term \\ alg. we prove that ( a simplified version consisted of ) our algorithm can recover $ x $ exactly with the minimum possible number of measurements expected in the noiseless dimensional case. in the general noisy case, we prove performance guarantees on the reconstruction accuracy under the frobenius norm. in simulations, our row - and - column design and \\ | alg algorithm show improved speed, variability and comparable and in some cases better accuracy measures compared to standard measurements designs and tensor algorithms. additionally our theoretical and experimental results suggest that the proposed row - and - column affine measurements scheme, together with our recovery algorithm, may independently provide a powerful framework for affine matrix reconstruction.", "histories": [["v1", "Sat, 23 May 2015 08:45:20 GMT  (463kb)", "http://arxiv.org/abs/1505.06292v1", "ICML 2015"]], "COMMENTS": "ICML 2015", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "authors": ["or zuk", "avishai wagner"], "accepted": true, "id": "1505.06292"}, "pdf": {"name": "1505.06292.pdf", "metadata": {"source": "CRF", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "authors": ["Avishai Wagner"], "emails": ["avishai.wagner@mail.huji.ac.il", "or.zuk@mail.huji.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n06 29\n2v 1\n[ cs\nWe propose and study a row-and-column affine measurement scheme for lowrank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and least-squares (LS), which we term SVLS . We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-andcolumn design and SVLS algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed rowand-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.\nKeywords: low-rank matrix recovery, row and column measurements, matrix completion, singular value decomposition"}, {"heading": "1 Introduction", "text": "In the low-rank affine matrix recovery problem, an unknown matrix X \u2208 Rn1\u00d7n2 with rank(X) = r is measured indirectly via an affine transformation A : Rn1\u00d7n2 \u2192 Rd\nand possibly with additive (typically Gaussian) noise z \u2208 Rd. Our goal is to recover X from the vector of noisy measurements b = A(X) + z. The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9]. The problem has been studied mathematically quite extensively in the last few years. Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.i.d. Gaussian weights [5, 22]. Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22]. However, it is desirable to study the problem with other affine transformations A beyond the two ensembles mentioned above for the following reasons: (i) In some applications we cannot control the measurements operator A, and different models for the measurements may be needed to allow a realistic analysis of the problem (ii) When we can control and design the measurement operator A, other measurement operators may outperform the two ensembles mentioned above with respect to optimizing different resources such as the number of measurements required, computation time and storage. The main goal of this paper is to present and study a different set of affine transformations, which we term row-and-column affine measurements. This setting may arise naturally in many applications, since it is often natural and possibly cheap to measure a single row or column of a matrix, or a linear combination of a few such rows and columns. For example, (i) In collaborative filtering, we may wish to recover a users-items preference matrix and have access to only a subset of the users, but can observe their preference scores for all items (ii) When recovering a protein-RNA interactions matrix in molecular biology, a single experiment may simultaneously measure the interactions of a specific protein with all RNA molecules [10].\nIn general, we can represent any affine transformation A in matrix representation A(X) = Avec(X), where vec(X) is a column vector obtained by stacking all columns of X on top of each other. In our row and column framework the measurement operator A is represented differently using two matrices A(R), A(C) which multiply X as a matrix (rather than multiplying the vector vec(X)) from left and right, respectively. We focus on two ensembles of A(R), A(C): (i) Matrix Completion from single Columns and Rows (RCMC). Here we observe single matrix entries in similar to standard matrix\ncompletion case, however the measured entries are not scattered randomly along the matrix, but instead we sample a few rows and a few columns, and measure all entries in these rows and columns. This ensemble is implemented by setting the rows (columns) of A(R) (A(C)) as random vectors from the standard basis of Rn1 (Rn2). (ii) Gaussian Row-and-Column (GRC) measurements. Here each set of measurements is a weighted linear combination of the matrix\u2019s rows (or columns) with the weights taken as i.i.d. Gaussians. This ensemble is implemented by setting the entries of A(R), A(C) as i.i.d. Gaussian random variables.\nThe measurement operators A in our RCMC and GRC models do not satisfy standard requirements which hold for GE and MC. It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required. However, the specific algebraic structure provided by the row-and-column measurements, allows us to derive efficient and simple algorithms, and to analyze their performance. In addition, we provide extensive simulation results, which demonstrate the improved accuracy and speed of our approach over existing measurement designs and algorithms. All of our algorithms and simulations are implemented in a Matlab software package available at https://github.com/avishaiwa/SVLS."}, {"heading": "1.1 Prior Work", "text": "Before giving a detailed derivation and analysis of our design and algorithms, we give an overview of existing designs and their properties. We concentrate on two properties: (i) storage required in order to represent the measurement operator, and (ii) measurement sparsity, defined as the sum over all measurements of the number of matrix entries participating in each measurement, that is S(A) = ||vec(A)||0. The latter property may be related to measurement costs, as well as to computational time.\nIn the Gaussian Ensemble model, the entries of the matrix A in the matrix representationA(X) = Avec(X) are i.i.d. Gaussian random variables, Aij \u223c N(0, 1). For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants. Recovery in this model is robust to noise, with only a small increase in number of measurements. The main disadvantage of this model is that the design requires O(dn1n2) storage space for A, which could be problematic for large matrices. Another possible disadvantage of this method is that measurements are dense - each measurement represents a linear combination of all O(n1n2) matrix entries, and the overall measurement sparsity of A(X) is also O(dn1n2), which could be problematic for large n1, n2.\nIn the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18]. This model has the lowest storage requirements (O(d)) and measurement sparsity (O(d)). However, recovery guarantees for this model are weaker: setting n = max(n1, n2), it is shown that \u0398(nrlog(n)) measurements are required to recover a rank r matrix of size n1 \u00d7 n2 [8]. In addition, unique recovery from this number of measurements requires additional incoherence conditions on the matrix X , and recovery of matrices which fail to satisfy such conditions (e.g. matrices with a few spikes) may require a much larger number of measurements.\nRecently a new design of rank one projections was proposed [3], where each measurement is of the form \u03b1TX\u03b2 and such that \u03b1 \u2208 Rn1 , \u03b2 \u2208 Rn2 have i.i.d standard Gaussian entries. It was proven that nuclear norm minimization can recover X with high probability in this design from O(n1r + n2r) measurements. This is the first model deviating from MC and GE we are aware of. This model is different from our row-and-column model, as each measurement is obtained by multiplying X from both sides, whereas in our model each measurement is obtained by multiplying X from either left or right. Moreover, in our model the measurements are not chosen independently from each other but come in groups of size n1 or n2 (corresponding to rows or columns A(R), A(C)). An advantage of rank one projection is that it leads to a significance reduction in measurement storage needed for A with overall O(dn1 +dn2) storage space. However, each measurement is still dense and involve all matrix elements, hence measurement sparsity is O(dn1n2). In contrast, our GRC model requires only O(d) storage for A, and every measurement depends only on O(n) elements, leading to a reduced overall time for all measurements O(dn1 + dn2). For RCMC, we need only O(dlog(n)n ) storage for A, and measurement sparsity is O(d)."}, {"heading": "2 Preliminaries and Notations", "text": "We denote by Rn1\u00d7n2 the space of matrices of size n1 \u00d7 n2, by On1\u00d7n2 the space of matrices of size n1 \u00d7 n2 with orthonormal columns, and by M(r)n1\u00d7n2 the space of matrices of size n1 \u00d7 n2 and rank 6 r. We denote n = max(n1, n2).\nWe denote by || \u00b7 ||F the matrix Frobenius norm, by || \u00b7 ||\u2217 the nuclear norm, and by || \u00b7 ||2 the spectral norm. For a vector, || \u00b7 || denotes the standard l2 norm.\nFor X \u2208 Rn1\u00d7n2we denote by span(X) the subspace of Rn1 spanned by the columns of X and define PX to be the orthogonal projection into span(X).\nFor a matrix X we denote by Xi\u2022 the i-th row, by X\u2022j the j-th column and by Xij the (i, j) element. For two sets of indices I, J , we denote by XIJ the sub-matrix\nobtained by taking the rows with indices in I and columns with indices in J of X . We denote by [k] the set of indices 1, .., k. We denote by vec(X) the (column) vector obtained by stacking all the columns of X on top of each other.\nWe use the notation X i.i.d.\u223c G to denote a random matrix X with i.i.d. entries Xij \u223c G. For a rank-r matrix X \u2208 M(r)n1\u00d7n2 let X = U\u03a3V T be the Singular Value Decomposition (SVD) where U \u2208 On1\u00d7r, V \u2208 Or\u00d7n2 and \u03a3 = diag(\u03c31(X), ..., \u03c3r(X)) with \u03c31(X) \u2265 \u03c32(X) \u2265 .. \u2265 \u03c3r(X) > 0 the (non-zero) singular values of X (we omit the zero singular values and their corresponding vectors from the decomposition). For a general matrix X \u2208 Rn1\u00d7n2 we denote by X(r) the top-r singular value decomposition of X , X(r) = U\u2022[r]\u03a3[r][r]V T\u2022[r].\nOur model assumes two affine transformations applied to X , representing rows and columns, B(C,0) = XA(C) and B(R,0) = A(R)X, achieved by multiplications with two matrices A(R) \u2208 Rk(R)\u00d7n1 and A(C) \u2208 Rn2\u00d7k(C) , respectively. We obtain noisy observations of these transformations, B(R), B(C) obtained by applying additive noise:\nA(R)X + Z(R) = B(R) ; XA(C) + Z(C) = B(C) (1)\nwhere the total number of measurements is d = k(R)n1 + n2k(C), and Z(R) \u2208 Rn1\u00d7k(R) ,Z\n(C) \u2208 Rk(C)\u00d7n2 are two zero-mean noise matrices. Our goal is to recover X from the observed measurements B(C) and B(R). To achieve this goal, we define the squared loss function\nF(X) = ||A(R)X \u2212B(R)||2F + ||XA(C) \u2212B(C)||2F (2)\nand solve the least squares problem:\nMinimizeF(X) s.t. X \u2208 M(r)n1\u00d7n2 . (3)\nIf Z(R), Z(C) i.i.d.\u223c N(0, \u03c42) , minimizing the loss function in eq. (2) is equivalent to maximizing the log-likelihood of the data, giving a statistical motivation for the above score. Problem (3) is non-convex due to the non-convex rank constraint rank(X) \u2264 r. Our problem is a specialization of the general affine matrix recovery problem [22], in which a matrix is measured using a general affine transformation A with b = A(X) + z. We consider next and throughout the paper two specific random ensembles of measurement matrices:\n1. Row and Column Matrix Completion (RCMC): In this ensemble each row of A(R) and each column of A(C) is a vector of the standard basis ej for some j -\nthus each measurement B(R)ij or B (C) ij is obtained from a single entry of X . We define a row-inclusion probability p(R) and column inclusion probability p(C)\nsuch that each row (column) of X will be measured with probability p(R) (p(C)). More precisely, we define r1, .., rn1 i.i.d. Bernoulli variables, P (ri = 1) = p (R), and include ei as a row in A(R) if and only if ri = 1. Similarly, we define c1...cn2 i.i.d. Bernoulli variables, P (ci = 1) = p(C), and include ei as a column in A(C) if and only if ci = 1. The expected number of observed rows (columns) is k(R) = n1p (R) (k(C) = n2p(C)). The model is very similar to the possibly more natural model of picking k(R) distinct rows and k(C) distinct columns at random for fixed k(R), k(C), but allows for easier analysis.\n2. Gaussian Rows and Columns (GRC): In this ensembleA(R), A(C) i.i.d.\u223c N(0, 1). Each observation B(R)ij or B (C) ij is obtained by a weighted sum of a single row\nor column of X , with i.i.d. Gaussian weights."}, {"heading": "2.1 Comparison to Standard Designs", "text": "Our proposed rows-and-columns design differs from standard designs appearing in the literature. It is instructive to compare our GRC ensemble to the Gaussian Ensemble (GE) [5], with the matrix representation A(X) = Avec(X) where A \u2208 Rd\u00d7n1n2 and A\ni.i.d.\u223c N(0, 1). For the latter, the following r-Restricted Isometry Property (RIP) can be used:\nDefinition 1. (r-RIP) Let A : Rn1\u00d7n2 \u2192 Rd be a linear map. For every integer r with 1 \u2264 r \u2264 min(n1, n2), define the r-Restricted Isometry Constant to be the smallest number \u01ebr such that\n(1\u2212 \u01ebr)||X ||F \u2264 ||A(X)|| \u2264 (1 + \u01ebr)||X ||F (4)\nholds for all matrices X of rank at most r.\nThe GE model satisfies the r-RIP condition for d = O(rn) with high probability [22]. Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability. Unlike GE, in our GRC model A(X) doesn\u2019t satisfy the r-RIP, and nuclear norm minimization fails. Instead, A(R), A(C) preserve matrix Frobenius norm in high probability - a weaker property which holds for any low-rank matrix. (see Lemma 7 in the Appendix).\nWe next compare RCMC to the standard Matrix Completion model [6], in which single entries are chosen at random to be observed. Unlike GE, for MC incoherence conditions on X are required in order to guarantee unique recovery of X [6] :\nDefinition 2. (Incoherence). Let U be a subspace of Rn of dimension r, and PU be the orthogonal projection on U . Then the coherence of U (with respect to the standard\nbasis {ei}) is defined as \u00b5(U)\u2261n\nr maxi||PU (ei)||2. (5)\nWe say that a matrix X \u2208 Rn1\u00d7n2 is \u00b5-incoherent if for the SV D X = U\u03a3V T we have max(\u00b5(U), \u00b5(V )) \u2264 \u00b5.\nWhen X is \u00b5-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability. In particular, nuclear norm minimization has gained popularity as a solver for the standard MC problem because it provides recovery guarantees and a convenient representation as a convex optimization problem with availability of many iterative solvers for the problem. However, nuclear norm minimization fails for the RCMC design, even when the matrix X is incoherent, as shown by the next example:\nExample 1. Take X \u2208 Rn\u00d7n for n3 \u2208 N with Xij = 1, \u2200(i, j) \u2208 [n] \u00d7 [n]. Thus ||X ||\u2217 = n. Take k(R) = k(C) = n3 . Set all unknown entries to 0.5, giving a matrix X0 of rank 2 with \u03c31(X0) = ( \u221a 2+1)n 3 , \u03c32(X0) = ( \u221a 2\u22121)n 3 . Therefore ||X0||\u2217 = n \u221a 2 3 < ||X ||\u2217 and nuclear norm minimization fails to recover the correct X .\nIn Section 3 we present our SVLS algorithm, which does not rely on nuclear-norm minimization. In Section 4 we show that SVLS successfully approximates X for the GRC ensemble."}, {"heading": "3 Algorithms for Recovery of X", "text": "In this section we give an efficient algorithm which we call SVLS (Singular Value Least Squares). SVLS is very easy to implement - for simplicity, we start with Algorithm 1 for the noiseless case and then present Algorithm 2 (SVLS) which is applicable for the general (noisy) case."}, {"heading": "3.1 Noiseless Case", "text": "In the noiseless case we reduce the optimization problem (3) to solving a system of linear equations [6], and provide Algorithm 1, which often leads to a closed-form estimator. We then give conditions under which with high probability, the closed-form solution is unique and is equal to the true matrix X . If rank(A(R)U\u0302) = r one can write the resulting estimator X\u0302 in closed-form as follows:\nX\u0302 = U\u0302Y = U\u0302 [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T B(R) (6)\nAlgorithm 1 does not treat the row and column measurements symmetrically. We can apply the same algorithm, but changing the role of rows and columns. The resulting\nAlgorithm 1 Input: A(R), A(C), B(R), B(C) and rank r\n1. Compute a basis (of size r) to the column space of B(C) using Gaussian elimi-\nnation, represented as the columns of a matrix U\u0302 \u2208 Rn1\u00d7r.\n2. Solve the linear system B(R)\u2022j = A (R)U\u0302Y\u2022j for each j = 1, .., n2 and write the\nsolutions as a matrix Y = Y\u20221...Y\u2022n2 .\n3. Output X\u0302 = U\u0302Y\nclosed form solution is then:\nX\u0302 = B(C)A(C)V\u0302 [V\u0302 TA(C)A(C) T V\u0302 ]\u22121V\u0302 T (7)\nfor an orthogonal matrix V\u0302 representing a basis for the rows of X . Since the algorithm uses Gaussian elimination steps for solving systems of linear equations, it is crucial that we have exact noiseless measurements. Next, we modify the algorithm to work also for noisy measurements."}, {"heading": "3.2 General (Noisy) Case", "text": "In the noisy case we seek a matrix X minimizing the loss F in eq. (2). The minimization problem is non-convex and there is no known algorithm with optimality guarantees. We propose Algorithm 2 (SVLS), which empirically returns a matrix estimator X\u0302 with a low value of the loss F . In addition, we prove in Section 4 recovery guarantees on the performance of SVLS.\nAlgorithm 2 SVLS Input: A(R), A(C), B(R), B(C) and rank r\n1. Compute U\u0302 , the r largest left singular vectors of B(C), (U\u0302 is a basis for the\ncolumns space of B(C)(r) ).\n2. Find the least-squares solution\nY\u0302 = argminY \u2016 B(R) \u2212A(R)U\u0302Y ||F . (8)\nIf rank(A(R)U\u0302) = r we can write Y\u0302 in closed form as before:\nY\u0302 = [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T B(R). (9)\n3. Compute the estimate X\u0302(R) = U\u0302 Y\u0302 .\n4. Repeat steps 1-3, replacing the roles of columns and rows to get an estimate\nX\u0302(C).\n5. Set X\u0302 = argminX\u0302(R),X\u0302(C)F(X), for the loss F(X) given in eq. (2)."}, {"heading": "3.2.1 Gradient Descent", "text": "The estimator X\u0302 returned by SVLS may not minimize the loss function F in eq. (2). We therefore perform an additional gradient descent stage starting from X\u0302 to achieve an estimator with lower loss (while still possibly only a local minimum since the problem is non-convex). SVLS can be thus viewed as a fast method for providing a desirable starting point for local-search algorithms. The details of the gradient descent are given in the Appendix, Section 7.2."}, {"heading": "3.3 Estimation of Unknown Rank", "text": "In real life problems, one doesn\u2019t know the true rank of a matrix and should estimate it from data. Our rows-and-columns sampling design is particularly suitable for rank estimation since rank(B(C,0)) = rank(B(R,0)) = rank(X) with high probability when enough rows and columns are sampled. In the noiseless case we can estimate rank(X) by r\u0302=rank(B(C,0)) or rank(B(R,0)).\nFor the noisy case we estimate rank(X) from B(C), B(R). We use the popular\nelbow method to estimate rank(B(C)) in the following way\nr\u0302(C) = argmaxi\u2208[k(C)\u22121]\n(\n\u03c3i(B (C))\n\u03c3i+1(B(C))\n)\n(10)\nWe compute similarly r\u0302(R) from B(R) and take the average as our rank estimator, r\u0302 = round ( r\u0302(C)+r\u0302(C)\n2\n)\n. We demonstrate the performance of our rank estimation\nusing simulations in the Appendix, Section 7.7.\nModern methods for rank estimation from singular values [13] can be similarly applied to B(R), B(C) and may yield more accurate rank estimates. After we estimate the rank, we can plug-in r\u0302 as the rank parameter in the SVLS algorithm and recover X ."}, {"heading": "3.4 Low Rank Approximation", "text": "In the low rank matrix approximation problem, the goal is to approximate a (possibly full rank) matrix X by the closest (in Frobenius norm) rank-r matrix X(r). By the Eckart-Young Theorem [12], this problem has a closed-form solution which is the truncated SV D of X . SV D is a powerful tool in affine matrix recovery and different algorithms such as SVT, OptSpace , SVP and others apply SVD. In [15] the authors try to find a low rank approximation to X using measurements XA(C) = B(C) and A(R)X = B(R). For large n1, n2 they give a single-pass algorithm which computes X(r) using only B(C) and B(R). We bring their algorithm in the Appendix, Section 7.6. The main difference between the above formulation and our problem in eq. (3) is the rank estimation. In [15] it is assumed that k(R) = k(C) = k and one estimates X(k) instead of a rank-r matrix which can lead to poor performance if r \u226a k. We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]\u2019s method, replacing X\u0302(R) and X\u0302(C) in steps 3,4 of SVLS by:\nX\u0302 (R) P = X\u0302 (R)V\u0302 V\u0302 T , X\u0302 (C) P = U\u0302U\u0302 T X\u0302(C). (11)\nHere V\u0302 is the r largest right singular vectors of B(R) and U\u0302 is the r largest left singular vectors of B(C). We call this new estimator SV LSP . Simulations show almost identical and in some cases slightly better performance of this modified algorithm compared to SVLS (see Appendix, Section 7.6). This modified estimator is however difficult to analyze rigorously, and therefore we present throughout the paper our results for the SVLS estimator."}, {"heading": "4 Performance Guarantees", "text": "In this section we give guarantees on the accuracy of the estimator X\u0302 returned by SVLS . Our guarantees are probabilistic, with respect to randomizing the design matrices\nA(R), A(C). For the noiseless case we give conditions which are close to optimal for exact recovery."}, {"heading": "4.1 Noiseless Case", "text": "A rank r matrix of size n1 \u00d7 n2 has r(n1 + n2 \u2212 r) degrees of freedom, and can therefore not be uniquely recovered by fewer measurements. Setting k(R) = k(C) = r gives precisely this minimal number of measurements. We next show that this number suffices, with probability 1, to guarantee accurate recovery of X in the GRC model. In the RCMC model the number of measurements is increased by a logarithmic factor in n and we need an additional incoherence assumption on X in order to guarantee accurate recovery with high probability. We first present two Lemmas which will be useful. Their proofs are given in the Appendix, Section 7.1.\nLemma 1. Let X1, X2 \u2208 M(r)n1\u00d7n2 and A(R) \u2208 Rk(R)\u00d7n1 , A(C) \u2208 Rn2\u00d7k(C) such that rank(A(R)X1) = rank(X1A(C)) = r. If A(R)X1 = A(R)X2 and X1A(C) = X2A (C) then X1 = X2.\nLemma 2. Let X \u2208 M(r)n1\u00d7n2 and A(R) \u2208 Rk(R)\u00d7n1 , A(C) \u2208 Rn2\u00d7k(C) such that rank(A(R)X) = rank(XA(C)) = r. For Algorithm 1 with inputsA(R), A(C), B(R,0), B(C,0) and r the output X\u0302 satisfies\nA(R)X = A(R)X\u0302, XA(C) = X\u0302A(C) (12)"}, {"heading": "4.1.1 Exact Recovery for GRC", "text": "For the noiseless case, we can recover X with the minimal number of measurements, as shown in Theorem 1 (proof given in the Appendix, Section 7.1):\nTheorem 1. Let X\u0302 be the output of Algorithm 1 in the GRC model with Z(C), Z(R) = 0 and k(R), k(C) \u2265 r. Then P (X\u0302 = X) = 1."}, {"heading": "4.1.2 Exact Recovery for RCMC", "text": "In the RCMC model, rows and columns of X are sampled with replacement. Since the same row can be sampled over and over, we cannot guarantee uniqueness of solution, as was the case for the GRC model, but rather wish to prove that exact recovery of X is possible with high probability. We assume the Bernoulli rows and columns model as described in Section 2 and assume for simplicity that k(R) = k(C) = k.\nTheorem 2. Let X = U\u03a3V T be the SV D of X \u2208 Rn1\u00d7n2 , and max(\u00b5(U), \u00b5(V )) < \u00b5. Take A(R) and A(C) as in the RCMC model without noise and probabilities p(R) =\nk n1 and p(C) = kn2 . Let \u03b2 > 1 such that CR \u221a \u03b2log(n)r\u00b5 k < 1 where CR is uniform constant and let X\u0302 be the output of Algorithm 1. Then P ( X\u0302 = X )\n> 1 \u2212 6min(n1, n2) \u2212\u03b2 .\nThe proof of Theorem 2 is in the Appendix, Section 7.3.\nRemark 1. Both row and column measurements are need in order to guarantee unique recovery. If, for example, we observe only rows then even with n \u2212 1 observed rows and rank r = 1 we can only determine the unobserved row up to a constant, and thus cannot recover X uniquely."}, {"heading": "4.2 General (Noisy) Case", "text": "In the noisy case we cannot guarantee exact recovery of X , and our goal is to minimize the error ||X \u2212 X\u0302 ||F for X\u0302 the output of SVLS. Here, we give bounds on the error for the GRC model. For simplicity, we show the result for k(R) = k(C) = k.\nWe focus on the high dimensional case k \u2264 n, where the number of measurements is low. In this case our bound is similar to the bound of the Gaussian Ensemble (GE). In [5] it is shown for GE that ||X \u2212 X\u0302||F < CG \u221a nr\u03c42\nd holds with high probability for\nsome constant CG. We next give an analogous result for our GRC model (proof in the Appendix, Section 7.4).\nTheorem 3. Let A(R) and A(C) with k \u2265 max(4r, 40) be as in the GRC model with noise matrices Z(R), Z(C). Let X\u0302 be the output of SVLS. Then there exist constants c, c(R), c(C) such that with probability > 1\u2212 5e\u2212ck:\n||X \u2212 X\u0302||F \u2264 \u221a r\nk\n[ c(C)||Z(C)||2 + c(R)||Z(R)||2 ] . (13)\nTheorem 3 applies for any Z(C) and Z(R). If k \u2264 n and Z(R), Z(C) i.i.d.\u223c N(0, \u03c42), then from eq. (35) we get max(||Z(R)||2, ||Z(C)||2) \u2264 4\u03c4 \u221a n with probability 1 \u2212 e\u22122n. We therefore get the next Corollary for i.i.d. additive Gaussian noise:\nCorollary 1. Let A(R), A(C) as in the GRC with n \u2265 k \u2265 max(4r, 40), model and Z(R), Z(C) i.i.d.\u223c N(0, \u03c42). Then there exist constants c, CGRC such that:\nP ( ||X \u2212 X\u0302||F \u2264 CGRC \u221a \u03c42nr\nk\n)\n> 1\u2212 5e\u2212ck \u2212 e\u22122n. (14)"}, {"heading": "5 Simulations Results", "text": "We studied the performance of our algorithm using simulations. We measured the reconstruction accuracy using the Relative Root-Mean-Squared-Error (RRMSE), defined as\nRRMSE \u2261 RRMSE(X, X\u0302) \u2261 ||X \u2212 X\u0302 ||F /||X ||F . (15)\nFor simplicity, we concentrated on square matrices with n1 = n2 = n and used an equal number of row and column measurements, k(R) = k(C) = k . In all simulations we sampled a random rank-r matrix X = UV T with U, V \u2208 Rn\u00d7r , U, V\ni.i.d.\u223c N(0, \u03c32). In all simulations we assumed that rank(X) is unknown and estimated using the\nelbow method in eq. (10)."}, {"heading": "5.1 Row-Column Matrix Completion (RCMC)", "text": "In the noiseless case we compared our design to standard MC. We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms. To allow for numerical errors, for each simulation yieldingX and X\u0302 we defined recovery as successful if their RRMSE was lower than 10\u22123, and for each value of d recorded the percentage of simulations for which recovery was successful. In Figure 1 we show results for n = 150, r = 3 and \u03c3 = 1. SVLS recovers X with probability 1 with the optimal number of measurements d = r(2n \u2212 r) = 894 yielding dn2 \u2248 0.04 while MC with OptSpace and SVT need roughly 3-fold and 8-fold more measurements, respectively, to guarantee exact recovery.\nThe improvement in accuracy is not due to our design or our algorithm alone, but due to their combination. We compared our method to OptSpace and SVT for RCMC. We sampled a matrix X with n = 100, r = 3, \u03c3 = 1 and noise level \u03c42 = 0.252, and varied the number of row and column measurements k. Figure 2 shows that while the performance of SVLS is very stable even for small k, the performance of OptSpace varies, with multiple instances achieving poor accuracy, and SVT which minimizes the nuclear norm achieves poor accuracy for all problem instances.\nRemark 2. The OptSpace algorithm has a trimming step which delete dense columns. We omitted this step in the RCMC model since it would delete all the known columns and rows and it\u2019s not stable for this type of measurements, but it still get better result than SVT.\nNext, we compared our RCMC to standard MC. We sampled X as before with U, V \u2208 R1000\u00d7r with standard Gaussian distribution, different rank and different noise ratio. The observations were corrupted by additive Gaussian noise Z with relative noise level NR \u2261 ||Z||F /||X ||F . Results, displayed in Table 1, show that SVLS is significantly faster than the other two algorithms. It is also more accurate than MC for small number of measurements, and comparable to MC for large number of measurements.\nFinally, we checked for RCMC and MC our performance only on unobserved entries, to examine if RRMSE is optimistic due to overfitting to observed entries. Results, shown in the Appendix, Section 7.8, indicate than no overfitting is observed."}, {"heading": "5.2 Gaussian Rows and Columns (GRC)", "text": "We tested the performance of the GRC model with A(R), A(C) i.i.d.\u223c N(0, 1n ) and with X = UV T where U, V i.i.d.\u223c N(0, 1\u221a\nr ). We compare our results to the Gaussian\nEnsemble model (GE) where for each n, A(X) was normalized to allow a fair comparison. In Figure 3 we take n = 100 and r = 2, and change the number of measurements d = 2nk (where A(R) \u2208 Rk\u00d7n and A(C) \u2208 Rn\u00d7k). We added Gaussian noise Z(R), Z(C) with different noise levels \u03c4 . For all noise levels, the performance of GRC was better than the performance of GE. The RRMSE error decays at a rate of \u221a k. For GE we used the APGL algorithm [26] for nuclear norm minimization.\nIn the next tests we ran SVLS for measurements with different noise levels. We take n = 1000 and k = 100 with different rank level every entry in Z(C), Z(R) i.i.d.\u223c N(0, \u03c42) and different values of \u03c4 . Results are shown in Figure 4. The change in the relative error RRMSE is linear in \u03c4 while the rate depends on r.\nWe next examined the behaviour of theRRMSE when n \u2192 \u221e and when n, k, r \u2192 \u221e together, while the ratios kn and dr are kept constant. Results (shown in the Appendix, Section 7.5) indicate that when properly scaled, the RRMSE error is not sensitive to the value of n and other parameters, in agreement with Theorem 3."}, {"heading": "6 Discussion", "text": "We introduced a new measurements ensemble for low rank matrix recovery where every measurements is an affine combination of a row or column of X . We focused on two models: matrix completion from single columns and rows (RCMC) and matrix recovery from Gaussian combination of columns and rows (GRC). We proposed a fast algorithm for this ensemble. For the RCMC model we proved that in the noiseless case our method recovers X with high probability and simulation results show that the\nRCMC model outperforms the standard approach for matrix completion in both speed and accuracy for models with small noise.\nFor the GRC model we proved that our method recoversX with the optimal number of measurements in the noiseless case and gave an upper bounds on the error for the noisy case. For RCMC, our simulations show that the RCMC design may achieve comparable or favorable results, compared to the standard MC design, especially for low noise level. Proving recovery guarantees for this RCMC model is an interesting future challenge.\nOur proposed measurement scheme is not restricted to recovery of low-rank matrices. One can employ this measurement scheme and recover X by minimizing other matrix norms. This direction can lead to new algorithms that may improve matrix recovery for real datasets."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 Proofs for Noiseless GRC Case", "text": "Proof of Lemma 1\nProof. First, rank(X2A(C)) = rank(X1A(C)) = r and similarly rank(A(R)X2) = rank(A(R)X1) = r. Since span(X1A(C)), span(X2A(C)) are subspaces of span(X1), span(X2) respectively, and dim(span(X2)) \u2264 r we get span(X2) = span(X2A(C)) = span(X1A\n(C)) = span(X1), and we define U \u2208 On1\u00d7r a basis for this subspace. For X1, X2 there are Y1, Y2 \u2208 Rr\u00d7n2 such that X1 = UY1, X2 = UY2. Therefore A(R)UY1 = A(R)UY2. Since rank(A(R)UY1) = r and U \u2208 On1\u00d7r we get rank(A(R)U) = r, hence the matrix UTA(R) T A(R)U is invertible, which gives Y1 = Y2, and therefore X1 = UY1 = UY2 = X2.\nProof of Lemma 2\nProof. span(XA(C)) \u2286 span(X) and rank(XA(C)) = rank(X) = r, therefore span(XA(C)) = span(X) and U\u0302 from stage 1 in Algorithm 1 is a basis for span(X). We can write X = U\u0302Y for some matrix Y \u2208 Rr\u00d7n2 . Since rank(A(R)U\u0302Y ) = rank(U\u0302) = r, we have rank(A(R)U\u0302) = r. Thus eq. (6) gives X\u0302 in closed form and we get:\nA(R)X\u0302 = A(R)U\u0302 [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T B(R,0) =\nA(R)U\u0302 [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T A(R)U\u0302Y =\nA(R)U\u0302Y = A(R)X. (16)\nX\u0302A(C) = U\u0302 [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T A(R)XA(C) =\nU\u0302 [U\u0302TA(R) T A(R)U\u0302 ]\u22121U\u0302TA(R) T A(R)U\u0302Y A(C) =\nU\u0302Y A(C) = XA(C). (17)\nLemma 3. Let V \u2208 On\u00d7r and A(C) \u2208 Rn\u00d7k be a random matrix A(C) i.i.d.\u223c N(0, \u03c32). Then V TA(C) i.i.d.\u223c N(0, \u03c32).\nProof. For any two matrices A \u2208 Rn1\u00d7n2 andB \u2208 Rm1\u00d7m2 we define their Kronecker product as a matrix in Rn1m1\u00d7n2m2 :\nA\u2297B =\n\n      \na11B a12B . . a1n2B\n. . . . .\n. . . . .\n. . . . .\nan11B an12B . . an1n2B\n\n      \n(18)\nNow, we have vec(V TA(C)) = (In \u2297 V T )vec(A(C)) and since vec(A(C)) \u223c N(0, \u03c3In) the vector (In \u2297 V T )vec(A(C)) is also a multivariate Gaussian vector with zero mean and covariance matrix:\nCOV ( V TA(C) ) = COV ( (In \u2297 V T )vec(A(C)) ) =\n(In \u2297 V T )COV ( vec(A(C)) ) (In \u2297 V T )T =\n\u03c32(In \u2297 V T )(In \u2297 V T )T = \u03c32Ir \u2297 In = \u03c32Inr. (19)\nProof of Theorem 1\nFor the GRC model, Lemmas 1,2 and 3 can be used to prove exact recovery of X with the minimal possible number of measurements:\nProof. Let U\u03a3V T be the SVD of X . From Lemma 3 the elements of the matrix V TA(C) have a continuous Gaussian distribution and since the measure of low rank matrices is zero and k(C) \u2265 r we get that P (rank(V TA(C)) = r) = 1. Since B(C) = U\u03a3V TA(C) we get P (rank(B(C)) = rank(U\u03a3V TA(C)) = r) = 1. In the same way P (rank(B(R)) = r) = 1. Combining Lemma 2 with Lemma 1 give us the required result."}, {"heading": "7.2 Gradient Descent", "text": "The gradient descent stage is performed directly in the space of rank r matrices, using the decomposition X\u0302=WS where W \u2208 Rn1\u00d7r and S \u2208 Rr\u00d7n2 and computing the gradient of the loss as a function of W and S,\nL(W,S) = F(WS) = ||A(R)WS \u2212B(R)||2F + ||WSA(C) \u2212B(C)||2F . (20)\nWe want to minimize eq. (20) but the loss L isn\u2019t convex and therefore gradient descent may fail to converge to a global optimum. We propose X\u0302 (the output of SVLS\n) as a starting point which may be close enough to enable gradient descent to converge to the global optimum, and in addition may accelerate convergence.\nThe gradient of L is (using the chain rule)\n\u2202L \u2202W = 2 [ A(R) T (A(R)WS \u2212B(R))ST + (WSA(C) \u2212B(C))A(C)T ST ]\n\u2202L \u2202S = 2 [ WTA(R) T (A(R)WS \u2212B(R)) +WT (WSA(C) \u2212B(C))A(C)T ] (21)"}, {"heading": "7.3 Proofs for Noiseless RCMC Case", "text": "We prove that if U \u2208 On1\u00d7r is orthonormal then with high probability we have p\u22121||UTA(R)TA(R)U \u2212 pIr||2 < 1. Because U is orthonormal, this is equivalent to\np\u22121||UUTA(R)TA(R)UUT \u2212 pUUT ||2 < 1 \u21d4 p\u22121||PUPA(R)T PU \u2212 pPU ||2 < 1 (22)\nwhere PU = UUT , PA(R)T = A (R)T A(R) and p(R) = p. We generalize Theorem\n4.1 from [6].\nLemma 4. Suppose A(R) as in the RCMC model with inclusion probability p, and U \u2208 On1\u00d7r with \u00b5(U) = n1r maxi||PU (ei)||2 = \u00b5. Then there is a numerical constant CR such that for all \u03b2 > 1, if CR \u221a\n\u03b2log(n1)r\u00b5 pn1 < 1 then:\nP\n(\np\u22121||PUPA(R)T PU \u2212 pPU ||2 < CR\n\u221a\n\u03b2log(n1)r\u00b5\npn1\n)\n> 1\u2212 3n\u2212\u03b21 (23)\nThe proof of Lemma 4 builds upon (yet generalizes) the proof of Theorem 4.1 from [6]. We next present a few lemmas which are required for the proof of Lemma 4. We start with a lemma from [7].\nLemma 5. If yi is a family of vectors in Rd and ri is a sequence of i.i.d. Bernoulli random variables with P (ri = 1) = p, then\nE ( p\u22121||\u03a3i(ri \u2212 p)yi \u2297 yi|| ) < C\n\u221a\nlog(d)\np maxi||yi|| (24)\nfor some numerical constant C provided that the right hand side is less than 1.\nWe next use a result from large deviations theory [25]:\nTheorem 4. Let Y1...Yn be a sequence of independent random variables taking values in a Banach space and define\nZ = supf\u2208F\nn \u2211\ni=1\nf(Yi) (25)\nwhere F is a real countable set of functions such that if f \u2208 F then \u2212f \u2208 F . Assume that |f | \u2264 B and E(f(Yi)) = 0 for every f \u2208 F and i \u2208 [n]. Then there exists a constant C such that for every t \u2265 0\nP ( |Z \u2212 E(Z)| \u2265 t ) \u2264 3exp ( \u2212t CB log(1 + t \u03c3 +Br ) )\n(26)\nwhere \u03c3 = supf\u2208F \u2211n i=1 E(f 2(Yi)).\nTheorem 4 is used in the proof of the next lemma which is taken from Theorem 4.2\nin [6]. We bring here the lemma and proof in our notations for convenience.\nLemma 6. Let U \u2208 On\u00d7r with incoherence constant \u00b5. Let ri be i.i.d. Bernoulli random variables with P (ri = 1) = p and let Yi = p\u22121(ri \u2212 p)PU (ei) \u2297 PU (ei) for i = 1, .., n. Let Y =\n\u2211n i=1 Yi and Z = ||Y ||2. Suppose E(Z) \u2264 1. Then for every\n\u03bb > 0 we have\nP ( |Z\u2212E(Z)| \u2265 \u03bb \u221a \u00b5rlog(n)\npn\n) \u2264 3exp ( \u2212\u03b3min(\u03bb2log(n), \u03bb \u221a pnlog(n)\n\u00b5r ) ) (27)\nfor some positive constant \u03b3.\nProof. We know that Z = ||Y ||2 = supf1,f2\u3008f1, Y f2\u3009 = supf1,f2 \u2211n i=1\u3008f1,Yif2\u3009, where the supremum is taken over a countable set of unit vectors f1, f2 \u2208 FV . Let F be the set of all functions f such that f(Y ) = \u3008f1, Y f2\u3009 for some unit vectors f1, f2 \u2208 FV . For every f \u2208 F and i \u2208 [n] we have E(f(Yi)) = 0. From the incoherence of U we conclude that\n|f(Yi)| = p\u22121|ri \u2212 p| \u00d7 |\u3008f1, PU (ei)\u3009| \u00d7 |\u3008PU (ei), f2\u3009| \u2264 p\u22121||PU (ei)||2 \u2264 p\u22121 r\nn \u00b5. (28)\nIn addition\nE(f2(Yi)) = p \u22121(1\u2212 p)\u3008f1, PU (ei)\u30092\u3008PU (ei), f2\u30092 \u2264\np\u22121||PU (ei)||2|\u3008PU (ei), f2\u30092| \u2264 p\u22121 r\nn \u00b5|\u3008PU (ei), f2\u3009|2. (29)\nSince \u2211n i=1 |\u3008PU (ei), f2\u3009|2 = \u2211n i=1 |\u3008ei, PU (f2)\u3009|2 = ||PU (f2)||2 \u2264 1, we get \u2211n\ni=1 E(f 2(Yi)) \u2264 p\u22121 rn\u00b5.\nWe can take B = 2p\u22121 rn\u00b5 and t = \u03bb \u221a \u00b5rlog(n) pn and from Theorem 4:\nP (|Z \u2212 E(Z)| \u2265 t) \u2264 3exp ( \u2212t KB log(1 + t 2 ) ) \u2264 3exp (\u2212tlog(2) KB min(1, t 2 ) )\n(30)\nwhere the last inequality is due to the fact that for every u > 0 we have log(1 + u) \u2265 log(2)min(1, u). Taking \u03b3 = \u2212log(2)/K finishes our proof.\nWe are now ready to prove Lemma 4\nProof. (Lemma 4) Represent any vector w \u2208 Rn1 in the standard basis as w = \u2211n1\ni=1\u3008w, ei\u3009ei. Therefore PU (w) = \u2211n1 i=1\u3008PU (w), ei\u3009ei = \u2211n1 i=1\u3008w,PU (ei)\u3009ei. Recall the ri Bernoulli variables which determine if ei is included as a row of A(R) as in Section 2 and define Yi and Z as in Lemma 6. We get\nPA(R)T PU (w) =\nn1 \u2211\ni=1\nri\u3008w,PU (ei)\u3009ei =\u21d2\nPUPA(R)T PU (w) =\nn1 \u2211\ni=1\nri\u3008w,PU (ei)\u3009PU (ei) (31)\nIn other words the matrix PUPA(R)T PU is given by\nPUPA(R)T PU =\nn1 \u2211\ni=1\nriPU (ei)\u2297 PU (ei) (32)\nU is \u00b5\u2212incoherent, thus maxi\u2208[n1]||PU (ei)|| \u2264 \u221a r\u00b5 n1 , hence from Lemma 5 we have for p large enough:\nE(p\u22121||PUPA(R)T PU \u2212 pPU ||2) < C \u221a log(n1)r\u00b5\npn1 \u2264 1. (33)\nFor \u03b2 > 1 which satisfy the lemma\u2019s requirement, take \u03bb = \u221a\n\u03b2 \u03b3 where \u03b3 as in\nTheorem 4. We get that if p > \u00b5log(n1)r\u03b2n1\u03b3 then from Lemma 6 with probability of at least 1\u2212 3n\u2212\u03b21 we have Z \u2264 C \u221a log(n1)r\u00b5 pn1 + 1\u221a\u03b3 \u221a log(n1)r\u00b5\u03b2 pn1\n. Taking CR = C + 1\u221a\u03b3 finishes our proof.\nProof of Theorem 2\nProof. From Lemma 4 and using a union bound we have that with probability > 1 \u2212 6min(n1, n2) \u2212\u03b2 , p(R) \u22121||p(R)Ir \u2212 UTA(R) T A(R)U ||2 < 1 and p(C) \u22121||p(C)Ir \u2212\nV TA(C)A(C) T V ||2 < 1. Since the singular values of p(R)Ir \u2212 UTA(R) T A(R)U are |p(R) \u2212 \u03c3i(UTA(R) T A(R)U)| for 1 \u2264 i \u2264 r, we have\np(R) \u2212 \u03c3r(UTA(R) T A(R)U) \u2264 \u03c31(p(R)Ir \u2212 UTA(R) T A(R)U) < p(R)\n\u21d2 0 < \u03c3r(UTA(R) T A(R)U) (34)\nand similarly for V TA(C)A(C) T V . Therefore rank(A(R)U) = rank(V TA(C)) = r and rank(A(R)X) = rank(XA(C)) = r with probability > 1 \u2212 6min(n1, n2)\u2212\u03b2 . From Lemma 2 we get A(R)X = A(R)X\u0302 XA(C) = X\u0302A(C) and from Lemma 1 we get X = X\u0302 with probability > 1\u2212 6min(n1, n2)\u2212\u03b2 ."}, {"heading": "7.4 Proofs for Noisy GRC Case", "text": "The proof of Theorem 3 is using strong concentration results on the largest and smallest singular values of n\u00d7 k matrix with i.i.d Gaussian entries:\nTheorem 5. [24] Let A \u2208 Rn\u00d7k be a random matrix A i.i.d.\u223c N(0, 1n ). Then, its largest and smallest singular values obey:\nP (\n\u03c31(A) > 1 + \u221a k\u221a n + t ) \u2264e\u2212nt2/2\nP ( \u03c3k(A) \u2264 1\u2212 \u221a k\u221a n \u2212 t ) \u2264e\u2212nt2/2. (35)\nCorollary 2. Let A \u2208 Rn\u00d7k be a random matrix A i.i.d.\u223c N(0, 1) where n \u2265 4k, and let A\u2020 be the Moore-Penrose pseudoinverse of A. Then\nP\n(\n||A\u2020||2 \u2264 6\u221a n\n)\n> 1\u2212 e\u2212n/18 (36)\nProof. Since A\u2020 is the pseudoinverse of A, ||A\u2020||2= 1\u03c3k(A) and from Theorem 5 we get \u03c3k(A) \u2265 \u221a n\u2212 \u221a k\u2212 t\u221an with probability \u2265 1\u2212 ent2/2 (notice the scaling by \u221an of the entries of A compared to Theorem 5). Therefore, if we take n \u2265 4k and t = 13 we get\nP\n(\n||A\u2020||2 \u2264 6\u221a n\n)\n= P\n( \u03c3k(A) \u2265 \u221a n\n6\n)\n\u2265 1\u2212 e\u2212n/18. (37)\nWe also use the following lemma from [23]:\nLemma 7. Let Q to be a finite set of vectors in Rn, let \u03b4 \u2208 (0, 1) and k be an integer such that\n\u01eb \u2261 \u221a 6log(2|Q|/\u03b4) k \u2264 3. (38)\nLet A \u2208 Rk\u00d7n be a random matrix with A i.i.d.\u223c N(0, 1k ). Then,\nP\n(\nmaxx\u2208Q\n\u2223 \u2223 \u2223 \u2223 ||Ax||2 ||x||2 \u2212 1 \u2223 \u2223 \u2223 \u2223 \u2264 \u01eb ) > 1\u2212 \u03b4. (39)\nLemma 7 is a direct result of the Johnson-Lindenstrauss lemma [11] applied to each vector in Q and using the union bound. Representing the vectors in Q as a matrix, Lemma 7 shows that A(R), A(C) preserve matrix Frobenius norm with high probability - a weaker property than the RIP which holds for any low-rank matrix.\nTo prove Theorem 3, we first represent ||X\u2212X\u0302||F as a sum three parts (Lemma 8), then give probabilistic upper bounds to each of the parts and finally use union bound. We define A(R) U\u0302 = A(R)U\u0302 and A(C) V T = V TA(C). From Lemma 3 A(R) U\u0302 , A (C) V T i.i.d.\u223c N(0, 1), hence rank(A(R) U\u0302 ) = rank(A (C) V T ) = r with probability 1. We assume w.l.o.g that X\u0302 = X\u0302(R) (see SVLS description). Therefore, from eq. (9) we have X\u0302 = U\u0302(A (R)T\nU\u0302 A\n(R) U\u0302 )\u22121A\n(R)T U\u0302 B(R).\nWe denote by A(R) U\u0302\n\u2020 = (A (R)T\nU\u0302 A\n(R) U\u0302 )\u22121A\n(R)T U\u0302 and A(C)V T\n\u2020 = A (C)T\nV T (A (C) V T A\n(C)T V T ) \u22121\nthe Moore-Penrose pseudoinverse of A(R) U\u0302 and A(C)V T , respectively. We next prove the following lemma\nLemma 8. Let A(R) and A(C) be as in the GRC model and Z(R), Z(C) be noise matrices. Let X\u0302 be the output of SVLS. Then:\n||X \u2212 X\u0302||F \u2264 I+ II+ III\nwhere:\nI \u2261 ||(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F (40)\nII \u2261||U\u0302A(R) U\u0302 \u2020 A(R)(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F (41)\nIII \u2261||U\u0302A(R) U\u0302\n\u2020 Z(R)||F . (42)\nProof. We represent ||X \u2212 X\u0302||F as follows\n||X \u2212 X\u0302||F =\n||X \u2212 U\u0302(A(R) T\nU\u0302 A\n(R) U\u0302 )\u22121A\n(R)T U\u0302 (A(R)X + Z(R))||F =\n||X \u2212 U\u0302A(R) U\u0302\n\u2020 A(R)X \u2212 U\u0302A(R)\nU\u0302\n\u2020 Z(R)||F \u2264\n||X \u2212 U\u0302A(R) U\u0302\n\u2020 A(R)X ||F + III (43)\nwhere we have used the triangle inequality. We next use the following equality\nXA(C)A (C)\nV T\n\u2020 V T = U\u03a3V TA(C)A (C)\nV T\n\u2020 V T = U\u03a3V T = X (44)\nto obtain:\n||X \u2212 U\u0302A(R) U\u0302\n\u2020 A(R)X ||F =\n||(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))X ||F =\n||(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))XA(C)A (C) V T \u2020 V T ||F =\n||(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))B(C,0)A (C) V T \u2020 ||F (45)\nwhere the last equality is true because V is orthogonal.\nSince U\u0302 is a basis for span(B(C)(r) ) there exists a matrix Y such that U\u0302Y = B (C) (r)\nand we get:\n(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))B (C) (r) = B (C) (r) \u2212 U\u0302A (R) U\u0302 \u2020 A(R)U\u0302Y = B (C) (r) \u2212 U\u0302Y = 0. (46)\nTherefore\n||(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))B(C,0)A (C) V T \u2020 ||F =\n||(In \u2212 U\u0302A(R)U\u0302 \u2020 A(R))(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F \u2264\n||(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F + ||U\u0302A(R)U\u0302 \u2020 A(R)(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F = I+ II\n(47)\nCombining eq. (43) and eq. (47) gives the required result.\nWe next bound each of the three parts in the formula of Lemma 8. We use the\nfollowing claim:\nClaim 1. ||B(C,0) \u2212B(C)(r) ||2 \u2264 2||Z(C)||2\nProof. We know that ||B(C) \u2212 B(C)(r) ||2 \u2264 ||B(C) \u2212 B(C,0)||2 since rank(B (C) (r) ) = rank(B(C,0)) = r with probability 1, and by definition B(C)(r) is the closest rank-r matrix to B(C) in Frobenius norm. Therefore from the triangle inequality\n||(B(C,0) \u2212B(C)(r) )||2 \u2264 ||B (C) \u2212B(C)(r) ||2 + ||B (C) \u2212B(C,0)||2 \u2264\n2||B(C,0) \u2212B(C)||2 = 2||Z(C)||2. (48)\nNow we are ready to prove Theorem 3. The proof uses the following inequalities\nfor matrix norms for any two matrices A,B:\n||AB||2 \u2264 ||A||2||B||2 ||AB||F \u2264 ||A||F ||B||2\nrank(A) 6 r \u21d2 ||A||F \u2264 \u221a r||A||2. (49)\nProof. (Theorem 3) We prove (probabilistic) upper bounds on the three terms appearing in Lemma 8.\n1. We have\nrank\n(\n(B(C,0) \u2212B(C)(r) )A (C) V T\n\u2020 )\n6 rank\n(\nA (C) V T\n\u2020 )\n6 r. (50)\nTherefore\nI = ||(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F \u2264 \u221a r||(B(C,0) \u2212B(C)(r) )||2||A (C) V T \u2020 ||2 (51)\nSince A(C) V T\ni.i.d.\u223c N(0, 1), from Corollary 2 we get P (\n||A(C) V T \u2020 ||2 \u2264 6\u221ak )\n\u2265 1\u2212 e\u2212k/18 for k \u2265 4r, hence with probability \u2265 1\u2212 e\u2212k/18,\nI \u2264 6 \u221a r\nk ||(B(C,0) \u2212B(C)(r) )||2. (52)\nFrom Claim 1 and eq. (40) we get a bound on I for some absolute constants C1, c1:\nP ( I \u2264 C1 \u221a r\nk ||Z(C)||2\n)\n> 1\u2212 e\u2212c1k. (53)\n2. U\u0302 is orthogonal and can be omitted from II without changing the norm. Apply-\ning the second inequality in eq. (49) twice, we get the inequality:\nII =||U\u0302A(R) U\u0302 \u2020 A(R)(B(C,0) \u2212B(C)(r) )A (C) V T \u2020 ||F \u2264\n||A(R) U\u0302 \u2020 ||2||A(R)(B(C,0) \u2212 B(C)(r) )||F ||A (C) V T \u2020 ||2. (54)\nFrom Corollary 2 we know that for k > 4r we have ||A(R) U\u0302 \u2020 ||2 \u2264 6\u221ak and ||A(C)V T \u2020 ||2 \u2264 6\u221ak , each with probability > 1\u2212 e \u2212k/18. Therefore,\nP ( II \u2264 36 k ||A(R)(B(C,0) \u2212B(C)(r) )||F ) > 1\u2212 2e\u2212k/18. (55)\nA(R) andB(C,0)\u2212B(C)(r) are independent and rank(B(C,0)\u2212B (C) (r) ) \u2264 2r. Therefore we can apply Lemma 7 with k such that k6 > log(2k) + k 18 (this holds for k \u2265 40) to get with probability > 1\u2212 2e\u2212k/18:\nII \u2264 36 k ||A(R)(B(C,0) \u2212B(C)(r) )||F \u2264\n36 \u221a 2k\nk ||(B(C,0) \u2212B(C)(r) )||F \u2264 36\n\u221a\n4 r\nk ||(B(C,0) \u2212B(C)(r) )||2. (56)\nFrom eq. (55) and (56) together with Claim 1 we have constants C2 and c2 such that,\nP ( II \u2264 C2||Z(C)||2 ) > 1\u2212 3e\u2212c2k. (57)\n3. rank(A(R) U\u0302\n\u2020 ) \u2264 r and from Corollary 2 we get P (\n||A(R) U\u0302 \u2020 ||2 \u2264 6\u221ak )\n> 1 \u2212 e\u2212k/18 for k > 4r. Therefore, with probability > 1\u2212 e\u2212k/18:\nIII =||U\u0302A(R) U\u0302 \u2020 Z(R)||F = ||A(R)U\u0302 \u2020 Z(R)||F \u2264\n\u221a r||A(R)\nU\u0302\n\u2020 Z(R)||2 \u2264 \u221a r||A(R)\nU\u0302\n\u2020 ||2||Z(R)||2 \u2264 6 \u221a r\u221a k ||Z(R)||2. (58)\nHence we have constants C3 and c3 such that, > 1\u2212 e\u2212c3k.\nP ( III \u2264 C3||Z(R)||2 ) > 1\u2212 e\u2212c3k. (59)\nCombining equations (53,57,59) with Lemma 8 and taking the union bound while\nsetting c(C) = C1 + C2, c(R) = C3 with c = min(c1, c2, c3) concludes our proof."}, {"heading": "7.5 Simulations for Large Values of n", "text": "We varied n between 10 and 1000, with results averaged over 100 different matrices of rank 3 at each point, and tried to recover them using k = 20 row and column measurements. Measurement matrices were A(R), A(C) i.i.d.\u223c 1n to allow similar norms for each measurement vector for different values of n. Recovery performance was insensitive to n. if we take A(R), A(C) i.i.d.\u223c N(0, 1) instead of N(0, 1n ), the scaling of our results is in agreement with Theorem 3.\nNext, we take n, k, r \u2192 \u221e while the ratios nk = 5 and kr = 4 are kept constant, and compute the relative error for different noise level. Again, the relative error converges rapidly to constant, independent of n, k, r ."}, {"heading": "7.6 Low Rank matrix Approximation", "text": "We bring here the one pass algorithm to approximate X from [15] for the convenience of the reader. The output of this algorithm isn\u2019t low rank if k > r. This algorithm is different from SV LSP and its purpose is to approximate a (possibly full rank) matrix by low rank matrix. We adjusted Algorithm 3 to our purpose with some changes. First, we estimate the rank of X using the elbow method from Section 3.3 and instead of calculating the QR decomposition of B(C) and B(R) T we find their r\u0302 largest singular vectors. Furthermore, we repeat part two in algorithm 3 while replacing the roles of columns and rows as in SVLS . This variation gives our modified algorithm SV LSP as described in Section 3.4.\nWe compared our SVLS to SV LSP which is presented in Section 3.4. We took X \u2208 M(10)1000\u00d71000 and \u03c3 = 1. We tried to recover X in the GRC model with k = 12 for 100 different matrices. For each matrix, we compared the RRMSE obtained for the outputs of SVLS and SV LSP . The RRMSE for SV LSP was lower than the RRMSE for SVLS in most cases but the differences were very small and negligible.\nAlgorithm 3 Input: A(R), A(C), B(R), B(C)\n1. compute Q(C)R(C) the QR decomposition of B(C), and Q(R)R(R) the QR de-\ncomposition of B(R) T\n2. Find the least-squares solution Y = argminC ||Q(C)B(C) \u2212CQ(R) T B(R) T ||F .\n3. Return the estimate X\u0302 = Q(C)Y Q(R) T ."}, {"heading": "7.7 Rank Estimation", "text": "We test the elbow method for estimating the rank of X (see eq. (10)). We take a matrix X of size 400\u00d7 400 and different ranks. We add Gaussian noise with \u03c3 = 0.25 while the measurements are sampled as in the RCMC model. For each number of measurements we sampled 100 matrices and took the average estimated rank. We\ncompute the estimator r\u0302 for different values of d, the number of measurements. We compare our method to the rank estimation which appears in OptSpace [17] for the standard MC problem. Our simulation results, shown in Figure 8, indicate that the RCMC model with the elbow method is a much better design for rank estimation of X ."}, {"heading": "7.8 Test Error", "text": "In matrix completion with MC and RCMC ensembles the RRMSE loss function measures the loss on both the observed and unobserved entries. This loss may be too optimistic when considering our prediction error only on unobserved entries. Thus, instead of including all measurements in calculation of the RRMSE we compute a different measure of prediction error, given by the RRMSE only on the unobserved entries. For each single-entry measurements operator A define E(A) the set of measured entries and E\u0304 it\u2019s complement, i.e. the set of unmeasured entries (i, j) \u2208 [n1]\u00d7 [n2]. We define XE\u0304 to be a matrix such that XE\u0304ij = Xij if (i, j) \u2208 E\u0304 and 0 otherwise. Instead of RRMSE(X, X\u0302) we now calculate RRMSE(XE\u0304, X\u0302E\u0304). This quantity measures\nour reconstruction only on the unseen matrix entries Xij , and is thus not influenced by overfitting. In Table 2 we performed exactly the same simulation as in Table 1 but with RRMSE(XE\u0304, X\u0302E\u0304). The results of OptSpace , SVT and SVLS stay similar to the results in Table 1 and our RRMSE loss function does not show overfitting."}], "references": [{"title": "Lambertian reflectance and linear subspaces", "author": ["Ronen Basri", "David W Jacobs"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1956}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["Tony T Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["Emmanuel J Cand\u00e8s", "Justin Romberg"], "venue": "Inverse Problems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Genotype imputation via matrix completion", "author": ["Eric C Chi", "Hua Zhou", "Gary K Chen", "Diego Ortega Del Vecchyo", "Kenneth Lange"], "venue": "Genome Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Genomic maps of long noncoding RNA occupancy reveal principles of RNAchromatin interactions", "author": ["Ci Chu", "Kun Qu", "Franklin L Zhong", "Steven E Artandi", "Howard Y Chang"], "venue": "Molecular cell,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Carl Eckart", "Gale Young"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1936}, {"title": "The optimal hard threshold for singular values", "author": ["Matan Gavish", "David L Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Quantum state tomography via compressed sensing", "author": ["David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert"], "venue": "Physical Review Letters,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Prateek Jain", "Raghu Meka", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Fixed point and Bregman iterative methods for matrix rank minimization", "author": ["Shiqian Ma", "Donald Goldfarb", "Lifeng Chen"], "venue": "Mathematical Programming,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM Review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Condition numbers of random matrices", "author": ["Stanislaw J Szarek"], "venue": "Journal of Complexity,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "New concentration inequalities in product spaces", "author": ["Michel Talagrand"], "venue": "Inventiones Mathematicae,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kim-Chuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 13, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 185, "endOffset": 189}, {"referenceID": 8, "context": "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 5, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 7, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 16, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 17, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 20, "context": "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.", "startOffset": 275, "endOffset": 296}, {"referenceID": 4, "context": "Gaussian weights [5, 22].", "startOffset": 17, "endOffset": 24}, {"referenceID": 21, "context": "Gaussian weights [5, 22].", "startOffset": 17, "endOffset": 24}, {"referenceID": 5, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 7, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 20, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 21, "context": "Remarkably, although the recovery problem is in general NP-hard, when r \u226a min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d \u226a n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].", "startOffset": 260, "endOffset": 274}, {"referenceID": 9, "context": "For example, (i) In collaborative filtering, we may wish to recover a users-items preference matrix and have access to only a subset of the users, but can observe their preference scores for all items (ii) When recovering a protein-RNA interactions matrix in molecular biology, a single experiment may simultaneously measure the interactions of a specific protein with all RNA molecules [10].", "startOffset": 387, "endOffset": 391}, {"referenceID": 5, "context": "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.", "startOffset": 76, "endOffset": 83}, {"referenceID": 21, "context": "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.", "startOffset": 76, "endOffset": 83}, {"referenceID": 4, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 136, "endOffset": 143}, {"referenceID": 21, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 136, "endOffset": 143}, {"referenceID": 15, "context": "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.", "startOffset": 201, "endOffset": 205}, {"referenceID": 5, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 7, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 19, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 20, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 25, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 164, "endOffset": 182}, {"referenceID": 16, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 239, "endOffset": 247}, {"referenceID": 17, "context": "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].", "startOffset": 239, "endOffset": 247}, {"referenceID": 7, "context": "However, recovery guarantees for this model are weaker: setting n = max(n1, n2), it is shown that \u0398(nrlog(n)) measurements are required to recover a rank r matrix of size n1 \u00d7 n2 [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "Recently a new design of rank one projections was proposed [3], where each measurement is of the form \u03b1X\u03b2 and such that \u03b1 \u2208 R1 , \u03b2 \u2208 R2 have i.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "Our problem is a specialization of the general affine matrix recovery problem [22], in which a matrix is measured using a general affine transformation A with b = A(X) + z.", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "It is instructive to compare our GRC ensemble to the Gaussian Ensemble (GE) [5], with the matrix representation A(X) = Avec(X) where A \u2208 Rd\u00d7n1n2 and A i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "The GE model satisfies the r-RIP condition for d = O(rn) with high probability [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 66, "endOffset": 73}, {"referenceID": 21, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 66, "endOffset": 73}, {"referenceID": 15, "context": "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "We next compare RCMC to the standard Matrix Completion model [6], in which single entries are chosen at random to be observed.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Unlike GE, for MC incoherence conditions on X are required in order to guarantee unique recovery of X [6] : Definition 2.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 15, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 16, "context": "When X is \u03bc-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.", "startOffset": 107, "endOffset": 118}, {"referenceID": 5, "context": "1 Noiseless Case In the noiseless case we reduce the optimization problem (3) to solving a system of linear equations [6], and provide Algorithm 1, which often leads to a closed-form estimator.", "startOffset": 118, "endOffset": 121}, {"referenceID": 12, "context": "Modern methods for rank estimation from singular values [13] can be similarly applied to B, B and may yield more accurate rank estimates.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "By the Eckart-Young Theorem [12], this problem has a closed-form solution which is the truncated SV D of X .", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "In [15] the authors try to find a low rank approximation to X using measurements XA = B and AX = B.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [15] it is assumed that k = k = k and one estimates X(k) instead of a rank-r matrix which can lead to poor performance if r \u226a k.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]\u2019s method, replacing X\u0302 and X\u0302 in steps 3,4 of SVLS by:", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]\u2019s method, replacing X\u0302 and X\u0302 in steps 3,4 of SVLS by:", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "In [5] it is shown for GE that ||X \u2212 X\u0302||F < CG \u221a", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.", "startOffset": 219, "endOffset": 223}, {"referenceID": 1, "context": "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.", "startOffset": 231, "endOffset": 234}, {"referenceID": 25, "context": "For GE we used the APGL algorithm [26] for nuclear norm minimization.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "[1] Ronen Basri and David W Jacobs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Tony T Cai and Anru Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Emmanuel J Cand\u00e8s and Yaniv Plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Emmanuel J Cand\u00e8s and Yaniv Plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Emmanuel J Cand\u00e8s and Benjamin Recht.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Emmanuel J Cand\u00e8s and Justin Romberg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Emmanuel J Cand\u00e8s and Terence Tao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Eric C Chi, Hua Zhou, Gary K Chen, Diego Ortega Del Vecchyo, and Kenneth Lange.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ci Chu, Kun Qu, Franklin L Zhong, Steven E Artandi, and Howard Y Chang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sanjoy Dasgupta and Anupam Gupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Carl Eckart and Gale Young.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Matan Gavish and David L Donoho.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Prateek Jain, Raghu Meka, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Yehuda Koren, Robert Bell, and Chris Volinsky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Shiqian Ma, Donald Goldfarb, and Lifeng Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Shai Shalev-Shwartz and Shai Ben-David.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Stanislaw J Szarek.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Michel Talagrand.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Kim-Chuan Toh and Sangwoon Yun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "1 from [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "1 from [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "We start with a lemma from [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "We next use a result from large deviations theory [25]:", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "2 in [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 23, "context": "[24] Let A \u2208 Rn\u00d7k be a random matrix A i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We also use the following lemma from [23]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Lemma 7 is a direct result of the Johnson-Lindenstrauss lemma [11] applied to each vector in Q and using the union bound.", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "6 Low Rank matrix Approximation We bring here the one pass algorithm to approximate X from [15] for the convenience of the reader.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "We compare our method to the rank estimation which appears in OptSpace [17] for the standard MC problem.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "(10) in the main text, and for the MC model we used the method described in [17].", "startOffset": 76, "endOffset": 80}], "year": 2015, "abstractText": "We propose and study a row-and-column affine measurement scheme for lowrank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and least-squares (LS), which we term SVLS . We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-andcolumn design and SVLS algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed rowand-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.", "creator": "LaTeX with hyperref package"}}}