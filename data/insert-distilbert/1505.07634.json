{"id": "1505.07634", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2015", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "abstract": "convex potential minimisation is the de facto approach to binary classification. however, long and servedio [ 2010 ] proved that under symmetric zero label noise ( sln ), minimisation of adding any convex potential over a linear function binary class can result in classification performance equivalent to random guessing. this ostensibly shows that convex losses are not sln - robust. in this paper, we propose a strongly convex, classification - calibrated loss and prove that it is sln - robust. the negative loss avoids the long and servedio [ 2010 ] result by virtue of being negatively unbounded. the loss assumption is a weakly modification of the hinge loss, where one does not clamp at zero ; hence, typically we call it the unhinged loss. we show that the optimal unhinged solution is equivalent somewhat to that of a strongly regularised svm, and is the limiting solution for any convex potential ; because this implies that strong l2 regularisation makes most standard learners sln - robust. experiments confirm the sln - robustness of the unhinged loss.", "histories": [["v1", "Thu, 28 May 2015 10:38:56 GMT  (419kb,D)", "http://arxiv.org/abs/1505.07634v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan van rooyen", "aditya krishna menon", "robert c williamson"], "accepted": true, "id": "1505.07634"}, "pdf": {"name": "1505.07634.pdf", "metadata": {"source": "CRF", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "authors": ["Brendan van Rooyen", "Aditya Krishna Menon", "Robert C. Williamson"], "emails": ["}@nicta.com.au"], "sections": [{"heading": "1 Learning with symmetric label noise", "text": "Binary classification is the canonical supervised learning problem. Given an instance space X, and samples from some distribution D over X \u00d7 {\u00b11}, the goal is to learn a scorer s : X \u2192 R with low misclassification error on future samples drawn from D. Our interest is in the more realistic scenario where the learner observes samples from a distribution D, which is a corruption of D where labels have some constant probability of being flipped. The goal is still to perform well with respect to the unobserved distribution D. This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988].\nLong and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \u201cSLN-robust\u201d and motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013].\nIn this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result of Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modification of the hinge loss where one does not clamp at zero; thus, we call it the unhinged loss. We show that this is the unique convex loss (up to scaling and translation) that satisfies a notion of \u201cstrong SLN-robustness \u201d (Proposition 4). In addition to being SLN-robust, this loss has several attractive properties, such as being classification-calibrated (Proposition 5), consistent when minimised on the corrupted distribution (Proposition 6), and having an easily computable optimal solution that is the difference of two kernel means (Equation 9). Finally, we show that this optimal solution is equivalent to that of a strongly regularised SVM (Proposition 7), and such a result holds more generally for any twice-differentiable convex potential (Proposition 8), implying that strong `2 regularisation endows most standard learners with SLN-robustness.\nar X\niv :1\n50 5.\n07 63\n4v 1\n[ cs\n.L G\n] 2\n8 M\nThe classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Scho\u0308lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section 5.1]. However, establishing this classifier\u2019s SLN-robustness, its equivalence to a highly regularised SVM solution, and showing the underlying loss uniquely satisfies a notion of strong SLN-robustness, to our knowledge is novel."}, {"heading": "2 Background and problem setup", "text": "Fix an instance space X. We denote by D some distribution over X\u00d7{\u00b11}, with random variables (X,Y) \u223c D. Any D may be expressed via the class-conditional distributions (P,Q) = (P(X | Y = 1),P(X | Y = \u22121)) and base rate \u03c0 = P(Y = 1), or equivalently via the marginal distribution M = P(X) and classprobability function \u03b7 : x 7\u2192 P(Y = 1 | X = x). We interchangeably write D as DP,Q,\u03c0 or DM,\u03b7."}, {"heading": "2.1 Classifiers, scorers, and risks", "text": "A scorer is any function s : X \u2192 R. A loss is any function ` : {\u00b11} \u00d7 R \u2192 R. We use `\u22121, `1 to refer to `(\u22121, \u00b7) and `(1, \u00b7). The `-conditional risk L` : [0, 1] \u00d7 R \u2192 R is defined as L` : (\u03b7, v) 7\u2192 \u03b7 \u00b7 `1(v) + (1 \u2212 \u03b7) \u00b7 `\u22121(v). Given a distribution D, the `-risk of a scorer s is defined as\nLD` (s) . = E\n(X,Y)\u223cD [`(Y, s(X))] , (1)\nor equivalently LD` (s) = E X\u223cM [L`(\u03b7(X), s(X))]. For a set S, LD` (S) is the set of `-risks for all scorers in S.\nA function class is any F \u2286 RX. Given some F, the set of restricted Bayes-optimal scorers for a loss ` are those scorers in F that minimise the `-risk:\nS D,F,\u2217 ` . = Argmin s\u2208F LD` (s).\nThe set of (unrestricted) Bayes-optimal scorers is SD,\u2217` = S D,F,\u2217 ` for F = RX. The restricted `-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer:\nregretD,F` (s) . = LD` (s)\u2212 inf t\u2208F LD` (t).\nBinary classification is concerned with the risk corresponding to the zero-one loss, `01 : (y, v) 7\u2192 Jyv < 0K + 12Jv = 0K. A loss ` is classification-calibrated if all its Bayes-optimal scorers are also optimal for zeroone loss: (\u2200D) SD,\u2217` \u2286 S D,\u2217 01 .A convex potential is any loss ` : (y, v) 7\u2192 \u03c6(yv), where \u03c6 : R\u2192 R+ is convex, non-increasing, differentiable with \u03c6\u2032(0) < 0, and \u03c6(+\u221e) = 0 [Long and Servedio, 2010, Definition 1]. All convex potential losses are classification-calibrated [Bartlett et al., 2006, Theorem 2.1]."}, {"heading": "2.2 Learning with symmetric label noise (SLN learning)", "text": "The problem of learning with symmetric label noise (SLN learning) is the following [Angluin and Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some notional \u201cclean\u201d distribution D, which we would like to observe, we instead observe samples from some corrupted distribution SLN(D, \u03c1), for some \u03c1 \u2208 [0, 1/2). The distribution SLN(D, \u03c1) is such that the marginal distribution of instances is unchanged, but each label is independently flipped with probability \u03c1. The goal is to learn a scorer from these corrupted samples such that LD01(s) is small.\nFor any quantity in D, we denote its corrupted counterparts in SLN(D, \u03c1) with a bar, e.g. M for the corrupted marginal distribution, and \u03b7 for the corrupted class-probability function; additionally, when \u03c1 is clear from context, we will occasionally refer to SLN(D, \u03c1) by D. By definition of the corruption process, the corruption marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7]\n(\u2200x \u2208 X) \u03b7(x) = (1\u2212 2\u03c1) \u00b7 \u03b7(x) + \u03c1. (2)"}, {"heading": "3 SLN-robustness: formalisation", "text": "For our purposes, a learner (`,F) comprises a loss `, and a function class F, with learning being the search for some s \u2208 F that minimises the `-risk. Informally, the learner (`,F) is \u201crobust\u201d to symmetric label noise (SLN-robust) if minimising ` over F gives the same classifier on both the clean distribution D, which the learner would like to observe, and SLN(D, \u03c1) for any \u03c1 \u2208 [0, 1/2), which the learner actually observes. We now formalise this notion, and review what is known about the existence of SLN-robust learners."}, {"heading": "3.1 SLN-robust learners: a formal definition", "text": "For some fixed instance space X, let \u2206 denote the set of distributions on X\u00d7{\u00b11}. Given a notional \u201cclean\u201d distribution D, Nsln : \u2206 \u2192 2\u2206 returns the set of possible corrupted versions of D the learner may observe, where labels are flipped with unknown probability \u03c1:\nNsln : D 7\u2192 { SLN(D, \u03c1) | \u03c1 \u2208 [ 0, 1\n2\n)} .\nEquipped with this, we define our notion of SLN-robustness.\nDefinition 1 (SLN-robustness). We say that a learner (`,F) is SLN-robust if\n(\u2200D \u2208 \u2206) (\u2200D \u2208 Nsln(D))LD01(S D,F,\u2217 ` ) = L D 01(S D,F,\u2217 ` ). (3)\nThat is, SLN-robustness requires that for any level of label noise in the observed distribution D, the classification performance (wrt D) of the learner is the same as if the learner directly observes D. Unfortunately, as we will now see, a widely adopted class of learners is not SLN-robust."}, {"heading": "3.2 Convex potentials with linear function classes are not SLN-robust", "text": "Fix X = Rd, and consider learners employing a convex potential `, and a function class of linear scorers\nFlin = {x 7\u2192 \u3008w, x\u3009 | w \u2208 Rd}.\nThis captures e.g. the linear SVM and logistic regression, which are widely studied in theory and applied in practice. Unfortunately, these learners are not SLN-robust: Long and Servedio [2010, Theorem 2] give an example where, when learning under symmetric label noise, for any convex potential `, the corrupted `-risk minimiser over Flin has classification performance equivalent to random guessing on D. This implies that (`,Flin) is not SLN-robust1 as per Definition 1. (All Proofs may be found in Appendix A.)\nProposition 1 (Long and Servedio [2010, Theorem 2]). Let X = Rd for any d \u2265 2. Pick any convex potential `. Then, (`,Flin) is not SLN-robust.\nThe widespread practical use of convex potential based learners makes Proposition 1 a disheartening result, and motivates the search for other learners that are SLN-robust.\n3.3 The fallout: what learners are SLN-robust? In light of Proposition 1, there are two ways to proceed in order to obtain SLN-robust learners: either we change the class of losses `, or we change the function class F.\nThe first approach has been pursued in a large body of work that embraces non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013]. However, while such losses avoid the conditions of Proposition 1, this does not automatically imply that they are SLN-robust when used with Flin. In Appendix B, we present evidence that some of these losses are in fact not SLN-robust when used with Flin.\nThe second approach is to instead consider suitably rich F that contains the Bayes-optimal scorer for D, e.g. by employing a universal kernel. With this choice, one can still use a convex potential loss; in fact, owing to Equation 2, using any classification-calibrated loss will result in an SLN-robust learner when F = RX.\n1Even if we weaken the notion of SLN-robustness to allow for a difference of \u2208 [0, 1/2] between the clean and corrupted minimisers\u2019 performance, Long and Servedio [2010, Theorem 2] implies that in the worst case = 1/2.\nProposition 2. Pick any classification-calibrated `. Then, (`,RX) is SLN-robust.\nBoth approaches have drawbacks. The first approach has a computational penalty, as it requires optimising a non-convex loss. The second approach has a statistical penalty, as estimation rates with a rich F will require a larger sample size. Thus, it appears that SLN-robustness involves a computational-statistical tradeoff.\nHowever, there is a variant of the first option: pick a loss that is convex, but not a convex potential. If an SLN-robust loss of this type exists, it affords the computational and statistical advantages of minimising convex risks with linear scorers. Manwani and Sastry [2013] demonstrated that square loss, `(y, v) = (1\u2212 yv)2, is one such loss. We will show that there is a simpler loss that is similarly convex, classification-calibrated, and SLN-robust, but is not in the class of convex potentials by virtue of being negatively unbounded. To derive this loss, it is helpful to interpret robustness in terms of a noise-correction procedure on loss functions."}, {"heading": "4 SLN-robustness: a noise-corrected loss perspective", "text": "The definition of SLN-robustness (Equation 3) involves optimal scorers with the same loss ` over two different distributions. We now re-express this to reason about optimal scorers on the same distribution, but with two different losses. This will help characterise the set of losses that are SLN-robust."}, {"heading": "4.1 Reformulating SLN-robustness via noise-corrected losses", "text": "Given any \u03c1 \u2208 [0, 1/2), Natarajan et al. [2013, Lemma 1] showed how to associate with a loss ` a noisecorrected counterpart `, such that for any D, LD` (s) = LD` (s). The loss ` is defined as follows.\nDefinition 2 (Noise-corrected loss). Given any loss ` and \u03c1 \u2208 [0, 1/2), the noise-corrected loss ` is\n(\u2200y \u2208 {\u00b11}) (\u2200v \u2208 R) `(y, v) = (1\u2212 \u03c1) \u00b7 `(y, v)\u2212 \u03c1 \u00b7 `(\u2212y, v) 1\u2212 2\u03c1 . (4)\nSince ` depends on the unknown parameter \u03c1, it is not directly usable to design an SLN-robust learner. Nonetheless, it is a useful theoretical construct, since the risk equivalence between LD` (s) and LD` (s) means that for any F, minimisation of the `-risk on D over F is equivalent to minimisation of the `-risk on D over F, i.e. SD,F,\u2217` = S D,F,\u2217 ` . With this, we can re-express the SLN-robustness of a learner (`,F) as\n(\u2200D \u2208 \u2206) (\u2200D \u2208 Nsln(D))LD01(S D,F,\u2217 ` ) = LD01(S D,F,\u2217 ` ). (5)\nThis reformulation is useful, because to characterise SLN-robustness of (`,F), we can now consider conditions on ` such that ` and its noise-corrected counterpart ` induce the same restricted Bayes-optimal scorers."}, {"heading": "4.2 Characterising a stronger notion of SLN-robustness", "text": "Manwani and Sastry [2013, Theorem 1] proved a sufficient condition on ` such that Equation 5 holds, namely,\n(\u2203C \u2208 R)(\u2200v \u2208 R) `1(v) + `\u22121(v) = C. (6)\nFor such a loss, ` is a scaled and translated version of `, so that trivially SD,F,\u2217` = S D,F,\u2217 `\n. Ideally, one would like to characterise when Equation 5 holds. While this is an open question, interestingly, we can show that under a stronger requirement on the losses ` and `, the condition in Equation 6 is also necessary. The stronger requirement is that the corresponding risks order all stochastic scorers identically. A stochastic scorer is simply a mapping f : X\u2192 \u2206R, where \u2206R is the set of distributions over the reals. In a slight abuse of notation, we denote the `-stochastic risk of f by\nLD` (f) = E (X,Y)\u223cD\n[ E\nS\u223cf(X) [`(Y,S)]\n] .\nEquipped with this, we define a notion of order equivalence of loss pairs.\nDefinition 3 (Order equivalent loss pairs). We say that a pair of losses (`, \u02dc\u0300) are order equivalent if\n(\u2200D) (\u2200f, g \u2208 \u2206XR )LD` (f) \u2264 LD` (g) \u21d0\u21d2 LD\u02dc\u0300 (f) \u2264 LD\u02dc\u0300 (g).\nClearly, if two losses are order equivalent, their corresponding risks have the same restricted minimisers. Consequently, if (`, `) are order equivalent for every \u03c1 \u2208 [0, 1/2), this implies that SD,F,\u2217` = S D,F,\u2217 `\nfor any F, which by Equation 5 means that for any F, the learner (`,F) is SLN-robust. We can thus think of order equivalence of (`, `) as signifying strong SLN-robustness of a loss `.\nDefinition 4 (Strong SLN-robustness). We say a loss ` is strongly SLN-robust if for every \u03c1 \u2208 [0, 1/2), (`, `) are order equivalent.\nWe establish that the sufficient condition of Equation 6 is also necessary for strong SLN-robustness of `.\nProposition 3. A loss ` is strongly SLN-robust if and only if it satisfies Equation 6.\nWe now return to our original goal, which was to find a convex ` that is SLN-robust for Flin (and ideally more general function classes). The above suggests that to do so, it is reasonable to consider as admissible those losses that satisfy Equation 6. Unfortunately, it is evident that if ` is convex, non-constant, and bounded below by zero, then it cannot possibly be admissible in this sense. But we now show that removing the boundedness restriction allows for the existence of a convex admissible loss."}, {"heading": "5 The unhinged loss: a convex, classification-calibrated, strongly SLNrobust loss", "text": "Consider the following simple, but non-standard convex loss:\n`unh1 (v) = 1\u2212 v and `unh\u22121 (v) = 1 + v.\nA peculiar property of the loss is that it is negatively unbounded, an issue we discuss in \u00a75.3. Compared to the hinge loss, the loss does not clamp at zero, i.e. it does not have a hinge. Thus, we call this the unhinged loss2. The loss has a number of attractive properties, the most immediate of which is its SLN-robustness."}, {"heading": "5.1 The unhinged loss is strongly SLN-robust", "text": "Since `unh1 (v) + ` unh \u22121 (v) = 0 we conclude from Proposition 3 that `\nunh is strongly SLN-robust, and thus that (`unh,F) is SLN-robust for any choice of F. Further, the following uniqueness property is not hard to show.\nProposition 4. Pick any convex loss `. Then,\n(\u2203C \u2208 R) `1(v) + `\u22121(v) = C \u21d0\u21d2 (\u2203A,B,D \u2208 R) `1(v) = \u2212A \u00b7 v +B, `\u22121(v) = A \u00b7 v +D.\nThat is, up to scaling and translation, `unh is the only convex loss that is strongly SLN-robust.\nReturning to the case of linear scorers, the above implies that (`unh,Flin) is SLN-robust. This does not contradict Proposition 1, since `unh is not a convex potential as it is negatively unbounded. Intuitively, this property allows the loss to compensate for the high penalty incurred by instances that are misclassified with high margin by allowing for a high gain for instances that correctly classified with high margin.\n2This loss has been considered in Sriperumbudur et al. [2009], Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.4. The analysis of its SLN-robustness is to our knowledge novel."}, {"heading": "5.2 The unhinged loss is classification calibrated", "text": "SLN-robustness is by itself insufficient for a learner to be useful. For example, a loss that is uniformly zero is strongly SLN-robust, but is useless as it is not classification-calibrated. Fortunately, the unhinged loss is classification-calibrated, as we now establish. For reasons that shall be discussed subsequently, we consider minimisation of the risk over FB = [\u2212B,+B]X, the set of scorers with range bounded by B \u2208 [0,\u221e).\nProposition 5. Fix ` = `unh. Then, for any DM,\u03b7, B \u2208 [0,\u221e), SD,FB ,\u2217` = {x 7\u2192 B \u00b7 sign(2\u03b7(x)\u2212 1)}.\nThus, for every B \u2208 [0,\u221e), the restricted Bayes-optimal scorer over FB has the same sign as the Bayesoptimal classifier for 0-1 loss. In the limiting case where F = RX, the optimal scorer is attainable if we operate over the extended reals R \u222a {\u00b1\u221e}, in which case we can conclude that `unh is classification-calibrated."}, {"heading": "5.3 Enforcing boundedness of the loss", "text": "While the classification-calibration of `unh is encouraging, Proposition 5 implies that its (unrestricted) Bayesrisk is \u2212\u221e. Thus, the regret of every non-optimal scorer s is identically +\u221e, which hampers analysis of consistency. In orthodox decision theory, analogous theoretical issues arise when attempting to establish basic theorems with unbounded losses [Ferguson, 1967, pg. 78], [Schervish, 1995, pg. 172].\nWe can side-step this issue by restricting attention to bounded scorers, so that `unh is effectively bounded. By Proposition 5, this does not affect the classification-calibration of the loss. In the context of linear scorers, boundedness of scorers can be achieved by regularisation: instead of working with Flin, one can instead use Flin,\u03bb = {x 7\u2192 \u3008w, x\u3009 | ||w||2 \u2264 1/ \u221a \u03bb}, where \u03bb > 0, so that Flin,\u03bb \u2286 FR/\u221a\u03bb for R = supx\u2208X ||x||2. Observe that restricting to bounded scorers does not affect the SLN-robustness of `unh, because (`unh,F) is SLN-robust for any F. Thus, for example, (`unh,Flin,\u03bb) is SLN-robust for any \u03bb > 0. As we shall see in \u00a76.3, working with Flin,\u03bb also lets us establish SLN-robustness of the hinge loss when \u03bb is large."}, {"heading": "5.4 Unhinged loss minimisation on corrupted distribution is consistent", "text": "Using bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss. This shows classification consistency of unhinged loss minimisation on the corrupted distribution.\nProposition 6. Fix ` = `unh. Then, for any D, \u03c1 \u2208 [0, 1/2), B \u2208 [1,\u221e), and scorer s \u2208 FB ,\nregretD01(s) \u2264 regret D,FB ` (s) =\n1\n1\u2212 2\u03c1 \u00b7 regretD,FB` (s).\nStandard rates of convergence via generalisation bounds are also trivial to derive; see Appendix D. We now turn to the question of how to minimise the unhinged loss when using a kernelised scorer."}, {"heading": "6 Learning with the unhinged loss and kernels", "text": "We now show that the optimal solution for the unhinged loss when employing regularisation and kernelised scorers has a simple form. This sheds further light on SLN-robustness and regularisation."}, {"heading": "6.1 The centroid classifier optimises the unhinged loss", "text": "Consider minimising the unhinged risk over some ball in a reproducing kernel Hilbert space H with kernel k, i.e. consider the function class of kernelised scorers FH,\u03bb = {s : x 7\u2192 \u3008w,\u03a6(x)\u3009H | ||w||H \u2264 1/ \u221a \u03bb} for some \u03bb > 0, where \u03a6: X\u2192 H is some feature mapping. Equivalently, given a distribution3 D, we want\nw\u2217unh,\u03bb = argmin w\u2208H E (X,Y)\u223cD [1\u2212 Y \u00b7 \u3008w,\u03a6(X)\u3009] + \u03bb 2 \u3008w,w\u3009H. (7)\n3Given a training sample S \u223c Dn, we can use plugin estimates as appropriate.\nThe first-order optimality condition implies that\nw\u2217unh,\u03bb = 1\n\u03bb \u00b7 E (X,Y)\u223cD [Y \u00b7 \u03a6(X)] . (8)\nThus, the optimal scorer for the unhinged loss is simply\ns\u2217unh,\u03bb : x 7\u2192 1\n\u03bb \u00b7 E (X,Y)\u223cD [Y \u00b7 k(X, x)] = x 7\u2192 1 \u03bb \u00b7 ( \u03c0 \u00b7 E X\u223cP [k(X, x)]\u2212 (1\u2212 \u03c0) \u00b7 E X\u223cQ [k(X, x)] ) . (9)\nThat is, we score an instance based on the difference of the aggregate similarity to the positive instances, and the aggregate similarity to the negative instances. This is equivalent to a nearest centroid classifier [Manning et al., 2008, pg. 181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.1]. The quantity w\u2217unh,\u03bb can be interpreted as the kernel mean map of D; see Appendix E for more related work.\nEquation 9 gives a simple way to understand the SLN-robustness of (`unh,FH,\u03bb): it is easy to establish (see Appendix C) that the optimal scorers on the clean and corrupted distributions only differ by a scaling, i.e.\n(\u2200x \u2208 X) E (X,Y)\u223cD [Y \u00b7 k(X, x)] = 1 1\u2212 2\u03c1 \u00b7 E (X,Y)\u223cD\n[ Y \u00b7 k(X, x) ] . (10)"}, {"heading": "6.2 Practical considerations", "text": "We note several points relating to practical usage of the unhinged loss with kernelised scorers. First, crossvalidation is not required to select \u03bb, since s\u2217unh,\u03bb depends trivially on the regularisation constant: changing \u03bb only changes the magnitude of scores, not their sign. Thus, regularisation simply controls the scale of the predicted scores, and for the purposes of classification, one can simply use \u03bb = 1.\nSecond, we can easily extend the scorers to use a bias regularised with strength 0 < \u03bbb 6= \u03bb. Tuning \u03bbb is equivalent to computing s\u2217unh,\u03bb as per Equation 9, and tuning a threshold on a holdout set.\nThird, when H = Rd for d small, we can store w\u2217unh,\u03bb explicitly, and use this to make predictions. For high (or infinite) dimensional H, we can make predictions directly via Equation 9. However, when learning with a training sample S \u223c Dn, this would require storing the entire sample for use at test time, which is undesirable. To alleviate this, for a translation-invariant kernel one can use random Fourier features [Rahimi and Recht, 2007] to find an approximate embedding of H into some low-dimensional Rd, and then store w\u2217unh,\u03bb as usual. Alternately, one can post hoc search for a sparse approximation to w \u2217 unh,\u03bb, for example using kernel herding [Chen et al., 2012]. We now show that under some assumptions, w\u2217unh,\u03bb coincides with the solution of two established methods; Appendix E discusses some further relationships, e.g. to the maximum mean discrepancy."}, {"heading": "6.3 Equivalence to a highly regularised SVM and other convex potentials", "text": "There is an interesting equivalence between the unhinged solution and that of a highly regularised SVM.\nProposition 7. Pick any D and \u03a6: X\u2192 H such that R = supx\u2208X ||\u03a6(x)||H <\u221e. For any \u03bb > 0, let\nw\u2217hinge,\u03bb = argmin w\u2208H E (X,Y)\u223cD\n[max(0, 1\u2212 Y \u00b7 \u3008w,\u03a6(x)\u3009H)] + \u03bb\n2 \u3008w,w\u3009H\nbe the soft-margin SVM solution. Then, if \u03bb \u2265 R2, w\u2217hinge,\u03bb = w\u2217unh,\u03bb.\nSince we know that (`unh,FH,\u03bb) is SLN-robust, it follows immediately that for `hinge : (y, v) 7\u2192 max(0, 1\u2212 yv), (`hinge,FH,\u03bb) is similarly SLN-robust provided \u03bb is sufficiently large. That is, strong `2 regularisation (and a bounded feature map) endows the hinge loss with SLN-robustness4.\nProposition 7 can be generalised to show that with sufficiently strong regularisation, the limiting solution of any twice differentiable convex potential will be the unhinged solution, i.e. , the centroid classifier. Intuitively, with strong regularisation, one only considers the behaviour of a loss near zero; but since a convex\n4By contrast, Long and Servedio [2010, Section 6] establish that `1 regularisation does not endow SLN-robustness.\npotential \u03c6 has \u03c6\u2032(0) < 0, it will be well-approximated by the unhinged loss near zero (which is simply the linear approximation to \u03c6). This shows that strong `2 regularisation endows most learners with SLNrobustness.\nProposition 8. Pick any D, bounded feature mapping \u03a6: X\u2192 H, and twice differentiable convex potential \u03c6. Let w\u2217\u03c6,\u03bb be the minimiser of the regularised \u03c6 risk. Then,\n(\u2200 > 0) (\u2203\u03bb0 > 0) (\u2200\u03bb > \u03bb0) \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 w\u2217\u03c6,\u03bb||w\u2217\u03c6,\u03bb||H \u2212 w \u2217 unh,\u03bb ||w\u2217unh,\u03bb||H \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2\nH\n\u2264 ."}, {"heading": "6.4 Equivalence to Fisher Linear Discriminant with whitened data", "text": "Recall that for binary classification on DM,\u03b7, the Fisher Linear Discriminant (FLD) finds a weight vector proportional to the minimiser of square loss `sq : (y, v) 7\u2192 (1\u2212 yv)2 [Bishop, 2006, Section 4.1.5],\nw\u2217sq,\u03bb = (EX\u223cM [XXT ] + \u03bbI)\u22121 \u00b7 E(X,Y)\u223cD[Y \u00b7 X]. (11)\nBy Equation 10, and the fact that the corrupted marginal M = M , we see that w\u2217sq,\u03bb is only changed by a scaling factor under label noise. This provides an alternate proof of the fact that (`sq,Flin) is SLN-robust5 [Manwani and Sastry, 2013, Theorem 2]. Clearly, the unhinged loss solution w\u2217unh,\u03bb is equivalent to the FLD and square loss solution w \u2217 sq,\u03bb when the input data is whitened i.e. E X\u223cM [ XXT ] = I . With a well-specified F, e.g. with a universal kernel, both the unhinged and square loss asymptotically recover the optimal classifier, but the unhinged loss does not require a matrix inversion. With a misspecified F, one cannot in general argue for the superiority of the unhinged loss over square loss, or vice-versa, as there is no universally good surrogate to the 0-1 loss [Reid and Williamson, 2010, Appendix A]; Appendix F, Appendix G illustrate examples where both losses may underperform."}, {"heading": "7 SLN-robustness of unhinged loss: empirical illustration", "text": "We now illustrate that the SLN-robustness of the unhinged loss is empirically manifest. We reiterate that with high regularisation, the unhinged solution is equivalent to an SVM (and in the limit to any classificationcalibrated loss) solution. Thus, the experiments do not aim to assert that the unhinged loss is \u201cbetter\u201d than other losses, but rather, to demonstrate that its SLN-robustness is not purely theoretical.\nWe first show that the unhinged risk minimiser performs well on the example of Long and Servedio [2010]. Figure 1 shows the distribution D, where X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3,\u2212\u03b3)} \u2282 R2, with marginal distribution M = { 14 , 1 4 , 1 2} and all three instances are deterministically positive. We pick \u03b3 = 1/2. From Figure 1, we see the unhinged minimiser perfectly classifies all three points, regardless of the level of label noise. The hinge risk minimiser is perfect when there is no label noise, but with even a small amount of label noise, achieves an error rate of 50%.\n0.5 1\n\u22121\n\u22120.5\n0.5\n1 Unhinged Hinge 0% noise Hinge 1% noise\nFigure 1: Long and Servedio [2010] dataset.\nHinge t-logistic Unhinged\n\u03c1 = 0 0.00\u00b1 0.00 0.00\u00b1 0.00 0.00\u00b1 0.00 \u03c1 = 0.1 0.15\u00b1 0.27 0.00\u00b1 0.00 0.00\u00b1 0.00 \u03c1 = 0.2 0.21\u00b1 0.30 0.00\u00b1 0.00 0.00\u00b1 0.00 \u03c1 = 0.3 0.38\u00b1 0.37 0.22\u00b1 0.08 0.00\u00b1 0.00 \u03c1 = 0.4 0.42\u00b1 0.36 0.22\u00b1 0.08 0.00\u00b1 0.00 \u03c1 = 0.49 0.47\u00b1 0.38 0.39\u00b1 0.23 0.34\u00b1 0.48\nTable 1: Mean and standard deviation of the 0-1 error over 125 trials on Long and Servedio [2010]. Grayed cells denote the best performer at that noise rate.\n5Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.\nWe next consider minimisers of the empirical risk from a random training sample: we construct a training set of 800 instances, injected with varying levels of label noise, and evaluate classification performance on a test set of 1000 instances. We compare the hinge, t-logistic (for t = 2) [Ding and Vishwanathan, 2010] and unhinged minimisers. For each loss, we use a linear scorer without a bias term, and set the regularisation strength \u03bb = 10\u221216. From Table 1, it is apparent that even at 40% label noise, the unhinged classifier is able to find a perfect solution. By contrast, both other losses suffer at even moderate noise rates.\nWe next report results on some UCI datasets, where we additionally tune a threshold so as to ensure the best training set 0-1 accuracy. Table 2 summarises results on a sample of four datasets. (Appendix H contains results with more datasets, performance metrics, and losses.) While the unhinged loss is sometimes outperformed at low noise, it tends to be much more robust at high levels of noise: even at noise close to 50%, it is often able to learn a classifier with some discriminative power."}, {"heading": "8 Conclusion and future work", "text": "We have proposed a convex, classification-calibrated loss, proved that is robust to symmetric label noise (SLN-robust), shown it is the unique loss that satisfies a notion of strong SLN-robustness, established that it is optimised by the nearest centroid classifier, and also shown how the nature of the optimal solution implies that most convex potentials, such as the SVM, are also SLN-robust when highly regularised. Future work includes studying losses robust to asymmetric noise, and outliers in instance space."}, {"heading": "Acknowledgments", "text": "NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The authors thank Cheng Soon Ong for valuable comments on a draft of this paper.\nProofs for \u201cLearning with Symmetric Label Noise: The Importance of Being Unhinged\u201d"}, {"heading": "A Proofs of results in main body", "text": "We now present proofs of all results in the main body.\nProof of Proposition 1. This result is stated implicitly in Long and Servedio [2010, Theorem 2]; the aim of this proof is simply to make the result explicit.\nLet X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3,\u2212\u03b3), (\u03b3,\u2212\u03b3)} \u2282 R2, for some \u03b3 < 1/6. Let the marginal distribution over X be uniform. Let \u03b7 : x 7\u2192 1, i.e. let every example be deterministically positive.\nNow suppose we observe some SLN(D, \u03c1), for \u03c1 \u2208 [0, 1/2). We minimise the `-risk some convex potential ` : (y, v) 7\u2192 \u03c6(y, v) using a linear function class6 Flin. Then, Long and Servedio [2010, Theorem 2] establishes that\n(\u2200s \u2208 SD,Flin,\u2217` )L D 01(s) =\n1 2 .\nOn the other hand, since D is linearly separable and a convex potential ` is classification-calibrated, we must have LD01(S D,Flin,\u2217 ` ) = 0. Consequently, for any convex potential `, (`,Flin) is not SLN-robust.\nProof of Proposition 2. Let \u03b7 be the class-probability function of D. By [Natarajan et al., 2013, Lemma 7],\n(\u2200x \u2208 X) sign(2\u03b7(x)\u2212 1) = sign(2\u03b7(x)\u2212 1),\nso that the optimal classifiers on the clean and corrupted distributions coincide. Therefore, intuitively, if the Bayes-optimal solution for loss recovers sign(2\u03b7(x) \u2212 1), it will also recover sign(2\u03b7(x) \u2212 1). Formally, since ` is classification-calibrated, for any D \u2208 \u2206, and s \u2208 SD,\u2217`\n(\u2200x \u2208 X) sign(s(x)) = sign(2\u03b7(x)\u2212 1),\nand similarly, for any D \u2208 Nsln(D), and s\u0304 \u2208 SD,\u2217`\n(\u2200x \u2208 X) sign(s\u0304(x)) = sign(2\u03b7(x)\u2212 1).\nThus, for any D,D, since the 0-1 risk of a scorer depends only on its sign,\nLD01(s) = LD01(s) = LD01(sign(2\u03b7 \u2212 1)) = LD01(sign(2\u03b7 \u2212 1)) = LD01(s\u0304).\nConsequently, i.e. (`,RX) is SLN-robust.\nProof of Proposition 3. (\u21d0= ). If ` satisfies Equation 6, then its noise corrected counterpart is\n(\u2200y \u2208 {\u00b11})(\u2200v \u2208 R) `(y, v) = 1 1\u2212 2\u03c1 \u00b7 `(y, v)\u2212 C \u00b7 \u03c1 1\u2212 2\u03c1 ,\nthat is, it is a scaled and translated version of `. Consequently, for any \u03c1, the corresponding risk will be a scaled and translated version of the `-risk. It is immediate that the two losses will be order equivalent for any \u03c1.\n( =\u21d2 ). Recall that S denotes the distribution of scores. For any stochastic scorer f , let\nSf : a 7\u2192 P(S = a) 6The result actually requires that one not include a bias term; with a bias term, it can be checked that the example as-stated has a\ntrivial solution.\nbe the corresponding marginal distribution of scores. Similarly, let\nMa : x 7\u2192 P(X = x | S = a)\nbe the conditional distribution of instances given a predicted score a \u2208 R. Finally, for any a \u2208 R, let Da = (Ma, \u03b7) be an induced distribution over X\u00d7 {\u00b11}.\nWith the above, we can rewrite the stochastic risk as\nLD` (f) = E S\u223cSf\n[ E\n(X,Y)\u223cDS [`(Y,S)] ] = E\nS\u223cSf\n[ LDS` (S) ] .\nThat is, we average, over all achievable scores according to f , the risks of that constant prediction on an appropriately reweighed version of the original distribution D. Then, for some fixed \u03c1 \u2208 [0, 1/2), the fact that ` and ` are order equivalent can be written\n(\u2200D) (\u2200f, g \u2208 \u2206XR ) E S\u223cSf\n[ LDS` (S) ] \u2264 E\nS\u223cSg\n[ LDS` (S) ] \u21d0\u21d2 E\nS\u223cSf\n[ LDS ` (S) ] \u2264 E\nS\u223cSg\n[ LDS ` (S) ] .\nNow define the utility functions UD : a 7\u2192 \u2212LDa` (a)\nand V D : a 7\u2192 \u2212LDa\n` (a).\nThen, order equivalence can be trivially re-expressed as\n(\u2200D) (\u2200f, g \u2208 \u2206XR ) E S\u223cSf\n[ UD(S) ] \u2265 E\nS\u223cSg\n[ UD(S) ] \u21d0\u21d2 E\nS\u223cSf\n[ V D(S) ] \u2265 E\nS\u223cSg\n[ V D(S) ] .\nThat is, for any fixed distribution D, the utility functions UD, V D specify the same ordering over distributions in \u2206R. Therefore, by DeGroot [1970, Section 7.9, Theorem 2], for any fixed D, they must be affinely related:\n(\u2200D) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R)UD(a) = \u03b1 \u00b7 V D(a) + \u03b2. Converting this back to losses, and using the definition of strong SLN-robustness,\n(\u2200D) (\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R)LDa` (a) = \u03b1 \u00b7 L Da ` (a) + \u03b2\nor (\u2200D) (\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200a \u2208 R) E\n(X,Y)\u223cDa\n[ `(Y, a)\u2212 (\u03b1 \u00b7 `(Y, a) + \u03b2) ] = 0.\nFor this to hold for all possible D, it must be true that\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200y, v) `(y, v) = \u03b1 \u00b7 `(y, v) + \u03b2.\nBy Lemma 9, the result follows.\nProof of Proposition 4. (\u21d0= ). Clearly for an ` satisfying the given condition, `1(v) + `\u22121(v) = B + C, a constant.\n( =\u21d2 ). By assumption, `1 is convex. By the given condition, equivalently, (\u2203C \u2208 R)C \u2212 `1 is convex. But this is in turn equivalent to\u2212`1 also being convex. The only possibility for both `1 and\u2212`1 being convex is that `1 is affine, hence showing the desired implication.\nProof of Proposition 5. Fix ` = `unh. It is easy to check that\n(\u2200\u03b7 \u2208 [0, 1]) (\u2200v \u2208 R)L`(\u03b7, v) = (1\u2212 2\u03b7) \u00b7 v, (12)\nand so\n(\u2200\u03b7 \u2208 [0, 1]) argmin v\u2208[\u2212B,+B] L`(\u03b7, v) = { +B if \u03b7 > 12 \u2212B else.\nProof of Proposition 6. Fix ` = `unh. Since by Equation 12 L`unh(\u03b7, v) = (1\u2212 2\u03b7) \u00b7 v, we have that\nLD` (s) = \u2212 E X\u223cM [(2\u03b7(X)\u2212 1) \u00b7 s(X)] ,\nand since the restricted Bayes-optimal scorer is x 7\u2192 B \u00b7 sign(2\u03b7(x)\u2212 1),\nLD,FB ,\u2217` = \u2212B \u00b7 EX\u223cM [|2\u03b7(X)\u2212 1|] .\nThus, regretD,FB` (s) = EX\u223cM [|2\u03b7(X)\u2212 1| \u00b7 (B \u2212 s(X) \u00b7 sign(2\u03b7(X)\u2212 1))]\nNow, since the scorer x 7\u2192 sign(2\u03b7(x) \u2212 1) \u2208 SD,\u221701 \u2229 FB , we have that regret D,FB 01 (s) = regret D 01(s).\nFurther, we have that\nregretD01(s) = E X\u223cM [|2\u03b7(X)\u2212 1| \u00b7 Js(X) \u00b7 sign(2\u03b7(X)\u2212 1) < 0K] .\nBut if B \u2265 1, Jv < 0K \u2264 B \u2212 v.\nThus, regretD,FB01 (s) \u2264 regret D,FB ` (s).\nFinally, by Equation 4, for ` = `unh,\n(\u2200y \u2208 {\u00b11}) (\u2200v \u2208 R) `(y, v) = 1 1\u2212 2\u03c1 \u00b7 `(y, v),\ni.e. the unhinged loss is its own noise-corrected loss, with a scaling factor of 11\u22122\u03c1 . Thus, since the `-regret on D and `-regret on D coincide,\nregretD,FB` (s) = regret D,FB `\n(s) = 1\n1\u2212 2\u03c1 \u00b7 regretD,FB` (s).\nProof of Proposition 7. On a distribution D, a soft-margin SVM solves\nmin w\u2208H E (X,Y)\u223cD\n[max(0, 1\u2212 Y \u00b7 \u3008w,\u03a6(x)\u3009H)] + \u03bb\n2 \u3008w,w\u30092H.\nLet w\u2217hinge,\u03bb denote the optimal solution to this objective. Now, by Shalev-Shwartz et al. [2007, Theorem 1],\n||w\u2217hinge,\u03bb||H \u2264 1\u221a \u03bb .\nNow suppose that R = supx\u2208X ||\u03a6(x)||H <\u221e. Then, by the Cauchy-Schwartz inequality,\n(\u2200x \u2208 X) |\u3008w\u2217hinge,\u03bb,\u03a6(x)\u3009H| \u2264 ||w\u2217hinge,\u03bb||H \u00b7 ||\u03a6(x)||H \u2264 R\u221a \u03bb .\nIt follows that if \u03bb \u2265 R2, then (\u2200x \u2208 X) |\u3008w\u2217hinge,\u03bb,\u03a6(x)\u3009H| \u2264 1. But this means that we never activate the flat portion of the hinge loss. Thus, for \u03bb \u2265 R2, the SVM objective is equivalent to\nmin w\u2208H E (X,Y)\u223cD\n[1\u2212 Y \u00b7 \u3008w,\u03a6(x)\u3009H] + \u03bb\n2 \u3008w,w\u30092H.\nwhich means the optimal solution will coincide with that of the regularised unhinged loss. Therefore, we can view unhinged loss minimisation as corresponding to learning a highly regularised SVM7.\n7This also holds if we add a regularised bias term. With an unregularised bias term, Bedo et al. [2006] showed that the limiting solution of a soft-margin SVM is distribution dependent.\nProof of Proposition 8. Fix some distribution D. Let\n\u00b5 = E (X,Y)\u223cD [Y \u00b7 \u03a6(X)]\nbe the optimal unhinged solution with regularisation strength \u03bb = 1. Observe that ||\u00b5||H \u2264 R = supx\u2208X ||\u03a6(x)||H < \u221e. For some r > 0, let\nw\u2217\u03c6 = argmin ||w||H\u2264r LD\u03c6 (w)\nbe the optimal \u03c6 solution with norm bounded by r. Similarly, let\nw\u2217unh = ||w\u2217\u03c6|| \u00b7 \u00b5\n||\u00b5||H\nbe the optimal unhinged solution with the same norm as the optimal \u03c6 solution. We will show that these two vectors have similar unhinged risks, and use this to show that the corresponding unit vectors must be close.\nBy definition, a convex potential has \u03c6\u2032(0) < 0. Without loss of generality, we can scale the potential so that \u03c6\u2032(0) = \u22121. Then, since \u03c6 is convex, it is lower bounded by the linear approximation at zero:\n(\u2200v \u2208 R)\u03c6(v)\u2212 \u03c6(0) \u2265 \u2212v.\nObserve that the RHS is the unhinged loss. Thus, the unhinged risk can be bounded by its \u03c6 counterpart. In particular, at the optimal \u03c6 solution,\nLunh(w\u2217\u03c6) \u2264 L\u03c6(w\u2217\u03c6)\u2212 \u03c6(0).\nTherefore, the difference between the unhinged and \u03c6 optimal solutions is\nLunh(w\u2217\u03c6)\u2212 Lunh(w\u2217unh) \u2264 L\u03c6(w\u2217\u03c6)\u2212 Lunh(w\u2217unh)\u2212 \u03c6(0) \u2264 L\u03c6(w\u2217unh)\u2212 Lunh(w\u2217unh)\u2212 \u03c6(0) = E\n(X,Y)\u223cD [\u03c6(Y\u3008w\u2217unh,\u03a6(X)\u3009H) + Y\u3008w\u2217unh,\u03a6(X)\u3009H]\u2212 \u03c6(0)\n= E (X,Y)\u223cD\n[ \u03c6\u0303(Y\u3008w\u2217unh,\u03a6(X)\u3009H) ] , (13)\nwhere \u03c6\u0303 : v 7\u2192 \u03c6(v) \u2212 \u03c6(0) + v. (The second line follows by definition of optimality of w\u2217\u03c6 amongst all vectors with norm bounded by r.) We have already established that \u03c6\u0303 \u2265 0. Now, by Taylor\u2019s remainder theorem,\n(\u2200v \u2208 (\u22121, 1)) \u03c6\u0303(v) \u2264 a 2 v2, (14)\nwhere a = maxv\u2208[\u22121,1] \u03c6\u2032\u2032(v). But by Cauchy-Schwartz, we can restrict attention in Equation 13 to the behaviour of \u03c6\u0303 in the interval\nI = [\u2212||w\u2217unh||H \u00b7R, ||w\u2217unh||H \u00b7R],\nwhere R = supx\u2208X ||\u03a6(x)||H <\u221e. Therefore, if r \u2264 1R , Equation 14 and a further application of CauchySchwartz yield\nLunh(w\u2217\u03c6)\u2212 Lunh(w\u2217unh) \u2264 a\n2 \u00b7 E (X,Y)\u223cD\n[ \u3008w\u2217unh,\u03a6(X)\u30092H) ] \u2264 a\n2 \u00b7 E X\u223cM\n[ ||w\u2217unh||2H \u00b7 ||\u03a6(X)||2H ] \u2264 aR 2\n2 \u00b7 ||w\u2217unh||2H.\nNow, the unhinged risk is LDunh(w) = \u2212\u3008w, \u00b5\u3009H.\nThus,\n\u2212\u3008w\u2217\u03c6, \u00b5\u3009H + \u3008w\u2217unh, \u00b5\u3009H \u2264 aR2\n2 \u00b7 ||w\u2217unh||2H.\nRearranging, and by definition of w\u2217unh,\n\u3008w\u2217\u03c6, \u00b5\u3009H \u2265 \u3008w\u2217unh, \u00b5\u3009H \u2212 aR2\n2 \u00b7 ||w\u2217unh||2H\n= ||w\u2217\u03c6||H \u00b7 ||\u00b5||H \u2212 aR2\n2 \u00b7 ||w\u2217\u03c6||2H = ||w\u2217\u03c6||H \u00b7 ||\u00b5||H \u00b7 ( 1\u2212 aR 2\n2||\u00b5||H \u00b7 ||w\u2217\u03c6||H ) \u2265 ||w\u2217\u03c6||H \u00b7 ||\u00b5||H \u00b7 ( 1\u2212 aR 2 2||\u00b5||H \u00b7 r ) ,\nwhere the last line is since by definition ||w\u2217\u03c6||H \u2264 r. Thus, for = aR 2\n2||\u00b5||H ,\u2329 w\u2217\u03c6 ||w\u2217\u03c6||H , \u00b5 ||\u00b5||H \u232a H \u2265 1\u2212 .\nIt follows that the two unit vectors can be made arbitrarily close to each other by decreasing r. Since this corresponds to increasing the strength of regularisation (by Lagrange duality), and since \u00b5||\u00b5||H corresponds to the normalised unhinged solution for any regularisation strength, the result follows.\nA.1 Additional helper lemmas Lemma 9. Pick any loss `. Suppose that\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2203\u03b1, \u03b2 \u2208 R) (\u2200y, v) `(y, v) = \u03b1 \u00b7 `(y, v) + \u03b2.\nThen, (\u2203C \u2208 R) (\u2200v \u2208 R) `1(v) + `\u22121(v) = C.\nProof of Lemma 9. By the definition of the noise-corrected loss (Equation 4), the given statement is that there exist \u03b1, \u03b2 : [0, 1/2)\u2192 R with\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) [ `1(v) `\u22121(v) ] = \u03b1(\u03c1) \u00b7 [ 1\u2212 \u03c1 \u03c1 \u03c1 1\u2212 \u03c1 ]\u22121 \u00b7 [ `1(v) `\u22121(v) ] + \u03b2(\u03c1).\nExpanding out the matrix inverse, (\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) [ `1(v) `\u22121(v) ] = \u03b1(\u03c1) 1\u2212 2\u03c1 \u00b7 [ 1\u2212 \u03c1 \u2212\u03c1 \u2212\u03c1 1\u2212 \u03c1 ] \u00b7 [ `1(v) `\u22121(v) ] + \u03b2(\u03c1).\nAdding together the two sets of equations,\n(\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) `1(v) + `\u22121(v) = \u03b1(\u03c1) \u00b7 (`1(v) + `\u22121(v)) + \u03b2(\u03c1),\nor (\u2200\u03c1 \u2208 [0, 1/2)) (\u2200v \u2208 R) (1\u2212 \u03b1(\u03c1)) \u00b7 (`1(v) + `\u22121(v)) = \u03b2(\u03c1).\nSince the RHS is independent of v, the LHS cannot depend on v, i.e. `1(v) + `\u22121(v) must be a constant.\nAdditional Discussion for \u201cLearning with Symmetric Label Noise: The Importance of Being Unhinged\u201d\u2019"}, {"heading": "B Evidence that non-convex losses and linear scorers may not be SLNrobust", "text": "We now present evidence that for ` being the TangentBoost loss,\n`(y, v) = (2 tan\u22121(yv)\u2212 1)2,\nor the t-logistic regression loss for t = 2, `(y, v) = log(1\u2212 yv + \u221a 1 + v2),\n(`,Flin) is not SLN-robust. We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010]. Of course, as these losses are non-convex, exact minimisation of the risk is challenging. However, as the search space is R2, we construct a grid of resolution 0.025 over [\u221210, 10]2. We then exhaustively compute the objective for all grid points, and seek the minimiser.\nWe apply this procedure to the Long and Servedio [2010] dataset with \u03b3 = 160 , and with a 30% noise rate. Figure 2 plots the results of the objective for the TangentBoost loss. We find that the minimiser is at w\u2217 = (0.2, 1.3). This results in a classifier with error rate of 12 on D. Similarly, from Figure 3, we find that the minimiser is w\u2217 = (1.025, 5.1), which also results in a classifier with error rate of 12 .\nThe shape of these plots suggests that the minimiser is indeed found in the interval [\u221210, 10]2. To further verify this, we performed L-BFGS minimisation of these losses using 100 different random initialisations, uniformly from [\u2212100, 100]2. We find that in each trial, the TangentBoost solution converges to w\u2217 = (0.2122, 1.3031), while the t-logistic solution converges to w\u2217 = (1.0372, 5.0873), both of which result in accuracy of 12 on D.\nB.1 In defence of non-convex losses: beyond SLN-robustness The above illustrates the possible non SLN-robustness of two non-convex losses. However, there may be other notions under which these losses are robust. For example, Ding and Vishwanathan [2010] defines\nrobustness to be a stability of the asymptotic maximum likelihood solution when adding a new labelled instance (chosen arbitrarily from X \u00d7 {\u00b11}), based on a definition in O\u2019Hagan [1979]. Intuitively, this captures robustness to outliers in the instance space, so that e.g. an adversarial mislabelling of an instance far from the true decision boundary does not adversely affect the learned model. Such a notion of robustness is clearly of practical interest, and future study of such alternate notions would be of value.\nB.2 Conjecture: (most) strictly proper composite losses are not SLN-robust More abstractly, we conjecture the above can be generalised to the following. Recall that a loss ` is strictly proper composite [Reid and Williamson, 2010] if its (unique) Bayes-optimal scorer is some strictly monotone transformation \u03c8 of the class-probability function: (\u2200D) SD,\u2217` = {\u03c8 \u25e6 \u03b7}.\nConjecture 1. Pick any strictly proper composite (but not necessarily convex) ` whose link function has range R. Then, (`,Flin) is not SLN-robust.\nWe believe the above is true for the following reason. Suppose D is some linearly separable distribution, with \u03b7 : x 7\u2192 J\u3008w\u2217, x\u3009 > 0K for some w\u2217. Then, minimising ` with Flin will be well-specified: the Bayesoptimal scorer is \u03c8(J\u3008w\u2217, x\u3009 > 0K). If the range of \u03c8 is R, then this is equivalent to\u221e\u00b7 (2J\u3008w\u2217, x\u3009 > 0K\u22121), which is in Flin if we allow for the extended reals. The resulting classifier will thus have 100% accuracy.\nHowever, by injecting any non-zero label noise, minimising ` with Flin will no longer be well-specified, as \u03b7 takes on the values {1 \u2212 \u03c1, \u03c1}, which cannot be the sole set of output scores for any linear scorer if |X| > 3. We believe it unlikely that every such misspecified solution have 100% accuracy on D. We further believe it likely that one can exhibit a scenario, possibly the same as the Long and Servedio [2010] example, where the resulting solution has accuracy 50%.\nTwo further comments are in order. First, if a loss is strictly proper composite, then it cannot satisfy Equation 6, and hence it cannot be strongly SLN-robust. (However, this does leave open the possibility that with Flin, the loss is SLN-robust.) Second, observe that the restriction that \u03c8 have range R is necessary to rule out cases such as square loss, where the link function has range [\u22121, 1]."}, {"heading": "C Preservation of mean maps", "text": "Pick any D, and \u03c1 \u2208 [0, 1/2). Then,\n(\u2200x \u2208 X) 2\u03b7(x)\u2212 1 = 2 \u00b7 ((1\u2212 2\u03c1) \u00b7 \u03b7(x) + \u03c1)\u2212 1 = (1\u2212 2\u03c1) \u00b7 (2\u03b7(x)\u2212 1).\nThus, for any feature mapping \u03a6: X\u2192 H, the kernel mean map of the clean distribution is\nE (X,Y)\u223cD [Y \u00b7 \u03a6(X)] = E X\u223cM [(2\u03b7(X)\u2212 1) \u00b7 \u03a6(X)]\n= 1\n(1\u2212 2\u03c1 \u00b7 E X\u223cM [(2\u03b7(X)\u2212 1) \u00b7 \u03a6(X)]\n= 1\n(1\u2212 2\u03c1) \u00b7 E (X,Y)\u223cSLN(D,\u03c1) [Y \u00b7 \u03a6(X)] ,\nwhich is a scaled version of the kernel mean map of the noisy distribution. That is, the kernel mean map is preserved under symmetric label noise. Instantiating the above with a specific instance x \u2208 X gives Equation 10."}, {"heading": "D Additional theoretical considerations", "text": "D.1 Generalisation bounds Generalisation bounds are readily derived for the unhinged loss. For a training sample S \u223c Dn, define the `-deviation of a scorer s : X\u2192 R to be the difference in its population and empirical `-risk,\ndevD,S` (s) = L D ` (s)\u2212 LS` (s).\nThis quantity is of interest because a standard result says that for the empirical risk minimiser sn over some function class F, regretD,F` (sn) \u2264 2 \u00b7sups\u2208F |dev D,S ` (s)| [Boucheron et al., 2005, Equation 2]. For unhinged loss, we have the following Rademacher based bound.\nProposition 10. Pick any D and n \u2208 N+. Let S \u223c Dn denote an empirical sample. For some B \u2208 R+, let s \u2208 FB . Then, with probability at least 1\u2212 \u03b4 over the choice of S, for ` = `unh,\ndevD,S` (s) \u2264 2 \u00b7 Rn(FB ,S) +B \u00b7 \u221a log 2\u03b4 2n\nwhere Rn(FB ,S) is the empirical Rademacher complexity of FB on sample S.\nProof of Proposition 10. The standard Rademacher-complexity generalisation bound [Bartlett and Mendelson, 2002, Theorem 7], [Boucheron et al., 2005, Theorem 4.1] states that with probability at least 1\u2212 \u03b4 over the choice of S,\ndevD,S` (s) \u2264 2 \u00b7 ||(`) \u2032||\u221e \u00b7 Rn(FB ,S) + ||`||\u221e \u00b7 \u221a log 2\u03b4 2n .\nFor the unhinged loss, ||(`unh)\u2032||\u221e = 1. Further, since we work over bounded scorers, ||`unh||\u221e = B. The result follows.\nProposition 10 holds equally when learning from a corrupted sample S\u0304 \u223c Dn. Since regretD,F `unh\n(sn) = 1\n1\u22122\u03c1 \u00b7 regret D,F `unh (sn) by Proposition 6, by minimising the unhinged loss on the corrupted sample, we can bound the regret on the clean distribution."}, {"heading": "E Additional relations to existing methods", "text": "We discuss some further connections of the unhinged loss to existing methods.\nE.1 Unhinging the SVM We can motivate the unhinged loss intuitively by studying the noise-corrected versions of the hinge loss, as per Equation 4. Figure 4 shows the noise corrected hinge loss for \u03c1 \u2208 {0, 0.2, 0.4}. We see that as the noise rate increases, the effect is to slightly unhinge the original loss, by removing its flat portion8. Thus, if we knew the noise rate \u03c1, we could use these slightly unhinged losses to learn.\nOf course, in general we do not know the noise rate. Further, the slightly unhinged losses are non-convex. So, in order to be robust to an arbitrary noise rate \u03c1, we can completely unhinge the loss, yielding\n`unh1 (v) = 1\u2212 v and `unh\u22121 (v) = 1 + v.\nE.2 Relation to centroid classifiers As established in \u00a76.1, the optimal unhinged classifier (Equation 9) is equivalent to a centroid classifier, where one replaces the positive and negative classes by their centroids, and performs classification based on the distance of an instance to the two centroids. Such a classifier has been proposed as a prototypical example of a simple kernel-based classifier [Scho\u0308lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section 5.1] Balcan et al. [2008, Definition 4] considers such classification rules using general similarity functions in place of kernels corresponding to an RKHS.\nThe optimal unhinged classifier is also closely related to the Rocchio classifier in information retrieval [Manning et al., 2008, pg. 181], and the nearest centroid classifier in computational genomics [Tibshirani et al., 2002]. The optimal kernelised scorer for these approaches is [Doloc-Mihu et al., 2003]\ns\u2217 : x 7\u2192 (\nE X\u223cP [k(X, x)]\u2212 E X\u223cQ [k(X, x)]\n) ,\ni.e. it does not weight each of the kernel means.\nE.3 Relation to kernel density estimation When working with an RKHS with a translation invariant kernel9, the optimal unhinged scorer (Equation 9) can be interpreted as follows: perform kernel density estimation on the positive and negative classes, and then classify instances according to Bayes\u2019 rule. For example, with a Gaussian RBF kernel, the classifier is equivalent to using a Gaussian kernel to compute density estimates of P,Q, and using these to classify. This is known as a kernel classification rule [Devroye et al., 1996, Chapter 10].\n8Another interesting observation is that these noise-corrected losses are negatively unbounded \u2013 that is, minimising hinge loss on D is equivalent to minimising a negatively unbounded loss on D. This is another justification for studying negatively unbounded losses.\n9For a general (not necessarily translation invariant) kernel, this is known as a potential function rule [Devroye et al., 1996, \u00a710.3]. The use of \u201cpotential\u201d here is distinct from that of a \u201cconvex potential\u201d.\nThis perspective suggests that in computing s\u2217unh,\u03bb, we may also estimate the corrupted class-probability\nfunction. In particular, observe that if we compute \u03c01\u2212\u03c0 \u00b7 E X\u223cP [k(X,x)]\nE X\u223cQ\n[k(X,x)] , similar to the Nadaraya-Watson estimator\n[Bishop, 2006, pg. 300], then this provides an estimate of \u03b7(x)1\u2212\u03b7(x) . Of course, such an approach will succumb to the curse of dimensionality10.\nAn alternative is to use the Probing reduction [Langford and Zadrozny, 2005], by computing an ensemble of cost-sensitive classifiers at varying cost ratios. To this end, observe that the following weighted unhinged (or whinge) loss,\n`whinge1 (v) = c1 \u00b7 \u2212v `whinge\u22121 (v) = c\u22121 \u00b7 v\nfor some c\u22121 \u2208 [0, 1] and c1 = 1\u2212 c\u22121, will have a restricted Bayes-optimal scorer of B \u00b7 sign(\u03b7(x)\u2212 c\u22121) over FB . Further, it will result in an optimal scorer that simply weights each of the kernel means,\ns\u2217whinge,\u03bb : x 7\u2192 1\n\u03bb \u00b7 E (X,Y)\u223cD [cY \u00b7 Y \u00b7 k(X, x)] ,\nmaking it trivial to compute as c is varied.\nE.4 Relation to the MMD witness The optimal weight vector for unhinged loss (Equation 8) can be expressed as\nw\u2217unh,\u03bb = 1\n\u03bb \u00b7 (\u03c0 \u00b7 \u00b5P \u2212 (1\u2212 \u03c0) \u00b7 \u00b5Q),\nwhere \u00b5P and \u00b5Q are the kernel mean maps with respect to H of the positive and negative class-conditionals distributions,\n\u00b5P = E X\u223cP [\u03a6(X)] \u00b5Q = E X\u223cQ [\u03a6(X)] .\nWhen \u03c0 = 12 , ||w \u2217 1 ||H is precisely the maximum mean discrepancy (MMD) [Gretton et al., 2012] between P and Q, using all functions in the unit ball of H. The mapping x 7\u2192 \u3008w\u22171 , x\u3009H itself is referred to as the witness function [Gretton et al., 2012, \u00a72.3]. While the motivation of MMD is to perform hypothesis testing so as to distinguish between two distributions P,Q, rather than constructing a suitable scorer, the fact that it arises from the optimal scorer for the unhinged loss has been previously noted [Sriperumbudur et al., 2009, Theorem 1]."}, {"heading": "F Example of poor classification with square loss", "text": "We illustrate that square loss with a linear function class may perform poorly even when the underlying distribution is linearly separable. We consider the dataset of Long and Servedio [2010], with no label noise. That is, we have X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3,\u2212\u03b3), (\u03b3,\u2212\u03b3)} \u2282 R2, and \u03b7 : x 7\u2192 1. Let X \u2208 R4\u00d72 be the feature matrix of the four data points. Then, the optimal weight vector learned by square loss is\nw\u2217 = (XTX)\u22121XT  1 1 1 1  = [ 8\u03b3+3 8\u03b32+3\n\u2212 \u03b3+13\u03b3\u00b7(8\u03b32+3)\n] .\n10This refers to the rate of convergence of the estimate of \u03b7 to the true \u03b7. By contrast, generalisation bounds establish that the rate of convergence of the estimate of the corresponding classifier to the Bayes-optimal classifier sign(2\u03b7(x) \u2212 1) is independent of the dimension of the feature space.\nIt is easy to check that the predicted scores are then\ns\u2217 = \n8\u03b3+3 8\u03b32+3 \u03b3\u00b7(8\u03b3+3)\n8\u03b32+3\u2212 5\u00b7(\u03b3\u22121)\u03b3 24\u03b33+9\u03b3\n(\u03b3\u22121)\u00b7\u03b3 24\u03b33+9\u03b3+\n\u03b3\u00b7(8\u03b3+3) 8\u03b32+3\n(\u03b3\u22121)\u00b7\u03b3 24\u03b33+9\u03b3+\n\u03b3\u00b7(8\u03b3+3) 8\u03b32+3\n .\nBut for \u03b3 < 112 , this means that the predicted scores for the last two examples are negative. That is, the resulting classifier will have 50% accuracy. (This does not contradict the robustness of square loss, as robustness simply requires that performance is the same with and without noise.)\nIt is initially surprising that square loss fails in this example, as we are employing a linear function class, and the true \u03b7 is expressible as a linear function. However, recall that the Bayes-optimal scorer for square loss is\nS D,\u2217 ` = {s : x 7\u2192 2\u03b7(x)\u2212 1}.\nIn this case, the Bayes-optimal scorer is\ns\u2217 : x 7\u2192 2Jx1 > 0K\u2212 1.\nThe application of a threshold means the that scorer is not expressible as a linear model. Therefore, the combination of loss and function class is in fact not well-specified for the problem.\nTo clarify this point, consider the use of the squared hinge loss, `(y, v) = max(0, 1 \u2212 yv)2. This loss induces a set of Bayes-optimal scorers, which are:\nS D,\u2217 ` = s | (\u2200x \u2208 X)  \u03b7(x) = 1 =\u21d2 s(x) \u2208 [1,\u221e) \u03b7(x) \u2208 (0, 1) =\u21d2 s(x) = 2\u03b7(x)\u2212 1 \u03b7(x) = 0 =\u21d2 s(x) \u2208 (\u2212\u221e, 1].  Crucially, we can find a linear scorer that is in this set: for, say, v = ( 1\u03b3 , 0), we clearly have \u3008v, x\u3009 \u2265 1 for every x \u2208 X, and so this is a Bayes-optimal scorer. Thus, minimising the square hinge loss on this distribution will indeed find a classifier with 100% accuracy."}, {"heading": "G Example of poor classification with unhinged loss", "text": "We illustrate that the unhinged loss with a linear function class may perform poorly even when the underlying distribution is linearly separable. (For another example where instances are on the unit ball, see Balcan et al. [2008, Figure 1].) Consider a distribution DM,\u03b7 uniformly concentrated on X = {x1, x2, x3} with x1 = (1, 2), x2 = (1,\u22124), x3 = (\u22121, 1), with \u03b7(x1) = \u03b7(x2) = 1 and \u03b7(x3) = 0, i.e. the first two instances are positive, and the third instance negative. Then it is evident that the optimal unhinged hyperplane, with regularisation strength 1, is w\u2217 = (1,\u22121). This will misclassify the first instance as being negative. Figure 5 illustrates.\nIt is easy to check that for this particular distribution, the optimal weight for square loss is w\u2217 = (1, 0). This results in perfect classification. Thus, we have a reversal of the scenario of the previous section \u2013 here, square loss classifies perfectly, while the unhinged loss classifies no better than random guessing.\nIt may appear that the above contradicts the classification-calibration of the unhinged loss: there certainly is a linear scorer that is Bayes-optimal over FB , namely, w\u2217 = (B, 0). The subtlety is that in this case, minimisation over the unit ball ||w||2 \u2264 1 (as implied by `2 regularisation) is unable to restrict attention to the desired scorer.\nThere are two ways to rectify examples such as the above. First, as in general, we can employ a suitably rich kernel, e.g. a Gaussian RBF kernel. It is not hard to verify that on this dataset, such a kernel will find a perfect classifier. Second, we can look to explicitly enforce that minimisation is over all w satisfying |\u3008w, xn\u3009| \u2264 1. This will result in a linear program (LP) that may be solved easily, but does not admit a closed\nform solution as in the case of minimising over the unit ball. It may be checked that the resulting LP will recover the optimal weight w\u2217 = (1, 0). While this approach is suitable for this particular example, issues arise when dealing with infinite dimensional feature mappings (as we lose the existence of a representer theorem without regularisation based on the norm in the Hilbert space [Yu et al., 2013]).\nAdditional Experiments for \u201cLearning with Symmetric Label Noise: The Importance of Being Unhinged\u201d"}, {"heading": "H Additional experimental results", "text": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier. Observe that the relatively poor performance of the square and TanBoost loss can be attributed to the findings of Appendix B, F.\nWe next report the 0-1 error and one minus the AUC for a range of datasets. We begin with a dataset of Mease and Wyner [2008], where X = [0, 1]20, and M is the uniform distribution. Further, we have \u03b7 : x 7\u2192 J\u3008w\u2217, x\u3009 > 2.5K for w\u2217 = [ 15 015 ] , i.e. there is a sparse separating hyperplane. Table 5 reports the results on this dataset injected with various levels of symmetric noise. On this dataset, the t-logistic loss generally performs the best.\nFinally, we report the 0-1 error and one minus the AUC on some UCI datasets in Tables 6 \u2013 7. Table 3 summarises statistics of the UCI data. Several datasets are imbalanced, meaning that 0-1 error is not the ideal measure of performance (as it can be made small with a trivial majority classifier). The AUC is thus arguably a better indication of performance for these datasets. We generally find that at high noise rates (40%), the AUC of the unhinged loss is superior to that of other losses."}], "references": [{"title": "Learning from noisy examples", "author": ["Dana Angluin", "Philip Laird"], "venue": "Machine Learning,", "citeRegEx": "Angluin and Laird.,? \\Q1988\\E", "shortCiteRegEx": "Angluin and Laird.", "year": 1988}, {"title": "A theory of learning with similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Nathan Srebro"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "An efficient alternative to SVM based recursive feature elimination with applications in natural language processing and bioinformatics", "author": ["Justin Bedo", "Conrad Sanderson", "Adam Kowalczyk"], "venue": "AI 2006: Advances in Artificial Intelligence,", "citeRegEx": "Bedo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bedo et al\\.", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Conference on Computational Learning Theory (COLT),", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Theory of classification: a survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "Super-samples from kernel herding", "author": ["Yutian Chen", "Max Welling", "Alex J. Smola"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Optimal Statistical Decisions", "author": ["Morris H. DeGroot"], "venue": null, "citeRegEx": "DeGroot.,? \\Q1970\\E", "shortCiteRegEx": "DeGroot.", "year": 1970}, {"title": "Robust classification with adiabatic quantum optimization", "author": ["Vasil Denchev", "Nan Ding", "Hartmut Neven", "S.V.N. Vishwanathan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Denchev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denchev et al\\.", "year": 2012}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "t-logistic regression. In Advances in Neural Information Processing Systems (NIPS), pages 514\u2013522", "author": ["Nan Ding", "S.V.N. Vishwanathan"], "venue": "Curran Associates, Inc.,", "citeRegEx": "Ding and Vishwanathan.,? \\Q2010\\E", "shortCiteRegEx": "Ding and Vishwanathan.", "year": 2010}, {"title": "Color retrieval in vector space model", "author": ["A. Doloc-Mihu", "V.V. Raghavan", "P. Bollmann-Sdorra"], "venue": "In ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval,", "citeRegEx": "Doloc.Mihu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Doloc.Mihu et al\\.", "year": 2003}, {"title": "Mathematical Statistics: A Decision Theoretic Approach", "author": ["Thomas S. Ferguson"], "venue": null, "citeRegEx": "Ferguson.,? \\Q1967\\E", "shortCiteRegEx": "Ferguson.", "year": 1967}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "Kearns.,? \\Q1998\\E", "shortCiteRegEx": "Kearns.", "year": 1998}, {"title": "Estimating class membership probabilities using classifier learners", "author": ["John Langford", "Bianca Zadrozny"], "venue": null, "citeRegEx": "Langford and Zadrozny.,? \\Q2005\\E", "shortCiteRegEx": "Langford and Zadrozny.", "year": 2005}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["Philip M. Long", "Rocco A. Servedio"], "venue": "Machine Learning,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "On the design of robust classifiers for computer vision", "author": ["Hamed Masnadi-Shirazi", "Vijay Mahadevan", "Nuno Vasconcelos"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Masnadi.Shirazi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masnadi.Shirazi et al\\.", "year": 2010}, {"title": "Evidence contrary to the statistical view of boosting", "author": ["David Mease", "Abraham Wyner"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mease and Wyner.,? \\Q2008\\E", "shortCiteRegEx": "Mease and Wyner.", "year": 2008}, {"title": "Learning with noisy labels", "author": ["Nagarajan Natarajan", "Inderjit S. Dhillon", "Pradeep D. Ravikumar", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Natarajan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2013}, {"title": "On outlier rejection phenomena in bayes inference", "author": ["Anthony O\u2019Hagan"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 41(3):pp", "citeRegEx": "O.Hagan.,? \\Q1979\\E", "shortCiteRegEx": "O.Hagan.", "year": 1979}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Composite binary losses", "author": ["Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2010\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2010}, {"title": "Information, divergence and risk for binary experiments", "author": ["Mark D Reid", "Robert C Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2011}, {"title": "Learning with kernels, volume 129", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Kernel Methods for Pattern Analysis, volume 47", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": "ISBN 0521813972", "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Kernel choice and classifiability for RKHS embeddings of probability distributions", "author": ["Bharath K. Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Gert R.G. Lanckriet", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "Learning SVMs from sloppily labeled data", "author": ["Guillaume Stempfel", "Liva Ralaivola"], "venue": "In Artificial Neural Networks (ICANN),", "citeRegEx": "Stempfel and Ralaivola.,? \\Q2009\\E", "shortCiteRegEx": "Stempfel and Ralaivola.", "year": 2009}, {"title": "Diagnosis of multiple cancer types by shrunken centroids of gene expression", "author": ["Robert Tibshirani", "Trevor Hastie", "Balasubramanian Narasimhan", "Gilbert Chu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Tibshirani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2002}, {"title": "Characterizing the representer theorem", "author": ["Yaoliang Yu", "Hao Cheng", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing.", "startOffset": 9, "endOffset": 34}, {"referenceID": 18, "context": "However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classificationcalibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded.", "startOffset": 9, "endOffset": 424}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988].", "startOffset": 83, "endOffset": 108}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing.", "startOffset": 84, "endOffset": 135}, {"referenceID": 0, "context": "This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \u201cSLN-robust\u201d and motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013]. In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result of Long and Servedio [2010] by virtue of being negatively unbounded.", "startOffset": 84, "endOffset": 869}, {"referenceID": 20, "context": "Manwani and Sastry [2013] demonstrated that square loss, `(y, v) = (1\u2212 yv), is one such loss.", "startOffset": 0, "endOffset": 26}, {"referenceID": 29, "context": "2This loss has been considered in Sriperumbudur et al. [2009], Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.", "startOffset": 34, "endOffset": 62}, {"referenceID": 26, "context": "[2009], Reid and Williamson [2011] in the context of maximum mean discrepancy; see Appendix E.", "startOffset": 8, "endOffset": 35}, {"referenceID": 33, "context": "181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.", "startOffset": 5, "endOffset": 30}, {"referenceID": 25, "context": "To alleviate this, for a translation-invariant kernel one can use random Fourier features [Rahimi and Recht, 2007] to find an approximate embedding of H into some low-dimensional R, and then store w\u2217 unh,\u03bb as usual.", "startOffset": 90, "endOffset": 114}, {"referenceID": 8, "context": "Alternately, one can post hoc search for a sparse approximation to w \u2217 unh,\u03bb, for example using kernel herding [Chen et al., 2012].", "startOffset": 111, "endOffset": 130}, {"referenceID": 18, "context": "We first show that the unhinged risk minimiser performs well on the example of Long and Servedio [2010]. Figure 1 shows the distribution D, where X = {(1, 0), (\u03b3, 5\u03b3), (\u03b3,\u2212\u03b3)} \u2282 R, with marginal distribution M = { 14 , 1 4 , 1 2} and all three instances are deterministically positive.", "startOffset": 79, "endOffset": 104}, {"referenceID": 18, "context": "Figure 1: Long and Servedio [2010] dataset.", "startOffset": 10, "endOffset": 35}, {"referenceID": 18, "context": "Table 1: Mean and standard deviation of the 0-1 error over 125 trials on Long and Servedio [2010]. Grayed cells denote the best performer at that noise rate.", "startOffset": 73, "endOffset": 98}, {"referenceID": 18, "context": "5Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.", "startOffset": 35, "endOffset": 60}, {"referenceID": 12, "context": "We compare the hinge, t-logistic (for t = 2) [Ding and Vishwanathan, 2010] and unhinged minimisers.", "startOffset": 45, "endOffset": 74}, {"referenceID": 4, "context": "With an unregularised bias term, Bedo et al. [2006] showed that the limiting solution of a soft-margin SVM is distribution dependent.", "startOffset": 33, "endOffset": 52}, {"referenceID": 18, "context": "We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010]. Of course, as these losses are non-convex, exact minimisation of the risk is challenging.", "startOffset": 77, "endOffset": 102}, {"referenceID": 18, "context": "We do this by looking at the minimisers of these losses on the 2D example of Long and Servedio [2010]. Of course, as these losses are non-convex, exact minimisation of the risk is challenging. However, as the search space is R, we construct a grid of resolution 0.025 over [\u221210, 10]. We then exhaustively compute the objective for all grid points, and seek the minimiser. We apply this procedure to the Long and Servedio [2010] dataset with \u03b3 = 1 60 , and with a 30% noise rate.", "startOffset": 77, "endOffset": 428}, {"referenceID": 18, "context": "Figure 2: Risk values for various weight vectors w = (w1, w2), TangentBoost, Long and Servedio [2010] dataset.", "startOffset": 77, "endOffset": 102}, {"referenceID": 12, "context": "For example, Ding and Vishwanathan [2010] defines", "startOffset": 13, "endOffset": 42}, {"referenceID": 18, "context": "Figure 3: Risk values for various weight vectors w = (w1, w2), t-logistic regression, Long and Servedio [2010] dataset.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "robustness to be a stability of the asymptotic maximum likelihood solution when adding a new labelled instance (chosen arbitrarily from X \u00d7 {\u00b11}), based on a definition in O\u2019Hagan [1979]. Intuitively, this captures robustness to outliers in the instance space, so that e.", "startOffset": 172, "endOffset": 187}, {"referenceID": 26, "context": "Recall that a loss ` is strictly proper composite [Reid and Williamson, 2010] if its (unique) Bayes-optimal scorer is some strictly monotone transformation \u03c8 of the class-probability function: (\u2200D) S ` = {\u03c8 \u25e6 \u03b7}.", "startOffset": 50, "endOffset": 77}, {"referenceID": 18, "context": "We further believe it likely that one can exhibit a scenario, possibly the same as the Long and Servedio [2010] example, where the resulting solution has accuracy 50%.", "startOffset": 87, "endOffset": 112}, {"referenceID": 33, "context": "181], and the nearest centroid classifier in computational genomics [Tibshirani et al., 2002].", "startOffset": 68, "endOffset": 93}, {"referenceID": 13, "context": "The optimal kernelised scorer for these approaches is [Doloc-Mihu et al., 2003]", "startOffset": 54, "endOffset": 79}, {"referenceID": 17, "context": "An alternative is to use the Probing reduction [Langford and Zadrozny, 2005], by computing an ensemble of cost-sensitive classifiers at varying cost ratios.", "startOffset": 47, "endOffset": 76}, {"referenceID": 15, "context": "When \u03c0 = 1 2 , ||w \u2217 1 ||H is precisely the maximum mean discrepancy (MMD) [Gretton et al., 2012] between P and Q, using all functions in the unit ball of H.", "startOffset": 75, "endOffset": 97}, {"referenceID": 18, "context": "We consider the dataset of Long and Servedio [2010], with no label noise.", "startOffset": 27, "endOffset": 52}, {"referenceID": 34, "context": "While this approach is suitable for this particular example, issues arise when dealing with infinite dimensional feature mappings (as we lose the existence of a representer theorem without regularisation based on the norm in the Hilbert space [Yu et al., 2013]).", "startOffset": 243, "endOffset": 260}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset.", "startOffset": 59, "endOffset": 84}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier.", "startOffset": 59, "endOffset": 154}, {"referenceID": 18, "context": "Table 4 reports the 0-1 error for a range of losses on the Long and Servedio [2010] dataset. TanBoost refers to the loss of Masnadi-Shirazi et al. [2010]. As before, we find the unhinged loss to generally find a good classifier. Observe that the relatively poor performance of the square and TanBoost loss can be attributed to the findings of Appendix B, F. We next report the 0-1 error and one minus the AUC for a range of datasets. We begin with a dataset of Mease and Wyner [2008], where X = [0, 1], and M is the uniform distribution.", "startOffset": 59, "endOffset": 484}, {"referenceID": 18, "context": "Table 4: Results on Long and Servedio [2010] dataset.", "startOffset": 20, "endOffset": 45}], "year": 2015, "abstractText": "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classificationcalibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong `2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss. 1 Learning with symmetric label noise Binary classification is the canonical supervised learning problem. Given an instance space X, and samples from some distribution D over X \u00d7 {\u00b11}, the goal is to learn a scorer s : X \u2192 R with low misclassification error on future samples drawn from D. Our interest is in the more realistic scenario where the learner observes samples from a distribution D, which is a corruption of D where labels have some constant probability of being flipped. The goal is still to perform well with respect to the unobserved distribution D. This is known as the problem of learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988]. Long and Servedio [2010] proved the following negative result on what is possible in SLN learning: there exists a linearly separableD where, when the learner observes some corruptionD with symmetric label noise of any nonzero rate, minimisation of any convex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \u201cSLN-robust\u201d and motivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013]. In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the result of Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modification of the hinge loss where one does not clamp at zero; thus, we call it the unhinged loss. We show that this is the unique convex loss (up to scaling and translation) that satisfies a notion of \u201cstrong SLN-robustness \u201d (Proposition 4). In addition to being SLN-robust, this loss has several attractive properties, such as being classification-calibrated (Proposition 5), consistent when minimised on the corrupted distribution (Proposition 6), and having an easily computable optimal solution that is the difference of two kernel means (Equation 9). Finally, we show that this optimal solution is equivalent to that of a strongly regularised SVM (Proposition 7), and such a result holds more generally for any twice-differentiable convex potential (Proposition 8), implying that strong `2 regularisation endows most standard learners with SLN-robustness. 1 ar X iv :1 50 5. 07 63 4v 1 [ cs .L G ] 2 8 M ay 2 01 5 The classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Sch\u00f6lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section 5.1]. However, establishing this classifier\u2019s SLN-robustness, its equivalence to a highly regularised SVM solution, and showing the underlying loss uniquely satisfies a notion of strong SLN-robustness, to our knowledge is novel. 2 Background and problem setup Fix an instance space X. We denote by D some distribution over X\u00d7{\u00b11}, with random variables (X,Y) \u223c D. Any D may be expressed via the class-conditional distributions (P,Q) = (P(X | Y = 1),P(X | Y = \u22121)) and base rate \u03c0 = P(Y = 1), or equivalently via the marginal distribution M = P(X) and classprobability function \u03b7 : x 7\u2192 P(Y = 1 | X = x). We interchangeably write D as DP,Q,\u03c0 or DM,\u03b7. 2.1 Classifiers, scorers, and risks A scorer is any function s : X \u2192 R. A loss is any function ` : {\u00b11} \u00d7 R \u2192 R. We use `\u22121, `1 to refer to `(\u22121, \u00b7) and `(1, \u00b7). The `-conditional risk L` : [0, 1] \u00d7 R \u2192 R is defined as L` : (\u03b7, v) 7\u2192 \u03b7 \u00b7 `1(v) + (1 \u2212 \u03b7) \u00b7 `\u22121(v). Given a distribution D, the `-risk of a scorer s is defined as L` (s) . = E (X,Y)\u223cD [`(Y, s(X))] , (1) or equivalently L` (s) = E X\u223cM [L`(\u03b7(X), s(X))]. For a set S, L` (S) is the set of `-risks for all scorers in S. A function class is any F \u2286 R. Given some F, the set of restricted Bayes-optimal scorers for a loss ` are those scorers in F that minimise the `-risk: S D,F,\u2217 ` . = Argmin s\u2208F L` (s). The set of (unrestricted) Bayes-optimal scorers is S ` = S D,F,\u2217 ` for F = R. The restricted `-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer: regret ` (s) . = L` (s)\u2212 inf t\u2208F L` (t). Binary classification is concerned with the risk corresponding to the zero-one loss, ` : (y, v) 7\u2192 Jyv < 0K + 12Jv = 0K. A loss ` is classification-calibrated if all its Bayes-optimal scorers are also optimal for zeroone loss: (\u2200D) S ` \u2286 S D,\u2217 01 .A convex potential is any loss ` : (y, v) 7\u2192 \u03c6(yv), where \u03c6 : R\u2192 R+ is convex, non-increasing, differentiable with \u03c6\u2032(0) < 0, and \u03c6(+\u221e) = 0 [Long and Servedio, 2010, Definition 1]. All convex potential losses are classification-calibrated [Bartlett et al., 2006, Theorem 2.1]. 2.2 Learning with symmetric label noise (SLN learning) The problem of learning with symmetric label noise (SLN learning) is the following [Angluin and Laird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some notional \u201cclean\u201d distribution D, which we would like to observe, we instead observe samples from some corrupted distribution SLN(D, \u03c1), for some \u03c1 \u2208 [0, 1/2). The distribution SLN(D, \u03c1) is such that the marginal distribution of instances is unchanged, but each label is independently flipped with probability \u03c1. The goal is to learn a scorer from these corrupted samples such that L01(s) is small. For any quantity in D, we denote its corrupted counterparts in SLN(D, \u03c1) with a bar, e.g. M for the corrupted marginal distribution, and \u03b7 for the corrupted class-probability function; additionally, when \u03c1 is clear from context, we will occasionally refer to SLN(D, \u03c1) by D. By definition of the corruption process, the corruption marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7] (\u2200x \u2208 X) \u03b7(x) = (1\u2212 2\u03c1) \u00b7 \u03b7(x) + \u03c1. (2)", "creator": "LaTeX with hyperref package"}}}