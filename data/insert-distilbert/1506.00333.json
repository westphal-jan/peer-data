{"id": "1506.00333", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Learning to Answer Questions from Image Using Convolutional Neural Network", "abstract": "in this paper, ultimately we propose explicitly to employ the convolutional neural network ( cc cnn ) for learning to answer questions from the image. our proposed experimental cnn provides an end - to - finish end framework for learning not only the image representation, the composition model for question, but also the inter - modal interaction modeled between the image composition and question, for efficiently the generation of answer. more specifically, the proposed model consists totally of three components : an effective image cnn to extract the image representation, one sentence cnn to encode the question, and one multimodal convolution layer to fuse ; the multimodal input of the image and question to obtain the joint representation built for the classification constraints in the space size of its candidate answer words. we demonstrate the efficacy of our proposed model modelled on gamma daquar and coco - qa datasets, two datasets recently created for the image question answering ( roi qa ), with performance substantially outperforming the state - of - designing the - arts.", "histories": [["v1", "Mon, 1 Jun 2015 03:09:49 GMT  (529kb,D)", "https://arxiv.org/abs/1506.00333v1", "10 pages, 3 figures"], ["v2", "Fri, 13 Nov 2015 09:54:59 GMT  (887kb,D)", "http://arxiv.org/abs/1506.00333v2", "7 pages, 4 figures. Accepted by AAAI 2016"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["lin ma", "zhengdong lu", "hang li"], "accepted": true, "id": "1506.00333"}, "pdf": {"name": "1506.00333.pdf", "metadata": {"source": "CRF", "title": "Learning to Answer Questions From Image Using Convolutional Neural Network", "authors": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "emails": ["forest.linma@gmail.com", "Lu.Zhengdong@huawei.com", "HangLi@huawei.com", "HL@huawei.com"], "sections": [{"heading": "Introduction", "text": "Recently, the multimodal learning between image and language (Ma et al. 2015; Makamura et al. 2013; Xu et al. 2015b) has become an increasingly popular research area of artificial intelligence (AI). In particular, there have been rapid progresses on the tasks of bidirectional image and sentence retrieval (Frome et al. 2013; Socher et al. 2014; Klein et al. 2015; Karpathy, Joulin, and Li 2014; Ordonez, Kulkarni, and Berg 2011), and automatic image captioning (Chen and Zitnick 2014; Karpathy and Li 2014; Donahue et al. 2014; Fang et al. 2014; Kiros, Salakhutdinov, and Zemel 2014a; Kiros, Salakhutdinov, and Zemel 2014b; Klein et al. 2015; Mao et al. 2014a; Mao et al. 2014b; Vinyals et al. 2014; Xu et al. 2015a). In order to further advance the multimodal learning and push the boundary of AI research, a new \u201cAI-complete\u201d task, namely the visual question answering (VQA) (Antol et al. 2015) or image question answering (QA) (Malinowski and Fritz 2014a; Malinowski and Fritz 2014b; Malinowski and Fritz 2015; Malinowski, Rohrbach, and Fritz 2015; Ren, Kiros, and Zemel 2015), is recently proposed. Generally, it takes an image and a free-form, natural-language like question about the image as the input and produces an answer to the image and question.\nImage QA differs with the other multimodal learning tasks between image and sentence, such as the automatic\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nimage captioning. The answer produced by the image QA needs to be conditioned on both the image and question. As such, the image QA involves more interactions between image and language. As illustrated in Figure 1, the image contents are complicated, containing multiple different objects. The questions about the images are very specific, which requires a detailed understanding of the image content. For the question \u201cwhat is the largest blue object in this picture?\u201d, we need not only identify the blue objects in the image but also compare their sizes to generate the correct answer. For the question \u201chow many pieces does the curtain have?\u201d, we need to identify the object \u201ccurtain\u201d in the non-salient region of the image and figure out its quantity.\nA successful image QA model needs to be built upon good representations of the image and question. Recently, deep neural networks have been used to learn image and sentence representations. In particular, convolutional neural networks (CNNs) are extensively used to learn the image representation for image recognition (Simonyan and Zisserman 2014; Szegedy et al. 2015). CNNs (Hu et al. 2014; Kim 2014; Kalchbrenner, Grefenstette, and Blunsom 2014) also demonstrate their powerful abilities on the sentence representation for paraphrase, sentiment analysis, and so on. Moreover, deep neural networks (Mao et al. 2014a; Karpathy, Joulin, and Li 2014; Karpathy and Li 2014; Vinyals et al. 2014) are used to capture the relations between image and sentence for image captioning and retrieval.\nar X\niv :1\n50 6.\n00 33\n3v 2\n[ cs\n.C L\n] 1\n3 N\nov 2\n01 5\nHowever, for the image QA task, the ability of CNN has not been studied.\nIn this paper, we employ CNN to address the image QA problem. Our proposed CNN model, trained on a set of triplets consisting of (image, question, answer), can answer free-form, natural-language like questions about the image. Our main contributions are:\n1. We propose an end-to-end CNN model for learning to answer questions about the image. Experimental results on public image QA datasets show that our proposed CNN model surpasses the state-of-the-art.\n2. We employ convolutional architectures to encode the image content, represent the question, and learn the interactions between the image and question representations, which are jointly learned to produce the answer conditioning on the image and question."}, {"heading": "Related Work", "text": "Recently, the visual Turing test, an open domain task of question answering based on real-world images, has been proposed to resemble the famous Turing test. In (Gao et al. 2015) a human judge will be presented with an image, a question, and the answer to the question by the computational models or human annotators. Based on the answer, the human judge needs to determine whether the answer is given by a human (i.e. pass the test) or a machine (i.e. fail the test). Geman et al. (Geman et al. 2015) proposed to produce a stochastic sequence of binary questions from a given test image, where the answer to the question is limited to yes/no. Malinowski et al. (Malinowski and Fritz 2014b; Malinowski and Fritz 2015) further discussed the associated challenges and issues with regard to visual Turing test, such as the vision and language representations, the common sense knowledge, as well as the evaluation.\nThe image QA task, resembling the visual Turing test, is then proposed. Malinowski et al. (Malinowski and Fritz 2014a) proposed a multi-world approach that conducts the semantic parsing of question and segmentation of image to produce the answer. Deep neural networks are also employed for the image QA task, which is more related to our research work. The work by (Malinowski, Rohrbach, and Fritz 2015; Gao et al. 2015) formulates the image QA task as a generation problem. Malinowski et al.\u2019s model (Malinowski, Rohrbach, and Fritz 2015), namely the NeuralImage-QA, feeds the image representation from CNN and the question into the long-short term memory (LSTM) to produce the answer. This model ignores the different characteristics of questions and answers. Compared with the questions, the answers tend to be short, such as one single word denoting the object category, color, number, and so on. The deep neural network in (Gao et al. 2015), inspired by the multimodal recurrent neural networks model (Mao et al. 2014b; Mao et al. 2014a), used two LSTMs for the representations of question and answer, respectively. In (Ren, Kiros, and Zemel 2015), the image QA task is formulated as a classification problem, and the socalled visual semantic embedding (VSE) model is proposed. LSTM is employed to jointly model the image and ques-\ntion by treating the image as an independent word, and appending it to the question at the beginning or ending position. As such, the joint representation of image and question is learned, which is further used for classification. However, simply treating the image as an individual word cannot help effectively exploit the complicated relations between the image and question. Thus, the accuracy of the answer prediction may not be ensured. In order to cope with these drawbacks, we proposed to employ an end-toend convolutional architectures for the image QA to capture the complicated inter-modal relationships as well as the representations of image and question. Experimental results demonstrate that the convolutional architectures can can achieve better performance for the image QA task."}, {"heading": "Proposed CNN for Image QA", "text": "For image QA, the problem is to predict the answer a given the question q and the related image I:\na = argmax a\u2208\u2126\np(a|q, I; \u03b8), (1)\nwhere \u2126 is the set containing all the answers. \u03b8 denotes all the parameters for performing image QA. In order to make a reliable prediction of the answer, the question q and image I need to be adequately represented. Based on their representations, the relations between the two multimodal inputs are further learned to produce the answer. In this paper, the ability of CNN is exploited for not only modeling image and sentence individually, but also capturing the relations and interactions between them.\nAs illustrated in Figure 2, our proposed CNN framwork for image QA consists of three individual CNNs: one image CNN encoding the image content, one sentence CNN generating the question representation, one multimodal convolution layer fusing the image and question representations together and generate the joint representation. Finally, the joint representation is fed into a softmax layer to produce the answer. The three CNNs and softmax layer are fully coupled for our proposed end-to-end image QA framework, with all the parameters (three CNNs and softmax) jointly learned in an end-to-end fashion."}, {"heading": "Image CNN", "text": "There are many research papers employing CNNs to generate image representations, which achieve the state-ofthe-art performances on image recognition (Simonyan and Zisserman 2014; Szegedy et al. 2015). In this paper, we employ the work (Simonyan and Zisserman 2014) to encode the image content for our image QA model:\n\u03bdim = \u03c3(wim(CNNim(I)) + bim), (2)\nwhere \u03c3 is a nonlinear activation function, such as Sigmoid and ReLU (Dahl, Sainath, and Hinton 2013). CNNim takes the image as the input and outputs a fixed length vector as the image representation. In this paper, by chopping out the top softmax layer and the last ReLU layer of the CNN (Simonyan and Zisserman 2014), the output of the last fully-connected layer is deemed as the image representation, which is a fixed length vector with dimension as 4096. Note that wim is a mapping matrix of the dimension d \u00d7 4096, with d much smaller than 4096. On one hand, the dimension of the image representation is reduced from 4096 to d. As such, the total number of parameters for further fusing image and question, specifically the multimodal convolution process, is significantly reduced. Consequently, fewer samples are needed for adequately training our CNN model. On the other hand, the image representation is projected to a new space, with the nonlinear activation function \u03c3 increasing the nonlinear modeling property of the image CNN. Thus its capability for learning complicated representations is enhanced. As a result, the multimodal convolution layer (introduced in the following section) can better fuse the question and image representations together and further exploit their complicated relations and interactions to produce the answer."}, {"heading": "Sentence CNN", "text": "In this paper, CNN is employed to model the question for image QA. As most convolution models (Lecun and Bengio 1995; Kalchbrenner, Grefenstette, and Blunsom 2014), we consider the convolution unit with a local \u201creceptive field\u201d and shared weights to capture the rich structures and composition properties between consecutive words. The sentence CNN for generating the question representation is illustrated in Figure 3. For a given question with each word represented as the word embedding (Mikolov et al. 2013), the sentence CNN with several layers of convolution and max-pooling is performed to generate the question representation \u03bdqt.\nConvolution For a sequential input \u03bd, the convolution unit for feature map of type-f on the `th layer is\n\u03bdi(`,f) def = \u03c3(w(`,f)~\u03bd i (`\u22121) + b(`,f)), (3)\nwhere w(`,f) are the parameters for the f feature map on the `th layer, \u03c3 is the nonlinear activation function, and ~\u03bdi(`\u22121) denotes the segment of (`\u22121)th layer for the convolution at location i , which is defined as follows.\n~\u03bdi(`\u22121) def = \u03bdi(`\u22121) \u2016 \u03bd i+1 (`\u22121) \u2016 \u00b7 \u00b7 \u00b7 \u2016 \u03bd i+srp\u22121 (`\u22121) , (4)\nwhere srp defines the size of local \u201creceptive field\u201d for convolution. \u201c\u2016\u201d concatenates the srp vectors into a long vector. In this paper, srp is chosen as 3 for the convolution process. The parameters within the convolution unit are shared for the whole question with a window covering 3 semantic components sliding from the beginning to the end. The input of the first convolution layer for the sentence CNN is the word embeddings of the question:\n~\u03bdi(0) def = \u03bdiwd \u2016 \u03bdi+1wd \u2016 \u00b7 \u00b7 \u00b7 \u2016 \u03bd i+srp\u22121 wd , (5)\nwhere \u03bdiwd is the word embedding of the i th word in the question.\nMax-pooling With the convolution process, the sequential srp semantic components are composed to a higher semantic representation. However, these compositions may not be the meaningful representations, such as \u201cis on the\u201d of the question in Figure 3. The max-pooling process following each convolution process is performed:\n\u03bdi(`+1,f) = max(\u03bd 2i (`,f), \u03bd 2i+1 (`,f) ). (6)\nFirstly, together with the stride as two, the max-pooling process shrinks half of the representation, which can quickly make the sentence representation. Most importantly, the max-pooling process can select the meaningful compositions while filter out the unreliable ones. As such, the meaningful composition \u201cof the chair\u201d is more likely to be pooled out, compared with the composition \u201cfront of the\u201d.\nThe convolution and max-pooling processes exploit and summarize the local relation signals between consecutive words. More layers of convolution and max-pooling can help to summarize the local interactions between words at larger scales and finally reach the whole representation of the question. In this paper, we employ three layers of convolution and max-pooling to generate the question representation \u03bdqt."}, {"heading": "Multimodal Convolution Layer", "text": "The image representation \u03bdim and question representation \u03bdqt are obtained by the image and sentence CNNs, respectively. We design a new multimodal convolution layer on top of them, as shown in Figure 4, which fuses the multimodal inputs together to generate their joint representation for further answer prediction. The image representation is treated as an individual semantic component. Based on the image representation and the two consecutive semantic components from the question side, the mulitmodal convolution\nis performed, which is expected to capture the interactions and relations between the two multimodal inputs.\n~\u03bdinmm def = \u03bdiqt \u2016 \u03bdim \u2016 \u03bdi+1qt , (7)\n\u03bdi(mm,f) def = \u03c3(w(mm,f)~\u03bd in mm + b(mm,f)), (8)\nwhere ~\u03bdinmm is the input of the multimodal convolution unit. \u03bdiqt is the segment of the question representation at location i. w(mm,f) and b(mm,f) are the parameters for the type-f feature map of the multimodal convolution layer.\nAlternatively, LSTM could be used to fuse the image and question representations, as in (Malinowski, Rohrbach, and Fritz 2015; Ren, Kiros, and Zemel 2015). For example, in the latter work, a bidirectional LSTM (Ren, Kiros, and Zemel 2015) is employed by appending the image representation to the beginning or ending position of the question. We argue that it is better to employ CNN than LSTM for the image QA task, due to the following reason, which has also been verified in the following experiment section. The relations between image and question are complicated. The image may interact with the high-level semantic representations composed from of a number of words, such as \u201cthe red bicycle\u201d in Figure 2. However, LSTM cannot effectively capture such interactions. Treating the image representation as an individual word, the effect of image will vanish at each time step of LSTM in (Ren, Kiros, and Zemel 2015). As a result, the relations between the image and the high-level semantic representations of words may not be well exploited. In contrast, our CNN model can effectively deal with the problem. The sentence CNN first compose the question into a high-level semantic representations. The multimodal convolution process further fuse the semantic representations of image and question together and adequately exploit their interactions.\nAfter the mutlimodal convolution layer, the multimodal representation \u03bdmm jointly modeling the image and question is obtained. \u03bdmm is then fed into a softmax layer as shown in Figure 2, which produces the answer to the given image and question pair."}, {"heading": "Experiments", "text": "In this section, we firstly introduce the configurations of our CNN model for image QA and how we train the proposed CNN model. Afterwards, the public image QA datasets and evaluation measurements are introduced. Finally, the experimental results are presented and analyzed."}, {"heading": "Configurations and Training", "text": "Three layers of convolution and max-pooling are employed for the sentence CNN. The numbers of the feature maps for the three convolution layers are 300, 400, and 400, respectively. The sentence CNN is designed on a fixed architecture, which needs to be set to accommodate the maximum length of the questions. In this paper, the maximum length of the question is chosen as 38. The word embeddings are obtained by the skip-gram model (Mikolov et al. 2013) with the dimension as 50. We use the VGG (Simonyan and Zisserman 2014) network as the image CNN. The dimension of \u03bdim is set as 400. The multimodal CNN takes the image and sentence representations as the input and generate the joint representation with the number of feature maps as 400.\nThe proposed CNN model is trained with stochastic gradient descent with mini batches of 100 for optimization, where the negative log likelihood is chosen as the loss. During the training process, all the parameters are tuned, including the parameters of nonlinear image mapping, image CNN, sentence CNN, multimodal convolution layer, and softmax layer. Moreover, the word embeddings are also fine-tuned. In order to prevent overfitting, dropout (with probability 0.1) is used."}, {"heading": "Image QA Datasets", "text": "We test and compare our proposed CNN model on the public image QA databases, specifically the DAQUAR (Malinowski and Fritz 2014a) and COCO-QA (Ren, Kiros, and Zemel 2015) datasets. DAQUAR-All (Malinowski and Fritz 2014a) This dataset consists of 6,795 training and 5,673 testing samples, which are generated from 795 and 654 images, respectively. The images are from all the 894 object categories. There are mainly three types of questions in this dataset, specifically the object type, object color, and number of objects. The answer may be a single word or multiple words. DAQUAR-Reduced (Malinowski and Fritz 2014a) This dataset is a reduced version of DAQUAR-All, comprising 3,876 training and 297 testing samples. The images are constrained to 37 object categories. Only 25 images are used for the testing sample generation. Same as the DAQUAR-All dataset, the answer may be a single word or multiple words. COCO-QA (Ren, Kiros, and Zemel 2015) This dataset consists of 79,100 training and 39,171 testing samples, which are generated from about 8,000 and 4,000 images, respectively. There are four types of questions, specifically the object, number, color, and location. The answers are all single-word."}, {"heading": "Evaluation Measurements", "text": "One straightforward way for evaluating image QA is to utilize accuracy, which measures the proportion of the correctly answered testing questions to the total testing questions. Besides accuracy, Wu-Palmer similarity (WUPS) (Wu and Palmer 1994; Malinowski and Fritz 2014a) is also used to measure the performances of different models on the image QA task. WUPS calculates the similarity between two words based on their common subsequence in a taxonomy tree.\nA threshold parameter is required for the calculation of WUPS. Same as the previous work (Ren, Kiros, and Zemel 2015; Malinowski and Fritz 2014a; Malinowski, Rohrbach, and Fritz 2015), the threshold parameters 0.0 and 0.9 are used for the measurements WUPS@0.0 and WUPS@0.9, respectively."}, {"heading": "Experimental Results and Analysis", "text": "Competitor Models We compare our models with recently developed models for the image QA task, specifically the multi-world approach (Malinowski and Fritz 2014a), the VSE model (Ren, Kiros, and Zemel 2015), and the Neural-Image-QA approach (Malinowski, Rohrbach, and Fritz 2015).\nPerformances on Image QA The performances of our proposed CNN model on the DAQUAR-All, DAQUARReduced, and COCO-QA datasets are illustrated in Table 1, 2, and 3, respectively. For DAQUAR-All and DAQUARReduced datasets with multiple words as the answer to the question, we treat the answer comprising multiple words as an individual class for training and testing.\nFor the DAQUAR-All dataset, we evaluate the performances of different image QA models on the full set (\u201cmultiple words\u201d). The answer to the image and question pair may be a single word or multiple words. Same as the work (Malinowski, Rohrbach, and Fritz 2015), a subset containing the samples with only a single word as the answer is created and employed for comparison (\u201csingle word\u201d). Our proposed CNN model significantly outperforms the multiworld approach and Neural-Image-QA in terms of accuracy, WUPS@0.0, and WUPS@0.9. Specifically, our proposed CNN model achieves over 20% improvement compared to Neural-Image-QA in terms of accuracy on both \u201cmultiple words\u201d and \u201csingle word\u201d. The results, shown in Table. 1, demonstrate that our CNN model can more accurately model the image and question as well as their interactions, thus yields better performances for the image QA task. Moreover, the language approach (Malinowski, Rohrbach, and Fritz 2015), which only resorts to the question performs inferiorly\nto the approaches that jointly model the image and question. The image component is thus of great help to the image QA task. One can also see that the performances on \u201cmultiple words\u201d are generally inferior to those on \u201csingle word\u201d.\nFor the DAQUAR-Reduced dataset, besides the NeuralImage-QA approach, the VSE model is also compared on \u201csingle word\u201d. Moreover, some of the methods introduced in (Ren, Kiros, and Zemel 2015) are also reported and compared. GUESS is the model which randomly outputs the answer according to the question type. BOW treats each word of the question equally and sums all the word vectors to predict the answer by logistic regression. LSTM is performed only on the question without considering the image, which is similar to the language approach (Malinowski, Rohrbach, and Fritz 2015). IMG+BOW performs the multinomial logistic regression based on the image feature and a BOW vector obtained by summing all the word vectors of the question. VIS+LSTM and 2-VIS+BLSTM are two versions of the VSE model. VIS+LSTM has only a single LSTM to encode the image and question in one direction, while 2-VIS+BLSTM uses a bidirectional LSTM to encode the image and question along with both directions to fully exploit the interactions between image and each word of the question. It can be observed that 2-VIS+BLSTM outperforms VIS+LSTM with a big margin. The same observation can also be found on the COCO-QA dataset, as shown in Table 3, demonstrating that the bidirectional LSTM can more accurately model the interactions between image and question than the single LSTM. Our proposed CNN model significantly outperforms the competitor models. More specifically, for the case of \u201csingle word\u201d, our proposed CNN achieves nearly 20% improvement in terms of accuracy over the best competitor model 2-VIS+BLSTM.\nFor the COCO-QA dataset, IMG+BOW outperforms VIS+LSTM and 2-VIS+BLSTM, demonstrating that the\nsimple multinomial logistic regression of IMG+BOW can better model the interactions between image and question, compared with the LSTMs of VIS+LSTM and 2- VIS+BLSTM. By averaging VIS+LSTM, 2-VIS+BLSTM, and IMG+BOW, the FULL model is developed, which summarizes the interactions between image and question from different perspectives thus yields a much better performance. As shown in Table 3, our proposed CNN model outperforms all the competitor models in terms of all the three evaluation measurements, even the FULL model. The reason may be that the image representation is of highly semantic meaning, which should interact with the high semantic components of the question. Our CNN model firstly uses the convolutional architectures to compose the words to highly semantic representations. Afterwards, we let the image meet the composed highly semantic representations and use convolutional architectures to exploit their relations and interactions for the answer prediction. As such, Our CNN model can well model the relations between image and question, and thus obtain the best performances.\nInfluence of Multimodal Convolution Layer The image and question needs to be considered together for the image QA. The multimodal convolution layer in our proposed CNN model not only fuses the image and question representations together but also learns the interactions and relations between the two multimodal inputs for further question prediction. The effect of the multimodal convolution layer is examined as follows. The image and question representations are simply concatenated together as the input of the softmax layer for the answer prediction. We train the network in the same manner as the proposed CNN model. The results are provided in Table 3. Firstly, it can be observed that without the multimodal convolution layer, the performance on the image QA has dropped. Comparing to the simple concatenation process fusing the image and question representations, our proposed multimodal convolution layer can well exploit the complicated relationships between image and question representations. Thus a better performance for the answer prediction is achieved. Secondly, the\napproach without multimodal convolution layer outperforms the IMG+BOW, VIS+LSTM and 2-VIS+BLSTM, in terms of accuracy. The better performance is mainly attributed to the composition ability of the sentence CNN. Even with the simple concatenation process, the image representation and composed question representation can be fuse together for a better image QA model.\nInfluence of Image CNN and Effectiveness of Sentence CNN As can be observed in Table 1, without the image content, the accuracy of human answering the question drops from 50% to 12%. Therefore, the image content is critical to the image QA task. Same as the work (Malinowski, Rohrbach, and Fritz 2015; Ren, Kiros, and Zemel 2015), we only use the question representation obtained from the sentence CNN to predict the answer. The results are listed in Table 3. Firstly, without the use of image representation, the performance of our proposed CNN significantly drops, which again demonstrates the importance of image component to the image QA. Secondly, the model only consisting of the sentence CNN performs better than LSTM and BOW for the image QA. It indicates that the sentence CNN is more effective to generate the question representation for image QA, compared with LSTM and BOW. Recall that the model without multimodal convolution layers outperforms IMG+BOW, VIS+LSTM, and 2-VIS+BLSTM, as explained above. By incorporating the image representation, the better modeling ability of our sentence CNN is demonstrated.\nMoreover, we examine the language modeling ability of the sentence CNN as follows. The words of the test questions are randomly reshuffled. Then the reformulated questions are sent to the sentence CNN to check whether the sentence CNN can still generate reliable question representations and make accurate answer predictions. For randomly reshuffled questions, the results on COCO-QA dataset are 40.74, 53.06, and 80.41 for the accuracy, WUPS@0.9, and WUPS@0.0, respectively, which are significantly inferior to that of natural-language like questions. The result indicates that the sentence CNN possesses the ability of modeling natural questions. The sentence CNN uses the convolution process to compose and summarize the neighboring words. And the reliable ones with higher semantic meanings will be pooled and composed further to reach the final sentence representation. As such, the sentence CNN can compose the natural-language like questions to reliable high semantic representations."}, {"heading": "Conclusion", "text": "In this paper, we proposed one CNN model to address the image QA problem. The proposed CNN model relies on convolutional architectures to generate the image representation, compose consecutive words to the question representation, and learn the interactions and relations between the image and question for the answer prediction. Experimental results on public image QA datasets demonstrate the superiority of our proposed model over the state-of-the-art methods."}, {"heading": "Acknowledgement", "text": "The work is partially supported by China National 973 project 2014CB340301. The authors are grateful to Baotian Hu and Zhenguo Li for their insightful discussions and comments."}], "references": [{"title": "D", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "Parikh"], "venue": "2015. VQA: visual question answering. arXiv", "citeRegEx": "Antol et al. 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "2014", "author": ["X. Chen", "Zitnick", "C. L"], "venue": "Learning a recurrent visual representation for image caption generation. arXiv", "citeRegEx": "Chen and Zitnick 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "G", "author": ["G.E. Dahl", "T.N. Sainath", "Hinton"], "venue": "E.", "citeRegEx": "Dahl. Sainath. and Hinton 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "Darrell"], "venue": "2014. Long-term recurrent convolutional networks for visual recognition and description. arXiv", "citeRegEx": "Donahue et al. 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "G", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "Zweig"], "venue": "2014. From captions to visual concepts and back. arXiv", "citeRegEx": "Fang et al. 2014", "shortCiteRegEx": null, "year": 1411}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome"], "venue": null, "citeRegEx": "Frome,? \\Q2013\\E", "shortCiteRegEx": "Frome", "year": 2013}, {"title": "W", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "Xu"], "venue": "2015. Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv", "citeRegEx": "Gao et al. 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "Visual turing test for computer vision systems", "author": ["Geman"], "venue": "In PNAS", "citeRegEx": "Geman,? \\Q2015\\E", "shortCiteRegEx": "Geman", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu"], "venue": null, "citeRegEx": "Hu,? \\Q2014\\E", "shortCiteRegEx": "Hu", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "2014", "author": ["A. Karpathy", "Li", "F.-F"], "venue": "Deep visual-semantic alignments for generating image descriptions. arXiv", "citeRegEx": "Karpathy and Li 2014", "shortCiteRegEx": null, "year": 1412}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Joulin Karpathy", "A. Li 2014] Karpathy", "A. Joulin", "F.-F. Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Salakhutdinov", "R. Zemel"], "venue": "In ICML", "citeRegEx": "R. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "R. et al\\.", "year": 2014}, {"title": "Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation", "author": ["Klein"], "venue": null, "citeRegEx": "Klein,? \\Q2015\\E", "shortCiteRegEx": "Klein", "year": 2015}, {"title": "and Bengio", "author": ["Y. Lecun"], "venue": "Y.", "citeRegEx": "Lecun and Bengio 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Multimodal convolutional neural networks for matching image and sentence", "author": ["Ma"], "venue": null, "citeRegEx": "Ma,? \\Q2015\\E", "shortCiteRegEx": "Ma", "year": 2015}, {"title": "Mutual learning of an object concept and language model based on mlda and npylm", "author": ["Makamura"], "venue": "In IROS", "citeRegEx": "Makamura,? \\Q2013\\E", "shortCiteRegEx": "Makamura", "year": 2013}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "M. Fritz 2014a] Malinowski", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "M", "author": ["M. Malinowski", "Fritz"], "venue": "2014b. Towards a visual turing challenge. arXiv", "citeRegEx": "Malinowski and Fritz 2014b", "shortCiteRegEx": null, "year": 1410}, {"title": "M", "author": ["M. Malinowski", "Fritz"], "venue": "2015. Hard to cheat: A turing test based on answering questions about images. arXiv", "citeRegEx": "Malinowski and Fritz 2015", "shortCiteRegEx": null, "year": 1501}, {"title": "M", "author": ["M. Malinowski", "M. Rohrbach", "Fritz"], "venue": "2015. Ask your neurons: A neural-based approach to answering questions about images. arXiv", "citeRegEx": "Malinowski. Rohrbach. and Fritz 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "2014a", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. L Yuille"], "venue": "Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv", "citeRegEx": "Mao et al. 2014a", "shortCiteRegEx": null, "year": 1412}, {"title": "2014b", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. L Yuille"], "venue": "Explain images with multimodal recurrent neural networks. arXiv", "citeRegEx": "Mao et al. 2014b", "shortCiteRegEx": null, "year": 1410}, {"title": "J", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "Dean"], "venue": "2013. Efficient estimation of word representations in vector space. arXiv", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 1301}, {"title": "T", "author": ["V. Ordonez", "G. Kulkarni", "Berg"], "venue": "L.", "citeRegEx": "Ordonez. Kulkarni. and Berg 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2015", "author": ["M. Ren", "R. Kiros", "R. S Zemel"], "venue": "Exploring models and data for image question answering. arXiv", "citeRegEx": "Ren. Kiros. and Zemel 2015", "shortCiteRegEx": null, "year": 1505}, {"title": "A", "author": ["K. Simonyan", "Zisserman"], "venue": "2014. Very deep convolutional networks for largescale image recognition. arXiv", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 1409}, {"title": "A", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "Ng"], "venue": "Y.", "citeRegEx": "Socher et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": null, "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["D. Erhan"], "venue": null, "citeRegEx": "Erhan,? \\Q2014\\E", "shortCiteRegEx": "Erhan", "year": 2014}, {"title": "M", "author": ["Z. Wu", "Palmer"], "venue": "S.", "citeRegEx": "Wu and Palmer 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Y", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Bengio"], "venue": "2015a. Show, attend and tell: Neural image caption generation with visual attention. arXiv", "citeRegEx": "Xu et al. 2015a", "shortCiteRegEx": null, "year": 1502}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}