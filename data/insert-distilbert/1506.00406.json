{"id": "1506.00406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations", "abstract": "in this paper, we propose two new features for easily estimating phrase based machine protein translation parameters. we only use monolingual data with the assumption that the phrase table is given to us, but all of its scores have eventually been removed. our method is based based on two recently introduced neural network oriented vector representation models for words and sentences. it is definitely the notable first use combining of these models in an end to end phrase based machine translation system. scores obtained from our method can recover more than 80 % of bleu loss caused by removing phrase table domain probabilities. we results also show that our features combined with the phrase table probabilities improve the bleu score by 0. 74 points", "histories": [["v1", "Mon, 1 Jun 2015 09:36:23 GMT  (367kb)", "http://arxiv.org/abs/1506.00406v1", null], ["v2", "Thu, 3 Sep 2015 16:44:32 GMT  (393kb)", "http://arxiv.org/abs/1506.00406v2", null], ["v3", "Tue, 24 May 2016 15:42:50 GMT  (223kb)", "http://arxiv.org/abs/1506.00406v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir pouya aghasadeghi", "mohadeseh bastan"], "accepted": false, "id": "1506.00406"}, "pdf": {"name": "1506.00406.pdf", "metadata": {"source": "CRF", "title": "Rebuilding Phrase Table Scores from Monolingual Resources Using Neural Networks Vector Representations", "authors": ["Amir Pouya Aghasadeghi", "Mohadeseh Bastan", "Shahram Khadivi"], "emails": ["aghasadeghi@aut.ac.ir", "m.bastan@aut.ac.ir", "khadivi@aut.ac.ir"], "sections": [{"heading": null, "text": "for estimating phrase based machine translation parameters. We only use monolingual data with the assumption that the phrase table is given to us, but all of its scores have been removed. Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first use of these models in an end to end phrase based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by 0.74 points."}, {"heading": "1 Introduction", "text": "Statistical machine translation systems are currently trained from large amounts of parallel corpora. These corpora are used for learning and tuning parameter of statistical model(Brown et al., 1993). Obtaining these parallel corpora are expensive and time consuming. Also, these corpora are not available in all language pairs. On the contrary, monolingual data are available for most of the languages and can be found from many different resources easily. Due to these reasons finding a method to train machine translation model with monolingual data instead of bilingual data has been a focus of many researchers in the last couple of years.\nThe idea of the possibility of learning translation model from monolingual corpora came from the similarity and regularities existing between languages. Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al., 2008;\n(Schafer and Yarowsky, 2002) and topic models(Mimno et al., 2009) are some examples of those features. It has been demonstrated that (Klementiev et al., 2012) by using these features alone for estimating translation model parameters most of BLEU loss is recoverable.\nAnother recent path of work in the field of natural language processing is learning continuous vector representations for words, sentences and documents using neural networks (Le and Mikolov, 2014; Mikolov, et al., 2013a; Mikolov et al., 2013c; Pennington et al., 2014). These vector representations can capture great amounts of syntactic and semantic features between words and sentences. Also, it has been shown by using a linear transformation matrix learned from a small bilingual dictionary and applying it to the source vectors obtained from words in a large monolingual corpora, these models can be used for translating words from source to target and can extend the seed dictionary (Mikolov et al., 2013b).\nIn this work we used two distributed representation of neural network models , continues bag of word, CBOW (Mikolov, et al., 2013a) and paragraph vector with distributed memory, PVDM(Le and Mikolov, 2014) both trained on monolingual corpora, to rebuild phrase table scores. We converted all the phrases in the phrase table using PV-DM and all the words in the lexical table using CBOW to a vector space. Afterward by multiplying a transformation matrix to source vectors, we calculated cosine similarities between vector pairs in projected source space and the target space. We used these new scores in an end to end phrase based statistical machine translation system (Koehn et al., 2003). Same as (Klementiev, et al., 2012) we assumed the phrase table is available to us, but all of its scores have been removed. We showed that these features alone can recover most of the BLEU loss caused by removing the scores from the phrase table.\nAlso by combining them with phrase table probabilities they can improve the quality of a machine translation system. In contrast with orthographic features, since both of these models are language independent our method is useful for related and unrelated language pairs and since none of these models need any extra metadata such as time stamp this method can be used with any type of monolingual corpora. The only bilingual data needed for our method is a small dictionary and a small amount of parallel sentences for training the transformation matrix needed for projecting the source space to the target space."}, {"heading": "2 Background", "text": "In the first part of this section we will briefly discuss different parameters of phrase based statistical machine translation. Later we will show how our model can replace those parameters using monolingual feature. In the second part a short history about usage of neural networks in different tasks of NLP and more detailed description of the models that we used in this work will be introduced."}, {"heading": "2.1 Parameters of phrase-based statistical", "text": "machine translation systems\nIn the beginning, statistical machine translation systems were some probabilistic models trained for finding word level translation. Those models were trained on large amounts of sentence aligned parallel corpora (Brown, et al., 1993). Due to the inefficiency of those word level models, currently they have been replaced with phrase based translation model (Koehn, et al., 2003) and hierarchical models (Chiang, 2007). In this section we will briefly describe the parameter of the classic phrase based machine translation system which we will replace in the following sections using our vector based approach.\nTraining a phrase based statistical machine translation model usually depends on the alignment between words extracted by word level translation models (Brown, et al., 1993). These models have several important parameters as follow:\n Phrase pairs: phrase pairs are pairs of words and part of sentences which are ex-\ntracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).\nIn this work the same as (Klementiev, et al., 2012) we assumed that the phrase table is given to us but all of its scores have been removed.\n Phrase translation probabilities: Each phrase pair has a list of associated feature\nfunctions (FFs). These include phrase translation probabilities, \u03c6(e|f) and \u03c6(f|e), which are typically calculated via maximum likelihood estimation (Klementiev, et al., 2012). In the following sections we will show how we replace \u03c6(e|f) and \u03c6(f|e) with phrase vectors similarities.\n Lexical Weightings: One way to validate the quality of a phrase translation pair is\nto check, how well its words translate to each other (Koehn, et al., 2003). For this purpose phrase based systems are using lexical weighting. Lexical weighting is usually computed using the internal alignments of phrases and lexical translation probability distribution. Lexical translation probability can be obtained by the equation (1). In the next section we will show how we replace these probabilities with word vectors similarities.\n'\n( , ) ( | )\n( ', ) f\ncount f e w f e\ncount f e   (1)\n Language Models: Language models are used to control the output quality of trans-\nlation systems by using n-gram probabilities obtained from monolingual sources. Since they are monolingual features we will directly use them in our model."}, {"heading": "2.2 Neural network approaches in various tasks of NLP and machine translation", "text": "Neural networks have recently been into a lot of attention in different fields of NLP. By using neural networks in text classification (Zhang and Zhou, 2006), sentiment analysis(Socher et al., 2013), summarization (Cao et al., 2015), statistical machine translation (Devlin et al., 2014; Mikolov et al., 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved. In machine translation, neural networks were first used by (Casta\u00f1o and Casacuberta, 1997; Castano et al., 1997)They used a neural network for example-based machine translation. By now many attempts have been made to improve machine translation using neural\nnetworks. Word representation by using a continuous vector was first done by (Rumelhart et al., 1988). Benjio also used a feedforward neural network to learn word representation whose work was followed by many others (Bengio et al., 2003). In this way, words (which are known as one of the most important components of natural language processing) with different lengths were converted to vectors of real numbers with fixed lengths and used in many applications.\nMikolov (2013a) proposed a new architecture for learning distributed representation of words. In their work, a word vector was learned by using a feedforward neural network. They claimed that neural networks work better than LSA for preserving linear regularities among words and also have less expenses than LDA for large datasets. The result of their work was a powerful representation of words in large datasets with low computation complexity. They offered two neural networks for learning word vectors. In CBOW model, a feedforward neural network with an input layer, a projection layer and an output layer were proposed. The projection layer is common among all words. Thus, the input of this neural network is a window of n future words and n history words of the current word. All the words are projected to a common space. By averaging these vectors the current word is predicted. In Skipgram model, the input is a word which is fed into the projection layer and the output is 2*n vectors for n future and n history words of the current word.\nAs a result, in this model the attempt is to maximize word classification according to the word\u2019s neighborhood in a common sentence.\nSimultaneously, they also tried to explore similarities among languages by using a transformation matrix between word vectors (Mikolov, et al., 2013b). They first created two models for source and target languages by using large monolingual datasets, then with a small bilingual dictionary the linear projection among languages was learned. For this purpose, dictionary\u2019s words with different lengths were projected to fixed length vectors by using CBOW or Skipgram model. In practice, when the monolingual data are limited, the Skipgram shows a better representation of words, but CBOW is faster and is recommended for larger datasets. Word vectors are computed with a projection matrix W by using the equation (2), where xi is the vector of the source word i and zi is the projection of the vector representing in the target space.\ni iWx z (2)\nFinding projection matrix (W) in his equation can be viewed as an optimization problem and it can be solved by minimizing the error rate using a gradient descent. Their results show that this method covers 92% of English to Spanish dictionary for words. Another advantage of this method is that in can be used for retrieving dictionary\u2019s missing words.\nThe main weakness of the Skipgram and CBOW models is that they can only be used on words and short phrases. In 2014 Le and Mikolov\nw(t)\nw(t-1)\nw(t+1)\nw(t+2)\nw(t-2)\nINPUT PROJECTION\nSUM\nOUTPUT\nw(t)\nw(t-1)\nw(t+1)\nw(t+2)\nw(t-2)\nINPUT PROJECTION OUTPUT\nCBOW Skip-gram\nFigure 1- CBOW and Skip-gram architecture, CBOW predicts the current word using the context and Skipgram predict surrounding words using the current word(Mikolov et al., 2013a)\nSUM\nproposed a distributed representation of paragraphs. This model learned paragraph vector with fixed lengths from a variable length text (e.g. Phrases, sentences, paragraphs and documents.). The word in the paragraph is predicted with this vector. This work is done in two steps: first the word vector is learned by using a feedforward neural network in CBOW or Skipgram model, then the word vectors are fed into another neural network with paragraph vector with the same length. All the vectors are concatenated or averaged and the output is the new paragraph vector.\nThis model was used to predict a word in sentence, paragraph, document or any variable length text. In this way, by considering a fixed length window of words in the text, the words vectors and text vector are fed into the neural network and the new text vector is generated. This process is continued until convergence. Their experiments show a 16% improvement in the state of the arts sentiment analysis error and 30% improvement in bag-of-the words text classification."}, {"heading": "3 Rebuilding phrase table scores using vector representations similarity", "text": "Phrase tables usually have 4 feature scores.\n Direct phrase translation probability \u03c6(e|f)\n Direct lexical weighting lex(e|f)\n Inverse phrase translation probability \u03c6(f|e)\n Inverse lexical weighting lex(f|e) In this section, we will describe how each of\nthese scores have been replaced using monolingual features. The results of our experiments can be found in section 5."}, {"heading": "3.1 Lexical weightings", "text": "As mentioned in section 2.1 the main purpose of lexical weightings is to reduce the impact of phrase scores which has been overestimated by MLE. Since lexical weighting is computed using word translation probability its calculation depends on bilingual data.\nVectors trained using CBOW or Skipgram models capture great linear regularities between similar or relate words. These regularities can be found in vectors trained in different languages from comparable corpora. Because of these regularities if a correct linear transformation matrix applied to source vectors, it can project them to the target space. It has been shown that the closest target vector to the projected source vector is the most probable translation of that word.\nIn our model we converted all of the source and target words existed in the lexicon table to vectors using CBOW model. After this part we needed a transformation matrix that can project those spaces to each other. This matrix can be found by selecting some pairs of words from the source and target side. We trained our matrices using most common words in source side and their translation. Even though one transformation matrix and its inverse should be enough for our purpose, we decided to train two different matrices, one for direct lexical weightings and one for inverse lexical weightings. In this way our model is more faulttolerant.\nAfter projecting source vectors and target vectors to each other, we calculated cosine similarity between each of the pairs existing in lexicon table (equation 3). In this equation S and T are source and target vectors.\n. cos( ) S T similarity\nS T   (3)\nBecause of properties of CBOW model, we get to the conclusion that word translation probabilities can be replaced by these similarities. Our results in sections 5 show that our assumption is correct.\nBy having word translation scores, computing lexical weightings for each of the phrases is a straightforward task. For this purpose we used equation (4) (Koehn, et al., 2003).\n( , )1\n1 ( | , ) ( | )\n|{j | (i, j) }|\nn\nw i j\ni j ai\nf e a w f e a P    \n (4)\nThe only difference between our model and bilingual lexical weightings, is that since our model cannot compute null alignment probability for different words we are using a small constant instead of it."}, {"heading": "3.2 Phrase probabilities", "text": "CBOW model cannot work with text inputs of different lengths. Since phrases have variable lengths we needed to use a different model to convert phrases to vectors. We found PV-DM suitable for this task. Our PV-DM network uses the same word to vector models used in the lexical weighting part.\nSame as the lexical weightings part we converted all of the unique phrases in the phrase table to vectors. The next step would be to find a method to project source space and target space to each other. Since PV-DM shares many properties from CBOW and Skipgram models, the phrase vector spaces can probably be mapped to each\nother using the same method described in part 3.1. Figure (2) shows source and target space for the following sentences. For presentation purposes we use a PCA on source and target space and reduce the space dimension to 2. As it can be seen, by applying a linear transformation two sentence spaces can be projected to each other. English Spanish\n1 This is not on! Esto no est\u00e1 bien. 2 It is absolutely not on! No est\u00e1 en absoluto bien.\n3 Take your courage in\nboth hands!\nNo es l\u00edcito meter miedo.\n4 The Committee on\nFisheries endorses this view.\nLa Comisi\u00f3n de Pesca est\u00e1 de acuerdo en ello.\n5 Chapter A-30 again\ncaused problems.\nEl cap\u00edtulo A-30 volvi\u00f3 a presentar muchos problemas\nSelecting the right data for training the transformation matrix was a challenge. There were three options available for training the transformation matrices; word pairs from a dictionary, high probability phrases from the phrase table or short sentences from a parallel corpus. Since phrase pairs in phrase table are found with heuristics, even selecting high probability phrase pairs could be inaccurate. Word vector space and phrase vector space are different and hence using word pairs for transformation matrix training did not yield promising results. Thus, we decided to use a small amount of short sentences from a parallel corpora to train our transformation matrices. Same as section 3.1 we trained two different transformation matrices for direct phrase score and inverse phrase score. We also calculated the phrase\nscores using the cosine similarity between vectors."}, {"heading": "4 Experiment Setup", "text": "For our experiments we used Spanish as the source language and English as the target language. We trained our phrase based statistical machine translation model using Moses system (Koehn et al., 2007) on full version of the parallel Europarl V5 (Koehn, 2005). We limited the maximum phrase length to 6 and removed the lexical reordering feature from Moses training. We used default Moses settings for all other parameters. In this way we could compare our model trained with monolingual data against the parallel system. Also, we trained our 4-gram language model using KenLM (Heafield, 2011) on the full English Wikipedia combined with Europarl English side. As our test and development sets we used WMT07, each set contained 2000 sentences.\nFor training CBOW model on the words we used English and Spanish Wikipedia. The size of these data sets can be found in the table (1). Our vector space for word and phrase were 200 dimensional space. We trained each of these models with 30 epochs.\nFigure 2 : PV-DM output for five different Spanish sentences (left) from Europarl corpus and their translation to English (right). All vectors were projected down to two dimensional space using PCA. As it can be seen same patterns exist between the source side and the target side, so these spaces can be projected to each other\u2019s using linear transformations.\nlabels are \u03c6(e|f)/lex(e|f)/\u03c6(f|e)/lex(f|e). We used \u2018M\u2019 for monolingual scores, \u2018B\u2019 for bilingual scores and \u2018C\u2019 for monolingual and bilingual scores combined. \u03c6 is phrase probability and the lex is lexical weighting score.\nWe trained four transformation matrices for projecting words and phrases vectors from the source side to target side and vice versa. For training word transformation matrices we used 10000 most common words in Spanish Wikipedia and translated them using google translate. For phrase vector transformation matrices we used 5000 unique short sentences (with lengths of 1 to 8) from Europarl corpus. We use the same method described in (Mikolov, et al., 2013b) for training our transformation matrices. For tuning model weights, in both Moses bases system and vector base system we used minimum error rate training (Och, 2003).\nEven though our model needs parallel data for finding phrase pairs, all other bilingual obtained scores have been removed from the phrase table for our experiences. The phrase table extracted by Moses has near 20 million phrase pairs. For every unique Spanish phrase there are many English phrase suggestions, because of this situation phrase score, which we obtain from monolingual sources play an important role in machine translation systems. Table (2) shows some statistics about the phrase table used in our experiences. More result about the impact of phrase scores can be found in the next section."}, {"heading": "5 Experiment Results", "text": "We replace phrase tables scores with 4 independent monolingual feature scores obtained from our vector based models. We ran our end to end statistical machine translation system for each of those scores and their combination. Also, we had combined those scores with original phrase table scores and calculated BLEU for both systems combined. Figure (3) show our results. Experiment 1 is the baseline system trained by Moses default settings. Experiment 2 show BLEU score after all the bilingual data has been removed. In experiment 3, we replace the direct phrase translation probability with our model direct vector based similarity. In experiment 4, we added our direct monolingual lexical weightings score to experiment 3 phrase table. We explore our inverse phrase probability score effect in experiment 5. In experiment 6, we replace all bilingual phrase table score with our monolingual alternatives scores. Experiment 7 shows all of our features combined with baseline system scores. A summary of our results can be found in the table (3).\nAs we have shown our method successfully recover more that 82% percent of the BLEU loss caused by removing bilingual scores from the phrase table. Also, combining our features improved the baseline system BLEU score by 0.74 points."}, {"heading": "6 Conclusion", "text": "In this paper, we introduced two new features for approximating phrase table scores using monolingual data. These features can be obtained without using any additional data or metadata such as time stamps and also they are language independent. By using these features we successfully recover more than 82% of the BLEU loss caused by removing the phrase table scores. Also by combining these features with bilingual obtained scores, we improved the BLEU score by 0.74 percent.\nReference\nBengio, Y., Ducharme, R., Vincent, P., & Janvin,\nC. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137-1155.\nBrown, P. F., Pietra, V. J. D., Pietra, S. A. D., &\nMercer, R. L. (1993). The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2), 263-311.\nCao, Z., Wei, F., Dong, L., Li, S., & Zhou, M.\n(2015). Ranking with Recursive Neural Networks and Its Application to MultiDocument Summarization. Paper presented at the Twenty-Ninth AAAI Conference on Artificial Intelligence.\nCasta\u00f1o, M. A., & Casacuberta, F. (1997). A\nconnectionist approach to machine translation. Paper presented at the EUROSPEECH.\nCastano, M. A., Casacuberta, F., & Vidal, E.\n(1997). Machine translation using neural networks and finite-state models. Theoretical and Methodological Issues in Machine Translation (TMI), 160-167.\nChiang, D. (2007). Hierarchical phrase-based\ntranslation. computational linguistics, 33(2), 201-228.\nCollobert, R., & Weston, J. (2008). A unified\narchitecture for natural language processing: Deep neural networks with multitask learning. Paper presented at the Proceedings of the 25th international conference on Machine learning.\nDevlin, J., Zbib, R., Huang, Z., Lamar, T.,\nSchwartz, R., & Makhoul, J. (2014). Fast and robust neural network joint models for\nstatistical machine translation. Paper presented at the 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June.\nHaghighi, A., Liang, P., Berg-Kirkpatrick, T., &\nKlein, D. (2008). Learning Bilingual Lexicons from Monolingual Corpora. Paper presented at the ACL.\nHeafield, K. (2011). KenLM: Faster and smaller\nlanguage model queries. Paper presented at the Proceedings of the Sixth Workshop on Statistical Machine Translation.\nKlementiev, A., Irvine, A., Callison-Burch, C., &\nYarowsky, D. (2012). Toward statistical machine translation without parallel corpora. Paper presented at the Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.\nKoehn, P. (2005). Europarl: A parallel corpus for\nstatistical machine translation. Paper presented at the MT summit.\nKoehn, P., Hoang, H., Birch, A., Callison-Burch,\nC., Federico, M., Bertoldi, N., et al. (2007). Moses: Open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions.\nKoehn, P., & Knight, K. (2002). Learning a\ntranslation lexicon from monolingual corpora. Paper presented at the Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9.\nKoehn, P., Och, F. J., & Marcu, D. (2003).\nStatistical phrase-based translation. Paper presented at the Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1.\nLe, Q. V., & Mikolov, T. (2014). Distributed\nrepresentations of sentences and documents. arXiv preprint arXiv:1405.4053.\nMikolov, T., Chen, K., Corrado, G., & Dean, J.\n(2013a). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\nMikolov, T., Karafi\u00e1t, M., Burget, L., Cernock\u00fd, J.,\n& Khudanpur, S. (2010). Recurrent neural network based language model. Paper presented at the INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010.\nMikolov, T., Le, Q. V., & Sutskever, I. (2013b).\nExploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G.\nS., & Dean, J. (2013c). Distributed representations of words and phrases and their compositionality. Paper presented at the Advances in Neural Information Processing Systems.\nMimno, D., Wallach, H. M., Naradowsky, J.,\nSmith, D. A., & McCallum, A. (2009). Polylingual topic models. Paper presented at the Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2.\nOch, F. J. (2003). Minimum error rate training in\nstatistical machine translation. Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1.\nOch, F. J., & Ney, H. (2004). The alignment\ntemplate approach to statistical machine translation. Computational linguistics, 30(4), 417-449.\nPennington, J., Socher, R., & Manning, C. D.\n(2014). Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.\nRapp, R. (1995). Identifying word translations in\nnon-parallel texts. Paper presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J.\n(1988). Learning representations by backpropagating errors. Cognitive modeling, 5.\nSchafer, C., & Yarowsky, D. (2002). Inducing\ntranslation lexicons via diverse similarity measures and bridge languages. Paper presented at the proceedings of the 6th conference on Natural language learningVolume 20.\nSocher, R., Perelygin, A., Wu, J. Y., Chuang, J.,\nManning, C. D., Ng, A. Y., et al. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at the Proceedings of the conference on empirical methods in natural language processing (EMNLP).\nTillmann, C. (2003). A projection extension\nalgorithm for statistical machine translation. Paper presented at the Proceedings of the 2003 conference on Empirical methods in natural language processing.\nVenugopal, A., Vogel, S., & Waibel, A. (2003).\nEffective phrase translation extraction from alignment models. Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsVolume 1.\nZhang, M.-L., & Zhou, Z.-H. (2006). Multilabel\nneural networks with applications to functional genomics and text categorization. Knowledge and Data Engineering, IEEE Transactions on, 18(10), 1338-1351."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Ranking with Recursive Neural Networks and Its Application to MultiDocument Summarization", "author": ["Z. Cao", "F. Wei", "L. Dong", "S. Li", "M. Zhou"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "A connectionist approach to machine translation. Paper presented at the EUROSPEECH", "author": ["M.A. Casta\u00f1o", "F. Casacuberta"], "venue": null, "citeRegEx": "Casta\u00f1o and Casacuberta,? \\Q1997\\E", "shortCiteRegEx": "Casta\u00f1o and Casacuberta", "year": 1997}, {"title": "Machine translation using neural networks and finite-state models", "author": ["M.A. Castano", "F. Casacuberta", "E. Vidal"], "venue": "Theoretical and Methodological Issues in Machine Translation (TMI),", "citeRegEx": "Castano et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Castano et al\\.", "year": 1997}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "computational linguistics, 33(2), 201-228.", "citeRegEx": "Chiang,? 2007", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Fast and robust neural network joint models for", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Learning Bilingual Lexicons from Monolingual Corpora. Paper presented at the ACL", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": null, "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "KenLM: Faster and smaller language model queries", "author": ["K. Heafield"], "venue": "Paper presented at the Proceedings of the Sixth Workshop on Statistical Machine Translation.", "citeRegEx": "Heafield,? 2011", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "Toward statistical machine translation without parallel corpora", "author": ["A. Klementiev", "A. Irvine", "C. Callison-Burch", "D. Yarowsky"], "venue": "Paper presented at the Proceedings of the 13th Conference of the European Chapter", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "Paper presented at the MT summit.", "citeRegEx": "Koehn,? 2005", "shortCiteRegEx": "Koehn", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th annual meeting", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N Bertoldi"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "Paper presented at the Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume", "citeRegEx": "Koehn and Knight,? \\Q2002\\E", "shortCiteRegEx": "Koehn and Knight", "year": 2002}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Paper presented at the Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Q.V. Le", "T. Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "Paper presented at the INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H.M. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "Paper presented at the Proceedings of the 2009 Conference on Empirical Methods in Natural Language", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "The alignment template approach to statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och and Ney,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Identifying word translations in non-parallel texts", "author": ["R. Rapp"], "venue": "Paper presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics.", "citeRegEx": "Rapp,? 1995", "shortCiteRegEx": "Rapp", "year": 1995}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Inducing translation lexicons via diverse similarity measures and bridge languages", "author": ["C. Schafer", "D. Yarowsky"], "venue": "Paper presented at the proceedings of the 6th conference on Natural language learning-", "citeRegEx": "Schafer and Yarowsky,? \\Q2002\\E", "shortCiteRegEx": "Schafer and Yarowsky", "year": 2002}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng", "A. Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A projection extension algorithm for statistical machine translation", "author": ["C. Tillmann"], "venue": "Paper presented at the Proceedings of the 2003 conference on Empirical methods in natural language processing.", "citeRegEx": "Tillmann,? 2003", "shortCiteRegEx": "Tillmann", "year": 2003}, {"title": "Effective phrase translation extraction from alignment models", "author": ["A. Venugopal", "S. Vogel", "A. Waibel"], "venue": "Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-", "citeRegEx": "Venugopal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Venugopal et al\\.", "year": 2003}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Zhang", "M.-L", "Zhou", "Z.-H"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "These corpora are used for learning and tuning parameter of statistical model(Brown et al., 1993).", "startOffset": 77, "endOffset": 97}, {"referenceID": 24, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al.", "startOffset": 22, "endOffset": 34}, {"referenceID": 8, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al., 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 13, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al., 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 26, "context": ", 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 20, "context": ", 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al., 2009) are some examples of those features.", "startOffset": 98, "endOffset": 118}, {"referenceID": 10, "context": "It has been demonstrated that (Klementiev et al., 2012) by using these features alone for estimating translation model parameters most of BLEU loss is recoverable.", "startOffset": 30, "endOffset": 55}, {"referenceID": 15, "context": "Another recent path of work in the field of natural language processing is learning continuous vector representations for words, sentences and documents using neural networks (Le and Mikolov, 2014; Mikolov, et al., 2013a; Mikolov et al., 2013c; Pennington et al., 2014).", "startOffset": 175, "endOffset": 269}, {"referenceID": 23, "context": "Another recent path of work in the field of natural language processing is learning continuous vector representations for words, sentences and documents using neural networks (Le and Mikolov, 2014; Mikolov, et al., 2013a; Mikolov et al., 2013c; Pennington et al., 2014).", "startOffset": 175, "endOffset": 269}, {"referenceID": 15, "context": ", 2013a) and paragraph vector with distributed memory, PVDM(Le and Mikolov, 2014) both trained on monolingual corpora, to rebuild phrase table scores.", "startOffset": 59, "endOffset": 81}, {"referenceID": 14, "context": "We used these new scores in an end to end phrase based statistical machine translation system (Koehn et al., 2003).", "startOffset": 94, "endOffset": 114}, {"referenceID": 5, "context": ", 2003) and hierarchical models (Chiang, 2007).", "startOffset": 32, "endOffset": 46}, {"referenceID": 22, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 28, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 29, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 27, "context": "By using neural networks in text classification (Zhang and Zhou, 2006), sentiment analysis(Socher et al., 2013), summarization (Cao et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": ", 2013), summarization (Cao et al., 2015), statistical machine translation (Devlin et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 7, "context": ", 2015), statistical machine translation (Devlin et al., 2014; Mikolov et al., 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 41, "endOffset": 84}, {"referenceID": 17, "context": ", 2015), statistical machine translation (Devlin et al., 2014; Mikolov et al., 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 41, "endOffset": 84}, {"referenceID": 6, "context": ", 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 31, "endOffset": 59}, {"referenceID": 3, "context": "In machine translation, neural networks were first used by (Casta\u00f1o and Casacuberta, 1997; Castano et al., 1997)They used a neural network for example-based machine translation.", "startOffset": 59, "endOffset": 112}, {"referenceID": 4, "context": "In machine translation, neural networks were first used by (Casta\u00f1o and Casacuberta, 1997; Castano et al., 1997)They used a neural network for example-based machine translation.", "startOffset": 59, "endOffset": 112}, {"referenceID": 25, "context": "Word representation by using a continuous vector was first done by (Rumelhart et al., 1988).", "startOffset": 67, "endOffset": 91}, {"referenceID": 0, "context": "Benjio also used a feedforward neural network to learn word representation whose work was followed by many others (Bengio et al., 2003).", "startOffset": 114, "endOffset": 135}, {"referenceID": 0, "context": "Benjio also used a feedforward neural network to learn word representation whose work was followed by many others (Bengio et al., 2003). In this way, words (which are known as one of the most important components of natural language processing) with different lengths were converted to vectors of real numbers with fixed lengths and used in many applications. Mikolov (2013a) proposed a new architecture for learning distributed representation of words.", "startOffset": 115, "endOffset": 376}, {"referenceID": 12, "context": "We trained our phrase based statistical machine translation model using Moses system (Koehn et al., 2007) on full version of the parallel Europarl V5 (Koehn, 2005).", "startOffset": 85, "endOffset": 105}, {"referenceID": 11, "context": ", 2007) on full version of the parallel Europarl V5 (Koehn, 2005).", "startOffset": 52, "endOffset": 65}, {"referenceID": 9, "context": "Also, we trained our 4-gram language model using KenLM (Heafield, 2011) on the full English Wikipedia combined with Europarl English side.", "startOffset": 55, "endOffset": 71}, {"referenceID": 21, "context": "For tuning model weights, in both Moses bases system and vector base system we used minimum error rate training (Och, 2003).", "startOffset": 112, "endOffset": 123}, {"referenceID": 9, "context": "Heafield, K. (2011). KenLM: Faster and smaller language model queries.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "Heafield, K. (2011). KenLM: Faster and smaller language model queries. Paper presented at the Proceedings of the Sixth Workshop on Statistical Machine Translation. Klementiev, A., Irvine, A., Callison-Burch, C., & Yarowsky, D. (2012). Toward statistical machine translation without parallel corpora.", "startOffset": 0, "endOffset": 234}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. Paper presented at the MT summit. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., et al. (2007). Moses: Open source toolkit for statistical machine translation.", "startOffset": 0, "endOffset": 212}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. Paper presented at the MT summit. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., et al. (2007). Moses: Open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Koehn, P., & Knight, K. (2002). Learning a translation lexicon from monolingual corpora.", "startOffset": 0, "endOffset": 431}, {"referenceID": 11, "context": "Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation.", "startOffset": 0, "endOffset": 42}, {"referenceID": 21, "context": "Och, F. J. (2003). Minimum error rate training in statistical machine translation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation.", "startOffset": 0, "endOffset": 29}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation.", "startOffset": 0, "endOffset": 193}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12. Rapp, R. (1995). Identifying word translations in non-parallel texts.", "startOffset": 0, "endOffset": 344}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12. Rapp, R. (1995). Identifying word translations in non-parallel texts. Paper presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by back-", "startOffset": 0, "endOffset": 564}, {"referenceID": 21, "context": "Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013). Recursive deep models for semantic compositionality over a sentiment treebank.", "startOffset": 1, "endOffset": 91}, {"referenceID": 21, "context": "Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at the Proceedings of the conference on empirical methods in natural language processing (EMNLP). Tillmann, C. (2003). A projection extension algorithm for statistical machine translation.", "startOffset": 1, "endOffset": 305}], "year": 2015, "abstractText": "In this paper, we propose two new features for estimating phrase based machine translation parameters. We only use monolingual data with the assumption that the phrase table is given to us, but all of its scores have been removed. Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first use of these models in an end to end phrase based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by 0.74 points.", "creator": "Microsoft\u00ae Word 2013"}}}