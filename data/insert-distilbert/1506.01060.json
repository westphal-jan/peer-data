{"id": "1506.01060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Global and Local Structure Preserving Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection", "abstract": "subspace learning is becoming more and more popular thanks to its capabilities of good interpretation. however, existing approaches don't adapt both local structure and self reconstruction very well. we propose local - structure adaptive sparse subspace learning ( assl ) model for assuming unsupervised feature selection. in this paper, we formulate the feature selection process as a subspace learning problem and incorporate a regularization term to preserve over the local structure of the data. furthermore, we successfully develop a sophisticated greedy algorithm to establish the basic model and an iterative strategy based on an accelerated block after coordinate descent is used to solve approximately the local - structure assl problem. we simultaneously also provide the global convergence analysis of the proposed assl algorithm. extensive experiments are conducted on real - world datasets to show the superiority variance of the proposed approach over from several state - of - simply the - art unsupervised feature selection approaches.", "histories": [["v1", "Tue, 2 Jun 2015 21:02:16 GMT  (377kb,D)", "http://arxiv.org/abs/1506.01060v1", "25 page, 6 figures and 51 references"], ["v2", "Mon, 19 Oct 2015 18:13:24 GMT  (411kb,D)", "http://arxiv.org/abs/1506.01060v2", "32 page, 6 figures and 60 references"]], "COMMENTS": "25 page, 6 figures and 51 references", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nan zhou", "yangyang xu", "hong cheng", "jun fang", "witold pedrycz"], "accepted": false, "id": "1506.01060"}, "pdf": {"name": "1506.01060.pdf", "metadata": {"source": "CRF", "title": "Local-Structure Adaptive Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection", "authors": ["Nan Zhoua", "Yangyang Xuc", "Hong Chenga", "Jun Fanga", "Witold Pedryczb"], "emails": ["nzhouuestc@gmai.com", "yangyang.xu@uwaterloo.ca", "hcheng@uestc.edu.cn", "JunFang@uestc.edu.cn", "wpedrycz@ualberta.ca"], "sections": [{"heading": null, "text": "Subspace learning is becoming more and more popular thanks to its capabilities of good interpretation. However, existing approaches do not adapt both local structure and self reconstruction very well. We propose local-structure adaptive sparse subspace learning (ASSL) model for unsupervised feature selection. In this paper, we formulate the feature selection process as a subspace learning problem and incorporate a regularization term to preserve the local structure of the data. Furthermore, we develop a greedy algorithm to establish the basic model and an iterative strategy based on an accelerated block coordinate descent is used to solve the local-structure ASSL problem. We also provide the global convergence analysis of the proposed ASSL algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches.\nKeywords: Machine learning, Feature selection, Subspace learning, Unsupervised learning"}, {"heading": "1. Introduction", "text": "With the advances in data processing, the dimensionality of the data also increases and can be extremely high in many fields such as computer vision, machine learning and image processing. The high dimensionality of the data not only greatly increases the time and storage space required to realize data analysis but also introduces much redundancy and noise which can decrease the accuracy of the ensuing method. Hence, dimensionality reduction is an important and often necessary preprocessing step to facilitate clustering and classification. There are two widely used dimensionality reduction approaches, namely: subspace learning and feature selection. Subspace learning aims to learn a projection which can map the original features into a lower-dimensional subspace by some transformation forming new features [1, 2]. Feature selection aims to select a subset of the features following a certain criterion [3]. Both the two approaches can successfully address the problems we mentioned at the beginning of the discussion [4, 5].\nEmail addresses: nzhouuestc@gmai.com (Nan Zhou), yangyang.xu@uwaterloo.ca (Yangyang Xu), hcheng@uestc.edu.cn (Hong Cheng), JunFang@uestc.edu.cn (Jun Fang), wpedrycz@ualberta.ca (Witold Pedrycz) Preprint submitted to Elsevier October 11, 2017\nar X\niv :1\n50 6.\n01 06\n0v 1\n[ cs\n.L G\n] 2\nJ un\n2 01\nIn general, subspace learning methods can be categorized into two classes: supervised and unsupervised subspace learning, depending on whether the subspace learning process is supplied with class labels. The most classic subspace learning method is Principal Component Analysis (PCA) [6] which considers the global structures of the data. Its goal is to find a set of mutually orthogonal basis function which can capture the directions of maximum variance in the data. The local structure always has more discriminative information for specific tasks for both supervised and unsupervised situation, hence many subspace learning methods are proposed by preserving the local structure of the data by graph embeding. Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10]. However, all these models only consider the local structure or global structure.\nFeature selection methods also can be categorized into supervised and unsupervised ones. The labels of the data contain essential discriminative information which can guide the feature selection process. Hence, the supervised feature selection methods can often efficiently select useful features. Commonly used supervised feature selection methods include Fisher score [11], Pearson correlation coefficient [12], and mutual information [13]. For some data mining tasks, however, we do not have label information, and this calls for unsupervised feature selection. Without label information to guide, unsupervised feature selection methods have to explore the hidden structure in the data. The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].\nThe blessing of dimensionality tells that the high dimensional data always has a lower dimensional structure that inspires us to pursue a small subset of the possibly overwhelmingly large number of features. In recent years, sparsity regularized methods have been widely used in many fields such as computer vision [19, 20], image processing [21], signal recovery [22] and so on. Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27]. It models the feature selection process as a regression problem and imposes the group sparsity on the weight matrix to select the useful features.\nThe paper comes with the following contributions:\n1. We propose a novel unsupervised subspace learning model, which considers both global reconstruction information and local structure information of the data. Because of the group sparsity regularization term, this model is suitable for feature selection tasks. The model is derived by relaxing an existing combinatorial model with its 0-1 variables relaxed to nonnegative ones, and since the solution of the original model is extremely sparse, it further employs a group sparsity regularization term to promote sparsity of the solution.\n2. Due to the combinatorial characteristics of the original model, we, for the first time, propose a greedy algorithm to solve the problem. The relaxed model results in a continuous but nonconvex program. Among various continuous optimization methods, we choose an accelerated block coordinate descent (BCD) method. The BCD method utilizes the bi-convexity structure of the problem and has been found to be efficient for our purposes.\n3. We establish a global convergence result of the BCD method for our problem by assuming the existence of a full rank limit point. Because of the peculiarity of the formulated problem, the result is new and not implied by any existing convergence results of BCD.\n4. We conduct extensive experimental studies. The proposed methods are tested on six realworld datasets coming from different areas and compared to five state-of-the-art unsupervised feature selection algorithms. The results demonstrate the superiority of our methods,\nin particular the second one, over all the other compared methods. In addition, we study the sensitivity of the second method to model parameters and observe that it can perform in a stable manner well within a large range of the parameters.\nThe paper is organized as follows. In Sect. 2, we give a brief review of recent related studies on feature selection. Sect. 3 reviews two local structure preservation methods and presents a local structure preserved sparse subspace learning model. In Sect. 4, we present an algorithms leading to the solution of the problem. Convergence results are also shown. Experimental results are reported in Sect. 5. Finally, we conclude this paper in Sect. 6."}, {"heading": "2. Related Studies", "text": "The well-known subspace learning method is PCA [6], which maximizes the global data structure information in the principal space and hence it is optimal for data reconstruction. Data reconstruction capturing the discriminative information plays a crucial role in pattern recognition [28]. Local structure always contains important discriminative information [29]. Therefore, many subspace learning methods preserve different local structures of the data for different problems and can get better performance than traditional PCA. Many of these methods use the linear extension of graph embeding (LGE) methods to preserve the local structure. With different choices of the graph adjoint matrix, LGE framework will lead to many subspace learning methods. Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8]. LDA is a supervised subspace learning method. It determines the linear projection which can maximally separate the data between different classes and clusters the data in the same class. LPP can be used in both supervised and unsupervised scenarios by adding or without the label information in the graph. It is used to find the projection which can preserve the nearest neighbors information. NPE is suitable for both supervised and unsupervised scenarios. It finds the linear projection which can best preserve the nearest neighbors\u2019 reconstruction property. However, all these locality preservation methods involve dense matrices eigen-decomposition which is very expensive in both time and memory. Considering the drawback, Cai et al. [30] proposed a two-step Spectral Regression (SR) method to transform the eigen-decomposition problem into two steps regression problem which is very efficient and flexible to add the regularization term in the regression step. However, this method only considers the local structure.\nThe blessing of dimensionality tells us the high dimensional data always has the redundancy, hence sparsity as a useful tool to reduce the redundancy has been widely used in many areas, such as computer vision, data processing and machine learning. Considering the redundancy of the subspace learning, Zou et al. [31] proposed an elegant sparse PCA method (SPCA), which transforms the traditional PCA problem into regression problem and uses the \u201cElastic Net\u201d framework to solve `1 regularization problem. However, sometimes, the samples in the subspace learning might be outliers, especially in the problem of background modeling or face recognition. To overcome this problem, Wang et al. [2] proposed a robust elastic net model (REN) which adds the M-estimator in the least square term of the SPCA model and improves the robustness of the SPCA. Both the SPCA and REN only consider the reconstruction information for the learned subspace. Considering the locality preservation, Moghaddam et al. [32] proposed a spectral bounds framework for sparse subspace learning. However, the greedy algorithm they proposed has very high computation complexity in backward elimination step, hence it is not suitable for high dimensional data. Cai et al. [33] proposed a unified sparse subspace learning method\nbased on spectral regression model, which adds the `1 regularization term in the regression step. However, all these sparse subspace learning methods only consider one type of information: local information or global reconstruction information.\nBecause of the connections between subspace learning and feature selection, recently, some feature selection methods that combine the feature selection process with subspace learning have been proposed. Cai et al. [5] combined the sparse subspace learning with feature selection and proposed the Multi-Cluster Feature Selection (MCFS) method. In the feature selection process, the transformation matrix can be seen as the weight matrix for feature selection. However, MCFS method uses the `1 regularization term to control the weight matrix. However, as mentioned by Gu et al. in [4], `1 term is not quite suitable for feature selection, and they used the `2,1 regularization term in the regression step of the subspace learning process to control the weight matrix and improve the feature selection result. However, both of these methods only use the local structure information in subspace learning. Considering the global reconstruction information, Wang et al. [34] proposed an unsupervised feature selection framework, which used the global reconstruction information in subspace learning and used the orthogonality to constrain the weight matrix for a specific feature selection task. However, in general, such a requirement cannot be satisfied since feature weight vectors are not necessarily orthogonal with each other in practice. In addition, it only considers the reconstruction information and does not take local structure of the data into consideration. In practical application, the local structure often contains essential discriminative information as demonstrated in [29]. Instead of enforcing the feature weight matrix to be orthogonal, our model uses a regularization term to encourage row group sparsity, which is more reasonable [4] and in addition, it includes a local structure preserving term to adapt the local structure of the data.\nTo facilitate the presentation of the material, Table 1 contains the notation used in this study."}, {"heading": "3. The Proposed Framework of Local Structure Adaptive Sparse Subspace Learning", "text": "In this section, we introduce our feature selection models that encourage global data fitting and also preserve local structure information of the data. The first model is of combinatorial nature, only allowing 0-1 valued variables. The modelling idea is intuitive, but it is not easy to find a good approximate solution to the problem. The second model relaxes the first one and becomes its continuous counterpart. Various optimization methods can be utilized to determine its solution. More importantly, we find that the relaxed model can most times give better practical performance than the original one; refer the numerical results reported in Section 5."}, {"heading": "3.1. A Generic Formulation", "text": "Given n data samples {pi}ni=1 located in the d-dimensional space, the objective of feature selection is to find a small set of features that can capture most useful information of the data and better serve for classification or clustering purpose. One natural way to measure the information content is to see how close the original data samples are to the learned subspace spanned by the selected features. Mathematically, the distance of a vector x to a subspace X can be represented as \u2016x \u2212 PX(x)\u20162, where PX denotes the projection onto X and \u2016 \u00b7 \u20162 is the Euclidean 2-norm. Hence, the feature selection problem can be described as follows\nmin W,H 1 2 \u2016X \u2212 XWH\u20162F s.t. W \u2208 {0, 1}d\u00d7K , W>1d\u00d71 = 1K\u00d71, \u2016W1K\u00d71\u20160 = K.\n(1)\nwhere X = [p1,p2, . . . ,pn]> \u2208 Rn\u00d7d. Concerning the proposed model, we make a few remarks:\n1. Given W, the optimal H produces the coefficients of all d features projected on the subspace spanned by the selected features. Hence, the (1) expresses the distance of X to the learned subspace. 2. The matrix W is the selection matrix with the entries of \u201c0\u201d or \u201c1\u201d. The constraint W>1d\u00d71 = 1K\u00d71 enforces that each column of W has only one \u201c1\u201d. Therefore, at most K features are selected.\n3. The constraint \u2016W1K\u00d71\u20160 = K guarantees that no feature will be selected more than once, and thus exactly K features will be chosen.\nThe recent work [34] mentions to use the 0-1 feature selection matrix, but it does not explicitly formulate an optimization model like the (1). As shown in [29], a local structure of the data can often include important discriminative information to distinguish different samples. To make the learned subspace preserve local structure, one can add a regularization term to the objective to promote such structural information, namely, to solve the regularized model\nmin W,H 1 2 \u2016X \u2212 XWH\u20162F + \u00b5Loc(W) s.t. W \u2208 {0, 1}d\u00d7K , W>1d\u00d71 = 1K\u00d71, \u2016W1K\u00d71\u2016`0 = K,\n(2)\nwhere Loc(W) is a local structure promoting regularization term, and \u00b5 is a parameter to balance the data fitting and regularization. In the next subsection, we introduce different forms of Loc(W)."}, {"heading": "3.2. Local Structure Preservation Methods", "text": "Local structure of the data often contain important information that can be used to distinguish the samples [5, 7]. A predictor utilizing local structure information can be much more efficient than that only using global information [29]. Therefore, one may anticipate that the learned lower dimensional subspace could preserve local structure of the training data. We briefly review two widely used local structure preservation methods."}, {"heading": "3.2.1. Local Linear Embedding", "text": "The Local Linear Embedding (LLE) [35] method first finds the setNm(p j) of m nearest neighbors for all j and then constructs the similarity matrix S as the (normalized) solution of the following problem\nmin S n\u2211 i=1 \u2016pi \u2212 n\u2211 j=1 S i jp j\u201622,\ns.t. S i j = 0, \u2200 j < Nm(pi), \u2200i. (3)\nOne can regard S i j as the coefficient of the j\u2212th sample when approximating the i\u2212th sample, and the coefficient is zero if the j-th sample is not the neighbor of the i-th one. After obtaining S from (3), LLE further normalizes it such that \u2211n j=1 S i j = 1. Then it computes the lower-dimensional representation Y = W>X> \u2208 RK\u00d7n through solving the problem\nmin W n\u2211 i=1 \u2016W>pi \u2212 n\u2211 j=1 S i jW>p j\u201622. (4)\nNote that if W is a selection matrix, W>p j becomes a lower-dimensional sample, keeping the K selected features by W and removing all other features. Let L = (I \u2212 S )>(I \u2212 S ), where I is the n \u00d7 n identity matrix. Then it is easy to see that (4) can be equivalently expressed as\nmin W\nTr(W>X>LXW). (5)"}, {"heading": "3.2.2. Linear Preserve Projection", "text": "In Linear Preserve Projection [36] (LPP) method, the similarity matrix S is generated by\nS i j = exp( \u2016pi\u2212ph\u2016 2 2 \u22122\u03c32 ) pi \u2208 Nm(p j) or p j \u2208 Nm(pi) 0 otherwise , (6)\nwhere Nm(pi) is the set of m nearest neighbors of pi. The LPP method requires the lowerdimensional representation to preserve the similarity of the original data and forms the linear transformation W by solving the following optimization problem\nmin W n\u2211 i, j=1 S i j\u2016W>pi \u2212W>p j\u201622. (7)\nLet L = D\u2212 S be the Laplacian matrix, where D is a diagonal matrix, called degree matrix, with diagonal elements Dii = \u2211n j=1 S i j, \u2200i. Then (7) can be equivalently expressed as\nmin W\nTr(W>X>LXW). (8)"}, {"heading": "3.3. Relaxed Formulation", "text": "The problem (2) is of combinatorial nature, and we do not have many choices to directly solve it. In the next section, we develop a greedy algorithm, which chooses K features one by one, with each selection decreasing the objective the most among all the remaining features. Although the greedy method can sometimes produce a satisfactory solution, it does not perform reliably. For\nthis reason, we seek an alternative way to select features by first relaxing (2) to a continuous problem and then employing a reliable optimization method to solve the relaxed problem. As observed in our tests, the relaxed method can perform comparably well with and most of times much better than the original one.\nAs remarked at the end of Section 3.1, any feasible solution W is nonnegative and has at most K non-zero rows. If K d (that usually is satisfied), then W has lots of zero rows. Based on these observations, we relax the 0-1 constraint to nonnegativity constraint and the hard constraints WT 1d\u00d71 = 1K\u00d71, \u2016W1K\u00d71\u2016`0 = K to g(W) \u2264 K, where g(W) measures the row-sparsity of W. One choice of g(W) is group Lasso [37], i.e.,\ng(W) = d\u2211\ni=1\n\u2016Wi.\u20162, (9)\nwhere Wi. denotes the i-th row of W. Some other alternatives of g can also be used for promoting row-sparsity, such as group infinity norm used in [38, 39] for multi-class support vector machine. Hence, we relax (2) to read as follows\nmin W,H 1 2 \u2016X \u2212 XWH\u20162F + \u00b5Loc(W)\ns.t. W \u2265 0, g(W) \u2264 K, (10)\nor equivalently\nmin W,H 1 2 \u2016X \u2212 XWH\u20162F + \u00b5Loc(W) + \u03b2g(W)\ns.t. W \u2265 0, (11)\nwhere \u03b2 is a parameter corresponding to K. We focus on (11) only because it is easier to solve than (10). The drawback of using (11) instead of (10) is that we need to pay more attention to choose the parameter \u03b2. However, as shown in Section 5, a wide range of values of \u03b2 can give satisfactory practical performance.\nBefore completing this section, let us make some remarks on the relaxed model. Originally, W is restricted to have exactly K non-zeros, so it should be extremely sparse. One can consider to include a sparsity-promoting term (e.g., `1-norm) in the objective of (11). However, doing so is not necessary since both g(W) and the nonnegativity constraint encourage sparsity of W, and numerically we notice that W output by our algorithm is indeed very sparse. Another point worth mentioning is that the elements of W given by (11) are real numbers and do not automatically select K features. For the purpose of feature selection, after obtaining a solution W, we choose the features corresponding to the K rows of W that have the largest norms because larger values imply more important roles played by the features.\nOur model is similar to the Matrix Factorization Feature Selection (MFFS) model proposed in [34]. The difference is that the MFFS model restricts the matrix W to be orthogonal while we only encourage the sparsity of W. Although orthogonal W makes their model closer to the original model (1), it increases difficulty of solving their problem. In addition, MFFS does not utilize local structure preserving term as we do and thus may lose some important local information. The numerical tests in Sec. 5 demonstrate that the proposed model along with an iterative method can produce better results than those obtained by using the MFFS method."}, {"heading": "4. Solving the Proposed Sparse Subspace Learning", "text": "In this section, we present heuristic algorithms to approximately solve (2) and (11). Throughout the rest of the paper, we assume that Loc(W) takes the function either as (5) or (8) and g(W) is given by (9). Due to the combinatorial structure of (2), we propose a greedy method to solve it. The problem (11) is smooth, and various optimization methods can be applied. Although its objective is nonconvex jointly with respect to W and H, it is convex with regard to one of them while the other one is fixed. Based on this property, we choose the block coordinate descent method to solve (11)."}, {"heading": "4.1. Its Greedy Strategy", "text": "In this subsection, a greedy algorithm is developed for selecting K out of d features based on (2). The idea is as follows: each time, we select one from the remaining unselected features such that the objective value is decreased the most. We begin the design of the algorithm by making the following observation.\nObservation 1. Let I1 and I2 be two index sets of features. Assume I1 \u2286 I2, and XI1 and XI2 are submatrices of X with columns indexed by I1 and I2 respectively. Then\nmin H1 \u2016X \u2212 XI1 H1\u20162F \u2265 minH2 \u2016X \u2212 XI2 H2\u20162F . (12)\nFrom the above observation, if the current index set of selected features is I, the data fitting will become better if we enlarge I by adding more features. Below we describe in details on how to choose such additional features. Assume X is normalized such that\n\u2016x j\u20162 = 1, j = 1, . . . , d, (13)\nwhere x j denotes the j-th column of X. Let I be the current index set of selected features. The optimal H to minH \u2016X \u2212 XIH\u2016F is given by\nH\u2217 = (X>IXI) \u2020X>IX, (14)\nwhere \u2020 denotes the Moore-Penrose pseudoinverse of a matrix. Now consider to add one more feature into I, say the j-th one. Then the lowest data fitting error is\nmin h \u2016X \u2212 XIH\u2217 \u2212 x jh\u20162F\n= min h \u2016h\u20162F \u2212 2\u3008h, x>j (X \u2212 XIH\u2217)\u3009 + \u2016X \u2212 XIH\u2217\u20162F = \u2212 \u2016x>j (X \u2212 XIH\u2217)\u201622 + \u2016X \u2212 XIH\u2217\u20162F ,\nwhere the last equality is achieved at h = x>j (X \u2212 XIH\u2217). Hence, we can choose j such that \u2016x>j (X \u2212 XIH\u2217)\u20162 is the largest among all features not present in I.\nCarrying out a comparison using \u2016x>j (X \u2212 XIH\u2217)\u20162, we find that \u2016x>j (X \u2212 XIH\u2217)\u20161 can serve better. It turns out that the latter is exactly the correlation between x j and the residual X \u2212 XIH\u2217. Denote the correlation between xi and X as\nCor(xi, X) = d\u2211\ns=1\n|x>i xs|.\nAs shown in [40], if Cor(xi, X) is large, then the columns of X can be better linearly represented by xi. To preserve local structure, we need also incorporate Loc(W). If the set of selected features is I, then\nLoc(W) = Tr(W>X>LXW) = \u2211 i\u2208I x>i Lxi.\nAssuming L = D \u2212 S , i.e., using the LPP method in section 3.2.2 (that is used throughout our tests), we have from (13) that\nmin j<I x>j Lx j \u21d4 maxj<I x > j S x j.\nTherefore, we can enlarge I by adding one more feature index j\u2217 such that\nj\u2217 = argmax j<I Cor(x j, X \u2212 XIH\u2217) + x>j S x j,\nwhere H\u2217 is given in (14), and we have set \u00b5 = 1 in (2) for simplicity. Algorithm 1 summarizes our greedy method, and for better balancing the correlation and local structure preserving terms, we normalize both of them in the 5th line of Algorithm 1.\nAlgorithm 1 Greedy Locally Preserved Subspace Learning (GLPSL) 1: Input: Data matrix X \u2208 Rn\u00d7d, the number of features which need to selected K. 2: Output: The index of selected features I \u2286 {1, . . . , d}, |I| = K. 3: Initialize residual R = X, candidate set \u2126 = {1, 2, . . . , d}, selected set I = \u2205. 4: for i = 1 to K do 5: i\u2190 arg maxi\u2208\u2126 Cor(xi,R)\u2211\nj\u2208\u2126 Cor(x j,R) + x>i S xi\u2211 j\u2208\u2126 x>j S xi\n. 6: \u2126\u2190 \u2126\\{i} and I = I \u222a {i}. 7: R\u2190 X \u2212 XI(X>IXI)\u2020X>IX. 8: end for"}, {"heading": "4.2. Its Application on Feature Selection", "text": "In this subsection, we present an alternative method for feature selection based on (11). Utilizing bi-convexity of the objective, we employ the accelerated block coordinate update (BCU) method proposed in [41] to solve (11). As explained in [41], BCU especially fits to solving bi-convex1 optimization problems like (11). Compared to traditional optimization methods such as gradient descent and the interior-point method, BCU can have much lower computational complexity and also maintain fast convergence. In addition, it has a guaranteed global sequence convergence when solving (11).\nFollowing the framework of BCU, our algorithm is derived by alternatingly updating W and H, one at a time while the other one is fixed at its most recent value. Specifically, let\nf (W,H) = 1 2 \u2016X \u2212 XWH\u20162F + \u00b5 2 Tr(W>X>LXW), (15) g\u03b2(W) = \u03b2\u2016W\u20162,1. (16)\n1More precisely, in [41], BCU is proposed to solve multi-convex optimization problems, which includes bi-convex problems as special cases.\nAt the k-th iteration, we perform the following updates:\nWk+1 = argmin W\u22650 \u3008\u2207W f (W\u0302k,Hk),W \u2212 W\u0302k\u3009 + Lkw 2 \u2016W \u2212 W\u0302k\u20162F + g\u03b2(W), (17a) Hk+1 = argmin H f (Wk+1,H), (17b)\nwhere we take Lkw as the Lipschitz constant of \u2207W f (W,Hk) and\nW\u0302k = Wk + \u03c9k(Wk \u2212Wk\u22121) (18)\nis an extrapolated point with weight \u03c9k \u2208 [0, 1], \u2200k. Note that the H-subproblem (17b) can be simply reduced to a linear equation and has the closed-form solution in the form\nHk+1 = [ (Wk+1)>X>X(Wk+1) ]\u2020 (Wk+1)>X>X, (19)\nwhere \u2020 denotes a Moore-Penrose pseudoinverse. If H is restricted to be nonnegative, in general, (17b) does not exhibit a closed-form solution. In this case, one can update H in the same manner as that of W, i.e., completing a block proximal-linearization update. In the following, we discuss in details on how to solve W-subproblem (17a) and the parameter settings."}, {"heading": "4.2.1. Parameter settings", "text": "By direct computation, it is not difficult to see that\n\u2207W f (W,H) = X>(XWH \u2212 X)H> + \u00b5X>LXW.\nFor any W\u0302, W\u0303, we have\n\u2016\u2207W f (W\u0302,H) \u2212 \u2207W f (W\u0303,H)\u2016F = \u2016X>(XW\u0302H \u2212 X)H> + \u00b5X>LXW\u0302 \u2212 X>(XW\u0303H \u2212 X)H> \u2212 \u00b5X>LXW\u0303\u2016F \u2264 \u2016X>(XW\u0302H \u2212 X)H> \u2212 X>(XW\u0303H \u2212 X)H>\u2016F + \u2016\u00b5X>LXW\u0302 \u2212 \u00b5X>LXW\u0303\u2016F = \u2016X>X(W\u0302 \u2212 W\u0303)HH>\u2016F + \u00b5\u2016X>LX(W\u0302 \u2212 W\u0303)\u2016F \u2264 (\u2016X>X\u20162\u2016HH>\u20162 + \u00b5\u2016X>LX\u20162)\u2016W\u0302 \u2212 W\u0303\u2016F ,\nwhere \u2016A\u20162 denotes the spectral norm and equals the largest singular value of A, the first inequality follows from the triangle inequality of norm, and the last inequality is from the fact \u2016AB\u2016F \u2264 \u2016A\u20162\u2016B\u2016F for any matrices A and B of appropriate sizes. Hence, \u2016X>X\u20162\u2016HH>\u20162 + \u00b5\u2016X>LX\u20162 is a Lipschitz constant of \u2207W f (W,H) with respect to W, and in (17a), we set\nLkw = \u2016Hk(Hk)>\u20162\u2016X>X\u20162 + \u00b5\u2016X>LX\u20162. (20)\nAs suggested by [41], we set the extrapolation weight as\n\u03c9k = min \u03c9\u0302k, \u03b4\u03c9 \u221a Lk\u22121w Lkw  , (21)\nwhere \u03b4\u03c9 < 1 is predetermined and \u03c9\u0302k = tk\u22121\u22121tk with\nt0 = 1, tk = 1 2\n( 1 + \u221a 1 + 4t2k\u22121 ) .\nThe weight w\u0302k has been used to accelerate proximal gradient method for convex optimization problem (cf. [42]). It is demonstrated in [43] that the extrapolation weight in (21) can significantly accelerate BCU for nonconvex problems.\nAlgorithm 2 Proximal operator for nonnegative group Lasso: W = Prox-NGL(Y, \u03bb) for i = 1, . . . , d do\nLet y be the i-th row of Y and I the index set of positive components of y Set w to a zero vector if \u2016yI\u20162 > \u03bb then\nLet wI = (\u2016yI\u20162 \u2212 \u03bb) yI\u2016yI\u20162 end if Set the i-th row of W to w\nend for"}, {"heading": "4.2.2. Solution of W-subproblem", "text": "Note that (17a) can be equivalently written as\nmin W\u22650 1 2 \u2225\u2225\u2225\u2225\u2225\u2225W \u2212 ( W\u0302k \u2212 1 Lkw \u2207W f (W\u0302k,Hk) )\u2225\u2225\u2225\u2225\u2225\u22252 F + 1 Lkw g\u03b2(W),\nwhich can be decomposed into d smaller independent problems, each one involving one row of W and coming in the form\nmin x\u22650 1 2 \u2016x \u2212 y\u201622 + \u03bb\u2016x\u20162. (22)\nWe show that (22) has a closed-form solution and thus (17a) can be solved explicitly.\nTheorem 1. Given y, let I = {i : yi > 0} be the index set of positive components of y. Then the solution x\u2217 of (22) is given as follows\n1. For any i < I, x\u2217i = 0; 2. If \u2016yI\u20162 \u2264 \u03bb, then x\u2217I = 0; otherwise, x\u2217I = (\u2016yI\u20162 \u2212 \u03bb) yI \u2016yI\u20162 .\nProof. For i < I, we must have x\u2217i = 0 because if x\u2217i > 0, setting the i-th component to zero and keeping all others unchanged will simultaneously decrease (xi \u2212 yi)2 and \u2016x\u20162. Hence, we can reduce (22) to the following form\nmin xI\u22650 1 2 \u2016xI \u2212 yI\u201622 + \u03bb\u2016xI\u20162. (23)\nWithout nonnegativity constraint on xI, the minimizer of (23) is given by item 2 of Theorem 1 (for example, see [44]). Note that the given x\u2217I is nonnegative. Hence, it solves (23), and this completes the proof.\nThe above proof gives a way to find the solution of (22). Using this method, we can explicitly form the solution of (17a) by the subroutine Prox-NGL in Algorithm 2, where Y \u2208 Rd\u00d7K and \u03bb > 0 are inputs, and W is the output. Arranging the above discusstion together, we have the pseudocode see Algorithm 3 for solving (11).\nAlgorithm 3 Local-Structure Adaptive Sparse Subspace Learning (ASSL) 1: Input: Data matrix X \u2208 Rn\u00d7d, the number of selected features K and parameter \u03b2, \u00b5. 2: Output: Index set of selected features I \u2286 {1, . . . , d} with |I| = K 3: Initialize W0, H0, choose a positive number \u03b4\u03c9 < 1; set k = 1. 4: while Not convergence do 5: Compute Lkw and \u03c9k according to (20) and (21) respectively. 6: Let W\u0302k = Wk + \u03c9k(Wk \u2212Wk\u22121). 7: Update Wk+1 \u2190 Prox-NGL(W\u0302k \u2212 1Lkw\u2207W f (W\u0302\nk,Hk), \u03b2Lkw ). 8: Update Hk+1 \u2190 (19). 9: if F(Wk+1,Hk+1) \u2265 F(Wk,Hk) then\n10: Set W\u0302k = Wk. 11: else 12: Let k \u2190 k + 1. 13: end if 14: end while 15: Normalize each column of W. 16: Sort \u2016Wi.\u20162, i = 1, . . . , d and select features correspondting to the largest K one."}, {"heading": "4.3. Convergence analysis", "text": "In this section, we analyze the convergence of Algorithm ASSL. Let us denote\n\u03b9+(W) = {\n0, if W \u2265 0, +\u221e, otherwise\nbe the indicator function of the nonnegative quadrant. Also, let us denote\nF(W,H) = f (W,H) + g\u03b2(W) + \u03b9+(W).\nThen the problem (11) is equivalent to\nmin W,H F(W,H),\nand the first-order optimality condition is 0 \u2208 \u2202F(W,H). Here, \u2202F denotes the subdifferential of F (see [45] for example) and equals \u2207F if F is differentiable and a set otherwise. By Proposition 2.1 of [46], 0 \u2208 \u2202F(W,H) is equivalent to\n0 \u2208 \u2202W F(W,H), and 0 = \u2207H F(W,H)\nnamely,\n0 \u2208 \u2207W f (W,H) + \u2202g\u03b2(W) + \u2202\u03b9+(W), (24a) 0 = \u2207H f (W,H). (24b)\nWe call (W,H) a critical point of (11) if it satisfies (24). In the following, we first present a subsequence convergence result without any assumption. Assuming existence of a full rank limit point, we further show a global sequence convergence result. The proofs of both results involve much technical material and thus are deferred to the appendix for the reader\u2019s convenience.\nTheorem 2 (Subsequence convergence). Let {(Wk,Hk)}\u221ek=1 be the sequence generated from Algorithm ASSL. Any finite limit point of {(Wk,Hk)}\u221ek=1 is a critical point.\nDue to the coercivity of g(W) and the nonincreasing monotonicity of the objective value, {Wk} must be bounded. However, in general, we cannot guarantee the boundedness of {Hk} because XWk may be rank-degenerate (i.e., not full rank). As shown in the next theorem, if we have rank-nondegeneracy of XWk in the limit, a stronger convergence result can be established.\nTheorem 3 (Global sequence convergence). Let {(Wk,Hk)}\u221ek=1 be the sequence generated from Algorithm ASSL. If there is a finite limit point (W\u0304, H\u0304) such that XW\u0304 is full-rank, then the entire sequence {(Wk,Hk)}\u221ek=1 must converge to (W\u0304, H\u0304)."}, {"heading": "5. Experimental Studies", "text": "In this section, the proposed algorithms ASSL and GLPSL are tested on six benchmark datasets and compared with five state-of-the-art unsupervised feature selection methods."}, {"heading": "5.1. Datasets", "text": "The six benchmark datasets we use come from different areas, and their characteristics are listed in Table 2. Yale64, WarpPIE, Orl64 and Orlraws2 are face images, each sample of the datasets representing a face image. Usps3 is a handwritten digit dataset that contains 9,298 handwritten digit images. Isolet3 is a speech signal dataset containing 30 speakers\u2019 speech signal of alphabet twice. All datasets are normalized such that the vector corresponding to each feature has unit `2-norm."}, {"heading": "5.2. Experimental Settings", "text": "Our algorithms are compared to the following state-of-the-art unsupervised feature selection methods:\n1. LS: Laplacian score (LS) method uses the Laplacian score to evaluate effectiveness of the features. It selects the features individually that retain the samples\u2019 local similarity specified by a similarity matrix. [7].\n2. MCFS: Multi-cluster feature selection (MCFS) is two-step method, and it formulates the feature selection process as a spectral information regression problem with `1-norm regularization term [5].\n2http://featureselection.asu.edu/datasets.php 3http://www.cad.zju.edu.cn/home/dengcai/Data/data.html\n3. UDFS: Unsupervised discriminative feature selection (UDFS) method combines the data\u2019s local discriminative property and the `2,1-norm sparse constraint in one convex model to select the features which have the highest power of local discriminative property [15]. 4. RSR: Regularized self-representation (RSR) feature selection method uses the `2,1-norm to measure the fitting error and also `2,1-norm to promote sparsity [25]. Specifically, it solves the following problem:\nmin W \u2016X \u2212 XW\u20162,1 + \u03b2\u2016W\u20162,1.\n5. MFFS: Matrix factorization feature selection (MFFS) method [34] is similar to ours. It performs the subspace learning and feature selection process simultaneously with the orthogonality constraint on the feature selection matrix W. Specifically, it solves the following problem:\nmin W,H 1 2 \u2016X \u2212 XWH\u20162F\ns.t. W \u2265 0, H \u2265 0, W>W = I. (25)\nThere are some parameters we need to set in advance. The number of selected features is taken from {20, 30, 40, 50, 60, 70, 80, 90, 100} for all datasets. We use the LPP method in section 3.2.2 to preserve local structure of the data in ASSL because both MCFS and LS use the Laplacian graph, and we set the number of nearest neighbors to m = 5 for LS, MCFS, UDFS and ASSL. The parameter m is required by LS, MCFS and ASSL to build a similarity matrix and UDFS to build the local total scatter and between-class scatter matrices. For simplicity, the parameter of local structure preserving term is fixed to be \u00b5 = 1 in ASSL for all the tests discussed in Sections 5.3.1 and 5.3.2. We study the sensitivity of ASSL to \u00b5 in Section 5.3.3. The sparsity parameter for UDFS, RSR, and ASSL is tuned from {0.01, 0.1, 1, 10, 40, 70, 100}. After completing the feature selection process, we use the K-means algorithm [47] to cluster the samples using the selected features. Because the performance of K-means depends on the initial point, we run it 20 times with different random starting points and report the average value.\nThe compared algorithms are evaluated based on their clustering results. For each sample of all datasets, we set its class number as the cluster number. To measure the clustering performance, we use clustering accuracy (ACC) and normalized mutual information (NMI), which are defined below. Let pi and qi be the predicted and true labels of the i\u2212th sample, respectively. The ACC is computed as\nACC = \u2211n\ni=1 \u03b4(qi,map(pi)) n , (26)\nwhere \u03b4(a, b) = 1 if a = b and \u03b4(a, b) = 0 otherwise, and map(\u00b7) is a permutation mapping that maps each predicted label to the equivalent true label. We use the Kuhn-Munkres algorithm [48] to realize such a mapping. High value of ACC indicates the predicted labels are close to the true ones, and thus the higher ACC is, the better the clustering results are. The NMI is used to measure the similarity of two clustering results. For two label vectors P and Q, it is defined as\nNMI(P,Q) = I(P,Q)\n\u221a H(P)H(Q) , (27)\nwhere I(P,Q) is the mutual information of P and Q, H(P) and H(Q) are the entropies of P and Q [49]. In our experiments, P contains the clustering labels using the selected features and Q the true labels of samples in the dataset. Higher value of NMI(P,Q) implies that P better predicts Q."}, {"heading": "5.3. Experimental results", "text": "In this subsection, we report the results of all tested methods. In addition, we study the sensitivity of the parameters present in (11)."}, {"heading": "5.3.1. Performance comparison", "text": "In Tables 3 and 4, we present the ACC and NMI values produced by different methods. For each method, we vary the number of selected features among {20, 30, 40, . . . , 100} and report the best result. From the tables, we see that ASSL performs the best among all the compared methods except for Yale64, Orl64 and WarpPIE in Table 3 and Yale64, Orl64 in Table 4, for each of which ASSL is the second best. In addition, we see that the greedy method GLPSL performs reasonably well in many cases but can be very bad in some cases such as Usps in both Tables, and this validates our reason to relax (2) and develop ASSL method. Finally, we see that ASSL outperforms MFFS for all datasets, and this is possibly due to the local structure preserving term used in ASSL."}, {"heading": "5.3.2. Compare the performance with all features", "text": "To illustrate the effect of feature selection to clustering, we compare the clustering results using all features and selected features given by different methods. Figure 1 plots the ACC value and Figure 2 the NMI value with respect to the number of selected features. The baseline corresponds to the results using all features. From the figures, we see that in most cases, the proposed ASSL method gives the best results, and selecting reasonably many features (but far less than the total number of features), it can give comparable and even better clustering results than those by using all features. Hence, the feature selection eliminates the redundancy of the data for clustering purpose. In addition, note that using fewer features can save the clustering time of the K-means method, and thus feature selection can improve both clustering accuracy and efficiency."}, {"heading": "5.3.3. On sensitivity of parameters", "text": "To further demonstrate the nice performance of the proposed ASSL method, we study its sensitivity with regard to the parameters K, \u00b5 and \u03b2 in (11). First, we fix \u00b5 = 1 and vary K and \u03b2. Figures 3 and 4 plot the ACC and NMI values given by ASSK for different K and \u03b2\u2019s. From the figures, we see that except for Isolet, ASSL performs stably well for different combinations of K and \u03b2, and thus the users can choose the parameters within a large interval to have satisfactory clustering performance. Secondly, we fix \u03b2 = 1 and vary K and \u00b5. Figures 5 and 6 plot the ACC and NMI values given by ASSL for different K and \u00b5\u2019s. Again, we see that ASSL performs stably well except for the Isolet dataset."}, {"heading": "6. Conclusions", "text": "We have proposed a new unsupervised feature selection model, which achieves sparse subspace learning and preserves local structure of the data simultaneously. The model is derived by relaxing an existing combinatorial model with 0-1 variables. A greedy algorithm has been developed, for the first time, to solve the combinatorial problem, and an accelerated block coordinate descent (BCD) method was applied to solve the relaxed continuous task. We have established the global convergence of the BCD method. Extensive numerical tests on real-world data demonstrated that the proposed method outperformed several state-of-the-art unsupervised feature selection methods."}, {"heading": "Acknowledgements", "text": "Y. Xu is partially supported by AFOSR. W. Pedrycz is partially supported by NSERC and CRC."}, {"heading": "Appendix A. Proof of Theorem 2", "text": "For simplicity, we assume \u03c9kW = 0, \u2200k, i.e., there is no extrapolation. The case of \u03c9kW . 0 is more complicated but can be treated similarly with more care taken to handle details; see [41] for example.\nThe following result is well-known (c.f. Lemma 2.1 of [41])\nF(Wk,Hk) \u2212 F(Wk+1,Hk) \u2265 LkW 2 \u2016Wk+1 \u2212Wk\u20162F \u2265 L\u00b5 2 \u2016Wk+1 \u2212Wk\u20162F , (A.1)\nwhere L\u00b5 = \u00b5\u2016X>LX\u20162 > 0. (A.2)\nBy Lemma 3.1 of [50], we have\n1 2 \u2016X \u2212 XWk+1Hk\u20162F \u2212 1 2 \u2016X \u2212 XWk+1Hk+1\u20162F = 1 2 \u2016XWk+1Hk \u2212 XWk+1Hk+1\u20162F (A.3)\nand XWk+1Hk \u2212 XWk+1Hk+1 = Uk+1(Uk+1)>(XWk+1Hk \u2212 X), (A.4)\nwhere Uk+1 contains the left r leading singular vectors of XWk+1 and r is the rank of XWk+1. Note that\nF(Wk+1,Hk) \u2212 F(Wk+1,Hk+1) = 1 2 \u2016X \u2212 XWk+1Hk\u20162F \u2212 1 2 \u2016X \u2212 XWk+1Hk+1\u20162F .\nHence, summing (A.1) and (A.3) over k and noting nonnegativity of F we obtain\n\u221e\u2211 k=0 ( L\u00b5 2 \u2016Wk+1 \u2212Wk\u20162F + 1 2 \u2016XWk+1Hk \u2212 XWk+1Hk+1\u20162F ) \u2264 F(W0,H0),\nand thus\nlim k\u2192\u221e\nWk+1 \u2212Wk = 0. (A.5)\nand\nlim k\u2192\u221e\nUk+1(Uk+1)> ( XWk+1Hk \u2212 X) = lim\nk\u2192\u221e XWk+1Hk \u2212 XWk+1Hk+1 = 0. (A.6)\nCombining the two equalities in (A.6), we have\nlim k\u2192\u221e\nUk(Uk)> ( XWkHk \u2212 X) = 0.\nSince {XWk} is bounded and (XWk)> = (XWk)>Uk(Uk)>, left multiplying (XWk)> in the above equation gives\nlim k\u2192\u221e\n(XWk)> ( XWkHk \u2212 X) = 0. (A.7)\nAssume (W\u0304, H\u0304) is a finite limit point of {(Wk,Hk)}\u221ek=1. Then there exists a subsequence {(Wk,Hk)}k\u2208K convergent to (W\u0304, H\u0304). If necessary, taking another subsequence, we can assume LkW \u2192 L\u0304 for some L\u0304 > 0 as K 3 k \u2192 \u221e. From (A.7), it holds that\n\u2207H f (W\u0304, H\u0304) = (XW\u0304)>(XW\u0304H\u0304 \u2212 X) = 0.\nIn addition, from the update rule of W, we have\nWk+1 = argmin W\u22650 \u3008\u2207W f (Wk,Hk),W \u2212Wk\u3009 + LkW 2 \u2016W \u2212Wk\u20162F + g\u03b2(W).\nLetting K 3 k \u2192 \u221e in the above equation and using (A.5) yield\nW\u0304 = argmin W\u22650 \u3008\u2207W f (W\u0304, H\u0304),W \u2212 W\u0304\u3009 + L\u0304 2 \u2016W \u2212 W\u0304\u20162F + g\u03b2(W),\nwhich implies\n0 \u2208 \u2207W f (W\u0304, H\u0304) + \u2202g\u03b2(W) + \u2202\u03b9+(W\u0304) = \u2202W F(W\u0304, H\u0304).\nTherefore, (W\u0304, H\u0304) is a critical point of (11)."}, {"heading": "Appendix B. Proof of Theorem 3", "text": "For simplicity of notation, we let Zk = (Wk,Hk) and Z\u0304 = (W\u0304, H\u0304). In addition, we assume \u03c9kW = 0,\u2200k as in the proof of Theorem 2. Again, the case of \u03c9kW . 0 can be shown similarly. Let \u03c3min(XW\u0304) > 0 be the smallest singular value of XW\u0304. By the continuity of singular value function and spectral norm of a matrix, there exists \u03b4 > 0 such that\n\u03c3min(XW) \u2265 \u03c3\u0304\n2 , and \u2016XW\u20162 \u2264 2\u2016XW\u0304\u20162, \u2200W \u2208 B(W\u0304, \u03b4), (B.1a)\n\u2016HH>\u20162 \u2264 2\u2016H\u0304H\u0304>\u20162, \u2200H \u2208 B(H\u0304, \u03b4), (B.1b)\nwhere \u03c3min(A) denotes the smallest singular value of matrix A, andB(Z\u0304, \u03b4) := {Z : \u2016Z\u2212Z\u0304\u2016F \u2264 \u03b4}. Since F is a semi-algebraic function, it exhibits the so-called Kurdyka-\u0141ojasiewicz property (c.f. [51]): in a neighborhood B(Z\u0304, \u03c1), there exists \u03c6(s) = cs1\u2212\u03b8 for some c > 0 and 0 \u2264 \u03b8 < 1 such that\n\u03c6\u2032(|F(Z) \u2212 F(Z\u0304)|)dist(0, \u2202F(Z)) \u2265 1, for any Z \u2208 B(Z\u0304, \u03c1) and F(Z) , F(Z\u0304). (B.2)\nLet Fk = F(Zk) \u2212 F(Z\u0304), and \u03c6k = \u03c6(Fk).\nWithout loss of generality, we assume Z0 is sufficiently close to Z\u0304 such that\n2\u2016Z0 \u2212 Z\u0304\u2016F + 3  \u221a\n2F0 L\u00b5 +\n\u221a 8F0\n\u03c32min(XW\u0304)  + C12C2 \u03c60 < \u03c1, (B.3) where L\u00b5 is defined in (A.2), and\nC1 = L\u03b4 + 2\u2016H\u0304H\u0304>\u20162\u2016XX>\u20162 + L\u00b5, (B.4)\nC2 = L\u00b5 2 + \u03c32min(XW\u0304) 8 . (B.5)\nIn the above equation, L\u03b4 is the Lipschitz constant of \u2207W f (W,H) in B(Z\u0304, \u03b4), i.e.,\n\u2016\u2207W f (Z\u0302) \u2212 \u2207W f (Z\u0303)\u2016F \u2264 L\u03b4\u2016Z\u0302 \u2212 Z\u0303\u2016F , \u2200Z\u0302, Z\u0303 \u2208 B(Z\u0304, \u03b4). (B.6)\nNote that L\u03b4 must be finite since f (W,H) is twice continuous differentiable and B(Z\u0304, \u03b4) is bounded. Otherwise if (B.3) does not hold, since Z\u0304 is a limit point of {Zk}, we can take an iterate Zk0 sufficiently close to Z\u0304 and let Zk0 be the new starting point. If neccessary, taking a smaller \u03c1, we assume\n\u03c1 + \u221a 2F0 L\u00b5 \u2264 \u03b4, (B.7)\nwhere \u03b4 is the quantity used in (B.1). From (A.1) and Fk+1 \u2264 Fk \u2264 F(Z\u0304), \u2200k, we have \u2016W1 \u2212W0\u2016F \u2264 \u221a\n2F0 L\u00b5 and thus\n\u2016W1 \u2212 W\u0304\u2016F \u2264 \u2016W1 \u2212W0\u2016F + \u2016W0 \u2212 W\u0304\u2016F \u2264 \u2016W0 \u2212 W\u0304\u2016F + \u221a\n2F0 L\u00b5 < \u03c1 \u2264 \u03b4. (B.8)\nHence, \u03c3min(XW1) \u2265 \u03c3min(XW\u0304)2 from (B.1a), and\nF(W1,H0) \u2212 F(W1,H1) \u2265 \u03c32min(XW 1) 2 \u2016H1 \u2212 H0\u20162F \u2265 \u03c32min(XW\u0304) 8 \u2016H1 \u2212 H0\u20162F ,\nwhich implies \u2016H1 \u2212 H0\u2016F \u2264 \u221a\n8F0 \u03c32min(XW\u0304) . Therefore,\n\u2016H1 \u2212 H\u0304\u2016F \u2264 \u2016H1 \u2212 H0\u2016F + \u2016H0 \u2212 H\u0304\u2016F \u2264 \u2016H0 \u2212 H\u0304\u2016F + \u221a\n8F0 \u03c32min(XW\u0304) . (B.9)\nCombining (B.8) and (B.9), we have \u2016Z1 \u2212 Z\u0304\u2016F \u2264 \u2016W1 \u2212 W\u0304\u2016F + \u2016H1 \u2212 H\u0304\u2016F \u2264 2\u2016Z0 \u2212 W\u0304\u2016F + \u221a\n2F0 L\u00b5 +\n\u221a 8F0\n\u03c32min(XW\u0304) ,\nwhich together with (B.3) implies Z1 \u2208 B(Z\u0304, \u03c1). Assume that for some integer K, Zk \u2208 B(Z\u0304, \u03c1),\u22000 \u2264 k \u2264 K. We go to show ZK+1 \u2208 B(Z\u0304, \u03c1) and thus by induction Zk \u2208 B(Z\u0304, \u03c1), \u2200k. Note that\n0 \u2208 \u2207W f (Wk\u22121,Hk\u22121) + Lk\u22121W (Wk \u2212Wk\u22121) + \u2202g\u03b2(Wk) + \u2202\u03b9+(Wk), 0 = \u2207H f (Wk,Hk).\nHence,\ndist(0, \u2202F(Zk)) \u2264\u2016\u2207W f (Wk,Hk) \u2212 \u2207W f (Wk\u22121,Hk\u22121)\u2016F + Lk\u22121W \u2016Wk \u2212Wk\u22121\u2016F \u2264C1\u2016Zk \u2212 Zk\u22121\u2016F , (B.10)\nwhere C1 is defined in (B.4). We have\n\u03c6k \u2212 \u03c6k+1 \u2265\u03c6\u2032(Fk)(Fk \u2212 Fk+1) ( from concavity of \u03c6) \u2265 Fk \u2212 Fk+1 C1\u2016Zk \u2212 Zk\u22121\u2016F ( from KL property (B.2)) \u2265 C2\u2016Zk+1 \u2212 Zk\u20162F C1\u2016Zk \u2212 Zk\u22121\u2016F , (B.11)\nwhere the last inequality follows from (B.5), (A.1) and\nF(Wk+1,Hk) \u2212 F(Wk+1,Hk+1) \u2265 \u03c32min(XW\u0304)\n8 \u2016Hk \u2212 Hk+1\u20162F .\nTransforming (B.11) gives\nC2\u2016Zk+1 \u2212 Zk\u20162F \u2264 C1\u2016Zk \u2212 Zk\u22121\u2016F(\u03c6k \u2212 \u03c6k+1)\n\u21d2 \u221a C2\u2016Zk+1 \u2212 Zk\u2016F \u2264 \u221a C1\u2016Zk \u2212 Zk\u22121\u2016F(\u03c6k \u2212 \u03c6k+1)\n\u21d2 \u221a C2\u2016Zk+1 \u2212 Zk\u2016F \u2264 \u221a\nC2 2 \u2016Zk \u2212 Zk\u22121\u2016F + C1 2 \u221a C2 (\u03c6k \u2212 \u03c6k+1).\nSumming the above inequality over k and arranging terms give\nK\u2211 k=1 \u2016Zk+1 \u2212 Zk\u2016F \u2264 \u2016Z1 \u2212 Z0\u2016F + C1 2C2 (\u03c61 \u2212 \u03c6K+1). (B.12)\nHence,\n\u2016ZK+1 \u2212 Z\u0304\u2016F \u2264 K\u2211\nk=1\n\u2016Zk+1 \u2212 Zk\u2016F + \u2016Z1 \u2212 Z\u0304\u2016F\n\u2264\u2016Z1 \u2212 Z\u0304\u2016F + \u2016Z1 \u2212 Z0\u2016F + C1 2C2 \u03c60 \u22642\u2016Z1 \u2212 Z0\u2016F + \u2016Z0 \u2212 Z\u0304\u2016F + C1 2C2 \u03c60\n(from (B.8) and (B.9)) \u2264 \u03c1, (B.13)\nwhich indicates ZK+1 \u2208 B(Z\u0304, \u03c1). By induction, we have Zk \u2208 B(Z\u0304, \u03c1), \u2200k, and thus (B.12) holds for all K. Therefore, {Zk}\u221ek=1 is a Cauchy sequence and converges. Since Z\u0304 is a limit point, it must hold that limk\u2192\u221e Zk = Z\u0304. This completes the proof.\nReference"}], "references": [{"title": "Feature extraction: foundations and applications", "author": ["I. Guyon"], "venue": "Vol. 207, Springer Science & Business Media", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A robust elastic net approach for feature learning", "author": ["L. Wang", "H. Cheng", "Z. Liu", "C. Zhu"], "venue": "Journal of Visual Communication and Image Representation 25 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research 3 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint feature selection and subspace learning", "author": ["Q. Gu", "Z. Li", "J. Han"], "venue": "in: IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Neighborhood preserving embedding", "author": ["X. He", "D. Cai", "S. Yan", "H.-J. Zhang"], "venue": "in: Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, Vol. 2, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Pattern Recognition Letters 26 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparsity preserving projections with applications to face recognition", "author": ["L. Qiao", "S. Chen", "X. Tan"], "venue": "Pattern Recognition 43 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Thirteen ways to look at the correlation coefficient", "author": ["J. Lee Rodgers", "W.A. Nicewander"], "venue": "The American Statistician 42 (1) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1988}, {"title": "Feature selection based on mutual information criteria of max-dependency", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "maxrelevance, and min-redundancy, Pattern Analysis and Machine Intelligence, IEEE Transactions on 27 (8) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised feature selection using feature similarity", "author": ["P. Mitra", "C. Murthy", "S.K. Pal"], "venue": "IEEE transactions on pattern analysis and machine intelligence 24 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "l2", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "1-norm regularized discriminative feature selection for unsupervised learning, in: IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22, Citeseer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou"], "venue": "Lu, Unsupervised feature selection using nonnegative spectral analysis., in: AAAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Global and local structure preservation for feature selection", "author": ["X. Liu", "L. Wang", "J. Zhang", "J. Yin", "H. Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on 25 (6) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "On similarity preserving feature selection", "author": ["Z. Zhao", "L. Wang", "H. Liu", "J. Ye"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 25 (3) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparsity induced similarity measure for label propagation", "author": ["H. Cheng", "Z. Liu", "J. Yang"], "venue": "in: Computer Vision, 2009 IEEE 12th International Conference on, IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Pattern-coupled sparse bayesian learning for recovery of block-sparse signals", "author": ["J. Fang", "Y. Shen", "H. Li", "P. Wang"], "venue": "IEEE Trans. Signal Processing 63 (2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in neural information processing systems 19 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Visual classification with multitask joint sparse representation", "author": ["X.-T. Yuan", "X. Liu", "S. Yan"], "venue": "Image Processing, IEEE Transactions on 21 (10) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature selection by regularized self-representation", "author": ["P. Zhu", "W. Zuo", "L. Zhang", "Q. Hu", "S.C. Shiu"], "venue": "Pattern Recognition 48 (2) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "X", "author": ["Y. Yang", "H.T. Shen", "F. Nie", "R. Ji"], "venue": "Zhou, Nonnegative spectral clustering with discriminative regularization., in: AAAI", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "in: Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, AAAI Press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear subspace learning-based dimensionality reduction", "author": ["X. Jiang"], "venue": "Signal Processing Magazine, IEEE 28 (2) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Local learning algorithms", "author": ["L. Bottou", "V. Vapnik"], "venue": "Neural computation 4 (6) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1992}, {"title": "Spectral regression for efficient regularized subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "in: Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics 15 (2) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "in: Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Subspace learning for unsupervised feature selection via matrix factorization", "author": ["S. Wang", "W. Pedrycz", "Q. Zhu", "W. Zhu"], "venue": "Pattern Recognition 48 (1) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290 (5500) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2000}, {"title": "Locality preserving projections", "author": ["X. Niyogi"], "venue": "in: Neural information processing systems, Vol. 16, MIT", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Variable selection for the multicategory svm via adaptive sup-norm regularization", "author": ["H.H. Zhang", "Y. Liu", "Y. Wu", "J. Zhu"], "venue": "Electronic Journal of Statistics 2 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "Information Theory, IEEE Transactions on 53 (12) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM Journal on imaging sciences 6 (3) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences 2 (1) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in optimization 1 (3) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational analysis", "author": ["R.T. Rockafellar", "R.J.-B. Wets"], "venue": "Vol. 317, Springer Science & Business Media", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the kurdyka-lojasiewicz inequality", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Mathematics of Operations Research 35 (2) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithm as 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied statistics ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1979}, {"title": "Matching theory", "author": ["L. Lov\u00e1sz", "M. Plummer"], "venue": "Vol. 367, American Mathematical Soc.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": "Springer Science & Business Media", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "The lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems", "author": ["J. Bolte", "A. Daniilidis", "A. Lewis"], "venue": "SIAM Journal on Optimization 17 (4) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Subspace learning aims to learn a projection which can map the original features into a lower-dimensional subspace by some transformation forming new features [1, 2].", "startOffset": 159, "endOffset": 165}, {"referenceID": 1, "context": "Subspace learning aims to learn a projection which can map the original features into a lower-dimensional subspace by some transformation forming new features [1, 2].", "startOffset": 159, "endOffset": 165}, {"referenceID": 2, "context": "Feature selection aims to select a subset of the features following a certain criterion [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "Both the two approaches can successfully address the problems we mentioned at the beginning of the discussion [4, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 4, "context": "Both the two approaches can successfully address the problems we mentioned at the beginning of the discussion [4, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "The most classic subspace learning method is Principal Component Analysis (PCA) [6] which considers the global structures of the data.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "Commonly used supervised feature selection methods include Fisher score [11], Pearson correlation coefficient [12], and mutual information [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "Commonly used supervised feature selection methods include Fisher score [11], Pearson correlation coefficient [12], and mutual information [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 83, "endOffset": 90}, {"referenceID": 6, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 108, "endOffset": 116}, {"referenceID": 14, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 108, "endOffset": 116}, {"referenceID": 15, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 139, "endOffset": 147}, {"referenceID": 16, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 139, "endOffset": 147}, {"referenceID": 17, "context": "In recent years, sparsity regularized methods have been widely used in many fields such as computer vision [19, 20], image processing [21], signal recovery [22] and so on.", "startOffset": 107, "endOffset": 115}, {"referenceID": 18, "context": "In recent years, sparsity regularized methods have been widely used in many fields such as computer vision [19, 20], image processing [21], signal recovery [22] and so on.", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 22, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 23, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 5, "context": "The well-known subspace learning method is PCA [6], which maximizes the global data structure information in the principal space and hence it is optimal for data reconstruction.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "Data reconstruction capturing the discriminative information plays a crucial role in pattern recognition [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Local structure always contains important discriminative information [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 26, "context": "[30] proposed a two-step Spectral Regression (SR) method to transform the eigen-decomposition problem into two steps regression problem which is very efficient and flexible to add the regularization term in the regression step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed an elegant sparse PCA method (SPCA), which transforms the traditional PCA problem into regression problem and uses the \u201cElastic Net\u201d framework to solve `1 regularization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed a robust elastic net model (REN) which adds the M-estimator in the least square term of the SPCA model and improves the robustness of the SPCA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[32] proposed a spectral bounds framework for sparse subspace learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[33] proposed a unified sparse subspace learning method 3", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] combined the sparse subspace learning with feature selection and proposed the Multi-Cluster Feature Selection (MCFS) method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "in [4], `1 term is not quite suitable for feature selection, and they used the `2,1 regularization term in the regression step of the subspace learning process to control the weight matrix and improve the feature selection result.", "startOffset": 3, "endOffset": 6}, {"referenceID": 30, "context": "[34] proposed an unsupervised feature selection framework, which used the global reconstruction information in subspace learning and used the orthogonality to constrain the weight matrix for a specific feature selection task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In practical application, the local structure often contains essential discriminative information as demonstrated in [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "Instead of enforcing the feature weight matrix to be orthogonal, our model uses a regularization term to encourage row group sparsity, which is more reasonable [4] and in addition, it includes a local structure preserving term to adapt the local structure of the data.", "startOffset": 160, "endOffset": 163}, {"referenceID": 30, "context": "The recent work [34] mentions to use the 0-1 feature selection matrix, but it does not explicitly formulate an optimization model like the (1).", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "As shown in [29], a local structure of the data can often include important discriminative information to distinguish different samples.", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "Local structure of the data often contain important information that can be used to distinguish the samples [5, 7].", "startOffset": 108, "endOffset": 114}, {"referenceID": 6, "context": "Local structure of the data often contain important information that can be used to distinguish the samples [5, 7].", "startOffset": 108, "endOffset": 114}, {"referenceID": 25, "context": "A predictor utilizing local structure information can be much more efficient than that only using global information [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "Local Linear Embedding The Local Linear Embedding (LLE) [35] method first finds the setNm(p j) of m nearest neighbors for all j and then constructs the similarity matrix S as the (normalized) solution of the following problem", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "Linear Preserve Projection In Linear Preserve Projection [36] (LPP) method, the similarity matrix S is generated by", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "One choice of g(W) is group Lasso [37], i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 34, "context": "Some other alternatives of g can also be used for promoting row-sparsity, such as group infinity norm used in [38, 39] for multi-class support vector machine.", "startOffset": 110, "endOffset": 118}, {"referenceID": 30, "context": "Our model is similar to the Matrix Factorization Feature Selection (MFFS) model proposed in [34].", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "As shown in [40], if Cor(xi, X) is large, then the columns of X can be better linearly represented by xi.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "Utilizing bi-convexity of the objective, we employ the accelerated block coordinate update (BCU) method proposed in [41] to solve (11).", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "As explained in [41], BCU especially fits to solving bi-convex1 optimization problems like (11).", "startOffset": 16, "endOffset": 20}, {"referenceID": 36, "context": "1More precisely, in [41], BCU is proposed to solve multi-convex optimization problems, which includes bi-convex problems as special cases.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "is an extrapolated point with weight \u03c9k \u2208 [0, 1], \u2200k.", "startOffset": 42, "endOffset": 48}, {"referenceID": 36, "context": "As suggested by [41], we set the extrapolation weight as", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "[42]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Without nonnegativity constraint on xI, the minimizer of (23) is given by item 2 of Theorem 1 (for example, see [44]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 39, "context": "Here, \u2202F denotes the subdifferential of F (see [45] for example) and equals \u2207F if F is differentiable and a set otherwise.", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "1 of [46], 0 \u2208 \u2202F(W,H) is equivalent to 0 \u2208 \u2202W F(W,H), and 0 = \u2207H F(W,H)", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "MCFS: Multi-cluster feature selection (MCFS) is two-step method, and it formulates the feature selection process as a spectral information regression problem with `1-norm regularization term [5].", "startOffset": 191, "endOffset": 194}, {"referenceID": 13, "context": "UDFS: Unsupervised discriminative feature selection (UDFS) method combines the data\u2019s local discriminative property and the `2,1-norm sparse constraint in one convex model to select the features which have the highest power of local discriminative property [15].", "startOffset": 257, "endOffset": 261}, {"referenceID": 21, "context": "RSR: Regularized self-representation (RSR) feature selection method uses the `2,1-norm to measure the fitting error and also `2,1-norm to promote sparsity [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 30, "context": "MFFS: Matrix factorization feature selection (MFFS) method [34] is similar to ours.", "startOffset": 59, "endOffset": 63}, {"referenceID": 41, "context": "After completing the feature selection process, we use the K-means algorithm [47] to cluster the samples using the selected features.", "startOffset": 77, "endOffset": 81}, {"referenceID": 42, "context": "We use the Kuhn-Munkres algorithm [48] to realize such a mapping.", "startOffset": 34, "endOffset": 38}, {"referenceID": 43, "context": "where I(P,Q) is the mutual information of P and Q, H(P) and H(Q) are the entropies of P and Q [49].", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "0 is more complicated but can be treated similarly with more care taken to handle details; see [41] for example.", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "1 of [41])", "startOffset": 5, "endOffset": 9}, {"referenceID": 44, "context": "[51]): in a neighborhood B(Z\u0304, \u03c1), there exists \u03c6(s) = cs1\u2212\u03b8 for some c > 0 and 0 \u2264 \u03b8 < 1 such that", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Subspace learning is becoming more and more popular thanks to its capabilities of good interpretation. However, existing approaches do not adapt both local structure and self reconstruction very well. We propose local-structure adaptive sparse subspace learning (ASSL) model for unsupervised feature selection. In this paper, we formulate the feature selection process as a subspace learning problem and incorporate a regularization term to preserve the local structure of the data. Furthermore, we develop a greedy algorithm to establish the basic model and an iterative strategy based on an accelerated block coordinate descent is used to solve the local-structure ASSL problem. We also provide the global convergence analysis of the proposed ASSL algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches.", "creator": "LaTeX with hyperref package"}}}