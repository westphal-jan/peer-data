{"id": "1506.01163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "abstract": "in this paper we formally propose the structured deep neural matrix network ( structured dnn ) as a structured and deep learning algorithm, learning to find the necessarily best structured object ( such as a label sequence ) given a structured input ( such explicitly as a vector sequence ) represented by globally considering the mapping relationships between the structure rather than modeling item by item.", "histories": [["v1", "Wed, 3 Jun 2015 08:41:05 GMT  (276kb,D)", "http://arxiv.org/abs/1506.01163v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yi-hsiu liao", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1506.01163"}, "pdf": {"name": "1506.01163.pdf", "metadata": {"source": "CRF", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "authors": ["Yi-Hsiu Liao", "Hung-Yi Lee", "Lin-shan Lee"], "emails": ["r03921048@ntu.edu.tw,", "hungyilee@ntu.edu.tw,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "work (Structured DNN) as a structured and deep learning algorithm, learning to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structure rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learned utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning algorithm. It was shown to beat structured SVM in preliminary experiments on TIMIT. Index Terms: speech recognition, structured learning, deep neural network, structured deep neural network."}, {"heading": "1. Introduction", "text": "Hidden Markov Models (HMMs [?]) have been the most successful approach for automatic speech recognition for long [?, ?]. With the maturity of machine learning, great efforts have been made to try to integrate more machine learning concepts into the HMM framework [?, ?] because HMMs are generative, while many machine learning approaches can be discriminative in addition [?, ?]. Using Deep Neural Networks (DNN) [?, ?] with HMM is a good example [?, ?, ?]. In general, HMMs consider the phoneme structure by states and the transitions among them, but trained primarily on frame level regardless of being based on DNN [?, ?] or Gaussian Mixture Model(or SGMM [?]). Under HMM framework [?], the hierarchical structure of an utterance is taken care of by the HMM and their states, the lexicon and the language model, which are respectively learned separetely from disjoint sets of knowlage sources. On the other hand, it is well known that there may exist some underlying overall structures for the utterances behind the signals which may be helpful to recognition. If we can learn such structures comprehensively from the signals of the entire utterance globally, the recognition scenario may be different.\nOn the other hand, structured learning has been substantially investigated in machine learning, which tries to learn the complicated structures exhibited by the data. Conditional Random Fields (CRF) [?, ?, ?, ?, ?, ?] and structured Support Vector Machine (SVM) [?, ?, ?] are good example approaches. Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from\ndifferent sets of knowledge sources [?], utilizing the nice properties of SVM [?] to classify the structured patterns of the utterance with maximized margin. However, both CRF and structured SVM are linear, therefore limited in analyzing speech signals. Another research [?] integrates DNN into structured learning but mainly based on Weighted Finite-State Transducers (WFST).\nIn this paper, we extend the above structured SVM approach to phoneme recognition using a structured DNN including nonlinear units in multi-layers, but similarly learning the global mapping relationships from an acoustic vector sequence to a phoneme label sequence for a whole utterance. Therefore, it is a Structured Deep Neural Network(Structured DNN).\nThe rest of the paper is organized as follows: Section 2 is the overall system architecture, Section 3 introduces structured feature vector, Section 4 describes details about experiment setup, Section 5 shows the experiment results, and Section 6 is conclusion and future work."}, {"heading": "2. Proposed Approach \u2013 Structured Deep Neural Network", "text": "The whole picture of the concept of the structured DNN for phoneme recognition is in Fig. 1. Given an utterance with an acoustic vector sequence x and a corresponding phoneme label sequence y, we can first obtain a structured feature vector \u03a8(x,y) representing x and y and the relationships between them as in Fig. 1(a) (details of \u03a8(x,y) are given in Section 3), and then feed it into either an SVM as in Fig. 1(b) or a DNN as in Fig. 1(c) to get a score by a scoring function F1(x,y; \u03b81) or F2(x,y; \u03b82), where \u03b81 and \u03b82 are the parameter sets for the SVM and DNN respectively. Because both x and y represent the entire utterance by a structure (sequence) and either SVM or DNN learns to map the pair of (x,y) to a score on the utterance level globally rather than on the frame level, this is structured learning optimized on the utterance level."}, {"heading": "2.1. Structured Learning Concepts", "text": "In structured learning, both the desired outputs yi and the input objects xi can be sequences, trees, lattices, or graphs, rather than simply classes or real numbers. In the context of supervised learning for phoneme recognition for utterances, we are given a set of training utterances, (x1,y1), ..., (xN ,yN ) \u2208 X \u00d7 Y , where xi is the acoustic vector sequence of the ith utterance, yi the corresponding reference phoneme label sequence, and we wish to assign correct phoneme label sequences to unknown utterance.\nWe first define a function f(x; \u03b8) = y : X \u2192 Y , mapping each acoustic vector sequence x to a phoneme label sequence y, where \u03b8 is the parameter set be learned. One way to achieve this is to assign every possible phoneme label sequence\nar X\niv :1\n50 6.\n01 16\n3v 1\n[ cs\n.L G\n] 3\nJ un\n2 01\n5\ny given an acoustic vector sequence x a score by a scoring function F (x,y; \u03b8) : X \u00d7 Y \u2192 R, and take the phoneme label sequence y giving the highest score as the output of f(x; \u03b8),\nf(x; \u03b8) = arg max y\u2208Y F (x,y; \u03b8). (1)"}, {"heading": "2.2. Structured SVM", "text": "Base on the maximized margin concept of SVM, we wish to maximize not only the score of the correct label sequence, but the margin between the score of the correct label sequence and those of the nearest incorrect label sequences, and required the scoring function F (x,y; \u03b81) to be linear,\nF1(x,y; \u03b81) = \u3008\u03b81,\u03a8(x,y)\u3009 , (2)\nwhere \u03a8(x,y) is the structured feature vector mentioned above and shown in Figure 1, representing the structured relationship between x and y , \u03b81 is in vector form and \u3008\u00b7, \u00b7\u3009 represents inner product. We can then train the parameter vector \u03b81 using training instances {(xi,yi), i = 1, 2, ...,L}, and then classify the desired label y for the acoustic vector sequence x of any unknown testing utterance using the scoring function F1(x,y; \u03b81) with the trained parameter set \u03b81. This problem can be solved with the well known SVM [?], and is referred to as structured SVM as in Fig. 1(b)."}, {"heading": "2.3. Structured Deep Neural Network (Structured DNN)", "text": "The assumption of the linear scoring function as in (2) made structured SVM limited. Instead, the proposed structured DNN uses a series of nonlinear transforms to build the scoring function F2(x,y; \u03b82) with L hidden layers to evaluate a single output value F2(x,y; \u03b82) as in Fig. 1(c).\nh1 = \u03c3(W0 \u00b7\u03a8(x,y)) hl = \u03c3(Wl\u22121 \u00b7 hl\u22121), 2 \u2264 l \u2264 L\nF2(x,y; \u03b82) = \u03c3(WL \u00b7 hL), (3)\nwhere Wi is weight matrix (including the bias) of layer i, \u03c3(\u00b7) a nonlinear transform (sigmoid is used), hi the output vector of hidden layer i, and the set of all DNN parameters (W0, W1, W2,..., WL) is \u03b82. Note that the last weight matrix WL is a vector, because this DNN gives only a single value as output.\nFor an utterance with acoustic vector sequence x = (x1,x2, ...,xM ), where xj is the j-th acoustic vector, there can be many possible phoneme label sequences y = (y1, y2, ..., yM ), where yj is the phoneme label for xj , and a reference phoneme label sequence t = (t1, t2, ..., tM ), where tj is the true phoneme label for xj . The label accuracy function Cx(t,y) for the utterance x can then be calculated as follows,\nCx(t,y) = 1\nM M\u2211 j=1 \u03b4(tj , yj), (4)\nwhere \u03b4(tj , yj) is 1 if tj = yj , 0 otherwise. Cx(t,y) in (4) is actually the frame accuracy or one minus the frame error rate. When we are more interested in minimizing the phone error rate, the definition of the label accuracy function Cx(x,y) in (4) can be modified to reflect that goal. In both cases, the parameter set \u03b82 of this DNN is trained by minimizing the following loss function,\nL(\u03b82) = \u2212 \u2211 x Cx(t,y) logF2(x,y; \u03b82), (5)\nwhere L(\u03b82) in (5) is summed over all training utterances, and this objective function is defined in a way similar to the cross entropy popularly used in DNN training."}, {"heading": "2.4. Inference with Structured DNN", "text": "With the structured DNN trained as above, given the acoustic vector sequence x of an unknown utterance, we need to find the best phoneme label sequence y for it. For structured SVM in subsection 2.2, due to the linear assumption, the learned model parameter \u03b81 contains enough information to execute the Viterbi algorithm to find the best label sequence. This is not true for structured DNN. From (1), in principle we need to search over all possible phoneme label sequences(KM forK phonemes and M acoustic vectors) for the given acoustic vector sequence and pick the one giving the highest score, which is computationally infeasible.\nInstead of searching through all possible phoneme label sequences, we can start from a random label sequence, and then change one phoneme label at a time by going through all phoneme labels with all other phoneme labels in the utterance fixed. We iterate the overall phoneme label sequences in this way until the sequence converges. This referred to as \u201dwithout lattice\u201d. We can also decode using WFST first to generate a lattice, and then, choose the phone label sequence from the lattice which give the highest score. Of course in this way, the performance is bounded by the quality of the lattice. This is referred to as \u201dwith lattice\u201d."}, {"heading": "2.5. Training of Structured DNN", "text": "For each training utterance, again we have KM possible label sequences. It is also impossible to train over all these label sequences for the training utterances. In structured SVM, there is a large margin training algorithm which finds training examples to produce the maximum margin. For structured DNN here, how to find and choose effective training examples is important. Besides the positive examples (reference phoneme label sequences for the training utterances), in this work negative examples (those other than reference label sequences) are chosen both by random and by inferencing using the current model. The latter is explained below.\nThe \u201dinferenced label sequences\u201d represent a feedback mechanism. When the current structured DNN model is used to decode a training utterance and obtain a phoneme label sequence, which is far from correct, we add this label sequence to help training data to adjust the model. Wtih the training data generated, standard backpropagation algorithm can be used to update the structured DNN parameters, and additional training sequences can be regenerated in each epoch. Because inference can be performed with or without lattice, same for training. For training with lattice, we choose N-best paths and N random paths from lattice as the inference label sequences."}, {"heading": "3. Structured Feature Vector \u03a8(x,y)or an utterance", "text": "Take the MFCC vector or phoneme posteriorgram vectors as the acoustic vectors for an utterance of M frames, x = {xj , j = 1, 2, ...M}, and the phoneme label for xj is yj . So the task is to decode x into the label sequence y = {yj , j = 1, 2, ...M}. Since the most successful and well known solution to this problem is with HMM, we try to encode what HMM has been doing into the feature vector \u03a8(x,y) to be used here. An HMM consists of a series of states, and two most important sets of parameters \u2013 the transition probabilities between states, and the observation probability distribution for each state. Such a structure is slightly complicated for the work here, so in the preliminary work we use a simplified HMM with only one state for each phoneme. With this simplification, these two sets of probabilistic parameters can be estimated for each utterance by adding up all the counts of the transition between labels (or states) and also adding up all the acoustic vectors for each label (phoneme or state), then normalizing the results with the length of the utterance. This is shown in Fig. 2(a).\nAssume K is the total number of different phonemes, we first define a K dimensional vector \u039b(yj) for yj with its k-th component being 1 and all other components being 0 if yj is the k-th phoneme. Tensor product \u2297 is helpful here, which is defined as\n\u2297 : RP \u00d7 RQ \u2192 RPQ, (a\u2297 b)i+(j\u22121)P \u2261 ai \u00d7 bj , (6)\nwhere a and b are two ordinary vectors with dimensions P and Q respectively. The right half of (6) says a \u2297 b is a vector of dimension PQ, whose [i+ (j \u2212 1)P ]-th component is the i-th component of a multiplied by the j-th component of b. With this expression, the feature vector \u03a8(x,y) in Fig. 1(a) to be used for evaluating the scoring function F1(x,y; \u03b81) in (2) or F2(x,y; \u03b82) in (3) can then be configured as the concatenation of two vectors,\n\u03a8(x,y) = 1\nM\n( \u2211M j=1 x\nj \u2297 \u039b(yj)\u2211M\u22121 j=1 \u039b(y j)\u2297 \u039b(yj+1)\n) , (7)\nwhere x = {x1,x2, ...,xM} and y = {y1, y2, ..., yM}. The upper half of the right hand side of (7) is to accumulate the distribution of all components of xj for each phoneme in the acoustic vector sequence x, and then locate them at different sections of components of the feature vector \u03a8(x,y) (corresponding to the observation probability distribution for each state or phoneme label estimated with the utterance). The lower half of the right hand side of (7), on the other hand, is to accumulate the transition counts between each pair of labels (phonemes or states) in the label sequence y (corresponding to state transition probabilities estimated for the utterance). After normalizing with the utterance length M (which is also helpful to give a good range for input of DNN) \u03a8(x,y) is then the concatenation of the two, so it keeps the primary statistical parameters of xj for different phonemes yj for all xj in x, and the transitions between states for all yj in y. With enough training utterances (x,y) and the corresponding function \u03a8(x,y), we can then learn the scoring function F1(x,y; \u03b8) or F2(x,y; \u03b82) by training the parameters \u03b81 or \u03b82. The vector \u03a8(x,y) in (7) can be easily extended to higher order Markov assumptions (transition to the next state depending on more than one previous states). For example, by replacing the upper half of (7) with \u2211N n=1 x\nn \u2297 \u039b(yn)\u2297 \u039b(yn+1) and the lower half of (7) with \u2211N\u22121 n=1 \u039b(y\nn)\u2297 \u039b(yn+1)\u2297 \u039b(yn+2) , we have the second order Markov assumption.\nConsider a simplified example for K = 3 (only 3 allowed phonemes A, B, C) and an utterance with length M = 4 as shown in Fig. 2(b). It is then easy to find that the upper half of \u03a8(x,y) is \u22114 n=1 x\nn \u2297 \u039b(yn) = (1.2, 2.6, 2.7, 2.3, 1.5, 2.5)\u2032, and the lower half of \u03a8(x,y) is \u22113 n=1 \u039b(y\nn)\u2297 \u039b(yn+1) = (0, 1, 0, 0, 1, 1, 0, 0, 0)\u2032. We therefore have \u03a8(x,y) = 1\n4 \u00b7\n(1.2, 2.6, 2.7, 2.3, 1.5, 2.5, 0, 1, 0, 0, 1, 1, 0, 0, 0)\u2032 ."}, {"heading": "4. Experimental Setup", "text": "Initial experiments were performed with TIMIT. We used the training set without dialect sentences for training and the core testing set (with 24 speakers and no dialect) for testing. The models were trained with a set of 48 phonemes and tested with a set of 39 phonemes, conformed to CMU/MIT standards [?]. we used an online library [?] for structured SVM, and modified the kaldi [?] code to implement structured DNN.\nOur experiment is based on Karel\u2019s recipe in kaldi for TIMIT script, which used LDA-MLLT-fMLLR features obtained from\nauxiliary GMM models, as RBM pre-training, frame crossentropy training and sMBR. On top of Karel\u2019s recipe, we used three sets of acoustic vectors, (a)LDA-MLLT-fMLLR feature (40 dimensions), or input to DNN in Karel\u2019s recipe; (b)phoneme posterior probability (48 dimensions) obtained from the 1943 DNN output (state posterior) from Karel\u2019s recipe by reducing the dimension to mono-phoneme size of 48 with an additional layer of DNN (1943\u00d748); and (c) phoneme posterior probability from filter bank(48 dimensions) obtained by a DNN (4-layer of 512 neurons) with filter bank input of context 4-1-4. They are respectively referred to as acoustic vectors (a)(b)(c) below."}, {"heading": "5. Experimental Results", "text": "The results are listed in Table 1. The structured DNN without lattice (3 hidden layers 200 neurons per layer) did not perform better than structured SVM, although it did learn some structured patterns, obviously because of the poor quality of the training data (most of them are random), as well as the fact that the inference algorithm of changing only one phoneme label at a time is actually prone to converge at local maximal. On the other hand, training/inferencing with lattice was much better, because it offered many effective training examples by picking up the top N paths from the lattice. This structured DNN with lattice gave a phone error rate (PER) of 18.77% which outperformed structured SVM and is actually slightly better than Karel\u2019s recipe of kaldi at 18.90% in Table 1. Note that in the experiments here, as explained in Section3, we simply assume a single state for a phoneme in (7), which is certainly oversimplified.\nWe further used the best acoustic vectors (b) of Table 1 and change the number N for the N-best path from lattice in training/inferencing, and the results are in Table 2. In the case that we already know the reference phone label sequence, we can\nchoose the path closest to the reference, most different from the reference, or randomly choose one, out of the N-best paths. These gave the oracle min, oracle max, and random, the first three rows in Table 2. For N = 500, 1000 or 2000, the structured DNN(SDNN) row is always better than random in N-best, but with a gap from oracle min. This verified that the structured DNN did learn some structure information. Of course, the oracle min here is a lower bound for structured DNN with lattice proposed here. The last row shows the difference between random and structured DNN, which increases as training data(N) grows, verified that larger data set provides better results.\nThe next experiment is to analyze the phone error rates (PER) for different choices of the key hyper-parameters for the structured DNN, L number of hidden layers and M number of neurons in each hidden layer. Figure 3 is the result, a visualized PER map for structured DNN with lattice, N = 500 using acoustic vector (b). The horizontal axis is M where M = 100, 200, ...1000, and the vertical axis is L where L = 1, 2, ...6. Therefore, the figure consists of 6 \u00d7 10 = 60 data points. The overall performance is approximately 18% to 21%, all outperforming the structured SVM, and more or less comparable to Karel\u2019s recipe. For this task, better PER seemed to be located at less hidden layers and less neurons. If M is small, we can use deep networks up to 4 hidden layers, whereas M is large, 2 hidden layers is enough. With larger L and larger M , DNN overfits training data. The best PER is on (L,M) = (1, 500) which was the case in Table 2."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper, we propose a new structured learning architecture, structured DNN for phoneme recognition which jointly considers the structures of acoustic vector sequences and ph/oneme label sequences globally. Preliminary test results show that the structured DNN out-performed the previously proposed structured SVM and provided a comparable result to state-of-the-art. We will work on multiple states per phone in the future, and explore the possibility of structured DNN."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In this paper we propose the Structured Deep Neural Network (Structured DNN) as a structured and deep learning algorithm, learning to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structure rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learned utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning algorithm. It was shown to beat structured SVM in preliminary experiments on TIMIT.", "creator": "LaTeX with hyperref package"}}}