{"id": "1506.01744", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2015", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "abstract": "we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin inversion marks from multiple human cell types. a natural model for chromatin data in one cell type is a hidden markov model ( hmm ) ; we model the relationship between multiple cell types by connecting their hidden states alphabet by a fixed tree of known structure. the main challenge with learning parameters of such models exist is that iterative methods such as em are very likely slow, while naive spectral methods instead result in time and space complexity exponential in selecting the number size of cell types. we exploit experimental properties of identifying the tree tree structure of the hidden states to provide spectral algorithms that are technically more computationally efficient approximation for current sampled biological datasets. we provide sample complexity bounds for enabling our algorithm and subsequently evaluate it experimentally on biological data from nine human cell types. finally, we show that beyond our specific model, some of our adaptive algorithmic ideas effectively can be applied to implementing other graphical models.", "histories": [["v1", "Thu, 4 Jun 2015 22:57:28 GMT  (76kb,D)", "http://arxiv.org/abs/1506.01744v1", "27 pages, 3 figures"]], "COMMENTS": "27 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST q-bio.GN stat.TH", "authors": ["chicheng zhang", "jimin song", "kamalika chaudhuri", "kevin c chen"], "accepted": true, "id": "1506.01744"}, "pdf": {"name": "1506.01744.pdf", "metadata": {"source": "CRF", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "authors": ["Chicheng Zhang", "Jimin Song", "Kevin C Chen"], "emails": ["chz038@eng.ucsd.edu", "song@dls.rutgers.edu", "kcchen@dls.rutgers.edu", "kamalika@eng.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we develop a latent variable model and efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types [7, 9]. Chromatin marks are chemical modifications on the genome which are important in many basic biological processes. After standard preprocessing steps, the data consists of a binary vector (one bit for each chromatin mark) for each position in the genome and for each cell type.\nA natural model for chromatin data in one cell type is a Hidden Markov Model (HMM) [8, 13], for which efficient spectral algorithms are known. On biological data sets, spectral algorithms have been shown to have several practical advantages over maximum likelihood-based methods, including speed, prediction accuracy and biological interpretability [24]. Here we extend the approach by modeling multiple cell types together. We model the relationships between cell types by connecting their hidden states by a fixed tree, the standard model in biology for relationships between cell types. This comparative approach leverages the information shared between the different data sets in a statistically unified and biologically motivated manner.\nFormally, our model is an HMM where the hidden state zt at time t has a structure represented by a tree graphical model of known structure. For each tree node u we can associate an individual hidden state zut that depends not only on the previous hidden state z u t\u22121 for the same tree node u but also on the individual hidden state of its parent node. Additionally, there is an observation variable xut for each node u, and the observation x u t is independent of other state and observation variables\nar X\niv :1\n50 6.\n01 74\n4v 1\n[ st\nat .M\nL ]\nconditioned on the hidden state variable zut . In the bioinformatics literature, [5] studied this model with the additional constraint that all tree nodes share the same emission parameters. In biological applications, the main outputs of interest are the learned observation matrices of the HMM and a segmentation of the genome into regions which can be used for further studies.\nA standard approach to unsupervised learning of HMMs is the Expectation-Maximization (EM) algorithm. When applied to HMMs with very large state spaces, EM is very slow. A recent line of work on spectral learning [18, 1, 23, 6] has produced much more computationally efficient algorithms for learning many graphical models under certain mild conditions, including HMMs. However, a naive application of these algorithms to HMMs with large state spaces results in computational complexity exponential in the size of the underlying tree.\nHere we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. This is achieved by three novel key ideas. Our first key idea is to show that we can treat each root-to-leaf path in the tree separately and learn its parameters using tensor decomposition methods. This step improves the running time because our trees typically have very low depth. Our second key idea is a novel tensor symmetrization technique that we call Skeletensor construction where we avoid constructing the full tensor over the entire root-to-leaf path. Instead we use carefully designed symmetrization matrices to reveal its range in a Skeletensor which has dimension equal to that of a single tree node. The third and final key idea is called Product Projections, where we exploit the independence of the emission matrices along the root-to-leaf path conditioned on the hidden states to avoid constructing the full tensors and instead construct compressed versions of the tensors of dimension equal to the number of hidden states, not the number of observations. Beyond our specific model, we also show that Product Projections can be applied to other graphical models and thus we contribute a general tool for developing efficient spectral algorithms.\nFinally we implement our algorithm and evaluate it on biological data from nine human cell types [7]. We compare our results with the results of [5] who used a variational EM approach. We also compare with spectral algorithms for learning HMMs for each cell type individually to assess the value of the tree model."}, {"heading": "1.1 Related Work", "text": "The first efficient spectral algorithm for learning HMM parameters was due to [18]. There has been an explosion of follow-up work on spectral algorithms for learning the parameters and structure of latent variable models [23, 6, 4]. [18] gives a spectral algorithm for learning an observable operator representation of an HMM under certain rank conditions. [23] and [3] extend this algorithm to the case when the transition matrix and the observation matrix respectively are rank-deficient. [19] extends [18] to Hidden Semi-Markov Models.\n[2] gives a general spectral algorithm for learning parameters of latent variable models that have a multi-view structure \u2013 there is a hidden node and three or more observable nodes that are not connected to any other nodes and are independent conditioned on the hidden node. Many latent variable models have this structure, including HMMs, tree graphical models, topic models and mixture models. [1] provides a simpler, more robust algorithm that involves decomposing a third order tensor. [21, 22, 25] provide algorithms for learning latent trees and of latent junction trees.\nSeveral algorithms have been designed for learning HMM parameters for chromatin modeling, including stochastic variational inference [16] and contrastive learning of two HMMs [26]. However, none of these methods extend directly to modeling multiple chromatin sequences simultaneously."}, {"heading": "2 The Model", "text": "Probabilistic Model. The natural probabilistic model for a single epigenomic sequence is a hidden Markov model (HMM), where time corresponds to position in the sequence. The observation at time t is the sequence value at position t, and the hidden state at t is the regulatory function in this position.\nIn comparative epigenomics, the goal is to jointly model epigenomic sequences from multiple species or cell-types. This is done by an HMM with a tree-structured hidden state [5](THS-HMM),1 where each node in the tree representing the hidden state has a corresponding observation node. Formally, we represent the model by a tuple H = (G,O, T ,W); Figure 1 shows a pictorial representation.\nG = (V,E) is a directed tree with known structure whose nodes represent individual cell-types or species. The hidden state zt and the observation xt are represented by vectors {zut } and {xut } indexed by nodes u \u2208 V . If (v, u) \u2208 E, then v is the parent of u, denoted by \u03c0(u); if v is a parent of u, then for all t, zvt is a parent of z u t . In addition, the observations have the following product structure: if u\u2032 6= u, then conditioned on zut , the observation xut is independent of zu \u2032 t and x u\u2032\nt as well as any zu \u2032\nt\u2032 and x u\u2032 t\u2032 for t 6= t\u2032. O is a set of observation matrices Ou = P (xut |zut ) for each u \u2208 V and T is a set of transition tensors Tu = P (zut+1|zut , z \u03c0(u) t+1 ) for each u \u2208 V . Finally,W is the set of initial distributions where Wu = P (zu1 |z \u03c0(u) 1 ) for each z u 1 .\nGiven a tree structure and a number of iid observation sequences corresponding to each node of the tree, our goal is to determine the parameters of the underlying THS-HMM and then use these parameters to infer the most likely regulatory function at each position in the sequences.\nBelow we use the notation D to denote the number of nodes in the tree and d to denote its depth. For typical epigenomic datasets, D is small to moderate (5-50) while d is very small (2 or 3) as it is difficult to obtain data with large d experimentally. Typically m, the number of possible values assumed by the hidden state at a single node, is about 6-25, while n, the number of possible observation values assumed by a single node is much larger (e.g. 256 in our dataset).\nTensors. An order-3 tensor M \u2208 Rn1 \u2297Rn2 \u2297Rn3 is a 3-dimensional array with n1n2n3 entries, with its (i1, i2, i3)-th entry denoted as Mi1,i2,i3 .\nGiven ni\u00d71 vectors vi, i = 1, 2, 3, their tensor product, denoted by v1\u2297v2\u2297v3 is the n1\u00d7n2\u00d7n3 tensor whose (i1, i2, i3)-th entry is (v1)i1(v2)i2(v3)i3 . A tensor that can be expressed as the tensor product of a set of vectors is called a rank 1 tensor. A tensor M is symmetric if and only if for any permutation \u03c0 : [3]\u2192 [3], Mi1,i2,i3 = M\u03c0(i1),\u03c0(i2),\u03c0(i3).\nLet M \u2208 Rn1 \u2297Rn2 \u2297Rn3 . If Vi \u2208 Rni\u00d7mi , then M(V1, V2, V3) is a tensor of size m1\u00d7m2\u00d7m3, whose (i1, i2, i3)-th entry is: M(V1, V2, V3)i1,i2,i3 = \u2211 j1,j2,j3 Mj1,j2,j3(V1)j1,i1(V2)j2,i2(V3)j3,i3 .\nSince a matrix is a order-2 tensor, we also use the following shorthand to denote matrix multiplication. Let M \u2208 Rn1 \u2297 Rn2 . If Vi \u2208 Rmi\u00d7ni , then M(V1, V2) is a matrix of size m1 \u00d7 m2, whose (i1, i2)-th entry is: M(V1, V2)i1,i2 = \u2211 j1,j2\nMj1,j2(V1)j1,i1(V2)j2,i2 . This is equivalent to V >1 MV2.\n1In the bioinformatics literature, this model is also known as a tree HMM.\nMeta-States and Observations, Co-occurrence Matrices and Tensors. Given observations xut and xut\u2032 at a single node u, we use the notation P u t,t\u2032 to denote their expected co-occurence frequencies: Pu,ut,t\u2032 = E[xut \u2297 xut\u2032 ], and P\u0302 u,u t,t\u2032 to denote their corresponding empirical version. The tensor Pu,u,ut,t\u2032,t\u2032\u2032 = E[xut \u2297 xut\u2032 \u2297 xut\u2032\u2032 ] and its empirical version P\u0302 u,u,u t,t\u2032,t\u2032\u2032 are defined similarly.\nOccasionally, we will consider the states or observations corresponding to a subset of nodes in G coalesced into a single meta-state or meta-observation. Given a connected subset S \u2286 V of nodes in the tree G that includes the root, we use the notation zSt and x S t to denote the meta-state represented by (zut , u \u2208 S) and the meta-observation represented by (xut , u \u2208 S) respectively. We define the observation matrix for S as OS = P (xSt |zSt ) \u2208 Rn\n|S|\u00d7m|S| and the transition matrix for S as TS = P (zSt+1|zSt ) \u2208 Rm |S|\u00d7m|S| , respectively.\nFor sets of nodes V1 and V2, we use the notation P V1,V2 t,t\u2032 to denote the expected co-occurrence frequencies of the meta-observations xV1t and x V2 t\u2032 . Its empirical version is denoted by P\u0302 V1,V2 t,t\u2032 . Similarly, we can define the notation PV1,V2,V3t,t\u2032,t\u2032\u2032 and its empirical version P\u0302 V1,V2,V3 t,t\u2032,t\u2032\u2032 .\nBackground on Spectral Learning for Latent Variable Models. Recent work by [1] has provided a novel elegant tensor decomposition method for learning latent variable models. Applied to HMMs, the main idea is to decompose a transformed version of the third order co-occurrence tensor of the first three observations to recover the parameters; [1] shows that given enough samples and under fairly mild conditions on the model, this provides an approximation to the globally optimal solution. The algorithm has three main steps. First, the third order tensor of the co-occurrences is symmetrized using the second order co-occurrence matrices to yield a symmetric tensor; this symmetric tensor is then orthogonalized by a whitening transformation. Finally, the resultant symmetric orthogonal tensor is decomposed via the tensor power method.\nIn biological applications, instead of multiple independent sequences, we have a single long sequence in the steady state. In this case, following ideas from [23], we use the average over t of the third order co-occurence tensors of three consecutive observations starting at time t. The second order co-occurence tensor is also modified similarly."}, {"heading": "3 Algorithm", "text": "A naive approach for learning parameters of HMMs with tree-structured hidden states is to directly apply the spectral method of [1]. Since this method ignores the structure of the hidden state, its running time is very high, \u2126(nDmD), even with optimized implementations. This motivates the design of more computationally efficient approaches.\nA plausible approach is to observe that at t = 1, the observations are generated by a tree graphical model; thus in principle one could learn the parameters of the underlying tree using existing algorithms [22, 21, 25]. However, this approach does not directly produce the HMM parameters; it also does not work for biological sequences because we do not have multiple independent samples at t = 1; instead we have a single long sequence at the steady state, and the steady state distribution of observations is not generated by a latent tree. Another plausible approach is to use the spectral junction tree algorithm of [25]; however, this algorithm does not provide the actual transition and observation matrix parameters which hold important biological information, and instead provides an observable operator representation.\nOur main contribution is to show that we can achieve a much better running time by exploiting the structure of the hidden state. Our algorithm is based on three key ideas \u2013 Partitioning, Skeletensor Construction and Product Projections. We explain these ideas next.\nPartitioning. Our first observation is that to learn the parameters at a node u, we can focus only on the unique path from the root to u. Thus we partition the learning problem on the tree into separate learning problems on these paths. This maintains correctness as proved in the Appendix.\nThe Partitioning step reduces the computational complexity since we now need to learn an HMM with md states and nd observations, instead of the naive method where we learn an HMM with mD states and nD observations. As d D in biological data, this gives us significant savings. Constructing the Skeletensor. A naive way to learn the parameters of the HMM corresponding to each root-to-node path is to work directly on the O(nd \u00d7 nd \u00d7 nd) co-occurrence tensor. Instead, we show that for each node u on a root-to-node path, a novel symmetrization method can be used to construct a much smaller skeleton tensor Tu of size n \u00d7 n \u00d7 n, which nevertheless captures the effect of the entire root-to-node path and projects it into the skeleton tensor, thus revealing the range of Ou. We call this the skeletensor.\nLet Hu be the path from the root to a node u, and let P\u0302 Hu,u,Hu 1,2,3 be the empirical n |Hu| \u00d7 n\u00d7 n|Hu| tensor of co-occurrences of the meta-observations Hu, u and Hu at times 1, 2 and 3 respectively. Based on the data we construct the following symmetrization matrices:\nS1 \u223c P\u0302u,Hu2,3 (P\u0302 Hu,Hu 1,3 ) \u2020, S3 \u223c P\u0302u,Hu2,1 (P\u0302 Hu,Hu 3,1 ) \u2020\nNote that S1 and S3 are n \u00d7 n|Hu| matrices. Symmetrizing P\u0302Hu,u,Hu1,2,3 with S1 and S3 gives us an n\u00d7n\u00d7n skeletensor, which can in turn be decomposed to give an estimate of Ou (see Lemma 3 in the Appendix).\nEven though naively constructing the symmetrization matrices and skeletensor takes O(Nn2d+1 + n3d) time, this procedure improves computational efficiency because tensor construction is a onetime operation, while the power method which takes many iterations is carried out on a much smaller tensor.\nProduct Projections. We further reduce the computational complexity by using a novel algorithmic technique that we call Product Projections. The key observation is as follows. Let Hu = {u0, u1, . . . , ud\u22121} be any root-to-node path in the tree and consider the HMM that generates the observations (xu0t , x u1 t , . . . , x ud\u22121 t ) for t = 1, 2, . . .. Even though the individual observations x uj t , j = 0, 1, . . . , d \u2212 1 are highly dependent, the range of OHu , the emission matrix of the HMM describing the path Hu, is contained in the product of the ranges of Ouj , where Ouj is the emission matrix at node uj (Lemma 4 in the Appendix). Furthermore, even though the Ouj matrices are difficult to find, their ranges can be determined by computing the SVDs of the observation co-occurrence matrices at uj .\nThus we can implicitly construct and store (an estimate of) the range of OHu . This also gives us estimates of the range of P\u0302Hu,Hu1,3 , the column spaces of P\u0302 u,Hu 2,1 and P\u0302 u,Hu 2,3 , and the range of the first and third modes of the tensor P\u0302Hu,u,Hu1,2,3 . Therefore during skeletensor construction we can avoid explicitly constructing S1, S3 and P\u0302 Hu,u,Hu 1,2,3 , and instead construct their projections onto their ranges. This reduces the time complexity of the skeletensor construction step to\nO(Nm2d+1 +m3d + dmn2) (recall that the range has dimension m.) While the number of hidden states m could be as high as n, this is a significant gain in practice, as n m in biological datasets (e.g. 256 observations vs. 6 hidden states).\nProduct projections are more efficient than random projections [17] on the co-occurrence matrix of meta-observations: the co-occurrence matrices are nd\u00d7nd matrices, and random projections would take \u2126(nd) time. Also, product projections differ from the suggestion of [15] since we exploit properties of the model to efficiently find good projections."}, {"heading": "3.1 The Full Algorithm", "text": "Our final algorithm follows from combining the three key ideas above. Algorithm 1 shows how to recover the observation matrices Ou at each node u. Once the Ous are recovered, one can use standard techniques to recover T and W ; details are described in Algorithm 2 in the Appendix."}, {"heading": "3.2 Product Projections beyond HMMs with Tree-structured Hidden States", "text": "The Product Projections technique is a general technique with applications beyond our model.\nAlgorithm 1 Algorithm for Observation Matrix Recovery\n1: Input: N samples of the three consecutive observations (x1, x2, x3) N i=1 generated by an HMM\nwith tree structured hidden state with known tree structure. 2: for u \u2208 V do 3: Perform SVD on P\u0302u,u1,2 to get the first m left singular vectors U\u0302\nu. 4: end for 5: for u \u2208 V do 6: Let Hu denote the set of nodes on the unique path from root r to u. Let U\u0302Hu = \u2297v\u2208HuU\u0302v . 7: Construct Projected Skeletensor. First, compute symmetrization matrices:\nS\u0302u1 = ((U\u0302 u)>P\u0302u,Hu2,3 U\u0302 Hu)((U\u0302Hu)>P\u0302Hu,Hu1,3 U\u0302 Hu)\u22121, S\u0302u3 = ((U\u0302 u)>P\u0302u,Hu2,1 U\u0302 Hu)((U\u0302Hu)>P\u0302Hu,Hu3,1 U\u0302 Hu)\u22121\n8: Compute symmetrized second and third co-occurrences for u:\nM\u0302u2 = (P\u0302 Hu,u 1,2 (U\u0302 Hu(S\u0302u1 ) >, U\u0302u) + P\u0302Hu,u1,2 (U\u0302 Hu(S\u0302u1 ) >, U\u0302u)>)/2\nM\u0302u3 = P\u0302 Hu,u,Hu 1,2,3 (U\u0302 Hu(S\u0302u1 ) >, U\u0302u, U\u0302Hu(S\u0302u3 ) >)\n9: Orthogonalization and Tensor Decomposition. Orthogonalize M\u0302u3 using M\u0302u2 and decompose to recover (\u03b8\u0302u1 , . . . , \u03b8\u0302 u m) as in [1] (See Algorithm 3 in the Appendix for details).\n10: Undo Projection onto Range. Estimate Ou as: O\u0302u = U\u0302u\u0398\u0302u, where \u0398\u0302u = (\u03b8\u0302u1 , . . . , \u03b8\u0302um). 11: end for\nApplication 1: HMM with more general hidden states. Consider an HMM with a hidden state represented by a general graphical modelG = (V,E) with an observation variable xut corresponding to each u \u2208 V . xut is independent of all other hidden state and observation nodes, conditioned on its corresponding hidden state variable zut . In this case, O\n|V | = \u2297u\u2208VOu. Similar graphical models have been used in biology to model gene expression time courses [12].\nApplication 2: HMM with rank-deficient observation matrix. Consider an HMM whose observation matrix O is rank-deficient. In this case, [3] suggests compressing sequences of successive observations of size s for s = 2, 3, . . . until the matrices Of = P (xt, xt+1, . . . , xt+s\u22121|zt) and Ob = P (xt, xt\u22121, . . . , xt\u2212s+1|zt) have rank m. A version of [18] is then run using observation sequence pairs P1:s,s+1:2s and triples P1:s,s+1,s+2:2s+1. In this case, we can show that both range(Of ) and range(Ob) are contained in range(O\u2297s); we can therefore use Product Projections to improve the \u2126(ns) running time to O(mO(s))."}, {"heading": "3.3 Performance Guarantees", "text": "We now provide performance guarantees on our algorithm. Since learning parameters of HMMs and many other graphical models is NP-Hard, spectral algorithms make simplifying assumptions on the properties of the model generating the data. Typically these assumptions take the form of some conditions on the rank of certain parameter matrices. We state below the conditions needed for our algorithm to successfully learn parameters of a HMM with tree structured hidden states. Observe that we need two kinds of rank conditions \u2013 node-wise and path-wise \u2013 to ensure that we can recover the full set of parameters on a root-to-node path.\nAssumption 1 (Node-wise Rank Condition). For all u \u2208 V , the matrix Ou has rank m, and the joint probability matrix Pu,u2,1 has rank m.\nAssumption 2 (Path-wise Rank Condition). For any u \u2208 V , let Hu denote the path from root to u. Then, the joint probability matrix PHu,Hu1,2 has rank m |Hu|.\nAssumption 1 is required to ensure that the skeletensor can be decomposed, and that U\u0302u indeed captures the range of Ou. Assumption 2 ensures that the symmetrization operation succeeds. This kind of assumption is very standard in spectral learning [18, 1].\n[3] has provided a spectral algorithm for learning HMMs involving fourth and higher order moments when Assumption 1 does not hold. We believe similar approaches will apply to our problem as well, and we leave this as an avenue for future work.\nIf Assumptions 1 and 2 hold, we can show that Algorithm 1 is consistent \u2013 provided enough samples are available, the model parameters learnt by the algorithms are close to the true model parameters. A finite sample guarantee is provided in the Appendix.\nTheorem 1. [Consistency] Suppose we run Algorithm 1 on the first three observation vectors {xi,1, xi,2, xi,3} from N iid sequences generated by an HMM with tree-structured hidden states. Then, for all nodes u \u2208 V , the recovered estimates O\u0302u satisfy the following property: with high probability over the iid samples, there exists a permutation \u03a0u of the columns of O\u0302u such that as \u2016Ou \u2212\u03a0uO\u0302u\u2016 \u2264 \u03b5(N) where \u03b5(N)\u2192 0 as N \u2192\u221e.\nObserve that the observation matrices (as well as the transition and initial probabilities) are recovered upto permutations of hidden states in a globally consistent manner."}, {"heading": "4 Experiments", "text": "Data and experimental settings. We ran our algorithm, which we call \u201cSpectral-Tree\u201d, on a chromatin dataset on human chromosome 1 from nine cell types (H1-hESC, GM12878, HepG2, HMEC, HSMM, HUVEC, K562, NHEK, NHLF) from the ENCODE project [7]. Following [5], we used a biologically motivated tree structure of a star tree with H1-hESC, the embryonic stem cell type, as the root. There are data for eight chromatin marks for each cell type which we preprocessed into binary vectors using a standard Poisson background assumption [11]. The chromosome is divided into 1,246,253 segments of length 200, following [11]. The observed data consists of a binary vector of length eight for each segment, so the number of possible observations is the number of all combinations of presence or absence of the chromatin marks (i.e. n = 28 = 256). We set the number of hidden states, which we interpret as chromatin states, to m = 6, similar to the choice of ENCODE. Our goals are to discover chromatin states corresponding to biologically important functional elements such as promoters and enhancers, and to label each chromosome segment with the most probable chromatin state.\nObserve that instead of the first few observations from N iid sequences, we have a single long sequence in the steady state per cell type; thus, similar to [23], we calculate the empirical cooccurrence matrices and tensors used in the algorithm based on two and three successive observations respectively (so, more formally, instead of P\u03021,2, we use the average over t of P\u0302t,t+1 and so on). Additionally, we use a projection procedure similar to [4] for rounding negative entries in the recovered observation matrices. Our experiments reveal that the rank conditions appear to be satisfied for our dataset.\nRun time and memory usage comparisons. First, we flattened the HMM with tree-structured hidden states into an ordinary HMM with an exponentially larger state space. Our Python implementation of the spectral algorithm for HMMs of [18] ran out of memory while performing singular value decomposition on the co-occurence matrix, even using sparse matrix libraries. This suggests that naive application of spectral HMM is not practical for biological data.\nNext we compared the performance of Spectral-Tree to a similar model which additionally constrained all transition and observation parameters to be the same on each branch [5]. That work used several variational approximations to the EM algorithm and reported that SMF (structured mean field) performed the best in their tests. Although we implemented Spectral-Tree in Matlab and did not optimize it for run-time efficiency, Spectral-Tree took \u223c2 hr, whereas the SMF algorithm took \u223c13 hr for 13 iterations to convergence. This suggests that spectral algorithms may be much faster than variational EM for our model.\nBiological interpretation of the observation matrices. Having examined the efficiency of Spectral-Tree, we next studied the accuracy of the learned parameters. We focused on the observation matrices which hold most of the interesting biological information. Since the full observation matrix is very large (28 \u00d7 6 where each row is a combination of chromatin marks), Figure 2 shows\nthe 8\u00d7 6 marginal distribution of each chromatin mark conditioned on each hidden state. SpectralTree identified most of the major types of functional elements typically discovered from chromatin data: repressive, strong enhancer, weak enhancer, promoter, transcribed region and background state (states 1-6, respectively, in Figure 2b). In contrast, the SMF algorithm used three out of the six states to model the large background state (i.e. the state with no chromatin marks). It identified repressive, transcribed and promoter states (states 2, 4, 5, respectively, in Figure 2a) but did not identify any enhancer states, which are one of the most interesting classes for further biological studies.\nWe believe these results are due to that fact that the background state in the data set is large: \u223c62% of the segments do not have chromatin marks for any cell type. The background state has lower biological interest but is modeled well by the maximum likelihood approach. In contast, biologically interesting states such as promoters and enhancers comprise a relatively small fraction of the genome. We cannot simply remove background segments to make the classes balanced because it would change the length distribution of the hidden states. Finally, we observed that our model estimated significantly different parameters for each cell type which captures different chromatin states (Appendix Figure 3). For example, we found enhancer states with strong H3K27ac in all cell types except for H1-hESC, where both enhancer states (3 and 6) had low signal for this mark. This mark is known to be biologically important in these cells for distinguishing active from poised enhancers [10]. This suggests that modeling the additional branch-specific parameters can yield interesting biological insights.\nComparison of the chromosome segments labels. We computed the most probable state for each chromosome segment using a posterior decoding algorithm. We tested the accuracy of the predictions using an experimentally defined data set and compared it to SMF and the spectral algorithm for HMMs run for individual cell types without the tree (Spectral-HMM). Specifically we assessed promoter prediction accuracy (state 5 for SMF and state 4 for Spectral-Tree in Figure 2) using CAGE data from [14] which was available for six of the nine cell types. We used the F1 score (harmonic mean of precision and recall) for comparison and found that Spectral-Tree was much more accurate than SMF for all six cell types (Table 1). This was because the promoter predictions of SMF were biased towards the background state so those predictions had slightly higher recall but much lower specificity.\nFinally, we compared our predictions to Spectral-HMM to assess the value of the tree model. H1hESC is the root node so Spectral-HMM and Spectral-Tree have the same model and obtain the same accuracy (Table 1). Spectral-Tree predicts promoters more accurately than Spectral-HMM for all other cell types except HepG2. However, HepG2 is the most diverged from the root among the cell types based on the Hamming distance between the chromatin marks. We hypothesize that for HepG2, the tree is not a good model which slightly reduces the prediction accuracy.\nOur experiments show that Spectral-Tree has improved computational efficiency, biological interpretability and prediction accuracy on an experimentally-defined feature compared to variational EM for a similar tree HMM model and a spectral algorithm for single HMMs. A previous study showed improvements for spectral learning of single HMMs over the EM algorithm [24]. Thus our\nalgorithms may be useful to the bioinformatics community in analyzing the large-scale chromatin data sets currently being produced."}, {"heading": "5 Acknowledgements", "text": "We thank NSF under IIS-1162581 for support. Part of this work was done while Chaudhuri was visiting the Spectral Learning program at the Simons Foundation in UC Berkeley."}, {"heading": "A Recovering the Transition Probabilities and Initial Probabilities", "text": "Algorithm 2 recovers the transition and initial probabilities, given estimates of observation matrices. Theorem 2 provides finite sample guarantees on Algorithm 1 in conjunction with Algorithm 2.\nAlgorithm 2 Recovering the Transition Probabilities and Initial Probabilities\n1: Input: N samples of the first three observations (x1, x2, x3) N i=1 generated by a tree HMM,\nEstimates of observation matrices O\u0302u. 2: for u \u2208 V do 3: if u is root r then 4: Compute W\u0302 r = (O\u0302u)\u2020P\u0302 r1 . 5: Compute Q\u0302r = (O\u0302r)\u2020P\u0302 r,r2,1 (O\u0302\nr)\u2020>. 6: Normalize over the zu2 coordinate to get T\u0302\nu. 7: else 8: Compute W\u0302u = (O\u0302u)\u2020Pu,\u03c0(u)1,1 (O\u0302 \u03c0(u))\u2020>. 9: Compute Q\u0302u = Pu,\u03c0(u),u2,2,1 ((O\u0302 u)\u2020T , (O\u0302\u03c0(u))\u2020>, (O\u0302u)\u2020>).\n10: Normalize over the zu2 coordinate to get T\u0302 u. 11: end if 12: end for"}, {"heading": "B Additional Notations", "text": "For a node u \u2208 V , when it is clear from context, we sometimes use H to denote Hu and d to denote du.\nDefine OH2 to be a n d \u00d7 md matrix whose rows are indexed by elements in [n]d and columns are indexed by elements in [m]d. In particular, (OH2 )(i1,...,id),(j1,...,jd) = P (x r 2 = i1, . . . , x u 2 = id|zr2 = j1, . . . , z u 2 = jd). Similarly we define O H 3 whose entries are (O H 3 )(i1,...,id),(j1,...,jd) = P (x r 3 = i1, . . . , x u 3 = id|zr2 = j1, . . . , zu2 = jd), and OH1 whose its entries are (OH1 )(i1,...,id),(j1,...,jd) = P (xr1 = i1, . . . , x u 1 = id|zr2 = j1, . . . , zu2 = jd). We define Ou2 to be a n \u00d7 md matrix, whose rows are indexed by elements in [n], and columns are indexed by elements in [m]d. Its entries are (Ou2 )i,(j1,...,jd) = P (x u 1 = i|zr2 = j1, . . . , zu2 = jd).\nDefine \u03c0H to be a vector representing the marginal probability of (zr2 , . . . , z u 2 ). In particular, its rows are indexed by elements in [m]d, and \u03c0H(i1,...,id) = P (z r 2 = i1, . . . , z u 2 = id). Define \u03c0\nu to be a vector representing the marginal probability of zu2 . In particular, its rows are indexed by elements in [m], and \u03c0ui = P (z u 2 = i). Define \u03c0 u min as mini \u03c0 u i . Define \u03c1\nH as themd dimensional vector representing the marginal probability of (zr1 , . . . , z u 1 ) whose entries are indexed by elements in [m]\nd. In particular, \u03c1H(i1,...,id) = P (z r 1 = i1, . . . , z u 1 = id). T\nH is defined as the md \u00d7 md matrix representing the conditional probability of zH2 given z H 1 , and its rows and columns are indexed by elements in [m]\nd, in particular, T(i1,...,id),(j1,...,jd) = P (z r 2 = i1, . . . , z u 2 = id|zr1 = j1, . . . , zu1 = jd).\nLet u be a node in V . Define Uu to be a matrix whose columns form an orthonormal basis of Ou. One way to get Uu is to take its columns to be the top m singular vectors of Ou. The specific choice of Uu does not affect our analysis, as we will be only looking at the projection matrix Uu(Uu)> throughout. Define UH to be \u2297v\u2208HUu. For a matrix M , define \u2016M\u2016 to be its operator norm, that is, max\u2016u\u2016=1,\u2016v\u2016=1 \u2016v>Mu\u2016. Define the Frobenius norm of M , \u2016M\u2016F to be square root of the sum of the square of its entries, that is,\u221a\u2211\ni,jM 2 ij . By standard results in linear algebra, \u2016M\u2016 \u2264 \u2016M\u2016F . Similarly, for a third order\ntensor T , define \u2016T\u2016 to be its operator norm, that is max\u2016u\u2016=1,\u2016v\u2016=1,\u2016w\u2016=1 T (u, v, w). Define the Frobenius norm of T , \u2016T\u2016F to be square root of the sum of the square of its entries, that is,\u221a\u2211\ni,j,k T 2 ijk. By standard results of linear algebra, \u2016T\u2016 \u2264 \u2016T\u2016F ."}, {"heading": "C Main Lemmas", "text": ""}, {"heading": "C.1 Partitioning Lemmas", "text": "Lemma 1 (Path Partitioning). Suppose observations and states {xvt , zvt }v\u2208V,t\u2208N are drawn from a THS-HMM represented by H = (G,T,O,W ), where G = (V,E), T = {Tv, v \u2208 V }, O = {Ov, v \u2208 V }, W = {Wv, v \u2208 V }. Let u \u2208 V , and let Hu denote nodes inside the unique path from root r to u. Then {xvt , zvt }u\u2208Hu,t\u2208N are generated by a THS-HMM represented by a tuple H\u0303 = (G\u0303, T\u0303 , O\u0303, W\u0303 ), where G\u0303 = (V\u0303 , E\u0303) is the induced subgraph on Hu. In particular, V\u0303 = Hu, E\u0303 = {(v, \u03c0(v))}v\u2208Hu ), T\u0303 = {Tv, v \u2208 Hu}, O\u0303 = {Ov, v \u2208 Hu}, W\u0303 = {Wv, v \u2208 Hu}.\nProof of Lemma 1. To show this lemma, we will calculate the marginal distribution of the variables {xvt , zvt }v\u2208Hu,t\u2208[\u03c4 ]. Observe that the full joint distribution of {xvt , zvt }v\u2208G,t\u2208[\u03c4 ] is equal to:\u220f\nv\u2208G Pr(zv1 ) \u03c4\u22121\u220f t=1 \u220f v\u2208Hu Pr(zvt+1|zvt , z \u03c0(v) t+1 ) \u03c4\u220f t=1 \u220f v\u2208G Pr(xvt |zvt )\nTo calculate the marginal over {xvt , zvt }v\u2208Hu,t\u2208[\u03c4 ], we eliminate the rest of the variables one by one. Observe that we can eliminate any observation variable xvt for v /\u2208 Hu without introducing any extra edges, as xvt is only connected to z v t . Moreover, marginalizing x v t gives: \u2211 x Pr(x v t = x|zvt = z) = 1.\nLet G\u0303 be the current tree; initially G\u0303 = G. We next eliminate the nodes {zvt , t = \u03c4, . . . , 1} for v /\u2208 Hu one by one where v /\u2208 Hu is a leaf node in G\u0303. We do this in the order zvT , zvT\u22121, . . . , zv1 ; once we have eliminated these nodes, we delete v from G\u0303, and we continue until only the nodes in Hu are left. To eliminate a zvt when {zvs , s > t} have been eliminated, we sum over: \u2211 z Pr(z v t = z|zvt\u22121, z \u03c0(v) t ) which also sums to 1.\nWe repeat this process until only the nodes {xvt , zvt }u\u2208Hu,t\u2208[T ] are left. Since we get 1 from eliminating each variable, the marginal we are left with is:\n\u220f v\u2208Hu Pr(zv1 ) T\u22121\u220f t=1 \u220f v\u2208Hu Pr(zvt+1|zvt , z \u03c0(v) t+1 ) T\u220f t=1 \u220f v\u2208Hu Pr(xvt |zvt ), (1)\nwhich is the marginal distribution of an HMM with tree-structured hidden states described by the tuple (G\u0303, T\u0303 , O\u0303, W\u0303 ). The lemma follows.\nThe following is a Corollary of Lemma 1. Corollary 1. If observations and states {xvt , zvt }v\u2208Hu,t\u2208N are drawn from a THS-HMM represented by (G\u0303, T\u0303 , O\u0303, W\u0303 ), then the sequence of coalesced observations and states {xHut , z Hu t }t\u2208N are drawn from an HMM.\nProof. The proof is a simple extension of Lemma 1. (1) gives us the marginal distribution of {xvt , zvt }v\u2208Hu,t\u2208N. Observe that for any t, conditioned on z Hu t , x Hu t is d-separated from all the other nodes of the graph \u2013 this is because for any node x in the graphical model, xHut , z Hu t and x either form a chain or or a fork structure whose middle node is zHut . Moreover, conditioned on z Hu t , zHut+1 is d-separated from the set of nodes {zHus } t\u22121 s=1. This is because z Hu s , z Hu t and z Hu t+1 form a chain structure whose middle node is zHut . The lemma thus follows."}, {"heading": "C.2 Skeletensor Lemmas", "text": "In this subsection, we justify our construction of a skeletensor. Let u be any node in the tree G and let H be the path from the root of G to u.\nRecall that we define OH1 to be the n d \u00d7 md matrix, whose entries are (OH1 )(i1,...,id),(j1,...,jd) = P (xr1 = i1, . . . , x u 1 = id|zr2 = j1, . . . , zu2 = jd). Similarly, OH3 is a nd \u00d7md matrix, with entries (OH3 )(i1,...,id),(j1,...,jd) = P (x r 3 = i1, . . . , x u 3 = id|zr2 = j1, . . . , zu2 = jd).\nWe begin by showing that under Assumptions 1 and 2, the matrices OH1 and O H 3 for the three-view mixture model induced by the HMM have full column rank. Lemma 2. Let u be a node in V . Recall that H = Hu is the set of nodes along the path from root r to u. Then: (1) The matrices diag(\u03c1H)(TH)>diag(\u03c0H)\u22121 and TH are of full rank. (2) The matrices OH1 and O H 3 are of full column rank.\nProof. By Lemma 1, xH1 , x H 2 , x H 3 are conditionally independent given h H 2 . Thus,\nPH,H1,2 = O H 1 diag(\u03c0 H)(OH2 ) >\nSince by Assumption 2, PH,H1,2 is of rank m d, this implies that the matrix OH1 must be of rank m d as well. By Proposition 4.2 of [2],\nOH1 = O Hdiag(\u03c1H)(TH)>diag(\u03c0H)\u22121\nThis implies that diag(\u03c1H)(TH)>diag(\u03c0H)\u22121 is of rank md, which is of full rank. Hence TH is of full rank. By Proposition 4.2 of [2],\nOH3 = O HTH\nThis shows OH3 is of full column rank.\nSecond, we discuss the infinite sample version of our symmetrization matrix. This will be extended in Lemma 8 in our detailed finite sample analysis. Lemma 3. Let u be a node in V . Recall that Hu is the set of nodes along the path from root r to u. Assume Pu,H2,3 , P H,H 1,3 , P u,H 2,1 are given (where P H,H 3,1 = (P H,H 1,3 )\nT ). Let the symmetrization matrices be:\nSu1 = P u,H 2,3 (P H,H 1,3 ) \u2020\nSu3 = P u,H 2,1 (P H,H 3,1 ) \u2020\nand the ground truth symmetrized pair-wise and triple-wise co-occurence tensors be:\nMu2 = P H,u 1,2 (S uT 1 , I)\nMu3 = P H,u,H 1,2,3 (S uT 1 , I, S uT 3 )\nThen, Mu2 = \u2211 i \u03c0ui (O u)i \u2297 (Ou)i\nMu3 = \u2211 i \u03c0ui (O u)i \u2297 (Ou)i \u2297 (Ou)i\nProof. By Lemma 1, xH1 , x u 2 , x H 3 are conditionally independent given z H 2 , thus\nPu,H2,3 = O u 2 diag(\u03c0 H)OHT3\nPH,H1,3 = O H 1 diag(\u03c0 H)OHT3\nLemma 2 implies that OH1 is of full column rank, and diag(\u03c0 H)OHT3 is of full row rank. Therefore by standard properties of pseudoinverse,\n(PH,H1,3 ) \u2020 = (diag(\u03c0H)OHT3 ) \u2020(OH1 ) \u2020\nTherefore, Su1 = O u 2 (O H 1 ) \u2020 Likewise, Su3 = O u 2 (O H 3 ) \u2020 Then, Mu2 = P H,u 1,2 (S uT 1 , I)\n= \u2211\ni1,...,iD\n\u03c0Hi1,...,iD (O u 2 )i1,...,iD \u2297 (Ou2 )i1,...,iD\n= \u2211\ni1,...,iD\n\u03c0Hi1,...,iD (O u)iD \u2297 (Ou)iD\n= \u2211 i \u03c0ui (O u)i \u2297 (Ou)i\nMu3 = P H,u,H 1,2,3 (S uT 1 , I, S uT 3 ) = \u2211\ni1,...,iD\n\u03c0Hi1,...,iD (O u 2 )i1,...,iD \u2297 (Ou2 )i1,...,iD \u2297 (Ou2 )i1,...,iD\n= \u2211\ni1,...,iD\n\u03c0Hi1,...,iD (O u)iD \u2297 (Ou)iD \u2297 (Ou)iD\n= \u2211 i \u03c0ui (O u)i \u2297 (Ou)i \u2297 (Ou)i"}, {"heading": "C.3 Product Projections Lemmas", "text": ""}, {"heading": "C.3.1 Product Projections in HMM with Tree Hidden States", "text": "Lemma 4. OH , the observation matrix of the HMM that generates the meta-states and metaobservations {zHt , xHt }t\u2208N, equals \u2297 v\u2208H O v .\nProof. We consider the observation matrix of the HMM that generates the meta-states and metaobservations {zHt , xHt }t\u2208N. The number of possible meta-hidden states zHt is md, indexed by (zvt )v\u2208H and the number of possible meta-observations x H t is n\nd, indexed by (xvt )v\u2208H . Thus, the observation matrix OH is of dimension nd \u00d7md. Entrywise,\n(OHu)(i1,...,id),(j1,...,jd)\n= P(xrt = i1, . . . , xut = id|zrt = j1, . . . , zut = jd) = Oi1,j1 . . . Oid,jd\n= ( \u2297 v\u2208H Ov)(i1,...,id),(j1,...,jd)\nWhere the second equality uses conditional independence. Therefore, OH = \u2297\nv\u2208H O v ."}, {"heading": "C.3.2 Product Projections Beyond HMM with Tree Hidden States", "text": "We consider the case of a simple HMM when the observation matrix O is not full rank. In this case, we first define forward and backward observation matrices O\u0303fs and O\u0303 b s formally. For a fixed s, O\u0303 f s is a ns \u00d7 m matrix, with rows indexed by a s-tuple (j1, . . . , js) \u2208 [n]s, and columns indexed by i \u2208 [m]. Entrywise,\n(O\u0303fs )(i1,...,is),j = P (xt = i1, xt+1 = i2, . . . , xt+s\u22121 = is|zt = j)\nSimilarly we define backward observation matrices O\u0303bs = P (xt, xt\u22121, . . . , xt\u2212s+1|zt). Entrywise,\n(O\u0303bs)(i1,...,is),j = P (xt = i1, xt\u22121 = i2, . . . , xt\u2212s+1 = is|zt = j) The claim is the range of the forward(backward) observation matrices is contained in the range of the s-wise Kronecker product of the original observation matrices. Lemma 5.\nrange(O\u0303fs ) \u2286 range(O\u2297s) range(O\u0303bs) \u2286 range(O\u2297s)\nProof. We prove the first relationship, since the proof of the second is almost identical. Note that by the law of total probability,\n(O\u0303fs )(i1,i2,...,is),j\n= P (xt = i1, xt+1 = i2, . . . , xt+s\u22121 = is|zt = j) = \u2211 j2,...,js P (xt = i1, xt+1 = i2, . . . , xt+s\u22121 = is|zt = j, zt+1 = j2, . . . , zt+s\u22121 = js)\n\u00d7P (zt+1 = j2 . . . , zt+s\u22121 = js|zt = j) = \u2211 j2,...,js Oi1,jOi2,j2 . . . Ois,jsP (zt+1 = j2 . . . , zt+s\u22121 = js|zt = j)\n= \u2211\nj2,...,js\n(O\u2297s)(i1,i2,...,is),(j,j2,...,js)P (zt+1 = j2 . . . , zt+s\u22121 = js|zt = j)\nThus, each column of O\u0303fs is a linear combination of the columns of O \u2297s, thus completing the proof."}, {"heading": "D Finite Sample Guarantees", "text": "Theorem 2 (Accuracy of Initial Distribution and Transition Probabilities). There exists a universal constant C such that the following hold. Suppose Algorithm 1 is given as input N iid observation triples (xi1, xi2, xi3)Ni=1 generated by a THS-HMM, and outputs estimates of observaton matrices O\u0302u, for each node u in the tree. Then Algorithm 2 is run on the same sample and has {O\u0302u}u\u2208V as input. If the size of sample N is greater than:\nC max ( D2\n\u03c322\u03c3 2 3\nln D \u03b4 , m\n\u03c321\u03c3 2 2\nln D\n\u03b4 ,\nm2\n\u03c361\u03c3 6 3\u03c0 3 min\nln D\n\u03b4 ,\nm\n\u03c322\u03c3 8 1\n2 ln D\n\u03b4 ,\nm2\n\u03c363\u03c3 14 1 \u03c0 4 min\n2 ln D\n\u03b4 ) where \u03c31 = minu\u2208V \u03c3m(Ou), \u03c32 = minu\u2208V \u03c3m(P u,u 1,2 ), \u03c33 = minu\u2208V \u03c3md(P Hu,Hu 1,3 ) and \u03c0min = minu,i \u03c0 u i , then with probability \u2265 1 \u2212 \u03b4 over the training examples, with probability 0.9 over the random initializations in Algorithm 1, there exist permutation matrices {\u03a0u}u\u2208V such that for all u \u2208 V , \u2016Ou \u2212 (O\u0302u\u03a0u)\u2016 \u2264 if u is the root node, then,\n\u2016W\u0302u \u2212 (\u03a0u)>Wu\u2016 \u2264\n\u2016Q\u0302u \u2212Qu(\u03a0u,\u03a0u)\u2016 \u2264 Otherwise,\n\u2016W\u0302u \u2212Wu(\u03a0u,\u03a0\u03c0(u))\u2016 \u2264\n\u2016Q\u0302u \u2212Qu(\u03a0u,\u03a0u,\u03a0\u03c0(u))\u2016 \u2264\nWe emphasize that our algorithm recovers the initial probability and transition probability tensors up to permutations of hidden states in a globally consistent manner. In contrast to [20] where some hidden nodes do not have observations directly associated with them, in our setting, each hidden state has an associated observation, which makes recovery of permutations easier. How to perform parameter recovery in a THS-HMM with internal hidden states where each hidden tree node does not have an associated observation is an interesting question for future work."}, {"heading": "E Proofs", "text": "Throughout this section, we first assume a technical condition on the sample size. This will result in concentration of the projection and the symmetrization matrices.\nAssumption 3. Recall that D = |V |. The sample size N is large enough that\n(N, \u03b4) \u2264 min (minu\u2208V \u03c3m(Pu,u1,2 ) minu\u2208V \u03c3md(PH,H1,3 )\n16D ,\nminu\u2208V \u03c3m(P u,u 1,2 ) minu\u2208V \u03c3m(O u)\n4 \u221a m\n, minu\u2208V \u03c3md(P\nH,H 1,3 ) 3 minu\u2208V \u03c3m(O u)3\u03c0 3/2 min\n1536c1m ) = min (\u03c32\u03c33 16D , \u03c32\u03c31 4 \u221a m , \u03c0 3/2 min\u03c3 3 1\u03c3 3 3\n1536c1m\n) (2)\nWhere c1 > 0 is a constant given in Lemma 11, and \u03c31, \u03c32, \u03c33 and \u03c0min are defined in Theorem 2."}, {"heading": "E.1 Raw Moments Concentration", "text": "We start with standard concentration of raw moments, which uses the fact that all the (vectorized) raw moments can be viewed as a probability vector. Let u be a node in V , recall that H is the set of nodes along the path from root r to u.\nLet (N, \u03b4) = \u221a\n1+ln(10D/\u03b4) N . Define event\nE = {\nfor all u \u2208 V : \u2016P\u0302u,u1,2 \u2212 P u,u 1,2 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302H,u1,2 \u2212 P H,u 1,2 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302u,H2,3 \u2212 P u,H 2,3 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302H,H1,3 \u2212 P H,H 1,3 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302H,u,H1,2,3 \u2212 P H,u,H 1,2,3 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302u1 \u2212 Pu1 \u2016F \u2264 (N, \u03b4) \u2016P\u0302u,u1,2 \u2212 P u,u 1,2 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302u,\u03c0(u)1,1 \u2212 P u,\u03c0(u) 1,1 \u2016F \u2264 (N, \u03b4)\n\u2016P\u0302u,\u03c0(u),u2,2,1 \u2212 P u,\u03c0(u),u 2,2,1 \u2016F \u2264 (N, \u03b4) } Lemma 6 (Concentration of Raw Moments). P(E) \u2265 1\u2212 \u03b4.\nProof. Applying Proposition 19 in [18] along with union bound."}, {"heading": "E.2 Subspace Concentration", "text": "Next we state a useful lemma that says that conditioned on the event E, performing an SVD on the empirical version of Pu,u1,2 = E[xu1 \u2297 xu2 ] gives us a good approximation to the range of Ou. Recall that Uu is a matrix whose columns form an orthonormal basis of Ou, and define UH is \u2297v\u2208HUu. Also, recall for a matrix U with orthonormal columns, the projection matrix onto range(U) is UU>.\nLemma 7 (Subspace Concentration). Supposes N is large enough such that Assumption 3 holds. U\u0302u is the output of line 3 of Algorithm 1. Let u be a node in V , recall that H is the set of nodes along the path from root r to u. Then conditioned on event E, we have: (1) \u2016Uu(Uu)> \u2212 U\u0302u(U\u0302u)>\u2016 \u2264 2 (N,\u03b4)\n\u03c3m(P u,u 1,2 ) ."}, {"heading": "In particular,", "text": "\u2016Uu(Uu)> \u2212 U\u0302u(U\u0302u)>\u2016 \u2264 min( minu\u2208V \u03c3m(P\nH,H 1,3 )\n8D ,\nminu\u2208V \u03c3m(O u)\n2 \u221a m\n)\n(2)\n\u2016UH(UH)> \u2212 U\u0302H(U\u0302H)>\u2016 \u2264 minu\u2208V \u03c3m(P\nHu,Hu 1,3 )\n8 (3)\n\u03c3m((U\u0302 u)>Ou) \u2265 \u03c3m(O\nu)\n2\nProof. (1) \u03a6u, the matrix of principal angles between range(U\u0302u) and range(Uu), is such that\n\u2016 sin \u03a6u\u2016\n\u2264 (N, \u03b4) \u03c3m(P u,u 1,2 )\u2212 (N, \u03b4)\n\u2264 2 (N, \u03b4) \u03c3m(P u,u 1,2 )\n(3)\nwhere the first inequality is by Theorem 4, by takingA = Pu,u1,2 and A\u0303 = P\u0302 u,u 1,2 ; the second inequality from Assumption 3, which implies that (N, \u03b4) \u2264 \u03c3m(Pu,u1,2 )/2. Thus, by Equation (2) in Assumption 3,\n\u2016 sin \u03a6u\u2016 \u2264 min( minu\u2208V \u03c3m(P\nHu,Hu 1,3 )\n8D ,\nminu\u2208V \u03c3m(O u)\n2 \u221a m\n)\nThe result follows from the fact that\n\u2016 sin \u03a6u\u2016 = \u2016Uu(Uu)> \u2212 U\u0302u(U\u0302u)>\u2016\n(2) First we enumerate the nodes in Hu : Hu = {v1, . . . , vl}.\n\u2016UH(UH)> \u2212 U\u0302H(U\u0302H)>\u2016 \u2264 \u2016(Uv1(Uv1)> \u2212 U\u0302v1(U\u0302v1)>)\u2297 . . .\u2297 (Uvl(Uvl)>)|\u2016+ . . .+ \u2016(Uv1(Uv1)>)\u2297 . . .\u2297 (Uvl(Uvl)> \u2212 U\u0302vl(U\u0302vl)>)\u2016 \u2264 \u2016Uv1(Uv1)> \u2212 U\u0302v1(U\u0302v1)>\u2016+ . . .+ \u2016Uvl(Uvl)> \u2212 U\u0302vl(U\u0302vl)>\u2016\n\u2264 \u2211 v\u2208H 2 (N, \u03b4) \u03c3m(P v,v 1,2 )\n\u2264 minu\u2208V \u03c3m(P\nH,H 1,3 )\n8\nwhere the first inequality is by triangle inequality, the second inequality uses standard facts about Kronecker product (\u2016A \u2297 B\u2016 = \u2016A\u2016\u2016B\u2016), the third inequality is from Equation (3), the fourth inequality is from Equation (2).\n(3) By item (1) we know that\n\u2016Uu(Uu)> \u2212 U\u0302u(U\u0302u)>\u2016 \u2264 \u03c3m(Ou)/(2 \u221a m)\nHence\n\u2016Uu(Uu)>Ou \u2212 U\u0302u(U\u0302u)>Ou\u2016 \u2264 \u2016Uu(Uu)> \u2212 U\u0302u(U\u0302u)>\u2016\u2016Ou\u2016 \u2264 \u03c3m(Ou)/2\nwhere the second inequality is from the fact that Ou is a column stochastic matrix, which implies that \u2016Ou\u2016 \u2264 \u2016Ou\u2016F \u2264 \u221a m. Therefore by Theorem 3,\n\u03c3m((U\u0302 u)>Ou) = \u03c3m(U\u0302 u(U\u0302u)>Ou) \u2265 \u03c3m(Ou)/2"}, {"heading": "E.3 Symmetrized Moment Concentration", "text": "Lemma 8. Suppose we are given a set of matrices U\u0302u, u \u2208 V such that (U\u0302u)>Ou is invertible for all u \u2208 V . Moreover, assume the expected second order moments Pu,H2,3 , P H,H 1,3 , P u,H 2,1 , and third order moments PH,u,H1,2,3 are given. Consider the symmetrization matrices:\nS\u0303u1 = ((U\u0302 u)>Pu,H2,3 U\u0302 H)((U\u0302H)>PH,H1,3 U\u0302 H)\u22121\nS\u0303u3 = ((U\u0302 u)>Pu,H2,1 U\u0302 H)((U\u0302H)>PH,H3,1 U\u0302 H)\u22121\nand the ground truth symmetrized second order and third order cooccurence matrices be:\nMu2 = P H,u 1,2 (U\u0302 H(S\u0303u1 ) >, U\u0302u)\nMu3 = P H,u,H 1,2,3 (U\u0302 H(S\u0303u1 ) >, U\u0302u, U\u0302H S\u0303uT3 )\nThen, Mu2 = \u2211 i \u03c0ui ((U\u0302 u)>Ou)i \u2297 ((U\u0302u)>Ou)i\nMu3 = \u2211 i \u03c0ui ((U\u0302 u)>Ou)i \u2297 ((U\u0302u)>Ou)i \u2297 ((U\u0302u)>Ou)i\nProof. Recall that by Lemma 2\nOH1 = O Hdiag(\u03c1H)(TH)>diag(\u03c0H)\u22121\nwhere diag(\u03c1H)(TH)>diag(\u03c0H)\u22121 is invertible. Thus,\n(U\u0302H)>OH1 = (U\u0302 H)>OHdiag(\u03c1H)(TH)>diag(\u03c0H)\u22121\nThis shows that (U\u0302H)>OH1 is invertible. On the other hand,\nOH3 = O HTH\nwhere TH is invertible. Thus,\n(U\u0302H)>OH3 = (U\u0302 H)>OHTH\nThis shows that (U\u0302H)>OH3 is invertible.\nTherefore,\nS\u0303u1\n= ((U\u0302u)>Ou2 diag(\u03c0 H)OHT3 U\u0302 H)((U\u0302H)>OH1 diag(\u03c0 H)OHT3 U\u0302 H)\u22121 = ((U\u0302u)>Ou2 )((U\u0302 H)>OH1 ) \u22121\nLikewise,\nS\u0303u3\n= ((U\u0302u)>Ou2 diag(\u03c0 H)OHT1 U\u0302 H)((U\u0302H)>OH3 diag(\u03c0 H)OHT1 U\u0302 H)\u22121 = ((U\u0302u)>Ou2 )((U\u0302 H)>OH3 ) \u22121\nThen,\nMu2 = P H,u 1,2 (U H(S\u0303u1 ) >, U\u0302u) = \u2211\ni1,...,iD\n\u03c0Hi1,...,iD ((U\u0302 u)>Ou2 )i1,...,iD \u2297 ((U\u0302u)>Ou2 )i1,...,iD\n= \u2211\ni1,...,iD\n\u03c0Hi1,...,iD ((U\u0302 u)>Ou)iD \u2297 ((U\u0302u)>Ou)iD\n= \u2211 i \u03c0ui ((U\u0302 u)>Ou)i \u2297 ((U\u0302u)>Ou)i\nMu3 = P H,u,H 1,2,3 (U\u0302 H(S\u0303u1 ) >, U\u0302u, U\u0302H S\u0303uT3 ) = \u2211\ni1,...,iD\n\u03c0Hi1,...,iD ((U\u0302 u)>Ou2 )i1,...,iD \u2297 ((U\u0302u)>Ou2 )i1,...,iD \u2297 ((U\u0302u)>Ou2 )i1,...,iD\n= \u2211\ni1,...,iD\n\u03c0Hi1,...,iD ((U\u0302 u)>Ou)iD \u2297 ((U\u0302u)>Ou)iD \u2297 ((U\u0302u)>Ou)iD\n= \u2211 i \u03c0ui ((U\u0302 u)>Ou)i \u2297 ((U\u0302u)>Ou)i \u2297 ((U\u0302u)>Ou)i\nWe next establish a result that shows that the symmetrization matrices S\u0302u1 and S\u0302 u 3 obtained in Line 7 of Algorithm 1 concentrate to S\u0303u1 and S\u0303 u 3 defined in Lemma 8. Recall from Algorithm 1 that:\nS\u0302u1 = ((U\u0302 u)>P\u0302u,Hu2,3 U\u0302 Hu)((U\u0302Hu)>P\u0302Hu,Hu1,3 U\u0302 Hu)\u22121, S\u0302u3 = ((U\u0302 u)>P\u0302u,Hu2,1 U\u0302 Hu)((U\u0302Hu)>P\u0302Hu,Hu3,1 U\u0302 Hu)\u22121\nLemma 9. Suppose N is large enough that Assumption 3 holds. Recall S\u0302u1 and S\u0302u3 are the outputs of line 7 in Algorithm 1\n, and S\u0303u1 and S\u0303 u 3 are defined in Lemma 8.\nConditioned on event E, the following hold for all u \u2208 V .\n\u2016S\u0303u1 \u2212 S\u0302u1 \u2016, \u2016S\u0303u3 \u2212 S\u0302u3 \u2016 \u2264 10 (N, \u03b4)\n\u03c3md(P H,H 1,3 ) 2\n\u2016S\u0303u1 \u2016, \u2016S\u0302u1 \u2016, \u2016S\u0303u3 \u2016, \u2016S\u0302u3 \u2016 \u2264 4\n\u03c3md(P H,H 1,3 )\nProof. (1) We first show that \u03c3md((U\u0302H)>P\u0302 H,H 1,3 U\u0302 H) \u2265 3\u03c3md(P H,H 1,3 )/4, and \u03c3md((U\u0302 H)>PH,H1,3 U\u0302 H) \u2265 \u03c3md(P H,H 1,3 )/2. Under Assumption 3, by Item (2) of Lemma 7, we know that\n\u2016UH(UH)> \u2212 U\u0302H(U\u0302H)>\u2016 \u2264 min u\u2208V \u03c3md(P H,H 1,3 )/8 (4)\nAs a result,\n\u2016U\u0302H(U\u0302H)>PH,H1,3 U\u0302H(U\u0302H)> \u2212 P H,H 1,3 \u2016\n= \u2016U\u0302H(U\u0302H)>PH,H1,3 U\u0302H(U\u0302H)> \u2212 UH(UH)>P H,H 1,3 U H(UH)>\u2016 (5)\n\u2264 \u2016(U\u0302H(U\u0302H)> \u2212 UH(UH)>)PH,H1,3 U\u0302H(U\u0302H)>\u2016\n+\u2016UH(UH)>PH,H1,3 (U\u0302H(U\u0302H)> \u2212 UH(UH)>)\u2016\n\u2264 \u2016(U\u0302H(U\u0302H)> \u2212 UH(UH)>)\u2016\u2016PH,H1,3 \u2016\u2016U\u0302H(U\u0302H)>\u2016\n+\u2016UH(UH)>\u2016\u2016PH,H1,3 \u2016\u2016(U\u0302H(U\u0302H)> \u2212 UH(UH)>)\u2016\n\u2264 \u03c3m(PH,H1,3 )/8 + \u03c3m(P H,H 1,3 )/8\n\u2264 \u03c3m(PH,H1,3 )/4 (6) where the first inequality is by triangle inequality, in the second inequality we use the fact that \u2016A \u00b7 B\u2016 \u2264 \u2016A\u2016\u2016B\u2016, the third inequality is from the fact that \u2016PH,H1,3 \u2016 \u2264 \u2016P H,H 1,3 \u2016F \u2264 1, \u2016U\u0302H(U\u0302H)>\u2016 = 1, \u2016UH(UH)>\u2016 = 1 and Equation (4). Therefore,\n\u03c3md((U\u0302 H)>PH,H1,3 U\u0302 H)\n= \u03c3md(U\u0302 H(U\u0302H)>PH,H1,3 U\u0302 H(U\u0302H)>)\n\u2265 \u03c3md(P H,H 1,3 )\u2212 \u2016U\u0302H(U\u0302H)>P H,H 1,3 U\u0302 H(U\u0302H)> \u2212 PH,H1,3 \u2016\n\u2265 3\u03c3md(P H,H 1,3 )/4 (7)\nwhere the first inequality is by Theorem 3, the second inequality is by Equation 6. In the meantime,\n\u2016(U\u0302H)>PH,H1,3 U\u0302H \u2212 (U\u0302H)>P\u0302 H,H 1,3 U\u0302 H\u2016\n\u2264 \u2016PH,H1,3 \u2212 P\u0302 H,H 1,3 \u2016\n\u2264 (N, \u03b4) \u2264 \u03c3dm(P H,H 1,3 )/4 (8)\nwhere in the first inequality we use the fact that \u2016U\u0302H\u2016 = 1, the second inequality is by the fact that if E happens, \u2016PH,H1,3 \u2212 P\u0302 H,H 1,3 \u2016 \u2264 (N, \u03b4), the third inequality follows from Assumption 3.\nTherefore\n\u03c3md((U\u0302 H)>P\u0302H,H1,3 U\u0302 H)\n\u2265 \u03c3md((U\u0302H)>P H,H 1,3 U\u0302 H)\u2212 \u2016(U\u0302H)>PH,H1,3 U\u0302H \u2212 (U\u0302H)>P\u0302 H,H 1,3 U\u0302 H\u2016\n\u2265 \u03c3m(PH,H1,3 )/2 where the first inequality is from Theorem 3, the second inequality is from Equation (8).\nWe now have\n\u2016S\u0303u1 \u2212 S\u0302u1 \u2016 = \u2016((U\u0302u)>Pu,H2,3 U\u0302H)((U\u0302H)>P H,H 1,3 U\u0302 H)\u22121 \u2212 ((U\u0302u)>P\u0302u,H2,3 U\u0302H)((U\u0302H)>P\u0302 H,H 1,3 U\u0302 H)\u22121\u2016\n\u2264 \u2016((U\u0302u)>(Pu,H2,3 \u2212 P\u0302 u,H 2,3 )U\u0302 H)((U\u0302H)>PH,H1,3 U\u0302 H)\u22121\u2016\n+\u2016((U\u0302u)>P\u0302u,H2,3 U\u0302H)(((U\u0302H)>P H,H 1,3 U\u0302 H)\u22121 \u2212 ((U\u0302H)>P\u0302H,H1,3 U\u0302H)\u22121\u2016\n\u2264 \u2016U\u0302u(Pu,H2,3 \u2212 P\u0302 u,H 2,3 )U\u0302 H\u2016\u2016((U\u0302H)>PH,H1,3 U\u0302H)\u22121\u2016\n+\u2016(U\u0302u)>P\u0302u,H2,3 U\u0302H\u2016\u2016((U\u0302H)>P H,H 1,3 U\u0302 H)\u22121 \u2212 ((U\u0302H)>P\u0302H,H1,3 U\u0302H)\u22121\u2016\n\u2264 2 (N, \u03b4) \u03c3md(P H,H 1,3 ) + 8 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2\n\u2264 10 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2 (9)\nIn the derivation above, the first inequality uses triangle inequality and the second inequality repeatedly uses the fact that \u2016A \u00b7B\u2016 \u2264 \u2016A\u2016\u2016B\u2016. The third inequality is obtained by bounding each term individually as follows:\n\u2016(U\u0302u)>(Pu,H2,3 \u2212 P\u0302 u,H 2,3 )U\u0302 H\u2016 \u2264 \u2016Pu,H2,3 \u2212 P\u0302 u,H 2,3 \u2016 \u2264 \u2016P u,H 2,3 \u2212 P\u0302 u,H 2,3 \u2016F \u2264 (N, \u03b4)\n\u2016((U\u0302H)>PH,H1,3 U\u0302H)\u22121\u2016 = 1/\u03c3md((U\u0302H)>P H,H 1,3 U\u0302 H) \u2264 2/\u03c3md(P H,H 1,3 )\n\u2016(U\u0302u)>P\u0302u,H2,3 U\u0302H\u2016 \u2264 \u2016P\u0302 u,H 2,3 \u2016 \u2264 \u2016P\u0302 u,H 2,3 \u2016F \u2264 1\n\u2016((U\u0302H)>PH,H1,3 U\u0302H)\u22121 \u2212 ((U\u0302H)>P\u0302 H,H 1,3 U\u0302 H)\u22121\u2016\n\u2264 2\u2016(U\u0302H)>(PH,H1,3 \u2212 P\u0302 H,H 1,3 )U\u0302 H\u2016max(\u2016((U\u0302H)>P\u0302H,H1,3 U\u0302H)\u22121\u2016, \u2016((U\u0302H)>P H,H 1,3 U\u0302 H)\u22121\u2016)\n\u2264 8 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2\nwhere the last inequality follows from Theorem 5.\nThe bound of \u2016S\u0303u3 \u2212 S\u0302u3 \u2016 is handled similarly. (2) First,\n\u2016S\u0303u1 \u2016 \u2264 \u2016(U\u0302u)>P u,H 2,1 U\u0302 H\u2016\u2016((U\u0302H)>PH,H3,1 U\u0302H)\u22121\u2016 \u2264 2\n\u03c3md(P H,H 1,3 )\nwhere the first inequality is by the fact that \u2016A \u00b7 B\u2016 \u2264 \u2016A\u2016\u2016B\u2016, the second inequality is by Equation (7).\nMeanwhile, Assumption 3 implies (N, \u03b4) \u2264 \u03c3md(P H,H 1,3 )/5, therefore from Equation (9),\n\u2016Su1 \u2212 S\u0302u1 \u2016 \u2264 2\n\u03c3md(P H,H 1,3 )\nHence by triangle inequality,\n\u2016S\u0302u1 \u2016 \u2264 4\n\u03c3md(P H,H 1,3 )\nThe bounds of \u2016S\u0303u3 \u2016 and \u2016S\u0302u3 \u2016 are handled similarly.\nBuilt upon the previous two lemmas, we next provide a result regarding the concentration of symmetrized moments.\nLemma 10. Suppose N is large enough that Assumption 3 holds. Let u be a node in V . Then on the event E, the following hold.\n\u2016Mu2 \u2212 M\u0302u2 \u2016 \u2264 14 (N, \u03b4)\n\u03c3md(P H,H 1,3 ) 2\n\u2016Mu3 \u2212 M\u0302u3 \u2016 \u2264 96 (N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\nProof. (1) Define Pu = PH,u1,2 (U\u0302 H , U\u0302u) and P\u0302u = P\u0302H,u1,2 (U\u0302 H , U\u0302u). Then,\n\u2016Mu2 \u2212 M\u0302u2 \u2016 = \u2016Pu((S\u0303u1 )>, I)\u2212 P\u0302u((S\u0302u1 )>, I)\u2016 \u2264 \u2016(Pu \u2212 P\u0302u)((S\u0303u1 )>, I)\u2016+ \u2016P\u0302u((S\u0303u1 )> \u2212 (S\u0302u1 )>, I)\u2016 \u2264 \u2016Pu \u2212 P\u0302u\u2016\u2016S\u0303u1 \u2016+ \u2016P\u0302u\u2016\u2016S\u0303u1 \u2212 S\u0302u1 \u2016\n\u2264 4 (N, \u03b4) \u03c3md(P H,H 1,3 ) + 10 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2 \u2264 14 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2 (10)\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that \u2016M(A,B)\u2016 \u2264 \u2016M\u2016\u2016A\u2016\u2016B\u2016, the third inequality is from the fact that \u2016Pu \u2212 P\u0302u\u2016 \u2264 \u2016PH,u1,2 \u2212 P\u0302H,u1,2 \u2016 \u2264 \u2016P H,u 1,2 \u2212 P\u0302 H,u 1,2 \u2016F \u2264 (N, \u03b4) and \u2016P\u0302u\u2016 \u2264 \u2016P\u0302 H,u 1,2 \u2016 \u2264 \u2016P\u0302 H,u 1,2 \u2016F \u2264 1, and Lemma 9.\nAs a result,\n\u2016Mu2 \u2212 M\u0302u2 \u2016 = \u2016(Pu((S\u0303u1 )>, I)> + Pu((S\u0303u1 )>, I))/2\u2212 (P\u0302u(S\u0302u1 , I)> + P\u0302u(S\u0302u1 , I))/2\u2016 \u2264 \u2016Pu((S\u0303u1 )>, I)> \u2212 P\u0302u((S\u0302u1 )>, I)>\u2016/2 + \u2016Pu((S\u0303u1 )>, I)\u2212 P\u0302u((S\u0302u1 )>, I)\u2016/2\n\u2264 14 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2\nwhere the first inequality follows from triangle inequality, the second inequality is from Equation (10).\n(2) Define Tu = PH,u,H1,2,3 (U\u0302 H , U\u0302u, U\u0302H) and T\u0302u = P\u0302H,u,H1,2,3 (U\u0302 H , U\u0302u, U\u0302H). Then,\n\u2016Mu3 \u2212 M\u0302u3 \u2016 = \u2016Tu((S\u0303u1 )>, I, (S\u0303u3 )>)\u2212 T\u0302u((S\u0302u1 )>, I, (S\u0302u3 )>)\u2016 \u2264 \u2016Tu \u2212 T\u0302u\u2016\u2016S\u0303u1 \u2016\u2016S\u0303u3 \u2016+ \u2016T\u0302u\u2016\u2016S\u0303u1 \u2212 S\u0302u1 \u2016\u2016S\u0303u3 \u2016+ \u2016T\u0302u\u2016\u2016S\u0302u1 \u2016\u2016S\u0303u3 \u2212 S\u0302u3 \u2016\n\u2264 16 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2 + 10 (N, \u03b4) \u03c3md(P H,H 1,3 ) 2\n4\n\u03c3md(P H,H 1,3 )\n+ 4\n\u03c3md(P H,H 1,3 )\n10 (N, \u03b4)\n\u03c3md(P H,H 1,3 ) 2\n\u2264 96 (N, \u03b4) \u03c3md(P H,H 1,3 ) 3\nwhere the first inequality is from triangle inequality, and the fact that \u2016T (A,B,C)\u2016 \u2264 \u2016T\u2016\u2016A\u2016\u2016B\u2016\u2016C\u2016, the second inequality is by the fact that \u2016Tu \u2212 T\u0302u\u2016 \u2264 \u2016PH,u,H1,2,3 \u2212 P\u0302 H,u,H 1,2,3 \u2016 \u2264 \u2016PH,u,H1,2,3 \u2212 P\u0302 H,u,H 1,2,3 \u2016F \u2264 (N, \u03b4), \u2016T\u0302u\u2016 \u2264 \u2016P\u0302 H,u,H 1,2,3 \u2016 \u2264 1, and Lemma 9, the third inequality is by algebra."}, {"heading": "E.4 Accucary of Tensor Decomposition", "text": "Algorithm 3 A Procedure That Finds Symmetric Decomposition based on Second and Third Order Moments\n1: Input: number of components m, perturbed version M\u03022 and M\u03023 of matrix M2 and tensor M3 satisfying M2 = \u2211m i=1 \u03c0i\u03b8i \u2297 \u03b8i, M3 = \u2211m i=1 \u03c0i\u03b8i \u2297 \u03b8i \u2297 \u03b8i 2: Output: {\u03b8\u0302i}mi=1, estimate of {\u03b8i}mi=1 3: Whiten. Perform an SVD on M\u03022 = U\u0302D\u0302U\u0302>, and let W\u0302 = U\u0302mD\u0302 \u22121/2 m (where U\u0302m is matrix that\ncontains the firstm columns of U\u0302 , D\u0302m is the diagonal matrix with D\u0302\u2019s first m diagonal entries), let G\u0302 = M\u03023(W\u0302 , W\u0302 , W\u0302 ). 4: Decompose Tensor. Apply robust tensor power iteration algorithm in [1] with input G\u0302 to get {v\u03021, . . . , v\u0302m} 5: for i = 1, 2, . . . ,m do 6: Let Z\u0302i = 1T\u0302 (v\u0302i,v\u0302i,v\u0302i) . 7: Recover \u03b8\u0302i = (W\u0302>)\u2020v\u0302i Z\u0302i 8: end for\nIn this section, we introduce a lemma that is implicit in [1] regarding using orthogonal decomposition as a subprocedure for full rank symmetric tensor decomposition. (See Theorem 5.1 of [1].) For completeness, we include the proof here.\nLemma 11. There are universal constants c1, c2 such that the following holds. Suppose a matrix M2 and a tensor M3 has the following structure:\nM2 = m\u2211 i=1 \u03c0i\u03b8i \u2297 \u03b8i\nM3 = m\u2211 i=1 \u03c0i\u03b8i \u2297 \u03b8i \u2297 \u03b8i\nwhere \u03c0i > 0 for all i. And we are given their perturbed version M\u03022 and M\u03023, such that\n\u2016M\u03022 \u2212M2\u2016 \u2264 EP\n\u2016M\u03023 \u2212M3\u2016 \u2264 ET where\nEP \u2264 \u03c3m(\u0398)2\u03c0min/2 (11)\nc1( ET\n\u03c3m(\u0398)3 + EP \u03c3m(\u0398)2 ) 1\n\u03c0 3/2 min\n\u2264 1 m\n(12)\nwhere \u0398 = (\u03b81, . . . \u03b8m) and \u03c0min = mini \u03c0i. Then the outputs {\u03b8i}mi=1 of Algorithm 3 on input M\u03022 and M\u03023 satisfies the following. With appropriate setting of parameters (with respect to parameter \u03b7), with probability 1\u2212 \u03b7, there is a permutation \u03c3 : [m]\u2192 [m] such that\n\u2016\u03b8i \u2212 \u03b8\u0302\u03c3(i)\u2016 \u2264 c2 \u03c31(\u0398)\n\u03c02min ( EP \u03c3m(\u0398)2 + ET \u03c3m(\u0398)3 )\nProof. 1. We first put \u0398 into canonical forms by appropriate scaling of its columns. Let \u0398\u0303 = (\u03b8\u03031, . . . , \u03b8\u0303m) = \u0398diag(\u03c0) 1 2 , we have\nM2 = m\u2211 i=1 \u03b8\u0303i \u2297 \u03b8\u0303i\nM3 = m\u2211 i=1 1 \u221a \u03c0i \u03b8\u0303i \u2297 \u03b8\u0303i \u2297 \u03b8\u0303i\nRecall that W\u0302 is defined as U\u0302mD\u0302 \u2212 12 m , where M\u03022 = U\u0302D\u0302U\u0302>. Hence W\u0302>M\u03022W\u0302 = Im. Suppose that W\u0302>M2W\u0302 has the following eigendecomposition:\nW\u0302>M2W\u0302 = A\u039bA >\nThen let W = W\u0302A\u039b\u2212 1 2A>, W is one of the matrices such that W>M2W = Im. Define M = W>\u0398\u0303, M\u0302 = W\u0302>\u0398\u0303.\n2. If Equation (11) holds, then Ep \u2264 \u03c3m(\u0398)2\u03c0min/2 \u2264 \u03c3m(M2)/2, then we have the following:\n\u2016W\u2016, \u2016W\u0302\u2016 \u2264 2 \u03c3m(\u0398\u0303)\n\u2016W \u2020\u2016, \u2016W\u0302 \u2020\u2016 \u2264 3\u03c31(\u0398\u0303)\n\u2016W \u2020 \u2212 W\u0302 \u2020\u2016 \u2264 6\u03c31(\u0398\u0303) \u03c3m(\u0398\u0303)2 EP\n\u2016\u0398\u0398\u2020 \u2212WW \u2020\u2016 \u2264 4EP \u03c3m(\u0398\u0303)\n\u2016M\u2016, \u2016M\u0302\u2016 \u2264 2\n\u2016M \u2212 M\u0302\u2016 \u2264 EP \u03c3m(\u0398\u0303)2\n3. Define G = M3(W,W,W ) = \u2211 i 1\u221a \u03c0i Mi \u2297Mi \u2297Mi, and recall that G\u0302 = M\u03023(W\u0302 , W\u0302 , W\u0302 ). We\nhave the following perturbation bound for G\u0302. Define R to be diagnoal tensor \u2211 i\n1\u221a \u03c0i ei \u2297 ei \u2297 ei.\nNote that \u2016R\u2016 \u2264 1\u221a\u03c0min . Therefore,\n\u2016G\u2212 G\u0302\u2016 = \u2016M3(W,W,W )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )\u2016 \u2264 \u2016(M3 \u2212 M\u03023)(W\u0302 , W\u0302 , W\u0302 )\u2016+ \u2016M3(W \u2212 W\u0302 ,W,W )\u2016+ \u2016M3(W\u0302 ,W \u2212 W\u0302 ,W )\u2016+ \u2016M3(W\u0302 , W\u0302 ,W \u2212 W\u0302 )\u2016 = \u2016(M3 \u2212 M\u03023)(W\u0302 , W\u0302 , W\u0302 )\u2016+ \u2016R(M \u2212 M\u0302,M,M)\u2016+ \u2016R(M\u0302,M \u2212 M\u0302,M)\u2016+ \u2016R(M\u0302, M\u0302 ,M \u2212 M\u0302)\u2016 \u2264 \u2016M3 \u2212 M\u03023\u2016\u2016W\u20163 + \u2016R\u2016\u2016M \u2212 M\u0302\u2016\u2016M\u20162 + \u2016R\u2016\u2016M\u0302\u2016\u2016M\u2016\u2016M \u2212 M\u0302\u2016+ \u2016R\u2016\u2016M\u0302\u20162\u2016M \u2212 M\u0302\u2016\n\u2264 8ET \u03c3m(\u0398\u0303)3 + 12EP \u221a \u03c0min\u03c3m(\u0398\u0303)2 := E (13)\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that \u2016T (A,B,C)\u2016 \u2264 \u2016T\u2016\u2016A\u2016\u2016B\u2016\u2016C\u2016, the third inequality is from results of our step 2 and the fact that \u2016M\u03023 \u2212M3\u2016 \u2264 ET .\n4. If Equation (12) holds, then E \u2264 C1m \u2264 C1 mini \u03c0\n\u22121/2 i\nm for C1 required by Theorem 5.1 in [1]. Thus, applying robust tensor power algorithm in [1], with probability at least 1 \u2212 \u03b7, there exist a permutation \u03c3 : [m]\u2192 [m] such that\n\u2016Mi \u2212 v\u0302\u03c3(i)\u2016 \u2264 8 \u221a \u03c0iE (14)\n5. We conclude by providing the reconstruction error bound. For notational simplicity, assume \u03c3(\u00b7) is identity mapping. Define\nZi = 1\nM3(WMi,WMi,WMi) =\n1 G(Mi,Mi,Mi) = \u221a \u03c0i\nand recall that\nZ\u0302i = 1\nM\u03023(W\u0302 v\u0302i, W\u0302 v\u0302i, W\u0302 v\u0302i) =\n1\nG\u0302(v\u0302i, v\u0302i, v\u0302i)\nThe recovery formula is\n\u03b8\u0302i = (W\u0302>)\u2020v\u0302i\nZ\u0302i\nFirst, | 1Zi \u2212 1 Z\u0302i | can be bounded as follows:\n| 1 Zi \u2212 1 Z\u0302i |\n= |G(Mi,Mi,Mi)\u2212 G\u0302(v\u0302i, v\u0302i, v\u0302i)| \u2264 |(G\u2212 G\u0302)(v\u0302i, v\u0302i, v\u0302i)|+ |G(Mi \u2212 v\u0302i, v\u0302i, v\u0302i)|+ |G(Mi,Mi \u2212 v\u0302i, v\u0302i)|+ |G(Mi,Mi,Mi \u2212 v\u0302i)| \u2264 \u2016G\u2212 G\u0302\u2016\u2016Mi\u20163 + \u2016G\u2016\u2016Mi \u2212 v\u0302i\u2016\u2016v\u0302i\u20162 + \u2016G\u2016\u2016Mi \u2212 v\u0302i\u2016\u2016v\u0302i\u2016\u2016Mi\u2016+ \u2016G\u2016\u2016Mi\u20162\u2016Mi \u2212 v\u0302i\u2016 \u2264 E + 3 \u03c0i\u221a \u03c0min E \u2264 4 \u03c0i\u221a \u03c0min E\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that \u2016A \u00b7B\u2016 \u2264 \u2016A\u2016\u2016B\u2016, the third inequality is by Equation (13) in step 3 and Equation (14) in step 4, the fourth inequality is by algebra.\nThen the reconstruction error can be bounded as follows:\n\u2016\u03b8i \u2212 (W\u0302 \u2020)>v\u0302i\nZ\u0302i \u2016\n\u2264 \u2016\u03b8i \u2212 (W \u2020)>Mi\nZi \u2016+ \u2016W \u2020(Mi \u2212 v\u0302i) Zi \u2016+ \u2016 (W \u2020 \u2212 W\u0302 \u2020)v\u0302i Zi \u2016+ \u2016( 1 Zi \u2212 1 Z\u0302i )W\u0302 \u2020v\u0302i\u2016\n\u2264 \u2016\u0398\u0398\u2020 \u2212WW \u2020\u2016\u2016\u03b8i\u2016+ \u2016W \u2020\u2016 Zi \u2016Mi \u2212 v\u0302i\u2016+ \u2016W \u2020 \u2212 W\u0302 \u2020\u2016 Zi \u2016v\u0302i\u2016+ | 1 Zi \u2212 1 Z\u0302i |\u2016W\u0302 \u2020\u2016\u2016v\u0302i\u2016\n\u2264 \u2016\u0398\u0398\u2020 \u2212WW \u2020\u2016 \u03c31(\u0398\u0303)\u221a \u03c0min + \u2016W \u2020\u2016 Zi \u2016Mi \u2212 v\u0302i\u2016+ \u2016W \u2020 \u2212 W\u0302 \u2020\u2016 Zi + | 1 Zi \u2212 1 Z\u0302i |\u2016W\u0302 \u2020\u2016\n\u2264 4EP \u03c3m(\u0398\u0303)2 \u03c31(\u0398\u0303)\u221a \u03c0min + 24\u03c31(\u0398\u0303)E + 6\u03c31(\u0398\u0303) \u03c3m(\u0398\u0303)2 EP \u221a \u03c0i + 12 \u221a \u03c0min E \u221a \u03c0i\u03c31(\u0398\u0303)\n\u2264 46\u03c31(\u0398\u0303)\u221a \u03c0min ( 8ET \u03c3m(\u0398\u0303)3 + 12EP \u221a \u03c0min\u03c3m(\u0398\u0303)2 )\n\u2264 c2 \u03c31(\u0398)\n\u03c02min ( EP \u03c3m(\u0398)2 + ET \u03c3m(\u0398)3 )\nWher the first inequality is by triangle inequality, the second inequality we use the fact that \u2016A\u00b7B\u2016 \u2264 \u2016A\u2016\u2016B\u2016 and the fact that Mi = W>\u03b8i \u221a \u03c0i, Zi = \u221a \u03c0i, \u0398\u0398\u2020\u03b8i = \u03b8i, WW \u2020 = (W \u2020)>W>, the third inequality uses the fact that \u2016\u03b8i\u2016 = \u2016\u0398\u0303ei\u2016/ \u221a \u03c0min \u2264 \u03c31(\u0398\u0303)/ \u221a \u03c0min and \u2016v\u0302i\u2016 = 1, in the fourth inequality we use results in item 2 and item 4, the fifth inequality is from the definiton of E and algebra, in the sixth inequality we use the fact that \u03c3m(\u0398) \u2264 \u03c3m(\u0398\u0303)\u03c0\u22121/2min and letting c2 = 552.\nNow we apply the above lemma into our symmetrized cooccurence matrices M\u03022 and M\u03023. Corollary 2. Suppose N is large enough such that Assumption 3 holds. Then, on event E, with probability 0.9 over the randomization of D calls of Algorithm 3, for all u \u2208 V , the matrices \u0398\u0302u = (\u03b8\u0302u1 , . . . , \u03b8\u0302 u m) obtained at the end of line 9 are such that there exists a permutation matrix \u03a0 u,\n\u2016(U\u0302u)>Ou \u2212 \u0398\u0302u\u03a0u\u2016 \u2264 2c2 m\n(\u03c0umin) 2\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3\nProof. By Assumption 3, we first see that conditioned on event E, by Lemma 9, \u03c3m(U\u0302uTO u\n) \u2265 \u03c3m(O\nu)/2. Thus the conditions of Lemma 11 hold, by taking \u0398 = (U\u0302u)>Ou, \u03c0 = \u03c0u. We thus get that with probability greater than 1 \u2212 0.1/D over the randomness of Algorithm 1, there is a permutation matrix \u03a0u such that for all i = 1, 2, . . . ,m,\n\u2016(U\u0302u)>Oui \u2212 (\u0398\u0302u\u03a0u)i\u2016\n\u2264 c2 \u03c31((U\u0302\nu)>Ou)\n(\u03c0umin) 2\n( (N, \u03b4)\n\u03c3md(P H,H 1,3 ) 2\u03c3m(Ou)2 +\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3 )\n\u2264 2c2 \u221a m\n(\u03c0umin) 2\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3\nwhere the second inequality we use the fact that \u03c31((U\u0302u)>Ou) = \u2016(U\u0302u)>Ou\u2016 \u2264 \u2016Ou\u2016 \u2264 \u221a m, since Ou is a column stochastic matrix. Therefore,\n\u2016(U\u0302u)>Ou \u2212 (\u0398\u0302u\u03a0u)\u2016 \u2264 \u2016(U\u0302u)>Ou \u2212 (\u0398\u0302u\u03a0u)\u2016F\n\u2264 2c2 m\n(\u03c0umin) 2\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3 (15)\nWe conclude the proof by applying union bound over all u \u2208 V ."}, {"heading": "F Putting Everything Together \u2013 Proof of Theorem 2", "text": "Proof. (Of Theorem 2) (1) We first give the recovery accuracy of observation matrices. The final step of recovery is O\u0302u = U\u0302u\u0398\u0302u. Note that if N is at least C max( D 2\n\u03c322\u03c3 2 3 ln D\u03b4 , m \u03c321\u03c3 2 2 ln D\u03b4 , m2 \u03c361\u03c3 6 3\u03c0 3 min ln D\u03b4 ), then Assumption 3 holds, hence conditioned on event E, we have\n\u2016U\u0302u(U\u0302u)>Ou \u2212Ou\u2016 = \u2016U\u0302u(U\u0302u)>Ou \u2212 U\u0302u(Uu)>Ou\u2016 \u2264 \u2016U\u0302u(U\u0302u)> \u2212 U\u0302u(Uu)>\u2016\u2016Ou\u2016 \u2264 2 \u221a m (N, \u03b4)\n\u03c3m(P u,u 1,2 )\n(16)\nwhere the first inequality is by the fact that \u2016A \u00b7B\u2016 \u2264 \u2016A\u2016\u2016B\u2016, the second inequality follows from the fact that \u2016Ou\u2016 \u2264 \u221a m and item (1) of Lemma 7.\nMeanwhile, by Corollary 2, we have\n\u2016U\u0302u(U\u0302u)>Ou \u2212 U\u0302u\u0398\u0302u\u03a0u\u2016 \u2264 \u2016(U\u0302u)>Ou \u2212 \u0398\u0302u\u03a0u\u2016\n\u2264 2c2 m\n(\u03c0umin) 2\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3\nThe above two facts let us conclude that provided the size of sample N is at least C max( m\n\u03c322\u03c3 8 1\n2 ln D \u03b4 ,\nm2 \u03c363\u03c3 14 1 \u03c0 4 min 2 ln D \u03b4 ) (where we choose C large enough),\n\u2016Ou \u2212 O\u0302u\u03a0u\u2016 \u2264 \u2016U\u0302u(U\u0302u)>Ou \u2212Ou\u2016+ \u2016U\u0302u(U\u0302u)>Ou \u2212 U\u0302u\u0398\u0302u\u03a0u\u2016 \u2264 2 \u221a m (N, \u03b4)\n\u03c3m(P u,u 1,2 )\n+ 2c2 m\n(\u03c0umin) 2\n(N, \u03b4)\n\u03c3md(P H,H 1,3 ) 3\u03c3m(Ou)3\n\u2264 min v\u2208V \u03c3m(O v)4 /32 (17) \u2264\nwhere the first inequality is by triangle inequality, the second inequality is by Equations (15) and (16), the third inequality follows from the choice of N , in the last inequality we use the fact that \u03c3m(Ou) \u2264 1. Therefore by Equation (17) and Theorem 3,\n\u03c3m(O\u0302 u\u03a0u) \u2265 \u03c3m(Ou)\u2212min\nv\u2208V \u03c3m(O\nv)4 /32 \u2265 \u03c3m(Ou)/2 (18)\n(2) We now provide guarantees on the accuracy of transition probabilities and initial probabilities. In particular, we prove \u2016Q\u0302u \u2212Qu(\u03a0u,\u03a0u,\u03a0\u03c0(u))\u2016 \u2264 , the other three inequalities can be handled similarly. As we have already seen from Equation (17), for all u \u2208 V ,\n\u2016(Ou)T\u2020 \u2212 (O\u0302u\u03a0u)T\u2020\u2016 \u2264 2 max(\u2016(O\u0302u)\u2020\u20162, \u2016(\u03a0uT (Ou))\u2020\u20162)\u2016Ou \u2212 O\u0302u\u03a0u\u2016 \u2264 min\nv\u2208V \u03c3m(O\nv)2 /16\nwhere the first inequality is by Theorem 5, the second inequality uses the fact that \u2016(O\u0302u)\u2020\u2016 = 1/\u03c3m(O u), \u2016(O\u0302u\u03a0u)\u2020\u2016 = 1/\u03c3m(O\u0302u\u03a0u) and Equation (18).\nConditioned on event E, by the choice of N , it is also true that the cooccurence tensor P\u0302u,\u03c0(u),u2,2,1 is such that\n\u2016P\u0302u,\u03c0(u),u2,2,1 \u2212 P u,\u03c0(u),u 2,2,1 \u2016 \u2264 min\nv\u2208V \u03c3m(O\nv)3 /32 (19)\nTherefore,\n\u2016Qu \u2212 Q\u0302u(\u03a0u,\u03a0\u03c0(u),\u03a0u)\u2016 = \u2016Pu,\u03c0(u),u2,2,1 ((Ou)T\u2020, (O\u03c0(u))T\u2020, (Ou)T\u2020)\u2212 P\u0302 u,\u03c0(u),u 2,2,1 ((O\u0302 u\u03a0u)T\u2020, (O\u0302\u03c0(u)\u03a0\u03c0(u))T\u2020, (O\u0302u\u03a0u)T\u2020)\u2016\n\u2264 \u2016(Pu,\u03c0(u),u2,2,1 \u2212 P\u0302 u,\u03c0(u),u 2,2,1 )((O u\u03a0u)T\u2020, (O\u03c0(u)\u03a0\u03c0(u))T\u2020, (Ou\u03a0u)T\u2020)\u2016\n+\u2016P\u0302u,\u03c0(u),u2,2,1 ((Ou)T\u2020 \u2212 (O\u0302u\u03a0u)T\u2020, (O\u03c0(u))T\u2020, (Ou)T\u2020)\u2016\n+\u2016P\u0302u,\u03c0(u),u2,2,1 ((O\u0302u\u03a0u)T\u2020, (O\u03c0(u))T\u2020 \u2212 (O\u0302\u03c0(u)\u03a0\u03c0(u))T\u2020, (O\u0302u)T\u2020)\u2016\n+\u2016P\u0302u,\u03c0(u),u2,2,1 ((O\u0302u\u03a0u)T\u2020, (O\u0302\u03c0(u)\u03a0\u03c0(u))T\u2020, (Ou)T\u2020 \u2212 (O\u0302u\u03a0u)T\u2020)\u2016\n\u2264 \u2016(Pu,\u03c0(u),u2,2,1 \u2212 P\u0302 u,\u03c0(u),u 2,2,1 )\u2016max v\u2208V \u2016Ov\u2020\u20163 + \u2016P\u0302u,\u03c0(u),u2,2,1 \u2016 \u00b7 \u2016(Ou)T\u2020 \u2212 (O\u0302u\u03a0u)T\u2020\u2016(max v\u2208V \u2016Ov\u2020\u20162 +\nmax v\u2208V \u2016Ov\u2020\u2016max v\u2208V \u2016O\u0302v\u2020\u2016+ max v\u2208V \u2016O\u0302v\u2020\u20162)\n\u2264\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that \u2016T (A,B,C)\u2016 \u2264 \u2016T\u2016\u2016A\u2016\u2016B\u2016\u2016C\u2016, the third inequality is by Equations (19) and (17)."}, {"heading": "G Matrix Perturbation Lemmas", "text": "Theorem 3 (Weyl\u2019s Theorem). If A, E are matrices in Rm\u00d7n with m \u2265 n. Then,\n|\u03c3i(A+ E)\u2212 \u03c3i(A)| \u2264 \u2016E\u2016\nTheorem 4 (Wedin\u2019s Theorem). If A, E are matrices in Rm\u00d7n with m \u2265 n. Let A have singular value decomposition:  U>1U>2\nU>3\nA ( V1 V2 ) = ( \u03a31 00 \u03a32 0 0 )"}, {"heading": "Let A\u0303 = A+ E have the singular value decomposition: U\u0303>1U\u0303>2", "text": "U\u0303>3  A\u0303 ( V\u03031 V\u03032 ) =  \u03a3\u03031 00 \u03a3\u03032 0 0  If there is \u03b4 > 0, \u03b1 > 0 such that mini \u03c3i(\u03a3\u03031) \u2265 \u03b1+ \u03b4, maxi \u03c3i(\u03a32) \u2264 \u03b1, then\n\u2016 sin \u03a6\u2016 \u2264 \u2016E\u2016 \u03b4\nwhere \u03a6 is the matrix of principal angles between range(U1) and range(U\u03031).\nTheorem 5. If A, E are matrices in Rm\u00d7n with m \u2265 n, let A\u0303 = A+ E. Then,\n\u2016A\u0303\u2020 \u2212A\u2020\u2016 \u2264 2 max(\u2016A\u0303\u2020\u20162, \u2016A\u2020\u20162)\u2016E\u2016"}, {"heading": "H Compressed observation matrices produced by Spectral-Tree for eight", "text": "ENCODE cell types"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We develop a latent variable model and an efficient spectral algorithm motivated<lb>by the recent emergence of very large data sets of chromatin marks from multiple<lb>human cell types. A natural model for chromatin data in one cell type is a Hidden<lb>Markov Model (HMM); we model the relationship between multiple cell types by<lb>connecting their hidden states by a fixed tree of known structure.<lb>The main challenge with learning parameters of such models is that iterative meth-<lb>ods such as EM are very slow, while naive spectral methods result in time and<lb>space complexity exponential in the number of cell types. We exploit properties<lb>of the tree structure of the hidden states to provide spectral algorithms that are<lb>more computationally efficient for current biological datasets. We provide sample<lb>complexity bounds for our algorithm and evaluate it experimentally on biological<lb>data from nine human cell types. Finally, we show that beyond our specific model,<lb>some of our algorithmic ideas can be applied to other graphical models.", "creator": "LaTeX with hyperref package"}}}