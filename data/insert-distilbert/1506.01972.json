{"id": "1506.01972", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "abstract": "we revisit an important analytic class of composite stochastic minimization problems that often arises from empirical risk minimization settings, often such as ladder lasso, cypress ridge regression, and logistic regression.", "histories": [["v1", "Fri, 5 Jun 2015 17:00:43 GMT  (1648kb,D)", "http://arxiv.org/abs/1506.01972v1", null], ["v2", "Fri, 5 Feb 2016 20:55:39 GMT  (879kb,D)", "http://arxiv.org/abs/1506.01972v2", null], ["v3", "Fri, 27 May 2016 19:14:20 GMT  (1411kb,D)", "http://arxiv.org/abs/1506.01972v3", "improved writing and included more experiments in this version"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS math.OC stat.ML", "authors": ["zeyuan allen zhu", "yang yuan"], "accepted": true, "id": "1506.01972"}, "pdf": {"name": "1506.01972.pdf", "metadata": {"source": "CRF", "title": "UniVR: A Universal Variance Reduction Framework for Proximal Stochastic Gradient Method", "authors": ["Zeyuan Allen-Zhu", "Yang Yuan"], "emails": ["zeyuan@csail.mit.edu", "yangyuan@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we consider the following composite convex minimization problem:\nmin x\u2208Rd\n{ F (x) def = f(x) + \u03a8(x) def = 1\nn\nn\u2211\ni=1\nfi(x) + \u03a8(x) } . (1.1)\nHere, f(x) = 1n \u2211n i=1 fi(x) is the finite average of n smooth functions fi(x), and \u03a8(x) is a relatively simple (but possibly non-differentiable) convex function, sometimes referred to as the proximal function. The goal of this paper is to find an approximate minimizer x \u2208 Rd satisfying F (x) \u2264 F (x\u2217) + \u03b5, where x\u2217 is the minimizer of F (x). Problems of this form arise in many places in machine learning, statistics, and operations research. For instance, many regularized empirical risk minimization (ERM) problems naturally fall into this category. In such problems, we are given n training examples {(a1, `1), . . . (an, `n)}, where each ai \u2208 Rd is the feature vector of example i, and each `i \u2208 R is the label of example i. The following classification and regression problems are well-known examples of ERM:\n\u2022 Ridge Regression: fi(x) = 12 (\u3008ai, x\u3009 \u2212 `i)2 + \u03c32 \u2016x\u201622 and \u03a8(x) = 0. \u2022 Lasso: fi(x) = 12 (\u3008ai, x\u3009 \u2212 `i)2 and \u03a8(x) = \u03c3\u2016x\u20161. \u2022 Elastic Net: fi(x) = 12 (\u3008ai, x\u3009 \u2212 `i)2 + \u03c312 \u2016x\u201622 and \u03a8(x) = \u03c32\u2016x\u20161. \u2022 `1-Regularized Logistic Regression: fi(x) = log(1 + exp(\u2212`i\u3008ai, x\u3009)) and \u03a8(x) = \u03c3\u2016x\u20161.\nClassical full-gradient first-order methods often consider the following proximal steps:\nxt+1 \u2190 arg min y\u2208Rd { 1 2\u03b7 \u2016y \u2212 xt\u201622 + \u3008\u2207f(xt), y\u3009+ \u03a8(y) } .\nar X\niv :1\n50 6.\n01 97\n2v 1\n[ cs\n.L G\n] 5\nJ un\nAbove, \u03b7 is the length of the gradient step, and if the proximal function \u03a8(y) equals zero, then xt+1 simply reduces to the classical form of gradient descent: xt+1 \u2190 xt \u2212 \u03b7\u2207f(xt). We remark here that the computation of the full gradient \u2207f(\u00b7) is usually very expensive \u2014for instance, for ERM problems it requires one to have a full pass of the possibly gigabyte-sized input data.\nIn the recent two decades, many researchers have started to consider stochastic update rules instead:\nxt+1 \u2190 arg min y\u2208Rd { 1 2\u03b7 \u2016y \u2212 xt\u201622 + \u3008\u03bet, y\u3009+ \u03a8(y) } ,\nwhere \u03bet is a random vector satisfying E[\u03bet] = \u2207f(xt) and is referred to as the stochastic gradient. Given the \u201cfinite average\u201d structure f(x) = 1n \u2211n i=1 fi(x), a popular choice for the stochastic gradient is to let \u03bet = \u2207fi(xt) for some randomly selected index i \u2208 [n]. Methods based on this choice are known as stochastic gradient descent (SGD) methods. Since the computation of \u2207fi(x) is usually n times faster than that of \u2207f(x), SGD has been successfully applied to many large-scale learning problems, see for instance [1, 20].\nMore recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19]. In all of these cited results, the authors have, in one way or another, shown that SGD can converge much faster if one makes a better choice of the stochastic gradient \u03bet, so that its variance E[\u2016\u03bet \u2212\u2207f(xt)\u201622] reduces as t increases. This is known as the variance reduction technique. One particular way to reduce the variance can be described as follows (see for instance Johnson and Zhang [6]). Keep a snapshot x\u0303 = xt after every m stochastic update steps (where m is some parameter), and compute the full gradient\u2207f(x\u0303) only for such snapshots. Then, set \u03bet = \u2207fi(xt)\u2212 \u2207fi(x\u0303) +\u2207f(x\u0303) as the stochastic gradient. One can verify that, under this choice of \u03bet, it satisfies E[\u03bet] = \u2207f(xt) and limt\u2192\u221e E[\u2016\u03bet \u2212\u2207f(xt)\u201622] = 0. Although many variance-reduction based methods have been proposed, most of them only apply to Problem (1.1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19]. However, in many machine learning applications, F (x) is simply not strongly convex. This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.\nOne way to get around this issue is to add a dummy regularizer \u03bb2 \u2016x\u201622 to F (x), and then to apply any of the above strong-convexity methods. However, the weight of this regularizer, \u03bb, needs to be chosen before the algorithm starts. This adds a lot of difficulty when applying such methods to real life: (1) one needs to tune \u03bb by repeatedly executing the algorithm, and (2) the error of the algorithm does not converge to zero as time goes (in fact, it converges toO(\u03bb) so one needs to know the desired accuracy before the algorithm starts). As we shall demonstrate in the experimental section, adding the dummy regularizer hurts the performance of the algorithm as well.\nAnother possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer. These methods are the so-called anytime algorithms: they can be interrupted at any time, and the error of the produced solution tends to zero as the number of steps increases. While direct methods are much more convenient for practical use, in the big-data scenario, existing direct methods are much slower than indirect methods (i.e., methods via dummy regularization).\nMore specifically, if the desired accuracy is \u03b5 and the smoothness of each fi(x) is L, then the gradient complexities (i.e., the number of full gradient evaluations divided by n)1 of the best known direct and indirect methods are respectively\nO ( n+L \u03b5 ) and O ( (n+ L\u03b5 ) log 1 \u03b5 ) .\nThus, direct and indirectly methods are incomparable both on the theoretical and practical side. On the theoretical side, in terms of gradient complexity, indirect methods have less dependency on n and slightly more dependency on L, while direct methods have slightly less dependency on L and more dependency on n. Meanwhile, in practice, when n is usually dominating, indirect methods are faster but less convenient, while direct methods are slower but more convenient.\n1Throughout this paper, we will use gradient complexity as an effective measure of an algorithm\u2019s running time. Usually, the total running time of an algorithm is O(d) multiplied with its gradient complexity, because each\u2207fi(x) can be computed in O(d) time.\nAlgorithm 1 UniVR(x\u03c6,m0, S, \u03b7) 1: x\u03030 \u2190 x\u03c6, x10 \u2190 x\u03c6 2: for s\u2190 1 to S do 3: \u00b5\u0303s\u22121 \u2190 \u2207F (x\u0303s\u22121) 4: ms \u2190 2s \u00b7m0 5: for t\u2190 0 to ms \u2212 1 do 6: Pick i uniformly at random in {1, \u00b7 \u00b7 \u00b7 , n}. 7: \u03be \u2190 \u2207fi(xst )\u2212\u2207fi(x\u0303s\u22121) + \u00b5\u0303s\u22121 8: xst+1 = arg miny\u2208Rd { 1 2\u03b7\u2016xst \u2212 y\u20162 + \u03a8(y) + \u3008\u03be, y\u3009 }\n9: end for 10: x\u0303s \u2190 1ms \u2211ms t=1 x s t 11: xs+10 \u2190 xsms 12: end for 13: return x\u0303S .\nOur Result. In this paper, we propose UniVR, a new method that can solve the non-strongly convex case of Problem (1.1) directly. On the theoretical side, UniVR admits a gradient complexity of only O(n log 1\u03b5 + L \u03b5 ), outperforming both the best known direct and indirect methods. On the practical side, UniVR is a direct anytime method, which is convenient to use.\nWith minor changes of parameters, our algorithm also matches the best known running time for the strongly-convex case. Therefore, UniVR unifies the two important cases of composite stochastic convex optimization theory, and provides an universal framework for solving (1.1). Interestingly enough, experimental results suggest that UniVR is faster than the previous methods for both cases.\nFinally, in both cases, UniVR demands a memory storage ofO(d), matching the best known memory requirement of SVRG [6], and is much cheaper thanO(nd) as required by many others (either direct or indirect methods). Small memory requirement is a demanding feature for machine learning in big data: it is often unrealistic to request a memory storage of O(nd), which is equivalent to the size of the large-scale input."}, {"heading": "2 Algorithm Description", "text": "Throughout this paper, we use \u2016 \u00b7 \u2016 to denote the Euclidean norm. We assume that each fi(\u00b7) is convex, differentiable and L-smooth (or has L-Lipschitz continuous gradient):\n\u2016\u2207fi(x)\u2212\u2207fi(y)\u2016 \u2264 L\u2016x\u2212 y\u2016, \u2200x, y \u2208 Rd . In addition, we assume that \u03a8(\u00b7) is convex and lower semicontinuous. Our algorithm for the non-strongly convex case is presented in Algorithm 1. Given an initial vector x\u03c6, our algorithm is divided into S epochs. The s-th epoch consists of ms stochastic gradient steps (see Line 8 of UniVR), where ms doubles between every consecutive two epochs. This \u201cdoubling\u201d feature distinguishes our method from all of the cited variance-reduction based methods.\nWithin each epoch, similar to SVRG [6, 19], we compute the full gradient \u00b5\u0303s\u22121 = \u2207f(x\u0303s\u22121) where x\u0303s\u22121 is the average point of the previous epoch. We then use \u00b5\u0303s\u22121 to define the variance-reduced version of the stochastic gradient xst (see Line 7 of UniVR). Unlike SVRG, our starting vector x s 0 of each epoch is set to be the ending vector xs\u22121ms\u22121 of the previous epoch, rather than the average of the previous epoch. This difference turns out to be essential in order to obtain our improved running time, both in terms of theory (see the proof of Theorem B.2) and practice (see Section 5).\nWe prove in this paper that if m0 and S are positive integers and \u03b7 = 1/7L, then the output satisfies\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 O (F (x\u03c6)\u2212 F (x\u2217) 2S + L\u2016x\u2217 \u2212 x\u03c6\u20162 2Sm0 ) .\nIn addition, the gradient complexity of UniVR is O(n \u00b7 S + 2S \u00b7 m0). As a consequence, if we are given parameters \u0398,\u2206 satisfying \u2016x\u03c6 \u2212 x\u2217\u20162 \u2264 \u0398 and F (x\u03c6) \u2212 F (x\u2217) \u2264 \u2206, then by setting S = log(\u2206/\u03b5) and m0 = L\u0398/\u2206, we have E[F (x\u0303S)\u2212F (x\u2217)] \u2264 O(\u03b5), and the gradient complexity of UniVR is O ( n log \u2206\u03b5 + L\u0398 \u03b5 ) .\nThe Strongly-Convex Case. If f(\u00b7) is also assumed to be \u03c3-strongly convex, that is, f(y) \u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009+ \u03c3\n2 \u2016y \u2212 x\u20162, \u2200x, y \u2208 Rd ,\nour algorithm can be easily modified with running time matching the state of the arts.\nIn short, we need to choose the same length m = 7L/\u03c3 for all the epochs, as well as choose a weighted rather than a uniform average when computing x\u0303s. We defer the description and the analogous proof of this slightly different algorithm UniVRsc to the appendix. If we are given parameter \u2206 satisfying F (x\u03c6)\u2212 F (x\u2217) \u2264 \u2206, then the gradient complexity of UniVRsc is O (( n+ L\u03c3 ) log \u2206\u03b5 ) ."}, {"heading": "3 Related Work", "text": "If the full gradient is used at each step of the algorithm, a simple gradient descent algorithm converges in O(L/\u03b5) steps and therefore has a gradient complexity O(nL/\u03b5) (see for instance the textbook of Nesterov [10]). This has been improved to O(n \u221a L/\u03b5) using Nesterov\u2019s accelerated gradient method [9]. If f(\u00b7) is also \u03c3-strongly convex, the standard and accelerated versions of the two methods have gradient complexities O(nL/\u03c3 log(1/\u03b5)) and O(n \u221a L/\u03c3 log(1/\u03b5)) respectively. However, in the big-data scenario (i.e., with large n), such performances are often unsatisfactory.\nIn the past two decades, a growing amount of attention has been paid towards stochastic gradient descent (SGD) algorithms. When the use of the full gradient \u2207f(x) is simply replaced with a stochastic gradient \u03bet = \u2207fi(xt), SGD achieves a convergence rate of O(1/\u03b52) [20, 22]. Later, a faster O(1/\u03b5) convergence rate was discovered for functions that are strongly convex [5, 14]. Both of these convergence rates turn out to be quite inefficient when we need a more accurate solution.\nIn order to improve SGD, in the past three years, several attempts have been made with the idea of (explicitly or implicitly) reducing the variance of the stochastic gradient. This category of results is summarized in Table 1 and discussed below.\nThe first published method that reduces the variance and overcomes the previous barrier of SGD methods is due to Schmidt, Le Roux, and Bach [12]. Their proposed SAG method selects a random index it \u2208 [n] for each iteration t, and adopts the choice \u03bet = 1n \u2211n j=1\u2207fj(yj), where yj = xt\u2032 and t\u2032 \u2264 t is essentially the largest index satisfying it\u2032 = j. Using the idea that \u03bet becomes closer to the actual gradient \u2207f(xt) when t increases, SAG obtains an O(log(1/\u03b5)) convergence (i.e., linear convergence) for strongly convex and smooth objectives, comparing to the O(1/\u03b5) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].\nThis O(log(1/\u03b5)) convergence has also been obtained by several concurrent or subsequent works. For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined \u03bet to be of a form slightly different from SAG. The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of \u201cepochs\u201d and defined \u03bet = \u2207if(xt) \u2212 \u2207if(xt) + \u2207f(x\u0303) like we do in this paper. The algorithm SDCA [16] has also been discovered to be intrinsically performing some \u201cvariance reduction\u201d procedure [2, 6, 13].\nAmong the variance-reduction algorithms mentioned above, only SAG, MISO, and SAGA can provide theoretical guarantees for directly solving non-strongly convex objectives (i.e., without adding a dummy regularizer). The best gradient complexity for direct methods before our work is O(n+L\u03b5 ) due to SAG and SAGA. On the other hand, if one uses indirect methods, the best gradient complexity is O ( (n+ L\u03b5 ) log 1 \u03b5 ) , where the asymptotic dependence on \u03b5 is weakened to log(1/\u03b5)\u03b5 .\nFinally, to stay in the general stochastic setting, in this paper we work directly with smooth functions fi(x), rather than the more structured fi(x) def = \u03c6i(\u3008x, ai\u3009). In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/\u03b5, \u221a nL/\u03b5 }) log 1\u03b5 ) . However, this class of methods require one to work with the dual of the objective, cannot be directly applied to the non-strongly convex case, and run only faster than the primal-only methods when n < \u221a L/\u03b5."}, {"heading": "4 Analysis for the Non-Strongly Convex Case", "text": "For each outer iteration s \u2208 [S] and inner iteration t \u2208 {0, 1, . . . ,ms\u22121} of UniVR, we denote by ist the selected random index i \u2208 [n] and \u03best the stochastic gradient \u03be = \u2207fist (xst )\u2212\u2207fist (x\u0303s\u22121)+\u00b5\u0303s\u22121.\naFollowing the tradition of machine learning literatures, we have assumed in this column that the initial objective distance to the minima, F (x\u03c6)\u2212 F (x\u2217), is a constant for a clean comparison. bFollowing the tradition of machine learning literatures, we have assumed in this column that for the initial vector x\u03c6, both F (x\u03c6)\u2212 F (x\u2217) and \u2016x\u03c6 \u2212 x\u2217\u20162 are constants for a clean comparison. cThis can be reduced to O(d+ n) if T , the total number of iterations, is specified beforehand.\nThen, using the convexity and smoothness of our objective, as well as the definition of our stochastic gradient step, we obtain the following lemma:\nLemma 4.1. For every u \u2208 Rd and t \u2208 {0, 1, . . . ,ms \u2212 1}, fixing xst and letting i = ist be the random variable, we have\nEist [ F (xst+1)\u2212 F (u) ] \u2264 Eist [ \u03b7 2(1\u2212 \u03b7L)\u2016\u03be s t \u2212\u2207f(xst )\u20162 + \u2016u\u2212 xst\u20162 \u2212 \u2016u\u2212 xst+1\u20162 2\u03b7 ] .\nProof. We first upper bound the left hand side: Eist [ F (xst+1)\u2212 F (u) ] = Eist [ f(xst+1)\u2212 f(u) + \u03a8(xst+1)\u2212\u03a8(u) ]\n\u00ac \u2264 Eist [ f(xst ) + \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+ L2 \u2016xst \u2212 xst+1\u20162 \u2212 f(u) + \u03a8(xst+1)\u2212\u03a8(u) ]\n \u2264 Eist [ \u3008\u2207f(xst ), xst \u2212 u\u3009+ \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+ L2 \u2016xst \u2212 xst+1\u20162 + \u03a8(xst+1)\u2212\u03a8(u) ]\n\u00ae = Eist [ \u3008\u03best , xst \u2212 u\u3009+ \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+ L2 \u2016xst \u2212 xst+1\u20162 + \u03a8(xst+1)\u2212\u03a8(u) ] . (4.1) Above, inequalities \u00ac and  are respectively due to the smoothness and convexity of f(\u00b7), and \u00ae is because Eist [\u03be s t ] = \u2207f(xst ). Next, using the definition of xst+1 we have \u3008\u03best , xst \u2212 u\u3009+ \u03a8(xst+1)\u2212\u03a8(u) = \u3008\u03best , xst \u2212 xst+1\u3009+ \u3008\u03best , xst+1 \u2212 u\u3009+ \u03a8(xst+1)\u2212\u03a8(u) \u00af \u2264 \u3008\u03best , xst \u2212 xst+1\u3009+ \u3008\u2212 1\n\u03b7 (xst+1 \u2212 xst ), xst+1 \u2212 u\u3009\n\u00b0 = \u3008\u03best , xst \u2212 xst+1\u3009+ \u2016u\u2212 xst\u20162 2\u03b7 \u2212 \u2016u\u2212 x s t+1\u20162 2\u03b7 \u2212 \u2016x s t+1 \u2212 xst\u20162 2\u03b7 .\nAbove, inequality \u00af holds for the following reason. Recall that the minimality of xst+1 = arg miny\u2208Rd{ 12\u03b7\u2016y \u2212 xst\u20162 + \u03a8(y) + \u3008\u03best , y\u3009} implies the existence of some subgradient g \u2208 \u2202\u03a8(xst+1) which satisfies 1 \u03b7 (x s t+1 \u2212 xst ) + \u03best + g = 0. Combining this with \u03a8(u) \u2212 \u03a8(xst+1) \u2265 \u3008g, u \u2212 xst+1\u3009, which is due to the convexity of \u03a8(\u00b7), we immediately have \u03a8(u) \u2212 \u03a8(xst+1) + \u3008 1\u03b7 (xst+1 \u2212 xst ) + \u03best , u \u2212 xst+1\u3009 \u2265 \u3008 1\u03b7 (xst+1 \u2212 xst ) + \u03best + g, u \u2212 xst+1\u3009 = 0. This gives inequality \u00af. In addition, \u00b0 can be verified by expanding the Euclidean norms.\nCombining the above two inequalities, we have Eist [ F (xst+1)\u2212 F (u) ]\n\u2264 Eist [ \u3008\u03best \u2212\u2207f(xst ), xst \u2212 xst+1\u3009 \u2212 1\u2212 \u03b7L 2\u03b7 \u2016xst \u2212 xst+1\u20162 + \u2016u\u2212 xst\u20162 \u2212 \u2016u\u2212 xst+1\u20162 2\u03b7 ]\n\u00b1 \u2264 Eist [ \u03b7 2(1\u2212 \u03b7L)\u2016\u03be s t \u2212\u2207f(xst )\u20162 + \u2016u\u2212 xst\u20162 \u2212 \u2016u\u2212 xst+1\u20162 2\u03b7 ] .\nAbove, \u00b1 is by the Cauchy-Schwarz inequality. The next lemma is classical and analogous to most of the variance reduction literatures (cf. [2, 6, 19]). We include its proof in Appendix A for the sake of completeness. Lemma 4.2. Eist [ \u2016\u03best \u2212\u2207f(xst )\u20162 ] \u2264 4L \u00b7 ( F (xst )\u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217) )\nWe are now ready to state the main theorem for the convergence of UniVR:\nTheorem 4.3. UniVR(x\u03c6,m0, S, \u03b7) satisfies if m0 and S are positive integers and \u03b7 = 1/7L, then\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 O (F (x\u03c6)\u2212 F (x\u2217) 2S + L\u2016x\u2217 \u2212 x\u03c6\u20162 2Sm0 ) . (4.2)\nIn addition, UniVR has a gradient complexity of O(S \u00b7 n+ 2S \u00b7m0). Proof. Combining Lemma 4.1 with u = x\u2217 and Lemma 4.2, we have\nEist [ F (xst+1)\u2212F (x\u2217) ] \u2264 2\u03b7L (1\u2212 \u03b7L) ( F (xst )\u2212F (x\u2217)+F (x\u0303s\u22121)\u2212F (x\u2217) ) + \u2016x\u2217 \u2212 xst\u20162 \u2212 Eist \u2016x\u2217 \u2212 xst+1\u20162 2\u03b7 .\nChoosing \u03b7 = 1/7L in the above inequality, summing it up over t = 0, 1, . . . ,ms \u2212 1, and dividing both sides by ms, we arrive at E [ms\u22121\u2211\nt=0\nF (xst+1)\nms \u2212F (x\u2217)\n] \u2264 E [1 3 (ms\u22121\u2211\nt=0\nF (xst )\nms \u2212F (x\u2217)+F (x\u0303s\u22121)\u2212F (x\u2217)\n) + \u2016x\u2217 \u2212 xs0\u20162 \u2212 \u2016x\u2217 \u2212 xsms\u20162\n2\u03b7 \u00b7ms\n] .\nAfter rearranging, this yields\n2E [ms\u22121\u2211\nt=0\nF (xst+1)\nms \u2212 F (x\u2217)\n] \u2264 E [ (F (xs0)\u2212 F (x\u2217))\u2212 (F (xsms)\u2212 F (x\u2217)) ms + F (x\u0303s\u22121)\u2212 F (x\u2217)\n+ \u2016x\u2217 \u2212 xs0\u20162 \u2212 \u2016x\u2217 \u2212 xsms\u20162\n2\u03b7/3 \u00b7ms\n] .\nNext, using the fact that F (x\u0303s) \u2264 \u2211ms\u22121t=0 F (xst+1) ms due to the convexity of F and the definition x\u0303s = \u2211ms\u22121 t=0 xst+1 ms , as well as the choice xsms = x s+1 0 , we rewrite the above inequality as\n2E [ F (x\u0303s)\u2212 F (x\u2217) ] \u2264 E [ (F (xs0)\u2212 F (x\u2217))\u2212 (F (xs+10 )\u2212 F (x\u2217)) ms + F (x\u0303s\u22121)\u2212 F (x\u2217) )\n+ \u2016x\u2217 \u2212 xs0\u20162 \u2212 \u2016x\u2217 \u2212 xs+10 \u20162\n2\u03b7/3 \u00b7ms\n] .\nAfter rearranging and using the fact ms = 2ms\u22121, we conclude that\n2E [ F (x\u0303s)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 xs+10 \u20162 4\u03b7/3 \u00b7ms + F (xs+10 )\u2212 F (x\u2217) 2ms ]\n\u2264 E [ F (x\u0303s\u22121)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 xs0\u20162 4\u03b7/3 \u00b7ms\u22121 + F (xs0)\u2212 F (x\u2217) 2ms\u22121 ] .\nIn sum, after telescoping for s = 1, 2, . . . , S, we have2\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 2\u2212S \u00b7 ( F (x\u03030)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 x10\u20162 4\u03b7/3 \u00b7m0 + F (x10)\u2212 F (x\u2217) 2m0 )\n2We can perform telescoping because we set our starting vector xs+10 of each epoch to equal the ending vector xsms of the previous epoch. This is different from SVRG, which chooses the average of the previous epoch as the starting vector. This difference is also beneficial in practice (see Section 5).\n\u2264 F (x \u03c6)\u2212 F (x\u2217) 2S\u22121 + \u2016x\u2217 \u2212 x\u03c6\u20162 2S \u00b7 4\u03b7m03 .\nThis finishes the proof of (4.2) due to the choice of \u03b7 = 1/7L. Finally, UniVR computes S times the full gradient \u2207f(\u00b7), and \u2211Ss=1ms = O(2Sm0) times the gradient \u2207fi(\u00b7). This gives a total gradient complexity O(S \u00b7 n+ 2S \u00b7m0). Corollary 4.4. If we are given some parameters \u0398,\u2206 \u2208 R+ satisfying \u2016x\u03c6 \u2212 x\u2217\u20162 \u2264 \u0398 and F (x\u03c6)\u2212 F (x\u2217) \u2264 \u2206, then by setting S = log(\u2206/\u03b5), m0 = L\u0398/\u2206, and \u03b7 = 1/7L, we have\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 O(\u03b5) , and the gradient complexity of UniVR is O ( n log ( \u2206 \u03b5 ) + L\u0398\u03b5 ) ."}, {"heading": "5 Experiment", "text": "In this section, we confirm our theoretical findings using three real-life datasets: (1) the Adult dataset (32, 561 examples and 123 features), (2) the Covtype dataset (581, 012 examples and 54 features), and (3) the 2nd class of the MNIST dataset (60, 000 examples and 780 features) [4]. Following [17], we have normalized each feature vector to have Euclidean norm 1.\nWe perform 3 classification tasks: Lasso, Ridge Regression (RR), and `1-Regularized Logistic Regression (LR). As described in the introduction, Lasso and LR do not admit strongly convex objectives, while RR has a strongly convex objective. We have picked the weight \u03c3 of, either the regularizer \u03c32 \u2016x\u201622 for RR or the regularizer \u03c3\u2016x\u201621 for Lasso and LR, as follows: \u2022 Adult: \u03c3 = 0.001, 0.01 and 0.001 for Lasso, LR, and RR respectively. \u2022 Covtype: \u03c3 = 0.008, 0.005 and 0.006 for Lasso, LR, and RR respectively. \u2022 MNIST: \u03c3 = 0.0005, 0.003 and 0.00005 for Lasso, LR, and RR respectively.\nWe have implemented the following algorithms: \u2022 UniVR with initial epoch size m0 = n/4 and step size \u03b7 = 0.3 (except for MNIST-Lasso and MNIST-LR, in which we choose \u03b7 = 1 and \u03b7 = 5 respectively). \u2022 SVRG [6, 19] with (their suggested) epoch size m = 2n and step size \u03b7 = 0.3 (except for MNIST-Lasso and MNIST-LR, in which we choose \u03b7 = 1 and \u03b7 = 5 respectively). Recall that in theory, SVRG is not designed for non-strongly convex objectives, and F (\u00b7) needs to be added by a dummy regularizer for Lasso and LR. However, in our experiment, we noticed that the performances of SVRG with and without dummy regularizers are pretty similar. Thus, we have neglected the regularized version of SVRG for a clean comparison. \u2022 SAG [12] and SAGA [2] both with step size 0.1.3 \u2022 SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).\nFor Option IV, we use step sizes 0.5 or 1 for all the experiments, which turn out to be the best after hand tuning. Since both these versions of SDCA work only with strongly convex objectives, a dummy regularizer needs to be introduced for Lasso and LR. We have therefore tuned the best regularizer weight for (at most) 16 different values of \u03b5, and connected the points together when we plot the performance curves for SDCA in Figure 1.\nPerformance Comparison. It is clear from Figure 1 that UniVR outperforms all other methods we have considered in this paper. Interestingly enough, although SVRG is not designed for non-strongly convex objectives, it outperforms SAG and SAGA, and runs only twice slower than UniVR.\nFinally, indirect methods via dummy regularization (i.e., SDCA) perform poor. In addition to the large amount of parameter tuning effort of the dummy regularizer that is not reflected in Figure 1, for small \u03b5, since the weight of the added dummy regularizer needs to be roughly proportional to \u03b5, the accuracy of such a method can hardly go below 10\u22129 for Lasso or LR.\nEffectiveness of Our New Techniques. There are two main differences that distinguish UniVR from the previous works. First, we double the epoch length between every consecutive two epochs; and second, our starting vector of each epoch is set to be the ending vector of the previous one.\n3In our experiment we find out that \u03b7 = 0.1 is a good choice for both SAG and SAGA for all the datasets.\nLet us first see what if in UniVR, the starting vector is set to be the average vector of the previous epoch (rather than the ending vector like in SVRG). We call this algorithm UniVR-broken. It is clear from Figure 2(a) that this algorithm is much slower than UniVR, and the choice of the average vector is not necessary: it slows down the algorithm by a constant factor.\nWe also notice that the doubling technique is very effective. For instance, in Figure 2(a), between the 40th and 72th pass of the dataset, our UniVR has stayed inside the same epoch (of length 32n) without ever recomputing the full gradient. Within this epoch, the objective decrease rate remains sharp, and thus a short epoch length (such as 2n in SVRG) is not really necessary. Our doubling technique is also robust against the choice of m0, as illustrated in Figure 2(b)."}, {"heading": "A Proof of Lemma 4.2", "text": "Lemma 4.2. Eist [ \u2016\u03best \u2212\u2207f(xst )\u20162 ] \u2264 4L \u00b7 ( F (xst )\u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217) )\nProof. The proof of this lemma is classical and is analogous to most of the variance reduction literatures (cf. [2, 6, 19]). Indeed,\nEist [ \u2016\u03best \u2212\u2207f(xst )\u20162 ] = Eist [\u2225\u2225(\u2207fist (xst )\u2212\u2207fist (x\u0303s\u22121) ) \u2212 ( \u2207f(xst )\u2212\u2207f(x\u0303s\u22121) )\u2225\u22252]\n\u00ac \u2264 Eist [\u2225\u2225\u2207fist (xst )\u2212\u2207fist (x\u0303s\u22121) \u2225\u22252] = Eist [\u2225\u2225(\u2207fist (xst )\u2212\u2207fist (x\u2217) ) \u2212 ( \u2207fist (x\u0303s\u22121)\u2212\u2207fist (x\u2217) )\u2225\u22252]\n \u2264 2 \u00b7 Eist [\u2225\u2225\u2207fist (xst )\u2212\u2207fist (x\u2217) \u2225\u22252 + \u2225\u2225\u2207fist (x\u0303s\u22121)\u2212\u2207fist (x\u2217) \u2225\u22252] .\nAbove, \u00ac is because for any random vector \u03b6 \u2208 Rd, it holds that E\u2016\u03b6 \u2212 E\u03b6\u20162 = E\u2016\u03b6\u20162 \u2212 \u2016E\u03b6\u20162, and  is because for any two vectors a, b \u2208 Rd, it holds that \u2016a\u2212 b\u20162 \u2264 2\u2016a\u20162 + 2\u2016b\u20162. Next, the classical smoothness assumption on a function fi yields (see for instance [10, Theorem 2.1.5]) \u2016\u2207fi(x) \u2212 \u2207fi(x\u2217)\u20162 \u2264 2L [ fi(x) \u2212 fi(x\u2217) \u2212 \u3008\u2207fi(x\u2217), x \u2212 x\u2217)\u3009. Plugging this into the above inequality, we have\nEist [ \u2016\u03best \u2212\u2207f(xst )\u20162 ]\n\u2264 4L \u00b7 Eist [ fist (x s t )\u2212 fist (x\u2217)\u2212 \u3008\u2207fist (x\u2217), xst \u2212 x\u2217\u3009+ fist (x\u0303s\u22121)\u2212 fist (x\u2217)\u2212 \u3008\u2207fist (x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009 ] = 4L \u00b7 ( f(xst )\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), xst \u2212 x\u2217\u3009+ f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009 ) = 4L \u00b7 ( f(xst )\u2212 f(x\u2217) + \u3008g\u2217, xst \u2212 x\u2217\u3009+ f(x\u0303s\u22121)\u2212 f(x\u2217) + \u3008g\u2217, x\u0303s\u22121 \u2212 x\u2217\u3009 ) \u2264 4L \u00b7 ( f(xst )\u2212 f(x\u2217) + \u03a8(xst )\u2212\u03a8(x\u2217) + f(x\u0303s\u22121)\u2212 f(x\u2217) + \u03a8(x\u0303s\u22121)\u2212\u03a8(x\u2217) ) = 4L \u00b7 ( F (xst )\u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217) ) .\nAbove, g\u2217 \u2208 \u2202\u03a8(x\u2217) is the subgradient of \u03a8 at x\u2217 that satisfies\u2207f(x\u2217) + g\u2217 = 0."}, {"heading": "B Analysis for the Strongly Convex Case", "text": "In this section, we show that if f(\u00b7) is also assumed to be \u03c3-strongly convex, our algorithm UniVR can be easily modified to have a gradient complexity matching the state of the arts for the stronglyconvex case.\nOur algorithm UniVRsc for the strongly convex case is presented in Algorithm 2. Given an initial vector x\u03c6, our algorithm is again divided into S epochs, where each epoch is of length m for the same m. Unlike UniVR, we choose a weighted average x\u0303s \u2190 1\u2211m\nt=1(1\u2212\u03c3\u03b7)\u2212t \u2211m t=1 xst (1\u2212\u03c3\u03b7)t rather\nthan a uniform average x\u0303s \u2190 1m \u2211m t=1 x s t in each epoch.\nAs in Section 4, for each outer iteration s \u2208 [S] and inner iteration t \u2208 {0, 1, . . . ,m \u2212 1} of UniVRsc, we denote by ist the selected random index i \u2208 [n] and \u03best the stochastic gradient \u03be = \u2207fist (xst )\u2212\u2207fist (x\u0303s\u22121) + \u00b5\u0303s\u22121. Then, the following lemma is a counterpart of Lemma 4.1 where the only difference is the use of the strong convexity parameter \u03c3:\nLemma B.1. For every u \u2208 Rd and t \u2208 {0, 1, . . . ,m \u2212 1}, fixing xst and letting i = ist be the random variable, we have\nEist [ F (xst+1)\u2212F (u) ] \u2264 Eist [ \u03b7 2(1\u2212 \u03b7L)\u2016\u03be s t \u2212\u2207f(xst )\u20162 + (1\u2212 \u03c3\u03b7)\u2016u\u2212 xst\u20162 \u2212 \u2016u\u2212 xst+1\u20162 2\u03b7 ] .\nProof. We first upper bound the left hand side using the strong convexity and smoothness of f(\u00b7): Eist [ F (xst+1)\u2212 F (u) ]\n= Eist [ f(xst+1)\u2212 f(u) + \u03a8(xst+1)\u2212\u03a8(u) ]\nAlgorithm 2 UniVRsc(x\u03c6,m, S, \u03b7) 1: x\u03030 \u2190 x\u03c6, x10 \u2190 x\u03c6 2: for s\u2190 1 to S do 3: \u00b5\u0303s\u22121 \u2190 \u2207F (x\u0303s\u22121) 4: for t\u2190 0 to m\u2212 1 do 5: Pick i uniformly at random in {1, \u00b7 \u00b7 \u00b7 , n}. 6: \u03be \u2190 \u2207fi(xst )\u2212\u2207fi(x\u0303s\u22121) + \u00b5\u0303s\u22121 7: xst+1 = arg miny\u2208Rd { 1 2\u03b7\u2016xst \u2212 y\u20162 + \u03a8(y) + \u3008\u03be, y\u3009 }\n8: end for 9: x\u0303s \u2190 1\u2211m t=1(1\u2212\u03c3\u03b7)\u2212t \u2211m t=1 xst (1\u2212\u03c3\u03b7)t\n10: xs+10 \u2190 xsm 11: end for 12: return x\u0303S .\n\u2264 Eist [ f(xst ) + \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+ L\n2 \u2016xst \u2212 xst+1\u20162 \u2212 f(u) + \u03a8(xst+1)\u2212\u03a8(u)\n]\n\u2264 Eist [ \u3008\u2207f(xst ), xst \u2212 u\u3009 \u2212 \u03c3\n2 \u2016xst \u2212 u\u20162 + \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+\nL 2 \u2016xst \u2212 xst+1\u20162 + \u03a8(xst+1)\u2212\u03a8(u)\n]\n= Eist [ \u3008\u03best , xst \u2212 u\u3009 \u2212 \u03c3\n2 \u2016xst \u2212 u\u20162 + \u3008\u2207f(xst ), xst+1 \u2212 xst \u3009+\nL 2 \u2016xst \u2212 xst+1\u20162 + \u03a8(xst+1)\u2212\u03a8(u)\n]\n(B.1)\nAbove, the term \u03c32 \u2016xst \u2212 u\u20162 is due to the \u03c3-strong convexity of f(\u00b7), and this is the only difference between the inequalities (B.1) and (4.1). Therefore, Lemma B.1 can be proven using exactly the identical rest of the proof of Lemma 4.1.\nWe are now ready to state the main theorem for the convergence of UniVRsc in the strongly convex case.\nTheorem B.2. UniVRsc(x\u03c6,m, S, \u03b7) satisfies if S is a positive integer, \u03b7 = 1/7L, and m = d 1\u03c3\u03b7 e, then\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 O(2\u2212S) \u00b7 ( F (x\u03c6)\u2212 F (x\u2217) ) . (B.2)\nIn addition, UniVRsc has a gradient complexity of O ( S \u00b7 (n+ L\u03c3 ) ) .\nProof. Combining Lemma B.1 with u = x\u2217 and Lemma 4.2, we have\nEist [ F (xst+1)\u2212 F (x\u2217) ] \u2264 2\u03b7L (1\u2212 \u03b7L) ( F (xst )\u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217) )\n+ (1\u2212 \u03c3\u03b7)\u2016x\u2217 \u2212 xst\u20162 \u2212 Eist \u2016x\u2217 \u2212 xst+1\u20162\n2\u03b7 . (B.3)\nNext, let us define \u03b2t def = 1(1\u2212\u03c3\u03b7)t for t = 0, 1, . . . ,m. Choosing \u03b7 = 1/7L, and multiplying inequality (B.3) by \u03b2t+1 on both sides, we obtain that for every t \u2208 {0, 1, . . . ,m\u2212 1},\n\u03b2t+1Eist [ F (xst+1)\u2212 F (x\u2217) ] \u2264 \u03b2t+1\n3\n( F (xst )\u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217) )\n+ \u03b2t\u2016x\u2217 \u2212 xst\u20162 \u2212 \u03b2t+1Eist \u2016x\u2217 \u2212 xst+1\u20162\n2\u03b7\n\u2264 \u03b2t 3\n( F (xst )\u2212 F (x\u2217) ) + \u03b2t+1\n3\n( F (x\u0303s\u22121)\u2212 F (x\u2217) )\n+ \u03b2t\u2016x\u2217 \u2212 xst\u20162 \u2212 \u03b2t+1Eist \u2016x\u2217 \u2212 xst+1\u20162\n2\u03b7 .\nSumming up the above inequality over t = 0, 1, . . . ,m\u22121, and dividing both sides by \u03b2 def= \u2211mt=1 \u03b2t, we arrive at\nE [m\u22121\u2211\nt=0\n\u03b2t+1F (x s t+1)\n\u03b2 \u2212 F (x\u2217)\n] \u2264 E [1 3 (m\u22121\u2211\nt=0\n\u03b2tF (x s t )\n\u03b2 \u2212 F (x\u2217) + F (x\u0303s\u22121)\u2212 F (x\u2217)\n)\n+ \u03b20\u2016x\u2217 \u2212 xs0\u20162 \u2212 \u03b2m\u2016x\u2217 \u2212 xsm\u20162 2\u03b7 \u00b7 \u03b2 ] .\nAfter rearranging, we obtain\n2E [m\u22121\u2211\nt=0\n\u03b2t+1F (x s t+1)\n\u03b2 \u2212 F (x\u2217)\n] \u2264 E [\u03b20(F (xs0)\u2212 F (x\u2217))\u2212 \u03b2m(F (xsm)\u2212 F (x\u2217)) \u03b2 + F (x\u0303s\u22121)\u2212 F (x\u2217)\n+ \u03b20\u2016x\u2217 \u2212 xs0\u20162 \u2212 \u03b2m\u2016x\u2217 \u2212 xsm\u20162 2\u03b7/3 \u00b7 \u03b2 ] .\nNext, using the fact that F (x\u0303s) \u2264 \u2211m\u22121t=0 \u03b2t+1F (x s t+1) \u03b2 due to the convexity of F and the definition x\u0303s def = 1\u03b2 \u2211m t=1 \u03b2tx s t , as well as the choice x s m = x s+1 0 , and the choice \u03b20 = 1 and \u03b2m = (1 \u2212 \u03c3\u03b7)\u2212m \u2265 (1\u2212 \u03c3\u03b7)\u22121/\u03c3\u03b7 > 2, we can rewrite the above inequality as\n2E [ F (x\u0303s)\u2212 F (x\u2217) ] \u2264 E [ (F (xs0)\u2212 F (x\u2217))\u2212 2(F (xs+10 )\u2212 F (x\u2217)) \u03b2 + F (x\u0303s\u22121)\u2212 F (x\u2217) )\n+ \u2016x\u2217 \u2212 xs0\u20162 \u2212 2\u2016x\u2217 \u2212 xs+10 \u20162 2\u03b7/3 \u00b7 \u03b2 ] .\nAfter rearranging, we conclude that\n2E [ F (x\u0303s)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 xs+10 \u20162 2\u03b7/3 \u00b7 \u03b2 + F (xs+10 )\u2212 F (x\u2217) \u03b2 ]\n\u2264 E [ F (x\u0303s\u22121)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 xs0\u20162 2\u03b7/3 \u00b7 \u03b2 + F (xs0)\u2212 F (x\u2217) \u03b2 ] .\nIn sum, after telescoping for s = 1, 2, . . . , S, we have\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 2\u2212S \u00b7 ( F (x\u03030)\u2212 F (x\u2217) + \u2016x \u2217 \u2212 x10\u20162 2\u03b7/3 \u00b7 \u03b2 + F (x 1 0)\u2212 F (x\u2217) )\n\u2264 F (x \u03c6)\u2212 F (x\u2217) 2S\u22121 + \u2016x\u2217 \u2212 x\u03c6\u20162\n2S \u00b7 2\u03b7\u03b23 \u2264 F (x\n\u03c6)\u2212 F (x\u2217) 2S\u22121 + F (x\u03c6)\u2212 F (x\u2217)\n2S \u00b7 2\u03c3\u03b7\u03b23 \u2264 O(2\u2212S) \u00b7 ( F (x\u03c6)\u2212 F (x\u2217) ) .\nAbove, the last inequality uses the fact that \u03b2 \u2265 m \u00b7 1 \u2265 1\u03c3\u03b7 , and this finishes the proof of (B.2). Finally, it is clear that UniVRsc computes S times the full gradient\u2207f(\u00b7), and Sm times the gradient \u2207fi(\u00b7). This gives a total gradient complexity O(S \u00b7 (n+m)) = O(S \u00b7 (n+ L\u03c3 )). Corollary B.3. If we are given some parameter \u2206 > 0 satisfying F (x\u03c6) \u2212 F (x\u2217) \u2264 \u2206, then by setting S = log(\u2206/\u03b5), \u03b7 = 1/7L, and m = d 1\u03c3\u03b7 e, we have\nE[F (x\u0303S)\u2212 F (x\u2217)] \u2264 O(\u03b5) , and the gradient complexity of UniVRsc is O ( log (\n\u2206 \u03b5 ) \u00b7 ( n+ L\u03c3 )) ."}], "references": [{"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems", "author": ["Aaron J. Defazio", "Tib\u00e9rio S. Caetano", "Justin Domke"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Rong-En Fan", "Chih-Jen Lin"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning", "author": ["Julien Mairal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov"], "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Feature selection, L1 vs. L2 regularization, and rotational invariance", "author": ["Andrew Y. Ng"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "SDCA without Duality", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Logarithmic regret algorithms for strongly convex repeated games", "author": ["Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Proximal Stochastic Dual Coordinate Ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "arXiv preprint arXiv:1211.2717,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "A Proximal Stochastic Gradient Method with Progressive Variance Reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization", "author": ["Yuchen Zhang", "Lin Xiao"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Since the computation of \u2207fi(x) is usually n times faster than that of \u2207f(x), SGD has been successfully applied to many large-scale learning problems, see for instance [1, 20].", "startOffset": 168, "endOffset": 175}, {"referenceID": 0, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 1, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 4, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 6, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 10, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 13, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 14, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 17, "context": "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].", "startOffset": 78, "endOffset": 106}, {"referenceID": 4, "context": "One particular way to reduce the variance can be described as follows (see for instance Johnson and Zhang [6]).", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].", "startOffset": 56, "endOffset": 74}, {"referenceID": 4, "context": "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].", "startOffset": 56, "endOffset": 74}, {"referenceID": 13, "context": "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].", "startOffset": 56, "endOffset": 74}, {"referenceID": 14, "context": "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].", "startOffset": 56, "endOffset": 74}, {"referenceID": 17, "context": "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].", "startOffset": 56, "endOffset": 74}, {"referenceID": 16, "context": "This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 6, "context": "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 10, "context": "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 4, "context": "Finally, in both cases, UniVR demands a memory storage ofO(d), matching the best known memory requirement of SVRG [6], and is much cheaper thanO(nd) as required by many others (either direct or indirect methods).", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Within each epoch, similar to SVRG [6, 19], we compute the full gradient \u03bc\u0303s\u22121 = \u2207f(x\u0303s\u22121) where x\u0303s\u22121 is the average point of the previous epoch.", "startOffset": 35, "endOffset": 42}, {"referenceID": 17, "context": "Within each epoch, similar to SVRG [6, 19], we compute the full gradient \u03bc\u0303s\u22121 = \u2207f(x\u0303s\u22121) where x\u0303s\u22121 is the average point of the previous epoch.", "startOffset": 35, "endOffset": 42}, {"referenceID": 8, "context": "If the full gradient is used at each step of the algorithm, a simple gradient descent algorithm converges in O(L/\u03b5) steps and therefore has a gradient complexity O(nL/\u03b5) (see for instance the textbook of Nesterov [10]).", "startOffset": 213, "endOffset": 217}, {"referenceID": 7, "context": "This has been improved to O(n \u221a L/\u03b5) using Nesterov\u2019s accelerated gradient method [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "When the use of the full gradient \u2207f(x) is simply replaced with a stochastic gradient \u03bet = \u2207fi(xt), SGD achieves a convergence rate of O(1/\u03b5) [20, 22].", "startOffset": 142, "endOffset": 150}, {"referenceID": 3, "context": "Later, a faster O(1/\u03b5) convergence rate was discovered for functions that are strongly convex [5, 14].", "startOffset": 94, "endOffset": 101}, {"referenceID": 12, "context": "Later, a faster O(1/\u03b5) convergence rate was discovered for functions that are strongly convex [5, 14].", "startOffset": 94, "endOffset": 101}, {"referenceID": 10, "context": "The first published method that reduces the variance and overcomes the previous barrier of SGD methods is due to Schmidt, Le Roux, and Bach [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 3, "context": ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/\u03b5) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].", "startOffset": 126, "endOffset": 133}, {"referenceID": 12, "context": ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/\u03b5) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].", "startOffset": 126, "endOffset": 133}, {"referenceID": 8, "context": ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/\u03b5) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined \u03bet to be of a form slightly different from SAG.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined \u03bet to be of a form slightly different from SAG.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined \u03bet to be of a form slightly different from SAG.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of \u201cepochs\u201d and defined \u03bet = \u2207if(xt) \u2212 \u2207if(xt) + \u2207f(x\u0303) like we do in this paper.", "startOffset": 20, "endOffset": 23}, {"referenceID": 17, "context": "The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of \u201cepochs\u201d and defined \u03bet = \u2207if(xt) \u2212 \u2207if(xt) + \u2207f(x\u0303) like we do in this paper.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "The algorithm SDCA [16] has also been discovered to be intrinsically performing some \u201cvariance reduction\u201d procedure [2, 6, 13].", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "The algorithm SDCA [16] has also been discovered to be intrinsically performing some \u201cvariance reduction\u201d procedure [2, 6, 13].", "startOffset": 116, "endOffset": 126}, {"referenceID": 4, "context": "The algorithm SDCA [16] has also been discovered to be intrinsically performing some \u201cvariance reduction\u201d procedure [2, 6, 13].", "startOffset": 116, "endOffset": 126}, {"referenceID": 11, "context": "The algorithm SDCA [16] has also been discovered to be intrinsically performing some \u201cvariance reduction\u201d procedure [2, 6, 13].", "startOffset": 116, "endOffset": 126}, {"referenceID": 15, "context": "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/\u03b5, \u221a nL/\u03b5 }) log 1\u03b5 ) .", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/\u03b5, \u221a nL/\u03b5 }) log 1\u03b5 ) .", "startOffset": 92, "endOffset": 95}, {"referenceID": 19, "context": "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/\u03b5, \u221a nL/\u03b5 }) log 1\u03b5 ) .", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "SVRG [6] Prox-SVRG [19] yes O(d) O ( (n+ L \u03c3 ) log 1 \u03b5 ) no O ( (n+ L \u03b5 ) log 1 \u03b5 )", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "SVRG [6] Prox-SVRG [19] yes O(d) O ( (n+ L \u03c3 ) log 1 \u03b5 ) no O ( (n+ L \u03b5 ) log 1 \u03b5 )", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Finito [3] no O(nd) O ( n log L/\u03c3 \u03b5 ) (only when n \u2265 L/\u03c3) no -", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "SDCA [16] Prox-SDCA [15] yes O(Td+ n) c O ( (n+ L \u03c3 ) log L/\u03c3+n \u03b5 ) no O ( (n+ L \u03b5 ) log L+n \u03b5 )", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "SDCA [16] Prox-SDCA [15] yes O(Td+ n) c O ( (n+ L \u03c3 ) log L/\u03c3+n \u03b5 ) no O ( (n+ L \u03b5 ) log L+n \u03b5 )", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "MISO [8] no O(nd) O ( nL \u03c3 log L/\u03c3 \u03b5 )", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "SAG [12] no O(nd) O ( (n+ L \u03c3 ) log L/(\u03c3n)+1 \u03b5 ) yes O ( n+L \u03b5 )", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "SAGA [2] yes O(nd) O ( (n+ L \u03c3 ) log min{L/\u03c3,n} \u03b5 ) yes O ( n+L \u03b5 )", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "[2, 6, 19]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[2, 6, 19]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 17, "context": "[2, 6, 19]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "In this section, we confirm our theoretical findings using three real-life datasets: (1) the Adult dataset (32, 561 examples and 123 features), (2) the Covtype dataset (581, 012 examples and 54 features), and (3) the 2nd class of the MNIST dataset (60, 000 examples and 780 features) [4].", "startOffset": 284, "endOffset": 287}, {"referenceID": 15, "context": "Following [17], we have normalized each feature vector to have Euclidean norm 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "\u2022 SVRG [6, 19] with (their suggested) epoch size m = 2n and step size \u03b7 = 0.", "startOffset": 7, "endOffset": 14}, {"referenceID": 17, "context": "\u2022 SVRG [6, 19] with (their suggested) epoch size m = 2n and step size \u03b7 = 0.", "startOffset": 7, "endOffset": 14}, {"referenceID": 10, "context": "\u2022 SAG [12] and SAGA [2] both with step size 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "\u2022 SAG [12] and SAGA [2] both with step size 0.", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "3 \u2022 SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).", "startOffset": 9, "endOffset": 17}, {"referenceID": 14, "context": "3 \u2022 SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).", "startOffset": 9, "endOffset": 17}], "year": 2017, "abstractText": "We revisit an important class of composite stochastic minimization problems that often arises from empirical risk minimization settings, such as Lasso, Ridge Regression, and Logistic Regression. We present a new algorithm UniVR based on stochastic gradient descent with variance reduction. Our algorithm supports non-strongly convex objectives directly, and outperforms all of the state-of-the-art algorithms, including both direct algorithms (SAG, MISO, and SAGA) and indirect algorithms (SVRG, ProxSVRG, SDCA, ProxSDCA, and Finito) for such objectives. Our algorithm supports strongly convex objectives as well, and matches the best known linear convergence rate. Experiments support our theory. As a result, UniVR closes an interesting gap in the literature because all the existing direct algorithms for the non-strongly convex case perform much slower than the indirect algorithms. We thus believe that UniVR provides a unification between the strongly and the non-strongly convex stochastic minimization theories.", "creator": "LaTeX with hyperref package"}}}