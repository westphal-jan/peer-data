{"id": "1506.02004", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Sparse Overcomplete Word Vector Representations", "abstract": "current distributed representations of words overwhelmingly show little resemblance to theories of lexical semantics. one the two former are dense and uninterpretable, the current latter largely based on familiar, discrete classes ( mainly e. g., supersenses ) and relations ( e. g., synonymy and hypernymy ). we propose methods that transform word vectors into sparse ( arbitrary and optionally binary ) vectors. the resulting representations are more similar to the interpretable features typically used in nlp, though they are discovered automatically from raw corpora. because the vectors are highly sparse, they are computationally easy to generally work with. most importantly, next we find that they outperform the original message vectors on automatic benchmark tasks.", "histories": [["v1", "Fri, 5 Jun 2015 18:20:43 GMT  (3064kb,D)", "http://arxiv.org/abs/1506.02004v1", "Proceedings of ACL 2015"]], "COMMENTS": "Proceedings of ACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "yulia tsvetkov", "dani yogatama", "chris dyer", "noah a smith"], "accepted": true, "id": "1506.02004"}, "pdf": {"name": "1506.02004.pdf", "metadata": {"source": "CRF", "title": "Sparse Overcomplete Word Vector Representations", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "emails": ["mfaruqui@cs.cmu.edu", "ytsvetko@cs.cmu.edu", "dyogatama@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).\nYet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also\nwhen we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).\nOur contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (\u00a72). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an \u201covercomplete\u201d representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006).\nOur work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex-\nar X\niv :1\n50 6.\n02 00\n4v 1\n[ cs\n.C L\n] 5\nJ un\n2 01\n5\nical semantic theories. Using a number of stateof-the-art word vectors as input, we find consistent benefits of our method on a suite of standard benchmark evaluation tasks (\u00a73). We also evaluate our word vectors in a word intrusion experiment with humans (Chang et al., 2009) and find that our sparse vectors are more interpretable than the original vectors (\u00a74).\nWe anticipate that sparse, binary vectors can play an important role as features in statistical NLP models, which still rely predominantly on discrete, sparse features whose interpretability enables error analysis and continued development. We have made an implementation of our method publicly available.1"}, {"heading": "2 Sparse Overcomplete Word Vectors", "text": "We consider methods for transforming dense word vectors to sparse, binary overcomplete word vectors. Fig. 1 shows two approaches. The one on the top, method A, converts dense vectors to sparse overcomplete vectors (\u00a72.1). The one beneath, method B, converts dense vectors to sparse and binary overcomplete vectors (\u00a72.2 and \u00a72.4).\nLet V be the vocabulary size. In the following, X \u2208 RL\u00d7V is the matrix constructed by stacking V non-sparse \u201cinput\u201d word vectors of length L (produced by an arbitrary word vector estimator). We will refer to these as initializing vectors. A \u2208 RK\u00d7V contains V sparse overcomplete word vectors of length K. \u201cOvercomplete\u201d representation learning implies that K > L."}, {"heading": "2.1 Sparse Coding", "text": "In sparse coding (Lee et al., 2006), the goal is to represent each input vector xi as a sparse linear combination of basis vectors, ai. Our experiments consider four initializing methods for these vectors, discussed in Appendix A. Given X, we seek to solve\narg min D,A\n\u2016X\u2212DA\u201622 + \u03bb\u2126(A) + \u03c4\u2016D\u201622, (1)\nwhere D \u2208 RL\u00d7K is the dictionary of basis vectors. \u03bb is a regularization hyperparameter, and \u2126 is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1\n1https://github.com/mfaruqui/ sparse-coding\npenalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (\u00a72.3):\narg min D,A V\u2211 i=1 \u2016xi\u2212Dai\u201622+\u03bb\u2016ai\u20161+\u03c4\u2016D\u201622 (2)\nwhere mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A."}, {"heading": "2.2 Sparse Nonnegative Vectors", "text": "Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai}. Thus, the new objective for nonnegative sparse vectors becomes:\narg min D\u2208RL\u00d7K\u22650 ,A\u2208R K\u00d7V \u22650 V\u2211 i=1 \u2016xi\u2212Dai\u201622+\u03bb\u2016ai\u20161+\u03c4\u2016D\u201622 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next."}, {"heading": "2.3 Optimization", "text": "We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2\u20133 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014).\nHowever, directly applying stochastic subgradient descent to an `1-regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: g\u0304t = 1t \u2211t t\u2032=1 gt\u2032 Here, the subgradients do not include terms for the regularizer; they are derivatives of the unregularized objective (\u03bb = 0, \u03c4 = 0)\nwith respect to ai. We define\n\u03b3 = \u2212sign(g\u0304t,i,j) \u03b7t\u221a Gt,i,j (|g\u0304t,i,j | \u2212 \u03bb),\nwhere Gt,i,j = \u2211t t\u2032=1 g 2 t\u2032,i,j . Now, using the average gradient, the `1-regularized objective is optimized as follows:\nat+1,i,j = { 0, if |g\u0304t,i,j | \u2264 \u03bb \u03b3, otherwise\n(4)\nwhere, at+1,i,j is the jth element of sparse vector ai at the tth update and g\u0304t,i,j is the corresponding average gradient. For obtaining nonnegative sparse vectors we take projection of the updated ai onto RK\u22650 by choosing the closest point in RK\u22650 according to Euclidean distance (which corresponds to zeroing out the negative elements):\nat+1,i,j =  0, if |g\u0304t,i,j | \u2264 \u03bb 0, if \u03b3 < 0 \u03b3, otherwise\n(5)"}, {"heading": "2.4 Binarizing Transformation", "text": "Our aim with method B is to obtain word representations that can emulate the binary-feature"}, {"heading": "1975, 1976, 1968, 1970, 1977, 1969", "text": "space designed for various NLP tasks. We could\nstate this as an optimization problem:\narg min D\u2208RL\u00d7K\nB\u2208{0,1}K\u00d7V\nV\u2211 i=1 \u2016xi \u2212Dbi\u201622 + \u03bb\u2016bi\u201611 + \u03c4\u2016D\u201622\n(6) where B denotes the binary (and also sparse) representation. This is an mixed integer bilinear program, which is NP-hard (Al-Khayyal and Falk, 1983). Unfortunately, the number of variables in the problem is \u2248 KV which reaches 100 million when V = 100, 000 and K = 1, 000, which is intractable to solve using standard techniques.\nA more tractable relaxation to this hard problem is to first constrain the continuous representation A to be nonnegative (i.e, ai \u2208 RK\u22650; \u00a72.2). Then, in order to avoid an expensive computation, we take the nonnegative word vectors obtained using Eq. 3 and project nonzero values to 1, preserving the 0 values. Table 2 shows a random set of word clusters obtained by (i) applying our method to Glove initial vectors and (ii) applying k-means clustering (k = 100). In \u00a73 we will find that these vectors perform well quantitatively."}, {"heading": "2.5 Hyperparameter Tuning", "text": "Methods A and B have three hyperparameters: the `1-regularization penalty \u03bb, the `2-regularization penalty \u03c4 , and the length of the overcomplete word vector representationK. We perform a grid search on \u03bb \u2208 {0.1, 0.5, 1.0} and K \u2208 {10L, 20L}, selecting values that maximizes performance on one \u201cdevelopment\u201d word similarity task (WS-353, discussed in \u00a7B) while achieving at least 90% sparsity in overcomplete vectors. \u03c4 was tuned on one collection of initializing vectors (Glove, discussed in \u00a7A) so that the vectors in D are near unit norm. The four vector representations and their corresponding hyperparameters selected by this procedure are summarized in Table 1. These hyperparameters were chosen for method A and retained for method B."}, {"heading": "3 Experiments", "text": "Using methods A and B, we constructed sparse overcomplete vector representations A and B resp., starting from four initial vector representations X; these are explained in Appendix A. We used one benchmark evaluation (WS-353) to tune hyperparameters, resulting in the settings shown in Table 1; seven other tasks were used to evaluate the quality of the sparse overcomplete repre-\nsentations. The first of these is a word similarity task, where the score is correlation with human judgments, and the others are classification accuracies of an `2-regularized logistic regression model trained using the word vectors. These tasks are described in detail in Appendix B."}, {"heading": "3.1 Effects of Transforming Vectors", "text": "First, we quantify the effects of our transformations by comparing their output to the initial (X) vectors. Table 3 shows consistent improvements of sparsifying vectors (method A). The exceptions are on the SimLex task, where our sparse vectors are worse than the skip-gram initializer and on par with the multilingual initializer. Sparsification is beneficial across all of the text classification tasks, for all initial vector representations. On average across all vector types and all tasks, sparse overcomplete vectors outperform their corresponding initializers by 4.2 points.2\nBinarized vectors (from method B) are also usually better than the initial vectors (also shown in Table 3), and tend to outperform the sparsified variants, except when initializing with Glove. On average across all vector types and all tasks, binarized overcomplete vectors outperform their corresponding initializers by 4.8 points and the continuous, sparse intermediate vectors by 0.6 points.\nFrom here on, we explore more deeply the sparse overcomplete vectors from method A (denoted by A), leaving binarization (method B) aside."}, {"heading": "3.2 Effect of Vector Length", "text": "How does the length of the overcomplete vector (K) affect performance? We focus here on the Glove vectors, where L = 300, and report average performance across all tasks. We consider K = \u03b1L where \u03b1 \u2208 {1, 2, 3, 5, 10, 15, 20}. Figure 2 plots the average performance across tasks against \u03b1. The earlier selection of K = 3, 000 (\u03b1 = 10) gives the best result; gains are monotonic in \u03b1 to that point and then begin to diminish."}, {"heading": "3.3 Alternative Transformations", "text": "We consider two alternative transformations. The first preserves the original vector length but\n2We report correlation on a 100 point scale, so that the average which includes accuracuies and correlation is equally representatitve of both.\nachieves a binary, sparse vector (B) by applying:\nbi,j = { 1 if xi,j > 0 0 otherwise\n(7)\nThe second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:\nai,j =  1 if xi,j \u2265M+ \u22121 if xi,j \u2264M\u2212\n0 otherwise (8)\nwhere M+ (M\u2212) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary.\nWe find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations."}, {"heading": "4 Interpretability", "text": "Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability, focusing on individual dimensions."}, {"heading": "4.1 Word Intrusion", "text": "Word intrusion experiments seek to quantify the extent to which dimensions of a learned word representation are coherent to humans. In one instance of the experiment, a human judge is presented with five words in random order and asked to select the \u201cintruder.\u201d The words are selected by the experimenter by choosing one dimension j of the learned representation, then ranking the words on that dimension alone. The dimensions are chosen in decreasing order of the variance of their values across the vocabulary. Four of the words are the top-ranked words according to j, and the \u201ctrue\u201d intruder is a word from the bottom half of the list, chosen to be a word that appears in the top 10% of some other dimension. An example of an instance is:\nnaval, industrial, technological, marine, identity\n(The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges.\nResults in Table 5 confirm that the sparse overcomplete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss\u2019 \u03ba increases from \u201cfair\u201d to \u201cmoderate\u201d agreement (Landis and Koch, 1977)."}, {"heading": "4.2 Qualitative Evaluation of Interpretability", "text": "If a vector dimension is interpretable, the topranking words for that dimension should display semantic or syntactic groupings. To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparsified GC vectors. We compare top-ranked words in the dimensions extracted from the two representations. The words are listed in Table 6, a dimension per row. Subjectively, we find the semantic groupings better in the sparse vectors than in the initial vectors.\nFigure 3 visualizes the sparsified GC vectors for six words. The dimensions are sorted by the average value across the three \u201canimal\u201d vectors. The animal-related words use many of the same dimensions (102 common active dimensions out of 500 total); in constrast, the three city names use\nmostly distinct vectors."}, {"heading": "5 Related Work", "text": "To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).\nSparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013; Yogatama et al., 2015). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).\nV 37 9 V 35 3 V 76 V 18 6 V 33 9 V 17 7 V 11 4 V 34 2 V 33 2 V 27 0 V 22 2 V 91 V 30 3 V 47 3 V 35 5 V 35 8 V 16 4 V 34 8 V 32 4 V 19 2 V 24 V 28 1 V 82 V 46 V 27 7 V 46 6 V 46 5 V 12 8 V 11 V 41 3 V 98 V 13 1 V 44 5 V 19 9 V 47 5 V 20 8 V 43 1 V 29 9 V 35 7 V 14 9 V 80 V 24 7 V 23 1 V 42 V 44 V 37 6 V 15 2 V 74 V 25 4 V 14 1 V 34 1 V 34 9 V 23 4 V 55 V 47 7 V 27 2 V 21 7 V 45 7 V 57 V 15 9 V 22 3 V 31 0 V 43 6 V 32 5 V 21 1 V 11 7 V 36 0 V 48 3 V 36 3 V 43 9 V 40 3 V 11 9 V 32 9 V 83 V 37 1 V 42 4 V 17 9 V 21 4 V 26 8 V 38 V 10 2 V 93 V 89 V 12 V 17 2 V 17 3 V 28 5 V 34 4 V 78 V 22 7 V 42 6 V 43 0 V 24 1 V 38 4 V 46 0 V 34 7 V 17 1 V 28 9 V 38 0 V 8 V 2 V 3 V 5 V 6 V 7 V 10 V 14 V 15 V 16 V 17 V 18 V 19 V 20 V 21 V 22 V 25 V 26 V 28 V 29 V 30 V 31 V 32 V 33 V 35 V 36 V 37 V 39 V 40 V 41 V 43 V 45 V 47 V 49 V 50 V 51 V 52 V 54 V 56 V 58 V 59 V 60 V 63 V 64 V 65 V 67 V 68 V 69 V 70 V 72 V 75 V 77 V 81 V 87 V 90 V 92 V 94 V 99 V 10 1 V 10 3 V 10 5 V 10 6 V 10 8 V 11 0 V 11 1 V 11 6 V 11 8 V 12 2 V 12 3 V 12 5 V 13 0 V 13 2 V 13 3 V 13 6 V 13 7 V 13 8 V 13 9 V 14 0 V 14 3 V 14 4 V 14 7 V 14 8 V 15 0 V 15 5 V 15 8 V 16 0 V 16 2 V 16 5 V 16 6 V 16 7 V 16 8 V 16 9 V 17 0 V 17 4 V 17 5 V 17 8 V 18 0 V 18 1 V 18 2 V 18 3 V 18 5 V 18 8 V 18 9 V 19 0 V 19 1 V 19 3 V 19 4 V 19 5 V 19 6 V 20 2 V 20 3 V 20 4 V 20 5 V 21 2 V 21 3 V 21 5 V 21 8 V 22 0 V 22 4 V 22 6 V 22 8 V 23 2 V 23 3 V 23 5 V 23 6 V 23 8 V 23 9 V 24 0 V 24 2 V 24 3 V 24 4 V 24 8 V 24 9 V 25 0 V 25 1 V 25 2 V 25 3 V 25 5 V 25 8 V 25 9 V 26 0 V 26 1 V 26 2 V 26 3 V 26 4 V 26 5 V 26 6 V 27 1 V 27 3 V 27 4 V 27 8 V 28 2 V 28 4 V 28 7 V 28 8 V 29 0 V 29 2 V 29 3 V 29 4 V 29 6 V 30 0 V 30 2 V 30 4 V 30 7 V 30 8 V 31 1 V 31 2 V 31 3 V 31 4 V 31 6 V 31 7 V 31 8 V 31 9 V 32 0 V 32 1 V 32 2 V 32 3 V 32 7 V 33 0 V 33 1 V 33 3 V 33 4 V 33 6 V 33 8 V 34 0 V 34 3 V 34 5 V 34 6 V 35 2 V 35 6 V 36 1 V 36 2 V 36 6 V 36 8 V 36 9 V 37 0 V 37 2 V 37 3 V 37 5 V 37 7 V 37 8 V 38 1 V 38 2 V 38 3 V 38 5 V 38 6 V 38 7 V 38 8 V 38 9 V 39 0 V 39 1 V 39 2 V 39 4 V 39 5 V 39 6 V 39 8 V 39 9 V 40 0 V 40 1 V 40 2 V 40 4 V 40 5 V 40 6 V 40 7 V 40 8 V 40 9 V 41 0 V 41 2 V 41 4 V 41 5 V 41 6 V 41 7 V 41 8 V 41 9 V 42 0 V 42 2 V 42 3 V 42 5 V 42 7 V 42 8 V 42 9 V 43 3 V 43 4 V 43 5 V 43 7 V 44 1 V 44 2 V 44 4 V 44 6 V 44 9 V 45 0 V 45 1 V 45 2 V 45 3 V 45 5 V 45 6 V 45 8 V 45 9 V 46 1 V 46 2 V 46 3 V 46 4 V 46 7 V 46 8 V 46 9 V 47 1 V 47 2 V 47 8 V 47 9 V 48 0 V 48 1 V 48 2 V 48 4 V 48 5 V 48 6 V 48 8 V 48 9 V 49 0 V 49 1 V 49 2 V 49 3 V 49 4 V 49 5 V 49 7 V 49 9 V 50 0 V 50 1 V 48 7 V 20 0 V 32 6 V 4 V 12 1 V 26 7 V 23 0 V 43 8 V 13 4 V 97 V 10 4 V 35 1 V 21 9 V 13 V 88 V 12 9 V 28 6 V 22 9 V 35 0 V 96 V 10 7 V 15 3 V 14 5 V 15 4 V 34 V 30 1 V 37 4 V 10 9 V 39 7 V 15 6 V 16 1 V 29 7 V 11 5 V 15 1 V 24 5 V 44 7 V 53 V 33 7 V 79 V 44 8 V 28 3 V 44 3 V 20 1 V 39 3 V 36 5 V 48 V 12 6 V 25 7 V 24 6 V 29 5 V 12 0 V 36 7 V 27 V 18 4 V 20 9 V 30 6 V 26 9 V 12 4 V 47 0 V 11 2 V 18 7 V 62 V 47 4 V 35 4 V 45 4 V 27 9 V 14 6 V 27 5 V 22 1 V 20 7 V 71 V 33 5 V 73 V 85 V 44 0 V 95 V 23 V 22 5 V 41 1 V 32 8 V 30 5 V 19 8 V 16 3 V 9 V 13 5 V 31 5 V 14 2 V 49 8 V 29 1 V 86 V 47 6 V 21 0 V 35 9 V 84 V 10 0 V 30 9 V 17 6 V 21 6 V 43 2 V 20 6 V 42 1 V 27 6 V 23 7 V 61 V 15 7 V 36 4 V 12 7 V 66 V 25 6 V 28 0 V 11 3 V 29 8 V 19 7 V 49 6\nboston seattle chicago dog horse fish\nFigure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white."}, {"heading": "6 Conclusion", "text": "We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and outperform the original vectors from which they are derived on a suite of semantics and syntactic evaluation benchmarks. We also find that the sparse vectors are more interpretable than the dense vectors by humans according to a word intrusion detection test."}, {"heading": "Acknowledgments", "text": "We thank Alona Fyshe for discussions on vector interpretability and three anonymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533.\nA Initial Vector Representations (X)\nOur experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method.\nGlove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3\n3http://www-nlp.stanford.edu/projects/ glove/\nSkip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word\u2019s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4\nGlobal Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5\nMultilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6"}, {"heading": "B Evaluation Benchmarks", "text": "Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors.\nWord Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. This dataset is used to tune sparse vector learning hyperparameters (\u00a72.5), while the remaining of the tasks discussed in this section are completely held out.\n4https://code.google.com/p/word2vec 5http://nlp.stanford.edu/\u02dcsocherr/\nACL2012_wordVectorsTextFile.zip 6http://cs.cmu.edu/\u02dcmfaruqui/soft.html\nA more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman\u2019s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings.\nSentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set.\nQuestion Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set.\n20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set.\nNP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of\n7http://qwone.com/\u02dcjason/20Newsgroups\nlength three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validation accuracy is reported on the remaining nine folds."}], "references": [{"title": "Jointly constrained biconvex programming", "author": ["Faiz A. Al-Khayyal", "James E. Falk."], "venue": "Mathematics of Operations Research, pages 273\u2013286.", "citeRegEx": "Al.Khayyal and Falk.,? 1983", "shortCiteRegEx": "Al.Khayyal and Falk.", "year": 1983}, {"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio."], "venue": "Computational Linguistics, 34(4):555\u2013596.", "citeRegEx": "Artstein and Poesio.,? 2008", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "The Berkeley FrameNet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proc. of ACL.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proc. of ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei."], "venue": "NIPS.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["Andrzej Cichocki", "Rafal Zdunek", "Anh Huy Phan", "Shun-ichi Amari."], "venue": "John Wiley & Sons.", "citeRegEx": "Cichocki et al\\.,? 2009", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Measuring agreement for multinomial data", "author": ["Mark Davies", "Joseph L Fleiss."], "venue": "Biometrics, pages 1047\u20131051.", "citeRegEx": "Davies and Fleiss.,? 1982", "shortCiteRegEx": "Davies and Fleiss.", "year": 1982}, {"title": "Stable recovery of sparse overcomplete representations in the presence of noise", "author": ["David L. Donoho", "Michael Elad", "Vladimir N. Temlyakov."], "venue": "IEEE Transactions on Information Theory, 52(1).", "citeRegEx": "Donoho et al\\.,? 2006", "shortCiteRegEx": "Donoho et al\\.", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Technical Report EECS-2010-24, University of California Berkeley.", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Dual averaging for distributed optimization: Convergence analysis and network scaling", "author": ["John C. Duchi", "Alekh Agarwal", "Martin J. Wainwright."], "venue": "IEEE Transactions on Automatic Control, 57(3):592\u2013606.", "citeRegEx": "Duchi et al\\.,? 2012", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proc. of EACL.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proc. of NAACL.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proc. of WWW.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."], "venue": "Biostatistics, 9(3):432\u2013 441.", "citeRegEx": "Friedman et al\\.,? 2008", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Interpretable semantic vectors from a joint model of brain- and text- based meaning", "author": ["Alona Fyshe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell."], "venue": "Proc. of ACL.", "citeRegEx": "Fyshe et al\\.,? 2014", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "A compositional and interpretable semantic space", "author": ["Alona Fyshe", "Leila Wehbe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell."], "venue": "Proc. of NAACL.", "citeRegEx": "Fyshe et al\\.,? 2015", "shortCiteRegEx": "Fyshe et al\\.", "year": 2015}, {"title": "Posterior vs", "author": ["Kuzman Ganchev", "Ben Taskar", "Fernando Pereira", "Jo\u00e3o Gama."], "venue": "parameter sparsity in latent variable models. In NIPS.", "citeRegEx": "Ganchev et al\\.,? 2009", "shortCiteRegEx": "Ganchev et al\\.", "year": 2009}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proc. of ICML.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Exponential priors for maximum entropy models", "author": ["Joshua Goodman."], "venue": "Proc. of NAACL.", "citeRegEx": "Goodman.,? 2004", "shortCiteRegEx": "Goodman.", "year": 2004}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["E. Grefenstette."], "venue": "arXiv:1304.5823.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Revisiting embedding features for simple semi-supervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proc. of EMNLP.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Asynchronous stochastic optimization for sequence training of deep neural networks", "author": ["Georg Heigold", "Erik McDermott", "Vincent Vanhoucke", "Andrew Senior", "Michiel Bacchiani."], "venue": "Proc. of ICASSP.", "citeRegEx": "Heigold et al\\.,? 2014", "shortCiteRegEx": "Heigold et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "CoRR, abs/1408.3456.", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Non-negative sparse coding", "author": ["Patrik O. Hoyer."], "venue": "Neural Networks for Signal Processing, 2002. Proc. of IEEE Workshop on.", "citeRegEx": "Hoyer.,? 2002", "shortCiteRegEx": "Hoyer.", "year": 2002}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proc. of ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Evaluation and extension of maximum entropy models with inequality constraints", "author": ["Jun\u2019ichi Kazama", "Jun\u2019ichi Tsujii"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kazama and Tsujii.,? \\Q2003\\E", "shortCiteRegEx": "Kazama and Tsujii.", "year": 2003}, {"title": "Detecting compositionality of multi-word expressions using nearest neighbours in vector space models", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kiela and Clark.,? 2013", "shortCiteRegEx": "Kiela and Clark.", "year": 2013}, {"title": "The measurement of observer agreement for categorical data", "author": ["J. Richard Landis", "Gary G. Koch."], "venue": "Biometrics, 33(1):159\u2013174.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "author": ["Angeliki Lazaridou", "Eva Maria Vecchi", "Marco Baroni."], "venue": "Proc. of EMNLP.", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung."], "venue": "Nature, 401(6755):788\u2013791.", "citeRegEx": "Lee and Seung.,? 1999", "shortCiteRegEx": "Lee and Seung.", "year": 1999}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng."], "venue": "NIPS.", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Exponential family sparse coding with application to self-taught learning", "author": ["Honglak Lee", "Rajat Raina", "Alex Teichman", "Andrew Y. Ng."], "venue": "Proc. of IJCAI.", "citeRegEx": "Lee et al\\.,? 2009", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "English Verb Classes and Alternations: A Preliminary Investigation", "author": ["Beth Levin."], "venue": "University of Chicago Press.", "citeRegEx": "Levin.,? 1993", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Learning overcomplete representations", "author": ["Michael Lewicki", "Terrence Sejnowski."], "venue": "Neural Computation, 12(2):337\u2013365.", "citeRegEx": "Lewicki and Sejnowski.,? 2000", "shortCiteRegEx": "Lewicki and Sejnowski.", "year": 2000}, {"title": "Combined distributional and logical semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the ACL, 1:179\u2013192.", "citeRegEx": "Lewis and Steedman.,? 2013", "shortCiteRegEx": "Lewis and Steedman.", "year": 2013}, {"title": "Combining formal and distributional models of temporal and intensional semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proc. of ACL.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proc. of COLING.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Non-negative matrix factorization for visual coding", "author": ["Weixiang Liu", "Nanning Zheng", "Xiaofeng Lu."], "venue": "Proc. of ICASSP.", "citeRegEx": "Liu et al\\.,? 2003", "shortCiteRegEx": "Liu et al\\.", "year": 2003}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Structured sparsity in structured prediction", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Pedro M.Q. Aguiar", "M\u00e1rio A.T. Figueiredo."], "venue": "Proc. of EMNLP.", "citeRegEx": "Martins et al\\.,? 2011", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: a lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Brian Murphy", "Partha Talukdar", "Tom Mitchell."], "venue": "Proc. of COLING.", "citeRegEx": "Murphy et al\\.,? 2012", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Research Design & Statistical Analysis", "author": ["Jerome L. Myers", "Arnold D. Well."], "venue": "Routledge.", "citeRegEx": "Myers and Well.,? 1995", "shortCiteRegEx": "Myers and Well.", "year": 1995}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni."], "venue": "Proc. of ACL.", "citeRegEx": "Paperno et al\\.,? 2014", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Factorial LDA: Sparse multi-dimensional text models", "author": ["Michael Paul", "Mark Dredze."], "venue": "NIPS.", "citeRegEx": "Paul and Dredze.,? 2012", "shortCiteRegEx": "Paul and Dredze.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Verbnet: A Broadcoverage, Comprehensive Verb Lexicon", "author": ["Karin Kipper Schuler."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Schuler.,? 2005", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proc. of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A bayesian lda-based model for semi-supervised partof-speech tagging", "author": ["Kristina Toutanova", "Mark Johnson."], "venue": "NIPS.", "citeRegEx": "Toutanova and Johnson.,? 2007", "shortCiteRegEx": "Toutanova and Johnson.", "year": 2007}, {"title": "From frequency to meaning : Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "JAIR, 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney."], "venue": "Proc. of ECML.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "Studying the recursive behaviour of adjectival modification with compositional distributional semantics", "author": ["Eva Maria Vecchi", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proc. of EMNLP.", "citeRegEx": "Vecchi et al\\.,? 2013", "shortCiteRegEx": "Vecchi et al\\.", "year": 2013}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Lin Xiao."], "venue": "NIPS.", "citeRegEx": "Xiao.,? 2009", "shortCiteRegEx": "Xiao.", "year": 2009}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proc. of CIKM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A Smith."], "venue": "Proc. of ACL.", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of ICML.", "citeRegEx": "Yogatama et al\\.,? 2015", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proc. of ACL.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Sparse topical coding", "author": ["Jun Zhu", "Eric P Xing."], "venue": "arXiv:1202.3778.", "citeRegEx": "Zhu and Xing.,? 2012", "shortCiteRegEx": "Zhu and Xing.", "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 87, "endOffset": 132}, {"referenceID": 3, "context": "Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.", "startOffset": 87, "endOffset": 132}, {"referenceID": 21, "context": ", 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 50, "context": ", 2014), and sentiment analysis (Socher et al., 2013).", "startOffset": 32, "endOffset": 53}, {"referenceID": 53, "context": "Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 52, "context": "Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 33, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 2, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 49, "context": "Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 152, "endOffset": 200}, {"referenceID": 42, "context": ", 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995).", "startOffset": 61, "endOffset": 75}, {"referenceID": 35, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 27, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 54, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 20, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 36, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 46, "context": "On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014).", "startOffset": 153, "endOffset": 291}, {"referenceID": 59, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 56, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 12, "context": "Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source.", "startOffset": 83, "endOffset": 143}, {"referenceID": 45, "context": "The transformation results in longer, sparser vectors, sometimes called an \u201covercomplete\u201d representation (Olshausen and Field, 1997).", "startOffset": 105, "endOffset": 132}, {"referenceID": 45, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al.", "startOffset": 207, "endOffset": 263}, {"referenceID": 34, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al.", "startOffset": 207, "endOffset": 263}, {"referenceID": 8, "context": "Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006).", "startOffset": 316, "endOffset": 337}, {"referenceID": 43, "context": "Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al.", "startOffset": 100, "endOffset": 141}, {"referenceID": 15, "context": "Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al.", "startOffset": 100, "endOffset": 141}, {"referenceID": 21, "context": ", 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014).", "startOffset": 70, "endOffset": 88}, {"referenceID": 5, "context": "We also evaluate our word vectors in a word intrusion experiment with humans (Chang et al., 2009) and find that our sparse vectors are more interpretable than the original vectors (\u00a74).", "startOffset": 77, "endOffset": 97}, {"referenceID": 31, "context": "In sparse coding (Lee et al., 2006), the goal is to represent each input vector xi as a sparse linear combination of basis vectors, ai.", "startOffset": 17, "endOffset": 35}, {"referenceID": 32, "context": "Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009).", "startOffset": 104, "endOffset": 122}, {"referenceID": 30, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 6, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 43, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 15, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 16, "context": "2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015).", "startOffset": 119, "endOffset": 224}, {"referenceID": 24, "context": "To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002).", "startOffset": 102, "endOffset": 115}, {"referenceID": 9, "context": "3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs.", "startOffset": 55, "endOffset": 84}, {"referenceID": 10, "context": "In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014).", "startOffset": 123, "endOffset": 165}, {"referenceID": 22, "context": "In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014).", "startOffset": 123, "endOffset": 165}, {"referenceID": 55, "context": "We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: \u1e21t = 1t \u2211t t\u2032=1 gt\u2032 Here, the subgradients do not include terms for the regularizer; they are derivatives of the unregularized objective (\u03bb = 0, \u03c4 = 0)", "startOffset": 100, "endOffset": 112}, {"referenceID": 0, "context": "This is an mixed integer bilinear program, which is NP-hard (Al-Khayyal and Falk, 1983).", "startOffset": 60, "endOffset": 87}, {"referenceID": 21, "context": "The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:", "startOffset": 42, "endOffset": 60}, {"referenceID": 5, "context": "(2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis.", "startOffset": 43, "endOffset": 63}, {"referenceID": 42, "context": "Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "8 (Guo et al., 2014) 75.", "startOffset": 2, "endOffset": 20}, {"referenceID": 1, "context": "Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss\u2019 \u03ba (Davies and Fleiss, 1982).", "startOffset": 121, "endOffset": 148}, {"referenceID": 7, "context": "Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss\u2019 \u03ba (Davies and Fleiss, 1982).", "startOffset": 163, "endOffset": 188}, {"referenceID": 28, "context": "The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss\u2019 \u03ba increases from \u201cfair\u201d to \u201cmoderate\u201d agreement (Landis and Koch, 1977).", "startOffset": 158, "endOffset": 181}, {"referenceID": 45, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al.", "startOffset": 90, "endOffset": 146}, {"referenceID": 34, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al.", "startOffset": 90, "endOffset": 146}, {"referenceID": 8, "context": "However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006).", "startOffset": 169, "endOffset": 190}, {"referenceID": 30, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 38, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 6, "context": "Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009).", "startOffset": 87, "endOffset": 149}, {"referenceID": 26, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 14, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 19, "context": "Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al.", "startOffset": 59, "endOffset": 122}, {"referenceID": 17, "context": ", 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 40, "context": ", 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 57, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al.", "startOffset": 29, "endOffset": 55}, {"referenceID": 4, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013; Yogatama et al., 2015).", "startOffset": 85, "endOffset": 129}, {"referenceID": 58, "context": ", 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013; Yogatama et al., 2015).", "startOffset": 85, "endOffset": 129}, {"referenceID": 51, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 185, "endOffset": 214}, {"referenceID": 47, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 245, "endOffset": 288}, {"referenceID": 60, "context": "Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012).", "startOffset": 245, "endOffset": 288}, {"referenceID": 48, "context": "Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus.", "startOffset": 40, "endOffset": 65}, {"referenceID": 41, "context": "The word2vec tool (Mikolov et al., 2013) is fast and widely-used.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": "These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012).", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "The first is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans.", "startOffset": 32, "endOffset": 58}, {"referenceID": 23, "context": "A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness).", "startOffset": 34, "endOffset": 53}, {"referenceID": 44, "context": "We calculate cosine similarity between the vectors of two words forming a test item and report Spearman\u2019s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings.", "startOffset": 135, "endOffset": 157}, {"referenceID": 50, "context": "Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": ", whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002).", "startOffset": 94, "endOffset": 113}, {"referenceID": 57, "context": "/test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs.", "startOffset": 31, "endOffset": 57}, {"referenceID": 39, "context": "(2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": "Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al.", "startOffset": 0, "endOffset": 24}], "year": 2015, "abstractText": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.", "creator": "TeX"}}}