{"id": "1506.02078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Visualizing and Understanding Recurrent Networks", "abstract": "coupled recurrent neural networks ( rnns ), and specifically a variant with long short - term memory ( lstm ), are enjoying renewed interest as a major result of successful applications in a historically wide range of machine learning problems that involve sequential data. however, while lstms provide exceptional results in practice, yet the source of their performance and their limitations remain rather poorly understood. using extended character - base level language models as theoretically an interpretable testbed, we aim to bridge this paradigm gap by providing a comprehensive analysis of their representations, predictions and error types. in particular, our experiments reveal the existence of interpretable cells that keep track of long - range dependencies systematically such as line lengths, quotes and brackets. moreover, an extensive analysis with finite horizon n - gram models suggest that these dependencies are actively discovered and utilized by the networks. finally, we provide detailed error analysis that suggests certain areas for further study.", "histories": [["v1", "Fri, 5 Jun 2015 22:33:04 GMT  (4031kb,D)", "http://arxiv.org/abs/1506.02078v1", null], ["v2", "Tue, 17 Nov 2015 02:42:24 GMT  (2671kb,D)", "http://arxiv.org/abs/1506.02078v2", "changing style, adding references, minor changes to text"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["andrej karpathy", "justin johnson", "li fei-fei"], "accepted": false, "id": "1506.02078"}, "pdf": {"name": "1506.02078.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Understanding Recurrent Networks", "authors": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "emails": ["karpathy@cs.stanford.edu", "jcjohns@cs.stanford.edu", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks, and specifically a variant with Long Short-Term Memory (LSTM) (14), have recently emerged as an effective model in a wide variety of applications that involve sequential data. These include language modeling (22), handwriting recognition and generation (9), machine translation (28; 1), speech recognition (10), video analysis (7) and image captioning (30; 19).\nHowever, the source of their impressive performance remains poorly understood. This raises concerns of interpretability and limits our ability design better architectures. A few recent ablation studies analyzed the effects on performance as they removed or modified various gates and connections (12; 5). While this analysis illuminates the performance-critical pieces of the architecture, it is still limited to measuring the test set perplexity alone.\nAn often cited advantage of the LSTM architecture is that it can store and retrieve information over long time scales using its gating mechanisms. However, it is not immediately clear that good solutions can be discovered with stochastic gradient descent and truncated backpropagation through time, and that these mechanisms are effectively utilized on real-world data. In this work we use character-level language models as an interpretable testbed for illuminating the long-range dependencies learned by the LSTM architecture. Our analysis reveals the existence of cells that robustly identify interpretable, high-level patterns such as line lengths, brackets and quotes. We further quantify the LSTM predictions with comprehensive comparison to n-gram models, where we find that LSTMs perform significantly better on characters that require long-range reasoning. Finally, we conduct a detailed error analysis in which we \u201cpeel the onion\u201d of errors with a sequence of oracles. These results allow us to quantify the extent of remaining errors in several categories and to suggest specific areas for further study."}, {"heading": "2 Related Work", "text": "Recurrent Networks. Recurrent Neural Networks (RNNs) have a long history of applications in various sequence learning tasks (31; 26; 25). Despite their early successes, the difficulty of training\n\u2217Both authors contributed equally to this work.\nar X\niv :1\n50 6.\n02 07\n8v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\nsimple recurrent networks (2; 24) has encouraged various proposals for improvements to their basic architecture. Among the most successful variants are the Long Short Term Memory networks (14), which can in principle store and retrieve information over long time periods with explicit gating mechanisms and a built-in constant error carousel. In the recent years there has been a renewed interest in further improving on the basic architecture by modifying the functional form as seen with Gated Recurrent Units (4), incorporating content-based soft attention mechanisms (1; 32), pushpop stacks (18), or more generally external memory arrays with both content-based and relative addressing mechanisms (11). In this work we focus the majority of our analysis on the LSTM due to its widespread popularity and proven track record.\nUnderstanding Recurrent Networks. While there is an abundance of work that modifies or extends the basic LSTM architecture, relatively little attention has been paid to understanding its computational properties or the source of its performance. Greff et al. (12) recently conducted a comprehensive study of different LSTM components and concluded that the forget gates are its most critical components. Chung et al. evaluated GRU compared to LSTMs (5). Pascanu et al. examined the effects of depth (23). These approaches study recurrent network architectures based on variations in the final performance, while our approach dives deep into the statistical patterns in their activations and predictions. We also conduct detailed error analysis that breaks down the the final performance into interpretable categories."}, {"heading": "3 Experimental Setup", "text": "We first describe three variants of a deep recurrent network (RNN, LSTM and the GRU models), then explain how they are used in sequence learning and finally describe the optimization."}, {"heading": "3.1 Recurrent Neural Network Models", "text": "The simplest instantiation of a deep recurrent network arranges hidden state vectors hlt in a twodimensional grid, where t = 1 . . . T is thought of as time and l = 1 . . . L is the depth. The bottom row of vectors h0t = xt at depth zero holds the input vectors xt and each vector in the top row {hLt } is used to predict an output vector yt. All intermediate vectors hlt are computed as a function of hlt\u22121 and h l\u22121 t . Through these hidden vectors, each output yt at some particular time step t becomes a function of all input vectors up to that time, {x1, . . . , xt}. The precise mathematical form of the recurrence (hlt\u22121 , h l\u22121 t )\u2192 hlt varies from model to model and we describe these details next.\nVanilla Recurrent Neural Network (RNN) has a recurrence of the form\nhlt = tanhW l ( hl\u22121t hlt\u22121 ) where we assume that all h \u2208 Rn. The parameter matrix W l on each layer has dimensions [n\u00d7 2n] and tanh is applied elementwise. Note that W varies between layers but is shared through time. We omit the bias vectors for brevity. Interpreting the equation above, the inputs from the layer below hl\u22121t and before in time h l t\u22121 are transformed and interact through additive interaction before the non-linearity. This is known to be a weak form of coupling (27). Both the LSTM and the GRU (discussed next) include more powerful multiplicative interactions.\nLong Short-Term Memory (LSTM) (14) was designed to address the difficulties of training RNNs (2). In particular, it was observed that the backgropagation dynamics led the gradients in an RNN to either vanish or explode. It was later found that the exploding gradient concern can be alleviated with a heuristic of clipping the gradients at some maximum value (24). On the other hand, LSTMs were designed to mitigate the vanishing gradient problem. In addition to a hidden state vector hlt, LSTMs also maintain a memory vector c l t. At each time step the LSTM can choose to read from, write to, or reset the cell using explicit gating mechanisms. The precise form of the update is as follows: ifo\ng\n = sigmsigmsigm\ntanh\nW l(hl\u22121t hlt\u22121 ) clt = f clt\u22121 + i g hlt = o tanh(clt)\nHere, the sigmoid function sigm and tanh are applied element-wise, and W l is a [4n\u00d7 2n] matrix. The three vectors i, f, o \u2208 Rn are each thought of as binary gates that control whether each memory cell is updated, whether it is reset to zero, and whether its local state is revealed in the hidden vector,\nrespectively. The activations of these gates are based on the sigmoid function and hence allowed to range smoothly between zero and one to keep the model differentiable. The vector g \u2208 Rn ranges between -1 and 1 and is used to additively modify the memory contents. This additive interaction is a critical feature of the LSTM\u2019s design, because during backpropagation a sum operation merely distributes gradients. This allows gradients on the memory cells c to flow backwards through time uninterrupted for long time periods, or at least until the flow is disrupted with the multiplicative interaction of an active forget gate. Lastly, note that an implementation of the LSTM requires one to maintain both hlt and c l t at every point in the network.\nGated Recurrent Unit (GRU) (4) was recently proposed as a simpler alternative to the LSTM:( r z ) = ( sigm sigm ) W lr ( hl\u22121t hlt\u22121 ) h\u0303lt = tanh(W l xh l\u22121 t + W l g(r hlt\u22121))\nhlt = (1\u2212 z) hlt\u22121 + z h\u0303lt Here, W lr are [2n\u00d7 2n] and W lg and W lx are [n\u00d7 n]. The GRU has the interpretation of computing a candidate hidden vector h\u0303lt and then smoothly interpolating towards it, as gated by z."}, {"heading": "3.2 Character-level Language Modeling", "text": "We use character-level language modeling as an interpretable testbed for sequence learning. In this setting, the input to the network is a sequence of characters and the network is trained to predict the next character in the sequence with a Softmax classifier at each time step. Concretely, assuming a fixed vocabulary of K characters we encode all characters with K-dimensional 1-of-K vectors {xt}, t = 1, . . . , T , and feed these to the recurrent network to obtain a sequence of D-dimensional hidden vectors at the last layer of the network {hLt }, t = 1, . . . , T . To obtain predictions for the next character in the sequence we project this top layer of activations to a sequence of vectors {yt}, where yt = Wyh L t and Wy is a [K \u00d7 D] parameter matrix. These vectors are interpreted as holding the (unnormalized) log probability of the next character in the sequence and the objective is to minimize the average cross-entropy loss over all targets."}, {"heading": "3.3 Optimization", "text": "Following previous work (28) we initialize all parameters uniformly in range [\u22120.08, 0.08]. We use mini-batch stochastic gradient descent with batch size 100 and RMSProp (6) per-parameter adaptive update with base learning rate 2\u00d710\u22123 and decay 0.95. These settings work robustly with all of our models. The network is unrolled for 100 time steps. We train each model for 50 epochs and decay the learning rate after 10 epochs by multiplying it with a factor of 0.95 after each additional epoch. We use early stopping based on validation performance and cross-validate the amount of dropout for each model individually. Our supplementary material contains additional details regarding efficient implementation and further experiments with other initialization ranges and updates."}, {"heading": "4 Experiments", "text": "Datasets. Two datasets previously used in the context of character-level language models are the Penn Treebank dataset (20) and the Hutter Prize 100MB of Wikipedia dataset (16) . However, both datasets contain a mix of common language and special markup. Our goal is not to compete with previous work but rather to study recurrent networks in a controlled setting and on both ends on the spectrum of degree of structure. Therefore, we chose to use Leo Tolstoy\u2019s War and Peace (WP) novel, which consists of 3,258,246 characters of almost entirely English text with minimal markup, and at the other end of the spectrum the source code of the Linux Kernel (LK). We shuffled all header and source files randomly and concatenated them into a single file to form the 6,206,996 character long dataset. We split the data into train/val/test splits as 80/10/10 for WP and 90/5/5 for LK. Therefore, there are approximately 300,000 characters in the validation/test splits in each case. The total number of characters in the vocabulary is 87 for WP and 101 for LK."}, {"heading": "4.1 Comparing Recurrent Networks", "text": "We first train several recurrent network models to support further analysis and to compare their performance in a controlled setting. In particular, we train models in the cross product of type (LSTM/RNN/GRU), number of layers (1/2/3), number of parameters (4 settings), and both datasets (WP/KL). The 4 parameter sizes were chosen to be in units of 1-layer LSTMs with 64, 128, 256, and 512 cells. With our character vocabulary sizes this gives approximately 50K, 130K, 400K, and 1.3M parameters respectively. The sizes of hidden layers in the other models were always chosen to match the closest prototype size as close as possible.\nThe test set results are shown in Figure 1. Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN. We also computed the fraction of times that each pair of models agree on the most likely character and use it to render a t-SNE (29) embedding (we found this more stable and robust than the KL divergence). The plot (Figure 1, right) further supports the claim that the LSTM and the GRU make similar predictions while the RNNs form their own cluster."}, {"heading": "4.2 Internal Mechanisms of an LSTM", "text": "Interpretable, long-range LSTM cells. LSTMs can in principle use their memory cells to remember long-range information and keep track of various attributes of text they are in. For instance, it is a simple exercise to write down toy cell weights that would allow the cell to keep track of whether it is inside a quoted string. However, to our knowledge, the existence of such cells has never been experimentally demonstrated on real-world data. In particular, it could be argued that even if the LSTM is in principle capable of using these operations, practical optimization challenges (i.e. SGD dynamics, or approximate gradients due to truncated backpropagation through time) might prevent it from discovering these solutions. In this experiment we verify that multiple interpretable cells do in fact exist in these networks. We show several examples in Figure 2. Note that truncated backpropagation prevents the gradient signal from noticing dependencies longer than 100 characters, but we still observe cells that reliably keep track of quotes or comment blocks for periods much longer than 100 characters. We hypothesize that these cells first develop on patterns shorter than 100 characters but then generalize to longer sequences.\nGate activation statistics. We also recorded and examined the statistics of gate activations in the networks while evaluating the test set. We were particularly interested in studying the distributions of saturation regimes in the networks, where we define a gate to be left or right-saturated if its activation is less than 0.1 or more than 0.9, respectively. We then compute the fraction of times that each gate\u2019s activation falls into either category and show these statistics in Figure 3. The amount of rightsaturated forget gate activations in an LSTM is particularly interesting, since this corresponds to cells that remember their value from the previous iteration. For instance, note that there are multiple cells that are almost always right-saturated and hence function as perfect integrators. Conversely, there are no cells that function in purely feed-forward fashion, as these would show as consistently left-saturated forget gates. The output gate statistics also reveal that there are no cells that get consistently revealed to the hidden state. Lastly, a surprising fact is that unlike the other two layers, the activations in the first layer are diffuse. This is a finding that we struggle to explain but it is present across all of our models, including GRUs."}, {"heading": "4.3 Understanding Long-Range Interactions", "text": "Good performance of LSTMs is frequently attributed to their ability to store long-range information. In this section we test this hypothesis by comparing an LSTM with baseline models that can only utilize information from a fixed number of previous steps. In particular, we consider two baselines:\n1. n-NN: A fully-connected neural network with one hidden layer and tanh nonlinearities. The input to the network is a sparse binary vector of dimension nK that concatenates the one-of-K encodings of n consecutive characters. We optimize the model as described in Section 3.3 and cross-validate the size of the hidden layer.\n2. n-gram: An unpruned (n + 1)-gram character-level language model using modified KneserNey smoothing (3). This is a standard smoothing method for language models (15). All models were trained using the popular KenLM software package (13).\nPerformance comparisons. The performance of both n-gram models is shown in Table 2. The n-gram and n-NN models perform nearly identically for small values of n, but for larger values the n-NN models start to overfit and the n-gram model performs better. Moreover, we see that on both datasets our best recurrent network outperforms the 20-gram model (1.077 vs. 1.195 on WP and 0.84 vs.0.889). It is difficult to make a direct model size comparison, but the 20-gram model file has 3GB, while our largest checkpoints are 11MB. However, the assumptions encoded in the Kneser-Ney smoothing model are intended for word-level modeling of natural language and may not be optimal for character-level data. Despite this concern, these results provide some evidence that the recurrent networks are effectively utilizing information beyond 20 characters.\nError Analysis. It is instructive to delve deeper into the errors made by both recurrent networks and n-gram models. In particular, we define a character to be an error if the probability assigned to it by a model is below 0.5. Figure 4 (left) shows the overlap between the test-set errors for the 3-layer LSTM, and the best n-NN and n-gram models. We see that the majority of errors are shared by all three models, but each model also has its own unique errors.\nTo gain deeper insight into the errors that are unique to the LSTM or the 20-gram model, we compute the mean probability assigned to each character in the vocabulary across the test set. In Figure 4 (middle,right) we display the 10 characters where each model has the largest advantage over the other. On the Linux Kernel dataset, the LSTM displays a large advantage on special characters that are used to structure C programs, including whitespace and brackets. The War and Peace dataset features an interesting long-term dependency with the carriage return, which occurs approximately every 70 characters. Figure 4 (right) shows that the LSTM has a distinct advantage on this character. To accurately predict the presence of the carriage return the model likely needs to keep track of its distance since the last carriage return. The cell example we\u2019ve highlighted in Figure 2 (top, left) seems particularly well-tuned for this specific task. Similarly, to predict a closing bracket or quotation mark, the model must be aware of the corresponding open bracket, which may have appeared many time steps ago. The fact that the LSTM performs significantly better than the 20- gram model on these characters provides strong evidence that the model is capable of effectively keeping track of long-range interactions.\nCase study: closing brace. Of these structural characters, the one that requires the longest-term reasoning is the closing brace (\u201c}\u201d) on the Linux Kernel dataset. Braces are used to denote blocks of code, and may be nested; as such, the distance between an opening brace and its corresponding closing brace can range from tens to hundreds of characters. This feature makes the closing brace an ideal test case for studying the ability of the LSTM to reason over various time scales. We group closing brace characters on the test set by the distance to their corresponding open brace and compute the mean probability assigned by the LSTM and the 20-gram model to closing braces within each group. The results are shown in Figure 5 (left). We see not only that the LSTM consistently outperforms the 20-gram model at predicting closing braces, but that the performance difference increases up to a distance of 60. After this point the performance difference between the models remains relatively constant, suggesting that no additional time horizon ranges are being captured and that perhaps the LSTM has trouble keeping track of braces longer than this amount.\nTraining dynamics. We can gain further insight into the training dynamics of the LSTM by comparing it with trained n-NN models during training using the (symmetric) KL divergence between the predictive distributions on the test set. We visualize the divergence and the difference in the mean loss in Figure 5 (right). Notably, we see that in the first few iterations the LSTM behaves like the 1- NN model but then diverges from it soon after. The LSTM then behaves most like the 2-NN, 3-NN, and 4-NN models in turn. This experiment suggests that the LSTM \u201cgrows\u201d its competence over increasingly longer dependencies. We believe that this insight is related to why Sutskever et al. (28)\nhad to reverse the source sentences in their encoder-decoder architecture for machine translation. Without the inversion it is as if the model was immediately presented with n-grams of very high n. This prevents the RNN from slowly \u201cgrowing\u201d its competence from shorter to longer dependencies."}, {"heading": "4.4 Error Analysis: Breaking Down the Failure Cases", "text": "In this section we break down LSTM\u2019s errors into categories to understand its limitations, the relative severity of each error type, and to suggest areas for further study. We focus on the War and Peace dataset where it is easier to analyze the errors. Our approach is to \u201cpeel the onion\u201d by iteratively removing the errors with a series of constructed oracles. We discuss these first:\nn-gram oracle. First, we construct optimistic n-gram oracles that eliminate errors that might be fixed with better modeling of the previous n characters. In particular, we evaluate our n-gram model (n = 1, . . . , 9) and remove a character error if it is correctly classified by any of these models. Dynamic n-long memory oracle. Consider the string \u201cJon yelled at Mary but Mary couldn\u2019t hear him.\u201d One interesting and consistent failure mode is that if the LSTM fails to recognize the first occurrence of \u201cMary\u201d then it will almost always also fail to recognize the second, with an identical pattern of errors. In principle the first mention should make the second easier: the LSTM could conceivably store a summary of previously seen characters in the data and fall back on this memory when it is uncertain. However, this does not appear to take place in practice. This observation is related to the improvements seen in \u201cdynamic evaluation\u201d (21; 17) of recurrent language models, where an RNN is allowed to train on the test set characters during evaluation as long as it sees them only once. This oracle optimistically removes errors in all words (starting with the second character) that can also be found as a substring in the last n characters (we use n \u2208 {100, 500, 1000, 50000}). Rare words oracle. Next, we construct an oracle that eliminates errors for rare words that occur only up to n times in the training data (n = 0, . . . , 5). Note that a character-level model could in principle hypothesize the spelling of unseen words by recognizing common letter patterns. Errors corrected by this oracle indicate its failure to do so. Word model oracle. We noticed that a large portion of the errors occur on the first character of each word. Intuitively, the task of selecting the next word in the sequence is much harder than completing the last few characters of a known word. We constructed an oracle that eliminated all errors after a space, quote or a newline. Interestingly, a high portion of errors can be found after a newline character, since the models have to learn that new line is identical to a space. Punctuation oracle. This oracle removes errors on all punctuation. Boost oracles. The remaining errors that do not show salient structures or patterns are removed by an oracle that boosts the probability of the correct letter by a fixed amount. These oracles allow us to understand the distribution of the difficulty of the remaining errors.\nWe now subject two LSTM models to the error analysis: First, our best LSTM model and second, the best LSTM model in the smallest model category (50K parameters). The smaller second model will allow us to quantify the reduction in each error category as we scale up our models. For the following analysis we define a character to be an error when its assigned probability is less than 0.5. We then iterate over oracles in the order presented above, remove each error type in turn and visualize the fractions of errors attributed to each type in Figure 6.\nThe error breakdown. The best LSTM model makes a total of 140K errors out of 330K test set characters (42%). Of these, the n-gram oracle eliminates 18%, suggesting that the model is not taking full advantage of the last 9 characters. The dynamic memory oracle eliminates 6% of the errors. In principle, a dynamic evaluation scheme could be used to mitigate this type of error, but we believe that a more principled approach could involve a model similar to Memory Networks (32), where the model is allowed to attend to a recent history of the sequence while making its next prediction. Finally, the rare words oracle accounts for 9% of the errors. This error type could be mitigated with transfer learning, or by increasing the size of the training set. The majority of the remaining errors (37%) follow a space, a quote, or a newline. This suggests that substantially longer time dependencies, and possibly hierarchical context models, could eliminate a large fraction of the errors. See Figure 6 for examples of each error type.\nErrors eliminated by scaling up. The smaller LSTM model makes a total of 184K errors (or 56% of the test set), approximately 44K more than the large model. Surprisingly, 36K of these errors (81%) are n-gram errors, 5K come from the boost category, and the remaining 3K are distributed across the other categories relatively evenly. That is, scaling the model up by a factor 26 in the number of parameters has almost entirely provided gains in the local, n-gram error rate and has left the other error categories untouched in comparison. This analysis suggests that in order to fully remove all errors it might be necessary to develop new architectural improvements instead of simply scaling up the basic model. For full details of this analysis refer to our supplementary material."}, {"heading": "5 Conclusion", "text": "We have presented a comprehensive analysis of Recurrent Neural Networks and their representations, predictions and error types. In particular, qualitative visualization experiments, cell activation statistics and in-depth comparisons to finite horizon n-gram models demonstrate that these networks learn powerful, long-range interactions. We have also conducted a detailed error analysis that illuminates the limitations of recurrent networks and allows us to suggest specific areas for further study. In particular, n-gram errors can be significantly reduced by scaling up the models and rare words could be addressed with bigger datasets. However, further architectural innovations may be needed to eliminate the remaining errors."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "Exact Model Sizes", "text": ""}, {"heading": "Training Details", "text": "In this section we provide additional details regarding training.\nEfficient implementation. It is impractical to compute the loss function over the entire training sequence, which can in practice be be millions of characters long. Instead, a good strategy is to take the input sequence of characters are reshape it into one data matrix X of size [B \u00d7N ], where B is the batch size (we use B = 100). We then march from left to right in chunks of length S (we use S = 100), and every non-overlapping block of [B \u00d7 S] is fed as input to the network. The ground truth labels matrix Y is the same as X , but shifted by 1 columnwise (i.e. next character in the sequence). It is important to keep track of the hidden state activations hlT at the last time step in the previous batch and copy them into the vectors hl0 in the next batch. We evaluate the loss and backpropagate only on the [B\u00d7S] chunk of data. In particular, note that the backpropagated signal is truncated S steps backwards in time, limiting the extent of long term interactions that the networks can learn. In practice the networks do learn much longer dependencies (as supported by our qualitative cell visualization experiments), but this can liekly only happen if the same dependencies also occur on time scales less than S and then happen to generalize to longer sequences.\nInitialization. Following previous work (28) we initialize all parameters uniformly in the range [\u22120.08, 0.08]. We experimented with multiple settings of this hyperparameter on multiple architectures and found its sensitivity to be relatively low. In particular, It is safer to err on the side of making this range smaller, since some models start to diverge when it is greater than 1, but the performance degrades gracefully, even down to 10\u22124 and less.\nOptimization. We experimented with multiple per-parameter learning rate updates and consistently found RMSProp (6) to provide the best results, and with the widest tolerance to the learning rate setting. Adagrad (8) also provided a wide tolerance but its performance did not match that of RMSProp. SGD with momentum was able to nearly match the performance of RMSProp but required a precise setting of the learning rate. For example, for a 3-layer LSTM trained with SGD converged only in the range [10\u22120.4, 10\u22120.8] and diverged otherwise, while RMSProp diverged outside of [10\u22122.2, 10\u22123.2] interval. We clipped the gradients elementwise in range [\u22125, 5] and found [\u22121, 1] to be too aggressive, at least in our setting (these settings correspond to case where we average the loss over both the batch size and the sequence length). We did not use L2 regularization because cross-validation always preferred very small coefficients. We used early stopping based on the validation performance. We decay the learning rate after 10 epochs by multiplying it with a factor of 0.95 after each additional epoch. We train each model for 50 epochs."}, {"heading": "Activation Distributions", "text": "During our analysis we noticed a distinct peak of the tanh c (Figure 7, bottom) at \u00b10.76, which is exactly tanh(1). This is caused by the fact that the largest amount that the LSTM can add or subtract to a cell is 1, since g is the output of tanh. This raises a concern because the hidden state h is computed as h = o tanh(c). Therefore if a cell happens to be zero because it was reset in the previous iteration, the LSTM does not have the capacity to fully saturate the cell in a single time step. We investigated a simple fix to this problem by changing the update to the form clt = f clt\u22121 + \u03b1i g where \u03b1 is a constant. We observed significant significant reduction in performance for \u03b1 < 0.25. Using \u03b1 = 2 allows the LSTM to saturate the cell in one step. In this case we observed a consistently faster convergence (e.g. 700 vs. 900 iterations to reach loss of 2.0) across all architectures. However, the final performance at epoch 50 remains unchanged."}, {"heading": "Weights", "text": "We examined the weights of a 1-layer LSTM to gain an understanding of the effect of the inputs on the LSTM\u2019s representation. First, we compute the total strength of influence that each character exhibits on the representation, which we quantify as the sum of absolute values of its weights to all gates. For an LSTM trained on the Linux Kernel dataset, the characters with the strongest influence turn out to be special characters (\\;}(\u2019{v.w+gb+), while the characters that exhibit weakest influence include rare unicode symbols, followed by `$789\u02c65. This suggests that the LSTMs learns to some extent ignore numerals.\nWe wanted to gain a more holistic understanding of the influence of each character on the activations of the network. To this effect, we embed the rows of the LSTM\u2019s weight matrix connecting the characters to the gates with t-SNE (29) (Figure 8). This figure shows, for instance, that numerals, which are clustered, give rise to very similar write/forget behavior in the LSTM.\nError Analysis"}, {"heading": "Cell state representation", "text": "In Figure 9 we use t-SNE to visualize the (gated) cell state tanh(c) of an LSTM, allowing us to gain insight into the type of information it encodes. In the lower left we see a cluster of cell states that occur immediately after a space, and in the upper right we see a cluster of cell states where a space is about to occur. This demonstrates that the LSTM uses its cell state both to record characters that have been recently seen and to hypothesize about characters that may appear next."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language, 13(4):359\u2013393,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.04390,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\u2013696, Sofia, Bulgaria, August", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Spoken language processing: A guide to theory, algorithm, and system development", "author": ["X. Huang", "A. Acero", "H.-W. Hon", "R. Foreword By-Reddy"], "venue": "Prentice Hall PTR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The human knowledge compression contest", "author": ["M. Hutter"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A dynamic language model for speech recognition", "author": ["F. Jelinek", "B. Merialdo", "S. Roukos", "M. Strauss"], "venue": "HLT, volume 91, pages 293\u2013295,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "CoRR, abs/1503.01007,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, 2nd April,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1312.6026,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1985}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks, 1(4):339\u2013356,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1988}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "CoRR, abs/1410.3916,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Recurrent Neural Networks (RNNs), and specifically a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.", "creator": "LaTeX with hyperref package"}}}