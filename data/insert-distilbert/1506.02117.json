{"id": "1506.02117", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Learning Multiple Tasks with Deep Relationship Networks", "abstract": "deep neural networks trained on large - scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer only and cross labeling mitigation. as deep features eventually transition from general to detail specific along the core network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the local task - specific layers. in this work, we propose promising a novel deep relationship network ( drn ) architecture for multi - task level learning by discovering correlated tasks based on multiple task - specific layers of a deep and convolutional neural network. drn models the task relationship by imposing matrix normal priors over the network parameters of all task - specific layers, including higher feature layers and classifier layer that are not transferable safely. by jointly learning the transferable features and task relationships, drn functionality is able to alleviate the dilemma of negative - transfer in the feature layers and under - transfer in the classifier layer. empirical evidence shows that drn yields state - of - scale the - art classification results on standard multi - domain object recognition datasets.", "histories": [["v1", "Sat, 6 Jun 2015 04:38:48 GMT  (170kb)", "http://arxiv.org/abs/1506.02117v1", null], ["v2", "Thu, 16 Feb 2017 07:46:11 GMT  (398kb,D)", "http://arxiv.org/abs/1506.02117v2", null], ["v3", "Fri, 26 May 2017 00:27:14 GMT  (385kb,D)", "http://arxiv.org/abs/1506.02117v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingsheng long", "jianmin wang", "philip s yu"], "accepted": true, "id": "1506.02117"}, "pdf": {"name": "1506.02117.pdf", "metadata": {"source": "CRF", "title": "Learning Multiple Tasks with Deep Relationship Networks", "authors": ["Mingsheng Long", "Jianmin Wang"], "emails": ["mingsheng@tsinghua.edu.cn,", "jimwang@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n02 11\n7v 1\n[ cs\n.L G\n] 6\nJ un"}, {"heading": "1 Introduction", "text": "Supervised learning machines trained with limited labeled samples will be prone to overfitting, while manual labeling of sufficient training data for emerging application domains is usually prohibitive. Therefore, it is imperative to establish effective algorithms for reducing the labeling cost, typically by leveraging off-the-shelf labeled data from relevant learning tasks. Multi-task learning is based on the idea that the performance can be improved using related tasks as an inductive bias [1]. Knowing the task relationship should enable the transfer of shared knowledge from relevant tasks so that only task-specific features need to be learned. This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].\nLearning the inherent task relatedness is a hard problem, since the training data of different tasks may be sampled from different probability distributions, and may be fitted by different inductive models. In the absence of prior knowledge on the task relatedness, the distribution shift may pose a major bottleneck in transferring knowledge across different tasks. Unfortunately, if cross-task knowledge transfer is impossible, then we will overfit each task due to limited amount of labeled data. One way to circumvent this dilemma is to use an external data source, e.g. ImageNet, to extract transferable features through which the shift in the inductive biases can be reduced so that different tasks can be correlated more effectively. This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships. However, this may result in under-transfer in the classifier layer as knowledge can not be transferred across different classifiers. Furthermore, the literature\u2019s latest findings reveal that deep features eventually transition from general to specific along the network, and feature transferability drops significantly\nin higher layers with increasing task discrepancy [19, 20], hence the sharing of all feature layers may be risky to negative-transfer. It remains an open problem how to exploit the task relationship across different tasks while accounting for the feature transferability in task-specific layers of the network.\nIn this work, we propose a new Deep Relationship Network (DRN) architecture for learning multiple tasks, which discovers the inherent task relationship based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors [21] over network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. In each layer, the task relationship is shared by all categories, which can naturally handle multi-class problems and can learn the task relationship more accurately when category-wise data is scarce. We further impose a Gaussian prior with a task mean parameter to capture the common task component, which is particularly effective for those transferable lower layers of the deep networks. By jointly learning the transferable features and the task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23]. Extensive empirical study shows that DRN yields state-of-the-art classification results on the standard multi-domain object recognition datasets."}, {"heading": "2 Related Work", "text": "Multi-task learning (MTL) [1] is an important learning paradigm that jointly learns multiple tasks by exploiting shared structural representation and improves generalization by using related tasks as an inductive bias. Multi-task learning is to mitigate the effort of manual labeling for machine learning, computer vision, natural language processing, and computational biology. There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14]. While these methods have been shown to produce state-of-the-art performance, they may be restricted by their shallow learning architecture that cannot suppress task-specific variations for task correlation.\nDeep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind samples [24, 23]. The learned deep representations can manifest invariant factors underlying different populations and are transferable across similar tasks [19]. Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained. Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships. However, this may result in under-transfer in the classifier layer as knowledge can not be adaptively propagated across different classifiers. To address this issue, Srivastava et al. [16] proposed tree-based priors to the weights in the classifier layer, which learns to organize the classes into a tree hierarchy and transfers knowledge to infrequent classes for label-scarcity mitigation. While their method is specifically designed for multi-class classification instead of multi-task learning, the sharing of all feature layers may still be vulnerable to negative-transfer, as the higher layers of deep networks are tailored to fit specific tasks and may not be safely transferable [19]. We propose a deep relationship network that jointly learns transferable features and task relationships to circumvent both under-transfer and negative-transfer."}, {"heading": "3 Deep Relationship Networks", "text": "In this work, we learn multiple tasks by jointly modeling transferable features and task relationships. Given T supervised learning tasks with data {Xt,yt}Tt=1, where Xt \u2208 R\nD0\u00d7Nt and yt \u2208 RNt are the design matrix and label vector of the t-th task, respectively drawn from D0-dimensional feature space and C-cardinality label space, i.e. each training example xtn \u2208 R\nD0 and ytn \u2208 {1, . . . , C}. Our goal is to construct a deep neural network for multiple tasks ytn = ft(x t n) that is able to learn transferable features and adaptive task relationships to bridge different tasks effectively and robustly."}, {"heading": "3.1 Model", "text": "We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22]. The main challenge is that in multi-task learning, each task is provided with only a limited amount of labeled data, which is insufficient to build reliable classifiers without overfitting. In this sense, it is vital to model the task relationships through which each pair of tasks can help with each other if they are related in the parameter space, and can remain independent if they are unrelated to mitigate negative-transfer. With this principle, we design a Deep Relationship Network (DRN) that can exploit both feature transferability and task relationship to enable multi-task learning. Figure 1 gives an illustration of the proposed DRN model.\nWe extend the AlexNet architecture [23], which is comprised of five convolutional layers (conv1\u2013 conv5) and three fully connected layers (fc6\u2013fc8). Each fc layer \u2113 learns a nonlinear mapping h\u2113n = a \u2113 ( W\u2113h\u2113\u22121n + b \u2113 ) , where h\u2113n is the \u2113-th layer hidden representation of point xn, W \u2113 and b\u2113 are the weight and bias parameters of the \u2113-th layer, and a\u2113 is the activation function, taken as rectifier units (ReLU) a\u2113(x) = max(0,x) for hidden layers or softmax units a\u2113 (x) = ex/\n\u2211|x| j=1 e xj for the output layer. Denote by F\u2113t = {W \u2113 t ,b \u2113 t} the network parameters of the t-th task in the \u2113-th layer, and Ft = {F\u2113t } 8 \u2113=1 the set of network parameters for the t-th task. The empirical error of CNN is\nmin ft\u2208Ft\n\u2211Nt\nn=1 J ( ft ( xtn ) , ytn ) , (1)\nwhere J is the cross-entropy loss function, and ft (xtn) is the conditional probability that the CNN assigns xtn to label y t n. We will not describe how to compute the convolutional layers as these layers can learn transferable features [19] and we will simply share their parameters {F\u2113t = F \u2113}5\u2113=1 across different tasks, without modeling the task relationships in these layers. To benefit from pre-training and fine-tuning, we copy these layers from a model pre-trained from ImageNet 2012 [19, 28], freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5, which can preserve the efficacy of fragile co-adaptation.\nAs revealed by the latest literature findings [19], the deep features in standard CNNs must eventually transition from general to specific along the network, and the feature transferability decreases while the task discrepancy increases, making the features in higher layers fc6\u2013fc8 unsafely transferable across different tasks. In other words, the fc layers are tailored to their original task at the expense of degraded performance on the target task, which may deteriorate multi-task learning based on deep neural networks. Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18]. However, it may be vulnerable if different tasks are not well correlated in the deep features, which is common as higher layers are not safely transferable and tasks may be dissimilar. Another issue not well studied is how to explore multi-class task relationships in multi-task learning.\nIn this work, we model feature transferability and task relationship based on multiple task-specific layers and multiple class-shared covariances in a Bayesian framework. Denote by {Xt} = {X} T t=1, {yt} = {y} T t=1 the data of T tasks, by w \u2113 t,c the network parameter associated with the c-th unit (c-th class for output layer) of the t-th task in the \u2113-th layer, by {\nW\u2113t } = { W\u2113t : 1 6 t 6 T, 1 6 \u2113 6 L }\nthe set of network parameters for all T tasks, where W\u2113t , [ w\u2113t,1 . . .w \u2113 t,C\u2113 ] \u2208 RD\u2113\u00d7C\u2113 is the network parameter of the t-th task in the \u2113-th layer, D\u2113 and C\u2113 are the number of units in layers \u2113\u22121 and \u2113, respectively. Note that, D0 is the input dimension and C8 = C is the label set cardinality. The Maximum a Posteriori (MAP) estimation of the model parameters given data is formalized as\np ( { W\u2113t }\u2113\nt\n\u2223 \u2223 \u2223 {Xt} , {yt} ) = p ( { W\u2113c }\u2113\nc\n) \u00d7 p ( {yt} \u2223 \u2223 \u2223 {Xt} , { W\u2113c }\u2113\nc\n)\n= L \u220f\n\u2113=L0\nC\u2113 \u220f\nc=1\np ( W\u2113c )\n\u00d7 T \u220f\nt=1\nNt \u220f\nn=1\np (\nytn\n\u2223 \u2223 \u2223 xtn, { W\u2113t }\u2113 ) , (2)\nwhere W\u2113c , [ w\u21131,c . . .w \u2113 T,c ] \u2208 RD\u2113\u00d7T is the network parameter of all T tasks associated with the c-th unit in the \u2113-th layer (again, will be the c-th category if it is the output layer), and note that {\nW\u2113t }\u2113 t = { W\u2113c }\u2113 c = { W\u2113c : 1 6 c 6 C\u2113, 1 6 \u2113 6 L }\n. Here we adopt the category-wise network parameter W\u2113c instead of the task-wise network parameter W \u2113 t because it is technically more viable to model the task relationship with W\u2113c as it contains the category-wise parameters for all T tasks. An important assumption of multi-class classification is that different categories are independent, which is why multi-class problems can be broken into one-vs-rest binary-class problems. We further assume that for the parameter prior p ( {\nW\u2113c }\u2113\nc\n)\n, the network parameter of each layer is independent\non the network parameters of the other layers, which is a common assumption made by most neural network methods [24]. Finally, we assume when the parameter is sampled from the prior, all tasks are independent. These three assumptions lead to the factorization of the posteriori in Equation (2).\nThe maximum likelihood estimation (MLE) part in Equation (2) will be modeled by the deep CNN in Equation (1), which can learn transferable features in its lower-layers (conv1\u2013conv5) to enhance multi-task learning. Using this property, we opt to share the network parameters of all these layers by W\u2113t = W\n\u2113, 1 6 \u2113 < L0, where L0 = 6, and a different value of L0 is also possible, depending on the sample sizes and the number of task parameters. This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer. We do not share task-specific layers (fc6\u2013fc8) so as to potentially mitigate the negative-transfer [19, 20].\nThe prior part in Equation (2) is crucial as it should be able to model the task relationship adaptively. In this paper, we define the following prior based on Gaussian and matrix normal distributions [21]:\np ( W\u2113c ) =\n(\nT \u220f\nt=1\nND\u2113 ( w\u2113t,c \u2223 \u2223m\u2113c, \u01eb 2ID\u2113 )\n)\nMND\u2113\u00d7T ( W\u2113c \u2223 \u2223 0D\u2113\u00d7T , ID\u2113 ,\u2126 \u2113 ) , (3)\nwhere m\u2113c \u2208 R D\u2113 is the mean parameter of the isotropic Gaussian distribution, and \u2126\u2113 is the covariance parameter of the matrix normal distribution, which is defined as MND\u00d7T (W|M,\u03a3,\u2126) = exp(\u2212 12 tr(\u03a3\n\u22121(W\u2212M)\u2126\u22121(W\u2212M)T)) (2\u03c0)DT/2|\u03a3|T/2|\u2126|D/2 . The first term of the prior on W\u2113c is to penalize the complexity\nof each centered column w\u2113t,c\u2212m \u2113 c separately, and the second term is to model the structure of W \u2113 c. Specifically, in the Gaussian prior, the mean vector m\u2113c models the common component shared by multiple tasks; and in the matrix normal prior, the row covariance matrix ID\u2113 models the relationships between features, and the column covariance matrix \u2126\u2113 models the relationships between task parameters {\nw\u2113t,c }T t=1 . Both parameters are learned from data to build adaptive task relationships.\nIntegrating Equations (1) and (3) and taking the negative logarithm of Equation (2), we obtain the MAP estimation of {W\u2113t} \u2113 t and the MLE estimation of {b \u2113 t} \u2113 t by solving the optimization problem:\nmin f\u2208F ,\u2126\u2208C\nT \u2211\nt=1\nNt \u2211\nn=1\nJ ( ft ( xtn ) , ytn ) +\nL \u2211\n\u2113=L0\nC\u2113 \u2211\nc=1\ntr (\nW\u2113c ( \u2126\u0304\u2113 + \u03bbL ) W\u2113c T ) , (4)\nwhere the penalty parameter \u03bb = 1/\u01eb2, \u2126\u0304\u2113 , ( \u2126\u2113 )\u22121\nis the inverse covariance matrix for modeling the task relationship, and L is the graph Laplacian matrix for modeling the common task component, with Ltt\u2032 = 1 \u2212 1/T if t = t\u2032 and Ltt\u2032 = \u22121/T otherwise. Note that we should jointly minimize the concave complexity penalty for \u2126\u2113, i.e.\n\u2211L \u2113=L0 ln \u2223 \u2223\u2126\u2113 \u2223 \u2223. We take similar strategy as [11, 13] and\nminimize its convex upper bound \u2211L\n\u2113=L0\n( tr ( \u2126\u2113 ) \u2212 T ) , which is reduced to the convex constraint:\nC , { \u2126\u2113 : \u2126\u2113 \u2208 RT\u00d7T ,\u2126\u2113 0, tr ( \u2126\u2113 ) = 1, L0 6 \u2113 6 L } . (5)\nOur work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 \u2113 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al. [19]), and previous relationship learning methods are not designed in a deep architecture (which cannot learn transferable features for multi-task learning); (2) We define the prior by a task covariance matrix \u2126\u2113 shared by all categories (which can learn the task relationship more accurately when category-wise data is scarce), while previous relationship learning methods define the prior for binary or regression problems; (3) We define the prior with a task mean parameter m\u2113c that captures the common task component of multiple tasks, while previous relationship learning methods define the prior with zero-mean Gaussian that cannot model the common task component. Hence the proposed DRN model is more effective for multi-task learning and robust to outlier tasks."}, {"heading": "3.2 Algorithm", "text": "As the optimization problem (4) is jointly non-convex with respect to the network parameters {W\u2113t} \u2113 t and task covariances {\u2126\u2113}\u2113, we adopt an alternating method that optimizes one set of variables with the others fixed. We first update W\u2113t , which needs reformulation of objective (4) in terms of {W \u2113 t} \u2113 t :\nO =\nT \u2211\nt=1\nNt \u2211\nn=1\nJ ( ft ( xtn ) , ytn ) +\nL \u2211\n\u2113=L0\nT \u2211\nt=1\nT \u2211\nt\u2032=1\n( \u2126\u0304\u2113tt\u2032 + \u03bbLtt\u2032 )\ntr ( W\u2113tW \u2113 t\u2032 T ) . (6)\nWhen we train deep CNN by mini-batch stochastic gradient descent (SGD), we only need to consider the gradient of objective (6) corresponding to each data point (xtn, y t n), which can be computed as\n\u2202O (xtn, y t n)\n\u2202W\u2113t =\n\u2202J (ft (x t n) , y t n)\n\u2202W\u2113t + 2\nT \u2211\nt\u2032=1\n( \u2126\u0304\u2113tt\u2032 + \u03bbLtt\u2032 ) W\u2113t\u2032 . (7)\nSuch a mini-batch SGD can be easily implemented via the Caffe framework for CNNs [28]. Since training a deep CNN requires a large amount of labeled data, which is prohibitive for many multitask learning problems, we fine-tune from an AlexNet model pre-trained on ImageNet 2012 as [19]. In each epoch of SGD, with the updated {W\u2113t} \u2113 t, we can update {\u2126 \u2113}\u2113 by a close-form solution as\n\u2126\u2113 = (A\u2113)\n1/2\ntr ( (A\u2113) 1/2 ) , where A\u2113tt\u2032 = tr\n( W\u2113tW \u2113 t\u2032 T ) , and \u2126\u0304\u2113 , ( \u2126\u2113 )\u22121 . (8)\nThough the derivation is similar to [11], our method learns the task relationship \u2126\u2113 from multiple classes, while [11] learns the task relationship \u2126\u2113 from binary classes. When the labeled data is very limited for each category, the binary-class method [11] may not infer the task relationship accurately.\nThe DRN algorithm scales linearly to the sample size. For each iteration, the time cost of all convolutional layers is O(\n\u2211Lcv \u2113=1 NF\u2113\u22121S 2 \u2113 \u00b7 F\u2113M 2 \u2113 ), where Lcv is the number of convolutional layers, F\u2113\nis the number of filters in the \u2113-th layer, S\u2113 is the spatial size of the filter, and M\u2113 is the size of output feature map; and the time cost of all fully connected layers is O(\n\u2211L \u2113=Lcv+1 N(D2\u2113C\u2113 + TD\u2113C\u2113)).\nFinally, the time complexity for updating task relationship matrices is O( \u2211L \u2113=L0 (T 2D2\u2113C\u2113 + T 3))."}, {"heading": "3.3 Discussion", "text": "We consider a fine-grained task relationship where different tasks may be correlated in different ways for different categories. This may be beneficial when the training set for each class is relatively large so that class-wise task relationship can be learned accurately, and the class-conditional distributions are very different across tasks. This leads to a DRN variant for learning class-wise task relationships:\nmin f\u2208F ,\u2126\u2208C\nT \u2211\nt=1\nNt \u2211\nn=1\nJ ( ft ( xtn ) , ytn ) +\nL \u2211\n\u2113=L0\nC\u2113 \u2211\nc=1\ntr (\nW\u2113c ( \u2126\u0304\u2113c + \u03bbL ) W\u2113c T ) , (9)\nwhere \u2126\u2113c is the class-wise task relationship, which decouples Equation (9) for different categories, i.e. equivalent to C one-vs-rest binary problems, but training C deep networks is more demanding.\nLearning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6]. We generalize DRN to handle the case that both the tasks and features are correlated by modifying the matrix normal prior as MND\u2113\u00d7T ( W\u2113c \u2223 \u22230D\u2113\u00d7T ,\u03a3 \u2113,\u2126\u2113 ) in Equation (3), which leads to a new DRN variant for learning both feature and task relationships:\nmin f,\u03a3,\u2126\nT \u2211\nt=1\nNt \u2211\nn=1\nJ ( ft ( xtn ) , ytn ) +\nL \u2211\n\u2113=L0\nC\u2113 \u2211\nc=1\ntr (\n\u03a3\u0304\u2113W\u2113c ( \u2126\u0304\u2113 + \u03bbL ) W\u2113c T ) , (10)\nwhere \u03a3\u2113 is the feature covariance matrix for encoding the feature relationship and \u03a3\u0304\u2113 is its inverse. Note that deep networks can implicitly learn from the feature covariance via the network parameters."}, {"heading": "4 Experiments", "text": "We compare the DRN model with state-of-the-art multi-task learning methods on real-world object recognition datasets, and verify the efficacy of the multi-layer and multi-class relationship learning. Different from most methods that use multi-task datasets where each task is binary classification or univariate regression, we evaluate on multi-task datasets where each task is multi-class classification."}, {"heading": "4.1 Setup", "text": "Office-Caltech [29, 30] This dataset is the standard benchmark for multi-task learning and transfer learning. The Office part [29] consists of 4,652 images in 31 categories collected from three distinct domains (tasks): Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which are images taken by Web camera and digital SLR camera under different environmental variations. This dataset is organized by selecting the 10 common categories shared by the Office dataset and the Caltech-256 (C) dataset [30], hence it consists of four domains (tasks).\nImageCLEF-DA1 This dataset is the benchmark for ImageCLEF domain adaptation challenge. It is organized by selecting the 12 common categories shared by the following four public datasets (tasks): Caltech-256 (C), ImageNet ILSVRC 2012 (I), Pascal VOC 2012 (P), and Bing (B). The 12 common categories are: aeroplane, bike, bird, boat, bottle, bus, car, dog, horse, monitor, motorbike, and people. For space limitation, we omit the dataset details that can be parsed from the Web. Both datasets are evaluated via SURF features for shallow methods and original images for deep methods.\nWe compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18]. Specifically, LR is performed on each task separately, without modeling the shared structure across tasks. MTFL learns the covariance structure of features and different tasks are independent given the covariance structure. RMTL is an extension to MTFL that captures the task relationships using a low-rank structure, and simultaneously identifies the outlier tasks using a group-sparse structure. MTRL infers the task relationships by the task covariance matrix, hence it can be adaptive to the transferability between tasks and circumvent the negative-transfer problem. The original MTCNN is applied to heterogeneous tasks (e.g. face landmark detection and head pose estimation) by sharing all the feature layers and learning multiple task classifiers, while we apply it to homogeneous tasks.\nTo study the efficacy of jointly learning transferable features and task relationships, we evaluate several variants of DRN: (1) DRN using only one network layer, i.e. layer fc8 for relationship learning, termed DRN8; (2) DRN using one-vs-rest binary classifier for relationship learning, termed DRNbi; (3) DRN without enforcing the task mean in isotropic Gaussian prior, termed DRNm. While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm. The plausible reason is that it may be not easy to extend these methods to natural multi-class formulation.\nWe mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5]. Note that the sample sizes may be imbalanced across different tasks, e.g. C and A are large domains with thousands of examples while W and D are small domains with only hundreds of\n1http://imageclef.org/2014/adaptation\nexamples. In this regard, the evaluation protocol can naturally reflect the performance of multi-task learning with different sample sizes. We compare the averages of accuracy for all tasks based on 10 random experiments, while standard errors are insignificant and are not listed. We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11]. For CNN-based methods, we adopt the fine-tuning architecture [19], however, due to limited training examples in our datasets, we fix convolutional layers conv1\u2013 conv3 that were copied from pre-trained model, fine-tune conv4\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8, both via back propagation. As the classifier is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use stochastic gradient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe [28], and cross-validate the learning rate between 10\u22125 and 10\u22122 by a multiplicative step-size 100.5."}, {"heading": "4.2 Results and Discussion", "text": "The multi-task classification results on the multi-class datasets Office-Caltech and ImageCLEF-DA based on 5%, 10%, and 20% sampled training data are shown in Tables 1 and 2, respectively. We can observe that the proposed DRN model significantly outperforms the comparison methods on all multi-task problems. The substantial performance boost verifies that our deep relationship networks via multi-layer and multi-class relationship learning is able to learn both safely transferable features and adaptive task relationships, which establishes more effective and robust multi-task deep learning.\nFrom the experimental results, we can make the following observations. (1) Conventional shallow multi-task learning methods MTFL, RMTL, and MTRL generally outperform single-task learning method STSR, which confirms the motivation of jointly learning multiple tasks by exploiting shared structures. Among the shallow multi-task methods, MTRL gives the best performance, showing that exploiting task relationship is more effective than extracting shallow feature subspace for multi-task learning. (2) Current state-of-the-art multi-task deep learning method MTCNN further outperforms conventional shallow multi-task learning methods by a very large margin, which certifies the vital importance of learning deep transferable features to enable knowledge transfer across different tasks. However, MTCNN assumes shared network parameters for all feature layers (conv1\u2013fc7) and learns\nthe classifier layer fc8 independently without inducing the task relationship. This results in negativetransfer in the feature layers [19] and under-transfer in the classifier layer. A defense for MTCNN is that it is designed for heterogeneous multi-task learning where task relationship is difficult to model.\nTo dive deeper into DRN, we present the results of three variants of DRN: DRN8, DRNbi and DRNm, all significantly outperform MTCNN but generally underperform DRN, which verify our motivation that jointly learning transferable features and adaptive task relationships can bridge multiple tasks more effectively. (1) The disadvantage of DRN8 is that it does not learn the adaptive task relationship in the lower fully connected layers fc6\u2013fc7, which are not completely transferable and may result in negative-transfer [19]. (2) The shortcoming of DRNbi is that it does not learn the task relationship based on multiple categories, hence the inferred class-wise task relationship may be inaccurate when labeled data is very limited, however, its performance will catch up with DRN when the class-wise training data grows large. (3) The weakness of DRNm is that it does not capture the common task component, hence it may result in under-transfer if the common task component really exists, especially for the lower feature layers of the deep network that are more safely transferable [19]. The proposed DRN model takes full advantages of all these factors and establishes the best performance."}, {"heading": "4.3 Visualization Analysis", "text": "We first show that DRN can better learn adaptive task relationships with deep features than MTRL with shallow features, by visualizing the task covariance matrices \u2126 learned by MTRL and DRN in Figures 2(a) and 2(b), respectively. Prior knowledge on task similarity in the Office-Caltech dataset [29] describes that tasks A, W and D are more similar with each other while they are significantly dissimilar to task C. DRN successfully captures this prior task relationship and further enhances task correlation across dissimilar tasks, which establishes stronger transferability for multi-task learning. Furthermore, all tasks are positively correlated (green color) in DRN, implying that all tasks can better reinforce each other. However, some of the tasks (D and C) are still negatively correlated (red color) in MTRL, implying these tasks should be drawn far apart and cannot improve with each other.\nTo demonstrate the transferability of DRN features, we follow [28, 20] and visualize in Figures 2(c) and 2(d) the t-SNE embeddings of the images in the Office-Caltech dataset with MTCNN features and DRN features, respectively. We observe that compared with MTCNN features, the data points with DRN features are discriminated better across different categories, i.e. each category has small intra-class variance and large inter-class margin; and the data points are also aligned better across different tasks, i.e. the embeddings of different tasks overlap well, which implies that different tasks can reinforce each other effectively and improve category discrimination performance. This verifies that with joint relationship discovery, DRN learns more transferable features for multi-task learning."}, {"heading": "5 Conclusion", "text": "We proposed a deep relationship network (DRN) model that integrates standard neural networks with matrix normal priors over the network parameters of all task-specific layers. The priors define the covariance structure over tasks and enable inductive transfer across related tasks. We devised a learning algorithm that fine-tunes from a pre-trained deep network and learns transferable features and task relationships jointly. Experiments show that the model achieves superior results on standard object recognition datasets. Future work includes data-dependent priors for multi-task deep learning."}], "references": [{"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A convex formulation for learning a shared predictive structure from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A probabilistic model for dirty multi-task feature selection", "author": ["D. Hern\u00e1ndez-Lobato", "J.M. Hern\u00e1ndez-Lobato", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "J.-P. Vert", "F.R. Bach"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "In UAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A regularization approach to learning task relationships in multitask learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Convex learning of multiple tasks and their structure", "author": ["C. Ciliberto", "Y. Mroueh", "T. Poggio", "L. Rosasco"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Multisource deep learning for human pose estimation", "author": ["W. Ouyang", "X. Chu", "X. Wang"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Matrix variate distributions", "author": ["A.K. Gupta", "D.K. Nagar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "In ICASSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multi-task learning is based on the idea that the performance can be improved using related tasks as an inductive bias [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 2, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 3, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 4, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 5, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 140, "endOffset": 155}, {"referenceID": 6, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 7, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 8, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 9, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 10, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 11, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 12, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 13, "context": "This fundamental idea has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [2, 3, 4, 5, 6], and multi-task relationship learning that models the inherent task relationship [7, 8, 9, 10, 11, 12, 13, 14].", "startOffset": 237, "endOffset": 266}, {"referenceID": 14, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 15, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 16, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 17, "context": "This idea has motivated some latest deep learning methods for learning multiple tasks [15, 16, 17, 18], which learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer but without inferring the task relationships.", "startOffset": 86, "endOffset": 102}, {"referenceID": 18, "context": "in higher layers with increasing task discrepancy [19, 20], hence the sharing of all feature layers may be risky to negative-transfer.", "startOffset": 50, "endOffset": 58}, {"referenceID": 19, "context": "in higher layers with increasing task discrepancy [19, 20], hence the sharing of all feature layers may be risky to negative-transfer.", "startOffset": 50, "endOffset": 58}, {"referenceID": 20, "context": "DRN models the task relationship by imposing matrix normal priors [21] over network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 122, "endOffset": 130}, {"referenceID": 22, "context": "Since deep models pre-trained with large-scale repositories such as ImageNet are representative for general-purpose tasks [19, 22], the proposed DRN model is trained by fine-tuning from the AlexNet model pre-trained on ImageNet [23].", "startOffset": 228, "endOffset": 232}, {"referenceID": 0, "context": "2 Related Work Multi-task learning (MTL) [1] is an important learning paradigm that jointly learns multiple tasks by exploiting shared structural representation and improves generalization by using related tasks as an inductive bias.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 2, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 3, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 212, "endOffset": 221}, {"referenceID": 4, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 261, "endOffset": 267}, {"referenceID": 5, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 261, "endOffset": 267}, {"referenceID": 6, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 7, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 8, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 383, "endOffset": 392}, {"referenceID": 9, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 10, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 11, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 12, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 13, "context": "There are generally two categories of approaches to multi-task learning: multi-task feature learning, which learns a shared feature representation such that the dataset bias across different tasks can be reduced [2, 3, 4] or the outlier tasks can be identified [5, 6]; and multi-task relationship learning, which explicitly models the task relationship in the forms of task grouping [7, 8, 9] or task covariance [10, 11, 12, 13, 14].", "startOffset": 412, "endOffset": 432}, {"referenceID": 23, "context": "Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind samples [24, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 22, "context": "Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind samples [24, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 18, "context": "The learned deep representations can manifest invariant factors underlying different populations and are transferable across similar tasks [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 69, "endOffset": 77}, {"referenceID": 19, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 69, "endOffset": 77}, {"referenceID": 25, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 99, "endOffset": 107}, {"referenceID": 26, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 99, "endOffset": 107}, {"referenceID": 14, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 15, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 16, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 17, "context": "Hence, deep neural networks have been explored for domain adaptation [25, 20], multimodal learning [26, 27] and multi-task learning [15, 16, 17, 18], where significant performance gains have been obtained.", "startOffset": 132, "endOffset": 148}, {"referenceID": 14, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 214, "endOffset": 222}, {"referenceID": 17, "context": "Based on the assumption that deep neural networks can learn transferable representations across different tasks, most prior multi-task deep learning methods for natural language processing [15] and computer vision [17, 18] learn a shared representation in the feature layers and multiple independent classifiers in the classifier layer without inferring the task relationships.", "startOffset": 214, "endOffset": 222}, {"referenceID": 15, "context": "[16] proposed tree-based priors to the weights in the classifier layer, which learns to organize the classes into a tree hierarchy and transfers knowledge to infrequent classes for label-scarcity mitigation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "While their method is specifically designed for multi-class classification instead of multi-task learning, the sharing of all feature layers may still be vulnerable to negative-transfer, as the higher layers of deep networks are tailored to fit specific tasks and may not be safely transferable [19].", "startOffset": 295, "endOffset": 299}, {"referenceID": 18, "context": "Since deep features eventually transition from general to specific along the network [19]: (1) convolutional layers conv1\u2013conv5 can learn transferable features, hence their parameters {F}l=1 are shared among the multiple tasks; (2) fully connected layers fc6\u2013fc8 are tailored to fit task-specific variations, hence their task-specific parameters {F t } 8 l=6 are jointly modeled via matrix normal priors for learning the task relationships.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 18, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 21, "context": "1 Model We start with the deep convolutional neural network (CNN) [23], a strong model to learn transferable features that are well adaptive to multiple tasks [18, 19, 22].", "startOffset": 159, "endOffset": 171}, {"referenceID": 22, "context": "We extend the AlexNet architecture [23], which is comprised of five convolutional layers (conv1\u2013 conv5) and three fully connected layers (fc6\u2013fc8).", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "We will not describe how to compute the convolutional layers as these layers can learn transferable features [19] and we will simply share their parameters {F t = F }l=1 across different tasks, without modeling the task relationships in these layers.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "To benefit from pre-training and fine-tuning, we copy these layers from a model pre-trained from ImageNet 2012 [19, 28], freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5, which can preserve the efficacy of fragile co-adaptation.", "startOffset": 111, "endOffset": 119}, {"referenceID": 27, "context": "To benefit from pre-training and fine-tuning, we copy these layers from a model pre-trained from ImageNet 2012 [19, 28], freeze conv1\u2013conv3 and fine-tune conv4\u2013conv5, which can preserve the efficacy of fragile co-adaptation.", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": "As revealed by the latest literature findings [19], the deep features in standard CNNs must eventually transition from general to specific along the network, and the feature transferability decreases while the task discrepancy increases, making the features in higher layers fc6\u2013fc8 unsafely transferable across different tasks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 15, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 16, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 17, "context": "Most previous methods generally assume that the multiple tasks are well correlated given the shared representation learned by the feature layers conv1\u2013fc7 of the deep neural network [15, 16, 17, 18].", "startOffset": 182, "endOffset": 198}, {"referenceID": 23, "context": ", the network parameter of each layer is independent on the network parameters of the other layers, which is a common assumption made by most neural network methods [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 16, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 17, "context": "This parameter sharing strategy is a relaxation of existing methods [16, 17, 18], which share all lower-layers except for the classifier layer.", "startOffset": 68, "endOffset": 80}, {"referenceID": 18, "context": "We do not share task-specific layers (fc6\u2013fc8) so as to potentially mitigate the negative-transfer [19, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 19, "context": "We do not share task-specific layers (fc6\u2013fc8) so as to potentially mitigate the negative-transfer [19, 20].", "startOffset": 99, "endOffset": 107}, {"referenceID": 20, "context": "In this paper, we define the following prior based on Gaussian and matrix normal distributions [21]: p ( W c )", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "We take similar strategy as [11, 13] and minimize its convex upper bound \u2211L l=L0 (", "startOffset": 28, "endOffset": 36}, {"referenceID": 12, "context": "We take similar strategy as [11, 13] and minimize its convex upper bound \u2211L l=L0 (", "startOffset": 28, "endOffset": 36}, {"referenceID": 14, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 15, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 16, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 17, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 58, "endOffset": 74}, {"referenceID": 10, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 11, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 12, "context": "Our work contrasts from previous multi-task deep learning [15, 16, 17, 18] and relationship learning methods [11, 12, 13] in several important aspects: (1) We define the prior on multiple task-specific layers L0 6 l 6 L, while previous deep learning methods do not define the prior and merely rely on the bet that the shared deep features are good enough for multi-task learning (which may not be true due to Yosinski et al.", "startOffset": 109, "endOffset": 121}, {"referenceID": 18, "context": "[19]), and previous relationship learning methods are not designed in a deep architecture (which cannot learn transferable features for multi-task learning); (2) We define the prior by a task covariance matrix \u03a9 shared by all categories (which can learn the task relationship more accurately when category-wise data is scarce), while previous relationship learning methods define the prior for binary or regression problems; (3) We define the prior with a task mean parameter mc that captures the common task component of multiple tasks, while previous relationship learning methods define the prior with zero-mean Gaussian that cannot model the common task component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "(7) Such a mini-batch SGD can be easily implemented via the Caffe framework for CNNs [28].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Since training a deep CNN requires a large amount of labeled data, which is prohibitive for many multitask learning problems, we fine-tune from an AlexNet model pre-trained on ImageNet 2012 as [19].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Though the derivation is similar to [11], our method learns the task relationship \u03a9 from multiple classes, while [11] learns the task relationship \u03a9 from binary classes.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Though the derivation is similar to [11], our method learns the task relationship \u03a9 from multiple classes, while [11] learns the task relationship \u03a9 from binary classes.", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "When the labeled data is very limited for each category, the binary-class method [11] may not infer the task relationship accurately.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 2, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 4, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 3, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 5, "context": "Learning shared features by exploiting feature covariance has been extensively studied for multitask feature learning [2, 3, 5, 4, 6].", "startOffset": 118, "endOffset": 133}, {"referenceID": 28, "context": "1 Setup Office-Caltech [29, 30] This dataset is the standard benchmark for multi-task learning and transfer learning.", "startOffset": 23, "endOffset": 31}, {"referenceID": 29, "context": "1 Setup Office-Caltech [29, 30] This dataset is the standard benchmark for multi-task learning and transfer learning.", "startOffset": 23, "endOffset": 31}, {"referenceID": 28, "context": "The Office part [29] consists of 4,652 images in 31 categories collected from three distinct domains (tasks): Amazon (A), which contains images downloaded from amazon.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "This dataset is organized by selecting the 10 common categories shared by the Office dataset and the Caltech-256 (C) dataset [30], hence it consists of four domains (tasks).", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 151, "endOffset": 154}, {"referenceID": 10, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 196, "endOffset": 204}, {"referenceID": 12, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 196, "endOffset": 204}, {"referenceID": 17, "context": "We compare with a variety of methods: Single-Task Softmax Regression (STSR), Multi-Task Feature Learning (MTFL) [3], Robust Multi-Task Learning (RMTL) [5], Multi-Task Relationship Learning (MTRL) [11, 13], and Multi-Task Deep Convolutional Neural Network (MTCNN) [18].", "startOffset": 263, "endOffset": 267}, {"referenceID": 2, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 4, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 10, "context": "While using binary classifier for multi-class problems is less effective due to unbalanced classes, and inefficient for large number of categories, most existing methods [3, 5, 11] generally follow this paradigm.", "startOffset": 170, "endOffset": 180}, {"referenceID": 10, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 12, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 4, "context": "We mainly follow standard evaluation protocol for multi-task learning and randomly select 5%, 10%, and 20% samples from each task as the training set and utilize the rest of the samples as the test set [11, 13, 5].", "startOffset": 202, "endOffset": 213}, {"referenceID": 10, "context": "Table 1: Multi-class accuracy on the Office-Caltech dataset with standard evaluation protocol [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "Table 2: Multi-class accuracy on the ImageCLEF-DA dataset with standard evaluation protocol [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 4, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 10, "context": "We perform parameter selection for all methods using five-fold cross-validation on the training set, which is consistent with the baseline methods [3, 5, 11].", "startOffset": 147, "endOffset": 157}, {"referenceID": 18, "context": "For CNN-based methods, we adopt the fine-tuning architecture [19], however, due to limited training examples in our datasets, we fix convolutional layers conv1\u2013 conv3 that were copied from pre-trained model, fine-tune conv4\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8, both via back propagation.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "9 momentum and the learning rate annealing strategy implemented in Caffe [28], and cross-validate the learning rate between 10 and 10 by a multiplicative step-size 10.", "startOffset": 73, "endOffset": 77}], "year": 2015, "abstractText": "Deep neural networks trained on large-scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer and labeling mitigation. As deep features eventually transition from general to specific along the network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the task-specific layers. In this work, we propose a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors over the network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. By jointly learning the transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and undertransfer in the classifier layer. Empirical evidence shows that DRN yields state-ofthe-art classification results on standard multi-domain object recognition datasets.", "creator": "LaTeX with hyperref package"}}}