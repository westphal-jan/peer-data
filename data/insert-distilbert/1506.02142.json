{"id": "1506.02142", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "abstract": "deep learning tools have recently also gained much attention in applied test machine learning. however such tools for both regression and classification don't allow us to capture extreme model uncertainty. bayesian models initially offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost.", "histories": [["v1", "Sat, 6 Jun 2015 12:30:43 GMT  (1715kb,D)", "http://arxiv.org/abs/1506.02142v1", "10 pages, 7 figures"], ["v2", "Thu, 27 Aug 2015 13:39:15 GMT  (2065kb,D)", "http://arxiv.org/abs/1506.02142v2", "10 pages, 7 figures"], ["v3", "Sun, 27 Sep 2015 15:15:31 GMT  (2068kb,D)", "http://arxiv.org/abs/1506.02142v3", "11 pages, 6 figures"], ["v4", "Sat, 31 Oct 2015 19:45:05 GMT  (2069kb,D)", "http://arxiv.org/abs/1506.02142v4", "11 pages, 6 figures; Minor corrections in experiments section"], ["v5", "Wed, 25 May 2016 18:48:52 GMT  (2384kb,D)", "http://arxiv.org/abs/1506.02142v5", "11 pages, 6 figures; ICML proceedings version"], ["v6", "Tue, 4 Oct 2016 16:50:26 GMT  (2383kb,D)", "http://arxiv.org/abs/1506.02142v6", "12 pages, 6 figures; fixed a mistake with standard error and added a new table with updated results (marked \"Update [October 2016]\"); Published in ICML 2016"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yarin gal", "zoubin ghahramani"], "accepted": true, "id": "1506.02142"}, "pdf": {"name": "1506.02142.pdf", "metadata": {"source": "CRF", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "authors": ["Yarin Gal", "Zoubin Ghahramani"], "emails": ["yg279@cam.ac.uk", "zg201@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3]. Tools such as the multilayer perceptron (MLP, also known as neural network), dropout, convolutional neural networks (convnets), and others are used extensively. However, these are fields in which representing model uncertainty is of crucial importance [4, 5]. Furthermore, there has been a recent shift in many of these fields towards the use of Bayesian uncertainty [6, 7, 8].\nStandard deep learning tools for regression and classification do not allow us to capture model uncertainty. In classification, predictive probabilities obtained at the end of the pipeline (the softmax output) are often erroneously interpreted as model confidence. A model can be uncertain about its predictions even with a high softmax output (fig. 1). Passing a point estimate of the mean of a function (solid line 1a) through a softmax (solid line 1b) results in highly confident extrapolations with x\u2217 (a point far from the training data) classified as class 1 with probability 1. However, passing the distribution (shaded area 1a) through a softmax (shaded area 1b) better reflects classification uncertainty far from the training data.\nRepresenting model uncertainty is important for the practical deployment of deep learning systems as well. With model confidence at hand we can take appropriate actions in different scenarios. For example, in the case of classification, a model might return a result with high uncertainty. In this case we might decide to pass the input to a human to classify. This can happen in a post office, sorting letters according to their zip code, or in a nuclear power plant with a system responsible for critical infrastructure [9]. Uncertainty is important in reinforcement learning (RL) as well [10]. With uncertainty information an agent can decide when to exploit and when to explore its environment. Recent advances in RL have made use of MLPs for Q-value function approximation. These are functions that estimate the quality of different actions an agent can make. Epsilon greedy search is often used in this setting, where the agent selects its best action following its current estimation with some probability, and explores otherwise. With uncertainty estimates over the agent\u2019s Q-value function, techniques such as Thompson sampling [11] can be used to learn much faster.\nBayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost. It is perhaps surprising then that it is possible to use many of the\nar X\niv :1\n50 6.\n02 14\n2v 1\n[ st\nat .M\nL ]\n6 J\nun 2\n01 5\nproperties often viewed as held by Bayesian models within deep learning without changing either the models or the optimisation. We show that the use of dropout in MLPs can be interpreted as a Bayesian approximation of a well known Bayesian model: the Gaussian process (GP) [12]. Dropout is used in almost all models in deep learning as a way to avoid over-fitting [13]. We develop tools to represent model uncertainty of existing dropout MLP models \u2013 extracting information that has been thrown away so far. This mitigates the problem of representing model uncertainty in deep learning without sacrificing either computational performance or test accuracy.\nModel uncertainty is often evaluated in exploratory research, studying the properties of the estimated model confidence on different tasks. In this paper we prove the link between the Gaussian process and dropout, and develop the tools necessary to represent uncertainty in deep learning. We then perform an extensive exploratory assessment of the properties of the uncertainty obtained from dropout MLPs and convnets on the tasks of regression and classification. We compare the uncertainty obtained from different model architectures and non-linearities, both on the tasks of interpolation and extrapolation. We show that model uncertainty is important for classification tasks using MNIST as a concrete example. Lastly, we give a quantitative assessment of model uncertainty in the setting of reinforcement learning, on a task similar to that used in deep reinforcement learning [14]."}, {"heading": "2 Related Research", "text": "It has long been known that infinitely wide (single hidden layer) MLPs with distributions placed over their weights converge to Gaussian processes [15, 16]. This known relation is through a limit argument that does not allow the transfer of properties from the Gaussian process to finite MLPs. Finite MLPs with distributions placed over the weights have been studied extensively as Bayesian neural networks [15, 17]. These offer robustness to over-fitting as well, but with challenging inference and additional computational costs. Variational inference has been applied to these models, but with limited success [18, 19, 20]. Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25]. These have been used to obtain new approximations for Bayesian neural networks that perform as well as dropout [26]. However these models come with a prohibitive computational cost. To represent uncertainty, the number of parameters in these models is doubled for the same network size. Further, they require more time to converge and do not improve on existing techniques. Given that reasonable uncertainty estimates can be cheaply obtained from common dropout models, this results in unnecessary additional computation."}, {"heading": "3 Dropout as a Bayesian Approximation", "text": "We show that a multilayer perceptron (MLP) with arbitrary depth and non-linearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to the probabilistic deep Gaussian process model [27]. We would like to stress that no simplifying assumptions are made on the use of dropout in the literature, and that the results derived are applicable to any network architecture that makes use of dropout exactly as it appears in practical applications. We show that the dropout objective, in effect, minimises the Kullback\u2013Leibler divergence between an approximate model and the deep Gaussian process. Due to space constraints we refer the reader to the appendix for an in depth review of dropout, Gaussian processes, and variational inference (section 2), as well as the main derivation (sections 3). The results are summarised here. In the next section we obtain uncertainty estimates for dropout MLPs.\nWe denote by E a loss function such as the softmax loss or the euclidean loss. We denote by Wi weight matrices of dimensions Ki \u00d7 Ki\u22121, and by bi the bias vectors of dimensions Ki for each layer i = 1, ..., L. We denote by y\u0304 the outputs of an MLP model and by y the observed outputs\ncorresponding to inputs x. During MLP optimisation, the loss term is scaled by the learning rate r1 and a regularisation term is added. We often use L2 regularisation weighted by some weight decay r2, resulting in a minimisation objective (often referred to as cost),\nLdropout := r1E(y, y\u0304) + r2 L\u2211\ni=1\n( ||Wi||22 + ||bi||22 ) . (1)\nWith dropout, we sample binary variables for every input point and for every network unit in each layer. Each binary variable takes value 1 with probability pi for layer i. A unit is dropped (i.e. its value is set to zero) for a given input if its corresponding binary variable takes value 0. We use the same binary variable values in the backward pass propagating the derivatives to the parameters. The probabilities pi might be optimised as well.\nThe deep Gaussian process is a powerful tool in statistics that allows us to model distributions over functions. Assume we are given a covariance function of the form K(x,y) = \u222b p(w)p(b)\u03c3(wTx+\nb)\u03c3(wTy + b)dwdb with some element-wise non-linearity \u03c3(\u00b7) and distributions p(w), p(b). It is straightforward to show that K(x,y) is a valid PSD covariance function \u2013 it is an example of a marginalised covariance function [28]. In section 3 in the appendix we show that a deep Gaussian process with L layers and covariance function K(x,y) can be approximated using the following parametric probabilistic model. Let bi be a Ki\u22121 dimensional binary random vector for each layer i. Write1 M\u0302i = Midiag(bi) with matrices Mi of dimensions Ki \u00d7 Ki\u22121 and let \u03c9 = {M\u0302i}Li=1. We use a parametrisation for \u03c9 which is similar to the well known Gaussian distribution re-parametrisation, where x \u223c N (\u00b5, s2) is written as x = \u00b5 + s with \u223c N (0, 1). When the Gaussian distributions is used in variational inference, \u00b5 and s would be treated as a variational parameter. In our case, Mi are treated as variational parameters. The variational distribution, q(\u03c9), is defined by\nbi,j \u223c Bernoulli(pi) for i = 1, ..., L, j = 1, ...,Ki\u22121 (2) given some probabilities pi. In effect, q(\u03c9) is a distribution over matrices whose columns can randomly be set to zero. The binary variable bi,j = 0 indicates that unit j in layer i\u2212 1 is dropped out as an input to layer i.\nGiven bias vectors mi of dimensions Ki for each layer, the approximate model q(y|x) is defined as q(y|x) = \u222b p(y|x,\u03c9)q(\u03c9)d\u03c9 (3)\np(y|x,\u03c9) = N ( y; y\u0302(x, M\u03021, ..., M\u0302L), \u03c4 \u22121ID )\n(4)\ny\u0302(x, M\u03021, ..., M\u0302L) =\n\u221a 1\nKL M\u0302L\u03c3\n( ... \u221a 1\nK1 M\u03022\u03c3\n( M\u03021x + m1 ) ... ) (5)\nfor some precision parameter \u03c4 > 0 and some element-wise non-linearity \u03c3(\u00b7).2 This approximate model can be derived by placing an approximating variational distribution over each component of a spectral decomposition of the GPs\u2019 covariance functions. This spectral decomposition maps each layer of the deep GP to a layer of explicitly represented hidden units (section 3.1 in the appendix).\nIn variational inference we minimise the KL divergence between the approximate model q(\u03c9) above and the posterior of the full deep GP \u2013 p(Y|X). The minimisation objective is obtained from this KL divergence by Monte Carlo integration over \u03c9 with a single sample (section 3.4 in the appendix):\nLGP-MC \u221d \u03b3\u03c4E(y, y\u0302) + \u03b3 L\u2211\ni=1\n( pi 2 ||Mi||22 + 1 2 ||mi||22 ) (6)\nwith E(y, y\u0302) = \u2212 log p(y|x,\u03c9) and bi,j realisations from the Bernoulli distribution. This is identical to eq. (1) for an appropriate setting of precision hyper-parameter \u03c4 and scale parameter \u03b3."}, {"heading": "4 Obtaining Model Uncertainty", "text": "We next derive new results extending on the above, showing that model uncertainty can be obtained from dropout MLP models.\n1The diag(\u00b7) operator maps a vector to a diagonal matrix whose diagonal is the elements of the vector. 2We derive the case for regression where y \u2208 RD but the extension to classification is simple. This is given\nin section 3.5 in the appendix.\nFollowing section 2.3 in the appendix, our approximate predictive distribution is given by q(y\u2217|x\u2217) = \u222b p(y\u2217|x\u2217,\u03c9)q(\u03c9)d\u03c9\nwhere \u03c9 = {M\u0302i}Li=1 is our set of random variables for a model with L layers. We will perform moment-matching and estimate the first two moments of the predictive distribution empirically. More specifically, we sample T sets of vectors of realisations from the Bernoulli distribution {bt1, ..., btL}Tt=1 with probabilities {p1, ..., pL} and estimate\nEq(y\u2217|x\u2217)(y\u2217) \u2248 1\nT T\u2211 t=1 y\u0302\u2217(x\u2217, M\u0302t1, ..., M\u0302 t L)\nfollowing proposition C in the appendix. We refer to this Monte Carlo estimate as MC dropout. In practice this is equivalent to performing T forward passes through the network and averaging the results.\nThis result has been presented in the literature before as model averaging. We have given a new derivation for this result which allows us to derive sensible uncertainty estimates as well. Srivastava et al. [13, section 7.5] have reasoned empirically that this quantity can be approximated by averaging the weights of the network (multiplying each Wi by pi at test time, referred to as standard dropout).\nWe estimate the predictive uncertainty in the same way:\nEq(y\u2217|x\u2217) ( (y\u2217)T (y\u2217) ) \u2248 \u03c4\u22121ID + 1\nT T\u2211 t=1 y\u0302\u2217(x\u2217, M\u0302t1, ..., M\u0302 t L) T y\u0302\u2217(x\u2217, M\u0302t1, ..., M\u0302 t L)\nfollowing proposition D in the appendix. In conclusion, to obtain the model\u2019s predictive variance we estimate: Varq(y\u2217|x\u2217) ( y\u2217 ) \u2248\n\u03c4\u22121ID + 1\nT T\u2211 t=1 y\u0302\u2217(x\u2217, M\u0302t1, ..., M\u0302 t L) T y\u0302\u2217(x\u2217, M\u0302t1, ..., M\u0302 t L)\u2212 Eq(y\u2217|x\u2217)(y\u2217)TEq(y\u2217|x\u2217)(y\u2217)\nwhich equals the sample variance of T forward passes through the MLP plus the inverse model precision. As the model precision is often embedded in the ratio between the learning rate r1 and weight decay r2 in MLPs, one would set \u03c4 = r1r2 .\nThe predictive distribution q(y\u2217|x\u2217) is expected to be highly multi-modal, and the above approximation only gives a glimpse into its properties. This is because the approximating variational distribution placed on each weight matrix column is bi-modal, and as a result the joint distribution over each layer\u2019s weights is multi-modal (section 3.2 in the appendix).\nNote that the dropout MLP model itself is not changed. To estimate the predictive mean and predictive uncertainty we simply collect the results of stochastic forward passes through the model. As a result, this information can be used with existing MLP models trained with dropout. Furthermore, the forward passes can be done concurrently, resulting in constant time complexity identical to that of standard dropout."}, {"heading": "5 Experiments", "text": "Model uncertainty is often evaluated in exploratory research, studying the properties of the estimated model confidence on different tasks. We next perform an extensive assessment of the properties of the uncertainty estimates obtained from dropout MLPs and convnets on the tasks of regression and classification. We compare the uncertainty obtained from different model architectures and nonlinearities, both on the tasks of interpolation and extrapolation. We show that model uncertainty is important for classification tasks using MNIST [29] as an example. Lastly, we give an example use of the model\u2019s uncertainty in a Bayesian pipeline. We give a quantitative assessment of the model\u2019s performance in the setting of reinforcement learning on a task similar to that used in deep reinforcement learning [14].\nUsing the results from the previous section, we begin by qualitatively evaluating the dropout MLP uncertainty on two regression tasks. We use two regression datasets and model scalar functions which are easy to visualise. These are tasks one would often come across in real-world data analysis. We use a subset of the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (referred to as CO2) [30] to evaluate model extrapolation, and the reconstructed solar irradiance dataset (referred to as solar) [31] to assess model\ninterpolation. The datasets are fairly small, with each dataset consisting of about 200 data points. We centre and normalise both datasets.\nFor the task of classification we evaluate the LeNet convolutional neural network model [32] on the MNIST dataset [29]. We show that softmax output probabilities cannot fully capture model uncertainty, and assess model uncertainty in realistic classification cases. Further, we plot a histogram of the samples and assess the moment matching simplifying assumptions above."}, {"heading": "5.1 Model Uncertainty in Regression Tasks \u2013 Extrapolation", "text": "We trained several models on the CO2 dataset. We use MLPs with either 4 or 5 hidden layers and 1024 hidden units. We use either ReLU non-linearities or TanH non-linearities in each network, and use dropout probabilities of either 0.1 or 0.2. We ran a stochastic gradient descent optimiser for 1,000,000 iterations (until convergence) with learning rate policy base-lr \u2217 (1 + \u03b3 \u2217 iter)\u2212p with \u03b3 = 0.0001, p = 0.25 and momentum 0.9. We initialise the bias at 0 and initialise the weights uniformly from [\u2212 \u221a 3/fan-in, \u221a 3/fan-in]. We use no mini-batch optimisation as the data is fairly small and with high frequencies. The learning rates used are 0.01 with weight decay of 1e\u221206 for CO2 and 0.05 with weight decay of 5e\u221206 for solar (corresponding to a high noise precision of 1e5). This is to model the low observation noise in the data due to the scaling and high frequencies.\nThe extrapolation results are shown figure 2. The model is trained on the training data (left to the dashed blue line), and tested on the entire dataset. Fig. 2a shows the results for standard dropout (i.e. with weight averaging and without assessing model uncertainty) for the 5 layer ReLU model. Fig. 2b shows the results obtained from a Gaussian process with a squared exponential covariance function for comparison. Fig. 2c shows the results of the same network as in fig. 2a, but with MC dropout to evaluate the predictive mean and uncertainty for the training and test sets. Lastly, fig. 2d shows the same using the TanH network with 5 layers (plotted with 8 times the standard deviation for visualisation purposes). The shades of blue represent model uncertainty: each colour gradient represents half a standard deviation (in total, predictive mean plus/minus 2 standard deviations are shown, representing 95% confidence). Not plotted are the models with 4 layers as these converge to the same results.\nExtrapolating the observed data, none of the models can capture the periodicity (although with a suitable covariance function the GP will capture it well). The standard dropout MLP model (fig. 2a) predicts value 0 for point x\u2217 (marked with a dashed red line) with high confidence, even though it is clearly not a sensible prediction. The GP model represents this by increasing its predictive uncertainty \u2013 in effect declaring that the predictive value might be 0 but the model is uncertain. This behaviour is captured in MC dropout as well. Even though the models in figures 2 have an incorrect predictive mean, the increased standard deviation expresses the models\u2019 uncertainty about the point.\nNote that the uncertainty is increasing far from the data for the ReLU model, whereas for the TanH model it stays bounded. This is not surprising, as ReLU and TanH approximate different GP covariance functions (section 3.1 in the appendix) and TanH saturates whereas ReLU does not. It is standard in GPs to have different uncertainty estimates for different covariance functions. For the TanH model we assessed the uncertainty using both dropout probability 0.1 and dropout probability 0.2. Models initialised with dropout probability 0.1 initially exhibit smaller uncertainty than the ones initialised with dropout probability 0.2, but towards the end of the optimisation when the model has converged the uncertainty is almost undistinguishable. It is worth mentioning that we attempted to fit the data with models with a smaller number of layers unsuccessfully."}, {"heading": "5.2 Model Uncertainty in Regression Tasks \u2013 Interpolation", "text": "For interpolation we repeat the experiment above with ReLU networks with 5 hidden layers and the same setup on a new dataset \u2013 solar irradiance. We use base learning rate of 5e\u22123 and weight decay of 5e\u22127.\nInterpolation results are shown in fig. 3. Fig. 3a shows interpolation of missing sections (bounded between pairs of dashed blue lines) for the Gaussian process with squared exponential covariance function, as well the function value on the training set. In red is the observed function, in green are the missing sections, and in blue is the model predictive mean. Fig. 3b shows the same for the ReLU dropout model with 5 layers.\nBoth models interpolate the data well, with increased uncertainty over the missing segments. However the GP\u2019s uncertainty is larger and captures the true function within its 2 standard deviation error bars. The observed uncertainty in MC dropout is similar to that of [33]. Note that this is usual with variational techniques, where model uncertainty is often under-estimated.\nThe number of forward iterations used to estimate the uncertainty (T ) was 1000 for drawing purposes. A much smaller numbers can be used to get a reasonable estimation to the predictive mean and uncertainty (see fig. 4 for example with T = 10)."}, {"heading": "5.3 Model Uncertainty in Classification Tasks \u2013 MNIST classification", "text": "To assess model classification confidence in a real world example we test a convolutional neural network trained on the MNIST dataset. We trained the LeNet convolutional neural network model with dropout applied after the fully connected inner-product layer (after the ReLU operation \u2013 the usual way dropout is used in convnets). We used dropout probability of 0.5. We trained the model for 1,000,000 iterations with the same learning rate policy as before with \u03b3 = 0.0001 and p = 0.75. We used Caffe [34] reference implementation for this experiment.\nWe evaluated the trained model on a continuously rotated image of the digit 1 (shown on the X axis of fig. 5). We scatter 100 forward passes of the softmax input (the output from the last fully connected layer, fig. 5a), as well as of the softmax output for each class (fig. 5b). For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].\nThe plots show the softmax input value and softmax output value for the 3 digits with the largest values for the inputs on the X axis. When the softmax input for a class is larger than that of all other classes (class 1 for the first 5 images, class 5 for the next 2 images, and class 7 for the rest in fig 5a), the model predicts the corresponding class. Looking at the softmax input values, if the uncertainty envelope of a class is far from that of other classes\u2019 (for example the left most image) then the input is classified with high confidence. On the other hand, if the uncertainty envelope intersects that of other classes (such as in the case of the middle input image), then even though the softmax output can be arbitrarily high (as far as 1 if the mean is far from the means of the other classes), the softmax output uncertainty can be as large as the entire space. This signifies the model\u2019s uncertainty in its softmax output value \u2013 i.e. in the prediction. In this scenario it is not reasonable to use probit to return class 5 for the middle image when its uncertainty is so high. One would expect the model to ask an external annotator for a label for this input.\nNote that the histogram shown in fig. 5a is fairly uni-modal, justifying the moment matching suggested in the previous section. This does not need to hold in general though as we only used a single dropout layer in this model. It is interesting to note that the model becomes very confident about the misclassified 7 labels."}, {"heading": "5.4 Model Uncertainty in Reinforcement Learning", "text": "In reinforcement learning an agent receives various rewards from different states, and its aim is to maximise its expected reward over time. The agent tries to learn to avoid transitioning into states with low rewards, and to pick actions that lead to better states instead. Uncertainty is of great importance in this task \u2013 with uncertainty information an agent can decide when to exploit rewards it knows of, and when to explore its environment.\nRecent advances in RL have made use of MLPs to estimate agents\u2019 Q-value functions (referred to as Q-networks), a function that estimates the quality of different actions the agent can make at different states. This has led to impressive results on Atari game simulations, where agents superseded human performance on a variety of games [14]. Epsilon greedy search was used in this setting, where the agent selects the best action following its current estimation with some probability, and explores otherwise. With our uncertainty estimates given by a dropout Q-network we can use techniques such as Thompson sampling [11] to converge faster than epsilon greedy while avoiding over-fitting.\nWe use code by [35] that replicated the results by [14] with a simpler 2D setting. We simulate an agent in a 2D world with 9 eyes pointing in different angles ahead (depicted in fig. 6). Each eye can sense a single pixel intensity of 3 colours. The agent navigates by using one of 5 actions controlling two motors at its base. An action turns the motors at different angles and different speeds. The environment consists of red circles which give the agent a positive reward for reaching, and green circles which result in a negative reward. The agent is further rewarded for not looking at (white) walls, and for walking in a straight line.\nFor the purpose of this experiment, we used future rewards discount of 0.7, no temporal window, and an experience replay of 30,000. The network starts learning after 1,000 steps, where in the initial 5,000 steps random actions are performed. The networks consist of two ReLU hidden layers of size\n50, with a learning rate and weight decay of 0.001. Stochastic gradient descent was used with no momentum and batch size of 64.\nThe original implementation makes use of epsilon greedy exploration with epsilon changing as\n= min ( 1,max ( min, 1\u2212\nage\u2212 burn-in steps-total\u2212 burn-in )) with steps-total of 200,000, burn-in of 3,000, and min = 0.05.\nWe trained the original model and an additional model with dropout with probability 0.1 applied after every non-linearity. To make use of the dropout Q-network\u2019s uncertainty estimates, we use Thompson sampling instead of epsilon greedy. In effect this means that we perform a single forward pass through the network every time we need to make an action. In replay, we perform a single forward pass and then back-propagate with the sampled Bernoulli random variables.\nIn fig. 7 we show a log plot of the average reward obtained by both the original implementation (in green) and our approach (in blue), as a function of the number of batches. Not plotted is the burn-in intervals of 25 batches (random moves). Thompson sampling gets reward larger than 1 within 25 batches from burn-in. Epsilon greedy takes 175 batches to achieve the same performance. It is interesting to note that our approach seems to get worse results after 1K batches. This is because we are still sampling random moves, whereas epsilon greedy only exploits at this stage."}, {"heading": "6 Conclusions and Future Research", "text": "We have built upon a probabilistic interpretation of dropout which allowed us to obtain model uncertainty out of existing deep learning models. We have studied the properties of this uncertainty in detail, and demonstrated possible applications of interleaving Bayesian models and deep learning models together with a reinforcement learning example. The developments allows us to treat existing models in a new way without the need to introduce additional parameters or computational burden to obtain uncertainty estimates.\nIn future research we aim to assess model uncertainty on adversarial inputs, such as corrupted images that classify incorrectly with high confidence [36]. Adding or subtracting a single pixel from each input dimension is perceived as almost unchanged input to a human eye, but can change classification probabilities considerably. In the high dimensional input space the new corrupted image lies far from the data, and one would expect model uncertainty to increase for such inputs.\nWe compared the dropout uncertainty to that of the Gaussian process model on a variety of tasks on which other GP approximations have been compared as well [33]. In future work we aim to compare the uncertainty of the model to that of Bayesian neural networks such as [26] as well."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Dr Yutian Chen, Mr Christof Angermueller, Mr Roger Frigola, Mr Rowan McAllister, Dr Gabriel Synnaeve, Mr Mark van der Wilk, and Mr Yan Wu for their helpful comments. Yarin Gal is supported by the Google European Fellowship in Machine Learning."}], "references": [{"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P Baldi", "P Sadowski", "D Whiteson"], "venue": "Nature communications, 5", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "J Mart\u0131\u0301nez", "author": ["O Anjos", "C Iglesias", "F Peres"], "venue": "\u00c1 Garc\u0131\u0301a, and J Taboada. Neural networks applied to discriminate botanical origin of honeys. Food chemistry, 175:128\u2013136", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "On the use of artificial neural networks in simulation-based manufacturing control", "author": ["S Bergmann", "S Stelzer", "S Strassburger"], "venue": "Journal of Simulation, 8(1):76\u201390", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Points of significance: Importance of being uncertain", "author": ["M Krzywinski", "N Altman"], "venue": "Nature methods, 10 (9)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic machine learning and artificial intelligence", "author": ["Z Ghahramani"], "venue": "Nature, 521(7553)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Experimental biology: Sometimes Bayesian statistics are better", "author": ["S Herzog", "D Ostwald"], "venue": "Nature, 494", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Editorial", "author": ["D Trafimow", "M Marks"], "venue": "Basic and Applied Social Psychology, 37(1)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network based intrusion detection system for critical infrastructures", "author": ["O Linda", "T Vollmer", "M Manic"], "venue": "Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for reinforcement learning", "author": ["C Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W R Thompson"], "venue": "Biometrika", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1933}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C E Rasmussen", "C K I Williams"], "venue": "The MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G Hinton", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["V Mnih", "K Kavukcuoglu", "D Silver", "A A Rusu", "J Veness"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian learning for neural networks", "author": ["R M Neal"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Computing with infinite networks", "author": ["C K I Williams"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["D J C MacKay"], "venue": "Neural computation, 4 (3)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["G E Hinton", "D Van Camp"], "venue": "Proceedings of the sixth annual conference on Computational learning theory", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Ensemble learning in Bayesian neural networks", "author": ["D Barber", "C M Bishop"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES, 168:215\u2013238", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Practical variational inference for neural networks", "author": ["A Graves"], "venue": "NIPS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Variational Bayesian inference with stochastic search", "author": ["D M Blei", "M I Jordan", "J W Paisley"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-encoding variational Bayes", "author": ["D P Kingma", "M Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D J Rezende", "S Mohamed", "D Wierstra"], "venue": "ICML", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M Titsias", "M L\u00e1zaro-Gredilla"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic variational inference", "author": ["M D Hoffman", "D M Blei", "C Wang", "J Paisley"], "venue": "The Journal of Machine Learning Research, 14(1):1303\u20131347", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Weight uncertainty in neural networks", "author": ["C Blundell", "J Cornebise", "K Kavukcuoglu", "D Wierstra"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Gaussian processes", "author": ["A Damianou", "N Lawrence"], "venue": "AISTATS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Marginalized kernels for biological sequences", "author": ["K Tsuda", "T Kin", "K Asai"], "venue": "Bioinformatics, 18", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "The mnist database of handwritten digits", "author": ["Y LeCun", "C Cortes"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "and the Carbon Dioxide Research Group", "author": ["C D Keeling", "T P Whorf"], "venue": "Atmospheric CO2 concentrations (ppmv) derived from in situ air samples collected at Mauna Loa Observatory, Hawaii", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Solar irradiance reconstruction", "author": ["J Lean"], "venue": "NOAA/NGDC Paleoclimatology Program, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving the Gaussian process sparse spectrum approximation by representing uncertainty in frequency inputs", "author": ["Y Gal", "R Turner"], "venue": "ICML", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y Jia", "E Shelhamer", "J Donahue", "S Karayev", "J Long", "R Girshick", "S Guadarrama", "T Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C Szegedy", "W Zaremba", "I Sutskever", "J Bruna", "D Erhan", "I Goodfellow", "R Fergus"], "venue": "ICLR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 1, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "Research in fields such as physics, biology, and manufacturing\u2014to name a few\u2014is using deep learning tools now more than ever [1, 2, 3].", "startOffset": 125, "endOffset": 134}, {"referenceID": 3, "context": "However, these are fields in which representing model uncertainty is of crucial importance [4, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "However, these are fields in which representing model uncertainty is of crucial importance [4, 5].", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "Furthermore, there has been a recent shift in many of these fields towards the use of Bayesian uncertainty [6, 7, 8].", "startOffset": 107, "endOffset": 116}, {"referenceID": 6, "context": "Furthermore, there has been a recent shift in many of these fields towards the use of Bayesian uncertainty [6, 7, 8].", "startOffset": 107, "endOffset": 116}, {"referenceID": 7, "context": "This can happen in a post office, sorting letters according to their zip code, or in a nuclear power plant with a system responsible for critical infrastructure [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "Uncertainty is important in reinforcement learning (RL) as well [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "With uncertainty estimates over the agent\u2019s Q-value function, techniques such as Thompson sampling [11] can be used to learn much faster.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "We show that the use of dropout in MLPs can be interpreted as a Bayesian approximation of a well known Bayesian model: the Gaussian process (GP) [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "Dropout is used in almost all models in deep learning as a way to avoid over-fitting [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Lastly, we give a quantitative assessment of model uncertainty in the setting of reinforcement learning, on a task similar to that used in deep reinforcement learning [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "It has long been known that infinitely wide (single hidden layer) MLPs with distributions placed over their weights converge to Gaussian processes [15, 16].", "startOffset": 147, "endOffset": 155}, {"referenceID": 14, "context": "It has long been known that infinitely wide (single hidden layer) MLPs with distributions placed over their weights converge to Gaussian processes [15, 16].", "startOffset": 147, "endOffset": 155}, {"referenceID": 13, "context": "Finite MLPs with distributions placed over the weights have been studied extensively as Bayesian neural networks [15, 17].", "startOffset": 113, "endOffset": 121}, {"referenceID": 15, "context": "Finite MLPs with distributions placed over the weights have been studied extensively as Bayesian neural networks [15, 17].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 17, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 18, "context": "Variational inference has been applied to these models, but with limited success [18, 19, 20].", "startOffset": 81, "endOffset": 93}, {"referenceID": 19, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 20, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 21, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 22, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 23, "context": "Recent advances in variational inference introduced new techniques such as sampling-based variational inference and stochastic variational inference [21, 22, 23, 24, 25].", "startOffset": 149, "endOffset": 169}, {"referenceID": 24, "context": "These have been used to obtain new approximations for Bayesian neural networks that perform as well as dropout [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "We show that a multilayer perceptron (MLP) with arbitrary depth and non-linearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to the probabilistic deep Gaussian process model [27].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "It is straightforward to show that K(x,y) is a valid PSD covariance function \u2013 it is an example of a marginalised covariance function [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "We show that model uncertainty is important for classification tasks using MNIST [29] as an example.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "We give a quantitative assessment of the model\u2019s performance in the setting of reinforcement learning on a task similar to that used in deep reinforcement learning [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "We use a subset of the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (referred to as CO2) [30] to evaluate model extrapolation, and the reconstructed solar irradiance dataset (referred to as solar) [31] to assess model", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "We use a subset of the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (referred to as CO2) [30] to evaluate model extrapolation, and the reconstructed solar irradiance dataset (referred to as solar) [31] to assess model", "startOffset": 267, "endOffset": 271}, {"referenceID": 30, "context": "For the task of classification we evaluate the LeNet convolutional neural network model [32] on the MNIST dataset [29].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "For the task of classification we evaluate the LeNet convolutional neural network model [32] on the MNIST dataset [29].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "The observed uncertainty in MC dropout is similar to that of [33].", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "We used Caffe [34] reference implementation for this experiment.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 0, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "For the 12 images, the model predicts classes [1 1 1 1 1 5 5 7 7 7 7 7].", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": "This has led to impressive results on Atari game simulations, where agents superseded human performance on a variety of games [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "With our uncertainty estimates given by a dropout Q-network we can use techniques such as Thompson sampling [11] to converge faster than epsilon greedy while avoiding over-fitting.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "We use code by [35] that replicated the results by [14] with a simpler 2D setting.", "startOffset": 51, "endOffset": 55}, {"referenceID": 33, "context": "In future research we aim to assess model uncertainty on adversarial inputs, such as corrupted images that classify incorrectly with high confidence [36].", "startOffset": 149, "endOffset": 153}, {"referenceID": 31, "context": "We compared the dropout uncertainty to that of the Gaussian process model on a variety of tasks on which other GP approximations have been compared as well [33].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In future work we aim to compare the uncertainty of the model to that of Bayesian neural networks such as [26] as well.", "startOffset": 106, "endOffset": 110}], "year": 2015, "abstractText": "Deep learning tools have recently gained much attention in applied machine learning. However such tools for regression and classification do not allow us to capture model uncertainty. Bayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in multilayer perceptron models (MLPs) can be interpreted as a Bayesian approximation. Results are obtained for modelling uncertainty for dropout MLP models \u2013 extracting information that has been thrown away so far, from existing models. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational performance or test accuracy. We perform an exploratory study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of extrapolation, interpolation, and classification. We show that model uncertainty is important for classification tasks using MNIST as an example, and use the model\u2019s uncertainty in a Bayesian pipeline, with deep reinforcement learning as a concrete example.", "creator": "LaTeX with hyperref package"}}}