{"id": "1506.02155", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Optimal Rates for Random Fourier Features", "abstract": "kernel methods represent one manifestation of the most powerful linear tools built in machine learning to tackle problems long expressed frequently in terms of function values and derivatives due to their capability to represent meaningful and model complex relations. while these methods show good versatility, they are computationally intensive and have consistently poor scalability to transform large data as they require operations on gram matrices. in clear order aiming to mitigate this serious computational limitation, few recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. random fourier features ( rff ) are among the most popular and sometimes widely applied constructions : they provide an easily computable, low - dimensional feature representation for shift - invariant kernels. despite surpassing the popularity of rffs, very little is understood theoretically about their approximation quality. in this mathematical paper, we systematically provide the first detailed theoretical analysis about the theoretical approximation quality of rffs by firmly establishing optimal ( in terms of the approximate rff dimension ) performance guarantees in uniform and $ l ^ r $ ( $ 1 \\ le r & lt ; \\ fine infty $ ) norms. we sometimes also effectively propose a rff approximation to derivatives of a kernel with a theoretical depth study on its approximation quality.", "histories": [["v1", "Sat, 6 Jun 2015 14:37:01 GMT  (25kb)", "http://arxiv.org/abs/1506.02155v1", null], ["v2", "Wed, 4 Nov 2015 22:58:57 GMT  (33kb)", "http://arxiv.org/abs/1506.02155v2", "To appear at NIPS-2015"]], "reviews": [], "SUBJECTS": "math.ST cs.LG math.FA stat.ML stat.TH", "authors": ["bharath k sriperumbudur", "zolt\u00e1n szab\u00f3 0001"], "accepted": true, "id": "1506.02155"}, "pdf": {"name": "1506.02155.pdf", "metadata": {"source": "CRF", "title": "Optimal Rates for Random Fourier Features", "authors": ["Bharath K. Sriperumbudur", "Zolt\u00e1n Szab\u00f3"], "emails": ["bks18@psu.edu", "zoltan.szabo@gatsby.ucl.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n02 15\n5v 1\n[ m\nat h.\nST ]"}, {"heading": "1 Introduction", "text": "Kernel methods [19] have enjoyed tremendous success in solving several fundamental problems of machine learning ranging from classification, regression, feature extraction, dependency estimation, causal discovery, Bayesian inference and hypothesis testing. Such a success owes to their capability to represent and model complex relations by mapping points into high (possibly infinite) dimensional feature spaces. At the heart of all these techniques is the kernel trick, which allows to implicitly compute inner products between these high dimensional feature maps, \u03bb via a kernel function k: k(x,y) = \u3008\u03bb(x), \u03bb(y)\u3009. However, this flexibility and richness of kernels has a price: by resorting to implicit computations these methods operate on the Gram matrix of the data, which raises serious computational challenges while dealing with large scale data. In order to resolve this bottleneck, numerous solutions have been proposed, such as low-rank matrix approximations [28, 9, 1], explicit feature maps designed for additive kernels [26, 14], hashing [22, 12], and random Fourier features (RFF) [16] constructed for shift-invariant kernels, the focus of the current paper.\nRFFs implement an extremely simple, yet efficient idea: instead of relying on the implicit feature map \u03bb associated with the kernel, by appealing to Bochner\u2019s theorem [27]\u2014any bounded, continuous, shift-invariant\n\u2217Contributed equally.\nkernel is the Fourier transform of a probability measure\u2014-[16] proposed an explicit low-dimensional random Fourier feature map \u03c6 obtained by empirically approximating the Fourier integral so that k(x,y) \u2248 \u3008\u03c6(x), \u03c6(y)\u3009. The advantage with this explicit low-dimensional feature representation is that the kernel machine can be efficiently solved in the primal form through fast linear solvers, thereby enabling to handle large scale data. Through numerical experiments, it has also been demonstrated that kernel algorithms constructed using the approximate kernel do not suffer from significant performance degradation [16]. Another advantage with the RFF approach is that unlike low rank matrix approximation approach [28, 9] which also speeds up kernel machines, it approximates the entire kernel function and not just the kernel matrix. This property is particularly useful while dealing with out of sample data and also in online learning applications. The RFF technique has found wide applicability in several areas such as fast function-to-function regression [15], differential privacy preserving [4] and causal discovery [13].\nDespite the success of the RFF method, surprisingly, very little is known about its performance guarantees. To the best of our knowledge, the only paper in the machine learning literature providing certain theoretical insight into the accuracy of kernel approximation via RFF is [16]: it shows that Am := sup{|k(x,y) \u2212 \u3008\u03c6(x), \u03c6(y)\u3009R2m | : x,y \u2208 S} = Op( \u221a\nlog(m)/m) for any compact set S \u2282 Rd, where m is the number of random Fourier features. However, since the approximation proposed by the RFF method involves empirically approximating the Fourier integral, the RFF estimator can be thought of as an empirical characteristic function (ECF). In the probability and statistics literature, the systematic study of ECF-s was initiated by [10] and followed up by [7, 6, 30]. While [10] shows the almost sure convergence of Am to zero, [7, Theorems 1 and 2] and [30, Theorems 6.2 and 6.3] show that the optimal rate is m\u22121/2. In addition, [10] shows that almost sure convergence cannot be attained over the entire space (i.e., Rd) if the characteristic function decays to zero at infinity. Due to this, [7, 30] study the convergence behavior of Am when the diameter of S grows with m and show that almost sure convergence of Am is guaranteed as long as the diameter of S is eo(m). Unfortunately, all these results (to the best of our knowledge) are asymptotic in nature and the only known finite sample guarantee by [16] is non-optimal. In this paper (see Section 3), we present a probabilistic inequality for Am that holds for any m and provides the optimal rate of m\u22121/2 along with guaranteeing the almost sure convergence of Am as long as the diameter of S is eo(m). Apart from the uniform convergence, we also study the convergence of k(x,y) \u2212 \u3008\u03c6(x), \u03c6(y)\u3009R2m in Lr-norm (1 \u2264 r < \u221e) and obtain an optimal convergence rate of m\u22121/2. Traditionally, kernel based algorithms involve computing the value of the kernel. Recently, kernel algorithms involving the derivatives of the kernel (i.e., the Gram matrix consists of derivatives of the kernel computed at training samples) have been used to address numerous machine learning tasks, e.g., semi-supervised or Hermite learning with gradient information [32, 21], nonlinear variable selection [17, 18], (multi-task) gradient learning [29] and fitting of infinite-dimensional exponential family distributions [23]. Given the importance of these derivative based kernel algorithms, similar to [16], which proposed an elegant approach to speed up the classical kernel based methods by approximating the kernel through a finite dimensional random feature map \u03c6, in Section 4, we propose a finite dimensional random feature map approximation to kernel derivatives (which can be used to speed up the above mentioned derivative based kernel algorithms). We present a finite sample bound that quantifies the quality of approximation in uniform and Lr-norms and show the rate of convergence to be m\u22121/2 in both these cases.\nA summary of our contributions are as follows. We\n1. provide the first detailed finite sample performance analysis of RFFs for approximating kernels and their derivatives. 2. prove uniform and Lr convergence on fixed compacts sets with optimal rate in terms of the RFF dimension (m);\n3. provide sufficient conditions for the growth rate of compact sets while preserving almost sure convergence uniformly and in Lr; specializing our result we match the best attainable asymptotic growth rate.\nVarious notations and definitions that are used through the paper are provided in Section 2 along with a brief review of RFF approximation proposed by [16]. The detailed proofs of the results in Sections 3 and 4 are provided in the supplementary material."}, {"heading": "2 Notations & Preliminaries", "text": "In this section, we introduce notations that are used throughout the paper and then present preliminaries on kernel approximation through random feature maps as introduced by [16].\nDefinitions & Notation: For a topological space X , C(X ) (resp. Cb(X )) denotes the space of all continuous (resp. bounded continuous) functions on X . For f \u2208 Cb(X ), \u2016f\u2016\u221e := supx\u2208X |f(x)| is the supremum norm of f (we also write it as \u2016f\u2016X ). Mb(X ) and M1+(X ) is the set of all finite Borel and probability measures on X , respectively. For \u00b5 \u2208 Mb(X ), Lr(X , \u00b5) denotes the Banach space of r-power (r \u2265 1) \u00b5-integrable functions. For X \u2286 Rd, we will use Lr(X ) for Lr(X , \u00b5) if \u00b5 is a Lebesgue measure on X . For f \u2208 Lr(X , \u00b5), \u2016f\u2016Lr(X ,\u00b5) := (\u222b X |f |r d\u00b5 )1/r\ndenotes the Lr-norm of f for 1 \u2264 r < \u221e and we write it as \u2016 \u00b7 \u2016Lr(X ) if X \u2286 Rd and \u00b5 is the Lebesgue measure. For any f \u2208 L1(X ,P) where P \u2208 M1+(X ), we define Pf := \u222b\nX f(x) dP(x) and Pmf := 1 m \u2211m i=1 f(Xi) where (Xi) m i=1 i.i.d.\u223c P, Pm := 1m \u2211m\ni=1 \u03b4Xi is the empirical measure and \u03b4x is a Dirac measure supported on x \u2208 X . supp(P) denotes the support of P. Pm := \u2297mj=1P denotes the m-fold product measure. For v := (v1, . . . , vd) \u2208 Rd, \u2016v\u20162 := \u221a \u2211d i=1 v 2 i . The diameter of A \u2286 Y where (Y, \u03c1) is a metric space is defined as |A|\u03c1 := sup{\u03c1(x, y) : x, y \u2208 Y}. If Y = Rd with \u03c1 = \u2016 \u00b7 \u20162, we denote the diameter of A as |A|. The volume of A \u2286 Rd is defined as vol(A) = \u222b\nA 1 dx respectively. For A \u2286 Rd, we define\nA\u2206 := A \u2212 A = {x \u2212 y : x,y \u2208 A}. conv(A) is the convex hull of A. For a function g defined on open set B \u2286 Rd \u00d7 Rd, \u2202p,qg(x,y) := \u2202\n|p|+|q|g(x,y) \u2202x\np1 1 \u00b7\u00b7\u00b7\u2202x pd d \u2202y q1 1 \u00b7\u00b7\u00b7\u2202y qd d , (x,y) \u2208 B, where p,q \u2208 Nd are multi-indices, |p| = \u2211dj=1 pj and N := {0, 1, 2, . . .}. Define vp = \u220fd j=1 v pj j . For positive sequences (an)n\u2208N, (bn)n\u2208N, an = o(bn) if limn\u2192\u221e anbn = 0. Xn = Op(rn) (resp. Oa.s.(rn)) denotes that Xn rn is bounded in probability (resp. almost surely). \u0393(t) = \u222b\u221e 0 x t\u22121e\u2212x dx is the Gamma function, \u0393 ( 1 2 ) = \u221a \u03c0 and \u0393(t+ 1) = t\u0393(t).\nRandom feature maps: Let k : Rd \u00d7 Rd \u2192 R be a bounded, continuous, positive definite, translationinvariant kernel, i.e., there exists a positive definite function \u03c8 : Rd \u2192 R such that k(x,y) = \u03c8(x \u2212 y), x,y \u2208 Rd where \u03c8 \u2208 Cb(Rd). By Bochner\u2019s theorem [27, Theorem 6.6], \u03c8 can be represented as the Fourier transform of a finite non-negative Borel measure \u039b on Rd, i.e.,\nk(x,y) = \u03c8(x\u2212 y) = \u222b\nRd\ne \u221a \u22121\u03c9T (x\u2212y)d\u039b(\u03c9) (\u22c6) =\n\u222b\nRd\ncos ( \u03c9 T (x\u2212 y) ) d\u039b(\u03c9), (1)\nwhere (\u22c6) follows from the fact that \u03c8 is real-valued and symmetric. Since \u039b(Rd) = \u03c8(0), k(x,y) = \u03c8(0) \u222b e \u221a \u22121\u03c9T (x\u2212y) dP(\u03c9) where P := \u039b\u03c8(0) \u2208 M1+(Rd). Therefore, w.l.o.g., we assume throughout the paper that \u03c8(0) = 1 and so \u039b \u2208 M1+(Rd). Based on (1), [16] proposed an approximation to k by replacing \u039b with its empirical measure, \u039bm constructed from (\u03c9i)mi=1 i.i.d.\u223c \u039b so that resultant approximation can be\nwritten as the Euclidean inner product of finite dimensional random feature maps, i.e.,\nk\u0302(x,y) = 1\nm\nm\u2211\ni=1\ncos ( \u03c9\nT i (x\u2212 y)\n) (\u2217) = \u3008\u03c6(x), \u03c6(y)\u3009\nR2m , (2)\nwhere \u03c6(x) = 1\u221a m (cos(\u03c9T1 x), . . . , cos(\u03c9 T mx), sin(\u03c9 T 1 x), . . . , sin(\u03c9 T mx)) and (\u2217) holds based on the basic trigonometric identity: cos(a\u2212 b) = cos a cos b+sina sin b. This elegant approximation to k is particularly useful in speeding up kernel-based algorithms as the finite-dimensional random feature map \u03c6 can be used to solve these algorithms in the primal thereby offering better computational complexity (than by solving them in the dual) while at the same time not lacking in performance. Apart from these practical advantages, [16, Claim 1] provides a theoretical guarantee that supx,y\u2208S |k\u0302(x,y)\u2212k(x,y)| \u2192 0 as m \u2192 \u221e for any compact set S \u2286 Rd. Formally, [16, Claim 1] showed that\u2014note that (3) is slightly different but more precise than the one in the statement of Claim 1 in [16]\u2014for any \u01eb > 0,\n\u039bm\n({\n(\u03c9i) m i=1 : sup x,y\u2208S |k\u0302(x,y) \u2212 k(x,y)| \u2265 \u01eb\n})\n\u2264 Cd ( |S|\u03c3\u01eb\u22121 ) 2d d+2 e\u2212 m\u01eb2 4(d+2) , (3)\nwhere \u03c32 := \u222b \u2016\u03c9\u20162 d\u039b(\u03c9) and Cd := 2 6d+2 d+2 (( 2 d ) d d+2 + ( d 2 ) 2 d+2 )\n\u2264 27d when d \u2265 2. The condition \u03c32 < \u221e implies that \u03c8 (and therefore k) is twice differentiable. From (3) it is clear that the probability has polynomial tails if \u01eb < |S|\u03c3 (i.e., small \u01eb) and Gaussian tails if \u01eb \u2265 |S|\u03c3 (i.e., large \u01eb) and can be equivalently written as\n\u039bm\n({\n(\u03c9i) m i=1 : sup x,y\u2208S |k\u0302(x,y) \u2212 k(x,y)| \u2265 C\nd+2 2d d |S|\u03c3 \u221a m\u22121 logm\n})\n\u2264 m \u03b14(d+2) (logm)\u2212 dd+2 , (4)\nwhere \u03b1 := 4d\u2212 C d+2 d\nd |S|2\u03c32. For |S| sufficiently large (i.e., \u03b1 < 0), it follows from (4) that\nsup x,y\u2208S\n|k\u0302(x,y) \u2212 k(x,y)| = Op ( |S| \u221a m\u22121 logm ) . (5)\nWhile (5) shows that k\u0302 is a consistent estimator of k in the topology of compact convergence (i.e., k\u0302 convergences to k uniformly over compact sets), the rate of convergence of \u221a\n(logm)/m is not optimal. In addition, the order of dependence on |S| is not optimal. While a faster rate (in fact, an optimal rate) of convergence is desired\u2014better rates in (5) can lead to better convergence rates for the excess error of the kernel machine constructed using k\u0302\u2014, the order of dependence on |S| is also important as it determines the rate at which |S| can be grown as a function of m when m \u2192 \u221e (see Remark 1(ii) for a discussion about the significance of growing |S|). In the following section, we present an analogue of (4)\u2014see Theorem 1\u2014that provides optimal rates and has correct dependence on |S|."}, {"heading": "3 Main results: Approximation of k", "text": "As discussed in Sections 1 and 2, while the random feature map approximation of k introduced by [16] has many practical advantages, it does not seem to be theoretically well understood. The existing theoretical results on the quality of approximation do not provide a complete picture owing to their non-optimality. In this section, we first present our main result (see Theorem 1) that improves upon (4) and provides a rate of m\u22121/2 with logarithm dependence on |S|. We then discuss the consequences of Theorem 1 along with its optimality in Remark 1. Next, in Corollary 2 and Theorem 3, we discuss the Lr-convergence (1 \u2264 r < \u221e) of k\u0302 to k over compact subsets of Rd.\nTheorem 1. Suppose k(x,y) = \u03c8(x \u2212 y), x, y \u2208 Rd where \u03c8 \u2208 Cb(Rd) is positive definite and \u03c32 :=\u222b \u2016\u03c9\u20162 d\u039b(\u03c9) < \u221e. Then for any \u03c4 > 0 and non-empty compact set S \u2282 Rd,\n\u039bm\n({\n(\u03c9i) m i=1 : sup x,y\u2208S |k\u0302(x,y) \u2212 k(x,y)| \u2265 h(d, |S|, \u03c3) +\n\u221a 2\u03c4\u221a\nm\n})\n\u2264 e\u2212\u03c4 ,\nwhere h(d, |S|, \u03c3) := 32 \u221a 2d log(|S|+ 1) + 32 \u221a 2d log(\u03c3 + 1) + 16 \u221a 2d(log(|S|+ 1))\u22121.\nProof (sketch). Note that supx,y\u2208S |k\u0302(x,y) \u2212 k(x,y)| = supg\u2208G |\u039bmg \u2212 \u039bg|, where G := {gx,y(\u03c9) = cos(\u03c9T (x \u2212 y)) : x,y \u2208 S}, which means the object of interest is the suprema of an empirical process indexed by G. Instead of bounding supg\u2208G |\u039bmg\u2212\u039bg| by using Hoeffding\u2019s inequality on a cover of G and then applying union bound as carried out in [16], we use the refined technique of applying concentration via McDiarmid\u2019s inequality, followed by symmetrization and bound the Rademacher average by Dudley entropy bound. The result is obtained by carefully bounding the L2(\u039bm)-covering number of G. The details are provided in Section B.1 of the supplementary material.\nRemark 1.\n(i) Theorem 1 shows that k\u0302 is a consistent estimator of k in the topology of compact convergence as m \u2192 \u221e with the rate of a.s. convergence being \u221a\nm\u22121 log |S| (almost sure convergence is guaranteed by the first Borel-Cantelli lemma). In comparison to (4), it is clear that Theorem 1 provides improved rates with better constants and logarithmic dependence on |S| instead of a linear dependence. (ii) Growing diameter: While Theorem 1 provides almost sure convergence uniformly over compact sets, one might wonder whether it is possible to achieve uniform convergence over Rd. [10, Section 2] showed that such a result is possible if \u039b is a discrete measure but not possible for \u039b that is absolutely continuous w.r.t. the Lebesgue measure (i.e., if \u039b has a density). Since uniform convergence of k\u0302 to k over Rd is not possible for many interesting k (e.g., Gaussian kernel), it is of interest to study the convergence on S whose diameter grows with m. Therefore, as mentioned in Section 2, the order of dependence of rates on |S| is critical. Suppose |Sm| \u2192 \u221e as m \u2192 \u221e (we write |Sm| instead of |S| to show the explicit dependence on m). Then Theorem 1 shows that k\u0302 is a consistent estimator of k in the topology of compact convergence if m\u22121 log |Sm| \u2192 0 as m \u2192 \u221e (i.e., |Sm| = eo(m)) in contrast to the result in (4) which requires |Sm| = o( \u221a\nm/ logm). In other words, Theorem 1 ensures consistency even when |Sm| grows exponential in m where as (4) ensures consistency only if |Sm| does not grow faster than \u221a m. (iii) Optimality: Note that \u03c8 is the characteristic function of \u039b \u2208 M1+(Rd) since \u03c8 is the Fourier transform of \u039b (by Bochner\u2019s theorem). Therefore, the object of interest supx,y\u2208S |k\u0302(x,y) \u2212 k(x,y)| = \u2016\u03c8\u0302 \u2212 \u03c8\u2016S\u2206 , is the uniform norm of the difference between \u03c8 and the empirical characteristic function \u03c8\u0302 = 1m \u2211m i=1 cos(\u3008\u03c9i, \u00b7\u3009), when both are restricted to a compact set S\u2206 \u2282 Rd. The question\nof the convergence behavior of \u2016\u03c8\u0302 \u2212 \u03c8\u2016S\u2206 is not new and has been studied in great detail in probability and statistics literature (e.g., see [10, 30] for d = 1 and [6, 7] for d > 1) where the characteristic function is not just a real-valued symmetric function (like \u03c8) but is Hermitian. [30, Theorems 6.2 and 6.3] show that the optimal rate of convergence of \u2016\u03c8\u0302 \u2212 \u03c8\u2016S\u2206 is m\u22121/2 when d = 1, which matches with our result in Theorem 1. Also Theorems 1 and 2 in [7] show that the logarithmic dependence on |Sm| is optimal. In particular, [7, Theorem 1] matches with the growing diameter result in Remark 1(ii), while [7, Theorem 2] shows that if \u039b is absolutely continuous w.r.t. the Lebesgue measure and if lim supm\u2192\u221e m\n\u22121 log |Sm| > 0, then there exists a positive \u01eb such that lim supm\u2192\u221e \u039b\nm(\u2016\u03c8\u0302 \u2212 \u03c8\u2016Sm,\u2206 \u2265 \u01eb) > 0. This means the rate |Sm| = eo(m) is not only the best possible in general for almost sure convergence, but if faster sequence |Sm| is considered then even stochastic convergence cannot be retained for any characteristic function vanishing at infinity along at\nleast one path. While these previous results match with that of Theorem 1 (and its consequences), we would like to highlight the fact that all these previous results are asymptotic in nature whereas Theorem 1 provides a finite sample probabilistic inequality that holds for any m. We are not aware of any such finite sample result except for the one in [16].\nUsing Theorem 1, one can obtain a probabilistic inequality for the Lr-norm of k\u0302 \u2212 k over any compact set S \u2282 Rd, as given by the following result (proved in Section B.2 of the supplement). Corollary 2. Suppose k satisfies the assumptions in Theorem 1. Then for any 1 \u2264 r < \u221e, \u03c4 > 0 and non-empty compact set S \u2282 Rd,\n\u039bm\n\n\n \n (\u03c9i)\nm i=1 : \u2016k\u0302 \u2212 k\u2016Lr(S) \u2265\n(\n\u03c0d/2|S|d 2d\u0393(d2 + 1)\n)2/r h(d, |S|, \u03c3) + \u221a 2\u03c4\u221a\nm\n \n\n\n \u2264 e\u2212\u03c4 ,\nwhere \u2016k\u0302 \u2212 k\u2016Lr(S) := \u2016k\u0302 \u2212 k\u2016Lr(S\u00d7S) = (\u222b\nS\n\u222b\nS |k\u0302(x,y) \u2212 k(x,y)|r dx dy\n) 1 r\n.\nCorollary 2 shows that \u2016k\u0302\u2212 k\u2016Lr(S) = Oa.s.(m\u22121/2|S|2d/r log |S|) and therefore if |Sm| \u2192 \u221e as m \u2192 \u221e, then consistency of k\u0302 in Lr(Sm)-norm is achieved as long as m\u22121/2|Sm|2d/r log |Sm| \u2192 0 as m \u2192 \u221e. This means, in comparison to the uniform norm in Theorem 1 where |Sm| can grow exponential in m\u03b4 (\u03b4 < 1), |Sm| cannot grow faster than m r 4d to achieve consistency in Lr-norm.\nInstead of using Theorem 1 to obtain a bound on \u2016k\u0302\u2212 k\u2016Lr(S) (this bound may be weak as \u2016k\u0302\u2212 k\u2016Lr(S) \u2264 \u2016k\u0302 \u2212 k\u2016S\u00d7Svol2/r(S) for any 1 \u2264 r < \u221e), a better bound (for 2 \u2264 r < \u221e) can be obtained by directly bounding \u2016k\u0302\u2212 k\u2016Lr(S), as shown in the following result, which is proved in Section B.3 of the supplement. Theorem 3. Suppose k(x,y) = \u03c8(x\u2212 y), x, y \u2208 Rd where \u03c8 \u2208 Cb(Rd) is positive definite. Then for any 1 \u2264 r < \u221e, \u03c4 > 0 and non-empty compact set S \u2282 Rd,\n\u039bm\n\n\n \n (\u03c9i)\nm i=1 : \u2016k\u0302 \u2212 k\u2016Lr(S) \u2265\n(\n\u03c0d/2|S|d 2d\u0393(d2 + 1)\n)2/r (\nC\u2032r m1\u2212max{ 1 2 , 1 r } + \u221a 2\u03c4\u221a m )      \u2264 e\u2212\u03c4 ,\nwhere C\u2032r is a universal constant that depends only on r but not on |S| and m.\nTheorem 3 shows an improved dependence on |S| without the extra log |S| factor given in Corollary 2 and therefore provides a better rate for 2 \u2264 r < \u221e when the diameter of S grows, i.e., \u2016k\u0302 \u2212 k\u2016Lr(Sm)\na.s.\u2192 0 if |Sm| = o(m r 4d ) as m \u2192 \u221e. However, for 1 \u2264 r < 2, Theorem 3 provides a slower rate than Corollary 2 and therefore it is appropriate to use the bound in Corollary 2. While one might wonder why we only considered the convergence of \u2016k\u0302 \u2212 k\u2016Lr(S) and not \u2016k\u0302 \u2212 k\u2016Lr(Rd), it is important to note that the latter is not well-defined because k\u0302 /\u2208 Lr(Rd) even if k \u2208 Lr(Rd)."}, {"heading": "4 Approximation of kernel derivatives", "text": "In the previous section we focused on the approximation of the kernel function where we presented uniform and Lr convergence guarantees on compact sets for the random Fourier feature approximation, and discussed how fast the diameter of these sets can grow to preserve uniform and Lr convergence almost surely. In this section, we propose an approximation to derivatives of the kernel and analyze the uniform and Lr convergence behavior of the proposed approximation. As motivated in Section 1, the question of approximating the derivatives of the kernel through finite dimensional random feature map is also important as it\nenables to speed up several interesting machine learning tasks that involve the derivatives of the kernel (e.g., [32, 21, 17, 18, 29, 23]).\nTo this end, we consider k as in (1) and define ha := cos(\u03c0a2 + \u00b7), a \u2208 N (in other words h0 = cos, h1 = \u2212 sin, h2 = \u2212 cos, h3 = sin and ha = hamod4). For p,q \u2208 Nd, assuming \u222b |\u03c9p+q| d\u039b(\u03c9) < \u221e, it follows from dominated convergence theorem that\n\u2202p,qk(x,y) =\n\u222b\nRd\n\u03c9 p(\u2212\u03c9)qh|p+q| ( \u03c9 T (x\u2212 y) ) d\u039b(\u03c9)\n=\n\u222b\nRd\n\u03c9 p+q [ h|p|(\u03c9 Tx)h|q|(\u03c9 Ty) + h3+|p|(\u03c9 Tx)h3+|q|(\u03c9 Ty) ] d\u039b(\u03c9),\nso that \u2202p,qk(x,y) can be approximated by replacing \u039b by \u039bm, resulting in\n\u2202\u0302p,qk(x,y) := sp,q(x,y) = 1\nm\nm\u2211\nj=1\n\u03c9 p j (\u2212\u03c9j)qh|p+q| ( \u03c9 T j (x\u2212 y) ) = \u3008\u03c6p(x), \u03c6q(y)\u3009 R2m , (6)\nwhere \u03c6p(u) := 1\u221a m\n( \u03c9\np 1 h|p|(\u03c9 T 1 u), \u00b7 \u00b7 \u00b7 ,\u03c9pmh|p|(\u03c9Tmu),\u03c9p1 h3+|p|(\u03c9T1 u), \u00b7 \u00b7 \u00b7 ,\u03c9pmh3+|p|(\u03c9Tmu)\n) and\n(\u03c9j) m j=1 i.i.d.\u223c \u039b. Now the goal is to understand the behavior of \u2016sp,q\u2212\u2202p,qk\u2016S\u00d7S and \u2016sp,q\u2212\u2202p,qk\u2016Lr(S) for r \u2208 [1,\u221e), i.e., obtain analogues of Theorems 1 and 3. As in the proof sketch of Theorem 1, while \u2016sp,q \u2212 \u2202p,qk\u2016S\u00d7S can be analyzed as the suprema of an empirical process indexed by a suitable function class (say G), some technical issues arise because G is not uniformly bounded. This means McDiarmid or Talagrand\u2019s inequality cannot be applied to achieve concentration and bounding Rademacher average by Dudley entropy bound may not be reasonable. While these issues can be tackled by resorting to more technical and refined methods, in this paper, we generalize (see Theorem 4 which is proved in Section B.1 of the supplement) Theorem 1 to derivatives under the restrictive assumption that supp(\u039b) is bounded (note that many popular kernels including the Gaussian do not satisfy this assumption). We also present another result (see Theorem 5) by generalizing the proof technique1 of [16] to unbounded functions where the boundedness assumption of supp(\u039b) is relaxed but at the expense of a slightly worse rate. Theorem 4. Let p,q \u2208 Nd, Tp,q := sup\u03c9\u2208supp(\u039b) |\u03c9p+q|, Cp,q := E\u03c9\u223c\u039b [ |\u03c9p+q| \u2016\u03c9\u201622 ] , and assume that C2p,2q < \u221e. Suppose supp(\u039b) is bounded if p 6= 0 and q 6= 0. Then for any \u03c4 > 0 and non-empty compact set S \u2282 Rd,\n\u039bm\n({\n(\u03c9i) m i=1 : sup x,y\u2208S |\u2202p,qk(x,y) \u2212 sp,q(x,y)| \u2265 H(d,p,q, |S|) + Tp,q\n\u221a 2\u03c4\u221a\nm\n})\n\u2264 e\u2212\u03c4 ,\nwhere\nH(d,p,q, |S|) = 32 \u221a\n2d T2p,2q\n[ \u221a\nU(p,q, |S|) + 1 2 \u221a U(p,q, |S|) +\n\u221a\nlog( \u221a\nC2p,2q + 1)\n]\nand U(p,q, |S|) = log(|S|T\u22121/22p,2q + 1). 1We also correct some technical issues in the proof of [16, Claim 1], where (i) a shift-invariant argument was applied to the non-shift invariant kernel estimator k\u0302(x,y) = 1 m \u2211m j=1 2 cos(\u03c9Tj x + bj) cos(\u03c9 T j y + bj) =\n1 m\n\u2211m\nj=1\n[\ncos(\u03c9Tj (x\u2212 y)) + cos(\u03c9 T j (x+ y) + 2bj)\n]\n, (ii) the convexity of S was not imposed leading to possibly\nundefined Lipschitz constant (L) and (iii) the randomness of \u2206\u2217 = argmax\u2206\u2208S\u2206 \u2225 \u2225\u2207[k(\u2206)\u2212 k\u0302(\u2206)] \u2225 \u2225 2 was not taken into account, thus the upper bound on the expectation of the squared Lipschitz constant (E[L2]) does not hold.\nRemark 2. (i) Note that Theorem 4 reduces to Theorem 1 if p = q = 0, in which case Tp,q = T2p,2q = 1. If p 6= 0 or q 6= 0, then the boundedness of supp(\u039b) implies that Tp,q < \u221e and T2p,2q < \u221e. (ii) Growth of |Sm|: By the same reasoning as in Remark 1(ii) and Corollary 2, it follows that \u2016\u2202p,qk \u2212 sp,q\u2016Sm a.s.\u2212\u2192 0 if |Sm| = eo(m) and \u2016\u2202p,qk \u2212 sp,q\u2016Lr(Sm) a.s.\u2212\u2192 0 if m\u22121/2|Sm|2d/r log |Sm| \u2192 0 (for 1 \u2264 r < \u221e) as m \u2192 \u221e. An exact analogue of Theorem 3 can be obtained (but with different constants) under the assumption that supp(\u039b) is bounded and it can be shown that for r \u2208 [2,\u221e), \u2016\u2202p,qk\u2212sp,q\u2016Lr(Sm)\na.s.\u2212\u2192 0 if |Sm| = o(m r 4d ).\nThe following result relaxes the boundedness of supp(\u039b) by imposing certain moment conditions on \u039b but at the expense of a slightly worse rate. The proof relies on applying Bernstein inequality at the elements of a net (which exists by the compactness of S) combined with a union bound, and extending the approximation error from the anchors by a probabilistic Lipschitz argument.\nTheorem 5. Let p,q \u2208 Nd, \u03c8 be continuously differentiable, z 7\u2192 \u2207z [\u2202p,qk(z)] be continuous, S \u2282 Rd be any non-empty compact set, Dp,q,S := supz\u2208conv(S\u2206) \u2016\u2207z [\u2202p,qk(z)]\u20162 and Ep,q := E\u03c9\u223c\u039b [|\u03c9p+q| \u2016\u03c9\u20162]. Assume that Ep,q < \u221e. Suppose \u2203L > 0, \u03c3 > 0 such that\nE\u03c9\u223c\u039b [ |f(z;\u03c9)|M ] \u2264 M !\u03c3\n2LM\u22122\n2 (\u2200M \u2265 2, \u2200z \u2208 S\u2206), (7)\nwhere f(z;\u03c9) = \u2202p,qk(z) \u2212 \u03c9p(\u2212\u03c9)qh|p+q| ( \u03c9 T z ) . Then\n\u039bm\n({\n(\u03c9i) m i=1 : sup x,y\u2208S |\u2202p,qk(x,y) \u2212 sp,q(x,y)| \u2265 \u01eb\n})\n\u2264 2d\u22121e \u2212 m\u01eb2 8\u03c32(1+ \u01ebL 2\u03c32 )\n+16Fd ( |S|(Dp,q,S + Ep,q) \u01eb ) d d+1 e \u2212 m\u01eb2 8(d+1)\u03c32(1+ \u01ebL 2\u03c32 ) ,(8)\nwhere Fd := d \u2212 1d+1 + d 1 d+1 .\nRemark 3.\n(i) By the continuity of z 7\u2192 \u2207z [\u2202p,qk(z)] and the compactness of S\u2206, it is easy to see Dp,q,S < \u221e. (7) holds if |f(z;\u03c9)| \u2264 L2 and E\u03c9\u223c\u039b [ |f(z;\u03c9)|2 ] \u2264 \u03c32 (\u2200z \u2208 S\u2206). If supp(\u039b) is bounded, then f is\nguaranteed (see Section B.5 in the supplement). (ii) In the special case when p = q = 0, our requirement boils down to the continuously differentiability\nof \u03c8, E0,0 = E\u03c9\u223c\u039b \u2016\u03c9\u20162 < \u221e, and (7). (iii) Note that (8) is similar to (3) and therefore based on the discussion in Section 2, it is clear that\nsupx,y\u2208S |\u2202p,qk(x,y) \u2212 sp,q(x,y)| = Oa.s.(|S| \u221a\nm\u22121 logm). But the advantage with Theorem 5 over [16, Claim 1] is that it can handle unbounded functions. In comparison to Theorem 4, we obtain slightly worse rates and it will be of interest to improve the rates of Theorem 5 while handling unbounded functions."}, {"heading": "5 Discussion", "text": "In this paper, we presented the first detailed theoretical analysis about the approximation quality of random Fourier features (RFF) that are proposed by [16] in the context of improving the computational complexity of kernel machines. While [16] provided a probabilistic bound on the uniform approximation (over compact subsets of Rd) of a kernel by random features, the result is not optimal. We improved this result by providing a finite sample bound with optimal rate of convergence and also analyzed the quality of approximation in Lr-norm (1 \u2264 r < \u221e). We also proposed a RFF approximation for derivatives of a kernel and provided\ntheoretical guarantees on the quality of approximation in uniform and Lr-norms over compact subsets of Rd.\nWhile all the results in this paper (and also in the literature) dealt with the approximation quality of RFF over only compact subsets of Rd, it is of interest to understand its behavior over entire Rd. However, as discussed in Remark 1(ii) and in the paragaph following Theorem 3, RFF cannot approximate the kernel uniformly or in Lr-norm over Rd. By truncating the Taylor series expansion of the exponential function, [5] proposed a non-random finite dimensional representation to approximate the Gaussian kernel which also enjoys the computational advantages of RFF. However, this representation also does not approximate the Gaussian kernel uniformly over Rd. Therefore, the question remains whether it is possible to approximate a kernel uniformly or in Lr-norm over Rd but still retaining the computational advantages associated with RFF."}, {"heading": "Acknowledgments", "text": "This work was supported by the Gatsby Charitable Foundation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Kernel methods represent one of the most powerful tools in machine learning to tackle problems<lb>expressed in terms of function values and derivatives due to their capability to represent and model<lb>complex relations. While these methods show good versatility, they are computationally intensive<lb>and have poor scalability to large data as they require operations on Gram matrices. In order<lb>to mitigate this serious computational limitation, recently randomized constructions have been<lb>proposed in the literature, which allow the application of fast linear algorithms. Random Fourier<lb>features (RFF) are among the most popular and widely applied constructions: they provide an<lb>easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the<lb>popularity of RFFs, very little is understood theoretically about their approximation quality. In this<lb>paper, we provide the first detailed theoretical analysis about the approximation quality of RFFs<lb>by establishing optimal (in terms of the RFF dimension) performance guarantees in uniform and<lb>L (1 \u2264 r < \u221e) norms. We also propose a RFF approximation to derivatives of a kernel with a<lb>theoretical study on its approximation quality.", "creator": "LaTeX with hyperref package"}}}