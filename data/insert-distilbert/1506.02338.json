{"id": "1506.02338", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Modeling Order in Neural Word Embeddings at Scale", "abstract": "natural language processing ( nlp ) numerical systems commonly leverage bag - of - words co - occurrence techniques to capture semantic and syntactic word relationships. the resulting word - level distributed representations often ignore morphological information, though character - level embeddings have proven valuable to nlp tasks. we propose of a brand new neural comb language model incorporating both word order and character order in its implicit embedding. the model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85. 58 8 % on a recent word - analogy prediction task, exceeding 20 best published syntactic tree word - analogy scores by a 58 % error margin. furthermore, the model includes several parallel training methods, although most notably allowing a skip - gram network with 160 billion parameters to be trained overnight on 3 multi - core cpus, 14x larger than the previous largest neural network.", "histories": [["v1", "Mon, 8 Jun 2015 02:21:46 GMT  (446kb,D)", "https://arxiv.org/abs/1506.02338v1", null], ["v2", "Wed, 10 Jun 2015 15:42:42 GMT  (446kb,D)", "http://arxiv.org/abs/1506.02338v2", null], ["v3", "Thu, 11 Jun 2015 03:00:29 GMT  (445kb,D)", "http://arxiv.org/abs/1506.02338v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew trask", "david gilmore", "matthew russell"], "accepted": true, "id": "1506.02338"}, "pdf": {"name": "1506.02338.pdf", "metadata": {"source": "META", "title": "Modeling Order in Neural Word Embeddings at Scale", "authors": ["Andrew Trask", "David Gilmore", "Matthew Russell"], "emails": ["ANDREW.TRASK@DIGITALREASONING.COM", "DAVID.GILMORE@DIGITALREASONING.COM", "MATTHEW.RUSSELL@DIGITALREASONING.COM"], "sections": [{"heading": "1. Introduction", "text": "NLP systems seek to automate the extraction of useful information from sequences of symbols in human language. These systems encounter difficulty due to the complexity and sparsity in natural language. Traditional systems have represented words as atomic units with success in a variety of tasks (Katz, 1987). This approach is limited by the curse of dimensionality and has been outperformed by neural network language models (NNLM) in a variety of tasks (Ben-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ngio et al., 2003; Morin & Bengio, 2005; Mnih & Hinton, 2009). NNLMs overcome the curse of dimensionality by learning distributed representations for words (G.E. Hinton, 1986; Bengio et al., 2003). Specifically, neural language models embed a vocabulary into a smaller dimensional linear space that models \u201cthe probability function for word sequences, expressed in terms of these representations\u201d (Bengio et al., 2003). The result is a vector space model (Maas & Ng, 2010) that encodes semantic and syntactic relationships and has defined a new standard for feature generation in NLP (Manning et al., 2008; Sebastiani, 2002; Turian et al., 2010).\nNNLMs generate word embeddings by training a symbol prediction task over a moving local-context window such as predicting a word given its surrounding context (Mikolov et al., 2013a;b). This work follows from the distributional hypothesis: words that appear in similar contexts have similar meaning (Harris). Words that appear in similar contexts will experience similar training examples, training outcomes, and converge to similar weights. The ordered set of weights associated with each word becomes that word\u2019s dense vector embedding. These distributed representations encode shades of meaning across their dimensions, allowing for two words to have multiple, real-valued relationships encoded in a single representation (Liang & Potts, 2015).\n(Mikolov et al., 2013c) introduced a new property of word embeddings based on word analogies such that vector operations between words mirror their semantic and syntactic relationships. The analogy \u201dking is to queen as man is to woman\u201d can be encoded in vector space by the equation king - queen = man - woman. A dataset of these analogies, the Google Analogy Dataset 1, is divided into two broad categories, semantic queries and syntactic queries. Semantic queries idenfity relationships such as \u201cFrance is to Paris\n1http://word2vec.googlecode.com/svn/trunk/\nar X\niv :1\n50 6.\n02 33\n8v 3\n[ cs\n.C L\n] 1\n1 Ju\nn 20\nas England is to London\u201d whereas syntactic queries identify relationships such as \u201crunning is to run as pruning is to prune\u201d. This is a standard by which distributed word embeddings may be evaluated.\nUntil recently, NNLMs have ignored morphology and word shape. However, including information about word structure in word representations has proven valuable for part of speech analysis (Santos & Zadrozny, 2014), word similarity (Luong et al., 2013), and information extraction (Qi et al., 2014).\nWe propose a neural network architecture that explicitly encodes order in a sequence of symbols and use this architecture to embed both word-level and character-level representations. When these two representations are concatenated, the resulting representations exceed best published results in both the semantic and syntactic evaluations of the Google Analogy Dataset."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Word-level Representations (Word2vec)", "text": "Our technique is inspired by recent work in learning vector representations of words, phrases, and sentences using neural networks (Mikolov et al., 2013a;b; Le & Mikolov, 2014). In the CBOW configuration of the negative sampling training method by (Mikolov et al., 2013a), each word is represented by a row-vector in matrix syn0 and is concatenated, summed, or averaged with other word vectors in a context window. The resulting vector is used in a classifier syn1 to predict the existence of the whole context with the the focus term (positive training) or absence of other randomly sampled words in the window (negative sampling). The scalar output is passed through a sigmoid function (\u03c3(z) = (1 + e(\u2212z)), returning the network\u2019s probability that the removed word exists in the middle of the window, without stipulation on the order of the context words. This optimizes the following objective:\nargmax \u03b8 \u220f (w,C)\u2208d p(w = 1|C; \u03b8) \u220f (w,C)\u2208d\u2032 p(w = 0|C; \u03b8)\nwhere d represents the document as a collection of contextword pairs (w,C) and C is an unordered group of words in a context window. d\u2032 is a set of random (w,C) pairs. \u03b8 will be adjusted such that p(w = 1, C; \u03b8) = 1 for context-word pairs that exist in d, and 0 for random context-word pairs that do not exist in d\u2032. In the skip-gram negative sampling work by (Mikolov et al., 2013a;b), each word in a context is trained in succession. This optimizes the following objective:\nargmax \u03b8 \u220f (w,c)\u2208d p(w = 1|c; \u03b8) \u220f (w,c)\u2208d\u2032 p(w = 0|c; \u03b8)\nwhere d represents the document as a collection of contextword pairs (w, c) and c represents a single word in the context. Modeling an element-wise probability that a word occurs given another word in the context, the element-wise nature of this probability allows (2) to be an equivalent objective to the skip-gram objective outlined in (Mikolov et al., 2013b; Goldberg & Levy, 2014).\nReducing the window size under these models constrains the probabilities to be more localized, as the probability that two words co-occur will reduce when the window reduces which can be advantageous for words subject to short-windowed statistical significance. For example, currency symbols often co-occur with numbers within a small window. Outside of a small window, currency symbols and numbers are not likely to co-occur. Thus, reducing the window size reduces noise in the prediction. Words such as city names, however, prefer wider windows to encode broader co-occurrence statistics with other words such as landmarks, street-names, and cultural words which could be farther away in the document.\nNeither skip-gram nor CBOW explicitly preserve word order in their word embeddings (Mikolov et al., 2013a;b; Le & Mikolov, 2014). Ordered concatenation of syn0 vectors does embed order in syn1, but this is obfuscated by the fact that the same embedding for each word must be linearly compatible with the feature detectors in every window position. In addition to changing the objective function, this has the effect of cancelling out features that are unique to\nonly one window position by those in other window positions that are attempting to be encoded in the same feature detector dimension. This effect prevents word embeddings from preserving order based features. The other methods (sum, average, and skip-gram) ignore all order completely in their modeling and model only co-occurrence based probability in their embeddings."}, {"heading": "2.2. Character-level Representations", "text": "Recent work has explored techniques to embed word shape and morphology features into word embeddings. The resulting embeddings have proven useful for a variety of NLP tasks."}, {"heading": "2.2.1. DEEP NEURAL NETWORK", "text": "(Santos & Zadrozny, 2014) proposed a Deep Neural Network (DNN) that \u201clearns character-level representation[s] of words and associate[s] them with usual word representations to perform POS tagging.\u201d The resulting embeddings were used to produce state-of-the-art POS taggers for both English and Portuguese data sets. The network architecture leverages the convolutional approach introduced in (Waibel et al., 1990) to produce local features around each character of the word and then combines them into a fixed-sized character-level embedding of the word. The character-level word embedding is then concatenated with a word-level embedding learned using word2vec. Using only these embeddings, (Santos & Zadrozny, 2014) achieves state-ofthe-art results in POS tagging without the use of handengineered features."}, {"heading": "2.2.2. RECURSIVE NEURAL NETWORK", "text": "(Luong et al., 2013) proposed a \u201cnovel model that is capable of building representations for morphologically complex words from their morphemes.\u201d The model leverages a recursive neural network (RNN) (Socher et al., 2011) to model morphology in a word embedding. Words are decomposed into morphemes using a morphological segmenter (Creutz & Lagus, 2007). Using the \u201cmorphemic vectors\u201d, word-level representations are constructed for complex words. In the experiments performed by (Luong et al., 2013), word embeddings were borrowed from (Huang et al., 2012) and (Collobert et al., 2011). After conducting a morphemic segmentation, complex words were then enhanced with morphological feature embeddings by using the morphemic vectors in the RNN to compute word representations \u201con the fly\u201d. The resulting model outperforms existing embeddings on word similarity tasks accross several data sets."}, {"heading": "3. The Partitioned Embedding Neural Network Model (PENN)", "text": "We propose a new neural language model called a Partitioned Embedding Neural Network (PENN). PENN improves upon word2vec by modeling the order in which words occur. It models order by partitioning both the embedding and classifier layers. There are two styles of training corresponding to the CBOW negative sampling and skip-gram negative sampling methods in word2vec, although they differ in key areas.\nThe first property of PENN is that each word embedding is partitioned. Each partition is trained differently from each other partition based on word order, such that each partition models a different probability distribution. These different probability distributions model different perspectives on the same word. The second property of PENN is that the classifier has different inputs for words from different window positions. The classifier is partitioned with equal partition dimensionality as the embedding. It is possible to have fewer partitions in the classifier than the embedding, such that a greater number of word embeddings are summed/averaged into fewer classifier partitions. This configuration has better performance when using smaller dimensionality feature vectors with large windows as it balances the (embedding partition size) / (window size) ratio. The following subsection presents the two opposite configurations under the PENN framework."}, {"heading": "3.1. Plausible Configurations", "text": ""}, {"heading": "3.1.1. WINDOWED", "text": "The simplest configuration of a PENN architecture is the windowed configuration, where each partition corresponds to a unique window position in which a word occurs. As\nillustrated in Figure 2, if there are two window positions (one on each side of the focus term), then each embedding would have two partitions. When a word is in partition p = +1 (the word before the focus term), the partition corresponding to that position is propagated forward, and subsequently back propagated into, with the p = -1 partition remaining unchanged."}, {"heading": "3.1.2. DIRECTIONAL", "text": "The opposite configuration to windowed PENN is the directional configuration. Instead of each partition corresponding to a window position, there are only two partitions. One partition corresponds to every positive, forward predicting window position (left of the focus term) and the other partition corresponds to every negative, backward predicting window position (right of the focus term). For each partition, all embeddings corresponding to that partition are summed or averaged when being propagated forward."}, {"heading": "3.2. Training Styles", "text": ""}, {"heading": "3.2.1. CONTINUOUS LIST OF WORDS (CLOW)", "text": "The Continuous List of Words (CLOW) training style under the PENN framework optimizes the following objective function:\nargmax \u03b8\n( \u220f\n(w,C)\u2208d \u220f \u2212c\u2264j\u2264c,j 6=0 p(w = 1|cjj ; \u03b8)\n\u220f (w,C)\u2208d\u2032 \u220f \u2212c\u2264j\u2264c,j 6=0 p(w = 0|cjj ; \u03b8))\nwhere cjj is the location specific representation (partition j) for the word at window position j relative to the focus word w. Closely related to the CBOW training method, the CLOW method models the probability that in an ordered list of words, a specific word is present in the middle of the list, given the presence and location of the other words. For each training example out of a windowed sequence of words, the middle focus term is removed. Then, a partition is selected from each remaining word\u2019s embedding based on that word\u2019s position relative to the focus term. These partitions are concatenated and propagated through the classifier layer. All weights are updated to model the probability that the presence of the focus term is 100% (positive training) and other randomly sampled words 0% (negative sampling)."}, {"heading": "3.2.2. SKIP-GRAM", "text": "The skip-gram training style under the PENN framework optimizes the following objective function\nargmax \u03b8\n( \u220f\n(w,C)\u2208d \u2211 \u2212c\u2264j\u2264c,j 6=0 p(wj = 1|cjj ; \u03b8)\n\u220f (w,C)\u2208d\u2032 \u2211 \u2212c\u2264j\u2264c,j 6=0 p(wj = 0|cjj ; \u03b8))\nwhere, like CLOW, cjj is the location specific representation (partition j) for the word at window position j relative to the focus word w. wj is the relative location specific probability (partition) of the focus term. PENN skip-gram is almost identical to the CLOW method with one key difference. Instead of each partition of a word being concatenated with partitions from neighboring words, each partition is fed forward and back propagated in isolation. This models the probability that, given a single word, the focus term is present a relative number of words away in a given direction. This captures information lost in the word2vec skip-gram architecture by modeling based on the relative location of a context word in the window as opposed to an arbitrary location within the window.\nThe intuition behind modeling w and c based on j at the same time becomes clear when considering the neural architecture of these embeddings. Partitioning the context word into j partitions gives a location specific representation for a word\u2019s relative position. Location specific representations are important even for words with singular meanings. Consider the word \u201cgoing\u201d, a word of singular meaning. This word\u2019s effect on a task predicting a word immediately before it is completely different than predicting a word immediately after it. The phrase \u201cam going\u201d is a plausible phrase. The phrase \u201cgoing am\u201d is not. Thus, forcing this word to have a consistent embedding across these\ntasks forces it to convey identical information optimizing for nonidentical problems.\nPartitioning the classifier incorporates this same principle with respect to the focus word. The focus word will read features presented to it in a different light with a different weighting given its position. For example, \u201cdollars\u201d is far more likely to be predicted accurately based on the word before it; whereas, it is not likely to be predicted correctly by a word ten window positions after. Thus, the classifier responsible for looking for features indicating that \u201cdollars\u201d is next should not have to be the same classifier that looks for features ten window positions into the future. Training separate classifier partitions based on window position avoids this phenomenon."}, {"heading": "3.3. Distributed Training Optimizations", "text": ""}, {"heading": "3.3.1. SKIP-GRAM", "text": "When skip-gram is used to model ordered sets of words under the PENN framework each classifier partition and its associated embedding partitions may be trained in fullparallel (with no inter-communication) and reach the exact same state as if they were not distributed. A special case of this is the windowed embedding configuration, where every window position can be trained in full parallel and concatenated (embeddings and classifiers) at the end of training. This allows very large, rich embeddings to be trained on relatively small, inexpensive machines in a small amount of time with each machine optimizing a part of the overall objective function. Given machine j, training skip-gram under the windowed embedding configuration optimizes the following objective function:\nargmax \u03b8\n( \u220f\n(w,C)\u2208d\np(wj = 1|cjj ; \u03b8)\n\u220f (w,C)\u2208d\u2032 p(wj = 0|cjj ; \u03b8))\nConcatenation of the weight matrices syn0 and syn1 then incorporates the sum over j back into the PENN skip-gram objective function during the forward propagation process, yielding identical training results as a network trained in a single-threaded, single-model PENN skip-gram fashion. This training style achieves parity training results with current state-of-the-art methods while training in parallel over as many as j separate machines."}, {"heading": "3.3.2. CLOW", "text": "The CLOW method is an excellent candidate for the ALOPEX distributed training algorithm (Unnikrishnan &\nVenugopal, 1994) because it trains on very few (often single) output probabilities at a time. Different classifier partitions may be trained on different machines, with each training example sending a short list of floats per machine across the network. They all share the same global error and continue on to the next iteration.\nA second, nontrivial optimization is found in the strong performance of the directional CLOW implementation with very small window sizes (pictured below with a window size of 1). Directional CLOW is able to achieve a parity score using a window size of 1, contrasted with word2vec using a window size of 10 when all other parameters are equal, reducing the overall training time by a factor of 10."}, {"heading": "4. Dense Interpolated Embedding Model", "text": "We propose a second new neural language model called a Dense Interpolated Embedding Model (DIEM). DIEM uses neural embeddings learned at the character level to generate a fixed-length syntactic embedding at the world level useful for syntactic word-analogy tasks, leveraging patterns in the characters as a human might when detecting syntactic features such as plurality."}, {"heading": "4.1. Method", "text": "Generating syntactic embeddings begins by generating character embeddings. Character embeddings are generated using vanilla word2vec by predicting a focus charac-\nAlgorithm 1 Dense Interpolated Embedding Pseudocode Input: wordlength I , list char embeddings (e.g. the word) chari, multiple M , char dim C, vector vm for i = 0 to I \u2212 1 do s =M * i/l for m = 0 to M \u2212 1 do d = pow(1 - (abs(s - m)) / M ,2) vm = vm + d * chari\nend for end for\nter given its context. This clusters characters in an intuitive way, vowels with vowels, numbers with numbers, and capitals with capitals. In this way, character embeddings represent morphological building blocks that are more or less similar to each other, based on how they have been used.\nOnce character embeddings have been generated, interpolation may begin over a word of length I . The final embedding size must be selected as a multiple M of the character embedding dimensionalityC. For each character in a word, its index i is first scaled linearly with the size of the final \u201csyntactic\u201d embedding such that s = M * i / l. Then, for each length C position m (out of M positions) in the final word embedding vm, a squared distance is calculated relative to the scaled index such that distance d = pow(1-(abs(s - j)) / M ,2). The character vector for the character at position i in the word is then scaled by d and added elementwise into position m of vector v.\nA more efficient form of this process caches a set of transformation matrices, which are cached values of di,m for words of varying size. These matrices are used to transform variable length concatenated character vectors into fixed length word embeddings via vector-matrix multiplication.\nThese embeddings are useful for a variety of tasks, including syntactic word-analogy queries. Furthermore, they are useful for syntactic query expansion, mapping sparse edge cases of a word (typos, odd capitalization, etc.) to a more common word and its semantic embedding."}, {"heading": "4.2. Distributed Use and Storage Optimizations", "text": "Syntactic vectors also provide significant scaling and generalization advantages over semantic vectors. New syntactic vectors may be inexpensively generated for words never before seen, giving loss-less generalization to any word from initial character training, assuming only that the word is made up of characters that have been seen. Syntactic embeddings can be generated in a fully distributed fashion and only require a small vector concatenation and vector-matrix multiplication per word. Secondly, the character vectors (typically length 32) and transformation ma-\ntrices (at most 20 or so of them) can be stored very efficiently relative to the semantic vocabularies, which can be several million vectors of dimensionality 1000 or more. Despite their significant positive impact on quality, DIEM optimally performs using 6+ orders of magnitude less storage space, and 5+ orders of magnitude fewer training examples than word-level semantic embeddings."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Evaluation Methods", "text": "We conduct experiments on the word-analogy task of (Mikolov et al., 2013a). It is made up of a variety of word similarity tasks, as described in (Luong et al., 2013). Known as the Google Analogy Dataset, it contains 19,544 questions asking \u201da is to b as c is to \u201d and is split into semantic and syntactic sections. Both sections are further divided into subcategories based on analogy type, as indicated in the results tables below.\nAll training occurs over the dataset available from the Google word2vec website2, using the packaged wordanalogy evaluation script. The dataset contains approximately 8 billion words collected from English News Crawl, 1-Billion-Word Benchmark, UMBC Webbase, and English Wikipedia. The dataset used leverages the default dataphrase2.txt normalization in all training, which includes both single tokens and phrases. Unless otherwise specified, all parameters for training and evaluating are identical to the default parameters specified in the default word2vec big model, which is freely available online."}, {"heading": "5.2. Embedding Partition Relative Evaluation", "text": "Figure 4 displays the relative accuracy of each partition in a PENN model as judged by row-relative word-analogy scores. Other experiments indicated that the pattern present in the heat-map is consistent across parameter tunings. There is a clear quality difference between window positions that predict forward (left side of the figure) and window positions that predict backward (right side of the figure). \u201ccurrency\u201d achieves most of its predictive power in short range predictions, whereas \u201ccapital-common countries\u201d is a much smoother gradient over the window. These patterns support the intuition that different window positions play different roles in different tasks."}, {"heading": "5.3. Evaluation of CLOW and CBOW", "text": "Table 3 shows the performance of the default CBOW implementation of word2vec relative to CLOW and DIEM when configured to 2000 dimensional embeddings. Between tables 3 and 4, we see that increasing dimension-\n2https://code.google.com/p/word2vec/\nality of baseline CBOW word2vec past 500 achieves suboptimal performance. Thus, a fair comparison of two models should be between optimal (as opposed to just identical) parameterization for each model. This is especially important given that PENN models are modeling a much richer probability distribution, given that order is being preserved. Thus, optimal parameter settings often require larger dimensionality. Unlike the original CBOW word2vec, we have found that bigger window size is not always better. Larger windows tend to create slightly more semantic embeddings, whereas smaller window sizes tend to create slightly more syntactic embeddings. This follows the intuition that syntax plays a huge role in grammar, which is dictated by rules about which words make sense to occur immediately next to each other. Words that are +5 words apart cluster based on subject matter and semantics as opposed to grammar. With respect to window size and overall quality, because partitions slice up the global vector for a word, increasing the window size decreases the size of each partition in the window if the global vector size remains constant. Since each embedding is attempting to model a very complex (hundreds of thousands of words) probability distribution, the partition size in each partition must remain high enough to model this distribution. Thus, modeling large windows for semantic embeddings is optimal when using either the directional embedding model, which has a fixed partition size of 2, or a large global vector size. The\ndirectional model with optimal parameters has slightly less quality than the windowed model with optimal parameters due to the vector averaging occurring in each window pane."}, {"heading": "5.4. Evaluation of DIEM Syntactic Vectors on Syntactic Tasks", "text": "Table 4 documents the change in syntactic analogy query quality as a result of the interpolated DIEM vectors. For\nthe DIEM experiment, each analogy query was first performed by running the query on CLOW and DIEM independently, and selecting the top thousand CLOW cosine similarities. We summed the squared cosine similarity of each of these top thousand with each associated cosine similarity returned by the DIEM and resorted. This was found to be an efficient estimation of concatenation that did not reduce quality.\nTable 5 documents the parameter selection for a combined neural network partitioned according to several training styles and dimensionalities. As in the experiments of Table 3, each analogy query was first performed by running the query on each model independently, selecting the top thousand cosine similarities. We summed the cosine similarity of each of these top thousand entries across all models (excluding DIEM for semantic queries) and resorted. (For\nnormalization purposes, DIEM scores were raised to the power of 10 and all other scores were raised to the power of 0.1 before summing)."}, {"heading": "5.5. High Level Comparisons", "text": "Our final results show a lift in quality and size over previous models with a 58% syntactic lift over the best published syntactic result, and a 40% overall lift over the best published overall result (Pennington et al., 2014). Table 5 also includes the highest word2vec scores we could achieve through better parameterization (which also exceeds the best published word2vec scores). Within PENN models, there exists a speed vs. performance tradeoff between SGDIEM and CLOW-DIEM. In this case, we achieve a 20x level of parallelism in SG-DIEM relative to CLOW, with each model training partitions of 250 dimensions (250 * 20 = 5000 final dimensionality). A 160 billion parameter network was also trained overnight on 3 multi-core CPUs, however it yielded 20000 dimensional vectors for each word and subsequently overfit the training data. This is because a dataset of 8 billion tokens with a negative sampling parameter of 10 has 80 billion training examples. Having more parameters than training examples overfits a dataset, whereas 40 billion performs at parity with current state of the art, as pictured in Table 5. Future work will experiment with larger datasets and vocabularies. The previous largest neural network contained 11.2 billion parameters (Coates et al., 2013), whereas CLOW and the largest SG contain 16 billion (trained all together) and 160 billion (trained across a cluster) parameters respectively as measured by the number of weights."}, {"heading": "6. Conclusion and Future Work", "text": "Encoding both word and character order in neural word embeddings is beneficial for word-analogy tasks, particularly syntactic tasks. These findings are based upon the intuition that order matters in human language and has been validated through the methods above. Future work will further investigate the scalability of these word embeddings to larger datasets with reduced runtimes."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Mathias", "Lagus", "Krista"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Distributed representations. Parallel dis-tributed processing: Explorations in the microstructure of cognition", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Goldberg", "Yoav", "Levy", "Omer"], "venue": "CoRR, abs/1402.3722,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Katz", "Slava M"], "venue": "In IEEE Transactions on Acoustics, Speech and Singal processing,", "citeRegEx": "Katz and M.,? \\Q1987\\E", "shortCiteRegEx": "Katz and M.", "year": 1987}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "CoRR, abs/1405.4053,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Reviews of Linguistics,", "citeRegEx": "Liang and Potts,? \\Q2015\\E", "shortCiteRegEx": "Liang and Potts", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong", "Minh-Thang", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "A probabilistic model for semantic word vectors", "author": ["Maas", "Andrew L", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Maas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2010}, {"title": "Introduction to information retrieval, volume 1", "author": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "tau Yih", "Wen", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep learning for character-based information extraction", "author": ["Qi", "Yanjun", "Das", "Sujatha G", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "Qi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Machine learning in automated text categorization", "author": ["Sebastiani", "Fabrizio"], "venue": "ACM Comput. Surv.,", "citeRegEx": "Sebastiani and Fabrizio.,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani and Fabrizio.", "year": 2002}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Alopex: A correlation-based learning algorithm for feedforward and recurrent neural networks", "author": ["KP Unnikrishnan", "Venugopal", "Kootala P"], "venue": "Neural Computation,", "citeRegEx": "Unnikrishnan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Unnikrishnan et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 15, "context": "8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin (Pennington et al., 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 1, "context": "Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network (Coates et al., 2013).", "startOffset": 235, "endOffset": 256}, {"referenceID": 0, "context": "NNLMs overcome the curse of dimensionality by learning distributed representations for words (G.E. Hinton, 1986; Bengio et al., 2003).", "startOffset": 93, "endOffset": 133}, {"referenceID": 0, "context": "Specifically, neural language models embed a vocabulary into a smaller dimensional linear space that models \u201cthe probability function for word sequences, expressed in terms of these representations\u201d (Bengio et al., 2003).", "startOffset": 199, "endOffset": 220}, {"referenceID": 10, "context": "The result is a vector space model (Maas & Ng, 2010) that encodes semantic and syntactic relationships and has defined a new standard for feature generation in NLP (Manning et al., 2008; Sebastiani, 2002; Turian et al., 2010).", "startOffset": 164, "endOffset": 225}, {"referenceID": 8, "context": "However, including information about word structure in word representations has proven valuable for part of speech analysis (Santos & Zadrozny, 2014), word similarity (Luong et al., 2013), and information extraction (Qi et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 16, "context": ", 2013), and information extraction (Qi et al., 2014).", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "(Luong et al., 2013) proposed a \u201cnovel model that is capable of building representations for morphologically complex words from their morphemes.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "\u201d The model leverages a recursive neural network (RNN) (Socher et al., 2011) to model morphology in a word embedding.", "startOffset": 55, "endOffset": 76}, {"referenceID": 8, "context": "In the experiments performed by (Luong et al., 2013), word embeddings were borrowed from (Huang et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 8, "context": "It is made up of a variety of word similarity tasks, as described in (Luong et al., 2013).", "startOffset": 69, "endOffset": 89}, {"referenceID": 15, "context": "Our final results show a lift in quality and size over previous models with a 58% syntactic lift over the best published syntactic result, and a 40% overall lift over the best published overall result (Pennington et al., 2014).", "startOffset": 201, "endOffset": 226}, {"referenceID": 1, "context": "2 billion parameters (Coates et al., 2013), whereas CLOW and the largest SG contain 16 billion (trained all together) and 160 billion (trained across a cluster) parameters respectively as measured by the number of weights.", "startOffset": 21, "endOffset": 42}], "year": 2015, "abstractText": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin (Pennington et al., 2014). Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network (Coates et al., 2013).", "creator": "LaTeX with hyperref package"}}}