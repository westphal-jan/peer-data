{"id": "1506.02516", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning to Transduce with Unbounded Memory", "abstract": "recently, strong results have been demonstrated by building deep recurrent abstract neural science networks on natural language transduction problems. \u2022 in this paper we explore the representational power of these models using primitive synthetic grammars designed to exhibit phenomena similar to those found in extended real transduction problems such as machine translation. understanding these experiments progressively lead us significantly to propose new memory - based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks, queues, and deques. we show that these architectures exhibit superior generalisation performance characteristic to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments.", "histories": [["v1", "Mon, 8 Jun 2015 14:23:30 GMT  (541kb,D)", "http://arxiv.org/abs/1506.02516v1", "12 pages, 4 figures"], ["v2", "Fri, 30 Oct 2015 16:24:40 GMT  (696kb,D)", "http://arxiv.org/abs/1506.02516v2", "12 pages, 4 figures"], ["v3", "Tue, 3 Nov 2015 14:07:29 GMT  (699kb,D)", "http://arxiv.org/abs/1506.02516v3", "14 pages, 4 figures, NIPS 2015"]], "COMMENTS": "12 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["edward grefenstette", "karl moritz hermann", "mustafa suleyman", "phil blunsom"], "accepted": true, "id": "1506.02516"}, "pdf": {"name": "1506.02516.pdf", "metadata": {"source": "CRF", "title": "Learning to Transduce with Unbounded Memory", "authors": ["Edward Grefenstette", "Karl Moritz Hermann"], "emails": ["etg@google.com", "kmh@google.com", "mustafasul@google.com", "pblunsom@google.com"], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) offer a compelling tool for processing natural language input in a straightforward sequential manner. Many natural language processing (NLP) tasks can be viewed as transduction problems, that is learning to convert one string into another. Machine translation is a prototypical example of transduction and recent results indicate that Deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2]. While elegant, the application of RNNs to transduction tasks requires hidden layers large enough to store representations of the longest strings likely to be encountered, implying wastage on shorter strings and a strong dependency between the number of parameters in the model and its memory.\nIn this paper we use a number of linguistically-inspired synthetic transduction tasks to explore the ability of RNNs to learn long-range reorderings and substitutions. Further, inspired by prior work on neural network implementations of stack data structures [3], we propose and evaluate transduction models based on Neural Stacks, Queues, and DeQues (double ended queues). Stack algorithms are well-suited to processing the hierarchical structures observed in natural language and we hypothesise that their neural analogues will provide an effective and learnable transduction tool. Our models provide a middle ground between simple RNNs and the recently proposed Neural Turing Machine (NTM) [4] which implements a powerful random access memory with read and write operations. Neural Stacks, Queues, and DeQues also provide a logically unbounded memory while permitting efficient constant time push and pop operations.\nOur results indicate that the models proposed in this work, and in particular the Neural DeQue, are able to consistently learn a range of challenging transductions. While Deep RNNs based on long short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the same length as seen in training, they fail to consistently generalise to longer strings. In contrast, our sequential memory-based algorithms are able to learn to reproduce the generating transduction algorithms, often generalising perfectly to inputs well beyond those encountered in training.\nar X\niv :1\n50 6.\n02 51\n6v 1\n[ cs\n.N E\n] 8\nJ un"}, {"heading": "2 Related Work", "text": "String transduction is central to many applications in NLP, from name transliteration and spelling correction, to inflectional morphology and machine translation. The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8]. RNNs offer an attractive alternative to symbolic transducers due to their simple algorithms and expressive representations [9]. However, as we show in this work, such models are limited in their ability to generalise beyond their training data and have a memory capacity that scales with the number of their trainable parameters.\nPrevious work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3]. We were inspired by the continuous pop and push operations of these architectures and the idea of an RNN controlling the data structure when developing our own models. The key difference is that our work adapts these operations to work within a recurrent continuous Stack/Queue/DeQue-like structure, the dynamics of which are fully decoupled from those of the RNN controlling it. In our models, the backwards dynamics are easily analysable in order to obtain the exact partial derivatives for use in error propagation, rather than having to approximate them as done in previous work.\nIn a parallel effort to ours, researchers are exploring the addition of memory to recurrent networks. The NTM and Memory Networks [4, 12, 13] provide powerful random access memory operations, whereas we focus on a more efficient and restricted class of models which we believe are sufficient for natural language transduction tasks. More closely related to our work, [14] have sought to develop a continuous stack controlled by an RNN. Note that this model\u2014unlike the work proposed here\u2014renders discrete push and pop operations continuous by \u201cmixing\u201d information across levels of the stack at each time step according to scalar push/pop action values. This means the model ends up compressing information in the stack, thereby limiting it as it effectively loses the unbounded memory nature of traditional symbolic models."}, {"heading": "3 Models", "text": "In this section, we present an extensible memory enhancement to recurrent layers which can be set up to act as a continuous version of a classical Stack, Queue, or DeQue (double-ended queue). We begin by describing the operations and dynamics of a neural Stack, before showing how to modify it to act as a Queue, and extend it to act as a DeQue."}, {"heading": "3.1 Neural Stack", "text": "Let a Neural Stack be a differentiable structure onto and from which continuous vectors are pushed and popped. Inspired by the neural pushdown automaton of [3], we render these traditionally discrete operations continuous by letting push and pop operations be real values in the interval (0, 1). Intuitively, we can interpret these values as the degree of certainty with which some controller wishes to push a vector v onto the stack, or pop the top of the stack.\nVt[i] = { Vt\u22121[i] if 1 \u2264 i < t vt if i = t\n(Note that Vt[i] = vi for all i \u2264 t) (1)\nst[i] =  max(0, st\u22121[i]\u2212max(0, ut \u2212 t\u22121\u2211 j=i+1 st\u22121[j])) if 1 \u2264 i < t\ndt if i = t (2)\nrt = t\u2211 i=1 (min(st[i],max(0, 1\u2212 t\u2211 j=i+1 st[j]))) \u00b7 Vt[i] (3)\nFormally, a Neural Stack, fully parametrised by an embedding size m, is described at some timestep t by a t\u00d7m value matrix Vt and a strength vector st \u2208 Rt. These form the core of a recurrent layer which is acted upon by a controller by receiving, from the controller, a value vt \u2208 Rm, a pop signal ut \u2208 (0, 1), and a push signal dt \u2208 (0, 1). It outputs a read vector rt \u2208 Rm. The recurrence of this\nlayer comes from the fact that it will receive as previous state of the stack the pair (Vt\u22121, st\u22121), and produce as next state the pair (Vt, st) following the dynamics described below. Here, Vt[i] represents the ith row (an m-dimensional vector) of Vt and st[i] represents the ith value of st.\nEquation 1 shows the update of the value component of the recurrent layer state represented as a matrix, the number of rows of which grows with time, maintaining a record of the values pushed to the stack at each timestep (whether or not they are still logically on the stack). Values are appended to the bottom of the matrix (top of the stack) and never changed.\nEquation 2 shows the effect of the push and pop signal in updating the strength vector st\u22121 to produce st. First, the pop operation removes objects from the stack. We can think of the pop value ut as the initial deletion quantity for the operation. We traverse the strength vector st\u22121 from the highest index to the lowest. If the next strength scalar is less than the remaining deletion quantity, it is subtracted from the remaining quantity and its value is set to 0. If the remaining deletion quantity is less than the next strength scalar, the remaining deletion quantity is subtracted from that scalar and deletion stops. Next, the push value is set as the strength for the value added in the current timestep.\nEquation 3 shows the dynamics of the read operation, which are similar to the pop operation. A fixed initial read quantity of 1 is set at the top of a temporary copy of the strength vector st which is traversed from the highest index to the lowest. If the next strength scalar is smaller than the remaining read quantity, its value is preserved for this operation and subtracted from the remaining read quantity. If not, it is temporarily set to the remaining read quantity, and the strength scalars of all lower indices are temporarily set to 0. The output rt of the read operation is the weighted sum of the rows of Vt, scaled by the temporary scalar values created during the traversal. An example of the stack read calculations across three timesteps, after pushes and pops as described above, is illustrated in Figure 1a. The third step shows how setting the strength s3[2] to 0 for V3[2] logically removes v2 from the stack, and how it is ignored during the read.\nThis completes the description of the forward dynamics of a neural Stack, cast as a recurrent layer, as illustrated in Figure 1b. All operations described in this section are differentiable1. The equations describing the backwards dynamics are provided in Appendix A of the supplementary materials.\nv1 0.8 v1 0.7\nv2 0.5\nv1 0.3\nv2 0\nv3 0.9\nr1 = 0.8 \u2219 v1 r2 = 0.5 \u2219 v2 + 0.5 \u2219 v1 r3 = 0.9 \u2219 v3 + 0 \u2219 v2 + 0.1 \u2219 v1\nt = 1 u1 = 0 d1 = 0.8 t = 2 u2 = 0.1 d2 = 0.5 t = 3 u3 = 0.9 d3 = 0.9\nrow 1\nrow 2\nrow 3\nst ac\nk gr\now s\nup w\nar ds\nv2 removed from stack"}, {"heading": "3.2 Neural Queue", "text": "A neural Queue operates the same way as a neural Stack, with the exception that the pop operation reads the lowest index of the strength vector st, rather than the highest. This represents popping and\n1The max(x, y) and min(x, y) functions are technically not differentiable for x = y. Following the work on rectified linear units [15], we arbitrarily take the partial differentiation of the left argument in these cases.\nreading from the front of the Queue rather than the top of the stack. These operations are described in Equations 4\u20135.\nst[i] =  max(0, st\u22121[i]\u2212max(0, ut \u2212 i\u22121\u2211 j=1 st\u22121[j])) if 1 \u2264 i < t\ndt if i = t (4)\nrt = t\u2211 i=1 (min(st[i],max(0, 1\u2212 i\u22121\u2211 j=1 st[j]))) \u00b7 Vt[i] (5)"}, {"heading": "3.3 Neural DeQue", "text": "A neural DeQue operates likes a neural Stack, except it takes a push, pop, and value as input for both \u201cends\u201d of the structure (which we call top and bot), and outputs a read for both ends. We write utopt and u bot t instead of ut, v top t and v bot t instead of vt, and so on. The state, Vt and st are now a 2t \u00d7 m-dimensional matrix and a 2t-dimensional vector, respectively. At each timestep, a pop from the top is followed by a pop from the bottom of the DeQue, followed by the pushes and reads. The dynamics of a DeQue, which unlike a neural Stack or Queue \u201cgrows\u201d in two directions, are described in Equations 6\u201311, below. Equations 7\u20139 decompose the strength vector update into three steps purely for notational clarity.\nVt[i] =  v bot t if i = 1\nvtopt if i = 2t Vt\u22121[i\u2212 1] if 1 < i < 2t\n(6)\nstopt [i] = max(0, st\u22121[i]\u2212max(0, u top t \u2212 2(t\u22121)\u22121\u2211 j=i+1 st\u22121[j])) if 1 \u2264 i < 2(t\u2212 1) (7)\nsbotht [i] = max(0, s top t [i]\u2212max(0, ubott \u2212 i\u22121\u2211 j=1 stopt [j])) if 1 \u2264 i < 2(t\u2212 1) (8)\nst[i] =  s both t [i\u2212 1] if 1 < i < 2t dbott if i = 1 dtopt if i = 2t\n(9)\nrtopt = 2t\u2211 i=1 (min(st[i],max(0, 1\u2212 2t\u2211 j=i+1 st[j]))) \u00b7 Vt[i] (10)\nrbott = 2t\u2211 i=1 (min(st[i],max(0, 1\u2212 i\u22121\u2211 j=1 st[j]))) \u00b7 Vt[i] (11)\nTo summarise, a neural DeQue acts like two neural Stacks operated on in tandem, except that the pushes and pops from one end may eventually affect pops and reads on the other, and vice versa."}, {"heading": "3.4 Interaction with a Controller", "text": "While the three memory modules described can be seen as recurrent layers, with the operations being used to produce the next state and output from the input and previous state being fully differentiable, they contain no tunable parameters to optimise during training. As such, they need to be attached to a controller in order to be used for any practical purposes. In exchange, they offer an extensible memory, the logical size of which is unbounded and decoupled from both the nature and parameters of the controller, and from the size of the problem they are applied to. Here, we describe how any RNN controller may be enhanced by a neural Stack, Queue or DeQue.\nWe begin by giving the case where the memory is a neural Stack, as illustrated in Figure 1c. Here we wish to replicate the overall \u2018interface\u2019 of a recurrent layer\u2014as seen from outside the dotted\nlines\u2014which takes the previous recurrent state Ht\u22121 and an input vector it, and transforms them to return the next recurrent state Ht and an output vector ot. In our setup, the previous state Ht\u22121 of the recurrent layer will be the tuple (ht\u22121, rt\u22121, (Vt\u22121, st\u22121)), where ht\u22121 is the previous state of the RNN, rt\u22121 is the previous stack read, and (Vt\u22121, st\u22121) is the previous state of the stack as described above. With the exception of h0, which is initialised randomly and optimised during training, all other initial states, r0 and (V0, s0), are set to 0-valued vectors/matrices and not updated during training.\nThe overall input it is concatenated with previous read rt\u22121 and passed to the RNN controller as input along with the previous controller state ht\u22121. The controller outputs its next state ht and a controller output o\u2032t, from which we obtain the push and pop scalars dt and ut and the value vector vt, which are passed to the stack, as well as the network output ot:\ndt = sigmoid(Wdo \u2032 t + bd) ut = sigmoid(Wuo \u2032 t + bu) vt = tanh(Wvo \u2032 t + bv) ot = tanh(Woo \u2032 t + bo)\nwhere Wd and Wu are vector-to-scalar projection matrices, and bd and bu are their scalar biases; Wv and Wo are vector-to-vector projections, and bd and bu are their vector biases, all randomly intialised and then tuned during training. Along with the previous stack state (Vt\u22121, st\u22121), the stack operations dt and ut and the value vt are passed to the neural stack to obtain the next read rt and next stack state (Vt, st), which are packed into a tuple with the controller state ht to form the next state Ht of the overall recurrent layer. The output vector ot serves as the overall output of the recurrent layer. The structure described here can be adapted to control a neural Queue instead of a stack by substituting one memory module for the other.\nThe only additional trainable parameters in either configuration, relative to a non-enhanced RNN, are the projections for the input concatenated with the previous read into the RNN controller, and the projections from the controller output into the various Stack/Queue inputs, described above. In the case of a DeQue, both the top read rtop and bottom read rbot must be preserved in the overall state. They are both concatenated with the input to form the input to the RNN controller. The output of the controller must have additional projections to output push/pop operations and values for the bottom of the DeQue. This roughly doubles the number of additional tunable parameters \u201cwrapping\u201d the RNN controller, compared to the Stack/Queue case."}, {"heading": "4 Experiments", "text": "In every experiment, integer-encoded source and target sequence pairs are presented to the candidate model as a batch of single joint sequences. The joint sequence starts with a start-of-sequence (SOS) symbol, and ends with an end-of-sequence (EOS) symbol, with a separator symbol separating the source and target sequences. Integer-encoded symbols are converted to 64-dimensional embeddings via an embedding matrix, which is randomly initialised and tuned during training. Separate wordto-index mappings are used for source and target vocabularies. Separate embedding matrices are used to encode input and output (predicted) embeddings."}, {"heading": "4.1 Synthetic Transduction Tasks", "text": "The aim of each of the following tasks is to read an input sequence, and generate as target sequence a transformed version of the source sequence, followed by an EOS symbol. Source sequences are randomly generated from a vocabulary of 128 meaningless symbols. The length of each training source sequence is uniformly sampled from unif {8, 64}, and each symbol in the sequence is drawn with replacement from a uniform distribution over the source vocabulary (ignoring SOS, and separator).\nA deterministic task-specific transformation, described for each task below, is applied to the source sequence to yield the target sequence. As the training sequences are entirely determined by the source sequence, there are close to 10135 training sequences for each task, and training examples are sampled from this space due to the random generation of source sequences. The following steps are followed before each training and test sequence are presented to the models, the SOS symbol (\u3008s\u3009) is prepended to the source sequence, which is concatenated with a separator symbol (|||) and the target sequences, to which the EOS symbol (\u3008/s\u3009) is appended.\nSequence Copying The source sequence is copied to form the target sequence. Sequences have the form:\n\u3008s\u3009a1 . . . ak|||a1 . . . ak\u3008/s\u3009\nSequence Reversal The source sequence is deterministically reversed to produce the target sequence. Sequences have the form:\n\u3008s\u3009a1a2 . . . ak|||ak . . . a2a1\u3008/s\u3009\nBigram flipping The source side is restricted to even-length sequences. The target is produced by swapping, for all odd source sequence indices i \u2208 [1, |seq|] \u2227 odd(i), the ith symbol with the (i+ 1)th symbol. Sequences have the form:\n\u3008s\u3009a1a2a3a4 . . . ak\u22121ak|||a2a1a4a3 . . . akak\u22121\u3008/s\u3009"}, {"heading": "4.2 ITG Transduction Tasks", "text": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17]. We present two simple ITG-based datasets with interesting linguistic properties and their underlying grammars. We show these grammars in Table 3, in Appendix B of the supplementary materials. For each synchronised non-terminal, an expansion is chosen according to the probability distribution specified by the rule probability p at the beginning of each rule. For each grammar, \u2018A\u2019 is always the root of the ITG tree.\nWe tuned the generative probabilities for recursive rules by hand so that the grammars generate left and right sequences of lengths 8 to 128 with relatively uniform distribution. We generate training data by rejecting samples that are outside of the range [8, 64], and testing data by rejecting samples outside of the range [65, 128]. For terminal symbol-generating rules, we balance the classes so that for k terminal-generating symbols in the grammar, each terminal-generating non-terminal \u2018X\u2019 generates a vocabulary of approximately 128/k, and each each vocabulary word under that class is equiprobable. These design choices were made to maximise the similarity between the experimental settings of the ITG tasks described here and the synthetic tasks described above.\nSubj\u2013Verb\u2013Obj to Subj\u2013Obj\u2013Verb A persistent challenge in machine translation is to learn to faithfully reproduce high-level syntactic divergences between languages. For instance, when translating an English sentence with a non-finite verb into German, a transducer must locate and move the verb over the object to the final position. We simulate this phenomena with a synchronous grammar which generates strings exhibiting verb movements. To add an extra challenge, we also simulate simple relative clause embeddings to test the models\u2019 ability to transduce in the presence of unbounded recursive structures.\nA sample output of the grammar is presented here, with spaces between words being included for stylistic purposes, and where s, o, and v indicate subject, object, and verb terminals respectively, i and o mark input and output, and rp indicates a relative pronoun:\nsi1 vi28 oi5 oi7 si15 rpi si19 vi16 oi10 oi24 ||| so1 oo5 oo7 so15 rpo so19 vo16 oo10 oo24 vo28\nGenderless to gendered grammar We design a small grammar to simulate translations from a language with gender-free articles to one with gender-specific definite and indefinite articles. A real world example of such a translation would be from English (the, a) to German (der/die/das, ein/eine/ein).\nThe grammar simulates sentences in (NP/(V/NP )) or (NP/V ) form, where every noun phrase can become an infinite sequence of nouns joined by a conjunction. Each noun in the source language has a neutral definite or indefinite article. The matching word in the target language then needs to be preceeded by its appropriate article. A sample output of the grammar is presented here, with spaces between words being included for stylistic purposes:\nwe11 the en19 and the em17 ||| wg11 das gn19 und der gm17"}, {"heading": "4.3 Evaluation", "text": "For each task, test data is generated through the same procedure as training data, with the key difference that the length of the source sequence is sampled from unif {65, 128}. As a result of this change, we not only are assured that the models cannot observe any test sequences during training, but are also measuring how well the sequence transduction capabilities of the evaluated models generalise beyond the sequence lengths observed during training. To control for generalisation ability, we also report accuracy scores on sequences separately sampled from the training set, which given the size of the sample space are unlikely to have ever been observed during actual model training.\nFor each round of testing, we sample 1000 sequences from the appropriate test set. For each sequence, the model reads in the source sequence and separator symbol, and begins generating the next symbol by taking the maximally likely symbol from the softmax distribution over target symbols produced by the model at each step. Based on this process, we give each model a coarse accuracy score, corresponding to the proportion of test sequences correctly predicted from beginning until end (EOS symbol) without error, as well as a fine accuracy score, corresponding to the average proportion of each sequence correctly generated before the first error. Formally, we have:\ncoarse = #correct\n#seqs fine =\n1\n#seqs #seqs\u2211 i=1 #correcti |targeti|\nwhere #correct and #seqs are the number of correctly predicted sequences (end-to-end) and the total number of sequences in the test batch (1000 in this experiment), respectively; #correcti is the number of correctly predicted symbols before the first error in the ith sequence of the test batch, and |targeti| is the length of the target segment that sequence (including EOS symbol)."}, {"heading": "4.4 Models Compared and Experimental Setup", "text": "For each task, we use as benchmarks the Deep LSTMs described in [1], with 1, 2, 4, and 8 layers. Against these benchmarks, we evaluate neural Stack-, Queue-, and DeQue-enhanced LSTMs. When running experiments, we trained and tested a version of each model where all LSTMs in each model have a hidden layer size of 256, and one for a hidden layer size of 512. The Stack/Queue/DeQue embedding size was arbitrarily set to 256, half the maximum hidden size. The number of parameters for each model are reported for each architecture in Table 1. We see that the neural Stack-, Queue-, and DeQue-enhanced LSTMs have the same number of trainable parameters as a two-layer Deep LSTM. These all come from the extra connections to and from the memory module, which itself has no trainable parameters, regardless of its logical size.\nModels are trained with minibatch RMSProp [18], with a batch size of 10. We grid-searched learning rates across the set {5 \u00d7 10\u22123, 1 \u00d7 10\u22123, 5 \u00d7 10\u22124, 1 \u00d7 10\u22124, 5 \u00d7 10\u22125}. We used gradient clipping [19], clipping all gradients above 1. Finally, to study the effect of random initialisation, we furthermore re-ran the experiments using random seeds in [1, 12], which were used to initialise the random number generator at the start of training. Average training perplexity was calculated every 100 batches. Training and test set accuracies were recorded every 1000 batches."}, {"heading": "5 Results and Discussion", "text": "Because of the impossibility of overfitting the datasets, we let the models train an unbounded number of steps, and report results at convergence. We present in Figure 2 the coarse- and fine-grained accuracies, for each task, of the best model of each architecture described in this paper alongside the best performing Deep LSTM benchmark. The best models were automatically selected based on average training perplexity. The LSTM benchmarks performed similarly across the range of random initialisations, so the effect of this procedure is primarily to try and select the better performing Stack/Queue/DeQue-enhanced LSTM. In most cases, this procedure does not yield the actual bestperforming model, and in practice a more sophisticated procedure such as ensembling [20] should produce better results.\nFor all experiments, the Neural Stack or Queue outperforms the Deep LSTM benchmarks, often by a significant margin. For most experiments, if a Neural Stack- or Queue-enhanced LSTM learns to partially or consistently solve the problem, then so does the Neural DeQue. For experiments where the enhanced LSTMs solve the problem completely (consistent accuracy of 1) in training, the accuracy persists in longer sequences in the test set, whereas benchmark accuracies drop for all experiments except the SVO to SOV and Gender Conjugation ITG transduction tasks. Across all tasks which the enhanced LSTMs solve, the convergence on the top accuracy happens orders of magnitude earlier for enhanced LSTMs than for benchmark LSTMs, as exemplified in Figure 2.\nThe results for the sequence inversion and copying tasks serve as unit tests for our models, as the controller mainly needs to learn to push the appropriate number of times and then pop continuously. Nonetheless, the failure of Deep LSTMs to learn such a regular pattern and generalise is itself indicative of the limitations of the benchmarks presented here, and of the relative expressive power of our models. Their ability to generalise perfectly to sequences up to twice as long as those attested during training is also notable, and also attested in the other experiments. Finally, this pair of experiments illustrates how while the neural Queue solves copying and the Stack solves reversal, a simple LSTM controller can learn to operate a DeQue as either structure, and solve both tasks.\nThe results of the Bigram Flipping task for all models are consistent with the failure to consistently correctly generate the last two symbols of the sequence. We hypothesise that both Deep LSTMs and\nour models economically learn to pairwise flip the sequence tokens, and attempt to do so half the time when reaching the EOS token. For the two ITG tasks, the success of Deep LSTM benchmarks relative to their performance in other tasks can be explained by their ability to exploit short local dependencies dominating the longer dependencies in these particular grammars.\nOverall, the rapid convergence, where possible, on a general solution to a transduction problem in a manner which propagates to longer sequences without loss of accuracy is indicative that an unbounded memory-enhanced controller can learn to solve these problems procedurally, rather than memorising the underlying distribution of the data."}, {"heading": "6 Conclusions", "text": "The experiments performed in this paper demonstrate that single-layer LSTMs enhanced by an unbounded differentiable memory capable of acting, in the limit, like a classical Stack, Queue, or DeQue, are capable of solving sequence-to-sequence transduction tasks for which Deep LSTMs falter. Even in tasks for which benchmarks obtain high accuracies, the memory-enhanced LSTMs converge earlier, and to higher accuracies, while requiring considerably fewer parameters than all but the simplest of Deep LSTMs. We therefore believe these constitute a crucial addition to our neural network toolbox, and that more complex linguistic transduction tasks such as machine translation or parsing will be rendered more tractable by their inclusion.\nAcknowledgements We thank Alex Graves, Demis Hassabis, Tomas Kocisky, Tim Rockta\u0308schel, Geoff Hinton, Ilya Sutskever, and many others for their helpful comments."}, {"heading": "A Analysis of the Backwards Dynamics of a Neural Stack", "text": "We describe here the backwards dynamics of the neural stack by examining the relevant partial derivatives of of the outputs with regard to the inputs, as defined in Equations 1\u20133. We use \u03b4ij to indicate the Kronecker delta (1 if i = j, 0 otherwise). The equations below hold for any valid row numbers i and n.\n\u2202Vt[i]\n\u2202Vt\u22121[n] = \u03b4in (12)\n\u2202Vt[i]\n\u2202vt = \u03b4it (13)\n\u2202st[i]\n\u2202dt = \u03b4it (14)\n\u2202rt \u2202Vt[n]\n= min(st[n],max(0, 1\u2212 t\u2211\nj=n+1\nst[j])) and \u2202rt \u2202vt = \u2202rt \u2202Vt[t] = dt (15)\n\u2202st[i]\n\u2202st\u22121[n] =  \u03b4in + 1 if i < t and st[i] > 0 and ut \u2212 t\u22121\u2211 j=i+1 st\u22121[j] > 0 \u03b4in if i < t and st[i] > 0 and ut \u2212 t\u22121\u2211 j=i+1 st\u22121[j] \u2264 0\n0 otherwise\n(16)\n\u2202st[i]\n\u2202ut =  -1 if i < t and st[i] > 0 and ut \u2212 t\u22121\u2211 j=i+1 st\u22121[j] > 0\n0 otherwise (17)\n\u2202rt \u2202st[n] = t\u2211 i=1 h(i, n) \u00b7 Vt[i]\nwhere h(i, n) =  \u03b4in if st[i] \u2264 max(0, 1\u2212 \u2211t j=i+1 st[j])) \u22121 if i < n and st[i] > max(0, 1\u2212 \u2211t\nj=i+1 st[j])) 0 otherwise\n(18)\nAll partial derivatives other than those obtained by the chain rule for derivatives can be assumed to be 0. The backwards dynamics for neural Queues an DeQues can be similarly derived from Equations 4\u201311.\nB Inversion Transduction Grammars used in Experiments\nWe present here, in Table 3, the inverse transduction grammars described in Section 4.2. Sets of terminal-generating rules are indicated by the form \u2018Xi \u2192 . . . \u2019, where i \u2208 [1, k] and p(Xi) \u2248 (100/k)\u22121 for k terminal generating non-terminal symbols (classes of terminals), so that the generated vocabulary is balanced across classes and of a size similar to other experiments."}, {"heading": "C Full Results", "text": "We show in Table 4 the full results for each task of the best performing models. The procedure for selecting the best performing model is described in Section 5."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The neural network pushdown automaton: Model, stack and learning", "author": ["GZ Sun", "C Lee Giles", "HH Chen", "YC Lee"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence", "author": ["Alex Graves"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Latent-variable modeling of string transductions with finite-state methods", "author": ["Markus Dreyer", "Jason R. Smith", "Jason Eisner"], "venue": "In Proceedings of the Conference on Empirical Methods in 9  Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Open- FST: A general and efficient weighted finite-state transducer library. In Implementation and Application of Automata, volume 4783 of Lecture Notes in Computer Science, pages 11\u201323", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu"], "venue": "Computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In Representation Learning Worksop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Sreerupa Das", "C Lee Giles", "Guo-Zheng Sun"], "venue": "In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Using prior knowledge in a {NNPDA} to learn context-free languages. Advances in neural information processing", "author": ["Sreerupa Das", "C Lee Giles", "Guo-Zheng Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "The theory of parsing, translation, and compiling", "author": ["Alfred V Aho", "Jeffrey D Ullman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1972}, {"title": "Machine translation with a stochastic grammatical channel", "author": ["Dekai Wu", "Hongsing Wong"], "venue": "In Proceedings of the 17th international conference on Computational linguistics-Volume", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Computing Research Repository (CoRR) abs/1211.5063,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Machine translation is a prototypical example of transduction and recent results indicate that Deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2].", "startOffset": 186, "endOffset": 192}, {"referenceID": 1, "context": "Machine translation is a prototypical example of transduction and recent results indicate that Deep RNNs have the ability to encode long source strings and produce coherent translations [1, 2].", "startOffset": 186, "endOffset": 192}, {"referenceID": 2, "context": "Further, inspired by prior work on neural network implementations of stack data structures [3], we propose and evaluate transduction models based on Neural Stacks, Queues, and DeQues (double ended queues).", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "While Deep RNNs based on long short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the same length as seen in training, they fail to consistently generalise to longer strings.", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "While Deep RNNs based on long short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the same length as seen in training, they fail to consistently generalise to longer strings.", "startOffset": 61, "endOffset": 67}, {"referenceID": 4, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 69, "endOffset": 75}, {"referenceID": 6, "context": "The most common approach leverages symbolic finite state transducers [6, 7], with approaches based on context free representations also being popular [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "RNNs offer an attractive alternative to symbolic transducers due to their simple algorithms and expressive representations [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 9, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 2, "context": "Previous work has touched on the topic of rendering discrete data structures such as stacks continuous, especially within the context of modelling pushdown automata with neural networks [10, 11, 3].", "startOffset": 186, "endOffset": 197}, {"referenceID": 10, "context": "The NTM and Memory Networks [4, 12, 13] provide powerful random access memory operations, whereas we focus on a more efficient and restricted class of models which we believe are sufficient for natural language transduction tasks.", "startOffset": 28, "endOffset": 39}, {"referenceID": 11, "context": "More closely related to our work, [14] have sought to develop a continuous stack controlled by an RNN.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "Inspired by the neural pushdown automaton of [3], we render these traditionally discrete operations continuous by letting push and pop operations be real values in the interval (0, 1).", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "The third step shows how setting the strength s3[2] to 0 for V3[2] logically removes v2 from the stack, and how it is ignored during the read.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "The third step shows how setting the strength s3[2] to 0 for V3[2] logically removes v2 from the stack, and how it is ignored during the read.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Following the work on rectified linear units [15], we arbitrarily take the partial differentiation of the left argument in these cases.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 241, "endOffset": 245}, {"referenceID": 14, "context": "The following tasks examine how well models can approach sequence transduction problems where the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8], a subclass of Synchronous Context-Free Grammars [16] often used in machine translation [17].", "startOffset": 280, "endOffset": 284}, {"referenceID": 6, "context": "We generate training data by rejecting samples that are outside of the range [8, 64], and testing data by rejecting samples outside of the range [65, 128].", "startOffset": 77, "endOffset": 84}, {"referenceID": 0, "context": "For each task, we use as benchmarks the Deep LSTMs described in [1], with 1, 2, 4, and 8 layers.", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "Models are trained with minibatch RMSProp [18], with a batch size of 10.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "We used gradient clipping [19], clipping all gradients above 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "Finally, to study the effect of random initialisation, we furthermore re-ran the experiments using random seeds in [1, 12], which were used to initialise the random number generator at the start of training.", "startOffset": 115, "endOffset": 122}], "year": 2015, "abstractText": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.", "creator": "LaTeX with hyperref package"}}}