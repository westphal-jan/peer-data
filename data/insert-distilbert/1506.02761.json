{"id": "1506.02761", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "abstract": "embedding words in a vector space has gained a lot of research topic attention in more recent years. while state - of - the - art methods provide efficient computation of word similarities via a low - dimensional matrix embedding, their motivation is often however left unclear. in this paper, we argue that word embedding can be naturally viewed as a ranking problem. then, based it on this insight, we propose a novel framework wordrank that efficiently estimates word representations via robust ranking. historically the performance of wordrank is measured in word similarity and cross word analogy benchmarks, and the results are compared to the state - of - the - art word equivalence embedding techniques. our algorithm produces a valid vector space with meaningful substructure, as evidenced by its performance of 77. 4 % image accuracy on a popular word similarity benchmark and 76 % on the google word analogy benchmark. wordrank performs especially well on small group corpora.", "histories": [["v1", "Tue, 9 Jun 2015 03:08:06 GMT  (108kb,D)", "http://arxiv.org/abs/1506.02761v1", null], ["v2", "Sat, 6 Feb 2016 06:02:56 GMT  (71kb,D)", "http://arxiv.org/abs/1506.02761v2", null], ["v3", "Tue, 23 Feb 2016 06:40:14 GMT  (69kb,D)", "http://arxiv.org/abs/1506.02761v3", null], ["v4", "Tue, 27 Sep 2016 21:11:56 GMT  (69kb,D)", "http://arxiv.org/abs/1506.02761v4", "Conference on Empirical Methods in Natural Language Processing (EMNLP), November 1-5, 2016, Austin, Texas, USA"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["shihao ji", "hyokun yun", "pinar yanardag", "shin matsushima", "s v n vishwanathan"], "accepted": true, "id": "1506.02761"}, "pdf": {"name": "1506.02761.pdf", "metadata": {"source": "CRF", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "authors": ["Shihao Ji", "Hyokun Yun", "Pinar Yanardag"], "emails": ["shihao.ji@intel.com", "yunhyoku@amazon.com", "ypinar@purdue.edu", "shin_matsushima@mist.", "vishy@ucsc.edu"], "sections": [{"heading": "1 Introduction", "text": "Embedding words into a vector space such that semantic and syntactic regularities between words are preserved, is an important sub-task for many applications of natural language processing. Mikolov et al. [18] generated considerable excitement in the machine learning and natural language processing communities by introducing a neural network based model, which they call word2vec. It was shown that word2vec produces state-of-the-art performance on both word similarity as well as word analogy tasks. The word similarity task is to retrieve words that are similar to a given word. On the other hand, word analogy requires answering queries of the form a:b;c:?, where a, b, and c are words from the vocabulary, and the answer to the query must be semantically related to c in the same way as b is related to a. This is best illustrated with a concrete example: Given the query king:queen;man:? we expect the model to output woman.\nThe impressive performance of word2vec led to a flurry of papers, which tried to explain and improve the performance of word2vec both theoretically [2] and empirically [13]. One interpretation of word2vec is that it is approximately maximizing the positive pointwise mutual information (PPMI), and Levy and Goldberg [13] showed that directly optimizing this gives good results. On the other hand, Pennington et al. [20] showed performance comparable to word2vec by using a modified matrix factorization model, which optimizes a log loss.\nSomewhat surprisingly, Levy et al. [14] showed that much of the performance gains of these new word embedding methods are due to certain hyperparameter optimizations and system-design choices. In other words, if one sets up careful experiments, then existing word embedding models\nar X\niv :1\n50 6.\n02 76\n1v 1\n[ cs\n.C L\n] 9\nmore or less perform comparably to each other. We conjecture that this is because, at a high level, all these methods are based on the following template: From a large text corpus eliminate infrequent words, and compute a |W|\u00d7|C|word-context co-occurrence count matrix; a context is a word which appears less than d distance away from given word in the text, where d is a tunable parameter. Let w \u2208 W be a word and c \u2208 C be a context, and letXw,c be the (potentially normalized) co-occurrence count. One learns a function f(w, c) which approximates a transformed version of Xw,c. Different methods differ essentially in the transformation function they use and the form of f [14]. For example, GloVe [20] uses f (w, c) = \u3008uw,vc\u3009 where uw and vc are k dimensional vectors, \u3008\u00b7, \u00b7\u3009 denotes the Euclidean dot product, and one approximates f (w, c) \u2248 logXw,c. On the other hand, as Levy and Goldberg [13] show, word2vec can be seen as using the same f(w, c) as GloVe but trying to approximate f (w, c) \u2248 PMI(Xw,c)\u2212 log k, where PMI(\u00b7) is the pairwise mutual information [6]. In this paper, we approach the word embedding task from a different perspective by formulating it as a ranking problem. That is, given a word w, we aim to output an ordered list (c1, c2, . . .) of context words from C such that words which co-occur with w appear at the top of the list. If rank(w, c) denotes the rank of c in the list, then typical ranking losses optimize the following objective: \u2211 (w,c)\u2208\u2126 \u03c1 (rank(w, c)), where \u2126 \u2282 W \u00d7 C is the set of word-context pairs that co-occur in the corpus, and \u03c1(\u00b7) is a monotonically increasing concave function (see Section 2 for a justification). Casting word embedding as ranking has two distinctive advantages. First, our method is discriminative rather than generative; in other words, instead of modeling (a transformation of) Xw,c directly, we only aim to model the relative order of Xw,\u00b7 values in each row. This formulation fits naturally to popular word embedding methods such as word analogy since instead of likelihood of each word1, we are interested in finding the most likely words in a given context. Second, casting word embedding as a ranking problem enables us to design models robust to noise [27]. This is an important problem in the domain of word embedding since the co-occurrence matrix might be noisy due to grammatical errors or unconventional use of language. In other words, certain words might co-occur purely because of chance. This phenomenon is more acute in smaller document corpora collected from diverse sources, and we will show in our experiments that our method can help mitigate this effect; with 17 million tokens our method performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark."}, {"heading": "2 Word Embedding via Ranking", "text": ""}, {"heading": "2.1 Notation", "text": "We use w to denote a word and c to denote a context. The set of all words, that is, the vocabulary is denoted asW and the set of all context words is denoted C. We will use \u2126 \u2282 W \u00d7 C to denote the set of all word-context pairs that were observed in the data, \u2126w to denote the set of contexts that co-occured with a given word w, \u2126c to denote the words that co-occurred with a given context word. The size of a set is denoted as |\u00b7|. The inner product between vectors is denoted as \u3008\u00b7, \u00b7\u3009."}, {"heading": "2.2 Ranking Model", "text": "Let uw denote the k-dimensional embedding of a word w, and vc denote that of a context c. For convenience, we collect embedding parameters for words and contexts as U := {uw}w\u2208W , and V := {vc}c\u2208C . We aim to capture the relevance of context c for word w by the inner product between their embedding vectors, \u3008uw,vc\u3009; the more relevant the context is, the larger we want the inner product to be. We achieve this by learning a ranking model that is parametrized by U and V. If we sort the set of contexts C for a given word w in terms of each context\u2019s inner product score with the word, the rank of a specific context c in this list can be written as [24]:\nrank (w, c) = \u2211\nc\u2032\u2208C\\{c}\nI (\u3008uw,vc\u3009 \u2212 \u3008uw,vc\u2032\u3009 \u2264 0) = \u2211\nc\u2032\u2208C\\{c}\nI (\u3008uw,vc \u2212 vc\u2032\u3009 \u2264 0) , (1)\n1Roughly speaking, this difference in viewpoint is analogous to the difference between pointwise loss function vs listwise loss functions used in ranking [12].\nwhere I(x \u2264 0) is a 0-1 loss function which is 1 if x \u2264 0 and 0 otherwise. Since I(x \u2264 0) is a discontinuous function, we follow the popular strategy in machine learning which replaces the 0-1 loss by its convex upper bound `(\u00b7), where `(\u00b7) can be any popular loss function for binary classification such as the hinge loss `(x) = max (0, 1\u2212 x) or the logistic loss `(x) = log2 (1 + 2\u2212x) [3]. This enables us to construct the following convex upper bound on the rank:\nrank (w, c) \u2264 rank (w, c) = \u2211\nc\u2032\u2208C\\{c}\n` (\u3008uw,vc \u2212 vc\u2032\u3009) . (2)\nIt is certainly desirable that the ranking model positions relevant contexts at the top of the list of contexts; this motivates us to write the objective function to minimize as:\nJ (U,V) := \u2211 w\u2208W \u2211 c\u2208\u2126w rw,c \u00b7 \u03c1 ( rank (w, c) + \u03b2 \u03b1 ) , (3)\nwhere rw,c is the weight between word w and context c quantifying the association between them, \u03c1(\u00b7) is a monotonically increasing ranking loss function that measures goodness of a rank, and \u03b1 > 0, \u03b2 > 0 are the hyperparameters of the model whose role will be discussed later. Following Pennington et al. [20], we use\nrw,c =\n{ (Xw,c/xmax)\nif Xw,c < xmax 1 otherwise,\n(4)\nwhere we set xmax = 100 and = 0.75 in our experiments. That is, we assign larger weights (with a saturation) to contexts that appear more often with the word of interest, and vice-versa. For the ranking loss function \u03c1(\u00b7), on the other hand, we consider the class of monotonically increasing and concave functions. While monotonicity is a natural requirement, we argue that concavity is also important so that the derivative of \u03c1 is always non-increasing; this implies that the ranking loss to be the most sensitive at the top of the list (where the rank is small) and becomes less sensitive at the lower end of the list (where the rank is high). Intuitively this is desirable, because we are interested in a small number of relevant contexts which frequently co-occur with a given word, and thus are willing to tolerate errors in rare contexts. This insensitivity at the bottom of the list makes the model robust to noise in the data either due to grammatical errors or unconventional use of language.\nWhat are interesting loss functions that can be used for \u03c1 (\u00b7)? Here are four possible alternatives, all of which have a natural interpretation (see the related work section for references and a discussion).\n\u03c1 (x) = \u03c10 (x) := x (identity) (5) \u03c1 (x) = \u03c11 (x) := log2 (1 + x) (logarithm) (6)\n\u03c1 (x) = \u03c12 (x) := 1\u2212 1\nlog2(2 + x) (negative DCG) (7)\n\u03c1 (x) = \u03c13 (x) := x1\u2212t \u2212 1\n1\u2212 t (logt with t 6= 1) (8)\nWe will explore the performance of each of these variants in our experiments. For now, we turn our attention to efficient stochastic optimization of the objective function (3)."}, {"heading": "2.3 Stochastic Optimization", "text": "Plugging (2) into (3), and replacing \u2211 w\u2208W \u2211 c\u2208\u2126c by \u2211 (w,c)\u2208\u2126, the objective function becomes:\nJ (U,V) = \u2211\n(w,c)\u2208\u2126\nrw,c \u00b7 \u03c1\n(\u2211 c\u2032\u2208C\\{c} ` (\u3008uw,vc \u2212 vc\u2032\u3009) + \u03b2\n\u03b1\n) . (9)\nThis function contains summations over \u2126 and C, both of which are expensive to compute for a large corpus. Although stochastic gradient descent (SGD) [4] can be used to replace the summation over \u2126 by random sampling, the summation over C cannot be avoided unless \u03c1(\u00b7) is a linear function. To work around this problem, we propose to optimize a linearized upper bound of the objective function obtained through a first-order Taylor approximation. Observe that due to concavity of \u03c1(\u00b7), we have\n\u03c1(x) \u2264 \u03c1 ( \u03be\u22121 ) + \u03c1\u2032 ( \u03be\u22121 ) \u00b7 ( x\u2212 \u03be\u22121 ) (10)\nfor any x and \u03be 6= 0. Moreover, the bound is tight when \u03be = x\u22121. This motivates us to introduce the set of auxiliary parameters \u039e := {\u03bew,c}(w,c)\u2208\u2126 and define the following upper bound of J (U,V): J (U,V,\u039e) := \u2211\n(w,c)\u2208\u2126\nrw,c \u00b7 \u03c1(\u03be\u22121wc ) + \u03c1\u2032(\u03be\u22121wc ) \u00b7 \u03b1\u22121\u03b2 + \u03b1\u22121 \u2211\nc\u2032\u2208C\\{c}\n` (\u3008uw,vc \u2212 vc\u2032\u3009)\u2212 \u03be\u22121w,c  . (11)\nNote that J (U,V) \u2264 J (U,V,\u039e) for any \u039e, due to (10)2. Also, minimizing (11) yields the same U and V as minimizing (9). To see this, suppose U\u0302 := {u\u0302w}w\u2208W and V\u0302 := {v\u0302c}c\u2208C minimizes (9). Then, by letting \u039e\u0302 := { \u03be\u0302w,c } (w,c)\u2208\u2126 where\n\u03be\u0302w,c = \u03b1\u2211\nc\u2032\u2208C\\{c} ` (\u3008u\u0302w, v\u0302c \u2212 v\u0302c\u2032\u3009) + \u03b2 , (12)\nwe have J ( U\u0302, V\u0302, \u039e\u0302 ) = J ( U\u0302, V\u0302 ) . Therefore, it suffices to optimize (11). However, unlike (9),\n(11) admits an efficient SGD algorithm. To see this, rewrite (11) as J (U,V,\u039e) := \u2211\n(w,c,c\u2032)\nrw,c \u00b7\n( \u03c1(\u03be\u22121w,c) + \u03c1\n\u2032(\u03be\u22121w,c) \u00b7 (\u03b1\u22121\u03b2 \u2212 \u03be\u22121w,c) |C| \u2212 1 + 1 \u03b1 \u03c1\u2032(\u03be\u22121w,c) \u00b7 ` (\u3008uw,vc \u2212 vc\u2032\u3009)\n) ,\n(13)\nwhere (w, c, c\u2032) \u2208 \u2126\u00d7 (C \\ {c}). Then, it can be seen that if we sample uniformly from (w, c) \u2208 \u2126 and c\u2032 \u2208 C \\ {c}, then j(w, c, c\u2032) :=\n|\u2126|\u00b7(|C| \u2212 1)\u00b7rw,c \u00b7\n( \u03c1(\u03be\u22121w,c) + \u03c1\n\u2032(\u03be\u22121w,c)\u00b7(\u03b1\u22121\u03b2 \u2212 \u03be\u22121w,c) |C| \u2212 1 + 1 \u03b1 \u03c1\u2032(\u03be\u22121w,c)\u00b7` (\u3008uw,vc \u2212 vc\u2032\u3009)\n) , (14)\nwhich does not contain any expensive summations, is an unbiased estimator of (13); that is, E [j(w, c, c\u2032)] = J (U,V,\u039e). On the other hand, one can optimize \u03bew,c exactly by using (12). Putting everything together yields a stochastic optimization algorithm WordRank, which can be specialized to a variety of ranking loss functions \u03c1(\u00b7) with weights rw,c (e.g., DCG is one of many possible instantiations). Algorithm 1 contains detailed pseudo-code. It can be seen that the algorithm is divided into two stages, a stage that updates (U,V) and another that updates \u039e. Note that the time complexity of the first stage is O(|\u2126|), since the cost of each update in line 8, 9 and 10 is independent of the size of the corpus. On the other hand, the time complexity of updating \u039e in line 15 is O(|\u2126| |C|), which can be expensive. To amortize this cost, we employ two tricks: We only update \u039e after a few iterations of updating U and V. Also, we exploit the fact that the most computationally involved operation in (12) involves a matrix and matrix multiplication which can be calculated efficiently via the SGEMM routine in BLAS [8]."}, {"heading": "2.4 Parallelization", "text": "The updates in Lines 8\u201310 have one remarkable property: To update uw, vc and vc\u2032 we only need to read the variables uw, vc, vc\u2032 and \u03bew,c. What this means is that updates to another triplet of variables uw\u0302, vc\u0302 and vc\u0302\u2032 can be performed independently. This observation is the key to developing a parallel optimization strategy, by distributing the computation of the updates among multiple processors. Due to lack of space, details including pseudo-code are relegated to Appendix A."}, {"heading": "2.5 Interpreting \u03b1 and \u03b2", "text": "The update (12) indicates that \u03be\u22121w,c is proportional to rank (w, c). On the other hand, one can observe that the loss function ` (\u00b7) in (14) is \u201cweighted\u201d by a \u03c1\u2032 ( \u03be\u22121w,c ) term. Since \u03c1 (\u00b7) is a concave function, its gradient \u03c1\u2032 (\u00b7) is a monotonically non-increasing function [22]. Consequently, when rank (w, c) and hence \u03be\u22121w,c is large, \u03c1\n\u2032 (\u03be\u22121w,c) is small. In other words, the loss function \u201cgives up\u201d on contexts 2One can simply set the auxiliary variables \u03bew,c = 1 when \u03c1 = \u03c10 because it is already a linear function.\nAlgorithm 1 WordRank algorithm. 1: \u03b7: step size 2: repeat 3: // Stage 1: Update U and V 4: repeat 5: Sample (w, c) uniformly from \u2126 6: Sample c\u2032 uniformly from C \\ {c} 7: // following three updates are executed simultaneously 8: uw \u2190 uw \u2212 \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 (vc \u2212 vc\u2032) 9: vc \u2190 vc \u2212 \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 uw 10: vc\u2032 \u2190 vc\u2032 + \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 uw 11: until U and V are converged 12: // Stage 2: Update \u039e 13: for w \u2208 W do 14: for c \u2208 C do 15: \u03bew,c = \u03b1/ (\u2211 c\u2032\u2208C\\{c} ` (\u3008uw,vc \u2212 vc\u2032\u3009) + \u03b2\n) 16: end for 17: end for 18: until U, V and \u039e are converged\nwith a high rank in order to focus its attention on top of the list. The rate at which the algorithm gives up is determined by the hyperparameters \u03b1 and \u03b2. Intuitively, \u03b1 can be viewed as a scale parameter while \u03b2 can be viewed as an offset parameter. An equivalent interpretation is that by choosing different values of \u03b1 and \u03b2 one can modify the behavior of the ranking loss \u03c1 (\u00b7) in a problem dependent fashion. In Appendix B we plot different \u03c11 functions by varying \u03b1 and \u03b2 to illustrate this effect. In our experiments, we found that setting \u03b1 = 100 and \u03b2 = 99 gave the best results."}, {"heading": "3 Related Work", "text": "We already discussed related work on word embedding in the introduction. Here we discuss related work from the perspective of the ranking model (3). The use of score functions \u3008uw,vc\u3009 for ranking is inspired by the latent collaborative retrieval framework of Weston et al. [25]. Writing the rank as a sum of indicator functions (1), and upper bounding it via a convex loss (2) is due to [23]. Using \u03c10 (\u00b7) (5) corresponds to the well-known pairwise ranking loss (see e.g., Lee and Lin [12]). On the other hand, Yun et al. [27] observe that if we set \u03c1 = \u03c12 as in (7), then \u2212J (U,V) corresponds to the DCG (Discounted Cumulative Gain) [17], one of the most popular ranking metrics. In their RobiRank algorithm they proposed the use of \u03c1 = \u03c11 (6), which they considered to be a special function for which one can derive an efficient stochastic optimization procedure. However, as we showed in this paper, the general class of monotonically increasing concave functions can be handled efficiently. Another important difference of our approach are the hyperparameters \u03b1 and \u03b2, which we use to modify the behavior of \u03c1, and which we find are critical to obtaining good empirical results. Ding and Vishwanathan [7] proposed the use of \u03c1 = logt in the context of robust binary classification, while here we are concerned with ranking. Our formulation is general and applies to a variety of ranking losses \u03c1 (\u00b7) with weights rw,c. Optimizing over U and V by distributing the computation across processors is inspired by work on distributed stochastic gradient for matrix factorization [10]."}, {"heading": "4 Experiments", "text": "In our experiments, we first evaluate the impact of the weighting rw,c and the ranking loss function \u03c1(\u00b7) on the test performance using a small dataset. We will then pick the best performing model and compare it against word2vec [19] and GloVe [20]. We closely follow the framework of Levy et al. [14] to set up a careful and fair comparison of the three methods. Our code and experiment scripts will be made available for download from http://anonymous.\nTraining Corpus Models are trained on a combined corpus of 7.2 billion tokens, which consists of the 2015 Wikipedia dump with 1.6 billion tokens, the WMT14 News Crawl3 with 1.7 billion tokens, the \u201cOne Billion Word Language Modeling Benchmark\u201d4 with almost 1 billion tokens, and UMBC webbase corpus5 with around 3 billion tokens. The Wikipedia dump is in XML format and the article content needs to be parsed from wiki markups, while the other corpora are already in plain text format. The pre-processing pipeline breaks the paragraphs into sentences, tokenizes and lowercases each corpus with the Stanford tokenizer. We further clean up the dataset by removing non-ASCII characters and punctuation, and discard sentences that are shorter than 3 tokens or longer than 500 tokens. In the end, we obtain a dataset with 7.2 billion tokens, with the first 1.6 billion tokens from Wikipedia. When we want to experiment with a smaller corpus, we extract a subset which contains the specified number of tokens.\nCo-occurrence matrix construction We use the GloVe code to construct the co-occurrence matrix X , and the same matrix is used to train GloVe and WordRank models. When constructing X , we must choose the size of the vocabulary, the context window and whether to distinguish left context from right context. We follow the findings and design choices of GloVe and use a symmetric window of sizewinwith a decreasing weighting function, so that word pairs that are dwords apart contribute 1/d to the total count. Specifically, when the corpus is small (e.g., 17M, 32M, 64M) we letwin = 15 and for larger corpora we let win = 10. The larger window size alleviates the data sparsity issue for small corpus at the expense of adding more noise to X . The parameter settings used in our experiments is summarized in Table 1.\nUsing the trained model It has been shown by Pennington et al. [20] that combining the uw and vc vectors with equal weights gives a small boost in performance. This vector combination was originally motivated as an ensemble method [20], and later Levy et al. [14] provided a different interpretation of its effect on the cosine similarity function, and show that adding context vectors effectively adds first-order similarity terms to the second-order similarity function. In our experiments, we find that vector combination boosts the performance in word analogy task when training set is small, but when dataset is large enough (e.g., 7.2 billion tokens), vector combination hurts performance a little. More interestingly, for the word similarity task, we find that vector combination is detrimental in all the cases, sometimes even substantially6. Therefore, we will always use uw on word similarity task, and use uw + vc on word analogy task unless otherwise noted."}, {"heading": "4.1 Evaluation", "text": "Word Similarity We use six datasets to evaluate word similarity: WS-353 [9] partitioned into two subsets: WordSim Similarity and WordSim Relatedness [1]; MEN [5]; Mechanical Turk [21]; Rare words [15]; and SimLex-999 [11]. They contain word pairs together with human-assigned similarity judgments. The word representations are evaluated by ranking the pairs according to their cosine similarities, and measuring the Spearman\u2019s rank correlation coefficient with the human judgments.\nWord Analogies For this task, we use the Google analogy dataset [18]. It contains 19544 word analogy questions, partitioned into 8869 semantic and 10675 syntactic questions. The semantic questions contain five types of semantic analogies, such as capital cities (Paris:France;Tokyo:?),\n3http://www.statmt.org/wmt14/translation-task.html 4http://www.statmt.org/lm-benchmark 5http://ebiquity.umbc.edu/resource/html/id/351 6This is possible since we optimize a ranking loss: the absolute scores don\u2019t matter as long as they yield a ordered list correctly. Thus, our algorithms uw and vc are less comparable to each other than those generated by GloVe, which employs a point-wise L2 loss.\ncurrency (USA:dollar;India:?) or people (king:queen;man:?). The syntactic questions contains nine types of analogies, such as plural nouns, opposite, or comparative, for example good:better;smart:?. A question is correctly answered only if the algorithm selects the word that is exactly the same as the correct word in the question: synonyms are thus counted as mistakes. There are two ways to answer these questions, namely, by using 3CosAdd or 3CosMul (see [13] for details). We will report scores by using 3CosAdd by default, and indicate when 3CosMul gives better performance.\nHandling questions with out of vocabulary words Some papers (e.g., [14]) filter out questions with out of vocabulary words when reporting performance. However, in our experiments, if any word of a question is out of vocabulary, the corresponding question will be marked as unanswerable and will get a score of zero. This decision is made so that when the size of vocabulary increases, the model performance is still comparable across different experiments.\n4.2 The impact of \u03c1(\u00b7) and rw,c\nIn Section 2.2 we argued the need for adding a weight rw,c. We also presented our framework which can deal with different \u03c1 functions. We now study the utility of these two ideas. We report results on the 17 million token dataset in Table 2, and note that similar trends were observed on larger datasets. For the similarity task, we use the WS-353 test set and for the analogy task we use the Google analogy test set. The best scores for each task are underlined. We set t = 1.5 for \u03c13. \u201cOff\u201d means that we used uniform weighting rw,c = 1, and \u201con\u201d means that rw,c was set as in (4). For comparison, we also include the results using RobiRank [27]7.\nWe find that (a) adding the weighting function improves performance in all cases, especially for word analogy. (b) among the four \u03c1 functions, \u03c11 = log performs the best over all. Moreover, \u03c10 performs the best on the word similarity task but suffers on the analogy task. Given these observations, which are consistent with the results on large scale datasets, in the large scale experiments we only consider WordRank with the best configuration, e.g., using \u03c11 with the weighting as in Eq. (4)."}, {"heading": "4.3 Comparison to state-of-the-art", "text": "In this section we compare the performance of WordRank with word2vec8 and GloVe9, by using the code provided by the respective authors. For a fair comparison, GloVe and WordRank are given as input the same co-occurrence matrix X; this eliminates differences in performance due to window size and other such artifacts, and the same parameters are used to word2vec. Moreover, the embedding dimensions used for each of the three methods is the same (see Table 1). With word2vec, we train the Skip-Gram with Negative Sampling (SGNS) model since it produces state-of-the-art performance, and is widely used in the NLP community [19]. For GloVe, we use the default parameters as suggested by [20]. The results can be seen in Figure 1 (also see Table 4 in Appendix C for additional details).\nAs can be seen, (a) when the size of corpus increases, in general all three algorithms improve their prediction accuracy on both tasks. This is to be expected since a larger corpus typically produces better statistics and less noise in the co-occurrence matrix X; (b) when the corpus size is small (e.g., 17M, 32M, 64M, 128M), WordRank yields the best performance among three, followed by word2vec and GloVe; (c) for a large corpus, word2vec and GloVe become very competitive to WordRank, and\n7We used the code provided by the authors at https://bitbucket.org/d_ijk_stra/robirank. Although related to an unweighted version of WordRank with the \u03c11 ranking loss function, we attribute the difference in performance to the use of the \u03b1 and \u03b2 hyperparameters in our algorithm and certain implementation details.\n8https://code.google.com/p/word2vec/ 9http://nlp.stanford.edu/projects/glove\neventually perform neck-to-neck on the word analogy task. This is consistent with the findings of [14] indicating that when the number of tokens is large even simple algorithms can perform well; (d) In all the cases, WordRank is dominant on word similarity task since it optimizes a ranking loss explicitly, which aligns more naturally with the objective of word similarity.\nAs a side note, on a similar 1.6-billion-token Wikipedia corpus, our word2vec and GloVe performance scores are somewhat better than or close to the results reported by Pennington et al. [20]; and our word2vec and GloVe scores on the 7.2-billion-token dataset are close to what they reported on a 42-billion-token dataset. We believe that this discrepancy is primary due to the extra attention we paid to pre-processing on the Wikipedia and other corpora.\nTo further evaluate the models on the word similarity task, we use the models trained on the 7.2- billion-token corpus to predict on the six word similarity datasets described in Section 4.1. Moreover, we breakdown the performance of the models on the Google word analogy dataset into the semantic and syntactic subtasks. Results are listed in Table 3. As can be seen, WordRank outperforms word2vec and GloVe on 5 of 6 similarity tasks, and 1 of 2 Google analogy subtasks."}, {"heading": "5 Discussion", "text": "We found that WordRank using the \u03c10 loss on the 7.2 billion token dataset achieves 77.4% accuracy on WS-353 (see Table 4 in Appendix C). To the best of our knowledge, this is better than any published result on this benchmark. Unfortunately, the corresponding performance of on the word analogy task is lower than achieved using other ranking losses. This is because word analogy is a retrieval problem, and therefore it is important to emphasize the performance at the top of the list in order to achieve good performance.\nThe additional flexibility of the WordRank framework means that it is computationally more expensive than either word2vec or GloVe. Therefore, we recommend the use of WordRank in settings where data is sparse, and one is willing to trade-off extra computation for better generalization."}, {"heading": "B Modifying \u03c11 using \u03b1 and \u03b2", "text": "Algorithm 2 Distributed WordRank algorithm. 1: \u03b7: step size 2: repeat 3: // Start outer iteration 4: Sample a partition over contexts { C(1), . . . , C(q)\n} 5: // Stage 1: Update U and V in parallel for all machine q \u2208 {1, 2, . . . , p} do in parallel 6: Fetch all vc \u2208 V(q) 7: repeat 8: Sample (w, c) uniformly from \u2126(q) \u2229 ( W(q) \u00d7 C(q)\n) 9: Sample c\u2032 uniformly from C(q) \\ {c}\n10: // following three updates are done simultaneously 11: uw \u2190 uw \u2212 \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 (vc \u2212 vc\u2032) 12: vc \u2190 vc \u2212 \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 uw 13: vc\u2032 \u2190 vc\u2032 + \u03b7 \u00b7 rw,c \u00b7 \u03c1\u2032(\u03be\u22121w,c) \u00b7 `\u2032 (\u3008uw,vc \u2212 vc\u2032\u3009) \u00b7 uw 14: until predefined time limit is exceeded 15: end for 16: // Stage 2: Update \u039e in parallel for all machine q \u2208 {1, 2, . . . , p} do in parallel 17: Fetch all vc \u2208 V 18: for w \u2208 W(q) do 19: for c \u2208 C do 20: \u03bew,c = \u03b1/ (\u2211 c\u2032\u2208C\\{c} ` (\u3008uw,vc \u2212 vc\u2032\u3009) + \u03b2\n) 21: end for 22: end for 23: end for 24: until U, V and \u039e are converged"}, {"heading": "C Additional Experimental Details", "text": "Table 4 is the tabular view of the data plotted in Figure 1 in order to provide additional experimental details.\nD Visualizing the results\nTo understand whether WordRank produces syntatically and semantically meaningful results, we did the following experiment: we use the model produced using 7.2 billion tokens, and compute the\nnearest neighbors of the word \u201ccat\u201d. We then visualize the words in two dimensions by using t-SNE, which is a well known technique for dimensionality reduction [16]. As can be seen in Figure 3, our model is indeed capable of capturing both semantic (e.g., cat, feline, kitten, tabby) and syntactic (e.g., leash, leashes, leashed) regularities of the English language."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Embedding words in a vector space has gained a lot of research attention in re-<lb>cent years. While state-of-the-art methods provide efficient computation of word<lb>similarities via a low-dimensional matrix embedding, their motivation is often left<lb>unclear. In this paper, we argue that word embedding can be naturally viewed<lb>as a ranking problem. Then, based on this insight, we propose a novel frame-<lb>work WordRank that efficiently estimates word representations via robust ranking.<lb>The performance of WordRank is measured in word similarity and word analogy<lb>benchmarks, and the results are compared to the state-of-the-art word embedding<lb>techniques. Our algorithm produces a vector space with meaningful substructure,<lb>as evidenced by its performance of 77.4% accuracy on a popular word similarity<lb>benchmark and 76% on the Google word analogy benchmark. WordRank per-<lb>forms especially well on small corpora.", "creator": "TeX"}}}