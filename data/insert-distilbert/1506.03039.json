{"id": "1506.03039", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "Measuring Sample Quality with Stein's Method", "abstract": "to improve precisely the efficiency of monte carlo estimation, practitioners originally are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational analytical speed. the reasoning is sound : a reduction in variance due to more rapid sampling can outweigh the bias introduced. however, the inexactness creates no new challenges for sampler and parameter selection, since standard measures of sample test quality like effective sample size calculation do well not formally account for asymptotic descriptive bias. to greatly address these challenges, we introduce a new computable quality curve measure based on stein's method that bounds the discrepancy between sample and target expectations over a large class of test functions. finally we use merely our tool to compare evaluating exact, biased, reliable and deterministic sample sequences and illustrate applications to hyperparameter subset selection, optimization convergence rate assessment, and quantifying bias - variance tradeoffs in posterior inference.", "histories": [["v1", "Tue, 9 Jun 2015 18:48:58 GMT  (285kb,D)", "https://arxiv.org/abs/1506.03039v1", "29 pages, 5 figures"], ["v2", "Fri, 19 Jun 2015 05:15:18 GMT  (354kb,D)", "http://arxiv.org/abs/1506.03039v2", "28 pages, 5 figures"], ["v3", "Sat, 12 Sep 2015 23:31:21 GMT  (293kb,D)", "http://arxiv.org/abs/1506.03039v3", "28 pages, 6 figures"], ["v4", "Mon, 11 Jan 2016 03:47:27 GMT  (1387kb,D)", "http://arxiv.org/abs/1506.03039v4", "17 pages, 6 figures"], ["v5", "Mon, 6 Mar 2017 18:59:16 GMT  (1389kb,D)", "http://arxiv.org/abs/1506.03039v5", "17 pages, 6 figures"]], "COMMENTS": "29 pages, 5 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.PR stat.ME", "authors": ["jackson gorham", "lester w mackey"], "accepted": true, "id": "1506.03039"}, "pdf": {"name": "1506.03039.pdf", "metadata": {"source": "CRF", "title": "Measuring Sample Quality with Stein\u2019s Method", "authors": ["Jackson Gorham"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "When faced with a complex target distribution, one often turns to Markov chain Monte Carlo (MCMC) [1] to approximate intractable expectations EP [h(Z)] = \u222b X p(x)h(x)dx with asymp-\ntotically exact sample estimates EQ[h(X)] = \u2211n i=1 q(xi)h(xi). These complex targets commonly arise as posterior distributions in Bayesian inference and as candidate distributions in maximum likelihood estimation [2]. In recent years, researchers [e.g., 3, 4, 5] have introduced asymptotic bias into MCMC procedures to trade off asymptotic correctness for improved sampling speed. The rationale is that more rapid sampling can reduce the variance of a Monte Carlo estimate and hence outweigh the bias introduced. However, the added flexibility introduces new challenges for sampler and parameter selection, since standard sample quality measures, like effective sample size, asymptotic variance, trace and mean plots, and pooled and within-chain variance diagnostics, presume eventual convergence to the target [1] and hence do not account for asymptotic bias.\nTo address this shortcoming, we develop a new measure of sample quality suitable for comparing asymptotically exact, asymptotically biased, and even deterministic sample sequences. The quality measure is based on Stein\u2019s method and is attainable by solving a linear program. After outlining our design criteria in Section 2, we relate the convergence of the quality measure to that of standard probability metrics in Section 3, develop a streamlined implementation based on geometric spanners in Section 4, and illustrate applications to hyperparameter selection, convergence rate assessment, and the quantification of bias-variance tradeoffs in posterior inference in Section 5. We discuss related work in Section 6 and defer all proofs to the appendix.\nNotation We denote the `2, `1, and `\u221e norms on Rd by \u2016\u00b7\u20162, \u2016\u00b7\u20161, and \u2016\u00b7\u2016\u221e respectively. We will often refer to a generic norm \u2016\u00b7\u2016 on Rd with associated dual norms \u2016w\u2016\u2217 , supv\u2208Rd:\u2016v\u2016=1 \u3008w, v\u3009 for vectors w \u2208 Rd, \u2016M\u2016\u2217 , supv\u2208Rd:\u2016v\u2016=1 \u2016Mv\u2016\n\u2217 for matrices M \u2208 Rd\u00d7d, and \u2016T\u2016\u2217 , supv\u2208Rd:\u2016v\u2016=1 \u2016T [v]\u2016\n\u2217 for tensors T \u2208 Rd\u00d7d\u00d7d. We denote the j-th standard basis vector by ej , the partial derivative \u2202\u2202xk by\u2207k, and the gradient of any R\nd-valued function g by\u2207g with components (\u2207g(x))jk , \u2207kgj(x).\nar X\niv :1\n50 6.\n03 03\n9v 5\n[ st\nat .M\nL ]\n6 M\nar 2\n01 7"}, {"heading": "2 Quality Measures for Samples", "text": "Consider a target distribution P with open convex support X \u2286 Rd and continuously differentiable density p. We assume that p is known up to its normalizing constant and that exact integration under P is intractable for most functions of interest. We will approximate expectations under P with the aid of a weighted sample, a collection of distinct sample points x1, . . . , xn \u2208 X with weights q(xi) encoded in a probability mass function q. The probability mass function q induces a discrete distributionQ and an approximation EQ[h(X)] = \u2211n i=1 q(xi)h(xi) for any target expectation EP [h(Z)]. We make no assumption about the provenance of the sample points; they may arise as random draws from a Markov chain or even be deterministically selected.\nOur goal is to compare the fidelity of different samples approximating a common target distribution. That is, we seek to quantify the discrepancy between EQ and EP in a manner that (i) detects when a sequence of samples is converging to the target, (ii) detects when a sequence of samples is not converging to the target, and (iii) is computationally feasible. We begin by considering the maximum deviation between sample and target expectations over a class of real-valued test functionsH,\ndH(Q,P ) = sup h\u2208H |EQ[h(X)]\u2212 EP [h(Z)]|. (1)\nWhen the class of test functions is sufficiently large, the convergence of dH(Qm, P ) to zero implies that the sequence of sample measures (Qm)m\u22651 converges weakly to P . In this case, the expression (1) is termed an integral probability metric (IPM) [6]. By varying the class of test functions H, we can recover many well-known probability metrics as IPMs, including the total variation distance, generated byH = {h : X \u2192 R | supx\u2208X |h(x)| \u2264 1}, and the Wasserstein distance (also known as the Kantorovich-Rubenstein or earth mover\u2019s distance), dW\u2016\u00b7\u2016 , generated by\nH =W\u2016\u00b7\u2016 , {h : X \u2192 R | supx 6=y\u2208X |h(x)\u2212h(y)| \u2016x\u2212y\u2016 \u2264 1}.\nThe primary impediment to adopting an IPM as a sample quality measure is that exact computation is typically infeasible when generic integration under P is intractable. However, we could skirt this intractability by focusing on classes of test functions with known expectation under P . For example, if we consider only test functions h for which EP [h(Z)] = 0, then the IPM value dH(Q,P ) is the solution of an optimization problem depending on Q alone. This, at a high level, is our strategy, but many questions remain. How do we select the class of test functions h? How do we know that the resulting IPM will track convergence and non-convergence of a sample sequence (Desiderata (i) and (ii))? How do we solve the resulting optimization problem in practice (Desideratum (iii))? To address the first two of these questions, we draw upon tools from Charles Stein\u2019s method of characterizing distributional convergence. We return to the third question in Section 4."}, {"heading": "3 Stein\u2019s Method", "text": "Stein\u2019s method [7] for characterizing convergence in distribution classically proceeds in three steps:\n1. Identify a real-valued operator T acting on a set G of Rd-valued1 functions of X for which EP [(T g)(Z)] = 0 for all g \u2208 G. (2)\nTogether, T and G define the Stein discrepancy,\nS(Q, T ,G) , sup g\u2208G |EQ[(T g)(X)]| = sup g\u2208G |EQ[(T g)(X)]\u2212 EP [(T g)(Z)]| = dT G(Q,P ),\nan IPM-type quality measure with no explicit integration under P . 2. Lower bound the Stein discrepancy by a familiar convergence-determining IPM dH. This\nstep can be performed once, in advance, for large classes of target distributions and ensures that, for any sequence of probability measures (\u00b5m)m\u22651, S(\u00b5m, T ,G) converges to zero only if dH(\u00b5m, P ) does (Desideratum (ii)).\n3. Upper bound the Stein discrepancy by any means necessary to demonstrate convergence to zero under suitable conditions (Desideratum (i)). In our case, the universal bound established in Section 3.3 will suffice.\n1Scalar functions g are more common in Stein\u2019s method, but we will find Rd-valued g more convenient.\nWhile Stein\u2019s method is typically employed as an analytical tool, we view the Stein discrepancy as a promising candidate for a practical sample quality measure. Indeed, in Section 4, we will adopt an optimization perspective and develop efficient procedures to compute the Stein discrepancy for any sample measure Q and appropriate choices of T and G. First, we assess the convergence properties of an equivalent Stein discrepancy in the subsections to follow."}, {"heading": "3.1 Identifying a Stein Operator", "text": "The generator method of Barbour [8] provides a convenient and general means of constructing operators T which produce mean-zero functions under P (2) . Let (Zt)t\u22650 represent a Markov process with unique stationary distribution P . Then the infinitesimal generator A of (Zt)t\u22650, defined by\n(Au)(x) = lim t\u21920 (E[u(Zt) | Z0 = x]\u2212 u(x))/t for u : Rd \u2192 R,\nsatisfies EP [(Au)(Z)] = 0 under mild conditions on A and u. Hence, a candidate operator T can be constructed from any infinitesimal generator.\nFor example, the overdamped Langevin diffusion, defined by the stochastic differential equation dZt = 1 2\u2207 log p(Zt)dt+ dWt for (Wt)t\u22650 a Wiener process, gives rise to the generator\n(APu)(x) = 1 2 \u3008\u2207u(x),\u2207 log p(x)\u3009+ 1 2 \u3008\u2207,\u2207u(x)\u3009. (3)\nAfter substituting g for 12\u2207u, we obtain the associated Stein operator 2\n(TP g)(x) , \u3008g(x),\u2207 log p(x)\u3009+ \u3008\u2207, g(x)\u3009. (4) The Stein operator TP is particularly well-suited to our setting as it depends on P only through the derivative of its log density and hence is computable even when the normalizing constant of p is not.\nIf we let \u2202X denote the boundary ofX (an empty set whenX = Rd) and n(x) represent the outward unit normal vector to the boundary at x, then we may define the classical Stein set\nG\u2016\u00b7\u2016 , { g : X \u2192 Rd \u2223\u2223\u2223\u2223 sup x 6=y\u2208X max ( \u2016g(x)\u2016\u2217, \u2016\u2207g(x)\u2016\u2217, \u2016\u2207g(x)\u2212\u2207g(y)\u2016 \u2217 \u2016x\u2212 y\u2016 ) \u2264 1 and\n\u3008g(x), n(x)\u3009 = 0,\u2200x \u2208 \u2202X with n(x) defined }\nof sufficiently smooth functions satisfying a Neumann-type boundary condition. The following proposition \u2013 a consequence of integration by parts \u2013 shows that G\u2016\u00b7\u2016 is a suitable domain for TP . Proposition 1. If EP [\u2016\u2207 log p(Z)\u2016] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 G\u2016\u00b7\u2016 .\nTogether, TP and G\u2016\u00b7\u2016 form the classical Stein discrepancy S(Q, TP ,G\u2016\u00b7\u2016), our chief object of study."}, {"heading": "3.2 Lower Bounding the Classical Stein Discrepancy", "text": "In the univariate setting (d = 1), it is known for a wide variety of targets P that the classical Stein discrepancy S(\u00b5m, TP ,G\u2016\u00b7\u2016) converges to zero only if the Wasserstein distance dW\u2016\u00b7\u2016(\u00b5m, P ) does [? 10]. In the multivariate setting, analogous statements are available for multivariate Gaussian targets [11, 12, 13], but few other target distributions have been analyzed. To extend the reach of the multivariate literature, we show in Theorem 2 that the classical Stein discrepancy also determines Wasserstein convergence for a large class of strongly log-concave densities, including the Bayesian logistic regression posterior under Gaussian priors. Theorem 2 (Stein Discrepancy Lower Bound for Strongly Log-concave Densities). If X = Rd, and log p is strongly concave with third and fourth derivatives bounded and continuous, then, for any probability measures (\u00b5m)m\u22651, S(\u00b5m, TP ,G\u2016\u00b7\u2016)\u2192 0 only if dW\u2016\u00b7\u2016(\u00b5m, P )\u2192 0.\nWe emphasize that the sufficient conditions in Theorem 2 are certainly not necessary for lower bounding the classical Stein discrepancy. We hope that the theorem and its proof will provide a template for lower bounding S(Q, TP ,G\u2016\u00b7\u2016) for other large classes of multivariate target distributions.\n2The operator TP has also found fruitful application in the design of Monte Carlo control variates [9]."}, {"heading": "3.3 Upper Bounding the Classical Stein Discrepancy", "text": "We next establish sufficient conditions for the convergence of the classical Stein discrepancy to zero. Proposition 3 (Stein Discrepancy Upper Bound). IfX \u223c Q and Z \u223c P with\u2207 log p(Z) integrable,\nS(Q, TP ,G\u2016\u00b7\u2016) \u2264 \u2016I\u2016E[\u2016X \u2212 Z\u2016] + E[\u2016\u2207 log p(X)\u2212\u2207 log p(Z)\u2016] + E [\u2225\u2225\u2207 log p(Z)(X \u2212 Z)>\u2225\u2225]\n\u2264 \u2016I\u2016E[\u2016X \u2212 Z\u2016] + E[\u2016\u2207 log p(X)\u2212\u2207 log p(Z)\u2016] + \u221a E [ \u2016\u2207 log p(Z)\u20162 ] E [ \u2016X \u2212 Z\u20162 ] .\nOne implication of Proposition 3 is that S(Qm, TP ,G\u2016\u00b7\u2016) converges to zero whenever Xm \u223c Qm converges in mean-square to Z \u223c P and\u2207 log p(Xm) converges in mean to\u2207 log p(Z)."}, {"heading": "3.4 Extension to Non-uniform Stein Sets", "text": "The analyses and algorithms in this paper readily accommodate non-uniform Stein sets of the form Gc1:3\u2016\u00b7\u2016 , { g : X \u2192 Rd \u2223\u2223\u2223\u2223 supx 6=y\u2208X max ( \u2016g(x)\u2016\u2217 c1 , \u2016\u2207g(x)\u2016 \u2217 c2 , \u2016\u2207g(x)\u2212\u2207g(y)\u2016 \u2217 c3\u2016x\u2212y\u2016 ) \u2264 1 and\n\u3008g(x), n(x)\u3009 = 0,\u2200x \u2208 \u2202X with n(x) defined\n} (5)\nfor constants c1, c2, c3 > 0 known as Stein factors in the literature. We will exploit this additional flexibility in Section 5.2 to establish tight lower-bounding relations between the Stein discrepancy and Wasserstein distance for well-studied target distributions. For general use, however, we advocate the parameter-free classical Stein set and graph Stein sets to be introduced in the sequel. Indeed, any non-uniform Stein discrepancy is equivalent to the classical Stein discrepancy in a strong sense: Proposition 4 (Equivalence of Non-uniform Stein Discrepancies). For any c1, c2, c3 > 0,\nmin(c1, c2, c3)S(Q, TP ,G\u2016\u00b7\u2016) \u2264 S(Q, TP ,Gc1:3\u2016\u00b7\u2016 ) \u2264 max(c1, c2, c3)S(Q, TP ,G\u2016\u00b7\u2016)."}, {"heading": "4 Computing Stein Discrepancies", "text": "In this section, we introduce an efficiently computable Stein discrepancy with convergence properties equivalent to those of the classical discrepancy. We restrict attention to the unconstrained domain X = Rd in Sections 4.1-4.3 and present extensions for constrained domains in Section 4.4."}, {"heading": "4.1 Graph Stein Discrepancies", "text": "Evaluating a Stein discrepancy S(Q, TP ,G) for a fixed (Q,P ) pair reduces to solving an optimization program over functions g \u2208 G. For example, the classical Stein discrepancy is the optimum S(Q, TP ,G\u2016\u00b7\u2016) = sup\ng\n\u2211n i=1 q(xi)(\u3008g(xi),\u2207 log p(xi)\u3009+ \u3008\u2207, g(xi)\u3009) (6)\ns.t. \u2016g(x)\u2016\u2217 \u2264 1, \u2016\u2207g(x)\u2016\u2217 \u2264 1, \u2016\u2207g(x)\u2212\u2207g(y)\u2016\u2217 \u2264 \u2016x\u2212 y\u2016,\u2200x, y \u2208 X . Note that the objective associated with any Stein discrepancy S(Q, TP ,G) is linear in g and, since Q is discrete, only depends on g and\u2207g through their values at each of the n sample points xi. The primary difficulty in solving the classical Stein program (6) stems from the infinitude of constraints imposed by the classical Stein set G\u2016\u00b7\u2016 . One way to avoid this difficulty is to impose the classical smoothness constraints at only a finite collection of points. To this end, for each finite graph G = (V,E) with vertices V \u2282 X and edges E \u2282 V 2, we define the graph Stein set,\nG\u2016\u00b7\u2016,Q,G , { g : X \u2192 Rd | \u2200x \u2208 V, max ( \u2016g(x)\u2016\u2217, \u2016\u2207g(x)\u2016\u2217 ) \u2264 1 and, \u2200 (x, y) \u2208 E, max ( \u2016g(x)\u2212g(y)\u2016\u2217 \u2016x\u2212y\u2016 , \u2016\u2207g(x)\u2212\u2207g(y)\u2016\u2217 \u2016x\u2212y\u2016 , \u2016g(x)\u2212g(y)\u2212\u2207g(x)(x\u2212y)\u2016\u2217 1 2\u2016x\u2212y\u2016 2 , \u2016g(x)\u2212g(y)\u2212\u2207g(y)(x\u2212y)\u2016\u2217 1 2\u2016x\u2212y\u2016 2 ) \u2264 1 } ,\nthe family of functions which satisfy the classical constraints and certain implied Taylor compatibility constraints at pairs of points in E. Remarkably, if the graph G1 consists of edges between all distinct sample points xi, then the associated complete graph Stein discrepancy S(Q, TP ,G\u2016\u00b7\u2016,Q,G1) is equivalent to the classical Stein discrepancy in the following strong sense.\nProposition 5 (Equivalence of Classical and Complete Graph Stein Discrepancies). If X = Rd, and G1 = (supp(Q), E1) with E1 = {(xi, xl) \u2208 supp(Q)2 : xi 6= xl}, then\nS(Q, TP ,G\u2016\u00b7\u2016) \u2264 S(Q, TP ,G\u2016\u00b7\u2016,Q,G1) \u2264 \u03bad S(Q, TP ,G\u2016\u00b7\u2016), where \u03bad is a constant, independent of (Q,P ), depending only on the dimension d and norm \u2016\u00b7\u2016.\nProposition 5 follows from the Whitney-Glaeser extension theorem for smooth functions [14, 15] and implies that the complete graph Stein discrepancy inherits all of the desirable convergence properties of the classical discrepancy. However, the complete graph also introduces order n2 constraints, rendering computation infeasible for large samples. To achieve the same form of equivalence while enforcing only O(n) constraints, we will make use of sparse geometric spanner subgraphs."}, {"heading": "4.2 Geometric Spanners", "text": "For a given dilation factor t \u2265 1, a t-spanner [16, 17] is a graph G = (V,E) with weight \u2016x\u2212 y\u2016 on each edge (x, y) \u2208 E and a path between each pair x\u2032 6= y\u2032 \u2208 V with total weight no larger than t\u2016x\u2032 \u2212 y\u2032\u2016. The next proposition shows that spanner Stein discrepancies enjoy the same convergence properties as the complete graph Stein discrepancy. Proposition 6 (Equivalence of Spanner and Complete Graph Stein Discrepancies). If X = Rd, Gt = (supp(Q), E) is a t-spanner, and G1 = (supp(Q), {(xi, xl) \u2208 supp(Q)2 : xi 6= xl}), then\nS(Q, TP ,G\u2016\u00b7\u2016,Q,G1) \u2264 S(Q, TP ,G\u2016\u00b7\u2016,Q,Gt) \u2264 2t 2 S(Q, TP ,G\u2016\u00b7\u2016,Q,G1).\nMoreover, for any `p norm, a 2-spanner with O(\u03badn) edges can be computed in O(\u03badn log(n)) expected time for \u03bad a constant depending only on d and \u2016\u00b7\u2016 [18]. As a result, we will adopt a 2-spanner Stein discrepancy, S(Q, TP ,G\u2016\u00b7\u2016,Q,G2), as our standard quality measure."}, {"heading": "4.3 Decoupled Linear Programs", "text": "The final unspecified component of our Stein discrepancy is the choice of norm \u2016\u00b7\u2016. We recommend the `1 norm, as the resulting optimization problem decouples into d independent finite-dimensional linear programs (LPs) that can be solved in parallel. More precisely, S(Q, TP ,G\u2016\u00b7\u20161,Q,(V,E)) equals\u2211d\nj=1 sup \u03b3j\u2208R|V |,\u0393j\u2208Rd\u00d7|V |\n\u2211|V | i=1 q(vi)(\u03b3ji\u2207j log p(vi) + \u0393jji) (7)\ns.t. \u2016\u03b3j\u2016\u221e \u2264 1, \u2016\u0393j\u2016\u221e \u2264 1, and \u2200 i 6= l : (vi, vl) \u2208 E, max ( |\u03b3ji\u2212\u03b3jl| \u2016vi\u2212vl\u20161 , \u2016\u0393j(ei\u2212el)\u2016\u221e \u2016vi\u2212vl\u20161 , |\u03b3ji\u2212\u03b3jl\u2212\u3008\u0393jei,vi\u2212vl\u3009| 1 2\u2016vi\u2212vl\u2016 2 1 , |\u03b3ji\u2212\u03b3jl\u2212\u3008\u0393jel,vi\u2212vl\u3009| 1 2\u2016vi\u2212vl\u2016 2 1 ) \u2264 1.\nWe have arbitrarily numbered the elements vi of the vertex set V so that \u03b3ji represents the function value gj(vi), and \u0393jki represents the gradient value\u2207kgj(vi)."}, {"heading": "4.4 Constrained Domains", "text": "A small modification to the unconstrained formulation (7) extends our tractable Stein discrepancy computation to any domain defined by coordinate boundary constraints, that is, to X = (\u03b11, \u03b21) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (\u03b1d, \u03b2d) with \u2212\u221e \u2264 \u03b1j < \u03b2j \u2264 \u221e for all j. Specifically, for each dimension j, we augment the j-th coordinate linear program of (7) with the boundary compatibility constraints\nmax ( |\u03b3ji| |vij\u2212bj | , |\u0393jki| |vij\u2212bj | , |\u03b3ji\u2212\u0393jji(vij\u2212bj)| 1 2 (vij\u2212bj)2 ) \u2264 1, for each i, bj \u2208 {\u03b1j , \u03b2j} \u2229 R, and k 6= j. (8) These additional constraints ensure that our candidate function and gradient values can be extended to a smooth function satisfying the boundary conditions \u3008g(z), n(z)\u3009 = 0 on \u2202X . Proposition 10 in the appendix shows that the spanner Stein discrepancy so computed is strongly equivalent to the classical Stein discrepancy on X . Algorithm 1 summarizes the complete solution for computing our recommended, parameter-free spanner Stein discrepancy in the multivariate setting. Notably, the spanner step is unnecessary in the univariate setting, as the complete graph Stein discrepancy S(Q, TP ,G\u2016\u00b7\u20161,Q,G1) can be computed directly by sorting the sample and boundary points and only enforcing constraints between consecutive points in this ordering. Thus, the complete graph Stein discrepancy is our recommended quality measure when d = 1, and a recipe for its computation is given in Algorithm 2.\nAlgorithm 1 Multivariate Spanner Stein Discrepancy input: Q, coordinate bounds (\u03b11, \u03b21), . . . , (\u03b1d, \u03b2d) with \u2212\u221e \u2264 \u03b1j < \u03b2j \u2264 \u221e for all j G2 \u2190 Compute sparse 2-spanner of supp(Q) for j = 1 to d do (in parallel)\nrj \u2190 Solve j-th coordinate linear program (7) with graph G2 and boundary constraints (8) return \u2211d j=1 rj\nAlgorithm 2 Univariate Complete Graph Stein Discrepancy input: Q, bounds (\u03b1, \u03b2) with \u2212\u221e \u2264 \u03b1 < \u03b2 \u2264 \u221e (x(1), . . . , x(n\u2032))\u2190 SORT({x1, . . . , xn, \u03b1, \u03b2} \u2229 R) return sup\u03b3\u2208Rn\u2032 ,\u0393\u2208Rn\u2032 \u2211n\u2032 i=1 q(x(i))(\u03b3i d dx log p(x(i)) + \u0393i)\ns.t. \u2016\u0393\u2016\u221e \u2264 1,\u2200i \u2264 n \u2032, |\u03b3i| \u2264 I [ \u03b1 < x(i) < \u03b2 ] , and, \u2200i < n\u2032,\nmax ( |\u03b3i\u2212\u03b3i+1| x(i+1)\u2212x(i) , |\u0393i\u2212\u0393i+1|x(i+1)\u2212x(i) , |\u03b3i\u2212\u03b3i+1\u2212\u0393i(x(i)\u2212x(i+1))|\n1 2 (x(i+1)\u2212x(i))2\n, |\u03b3i\u2212\u03b3i+1\u2212\u0393i+1(x(i)\u2212x(i+1))|\n1 2 (x(i+1)\u2212x(i))2\n) \u2264 1"}, {"heading": "5 Experiments", "text": "We now turn to an empirical evaluation of our proposed quality measures. We compute all spanners using the efficient C++ greedy spanner implementation of Bouts et al. [19] and solve all optimization programs using Julia for Mathematical Programming [20] with the default Gurobi 6.0.4 solver [21]. All reported timings are obtained using a single core of an Intel Xeon CPU E5-2650 v2 @ 2.60GHz."}, {"heading": "5.1 A Simple Example", "text": "We begin with a simple example to illuminate a few properties of the Stein diagnostic. For the target P = N (0, 1), we generate a sequence of sample points i.i.d. from the target and a second sequence i.i.d. from a scaled Student\u2019s t distribution with matching variance and 10 degrees of freedom. The left panel of Figure 1 shows that the complete graph Stein discrepancy applied to the first nGaussian sample points decays to zero at an n\u22120.52 rate, while the discrepancy applied to the scaled Student\u2019s t sample remains bounded away from zero. The middle panel displays optimal Stein functions g recovered by the Stein program for different sample sizes. Each g yields a test function h , TP g, featured in the right panel, that best discriminates the sample Q from the target P . Notably, the Student\u2019s t test functions exhibit relatively large magnitude values in the tails of the support."}, {"heading": "5.2 Comparing Discrepancies", "text": "We show in Theorem 9 in the appendix that, when d = 1, the classical Stein discrepancy is the optimum of a convex quadratically constrained quadratic program with a linear objective, O(n) variables, and O(n) constraints. This offers the opportunity to directly compare the behavior of the graph and classical Stein discrepancies. We will also compare to the Wasserstein distance dW\u2016\u00b7\u2016 ,\nwhich is computable for simple univariate target distributions [22] and provably lower bounds the non-uniform Stein discrepancies (5) with c1:3 = (0.5, 0.5, 1) for P = Unif(0, 1) and c1:3 = (1, 4, 2) for P = N (0, 1) [? 23]. For N (0, 1) and Unif(0, 1) targets and several random number generator seeds, we generate a sequence of sample points i.i.d. from the target distribution and plot the nonuniform classical and complete graph Stein discrepancies and the Wasserstein distance as functions of the first n sample points in Figure 2. Two apparent trends are that the graph Stein discrepancy very closely approximates the classical and that both Stein discrepancies track the fluctuations in Wasserstein distance even when a magnitude separation exists. In the Unif(0, 1) case, the Wasserstein distance in fact equals the classical Stein discrepancy because TP g = g\u2032 is a Lipschitz function."}, {"heading": "5.3 Selecting Sampler Hyperparameters", "text": "Stochastic Gradient Langevin Dynamics (SGLD) [3] with constant step size is a biased MCMC procedure designed for scalable inference. It approximates the overdamped Langevin diffusion, but, because no Metropolis-Hastings (MH) correction is used, the stationary distribution of SGLD deviates increasingly from its target as grows. If is too small, however, SGLD explores the sample space too slowly. Hence, an appropriate choice of is critical for accurate posterior inference. To illustrate the value of the Stein diagnostic for this task, we adopt the bimodal Gaussian mixture model (GMM) posterior of [3] as our target. For a range of step sizes , we use SGLD with minibatch size 5 to draw 50 independent sequences of length n = 1000, and we select the value of with the highest median quality \u2013 either the maximum effective sample size (ESS, a standard diagnostic based on autocorrelation [1]) or the minimum spanner Stein discrepancy \u2013 across these sequences. The average discrepancy computation consumes 0.4s for spanner construction and 1.4s per coordinate linear program. As seen in Figure 3a, ESS, which does not detect distributional bias, selects the largest step size presented to it, while the Stein discrepancy prefers an intermediate value. The rightmost plot of Figure 3b shows that a representative SGLD sample of size n using the selected by ESS is greatly overdispersed; the leftmost is greatly underdispersed due to slow mixing. The middle sample, with selected by the Stein diagnostic, most closely resembles the true posterior."}, {"heading": "5.4 Quantifying a Bias-Variance Trade-off", "text": "The approximate random walk MH (ARWMH) sampler [5] is a second biased MCMC procedure designed for scalable posterior inference. Its tolerance parameter controls the number of datapoint likelihood evaluations used to approximate the standard MH correction step. Qualitatively, a larger implies fewer likelihood computations, more rapid sampling, and a more rapid reduction of variance. A smaller yields a closer approximation to the MH correction and less bias in the sampler stationary distribution. We will use the Stein discrepancy to explicitly quantify this bias-variance trade-off.\nWe analyze a dataset of 53 prostate cancer patients with six binary predictors and a binary outcome indicating whether cancer has spread to surrounding lymph nodes [24]. Our target is the Bayesian logistic regression posterior [1] under a N (0, I) prior on the parameters. We run RWMH ( = 0) and ARWMH ( = 0.1 and batch size = 2) for 105 likelihood evaluations, discard the points from the first 103 evaluations, and thin the remaining points to sequences of length 1000. The discrepancy computation time for 1000 points averages 1.3s for the spanner and 12s for a coordinate LP. Figure 4 displays the spanner Stein discrepancy applied to the first n points in each sequence as a function of the likelihood evaluation count. We see that the approximate sample is of higher Stein quality for smaller computational budgets but is eventually overtaken by the asymptotically exact sequence.\nTo corroborate our result, we use a Metropolis-adjusted Langevin chain [25] of length 107 as a surrogateQ\u2217 for the target and compute several error measures for each sampleQ: normalized probability error maxl |E[\u03c3(\u3008X,wl\u3009)\u2212 \u03c3(\u3008Z,wl\u3009)]|/\u2016wl\u2016\u221e, mean error maxj |E[Xj\u2212Zj ]| maxj |EQ\u2217 [Zj ]| , and second moment error maxj,k |E[XjXk\u2212ZjZk]|maxj,k |EQ\u2217 [ZjZk]| for X \u223c Q, Z \u223c Q \u2217, \u03c3(t) , 11+e\u2212t , and wl the l-th datapoint covariate vector. The measures, also found in Figure 4, accord with the Stein discrepancy quantification."}, {"heading": "5.5 Assessing Convergence Rates", "text": "The Stein discrepancy can also be used to assess the quality of deterministic sample sequences. In Figure 5 in the appendix, for P = Unif(0, 1), we plot the complete graph Stein discrepancies of the first n points of an i.i.d. Unif(0, 1) sample, a deterministic Sobol sequence [26], and a deterministic kernel herding sequence [27] defined by the norm \u2016h\u2016H = \u222b 1 0\n(h\u2032(x))2dx. We use the median value over 50 sequences in the i.i.d. case and estimate the convergence rate for each sampler using the slope of the best least squares affine fit to each log-log plot. The discrepancy computation time averages 0.08s for n = 200 points, and the recovered rates of n\u22120.49 and n\u22121 for the i.i.d. and Sobol sequences accord with expected O(1/ \u221a n) and O(1/n) bounds from the literature [28, 29]. As witnessed also in other metrics [30], the herding rate of n\u22120.96 outpaces its best known bound of dH(Qn, P ) = O(1/ \u221a n), suggesting an opportunity for sharper analysis."}, {"heading": "6 Discussion of Related Work", "text": "We have developed a quality measure suitable for comparing biased, exact, and deterministic sample sequences by exploiting an infinite class of known target functionals. The diagnostics of [31, 32] also account for asymptotic bias but lose discriminating power by considering only a finite collection of functionals. For example, for a N (0, 1) target, the score statistic of [32] cannot distinguish two samples with equal first and second moments. Maximum mean discrepancy (MMD) on a characteristic Hilbert space [33] takes full distributional bias into account but is only viable when the expected kernel evaluations are easily computed under the target. One can approximate MMD, but this requires access to a separate trustworthy ground-truth sample from the target."}, {"heading": "A Proof of Proposition 1", "text": "Our integrability assumption together with the boundedness of g and \u2207g imply that EP [\u3008\u2207, g(Z)\u3009] and EP [\u3008g(Z),\u2207 log p(Z)\u3009] exist. Define the `\u221e ball of radius r, Br = {x \u2208 Rd : \u2016x\u2016\u221e \u2264 r}. Since X is convex, the intersection X \u2229 Br is compact and convex with Lipschitz boundary \u2202(X \u2229 Br). Thus, the divergence theorem (integration by parts) implies that\nEP [(TP g)(Z)] = EP [\u3008\u2207, g(Z)\u3009+ \u3008g(Z),\u2207 log p(Z)\u3009] = \u222b X \u3008\u2207, p(z)g(z)\u3009 dz\n= lim r\u2192\u221e \u222b X\u2229Br \u3008\u2207, p(z)g(z)\u3009 dz = lim r\u2192\u221e \u222b \u2202(X\u2229Br) \u3008g(z), nr(z)\u3009p(z) dz\nfor nr the outward unit normal vector to \u2202(X \u2229Br). The final quantity in this expression equates to zero, as \u3008g(x), n(x)\u3009 = 0 for all x on the boundary \u2202X , g is bounded, and limm\u2192\u221e p(xm) = 0 for any (xm)\u221em=1 with xm \u2208 X for all m and \u2016xm\u2016\u221e \u2192\u221e."}, {"heading": "B Proof of Theorem 2: Stein Discrepancy Lower Bound for Strongly Log-concave Densities", "text": "We let Ck(X ) denote the set of real-valued functions on X with k continuous derivatives and dM\u2016\u00b7\u2016 denote the smooth function distance, the IPM generated by\nM\u2016\u00b7\u2016 , { h \u2208 C3(X ) \u2223\u2223\u2223 supx\u2208X max(\u2016\u2207h(x)\u2016\u2217,\u2225\u2225\u22072h(x)\u2225\u2225\u2217,\u2225\u2225\u22073h(x)\u2225\u2225\u2217) \u2264 1}. We additionally define the operator norms \u2016v\u2016op , \u2016v\u20162 for vectors v \u2208 Rd, \u2016M\u2016op , supv\u2208Rd:\u2016v\u20162=1 \u2016Mv\u20162 for matrices M \u2208 R\nd\u00d7d , and \u2016T\u2016op , supv\u2208Rd:\u2016v\u20162=1 \u2016T [v]\u2016op for tensors T \u2208 Rd\u00d7d\u00d7d. The following result, proved in the companion paper [34], establishes the existence of explicit constants (Stein factors) c1, c2, c3 > 0, such that, for any test function h \u2208M\u2016\u00b7\u2016, the Stein equation\nh(x)\u2212 EP [h(Z)] = (TP gh)(x) has a solution gh = 12\u2207uh belonging to the non-uniform Stein set G c1:3 \u2016\u00b7\u2016 .\nTheorem 7 (Stein Factors for Strongly Log-concave Densities [34, Theorem 2.1]). Suppose that X = Rd and that log p \u2208 C4(X ) is k-strongly concave with\nsup z\u2208X\n\u2225\u2225\u22073 log p(z)\u2225\u2225 op \u2264 L3 and sup\nz\u2208X\n\u2225\u2225\u22074 log p(z)\u2225\u2225 op \u2264 L4.\nFor each x \u2208 X , let (Zt,x)t\u22650 represent the overdamped Langevin diffusion with infinitestimal generator\n(Au)(x) = 1 2 \u3008\u2207u(x),\u2207 log p(x)\u3009+ 1 2 \u3008\u2207,\u2207u(x)\u3009 (9)\nand initial state Z0,x = x. Then, for each h \u2208 C3(X ) with bounded first, second, and third derivatives, the function uh(x) , \u222b \u221e\n0\nEP [h(Z)]\u2212 E[h(Zt,x)] dt\nsolves the the Stein equation\nh(x)\u2212 EP [h(Z)] = (Auh)(x) (10)\nand satisfies\nsup z\u2208X \u2016\u2207uh(z)\u20162 \u2264\n2 k sup z\u2208X \u2016\u2207h(z)\u20162,\nsup z\u2208X \u2225\u2225\u22072uh(z)\u2225\u2225op \u2264 2L3k2 supz\u2208X\u2016\u2207h(z)\u20162 + 1k supz\u2208X\u2225\u2225\u22072h(z)\u2225\u2225op, and sup\nz,y\u2208X ,z 6=y \u2225\u2225\u22072uh(z)\u2212\u22072uh(y)\u2225\u2225op \u2016z \u2212 y\u20162 \u2264 6L 2 3 k3 sup z\u2208X \u2016\u2207h(z)\u20162 + L4 k2 sup z\u2208X \u2016\u2207h(z)\u20162\n+ 3L3 k2 sup z\u2208X\n\u2225\u2225\u22072h(z)\u2225\u2225 op + 2\n3k sup z\u2208X\n\u2225\u2225\u22073h(z)\u2225\u2225 op .\nHence, by the equivalence of non-uniform Stein discrepancies (Proposition 4), dM\u2016\u00b7\u2016(\u00b5, P ) \u2264 S(\u00b5, TP ,Gc1:3\u2016\u00b7\u2016 ) \u2264 max(c1, c2, c3)S(\u00b5, TP ,G\u2016\u00b7\u2016) for any probability measure \u00b5.\nThe desired result now follows from Lemma 8, which implies that the Wasserstein distance dW\u2016\u00b7\u2016(\u00b5m, P )\u2192 0 whenever dM\u2016\u00b7\u2016(\u00b5m, P )\u2192 0 for a sequence of probability measures (\u00b5m)m\u22651.\nLemma 8 (Smooth-Wasserstein Inequality). If \u00b5 and \u03bd are probability measures on Rd, and \u2016v\u2016 \u2265 \u2016v\u20162 for all v \u2208 Rd, then\ndM\u2016\u00b7\u2016(\u00b5, \u03bd) \u2264 dW\u2016\u00b7\u2016(\u00b5, \u03bd) \u2264 3 max ( dM\u2016\u00b7\u2016(\u00b5, \u03bd), 3 \u221a dM\u2016\u00b7\u2016(\u00b5, \u03bd) \u221a 2E[\u2016G\u2016]2 ) .\nfor G a standard normal random vector in Rd.\nLemma 2.2 of the companion paper [34] establishes this result for the case \u2016\u00b7\u2016 = \u2016\u00b7\u20162; we omit the proof of the generalization which closely mirrors that of the Euclidean norm case."}, {"heading": "C Proof of Proposition 3: Stein Discrepancy Upper Bound", "text": "Fix any g in G\u2016\u00b7\u2016 . By Proposition 1, E[(TP g)(Z)] = 0. The Lipschitz and boundedness constraints on g and \u2207g now yield\nEQ[(TP g)(X)] = E[(TP g)(X)\u2212 (TP g)(Z)] = E[\u3008g(X),\u2207 log p(X)\u3009 \u2212 \u3008g(Z),\u2207 log p(Z)\u3009+ \u3008\u2207, g(X)\u2212 g(Z)\u3009] = E[\u3008g(X),\u2207 log p(X)\u2212\u2207 log p(Z)\u3009+ \u3008g(X)\u2212 g(Z),\u2207 log p(Z)\u3009] + E[\u3008\u2207, g(X)\u2212 g(Z)\u3009] \u2264 E[\u2016\u2207 log p(X)\u2212\u2207 log p(Z)\u2016] + E\n[\u2225\u2225\u2207 log p(Z)(X \u2212 Z)>\u2225\u2225]+ \u2016I\u2016E[\u2016X \u2212 Z\u2016]. To derive the second advertised inequality, we use the definition of the matrix norm, the FenchelYoung inequality for dual norms, the definition of the matrix dual norm, and the Cauchy-Schwarz inequality in turn:\nE [\u2225\u2225\u2207 log p(Z)(X \u2212 Z)>\u2225\u2225] = E[ sup\nM :\u2016M\u2016\u2217=1 \u3008\u2207 log p(Z),M(X \u2212 Z)\u3009\n]\n\u2264 E [ sup\nM :\u2016M\u2016\u2217=1 \u2016\u2207 log p(Z)\u2016\u2016M(X \u2212 Z)\u2016\u2217\n]\n\u2264 E[\u2016\u2207 log p(Z)\u2016\u2016X \u2212 Z\u2016] \u2264 \u221a E [ \u2016\u2207 log p(Z)\u20162 ] E [ \u2016X \u2212 Z\u20162 ] .\nSince our bounds hold uniformly for all g in G\u2016\u00b7\u2016 , the proof is complete."}, {"heading": "D Proof of Proposition 4: Equivalence of Non-uniform Stein Discrepancies", "text": "Fix any c1, c2, c3 > 0, and let cmax = max(c1, c2, c3) and cmin = min(c1, c2, c3). Since the Stein discrepancy objective is linear in g, we have aS(Q, TP ,G\u2016\u00b7\u2016) = S(Q, TP , aG\u2016\u00b7\u2016) for any a > 0. The result now follows from the observation that cminG\u2016\u00b7\u2016 \u2286 Gc1:3\u2016\u00b7\u2016 \u2286 cmaxG\u2016\u00b7\u2016 ."}, {"heading": "E Proof of Proposition 5: Equivalence of Classical and Complete Graph Stein Discrepancies", "text": "The first inequality follows from the fact that G\u2016\u00b7\u2016 \u2286 G\u2016\u00b7\u2016,Q,G1 . By the Whitney-Glaeser extension theorem [15, Thm. 1.4] of Glaeser [14], for every function g \u2208 G\u2016\u00b7\u2016,Q,G1 , there exists a function g\u0303 \u2208 \u03bad G\u2217\u2016\u00b7\u2016 with g(xi) = g\u0303(xi) and \u2207g(xi) = \u2207g\u0303(xi) for all xi in the support of Q. Here \u03bad is a constant, independent of (Q,P ), depending only on the dimension d and norm \u2016\u00b7\u2016. Since the Stein discrepancy objective is linear in g and depends on g only through the values g(xi) and\u2207g(xi), we have S(Q, TP ,G\u2016\u00b7\u2016,Q,G1) \u2264 S(Q, TP , \u03badG\u2016\u00b7\u2016) = \u03bad S(Q, TP ,G\u2016\u00b7\u2016)."}, {"heading": "F Proof of Proposition 6: Equivalence of Spanner and Complete Graph Stein Discrepancies", "text": "The first inequality follows from the fact that G\u2016\u00b7\u2016,Q,G1 \u2286 G\u2016\u00b7\u2016,Q,Gt . Fix any g \u2208 G\u2016\u00b7\u2016,Q,Gt and any pair of points z, z\u2032 \u2208 supp(Q). By the definition of G\u2016\u00b7\u2016,Q,Gt , we have max ( \u2016g(z)\u2016\u2217, \u2016\u2207g(z)\u2016\u2217 ) \u2264 1. By the t-spanner property, there exists a sequence of points z0, z1, z2, . . . , zL\u22121, zL \u2208 supp(Q) with z0 = z and zL = z\u2032 for which (zl\u22121, zl) \u2208 E for all 1 \u2264 l \u2264 L and \u2211L l=1\u2016zl\u22121 \u2212 zl\u2016 \u2264\nt\u2016z0 \u2212 zL\u2016. Since max ( \u2016g(zl\u22121)\u2212g(zl)\u2016\u2217 \u2016zl\u22121\u2212zl\u2016 , \u2016\u2207g(zl\u22121)\u2212\u2207g(zl)\u2016\u2217 \u2016zl\u22121\u2212zl\u2016 ) \u2264 1 for each l, the triangle inequality implies that\n\u2016\u2207g(z0)\u2212\u2207g(zL)\u2016\u2217 \u2264 L\u2211 l=1 \u2016\u2207g(zl\u22121)\u2212\u2207g(zl)\u2016\u2217 \u2264 L\u2211 l=1 \u2016zl\u22121 \u2212 zl\u2016 \u2264 t\u2016z0 \u2212 zL\u2016.\nIdentical reasoning establishes that \u2016g(z0)\u2212 g(zL)\u2016\u2217 \u2264 t\u2016z0 \u2212 zL\u2016.\nFurthermore, since \u2016g(zl\u22121)\u2212 g(zl)\u2212\u2207g(zl)(zl\u22121 \u2212 zl)\u2016\u2217 \u2264 12\u2016zl\u22121 \u2212 zl\u2016 2 for each l, the triangle inequality and the definition of the tensor norm \u2016\u00b7\u2016\u2217 imply that \u2016g(z0)\u2212 g(zL)\u2212\u2207g(zL)(z0 \u2212 zL)\u2016\u2217\n\u2264 L\u2211 l=1 \u2016g(zl\u22121)\u2212 g(zl)\u2212\u2207g(zl)(zl\u22121 \u2212 zl)\u2016\u2217 + \u2016(\u2207g(zl)\u2212\u2207g(zL))(zl\u22121 \u2212 zl)\u2016\u2217\n\u2264 L\u2211 l=1 1 2 \u2016zl\u22121 \u2212 zl\u20162 + \u2016\u2207g(zl)\u2212\u2207g(zL)\u2016\u2217\u2016zl\u22121 \u2212 zl\u2016\n\u2264 L\u2211 l=1 1 2 \u2016zl\u22121 \u2212 zl\u20162 + L\u22121\u2211 l\u2032=l \u2016\u2207g(zl\u2032)\u2212\u2207g(zl\u2032+1)\u2016\u2217\u2016zl\u22121 \u2212 zl\u2016 \u2264 L\u2211 l=1 \u2016zl\u22121 \u2212 zl\u2016 ( 1 2 \u2016zl\u22121 \u2212 zl\u2016+ L\u22121\u2211 l\u2032=l \u2016zl\u2032 \u2212 zl\u2032+1\u2016 ) \u2264 ( L\u2211 l=1 \u2016zl\u22121 \u2212 zl\u2016 )2 \u2264 t2\u2016z0 \u2212 zL\u20162.\nSince z, z\u2032 were arbitrary, and the Stein discrepancy objective is linear in g, we conclude that S(Q, TP ,G\u2016\u00b7\u2016,Q,Gt) \u2264 S(Q, TP , 2t2G\u2016\u00b7\u2016,Q,G1) = 2t2 S(Q, TP ,G\u2016\u00b7\u2016,Q,G1)."}, {"heading": "G Finite-dimensional Classical Stein Program", "text": "Theorem 9 (Finite-dimensional Classical Stein Program). If X = (\u03b1, \u03b2) for \u2212\u221e \u2264 \u03b1 < \u03b2 \u2264 \u221e, and x(1) < \u00b7 \u00b7 \u00b7 < x(n\u2032) represent the sorted values of {x1, . . . , xn, \u03b1, \u03b2} \u2229R, then the non-uniform\nclassical Stein discrepancy S(Q, TP ,Gc1:3\u2016\u00b7\u2016 ) is the optimal value of the convex program\nmax g\n\u2211n\u2032 i=1q(x(i)) d dx log p(x(i))g(x(i)) + q(x(i))g \u2032(x(i)) (11a)\ns.t. \u2200i \u2208 {1, . . . , n\u2032 \u2212 1}, |g\u2032(x(i))| \u2264 c2, |g(x(i+1))\u2212 g(x(i))| \u2264 c2(x(i+1) \u2212 x(i)), (11b)\ng(x(i))\u2212 g(x(i+1)) + 1\n4c3\n( g\u2032(x(i))\u2212 g\u2032(x(i+1)) )2 + x(i+1) \u2212 x(i)\n2\n( g\u2032(x(i)) + g \u2032(x(i+1)) )\n+ 1\nc3 (Lb)\n2 + \u2264 c3 4 (x(i+1) \u2212 x(i))2, (11c)\ng(x(i+1))\u2212 g(x(i)) + 1\n4c3\n( g\u2032(x(i))\u2212 g\u2032(x(i+1)) )2 \u2212 x(i+1) \u2212 x(i) 2 ( g\u2032(x(i)) + g \u2032(x(i+1)) )\n+ 1\nc3 (Lu)\n2 + \u2264 c3 4 (x(i+1) \u2212 x(i))2, and (11d)\n\u2200i \u2208 {1, . . . , n\u2032}, |g(x(i))| \u2264 I [ \u03b1 < x(i) < \u03b2 ] (c1 \u2212 1\n2c3 g\u2032(x(i)) 2) (11e)\nwhere (r)+ , max(r, 0),\nLb , c3 2 (x(i+1) \u2212 x(i))\u2212 1 2 ( g\u2032(x(i)) + g \u2032(x(i+1)) ) \u2212 c2, and\nLu , c3 2 (x(i+1) \u2212 x(i)) + 1 2 ( g\u2032(x(i)) + g \u2032(x(i+1)) ) \u2212 c2.\nWe say the program (11) is finite-dimensional, because it suffices to optimize over vectors \u03b3,\u0393 \u2208 Rn\u2032 representing the function values (\u03b3i = g(x(i))) and derivative values (\u0393i = g\u2032(x(i))) at each sample or boundary point x(i). Indeed, by introducing slack variables, this program is representable as a convex quadratically constrained quadratic program with O(n) constraints, O(n) variables, and a linear objective. Moreover, the pairwise constraints in this program are only enforced between neighboring points in the sequence of ordered locations x(i). Hence the resulting constraint matrix is sparse and banded, making the problem particularly amenable to efficient optimization.\nProof Throughout, we say that g\u0303 is an extension of g if g\u0303(x(i)) = g(x(i)) and g\u0303\u2032(x(i)) = g\u2032(x(i)) for each x(i) \u2208 supp(Q). Since the Stein objective only depends on g and g\u2032 through their values at sample points, g and any extension g\u0303 have identical objective values.\nWe will establish our result by showing that every g \u2208 Gc1:3\u2016\u00b7\u2016 is feasible for the program (11), so that S(Q, TP ,Gc1:3\u2016\u00b7\u2016 ) lower bounds the optimum of (11), and that every feasible g for (11) has an extension in g\u0303 \u2208 Gc1:3\u2016\u00b7\u2016 , so that S(Q, TP ,G c1:3 \u2016\u00b7\u2016 ) also upper bounds the optimum of (11).\nG.1 Feasibility of Gc1:3\u2016\u00b7\u2016\nFix any g \u2208 Gc1:3\u2016\u00b7\u2016 . Also, since g \u2032 is c2-bounded and c3-Lipschitz, the constraints (11b) must be satisfied. Consider now the c2-bounded and c3-Lipschitz extensions of g\u2032\nB(t) , max(\u2212c2, max 1\u2264i\u2264n\u2032\n[ g\u2032(x(i))\u2212 c3|t\u2212 x(i)| ] ) and\nU(t) , min(c2, min 1\u2264i\u2264n\u2032\n[ g\u2032(x(i)) + c3|t\u2212 x(i)| ] ).\nWe know that B(t) \u2264 g\u2032(t) \u2264 U(t) for all t, for, if not, there would be a point t0 and a point x(i) such that |g\u2032(x(i))\u2212 g\u2032(t0)| > c3|x(i) \u2212 t0|, which combined with the c3-Lipschitz property would be a contradiction. Thus, for each sample x(i), the fundamental theorem of calculus gives\ng(x(i+1))\u2212 g(x(i)) = \u222b x(i+1) x(i) g\u2032(t) dt \u2265 \u222b x(i+1) x(i) B(t) dt.\nThe right-hand side of this inequality evaluates precisely to the right-hand side of the constraint (11c). An analogous upper bound using U(t) yields (11d).\nFinally, consider any point x(i). If x(i) \u2208 {\u03b1, \u03b2}, then (11e) is satisfied as g(z) = 0 for any point z on the boundary. Suppose instead that \u03b1 < x(i) < \u03b2. Without loss of generality, we may assume\nthat g\u2032(x(i)) \u2265 0. Since g\u2032 is c3-Lipschitz, we have g\u2032(t) \u2265 g\u2032(x(i))\u2212c3|t\u2212x(i)| for all t. Integrating both sides of this inequality from x(i) to xu = x(i) + g\u2032(x(i))/c3, we obtain\ng(xu)\u2212 g(x(i)) = \u222b xu x(i) g\u2032(t) dt \u2265 \u222b xu x(i) g\u2032(x(i))\u2212 c3(t\u2212 x(i)) dt = g\u2032(x(i))2/(2c3)\nSince g(xu) \u2264 c1, we have 12c3 g \u2032(x(i)) 2 + g(x(i)) \u2264 c1. Similarly, by integrating the inequality from xb = x(i) \u2212 g\u2032(x(i))/c3 to x(i), we have g(xb)\u2212 g(x(i)) \u2265 g\u2032(x(i))2/(2c3), which combined with g(xb) \u2264 c1 yields (11e).\nG.2 Extending Feasible Solutions\nSuppose now that g is any function feasible for the program (11). We will construct an extension g\u0303 \u2208 Gc1:3\u2016\u00b7\u2016 by first working independently over each interval (x(i), x(i+1)). Fix an index i < n\n\u2032. Our strategy is to identify a pair of c2-bounded, c3-Lipschitz functionsmi andMi defined on the interval [x(i), x(i+1)] which satisfy mi(x) \u2264 Mi(x) for all x \u2208 [x(i), x(i+1)], mi(x) = Mi(x) = g\u2032(x) for x \u2208 {x(i), x(i+1)}, and \u222b x(i+1) x(i) mi(t)dt \u2264 g(x(i+1)) \u2212 g(x(i)) \u2264 \u222b x(i+1) x(i)\nMi(t)dt. For any such (mi,Mi) pair, there exists \u03b6i \u2208 [0, 1] satisfying\ng(x(i+1))\u2212 g(x(i)) = \u222b x(i+1) x(i) \u03b6imi(t) + (1\u2212 \u03b6i)Mi(t)dt,\nand hence we will define the extension\ng\u0303(x) = g(x(i)) + \u222b x x(i) \u03b6imi(t) + (1\u2212 \u03b6i)Mi(t)dt.\nBy convexity, the extension derivative g\u0303\u2032 is c2-bounded and c3-Lipschitz, so we will only need to check that supx\u2208X |g\u0303(x)| \u2264 c1. The maximum magnitude values of g\u0303 occur either at the interval endpoints, which are c1-bounded by (11e), or at critical points x satisfying g\u0303\u2032(x) = 0, so it suffices to ensure that g\u0303 is c1-bounded at all critical points.\nWe will use the c2-bounded, c3-Lipschitz functions B and U as building blocks for our extension, since they satisfy B(t) = U(t) = g\u2032(t) for t \u2208 {x(i), x(i+1)} and B(t) \u2264 g\u2032(t) \u2264 U(t),\nB(t) = max(\u2212c2, g\u2032(x(i))\u2212 c3(t\u2212 x(i)), g\u2032(x(i+1))\u2212 c3(x(i+1) \u2212 t)), and U(t) = min(c2, g\n\u2032(x(i)) + c3(t\u2212 x(i)), g\u2032(x(i+1)) + c3(x(i+1) \u2212 t)), for t \u2208 [x(i), x(i+1)]. We need only consider three cases.\nCase 1: B and U are never negative or never positive on [x(i), x(i+1)]. For this case, we will choosemi = B andMi = U . By (11c) and (11d) we know \u222b x(i+1) x(i)\nmi(t)dt \u2264 g(x(i+1))\u2212g(x(i)) \u2264\u222b x(i+1) x(i)\nMi(t)dt. Since B and U never change signs, g\u0303 will be monotonic and hence c1-bounded for any choice of \u03b6i.\nCase 2: Exactly one of B and U changes sign on [x(i), x(i+1)]. Without loss of generality, we may assume that g\u2032(x(i)), g\u2032(x(i+1)) \u2265 0 and that B changes sign. Consider the quantity \u03c6 ,\u222b x(i+1) x(i) max{B(t), 0}dt. If g(x(i+1))\u2212 g(x(i)) \u2264 \u03c6, we let mi = B and Mi = max{B, 0}.\nSince, on the interval [x(i), x(i+1)],B is piecewise linear with at most two pieces that can take on the value 0, B has at most two roots within this interval. However, since B(x) is continuous, negative for some value of x, and nonnegative at x \u2208 {x(i), x(i+1)}, we know B has at least two roots. Thus let r1 < r2 be the roots of B(x). For any choice of \u03b6i, the convex combination \u03b6imi + (1\u2212 \u03b6i)Mi will be exactly B outside (r1, r2). Moreover, if \u03b6i 6= 0, then this combination will be less than 0 on (r1, r2), and if \u03b6i = 0, the combination will be 0 on the whole interval. Hence it suffices to only check the critical points r1 and r2. By (11e), mi(r) = Mi(r) = B(r) \u2208 [\u2212c1, c1] for r \u2208 {r1, r2}, and so g\u0303 will be c1-bounded.\nIf instead g(x(i+1))\u2212g(x(i)) > \u03c6, we can recycle the argument from Case 1 withmi = max{B, 0} and Mi = U and conclude that g\u0303 is c1-bounded.\nCase 3: Both B and U change sign on [x(i), x(i+1)]. Without loss of generality, we may assume that g\u2032(x(i)) \u2265 0, g\u2032(x(i+1)) < 0. SinceB continuously interpolates between g\u2032(x(i)) and g\u2032(x(i+1)) on [x(i), x(i+1)], it must have a root r. Let wi \u2208 [x(i), x(i+1)] be the point where B changes from one linear portion to another. Then because B is monotonic on each linear portion, the fact that B(wi) \u2264 B(x(i+1)) < 0 means that B cannot have a root between [wi, x(i+1)] and hence has at most one root on [x(i), x(i+1)]. Hence r is the unique root of B.\nIn a similar fashion, let us define s as the root of U , and since B(x) \u2264 U(x) for all x, we have s \u2265 r. Define\nW (x) ,  B(x) x \u2208 [x(i), r) 0 x \u2208 [r, s] U(x) t \u2208 (s, y],\nand \u03c8 , \u222b x(i+1) x(i)\nW (t)dt. As in Case 2, we will consider two subcases. If g(x(i+1))\u2212 g(x(i)) \u2264 \u03c8, we will let mi = B and Mi = W . By (11e), mi(r) = Mi(r) = B(r) \u2208 [\u2212c1, c1], and since this is the only critical point, g\u0303 will be c1-bounded.\nFor the other case, in which g(x(i+1)) \u2212 g(x(i)) > \u03c8, we choose mi = W and Mi = U . Then (11e) imply that mi(s) = Mi(s) = U(s) \u2208 [\u2212c1, c1], and, since this is the only critical point, the extension is well-defined on (x(i), x(i+1)).\nDefining g\u0303 outside of the interval [x1, xn\u2032 ] It only remains to define our extension g\u0303 outside of the interval [x1, xn\u2032 ] when either \u03b1 or \u03b2 is infinite. Suppose \u03b1 = \u2212\u221e. We extend g\u0303 to each x \u2208 (\u2212\u221e, x1) using the construction\ng\u0303(x) , \u222b x \u2212\u221e I[t \u2208 (x1 \u2212 |g\u2032(x1)|/c3, x1)](g\u2032(x1)\u2212 c3 sign(g\u2032(x1))t) dt.\nThis extension ensures that g\u0303\u2032 is c2-bounded and c3-Lipschitz. Moreover, the constraint (11e) guarantees that |g\u0303(x)| \u2264 c1. Analogous reasoning establishes an extension to (xn\u2032 ,\u221e)."}, {"heading": "H Equivalence of Constrained Classical and Spanner Stein Discrepancies", "text": "For P with support X = (\u03b11, \u03b21)\u00d7\u00b7 \u00b7 \u00b7\u00d7 (\u03b1d, \u03b2d) for\u2212\u221e \u2264 \u03b1j < \u03b2j \u2264 \u221e, Algorithm 1 computes a Stein discrepancy based on the graph Stein set\nG\u2016\u00b7\u20161,Q,(V,E) , { g : X \u2192 Rd | \u2200x \u2208 V, j, k \u2208 {1, . . . , d} with k 6= j, and bj \u2208 {\u03b1j , \u03b2j} \u2229 R,\nmax ( \u2016g(x)\u2016\u221e, \u2016\u2207g(x)\u2016\u221e, |gj(x)| |xj\u2212bj | , |\u2207kgj(x)| |xj\u2212bj | , |gj(x)\u2212\u2207jgj(x)(xj\u2212bj)| 1 2 (xj\u2212bj)2 ) \u2264 1, and, \u2200 (x, y) \u2208 E,\nmax ( \u2016g(x)\u2212g(y)\u2016\u221e \u2016x\u2212y\u20161 , \u2016\u2207g(x)\u2212\u2207g(y)\u2016\u221e \u2016x\u2212y\u20161 , \u2016g(x)\u2212g(y)\u2212\u2207g(x)(x\u2212y)\u2016\u221e 1 2\u2016x\u2212y\u2016 2 1 , \u2016g(x)\u2212g(y)\u2212\u2207g(y)(x\u2212y)\u2016\u221e 1 2\u2016x\u2212y\u2016 2 1 ) \u2264 1 } ,\nOur next result shows that the graph Stein discrepancy based on a t-spanner is strongly equivalent to the classical Stein discrepancy. Proposition 10 (Equivalence of Constrained Classical and Spanner Stein Discrepancies). If X = (\u03b11, \u03b21)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (\u03b1d, \u03b2d), and Gt = (supp(Q), E) is a t-spanner, then\nS(Q, TP ,G\u2016\u00b7\u20161) \u2264 S(Q, TP ,G\u2016\u00b7\u20161,Q,Gt) \u2264 t 2\u03bad S(Q, TP ,G\u2016\u00b7\u20161),\nwhere \u03bad is a constant, independent of (Q,P,Gt, t), depending only on the dimension d.\nProof\nEstablishing the first inequality Fix any g \u2208 G\u2016\u00b7\u20161 , z \u2208 supp(Q), and j, k \u2208 {1, . . . , d} with k 6= j, and consider any j-th coordinate boundary projection point\nb \u2208 {z + ej(\u03b1j \u2212 zj), z + ej(\u03b2j \u2212 zj)} \u2229 Rd.\nSince b \u2208 \u2202X , we must have \u3008g(b), n(b)\u3009 = \u3008g(b), ej\u3009 = gj(b) = 0. Moreover, for each dimension k 6= j, we have \u2207kgj(x) = 0, since otherwise, \u3008g(b+ \u03b4ek), n(b+ \u03b4ek)\u3009 = gj(b + \u03b4ek) 6= 0 for some \u03b4 \u2208 R and b+ \u03b4ek \u2208 \u2202X by the continuity of\u2207gj . The smoothness constraints of the classical Stein set G\u2016\u00b7\u20161 now imply that\n|gj(z)| = |gj(z)\u2212 gj(b)| \u2264 |zj \u2212 bj |, |\u2207kgj(x)| = |\u2207kgj(z)\u2212\u2207kgj(b)| \u2264 |zj \u2212 bj |,\nand\n|gj(z)\u2212\u2207jgj(x)(zj \u2212 bj)| = |gj(b)\u2212 gj(z)\u2212 \u3008\u2207gj(z), b\u2212 z\u3009| \u2264 1\n2 (zj \u2212 bj)2\nso that all graph Stein set boundary compatibility constraints are satisfied. Hence, we have the containment G\u2016\u00b7\u20161 \u2286 G\u2016\u00b7\u20161,Q,Gt , which implies the first advertised inequality.\nEstablishing the second inequality To establish the second inequality, it suffices to show that for any g\u0303 \u2208 G\u2016\u00b7\u20161,Q,Gt , each j \u2208 {1, . . . , d}, and \u03b6 , t, there exists a function gj satisfying\ngj(z) = g\u0303j(z), \u2207gj(z) = \u2207g\u0303j(z), gj(b) = 0, \u2207kgj(b) = 0, \u2200k 6= j, (12) |gj(b)\u2212 gj(z)| \u2264 \u2016b\u2212 z\u20161, (13) \u2016\u2207gj(b)\u2212\u2207gj(z)\u2016\u221e \u2264 \u03b6\u2016b\u2212 z\u20161, \u2016\u2207gj(b)\u2212\u2207gj(b \u2032)\u2016\u221e \u2264 \u03b6\u2016b\u2212 b \u2032\u20161, (14)\n|gj(b)\u2212 gj(z)\u2212 \u3008\u2207gj(z), b\u2212 z\u3009| \u2264 \u03b6\n2 \u2016b\u2212 z\u201621, (15)\n|gj(z)\u2212 gj(b)\u2212 \u3008\u2207gj(b), z \u2212 b\u3009| \u2264 3\u03b6\n2 \u2016b\u2212 z\u201621, and (16)\n|gj(b)\u2212 gj(b\u2032)\u2212 \u3008\u2207gj(b\u2032), b\u2212 b\u2032\u3009| \u2264 \u03b6\n2 \u2016b\u2212 b\u2032\u201621 (17)\nfor all z \u2208 supp(Q) and all b, b\u2032 in the j-th coordinate boundary set\nBj , {b \u2208 Rd : b = z + ej(\u03b1j \u2212 zj) or b = z + ej(\u03b2j \u2212 zj) for some z \u2208 X}. Indeed, since such gj will satisfy max ( |gj(z)|, \u2016\u2207gj(z)\u2016\u221e ) \u2264 1 for all z \u2208 supp(Q) \u222aBj and\nmax ( |gj(x)\u2212gj(y)| \u2016x\u2212y\u20161 , \u2016\u2207gj(x)\u2212\u2207gj(y)\u2016\u221e \u2016x\u2212y\u20161 , |gj(x)\u2212gj(y)\u2212\u2207gj(x)(x\u2212y)| 1 2\u2016x\u2212y\u2016 2 1 , |gj(x)\u2212gj(y)\u2212\u2207gj(y)(x\u2212y)| 1 2\u2016x\u2212y\u2016 2 1 ) \u2264 2t2\nfor all x, y \u2208 supp(Q) by the argument of Appendix F, the Whitney-Glaeser extension theorem [15, Thm. 1.4] of Glaeser [14] will then imply that there exists g\u2217 \u2208 t2\u03bad G\u2016\u00b7\u20161 , for a constant \u03bad independent of g\u0303 depending only on d, with g\u2217(z) = g(z) and \u2207g\u2217(z) = \u2207g(z) for all z \u2208 supp(Q). Since g\u0303 and g\u2217 will have matching Stein discrepancy objective values, and each objective is linear in g, the second advertised inequality will then follow.\nFix g\u0303 \u2208 G\u2016\u00b7\u20161,Q,Gt and j \u2208 {1, . . . , d}. We will now construct a function gj satisfying the desired properties. Since gj and \u2207gj are determined on supp(Q), and gj and \u2207kgj are determined on Bj for k 6= j by the constraints (12), it remains to define\u2207jgj on Bj . We choose the extension\n\u2207jgj(b) , min z\u2208supp(Q) {\u2207jgj(z) + \u03b6\u2016z \u2212 b\u20161} for all b \u2208 Bj .\nFix any z \u2208 supp(Q) and b \u2208 Bj , and let b\u2217 = z + ej(bj \u2212 zj). The argument of Appendix F implies that \u2207jgj is \u03b6-Lipschitz on supp(Q), and hence it is also \u03b6-Lipschitz on supp(Q) \u222a Bj . Since\n|\u2207kgj(z)\u2212\u2207kgj(b)| = |\u2207kgj(z)| \u2264 |zj \u2212 bj | \u2264 \u2016z \u2212 b\u20161 for all k 6= j, we have (14). Moreover, the boundary compatibility constraints of G\u2016\u00b7\u20161,Q,Gt imply\n|gj(b)\u2212 gj(z)| = |gj(z)| \u2264 \u2016b\u2217 \u2212 z\u20161 \u2264 \u2016b\u2212 z\u20161,\nestablishing (13). We next invoke the triangle inequality, the boundary compatibility conditions of G\u2016\u00b7\u20161,Q,Gt , Ho\u0308lder\u2019s inequality, the Lipschitz derivative property (14), and the fact \u2016z \u2212 b\u20161 =\n\u2016b\u2217 \u2212 z\u20161 + \u2016b\u2217 \u2212 b\u20161 in turn to establish (15):\n|gj(b)\u2212 gj(z)\u2212 \u3008\u2207gj(z), b\u2212 z\u3009| = |gj(z)\u2212\u2207jgj(z)(zj \u2212 bj)\u2212 \u3008\u2207gj(z), b\u2217 \u2212 b\u3009| \u2264 |gj(z)\u2212\u2207jgj(z)(zj \u2212 bj)|+ |\u3008\u2207gj(b\u2217)\u2212\u2207gj(z), b\u2217 \u2212 b\u3009|\n\u2264 1 2 \u2016b\u2217 \u2212 z\u201621 + \u2016\u2207gj(b \u2217)\u2212\u2207gj(z)\u2016\u221e\u2016b \u2217 \u2212 b\u20161 \u2264 1 2 \u2016b\u2217 \u2212 z\u201621 + \u03b6\u2016b \u2217 \u2212 z\u20161\u2016b \u2217 \u2212 b\u20161\n\u2264 \u03b6 2 (\u2016b\u2217 \u2212 z\u20161 + \u2016b \u2217 \u2212 b\u20161) 2 = \u03b6 2 \u2016b\u2212 z\u201621.\nA parallel argument yields (17). Finally, we may deduce (16), as\n|gj(z)\u2212 gj(b)\u2212 \u3008\u2207gj(b), z \u2212 b\u3009| \u2264 |gj(z)\u2212\u2207jgj(z)(zj \u2212 bj)|+ |\u2207jgj(b)\u2212\u2207jgj(z)||zj \u2212 bj |\n\u2264 1 2 (zj \u2212 bj)2 + \u03b6\u2016b\u2212 z\u20161|zj \u2212 bj | \u2264 3\u03b6 2 \u2016b\u2212 z\u201621\nby the triangle inequality, the definition of G\u2016\u00b7\u20161,Q,Gt , and the Lipschitz property (14)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Madeleine Udell for her generous advice concerning optimization in Julia, Quirijn Bouts and Kevin Buchin for sharing their wise counsel and greedy spanner implementation, Francis Bach for sharing his pseudosampling code, Andreas Eberle for his triple coupling pointers, and Jessica Hwang for her feedback on various versions of this manuscript.\nThis work was supported by the Frederick E. Terman Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747."}], "references": [{"title": "Handbook of Markov chain Monte Carlo", "author": ["S. Brooks", "A. Gelman", "G. Jones", "X.-L. Meng"], "venue": "CRC press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C.J. Geyer"], "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface, pages 156\u2013163", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y. Teh"], "venue": "ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "Proc. 29th ICML, ICML\u201912", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget", "author": ["A. Korattikara", "Y. Chen", "M. Welling"], "venue": "Proc. of 31st ICML, ICML\u201914", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["A. M\u00fcller"], "venue": "Ann. Appl. Probab., 29 (2):pp. 429\u2013443", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "A bound for the error in the normal approximation to the distribution of a sum of dependent random variables", "author": ["C. Stein"], "venue": "Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1971}, {"title": "Stein\u2019s method and Poisson process convergence", "author": ["A.D. Barbour"], "venue": "J. Appl. Probab., (Special Vol. 25A): 175\u2013184", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Control functionals for Monte Carlo integration", "author": ["C. Oates", "M. Girolami", "N. Chopin"], "venue": "To appear in JRSS, Series B", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Nonnormal approximation by Stein\u2019s method of exchangeable pairs with application to the Curie-Weiss model", "author": ["S. Chatterjee", "Q. Shao"], "venue": "Ann. Appl. Probab., 21(2):464\u2013483", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Multivariate normal approximation with Stein\u2019s method of exchangeable pairs under a general linearity condition", "author": ["G. Reinert", "A. R\u00f6llin"], "venue": "Ann. Probab., 37(6):2150\u20132173", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multivariate normal approximation using exchangeable pairs", "author": ["S. Chatterjee", "E. Meckes"], "venue": "ALEA Lat. Am. J. Probab. Math. Stat., 4:257\u2013283", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "On Stein\u2019s method for multivariate normal approximation", "author": ["E. Meckes"], "venue": "High dimensional probability V: the Luminy volume, volume 5 of Inst. Math. Stat. Collect., pages 153\u2013178. Inst. Math. Statist., Beachwood, OH", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "\u00c9tude de quelques alg\u00e8bres tayloriennes", "author": ["G. Glaeser"], "venue": "J. Analyse Math., 6:1\u2013124; erratum, insert to 6 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1958}, {"title": "The Whitney extension problem and Lipschitz selections of set-valued mappings in jetspaces", "author": ["P. Shvartsman"], "venue": "Trans. Amer. Math. Soc., 360(10):5529\u20135550", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "There is a Planar Graph Almost As Good As the Complete Graph", "author": ["P. Chew"], "venue": "Proc. 2nd SOCG, pages 169\u2013177, New York, NY", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Graph spanners", "author": ["D. Peleg", "A. Sch\u00e4ffer"], "venue": "J. Graph Theory, 13(1):99\u2013116", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "Fast construction of nets in low-dimensional metrics and their applications", "author": ["S. Har-Peled", "M. Mendel"], "venue": "SIAM J. Comput., 35(5):1148\u20131184", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["Q.W. Bouts"], "venue": "P. ten Brink, and K. Buchin. A framework for Computing the Greedy Spanner. In Proc. of 30th SOCG, pages 11:11\u201311:19, New York, NY", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing in operations research using Julia", "author": ["M. Lubin", "I. Dunning"], "venue": "INFORMS Journal on Computing, 27(2):238\u2013248", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Calculation of the Wasserstein distance between probability distributions on the line", "author": ["S. Vallender"], "venue": "Theory Probab. Appl., 18(4):784\u2013786", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1974}, {"title": "Stein\u2019s method of exchangeable pairs for the Beta distribution and generalizations", "author": ["C. D\u00f6bler"], "venue": "arXiv:1411.4477", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "boot: Bootstrap R (S-Plus) Functions, 2015. R package version 1.3-15", "author": ["A. Canty", "B. Ripley"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Exponential convergence of Langevin distributions and their discrete approximations", "author": ["G. Roberts", "R. Tweedie"], "venue": "Bernoulli, 2(4):341\u2013363", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "On the distribution of points in a cube and the approximate evaluation of integrals", "author": ["I. Sobol"], "venue": "USSR Comput. Math. and Math. Phys, (7):86\u2013112", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1967}, {"title": "Super-samples from kernel herding", "author": ["Y. Chen", "M. Welling", "A. Smola"], "venue": "UAI", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Central limit theorems for the Wasserstein distance between the Empirical and the True Distributions", "author": ["E. del Barrio", "E. Gin\u00e9", "C. Matr\u00e1n"], "venue": "Ann. Probab., 27(2):1009\u20131071,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Low discrepancy sequences in high dimensions: How well are their projections distributed", "author": ["X. Wang", "I. Sloan"], "venue": "J. Comput. Appl. Math.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "On the equivalence between herding and conditional gradient algorithms", "author": ["F. Bach", "S. Lacoste-Julien", "G. Obozinski"], "venue": "Proc. 29th ICML, ICML\u201912", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Gibbs sampler convergence criteria", "author": ["A. Zellner", "C. Min"], "venue": "JASA, 90(431):921\u2013927", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1995}, {"title": "Output assessment for Monte Carlo simulations via the score statistic", "author": ["Y. Fan", "S.P. Brooks", "A. Gelman"], "venue": "J. Comp. Graph. Stat., 15(1)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A kernel method for the two-sampleproblem", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Adv. NIPS 19, pages 513\u2013520", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Multivariate Stein factors for a class of strongly log-concave distributions", "author": ["L. Mackey", "J. Gorham"], "venue": "arXiv:1512.07392", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction When faced with a complex target distribution, one often turns to Markov chain Monte Carlo (MCMC) [1] to approximate intractable expectations EP [h(Z)] = \u222b X p(x)h(x)dx with asymptotically exact sample estimates EQ[h(X)] = \u2211n i=1 q(xi)h(xi).", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "These complex targets commonly arise as posterior distributions in Bayesian inference and as candidate distributions in maximum likelihood estimation [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "However, the added flexibility introduces new challenges for sampler and parameter selection, since standard sample quality measures, like effective sample size, asymptotic variance, trace and mean plots, and pooled and within-chain variance diagnostics, presume eventual convergence to the target [1] and hence do not account for asymptotic bias.", "startOffset": 298, "endOffset": 301}, {"referenceID": 5, "context": "In this case, the expression (1) is termed an integral probability metric (IPM) [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "3 Stein\u2019s Method Stein\u2019s method [7] for characterizing convergence in distribution classically proceeds in three steps: 1.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "1 Identifying a Stein Operator The generator method of Barbour [8] provides a convenient and general means of constructing operators T which produce mean-zero functions under P (2) .", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "In the multivariate setting, analogous statements are available for multivariate Gaussian targets [11, 12, 13], but few other target distributions have been analyzed.", "startOffset": 98, "endOffset": 110}, {"referenceID": 11, "context": "In the multivariate setting, analogous statements are available for multivariate Gaussian targets [11, 12, 13], but few other target distributions have been analyzed.", "startOffset": 98, "endOffset": 110}, {"referenceID": 12, "context": "In the multivariate setting, analogous statements are available for multivariate Gaussian targets [11, 12, 13], but few other target distributions have been analyzed.", "startOffset": 98, "endOffset": 110}, {"referenceID": 8, "context": "The operator TP has also found fruitful application in the design of Monte Carlo control variates [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "Proposition 5 follows from the Whitney-Glaeser extension theorem for smooth functions [14, 15] and implies that the complete graph Stein discrepancy inherits all of the desirable convergence properties of the classical discrepancy.", "startOffset": 86, "endOffset": 94}, {"referenceID": 14, "context": "Proposition 5 follows from the Whitney-Glaeser extension theorem for smooth functions [14, 15] and implies that the complete graph Stein discrepancy inherits all of the desirable convergence properties of the classical discrepancy.", "startOffset": 86, "endOffset": 94}, {"referenceID": 15, "context": "2 Geometric Spanners For a given dilation factor t \u2265 1, a t-spanner [16, 17] is a graph G = (V,E) with weight \u2016x\u2212 y\u2016 on each edge (x, y) \u2208 E and a path between each pair x\u2032 6= y\u2032 \u2208 V with total weight no larger than t\u2016x\u2032 \u2212 y\u2032\u2016.", "startOffset": 68, "endOffset": 76}, {"referenceID": 16, "context": "2 Geometric Spanners For a given dilation factor t \u2265 1, a t-spanner [16, 17] is a graph G = (V,E) with weight \u2016x\u2212 y\u2016 on each edge (x, y) \u2208 E and a path between each pair x\u2032 6= y\u2032 \u2208 V with total weight no larger than t\u2016x\u2032 \u2212 y\u2032\u2016.", "startOffset": 68, "endOffset": 76}, {"referenceID": 17, "context": "Moreover, for any `p norm, a 2-spanner with O(\u03badn) edges can be computed in O(\u03badn log(n)) expected time for \u03bad a constant depending only on d and \u2016\u00b7\u2016 [18].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "[19] and solve all optimization programs using Julia for Mathematical Programming [20] with the default Gurobi 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[19] and solve all optimization programs using Julia for Mathematical Programming [20] with the default Gurobi 6.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "which is computable for simple univariate target distributions [22] and provably lower bounds the non-uniform Stein discrepancies (5) with c1:3 = (0.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "3 Selecting Sampler Hyperparameters Stochastic Gradient Langevin Dynamics (SGLD) [3] with constant step size is a biased MCMC procedure designed for scalable inference.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "To illustrate the value of the Stein diagnostic for this task, we adopt the bimodal Gaussian mixture model (GMM) posterior of [3] as our target.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "For a range of step sizes , we use SGLD with minibatch size 5 to draw 50 independent sequences of length n = 1000, and we select the value of with the highest median quality \u2013 either the maximum effective sample size (ESS, a standard diagnostic based on autocorrelation [1]) or the minimum spanner Stein discrepancy \u2013 across these sequences.", "startOffset": 270, "endOffset": 273}, {"referenceID": 4, "context": "4 Quantifying a Bias-Variance Trade-off The approximate random walk MH (ARWMH) sampler [5] is a second biased MCMC procedure designed for scalable posterior inference.", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "We analyze a dataset of 53 prostate cancer patients with six binary predictors and a binary outcome indicating whether cancer has spread to surrounding lymph nodes [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 0, "context": "Our target is the Bayesian logistic regression posterior [1] under a N (0, I) prior on the parameters.", "startOffset": 57, "endOffset": 60}, {"referenceID": 23, "context": "To corroborate our result, we use a Metropolis-adjusted Langevin chain [25] of length 10 as a surrogateQ\u2217 for the target and compute several error measures for each sampleQ: normalized probability error maxl |E[\u03c3(\u3008X,wl\u3009)\u2212 \u03c3(\u3008Z,wl\u3009)]|/\u2016wl\u2016\u221e, mean error maxj |E[Xj\u2212Zj ]| maxj |EQ\u2217 [Zj ]| , and second moment error maxj,k |E[XjXk\u2212ZjZk]| maxj,k |EQ\u2217 [ZjZk]| for X \u223c Q, Z \u223c Q \u2217, \u03c3(t) , 1 1+e\u2212t , and wl the l-th datapoint covariate vector.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "Unif(0, 1) sample, a deterministic Sobol sequence [26], and a deterministic kernel herding sequence [27] defined by the norm \u2016h\u2016H = \u222b 1 0 (h\u2032(x))2dx.", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "Unif(0, 1) sample, a deterministic Sobol sequence [26], and a deterministic kernel herding sequence [27] defined by the norm \u2016h\u2016H = \u222b 1 0 (h\u2032(x))2dx.", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "and Sobol sequences accord with expected O(1/ \u221a n) and O(1/n) bounds from the literature [28, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 27, "context": "and Sobol sequences accord with expected O(1/ \u221a n) and O(1/n) bounds from the literature [28, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 28, "context": "As witnessed also in other metrics [30], the herding rate of n\u22120.", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "The diagnostics of [31, 32] also account for asymptotic bias but lose discriminating power by considering only a finite collection of functionals.", "startOffset": 19, "endOffset": 27}, {"referenceID": 30, "context": "The diagnostics of [31, 32] also account for asymptotic bias but lose discriminating power by considering only a finite collection of functionals.", "startOffset": 19, "endOffset": 27}, {"referenceID": 30, "context": "For example, for a N (0, 1) target, the score statistic of [32] cannot distinguish two samples with equal first and second moments.", "startOffset": 59, "endOffset": 63}, {"referenceID": 31, "context": "Maximum mean discrepancy (MMD) on a characteristic Hilbert space [33] takes full distributional bias into account but is only viable when the expected kernel evaluations are easily computed under the target.", "startOffset": 65, "endOffset": 69}, {"referenceID": 32, "context": "The following result, proved in the companion paper [34], establishes the existence of explicit constants (Stein factors) c1, c2, c3 > 0, such that, for any test function h \u2208M\u2016\u00b7\u2016, the Stein equation h(x)\u2212 EP [h(Z)] = (TP gh)(x) has a solution gh = 12\u2207uh belonging to the non-uniform Stein set G c1:3 \u2016\u00b7\u2016 .", "startOffset": 52, "endOffset": 56}, {"referenceID": 32, "context": "2 of the companion paper [34] establishes this result for the case \u2016\u00b7\u2016 = \u2016\u00b7\u20162; we omit the proof of the generalization which closely mirrors that of the Euclidean norm case.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "4] of Glaeser [14], for every function g \u2208 G\u2016\u00b7\u2016,Q,G1 , there exists a function g\u0303 \u2208 \u03bad G\u2217 \u2016\u00b7\u2016 with g(xi) = g\u0303(xi) and \u2207g(xi) = \u2207g\u0303(xi) for all xi in the support of Q.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "For any such (mi,Mi) pair, there exists \u03b6i \u2208 [0, 1] satisfying g(x(i+1))\u2212 g(x(i)) = \u222b x(i+1) x(i) \u03b6imi(t) + (1\u2212 \u03b6i)Mi(t)dt, and hence we will define the extension g\u0303(x) = g(x(i)) + \u222b x x(i) \u03b6imi(t) + (1\u2212 \u03b6i)Mi(t)dt.", "startOffset": 45, "endOffset": 51}], "year": 2017, "abstractText": "To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein\u2019s method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.", "creator": "LaTeX with hyperref package"}}}