{"id": "1506.04820", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Online Gradient Boosting", "abstract": "we extend the theory of boosting for regression problems to the online learning setting. generalizing from the batch setting for boosting, the functional notion of a weak machine learning algorithm is further modeled as an online learning algorithm with linear loss functions that competes with radically a base class of regression functions, while a strong learning algorithm is an invariant online learning algorithm with convex loss functions that competes with considerably a larger class of regression functions. our main result is an intuitive online gradient boosting algorithm which converts a weak online multiple learning algorithm into a strong one where the larger class of functions is the linear parameter span of the base class. we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one exactly where the larger class of functions is the convex hull of the base class, and prove its optimality.", "histories": [["v1", "Tue, 16 Jun 2015 02:20:32 GMT  (19kb)", "https://arxiv.org/abs/1506.04820v1", null], ["v2", "Fri, 30 Oct 2015 20:04:31 GMT  (20kb)", "http://arxiv.org/abs/1506.04820v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alina beygelzimer", "elad hazan", "satyen kale", "haipeng luo"], "accepted": true, "id": "1506.04820"}, "pdf": {"name": "1506.04820.pdf", "metadata": {"source": "CRF", "title": "Online Gradient Boosting", "authors": ["Alina Beygelzimer", "Haipeng Luo"], "emails": ["beygel@yahoo-inc.com", "ehazan@cs.princeton.edu", "satyen@yahoo-inc.com", "haipengl@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n04 82\n0v 2\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "Boosting algorithms [21] are ensemble methods that convert a learning algorithm for a base class of models with weak predictive power, such as decision trees, into a learning algorithm for a class of models with stronger predictive power, such as a weighted majority vote over base models in the case of classification, or a linear combination of base models in the case of regression.\nBoosting methods such as AdaBoost [9] and Gradient Boosting [10] have found tremendous practical application, especially using decision trees as the base class of models. These algorithms were developed in the batch setting, where training is done over a fixed batch of sample data. However, with the recent explosion of huge data sets which do not fit in main memory, training in the batch setting is infeasible, and online learning techniques which train a model in one pass over the data have proven extremely useful.\nA natural goal therefore is to extend boosting algorithms to the online learning setting. Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2]. Of these, the work by Chen et al. [4] provided the first theoretical study of online boosting for classification, which was later generalized by Beygelzimer et al. [2] to obtain optimal and adaptive online boosting algorithms.\nHowever, extending boosting algorithms for regression to the online setting has been elusive and escaped theoretical guarantees thus far. In this paper, we rigorously formalize the setting of online boosting for regression and then extend the very commonly used gradient boosting methods [10, 19] to the online setting, providing theoretical guarantees on their performance.\nThe main result of this paper is an online boosting algorithm that competes with any linear combination the base functions, given an online linear learning algorithm over the base class. This algorithm is the online analogue of the batch boosting algorithm of Zhang and Yu [24], and in fact our algorithmic technique, when specialized to the batch boosting setting, provides exponentially better convergence guarantees.\nWe also give an online boosting algorithm that competes with the best convex combination of base functions. This is a simpler algorithm which is analyzed along the lines of the Frank-Wolfe algorithm [8]. While the algorithm has weaker theoretical guarantees, it can still be useful in practice. We also prove that this algorithm obtains the optimal regret bound (up to constant factors) for this setting.\nFinally, we conduct some proof-of-concept experiments which show that our online boosting algorithms do obtain performance improvements over different classes of base learners."}, {"heading": "1.1 Related Work", "text": "While the theory of boosting for classification in the batch setting is well-developed (see [21]), the theory of boosting for regression is comparatively sparse.The foundational theory of boosting for regression can be found in the statistics literature [14, 13], where boosting is understood as a greedy stagewise algorithm for fitting of additive models. The goal is to achieve the performance of linear combinations of base models, and to prove convergence to the performance of the best such linear combination.\nWhile the earliest works on boosting for regression such as [10] do not have such convergence proofs, later works such as [19, 6] do have convergence proofs but without a bound on the speed of convergence. Bounds on the speed of convergence have been obtained by Duffy and Helmbold [7] relying on a somewhat strong assumption on the performance of the base learning algorithm. A different approach to boosting for regression was taken by Freund and Schapire [9], who give an algorithm that reduces the regression problem to classification and then applies AdaBoost; the corresponding proof of convergence relies on an assumption on the induced classification problem which may be hard to satisfy in practice. The strongest result is that of Zhang and Yu [24], who prove convergence to the performance of the best linear combination of base functions, along with a bound on the rate of convergence, making essentially no assumptions on the performance of the base learning algorithm. Telgarsky [22] proves similar results for logistic (or similar) loss using a slightly simpler boosting algorithm.\nThe results in this paper are a generalization of the results of Zhang and Yu [24] to the online setting. However, we emphasize that this generalization is nontrivial and requires different algorithmic ideas and proof techniques. Indeed, we were not able to directly generalize the analysis in [24] by simply adapting the techniques used in recent online boosting work [4, 2], but we made use of the classical Frank-Wolfe algorithm [8]. On the other hand, while an important part of the convergence analysis for the batch setting is to show statistical consistency of the algorithms [24, 1, 22], in the online setting we only need to study the empirical convergence (that is, the regret), which makes our analysis much more concise."}, {"heading": "2 Setup", "text": "Examples are chosen from a feature space X , and the prediction space is Rd. Let \u2016 \u00b7 \u2016 denote some norm in Rd. In the setting for online regression, in each round t for t = 1, 2, . . . , T , an adversary selects an example xt \u2208 X and a loss function \u2113t : Rd \u2192 R, and presents xt to the online learner. The online learner outputs a prediction yt \u2208 Rd, obtains the loss function \u2113t, and incurs loss \u2113t(yt).\nLet F denote a reference class of regression functions f : X \u2192 Rd, and let C denote a class of loss functions \u2113 : Rd \u2192 R. Also, let R : N \u2192 R+ be a non-decreasing function. We say that the function class F is online learnable for losses in C with regret R if there is an online learning algorithm A,\nthat for every T \u2208 N and every sequence (xt, \u2113t) \u2208 X \u00d7 C for t = 1, 2, . . . , T chosen by the adversary, generates predictions1 A(xt) \u2208 Rd such that\nT \u2211\nt=1\n\u2113t(A(xt)) \u2264 inf f\u2208F\nT \u2211\nt=1\n\u2113t(f(xt)) +R(T ). (1)\nIf the online learning algorithm is randomized, we require the above bound to hold with high probability.\nThe above definition is simply the online generalization of standard empirical risk minimization (ERM) in the batch setting. A concrete example is 1-dimensional regression, i.e. the prediction space is R. For a labeled data point (x, y\u22c6) \u2208 X \u00d7 R, the loss for the prediction y \u2208 R is given by \u2113(y\u22c6, y) where \u2113(\u00b7, \u00b7) is a fixed loss function that is convex in the second argument (such as squared loss, logistic loss, etc). Given a batch of T labeled data points {(xt, y\u22c6t ) | t = 1, 2, . . . , T } and a base class of regression functions F (say, the set of bounded norm linear regressors), an ERM algorithm finds the function f \u2208 F that minimizes \u2211T\nt=1 \u2113(y \u22c6 t , f(xt)).\nIn the online setting, the adversary reveals the data (xt, y \u22c6 t ) in an online fashion, only presenting the true label y\u22c6t after the online learner A has chosen a prediction yt. Thus, setting \u2113t(yt) = \u2113(y\u22c6t , yt), we observe that if A satisfies the regret bound (1), then it makes predictions with total loss almost as small as that of the empirical risk minimizer, up to the regret term. If F is the set of all bounded-norm linear regressors, for example, the algorithm A could be online gradient descent [25] or online Newton Step [16].\nAt a high level, in the batch setting, \u201cboosting\u201d is understood as a procedure that, given a batch of data and access to an ERM algorithm for a function class F (this is called a \u201cweak\u201d learner), obtains an approximate ERM algorithm for a richer function class F \u2032 (this is called a \u201cstrong\u201d learner). Generally, F \u2032 is the set of finite linear combinations of functions in F . The efficiency of boosting is measured by how many times, N , the base ERM algorithm needs to be called (i.e., the number of boosting steps) to obtain an ERM algorithm for the richer function within the desired approximation tolerance. Convergence rates [24] give bounds on how quickly the approximation error goes to 0 and N \u2192 \u221e.\nWe now extend this notion of boosting to the online setting in the natural manner. To capture the full generality of the techniques, we also specify a class of loss functions that the online learning algorithm can work with. Informally, an online boosting algorithm is a reduction that, given access to an online learning algorithm A for a function class F and loss function class C with regret R, and a bound N on the total number of calls made in each iteration to copies of A, obtains an online learning algorithm A\u2032 for a richer function class F \u2032, a richer loss function class C\u2032, and (possibly larger) regret R\u2032. The bound N on the total number of calls made to all the copies of A corresponds to the number of boosting stages in the batch setting, and in the online setting it may be viewed as a resource constraint on the algorithm. The efficacy of the reduction is measured by R\u2032 which is a function of R, N , and certain parameters of the comparator class F \u2032 and loss function class C\u2032. We desire online boosting algorithms such that 1\nT R\u2032(T ) \u2192 0 quickly as N \u2192 \u221e and T \u2192 \u221e. We make the notions of\nrichness in the above informal description more precise now.\nComparator function classes. A given function class F is said to be D-bounded if for all x \u2208 X and all f \u2208 F , we have \u2016f(x)\u2016 \u2264 D. Throughout this paper, we assume that F is symmetric:2 i.e. if\n1There is a slight abuse of notation here. A(\u00b7) is not a function but rather the output of the online learning algorithm A computed on the given example using its internal state.\n2This is without loss of generality; as will be seen momentarily, our base assumption only requires an online learning algorithm A for F for linear losses \u2113t. By running the Hedge algorithm on two copies of A, one of which receives the actual loss functions \u2113t and the other recieves \u2212\u2113t, we get an algorithm which competes with negations of functions in F and the constant zero function as well. Furthermore, since the loss functions are convex (indeed, linear) this can be made into a deterministic reduction by choosing the convex combination of the outputs of the two copies of A with mixing weights given by the Hedge algorithm.\nf \u2208 F , then \u2212f \u2208 F , and it contains the constant zero function, which we denote, with some abuse of notation, by 0.\nGiven F , we define two richer function classes F \u2032: the convex hull of F , denoted CH(F), is the set of convex combinations of a finite number of functions in F , and the span of F , denoted span(F), is the set of linear combinations of finitely many functions in F . For any f \u2208 span(F), define \u2016f\u20161 := inf { max{1,\u2211g\u2208S |wg|} : f = \u2211 g\u2208S wgg, S \u2286 F , |S| < \u221e, wg \u2208 R } . Since functions in span(F) are not bounded, it is not possible to obtain a uniform regret bound for all functions in span(F): rather, the regret of an online learning algorithm A for span(F) is specified in terms of regret bounds for individual comparator functions f \u2208 span(F ), viz.\nRf (T ) :=\nT \u2211\nt=1\n\u2113t(A(xt))\u2212 T \u2211\nt=1\n\u2113t(f(xt)).\nLoss function classes. The base loss function class we consider is L, the set of all linear functions \u2113 : Rd \u2192 R, with Lipschitz constant bounded by 1. A function class F that is online learnable with the loss function class L is called online linear learnable for short. The richer loss function class we consider is denoted by C and is a set of convex loss functions \u2113 : Rd \u2192 R satisfying some regularity conditions specified in terms of certain parameters described below.\nWe define a few parameters of the class C. For any b > 0, let Bd(b) = {y \u2208 Rd : \u2016y\u2016 \u2264 b} be the ball of radius b. The class C is said to have Lipschitz constant Lb on Bd(b) if for all \u2113 \u2208 C and all y \u2208 Bd(b) there is an efficiently computable subgradient \u2207\u2113(y) with norm at most Lb. Next, C is said to be \u03b2b-smooth on B d(b) if for all \u2113 \u2208 C and all y,y\u2032 \u2208 Bd(b) we have\n\u2113(y\u2032) \u2264 \u2113(y) +\u2207\u2113(y) \u00b7 (y\u2032 \u2212 y) + \u03b2b 2 \u2016y\u2212 y\u2032\u20162.\nNext, define the projection operator \u03a0b : R d \u2192 Bd(b) as \u03a0b(y) := argminy\u2032\u2208Bd(b) \u2016y\u2212y\u2032\u2016, and define \u01ebb := supy\u2208Rd, \u2113\u2208C \u2113(\u03a0b(y))\u2212\u2113(y) \u2016\u03a0b(y)\u2212y\u2016 ."}, {"heading": "3 Online Boosting Algorithms", "text": "The setup is that we are given a D-bounded reference class of functions F with an online linear learning algorithm A with regret bound R(\u00b7). For normalization, we also assume that the output of A at any time is bounded in norm by D, i.e. \u2016A(xt)\u2016 \u2264 D for all t. We further assume that for every b > 0, we can compute3 a Lipschitz constant Lb, a smoothness parameter \u03b2b, and the parameter \u01ebb for the class C over Bd(b). Furthermore, the online boosting algorithm may make up to N calls per iteration to any copies of A it maintains, for a given a budget parameter N .\nGiven this setup, our main result is an online boosting algorithm, Algorithm 1, competing with span(F). The algorithm maintains N copies of A, denoted Ai, for i = 1, 2, . . . , N . Each copy corresponds to one stage in boosting. When it receives a new example xt, it passes it to each Ai and obtains their predictions Ai(xt), which it then combines into a prediction for yt using a linear combination. At the most basic level, this linear combination is simply the sum of all the predictions scaled by a step size parameter \u03b7. Two tweaks are made to this sum in step 8 to facilitate the analysis:\n1. While constructing the sum, the partial sum yi\u22121t is multiplied by a shrinkage factor (1\u2212 \u03c3it\u03b7). This shrinkage term is tuned using an online gradient descent algorithm in step 14. The goal of the tuning is to induce the partial sums yi\u22121t to be aligned with a descent direction for the loss functions, as measured by the inner product \u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t .\n3It suffices to compute upper bounds on these parameters.\nAlgorithm 1 Online Gradient Boosting for span(F) Require: Number of weak learners N , step size parameter \u03b7 \u2208 [ 1\nN , 1],\n1: Let B = min{\u03b7ND, inf{b \u2265 D : \u03b7\u03b2bb2 \u2265 \u01ebbD}}. 2: Maintain N copies of the algorithm A, denoted Ai for i = 1, 2, . . . , N . 3: For each i, initialize \u03c3i1 = 0. 4: for t = 1 to T do 5: Receive example xt. 6: Define y0t = 0. 7: for i = 1 to N do 8: Define yit = \u03a0B((1 \u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt)). 9: end for\n10: Predict yt = y N t . 11: Obtain loss function \u2113t and suffer loss \u2113t(yt). 12: for i = 1 to N do 13: Pass loss function \u2113it(y) = 1 LB\n\u2207\u2113t(yi\u22121t ) \u00b7 y to Ai. 14: Set \u03c3it+1 = max{min{\u03c3it + \u03b1t\u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t ), 1}, 0}, where \u03b1t = 1LBB\u221at . 15: end for 16: end for\n2. The partial sums yit are made to lie in B d(B), for some parameter B, by using the projection\noperator \u03a0B. This is done to ensure that the Lipschitz constant and smoothness of the loss function are suitably bounded.\nOnce the boosting algorithm makes the prediction yt and obtains the loss function \u2113t, each Ai is updated using a suitably scaled linear approximation to the loss function at the partial sum yi\u22121t , i.e. the linear loss function 1\nLB \u2207\u2113t(yi\u22121t ) \u00b7 y. This forces Ai to produce predictions that are aligned with\na descent direction for the loss function. We provide the analysis of the algorithm in Section 4.2. The analysis yields the following regret bound for the algorithm:\nTheorem 1. Let \u03b7 \u2208 [ 1 N , 1] be a given parameter. Let B = min{\u03b7ND, inf{b \u2265 D : \u03b7\u03b2bb2 \u2265 \u01ebbD}}. Algorithm 1 is an online learning algorithm for span(F) and losses in C with the following regret bound for any f \u2208 span(F):\nR\u2032f (T ) \u2264 ( 1\u2212 \u03b7\u2016f\u20161\n)N\n\u22060 + 3\u03b7\u03b2BB 2\u2016f\u20161T + LB\u2016f\u20161R(T ) + 2LBB\u2016f\u20161 \u221a T ,\nwhere \u22060 := \u2211T t=1 \u2113t(0)\u2212 \u2113t(f(xt)).\nThe regret bound in this theorem depends on several parameters such as B, \u03b2B and LB. In applications of the algorithm for 1-dimensional regression with commonly used loss functions, however, these parameters are essentially modest constants; see Section 3.1 for calculations of the parameters for various loss functions. Furthermore, if \u03b7 is appropriately set (e.g. \u03b7 = (logN)/N), then the average regret R\u2032f (T )/T clearly converges to 0 as N \u2192 \u221e and T \u2192 \u221e. While the requirement that N \u2192 \u221e may raise concerns about computational efficiency, this is in fact analogous to the guarantee in the batch setting: the algorithms converge only when the number of boosting stages goes to infinity. Moreover, our lower bound (Theorem 3) shows that this is indeed necessary.\nWe also present a simpler boosting algorithm, Algorithm 2, that competes with CH(F). Algorithm 2 is similar to Algorithm 1, with some simplifications: the final prediction is simply a convex combination of the predictions of the base learners, with no projections or shrinkage necessary. While Algorithm 1 is more general, Algorithm 2 may still be useful in practice when a bound on the norm of the comparator function is known in advance, using the observations in Section 5.2. Furthermore, its\nanalysis is cleaner and easier to understand for readers who are familiar with the Frank-Wolfe method, and this serves as a foundation for the analysis of Algorithm 1. This algorithm has an optimal (up to constant factors) regret bound as given in the following theorem, proved in Section 4.1. The upper bound in this theorem is proved along the lines of the Frank-Wolfe [8] algorithm, and the lower bound using information-theoretic arguments.\nTheorem 2. Algorithm 2 is an online learning algorithm for CH(F) for losses in C with the regret bound\nR\u2032(T ) \u2264 8\u03b2DD 2\nN T + LDR(T ).\nFurthermore, the dependence of this regret bound on N is optimal up to constant factors.\nThe dependence of the regret bound on R(T ) is unimprovable without additional assumptions: otherwise, Algorithm 2 will be an online linear learning algorithm over F with better than R(T ) regret.\nAlgorithm 2 Online Gradient Boosting for CH(F) 1: Maintain N copies of the algorithm A, denoted A1,A2, . . . ,AN , and let \u03b7i = 2i+1 for i =\n1, 2, . . . , N . 2: for t = 1 to T do 3: Receive example xt. 4: Define y0t = 0. 5: for i = 1 to N do 6: Define yit = (1\u2212 \u03b7i)yi\u22121t + \u03b7iAi(xt). 7: end for 8: Predict yt = y N t .\n9: Obtain loss function \u2113t and suffer loss \u2113t(yt). 10: for i = 1 to N do 11: Pass loss function \u2113it(y) = 1 LD\n\u2207\u2113t(yi\u22121t ) \u00b7 y to Ai. 12: end for\n13: end for\nUsing a deterministic base online linear learning algorithm. If the base online linear learning algorithmA is deterministic, then our results can be improved, because our online boosting algorithms are also deterministic, and using a standard simple reduction, we can now allow C to be any set of convex functions (smooth or not) with a computable Lipschitz constant Lb over the domain B\nd(b) for any b > 0.\nThis reduction converts arbitrary convex loss functions into linear functions: viz. if yt is the output of the online boosting algorithm, then the loss function provided to the boosting algorithm as feedback is the linear function \u2113\u2032t(y) = \u2207\u2113t(yt) \u00b7 y. This reduction immediately implies that the base online linear learning algorithm A, when fed loss functions 1\nLD \u2113\u2032t, is already an online learning\nalgorithm for CH(F) with losses in C with the regret bound R\u2032(T ) \u2264 LDR(T ). As for competing with span(F), since linear loss functions are 0-smooth, we obtain the following easy corollary of Theorem 1:\nCorollary 1. Let \u03b7 \u2208 [ 1 N , 1] be a given parameter, and set B = \u03b7ND. Algorithm 1 is an online learning algorithm for span(F) for losses in C with the following regret bound for any f \u2208 span(F):\nR\u2032f (T ) \u2264 ( 1\u2212 \u03b7\u2016f\u20161\n)N \u22060 + LB\u2016f\u20161R(T ) + 2LBB\u2016f\u20161 \u221a T ,\nwhere \u22060 := \u2211T t=1 \u2113t(0)\u2212 \u2113t(f(xt))."}, {"heading": "3.1 The parameters for several basic loss functions", "text": "In this section we consider the application of our results to 1-dimensional regression, where we assume, for normalization, that the true labels of the examples and the predictions of the functions in the class F are in [\u22121, 1]. In this case \u2016 \u00b7\u2016 denotes the absolute value norm. Thus, in each round, the adversary chooses a labeled data point (xt, y \u22c6 t ) \u2208 X \u00d7 [\u22121, 1], and the loss for the prediction yt \u2208 [\u22121, 1] is given by \u2113t(yt) = \u2113(y \u22c6 t , yt) where \u2113(\u00b7, \u00b7) is a fixed loss function that is convex in the second argument. Note that D = 1 in this setting. We give examples of several such loss functions below, and compute the parameters Lb, \u03b2b and \u01ebb for every b > 0, as well as B from Theorem 1.\n1. Linear loss: \u2113(y\u22c6, y) = \u2212y\u22c6y. We have Lb = 1, \u03b2b = 0, \u01ebb = 1, and B = \u03b7N .\n2. p-norm loss, for some p \u2265 2: \u2113(y\u22c6, y) = |y\u22c6\u2212y|p. We have Lb = p(b+1)p\u22121, \u03b2b = p(p\u22121)(b+1)p\u22122, \u01ebb = max{p(1\u2212 b)p\u22121, 0}, and B = 1.\n3. Modified least squares: \u2113(y\u22c6, y) = 12 max{1 \u2212 y\u22c6y, 0}2. We have Lb = b + 1, \u03b2b = 1, \u01ebb = max{1\u2212 b, 0}, and B = 1.\n4. Logistic loss: \u2113(y\u22c6, y) = ln(1 + exp(\u2212y\u22c6y)). We have Lb = exp(b)1+exp(b) , \u03b2b = 14 , \u01ebb = exp(\u2212b) 1+exp(\u2212b) , and\nB = min{\u03b7N, ln(4/\u03b7)}."}, {"heading": "4 Analysis", "text": "In this section, we analyze Algorithms 1 and Algorithm 2."}, {"heading": "4.1 Competing with convex combinations of the base functions", "text": "We give the analysis of Algorithm 2 before that of Algorithm 1 since it is easier to understand and provides the foundation for the analysis of Algorithm 1.\nProof of Theorem 2. First, note that for any i = 1, 2, . . . , N , since \u2113it is a linear function, we have\ninf f\u2208CH(F)\nT \u2211\nt=1\n\u2113it(f(xt)) = inf f\u2208F\nT \u2211\nt=1\n\u2113it(f(xt)).\nLet f be any function in CH(F). The equality above and the fact that Ai is an online learning algorithm for F with regret bound R(\u00b7) for the 1-Lipschitz linear loss functions \u2113it(y) = 1LD\u2207\u2113t(y i\u22121 t )\u00b7y imply that T \u2211\nt=1\n1\nLD \u2207\u2113t(yi\u22121t ) \u00b7 Ai(xt) \u2264\nT \u2211\nt=1\n1\nLD \u2207\u2113t(yi\u22121t ) \u00b7 f(xt) +R(T ). (2)\nNow define, for i = 0, 1, 2, . . . , N , \u2206i = \u2211T t=1 \u2113t(y i t)\u2212 \u2113t(f(xt)). We have\n\u2206i =\nT \u2211\nt=1\n\u2113t(y i\u22121 t + \u03b7i(Ai(xt)\u2212 yi\u22121t ))\u2212 \u2113t(f(xt))\n\u2264 T \u2211\nt=1\n\u2113t(y i\u22121 t )\u2212 \u2113t(f(xt)) + \u03b7i\u2207\u2113t(yi\u22121t ) \u00b7 (Ai(xt)\u2212 yi\u22121t ) + \u03b72i \u03b2D 2 \u2016Ai(xt)\u2212 yi\u22121t \u20162\n(by \u03b2D-smoothness of C)\n\u2264 [ T \u2211\nt=1\n\u2113t(y i\u22121 t )\u2212 \u2113t(f(xt)) + \u03b7i\u2207\u2113t(yi\u22121t ) \u00b7 (f(xt)\u2212 yi\u22121t ) + 2\u03b72i \u03b2DD2\n]\n+ \u03b7iLDR(T )\n(by (2) and using the bound \u2016Ai(xt)\u2212 yi\u22121t \u2016 \u2264 2D)\n\u2264 [ T \u2211\nt=1\n\u2113t(y i\u22121 t )\u2212 \u2113t(f(xt))\u2212 \u03b7i(\u2113t(yi\u22121t )\u2212 \u2113t(f(xt))) + 2\u03b72i \u03b2DD2\n]\n+ \u03b7iLDR(T )\n( by convexity, \u2113t(y i\u22121 t ) +\u2207\u2113(yi\u22121t ) \u00b7 (f(xt)\u2212 yi\u22121t ) \u2264 \u2113t(f(xt)) ) \u2264 (1\u2212 \u03b7i)\u2206i\u22121 + 2\u03b72i \u03b2DD2T + \u03b7iLDR(T ).\nFor i = 1, since \u03b71 = 1, the above bound implies that \u22061 \u2264 2\u03b2DD2T + LDR(T ). Starting from this base case, an easy induction on i \u2265 1 proves that \u2206i \u2264 8\u03b2DD 2\ni T + LDR(T ). Applying this bound for\ni = N completes the proof.\nWe now show that the dependence of the regret bound of Algorithm 2 on the parameter N is optimal up to constant factors.\nTheorem 3. Let N be any specified bound on the total number of calls in each iteration to all copies of the base online linear learning algorithm. Then there is a setting of 1-dimensional prediction with a 1-bounded comparator function class F , an online linear optimization algorithm A over F , and a class C of loss functions that is 1-smooth on R such that any online boosting algorithm for CH(F) with losses in C respecting the bound N has regret at least \u2126( T\nN ).\nProof. Consider the following construction. At a high level, the setting is 1-dimensional regression with C corresponding to squared loss. The domain X = N and true labels of examples are in [0, 1].\nDefine p1 = 1 2 + \u01eb and p2 = 1 2 \u2212 \u01eb, where \u01eb = 110\u221aN , and let D1 and D2 be two distributions over {0, 1}N where each bit is Bernoulli random variable with parameter p1 and p2 respectively, chosen independently of the other bits. Consider a sequence of examples (xt, y \u22c6 t ) \u2208 N \u00d7 [0, 1] generated as follows: xt = t, and the label y \u22c6 t is chosen from {p1, p2} uniformly at random in each round.\nLet for c = 14000 . The function class F consists of a large number, M = 1cN , of functions fi, i \u2208 [M ]. For each t and i, we set fi(xt) = 1 w.p. y\u22c6t , and 0 w.p. 1 \u2212 y\u22c6t , independently of all other values of t and i.\nThe base online linear learning algorithm A is simply Hedge over the M functions. In each round, the Hedge algorithm selects one of the M functions in F and uses that to predict the label, and for any sequence of T examples, with high probability, incurs regret R(T ) = O( \u221a\nlog(M)T ). We set C to be set of squared loss functions, i.e. functions of the form \u2113(y) = 12 (y \u2212 y\u22c6)2 for y\u22c6 \u2208 [0, 1]. Note that these loss functions are 1-smooth and D = 1. In round t, the loss function is \u2113t(y) = 1 2 (y \u2212 y\u22c6t )2.\nConsider the function f\u0304 = 1 M\n\u2211\ni\u2208[M ] fi, which is in CH(F). Given any input sequence (xt, y\u22c6t ) for t = 1, 2, . . . , T it is easy to calculate that E[ 12 (f\u0304(xt)\u2212 y\u22c6t )2] = y\u22c6t (1\u2212y\u22c6t ) 2M \u2264 12M , and since the examples and predictions of functions on the examples are independent across iterations, a simple application\nof the multiplicative Chernoff bound implies that if T \u2265 12M , then with probability at least 0.9, we have \u2211T\nt=1 1 2 (f\u0304(xt)\u2212 y\u22c6t )2 \u2264 TM .\nNow suppose there is an online boosting algorithm making at most N calls total to all copies of A in each iteration, that for any large enough T and for any sequence (xt, y\u22c6t ) for t = 1, 2, . . . , T , outputs predictions yt such that with high probability, say at least 0.9, we have \u2211T t=1 1 2 (yt \u2212 y\u22c6t )2 \u2264 \u2211T\nt=1 1 2 (f\u0304(xt)\u2212y\u22c6t )2+ cTN . Then by a union bound, with probability at least 0.8, we have\n\u2211T\nt=1 1 2 (yt\u2212\ny\u22c6t ) 2 \u2264 cT N + T M \u2264 2cT N . By Markov\u2019s inequality and a union bound, with probability at least 0.7, for a uniform random time \u03c4 \u2208 [T ], we have\n1 2 (y\u03c4 \u2212 y\u22c6\u03c4 )2 \u2264 20c N =\n\u01eb2 2 , (3)\nor in other words, y\u03c4 is on the same side of 1 2 as y \u22c6 \u03c4 , and thus can be used to identify y \u22c6 \u03c4 . In the rest of the proof, we will use this fact, along with fact the total variation distance between D1 and D2, denoted dTV(D1, D2), is small, to derive a contradiction.\nDefine the random variable Y : {0, 1}N \u2192 R as follows. For any bit string s = \u3008s1, s2, . . . , sN \u3009 \u2208 {0, 1}N , choose a random round \u03c4 \u2208 [T ], and simulate the online boosting process until round \u03c4\u22121 by sampling y\u22c6t \u2019s and the outputs of fi(xt) for all t \u2264 \u03c4\u22121 and i \u2208 [M ] from the appropriate distributions. In round \u03c4 , let fi1 , fi2 , . . . , fiN be the functions that are obtained from the at most N calls to copies of A (there could be repetitions). Assign fij (x\u03c4 ) = sj for j \u2208 [N ] (being careful with repeated functions and repeating outputs appropriately), and run the booster with these outputs to obtain y\u03c4 , and set Y (s) = y\u03c4 . Let Pr[\u00b7] denotes probability of events in this process for generating Y (s) given s.\nLet E1[X(s)] and E2[X(s)] denote expectation of a random variable X : {0, 1}N \u2192 R when s is drawn from D1 and D2 respectively, and let E0[X(I, s)] denote expectation of a random variable X : {1, 2} \u00d7 {0, 1}N \u2192 R when I is chosen from {1, 2} uniformly at random and then s is sampled from DI . The above analysis (inequality (3)) implies that\n0.7 \u2264 E0[Pr[|Y (s)\u2212 pI | \u2264 \u01eb]] = 12E1[Pr[|Y (s)\u2212 p1| \u2264 \u01eb]] + 12E2[Pr[|Y (s)\u2212 p2| \u2264 \u01eb]].\nNow define a random variable X : {0, 1}N \u2192 R as X(s) = Pr[Y (s) \u2265 12 ]. Since\nPr[Y (s) \u2265 12 ] \u2265 Pr[|Y (s)\u2212 p1| \u2264 \u01eb] and 1\u2212 Pr[Y (s) \u2265 12 ] \u2265 Pr[|Y (s)\u2212 p2| \u2264 \u01eb],\nwe conclude, using the above bound, that E1[X(s)] \u2212 E2[X(s)] \u2265 0.4. This is a contradiction, since because X(s) \u2208 [0, 1], we have\nE1[X(s)]\u2212 E2[X(s)] \u2264 dTV(D1, D2) < 4 \u221a \u01eb2N = 0.4,\nwhere the bound on dTV(D1, D2) is standard, for e.g. see [15]. This gives us the desired contradiction.\nThe above result can be easily extended to any given parameters \u03b2 and D so that the F is Dbounded and C is \u03b2-smooth on R, giving a lower bound of \u2126(\u03b2D2T\nN ) on the regret of an online boosting\nalgorithm for CH(F) with losses in C: we simply scale all function and label values by D, and consider the loss functions \u2113(y, y\u22c6) = \u03b22 (y \u2212 y\u22c6)2. If there were an online boosting algorithm for CH(F) with these loss functions with regret o(\u03b2D\n2T N\n), then by scaling down the predictions by D, we obtain an online boosting algorithm for exactly the setting in the proof of Theorem 3 with a regret bound of o( T\nN ), which is a contradiction."}, {"heading": "4.2 Competing with the span of the base functions", "text": "In this section we show that Algorithm 1 satisfies the regret bound claimed in Theorem 1.\nProof of Theorem 1. Let f = \u2211 g\u2208S wgg, for some finite subset S of F , where wg \u2208 R. Since F is symmetric, we may assume that all wg \u2265 0, and let W := \u2211\ng wg. Furthermore, we may assume that 0 \u2208 S with weight w0 = max{1 \u2212 \u2211\ng\u2208S, g 6=0 wg, 0}, so that W \u2265 1. Note that \u2016f\u20161 is exactly the infimum of W over all such ways of expressing f as a finite weighted sum of functions in F . We now prove that bound stated in the theorem holds with \u2016f\u20161 replaced by W ; the theorem then follows simply by taking the infimum of the bound over all such ways of expressing f .\nNow, for each i \u2208 [N ], the update in line 14 of Algorithm 1 is exactly online gradient descent [25] on the domain [0, 1] with linear loss functions \u03c3 7\u2192 \u2212\u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t \u03c3. Note that the derivative of this loss function is bounded as follows: | \u2212 \u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t | \u2264 LBB. Since 1W \u2208 [0, 1], the standard analysis of online gradient descent then implies that the sequence \u03c3it for t = 1, 2, . . . , T satisfies\nT \u2211\nt=1\n\u2212\u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t \u03c3it \u2264 T \u2211\nt=1\n\u2212\u2207\u2113t(yi\u22121t ) \u00b7 yi\u22121t 1\nW + 2LBB\n\u221a T . (4)\nNext, since f = \u2211 g\u2208S wgg with wg \u2265 0, we have\n1\nW\nT \u2211\nt=1\n\u2207\u2113t(yit) \u00b7 f(xt) = 1 \u2211\ng\u2208S wg\nT \u2211\nt=1\n\u2211 g\u2208S wg\u2207\u2113t(yit) \u00b7 g(xt) \u2265 min g\u2208S\nT \u2211\nt=1\n\u2207\u2113t(yit) \u00b7 g(xt). (5)\nLet g\u22c6 \u2208 argming\u2208S \u2211T t=1 \u2207\u2113t(yit) \u00b7 g(xt). Since Ai is an online learning algorithm for F with regret bound R(\u00b7) for the 1-Lipschitz linear loss functions \u2113it(y) = 1LB\u2207\u2113t(y i\u22121 t ) \u00b7y, and g\u22c6 \u2208 F , multiplying the regret bound (1) by LB we have\nT \u2211\nt=1\n\u2207\u2113t(yi\u22121t ) \u00b7 Ai(xt) \u2264 T \u2211\nt=1\n\u2207\u2113t(yi\u22121t ) \u00b7 g\u22c6(xt) + LBR(T ) \u2264 1\nW\nT \u2211\nt=1\n\u2207\u2113t(yi\u22121t ) \u00b7 f(xt) + LBR(T ) (6)\nby (5). Now, we analyze how much excess loss is potentially introduced due to the projection in line 8. First, note that if B = \u03b7ND, then the projection has no effect since (1\u2212\u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt) \u2208 Bd(B), and in this case \u2113t(y i t) = \u2113t((1 \u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt)). If B < \u03b7ND, then by the definition of B, \u03b7\u03b2BB 2 \u2265 \u01ebBD, and since (1\u2212 \u03c3it\u03b7)yi\u22121t \u2208 Bd(B) and \u2016\u03b7Ai(xt))\u2016 \u2264 \u03b7D, and we have\n\u2113t(y i t) = \u2113t(\u03a0B((1\u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt))) \u2264 \u2113t((1\u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt)) + \u03b7\u01ebBD.\nIn either case, we have\n\u2113t(y i t) \u2264 \u2113t((1 \u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt)) + \u03b72\u03b2BB2. (7)\nWe now move to the main part of the analysis. Define for i = 0, 1, 2, . . . , N , \u2206i := \u2211T t=1 \u2113t(y i t)\u2212\n\u2113t(f(xt)). We have\n\u2206i \u2264 [ T \u2211\nt=1\n\u2113t((1\u2212 \u03c3it\u03b7)yi\u22121t + \u03b7Ai(xt))\u2212 \u2113t(f(xt)) ] + \u03b72\u03b2BB 2T\n\u2264 \u2206i\u22121 + [ T \u2211\nt=1\n\u03b7\u2207\u2113t(yi\u22121t ) \u00b7 (Ai(xt)\u2212 \u03c3ityi\u22121t ) + \u03b2B\u03b7\n2\n2 \u2016Ai(xt)\u2212 \u03c3ityi\u22121t \u20162\n]\n+ \u03b72\u03b2BB 2T\n(by \u03b2B-smoothness) \u2264 \u2206i\u22121 + [ T \u2211\nt=1\n\u03b7\nW \u2207\u2113t(yi\u22121t ) \u00b7 (f(xt)\u2212 yi\u22121t )\n]\n+ 3\u03b72\u03b2BB 2T + \u03b7LBR(T ) + 2\u03b7LBB \u221a T\n(by (4), (6) and the fact that \u2016Ai(xt)\u2212 \u03c3ityi\u22121t \u2016 \u2264 D +B \u2264 2B) \u2264 (\n1\u2212 \u03b7 W ) \u2206i\u22121 + 3\u03b7 2\u03b2BB\n2T + \u03b7LBR(T ) + 2\u03b7LBB \u221a T ,\nsince, by convexity of \u2113t we have \u2113t(y i\u22121 t )+\u2207\u2113(yi\u22121t ) \u00b7 (f(xt)\u2212yi\u22121t ) \u2264 \u2113t(f(xt)). Applying the above bound iteratively, we get\n\u2206N \u2264 ( 1\u2212 \u03b7 W )N \u22060 +\nN \u2211\ni=1\n( 1\u2212 \u03b7 W )i\u22121 \u00b7 (3\u03b72\u03b2BB2T + \u03b7LBR(T ) + 2\u03b7LBB \u221a T )\n\u2264 ( 1\u2212 \u03b7 W )N \u22060 + 3\u03b7\u03b2BB 2WT + LBWR(T ) + 2LBBW\n\u221a T .\nThis completes the proof."}, {"heading": "5 Variants of the boosting algorithms", "text": "Our boosting algorithms and the analysis are considerably flexible: it is easy to modify the algorithms to work with a different (and perhaps more natural) kind of base learner which does greedy fitting, or incorporate a scaling of the base functions which improves performance. Also, when specialized to the batch setting, our algorithms provide better convergence rates than previous work."}, {"heading": "5.1 Fitting to actual loss functions", "text": "The choice of an online linear learning algorithm over the base function class in our algorithms was made to ease the analysis. In practice, it is more common to have an online algorithm which produce predictions with comparable accuracy to the best function in hindsight for the actual sequence of loss functions. In particular, a common heuristic in boosting algorithms such as the original gradient boosting algorithm by Friedman [10] or the matching pursuit algorithm of Mallat and Zhang [18] is to build a linear combination of base functions by iteratively augmenting the current linear combination via greedily choosing a base function and a step size for it that minimizes the loss with respect to the residual label. Indeed, the boosting algorithm of Zhang and Yu [24] also uses this kind of greedy fitting algorithm as the base learner.\nIn the online setting, we can model greedy fitting as follows. We first fix a step size \u03b1 \u2265 0 in advance. Then, in each round t, the base learner A receives not only the example xt, but also an offset y\u2032t \u2208 Rd for the prediction, and produces a prediction A(xt) \u2208 Rd, after which it receives the loss function \u2113t and suffers loss \u2113t(y \u2032 t + \u03b1A(xt)). The predictions of A satisfy\nT \u2211\nt=1\n\u2113t(y \u2032 t + \u03b1A(xt)) \u2264 inf\nf\u2208F\nT \u2211\nt=1\n\u2113t(y \u2032 t + \u03b1f(xt)) +R(T ),\nwhere R is the regret. Our algorithms can be made to work with this kind of base learner as well. The details can be found in Section A.1 of the supplementary material."}, {"heading": "5.2 Improving the regret bound via scaling", "text": "Given an online linear learning algorithm A over the function class F with regret R, then for any scaling parameter \u03bb > 0, we trivially obtain an online linear learning algorithm, denoted \u03bbA, over a \u03bb-scaling of F , viz. \u03bbF := {\u03bbf | f \u2208 F}, simply by multiplying the predictions of A by \u03bb. The corresponding regret scales by \u03bb as well, i.e. it becomes \u03bbR.\nThe performance of Algorithm 1 can be improved by using such an online linear learning algorithm over \u03bbF for a suitably chosen scaling \u03bb \u2265 1 of the function class F . The regret bound from Theorem 1 improves because the 1-norm of f measured with respect to \u03bbF , i.e. \u2016f\u2016\u20321 = max{1, \u2016f\u20161\u03bb }, is smaller than \u2016f\u20161, but degrades because the parameter B\u2032 = min{\u03b7N\u03bbD, inf{b \u2265 \u03bbD : \u03b7\u03b2bb2 \u2265 \u01ebb\u03bbD}} is larger than B. But, as detailed in Section A.2 of the supplementary material, in many situations the improvement due to the former compensates for the degradation due to the latter, and overall we can get improved regret bounds using a suitable value of \u03bb."}, {"heading": "5.3 Improvements for batch boosting", "text": "Our algorithmic technique can be easily specialized and modified to the standard batch setting with a fixed batch of training examples and a base learning algorithm operating over the batch, exactly as in [24]. The main difference compared to the algorithm of [24] is the use of the \u03c3 variables to scale the coefficients of the weak hypotheses appropriately. While a seemingly innocuous tweak, this allows us to derive analogous bounds to those of Zhang and Yu [24] on the optimization error that show that our boosting algorithm converges exponential faster. A detailed comparison can be found in Section A.3 of the supplementary material."}, {"heading": "6 Experimental Results", "text": "Is it possible to boost in an online fashion in practice with real base learners? To study this question, we implemented and evaluated Algorithms 1 and 2 within the Vowpal Wabbit (VW) open source machine learning system [23]. The three online base learners used were VW\u2019s default linear learner (a variant of stochastic gradient descent), two-layer sigmoidal neural networks with 10 hidden units, and regression stumps.\nRegression stumps were implemented by doing stochastic gradient descent on each individual feature, and predicting with the best-performing non-zero valued feature in the current example.\nAll experiments were done on a collection of 14 publically available regression and classification datasets (described in Section B in the supplementary material) using squared loss. The only parameters tuned were the learning rate and the number of weak learners, as well as the step size parameter for Algorithm 1. Parameters were tuned based on progressive validation loss on half of the dataset; reported is propressive validation loss on the remaining half. Progressive validation is a standard online validation technique, where each training example is used for testing before it is used for updating the model [3].\nThe following table reports the average and the median, over the datasets, relative improvement in squared loss over the respective base learner. Detailed results can be found in Section B in the supplementary material.\nBase learner Average relative improvement Median relative improvement Algorithm 1 Algorithm 2 Algorithm 1 Algorithm 2\nSGD 1.65% 1.33% 0.03% 0.29% Regression stumps 20.22% 15.9% 10.45% 13.69% Neural networks 7.88% 0.72% 0.72% 0.33%\nNote that both SGD (stochastic gradient descent) and neural networks are already very strong learners. Naturally, boosting is much more effective for regression stumps, which is a weak base learner."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper we generalized the theory of boosting for regression problems to the online setting and provided online boosting algorithms with theoretical convergence guarantees. Our algorithmic technique also improves convergence guarantees for batch boosting algorithms. We also provide experimental evidence that our boosting algorithms do improve prediction accuracy over commonly used base learners in practice, with greater improvements for weaker base learners. The main remaining open question is whether the boosting algorithm for competing with the span of the base functions is optimal in any sense, similar to our proof of optimality for the the boosting algorithm for competing with the convex hull of the base functions."}, {"heading": "B Description of Data Sets and Detailed Experimental Re-", "text": "sults\nThe datasets come from the UCI repository and various KDD Cup challenges. Below, d is the number of unique features in the dataset, and s is the average number of features per example.\nDataset Number of Total number of Average number of Task Label instances features features per example range\na9a 48,841 123 14 classification [\u22121, 1] abalone 4,177 10 9 regression [1, 29] activity 165,632 20 18.5 classification [\u22121, 1] adult 48,842 105 12 classification [0, 1] bank 45,211 45 15 classification [\u22121, 1] cal housing 20,640 9 9 regression [0, 1] casp 45,730 10 10 regression [0, 1] census 299,284 401 32 classification [\u22121, 1] covtype 581,011 54 12 classification [\u22121, 1] kddcup04 (phy) 50,000 74 32 classification [0, 1] letter 20,000 16 15.6 classification [\u22121, 1] shuttle 43,500 9 8 classification [\u22121, 1] slice 53,500 385 135 regression [0, 1] year 463,715 90 90 regression [0, 1]\nThe following table provides the online squared losses summarized in Section 6.\nSGD Regression stumps Neural Networks\nDataset Baseline Alg 1 Alg 2 Baseline Alg 1 Alg 2 Baseline Alg 1 Alg 2\nkddcup04/phy 0.7475 0.7466 0.7470 0.9201 0.7733 0.7924 0.7441 0.7480 0.7446 cal housing 0.0094 0.0094 0.0104 0.0151 0.0138 0.0124 0.0096 0.0096 0.0107 casp 0.0632 0.0631 0.0630 0.0741 0.0741 0.0742 0.0639 0.0632 0.0631 a9a 0.4261 0.4283 0.4249 0.5749 0.5074 0.5758 0.4256 0.4266 0.4246 abalone 3.7263 3.7482 3.7154 6.7791 3.8273 4.2270 3.7380 3.7255 3.7212 activity 0.0334 0.0337 0.0316 0.4492 0.1454 0.3141 0.0192 0.0143 0.0186 adult 0.1055 0.1057 0.1056 0.1388 0.1261 0.1250 0.1081 0.1062 0.1081 bank 0.2971 0.2968 0.2973 0.3774 0.3240 0.3257 0.2962 0.2969 0.2969 census 0.1544 0.1545 0.1553 0.2073 0.1884 0.1789 0.1531 0.1531 0.1523 covtype 0.7256 0.7270 0.7286 0.7910 0.7986 0.7911 0.6807 0.6465 0.6757 letter 0.6441 0.5698 0.6108 0.7420 0.7087 0.7168 0.6542 0.5729 0.6108 shuttle 0.1616 0.1547 0.1577 0.8551 0.3678 0.4354 0.0760 0.0694 0.0802 slice 0.0076 0.0067 0.0065 0.0559 0.0362 0.0410 0.0054 0.0022 0.0044 year 0.0116 0.0119 0.0115 0.0152 0.0140 0.0141 0.0116 0.0119 0.0122"}], "references": [{"title": "AdaBoost is consistent", "author": ["Peter L. Bartlett", "Mikhail Traskin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Optimal and adaptive algorithms for online boosting", "author": ["Alina Beygelzimer", "Satyen Kale", "Haipeng Luo"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Beating the hold-out: Bounds for k-fold and progressive cross-validation", "author": ["Avrim Blum", "Adam Kalai", "John Langford"], "venue": "In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "An Online Boosting Algorithm with Theoretical Justifications", "author": ["Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Boosting with Online Binary Learners for the Multiclass Bandit Problem", "author": ["Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Logistic regression, AdaBoost and Bregman distances", "author": ["Michael Collins", "Robert E. Schapire", "Yoram Singer"], "venue": "In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Boosting methods for regression", "author": ["Nigel Duffy", "David Helmbold"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "An algorithm for quadratic programming", "author": ["Marguerite Frank", "Philip Wolfe"], "venue": "Naval Res. Logis. Quart.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1956}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "On-line boosting and vision", "author": ["Helmut Grabner", "Horst Bischof"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Semi-supervised on-line boosting for robust tracking", "author": ["Helmut Grabner", "Christian Leistner", "Horst Bischof"], "venue": "In ECCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Generalized Additive Models", "author": ["Trevor Hastie", "R. J Robet Tibshirani"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Gradient feature selection for online boosting", "author": ["Xiaoming Liu", "Ting Yu"], "venue": "In ICCV, pages", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["St\u00e9phane G. Mallat", "Zhifeng Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Boosting algorithms as gradient descent", "author": ["Llew Mason", "Jonathan Baxter", "Peter Bartlett", "Marcus Frean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Online bagging and boosting", "author": ["Nikunj C. Oza", "Stuart Russell"], "venue": "In Eighth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Boosting with the logistic loss is consistent", "author": ["Matus Telgarsky"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Boosting with early stopping: Convergence and consistency", "author": ["Tong Zhang", "Bin Yu"], "venue": "Annals of Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}], "referenceMentions": [{"referenceID": 20, "context": "Boosting algorithms [21] are ensemble methods that convert a learning algorithm for a base class of models with weak predictive power, such as decision trees, into a learning algorithm for a class of models with stronger predictive power, such as a weighted majority vote over base models in the case of classification, or a linear combination of base models in the case of regression.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Boosting methods such as AdaBoost [9] and Gradient Boosting [10] have found tremendous practical application, especially using decision trees as the base class of models.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "Boosting methods such as AdaBoost [9] and Gradient Boosting [10] have found tremendous practical application, especially using decision trees as the base class of models.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 10, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 16, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 11, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 3, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 4, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 1, "context": "Indeed, there has already been some work on online boosting for classification problems [20, 11, 17, 12, 4, 5, 2].", "startOffset": 88, "endOffset": 113}, {"referenceID": 3, "context": "[4] provided the first theoretical study of online boosting for classification, which was later generalized by Beygelzimer et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] to obtain optimal and adaptive online boosting algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In this paper, we rigorously formalize the setting of online boosting for regression and then extend the very commonly used gradient boosting methods [10, 19] to the online setting, providing theoretical guarantees on their performance.", "startOffset": 150, "endOffset": 158}, {"referenceID": 18, "context": "In this paper, we rigorously formalize the setting of online boosting for regression and then extend the very commonly used gradient boosting methods [10, 19] to the online setting, providing theoretical guarantees on their performance.", "startOffset": 150, "endOffset": 158}, {"referenceID": 22, "context": "This algorithm is the online analogue of the batch boosting algorithm of Zhang and Yu [24], and in fact our algorithmic technique, when specialized to the batch boosting setting, provides exponentially better convergence guarantees.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "This is a simpler algorithm which is analyzed along the lines of the Frank-Wolfe algorithm [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 20, "context": "1 Related Work While the theory of boosting for classification in the batch setting is well-developed (see [21]), the theory of boosting for regression is comparatively sparse.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "The foundational theory of boosting for regression can be found in the statistics literature [14, 13], where boosting is understood as a greedy stagewise algorithm for fitting of additive models.", "startOffset": 93, "endOffset": 101}, {"referenceID": 12, "context": "The foundational theory of boosting for regression can be found in the statistics literature [14, 13], where boosting is understood as a greedy stagewise algorithm for fitting of additive models.", "startOffset": 93, "endOffset": 101}, {"referenceID": 9, "context": "While the earliest works on boosting for regression such as [10] do not have such convergence proofs, later works such as [19, 6] do have convergence proofs but without a bound on the speed of convergence.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "While the earliest works on boosting for regression such as [10] do not have such convergence proofs, later works such as [19, 6] do have convergence proofs but without a bound on the speed of convergence.", "startOffset": 122, "endOffset": 129}, {"referenceID": 5, "context": "While the earliest works on boosting for regression such as [10] do not have such convergence proofs, later works such as [19, 6] do have convergence proofs but without a bound on the speed of convergence.", "startOffset": 122, "endOffset": 129}, {"referenceID": 6, "context": "Bounds on the speed of convergence have been obtained by Duffy and Helmbold [7] relying on a somewhat strong assumption on the performance of the base learning algorithm.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "A different approach to boosting for regression was taken by Freund and Schapire [9], who give an algorithm that reduces the regression problem to classification and then applies AdaBoost; the corresponding proof of convergence relies on an assumption on the induced classification problem which may be hard to satisfy in practice.", "startOffset": 81, "endOffset": 84}, {"referenceID": 22, "context": "The strongest result is that of Zhang and Yu [24], who prove convergence to the performance of the best linear combination of base functions, along with a bound on the rate of convergence, making essentially no assumptions on the performance of the base learning algorithm.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Telgarsky [22] proves similar results for logistic (or similar) loss using a slightly simpler boosting algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "The results in this paper are a generalization of the results of Zhang and Yu [24] to the online setting.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Indeed, we were not able to directly generalize the analysis in [24] by simply adapting the techniques used in recent online boosting work [4, 2], but we made use of the classical Frank-Wolfe algorithm [8].", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "Indeed, we were not able to directly generalize the analysis in [24] by simply adapting the techniques used in recent online boosting work [4, 2], but we made use of the classical Frank-Wolfe algorithm [8].", "startOffset": 139, "endOffset": 145}, {"referenceID": 1, "context": "Indeed, we were not able to directly generalize the analysis in [24] by simply adapting the techniques used in recent online boosting work [4, 2], but we made use of the classical Frank-Wolfe algorithm [8].", "startOffset": 139, "endOffset": 145}, {"referenceID": 7, "context": "Indeed, we were not able to directly generalize the analysis in [24] by simply adapting the techniques used in recent online boosting work [4, 2], but we made use of the classical Frank-Wolfe algorithm [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 22, "context": "On the other hand, while an important part of the convergence analysis for the batch setting is to show statistical consistency of the algorithms [24, 1, 22], in the online setting we only need to study the empirical convergence (that is, the regret), which makes our analysis much more concise.", "startOffset": 146, "endOffset": 157}, {"referenceID": 0, "context": "On the other hand, while an important part of the convergence analysis for the batch setting is to show statistical consistency of the algorithms [24, 1, 22], in the online setting we only need to study the empirical convergence (that is, the regret), which makes our analysis much more concise.", "startOffset": 146, "endOffset": 157}, {"referenceID": 21, "context": "On the other hand, while an important part of the convergence analysis for the batch setting is to show statistical consistency of the algorithms [24, 1, 22], in the online setting we only need to study the empirical convergence (that is, the regret), which makes our analysis much more concise.", "startOffset": 146, "endOffset": 157}, {"referenceID": 15, "context": "If F is the set of all bounded-norm linear regressors, for example, the algorithm A could be online gradient descent [25] or online Newton Step [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "Convergence rates [24] give bounds on how quickly the approximation error goes to 0 and N \u2192 \u221e.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "The upper bound in this theorem is proved along the lines of the Frank-Wolfe [8] algorithm, and the lower bound using information-theoretic arguments.", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "The domain X = N and true labels of examples are in [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "Consider a sequence of examples (xt, y \u22c6 t ) \u2208 N \u00d7 [0, 1] generated as follows: xt = t, and the label y \u22c6 t is chosen from {p1, p2} uniformly at random in each round.", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "functions of the form l(y) = 1 2 (y \u2212 y) for y \u2208 [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "This is a contradiction, since because X(s) \u2208 [0, 1], we have E1[X(s)]\u2212 E2[X(s)] \u2264 dTV(D1, D2) < 4 \u221a \u01eb2N = 0.", "startOffset": 46, "endOffset": 52}, {"referenceID": 14, "context": "see [15].", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Now, for each i \u2208 [N ], the update in line 14 of Algorithm 1 is exactly online gradient descent [25] on the domain [0, 1] with linear loss functions \u03c3 7\u2192 \u2212\u2207lt(y t ) \u00b7 yi\u22121 t \u03c3.", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "Since 1 W \u2208 [0, 1], the standard analysis of online gradient descent then implies that the sequence \u03c3 t for t = 1, 2, .", "startOffset": 12, "endOffset": 18}, {"referenceID": 9, "context": "In particular, a common heuristic in boosting algorithms such as the original gradient boosting algorithm by Friedman [10] or the matching pursuit algorithm of Mallat and Zhang [18] is to build a linear combination of base functions by iteratively augmenting the current linear combination via greedily choosing a base function and a step size for it that minimizes the loss with respect to the residual label.", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "In particular, a common heuristic in boosting algorithms such as the original gradient boosting algorithm by Friedman [10] or the matching pursuit algorithm of Mallat and Zhang [18] is to build a linear combination of base functions by iteratively augmenting the current linear combination via greedily choosing a base function and a step size for it that minimizes the loss with respect to the residual label.", "startOffset": 177, "endOffset": 181}, {"referenceID": 22, "context": "Indeed, the boosting algorithm of Zhang and Yu [24] also uses this kind of greedy fitting algorithm as the base learner.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "3 Improvements for batch boosting Our algorithmic technique can be easily specialized and modified to the standard batch setting with a fixed batch of training examples and a base learning algorithm operating over the batch, exactly as in [24].", "startOffset": 239, "endOffset": 243}, {"referenceID": 22, "context": "The main difference compared to the algorithm of [24] is the use of the \u03c3 variables to scale the coefficients of the weak hypotheses appropriately.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "While a seemingly innocuous tweak, this allows us to derive analogous bounds to those of Zhang and Yu [24] on the optimization error that show that our boosting algorithm converges exponential faster.", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Progressive validation is a standard online validation technique, where each training example is used for testing before it is used for updating the model [3].", "startOffset": 155, "endOffset": 158}], "year": 2015, "abstractText": "We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.", "creator": "LaTeX with hyperref package"}}}