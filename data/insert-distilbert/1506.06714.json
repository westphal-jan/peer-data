{"id": "1506.06714", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses", "abstract": "we present a novel interactive response generation system that can be trained often end to end on large quantities of unstructured twitter diary conversations. presently a neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical response models, allowing essentially the system to take into account previous intuitive dialog utterances. our dynamic - context generative models show consistent gains over both context - sensitive and non - context - sensitive machine message translation and other information retrieval baselines.", "histories": [["v1", "Mon, 22 Jun 2015 18:29:03 GMT  (355kb,D)", "http://arxiv.org/abs/1506.06714v1", "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proc. of NAACL-HLT. Pages 196-205"]], "COMMENTS": "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proc. of NAACL-HLT. Pages 196-205", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["alessandro sordoni", "michel galley", "michael auli", "chris brockett", "yangfeng ji", "margaret mitchell", "jian-yun nie", "jianfeng gao", "bill dolan"], "accepted": true, "id": "1506.06714"}, "pdf": {"name": "1506.06714.pdf", "metadata": {"source": "CRF", "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses\u2217", "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "emails": ["donia@iro.umontreal.ca)", "ley@microsoft.com)."], "sections": [{"heading": "1 Introduction", "text": "Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive. However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally. The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is \u201ctranslated\u201d into a plausible looking response.\n\u2217This paper appeared in the proceedings of NAACL-HLT 2015 (submitted December 4, 2014, accepted February 20, 2015, and presented June 1, 2015).\n\u2020The entirety of this work was conducted while at Microsoft Research.\n\u2021Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com).\nHowever, an approach such as that presented in Ritter et al. (2011) does not address the challenge of generating responses that are sensitive to the context of the conversation. Broadly speaking, context may be linguistic or involve grounding in the physical or virtual world, but we here focus on linguistic context. The ability to take into account previous utterances is key to building dialog systems that can keep conversations active and engaging. Figure 1 illustrates a typical Twitter dialog where the contextual information is crucial: the phrase \u201cgood luck\u201d is plainly motivated by the reference to \u201cyour game\u201d in the first utterance. In the MT model, such contextual sensitivity is difficult to capture; moreover, naive injection of context information would entail unmanageable growth of the phrase table at the cost of increased sparsity, and skew towards rarely-seen context pairs. In most statistical approaches to machine translation, phrase pairs do not share statistical weights regardless of their intrinsic semantic commonality.\nWe propose to address the challenge of contextsensitive response generation by using continuous representations or embeddings of words and phrases\nar X\niv :1\n50 6.\n06 71\n4v 1\n[ cs\n.C L\n] 2\n2 Ju\nn 20\nto compactly encode semantic and syntactic similarity. We argue that embedding-based models afford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (Ritter et al., 2011). To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010). These models first encode past information in a hidden continuous representation, which is then decoded by the RLM to promote plausible responses that are simultaneously fluent and contextually relevant. Unlike typical complex task-oriented multi-modular dialog systems (Young, 2002; Stent and Bangalore, 2014), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.\nThis paper makes the following contributions. We present a neural network architecture for response generation that is both context-sensitive and datadriven. As such, it can be trained from end to end on massive amounts of social media data. To our knowledge, this is the first application of a neural-network model to open-domain response generation, and we believe that the present work will lay groundwork for more complex models to come. We additionally introduce a novel multi-reference extraction technique that shows promise for automated evaluation."}, {"heading": "2 Related Work", "text": "Our work naturally lies in the path opened by Ritter et al. (2011), but we generalize their approach by exploiting information from a larger context. Ritter et al. and our work represent a radical paradigm shift from other work in dialog. More traditional dialog systems typically tease apart dialog management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be considered a first attempt to accomplish both tasks jointly. While there are previous uses of machine learning for response generation (Walker et al., 2003), dialog state tracking (Young et al., 2010), and user modeling (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural\nnetwork model is completely latent and directly optimized towards end-to-end performance. In this sense, we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing.\nContinuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters.\nOur work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences. We propose a set of conditional RLMs where contextual information (i.e., past utterances) is encoded in a continuous context vector to help generate the response. Our models differ from most previous work in the way the context vector is constructed. For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model. In our models, the context vector is learned along with the conditional RLM that generates the response. Additionally, the learned context encodings do not exclusively capture contentful words. Indeed, even \u201cstop words\u201d can carry discriminative power in this task; for example, all words in the utterance \u201chow are you?\u201d are commonly characterized as stop words, yet this is a contentful dialog utterance."}, {"heading": "3 Recurrent Language Model", "text": "We give a brief overview of the Recurrent Language Model (RLM) (Mikolov et al., 2010) architecture that our models extend. A RLM is a generative model of sentences, i.e., given sentence s = s1, . . . , sT , it\nestimates:\np(s) = T\u220f t=1 p(st|s1, . . . , st\u22121). (1)\nThe model architecture is parameterized by three weight matrices, \u0398RNN = \u3008Win,Wout,Whh\u3009: an input matrixWin, a recurrent matrixWhh and an output matrix Wout, which are usually initialized randomly. The rows of the input matrix Win \u2208 RV\u00d7K contain the K-dimensional embeddings for each word in the language vocabulary of size V . Let us denote by st both the vocabulary token and its one-hot representation, i.e., a zero vector of dimensionality V with a 1 corresponding to the index of the st token. The embedding for st is then obtained by s>t Win. The recurrent matrix Whh \u2208 RK\u00d7K keeps a history of the subsequence that has already been processed. The output matrix Wout \u2208 RK\u00d7V projects the hidden state ht into the output layer ot, which has an entry for each word in the vocabulary V . This value is used to generate a probability distribution for the next word in the sequence. Specifically, the forward pass proceeds with the following recurrence, for t = 1, . . . , T :\nht = \u03c3(s > t Win + h > t\u22121Whh), ot = h > t Wout (2)\nwhere \u03c3 is a non-linear function applied elementwise, in our case the logistic sigmoid. The recurrence is seeded by setting h0 = 0, the zero vector. The probability distribution over the next word given the previous history is obtained by applying the softmax activation function:\nP (st = w|s1, . . . , st\u22121) = exp(otw)\u2211V v=1 exp(otv) . (3)\nThe RLM is trained to minimize the negative loglikelihood of the training sentence s:\nL(s) = \u2212 T\u2211 t=1 logP (st|s1, . . . , st\u22121). (4)\nThe recurrence is unrolled backwards in time using the back-propagation through time (BPTT) algorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps."}, {"heading": "4 Context-Sensitive Models", "text": "We distinguish three linguistic entities in a conversation between two users A and B: the context1 c, the message m and response r. The context c represents a sequence of past dialog exchanges of any length; then B emits a message m to which A reacts by formulating its response r (see Figure 1).\nWe use three context-based generation models to estimate a generation model of the response r, r = r1, . . . , rT , conditioned on past information c and m:\np(r|c,m) = T\u220f t=1 p(rt|r1, . . . , rt\u22121, c,m). (5)\nThese three models differ in the manner in which they compose the context-message pair (c,m)."}, {"heading": "4.1 Tripled Language Model", "text": "In our first model, dubbed RLMT, we straightforwardly concatenate each utterance c, m, r into a single sentence s and train the RLM to minimize L(s). Given c and m, we compute the probability of the response as follows: we perform the forward propagation over the known utterances c andm to obtain a hidden state encoding useful information about previous utterances. Subsequently, we compute the likelihood of the response from that hidden state.\nAn issue with this simple approach is that the concatenated sentence s will be very long on average, especially if the context comprises multiple utterances. Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013). We will consider\n1In this work, the context is purely linguistic, but future work might integrate further contextual information, e.g., geographical location, time information, or other forms of grounding.\nRLMT as an additional context-sensitive baseline for the models we present next."}, {"heading": "4.2 Dynamic-Context Generative Model I", "text": "The above limitation of RLMT can be addressed by strengthening the context bias. In our second model (DCGM-I), the context and the message are encoded into a fixed-length vector representation the is used by the RLM to decode the response. This is illustrated in Figure 3 (left). First, we consider c andm as a single sentence and compute a single bag-of-words representation bcm \u2208 RV . Then, bcm is provided as input to a multilayered non-linear forward architecture that produces a fixed-length representation that is used to bias the recurrent state of the decoder RLM. At training time, both the context encoder and the RLM decoder are learned so as to minimize the negative log-probability of the generated response.\nThe parameters of the model are \u0398DCGM-I = \u3008Win,Whh,Wout, {W `f}L`=1\u3009, where {W `f}L`=1 are the weights for the L layers of the feed-forward context networks. The fixed-length context vector kL is obtained by forward propagation of the network:\nk1 = b > cmW 1 f\nk` = \u03c3(k > `\u22121W ` f ) for ` = 2, \u00b7 \u00b7 \u00b7 , L\n(6)\nThe rows of W 1f contain the embeddings of the vo-\ncabulary.2 These are different from those employed in the RLM and play a crucial role in promoting the specialization of the context encoder to a distinct task. The hidden layer of the decoder RLM takes the following form:\nht = \u03c3(h > t\u22121Whh + kL + s > t Win) (7a)\not = h > t Wout (7b)\np(st+1|s1, . . . , st\u22121, c,m) = softmax(ot) (7c)\nThis model conditions on the previous utterances via biasing the hidden layer state on the context representation kL. Note that the context representation does not change through time. This is useful because: (a) it forces the context encoder to produce a representation general enough to be useful for generating all words in the response and (b) it helps the RLM decoder to remember context information when generating long responses."}, {"heading": "4.3 Dynamic-Context Generative Model II", "text": "Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r. Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right). Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014).\nThe forward equations for the context encoder are:\nk1 = [b > c W 1 f , b > mW 1 f ],\nk` = \u03c3(k > `\u22121W ` f ) for ` = 2, \u00b7 \u00b7 \u00b7 , L\n(8)\nwhere [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7.\n2Notice that the first layer of the encoder network is linear. We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013)."}, {"heading": "5 Experimental Setting", "text": ""}, {"heading": "5.1 Dataset Construction", "text": "For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence. Hence, our dataset is composed of \u201ctriples\u201d \u03c4 \u2261 (c\u03c4 ,m\u03c4 , r\u03c4 ) consisting of three sentences. We mined 127M context-messageresponse triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012. Only those triples where context and response were generated by the same user were extracted. To minimize noise, we selected triples that contained at least one frequent bigram that appeared more than 3 times in the corpus. This produced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3. The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation."}, {"heading": "5.2 Automatic Evaluation", "text": "We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using mined multi-references, BLEU rankings align well with human judgments. This lays groundwork for interesting future correlation studies.\nMulti-reference extraction We use the following algorithm to better cover the space of reasonable responses. Given a test triple \u03c4 \u2261 (c\u03c4 ,m\u03c4 , r\u03c4 ), our\n3The Twitter ids of the tuning and test sets along with the code for the neural network models may be obtained from http://research.microsoft.com/convo/\nCorpus # Triples Avg # Ref [Min,Max] # Ref\ngoal is to mine other responses {r\u03c4\u0303} that fit the context and message pair (c\u03c4 ,m\u03c4 ). To this end, we first select a set of 15 candidate triples {\u03c4\u0303} using an IR system. The IR system is calibrated in order to select candidate triples \u03c4\u0303 for which both the message m\u03c4\u0303 and the response r\u03c4\u0303 are similar to the original message m\u03c4 and response r\u03c4 . Formally, the score of a candidate triple is:\ns(\u03c4\u0303 , \u03c4) = d(m\u03c4\u0303 ,m\u03c4 ) (\u03b1d(r\u03c4\u0303 , r\u03c4 )+(1\u2212\u03b1) ), (9)\nwhere d is the bag-of-words BM25 similarity function (Robertson et al., 1995), \u03b1 controls the impact of the similarity between the responses and is a smoothing factor that avoids zero scores for candidate responses that do not share any words with the reference response. We found that this simple formula provided references that were both diverse and plausible. Given a set of candidate triples {\u03c4\u0303}, human evaluators are asked to rate the quality of the response within the new triples {(c\u03c4 ,m\u03c4 , r\u03c4\u0303 )}. After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively."}, {"heading": "5.3 Feature Sets", "text": "The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets:\nMT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood \u201ctranslation\u201d probabilities, word and\nphrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher\u2019s exact test (Ritter et al., 2011). We also included MT decoder features specifically motivated by the response generation task: Jaccard distance between source and target phrase, Fisher\u2019s exact probability, and a score relating the lengths of source and target phrases.\nIR We also use an IR feature built from an index of triples, whose implementation roughly matches the IRstatus approach described in Ritter et al. (2011): For a test triple \u03c4 , we choose r\u03c4\u0303 as the candidate response iff \u03c4\u0303 = arg max\u03c4\u0303 d(m\u03c4 ,m\u03c4\u0303 ).\nCMM Neither MT nor IR traditionally take into account contextual information. Therefore, we take into consideration context and message matches (CMM), i.e., exact matches between c, m and r. We define 8 features as the [1-4]-gram matches between c and the candidate reply r and the [1-4]-gram matches between m and the candidate reply r. These exact matches help capture and promote contextual information in the replies.\nRLMT, DCGM-I, DCGM-II We consider the RLM trained on the concatenated triples, denoted as RLMT (Section 4.1), to be a context-sensitive RLM baseline. Each neural network model contributes an additional feature corresponding to the likelihood of the candidate response given context and message."}, {"heading": "5.4 Model Training", "text": "The proposed models are trained on a 4M subset of the triple data. The vocabulary consists of the most frequent V = 50K words. In order to speed up training, we use the Noise-Contrastive Estimation (NCE) loss, which avoids repeated summations over V by\napproximating the probability of the target word (Gutmann and Hyva\u0308rinen, 2010). Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate \u03b1 = 0.1, which we found to work well on held-out data. In order to stabilize learning, we clip the gradients to a fixed range [\u221210, 10], as suggested in Mikolov et al. (2010). All the parameters of the neural models are sampled from a normal distribution N (0, 0.01) while the recurrent weight Whh is initialized as a random orthogonal matrix and scaled by 0.01. To prevent over-fitting, we evaluate performance on a held-out set during training and stop when the objective increases. The size of the RLM hidden layer is set to K = 512, where the context encoder is a 512, 256, 512 multilayer network. The bottleneck in the middle compresses context information that leads to similar responses and thus achieves better generalization. The last layer embeds the context vector into the hidden space of the decoder RLM."}, {"heading": "5.5 Rescoring Setup", "text": "We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system. In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues. The different n-best lists will provide a comprehensive testbed for our experiments. First, we augment the n-best list of the tuning set with the scores of the model of interest. Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we rescore the test n-best list with the new weights."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Lower and Upper Bounds", "text": "Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline. The RANDOM system comprises responses randomly extracted from the triples corpus. HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores\n4For the human score, we compute corpus-level BLEU with a sampling scheme that randomly leaves out one reference - the human sentence to score - for each reference set. This sampling scheme (repeated with 100 trials) is also applied for the MT and\nare lower than those usually reported in SMT tasks, the ranking of the three systems is unambiguous."}, {"heading": "6.2 BLEU and METEOR", "text": "The results of automatic evaluation using BLEU and METEOR are presented in Table 3, where some broad patterns emerge. First, both metrics indicate that a phrase-based MT decoder outperforms a purely IR approach. Second, adding CMM features to the baseline systems helps. Third, the neural network models contribute measurably to improvement: RLMT and DCGM models outperform baselines, and DCGM models provide more consistent gains than RLMT.\nMT vs. IR BLEU and METEOR scores indicate that the phrase-based MT decoder outperforms a purely IR approach, despite the fact that IR proposes fluent human generated responses. This may be because the IR model only loosely captures important patterns between message and response: It ranks candidate responses solely by the similarity of their message with the message of the test triple (\u00a75.3). As a result, the top ranked response is likely to drift from the purpose of the original conversation. The MT approach, by contrast, more directly models statistical patterns between message and response.\nCMM MT+CMM, totaling 17 features (9 from MT + 8 CMM), improves 0.38 BLEU points, a 9.5% relative improvement, over the baseline MT model. IR+CMM, with 10 features (IR + word penalty + 8 CMM), benefits even more, attaining 1.8 BLEU points and 1.5 METEOR points over the IR base-\nRANDOM system so as to make BLEU scores comparable.\nline. Figure 4 (a) and (b) plots the magnitude of the learned CMM feature weights for MT+CMM and IR+CMM. CMM features help in both these hypothesis spaces and especially on the IR n-best list. Figure 4 (b) supports the hypothesis formulated in the previous paragraph: Since IR solely captures intermessage similarities, the matches between message and response are important, while context matches help in providing additional gains. The phrase-based statistical patterns captured by the MT system do a good job in explaining away 1-gram and 2-gram message matches (Figure 4 (a)) and the performance gain mainly comes from context matches. On the other hand, we observe that 4-gram matches may be important in selecting appropriate responses. Inspection of the tuning set reveals instances where responses contain long subsequences of their corresponding messages, e.g., m = \u201cgood night best friend, I love you\u201d, r = \u201cI love you too, good night best friend\u201d. Although infrequent, such higher-order n-gram matches, when they occur, may provide a more robust signal of the quality of the response than 1- and 2-gram matches, given the highly conversational nature of our dataset.\nRLMT and DCGM Both RLMT and DCGM models outperform their respective MT and IR baselines. Both models also exhibit similar performance and show improvements over the MT+CMM models, albeit using a lower dimensional feature space. We believe that their similar performance is due to the limited diversity of MT n-best list together with gains in fluency stemming from the strong language model provided by the RLM. In the case of IR models, on the other hand, there is more headroom for improvement and fluency is already guaranteed. Any\ngains must come from context and message matches. Hence, RLMT underperforms with respect to both DCGM and IR+CMM. The DCGM models appear to have better capacity to retain contextual information and thus achieve similar performance to IR+CMM despite their lack of exact n-gram match information.\nIn the present experimental setting, no striking performance difference can be observed between the two versions of the DCGM architecture. If multiple sequences were used as context, we expect that the DCGM-II model would likely benefit more owing to the separate encoding of message and context.\nDCGM+CMM We also investigated whether mixing exact CMM n-gram overlap with semantic information encoded by the DCGM models can bring additional gains. DCGM-{I-II}+CMM systems each totaling 10 features show increases of up to 0.48 BLEU points over MT+CMM and up to 0.88 BLEU over the model based on Ritter et al. (2011). METEOR improvements similarly align with BLEU improvements both for MT and IR lists. We take this\nas evidence that CMM exact matches and DCGM semantic matches interact positively, a finding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embeddings correlate positively with classic co-occurrencebased estimations. Analysis of CMM feature weights in Figure 4 (c) and (d) suggests that 1-gram matches are explained away by the DCGM model, but that higher order matches are important. It appears that DCGM models might be improved by preserving word-order information in context and message encodings."}, {"heading": "6.3 Human Evaluation", "text": "Human evaluation was conducted using crowdsourced annotators. Annotators were asked to compare the quality of system output responses pairwise (\u201cWhich is better?\u201d) in relation to the context and message strings in the 2114 item test set. Identical strings were held out, so that the annotators only saw those outputs that differed. Paired responses were presented in random order to the annotators, and each pair of responses was judged by 5 annotators.\nTable 4 summarizes the results of human evaluation, giving the difference in mean scores (pairwise preference margin) between systems and 95% confidence intervals generated using Welch\u2019s t-test. Identical strings not shown to raters are incorporated with an automatically assigned score of 0.5. The pattern in these results is clear and consistent: context-sensitive systems (+CMM) outperform non-context-sensitive systems, with preference gains as high as approximately 5.3% in the case of DCGM-II+CMM versus IR, and about 3.1% in the case of DCGM-II+CMM versus MT. Similarly, context-sensitive DCGM systems outperform non-DCGM context-sensitive systems by 1.5% (MT) and 2.3% (IR). These results are consistent with the automated BLEU rankings and confirm that our best performing DCGM models outperform both raw baseline and the context-sensitive baseline using CMM features."}, {"heading": "6.4 Discussion", "text": "Table 5 provides examples of responses generated on the tuning corpus by the MT-based DCGM-II+CMM system, our best system in terms of both BLEU and human evaluation. Responses from this system are on average shorter (8.95 tokens) than the original human\n# Context c Message m Generated Response r\n1 noone can help this time its just i need a support system well im here to support you. whenever you need me. thank you, i appreciate it thank you\n2 nothin much, and how\u2019s the book?! its good but i\u2019m only like halfway through cuz i don\u2019t feel like reading. i\u2019m so bored ... that\u2019s good! i have the book but i\u2019m bored too.\n3 i know! what have you been up to? nothing really. just enjoying summer and working. what about you?\nthat\u2019s good! nothing much just listening to music and you?\n4 thanks, your way better though i\u2019m just good at expressing it that\u2019s but trust me you are freakin amazing and waay better i know, i don\u2019t trust you, i love you too! thank you\n5 thanks. yeah i am good thanks! good, but angry! sorry you\u2019re angry, been there, i hope it dies down. i hope so too, i\u2019m not angry!!\n6 figures. bass. never ate bass. bass is extremely mild with very few bones. my favorite after catfish that\u2019s one of the bones, chuck bass is mine too.\n7 today i will walk for miles. clear sky clear mind. love walking. it\u2019s too hot to walk this month. i haven\u2019t moved. frozen. heat freeze. it\u2019s too hot for you to get up to honey if you have frozen yogurt to you.\n8 arresting people over \u201cnasty tweets\u201d? please. i\u2019m not condoning what the lad did but arresting him is a tad silly.\nit appears he may have sent death threats to others while all this was kicking off too. i did! he was armed with nuclear threats? that\u2019s what\u2019s happening to you.\nTable 5: Sample responses produced by the MT-based DCGM-II+CMM system.\nresponses in the tuning set (11.5 tokens). Overall, the outputs tend to be generic or commonplace, but are often reasonably plausible in the context as in examples 1-3, especially where context and message contain common conversational elements. Example 2 illustrates the impact of context-sensitivity: the word \u201cbook\u201d in the response is not found in the message. Nonetheless, longer generated responses are apt to degrade both syntactically and in terms of content. We notice that longer responses are likely to present information that conflicts either internally within the response itself, or is at odds with the context, as in examples 4-5. This is not unsurprising, since our model lacks mechanisms both for reflecting agent intent in the response and for maintaining consistency with respect to sentiment polarity. Longer context and message components may also result in responses that wander off-topic or lapse into incoherence as in 6-8, especially when relatively low frequency unigrams (\u201cbass\u201d, \u201cthreat\u201d) are echoed in the response. In general, we expect that larger datasets and incorporation of more extensive contexts into the model will help yield more coherent results in these cases. Consistent representation of agent intent is outside the scope of this work, but will likely remain a significant challenge."}, {"heading": "7 Conclusion", "text": "We have formulated a neural network architecture for data-driven response generation trained from social media conversations, in which generation of responses is conditioned on past dialog utterances that provide contextual information. We have proposed a novel multi-reference extraction technique allowing for robust automated evaluation using standard SMT metrics such as BLEU and METEOR. Our context-sensitive models consistently outperform both context-independent and context-sensitive baselines by up to 11% relative improvement in BLEU in the MT setting and 24% in the IR setting, albeit using a minimal number of features. As our models are completely data-driven and self-contained, they hold the potential to improve fluency and contextual relevance in other types of dialog systems.\nOur work suggests several directions for future research. We anticipate that there is much room for improvement if we employ more complex neural network models that take into account word order within the message and context utterances. Direct generation from neural network models is an interesting and potentially promising next step. Future progress in this area will also greatly benefit from thorough study of automated evaluation metrics."}, {"heading": "Acknowledgments", "text": "We thank Alan Ritter, Ray Mooney, Chris Quirk, Lucy Vanderwende, Susan Hendrich and Mouni Reddy for helpful discussions, as well as the three anonymous reviewers for their comments."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journ. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "Proc. of EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proc. of ACL", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journ. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014a] Jianfeng Gao", "Xiaodong He", "Wen tau Yih", "Li Deng"], "venue": "In Proc. of ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014b] Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "User simulation for spoken dialogue systems: Learning and evaluation", "author": ["James Henderson", "Oliver Lemon"], "venue": "In Proc. of Interspeech/ICSLP", "citeRegEx": "Georgila et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Georgila et al\\.", "year": 2006}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Hyv\u00e4rinen2010] Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proc. of AISTATS,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. of CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "Proc. of EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Improved backing-off for M-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Context Dependent Recurrent Neural Network Language Model", "author": ["Mikolov", "Zweig2012] Tomas Mikolov", "Geoffrey Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. of INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "The alignment template approach to machine translation", "author": ["Och", "Ney2004] Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Och et al\\.", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proc. of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "Proc. of ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B. Dolan"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Neurocomputing: Foundations of Research,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proc. of CIKM,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Natural Language Generation in Interactive Systems", "author": ["Stent", "Bangalore2014] Amanda Stent", "Srinivas Bangalore"], "venue": null, "citeRegEx": "Stent et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "Proc. of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Rashmi Prasad", "Amanda Stent"], "venue": "In Proc. of EUROSPEECH", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Young et al.2010] Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Talking to machines (statistically speaking)", "author": ["Steve Young"], "venue": "In Proc. of INTERSPEECH", "citeRegEx": "Young.,? \\Q2002\\E", "shortCiteRegEx": "Young.", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is \u201ctranslated\u201d into a plausible looking response.", "startOffset": 12, "endOffset": 33}, {"referenceID": 20, "context": "However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of generating responses that are sensitive to the context of the conversation.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "We argue that embedding-based models afford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (Ritter et al., 2011).", "startOffset": 219, "endOffset": 240}, {"referenceID": 15, "context": "To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010).", "startOffset": 157, "endOffset": 179}, {"referenceID": 27, "context": "Unlike typical complex task-oriented multi-modular dialog systems (Young, 2002; Stent and Bangalore, 2014), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.", "startOffset": 66, "endOffset": 106}, {"referenceID": 27, "context": "More traditional dialog systems typically tease apart dialog management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be considered a first attempt to accomplish both tasks jointly.", "startOffset": 72, "endOffset": 85}, {"referenceID": 25, "context": "While there are previous uses of machine learning for response generation (Walker et al., 2003), dialog state tracking (Young et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 26, "context": ", 2003), dialog state tracking (Young et al., 2010), and user modeling (Georgila et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 9, "context": ", 2010), and user modeling (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular, the labels and attributes defining dialog states.", "startOffset": 27, "endOffset": 50}, {"referenceID": 19, "context": "Our work naturally lies in the path opened by Ritter et al. (2011), but we generalize their approach by exploiting information from a larger context.", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al.", "startOffset": 163, "endOffset": 202}, {"referenceID": 22, "context": "Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al.", "startOffset": 163, "endOffset": 202}, {"referenceID": 0, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 3, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 24, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.", "startOffset": 35, "endOffset": 128}, {"referenceID": 2, "context": ", 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).", "startOffset": 36, "endOffset": 85}, {"referenceID": 0, "context": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems.", "startOffset": 36, "endOffset": 227}, {"referenceID": 15, "context": "Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences.", "startOffset": 70, "endOffset": 92}, {"referenceID": 13, "context": "Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences. We propose a set of conditional RLMs where contextual information (i.e., past utterances) is encoded in a continuous context vector to help generate the response. Our models differ from most previous work in the way the context vector is constructed. For example, Mikolov and Zweig (2012) and Auli et al.", "startOffset": 71, "endOffset": 489}, {"referenceID": 0, "context": "For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model.", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "We give a brief overview of the Recurrent Language Model (RLM) (Mikolov et al., 2010) architecture that our models extend.", "startOffset": 63, "endOffset": 85}, {"referenceID": 21, "context": "The recurrence is unrolled backwards in time using the back-propagation through time (BPTT) algorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps.", "startOffset": 102, "endOffset": 126}, {"referenceID": 19, "context": "Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013).", "startOffset": 104, "endOffset": 126}, {"referenceID": 19, "context": "We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013).", "startOffset": 154, "endOffset": 176}, {"referenceID": 18, "context": "We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.", "startOffset": 35, "endOffset": 58}, {"referenceID": 20, "context": "MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 20, "context": ", long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher\u2019s exact test (Ritter et al., 2011).", "startOffset": 107, "endOffset": 128}, {"referenceID": 20, "context": "IR We also use an IR feature built from an index of triples, whose implementation roughly matches the IRstatus approach described in Ritter et al. (2011): For a test triple \u03c4 , we choose r\u03c4\u0303 as the candidate response iff \u03c4\u0303 = arg max\u03c4\u0303 d(m\u03c4 ,m\u03c4\u0303 ).", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate \u03b1 = 0.", "startOffset": 45, "endOffset": 65}, {"referenceID": 6, "context": "Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate \u03b1 = 0.1, which we found to work well on held-out data. In order to stabilize learning, we clip the gradients to a fixed range [\u221210, 10], as suggested in Mikolov et al. (2010). All the parameters of the neural models are sampled from a normal distribution N (0, 0.", "startOffset": 46, "endOffset": 291}, {"referenceID": 17, "context": "Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features.", "startOffset": 34, "endOffset": 45}, {"referenceID": 18, "context": "88 BLEU over the model based on Ritter et al. (2011). METEOR improvements similarly align with BLEU improvements both for MT and IR lists.", "startOffset": 32, "endOffset": 53}, {"referenceID": 7, "context": "We take this as evidence that CMM exact matches and DCGM semantic matches interact positively, a finding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embeddings correlate positively with classic co-occurrencebased estimations.", "startOffset": 124, "endOffset": 143}], "year": 2015, "abstractText": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.", "creator": "LaTeX with hyperref package"}}}