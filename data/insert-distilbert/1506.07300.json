{"id": "1506.07300", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications", "abstract": "the computational cost approach of many efficient signal data processing and machine learning techniques is often entirely dominated by the recreational cost of applying certain linear operators to high - dimensional vectors. this 1986 paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension constraints by approximately factorizing the corresponding matrix into few sparse factors. the approach relies on recent advances in non - convex optimization. it is partially first explained and analyzed in details and then demonstrated experimentally on addressing various problems including dictionary learning for image denoising, and the approximation validity of large matrices arising in inverse problems.", "histories": [["v1", "Wed, 24 Jun 2015 10:02:13 GMT  (570kb,D)", "https://arxiv.org/abs/1506.07300v1", null], ["v2", "Tue, 29 Mar 2016 07:56:08 GMT  (491kb,D)", "http://arxiv.org/abs/1506.07300v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luc le magoarou", "r\\'emi gribonval"], "accepted": false, "id": "1506.07300"}, "pdf": {"name": "1506.07300.pdf", "metadata": {"source": "CRF", "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications", "authors": ["Luc Le Magoarou", "R\u00e9mi Gribonval"], "emails": ["(luc.le-magoarou@inria.fr)", "(remi.gribonval@inria.fr)", "pubs-permissions@ieee.org."], "sections": [{"heading": null, "text": "Index Terms\u2014Sparse representations, fast algorithms, dictionary learning, low complexity, image denoising, inverse problems.\nI. INTRODUCTION\nSPARSITY has been at the heart of a plethora of signalprocessing and data analysis techniques over the last two decades. These techniques usually impose that the objects of interest be sparse in a certain domain. They owe their success to the fact that sparse objects are easier to manipulate and more prone to interpretation than dense ones especially in high dimension. However, to efficiently manipulate highdimensional data, it is not sufficient to rely on sparse objects: efficient operators are also needed to manipulate these objects.\nThe n-dimensional Discrete Fourier Transform (DFT) is certainly the most well known linear operator with an efficient implementation: the Fast Fourier Transform (FFT) [3], allows to apply the operator in O(n log n) arithmetic operations instead of O(n2) in its dense form. Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6]. For all these fast linear transforms, the matrix A corresponding to the dense form of the operator admits a multi-layer sparse expression,\nA = J\u220f j=1 Sj , (1)\ncorresponding to a multi-layer factorization1 into a small number J of sparse factors Sj . Following the definition of a linear\nLuc Le Magoarou (luc.le-magoarou@inria.fr) and Re\u0301mi Gribonval (remi.gribonval@inria.fr) are both with Inria, Rennes, France, PANAMA team. This work was supported in part by the European Research Council, PLEASE project (ERC-StG- 2011-277906). Parts of this work have been presented at the conferences ICASSP 2015 [1] and EUSIPCO 2015 [2]. Copyright (c) 2014 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\n1The product being taken from right to left: \u220fJ\nj=1 Sj = SJ \u00b7 \u00b7 \u00b7S1\nalgorithm given in [7], this multi-layer sparse factorization is actually the natural representation of any fast linear transform.\nFor example each step of the butterfly radix-2 FFT can be seen as the multiplication by a sparse matrix having only two non-zero entries per row and per column. This fact is further illustrated in the case of the Hadamard transform on Figure 1. For other examples, see e.g. [1, Appendix A].\nInspired by these widely used transforms, our objective is to find approximations of operators of interest encountered in concrete applications, as products of sparse matrices as in (1). Such approximations will be called Flexible Approximate MUlti-layer Sparse Transforms (FA\u00b5ST).\nAs a primary example of potential application of such approximations, consider linear inverse problems, where data and model parameters are linked through a linear operator. State of the art algorithms addressing such problems with sparse regularization [8]\u2013[12] are known to heavily rely on matrix-vector products involving both this operator and its adjoint. As illustrated in Section V on a biomedical inverse problem, replacing the operator by an accurate FA\u00b5ST has the potential to substantially accelerate these methods.\nTo choose a regularizer for inverse problems, dictionary learning is a common method used to learn the domain in which some training data admits a sparse representation [13]. Its applicability is however also somewhat limited by the need to compute many matrix-vector products involving the learned dictionary and its adjoint, which are in general dense matrices. We will see that recent approaches to learn fast dictionaries [14], [15] can be seen as special cases of the FA\u00b5ST dictionary learning approach developed in Section VI, where the learned dictionaries are constrained to be FA\u00b5ST.\nBeyond the above considered examples, any task where it is required to apply a linear operator in high dimension would obviously benefit from a FA\u00b5ST corresponding to the considered operator. For example, in the emerging area of signal processing on graphs [16], novel definitions of usual operators such as the Fourier or wavelet transforms have been\nar X\niv :1\n50 6.\n07 30\n0v 2\n[ cs\n.L G\n] 2\n9 M\nar 2\n01 6\nintroduced. They have no known general sparse forms, and consequently no associated fast algorithms. Finding multilayer sparse approximations of these usual operators on graphs would certainly boost the dissemination and impact of graph signal processing techniques. Objective. The quest for multi-layer sparse approximations of large linear operators, which is the core objective of this paper, actually amounts to a matrix factorization problem, where the matrix A corresponding to the dense form of the operator is to be decomposed into the product of sparse factors Sj , so as to satisfy an approximate form of (1). Contributions. This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions: \u2022 A general framework for multi-layer sparse approxima-\ntion (MSA) is introduced, that allows to incorporate various constraints on the sought sparse form; \u2022 Recent advances in non-convex optimization [18] are exploited to tackle the resulting non-convex optimization problem with local convergence guarantees; \u2022 A heuristic hierarchical factorization algorithm leveraging these optimization techniques is proposed, that achieves factorizations empirically stable to initialization; \u2022 The versatility of the framework is illustrated with extensive experiments on two showcase applications, linear inverse problems and dictionary learning, demonstrating its practical benefits.\nThe remaining of the paper is organized as follows. The problem is formulated, related to prior art and the expected benefits of FA\u00b5STs are systematically explained in Section II. A general optimization framework for the induced matrix factorization problem is introduced in Section III and Section IV, and as a first illustration we demonstrate that it is possible to reverse-engineer the Hadamard transform. Several applications and experiments on various tasks, illustrating the versatility of the proposed approach are performed in sections V and VI."}, {"heading": "II. PROBLEM FORMULATION", "text": "Notation. Throughout this paper, matrices are denoted by bold upper-case letters: A; vectors by bold lower-case letters: a; the ith column of a matrix A by: ai; and sets by calligraphic symbols: A. The standard vectorization operator is denoted by vec(\u00b7). The `0-norm is denoted by \u2016\u00b7\u20160 (it counts the number of non-zero entries), \u2016\u00b7\u2016F denotes the Frobenius norm, and \u2016\u00b7\u20162 the spectral norm. By abuse of notations, \u2016A\u20160 = \u2016vec(A)\u20160. The identity matrix is denoted Id."}, {"heading": "A. Objective", "text": "The goal of this paper is to introduce a method to get a FA\u00b5ST associated to an operator of interest. Consider a linear operator corresponding to the matrix A \u2208 Rm\u00d7n. The objective is to find sparse factors Sj \u2208 Raj+1\u00d7aj , j \u2208 {1 . . . J} with a1 = n and aJ+1 = m such that A \u2248 \u220fJ j=1 Sj . This naturally leads to an optimization problem of the form:\nMinimize S1,...,SJ\n\u2225\u2225A\u2212 J\u220f j=1 Sj \u2225\u22252\n\ufe38 \ufe37\ufe37 \ufe38 Data fidelity\n+\nJ\u2211 j=1\ngj(Sj)\ufe38 \ufe37\ufe37 \ufe38 Sparsity-inducing penalty , (2)\nto trade-off data fidelity and sparsity of the factors."}, {"heading": "B. Expected benefits of FA\u00b5STs", "text": "A multi-layer sparse approximation of an operator A brings several benefits, provided the relative complexity of the factorized form is small with respect to the dimensions of A. For the sake of conciseness, let us introduce sj = \u2016Sj\u20160 the total amount of non-zero entries in the jth factor, and stot = \u2211J j=1 sj the total number of non-zero entries in the whole factorization.\nDefinition II.1. The Relative Complexity (abbreviated RC) is the ratio between the total number of non-zero entries in the FA\u00b5ST and the number of non-zero entries of A:\nRC := stot \u2016A\u20160 . (3)\nIt is also interesting to introduce the Relative Complexity Gain (RCG), which is simply the inverse of the Relative Complexity (RCG = 1/RC).\nThe aforementioned condition for the factorized form to be beneficial writes: RC 1 or equivalently RCG 1.\nFA\u00b5STs reduce computational costs in all aspects of their manipulation, namely a lower Storage cost, a higher Speed of multiplication and an improved Statistical significance.\n1) Storage cost: Using the Coordinate list (COO) storage paradigm [19], one can store a FA\u00b5ST using O(stot) floats and integers. Indeed each non-zero entry (float) in the factorization can be located using three integers (one for the factor, one for the row and one for the column), which makes stot floats and 3stot integers to store. One needs also J + 1 supplementary integers to denote the size of the factors a1 to aJ+1. In summary the storage gain is of the order of RCG.\n2) Speed of multiplication: Applying the FA\u00b5ST or its transpose to a vector v \u2208 Rn can be easily seen to require at most O(stot) floating point operations (flops), instead of O(mn) for a classical dense operator of same dimension, so the computational gain is, like the storage gain, of the order of RCG.\n3) Statistical significance: Another interesting though less obvious benefit of FA\u00b5STs over dense operators arises when the operator has to be estimated from training data as in dictionary learning. In this case the reduced number of parameters to learn \u2013O(stot) compared to O(mn) for dense operators\u2013 leads to better statistical properties. More specifically, the sample complexity is reduced [20], and better generalization properties are expected. The sample complexity gain is of the order of RCG, as will be shown in the case of dictionary learning. The impact of these gains will be illustrated experimentally in section VI on image denoising with learned dictionaries."}, {"heading": "C. Related work", "text": "Similar matrix factorization problems have been studied in several domains. Some are very classical tools from numerical linear algebra, such as the truncated SVD, while other emerged more recently in signal processing and machine learning.\n1) The truncated SVD: To reduce the computational complexity of a linear operator, the most classical approach is perhaps to compute a low-rank approximations with the truncated SVD. Figure 2 compares the approximation-complexity trade-offs achieved via a low-rank approximation (truncated SVD) and via a multi-layer sparse approximation, on a forward operator associated to an MEG inverse problem. The truncated SVD and four FA\u00b5STs computed using different configurations (more details in Section V - Figure 8) are compared in terms of relative operator norm error: \u2016A\u2212 A\u0302\u20162/\u2016A\u20162. It is readily observed that the FA\u00b5STs achieve significantly better complexity/error trade-offs.\n2) Local low-rank approximations: Given the limitations of global low-rank approximation by truncated SVD illustrated in Figure 2, the numerical linear algebra community has developed local approximations of operators by low-rank patches. This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23]. All the difficulty of these methods resides in the choice of the patches, which is done according to the regularity of the subsumed continuous kernel. This can be seen as approximating the operator by a FA\u00b5ST, where the support of each factor is determined analytically. The approach proposed in this paper is data-driven rather than analytic.\n3) Wavelet-based compression: This operator compression paradigm, introduced in [24], is based on the use of orthogonal wavelets in the column domain (associated to a matrix \u03a61), and in the row domain (associated to a matrix \u03a62) of the matrix A to approximate. By orthogonality of the matrices \u03a61 and \u03a62, we have A = \u03a61\u03a6T1 A\u03a62\u03a6 T 2 . The wavelet-based compression scheme relies on the fact that B , \u03a6T1 A\u03a62 is compressible (provided appropriate wavelets are chosen relative to the subsumed kernel). This implies B \u2248 B\u0302 where B\u0302 is sparse so that A \u2248 A\u0302 = \u03a61B\u0302\u03a6T2 . Fast multiplication by A\u0302 is possible as soon as B\u0302 is sparse enough and the wavelet transforms \u03a61 and \u03a62 have fast implementations. This can be seen as approximating A by a FA\u00b5ST.\n4) Dictionary learning: Given a collection of training vectors y`, 1 \u2264 ` \u2264 L gathered as the columns of a matrix Y, the objective of dictionary learning [13], [25] is to approximate Y by the product of a dictionary D and a coefficients matrix \u0393 with sparse columns, Y \u2248 D\u0393.\nTo learn dictionaries with improved computational efficiency, two main lines of work have begun to explore approaches related to multi-layer sparse approximation. In [14],\nthe authors propose the sparse-KSVD algorithm (KSVDS) to learn a dictionary whose atoms are sparse linear combinations of atoms of a so-called base dictionary Dbase. The base dictionary should be associated with a fast algorithm (in practice, this means that it is a FA\u00b5ST) so that the whole learned dictionary is itself a FA\u00b5ST. It can be seen as having the J\u22121 leftmost factors fixed in (2), their product being precisely Dbase, while the first factor S1 is the sparse representation of the dictionary over the base dictionary, i.e., D = DbaseS1.\nA limitation of the sparse-KSVD formulation is that the learned dictionary is highly biased toward the base dictionary, which decreases adaptability to the training data. In [15], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions using sparse kernels with known supports, so that the dictionary is a sparse operator that is fast to manipulate. This problem can be seen as (2), with the penalties gjs associated to the J \u2212 1 leftmost factors imposing sparse circulant matrices with prescribed supports. This formulation is powerful, as demonstrated in [15], but limited in nature to the case where the dictionary is well approximated by a product of sparse circulant matrices, and requires knowledge of the supports of the sparse factors.\n5) Inverse problems: In the context of sparse regularization of linear inverse problems, one is given a signal y and a measurement matrix M and wishes to compute a sparse code \u03b3 such that y \u2248 M\u03b3, see e.g. [26]. Most modern sparse solvers rely on some form of iterative thresholding and heavily rely on matrix-vector products with the measurement matrix and its transpose. Imposing \u2013and adjusting\u2013 a FA\u00b5ST structure to approximate these matrices as proposed here has the potential to further accelerate these methods through fast matrix-vector multiplications. This is also likely to bring additional speedups to recent approaches accelerating iterative sparse solvers through learning [27]\u2013[29].\n6) Statistics \u2013 factor analysis: A related problem is to approximately diagonalize a covariance matrix by a unitary matrix in factorized form (1), which can be addressed greedily [30], [31] using a fixed number of elementary Givens rotations. Here we consider a richer family of sparse factors and leverage recent non-convex optimization techniques.\n7) Machine learning: Similar models were explored with various points of view in machine learning. For example, sparse multi-factor NMF [32] can be seen as solving problem (2) with the Kullback-Leibler divergence as data fidelity term and all factors Sjs constrained to be non-negative. Optimization relies on multiplicative updates, while the approach proposed here relies on proximal iterations.\n8) Deep learning: In the context of deep neural networks, identifiability guarantees on the network structure have been established with a generative model where consecutive network layers are sparsely connected at random, and nonlinearities are neglected [33], [34]. The network structure in these studies matches the factorized structure (1), with each of the leftmost factors representing a layer of the network and the last one being its input. Apart from its hierarchical flavor, the identification algorithm in [33], [34] has little in common with the proximal method proposed here.\n9) Signal processing on graphs: Similar matrix factorizations problems arise in this domain, with the objective of defining wavelets on graphs. First, in [35] the authors propose to approximately diagonalize part of the graph Laplacian operator using elementary rotations. More precisely, the basis in which the Laplacian is expressed is greedily changed, requiring that at each step the change is made by a sparse elementary rotation (so that the wavelet transform is multilayer sparse), and variables are decorrelated. The Laplacian ends up being diagonal in all the dimensions corresponding to the wavelets and dense in a small part corresponding to the scaling function (the algorithm ends up being very similar to the one proposed in [31]). Second, in [36] the authors propose to define data adaptive wavelets by factorizing some training data matrix made of signals on the graph of interest. The constraint they impose to the wavelet operator results in a multi-layer sparse structure, where each sparse factor is further constrained to be a lifted wavelet building block. The optimization algorithm they propose relies on deep learning techniques, more precisely layer-wise training of stacked autoencoders [37]."}, {"heading": "III. OPTIMIZATION FRAMEWORK", "text": ""}, {"heading": "A. Objective function", "text": "In this paper, the penalties gj(\u00b7) appearing in the general form of the optimization problem (2) are chosen as indicator functions \u03b4Ej (\u00b7) of constraint sets of interest Ej . To avoid the scaling ambiguities arising naturally when the constraint sets are (positively) homogeneous2, it is common [15], [32] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term. For that, let us introduce the sets of normalized factors Nj = {S \u2208 Raj+1\u00d7aj : \u2016S\u2016F = 1}, and impose the following form for the constraints sets: Ej = Nj \u2229 Sj , where Sj imposes sparsity explicitly or implicitly. This results in the following optimization problem:\nMinimize \u03bb,S1,...,SJ \u03a8(S1, . . . ,SJ , \u03bb) := 1 2 \u2225\u2225\u2225A\u2212 \u03bb J\u220f j=1 Sj \u2225\u2225\u22252 F\n+ J\u2211 j=1 \u03b4Ej (Sj).\n(4) As will be made clear below, the used minimization algorithm relies on projections onto the constraint sets Ej : the choice of the \u201csparsity-inducing\u201d part of the constraint sets Sj is quite free provided that the projection operator onto these sets is known.\nA comon choice is to limit the total number of non-zero entries in the factors to sj . The constraint sets then take the form Ej = {S \u2208 Raj+1\u00d7aj : \u2016S\u20160 \u2264 sj , \u2016S\u2016F = 1}. Another natural choice is to limit to kj the number of nonzero entries per row or column in the factors, which gives for example in the case of the columns Ej = {S \u2208 Raj+1\u00d7aj : \u2016si\u20160 \u2264 kj \u2200i, \u2016S\u2016F = 1}. Other possible constraint sets can be chosen to further impose non-negativity, a circulant structure, a prescribed support, etc., see for example [15].\n2This is the case of many standard constraint sets. In particular all unions of subspaces, such as the sets of sparse or low-rank matrices, are homogeneous.\nBesides the few examples given above, many more choices of penalties beyond indicator functions of constraint sets can be envisioned in the algorithmic framework described below. Their choice is merely driven by the application of interest, as long as they are endowed with easy to compute projections onto the constraint sets (in fact, efficient proximal operators), and satisfy some technical assumptions (detailed below) that are very often met in practice. We leave the full exploration of this rich field and its possible applications to further work."}, {"heading": "B. Algorithm overview", "text": "Problem (4) is highly non-convex, and the sparsity-inducing penalties are typically non-smooth. Stemming on recent advances in non-convex optimization, it is nevertheless possible to propose an algorithm with convergence guarantees to a stationary point of the problem. In [18], the authors consider cost functions depending on N blocks of variables of the form:\n\u03a6(x1, . . . ,xN ) := H(x1, . . . ,xN ) + N\u2211 j=1 fj(xj), (5)\nwhere the function H is smooth, and the penalties fjs are proper and lower semi-continuous (the exact assumptions are given below). It is to be stressed that no convexity of any kind is assumed. Here, we assume for simplicity that the penalties fjs are indicator functions of constraint sets Tj . To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM) [18], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step. The structure of the PALM algorithm is given in Figure 3, where PTj (\u00b7) is the projection operator onto the set Tj and cij defines the step size and depends on the Lipschitz constant of the gradient of H .\nThe following conditions are sufficient (not necessary) to ensure that each bounded sequence generated by PALM converges to a stationary point of its objective [18, Theorem 3.1] (the sequence converges, which implies convergence of the value of the cost function):\n(i) The fjs are proper and lower semi-continuous. (ii) H is smooth.\n(iii) \u03a6 is semi-algebraic [18, Definition 5.1]. (iv) \u2207xjH is globally Lipschitz for all j, with Lipschitz\nmoduli Lj(x1. . .xj\u22121,xj+1. . .xN ). (v) \u2200i, j, cij > Lj(x i+1 1 . . .x i+1 j\u22121,x i j+1. . .x i N ) (the inequality\nneed not be strict for convex fj)."}, {"heading": "C. Algorithm details", "text": "PALM can be instantiated for the purpose of handling the objective of (4). It is quite straightforward to see that there is\na match between (4) and (5) by taking N = J + 1, xj = Sj for j \u2208 {1 . . . J}, xM+1 = \u03bb, H as the data fidelity term, fj(\u00b7) = \u03b4Ej (.) for j \u2208 {1 . . . J} and fJ+1(\u00b7) = \u03b4EJ+1(\u00b7) = \u03b4R(\u00b7) = 0 (there is no constraint on \u03bb). This match allows to apply PALM to compute multi-layer sparse approximations, with guaranteed convergence to a stationary point.\n1) Projection operator: PALM relies on projections onto the constraint sets for each factor at each iteration, so the projection operator should be simple and easy to compute. For example, in the case where the Ejs are sets of sparse normalized matrices, namely Ej = {S \u2208 Raj\u00d7aj+1 : \u2016vec(S)\u20160 \u2264 sj , \u2016S\u2016F = 1} for j \u2208 {1 . . . J}, then the projection operator PEj (\u00b7) simply keeps the sj greatest entries (in absolute value) of its argument, sets all the other entries to zero, and then normalizes its argument so that it has unit norm (the proof is given in Appendix A). Regarding EJ+1 = R, the projection operator is the identity mapping. The projection operators for other forms of sparsity constraints that could be interesting in concrete applications are also given in Appendix A: Proposition A.1 covers the following examples: \u2022 Global sparsity constraints. \u2022 Row or column sparsity constraints. \u2022 constrained support. \u2022 Triangular matrices constraints. \u2022 Diagonal matrices constraints.\nProposition A.2 covers in addition: \u2022 Circulant, Toeplitz or Hankel matrices with fixed support\nor prescribed sparsity. \u2022 Matrices that are constant by row or column. \u2022 More general classes of piece-wise constant matrices with\npossible sparsity constraints. 2) Gradient and Lipschitz modulus: To specify the iterations of PALM specialized to the multi-layer sparse approximation problem, let us fix the iteration i and the factor j, and denote Sij the factor being updated, L := \u220fJ `=j+1 S i ` what is\non its left and R := \u220fj\u22121 `=1 S i+1 ` what is on its right (with the\nconvention \u220f `\u2208\u2205 S` = Id). These notations give, when updating the jth factor Sij : H(S i+1 1 , . . . ,S i+1 j\u22121,S i j , . . . ,S i J , \u03bb i) = H(L,Sij ,R, \u03bb i) = 12\u2016A \u2212 \u03bb\niLSijR\u20162F . The gradient of this smooth part of the objective with respect to the jth factor reads:\n\u2207SijH(L,S i j ,R, \u03bb i) = \u03bbiLT (\u03bbiLSijR\u2212A)RT , which Lipschitz modulus with respect to \u2225\u2225Sij\u2225\u2225F is Lj(L,R, \u03bb i) = (\u03bbi)2 \u2016R\u201622 . \u2016L\u2016 2 2 (as shown in Appendix B). Once all the J factors are updated, let us now turn to the update of \u03bb. Denoting A\u0302 = \u220fJ j=1 S i+1 j brings:\nH(Si+11 , . . . ,S i+1 J , \u03bb i) = 12\u2016A\u2212 \u03bb iA\u0302\u20162F , and the gradient\nwith respect to \u03bbi reads:\n\u2207\u03bbiH(Si+11 , . . . ,S i+1 J , \u03bb i) = \u03bbiTr(A\u0302T A\u0302)\u2212 Tr(AT A\u0302).\n3) Default initialization, and choice of the step size: Except when specified otherwise, the default initialization is with \u03bb0 = 1, S01 = 0, and S 0 j = Id for j \u2265 2, with the convention that for rectangular matrices the identity has ones on the main diagonal and zeroes elsewhere. In practice the\nstep size is chosen by taking cij = (1 + \u03b1).(\u03bb i)2 \u2016R\u201622 . \u2016L\u2016 2 2 with \u03b1 = 10\u22123. Such a determination of the step size is computationally costly, and alternatives could be considered in applications (a decreasing step size rule for example).\n4) Summary: An explicit version of the algorithm, called PALM for Multi-layer Sparse Approximation (palm4MSA), is given in Figure 4, in which the factors are updated alternatively by a projected gradient step (line 6) with a step-size controlled by the Lipschitz modulus of the gradient (line 5). We can solve for \u03bb directly at each iteration (line 9) because of the absence of constraint on it (thanks to the second part of the convergence condition (v) of PALM)."}, {"heading": "IV. HIERARCHICAL FACTORIZATION", "text": "The algorithm presented in Figure 4 factorizes an input matrix corresponding to an operator of interest into J sparse factors and converges to a stationary point of the problem stated in (4). In practice, one is only interested in the stationary points where the data fitting term of the cost function is small, however as for any generic non-convex optimization algorithm there is no general convergence guarantee to such a stationary point. This fact is illustrated by a very simple experiment where the algorithm palm4MSA is applied to an input operator A \u2208 Rn\u00d7n with a known sparse form A = \u220fN j=1 Sj , such as the Hadamard transform (in that case N = log2 n). The naive approach consists in setting directly J = N in palm4MSA, and setting the constraints so as to reflect the actual sparsity of the true factors (as depicted in Figure 1). This simple strategy performs quite poorly in practice for most initializations, and the attained local minimum is very often not satisfactory (the data fidelity part of the objective function is large)."}, {"heading": "A. Parallel with deep learning", "text": "Similar issues were faced in the neural network community, where it was found difficult to optimize the weights of neural networks comprising many hidden layers (called deep neural networks, see [38] for a survey on the topic). Until recently, deep networks were often neglected in favor of shallower\narchitectures. However in the last decade, it was proposed [39] to optimize the network not as one big block, but one layer at a time, and then globally optimizing the whole network using gradient descent. This heuristic was shown experimentally to work well on various tasks [37]. More precisely, what was proposed is to perform first a pre-training of the layers (each being fed the features produced by the one just below, and the lowermost being fed the data), in order to initialize the weights in a good region to perform then a global fine tuning of all layers by simple gradient descent."}, {"heading": "B. Proposed hierarchical algorithm", "text": "We noticed experimentally that taking fewer factors (J small) and allowing more non-zero entries per factor led to better approximations. This suggested to adopt a hierarchical strategy reminiscent of pre-training of deep networks, in order to iteratively compute only factorization with 2 factors. Indeed, when A = \u220fN j=1 Sj is the product of N sparse factors, it is\nalso the product A = T1S1 of 2 factors with T1 = \u220fN j=2 Sj , so that S1 is sparser than T1. 1) Optimization strategy: The proposed hierarchical strategy consists in iteratively factorizing the input matrix A into 2 factors, one being sparse (corresponding to S1), and the other less sparse (corresponding to T1). The process is repeated on the less sparse factor T1 until the desired number J of factors is attained. At each step, a global optimization of all the factors introduced so far can be performed in order to fit the product to the original operator A.\n2) Choice of sparsity constraint: A natural question is that of how to tune the sparsity of the factors and residuals along the process. Denoting T` = \u220fJ j=`+1 Sj , a simple calculation shows that if we expect each Sj to have roughly O(k) non-zero entries per row, then T` cannot have more than O(kJ\u2212(`+1)) non-zero entries per row. This suggests to decrease exponentially the number of non-zero entries in T` with ` and to keep constant O(k) the number of non-zero entries per row in Sj . This choice of the sparsity constraints is further studied with experiments in Section V.\n3) Implementation details: The proposed hierarchical strategy3 is summarized in the algorithm given in Figure 5, where the constraint sets related to the two factors need to be specified for each step: E\u0303` denotes the constraint set related to the left factor T`, and E` the one for the right factor S` at the `th factorization. Roughly we can say that line 3 of the algorithm is here to yield complexity savings. Line 5 is here to improve data fidelity: this global optimization step with palm4MSA is initialized with the current values of T` and {Sj}`j=1. The hierarchical strategy uses palm4MSA J \u2212 1 times with an increasing number of factors, and with a good initialization provided by the factorization in two factors. This makes its cost roughly J\u22121 times greater than the cost of the basic palm4MSA with J factors.\n3A toolbox implementing all the algorithms and experiments performed in this paper is available at http://faust.gforge.inria.fr All experiments were performed in Matlab on an laptop with an intel(R) core(TM) i7-3667U @ 2.00GHz (two cores).\nIn greedy layerwise training of deep neural networks, the factorizations in two (line 3) would correspond to the pretraining and the global optimization (line 5) to the fine tuning.\nRemark The hierarchical strategy can also be applied the other way around (starting from the left), just by transposing the input. We only present here the version that starts from the right because the induced notations are simpler. It is also worth being noted that stopping criteria other than the total number of factors can be set. For example, we could imagine to keep factorizing the residual until the approximation error at the global optimization step starts rising or exceeds some pre-defined threshold.\nC. Illustration: reverse-engineering the Hadamard transform\nAs a first illustration of the proposed approach, we tested the hierarchical factorization algorithm of Figure 5 when A is the dense square matrix associated to the Hadamard transform in dimension n = 2N . The algorithm was run with J = N factors, E\u0303` = {T \u2208 Rn\u00d7n, \u2016T\u20160 \u2264 n2 2` , \u2016T\u2016F = 1}, and E` = {S \u2208 Rn\u00d7n, \u2016S\u20160 \u2264 2n, \u2016S\u2016F = 1}. In stark contrast with the direct application of palm4MSA with J = N , an exact factorization is achieved. Indeed, the first step reached an exact factorization A = T1S1 independently of the initialization. With the default initialization (Section III-C3), the residual T1 was observed to be still exactly factorizable. All steps (` > 1) indeed also yielded exact factorizations T`\u22121 = T`S`, provided the default initialization was used at each step ` \u2208 {1, . . . , J \u2212 1}.\nFigure 6 illustrates the result of the proposed hierarchical strategy in dimension n = 32. The obtained factorization is exact and as good as the reference one (cf Figure 1) in terms of complexity savings. The running time of the factorization algorithm is less than a second. Factorization of the Hadamard matrix in dimension up to n = 1024 showed identical behaviour, with running times O(n2) up to ten minutes."}, {"heading": "V. ACCELERATING INVERSE PROBLEMS", "text": "A natural application of FA\u00b5STs is linear inverse problems, where a high-dimensional vector \u03b3 needs to be retrieved from\nsome observed data y \u2248 M\u03b3. As already evoked in Section II-B, iterative proximal algorithms can be expected to be significantly sped up if M is well approximated with a FA\u00b5ST of low relative complexity, for example using the proposed hierarchical factorization algorithm applied to A = M.\nIn practice, one needs to specify the total number of factors J and the constraint sets E\u0303`, E`. A preliminary study on synthetic data was carried out in our technical report [1], showing that a flexible trade-off between relative complexity and adaptation to the input matrix can be achieved. Here we leverage the rule of thumb presented in Section III-C to deepen the investigation of this question for a matrix M arising in a real-world biomedical linear inverse problem."}, {"heading": "A. Factorization compromise: MEG operator", "text": "In this experiment, we explore the use of FA\u00b5ST in the context of functional brain imaging using magnetoencephalography (MEG) and electroencephalography (EEG) signals. Source imaging with MEG and EEG delivers insights into the active brain at a millisecond time scale in a non-invasive way. To achieve this, one needs to solve the bioelectromagnetic inverse problem. It is a high dimensional ill-posed regression problem requiring proper regularization. As it is natural to assume that a limited set of brain foci are active during a cognitive task, sparse focal source configurations are commonly promoted using convex sparse priors [40], [41]. The bottleneck in the optimization algorithms are the dot products with the forward matrix and its transpose.\nThe objective of this experiment is to observe achievable trade-offs between relative complexity and accuracy. To this end, we consider an MEG gain matrix M \u2208 R204\u00d78193 (m = 204 and n = 8193), computed using the MNE software [42] implementing a Boundary Element Method (BEM). In this setting, sensors and sources are not on a regular spatial grid. Note that in this configuration, one cannot easily rely on classical operator compression methods presented in sections II-C2, that rely on the analytic expression of the kernel underlying M, or on those presented in section II-C3, that rely on some regularity of the input and output domains to define wavelets. Hence, in order to observe the complexity/accuracy trade-offs, M was factorized into J sparse factors using the hierarchical factorization algorithm of Figure 5.\n1) Settings: The rightmost factor S1 was of the size of M, but with k-sparse columns, corresponding to the constraint set E1 = {S \u2208 R204\u00d78193, \u2016si\u20160 \u2264 k, \u2016S\u2016F = 1}. All\nother factors Sj , j \u2208 {2, . . . , J} were set square, with global sparsity s, i.e. E` = {S \u2208 R204\u00d7204, \u2016S\u20160 \u2264 s, \u2016S\u2016F = 1}.\nThe \u201cresidual\u201d at each step T`, ` \u2208 {1, . . . , J\u22121} was also set square, with global sparsity geometrically decreasing with `, controlled by two parameters \u03c1 and P . This corresponds to the constraint sets4 E\u0303` = {T \u2208 R204\u00d7204, \u2016T\u20160 \u2264 P\u03c1`\u22121, \u2016T\u2016F = 1} for ` \u2208 {1, . . . , J \u2212 1}. The controlling parameters are set to: \u2022 Number of factors: J \u2208 {2, . . . , 10}. \u2022 Sparsity of the rightmost factor: k \u2208 {5, 10, 15, 20, 25, 30}.\n\u2022 Sparsity of the other factors: s \u2208 {2m, 4m, 8m}. \u2022 Rate of decrease of the residual sparsity: \u03c1 = 0.8.\nThe parameter P controlling the global sparsity in the residual was found to have only limited influence, and was set to P = 1.4 \u00d7m2. Other values for \u03c1 were tested, leading to slightly different but qualitatively similar complexity/accuracy tradeoffs not shown here. The factorization setting is summarized in Figure 7, where the sparsity of each factor is explicitly given.\n2) Results: Factorizations were computed for 127 parameter settings. The computation time for each factorization was around (J \u2212 1) \u00d7 10 minutes. Figure 8 displays the tradeoff between speed (the RCG measure (3)) and approximation error:\nRE := \u2225\u2225M\u2212 \u03bb\u220fJj=1 Sj\u2225\u22252 \u2016M\u20162 , (6)\nof each obtained FA\u00b5ST. We observe that: \u2022 The overall relative complexity of the obtained fac-\ntorization is essentially controlled by the parameter k. This seems natural, since k controls the sparsity of the rightmost factor which is way larger than the other ones. \u2022 The trade-off between complexity and approximation for a given k is mainly driven by the number of factors J : higher values of J lead to lower relative complexities, but a too large J leads to a higher relative error. Taking J = 2 (black dots) never yields the best compromise, hence the relevance of truly multi-layer sparse approximations. \u2022 For a fixed k, one can distinguish nearby trade-off curves corresponding to different sparsity levels s of the intermediate factors. The parameter s actually controls the horizontal spacing between two consecutive points on the same curve: a higher s allows to take a higher J without increasing the error, but in turn leads to a higher relative complexity for a given number J of factors.\nIn summary, one can distinguish as expected a trade-off between relative complexity and approximation error. The\n4Compared to a preliminary version of this experiment [17] where the residual was normalized columnwise at the first step, here it is normalized globally. This leads to slightly better results.\nconfiguration exhibiting the lowest relative error for each value of k is highlighted on Figure 8, this gives M\u030225, M\u030216, M\u030211, M\u03028, M\u03027, M\u03026, where the subscript indicates the achieved RCG (rounded to the closest integer). For example, M\u03026 can multiply vectors with 6 times less flops than M (saving 84% of computation), and M\u030225 can multiply vectors with 25 times less flops than M (96% savings). These six matrices are those appearing on Figure 2 to compare FA\u00b5STs to the truncated SVD. They will next be used to solve an inverse problem and compared to results obtained with M.\nRemark Slightly smaller approximation errors can be obtained by imposing a global sparsity constraint to the rightmost factor, i.e., E1 = {S \u2208 R204\u00d78193, \u2016S\u20160 \u2264 kn, \u2016S\u2016F = 1}. This is shown on Figure 8 by the points linked by a dashed line to the six matrices outlined above. However, such a global sparsity constraint also allows the appearance of null columns in the approximations of M, which is undesirable for the application considered next."}, {"heading": "B. Source localization experiment", "text": "We now assess the impact of replacing the MEG gain matrix M \u2208 R204\u00d78193 by a FA\u00b5ST approximation for brain source localization. For this synthetic experiment, two brain sources chosen located uniformly at random were activated with gaussian random weights, giving a 2-sparse vector \u03b3 \u2208 R8193, whose support encodes the localization of the sources. Observing y := M\u03b3, the objective is to estimate (the support of) \u03b3. The experiment then amounts to solving the inverse problem to get \u03b3\u0302 from the measurements y = M\u03b3 \u2208 R204, using either M or a FA\u00b5ST M\u0302 during the recovery process.\nThree recovery methods were tested: Orthogonal Matching Pursuit (OMP) [10] (choosing 2 atoms), `1-regularized least squares (l1ls) [43] and Iterative Hard Thresholding (IHT) [11]). They yielded qualitatively similar results, and for the sake of conciseness we present here only the results for OMP.\nThe matrices used for the recovery are the actual matrix M and its FA\u00b5ST approximations M\u030225, M\u030216, M\u030211, M\u03028, M\u03027 and M\u03026. The expected computational gain of using a FA\u00b5ST instead of M is of the order of RCG, since the computational cost of OMP is dominated by products with MT .\nThree configurations were considered when generating the location of the sources, in term of distance d (in centimeters) between the sources. For each configuration, 500 vectors y = M\u03b3 were generated, and OMP was run using each matrix. The distance between each actual source and the closest retrieved source was measured. Figure 9 displays the statistics of this distance for all scenarios: \u2022 As expected, localization is better when the sources are\nmore separated, independently of the choice of matrix. \u2022 Most importantly, the performance is almost as good\nwhen using FA\u00b5STs M\u03026, M\u03027, M\u03028 and M\u030211 than when using the actual matrix M, although the FA\u00b5STs are way more computationally efficient (6 to 11 times less computations). For example, in the case of well separated sources (d > 8), the FA\u00b5STs allow to retrieve exactly the sought sources more than 75% of the time, which is almost as good as when using the actual matrix M. \u2022 The performance with the two other FA\u00b5STs M\u030216 and M\u030225 is a bit poorer, but they are even more computationally efficient matrix (16 and 25 times less computations). For example, in the case of well separated sources (d > 8), they allow to retrieve exactly the sought sources more than 50% of the time.\nThese observations confirm it is possible to slighlty trade-off localization performance for substantial computational gains, and that FA\u00b5STs can be used to speed up inverse problems without a large precision loss."}, {"heading": "VI. LEARNING FAST DICTIONARIES", "text": "Multi-layer sparse approximations of operators are particularly suited for choosing efficient dictionaries for data processing tasks."}, {"heading": "A. Analytic vs. learned dictionaries", "text": "Classically, there are two paths to choose a dictionary for sparse signal representations [13].\nHistorically, the only way to come up with a dictionary was to analyze mathematically the data and derive a \u201csimple\u201d formula to construct the dictionary. Dictionaries designed this way are called analytic dictionaries [13] (e.g., associated\nto Fourier, wavelets and Hadamard transforms). Due to the relative simplicity of analytic dictionaries, they usually have a known sparse form such as the Fast Fourier Transform (FFT) [3] or the Discrete Wavelet Transform (DWT) [6].\nOn the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data [44]\u2013[46]. Given some raw data Y \u2208 Rm\u00d7L, the principle of dictionary learning is to approximate Y by the product of a dictionary D \u2208 Rm\u00d7n and a coefficient matrix \u0393 \u2208 Rn\u00d7L with sparse columns:\nY \u2248 D\u0393.\nSuch learned dictionaries are usually well adapted to the data at hand. However, being in general dense matrices with no apparent structure, they do not lead to fast algorithms and are costly to store. We typically have L max(m,n) (for sample complexity reasons), which implies to be very careful about the computational efficiency of learning in that case."}, {"heading": "B. The best of both worlds", "text": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? This question has begun to be explored recently [14], [15], and actually amounts to learning of dictionary that are FA\u00b5STs. More precisely, given Y, the objective is to learn a dictionary being a FA\u00b5ST (as in (1)):\nD = J\u220f j=1 Sj .\nThis can be done by inserting a dictionary factorization step into the traditional structure of dictionary learning algorithms [13], as illustrated on Figure 10. A consequence is that the coefficients update can be sped up by exploiting the FA\u00b5ST\nstructure of the dictionary. The approach described below uses a batch method for dictionary update, but the approach is a priori also compatible with stochastic gradient descent in the dictionary update for even more efficiency.\nIn practice we propose to slightly modify the hierarchical factorization algorithm of Figure 5. The idea is to take a dictionary D learned on some training data Y (with any classical dictionary learning method, such as K-SVD [45]) and to hierarchically factorize it, taking into account and jointly updating the coefficients matrix \u0393.\nThe resulting hierarchical factorization algorithm adapted to dictionary learning is given in Figure 11. The only differences with the hierarchical factorization algorithm given previously is that the coefficients matrix is taken into account (but kept fixed) in the global optimization step, and that an update of the coefficients by sparse coding is added after this global optimization step, in order to keep the error with respect to the data matrix low. This sparse coding step can actually be done by any algorithm (OMP, IHT, ISTA...), denoted by the general sparseCoding algorithm in Figure 11.\nRemark As noted in section IV-B3, the dictionary factorization is presented here starting from the right. It could as well be performed starting from the left.\nC. Image denoising experiment\nIn order to illustrate the advantages of FA\u00b5ST dictionaries over classical dense ones, an image denoising experiment is performed here. The experimental scenario for this task follows a simplified dictionary based image denoising workflow. First, L = 10000 patches yi of size 8 \u00d7 8 (dimension m = 64) are randomly picked from an input 512 \u00d7 512 noisy image (with various noise levels, of variance \u03c3 \u2208 {10, 15, 20, 30, 50}), and a dictionary is learned on these patches. Then the learned dictionary is used to denoise the entire input image by computing the sparse representation of all its patches in the dictionary using OMP, allowing each patch to use 5 dictionary atoms. The image is reconstructed by averaging the overlapping patches. Experimental settings. Several configurations were tested. The number of atoms n was taken in {128, 256, 512}. Inspired\nby usual fast transforms, a number of factors J close to the logarithm of the signal dimension m = 64 was chosen, here J = 4. The sizes of the factors were: SJ , . . . ,S2 \u2208 Rm\u00d7m, S1 \u2208 Rm\u00d7n, and \u0393 \u2208 Rn\u00d7L. The algorithm of Figure 11 was used, with the initial dictionary learning being done by K-SVD [45] and sparseCoding being OMP, allowing each patch to use 5 dictionary atoms. Regarding the constraint sets, we took them exactly like in section V-A, taking s/m \u2208 {2, 3, 6, 12}, \u03c1 \u2208 {0.4, 0.5, 0.7, 0.9}, P = 642 and k = s/m. For each dictionary size n, this amounts to a total of sixteen different configurations leading to different relative complexity values. The stopping criterion for palm4MSA was a number of iterations Ni = 50. Note that the usage of OMP here and at the denoising stage is a bit abusive since the dictionary does not have unit-norm columns (the factors are normalized instead), but it was used anyway, resulting in a sort of weighted OMP, where some atoms have more weight than others. Baselines. The proposed method was compared to Dense Dictionary Learning (DDL). K-SVD is used here to perform DDL, but other algorithms have been tested (such as online dictionary learning [46]), leading to similar qualitative results. In order to assess the generalization performance and to be as close as possible to the matrix factorization framework studied theoretically in [20], DDL is performed following the same denoising workflow than our method (dictionary learned on 10000 noisy patches used to denoise the whole image, allowing five atoms per patch). The implementation described in [47] was used, running 50 iterations (empirically sufficient to ensure convergence).\nNote that better denoising performance can be obtained by inserting dictionary learning into a more sophisticated denoising workflows, see e.g. [48]. State of the art denoising algorithms indeed often rely on clever averaging procedure called \u201caggregation\u201d. Our purpose here is primarily to illustrate the potential of the proposed FA\u00b5ST structure for denoising. While such workflows are fully compatible with the FA\u00b5ST\nstructure, we leave the implementation and careful benchmarking of the resulting denoising systems to future work.\nAs a last baseline, we used the above denoising scheme with an overcomplete DCT of 128, 256 or 512 atoms. Results. The experiment is done on the standard image database taken from [49] (12 standard grey 512\u00d7512 images). In Figure 12 are shown the results for three images: the one for which FA\u00b5ST dictionaries perform worst (\u201cMandrill\u201d), the one for which they perform best (\u201cWomanDarkHair\u201d) and the typical behaviour (\u201cPirate\u201d). Several comments are in order: \u2022 First of all, it is clear that with the considered simple\ndenoising workflow, FA\u00b5ST dictionaries perform better than DDL at strong noise levels, namely \u03c3 = 30 and \u03c3 = 50. This can be explained by the fact that when training patches are very noisy, DDL is prone to overfitting (learning the noise), whereas the structure of FA\u00b5STs seems to prevent it. On the other hand, for low noise levels, we pay the lack of adaptivity of FA\u00b5STs compared to DDL. Indeed, especially for very textured images (\u201cMandrill\u201d typically), the dictionary must be very flexible in order to fit such complex training patches, so DDL performs better. FA\u00b5ST dictionaries also perform better than DCT dictionaries at high noise levels. \u2022 Second, it seems that sparser FA\u00b5STs (with fewer parameters) perform better than denser ones for high noise levels. This can be explained by the fact that they are less prone to overfitting because of their lower number of parameters, implying fewer degrees of freedom. However, this is not true for low noise levels or with too few parameters, since in that case the loss of adaptivity with respect to the training patches is too important."}, {"heading": "D. Sample complexity of FA\u00b5STs", "text": "The good performance of FA\u00b5ST dictionaries compared to dense ones observed above may be surprising, since the more constrained structure of such dictionaries (compared to dense dictionaries) may bar them from providing good approximations of the considered patches. A possible element of explanation stems from the notion of sample complexity: as evoked in section II-B, the statistical significance of learned multi-layer sparse operators is expected to be improved compared to that of dense operators, thanks to a reduced sample complexity.\nIn the context of dictionary learning, the sample complexity indicates how many training samples L should be taken in order for the empirical risk to be (with high probability) uniformly close to its expectation [50], [51]. In [20], a general bound on the deviation between the empirical risk and its expectation is provided, which is proportional to the covering dimension of the dictionary class.\nFor dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51]. For FA\u00b5ST dictionaries we establish in Appendix C the following theorem.\nTheorem VI.1. For multi-layer sparse operators, the covering dimension is bounded by stot.\nA consequence is that for the same number of training samples L, a better generalization performance is expected\nfrom FA\u00b5ST dictionaries compared to dense ones, the gain being of the order of RCG. This fact is likely to explain the empirical success of FA\u00b5STs compared to dense dictionaries observed in section VI-C at low SNR. Of course, when stot becomes to small, the limited approximation capacity of FA\u00b5ST dictionaries imposes a trade-off between approximation and generalization."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "In this paper, a novel multi-layer matrix factorization framework was introduced, which allows to approximate a given linear operator by the composition of several ones. The underlying factorization algorithm stems on recent advances in non-convex optimization and has convergence guarantees. The proposed approach consists in hierarchically factorizing the input matrix in the hope of attaining better local minima, as is done for example in deep learning. The factorization algorithm that is used is pretty general and is able to take into account various constraints. One practical constraint of interest is sparsity (of various forms), which has several interesting properties. Indeed, multi-layer sparsely factorized linear operators have several advantages over classical dense ones, such as an increased speed of manipulation, a lighter storage footpring, and a higher statistical significance when estimated on training data.\nThe interest and versatility of the proposed factorization approach was demonstrated with various experiments, including a source localization one where the proposed method performs well with a greatly reduced computational cost compared to previous techniques. We performed also image denoising\nexperiments demonstrating that the proposed method has better generalization performance than dense dictionary learning with an impact at low SNR.\nIn the future, several developments are expected. On the theoretical side, we envision bounds on the trade-off between approximation quality and relative complexity. On the experimental side, new applications for FA\u00b5STs are to be explored. For example, signal processing on graphs is a relatively new discipline where computationally efficient operators can be envisioned using learned FA\u00b5ST dictionaries, or a FA\u00b5ST approximation of graph Fourier transforms. Moreover, the factorization cost being quite high, one could envision ways to approximate matrices by FA\u00b5STs without accessing the whole matrix, in order to reduce this cost. For example, one could imagine to have observations of the form (xi,yi = Axi) and try to minimize a data fitting term of the form\u2211n i=1\n\u2225\u2225yi \u2212\u220fJj=1 Sjxi\u2225\u222522."}, {"heading": "APPENDIX A PROJECTION OPERATORS", "text": "In this appendix are given the projection operators onto\nseveral constraint sets of interest for practical applications."}, {"heading": "A. General sparsity constraint", "text": "Sparsity is the most obvious constraint to put on the factors for operator sparse approximations. Consider first the following general sparsity constraint set:\nE := {S \u2208 Rp\u00d7q : \u2016SHi\u20160 \u2264 si\u2200i \u2208 {1, . . . ,K}, \u2016S\u2016F = 1},\nwhere {H1, . . . ,HK} forms a partition of the index set, si \u2208 N, \u2200i \u2208 {1, . . . ,K}, and ST is the matrix whose entries match those of S on T and are set to zero elsewhere. Given some matrix U \u2208 Rp\u00d7q , we wish to compute its projection onto the set E : PE(U) \u2208 arg min\nS {\u2016S\u2212U\u20162F : S \u2208 E}.\nProposition A.1. Projection operator formula.\nPE(U) = UI \u2016UI\u2016F\nwhere I is the index set corresponding to the union of the si entries of UHi with largest absolute value, \u2200i \u2208 {1, . . . ,K}.\nProof. Let S be an element of E and J its support. We have \u2016S\u2212U\u20162F = 1 + \u2016U\u2016 2 F \u2212 2\u3008vec(UJ ), vec(S)\u3009.For a given support, the matrix S \u2208 E maximizing \u3008vec(UJ ), vec(S)\u3009 is S = UJ / \u2016UJ \u2016F . For this matrix, \u3008vec(UJ ), vec(S)\u3009 = \u2016UJ \u2016F = \u221a\u2211K i=1 \u2016UJ\u2229Hi\u2016 2 Fwhich is maximized if J \u2229Hi corresponds to the si entries with largest absolute value of U within Hi, \u2200i \u2208 {1, . . . ,K}."}, {"heading": "B. Sparse and piecewise constant constraints", "text": "Given K pairwise disjoint sets Ci indexing matrix entries, consider now the constraint set corresponding to unit norm matrices that are constant over each index set Ci, zero outside these sets, with no more than s non-zero areas. In other words: Ec := {S \u2208 Rp\u00d7q : \u2203a\u0303 = (a\u0303i)Ki=1, \u2016a\u0303\u20160 \u2264 s,SCi = a\u0303i\u2200i \u2208 {1, . . . ,K},S\u22c3\ni Ci = 0, and \u2016S\u2016F = 1}.\nDefine u\u0303 := (u\u0303i)Ki=1 with u\u0303i := \u2211\n(m,n)\u2208Ci umn, and denote J\u0303 \u2282 {1, . . . ,K} the support of a\u0303.\nProposition A.2. The projection of U onto Ec is obtained with J\u0303 the collection of s indices i yielding the highest |u\u0303i|/ \u221a |Ci|,\na\u0303i := u\u0303i/ \u221a\u2211 i\u2208J\u0303 |Ci|u\u03032i ) if i \u2208 J\u0303 , a\u0303i := 0 otherwise.\nProof. Let S be an element of Ec, and J\u0303 \u2282 {1, . . . ,K} be the support of the associated a\u0303. We proceed as for the previous proposition and notice that \u3008vec(U), vec(S)\u3009 =\u2211 i\u2208J\u0303 \u3008vec(UCi), vec(S)\u3009 = \u2211 i\u2208J\u0303 u\u0303ia\u0303i = \u3008u\u0303J\u0303 , a\u0303\u3009. By the\nchanges of variable b\u0303i = \u221a |Ci|.a\u0303i and v\u0303i = u\u0303i/ \u221a |Ci| we get \u3008u\u0303J\u0303 , a\u0303\u3009 = \u3008v\u0303J\u0303 , b\u0303\u3009 with b\u0303 := (b\u0303i)Ki=1. Given J\u0303 , maximizing this scalar product under the constraint 1 = \u2016S\u2016F = \u2016b\u0303\u20162 yields b\u0303\u2217 := v\u0303J\u0303 /\u2016v\u0303J\u0303 \u20162, and \u3008v\u0303J\u0303 , b\u0303\u2217\u3009 = \u2016v\u0303J\u0303 \u20162. Maximizing over J\u0303 is achieved by selecting the s entries of v\u0303 := (v\u0303)Ki=1 with largest absolute value (Proposition A.1). Going back to the original variables gives the result."}, {"heading": "APPENDIX B LIPSCHITZ MODULUS", "text": "To estimate the Lipschitz modulus of the gradient of the smooth part of the objective we write:\u2225\u2225\u2225\u2207SijH(L,S1,R, \u03bbi)\u2212\u2207SijH(L,S2,R, \u03bbi))\u2225\u2225\u2225F\n= (\u03bbi)2 \u2225\u2225LTL(S1 \u2212 S2)RRT\u2225\u2225F \u2264 (\u03bbi)2 \u2016R\u201622 . \u2016L\u2016 2 2 \u2016S1 \u2212 S2\u2016F ."}, {"heading": "APPENDIX C COVERING DIMENSION", "text": "The covering number N (A, ) of a set A is the minimum number of balls of radius needed to cover it. The precise definition of covering numbers is given in [20]. The upperbox counting dimension of the set, loosely referred to as the covering dimension in the text is d(A) = lim \u21920 logN (A, )log 1/ . We are interested in the covering dimension of the set of FA\u00b5STs Dspfac. We begin with the elementary sets Ej = {A \u2208 Raj\u00d7aj+1 : \u2016A\u20160 \u2264 sj , \u2016A\u2016F = 1}. These sets can be seen as sets of sparse normalized vectors of size aj \u00d7 aj+1. This leads following [20] (with the Frobenius norm) to:\nN (Ej , ) \u2264 ( ajaj+1 sj )( 1 + 2 )sj .\nDefining M := E1 \u00d7 . . . \u00d7 EJ and using [20, lemma 16] gives N (M, ) \u2264 \u220fJ j=1 ( ajaj+1 sj ) (1 + 2 )\nsj (wrt to the max metric over the index j using the Frobenius norm). Using\u2211J j=1 \u2016xj \u2212 yj\u2016F \u2264 Jmaxj \u2016xj \u2212 yj\u2016F gives N (M, ) \u2264\u220fJ j=1 ( ajaj+1 sj ) (1 + 2J ) sj wrt to the metric defined by\n\u03c1(x, y) = \u2211J j=1 \u2016xj \u2212 yj\u2016F . Defining the mapping:\n\u03a6 : M := E1 \u00d7 . . .\u00d7 EJ \u2192 Dspfac (D1,D2, . . . ,DJ) 7\u2192 DJ . . .D2D1,\nwhere Dspfac is the set of FA\u00b5STs of interest, and using the distance measures \u03c1(x, y) = \u2211J j=1 \u2016xj \u2212 yj\u2016F in M and\n\u03c11(x, y) = \u2016x\u2212 y\u2016F in Dspfac we get that the mapping \u03a6 is a contraction (by induction). We can conclude that:\nN (Dspfac, ) \u2264 \u220fJ\nj=1 ( ajaj+1 sj )( 1 + 2J )sj ,\nwrt the \u03c11 metric. Using ( n p ) \u2264 n p p! and n! \u2265 \u221a 2\u03c0n ( n e )n yields N (Dspfac, ) \u2264 \u220fJ j=1\n1\u221a 2\u03c0sj ( e sj ajaj+1(1 + 2J ) )sj .\nDefining Cj = e.aj .aj+1.(2J+1) sj . 2sj \u221a 2\u03c0sj , we have N (Dspfac, ) \u2264 (C ) h, with h = \u2211J j=1 sj and C = maxj Cj . We thus have d(Dspfac) \u2264\nh = \u2211J j=1 sj = stot."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors wish to thank Franc\u0327ois Malgouyres and Olivier Chabiron for discussions that helped in producing this work. The authors also express their gratitude to Alexandre Gramfort for providing the MEG data and contributing to [17]. Finally, the authors thank the reviewers for their valuable comments."}], "references": [{"title": "Learning computationally efficient dictionaries and their implementation as fast transforms,", "author": ["L. Le Magoarou", "R. Gribonval"], "venue": "CoRR, vol. abs/1406.5388,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Chasing butterflies: In search of efficient dictionaries,", "author": ["L. Le Magoarou", "R. Gribonval"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An algorithm for the machine calculation of complex Fourier series,", "author": ["J. Cooley", "J. Tukey"], "venue": "Mathematics of Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1965}, {"title": "Computation of the fast walsh-fourier transform,", "author": ["J. Shanks"], "venue": "Computers, IEEE Transactions on, vol. C-18,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1969}, {"title": "A fast computational algorithm for the discrete cosine transform,", "author": ["W.-H. Chen", "C. Smith", "S. Fralick"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1977}, {"title": "A theory for multiresolution signal decomposition: the wavelet representation,", "author": ["S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "The linear complexity of computation,", "author": ["J. Morgenstern"], "venue": "J. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "Iterative thresholding for sparse approximations,", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Dictionaries for Sparse Representation Modeling,", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation,", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Toward fast transform learning,", "author": ["O. Chabiron", "F. Malgouyres", "J.-Y. Tourneret", "N. Dobigeon"], "venue": "International Journal of Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "FA\u03bcST: speeding up linear transforms for tractable inverse problems,", "author": ["L. Le Magoarou", "R. Gribonval", "A. Gramfort"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Proximal Alternating Linearized Minimization for nonconvex and nonsmooth problems,", "author": ["J. Bolte", "S. Sabach", "M. Teboulle"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "SciPy: Open source scientific tools for Python,", "author": ["E. Jones", "T. Oliphant", "P. Peterson"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Sample complexity of dictionary learning and other matrix factorizations,", "author": ["R. Gribonval", "R. Jenatton", "F. Bach", "M. Kleinsteuber", "M. Seibert"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Rapid solution of integral equations of classical potential theory,", "author": ["V. Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "A Sparse Matrix Arithmetic Based on H-matrices. Part I: Introduction to H-matrices,", "author": ["W. Hackbusch"], "venue": "Computing, vol. 62,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Fast computation of fourier integral operators,", "author": ["E. Cand\u00e8s", "L. Demanet", "L. Ying"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Fast wavelet transforms and numerical algorithms i,", "author": ["G. Beylkin", "R. Coifman", "V. Rokhlin"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images,", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Learning fast approximations of sparse coding,", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proceedings of the 27th Annual International Conference on Machine Learning, ser. ICML", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "k-sparse autoencoders,", "author": ["A. Makhzani", "B. Frey"], "venue": "CoRR, vol. abs/1312.5663,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Treelets - an adaptive multiscale basis for sparse unordered data,", "author": ["A.B. Lee", "B. Nadler", "L. Wasserman"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The sparse matrix transform for covariance estimation and analysis of high dimensional signals,", "author": ["G. Cao", "L. Bachega", "C. Bouman"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "On algorithms for sparse multi-factor NMF,", "author": ["S. Lyu", "X. Wang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sparse matrix factorization,", "author": ["B. Neyshabur", "R. Panigrahy"], "venue": "CoRR, vol. abs/1311.3315,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Provable bounds for learning some deep representations,", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "CoRR, vol. abs/1310.6343,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Wavelets on graphs via deep learning,", "author": ["R. Rustamov", "L. Guibas"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Greedy layerwise training of deep networks,", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Learning deep architectures for ai,", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Reducing the dimensionality of data with neural networks,", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Combining sparsity and rotational invariance in EEG/MEG source reconstruction,", "author": ["S. Haufe", "V.V. Nikulin", "A. Ziehe", "K.-R. M\u00fcller", "G. Nolte"], "venue": "NeuroImage, vol. 42,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Mixed-norm estimates for the M/EEG inverse problem using accelerated gradient methods,", "author": ["A. Gramfort", "M. Kowalski", "M.S. H\u00e4m\u00e4l\u00e4inen"], "venue": "Phys. Med. Biol.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "MNE software for processing MEG and EEG data ,", "author": ["A. Gramfort", "M. Luessi", "E. Larson", "D.A. Engemann", "D. Strohmeier", "C. Brodbeck", "L. Parkkonen", "M.S. H\u00e4m\u00e4l\u00e4inen"], "venue": "NeuroImage, vol. 86,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "An interiorpoint method for large-scale l1-regularized least squares,", "author": ["S.-J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "Method of optimal directions for frame design,", "author": ["K. Engan", "S. Aase", "J. Hakon Husoy"], "venue": "in Acoustics, Speech, and Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1999}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit,", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "The sample complexity of dictionary learning,", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "K-Dimensional Coding Schemes in Hilbert Spaces,", "author": ["A. Maurer", "M. Pontil"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "The n-dimensional Discrete Fourier Transform (DFT) is certainly the most well known linear operator with an efficient implementation: the Fast Fourier Transform (FFT) [3], allows to apply the operator in O(n log n) arithmetic operations instead of O(n) in its dense form.", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 194, "endOffset": 197}, {"referenceID": 0, "context": "Parts of this work have been presented at the conferences ICASSP 2015 [1] and EUSIPCO 2015 [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Parts of this work have been presented at the conferences ICASSP 2015 [1] and EUSIPCO 2015 [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "1The product being taken from right to left: \u220fJ j=1 Sj = SJ \u00b7 \u00b7 \u00b7S1 algorithm given in [7], this multi-layer sparse factorization is actually the natural representation of any fast linear transform.", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "State of the art algorithms addressing such problems with sparse regularization [8]\u2013[12] are known to heavily rely on matrix-vector products involving both this operator and its adjoint.", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "To choose a regularizer for inverse problems, dictionary learning is a common method used to learn the domain in which some training data admits a sparse representation [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "We will see that recent approaches to learn fast dictionaries [14], [15] can be seen as special cases of the FA\u03bcST dictionary learning approach developed in Section VI, where the learned dictionaries are constrained to be FA\u03bcST.", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "We will see that recent approaches to learn fast dictionaries [14], [15] can be seen as special cases of the FA\u03bcST dictionary learning approach developed in Section VI, where the learned dictionaries are constrained to be FA\u03bcST.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "For example, in the emerging area of signal processing on graphs [16], novel definitions of usual operators such as the Fourier or wavelet transforms have been ar X iv :1 50 6.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "\u2022 A general framework for multi-layer sparse approximation (MSA) is introduced, that allows to incorporate various constraints on the sought sparse form; \u2022 Recent advances in non-convex optimization [18] are exploited to tackle the resulting non-convex optimization problem with local convergence guarantees; \u2022 A heuristic hierarchical factorization algorithm leveraging these optimization techniques is proposed, that achieves factorizations empirically stable to initialization; \u2022 The versatility of the framework is illustrated with extensive experiments on two showcase applications, linear inverse problems and dictionary learning, demonstrating its practical benefits.", "startOffset": 199, "endOffset": 203}, {"referenceID": 15, "context": "1) Storage cost: Using the Coordinate list (COO) storage paradigm [19], one can store a FA\u03bcST using O(stot) floats and integers.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "More specifically, the sample complexity is reduced [20], and better generalization properties are expected.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "3) Wavelet-based compression: This operator compression paradigm, introduced in [24], is based on the use of orthogonal", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "4) Dictionary learning: Given a collection of training vectors y`, 1 \u2264 ` \u2264 L gathered as the columns of a matrix Y, the objective of dictionary learning [13], [25] is to approximate Y by the product of a dictionary D and a coefficients matrix \u0393 with sparse columns, Y \u2248 D\u0393.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "In [14], the authors propose the sparse-KSVD algorithm (KSVDS) to learn a dictionary whose atoms are sparse linear combinations of atoms of a so-called base dictionary Dbase.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [15], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions using sparse kernels with known supports, so that the dictionary is a sparse operator that is fast to manipulate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "This formulation is powerful, as demonstrated in [15], but limited in nature to the case where the dictionary is well approximated by a product of sparse circulant matrices, and requires knowledge of the supports of the sparse factors.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is also likely to bring additional speedups to recent approaches accelerating iterative sparse solvers through learning [27]\u2013[29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "This is also likely to bring additional speedups to recent approaches accelerating iterative sparse solvers through learning [27]\u2013[29].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "6) Statistics \u2013 factor analysis: A related problem is to approximately diagonalize a covariance matrix by a unitary matrix in factorized form (1), which can be addressed greedily [30], [31] using a fixed number of elementary Givens rotations.", "startOffset": 179, "endOffset": 183}, {"referenceID": 25, "context": "6) Statistics \u2013 factor analysis: A related problem is to approximately diagonalize a covariance matrix by a unitary matrix in factorized form (1), which can be addressed greedily [30], [31] using a fixed number of elementary Givens rotations.", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "For example, sparse multi-factor NMF [32] can be seen as solving problem (2) with the Kullback-Leibler divergence as data fidelity", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "linearities are neglected [33], [34].", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "linearities are neglected [33], [34].", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "Apart from its hierarchical flavor, the identification algorithm in [33], [34] has little in common with the proximal method proposed here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "Apart from its hierarchical flavor, the identification algorithm in [33], [34] has little in common with the proximal method proposed here.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "The Laplacian ends up being diagonal in all the dimensions corresponding to the wavelets and dense in a small part corresponding to the scaling function (the algorithm ends up being very similar to the one proposed in [31]).", "startOffset": 218, "endOffset": 222}, {"referenceID": 29, "context": "Second, in [36] the authors propose to define data adaptive wavelets by factorizing some training data matrix made of signals on the graph of interest.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "The optimization algorithm they propose relies on deep learning techniques, more precisely layer-wise training of stacked autoencoders [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "To avoid the scaling ambiguities arising naturally when the constraint sets are (positively) homogeneous2, it is common [15], [32] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "To avoid the scaling ambiguities arising naturally when the constraint sets are (positively) homogeneous2, it is common [15], [32] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": ", see for example [15].", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "In [18], the authors consider cost functions depending on N blocks of variables of the form:", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM) [18], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step.", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "where it was found difficult to optimize the weights of neural networks comprising many hidden layers (called deep neural networks, see [38] for a survey on the topic).", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "However in the last decade, it was proposed [39] to optimize the network not as one big block, but one layer at a time, and then globally optimizing the whole network using gradient descent.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "This heuristic was shown experimentally to work well on various tasks [37].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "A preliminary study on synthetic data was carried out in our technical report [1], showing that a flexible trade-off between relative complexity and adaptation to the input matrix can be achieved.", "startOffset": 78, "endOffset": 81}, {"referenceID": 33, "context": "As it is natural to assume that a limited set of brain foci are active during a cognitive task, sparse focal source configurations are commonly promoted using convex sparse priors [40], [41].", "startOffset": 180, "endOffset": 184}, {"referenceID": 34, "context": "As it is natural to assume that a limited set of brain foci are active during a cognitive task, sparse focal source configurations are commonly promoted using convex sparse priors [40], [41].", "startOffset": 186, "endOffset": 190}, {"referenceID": 35, "context": "To this end, we consider an MEG gain matrix M \u2208 R204\u00d78193 (m = 204 and n = 8193), computed using the MNE software [42] implementing a Boundary Element Method (BEM).", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "4Compared to a preliminary version of this experiment [17] where the residual was normalized columnwise at the first step, here it is normalized globally.", "startOffset": 54, "endOffset": 58}, {"referenceID": 36, "context": "Three recovery methods were tested: Orthogonal Matching Pursuit (OMP) [10] (choosing 2 atoms), `1-regularized least squares (l1ls) [43] and Iterative Hard Thresholding (IHT) [11]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 7, "context": "Three recovery methods were tested: Orthogonal Matching Pursuit (OMP) [10] (choosing 2 atoms), `1-regularized least squares (l1ls) [43] and Iterative Hard Thresholding (IHT) [11]).", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "Classically, there are two paths to choose a dictionary for sparse signal representations [13].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Dictionaries designed this way are called analytic dictionaries [13] (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Due to the relative simplicity of analytic dictionaries, they usually have a known sparse form such as the Fast Fourier Transform (FFT) [3] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Due to the relative simplicity of analytic dictionaries, they usually have a known sparse form such as the Fast Fourier Transform (FFT) [3] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 37, "context": "On the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data [44]\u2013[46].", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "On the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data [44]\u2013[46].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? This question has begun to be explored recently [14], [15], and actually amounts to learning", "startOffset": 197, "endOffset": 201}, {"referenceID": 11, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? This question has begun to be explored recently [14], [15], and actually amounts to learning", "startOffset": 203, "endOffset": 207}, {"referenceID": 9, "context": "This can be done by inserting a dictionary factorization step into the traditional structure of dictionary learning algorithms [13], as illustrated on Figure 10.", "startOffset": 127, "endOffset": 131}, {"referenceID": 38, "context": "DDL, but other algorithms have been tested (such as online dictionary learning [46]), leading to similar qualitative results.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "In order to assess the generalization performance and to be as close as possible to the matrix factorization framework studied theoretically in [20], DDL is performed following the", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "The implementation described in [47] was used, running 50 iterations (empirically sufficient to ensure convergence).", "startOffset": 32, "endOffset": 36}, {"referenceID": 40, "context": "indicates how many training samples L should be taken in order for the empirical risk to be (with high probability) uniformly close to its expectation [50], [51].", "startOffset": 151, "endOffset": 155}, {"referenceID": 41, "context": "indicates how many training samples L should be taken in order for the empirical risk to be (with high probability) uniformly close to its expectation [50], [51].", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "In [20], a general bound on the deviation between the empirical risk and its expectation is provided, which is proportional to the covering dimension of the dictionary class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "The precise definition of covering numbers is given in [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This leads following [20] (with the Frobenius norm) to:", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "The authors also express their gratitude to Alexandre Gramfort for providing the MEG data and contributing to [17].", "startOffset": 110, "endOffset": 114}], "year": 2016, "abstractText": "The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems.", "creator": "LaTeX with hyperref package"}}}