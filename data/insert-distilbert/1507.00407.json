{"id": "1507.00407", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2015", "title": "Fast Convergence of Regularized Learning in Games", "abstract": "we show that natural classes of regularized learning algorithms with a form condition of recency bias achieve necessarily faster convergence rates to approximate efficiency and to correlated equilibria performance in successfully multiplayer normal form games. when each player node in a game uses an algorithm from our class, their individual regret decays at $ o ( t ^ { - 3 / 4 } ) $, while the sum of utilities converges to an approximate optimum at $ o ( log t ^ { - 1 } ) $ - - an improvement upon the worst competitive case $ o ( t ^ { - 1 / 2 } ) $ rates. we show a black - box reduction for any algorithm in the previous class to achieve $ o ( t ^ { - 1 / 2 } ) $ rates against an adversary, while maintaining the faster rates against algorithms in the class. our results extend those of [ dmitri rakhlin, and shridharan 2013 ] and [ daskalakis et al. 2014 ], who only analyzed two - player zero - sum games for specific algorithms.", "histories": [["v1", "Thu, 2 Jul 2015 01:55:40 GMT  (362kb,D)", "https://arxiv.org/abs/1507.00407v1", null], ["v2", "Tue, 21 Jul 2015 21:58:16 GMT  (120kb,D)", "http://arxiv.org/abs/1507.00407v2", null], ["v3", "Wed, 5 Aug 2015 14:51:56 GMT  (117kb,D)", "http://arxiv.org/abs/1507.00407v3", null], ["v4", "Tue, 8 Dec 2015 17:22:43 GMT  (112kb,D)", "http://arxiv.org/abs/1507.00407v4", null], ["v5", "Thu, 10 Dec 2015 21:52:29 GMT  (112kb,D)", "http://arxiv.org/abs/1507.00407v5", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["vasilis syrgkanis", "alekh agarwal", "haipeng luo", "robert e schapire"], "accepted": true, "id": "1507.00407"}, "pdf": {"name": "1507.00407.pdf", "metadata": {"source": "CRF", "title": "Fast Convergence of Regularized Learning in Games", "authors": ["Vasilis Syrgkanis", "Alekh Agarwal"], "emails": ["vasy@microsoft.com", "alekha@microsoft.com", "haipengl@cs.princeton.edu", "schapire@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "What happens when players in a game interact with one another, all of them acting independently and selfishly to maximize their own utilities? If they are smart, we intuitively expect their utilities \u2014 both individually and as a group \u2014 to grow, perhaps even to approach the best possible. We also expect the dynamics of their behavior to eventually reach some kind of equilibrium. Understanding these dynamics is central to game theory as well as its various application areas, including economics, network routing, auction design, and evolutionary biology.\nIt is natural in this setting for the players to each make use of a no-regret learning algorithm for making their decisions, an approach known as decentralized no-regret dynamics. No-regret algorithms are a strong match for playing games because their regret bounds hold even in adversarial environments. As a benefit, these bounds ensure that each player\u2019s utility approaches optimality. When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds. Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13]. (See [3, 21] for excellent overviews.) For all of these, the average regret vanishes at the worst-case rate of O(1/ \u221a T ), which is unimprovable in fully adversarial scenarios.\nHowever, the players in our setting are facing other similar, predictable no-regret learning algorithms, a chink that hints at the possibility of improved convergence rates for such dynamics. This was first observed and exploited by Daskalakis et al. [4]. For two-player zero-sum games, they developed a decentralized variant of Nesterov\u2019s accelerated saddle point algorithm [16] and showed that each player\u2019s average regret converges at the remarkable rate ofO(1/T ). Although the resulting\nar X\niv :1\n50 7.\n00 40\n7v 5\n[ cs\n.G T\n] 1\ndynamics are somewhat unnatural, in later work, Rakhlin and Sridharan [18] showed surprisingly that the same convergence rate holds for a simple variant of Mirror Descent with the seemingly minor modification that the last utility observation is counted twice.\nAlthough major steps forward, both these works are limited to two-player zero-sum games, the very simplest case. As such, they do not cover many practically important settings, such as auctions or routing games, which are decidedly not zero-sum, and which involve many independent actors.\nIn this paper, we vastly generalize these techniques to the practically important but far more challenging case of arbitrary multi-player normal-form games, giving natural no-regret dynamics whose convergence rates are much faster than previously possible for this general setting.\nContributions. We show that the average welfare of the game, that is, the sum of player utilities, converges to approximately optimal welfare at the rate O(1/T ), rather than the previously known rate of O(1/ \u221a T ). Concretely, we show a natural class of regularized no-regret algorithms with recency bias that achieve welfare at least (\u03bb/(1 + \u00b5))OPT \u2212 O(1/T ), where \u03bb and \u00b5 are parameters in a smoothness condition on the game introduced by Roughgarden [19]. For the same class of algorithms, we show that each individual player\u2019s average regret converges to zero at the rateO ( T\u22123/4 ) . Thus, our results entail an algorithm for computing coarse correlated equilibria in a decentralized manner with significantly faster convergence than existing methods.\nWe additionally give a black-box reduction that preserves the fast rates in favorable environments, while robustly maintaining O\u0303(1/ \u221a T ) regret against any opponent in the worst case.\nEven for two-person zero-sum games, our results for general games expose a hidden generality and modularity underlying the previous results [4, 18]. First, our analysis identifies stability and recency bias as key structural ingredients of an algorithm with fast rates. This covers the Optimistic Mirror Descent of Rakhlin and Sridharan [18] as an example, but also applies to optimistic variants of Follow the Regularized Leader (FTRL), including dependence on arbitrary weighted windows in the history as opposed to just the utility from the last round. Recency bias is a behavioral pattern commonly observed in game-theoretic environments [10]; as such, our results can be viewed as a partial theoretical justification. Second, previous approaches in [4, 18] on achieving both faster convergence against similar algorithms while at the same time O\u0303(1/ \u221a T ) regret rates against adversaries were shown via ad-hoc modifications of specific algorithms. We give a black-box modification which is not algorithm specific and works for all these optimistic algorithms.\nFinally, we simulate a 4-bidder simultaneous auction game, and compare our optimistic algorithms against Hedge [8] in terms of utilities, regrets and convergence to equilibria."}, {"heading": "2 Repeated Game Model and Dynamics", "text": "Consider a static game G among a set N of n players. Each player i has a strategy space Si and a utility function ui : S1 \u00d7 . . .\u00d7 Sn \u2192 [0, 1] that maps a strategy profile s = (s1, . . . , sn) to a utility ui(s). We assume that the strategy space of each player is finite and has cardinality d, i.e. |Si| = d. We denote with w = (w1, . . . ,wn) a profile of mixed strategies, where wi \u2208 \u2206(Si) and wi,x is the probability of strategy x \u2208 Si. Finally let Ui(w) = Es\u223cw[ui(s)], the expected utility of player i. We consider the setting where the game G is played repeatedly for T time steps. At each time step t each player i picks a mixed strategy wti \u2208 \u2206(Si). At the end of the iteration each player i observes the expected utility he would have received had he played any possible strategy x \u2208 Si. More formally, let uti,x = Es\u2212i\u223cwt\u2212i [ui(x, s\u2212i)], where s\u2212i is the set of strategies of all but the i th player, and let uti = (u t i,x)x\u2208Si . At the end of each iteration each player i observes u t i. Observe that the expected utility of a player at iteration t is simply the inner product \u3008wti ,uti\u3009.\nNo-regret dynamics. We assume that the players each decide their strategy wti based on a vanishing regret algorithm. Formally, for each player i, the regret after T time steps is equal to the maximum gain he could have achieved by switching to any other fixed strategy:\nri(T ) = sup w\u2217i \u2208\u2206(Si) T\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a .\nThe algorithm has vanishing regret if ri(T ) = o(T ).\nApproximate Efficiency of No-Regret Dynamics. We are interested in analyzing the average welfare of such vanishing regret sequences. For a given strategy profile s the social welfare is defined as the sum of the player utilities: W (s) = \u2211 i\u2208N ui(s). We overload notation to denote W (w) = Es\u223cw[W (s)]. We want to lower bound how far the average welfare of the sequence is, with respect to the optimal welfare of the static game:\nOPT = max s\u2208S1\u00d7...\u00d7Sn W (s).\nThis is the optimal welfare achievable in the absence of player incentives and if a central coordinator could dictate each player\u2019s strategy. We next define a class of games first identified by Roughgarden [19] on which we can approximate the optimal welfare using decoupled no-regret dynamics.\nDefinition 1 (Smooth game [19]). A game is (\u03bb, \u00b5)-smooth if there exists a strategy profile s\u2217 such that for any strategy profile s: \u2211 i\u2208N ui(s \u2217 i , s\u2212i) \u2265 \u03bbOPT \u2212 \u00b5W (s).\nIn words, any player using his optimal strategy continues to do well irrespective of other players\u2019 strategies. This condition directly implies near-optimality of no-regret dynamics as we show below.\nProposition 2. In a (\u03bb, \u00b5)-smooth game, if each player i suffers regret at most ri(T ), then:\n1\nT T\u2211 t=1 W (wt) \u2265 \u03bb 1 + \u00b5 OPT \u2212 1 1 + \u00b5 1 T \u2211 i\u2208N ri(T ) = 1 \u03c1 OPT \u2212 1 1 + \u00b5 1 T \u2211 i\u2208N ri(T ),\nwhere the factor \u03c1 = (1 + \u00b5)/\u03bb is called the price of anarchy (POA).\nThis proposition is essentially a more explicit version of Roughgarden\u2019s result [19]; we provide a proof in the appendix for completeness. The result shows that the convergence to POA is driven by the quantity 11+\u00b5 1 T \u2211 i\u2208N ri(T ). There are many algorithms which achieve a regret rate of\nri(T ) = O( \u221a\nlog(d)T ), in which case the latter theorem would imply that the average welfare converges to POA at a rate of O(n \u221a log(d)/T ). As we will show, for some natural classes of no-regret algorithms the average welfare converges at the much faster rate of O(n2 log(d)/T )."}, {"heading": "3 Fast Convergence to Approximate Efficiency", "text": "In this section, we present our main theoretical results characterizing a class of no-regret dynamics which lead to faster convergence in smooth games. We begin by describing this class.\nDefinition 3 (RVU property). We say that a vanishing regret algorithm satisfies the Regret bounded by Variation in Utilities (RVU) property with parameters \u03b1 > 0 and 0 < \u03b2 \u2264 \u03b3 and a pair of dual norms (\u2016 \u00b7 \u2016, \u2016 \u00b7 \u2016\u2217)1 if its regret on any sequence of utilities u1,u2, . . . ,uT is bounded as\nT\u2211 t=1 \u2329 w\u2217 \u2212wt,ut \u232a \u2264 \u03b1+ \u03b2 T\u2211 t=1 \u2016ut \u2212 ut\u22121\u20162\u2217 \u2212 \u03b3 T\u2211 t=1 \u2016wt \u2212wt\u22121\u20162. (1)\nTypical online learning algorithms such as Mirror Descent and FTRL do not satisfy the RVU property in their vanilla form, as the middle term grows as \u2211T t=1 \u2016ut\u20162\u2217 for these methods. However, Rakhlin and Sridharan [17] give a modification of Mirror Descent with this property, and we will present a similar variant of FTRL in the sequel.\nWe now present two sets of results when each player uses an algorithm with this property. The first discusses the convergence of social welfare, while the second governs the convergence of the individual players\u2019 utilities at a fast rate.\n1The dual to a norm \u2016 \u00b7 \u2016 is defined as \u2016v\u2016\u2217 = sup\u2016u\u2016\u22641 \u3008u, v\u3009."}, {"heading": "3.1 Fast Convergence of Social Welfare", "text": "Given Proposition 2, we only need to understand the evolution of the sum of players\u2019 regrets\u2211T t=1 ri(T ) in order to obtain convergence rates of the social welfare. Our main result in this section bounds this sum when each player uses dynamics with the RVU property. Theorem 4. Suppose that the algorithm of each player i satisfies the property RVU with parameters \u03b1, \u03b2 and \u03b3 such that \u03b2 \u2264 \u03b3/(n\u2212 1)2 and \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u20161. Then \u2211 i\u2208N ri(T ) \u2264 \u03b1n.\nProof. Since ui(s) \u2264 1, definitions imply: \u2016uti\u2212u t\u22121 i \u2016\u2217 \u2264 \u2211 s\u2212i \u2223\u2223\u2223\u220fj 6=i wtj,sj \u2212\u220fj 6=i wt\u22121j,sj \u2223\u2223\u2223 . The latter is the total variation distance of two product distributions. By known properties of total variation (see e.g. [12]), this is bounded by the sum of the total variations of each marginal distribution:\u2211\ns\u2212i \u2223\u2223\u2223\u2223\u2223\u2223 \u220f j 6=i wtj,sj \u2212 \u220f j 6=i wt\u22121j,sj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2211 j 6=i \u2016wtj \u2212wt\u22121j \u2016 (2)\nBy Jensen\u2019s inequality, (\u2211\nj 6=i \u2016wtj \u2212w t\u22121 j \u2016\n)2 \u2264 (n\u2212 1) \u2211 j 6=i \u2016wtj \u2212w\nt\u22121 j \u20162, so that\u2211 i\u2208N \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2264 (n\u2212 1) \u2211 i\u2208N \u2211 j 6=i \u2016wtj \u2212wt\u22121j \u2016 2 = (n\u2212 1)2 \u2211 i\u2208N \u2016wti \u2212wt\u22121i \u2016 2.\nThe theorem follows by summing up the RVU property (1) for each player i and observing that the summation of the second terms is smaller than that of the third terms and thereby can be dropped.\nRemark: The rates from the theorem depend on \u03b1, which will be O(1) in the sequel. The above theorem extends to the case where \u2016 \u00b7 \u2016 is any norm equivalent to the `1 norm. The resulting requirement on \u03b2 in terms of \u03b3 can however be more stringent. Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 18], as long as each player\u2019s algorithm satisfies the RVU property with a common bound on the constants.\nWe now instantiate the result with examples that satisfy the RVU property with different constants."}, {"heading": "3.1.1 Optimistic Mirror Descent", "text": "The optimistic mirror descent (OMD) algorithm of Rakhlin and Sridharan [17] is parameterized by an adaptive predictor sequence Mti and a regularizer\n2 R which is 1-strongly convex3 with respect to a norm \u2016 \u00b7 \u2016. Let DR denote the Bregman divergence associated with R. Then the update rule is defined as follows: let g0i = argming\u2208\u2206(Si)R(g) and\n\u03a6(u,g) = argmax w\u2208\u2206(Si)\n\u03b7 \u00b7 \u3008w,u\u3009 \u2212DR(w,g),\nthen: wti = \u03a6(M t i,g t\u22121 i ), and g t i = \u03a6(u t i,g t\u22121 i )\nThen the following proposition can be obtained for this method. Proposition 5. The OMD algorithm using stepsize \u03b7 and Mti = u t\u22121 i satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7, \u03b3 = 1/(8\u03b7), where R = maxi supf DR(f,g 0 i ).\nThe proposition follows by further crystallizing the arguments of Rakhlin and Sridaran [18], and we provide a proof in the appendix for completeness. The above proposition, along with Theorem 4, immediately yields the following corollary, which had been proved by Rakhlin and Sridharan [18] for two-person zero-sum games, and which we here extend to general games. Corollary 6. If each player runs OMD with Mti = u t\u22121 i and stepsize \u03b7 = 1/( \u221a 8(n\u2212 1)), then we\nhave \u2211 i\u2208N ri(T ) \u2264 nR/\u03b7 \u2264 n(n\u2212 1) \u221a 8R = O(1).\nThe corollary follows by noting that the condition \u03b2 \u2264 \u03b3/(n\u2212 1)2 is met with our choice of \u03b7. 2Here and in the sequel, we can use a different regularizer Ri for each player i, without qualitatively affecting any of the results. 3R is 1-strongly convex ifR ( u+v 2 ) \u2264 R(u)+R(v) 2 \u2212 \u2016u\u2212v\u2016 2 8 , \u2200u, v."}, {"heading": "3.1.2 Optimistic Follow the Regularized Leader", "text": "We next consider a different class of algorithms denoted as optimistic follow the regularized leader (OFTRL). This algorithm is similar but not equivalent to OMD, and is an analogous extension of standard FTRL [13]. This algorithm takes the same parameters as for OMD and is defined as follows: Let w0i = argminw\u2208\u2206(Si)R(w) and:\nwTi = argmax w\u2208\u2206(Si)\n\u2329 w,\nT\u22121\u2211 t=1 uti + M T i\n\u232a \u2212 R(w)\n\u03b7 .\nWe consider three variants of OFTRL with different choices of the sequence Mti, incorporating the recency bias in different forms.\nOne-step recency bias: The simplest form of OFTRL uses Mti = u t\u22121 i and obtains the following result, where R = maxi ( supf\u2208\u2206(Si)R(f)\u2212 inff\u2208\u2206(Si)R(f) ) .\nProposition 7. The OFTRL algorithm using stepsize \u03b7 and Mti = u t\u22121 i satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7 and \u03b3 = 1/(4\u03b7).\nCombined with Theorem 4, this yields the following constant bound on the total regret of all players: Corollary 8. If each player runs OFTRL with Mti = u t\u22121 i and \u03b7 = 1/(2(n \u2212 1)), then we have\u2211\ni\u2208N ri(T ) \u2264 nR/\u03b7 \u2264 2n(n\u2212 1)R = O(1).\nRakhlin and Sridharan [17] also analyze an FTRL variant, but require a self-concordant barrier for the constraint set as opposed to an arbitrary strongly convex regularizer, and their bound is missing the crucial negative terms of the RVU property which are essential for obtaining Theorem 4.\nH-step recency bias: More generally, given a window size H , one can define Mti =\u2211t\u22121 \u03c4=t\u2212H u \u03c4 i /H . We have the following proposition.\nProposition 9. The OFTRL algorithm using stepsize \u03b7 and Mti = \u2211t\u22121 \u03c4=t\u2212H u \u03c4 i /H satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7H2 and \u03b3 = 1/(4\u03b7).\nSetting \u03b7 = 1/(2H(n\u2212 1)), we obtain the analogue of Corollary 8, with an extra factor of H .\nGeometrically discounted recency bias: The next proposition considers an alternative form of recency bias which includes all the previous utilities, but with a geometric discounting. Proposition 10. The OFTRL algorithm using stepsize \u03b7 and Mti = 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4\n\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4u\u03c4i satisfies\nthe RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7/(1\u2212 \u03b4)3 and \u03b3 = 1/(8\u03b7).\nNote that these choices for Mti can also be used in OMD with qualitatively similar results."}, {"heading": "3.2 Fast Convergence of Individual Utilities", "text": "The previous section shows implications of the RVU property on the social welfare. This section complements these with a similar result for each player\u2019s individual utility. Theorem 11. Suppose that the players use algorithms satisfying the RVU property with parameters \u03b1 > 0, \u03b2 > 0, \u03b3 \u2265 0. If we further have the stability property \u2016wti \u2212 w t+1 i \u2016 \u2264 \u03ba, then for any\nplayer \u2211T t=1 \u3008w\u2217i \u2212wti ,uti\u3009 \u2264 \u03b1+ \u03b2\u03ba2(n\u2212 1)2T.\nSimilar reasoning as in Theorem 4 yields: \u2016uti \u2212 u t\u22121 i \u20162\u2217 \u2264 (n\u2212 1) \u2211 j 6=i \u2016wtj \u2212w t\u22121 j \u20162 \u2264 (n\u2212 1)2\u03ba2, and summing the terms gives the theorem.\nNoting that OFTRL satisfies the RVU property with constants given in Proposition 7 and stability property with \u03ba = 2\u03b7 (see Lemma 20 in the appendix), we have the following corollary.\nCorollary 12. If all players use the OFTRL algorithm with Mti = u t\u22121 i and \u03b7 = (n\u22121)\u22121/2T\u22121/4, then we have \u2211T t=1 \u3008w\u2217i \u2212wti ,uti\u3009 \u2264 (R+ 4) \u221a n\u2212 1 \u00b7 T 1/4.\nSimilar results hold for the other forms of recency bias, as well as for OMD. Corollary 12 gives a fast convergence rate of the players\u2019 strategies to the set of coarse correlated equilibria (CCE) of the game. This improves the previously known convergence rate \u221a T (e.g. [11]) to CCE using natural, decoupled no-regret dynamics defined in [4]."}, {"heading": "4 Robustness to Adversarial Opponent", "text": "So far we have shown simple dynamics with rapid convergence properties in favorable environments when each player in the game uses an algorithm with the RVU property. It is natural to wonder if this comes at the cost of worst-case guarantees when some players do not use algorithms with this property. Rakhlin and Sridharan [18] address this concern by modifying the OMD algorithm with additional smoothing and adaptive step-sizes so as to preserve the fast rates in the favorable case while still guaranteeing O(1/ \u221a T ) regret for each player, no matter how the opponents play. It is not so obvious how this modification might extend to other procedures, and it seems undesirable to abandon the black-box regret transformations we used to obtain Theorem 4. In this section, we present a generic way of transforming an algorithm which satisfies the RVU property so that it retains the fast convergence in favorable settings, but always guarantees a worst-case regret of O\u0303(1/ \u221a T ).\nIn order to present our modification, we need a parametric form of the RVU property which will also involve a tunable parameter of the algorithm. For most online learning algorithms, this will correspond to the step-size parameter used by the algorithm. Definition 13 (RVU(\u03c1) property). We say that a parametric algorithm A(\u03c1) satisfies the Regret bounded by Variation in Utilities(\u03c1) (RVU(\u03c1)) property with parameters \u03b1, \u03b2, \u03b3 > 0 and a pair of dual norms (\u2016 \u00b7 \u2016, \u2016 \u00b7 \u2016\u2217) if its regret on any sequence of utilities u1,u2, . . . ,uT is bounded as\nT\u2211 t=1 \u2329 w\u2217 \u2212wt,ut \u232a \u2264 \u03b1 \u03c1 + \u03c1\u03b2 T\u2211 t=1 \u2016ut \u2212 ut\u22121\u20162\u2217 \u2212 \u03b3 \u03c1 T\u2211 t=1 \u2016wt \u2212wt\u22121\u20162. (3)\nIn both OMD and OFTRL algorithms from Section 3, the parameter \u03c1 is precisely the stepsize \u03b7. We now show an adaptive choice of \u03c1 according to an epoch-based doubling schedule.\nBlack-box reduction. Given a parametric algorithm A(\u03c1) as a black-box we construct a wrapper A\u2032 based on the doubling trick: The algorithm of each player proceeds in epochs. At each epoch r the player i has an upper bound ofBr on the quantity \u2211T t=1 \u2016uti\u2212u t\u22121 i \u20162\u2217. We start with a parameter \u03b7\u2217 and B1 = 1, and for \u03c4 = 1, 2, . . . , T repeat:\n1. Play according to A(\u03b7r) and receive u\u03c4i . 2. If \u2211\u03c4 t=1 |uti \u2212 u t\u22121 i \u20162\u2217 \u2265 Br:\n(a) Update r \u2190 r + 1, Br \u2190 2Br, \u03b7r = min {\n\u03b1\u221a Br , \u03b7\u2217\n} , with \u03b1 as in Equation (3).\n(b) Start a new run of A with parameter \u03b7r. Theorem 14. Algorithm A\u2032 achieves regret at most the minimum of the following two terms:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 log(T )\n( 2 + \u03b1\n\u03b7\u2217 + (2 + \u03b7\u2217 \u00b7 \u03b2) T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 ) \u2212 \u03b3 \u03b7\u2217 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2; (4)\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 log(T ) 1 + \u03b1 \u03b7\u2217 + (1 + \u03b1 \u00b7 \u03b2) \u00b7 \u221a\u221a\u221a\u221a2 T\u2211 t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217  (5) That is, the algorithm satisfies the RVU property, and also has regret that can never exceed O\u0303( \u221a T ). The theorem thus yields the following corollary, which illustrates the stated robustness of A\u2032. Corollary 15. Algorithm A\u2032, with \u03b7\u2217 = \u03b3(2+\u03b2)(n\u22121)2 log(T ) , achieves regret O\u0303( \u221a T ) against any adversarial sequence, while at the same time satisfying the conditions of Theorem 4. Thereby, if all players use such an algorithm, then: \u2211 i\u2208N ri(T ) \u2264 n log(T )(\u03b1/\u03b7\u2217 + 2) = O\u0303(1).\nProof. Observe that for such \u03b7\u2217, we have that: (2 + \u03b7\u2217 \u00b7 \u03b2) log(T ) \u2264 (2 + \u03b2) log(T ) \u2264 \u03b3\u03b7\u2217(n\u22121)2 . Therefore, algorithm A\u2032, satisfies the sufficient conditions of Theorem 4.\nIf A(\u03c1) is the OFTRL algorithm, then we know by Proposition 7 that the above result applies with \u03b1 = R = maxwR(w), \u03b2 = 1, \u03b3 = 14 and \u03c1 = \u03b7. Setting \u03b7\u2217 = \u03b3 (2+\u03b2)(n\u22121)2 = 1 12(n\u22121)2 , the resulting algorithm A\u2032 will have regret at most: O\u0303(n2 \u221a T ) against an arbitrary adversary, while if\nall players use algorithm A\u2032 then \u2211 i\u2208N ri(T ) = O(n 3 log(T )).\nAn analogue of Theorem 11 can also be established for this algorithm:\nCorollary 16. If A satisfies the RVU(\u03c1) property, and also \u2016wti \u2212 w t\u22121 i \u2016 \u2264 \u03ba\u03c1, then A\u2032 with \u03b7\u2217 = T \u22121/4 achieves regret O\u0303(T 1/4) if played against itself, and O\u0303( \u221a T ) against any opponent.\nOnce again, OFTRL satisfies the above conditions with \u03ba = 2, implying robust convergence."}, {"heading": "5 Experimental Evaluation", "text": "We analyzed the performance of optimistic follow the regularized leader with the entropy regularizer, which corresponds to the Hedge algorithm [8] modified so that the last iteration\u2019s utility for each strategy is double counted; we refer to it as Optimistic Hedge. More formally, the probability of player i playing strategy j at iteration T is proportional to exp ( \u2212\u03b7 \u00b7 (\u2211T\u22122 t=1 u t ij + 2u T\u22121 ij )) , rather\nthan exp ( \u2212\u03b7 \u00b7 \u2211T\u22121 t=1 u t ij ) as is standard for Hedge.\nWe studied a simple auction where n players are bidding for m items. Each player has a value v for getting at least one item and no extra value for more items. The utility of a player is the value for the allocation he derived minus the payment he has to make. The game is defined as follows: simultaneously each player picks one of the m items and submits a bid on that item (we assume bids to be discretized). For each item, the highest bidder wins and pays his bid. We let players play this game repeatedly with each player invoking either Hedge or optimistic Hedge. This game, and generalizations of it, are known to be (1 \u2212 1/e, 0)-smooth [22], if we also view the auctioneer as a player whose utility is the revenue. The welfare of the game is the value of the resulting allocation, hence not a constant-sum game. The welfare maximization problem corresponds to the unweighted bipartite matching problem. The POA captures how far from the optimal matching is the average allocation of the dynamics. By smoothness we know it converges to at least 1\u2212 1/e of the optimal.\nFast convergence of individual and average regret. We run the game for n = 4 bidders and m = 4 items and valuation v = 20. The bids are discretized to be any integer in [1, 20]. We find that the sum of the regrets and the maximum individual regret of each player are remarkably lower under Optimistic Hedge as opposed to Hedge. In Figure 1 we plot the maximum individual regret as well as the sum of the regrets under the two algorithms, using \u03b7 = 0.1 for both methods. Thus convergence to the set of coarse correlated equilibria is substantially faster under Optimistic Hedge,\nconfirming our results in Section 3.2. We also observe similar behavior when each player only has value on a randomly picked player-specific subset of items, or uses other step sizes.\nMore stable dynamics. We observe that the behavior under Optimistic Hedge is more stable than under Hedge. In Figure 2, we plot the expected bid of a player on one of the items and his expected utility under the two dynamics. Hedge exhibits the sawtooth behavior that was observed in generalized first price auction run by Overture (see [5, p. 21]). In stunning contrast, Optimistic Hedge leads to more stable expected bids over time. This stability property of optimistic Hedge is one of the main intuitive reasons for the fast convergence of its regret.\nWelfare. In this class of games, we did not observe any significant difference between the average welfare of the methods. The key reason is the following: the proof that no-regret dynamics are approximately efficient (Proposition 2) only relies on the fact that each player does not have regret against the strategy s\u2217i used in the definition of a smooth game. In this game, regret against these strategies is experimentally comparable under both algorithms, even though regret against the best fixed strategy is remarkably different. This indicates a possibility for faster rates for Hedge in terms of welfare. In Appendix H, we show fast convergence of the efficiency of Hedge for costminimization games, though with a worse POA ."}, {"heading": "6 Discussion", "text": "This work extends and generalizes a growing body of work on decentralized no-regret dynamics in many ways. We demonstrate a class of no-regret algorithms which enjoy rapid convergence when played against each other, while being robust to adversarial opponents. This has implications in computation of correlated equilibria, as well as understanding the behavior of agents in complex multi-player games. There are a number of interesting questions and directions for future research which are suggested by our results, including the following:\nConvergence rates for vanilla Hedge: The fast rates of our paper do not apply to algorithms such as Hedge without modification. Is this modification to satisfy RVU only sufficient or also necessary? If not, are there counterexamples? In the supplement, we include a sketch hinting at such a counterexample, but also showing fast rates to a worse equilibrium than our optimistic algorithms.\nConvergence of players\u2019 strategies: The OFTRL algorithm often produces much more stable trajectories empirically, as the players converge to an equilibrium, as opposed to say Hedge. A precise quantification of this desirable behavior would be of great interest.\nBetter rates with partial information: If the players do not observe the expected utility function, but only the moves of the other players at each round, can we still obtain faster rates?"}, {"heading": "A Proof of Proposition 2", "text": "Proposition 2. In a (\u03bb, \u00b5)-smooth game, if each player i suffers regret at most ri(T ), then:\n1\nT T\u2211 t=1 W (wt) \u2265 \u03bb 1 + \u00b5 OPT \u2212 1 1 + \u00b5 1 T \u2211 i\u2208N ri(T ) = 1 \u03c1 OPT \u2212 1 1 + \u00b5 1 T \u2211 i\u2208N ri(T ),\nwhere the factor \u03c1 = (1 + \u00b5)/\u03bb is called the price of total anarchy (POA).\nProof. Since each player i has regret ri(T ), we have that:\nT\u2211 t=1 \u2329 wti ,u t i \u232a \u2265 T\u2211 t=1 uti,s\u2217i \u2212 ri(T ) (6)\nSumming over all players and using the smoothness property:\nT\u2211 t=1 W (wt) = T\u2211 t=1 \u2211 i\u2208N \u2329 wti ,u t i \u232a \u2265 T\u2211 t=1 \u2211 i\u2208N uti,s\u2217i \u2212 \u2211 i\u2208N ri(T )\n= T\u2211 t=1 Es\u223cwt [\u2211 i\u2208N ui(s \u2217 i , s\u2212i) ] \u2212 \u2211 i\u2208N ri(T )\n\u2265 T\u2211 t=1 (\u03bbOPT \u2212 \u00b5Es\u223cwt [W (s)])\u2212 \u2211 i\u2208N ri(T )\n= T\u2211 t=1 ( \u03bbOPT \u2212 \u00b5W (wt) ) \u2212 \u2211 i\u2208N ri(T )\nBy re-arranging we get the result."}, {"heading": "B Proof of Proposition 5", "text": "Proposition 5. The OMD algorithm using stepsize \u03b7 and Mti = u t\u22121 i satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7, \u03b3 = 1/(8\u03b7), where R = maxi supf DR(f,g 0 i ).\nWe will use the following theorem of [18].\nTheorem 17 (Raklin and Sridharan [18]). The regret of a player under optimistic mirror descent and with respect to any w\u2217i \u2208 \u2206(Si) is upper bounded by: T\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + T\u2211 t=1 \u2016uti \u2212Mti\u2016\u2217\u2016wti \u2212 gti\u2016 \u2212 1 2\u03b7 T\u2211 t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 gt\u22121i \u2016 2 ) (7) where R = supf DR(f, g0).\nWe show that if the players use optimistic mirror descent with Mti = u t\u22121 i , then the regret of each player satisfies the sufficient condition presented in the previous section. Some of the key facts (Equations (9) and (10)) that we use in the following proof appear in [18]. However, the formulation of the regret that we present in the following theorem is not immediately clear in their proof, so we present it here for clarity and completeness.\nTheorem 18. The regret of a player under optimistic mirror descent with Mti = u t\u22121 i and with respect to any w\u2217i \u2208 \u2206(Si) is upper bounded by: T\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 8\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2 (8)\nProof. By Theorem 17, instantiated for Mti = u t\u22121 i , we get:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016\u2217\u2016w t i \u2212 gti\u2016\n\u2212 1 2\u03b7 T\u2211 t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 gt\u22121i \u2016 2 )\nUsing the fact that for any \u03c1 > 0:\n\u2016uti \u2212Mti\u2016\u2217\u2016wti \u2212 gti\u2016 \u2264 \u03c1\n2 \u2016uti \u2212Mti\u20162\u2217 +\n1\n2\u03c1 \u2016wti \u2212 gti\u20162 (9)\nWe get: T\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03c1 2 T\u2211 t=1 \u2016uti\u2212ut\u22121i \u2016 2 \u2217\u2212 ( 1 2\u03b7 \u2212 1 2\u03c1 ) T\u2211 t=1 \u2016wti\u2212gti\u20162\u2212 1 2\u03b7 T\u2211 t=1 \u2016wti\u2212gt\u22121i \u2016 2\nFor \u03c1 = 2\u03b7, the latter simplifies to: T\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212 gti\u20162 \u2212 1 2\u03b7 T\u2211 t=1 \u2016wti \u2212 gt\u22121i \u2016 2\n\u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212 gti\u20162 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212 gt\u22121i \u2016 2\nLast we use the fact that:\n\u2016wti \u2212wt\u22121i \u2016 2 \u2264 2\u2016wti \u2212 gt\u22121i \u2016 2 + 2\u2016wt\u22121i \u2212 g t\u22121 i \u2016 2 (10)\nSumming over all timesteps: T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2 \u2264 2 T\u2211 t=1 \u2016wti \u2212 gt\u22121i \u2016 2 + 2 T\u2211 t=1 \u2016wt\u22121i \u2212 g t\u22121 i \u2016 2\n\u2264 2 T\u2211 t=1 \u2016wti \u2212 gt\u22121i \u2016 2 + 2 T\u2211 t=1 \u2016wti \u2212 gti\u20162\nDividing over by 18\u03b7 and applying it in the previous upper bound on the regret, we get:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 8\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2"}, {"heading": "C Proof of Proposition 7", "text": "Proposition 7. The OFTRL algorithm using stepsize \u03b7 and Mti = u t\u22121 i satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7 and \u03b3 = 1/(4\u03b7).\nWe first show that these algorithms achieve the same regret bounds as optimistic mirror descent. This result does not appear in previous work in any form.\nEven though the algorithms do not make use of a secondary sequence, we will still use in the analysis the notation:\ngTi = argmax g\u2208\u2206(Si)\n\u2329 g,\nT\u2211 t=1 uti\n\u232a \u2212 R(g)\n\u03b7 .\nThese secondary variables are often called be the leader sequence as they can see one step in the future.\nTheorem 19. The regret of a player under optimistic FTRL and with respect to any w\u2217i \u2208 \u2206(Si) is upper bounded by:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + T\u2211 t=1 \u2016uti \u2212Mti\u2016\u2217\u2016wti \u2212 gti\u2016 \u2212 1 2\u03b7 T\u2211 t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 gt\u22121i \u2016 2 )\n(11) where R = supf R(f)\u2212 inff R(f).\nProof. First observe that:\n\u2329 w\u2217i \u2212wti ,uti \u232a = \u2329 gti \u2212wti ,uti \u2212Mti \u232a + \u2329 gti \u2212wti ,Mti \u232a + \u2329 w\u2217i \u2212 gti ,uti \u232a (12)\nWithout loss of generality we will assume that inff R(f) = 0. Since \u3008gti \u2212wti ,uti \u2212Mti\u3009 \u2264 \u2016gti \u2212 wti\u2016\u2016uti \u2212Mti\u2016\u2217, it suffices to show that for any w\u2217i \u2208 \u2206(Si):\nT\u2211 t=1 (\u2329 gti \u2212wti ,Mti \u232a + \u2329 w\u2217i \u2212 gti ,uti \u232a) \u2264 R(w \u2217 i ) \u03b7 \u2212 1 2\u03b7 T\u2211 t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 gt\u22121i \u2016 2 ) (13)\nFor shorthand notation let: IT = 12\u03b7 \u2211T t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 g t\u22121 i \u20162 ) . By induction assume that for all w\u2217i :\nT\u22121\u2211 t=1 (\u2329 gti \u2212wti ,Mti \u232a \u2212 \u2329 gti ,u t i \u232a) \u2264 \u2212 T\u22121\u2211 t=1 \u2329 w\u2217i ,u t i \u232a + R(w\u2217i ) \u03b7 \u2212 IT\u22121\n= \u2212 \u2329 w\u2217i , T\u22121\u2211 t=1 uti \u232a + R(w\u2217i ) \u03b7 \u2212 IT\u22121\nApply the above for w\u2217i = g T\u22121 i and add \u2329 gTi \u2212wTi ,MTi \u232a \u2212 \u2329 gTi ,u T i \u232a on both sides: T\u2211 t=1 (\u2329 gti \u2212wti ,Mti \u232a \u2212 \u2329 gti ,u t i \u232a) \u2264 \u2212 \u2329 gT\u22121i , T\u22121\u2211 t=1 uti \u232a + R(gT\u22121i ) \u03b7 \u2212 IT\u22121 + \u2329 gTi \u2212wTi ,MTi \u232a \u2212 \u2329 gTi ,u T i\n\u232a \u2264 \u2212 \u2329 wTi ,\nT\u22121\u2211 t=1 uti \u232a + R(wTi ) \u03b7 \u2212 IT\u22121 + \u2329 gTi \u2212wTi ,MTi \u232a \u2212 \u2329 gTi ,u T i \u232a \u2212 1\n2\u03b7 \u2016wTi \u2212 gT\u22121i \u2016 2\n= \u2212 \u2329 wTi , T\u22121\u2211 t=1 uti + M T i \u232a + R(wTi ) \u03b7 \u2212 IT\u22121 + \u2329 gTi ,M T i \u232a \u2212 \u2329 gTi ,u T i \u232a \u2212 1\n2\u03b7 \u2016wTi \u2212 gT\u22121i \u2016 2\n\u2264 \u2212 \u2329 gTi , T\u22121\u2211 t=1 uti + M T i \u232a + R(gTi ) \u03b7 \u2212 IT\u22121 + \u2329 gTi ,M T i \u232a \u2212 \u2329 gTi ,u T i \u232a \u2212 1\n2\u03b7 \u2016wTi \u2212 gT\u22121i \u2016 2 \u2212 1 2\u03b7 \u2016wTi \u2212 gTi \u20162\n= \u2212 \u2329 gTi , T\u2211 t=1 uti \u232a + R(gTi ) \u03b7 \u2212 IT\n\u2264 \u2212 \u2329 q\u2217i , T\u2211 t=1 uti \u232a + R(q\u2217i ) \u03b7 \u2212 IT\nThe inequalities follow by the optimality of the corresponding variable that was changed and by the strong convexity of R(\u00b7). The final vector q\u2217i is an arbitrary vector in \u2206(Si). The base case of T = 0 follows trivially byR(f) \u2265 0 for all f . This concludes the inductive proof.\nThus optimistic FTRL achieves the exact same form of regret presented in Theorem 17 for optimistic mirror descent. Hence, the equivalent versions of Theorem 18 and Corollary 6 hold also for the optimistic FTRL algorithm. In fact we are able to show slightly stronger bounds for optimistic FTRL, based on the following lemmas.\nLemma 20 (Stability). For the optimistic FTRL algorithm:\n\u2016wti \u2212 gti\u2016 \u2264 \u03b7 \u00b7 \u2016Mti \u2212 uti\u2016\u2217 (14) \u2016gti \u2212wt+1i \u2016 \u2264 \u03b7 \u00b7 \u2016M t+1 i \u2016\u2217 (15)\nProof. Let FT (f) = \u2329 f , \u2211T\u22121 t=1 u t i + M T i \u232a \u2212 \u03b7\u22121R(f) and GT (f) = \u2329 f , \u2211T t=1 u t i \u232a \u2212 \u03b7\u22121R(f).\nObserve that: FT (f)\u2212GT (f) = \u2329 f ,MTi \u2212 uTi \u232a and FT+1(f)\u2212GT (f) = \u2329 f ,MT+1i \u232a .\nPart 1 By the optimality of wTi and gTi and the strong convexity ofR(\u00b7):\nFT (w T i ) \u2265 FT (gTi ) +\n1\n2\u03b7 \u2016wTi \u2212 gTi \u20162\nGT (g T i ) \u2265 GT (wTi ) +\n1\n2\u03b7 \u2016wTi \u2212 gTi \u20162\nAdding both inequalities and using the previous observations:\n1 \u03b7 \u2016wTi \u2212 gTi \u20162 \u2264\n\u2329 wTi \u2212 gTi ,MTi \u2212 uTi \u232a \u2264 \u2016wTi \u2212 gTi \u2016 \u00b7 \u2016MTi \u2212 uTi \u2016\u2217\nDividing over by \u2016wTi \u2212 gTi \u2016 gives the first inequality of the lemma.\nPart 2 By the optimality of gTi and w T+1 i and strong convexity:\nFT+1(w T+1 i ) \u2265 FT+1(g T i ) +\n1\n2\u03b7 \u2016wT+1i \u2212 g T i \u20162\nGT (g T i ) \u2265 GT (wT+1i ) +\n1\n2\u03b7 \u2016wT+1i \u2212 g T i \u20162\nAdding the inequalities:\n1 \u03b7 \u2016wT+1i \u2212 g T i \u20162 \u2264\n\u2329 wT+1i \u2212 g T i ,M T+1 i \u232a \u2264 \u2016wT+1i \u2212 g T i \u2016 \u00b7 \u2016MT+1i \u2016\u2217\nDividing over by \u2016wT+1i \u2212 gTi \u2016, yields second inequality of the lemma.\nGiven Theorem 19 and Lemma 20, the proposition immediately follows since\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212Mti\u20162\u2217 \u2212 1 2\u03b7 T\u2211 t=1 ( \u2016wti \u2212 gti\u20162 + \u2016wti \u2212 gt\u22121i \u2016 2 ) .\nReplacing Mti with u t\u22121 i and using Inequality (10), yields the result."}, {"heading": "D Proof of Proposition 9", "text": "Proposition 9. The OFTRL algorithm using stepsize \u03b7 and Mti = \u2211t\u22121 \u03c4=t\u2212H u \u03c4 i /H satisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7H2 and \u03b3 = 1/(4\u03b7).\nThe proposition is equivalent to the following lemma, which we will state and prove in this appendix.\nLemma 21. For the optimistic FTRL algorithm with Mti = 1 H \u2211t\u22121 \u03c4=t\u2212H u \u03c4 i , the regret is upper bounded by:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7H2 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2 (16)\nwhereR = supf R(f)\u2212inff R(f). Thus we get \u2211 i ri(T ) \u2264 nR \u03b7 = 2n(n\u22121)HR for \u03b7 = 1 2H(n\u22121) .\nProof. Similar to Proposition 7, by Theorem 19, Lemma 20 and Inequality (10) we get:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212Mti\u20162\u2217 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\n= R\n\u03b7 + \u03b7 T\u2211 t=1 \u2225\u2225\u2225\u2225\u2225uti \u2212 1H t\u22121\u2211 \u03c4=t\u2212H u\u03c4i \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n\u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\n= R\n\u03b7 + \u03b7 T\u2211 t=1\n( 1\nH t\u22121\u2211 \u03c4=t\u2212H \u2225\u2225uti \u2212 u\u03c4i \u2225\u2225\u2217 )2 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\nBy triangle inequality:\n1\nH t\u22121\u2211 \u03c4=t\u2212H \u2225\u2225uti \u2212 u\u03c4i \u2225\u2225\u2217 \u2264 1H t\u22121\u2211 \u03c4=t\u2212H t\u22121\u2211 q=\u03c4 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 =\nt\u22121\u2211 \u03c4=t\u2212H t\u2212 \u03c4 H \u2225\u2225u\u03c4+1i \u2212 u\u03c4i \u2225\u2225\u2217 \u2264 t\u22121\u2211 \u03c4=t\u2212H \u2225\u2225u\u03c4+1i \u2212 u\u03c4i \u2225\u2225\u2217\nBy Cauchy-Schwarz:( t\u22121\u2211\n\u03c4=t\u2212H\n\u2225\u2225u\u03c4+1i \u2212 u\u03c4i \u2225\u2225\u2217 )2 \u2264 H t\u22121\u2211 \u03c4=t\u2212H \u2225\u2225u\u03c4+1i \u2212 u\u03c4i \u2225\u22252\u2217 Thus we can derive that:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7H T\u2211 t=1 t\u22121\u2211 \u03c4=t\u2212H \u2225\u2225u\u03c4+1i \u2212 u\u03c4i \u2225\u22252\u2217 \u2212 14\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\n\u2264 R \u03b7 + \u03b7H2 T\u2211 t=1 \u2225\u2225uti \u2212 ut\u22121i \u2225\u22252\u2217 \u2212 14\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2"}, {"heading": "E Proof of Proposition 10", "text": "Proposition 10. The OFTRL algorithm using stepsize \u03b7 and Mti = 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4\n\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4u\u03c4i\nsatisfies the RVU property with constants \u03b1 = R/\u03b7, \u03b2 = \u03b7/(1\u2212 \u03b4)3 and \u03b3 = 1/(8\u03b7). The proposition is equivalent to the following lemma which we will prove in this appendix. Lemma 22. For the optimistic FTRL algorithm with Mti = 1\u2211t\n\u03c4=0 \u03b4 \u2212\u03c4\n\u2211t\u22121 \u03c4=0 \u03b4\n\u2212\u03c4u\u03c4i for some discount rate \u03b4 \u2208 (0, 1), the regret is upper bounded by:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 (1\u2212 \u03b4)3 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 1 8\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2 (17)\nwhere R = supf R(f) \u2212 inff R(f). Thus we get \u2211 i ri(T ) \u2264 nR \u03b7 = 2n(n \u2212 1) 1 (1\u2212\u03b4)3/2R for \u03b7 = (1\u2212\u03b4) 3/2\n2(n\u22121) .\nProof. We show the theorem for the case of optimistic FTRL. The OMD case follows analogously. Similar to Lemma 21 the regret is upper bounded by:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 R \u03b7 + \u03b7 T\u2211 t=1 \u2016uti \u2212Mti\u20162\u2217 \u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\n= R\n\u03b7 + \u03b7 T\u2211 t=1 \u2225\u2225\u2225\u2225\u2225uti \u2212 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4u\u03c4i \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n\u2212 1 4\u03b7 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2\nWe will now show that:\nT\u2211 t=1 \u2225\u2225\u2225\u2225\u2225uti \u2212 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4u\u03c4i \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n\u2264 1 (1\u2212 \u03b4)3 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217\nwhich will conclude the proof.\nFirst observe by triangle inequality:\n\u2225\u2225\u2225\u2225\u2225uti \u2212 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4u\u03c4i \u2225\u2225\u2225\u2225\u2225 \u2217 = 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4\u2016uti \u2212 u\u03c4i \u2016\u2217\n\u2264 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4 t\u22121\u2211 q=\u03c4 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 =\n1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 q=0 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 q\u2211 \u03c4=0 \u03b4\u2212\u03c4\n= 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 q=0 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 \u03b4\u2212q 1\u2212 \u03b4q+11\u2212 \u03b4 \u2264 1\n1\u2212 \u03b4 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217\nBy Cauchy-Schwarz:\n( 1\n1\u2212 \u03b4 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 )2 =\n1 (1\u2212 \u03b4)2 1(\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 )2 ( t\u22121\u2211 q=0 \u03b4\u2212q/2 \u00b7 \u03b4\u2212q/2 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u2225\u2217 )2\n\u2264 1 (1\u2212 \u03b4)2 1(\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 )2 t\u22121\u2211 q=0 \u03b4\u2212q \u00b7 t\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\n= 1 (1\u2212 \u03b4)2 1\u2211t\u22121\n\u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\n= 1 (1\u2212 \u03b4)2 1\u2211t\u22121\n\u03c4=0 \u03b4 t\u2212\u03c4 t\u22121\u2211 q=0 \u03b4t\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\n\u2264 1 \u03b4(1\u2212 \u03b4)2 t\u22121\u2211 q=0 \u03b4t\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\nCombining we get:\n\u2225\u2225\u2225\u2225\u2225uti \u2212 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4u\u03c4i \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n\u2264 1 \u03b4(1\u2212 \u03b4)2 t\u22121\u2211 q=0 \u03b4t\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\nSumming over all t and re-arranging we get:\nT\u2211 t=1 \u2225\u2225\u2225\u2225\u2225uti \u2212 1\u2211t\u22121 \u03c4=0 \u03b4 \u2212\u03c4 t\u22121\u2211 \u03c4=0 \u03b4\u2212\u03c4u\u03c4i \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n\u2264 1 \u03b4(1\u2212 \u03b4)2 T\u2211 t=1 t\u22121\u2211 q=0 \u03b4t\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217\n= 1 \u03b4(1\u2212 \u03b4)2 T\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217 T\u2211 t=q+1 \u03b4t\n= 1 \u03b4(1\u2212 \u03b4)2 T\u22121\u2211 q=0 \u03b4\u2212q \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217 \u03b4(\u03b4q \u2212 \u03b4T )1\u2212 \u03b4\n= 1 (1\u2212 \u03b4)3 T\u22121\u2211 q=0 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217 (1\u2212 \u03b4T\u2212q) \u2264 1\n(1\u2212 \u03b4)3 T\u22121\u2211 q=0 \u2225\u2225\u2225uq+1i \u2212 uqi\u2225\u2225\u22252\u2217"}, {"heading": "F Proof of Theorem 14", "text": "Theorem 14. Algorithm A\u2032 achieves regret at most the minimum of the following two terms: T\u2211\nt=1\n\u2329 w\u2217i \u2212wti ,uti \u232a \u2264 log(T ) ( 2 + \u03b1\n\u03b7\u2217 + (2 + \u03b7\u2217 \u00b7 \u03b2) T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 ) \u2212 \u03b3 \u03b7\u2217 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2;\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 log(T ) 1 + \u03b1 \u03b7\u2217 + (1 + \u03b1 \u00b7 \u03b2) \u00b7 \u221a\u221a\u221a\u221a2 T\u2211 t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217 \nProof. We break the proof in the two corresponding parts. First part. Consider a round r and let Tr be its final iteration. Also let Ir = \u2211Tr t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217. First observe that by the definition of Br:\n1 2 Ir \u2264 Br \u2264 2 \u00b7 Ir + 1 (18)\nBy the definition of \u03b7, we know that\n1 \u03b7\u2217 \u2264 1 \u03b7 < 1 \u03b7\u2217 + \u221a Br \u03b1 . (19)\nBy the regret guarantee of algorithm A(\u03b7r), we have that: Tr\u2211\nt=Tr\u22121+1\n\u2329 w\u2217i \u2212wti ,uti \u232a \u2264 \u03b1\n\u03b7 + \u03b7 \u00b7 \u03b2 Tr\u2211 t=Tr\u22121+1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 \u03b3 \u03b7 Tr\u2211 t=Tr\u22121+1 \u2016wti \u2212wt\u22121i \u2016 2\n\u2264 \u03b1 \u03b7\u2217\n+ \u221a Br + \u03b7\u2217 \u00b7 \u03b2 Tr\u2211 t=Tr\u22121+1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 \u03b3 \u03b7\u2217 Tr\u2211 t=Tr\u22121+1 \u2016wti \u2212wt\u22121i \u2016 2\n\u2264 \u03b1 \u03b7\u2217\n+ \u221a Br + \u03b7\u2217 \u00b7 \u03b2 T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 \u03b3 \u03b7\u2217 Tr\u2211 t=Tr\u22121+1 \u2016wti \u2212wt\u22121i \u2016 2\nSince \u221a Br \u2264 Br + 1 \u2264 2 \u00b7 Ir + 2:\nTr\u2211 t=Tr\u22121+1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 \u03b1 \u03b7\u2217 + 2 + (2 + \u03b7\u2217 \u00b7 \u03b2) T\u2211 t=1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2212 \u03b3 \u03b7\u2217 Tr\u2211 t=Tr\u22121+1 \u2016wti \u2212wt\u22121i \u2016 2\nSince at each round we are doubling the bound Br and since \u2211T t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217 \u2264 T , there are at most log(T ) rounds. Summing up the above inequality for each of the at most log(T ) rounds, yields the claimed bound in Equation (4).\nSecond part. Again consider any round r. By Equations (18), (19), the fact that \u03b7 \u2264 \u03b1\u221a Br \u2264 \u03b1 \u221a 2\u221a Ir and by the regret of algorithm A(\u03b7r):\nTr\u2211 t=Tr\u22121+1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 \u03b1 \u03b7\u2217 + \u221a Br + \u03b7 \u00b7 \u03b2 Tr\u2211 t=Tr\u22121+1 \u2016uti \u2212 ut\u22121i \u2016 2 \u2217\n\u2264 \u03b1 \u03b7\u2217\n+ \u221a Br + \u03b7 \u00b7 \u03b2 \u00b7 Ir\n\u2264 \u03b1 \u03b7\u2217\n+ \u221a Br + \u03b1 \u00b7 \u03b2 \u00b7 \u221a 2Ir\n\u2264 \u03b1 \u03b7\u2217\n+ \u221a 2Ir + 1 + \u03b1 \u00b7 \u03b2 \u00b7 \u221a 2Ir\n\u2264 \u03b1 \u03b7\u2217\n+ 1 + \u221a 2Ir + \u03b1 \u00b7 \u03b2 \u00b7 \u221a 2Ir\n\u2264 \u03b1 \u03b7\u2217 + 1 + (1 + \u03b1 \u00b7 \u03b2) \u221a\u221a\u221a\u221a2 T\u2211 t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217\nAgain since the number of rounds is at most log(T ), by summing up the above bound for each round r, we get the second part of the theorem."}, {"heading": "G Proof of Corollary 16", "text": "Corollary 16. If A satisfies the RVU(\u03c1) property, and also \u2016wti \u2212 w t\u22121 i \u2016 \u2264 \u03ba\u03c1, then A\u2032 with \u03b7\u2217 = T \u22121/4 achieves regret O\u0303(T 1/4) if played against itself, and O\u0303( \u221a T ) against any opponent.\nProof. Observe that at any round of A\u2032, algorithm A is run with \u03b7r \u2264 \u03b7\u2217. Thus by the property of algorithm A, we have that at every iteration: \u2016wti \u2212 w t\u22121 i \u2016 \u2264 \u03ba\u03b7\u2217 = \u03baT\u22121/4. If all players use algorithm A\u2032, then by similar reasoning as in Theorem 4 we know that:\n\u2016uti \u2212 ut\u22121i \u2016 2 \u2217 \u2264 (n\u2212 1) \u2211 j 6=i \u2016wtj \u2212wt\u22121j \u2016 2 \u2264 (n\u2212 1)2\u03b32\u03b72\u2217 = (n\u2212 1)2\u03ba2T\u22121/2\nHence, by Equation 5, the regret of each player is bounded by:\nT\u2211 t=1 \u2329 w\u2217i \u2212wti ,uti \u232a \u2264 log(T )  \u03b1 \u03b7\u2217 + (1 + \u03b1 \u00b7 \u03b2) \u00b7 \u221a\u221a\u221a\u221a T\u2211 t=1 \u2016uti \u2212 u t\u22121 i \u20162\u2217  \u2264 log(T ) ( \u03b1T 1/4 + (1 + \u03b1 \u00b7 \u03b2) \u00b7 \u221a T \u00b7 (n\u2212 1)2\u03ba2T\u22121/2\n) = log(T ) ( \u03b1T 1/4 + (1 + \u03b1 \u00b7 \u03b2) \u00b7 (n\u2212 1)\u03baT 1/4 ) = O\u0303(T 1/4)"}, {"heading": "H Fast convergence via a first order regret bound for cost-minimization", "text": "In this section, we show how a different regret bound can also lead to a fast convergence rate for a smooth game. For some technical reasons we consider cost instead of utility throughout this section. We use ci : S1 \u00d7 . . . \u00d7 Sn \u2192 [0, 1] to denote the cost function, and similarly to previous sections C(s) = \u2211 i\u2208N ci(s), C(w) = Es\u223cw[C(s)],OPT\n\u2032 = mins\u2208S1\u00d7...\u00d7Sn C(s). A game is (\u03bb, \u00b5)-smooth if there exists a strategy profile s\u2217, such that for any strategy profile s:\u2211\ni\u2208N ci(s\n\u2217 i , s\u2212i) \u2264 \u03bbOPT\u2032 + \u00b5C(s). (20)\nNow suppose each player i uses a no-regret algorithm to produce wti on each round and receives cost cti,s = Es\u2212i\u223cwt\u2212i [ci(s, s\u2212i)] for each strategy s \u2208 Si. Moreover, for any fixed strategy s, the no-regret algorithm ensures\nT\u2211 t=1 \u2329 wti , c t i \u232a \u2212 T\u2211 t=1 cti,s \u2264 A1 \u221a\u221a\u221a\u221alog d( T\u2211 t=1 cti,s ) +A2 log d (21)\nfor some absolute constants A1 and A2. Note that this form of first order bound can be achieved by a variety of algorithms such as Hedge with appropriate learning rate tuning. Under this setup, we prove the following: Theorem 23. If a game is (\u03bb, \u00b5)-smooth and each player uses a no-regret algorithm with a regret satisfying Eq. (21), then we have\n1\nT T\u2211 t=1 C(wt) \u2264 \u03bb(1 + \u00b5) \u00b5(1\u2212 \u00b5) OPT\u2032 + An log d T\nwhere A = A 2 1\u00b5\n(1\u2212\u00b5)2 + 2A2 1\u2212\u00b5 .\nProof. Using the regret bound and Cauchy-Schwarz inequality, we have T\u2211 t=1 C(wt) = T\u2211 t=1 \u2211 i\u2208N \u2329 wti , c t i \u232a\n\u2264 T\u2211 t=1 \u2211 i\u2208N cti,s\u2217i +A1 \u221a log d \u2211 i\u2208N \u221a\u221a\u221a\u221a T\u2211 t=1 cti,s\u2217i +A2n log d \u2264 T\u2211 t=1 \u2211 i\u2208N cti,s\u2217i +A1 \u221a n log d \u221a\u221a\u221a\u221a T\u2211 T=1 \u2211 i\u2208N cti,s\u2217i +A2n log d. (22)\nBy the smoothness assumption, we have\u2211 i\u2208N cti,s\u2217i = Es\u223cwt [\u2211 i\u2208N ci(s \u2217 i , s\u2212i) ] \u2264 \u03bbOPT\u2032 + \u00b5Es\u223cwt [C(s)] = \u03bbOPT\u2032 + \u00b5C(wt),\nand therefore \u2211T t=1 \u2211 i\u2208N c t i,s\u2217i \u2264 x2 where we define x = \u221a \u03bbTOPT\u2032 + \u00b5 \u2211T t=1 C(w\nt). Now applying this bound in Eq. (22), we continue with\n1\n\u00b5\n( x2 \u2212 \u03bbTOPT\u2032 ) \u2264 x2 + (A1 \u221a n log d)x+A2n log d.\nRearranging gives a quadratic inequality ax2 + bx+ c \u2264 0 with\na = 1\u2212 \u00b5 \u00b5 , b = \u2212A1 \u221a n ln d, c = \u2212\u03bb \u00b5 TOPT\u2032 \u2212A2n log d,\nand solving for x gives\nx \u2264 \u00b5 2(1\u2212 \u00b5)\n(\u2212b+ \u221a b2 \u2212 4ac) \u2264 \u00b5 1\u2212 \u00b5 \u221a b2 \u2212 2ac.\nFinally solving for \u2211T t=1 C(w t) (hidden in the definition of x) gives the bound stated in the theorem.\nNote that the price of total anarchy is larger than the one achieved by previous analysis by a multiplicative factor of 1 + 1\u00b5 , but the convergence rate is much faster (n times faster compared to optimistic mirror descent or optimistic FTRL)."}, {"heading": "I Extension to continuous strategy space games", "text": "In this section we extend our results to continuous strategy space games such as for instance \u201dsplittable selfish routing games\u201d (see e.g. [20]). These are games where the price of anarchy has been well studied and quite well motivated from internet routing. In these games we consider the dynamics where the players simply observe the past play of their opponents and not the expected past play. We consider dynamics where players don\u2019t use mixed strategies, but are simply doing online convex optimization algorithms on their continuous strategy spaces. Such learning on continuous games has also been studied in more restrictive settings in [6].\nIn this setting we will consider the following setting: each player i has a strategy space Si which is a closed convex set in Rd. In this setting we will denote with wi \u2208 Si a strategy of a player4. Given a profile of strategies w = (w1, . . . ,wn), each player incurs a cost ci(w) (equivalently a utility function ui(w).\nWe make the following two assumptions on the costs:\n1. (Convex in player strategy) For each player i and for each profile of opponent strategies w\u2212i, the function ci(\u00b7,w\u2212i) is convex in wi. 2. (Lipschitz gradient) For each player i, the function \u03b4i(w) = \u2207ici(w),5 is L-Lipschitz continuous with respect to the \u2016 \u00b7 \u20161 norm and if w\u2212i \u2208 R(n\u22121)d is viewed as a vector in the (n\u2212 1) \u00b7 d dimensional space, i.e.:\n\u2016\u03b4i(w)\u2212 \u03b4i(y)\u2016\u2217 \u2264 L \u00b7 \u2211 j \u2016wj \u2212 yj\u2016 (23)\nObserve that a sufficient condition for Property (2) is that the function \u03b4i(w) is coordinate-wise L-lipschitz with respect to the \u2016 \u00b7 \u2016 norm. Lemma 24. If for any j:\n\u2016\u03b4i(w)\u2212 \u03b4i(yj ,w\u2212j)\u2016\u2217 \u2264 L\u2016wj \u2212 yj\u2016 (24) then \u03b4i(\u00b7) satisfies Property (2).\nProof. For any two vectors w and y, think of switching from the one to the other by switching sequentially each player from his strategy wi to yi, keeping the remaining players fixed and in some pre-fixed player order. The difference \u2016\u03b4i(w) \u2212 \u03b4i(y)\u2016\u2217 is upper bounded by the sum of the differences of these sequential switches. The difference of each such unilateral switch for each player j is turn upper bounded by \u2016wj \u2212 yj\u2016, by the property assumed in the Lemma. The lemma then follows.\nExample. (Connection to discrete game). We can view the discrete action games as a special case of the latter setting, by re-naming mixed strategies in the discrete game to pure strategies in the continuous space game. Under this mapping, the continuous strategy space is the simplex in Rd, where d is the number of pure strategies of the discrete game. Moreover the costs ci(w) (equiv. utilities) are multi-linear, i.e. ci(w) = \u2211 s Ci(s) \u220f j wj,s. Obviously, these multi-linear costs satisfy assumption 1, i.e. they are convex (in fact linear) in a players strategy.\nThe second assumption is also satisfied, albeit with a slightly more involved proof, which appears in the proof of Theorem 4. Basically, observe that\n\u03b4i,si(w) = \u2211 s\u2212i Ci(si, s\u2212i) \u220f j 6=i wj,sj (25)\n4We will use wi instead of si for a pure strategy, since pure strategies of the continuous game will be sort of treated equivalently to mixed strategies in the discrete game we described in Section 2\n5With \u2207ici(w) we denote the gradient of the function with respect the strategy of player i and fixing the strategy of other players. Equivalently for each fixed w\u2212i it is the gradient of the function ci(\u00b7,w\u2212i).\nAssuming Ci(s) \u2264 1:\n|\u03b4i,si(w)\u2212 \u03b4i,si(y)| \u2264 \u2211 s\u2212i \u2223\u2223\u2223\u2223\u2223\u2223 \u220f j 6=i wj,sj \u2212 \u220f j 6=i yj,sj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2211 j 6=i \u2016wj \u2212 yj\u20161 (26)\nWhere the last inequality holds by the properties of total variation distance.\nExample. (Splittable congestion games). In this game each player i has an amount of flow fi \u2264 B he wants to route from a source si to a sink ti in an undirected graphG = (V,E). Each edge e \u2208 E is associated with a latency function `e(fe) which maps an amount of flow fe passing through the edge to a latency. We will assume that latency functions are convex, increasing and twice differentiable. We will also assume that both `e(\u00b7) and `\u2032e(\u00b7) are K-lipschitz functions of the flow. We will denote with Pi the set of (si, ti) paths in the graph. Then the set of feasible strategies for each player is all possible ways of splitting his flow fi onto these pats Pi. Denote with wp the amount of flow a player routes on path p \u2208 Pi, then the strategy space is:\nSi = wi \u2208 R|Pi|+ : \u2211 p\u2208Pi wi,p = fp  (27) The latter is obviously a closed convex set in R|Pi|.\nFor an edge e, let fi,e(wi) = \u2211 p\u2208Pi:e\u2208p wi,p to be the flow on edge e caused by player i and with\nfe(w) = \u2211 i fi,e(wi) to be the total flow on the edge e. Then the cost of a player is:\nci(w) = \u2211 e fi,e(wi) \u00b7 `(fe(w)) (28)\nFirst observe that the functions ci(\u00b7) are convex with respect to a player\u2019s strategy wi. This follows since the cost is linear across edges, thus we need to show convexity locally at each edge. The latency function on an edge is a convex function of the total flow, hence also x`e(x + b) is also a convex function of x. Now observe that the cost from each edge is of the form fi,e(wi)`e(fi,e(wi) + b) which is convex with respect to fi,e(wi). In turn, fi,e(\u00b7) is a linear function of wi. Thus whole cost function is convex in wi.\nLast we need to show that the second condition on the cost functions is satisfied for some lipschitzness factor L. This will be a consequence of the K-lipschitzness of the latency functions. Denote with `ie(w) = `e(fe(w)) + fi,e(wi) \u00b7 `\u2032e(fe(w)). Then, observe that:\n\u03b4i,p(w) = \u2211 e\u2208p (`e(fe(w)) + fi,e(wi) \u00b7 `\u2032e(fe(w))) = \u2211 e\u2208p `ie(w) (29)\nSince both `e(\u00b7) and `\u2032e(\u00b7) are K-lipschitz and fi,e(wi) \u2264 B, we have that: |\u03b4i,p(w)\u2212 \u03b4i,p(y)| \u2264 \u2211 e\u2208p |`ie(w)\u2212 `ie(y)|\n\u2264 \u2211 e\u2208p |`e(fe(w))\u2212 `e(fe(y))|+B \u2211 e\u2208p |`\u2032e(fe(w))\u2212 `\u2032e(fe(y))|\n\u2264 K(1 +B) \u2211 e\u2208p |fe(w)\u2212 fe(y)| \u2264 K(1 +B) \u2211 e\u2208p \u2211 j\u2208[n] |fj,e(wj)\u2212 fj,e(yj)|\n\u2264 K(1 +B) \u2211 e\u2208p \u2211 j\u2208[n] \u2211 q\u2208Pj :e\u2208q |wj,q \u2212 yj,q|\n= K(1 +B) \u2211 j\u2208[n] \u2211 q\u2208Pj \u2211 e\u2208p\u2229q |wj,q \u2212 yj,q| \u2264 K(1 +B)m \u2211 j\u2208[n] \u2211 q\u2208Pj |wj,q \u2212 yj,q|\n\u2264 K(1 +B)m \u2211 j\u2208[n] \u2016wj \u2212 yj\u20161\nThus we get that the second condition is satisfied with L = 2Km.\nFor these games we will assume that the players are performing some form of regularized learning using the gradients of their utilities as proxies. For fast convergence we would require that the algorithms they use satisfy the following property, which is a generalization of Theorem 4. Theorem 25. Consider a repeated continuous strategy space game where the cost functions satisfy properties 1, 2. Suppose that the algorithm of each player i satisfies the property that for any w\u2217i \u2208 Si\nT\u2211 t=1 ci(w t)\u2212 ci(w\u2217i ,wt\u2212i) \u2264 \u03b1+ \u03b2 T\u2211 t=1 \u2016\u03b4i(wt)\u2212 \u03b4i(wt\u22121)\u20162\u2217 \u2212 \u03b3 T\u2211 t=1 \u2016wti \u2212wt\u22121i \u2016 2 (30)\nfor some \u03b1 > 0 and 0 < \u03b2 \u2264 \u03b3L2\u00b7n2 and with \u2016 \u00b7 \u2016 we denote the \u2016 \u00b7 \u20161 norm. Then:\u2211 i\u2208N ri(T ) \u2264 n \u00b7 \u03b1 = O(1) (31)\nProof. By property 2, we have that:\nT\u2211 t=1 \u2016\u03b4i(wt)\u2212 \u03b4i(wt\u22121)\u20162\u2217 \u2264 L2 T\u2211 t=1 \u2211 j\u2208[n] \u2016wtj \u2212wt\u22121j \u2016 2 \u2264 L2n T\u2211 t=1 \u2211 j\u2208[n] \u2016wtj \u2212wt\u22121j \u2016 2\nBy summing up the regret inequality for each player and using the above bound we get:\u2211 i\u2208N ri(T ) \u2264 n \u00b7 \u03b1+ \u03b2L2n2 T\u2211 t=1 \u2211 j\u2208[n] \u2016wtj \u2212wt\u22121j \u2016 2 \u2212 \u03b3 T\u2211 t=1 \u2211 i\u2208[n] \u2016wti \u2212wt\u22121i \u2016 2 (32)\nIf \u03b2L2n2 \u2264 \u03b3, the theorem follows.\nAll the algorithms that we described in the previous sections can be adapted to satisfy the bound required by Theorem 25, by simply using the gradient of the cost as a proxy of the cost instead of the actual cost. This follows by standard arguments. Hence if players follow for instance the following adaptation of the regularized leader algorithm:\nwTi = argmax w\u2208Si\n\u2329 w,\nT\u22121\u2211 t=1 \u03b4i(w t) + \u03b4i(w T\u22121)\n\u232a \u2212 R(w)\n\u03b7 (33)\nthen by Proposition 7 we get that their regret satisfies the conditions of Theorem 25 for a = R\u03b7 , \u03b2 = \u03b7 and \u03b3 = 14\u03b7 , where R = argmaxwi\u2208Si R(wi). We need that \u03b7L\n2n2 \u2264 14\u03b7 or equivalently \u03b7 \u2264 12Ln . Thus for \u03b7 = 1 2Ln , if all players are using the latter algorithm we get regret of at most n \u00b7 R\u03b7 = 2Ln 2R\nExample. (Splittable congestion games). Consider the case of congestion games with splittable flow, where all the latencies and their derivatives are K-Lipschitz and the flow of each player is at most B. In that setting, suppose that we use the entropic regularizer. Then for each player i, R \u2264 B \u00b7 log(|Pi|). The number of possible (s, t) paths is at most 2m, which yields R \u2264 B \u00b7 m. Hence, by using the linearized follow the regularized leader, we get that the total regret is at most 2Ln2R \u2264 2K(B + 1)Bm2n2.\nJ \u2126( \u221a T ) Lower Bounds on Regret for other Dynamics\nWe consider a two-player zero-sum game which can be described by a utility matrix A. Assume the row player uses MWU with a fixed learning rate \u03b7, and the column player plays the best response, that is, a pure strategy that minimizes the row player\u2019s expected utility for the current round. Then the following theorem states that no matter how \u03b7 is set, there is always a game A such that the regret of the row player is at least \u2126( \u221a T ).\nTheorem 26. In the setting described above, let r(T ) and r\u2032(T ) be the regret of the row player for the game A = (\n1 0 0 1\n) and A\u2032 = ( 1 0 ) respectively after T rounds. Then max{r(T ), r\u2032(T )} \u2265\n\u2126( \u221a T ).\nProof. For game A, according to the setup, one can verify that the row player will play a uniform distribution and receive utility 12 on round twhere t is odd, and for the next round t+1, the row player will put slightly more weights on one row and the column player will pick the column that has 0 utility for that row. Specifically, the expected utility of the row player is e \u03b7(t\u22121)/2\ne\u03b7(t\u22121)/2+e\u03b7(t+1)/2 = 11+e\u03b7 .\nTherefore, the regret is (assuming T is even for simplicity)\nr(T ) = T 2 \u2212 T 2\n( 1\n2 +\n1\n1 + e\u03b7\n) = T\n2 \u00b7 e \u03b7 \u2212 1 e\u03b7 + 1 .\nFor game A\u2032, the expected utility of the row player on round t is e \u03b7(t\u22121)\ne\u03b7(t\u22121)+1 , and thus the regret is\nr\u2032(T ) = T \u2212 T\u2211 t=1 e\u03b7(t\u22121) e\u03b7(t\u22121) + 1 = T\u2211 t=1\n1\ne\u03b7(t\u22121) + 1 \u2265 T\u2211 t=1\n1\n2e\u03b7(t\u22121) =\n1\u2212 e\u2212T\u03b7\n2(1\u2212 e\u2212\u03b7) .\nNow if \u03b7 \u2265 1, then r(T ) \u2265 T2 \u00b7 e\u22121 e+1 = \u2126(T ). If \u03b7 \u2264 1 T , then r \u2032(T ) \u2265 1\u2212e \u22121 2(1\u2212e\u2212 1 T ) \u2265 T (1\u2212e \u22121) 2 = \u2126(T ). Finally when 1T \u2264 \u03b7 \u2264 1, we have\nr(T )+r\u2032(T ) \u2265 T 2 \u00b7 e \u03b7 \u2212 1 e+ 1 + 1\u2212 e\u22121 2(1\u2212 e\u2212\u03b7) \u2265 T 2 \u00b7 e \u03b7 \u2212 1 e+ 1 + 1\u2212 e\u22121 2(e\u03b7 \u2212 1) \u2265 \u221a T \u00b7 1\u2212 e \u22121 e+ 1 = \u2126( \u221a T ). To sum up, we have max{r(T ), r\u2032(T )} \u2265 \u2126( \u221a T )."}], "references": [{"title": "Learning, regret minimization, and equilibria", "author": ["A. Blum", "Y. Mansour"], "venue": "Algorithmic Game Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Regret minimization and the price of total anarchy", "author": ["Avrim Blum", "MohammadTaghi Hajiaghayi", "Katrina Ligett", "Aaron Roth"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Near-optimal no-regret algorithms for zero-sum games", "author": ["Constantinos Daskalakis", "Alan Deckelbaum", "Anthony Kim"], "venue": "Games and Economic Behavior,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "author": ["Benjamin Edelman", "Michael Ostrovsky", "Michael Schwarz"], "venue": "Working Paper 11765,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "On the convergence of regret minimization dynamics in concave games", "author": ["Eyal Even-dar", "Yishay Mansour", "Uri Nadav"], "venue": "In Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Calibrated learning and correlated equilibrium", "author": ["Dean P. Foster", "Rakesh V. Vohra"], "venue": "Games and Economic Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Recency, records and recaps: Learning and nonequilibrium behavior in a simple decision problem", "author": ["Drew Fudenberg", "Alexander Peysakhovich"], "venue": "In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Problem complexity and method efficiency in optimization", "author": ["AS Nemirovsky", "DB Yudin"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1983}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "COLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Intrinsic robustness of the price of anarchy", "author": ["T. Roughgarden"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Local smoothness and the price of anarchy in atomic splittable congestion games", "author": ["Tim Roughgarden", "Florian Schoppmann"], "venue": "In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Our results extend those of Rakhlin and Shridharan [18] and Daskalakis et al.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "[4], who only analyzed two-player zero-sum games for specific algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 114, "endOffset": 121}, {"referenceID": 17, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 114, "endOffset": 121}, {"referenceID": 6, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 0, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 8, "context": "When played against one another, it can also be shown that the sum of utilities approaches an approximate optimum [2, 19], and the player strategies converge to an equilibrium under appropriate conditions [7, 1, 9], at rates governed by the regret bounds.", "startOffset": 205, "endOffset": 214}, {"referenceID": 12, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 75, "endOffset": 82}, {"referenceID": 7, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 75, "endOffset": 82}, {"referenceID": 13, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Well-known families of no-regret algorithms include multiplicative-weights [14, 8], Mirror Descent [15], and Follow the Regularized/Perturbed Leader [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "(See [3, 21] for excellent overviews.", "startOffset": 5, "endOffset": 12}, {"referenceID": 19, "context": "(See [3, 21] for excellent overviews.", "startOffset": 5, "endOffset": 12}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "For two-player zero-sum games, they developed a decentralized variant of Nesterov\u2019s accelerated saddle point algorithm [16] and showed that each player\u2019s average regret converges at the remarkable rate ofO(1/T ).", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "dynamics are somewhat unnatural, in later work, Rakhlin and Sridharan [18] showed surprisingly that the same convergence rate holds for a simple variant of Mirror Descent with the seemingly minor modification that the last utility observation is counted twice.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Concretely, we show a natural class of regularized no-regret algorithms with recency bias that achieve welfare at least (\u03bb/(1 + \u03bc))OPT \u2212 O(1/T ), where \u03bb and \u03bc are parameters in a smoothness condition on the game introduced by Roughgarden [19].", "startOffset": 239, "endOffset": 243}, {"referenceID": 3, "context": "Even for two-person zero-sum games, our results for general games expose a hidden generality and modularity underlying the previous results [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 16, "context": "Even for two-person zero-sum games, our results for general games expose a hidden generality and modularity underlying the previous results [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 16, "context": "This covers the Optimistic Mirror Descent of Rakhlin and Sridharan [18] as an example, but also applies to optimistic variants of Follow the Regularized Leader (FTRL), including dependence on arbitrary weighted windows in the history as opposed to just the utility from the last round.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Recency bias is a behavioral pattern commonly observed in game-theoretic environments [10]; as such, our results can be viewed as a partial theoretical justification.", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "Second, previous approaches in [4, 18] on achieving both faster convergence against similar algorithms while at the same time \u00d5(1/ \u221a T ) regret rates against adversaries were shown via ad-hoc modifications of specific algorithms.", "startOffset": 31, "endOffset": 38}, {"referenceID": 16, "context": "Second, previous approaches in [4, 18] on achieving both faster convergence against similar algorithms while at the same time \u00d5(1/ \u221a T ) regret rates against adversaries were shown via ad-hoc modifications of specific algorithms.", "startOffset": 31, "endOffset": 38}, {"referenceID": 7, "context": "Finally, we simulate a 4-bidder simultaneous auction game, and compare our optimistic algorithms against Hedge [8] in terms of utilities, regrets and convergence to equilibria.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "\u00d7 Sn \u2192 [0, 1] that maps a strategy profile s = (s1, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 17, "context": "We next define a class of games first identified by Roughgarden [19] on which we can approximate the optimal welfare using decoupled no-regret dynamics.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Definition 1 (Smooth game [19]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "This proposition is essentially a more explicit version of Roughgarden\u2019s result [19]; we provide a proof in the appendix for completeness.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "However, Rakhlin and Sridharan [17] give a modification of Mirror Descent with this property, and we will present a similar variant of FTRL in the sequel.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 18], as long as each player\u2019s algorithm satisfies the RVU property with a common bound on the constants.", "startOffset": 109, "endOffset": 116}, {"referenceID": 16, "context": "Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 18], as long as each player\u2019s algorithm satisfies the RVU property with a common bound on the constants.", "startOffset": 109, "endOffset": 116}, {"referenceID": 15, "context": "1 Optimistic Mirror Descent The optimistic mirror descent (OMD) algorithm of Rakhlin and Sridharan [17] is parameterized by an adaptive predictor sequence Mi and a regularizer 2 R which is 1-strongly convex3 with respect to a norm \u2016 \u00b7 \u2016.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The proposition follows by further crystallizing the arguments of Rakhlin and Sridaran [18], and we provide a proof in the appendix for completeness.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "The above proposition, along with Theorem 4, immediately yields the following corollary, which had been proved by Rakhlin and Sridharan [18] for two-person zero-sum games, and which we here extend to general games.", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "This algorithm is similar but not equivalent to OMD, and is an analogous extension of standard FTRL [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "Rakhlin and Sridharan [17] also analyze an FTRL variant, but require a self-concordant barrier for the constraint set as opposed to an arbitrary strongly convex regularizer, and their bound is missing the crucial negative terms of the RVU property which are essential for obtaining Theorem 4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "[11]) to CCE using natural, decoupled no-regret dynamics defined in [4].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[11]) to CCE using natural, decoupled no-regret dynamics defined in [4].", "startOffset": 68, "endOffset": 71}, {"referenceID": 16, "context": "Rakhlin and Sridharan [18] address this concern by modifying the OMD algorithm with additional smoothing and adaptive step-sizes so as to preserve the fast rates in the favorable case while still guaranteeing O(1/ \u221a T ) regret for each player, no matter how the opponents play.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "5 Experimental Evaluation We analyzed the performance of optimistic follow the regularized leader with the entropy regularizer, which corresponds to the Hedge algorithm [8] modified so that the last iteration\u2019s utility for each strategy is double counted; we refer to it as Optimistic Hedge.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "The bids are discretized to be any integer in [1, 20].", "startOffset": 46, "endOffset": 53}, {"referenceID": 18, "context": "The bids are discretized to be any integer in [1, 20].", "startOffset": 46, "endOffset": 53}, {"referenceID": 0, "context": "References [1] A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Nicolo Cesa-Bianchi and Gabor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Eyal Even-dar, Yishay Mansour, and Uri Nadav.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Dean P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Drew Fudenberg and Alexander Peysakhovich.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sergiu Hart and Andreu Mas-Colell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Adam Kalai and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Nick Littlestone and Manfred K Warmuth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] AS Nemirovsky and DB Yudin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Tim Roughgarden and Florian Schoppmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We will use the following theorem of [18].", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Theorem 17 (Raklin and Sridharan [18]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Some of the key facts (Equations (9) and (10)) that we use in the following proof appear in [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "\u00d7 Sn \u2192 [0, 1] to denote the cost function, and similarly to previous sections C(s) = \u2211 i\u2208N ci(s), C(w) = Es\u223cw[C(s)],OPT \u2032 = mins\u2208S1\u00d7.", "startOffset": 7, "endOffset": 13}, {"referenceID": 18, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Such learning on continuous games has also been studied in more restrictive settings in [6].", "startOffset": 88, "endOffset": 91}], "year": 2015, "abstractText": "We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T\u22123/4), while the sum of utilities converges to an approximate optimum at O(T\u22121)\u2013an improvement upon the worst case O(T\u22121/2) rates. We show a blackbox reduction for any algorithm in the class to achieve \u00d5(T\u22121/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [18] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.", "creator": "LaTeX with hyperref package"}}}