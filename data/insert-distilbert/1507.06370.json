{"id": "1507.06370", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2015", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "abstract": "this paper establishes a statistical versus computational trade - off for solving a basic high - dimensional machine learning problem via a basic convex fourier relaxation method. less specifically, we consider the { \\ em sparse principal component analysis } ( typically sparse pca ) problem, and the family classes of { \\ em sum - checks of - big squares } ( sos, aka lasserre / parillo ) convex relaxations. sometimes it was well known that something in large dimension $ p $, a planted $ k $ - sparse unit vector can be { \\ em in principle } detected using only $ n \\ approx not k \\ log p $ ( gaussian relaxation or bernoulli ) samples, but all { \\ em ` efficient } ( polynomial time ) algorithms known always require $ 00 n \\ approx f k ^ 2 \\ log p $ samples. it was also known that this quadratic term gap cannot be improved by checking the the most basic { \\ em semi - definite } ( sdp, aka spectral ) relaxation, strictly equivalent to a degree - 2 sos algorithms. here we prove that also degree - 4 sos algorithms cannot improve this quadratic gap. this average - level case lower bound adds to : the small probability collection of metric hardness approximation results in machine learning for this powerful family of convex relaxation statistical algorithms. moreover, our design of moments ( or \" pseudo - euclidean expectations \" ) for this lower maximum bound is much quite bit different than previous lower bounds. establishing lower bounds for higher critical degree sos algorithms for remains a challenging problem.", "histories": [["v1", "Thu, 23 Jul 2015 01:50:43 GMT  (479kb)", "https://arxiv.org/abs/1507.06370v1", null], ["v2", "Sun, 18 Oct 2015 05:50:16 GMT  (491kb)", "http://arxiv.org/abs/1507.06370v2", "to appear at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "authors": ["tengyu ma", "avi wigderson"], "accepted": true, "id": "1507.06370"}, "pdf": {"name": "1507.06370.pdf", "metadata": {"source": "CRF", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "authors": ["Tengyu Ma", "Avi Wigderson"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n06 37\n0v 2\n[ cs\n.L G\n] 1\n8 O"}, {"heading": "1 Introduction", "text": "We start with a general discussion of the tension between sample size and computational efficiency in statistical and learning problems. We then describe the concrete model and problem at hand: Sumof-Squares algorithms and the Sparse-PCA problem. All are broad topics studied from different viewpoints, and the given references provide more information."}, {"heading": "1.1 Statistical vs. computational sample-size", "text": "Modern machine learning and statistical inference problems are often high dimensional, and it is highly desirable to solve them using far less samples than the ambient dimension. Luckily, we often know, or assume, some underlying structure of the objects sought, which allows such savings in principle. Typical such assumption is that the number of real degrees of freedom is far smaller than the dimension; examples include sparsity constraints for vectors, and low rank for matrices and tensors. The main difficulty that occurs in nearly all these problems is that while information\n\u2217Supported in part by Simons Award for Graduate Students in Theoretical Computer Science \u2020Supported in part by NSF grant CCF-1412958\ntheoretically the sought answer is present (with high probability) in a small number of samples, actually computing (or even approximating) it from these many samples is a computationally hard problem. It is often expressed as a non-convex optimization program which is NP-hard in the worst case, and seemingly hard even on random instances.\nGiven this state of affairs, relaxed formulations of such non-convex programs were proposed, which can be solved efficiently, but sometimes to achieve accurate results seem to require far more samples than existential bounds provide. This phenomenon has been coined the \u201cstatistical versus computational trade-off\u201d by Chandrasekaran and Jordan [CJ13], who motivate and formalize one framework to study it in which efficient algorithms come from the Sum-of-Squares family of convex relaxations (which we shall presently discuss). They further give a detailed study of this trade-off for the basic de-noising problem [Joh02, Don95, DJ98] in various settings (some exhibiting the trade-off and others that do not). This trade-off was observed in other practical machine learning problems, in particular for the Sparse PCA problem that will be our focus, by Berthet and Rigollet [BR13a].\nAs it turns out, the study of the same phenomenon was proposed even earlier in computational complexity, primarily from theoretical motivations. Decatur, Goldreich and Ron [DGR97] initiate the study of \u201ccomputational sample complexity\u201d to study statistical versus computation tradeoffs in sample-size. In their framework efficient algorithms are arbitrary polynomial time ones, not restricted to any particular structure like convex relaxations. They point out for example that in the distribution-free PAC-learning framework of Vapnik-Chervonenkis and Valiant, there is often no such trade-off. The reason is that the number of samples is essentially determined (up to logarithmic factors, which we will mostly ignore here) by the VC-dimension of the given concept class learned, and moreover, an \u201cOccam algorithm\u201d (computing any consistent hypothesis) suffices for classification from these many samples. So, in the many cases where efficiently finding a hypothesis consistent with the data is possible, enough samples to learn are enough to do so efficiently! This paper also provide examples where this is not the case in PAC learning, and then turns to an extensive study of possible trade-offs for learning various concept classes under the uniform distribution. This direction was further developed by Servedio [Ser00].\nThe fast growth of Big Data research, the variety of problems successfully attacked by various heuristics and the attempts to find efficient algorithms with provable guarantees is a growing area of interaction between statisticians and machine learning researchers on the one hand, and optimization and computer scientists on the other. The trade-offs between sample size and computational complexity, which seems to be present for many such problems, reflects a curious \u201cconflict\u201d between these fields, as in the first more data is good news, as it allows more accurate inference and prediction, whereas in the second it is bad news, as a larger input size is a source of increased complexity and inefficiency. More importantly, understanding this phenomenon can serve as a guide to the design of better algorithms from both a statistical and computational viewpoints, especially for problems in which data acquisition itself is costly, and not just computation. A basic question is thus for which problems is such trade-off inherent, and to establish the limits of what is achievable by efficient methods.\nEstablishing a trade-off has two parts. One has to prove an existential, information theoretic upper bound on the number of samples needed when efficiency is not an issue, and then prove a computational lower bound on the number of samples for the class of efficient algorithms at hand. Needless to say, it is desirable that the lower bounds hold for as wide a class of algorithms as possible, and that it will match the best known upper bound achieved by algorithms from this class. The most general one, the computational complexity framework of [DGR97, Ser00] allows all polynomial-time\nalgorithms. Here one cannot hope for unconditional lower bounds, and so existing lower bounds rely on computational assumptions, e.g.\u201dcryptographic assumptions\u201d, e.g. that factoring integers has no polynomial time algorithm, or other average case assumptions. For example, hardness of refuting random 3CNF was used for establishing the sample-computational tradeoff for learning halfspaces [DLS13], and hardness of finding planted clique in random graphs was used for tradeoff in sparse PCA [BR13a, GMZ14]. On the other hand, in frameworks such as [CJ13], where the class of efficient algorithms is more restricted (e.g. a family of convex relaxations), one can hope to prove unconditional lower bounds, which are called \u201cintegrality gaps\u201d in the optimization and algorithms literature. Our main result is of this nature, adding to the small number of such lower bounds for machine learning problems.\nWe now turn to describe and motivate SoS convex relaxations algorithms, and then the Sparse PCA problem."}, {"heading": "1.2 Sum-of-Squares convex relaxations", "text": "Sum-of-Squares algorithms (sometimes called the Lasserre hierarchy) encompasses perhaps the strongest known algorithmic technique for a diverse set of optimization problems. It is a family of convex relaxations introduced independently around the year 2000 by Lasserre [Las01], Parillo [Par00], and in the (equivalent) context of proof systems by Grigoriev [Gri01b]. These papers followed better and better understanding in real algebraic geometry [Art27, Kri64, Ste74, Sho87, Sch91, Put93, Nes00]of David Hilbert\u2019s famous 17th problem on certifying the non-negativity of a polynomial by writing it as a sum of squares (which explains the name of this method). We only briefly describe this important class of algorithms; far more can be found in the book [Las15] and the excellent extensive survey [Lau09].\nThe SoS method provides a principled way of adding constraints to a linear or convex program in a way that obtains tighter and tighter convex sets containing all solutions of the original problem. This family of algorithms is parametrized by their degree d (sometimes called the number of rounds); as d gets larger, the approximation becomes better, but the running time becomes slower, specifically nO(d). Thus in practice one hopes that small degree (ideally constant) would provide sufficiently good approximation, so that the algorithm would run in polynomial time. This method extends the standard semi-definite relaxation (SDP, sometimes called spectral), that is captured already by degree-2 SoS algorithms. Moreover, it is more powerful than two earlier families of relaxations: the Sherali-Adams [SA90] and Lova\u0301sz-Scrijver [LS91] hierarchies.\nThe introduction of these algorithms has made a huge splash in the optimization community, and numerous applications of it to problems in diverse fields were found that greatly improve solution quality and time performance over all past methods. For large classes of problems they are considered the strongest algorithmic technique known. Relevant to us is the very recent growing set of applications of constant-degree SoS algorithms to machine learning problems, such as [BKS15, BKS14, BM15]. The survey [BS14] contains some of these exciting developments. Section 2.3 contains some self-contained material about the general framework SoS algorithms as well.\nGiven their power, it was natural to consider proving lower bounds on what SoS algorithms can do. There has been an impressive progress on SoS degree lower bounds (via beautiful techniques) for a variety of combinatorial optimization problems [Gri01a, Gri01b, Sch08, MPW15]. However, for machine learning problems relatively few such lower bounds (above SDP level) are known [BM15, WGL15] and follow via reductions to the above bounds. So it is interesting to enrich the set of techniques for proving such limits on the power of SoS for ML. The lower bound we prove indeed\nseem to follow a different route than previous such proofs."}, {"heading": "1.3 Sparse PCA", "text": "Sparse principal component analysis, the version of the classical PCA problem which assumes that the direction of variance of the data has a sparse structure, is by now a central problem of high-diminsional statistical analysis. In this paper we focus on the single-spiked covariance model introduced by Johnstone [Joh01]. One observes n samples from p-dimensional Gaussian distribution with covariance\n\u03a3 = \u03bbvvT + I (1.1)\nwhere (the planted vector) v is assumed to be a unit-norm sparse vector with at most k non-zero entries, and \u03bb > 0 represents the strength of the signal. The task is to find (or estimate) the sparse vector v. More general versions of the problem allow several sparse directions/components and general covariance matrix [Ma13, VL13]. Sparse PCA and its variants have a wide variety of applications ranging from signal processing to biology: see, e.g., [ABN+99, JL09, Che11, JOB10].\nThe hardness of Sparse PCA, at least in the worst case, can be seen through its connection to the (NP-hard) Clique problem in graphs. Note that if \u03a3 is a {0, 1} adjacency matrix of a graph (with 1\u2019s on the diagonal), then it has a k-sparse eigenvector v with eigenvalue k if and only if the graph has a k-clique. This connection between these two problems is actually deeper, and will appear again below, for our real, average case version above.\nFrom a theoretical point of view, Sparse PCA is one of the simplest examples where we observe a gap between the number of samples needed information theoretically and the number of samples needed for a polynomial time estimator: It has been well understood [VL12, PJ12, BR13b] that information theoretically, given n = O(k log p) samples1, one can estimate v up to constant error (in euclidean norm), using a non-convex (therefore not polynomial time) optimization algorithm. On the other hand, all the existing provable polynomial time algorithms [JL09, AW09, VL13, DM14], which use either diagonal thresholding (for the single spiked model) or semidefinite programming (for general covariance), first introduced for this problem in [dGJL07], need at least quadratically many samples to solve the problem, namely n = O(k2). Moreover, Krauthgamer, Nadler and Vilenchik [KNV15] and Berthet and Rigollet [BR13b] have shown that for semi-definite programs (SDP) this bound is tight. Specifically, the natural SDP cannot even solve the detection problem: to distinguish the data in equation 1.1 above from the null hypothesis in which no sparse vector is planted, namely the n samples are drawn from the Gaussian distribution with covariance matrix I.\nRecall that the natural SDP for this problem (and many others) is just the first level of the SoS hierarchy, namely degree-2. Given the importance of the Sparse PCA, it is an intriguing question whether one can solve it efficiently with far fewer samples by allowing degree-d SoS algorithms with larger d. A very interesting conditional negative answer was suggested by Berthet and Rigollet [BR13b]. They gave an efficient reduction from Planted Clique2 problem to Sparse PCA, which shows in particular that degree-d SoS algorithms for Sparse PCA will imply similar ones for Planted Clique. Gao, Ma and Zhou [GMZ14] strengthen the result by establishing the hardness of the Gaussian single-spiked covariance model, which is an interesting subset3 of models considered by [BR13a]. These are useful as nontrivial constant-degree SoS lower bounds for\n1We treat \u03bb as a constant so that we omit the dependence on it for simplicity throughout the introduction section 2An average case version of the Clique problem in which the input is a random graph in which a much larger than\nexpected clique is planted. 3Note that lower bounds for special cases are stronger than those for general cases\nPlanted Clique were recently proved by [MPW15, DM15] (see there for the precise description, history and motivation for Planted Clique). As [BR13b, GMZ14] argue, strong yet believed bounds, if true, would imply that the quadratic gap is tight for any constant d. Before the submission of this paper, the known lower bounds above for planted clique were not strong enough yet to yield any lower bound for Sparse PCA beyond the minimax sample complexity. We also note that the recent progress [RS15, HKP15] that show the tight lower bounds for planted clique, together with the reductions of [BR13a, GMZ14], also imply the tight lower bounds for Sparse PCA, as shown in this paper."}, {"heading": "1.4 Our contribution", "text": "We give a direct, unconditional lower bound proof for computing Sparse PCA using degree-4 SoS algorithms, showing that they too require n = \u2126\u0303(k2) samples to solve the detection problem (Theorem 3.1), which is tight up to polylogarithmic factors when the strength of the signal \u03bb is a constant. Indeed the theorem gives a lower bound for every strength \u03bb, which becomes weaker as \u03bb gets larger. Our proof proceeds by constructing the necessary pseudo-moments for the SoS program that achieve too high an objective value (in the jargon of optimization, we prove an \u201cintegrality gap\u201d for these programs). As usual in such proofs, there is tension between having the pseudo-moments satisfy the constraints of the program and keeping them positive semidefinite (PSD). Differing from past lower bound proofs, we construct two different PSD moments, each approximately satisfying one sets of constraints in the program and is negligible on the rest. Thus, their sum give PSD moments which approximately satisfy all constraints. We then perturb these moments to satisfy constraints exactly, and show that with high probability over the random data, this perturbation leaves the moments PSD.\nWe note several features of our lower bound proof which makes the result particularly strong and general. First, it applies not only for the Gaussian distribution, but also for Bernoulli and other distributions. Indeed, we give a set of natural (pseudorandomness) conditions on the sampled data vectors under which the SoS algorithm is \u201cfooled\u201d, and show that these conditions are satisfied with high probability under many similar distributions (possessing strong concentration of measure). Next, our lower bound holds even if the hidden sparse vector is discrete, namely its entries come from the set {0,\u00b1 1\u221a\nk }. We also extend the lower bound for the detection problem to apply also\nto the estimation problem, in the regime when the ambient dimension is linear in the number of samples, namely n \u2264 p \u2264 Bn for constant B (see Theorem 3.2). Organization: Section 2 provides more backgrounds of sparse PCA and SoS algorithms. Then we state our main results in Section 3. In Section 4, we design the pseudo-moments and state their properties and then in Section 5 we prove our main theorems using these moments. Section 6 and 7 contain the analysis of the moments. Section 8 lists the tools that we heavily used for proving concentration inequalities in the analysis. Finally we conclude with a discussion of further directions of study in Section 9."}, {"heading": "2 Formal description of the model and problem", "text": "Notation: We use \u2016 \u00b7 \u2016 to denote the euclidean norm of a vector and spectral norm of a matrix, \u2016 \u00b7 \u2016q to denote the q-norm of a vector, and | \u00b7 |0 is the number of nonzero entries of a vector. We use [m] to denote the set of integers {1, . . . ,m}.\nWe write M 0 if M is a positive semidefinite matrix. Rn[x]d is used to denote the set of real polynomials with n variables and degree at most d. We will drop the subscript n when it is clear from context. We will assume that n, k, p are all sufficiently large4, and that n \u2264 p. Throughout this paper, by \u201cwith high probability some event happens\u201d, we mean the failure probability is bounded by p\u2212c for every constant c, as p tends to infinity. We use the asymptotic notation O\u0303(\u00b7) and \u2126\u0303(\u00b7) to hide the logarithmic dependency (in p). That is, m \u2264 O\u0303(f(n, p, k)) means that there exists universal constant r \u2265 0 (which is less than 3 typically in this paper) and C such that m \u2264 Cf(n, p, k) logr p, and m \u2265 \u2126\u0303(f(n, p, k)) means that there exist constants r and c such that m \u2265 cf(n, p, k)/ logr p."}, {"heading": "2.1 Sparse PCA estimation and detection problems", "text": "We will consider the simplest setting of sparse PCA, which is called single-spiked covariance model in literature [Joh01] (note that restricting to a special case makes our lower bound hold in all generalizations of this simple model). In this model, the task is to recover a single sparse vector from noisy samples as follows. The \u201chidden data\u201d is an unknown k-sparse vector v \u2208 Rp with |v|0 = k and \u2016v\u2016 = 1. To make the task easier (and so the lower bound stronger), we even assume that v has discrete entries, namely that vi \u2208 {0,\u00b1 1\u221ak} for all i \u2208 [p]. We observe n noisy samples X1, . . . ,Xn \u2208 Rp that are generated as follows. Each is independently drawn as\nXj = \u221a \u03bbgjv + \u03bej (2.1)\nfrom a distribution which generalizes both Gaussian and Bernoulli noise to v. Namely, the gj \u2019s are i.i.d real random variable with mean 0 and variance 1, and \u03bej \u2019s are i.i.d random vectors which have independent entries with mean zero and variance 1. Therefore under this model, the covariance of Xi is equal to \u03bbvvT + I. Moreover, we assume that gj and entries of \u03bej are sub-gaussian5 with variance proxy O(1). Given these samples, the estimation problem is to approximate the unknown sparse vector v.\nIt is also interesting to also consider the sparse component detection problem [BR13b, BR13a], which is the decision problem of distinguishing from random samples the following two distributions\nH0: data X j = \u03bej is purely random\nHv: data X j = \u03bej + \u221a \u03bbgjv contains a hidden sparse signal with strength \u03bb.\nRigollet [MR14] observed that a polynomial time algorithm for estimation version of sparse PCA with constant error implies that an algorithm for the detection problem with twice number of the samples. Thus, for polynomial time lower bounds, it suffices to consider the detection problem.\nWe will use X as a shorthand for the p\u00d7 n matrix [ X1, . . . ,Xn ] . We denote the rows of X as XT1 , . . . ,X T p , therefore Xi\u2019s are n-dimensional column vectors. The empirical covariance matrix is defined as \u03a3\u0302 = 1nXX T .\n4Or we assume that they go to infinity as typically done in statistics. 5A real random variable X is subgaussian with variance proxy \u03c32 if it has similar tail behavior as gaussian\ndistribution with variance \u03c32. More formally, if for any t \u2208 R, E[exp(tX)] \u2264 exp(t 2\u03c32/2)"}, {"heading": "2.2 Statistically optimal estimator/detector", "text": "It is well known that the following non-convex program achieves optimal statistical minimax rate for the estimation problem and the optimal sample complexity for the detection problem. Note that we scale the variables x up by a factor of \u221a k for simplicity (the hidden vector now has entries from {0,\u00b11}).\n\u03bbkmax(\u03a3\u0302) = 1\nk \u00b7max \u3008\u03a3\u0302, xxT \u3009 (2.2) subject to \u2016x\u201622 = k (2.3) \u2016x\u20160 = k (2.4)\nProposition 2.1 ([AW09], [BR13b], [VL12] informally stated). The non-convex program (2.2) statistically optimally solves the sparse PCA problem when n \u2265 Ck/\u03bb2 log p for some sufficiently large C. Namely, the following hold with high probability. If X is generated from Hv, then optimal solution xopt of program (2.2) satisfies \u2016 1k \u00b7 xoptxTopt \u2212 vvT \u2016 \u2264 13 , and the objective value \u03bbkmax(\u03a3\u0302) is at least 1 + 2\u03bb3 . On the other hand, if X is generated from null hypothesis H0, then \u03bb k max(\u03a3\u0302) is at most 1 + \u03bb3 .\nTherefore, for the detection problem, once can simply use the test \u03bbkmax(\u03a3\u0302) > 1+ \u03bb 2 to distinguish the case of H0 and Hv, with n = \u2126\u0303(k/\u03bb 2) samples. However, this test is highly inefficient, as the best known ways for computing \u03bbkmax(\u03a3\u0302) take exponential time! We now turn to consider efficient ways of solving this problem."}, {"heading": "2.3 Sum of Squares (Lasserre) Relaxations", "text": "Here we will only briefly introduce the basic ideas of Sum-of-Squares (Lasserre) relaxation that will be used for this paper. We refer readers to the extensive [Las15, Lau09, BS14] for detailed discussions of sum of squares algorithms and proofs and their applications to algorithm design.\nLet R[x]d denote the set of all real polynomials of degree at most d with n variables x1, . . . , xn. We start by defining the notion of pseudo-moment (sometimes called pseudo-expectation ). The intuition is that these pseudo-moments behave like the actual first d moments of a real probability distribution.\nDefinition 2.2 (pseudo-moment). A degree-d pseudo-moments M is a linear operator that maps R[x]d to R and satisfies M(1) = 1 and M(p\n2(x)) \u2265 0 for all real polynomials p(x) of degree at most d/2.\nFor a mutli-set S \u2282 [n], we use xS to denote the monomial \u220fi\u2208S xi. SinceM is a linear operator, it can be clearly described by all the values of M on the monomial of degree d, that is, all the values of M(xS) for mutli-set S of size at most d uniquely determines M . Moreover, the nonnegativity constraint M(p(x)2) \u2265 0 is equivalent to the positive semidefiniteness of the matrix-form (as defined below), and therefore the set of all pseudo-moments is convex.\nDefinition 2.3 (matrix-form). For an even integer d and any degree-d pseudo-moments M , we define the matrix-form of M as the trivial way of viewing all the values of M on monomials as a matrix: we use mat(M) to denote the matrix that is indexed by multi-subset S of [n] with size at most d/2, and mat(M)S,T = M(x SxT ).\nGiven polynomials p(x) and q1(x), . . . , qm(x) of degree at most d, and a polynomial program,\nMaximize p(x) (2.5)\nSubject to qi(x) = 0,\u2200i \u2208 [m]\nWe can write a sum of squares based relaxation in the following way: Instead of searching over x \u2208 Rn, we search over all the possible \u201cpseudo-moments\u201d M of a hypothetical distribution over solutions x, that satisfy the constraints above. The key of the relaxation is to consider only moments up to degree d. Concretely, we have the following semidefinite program in roughly nd variables.\nVariables M(xS) \u2200S : |S| \u2264 d Maximize M(p(x)) (2.6)\nSubject to M(qi(x)x K) = 0 \u2200i,K : |K|+ deg(qi) \u2264 d\nmat(M) 0\nNote that (2.6) is a valid relaxation because for any solution x\u2217 of (2.5), if we define M(xS) to be M(xS) = xS\u2217 , then M satisfies all the constraints and the objective value is p(x\u2217). Therefore it is guaranteed that the optimal value of (2.6) is always larger than that of (2.5).\nFinally, the key point is that this program can be solved efficiently, in polynomial time in its size, namely in time nO(d). As d grows, the constraints added make the \u201cpseudo-distribution\u201d defined by the moments closer and closer to an actual distribution, thus providing a tighter relaxation, at the cost of a larger running time to solve it.\nIn the next section we apply this relaxation to the Sparse PCA problem and state our results."}, {"heading": "3 Main Results", "text": "To exploit the sum of squares relaxation framework as described in Section 2.3], we first convert the statistically optimal estimator/detector (2.2) into the \u201cpolynomial\u201d program version below.\nMaximize \u3008\u03a3\u0302, xxT \u3009 (3.1) subject to \u2016x\u201622 = k (3.2)\nx3i = xi,\u2200i \u2208 [p] (3.3) |x|1 \u2264 k (3.4)\nNote that the non-convex sparsity constraint (2.4) is replaced by the polynomial constraint 3.3, which ensures that any solution vector x has entries in {0,\u00b11}, and so together with the constraint (3.2) guarantees that it has precisely k non-zero entries, each of absolute value 1. Note that constraint (3.3) implies other natural constraints that one may add to the program in order to make it stronger: for example, the upper bound on each entry xi, the lower bound on the non-zero entries of xi, and the constraint \u2016x\u20164 \u2265 k which has been used as a surrogate for k-sparse vectors in [BKS14, BKS15]. Note that we have also added an \u21131 sparsity constraint (3.4) (which can be easily made into a polynomial constraint) as is often used in practice and makes our lower bound even stronger. Of course, it is formally implied by the other constraints, but not in low-degree SoS.\nNow we are ready to apply the sum-of-squares relaxation scheme described in Section 2.3) to the polynomial program above as . For degree-4 relaxation we obtain the following semidefinite\nprogram SoS4(\u03a3\u0302), which we view as an algorithm for both detection and estimation problems. Note that the same objective function, with only the three constraints (C1), (C2), (C6) gives the degree-2 relaxation, which is precisely the standard SDP relaxation of Sparse PCA studied in [AW09, BR13b, KNV15]. So clearly SoS4(\u03a3\u0302) subsumes the SDP relaxation.\nAlgorithm 1 SoS4(\u03a3\u0302): Degree-4 Sum of Squares Relaxation Input: \u03a3\u0302 = 1nXX T where X = [ X1, . . . ,Xn ] \u2208 Rp\u00d7n Solve the following semidefinite programming and obtain optimal objective value SoS4(\u03a3\u0302) and maximizer M\u2217.\nVariables: M(S), for all mutli-sets S of size at most 4.\nSoS4(\u03a3\u0302) = max \u2211\ni,j\nM(xixj)\u03a3\u0302ij (Obj)\nsubject to \u2211\ni\u2208[p] M(x2i ) = k (C1)\n\u2211\ni,j\u2208[p] |M(xixj)| \u2264 k2 (C2) M(x3i xj) = M(xixj), \u2200i, j \u2208 [p] (C3)\u2211 i\u2208[p] M(x2i xsxt) = k \u00b7M(xsxt), \u2200s, t \u2208 [p] (C4)\n\u2211\ni,j,s,t\u2208[p] |M(xixjxsxt)| \u2264 k4 (C5)\nM 0 (C6)\nOutput: 1. For detection problem : output Hv if SoS4(\u03a3\u0302) > (1 + 1 2\u03bb)k, H0 otherwise\n2. For estimation problem: output M\u22172 = (M \u2217(xixj))i,j\u2208[p]\nBefore stating the lower bounds for both detection and estimation in the next two subsections, we comment on the choices made for the outputs of the algorithm in both, as clearly other choices can be made that would be interesting to investigate. For detection, we pick the natural threshold (1 + 12\u03bb)k from the statistically optimal detection algorithm of Section 2.2. Our lower bound of the objective under H0 is actually a large constant multiple of \u03bbk, so we could have taken a higher threshold. To analyze even higher ones would require analyzing the behavior of SoS4 under the (planted) alternative distribution Hv. For estimation we output the maximizer M \u2217 2 of the objective function, and prove that it is not too correlated with the rank-1 matrix vvT in the planted distribution Hv. This suggest, but does not prove, that the leading eigenvector of M \u2217 2 (which is a natural estimator for v) is not too correlated with v. We finally note that Rigollet\u2019s efficient reduction from detection to estimation is not in the SoS framework, and so our detection lower bound does not automatically imply the one for estimation."}, {"heading": "3.1 Lower bounds for detection problem", "text": "For the detection problem, we prove that SoS4(\u03a3\u0302) gives a large objective value on null hypothesis H0. Theorem 3.1. There exists absolute constant C and r such that for 1 \u2264 \u03bb < min{k1/4,\u221an} and any p \u2265 C\u03bbn, k \u2265 C\u03bb7/6\u221an logr p, the following holds. When the data X is drawn from the null hypothesis H0, then with high probability (1\u2212p\u221210), the objective value of degree-4 sum of squares relaxation SoS4(\u03a3\u0302) is at least 10\u03bbk. Consequently, Algorithm 1 can\u2019t solve the detection problem.\nTo parse the theorem and to understand its consequence, consider first the case when \u03bb is a constant (which is also arguably the most interesting regime). Then the theorem says that when we have only n \u226a k2 samples, degree-4 SoS relaxation SoS4 still overfits heavily to the randomness of the data X under the null hypothesis H0. Therefore, using SoS4(\u03a3\u0302) > (1 + \u03bb 2 )k (or even 10\u03bbk) as a threshold will fail with high probability to distinguish H0 and Hv. We note that for constant \u03bb our result is essentially tight in terms of the dependencies between n, k, p. The condition p = \u2126\u0303(n) is necessary since otherwise when p = o(n), even without the sum of squares relaxation, the objective value is controlled by (1 + o(1))k since \u03a3\u0302 has maximum eigenvalue 1 + o(1) in this regime. Furthermore, as mentioned in the introduction, k \u2265 \u2126\u0303(\u221an) is also necessary (up to poly-logarithmic factors), since when n \u226b k2, a simple diagonal thresholding algorithm works for this simple single-spike model.\nWhen \u03bb is not considered as a constant, the dependence of the lower bound on \u03bb is not optimal, but close. Ideally one could expect that as long as k \u226b \u03bb\u221an, and p \u2265 \u03bbn, the objective value on the null hypothesis is at least \u2126(\u03bbk). Tightening the \u03bb1/6 slack, and possibly extending the range of \u03bb are left to future study."}, {"heading": "3.2 Lower bounds for the estimation problem", "text": "For estimation problem, we prove that M\u22172 output by Algorithm 1 is not too correlated with the desired rank-1 matrix vvT .\nTheorem 3.2. For any constant B there exists absolute constants C and r such that for \u03bb \u2264 B/2, Bn \u2265 p \u2265 2\u03bbn and o(p) \u2265 k \u2265 C\u221an logr p, suppose the data X is drawn hypothesis Hv (model (2.1)), then with high probability (1\u2212 p\u221210) over the randomness of the data, Algorithm 1 will output M\u22172 such that \u2016 1k \u00b7M\u22172 \u2212 vvT \u2016 \u2265 1/5.\nWe observe that the result is of the same nature (and arguably near-optimal for estimation problem) as [KNV15] achieve the for degree-2 SoS relaxation. The proof follows simply from combining our detection lower bound Theorem 3.1 and arguments similar to [KNV15]. Finally we address a threshold-like behavior of the estimation error. Note that while our Theorem proves that n = \u2126\u0303(k2) samples is necessary for efficient algorithms to get even constant estimation error, it is known [YZ13, Ma13, WLL14] that slightly more samples, n = O\u0303(k2), can already achieve in polynomial time a much smaller (and optimal) estimation error, namely O( \u221a (k log p)/n)."}, {"heading": "4 Design of Pseudo-moments", "text": "We start with a sketch of our approach to the design of the moments M at a very high level, highlighting aspects of their design which are different than in previous lower bounds. First, there\nare some natural choices to make. We define the degree-2 moments M\u0303 from the input as the empirical covariance matrix, as was done in the proof of the SDP lower bound. This already gives a large objective value (see Lemma 4.2). We also define taking odd moments (degree 1 and 3) to be 0.\nThe difficult part is designing the degree-4 moments consistently with the constraints and M\u0303 . We do this in stages, first approximating the constraints (indeed even M\u0303 only approximately satisfies, in a way we will specify in Section 4.1, constraints (C1) and (C2)) and later in Section 4.2 correcting the moments to satisfy the constraints precisely. Moreover, we separately use different 4-moments for different constraints and then combine them, as follows. We define two different degree-4 PSD moments P and Q such that (with high probability) P almost satisfies constraints (C3), (C5) and (C6), and negligible for constraint (C4) (see Lemma 4.4), whereas Q almost satisfies constraints (C5), (C4) and (C6), and negligible for (C3) (Lemma 4.5). Therefore taking the sum P + Q will almost satisfy all the constraints (Lemma 4.6), which completes the design of the approximate moments. Finally we \u201clocally\u201d adjust P +Q so that the resulting moments M exactly satisfy all the constraints (Theorem 4.7), and remain PSD with high probability.\nAll moments will be defined from the data matrix X, to which we first apply a simple preprocessing step: we scale all its rows to have square norm n (around which they are concentrated). We abuse notation and call the scaled matrix X as well. Note that when the noise model in the null hypothesis H0 is Bernoulli, namely the entries of X are chosen as unbiased independent \u00b11 variables, the rows are automatically scaled, which motivates our abuse of notation. We suggest that the reader thinks of this distribution, even though the proof works for a much wider class of distributions.\nThe properties above of our moments will be proved under the assumption that the scaled matrix X satisfies the \u201cpseudo-randomness\u201d condition below. This set-up allows us to encapsulate what we really need the data to satisfy, and thus prove our lower bound not only for Gaussian or Bernoulli noise, but actually for a larger family containing both. Namely, we later prove in Section 7, via a series of concentration inequalities, that when data is drawn from null hypothesis H0, its scaling X satisfies the pseudorandomness condition with very high probability under all these noise models. Note that this condition is actually a sequence of statements about deviation from the mean of various polynomials in the data - these will become natural once we define our moments.\nCondition 4.1 (Pseudorandomness Condition). Our constructions of the moments will only require\nthe following pseudorandom conditions about the (scaled) data matrix X,\n\u2016Xi\u20162 = n \u2200i \u2208 [p] (P1) |\u3008Xi,Xj\u3009| \u2264 O\u0303(\n\u221a n) \u2200i 6= j (P2)\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p]\\i\u222aj \u3008Xi,X\u2113\u30093\u3008Xj ,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n1.5p), \u2200i 6= j (P3) \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n2p.5) \u2200 disctinct i, j, s, t (P4) \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[p] \u3008Xi,Xs\u3009\u3008Xi,Xt\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n.5p) \u2200 distinct s, t (P5)\n\u2211\ni,\u2113\u2208[p] \u3008Xi,X\u2113\u30092\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2264 O\u0303(n1.5p2), \u2200 distinct s, t (P6) \u2016XXT \u20162F \u2265 (1\u2212 o(1))np2 (P7)"}, {"heading": "4.1 Approximate Pseudo-moments", "text": "In this section, we design a pseudo-moments M\u0303 that approximately satisfies the all the constraints. Then in the next subsection we will locally adjust it to obtain one that exactly satisfies all of the constraints.\nWe begin by designing a (partial) degree-2 moments that gives large objective value, which will be later used for the degree-4 moments. The design is essentially the same as [KNV15] though we only work with null hypothesis for now. For the purpose of this section, we suggest the reader to think of X as having uniform {\u00b11} entires for simplicity, though as we will see later, we assume that X satisfies certain pseudorandomness condition which holds if X is chosen from a variety of natural stochastic models (with row normalization). We define M\u0303 : R[x]2 \u2192 R as follows:\nM\u0303(xixj) , \u03b3kn\np2 \u03a3\u0302ij =\n\u03b3k p2 \u3008Xi,Xj\u3009 \u2200i, j \u2208 [p] (4.1)\nM\u0303(xi) , 0 \u2200i \u2208 [p] M\u0303(1) , 1\nwhere \u03b3 is a constant that to be tuned later according to the signal strength \u03bb. Note that by design mat(M\u0303 ) is a PSD matrix. We can check straightforwardly that M\u0303 satisfies constraint (C2) and gives a large objective value (Obj). Lemma 4.2. There exists constant C such that for p \u2265 \u03b3n and k \u2265 C\u03b3\u221an log p, supposeX satisfies Condition 4.1, then M\u0303 is a valid degree-2 pseudo-moments and satisfies the sparsity constraint (C2).\n\u2211\ni,j\u2208[p] |M\u0303 (xixj)| \u2264 k2/2 (4.2)\nand has objective value \u2211\ni,j\u2208[p] M\u0303(xixj)\u03a3\u0302ij \u2265 (1\u2212 o(1))\u03b3k\nMoreover, we also have M\u0303(x2i ) = \u03b3kn p2 \u2264 k p , and M\u0303 (xixj) \u2264 O\u0303(\n\u03b3k \u221a n\np2 ).\nProof. The proof follows simple calculation and concentration inequality. Since \u2016Xi\u20162 = n for all i and with high probability over the randomness of X, for all i 6= j, |\u3008Xi,Xj\u3009| \u2264 O\u0303( \u221a n), we obtain that M\u0303(x2i ) = \u03b3kn p2 \u2264 kp , and M\u0303(xixj) \u2264 O\u0303( \u03b3k \u221a n p2 ). Then to verify equation (4.2), we have\n\u2211\ni,j\u2208[p] |M\u0303(xixj)| \u2264\n\u2211\ni\n|M(x2i )|+ \u2211\ni 6=j |M(xixj)| \u2264 k + O\u0303(\u03b3k\n\u221a n) \u2264 k2/2\nwhen k \u226b \u03b3\u221an. Finally, we can verify the objective value is large \u2211\ni,j\u2208[p] M\u0303(xixj)\u03a3\u0302ij =\n\u03b3k\np2n\n\u2211\ni,j\n\u3008Xi,Xj\u30092 = \u03b3k\np2n \u2016XXT \u20162F \u2265 (1\u2212 o(1))\u03b3k\nwhere we use the fact that \u2016XXT \u20162F \u2265 (1\u2212 o(1))p2n (see property (P7) in Condition 4.1).\nNote that M\u0303 doesn\u2019t satisfies constraint (C1) exactly. However, we could simply fix this by defining M \u2032(xixj) = M\u0303(xixj) for all i 6= j and M \u2032(x2i ) = k/p. However, note that we will use a perturbation of M \u2032 in our final design in Section 4.2 so that it is consistent with the degree-4 moments. Corollary 4.3. There exists absolute constant C such that for p \u2265 \u03b3n and k \u2265 C\u03b3\u221an log p, there exists a degree-2 pseudo-moments M \u2032 that satisfies constraints (C1), (C2) and give objective value at least (1\u2212 o(1))\u03b3k.\nNow we define a degree-4 pseudo-moment that approximately satisfies all the constraints in SoS4(\u03a3\u0302) and give a large objective value. We keep the current (approximate) design M\u0303 for degree2 moments, since the degree-2 moments defined in previous section seems to be nearly optimal and enjoys many good properties. Then we define M\u0303(S) = 0 for any multi-set S of size 3, because apparently degree-3 moments don\u2019t play any role the semidefinite relaxation.\nThe main difficulty is to define M\u0303(S) for S of size 4. Here we have three constraints (C3), (C4), and (C5), and the PSDness constraint that implicitly compete with each other. We took\nthe following approach. We let M\u0303 be a sum of two matrices matrix P and Q. We ensure that P \u201calmost\u201d (as will be specified later) satisfies (C3) and (C5), and is negligible for constraints C4. In turn Q is negligible for constraints (C3) and \u201calmost\u201d satisfies constraint (C4) and (C5). Therefore P +Q will \u201calmost\u201d satisfies constraints (C3)) and (C4)), and satisfy the sparsity constraint (C5). Moreover, P and Q will be PSD by definition. Concretely, we define\nM\u0303(i, j, s, t) , P (i, j, s, t) +Q(i, j, s, t)\nwhere P and Q are defined as\nP (i, j, s, t) = \u03b3k\np2n3\n\u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\nQ(i, j, s, t) = \u03b3k2\np3n (\u3008Xs,Xt\u3009\u3008Xi,Xj\u3009+ \u3008Xi,Xt\u3009\u3008Xs,Xj\u3009+ \u3008Xj ,Xt\u3009\u3008Xi,Xs\u3009)\nWe note that P and Q are well defined pseudo-moments because they are invariant to the permutation of indices and naturally PSD. To see the PSDness, note that P is a sum of p rank-1 PSD matrices. Moreover, Q is also PSD: the part that correpsonds to \u3008Xs,Xt\u3009\u3008Xi,Xj\u3009 is simply a rank-1 PSD matrix; \u3008Xi,Xt\u3009\u3008Xs,Xj\u3009 can be written as \u3008Xt \u2297Xs,Xi \u2297Xj\u3009 and therefore it also contributes a PSD matrix to Q. Similarly, \u3008Xj ,Xt\u3009\u3008Xs,Xi\u3009 can be written as \u3008Xs \u2297Xt,Xi \u2297Xj\u3009, and it also contributes a PSD matrix.\nIn the next two lemmas (one for P and one for Q), we formalize the intuition above by showing that, the deviation from P and Q exactly satisfying the constraints is captured by error matrices E ,F ,G. We bound the magnitude of these error matrices and establish the PSDness of some of them so that later we can fix them for the exact satisfaction of the constraints. Lemma 4.4. There exists some absolute constant C and r such that for 1 \u2264 \u03b3 \u2264 min{k1/4,\u221an}, 1 \u2264 \u03b3 \u2264 n, p = 1.1\u03b3n, and k \u2265 C \u00b7 \u03b37/6\u221an logr p, suppose X satisfies pseudorandomness condition 4.1, then P almost satisfies constraint (C3)and (C5), naturally satisfies PSD constraint (C6), and is negligible for constraint (C4) in the sense that\nP (x3ixj) = M\u0303 (xixj) + Fij , \u2200i, j \u2208 [p] (4.3)\u2211\ni P (x2i xsxt) = Est \u2200s, t \u2208 [p] (4.4) \u2211\ni,j,s,t\n|P (xixjxsxt)| \u2264 k4/3 (4.5)\nwhere F and E are p\u00d7 p matrices that satisfy\n1. 0 \u2264 Fii \u2264 O\u0303 ( \u03b3k pn ) , |Fij | \u2264 O\u0303 ( \u03b3k pn1.5 ) for any i and j 6= i. 2. E is PSD with |Ess| \u2264 O\u0303 ( \u03b3k n ) , and |Est| \u2264 O\u0303( \u03b3kn\u221an) for any s 6= t.\nLemma 4.5. There exists some absolute constant C and r such that for 1 \u2264 \u03b3 \u2264 n, p = 1.1\u03b3n and k \u2265 C \u00b7 \u03b3\u221an logr p, suppose X satisfies pseudorandomness condition 4.1, then Q is negligible for constraint (C3)) and almost satisfies constraint (C4) and (C5) in the sense that,\nQ(x3ixj) = 3k\np M\u0303(xixj) \u2200i, j (4.6)\n\u2211\ni Q(x2i xsxt) = kM\u0303(xsxt) + Gst, \u2200s, t (4.7) \u2211\ni,j,s,t\n|Q(xixjxsxt)| \u2264 k4/3 (4.8)\nwhere G is a p\u00d7 p PSD matrix |Gss| \u2264 O\u0303 ( \u03b3k2\np2\n) and |Gst| \u2264 O\u0303 ( \u03b3k2\np2 \u221a n\n) for any s 6= t.\nNow we are ready to prove that M\u0303 = P + Q almost satisfies all other constraints (C1)- (C6) approximately.\nLemma 4.6. Define M\u0303(xixjxsxt) = P (xixjxsxt) +Q(xixjxsxt) for all i, j, s, t \u2208 [p], then we have under the condition of Lemma 4.4,\nM\u0303(x3i xj) = M\u0303(xixj) + F \u2032ij (4.9)\u2211 i\u2208[p] M\u0303(x2ixsxt) = kM\u0303 (xsxt) + E \u2032st \u2200s, t (4.10)\n\u2211\ni,j,s,t\n|M\u0303 (xixjxsxt)| \u2264 2k4/3 (4.11)\nwhere F \u2032 and E \u2032 are p\u00d7 p matrices that satisfy\n1. |F \u2032ii| \u2264 O\u0303 ( \u03b3k2n p3 ) and |F \u2032ij | \u2264 O\u0303 ( \u03b3k2 \u221a n p3 ) for all i 6= j. 2. E \u2032 is a PSD matrix with E \u2032ss \u2264 O\u0303 ( \u03b3k n ) and |E \u2032st| \u2264 O\u0303( \u03b3kn\u221an) for s 6= t.\nProof of Lemma 4.6 using Lemma 4.4 and Lemma 4.5. Note that by definition of M\u0303 and Lemma 4.4 and Lemma 4.5, we have F \u2032ij = Fij + 3kp M\u0303(xixj) and E \u2032 = E + G. The bound for F \u2032 follows the bound for F and the facts that M\u0303(x2i ) = \u03b3knp2 and |M\u0303(xixj)| \u2264 O\u0303 ( \u03b3k \u221a n p2 ) . The PSDness of E \u2032 and the bounds for it follows straightforwardly from those of E and G. Equation (4.11) follows equation (4.4) and equation (4.8)."}, {"heading": "4.2 Exact Pseudo-moments", "text": "Note that M\u0303 only satisfies the constraints approximately up to some additive errors (which are carefully bounded for the purpose of the next theorem). We fix this issue by defining the actual\npseudo-moments M based (on a carefully chosen) local adjustment of M\u0303 . Concretely, we define M(1) = 1 and for all add degree monomial x\u03b1, M(x\u03b1) = 0. For distinct i, j, s, t, we define M(xixjxsxt) , M\u0303(xixjxsxt) and M(x 2 i xsxt) , M\u0303(x 2 ixsxt). For distinct s, t, we define\nM(x3sxt) = M(xsxt) , M\u0303(xsxt) + 1 k \u2212 2 ( E \u2032st \u2212 2F \u2032st ) (4.12)\nand M(x2sx 2 t ) , M\u0303(x 2 sx 2 s) + \u03b4 where \u03b4 a constant (will be proved to be nonnegative) such that\n\u2211 i 6=j M(x2sx 2 t ) = \u2211 s 6=t ( M\u0303(x2sx 2 t ) + \u03b4 ) = k2 \u2212 k (4.13)\nThen we define\nM(x4i ) = M(x 2 i ) ,\n1 k \u2212 1 \u2211\nj:j 6=i M(x2i x 2 j) (4.14)\nTherefore we can see by construction, it is almost obvious that M satisfies all the linear constraints (C1), (C3), (C4) exactly. Moreover, since E \u2032 and F \u2032 are small error matrices, most of the entries M(xixjxsxt) are equal or close to M\u0303(xixjxsxt). Note that M\u0303 satisfies the rest of constraints (C2), (C5) and (C6) (even with some slackness). We will prove that the difference between\nM and M\u0303 is small enough so that these constraints are still satisfied by M .\nTheorem 4.7. Under the condition of Lemma 4.4, suppose X satisfies pseudorandomness condition 4.1, then the pseudo-moments M defined above satisfies all the constraint (C1)-(C6) of the semidefinite programming and has objective value larger than (1\u2212 o(1))\u03b3k.\nProof. We prove that M satisfies all the constraints in an order that is most convenient for the proof, and check the objective value at the end.\n\u2022 Constraint (C3): This is satisfied by the definition of M .\n\u2022 Constraint (C4): By the definition, we can see thatM(x3sxt) is also a perturbation of M\u0303(x3sxt):\nM(x3sxt) = M\u0303(xsxt) + 1 k \u2212 2 ( E \u2032st \u2212 2F \u2032st ) = M\u0303(x3sxt) + E \u2032st k \u2212 2 \u2212 k k \u2212 2F \u2032 st (4.15)\nIt follows that for s 6= t,\n\u2211 i\u2208[p] M(x2i xsxt) = 2M(xsxt) + \u2211 i\u2208[p]\\{s,t} M\u0303(x2ixsxt)\n= 2M\u0303 (xsxt) + 2 k \u2212 2 ( E \u2032st \u2212 2F \u2032st ) + \u2211\ni\u2208[p]\\{s,t} M\u0303(x2i xsxt)\n= M\u0303(x3sxt) + M\u0303(xsx 3 t )\u2212 2F \u2032st +\n2 k \u2212 2 ( E \u2032st \u2212 2F \u2032st ) + \u2211\ni\u2208[p]\\{s,t} M\u0303(x2i xsxt)\n= kM\u0303 (xsxt) + E \u2032st \u2212 2F \u2032st + 2 k \u2212 2 ( E \u2032st \u2212 2F \u2032st ) = kM(xsxt)\nwhere the second equality uses definition (4.12) and the third uses equation (4.9), and the fourth uses (4.10) and the last equality uses the definition (4.12) again.\nMoreover, for the case when s = t, we have that\n\u2211 i\u2208[p] M(x2ix 2 s) = M(x 4 s) + \u2211 i\u2208[p]\\{s} M(x2i x 2 s)\n= M(x2s) + (k \u2212 1)M(x2s) = kM(x2s)\nwhere we used the definition (4.14) of M(x4s) and M(x 2 s). Therefore we verified that M satisfies constraint (C4).\n\u2022 Constraint (C1): Using equation (4.13) and (4.14), we have\n\u2211 i\u2208[p] M(x2i ) = 1 k \u2212 1 \u2211 i 6=j M(x2i x 2 j ) = k\n\u2022 Constraint (C6): Next we check the PSDness of matrix mat(M). Note that mat(M) is indexed by all the mutli subset of [p] of size at most 2, and it consists of 3 blocks mat(M) = blkdiag(M4,M2,M0), where\nM4 = (mat(M)S,T )|S|=2,|T |=2\nM2 = (mat(M)S,T )|S|=1,|T |=1\nM0 = 1\nTherefore it suffices to check that M0, M2 and M4 are all PSD. M0 is trivially PSD. We can write M2 in the following form\nM2 = (M(xsxt))s,t\u2208[p] = ( M\u0303(xsxt) ) s,t\u2208[p] +\u2206\nwhere \u2206 = M2 \u2212 ( M\u0303(xsxt) ) s,t\u2208[p]\n. By equation (4.12), we have that for s 6= t, \u2206st = 1\nk\u22122 (E \u2032st \u2212 2F \u2032st) for all s 6= t. Moreover, by definition of M(x2s) and M(x2sx2t ), we have that\nM(x2s) = 1 k \u2212 1 \u2211\ns:s 6=t M(x2sx 2 t ) =\n1 k \u2212 1 \u2211\ns:s 6=t\n( M\u0303(x2sx 2 t ) + \u03b4 )\n= 1 k \u2212 1 ( kM\u0303(x2s) + E \u2032ss \u2212 M\u0303(x4i ) ) + p\u2212 1 k \u2212 1 \u00b7 \u03b4 = 1\nk \u2212 1 ( kM\u0303(x2s) + E \u2032ss \u2212 M\u0303(x2s)\u2212F \u2032ss ) + p\u2212 1 k \u2212 1 \u00b7 \u03b4\n= M\u0303 (x2s) + 1 k \u2212 1 ( E \u2032ss \u2212F \u2032ss ) + p\u2212 1 k \u2212 1 \u00b7 \u03b4 (4.16)\nwhere second line uses equation (4.10) and the third line uses (4.9), and therefore \u2206ss = p\u22121 k\u22121 \u00b7 \u03b4+ 1k\u22121 (E \u2032ss \u2212F \u2032ss). We extract the PSD matrix 1k\u22122 \u00b7 E \u2032 form \u2206 and obtain \u2206\u2032 = \u2206\u2212 1k\u22122 \u00b7 E \u2032. Then by this definition, \u2206\u2032ss = p\u22121 k\u22121 \u00b7 \u03b4 + 1k\u22121 (E \u2032ss \u2212F \u2032ss) \u2212 1k\u22122E \u2032ss, and \u2206\u2032st = \u2212 2k\u22122F \u2032st. We use Gershgorin Circle Theorem to establish the PSDness of \u2206\u2032. By Lemma 4.6, we have\n|Fij | \u2264 O\u0303 ( \u03b3k2 \u221a n\np3\n) Therefore\n\u2211 j:j 6=i |\u2206\u2032ij | \u2264 p \u00b7 4 k \u2212 2O\u0303 ( \u03b3k2 \u221a n p3 ) \u2264 o ( k p )\nwhere we used the fact that \u221a n/p = o(1) which follows form p = 1.1\u03b3n and \u03b3 \u2264 \u221an.\nUsing equation (4.16) and constrain (C1) we have that\nk = \u2211\ns\nM(x2s) = \u2211\ns\nM\u0303(x2s) + \u2211\ns\n1 k \u2212 1 ( E \u2032ss \u2212F \u2032ss ) + p(p\u2212 1) k \u2212 1 \u00b7 \u03b4 (4.17)\n\u2264 \u03b3kn p + O\u0303(1) + p(p\u2212 1) k \u2212 1 \u00b7 \u03b4 (4.18)\nIt follows that \u03b4 \u2265 (1\u2212o(1))\u00b7 k(k\u22121)12p(p\u22121) . Therefore we obtain that \u2206\u2032ii = p\u22121 k\u22121 \u00b7\u03b4+ 1k\u22121 (E \u2032ss \u2212F \u2032ss)\u2212\n1 k\u22122E \u2032ss \u2265 112(1 \u2212 o(1))kp \u2212 O\u0303( \u03b3 n ) \u2212 O\u0303 ( \u03b3kn p3 ) = 112(1 \u2212 o(1))kp . Therefore we obtain \u2206\u2032ii \u2265\u2211\nj:j 6=i |\u2206\u2032ij| and by Gershgorin Circle Theorem \u2206\u2032 is PSD. Now we examine M4. we write M4 as\nM4 = mat(P ) + mat(Q) + \u0393\nwhere \u0393 = M4 \u2212 (mat(P ) + mat(Q)). One can observe that \u0393 has only non-zero entries of the form\n\u0393ii,ii = M(x 4 i )\u2212 P (x4i )\u2212Q(x4i ) = M(x4i )\u2212 M\u0303(x4i ) = M(x2i )\u2212 M\u0303(x2i )\u2212F \u2032ii\n= (p\u2212 1) k \u2212 1 \u00b7 \u03b4 + 1 k \u2212 1E \u2032 ii \u2212 k k \u2212 1Fii (4.19)\nand\n\u2200i 6= j,\u0393ii,jj = \u0393ij,ij = \u0393ij,ji = M(x2i x2j)\u2212 P (x2ix2j )\u2212Q(x2i x2j) = M(x2i x 2 j)\u2212 M\u0303(x2i x2j) = \u03b4 (4.20)\nand\n\u2200i 6= j,\u0393ii,ij = \u0393ii,ji = M(x3i xj)\u2212 P (x3i xj)\u2212Q(x3ixj)\n= M(x3i xj)\u2212 M\u0303(x3i xj) = E \u2032st\nk \u2212 2 \u2212 k k \u2212 2Fst (4.21)\nwhere the last equality uses equation (4.15). Now we are ready to prove PSDness of \u0393. We further decompose \u0393 as \u0393 = \u0393\u2032+blkdiag(\u039b\u2032, 0) where \u039b\u2032 is the p \u00d7 p matrix with \u039b\u2032 = \u03b411T 6. Note that \u039b\u2032 is a PSD matrix and therefore it suffices to prove that \u0393\u2032 = \u0393\u2212 blkdiag(\u039b\u2032, 0) is a PSD matrix. Note that \u0393\u2032 has ij-th column the same as ji-th column, and therefore it\u2019s only of rank at most p+p(p\u22121)/2. We define \u0393\u2032\u2032 be the p+p(p\u22121)/2 by p+p(p\u22121)/2 submatrix of \u0393\u2032,that is indexed by subsets (i, i) for i \u2208 [p] and (i, j) for i < j. Therefore it suffices to prove that \u0393\u2032\u2032 is PSD. We prove it using Gershgorin Circle Theorem. Note that by equation (4.19), we have that \u0393\u2032\u2032ii,ii = \u0393 \u2032 ii,ii \u2212\u039b\u2032ii,ii = (p\u2212k) k\u22121 \u00b7 \u03b4+ 1k\u22121E \u2032ii \u2212 kk\u22121Fii. Therefore by the lower bound for \u03b4 and Lemma 4.6, we obtain, \u0393\u2032\u2032ii,ii \u2265 (1\u2212o(1)) \u01ebkp . Moreover, \u0393\u2032\u2032ii,ij = \u0393ii,ij = E \u2032st k\u22122\u2212 kk\u22122Fst and therefore |\u0393\u2032\u2032ii,ij| \u2264 | E \u2032st k\u22122 |+| kk\u22122Fst| \u2264 O\u0303( \u03b3 n1.5 )+O\u0303 ( \u03b3k2 \u221a n p3 ) \u2264 O\u0303( \u03b3n1.5 ). Furthermore, for i < j, \u0393 \u2032\u2032 ij,ij = \u0393ij,ij = \u03b4 \u2265 (1 \u2212 o(1)) \u01ebk 2 p2 . Finally observe that \u0393\u2032\u2032ii,jj = 0 by definition and all other entries of \u0393 \u2032\u2032 are trivially 0 because the corresponding entries of \u0393 and \u039b\u2032 vanish. Therefore we are ready to use a variant of Gershogorin Circle Theorem (Lemma 8.8) to prove the PSDness of \u0393\u2032. Taking \u03b1 = 1/\u03b32, we have for any i,\n6\u039b\u2032 is index by ii, i = 1, . . . , p\n\u03b1 \u2211\ns,t:(s,t)6=(i,i),s<t |\u0393\u2032\u2032ii,st| =\n\u2211 j\u2208[p] |\u0393\u2032\u2032ii,jj|+ \u2211 j:j>i |\u0393\u2032\u2032ii,ij |+ \u2211 j:j<i |\u0393\u2032\u2032ii,ji|\n\u2264 \u03b1p \u00b7 O\u0303( \u03b3 n1.5 ) = o\n( k\np\n) \u2264 \u0393\u2032\u2032ii,ii\nwhere we used the fact that k \u226b \u03b3\u221an and \u01eb is a constant. Moreover, for any i < j, we have that\n1\n\u03b1\n\u2211\n(s,t):(s,t)6=(i,j),s<t |\u0393\u2032\u2032ij,st| \u2264 |\u0393\u2032\u2032ij,ii|+ |\u0393\u2032\u2032ij,jj|\n\u2264 O\u0303( \u03b3 3\nn1.5 ) = o\n( k2\np2\n) \u2264 \u0393\u2032\u2032ij,ij\nwhere we used k \u2265 \u03b34 and k \u226b \u03b3\u221an. Therefore by Lemma 8.8, we obtain that \u0393\u2032\u2032 is PSD.\n\u2022 Constraint (C2): Using Lemma 4.2 and equation (4.12), we have that \u2211\ns,t\n|M(xsxt)| \u2264 \u2211\nss\nM(x2s) + \u2211\ns 6=t |M(xsxt)\u2212 M\u0303(xsxt)|+\n\u2211 s 6=t |M\u0303 (xsxt)|\n\u2264 k + p2O\u0303( \u03b3 n1.5 ) + k2/2 \u2264 k2\n\u2022 Constraint (C5): Finally, we check that M satisfies the sparsity constraint (C5).\n\u2211\ni,j,s,t\n|M(xixjxsxt)| \u2264 \u2211\ni,j,s,t\n|\u0393ij,st|+ \u2211\ni,j,s,t\n|M\u0303 (xixjxsxt)|\n\u2264 k4\nwhere we used (4.11) and the (trivial) facts that \u0393ij,st \u2264 O(k/p) for any i, j, s, t and there are only at most O(p2) nonzero entries in \u0393.\n\u2022 Objective value (Obj): Note that by constraint (C1) and Lemma 4.2, we have that\u2211iM(x2i )\u03a3\u0302ii = k \u2265 \u2211i M\u0303(x2i )\u03a3\u0302ii, then\n\u2211\ni,j\nM(xixj)\u03a3\u0302i,j \u2265 \u2211\ni,j\nM\u0303(xixj)\u03a3\u0302i,j \u2212 \u2211\ni 6=j |M(xixj)\u2212 M\u0303(xixj)||\u03a3\u0302ij |\n\u2265 (1\u2212 o(1))\u03b3k \u2212 p2 \u00b7 O\u0303 ( \u03b3\nn \u221a n\n) \u00b7 O\u0303 ( 1\u221a n )\n\u2265 (1\u2212 o(1))\u03b3k\nwhere in the second inequality we used Lemma 4.2 and the facts that \u03a3\u0302ij = 1 n\u3008Xi,Xj\u3009 \u2264\nO\u0303(1/ \u221a n) and |E \u2032|ij + |F \u2032ij | \u2264 O\u0303 ( \u03b3 n1.5 ) , and the last line uses the fact taht \u03b34 \u2264 k."}, {"heading": "5 Proof of Theorem 3.1 and Theorem 3.2", "text": "In this section, we prove our main Theorems using the technical results of the previous sections. Before getting in the proof, we start with the observation that in order to get a lower bound of objective value 10\u03bbk, it suffices to consider the special case when p = 10\u03bbn. The reason is that the objective value of SoS4 is increasing in p while fixing all other paramters: Suppose p\n\u2032 \u2264 p and \u03a3\u0302\u2032 \u2208 Rp\u2032\u00d7p\u2032 is a submatrix of \u03a3\u0302, and M\u2217\u2032 : Rp\u2032 [x]4 is the maximizer of SoS4(\u03a3\u0302\u2032). Then we can extend M \u2032 to M : Rp[x]4 \u2192 R by simply defining that M(xS) = M\u2217 \u2032 (xS) if S \u2282 [p\u2032] and 0 otherwise. This preserves all the constraint and objective value. Thus we proved that the objective value for \u03a3 is at least the one for \u03a3\u2032. Formally, we have\nProposition 5.1. Fixing \u03bb, k, n, given a data matrix X \u2208 Rp\u00d7n, and any submatrix matrix Y \u2208 R p\u2032\u00d7n of X with p\u2032 \u2264 p1, let \u03a3\u0302X and \u03a3\u0302Y be the covariance matrices of X and Y , then we have that SoS4(\u03a3\u0302X) \u2265 SoS4(\u03a3\u0302Y ). Now we are ready to prove our main Theorem 3.1. The idea is very simple: we normalize the data matrix X so that the resulting matrix X\u0304 satisfies the the pseudorandomness condition 4.1. Then we apply Theorem 4.7 and obtain a moment matrix which give large objective value with respect to X\u0304. Then we argue that the difference between X\u0304 from X is negligible so that the same moment matrix has also large objective value with respect to X.\nProof of Theorem 3.1. Using the observation above, we take p = 1.1\u03b3n with \u03b3 = 11\u03bb, and we define X\u0304 to matrix obtained by normalizing rows of X to euclidean norm \u221a n. Then by Theorem 7.1 it satisfies the pseudorandomenss condition 4.1. Let \u03a3\u0302\u2032 = 1nX\u0304X\u0304 T be the covariance matrix defined by X\u0304 . By Theorem 4.7 we have that SoS4(\u03a3\u0302 \u2032) \u2265 (1 \u2212 o(1))\u03b3k \u2265 11\u03bbk. Moreover, let M be the moment defined in Theorem 4.7, and M2 its restriction to degree-2 moments, that is, M2 = (mat(M)S,T )|S|=|T |=1. We are going to show that the entry-wise difference between \u03a3\u0302 and \u03a3\u0302 \u2032 are small enough so that \u3008M2, \u03a3\u0302\u3009 is close to \u3008M2, \u03a3\u0302\u2032\u3009. Note that since \u2016Xi\u20162 = n\u00b1O\u0303( \u221a n), therefore for any i 6= j, \u03a3\u0302\u2032ij = \u03a3\u0302ij \u2016Xi\u2016\u2016Xj\u2016 = \u03a3\u0302ij\u00b1O\u0303( 1\u221a n )|\u03a3\u0302|ij = \u03a3\u0302ij \u00b1 O\u0303( 1n). For i = j, we have similarly that \u03a3\u0302\u2032ii = \u03a3\u0302ii\u00b1 O\u0303( 1\u221an). We bound the difference between \u3008M2, \u03a3\u0302\u2032\u3009 and \u3008M2, \u03a3\u0302\u2032\u3009 by the sum of the entry-wise differences:\n|\u3008M2, \u03a3\u0302\u2032 \u2212 \u03a3\u0302\u3009| \u2264 \u2211\ni\nM(x2i )|\u03a3\u0302ii \u2212 \u03a3\u0302\u2032ii|+ \u2211\ni 6=j M(xixj)|\u03a3\u0302ij \u2212 \u03a3\u0302\u2032ij|\n\u2264 p \u00b7 O(k/p) \u00b7 O\u0303( 1\u221a n ) + p2 \u00b7 O\u0303(\u03b3k\n\u221a n\np2 ) \u00b7 O\u0303( 1 n ) = o(k)\nTherefore \u3008M2, \u03a3\u0302\u3009 \u2265 (1\u2212o(1))\u03b3k\u2212o(k) = (1\u2212o(1))\u03b3k. Therefore the momentM gives objective value (1\u2212 o(1))\u03b3k for data \u03a3\u0302, and therefore SoS4(\u03a3\u0302) \u2265 (1\u2212 o(1))\u03b3k \u2265 10\u03bbk.\nThen we prove that Theorem 3.1 together with the arguments in [KNV15] implies Theorem 3.2. The intuition behind is the following: Suppose M\u22172 is very close to vv\nT , then it is close to rank-1 and its leading eigenvector is close to v\u0302. However, since we prove that the objective value is large (which is true also in the planted vector case), M\u22172 needs to be highly correlated with \u03a3\u0302, which implies its leading eigenvector v\u0302 needs to be correlated with \u03a3\u0302, which in turns implies that v is correlated with \u03a3\u0302. However, it turns out that v is not correlated enough with \u03a3\u0302, which leads to a contradiction.\nProof of Theorem 3.2. We first prove that the optimal value of SoS4(\u03a3\u0302) for hypothesis Hv is also at least 0.99kp/n. Suppose v has support S of size k. We consider the restriction of linear operator M to the subset T = [p]\\S, denoted by MT . That is, we have that MT (x\u03b1) = 0 for any monomial x\u03b1 that contains a factor xi with i \u2208 S, and otherwise MT (x\u03b1) = M(x\u03b1). We also consider the data matrix XT obtained by restricting to the rows indexed by T . Note that since XT doesn\u2019t contain the signal, and k \u226b \u221an), using Theorem 4.7 with \u03b3 = (p\u2212 k)/(1.01n), we have that there exists pseudo-moment M\u2217T which gives objective value \u2265 (1\u2212o(1))\u03b3k \u2265 0.99pk/n with respective to covariance matrix \u03a3\u0302T = 1 nXTX T T . Note that by Proposition 5.1, SoS4(\u03a3\u0302) \u2265 SoS4(\u03a3\u0302T ) and therefore we obtain that under hypothesis Hv, with high probability, SoS4(\u03a3\u0302) \u2265 0.99kp/n. Now suppose M\u2217 is the maximizer of SoS4(\u03a3\u0302), and M\u22172 = (M \u2217(xixj))i,j\u2208[p]. For the sake of contradiction, we assume that \u2016 1kM\u22172 \u2212 vvT \u2016 \u2264 1/5. We first show that this implies that M\u22172 has an eigenvector v\u0302 that is close to v and its eigenvalue is large. Indeed we have \u2016 1kM2\u2016 \u2265 \u2016vvT \u2016 \u2212 1/5 = 4/5. Therefore the top eigenvector of 1kM\u22172 has eigenvalue larger than 4/5. Then we can decompose the difference between 1k \u00b7M\u22172 and vvT into 1k \u00b7M\u22172 \u2212 vvT = 45 \u00b7 ( v\u0302v\u0302T \u2212 vvT ) +((\n1 kM \u2217 2 \u2212 45 v\u0302v\u0302T ) \u2212 15vvT ) . Note that since ( 1 kM \u2217 2 \u2212 45 v\u0302v\u0302T ) is a PSD matrix with eigenvalue at most\n1/5, we have \u2016 ((\n1 kM \u2217 2 \u2212 45 v\u0302v\u0302T ) \u2212 15vvT ) \u2016 \u2264 2/5 by triangle inequality. Then by triangle inequality\nand our assumption again we obtain that\n1 5 \u2265 \u20161 k \u00b7M\u22172 \u2212 vvT \u2016 \u2265 \u2016 4 5\n( v\u0302v\u0302T \u2212 vvT ) \u2016\u2212\u2016\n(( 1\nk M\u22172 \u2212\n4 5 v\u0302v\u0302T\n) \u2212 1\n4 vvT\n) \u2016 \u2265 \u20164\n5\n( v\u0302v\u0302T \u2212 vvT ) \u2016\u2212 2\n5\nTherefore we obtain that \u2016vvT \u2212 v\u0302v\u0302T \u2016 \u2264 3/5 and therefore \u2016vvT \u2212 v\u0302v\u0302T \u20162F \u2264 2\u2016vvT \u2212 v\u0302v\u0302T \u20162 \u2264 1. It follow that |\u3008v, v\u0302\u3009|2 = 1\u2212 12\u2016vvT \u2212 v\u0302v\u0302T \u20162F \u2265 1/2.\nNext we are going to show that it is impossible for M\u22172 to have an eigenvector that is close to v with a large eigenvalue and therefore we will get a contradiction. Let v\u0302 = \u03b1v + \u03b2s where s is orthogonal to v and \u03b12 + \u03b22 = 1 and \u03b1 \u2265 \u221a 1/2, and \u03b2 \u2264 \u221a 1/2. Then using triangle inequality we have that \u2016v\u0302\u2016\u03a3\u0302 \u2264 \u2016\u03b1v\u2016\u03a3\u0302 + \u2016\u03b2s\u2016\u03a3\u0302 \u2264 \u221a O(\u03bb)\u03b1 + \u221a \u2016\u03a3\u0302\u2016\u03b2. Proposition 5.3 of [KNV15] implies that for sufficiently large C and \u03bb \u2265 1, when p/n \u2265 C\u03bb, \u2016\u03a3\u0302\u2016 \u2264 1.01p/n. Therefore under our assumption we have that \u2016\u03a3\u0302\u2016 \u2264 1.01p/n. It follows \u03b2 \u2264 \u221a 1/2 that \u2016v\u0302\u2016\u03a3\u0302 \u2264 \u221a O(\u03bb)\u03b1+ \u221a \u03b2 \u00b7 \u221a p/n \u2264\u221a\nO(\u03bb) + \u221a p/2n. Therefore, we have that\n0.99p/n \u2264 1 k \u00b7 SoS4(\u03a3\u0302) = 1 k \u00b7 \u3008M2, \u03a3\u0302\u3009 = 1 k \u00b7 \u3008M\u22172 \u2212 4k 5 \u00b7 v\u0302v\u0302T , \u03a3\u0302\u3009+ 4 5 \u3008v\u0302v\u0302T , \u03a3\u0302\u3009\n\u2264 1 k tr(M\u22172 \u2212 4 5 \u00b7 v\u0302v\u0302T )\u2016\u03a3\u0302\u20162 + 4 5 \u2016v\u0302\u20162 \u03a3\u0302 \u2264 1 5 \u2016\u03a3\u0302\u2016+O(\u03b12\u03bb) +O( \u221a \u03bbp/n) + 2 5 \u00b7 p/n \u2264 4 5 \u00b7 p n +O( \u221a \u03bbp/n)\nwhere in the third line we used the fact that \u2016v\u0302\u2016\u03a3\u0302 \u2264 \u221a O(\u03bb) + \u221a p/2n, and the last line we used \u2016\u03a3\u0302\u2016 \u2264 1.01p/n.Note that this is a contradiction since we assumed that p/n \u2265 C\u03bb for sufficiently large C."}, {"heading": "6 Analysis of matrices P and Q", "text": "In this section we prove Lemma 4.4 and 4.5. They basically follow direct calculation and the pseudorandomness properties of data matrix X listed in Condition 4.1.\nProof of Lemma 4.4. Note that since p = 1.1\u03b3n and 1 \u2264 \u03b3 \u2264 n, we have that O(n2) \u2265 p \u2265 n. We verify equations (4.3), (4.4) and (4.5) and the bounds for F and E one by one.\n\u2022 Equation (4.3): For the case when i = j, we verify P (x4i ) using property (P1) and (P2),\nP (x4i ) = \u03b3k\np2n3\n \u3008Xi,Xi\u30094 + \u2211\n\u2113\u2208[p]\\i \u3008Xi,X\u2113\u30094\n \n\u2264 \u03b3k p2n3\n( n3\u3008Xi,Xi\u3009+ O\u0303(pn2) )\n= M\u0303(x2i ) + O\u0303\n( \u03b3k\npn\n)\nFor distinct i, j, we have that\nP (x3ixj) = \u03b3k\np2n3\n \u3008Xi,Xi\u30093\u3008Xi,Xj\u3009+ \u3008Xi,Xj\u30093\u3008Xi,Xi\u3009+ \u2211\n\u2113\u2208[p]\\i\u222aj \u3008Xi,X\u2113\u30093\u3008Xj ,X\u2113\u3009\n \n= \u03b3k\np2n3\n( n3\u3008Xi,Xj\u3009 \u00b1 O\u0303(n2.5)\u00b1 O\u0303(pn1.5) )\n= M\u0303(xixj)\u00b1 O\u0303 ( \u03b3k\np2n.5\n) \u00b1 O\u0303 ( \u03b3k\npn1.5\n)\n= M\u0303(xixj)\u00b1 O\u0303 ( \u03b3k\npn1.5\n)\nwhere in the second equality we use equation (P3), and p \u2265 n.\n\u2022 Equation (4.5): Note that for distinct i, j, s, t, by equation (P4), we have\n|P (xixjxsxt)| = \u03b3k\np2n3 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303 ( \u03b3k p1.5n )\nTherefore taking the sum over all distinct i, j, s, t we have\n\u2211\ni,j,s,t distinct\n|P (xixjxsxt)| \u2264 O\u0303 ( \u03b3k\np1.5n\n) \u00b7 p4 = O\u0303 ( \u03b3p2.5k\nn\n) \u2264 k4/4 (6.1)\nwhere we used k \u226b \u03b37/6\u221an, which implies that k3 \u226b \u03b3p2.5/n.\nBy equation (4.2) and equation (4.3), we have that\n\u2211\ni,j\n|P (x3i xj)| \u2264 \u2211\ni,j\n|M\u0303(xixj)|+p2 \u00b7 O\u0303 ( \u03b3k\npn1.5\n) +p \u00b7 O\u0303 ( \u03b3k\npn\n) \u2264 k2/2+ O\u0303 ( \u03b3k \u221a n ) \u2264 k2 (6.2)\nwhere we used the fact that p \u2264 n2 and k \u226b \u03b3\u221an. For distinct i, s, t, we have that \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u30092\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113 6=i,s,t \u3008Xi,X\u2113\u30092\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u3008Xi,Xi\u30092\u3008Xs,Xi\u3009\u3008Xt,Xi\u3009 \u2223\u2223\n+ \u2223\u2223\u3008Xi,Xs\u30092\u3008Xs,Xs\u3009\u3008Xt,Xs\u3009 \u2223\u2223+ \u2223\u2223\u3008Xi,Xt\u30092\u3008Xs,Xt\u3009\u3008Xt,Xt\u3009 \u2223\u2223 = O\u0303(pn2) + O\u0303(n3) + O\u0303(n2.5) = O\u0303(pn2)\nIt follows that\n|P (x2i xsxt)| = \u03b3k\np2n3 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u30092\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303 ( \u03b3k pn )\nand therefore,\n\u2211\ni,s,t disctinct\n|P (x2i xsxt)| \u2264 p3 \u00b7 O\u0303 ( \u03b3k\npn\n) = O\u0303 ( \u03b3kp2\nn\n) (6.3)\nTherefore, combining equation (6.1), (6.2), (6.3), we obtain that\n\u2211\ni,j,s,t\n|P (xixjxsxt)| \u2264 k2 + O\u0303 ( \u03b3kp2\nn\n) + k4/4 \u2264 k4/3\n\u2022 Equation (4.4): Finally it remains to bound E . Note that E is a sum of submatrices of P and therefore it is PSD. Moreover,\nEss = \u2211\ni\u2208[p] P (x2i x 2 s) =\n\u03b3k\np2n3\n\u2211\ni\n\u2211\n\u2113\n\u3008Xi,X\u2113\u30092\u3008Xs,X\u2113\u30092\n= \u03b3k\np2n3\n\u2211\ni\n\u3008Xi,X\u2113\u30092 \u2211\n\u2113\n\u3008Xs,X\u2113\u30092\n\u2264 \u03b3k p2n3\n\u00b7 O\u0303(p2n2) = O\u0303 ( \u03b3k\nn\n)\nwhere the last inequality uses equation (P2). Finally we bound Est using equation (P6) \u2211\ni\u2208[p] P (x2ixsxt) =\n\u03b3k\np2n3\n\u2211\ni\n\u2211\n\u2113\n\u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\n\u2264 \u03b3k p2n3 O\u0303(p2n1.5) = O\u0303( \u03b3k n1.5 )\nProof of Lemma 4.5. Again we verify equation (4.6), (4.7) and (4.8) in order.\n\u2022 Equation (4.6): By definition we have that for any i, j,\nQ(x3i xj) = \u03b3k2\np3n \u00b7 3n\u3008Xi,Xj\u3009 =\n3\u03b3k2\np3 \u3008Xi,Xj\u3009 =\n3k\np M\u0303 (xixj)\n\u2022 Equation (4.8): For the sparsity constraint, we note first that for distinct i, j, s, t, using property (P2), we have\n|Q(xixjxsxt)| \u2264 \u03b3k2\np3n \u00b7 O\u0303(n) = O\u0303\n( \u03b3k2\np3\n)\nand therefore taking sum, we have\n\u2211\ni,j,s,t disctinct\n|Q(xixjxsxt)| \u2264 O\u0303(\u03b3k2p) \u2264 k4/6 (6.4)\nwhere we used the fact that k2 \u226b=\u226b c2n. We bound other terms as follows: For any i, j, s, t \u2208 [p], we have that\nQ(xixjxsxt) \u2264 \u03b3k2 p3n \u00b7 3n2 = 3\u03b3k 2n p3\nThere are only at most O(p3) different choices of i, j, s, t such that i, j, s, t are not distinct, therefore we have \u2211\ni,j not distinct\n|Q(x3ixj)| \u2264 3\u03b3k2n\np3 \u00b7O(p3) \u2264 k4/6 (6.5)\nwhere we used the fact that k \u226b \u03b3\u221an and \u03b3 \u2265 1. Combining equation (6.4) and (6.5), we obtain that\n\u2211\ni,j,s,t\n|Q(xixjxsxt)| \u2264 k4/3\n\u2022 Equation (4.7): For any s, t, we have\n\u2211\ni\nQ(x2ixsxt) = \u03b3k2\np3n\n\u2211 i\u2208[p] (n\u3008Xs,Xt\u3009+ 2\u3008Xi,Xs\u3009\u3008Xi,Xt\u3009)\n= \u03b3k2\np2 \u3008Xs,Xt\u3009+\n2\u03b3k2\np3n\n\u2211 i\u2208[p] \u3008Xi,Xs\u3009\u3008Xi,Xt\u3009\nTherefore Gst = 2\u03b3k 2\np3n \u2211 i\u2208[p]\u3008Xi,Xs\u3009\u3008Xi,Xt\u3009 forms a PSD matrix. Moreover, when s 6= t,\nusing equation (P5), we have that\n\u2211\ni\nQ(x2i xsxt) = kM\u0303 (xsxt)\u00b1 2\u03b3k2\np3n \u00b7 O\u0303(p\n\u221a n)\n= kM\u0303 (xsxt)\u00b1 O\u0303 ( \u03b3k2\np2 \u221a n\n)\nWhen s = t, we have that\n\u2211\ni\nQ(x2ix 2 s) = kM\u0303(xsxt)\u00b1\n2\u03b3k2\np3n \u00b7 O\u0303 (pn)\n= kM\u0303(xsxt)\u00b1 O\u0303 ( \u03b3k2\np2\n)"}, {"heading": "7 Pseudo-randomness of X", "text": "In this section, we prove that basically as long as the noise model is subgaussian and has variance 1(which generalizes the standard Bernoulli and Gaussian distributions), after normalizing the rows of the data matrix X \u223c H0, it satisfies the pseudorandomness condition 4.1. Theorem 7.1. Suppose independent random variables X1, . . . ,Xp \u2208 Rn satisfy that for any i, Xi has a i.i.d entries with mean zero, variance 1, and subgaussian variance proxy 7 O(1), then the matrix X\u0304 with XTi \u2016Xi\u2016 as rows satisfies the pseudorandomness condition 4.1.\nThe proof of the Theorem relies on the following Proposition and Theorem 7.4. The proposition says that XTi \u2016Xi\u2016 still satisfies good properties like symmetry and that each entries has a subgaussian tail, even though its entries are no longer independent due to normalization. These properties will be encapsulated in the definition of a good random variable following the proposition. Then we prove in Theorem 7.4 that these properties suffice for establishing the pseudorandomness Condition 4.1 with high probability. We will heavily use the \u03c8\u03b1-Orlicz norm (denoted \u2016\u00b7\u2016\u03c8\u03b1) of a random variable, defined in Definition 8.1, and its properties, summarized in the next (toolbox) section. Intuitively, \u2016 \u00b7\u2016\u03c82 norm is a succinct and convenient way to capture the \u201csubgaussianity\u201d of a random variable.\n7A real random variable X is subgaussian with variance proxy \u03c32 if it has similar tail behavior as gaussian distribution with variance \u03c32, and formally if for any t \u2208 R, E[exp(tX)] \u2264 exp(t 2\u03c32/2)\nProposition 7.2. Suppose y \u2208 Rn has i.i.d entries with variance 1 and mean zero, and gaussian variance proxy O(1), then random variable x = y\u2016y\u2016 satisfies the following properties:\n1. \u2016x\u20162 = n, almost surely.\n2. for any vector a \u2208 Rn with \u2016a\u20162 \u2264 2n, \u2016\u3008x, a\u3009\u20162\u03c82 \u2264 O(n).\n3. \u2016|x|\u221e\u2016\u03c82 \u2264 O\u0303(1)\n4. E[x2i ] = 1, E[x 4 i ] = C4, and E[x 2 ix 2 j ] = C2,2 for all i and j 6= i, where C4, C2,2 = O(1) are\nconstants that don\u2019t depend on i, j\n5. For any monomial x\u03b1 with an odd degree, E[x\u03b1] = 0.\nFor simplicity, we call a random variable good if it satisfies the five properties listed in the proposition above. Goodness will be invoked in most statements below.\nDefinition 7.3 (goodness). A random variable x \u2208 Rn is called good, if it satisfies the conclusion of Proposition 7.2.\nWe will show a random matrix X with good rows satisfies the pseudo-randomness Condition 4.1 with high probability.\nTheorem 7.4. Suppose independent n-dimensional random vectors X1, . . . ,Xp with p \u2265 n are all good, then X1, . . . ,Xp satisfies the pseudorandomness condition 4.1 with high probability.\nThe general approach to prove the theorem is just to use the concentration of measure. The only caveat here is that in most of cases, the random variables that we are dealing with are not bounded a.s. so we can\u2019t use Chernoff bound or Bernstein inequality directly. However, though these random variables are not bounded a.s., they typically have a light tail, that is, their \u03c8\u03b1 norms can be bounded. Then we are going to apply Theorem 8.4 of Ledoux and Talagrand\u2019s, a extended version of Bernstein inequality with only \u03c8\u03b1 norm boundedness required. We will also use other known technical results listed in the toolbox Section 8.\nProof of Theorem 7.4. Equation (P1) and (P2) follows the assumptions on Xi\u2019s and union bound. Equation (P3) is proved in Lemma 7.5 by taking u = Xs and v = Xt and view the rest of Xi;s as Zj \u2019s in the statement of Lemma 7.5. Equation (P4) is proved in Lemma 7.6, (P5) in Lemma 7.8, (P6) in Lemma 7.10, and equation P7 is proved in Lemma 7.15.\nLemma 7.5. For any good random variable x, we have that for fixed u, v with \u2016u\u20162 = \u2016v\u20162 = n , |u|\u221e \u2264 O\u0303(1), |v|\u221e \u2264 O\u0303(1), and \u3008u, v\u3009 \u2264 O\u0303( \u221a n),\n\u2223\u2223 E [ \u3008x, u\u30093\u3008x, v\u3009 ]\u2223\u2223 \u2264 O\u0303(n1.5)\nand moreover, for p \u2265 n and a sequence of good independent random variables Z1, . . . , Zp, we have that with high probability, \u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\n\u3008Zi, u\u30093\u3008Zi, v\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n 1.5p)\nProof. We calculate the expectation as follows\nE\n[ \u3008x, u\u30093\u3008x, v\u3009 ] = E    \u2211\ni\nu2ix 2 i + 2\n\u2211\ni<j\nuiujxixj\n   \u2211\ni\nviuix 2 i +\n\u2211 i 6=j uivjxixj\n   \n= E\n[ \u2211\ni\nu3i vix 4 i ] + E  \u2211\ni 6=j u2iujvjx 2 i x 2 j\n + E  \u2211\ni 6=j uiuj(uivj + viuj)x\n2 ix 2 j\n \n= (C4 \u2212 C2,2) \u2211\ni\nu3i vi + C2,2n \u2211\ni\nuivi +C2,2 \u2211\ni 6=j (u2i ujvj + u 2 juivi)\n= (C4 \u2212 3C2,2) \u2211\ni\nu3i vi + 3C2,2n \u2211\ni\nuivi\nTherefore by our assumption on u and v we obtain that\n\u2223\u2223 E [ \u3008x, u\u30093\u3008x, v\u3009 ]\u2223\u2223 \u2264 O\u0303(n) +O(n)|\u3008u, v\u3009| \u2264 O\u0303(n1.5)\nNow we prove the second statement. Since \u2016\u3008Zi, u\u3009\u2016\u03c82 \u2264 O( \u221a n), by Lemma 8.5 we have that\n\u2016\u3008Zi, u\u30093\u3008Zi, v\u3009\u2016\u03c81/2 \u2264 O(n2), and it follows Lemma 8.6 that \u2016\u3008Zi, u\u30093\u3008Zi, v\u3009\u2212E [ \u3008Zi, u\u30093\u3008Zi, v\u3009 ] \u2016\u03c81/2 \u2264 O(n2) Then by Lemma 8.4 we obtain that with high probability,\np\u2211\ni=1\n\u3008Zi, u\u30093\u3008Zi, v\u3009 \u2212 E [ p\u2211\ni=1\n\u3008Zi, u\u30093\u3008Zi, v\u3009 ] \u2264 O\u0303(n2\u221ap)\nNote that we have proved that \u2223\u2223E [\u2211p i=1\u3008Zi, u\u30093\u3008Zi, v\u3009 ]\u2223\u2223 = O\u0303(n1.5), therefore we obtain the desired result.\nLemma 7.6. Suppose p \u2265 n and X1, . . . ,Xp are good independent random variables, then with high probability, for any distinct i, j, s, t,\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n2\u221ap)\nProof. Fixing i, j, s, t, we can write\n\u2211 \u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 =\n\u2211\n\u2113\u2208[p]\\{i,j,s,t} \u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\n+ n\u3008Xj ,Xi\u3009\u3008Xs,Xi\u3009\u3008Xt,Xi\u3009+ n\u3008Xi,Xj\u3009\u3008Xs,Xj\u3009\u3008Xt,Xj\u3009 + n\u3008Xi,Xs\u3009\u3008Xj ,Xs\u3009\u3008Xt,Xs\u3009+ n\u3008Xi,Xt\u3009\u3008Xj ,Xt\u3009\u3008Xs,Xt\u3009\nUsing Lemma 7.7, the first term on RHS is bounded by O\u0303(n2 \u221a p) with high probability over the randomness of X\u2113, \u2113 \u2208 [p]\\{i, j, s, t}. The rest of the four terms are bounded by O\u0303(n2.5). Therefore putting together \u2016\u2211\u2113\u2208[p]\u3008Xi,X\u2113\u3009\u3008Xj ,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\u2016 \u2264 O\u0303(n2 \u221a p) for any fixed i, j, s, t with high probability and taking union bound we get the result.\nLemma 7.7. For any good random variable x, and for fixed a, b, c, d such that max{|a|\u221e, |b|\u221e, |c|\u221e, |d|\u221e} = O\u0303(1), and all the pair-wise inner products between a, b, c, d have magnitude at most O\u0303( \u221a n), we have that |E [\u3008x, a\u3009\u3008x, b\u3009\u3008x, c\u3009\u3008x, d\u3009]| = O\u0303(n)\nand moreover, for p \u2265 n and a sequence independent random variable Z1, . . . , Zp such that each Zi satisfies the conclusion of proposition 7.2, we have that with high probability,\n\u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\n\u3008Zi, a\u3009\u3008Zi, v\u3009\u3008Zi, c\u3009\u3008Zi, d\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n 2\u221ap)\nProof. We calculate the mean\nE [\u3008x, a\u3009\u3008x, b\u3009\u3008x, c\u3009\u3008x, d\u3009] = E\n \u2211\ni\u2208[p] aibicidix\n4 i +    \u2211\ni 6=j aibicjdjx\n2 ix 2 j     \nwhere we use {\u2211\ni 6=j aibicjdjx 2 ix 2 j } to denote the sum of aibicjdjx 2 i x 2 j and all its permutations\nwith repect to a, b, c, d. Note that\n\u2223\u2223\u2223\u2223\u2223\u2223 E  \u2211 i 6=j aibicjdjx 2 ix 2 j   \u2223\u2223\u2223\u2223\u2223\u2223 = C2,2 \u2223\u2223\u2223\u2223\u2223\u2223 \u3008a, b\u3009\u3008c, d\u3009 \u2212 \u2211 i\u2208[p] aibicidi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n)\nand \u2223\u2223\u2223\u2223\u2223\u2223 E  \u2211 i\u2208[p] aibicidix 4 i   \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 C4 \u2211 i\u2208[p] aibicidi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n)\nand therefore we have |E [\u3008x, a\u3009\u3008x, b\u3009\u3008x, c\u3009\u3008x, d\u3009]| \u2264 O\u0303(n). Since \u3008x, a\u3009 has \u03c82 norm \u221a n and similar for the other three terms, we have that by Lemma 8.5\nthat \u2016\u3008x, a\u3009\u3008x, b\u3009\u3008x, c\u3009\u3008x, d\u3009\u2016\u03c81/2 \u2264 O(n2). Therefore using Theorem 8.4 we have that \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u3008Zi, a\u3009\u3008Zi, v\u3009\u3008Zi, c\u3009\u3008Zi, d\u3009 \u2212 E[ p\u2211\ni=1 \u3008Zi, a\u3009\u3008Zi, v\u3009\u3008Zi, c\u3009\u3008Zi, d\u3009] \u2225\u2225\u2225\u2225\u2225 \u03c81/2 \u2264 O\u0303(n2\u221ap)\nLemma 7.8. Suppose p \u2265 n and X1, . . . ,Xp are good independent random variables, then with high probability, for any distinct s, t,\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[p] \u3008Xi,Xs\u3009\u3008Xi,Xt\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(p \u221a n)\nProof. With high probability over the randomness of Xi, i \u2208 [p]\\{s, t}, \u2211\ni\u2208[p] \u3008Xi,Xs\u3009\u3008Xi,Xt\u3009 =\n\u2211\ni\u2208[p]\\{s,t} \u3008Xi,Xs\u3009\u3008Xi,Xt\u3009+ 2\u3008Xs,Xt\u3009 \u2264 O\u0303(p\n\u221a n) + O\u0303( \u221a n)\nwhere the last inequality is by Lemma 7.9. Taking union bound we complete the proof.\nLemma 7.9. For p \u2265 n and a sequence of good independent random variable Z1, . . . , Zp, and any two fixed vectors u, v with |u|\u221e \u2264 O\u0303(1) and |v|\u221e \u2264 O\u0303(1), and \u3008u, v\u3009 \u2264 O\u0303( \u221a n), we have that with high probability, \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[p] \u3008Zi, u\u3009\u3008Zi, v\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(p \u221a n) Proof. E[\u3008Zi, u\u3009\u3008Zi, v\u3009] = \u3008u, v\u3009 \u2264 O\u0303( \u221a n), and therefore \u2223\u2223\u2223E [\u2211 i\u2208[p]\u3008Zi, u\u3009\u3008Zi, v\u3009 ]\u2223\u2223\u2223 \u2264 O\u0303(p \u221a n). Note that \u2016\u3008Zi, u\u3009\u2016\u03c82 \u2264 O( \u221a n) and therefore \u2016\u3008Zi, u\u3009\u3008Zi, v\u3009\u2016\u03c81 \u2264 O(n). By Theorem 8.4, we have the desired result.\nLemma 7.10. Suppose p \u2265 n and X1, . . . ,Xp are good independent random variables, then with high probability, for any distinct s, t,\n\u2211\ni,\u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2264 O\u0303(p2n1.5)\nProof. We expand the target as follows: \u2211 i,\u2113\u2208[p] \u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 =\n\u2211\ni\u2208[p],\u2113\u2208[p]\\s\u222at \u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\n+ \u2211\ni\n\u3008Xi,Xs\u30092\u3008Xs,Xs\u3009\u3008Xt,Xs\u3009+ \u2211\ni\n\u3008Xi,Xt\u30092\u3008Xt,Xt\u3009\u3008Xs,Xt\u3009\n= \u2211\ni\u2208[p]\\s\u222at,\u2113\u2208[p]\\s\u222at \u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009\n+ \u2211\ni\n\u3008Xi,Xs\u30092\u3008Xs,Xs\u3009\u3008Xt,Xs\u3009+ \u2211\ni\n\u3008Xi,Xt\u30092\u3008Xt,Xt\u3009\u3008Xs,Xt\u3009\n+ \u2211\n\u2113\u2208[p]\\s\u222at \u3008Xs,X\u2113\u30093\u3008X\u2113,Xt\u3009+\n\u2211\n\u2113\u2208[p]\\s\u222at \u3008Xt,X\u2113\u30093\u3008X\u2113,Xs\u3009\nBy equation (P3), we have that\n\u2211\n\u2113\u2208[p]\\s\u222at \u3008Xs,X\u2113\u30093\u3008X\u2113,Xt\u3009 \u2264 O\u0303(pn1.5)\nSince \u3008Xs,Xt\u3009 \u2264 O\u0303( \u221a n) and \u2211 i\u2208[p]\u3008Xi,Xs\u30092 = n2 + \u2211 i 6=s\u3008Xi,Xs\u30092 \u2264 O\u0303(np), we have that\n\u2211\ni\n\u3008Xi,Xs\u30092\u3008Xs,Xs\u3009\u3008Xt,Xs\u3009 \u2264 O\u0303(pn2.5)\nInvoking Lemma 7.11 with u = Xs and v = Xt fixed and view X\u2113, \u2113 \u2208 [p]\\s\u222a t as random variables Zi\u2019s, we have that with high probability,\n\u2211\ni\u2208[p]\\s\u222at,\u2113\u2208[p]\\s\u222at \u3008Xi,X\u2113\u3009\u3008Xi,X\u2113\u3009\u3008Xs,X\u2113\u3009\u3008Xt,X\u2113\u3009 \u2264 O\u0303(p2n1.5)\nHence combining the three equations above, taking union bound over all choices of s, t, we obtain the desired result.\nLemma 7.11. For p \u2265 n and a sequence of good independent random variables Z1, . . . , Zp, and any two fixed vectors u, v with |u|\u221e \u2264 O\u0303(1) and |v|\u221e \u2264 O\u0303(1), and \u3008u, v\u3009 \u2264 O\u0303( \u221a n), we have that with high probability, \u2211\ni\u2208[p]\n\u2211 j\u2208[p] \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2264 O\u0303(p2n1.5)\nProof. We first extract the consider those cases with i = j separately by expanding \u2211\ni\u2208[p]\n\u2211 j\u2208[p] \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 = \u2211 i 6=j \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 + \u2211 i \u3008Zi, Zi\u30092\u3008Zi, u\u3009\u3008Zi, v\u3009\n= \u2211\ni 6=j \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 + O\u0303(pn2.5) (7.1)\nwhere the the last line uses Lemma 7.9. Let Y1, . . . , Yp are independent random variables that have the same distribution as Z1, . . . , Zp, respectively, then by Theorem 8.7, we can decouple the sum of functions of Zi, jZj into a sum that of functions of Zi and Yj,\nPr\n \u2211\ni 6=j \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2265 t\n  \u2264 C Pr  \u2211\ni 6=j \u3008Yi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2265 t/C\n \nNow we can invoke Lemma 7.12 which deals with RHS of the equation above, and obtain that with high probability \u2211\ni 6=j \u3008Yi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2264 O\u0303(p2n1.5)\nTherefore, with high probability, \u2211\ni 6=j \u3008Zi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2264 O\u0303(p2n1.5)\nThen combine with equation (7.1) we obtain the desired result.\nLemma 7.12. For p \u2265 n and a sequence of good independent random variables Z1, . . . , Zp, let Y1, . . . , Yp be independent random variables which have the same distribution as Z1, . . . , Zp, respectively, then for any two fixed vectors u, v with |u|\u221e \u2264 O\u0303(1) and |v|\u221e \u2264 O\u0303(1), and \u3008u, v\u3009 \u2264 O\u0303( \u221a n), with high probability,\n\u2211\ni\u2208[p]\n\u2211 j\u2208[p] \u3008Yi, Zj\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2264 O\u0303(p2n1.5)\nProof. Let B = \u2211\ni\u2208[p] YiY T i . Therefore by Lemma 7.13, we have that with high probability over\nthe randomness of Y , \u2016B\u20162 \u2264 O\u0303(p), tr(B) = pn. Moreover, by Lemma 7.9, we have that with high probability, |uTBv| \u2264 O\u0303(p\u221an). Note that these bounds only depend on the randomness of Y , and conditioning on all these bounds are true, we can still use the randomness of Zi\u2019s for concentration. We invoke Lemma 7.14 and obtain that\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i,j\u2208[p] \u3008Zj , Yi\u30092\u3008Zj , u\u3009\u3008Zj , v\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 p\u2211 j=1 ZTj BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(p2n1.5)\nLemma 7.13. For p \u2265 n and a sequence of good independent random variables Z1, . . . , Zp, we have that with high probability, \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208[p] ZiZ T i \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 O\u0303(p)\nProof. We use matrix Bernstein inequality. First of all, we have that E[ZiZ T i ] = In\u00d7n, and therefore\nE [\u2211 i\u2208[p]ZiZ T i ] = pIn\u00d7n. Moreover, we check the variance of the ZiZTi :\nE[ZiZ T i ZiZ T i ] = nE[ZiZ T i ] = nIn\u00d7n\nFinally we observe that \u2016ZiZTi \u2016 \u2264 n. Thus applying matrix Bernstein inequality we obtain that with high probability,\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208[p] ZiZ T i \u2212 pIn\u00d7n \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 O\u0303(\u221anp+ n) = O\u0303(\u221anp)\nLemma 7.14. For p \u2265 n and a sequence of good independent random variables Z1, . . . , Zp, and for any fixed symmetric PSD matrix B \u2208 Rn\u00d7n with \u2016B\u2016 \u2264 O\u0303(p), tr(B) \u2264 2pn, and any two fixed vectors u, v with |u|\u221e \u2264 O\u0303(1) and |v|\u221e \u2264 O\u0303(1), and \u3008u, v\u3009 \u2264 O\u0303( \u221a n), we have that with high probability over the randomness of Zi\u2019s, \u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\nZTi BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(p 2n1.5)\nProof. Let W = xTBx\u3008x, u\u3009\u3008x, v\u3009, where x is a random variable that satisfies the conclusion of Proposition 7.2. We first calculate the expectation of W ,\nE [W ] = E\n   \u2211\ni\nBiix 2 i +\n\u2211 i 6=j xixjBij\n   \u2211\ni\nuivix 2 i +\n\u2211 i 6=j xixjuivj\n   \n= (C4 \u2212 C2,2) \u2211\ni\nBiiuivi + C2,2tr(B)\u3008u, v\u3009 + E\n \u2211\ni 6=j Bij(uivj + ujvi)x\n2 i x 2 j\n \n= (C4 \u2212 3C2,2) \u2211\ni\nBiiuivi + tr(B)\u3008u, v\u3009\nTherefore by the fact that |u|\u221e \u2264 O\u0303(1) and tr(B) \u2264 2pn, we obtain that |E[W ]| \u2264 O\u0303(pn1.5). Observe that ZTj BZj \u2264 O\u0303(pn) a.s. (with respect to the randomness of Zj), and \u2016\u3008Zi, u\u3009\u3008Zi, v\u3009\u2016\u03c81 \u2264\nO(n), therefore we have that \u2016ZTj BZj\u3008Zi, u\u3009\u3008Zi, v\u3009\u2016\u03c81 \u2264 O(pn2). Using Theorem 8.4, we obtain that with high probability,\n\u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\nZTi BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 \u2212 E [ p\u2211\ni=1\nZTi BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 ]\u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(n 2p1.5)\nUsing the fact that E [ ZTi BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 ] \u2264 O\u0303(pn1.5) we obtain that with high probability\n\u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\nZTi BZi\u3008Zi, u\u3009\u3008Zi, v\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(p 2n1.5)\nLemma 7.15. Suppose p \u2265 n and X1, . . . ,Xp are good independent random variables, then with high probability,\n\u2016XXT \u20162F \u2265 (1\u2212 o(1))p2n\nProof. We first i and examine \u2211 j 6=i\u3008Xj ,Xi\u30092 first. We have that E[ \u2211\nj 6=i\u3008Xj ,Xi\u30092] = (p\u22121)\u2016Xi\u20162 = (p \u2212 1)n. Moreover, \u2016\u3008Xj ,Xi\u30092\u2016\u03c81 \u2264 O(n) (where Xj is viewed as random and Xi is viewed as fixed). Therefore by Theorem 8.4, we obtain that with high probability over the randomness of Xj \u2019s, (j 6= i), \u2211 j 6=i\u3008Xj ,Xi\u30092 = (p \u2212 1)n \u00b1 O\u0303(n \u221a p) = (1\u00b1 o(1))pn. Therefore taking union bound over all i, and taking the sum we obtain that\n\u2016XXT \u20162F \u2265 \u2211\ni\n\u2211 j 6=i \u3008Xj ,Xi\u30092 \u2265 (1\u2212 o(1))p2n"}, {"heading": "8 Toolbox", "text": "This section contains a collection of known technical results which are useful in proving the concentration bounds of Section 7. We note that when the data matrix X takes uniformly {\u00b11} entries, then X satisfies Proposition 7.2 without any normalization and actually due to the independence of the entries, it\u2019s much easier to prove that it satisfies Condition 4.1.\nDefinition 8.1 (Orlicz norm \u2016 \u00b7 \u2016\u03c8\u03b1). For 1 \u2264 \u03b1 < \u221e, let \u03c8\u03b1(x) = exp(x\u03b1) \u2212 1. For 0 < \u03b1 < 1, let \u03c8\u03b1(x) = x\n\u03b1 \u2212 1 for large enough x \u2265 x\u03b1, and \u03c8\u03b1 is linear in [0, x\u03b1]. The Orlicz norm \u03c8\u03b1 of a random variable X is defined as\n\u2016X\u2016\u03c8\u03b1 , inf{c \u2208 (0,\u221e) | E [\u03c8\u03b1(|X|/c) \u2264 1] (8.1)\nNote that by definition \u03c8\u03b1 is convex and increasing. The following Theorem of Ledoux and Talagrand\u2019s is our main tool for proving concentration inequalities in Section 7.\nTheorem 8.2 (Theorem 6.21 of [LT13]). There exists a constant K\u03b1 depending on \u03b1 such that for a sequence of independent mean zero random variables X1, . . . ,Xn in L\u03c8\u03b1 , if 0 < \u03b1 \u2264 1,\n\u2225\u2225\u2225\u2225\u2225 \u2211\ni\nXi \u2225\u2225\u2225\u2225\u2225 \u03c8\u03b1 \u2264 K\u03b1 (\u2225\u2225\u2225\u2225\u2225 \u2211 i Xi \u2225\u2225\u2225\u2225\u2225 1 + \u2225\u2225\u2225\u2225maxi \u2016Xi\u2016 \u2225\u2225\u2225\u2225 \u03c8\u03b1 ) (8.2)\nand if 1 < \u03b1 \u2264 2, \u2225\u2225\u2225\u2225\u2225 \u2211\ni\nXi \u2225\u2225\u2225\u2225\u2225 \u03c8\u03b1 \u2264 K\u03b1 (\u2225\u2225\u2225\u2225\u2225 \u2211 i Xi \u2225\u2225\u2225\u2225\u2225 1 + ( \u2211 i \u2016Xi\u2016\u03b2\u03c8\u03b1) 1/\u03b2 ) (8.3)\nwhere 1/\u03b1+ 1/\u03b2 = 1.\nThe following convenient Lemma allows us to control the second part of RHS of (8.2) easily.\nLemma 8.3 ([vdVW00]). There exists absolute constant c, such that for any real valued random variables X1, . . . ,Xn, we have that\n\u2225\u2225\u2225\u2225max1\u2264i\u2264n |Xi| \u2225\u2225\u2225\u2225 \u03c8\u03b1 \u2264 c\u03c8\u22121\u03b1 (n) max 1\u2264i\u2264n \u2016Xi\u2016\u03c8\u03b1\nUsing Lemma 8.3 and Theorem 8.2, we obtain straightforwardly the following theorem that will be used many times for proving concentration bounds in this paper.\nTheorem 8.4. For any 0 < \u03b1 \u2264 1, there exists a constant K\u03b1 such that for a sequence of independent random variables X1, . . . ,Xn,\n\u2225\u2225\u2225\u2225\u2225 \u2211\ni\nXi \u2212 E[ \u2211\ni\nXi] \u2225\u2225\u2225\u2225\u2225 \u03c8\u03b1 \u2264 K\u03b1 \u221a n log n \u00b7max i \u2016Xi\u2016\u03c8\u03b1 (8.4)\nwhich implies that with high probability over the randomness of Xi\u2019s,\n\u2223\u2223\u2223\u2223\u2223 \u2211\ni\nXi \u2212 E[ \u2211\ni\nXi] \u2223\u2223\u2223\u2223\u2223 \u2264 O\u0303(K\u03b1 \u221a n \u00b7max i \u2016Xi\u2016\u03c8\u03b1)\nThe following two lemmas are used to bound the Orlicz norms of random variables.\nLemma 8.5. There exists constant D\u03b1 depending on \u03b1 such that, if two (possibly correlated) random variables X, Y have \u03c8\u03b1 Orlicz norm bounded by \u2016X\u2016\u03c8\u03b1 \u2264 a and \u2016Y \u2016\u03c8\u03b1 \u2264 b then \u2016XY \u2016\u03c8\u03b1/2 \u2264 D\u03b1ab\nProof. For any x,y, a, b, \u03b1 > 0,\nexp (|xy|)\u03b1/2 \u2212 1 \u2264 exp ( 1\n2 |x|\u03b1 + 1 2 |y|\u03b1\n) \u2212 1\n\u2264 1 2 ((exp |x|\u03b1 \u2212 1) + (exp |y|\u03b1 \u2212 1))\nMoreover, note that by definition of \u03c8\u03b1, there exists constant C\u03b1 and C \u2032 \u03b1 such that for x \u2265 0, C \u2032\u03b1(exp(x \u03b1) \u2212 1) \u2265 \u03c8\u03b1(x) \u2265 C\u03b1(exp(x\u03b1) \u2212 1). Therefore we have that there exists a constant E\u03b1 such that \u03c8\u03b1/2(|xy|) \u2264 E\u03b12 (\u03c8\u03b1(|x|)+\u03c8\u03b1(|y|)). Also note that for any constant c, there exsits constant c\u2032 such that \u03c8\u03b1(x/c\u2032) \u2264 \u03c8\u03b1(x)/c. Therefore, choosing D\u03b1 such that \u03c8\u03b1/2(x/D\u03b1) \u2264 \u03c8\u03b1(x)/E\u03b1 for all x \u2265 0 we obtain that\nE [ \u03c8\u03b1/2(\n|XY | abD\u03b1 )\n] \u2264 E [ \u03c8\u03b1/2(\n|XY | ab )\n] /E\u03b1 \u2264 1\n2 (E[\u03c8\u03b1(|X|/a)] + E[\u03c8\u03b1(|Y |/b)]) \u2264 1\nLemma 8.6. Suppose random variable X has \u03c8\u03b1-Orlicz norm a, then X \u2212 E[X] has \u03c8\u03b1 Orlicz norm at most 2a.\nProof. First of all, since \u03c8\u03b1 is convex and increasing on [0,\u221e), we have that E [\u03c8\u03b1(|X|/a)] \u2265 \u03c8\u03b1(E[|X|]/a) \u2265 \u03c8\u03b1(|E[X]|/a). Then we have that\nE [ \u03c8\u03b1(\n|X \u2212 E[X]| 2a )\n] \u2264 E[\u03c8\u03b1(\n|X| 2a + |E[X]| 2a\n)] \u2264 E [ 1\n2 \u03c8\u03b1(|X|/a) +\n1 2 \u03c8\u03b1(|E[X]|/a)\n] \u2264 E[\u03c8\u03b1(|X|/a)] \u2264 1\nwhere we used the convexity of \u03c8\u03b1 and the fact that E [\u03c8\u03b1(|X|/a)] \u2265 \u03c8\u03b1(|E[X]|/a)\nThe following Theorem of [PMS95] is useful to decouple the randomness of a sum of correlated random variables into a form that is easier to control.\nTheorem 8.7 (Special case of Theorem 1 of [PMS95]). Let X1, . . . ,Xn, Y1, . . . , Yn are independent random variables on a measurable space over S, where Xi and Yi has the same distribution for i = 1, . . . , n. Let fij(\u00b7, \u00b7) be a family of functions taking S \u00d7 S to a Banach space (B, \u2016 \u00b7 \u2016). Then there exists absolute constant C, such that for all n \u2265 2, t > 0,\nPr   \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i 6=j fij(Xi,Xj) \u2225\u2225\u2225\u2225\u2225\u2225 \u2265 t   \u2264 C Pr   \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i 6=j fij(Xi, Yj) \u2225\u2225\u2225\u2225\u2225\u2225 \u2265 t/C  \nThe following lemma provides a simple way to prove the PSDness of a matrix that has large value on the diagonal and small off-diagonal values.\nLemma 8.8 (Consequence of Gershgorin Circle Theorem). Suppose a matrix \u0393 is of the form\n\u0393 = [ A B C D ] where A,D are square diagonal matrices, and C is of dimension n \u00d7 m. Then \u0393 is PSD if there exists \u03b1 > 0 such that the following holds: Aii \u2265 1\u03b1 \u2211\nj\u2208[n] |Cij |,\u2200 \u2208 [p] and Djj \u2265 \u03b1 \u2211 i\u2208[m] |Cij |,\u2200j \u2208 [p].\nProof. Let vector u = (\u03b11m, \u03b1 \u221211n) and v = (\u03b1\u221211m, \u03b11n), where 1n is n-dimensional all 1\u2019s vector. Then \u0393 can be written as \u0393 = vvT \u2299 (uuT \u2299\u0393), where \u2299 denotes the entries-wise product of two matrices (That is, A\u2299B is a matrix with entry AijBij). Using the Gershgorin Circle Theorem and the conditions of the Lemma we obtain that uuT \u2299 \u0393 is PSD and therefore \u0393 is PSD."}, {"heading": "9 Conclusions and future directions", "text": "In this paper we prove a lower bounds on the number of samples required to solve the Sparse PCA problem by degree-4 SoS algorithms. This extends the (spectral) degree-2 SoS lower bound for the problem, establishing the quadratic gap from the number of samples required by the (inefficient) information theoretic bound. It remains an interesting problem to extend our lower bounds to higher degree SoS algorithms (or even better, show that with some constant degree, one can solve the problem with fewer samples). One specific difficulty we encountered in trying to extend the lower bound to higher degree was the polynomial constraint x3i = xi, capturing the discreteness of the hidden sparse vector. The SoS formulation of the problem without this condition is interesting as well, and lower bound for it may be easier.\nAs mentioned, it is possible that the best way to prove strong SoS lower bounds for Sparse PCA is via the reduction of Berthet and Rigollet\u2019s [BR13a], namely by improving existing lower bounds for the Planted Clique problem. However, we note that this approach is limited as well, as it seems\nthat sparse PCA is significantly harder. Specifically, Planted Clique has a simple O(log n)-degree SoS algorithm (and thus a quasi-polynomial time) optimal solution, whereas for Sparse PCA we know of no better sample-optimal algorithm than one running in exponential pO(k) time. It is thus conceivable that one can even prove \u2126(k)-degree SoS lower bounds for this problem.\nMore generally, we believe that statistical and machine learning problems provide a new and challenging setting for testing the power and limits and SoS algorithms. While we have fairly strong techniques for proving optimal SoS lower bounds for combinatorial optimization problems, we lack similar ones for ML problems. In particular, many other problems besides Sparse PCA seem to exhibit the apparent trade-off between the number of samples required information theoretically versus via computationally efficient techniques, offering fertile ground for attempting SoS lower bounds establishing such trade-offs.\nFinally it would be nice to see more reductions between problems of statistical and ML nature, as the one by [BR13a]. Efficient reductions have proved extremely powerful in computational complexity theory and optimization, enabling the framework of complexity classes and complete problems. Creating such a framework within machine learning will hopefully expose structure on the relative difficulty of problems in this vast area, highlighting some problems as more central to attack, and enabling both new algorithms and new lower bounds.\nAcknowledgments: We would like to thank Sanjeev Arora, Boaz Barak, Philippe Rigollet and David Steurer for helpful discussions throughout various stages of this work."}], "references": [{"title": "Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays", "author": ["U. Alon", "N. Barkai", "D.A. Notterman", "K. Gish", "S. Ybarra", "D. Mack", "A.J. Levine"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Alon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1999}, {"title": "\u00dcber die zerlegung definiter funktionen in quadrate", "author": ["Emil Artin"], "venue": "In Abhandlungen aus dem mathematischen Seminar der Universita\u0308t Hamburg,", "citeRegEx": "Artin.,? \\Q1927\\E", "shortCiteRegEx": "Artin.", "year": 1927}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash A. Amini", "Martin J. Wainwright"], "venue": "Ann. Statist., 37(5B):2877\u20132921,", "citeRegEx": "Amini and Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2009}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In STOC,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Tensor prediction, rademacher complexity and random 3-xor", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "CoRR, abs/1501.06521,", "citeRegEx": "Barak and Moitra.,? \\Q2015\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2015}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Sum-of-squares proofs and the quest toward optimal algorithms", "author": ["Boaz Barak", "David Steurer"], "venue": "In Proceedings of International Congress of Mathematicians (ICM),", "citeRegEx": "Barak and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Barak and Steurer.", "year": 2014}, {"title": "Adaptive elastic-net sparse principal component analysis for pathway association testing", "author": ["Xi Chen"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "Chen.,? \\Q2011\\E", "shortCiteRegEx": "Chen.", "year": 2011}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I. Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I. Jordan", "Gert R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Computational sample complexity", "author": ["Scott Decatur", "Oded Goldreich", "Dana Ron"], "venue": "In Proceedings of the Tenth Annual Conference on Computational Learning Theory,", "citeRegEx": "Decatur et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Decatur et al\\.", "year": 1997}, {"title": "Minimax estimation via wavelet shrinkage", "author": ["David L. Donoho", "Iain M. Johnstone"], "venue": "Ann. Statist., 26(3):879\u2013921,", "citeRegEx": "Donoho and Johnstone.,? \\Q1998\\E", "shortCiteRegEx": "Donoho and Johnstone.", "year": 1998}, {"title": "More data speeds up training time in learning halfspaces over sparse vectors", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Daniely et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2013}, {"title": "Sparse PCA via covariance thresholding", "author": ["Yash Deshpande", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Deshpande and Montanari.,? \\Q2014\\E", "shortCiteRegEx": "Deshpande and Montanari.", "year": 2014}, {"title": "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems", "author": ["Y. Deshpande", "A. Montanari"], "venue": "ArXiv e-prints,", "citeRegEx": "Deshpande and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Deshpande and Montanari.", "year": 2015}, {"title": "De-noising by soft-thresholding", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Donoho.,? \\Q1995\\E", "shortCiteRegEx": "Donoho.", "year": 1995}, {"title": "Sparse CCA: Adaptive Estimation and Computational Barriers", "author": ["C. Gao", "Z. Ma", "H.H. Zhou"], "venue": "ArXiv e-prints,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Complexity of positivstellensatz proofs for the knapsack", "author": ["D. Grigoriev"], "venue": "computational complexity,", "citeRegEx": "Grigoriev.,? \\Q2001\\E", "shortCiteRegEx": "Grigoriev.", "year": 2001}, {"title": "Linear lower bound on degrees of positivstellensatz calculus proofs for the parity", "author": ["Dima Grigoriev"], "venue": "Theoretical Computer Science,", "citeRegEx": "Grigoriev.,? \\Q2001\\E", "shortCiteRegEx": "Grigoriev.", "year": 2001}, {"title": "Sos and planted clique: Tight analysis of MPW moments at all degrees and an optimal lower bound at degree four", "author": ["Samuel B. Hopkins", "Pravesh K. Kothari", "Aaron Potechin"], "venue": "CoRR, abs/1507.05230,", "citeRegEx": "Hopkins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2015}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["Iain M. Johnstone", "Arthur Yu Lu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Johnstone and Lu.,? \\Q2009\\E", "shortCiteRegEx": "Johnstone and Lu.", "year": 2009}, {"title": "Structured sparse principal component analysis", "author": ["Rodolphe Jenatton", "Guillaume Obozinski", "Francis R. Bach"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Jenatton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2010}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["Iain M. Johnstone"], "venue": "Ann. Statist., 29(2):295\u2013327,", "citeRegEx": "Johnstone.,? \\Q2001\\E", "shortCiteRegEx": "Johnstone.", "year": 2001}, {"title": "Function estimation and gaussian sequence models", "author": ["IM Johnstone"], "venue": "Unpublished manuscript,", "citeRegEx": "Johnstone.,? \\Q2002\\E", "shortCiteRegEx": "Johnstone.", "year": 2002}, {"title": "Do semidefinite relaxations solve sparse pca up to the information limit", "author": ["Robert Krauthgamer", "Boaz Nadler", "Dan Vilenchik"], "venue": "The Annals of Statistics,", "citeRegEx": "Krauthgamer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krauthgamer et al\\.", "year": 2015}, {"title": "Anneaux pr\u00e9ordonn\u00e9s", "author": ["Jean-Louis Krivine"], "venue": "Journal d\u2019analyse mathe\u0301matique,", "citeRegEx": "Krivine.,? \\Q1964\\E", "shortCiteRegEx": "Krivine.", "year": 1964}, {"title": "Global optimization with polynomials and the problem of moments", "author": ["Jean B. Lasserre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lasserre.,? \\Q2001\\E", "shortCiteRegEx": "Lasserre.", "year": 2001}, {"title": "An introduction to polynomial and semi-algebraic optimization. Cambridge Texts in Applied Mathematics", "author": ["Jean Bernard Lasserre"], "venue": null, "citeRegEx": "Lasserre.,? \\Q2015\\E", "shortCiteRegEx": "Lasserre.", "year": 2015}, {"title": "Sums of squares, moment matrices and optimization over polynomials", "author": ["Monique Laurent"], "venue": "Emerging Applications of Algebraic Geometry,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Cones of matrices and set-functions and 01 optimization", "author": ["L. Lov\u00e1sz", "A. Schrijver"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lov\u00e1sz and Schrijver.,? \\Q1991\\E", "shortCiteRegEx": "Lov\u00e1sz and Schrijver.", "year": 1991}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Sparse principal component analysis and iterative thresholding", "author": ["Zongming Ma"], "venue": "Ann. Statist., 41(2):772\u2013801,", "citeRegEx": "Ma.,? \\Q2013\\E", "shortCiteRegEx": "Ma.", "year": 2013}, {"title": "Sum-of-squares lower bounds for planted clique", "author": ["Raghu Meka", "Aaron Potechin", "Avi Wigderson"], "venue": "CoRR, abs/1503.06447,", "citeRegEx": "Meka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meka et al\\.", "year": 2015}, {"title": "Squared functional systems and optimization problems", "author": ["Yurii Nesterov"], "venue": "High Performance Optimization,", "citeRegEx": "Nesterov.,? \\Q2000\\E", "shortCiteRegEx": "Nesterov.", "year": 2000}, {"title": "Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization", "author": ["Pablo A. Parrilo"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "Parrilo.,? \\Q2000\\E", "shortCiteRegEx": "Parrilo.", "year": 2000}, {"title": "Augmented sparse principal component analysis for high dimensional data", "author": ["Debashis Paul", "Iain M Johnstone"], "venue": "arXiv preprint arXiv:1202.1242,", "citeRegEx": "Paul and Johnstone.,? \\Q2012\\E", "shortCiteRegEx": "Paul and Johnstone.", "year": 2012}, {"title": "Decoupling inequalities for the tail probabilities of multivariate u-statistics", "author": ["Victor H. de la Pena", "S.J. Montgomery-Smith"], "venue": "The Annals of Probability,", "citeRegEx": "Pena and Montgomery.Smith.,? \\Q1995\\E", "shortCiteRegEx": "Pena and Montgomery.Smith.", "year": 1995}, {"title": "Positive polynomials on compact semi-algebraic sets", "author": ["Mihai Putinar"], "venue": "Indiana University Mathematics Journal,", "citeRegEx": "Putinar.,? \\Q1993\\E", "shortCiteRegEx": "Putinar.", "year": 1993}, {"title": "Tight lower bounds for planted clique in the degree-4", "author": ["Prasad Raghavendra", "Tselil Schramm"], "venue": "SOS program. CoRR,", "citeRegEx": "Raghavendra and Schramm.,? \\Q2015\\E", "shortCiteRegEx": "Raghavendra and Schramm.", "year": 2015}, {"title": "A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems", "author": ["Hanif D. Sherali", "Warren P. Adams"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Sherali and Adams.,? \\Q1990\\E", "shortCiteRegEx": "Sherali and Adams.", "year": 1990}, {"title": "Linear level lasserre lower bounds for certain k-csps", "author": ["Grant Schoenebeck"], "venue": "In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Schoenebeck.,? \\Q2008\\E", "shortCiteRegEx": "Schoenebeck.", "year": 2008}, {"title": "Computational sample complexity and attribute-efficient learning", "author": ["Rocco A. Servedio"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Servedio.,? \\Q2000\\E", "shortCiteRegEx": "Servedio.", "year": 2000}, {"title": "An approach to obtaining global extremums in polynomial mathematical programming problems", "author": ["N.Z. Shor"], "venue": null, "citeRegEx": "Shor.,? \\Q1987\\E", "shortCiteRegEx": "Shor.", "year": 1987}, {"title": "Weak Convergence and Empirical Processes: With Applications to Statistics (Springer Series in Statistics)", "author": ["Aad van der Vaart", "Jon Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner.,? \\Q2000\\E", "shortCiteRegEx": "Vaart and Wellner.", "year": 2000}, {"title": "Minimax rates of estimation for sparse PCA in high dimensions", "author": ["Vincent Q. Vu", "Jing Lei"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Vu and Lei.,? \\Q2012\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2012}, {"title": "Minimax sparse principal subspace estimation in high dimensions", "author": ["Vincent Q. Vu", "Jing Lei"], "venue": "Ann. Statist., 41(6):2905\u20132947,", "citeRegEx": "Vu and Lei.,? \\Q2013\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2013}, {"title": "Statistical Limits of Convex Relaxations", "author": ["Z. Wang", "Q. Gu", "H. Liu"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Tighten after relax: Minimax-optimal sparse PCA in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Yuan and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "This paper establishes a statistical versus computational trade-off for solving a basic highdimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the Sparse Principal Component Analysis (Sparse PCA) problem, and the family of Sum-of-Squares (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension p, a planted k-sparse unit vector can be in principle detected using only n \u2248 k log p (Gaussian or Bernoulli) samples, but all efficient (polynomial time) algorithms known require n \u2248 k samples. It was also known that this quadratic gap cannot be improved by the the most basic semi-definite (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This averagecase lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or \u201cpseudoexpectations\u201d) for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.", "creator": "LaTeX with hyperref package"}}}