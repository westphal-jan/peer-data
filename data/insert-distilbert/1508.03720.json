{"id": "1508.03720", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2015", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path", "abstract": "relation classification is an important research arena primarily in expanding the field of natural language processing ( nlp ). in this paper, structurally we present sdp - lstm, a novel neural network to classify the relation of two entities in a sentence. therefore our neural architecture now leverages balancing the shortest dependency path ( sdp ) between two entities ; multichannel recurrent neural networks, with long short term mutual memory ( lstm ) units, pick up mutually heterogeneous information along the sdp. our proposed model layout has several distinct features : ( 1 ) setting the shortest dependency paths retain most crucial relevant information ( to relation classification ), while eliminating irrelevant words in the sentence. ( 2 ) the multichannel lstm networks help allow effective information using integration from heterogeneous sources over the dependency paths. ( 3 ) a customized dropout strategy matrix regularizes the neural reference network to alleviate overfitting. visually we test externally our model on the semeval 2010 relation classification task, and achieve an $ 35 f _ 1 $ - score of 83. 71 7 \\ %, higher efficiency than similar competing methods in the literature.", "histories": [["v1", "Sat, 15 Aug 2015 11:15:32 GMT  (104kb,D)", "http://arxiv.org/abs/1508.03720v1", "EMNLP '15"]], "COMMENTS": "EMNLP '15", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xu yan", "lili mou", "ge li", "yunchuan chen", "hao peng", "zhi jin"], "accepted": true, "id": "1508.03720"}, "pdf": {"name": "1508.03720.pdf", "metadata": {"source": "CRF", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths", "authors": ["Yan Xu", "Lili Mou", "Ge Li", "\u2020\u2217Yunchuan", "Hao Peng", "Zhi Jin"], "emails": ["chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Relation classification is an important NLP task. It plays a key role in various scenarios, e.g., information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence \u201cA trillion gallons of [water]e1 have been poured into an empty [region]e2 of outer\n\u2217Corresponding authors.\nspace,\u201d the entities water and region are of relation Entity-Destination(e1, e2).\nTraditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; Santos et al., 2015). However, human engineering\u2014that is, incorporating human knowledge to the network\u2019s architecture\u2014is still important and beneficial.\nThis paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the following observations.\n\u2022 Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014). To determine the two entities\u2019 relation, we find it mostly sufficient to use only the words along the SDP: they concentrate on most relevant information while diminishing less relevant noise. Figure 1 depicts the dependency parse tree of the aforementioned sentence. Words along the SDP form a trimmed phrase (gallons of water poured into region) of the original sentence, which conveys much information about the target relation. Other words, such as a, trillion, outer space, are less informative and may bring noise if not dealt with properly. ar X\niv :1\n50 8.\n03 72\n0v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 5\n\u2022 Direction matters. Dependency trees are a kind of directed graph. The dependency relation between into and region is PREP; such relation hardly makes any sense if the directed edge is reversed. Moreover, the entities\u2019 relation distinguishes its directionality, that is, r(a, b) differs from r(b, a), for a same given relation r and two entities a, b. Therefore, we think it necessary to let the neural model process information in a directionsensitive manner. Out of this consideration, we separate an SDP into two sub-paths, each from an entity to the common ancestor node. The extracted features along the two subpaths are concatenated to make final classification.\n\u2022 Linguistic information helps. For example, with prior knowledge of hyponymy, we know \u201cwater is a kind of substance.\u201d This is a hint that the entities, water and region, are more of Entity-Destination relation than, say, Communication-Topic. To gather heterogeneous information along SDP, we design a multichannel recurrent neural network. It makes use of information from various sources, including words themselves, POS tags, WordNet hypernyms, and the grammatical relations between governing words and their children.\nFor effective information propagation and integration, our model leverages LSTM units during recurrent propagation. We also customize a new dropout strategy for our SDP-LSTM network to alleviate the problem of overfitting. To the best of our knowledge, we are the first to use LSTMbased recurrent neural networks for the relation classification task.\nWe evaluate our proposed method on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature.\nIn the rest of this paper, we review related work in Section 2. In Section 3, we describe our SDPLSTM model in detail. Section 4 presents quantitative experimental results. Finally, we have our conclusion in Section 5."}, {"heading": "2 Related Work", "text": "Relation classification is a widely studied task in the NLP community. Various existing meth-\nods mainly fall into three classes: feature-based, kernel-based, and neural network-based.\nIn feature-based approaches, different sets of features are extracted and fed to a chosen classifier (e.g., logistic regression). Generally, three types of features are often used. Lexical features concentrate on the entities of interest, e.g., entities per se, entity POS, entity neighboring information. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class, entity mention. Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (Zhou et al., 2005).\nKernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can bene-\nfit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial.\nDeep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences\u2019 parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8.\nIn addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc."}, {"heading": "3 The Proposed SDP-LSTM Model", "text": "In this section, we describe our SDP-LSTM model in detail. Subsection 3.1 delineates the overall architecture of our model. Subsection 3.2 presents the rationale of using SDPs. Four different information channels along the SDP are explained in Subsection 3.3. Subsection 3.4 introduces the recurrent neural network with long short term memory, which is built upon the dependency path. Subsection 3.5 customizes a dropout strategy for our network to alleviate overfitting. We finally present our training objective in Subsection 3.6."}, {"heading": "3.1 Overview", "text": "Figure 2 depicts the overall architecture of our SDP-LSTM network.\nFirst, a sentence is parsed to a dependency tree\nby the Stanford parser;1 the shortest dependency path (SDP) is extracted as the input of our network. Along the SDP, four different types of information\u2014referred to as channels\u2014are used, including the words, POS tags, grammatical relations, and WordNet hypernyms. (See Figure 2a.) In each channel, discrete inputs, e.g., words, are mapped to real-valued vectors, called embeddings, which capture the underlying meanings of the inputs.\nTwo recurrent neural networks (Figure 2b) pick up information along the left and right sub-paths of the SDP, respecitvely. (The path is separated by the common ancestor node of two entities.) Long short term memory (LSTM) units are used in the recurrent networks for effective information propagation. A max pooling layer thereafter gathers information from LSTM nodes in each path.\nThe pooling layers from different channels are concatenated, and then connected to a hidden layer. Finally, we have a softmax output layer for classification. (See again Figure 2a.)"}, {"heading": "3.2 The Shortest Dependency Path", "text": "The dependency parse tree is naturally suitable for relation classification because it focuses on the action and agents in a sentence (Socher et al., 2014). Moreover, the shortest path between entities, as discussed in Section 1, condenses most illuminating information for entities\u2019 relation.\nWe also observe that the sub-paths, separated by the common ancestor node of two entities, provide strong hints for the relation\u2019s directionality. Take Figure 1 as an example. Two entities water and region have their common ancestor node, poured, which separates the SDP into two parts:\n[water]e1 \u2192 of\u2192 gallons\u2192 poured\nand poured\u2190 into\u2190 [region]e2\nThe first sub-path captures information of e1, whereas the second sub-path is mainly about e2. By examining the two sub-paths separately, we know e1 and e2 are of relation Entity-Destination(e1, e2), rather than Entity-Destination(e2, e1).\nFollowing the above intuition, we design two recurrent neural networks, which propagate\n1 http://nlp.stanford.edu/software/lex-parser.shtml\nbottom-up from the entities to their common ancestor. In this way, our model is directionsensitive."}, {"heading": "3.3 Channels", "text": "We make use of four types of information along the SDP for relation classification. We call them channels as these information sources do not interact during recurrent propagation. Detailed channel descriptions are as follows.\n\u2022 Word representations. Each word in a given sentence is mapped to a real-valued vector by looking up in a word embedding table. Unsupervisedly trained on a large corpus, word embeddings are thought to be able to well capture words\u2019 syntactic and semantic information (Mikolov et al., 2013b).\n\u2022 Part-of-speech tags. Since word embeddings are obtained on a generic corpus of a large scale, the information they contain may not agree with a specific sentence. We deal with this problem by allying each input word with its POS tag, e.g., noun, verb, etc. In our experiment, we only take into use a coarse-grained POS category, containing 15 different tags.\n\u2022 Grammatical relations. The dependency relations between a governing word and its children makes a difference in meaning. A same word pair may have different dependency relation types. For example, \u201cbeats nsubj\u2212\u2212\u2212\u2192 it\u201d is distinct from \u201cbeats dobj\u2212\u2212\u2212\u2192 it.\u201d Thus, it is necessary to capture such gram-\nmatical relations in SDPs. In our experiment, grammatical relations are grouped into 19 classes, mainly based on a coarse-grained classification (De Marneffe et al., 2006).\n\u2022 WordNet hypernyms. As illustrated in Section 1, hyponymy information is also useful for relation classification. (Details are not repeated here.) To leverage WordNet hypernyms, we use a tool developed by Ciaramita and Altun (2006).2 The tool assigns a hypernym to each word, from 41 predefined concepts in WordNet, e.g., noun.food, verb.motion, etc. Given its hypernym, each word gains a more abstract concept, which helps to build a linkage between different but conceptual similar words.\nAs we can see, POS tags, grammatical relations, and WordNet hypernyms are also discrete (like words per se). However, no prevailing embedding learning method exists for POS tags, say. Hence, we randomly initialize their embeddings, and tune them in a supervised fashion during training. We notice that these information sources contain much fewer symbols, 15, 19, and 41, than the vocabulary size (greater than 25,000). Hence, we believe our strategy of random initialization is feasible, because they can be adequately tuned during supervised training."}, {"heading": "3.4 Recurrent Neural Network with Long Short Term Memory Units", "text": "The recurrent neural network is suitable for modeling sequential data by nature, as it keeps a hid-\n2http://sourceforge.net/projects/supersensetag\nct\n~\n~\nht\ng i\n~\no ~ xt\nf\nt-1c\nht-1\nxt\nht-1\nht-1xt\n~\nht-1xt\nFigure 3: A long short term memory unit. h: hidden unit. c: memory cell. i: input gate. f : forget gate. o: output gate. g: candidate cell. \u2297: element-wise multiplication. \u223c: activation function.\nden state vector h, which changes with input data at each step accordingly. We use the recurrent network to gather information along each sub-path in the SDP (Figure 2b).\nThe hidden state ht, for the t-th word in the sub-path, is a function of its previous state ht\u22121 and the current word xt. Traditional recurrent networks have a basic interaction, that is, the input is linearly transformed by a weight matrix and nonlinearly squashed by an activation function. Formally, we have\nht = f(Winxt +Wrecht\u22121 + bh)\nwhere Win and Wrec are weight matrices for the input and recurrent connections, respectively. bh is a bias term for the hidden state vector, and fh a non-linear activation function (e.g., tanh).\nOne problem of the above model is known as gradient vanishing or exploding. The training of neural networks requires gradient backpropagation. If the propagation sequence (path) is too long, the gradient may probably either grow, or decay, exponentially, depending on the magnitude of Wrec. This leads to the difficulty of training.\nLong short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant\nintroduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2014).\nConcretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1\u20136 as bellow).\nThe three adaptive gates it, ft, and ot depend on the previous state ht\u22121 and the current input xt (Equations 1\u20133). An extracted feature vector gt is also computed, by Equation 4, serving as the candidate memory cell.\nit = \u03c3(Wi \u00b7xt + Ui \u00b7ht\u22121 + bi) (1) ft = \u03c3(Wf \u00b7xt + Uf \u00b7ht\u22121 + bf ) (2) ot = \u03c3(Wo \u00b7xt + Uo \u00b7ht\u22121 + bo) (3) gt = tanh(Wg \u00b7xt + Ug \u00b7ht\u22121 + bg) (4)\nThe current memory cell ct is a combination of the previous cell content ct\u22121 and the candidate content gt, weighted by the input gate it and forget gate ft, respectively. (See Equation 5 below.)\nct = it \u2297 gt + ft \u2297 ct\u22121 (5)\nThe output of LSTM units is the the recurrent network\u2019s hidden state, which is computed by Equation 6 as follows.\nht = ot \u2297 tanh(ct) (6)\nIn the above equations, \u03c3 denotes a sigmoid function; \u2297 denotes element-wise multiplication."}, {"heading": "3.5 Dropout Strategies", "text": "A good regularization approach is needed to alleviate overfitting. Dropout, proposed recently by Hinton et al. (2012), has been very successful on feed-forward networks. By randomly omitting feature detectors from the network during training, it can obtain less interdependent network units and achieve better performance. However, the conventional dropout does not work well with recurrent neural networks with LSTM units, since dropout may hurt the valuable memorization ability of memory units.\nAs there is no consensus on how to drop out LSTM units in the literature, we try several dropout strategies for our SDP-LSTM network:\n\u2022 Dropout embeddings;\n\u2022 Dropout inner cells in memory units, including it, gt, ot, ct, and ht; and\n\u2022 Dropout the penultimate layer.\nAs we shall see in Section 4.2, dropping out LSTM units turns out to be inimical to our model, whereas the other two strategies boost in performance.\nThe following equations formalize the dropout operations on the embedding layers, where D denotes the dropout operator. Each dimension in the embedding vector, xt, is set to zero with a predefined dropout rate.\nit = \u03c3(Wi \u00b7D(xt) + Ui \u00b7ht\u22121 + bi) (7) ft = \u03c3(Wf \u00b7D(xt) + Uf \u00b7ht\u22121 + bf ) (8) ot = \u03c3(Wo \u00b7D(xt) + Uo \u00b7ht\u22121 + bo) (9)\ngt = tanh ( Wg \u00b7D(xt) + Ug \u00b7ht\u22121 + bg ) (10)"}, {"heading": "3.6 Training Objective", "text": "The SDP-LSTM described above propagates information along a sub-path from an entity to the common ancestor node (of the two entities). A max pooling layer packs, for each sub-path, the recurrent network\u2019s states, h\u2019s, to a fixed vector by taking the maximum value in each dimension.\nSuch architecture applies to all channels, namely, words, POS tags, grammatical relations, and WordNet hypernyms. The pooling vectors in these channels are concatenated, and fed to a fully connected hidden layer. Finally, we add a softmax output layer for classification. The training objective is the penalized cross-entropy error, given by J = \u2212 nc\u2211 i=1 ti log yi+\u03bb ( \u03c9\u2211 i=1 \u2016Wi\u20162F + \u03c5\u2211 i=1 \u2016Ui\u20162F )\nwhere t \u2208 Rnc is the one-hot represented ground truth and y \u2208 Rnc is the estimated probability for each class by softmax. (nc is the number of target classes.) \u2016 \u00b7 \u2016F denotes the Frobenius norm of a matrix; \u03c9 and \u03c5 are the numbers of weight matrices (for W \u2019s and U \u2019s, respectively). \u03bb is a hyperparameter that specifies the magnitude of penalty on weights. Note that we do not add `2 penalty to biase parameters.\nWe pretrained word embeddings by word2vec (Mikolov et al., 2013a) on the English Wikipedia corpus; other parameters are initialized randomly. We apply stochastic gradient descent (with minibatch 10) for optimization; gradients are computed by standard back-propagation. Training details are further introduced in Section 4.2."}, {"heading": "4 Experiments", "text": "In this section, we present our experiments in detail. Our implementation is built upon Mou et al. (2015). Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings. In Section 4.3, we compare SDP-LSTM\u2019s performance with other methods in the literature. We also analyze the effect of different channels in Section 4.4."}, {"heading": "4.1 Dataset", "text": "The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification (Hendrickx et al., 2010). The dataset contains 8,000 sentences for training, and 2,717 for testing. We split 1/10 samples out of the training set for validation.\nThe target contains 19 labels: 9 directed relations, and an undirected Other class. The directed relations are list as below. \u2022 Cause-Effect \u2022 Component-Whole \u2022 Content-Container \u2022 Entity-Destination \u2022 Entity-Origin \u2022 Message-Topic \u2022 Member-Collection \u2022 Instrument-Agency \u2022 Product-Producer In the following are illustrated two sample sentences with directed relations.\n[People]e1 have been moving back into [downtown]e2 .\nFinancial [stress]e1 is one of the main causes of [divorce]e2 .\nThe target labels are Entity-Destination (e1, e2), and Cause-Effect(e1, e2), respectively.\nThe dataset also contains an undirected Other class. Hence, there are 19 target labels in total. The undirected Other class takes in entities that do not fit into the above categories, illustrated by the following example.\nA misty [ridge]e1 uprises from the [surge]e2 .\nWe use the official macro-averaged F1-score to evaluate model performance. This official measurement excludes the Other relation. Nonetheless, we have no special treatment of Other class in our experiments, which is typical in other studies."}, {"heading": "4.2 Hyperparameters and Training Details", "text": "This subsection presents hyperparameter tuning for our model. We set word-embeddings to be 200-dimensional; POS, WordNet hyponymy, and grammatical relation embeddings are 50- dimensional. Each channel of the LSTM network contains the same number of units as its source embeddings (either 200 or 50). The penultimate hidden layer is 100-dimensional. As it is not feasible to perform full grid search for all hyperparameters, the above values are chosen empirically.\nWe add `2 penalty for weights with coefficient 10\u22125, which was chosen by validation from the set {10\u22122, 10\u22123, \u00b7 \u00b7 \u00b7 , 10\u22127}.\nWe thereafter validate the proposed dropout strategies in Section 3.5. Since network units in different channels do not interact with each other during information propagation, we herein take one channel of LSTM networks to assess the efficacy. Taking the word channel as an example, we first drop out word embeddings. Then with a fixed dropout rate of word embeddings, we test the effect of dropping out LSTM inner cells and the penultimate units, respectively.\nWe find that, dropout of LSTM units hurts the model, even if the dropout rate is small, 0.1, say (Figure 4b). Dropout of embeddings improves model performance by 2.16% (Figure 4a); dropout of the penultimate layer further improves by 0.16% (Figure 4c). This analysis also provides, for other studies, some clues for dropout in LSTM networks."}, {"heading": "4.3 Results", "text": "Table 4 compares our SDT-LSTM with other stateof-the-art methods. The first entry in the ta-\nble presents the highest performance achieved by traditional feature engineering. Hendrickx et al. (2010) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.2%.\nNeural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification. They extend the basic RNN with matrix-vector interaction and achieve an F1score of 82.4%.\nZeng et al. (2014) treat a sentence as sequential data and exploit the convolutional neural network (CNN); they also integrate word position information into their model. Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure. In this way, they achieve the state-of-the-art result with the F1score of 84.1%. Without such special treatment, their F1-score is 82.7%.\nYu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM) for relation classification, which combines unlexicalized linguistic contexts and word embeddings. They achieve an F1-score of 83.0%.\nOur proposed SDT-LSTM model yields an F1score of 83.7%. It outperforms existing competing approaches, in a fair condition of softmax with cross-entropy error.\nIt is worth to note that we have also conducted two controlled experiments: (1) Traditional RNN without LSTM units, achieving an F1-score of 82.8%; (2) LSTM network over the entire dependency path (instead of two sub-paths), achieving\nan F1-score of 82.2%. These results demonstrate the effectiveness of LSTM and directionality in relation classification."}, {"heading": "4.4 Effect of Different Channels", "text": "This subsection analyzes how different channels affect our model. We first used word embeddings only as a baseline; then we added POS tags, grammatical relations, and WordNet hypernyms, respectively; we also combined all these channels into our models. Note that we did not try the latter three channels alone, because each single of them (e.g., POS) does not carry much information.\nWe see from Table 2 that word embeddings alone in SDP-LSTM yield a remarkable performance of 82.35%, compared with CNNs 69.7%, RNNs 74.9\u201379.1%, and FCM 80.6%.\nAdding either grammatical relations or WordNet hypernyms outperforms other existing methods (data cleaning not considered here). POS tagging is comparatively less informative, but still boosts the F1-score by 0.63%.\nWe notice that, the boosts are not simply added when channels are combined. This suggests that these information sources are complementary to each other in some linguistic aspects. Nonetheless, incorporating all four channels further pushes the F1-score to 83.70%."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel neural network model, named SDP-LSTM, for relation classification. It learns features for relation classification iteratively along the shortest dependency path. Several types of information (word themselves, POS tags, grammatical relations and WordNet hypernyms) along the path are used. Meanwhile, we leverage LSTM units for long-range information propagation and integration. We demonstrate the effectiveness of SDP-LSTM by evaluating the model on SemEval-2010 relation classification task, outperforming existing state-of-art methods (in a fair condition without data cleaning). Our result sheds some light in the relation classification task as follows. \u2022 The shortest dependency path can be a valu-\nable resource for relation classification, covering mostly sufficient information of target\nrelations. \u2022 Classifying relation is a challenging task due\nto the inherent ambiguity of natural languages and the diversity of sentence expression. Thus, integrating heterogeneous linguistic knowledge is beneficial to the task. \u2022 Treating the shortest dependency path as two\nsub-paths, mapping two different neural networks, helps to capture the directionality of relations. \u2022 LSTM units are effective in feature detec-\ntion and propagation along the shortest dependency path."}, {"heading": "Acknowledgments", "text": "This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301."}], "references": [{"title": "Open information extraction for the web", "author": ["Banko et al.2007] M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio et al.2013] Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005] R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] R. Bunescu", "R. Mooney"], "venue": "In Annual meeting-association for Computational Linguistics,", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding", "author": ["Chen et al.2014] Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokan Tur"], "venue": "In Spoken Language Technology Workshop", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Altun2006] M. Ciaramita", "Y. Altun"], "venue": "In Proceedings of the", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["B. MacCartney", "C.D. Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Chain based rnn for relation classification", "author": ["Ebrahimi", "Dou2015] J. Ebrahimi", "D. Dou"], "venue": "In HLT-NAACL", "citeRegEx": "Ebrahimi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ebrahimi et al\\.", "year": 2015}, {"title": "RelEx\u2014Relation extraction using dependency parse", "author": ["Fundel et al.2007] K. Fundel", "R. K\u00fcffner", "R. Zimmer"], "venue": "trees. Bioinformatics,", "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["M. Miwa", "Y. Tsuruoka", "T. Chikayama"], "venue": "In EMNLP,", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["S.N. Kim"], "venue": "Kozareva", "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al.2012] G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107\u2013116", "author": ["S. Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["N. Kambhatla"], "venue": "In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions,", "citeRegEx": "Kambhatla.,? \\Q2004\\E", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013a] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov et al.2013b] T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "In HLT-NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] L. Mou", "H. Peng", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "arXiv preprint arXiv:1504.01106", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] B. Plank", "A. Moschitti"], "venue": "ACL", "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["B. Xiang", "B. Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher et al.2011] R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher et al.2012] R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207\u2013218", "author": ["Socher et al.2014] R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Medical relation extraction with manifold models", "author": ["Wang", "Fan2014] C. Wang", "J. Fan"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A re-examination of dependency path kernels for relation extraction", "author": ["M. Wang"], "venue": "In IJCNLP,", "citeRegEx": "Wang.,? \\Q2008\\E", "shortCiteRegEx": "Wang.", "year": 2008}, {"title": "Open information extraction using wikipedia", "author": ["Wu", "Weld2010] F. Wu", "D.S. Weld"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Learning non-taxonomic relations on demand for ontology extension", "author": ["Y. Xu", "G. Li", "L. Mou", "Y. Lu"], "venue": "International Journal of Software Engineering and Knowledge Engineering,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Van Durme2014] X. Yao", "B. Van Durme"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Factor-based compositional embedding models", "author": ["M. Yu", "M.R. Gormley", "M. Dredze"], "venue": "In The NIPS 2014 Learning Semantics Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] W. Zaremba", "I. Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Kernel methods for relation extraction", "author": ["Zelenko et al.2003] D. Zelenko", "C. Aone", "A. Richardella"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] G.D. Zhou", "J. Su", "J. Zhang", "M. Zhang"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Long short-term memory over tree structures", "author": ["X. Zhu", "P. Sobhani", "H. Guo"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": ", information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc.", "startOffset": 152, "endOffset": 169}, {"referenceID": 13, "context": "Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 30, "context": "Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005).", "startOffset": 122, "endOffset": 170}, {"referenceID": 1, "context": "Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 31, "context": ", 2013), and have exhibited considerable potential (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 51, "endOffset": 91}, {"referenceID": 19, "context": ", 2013), and have exhibited considerable potential (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 51, "endOffset": 91}, {"referenceID": 8, "context": "\u2022 Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014).", "startOffset": 44, "endOffset": 84}, {"referenceID": 4, "context": "\u2022 Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014).", "startOffset": 44, "endOffset": 84}, {"referenceID": 32, "context": ", hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (Zhou et al., 2005).", "startOffset": 94, "endOffset": 113}, {"referenceID": 13, "context": "Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification.", "startOffset": 0, "endOffset": 17}, {"referenceID": 29, "context": "Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees.", "startOffset": 0, "endOffset": 22}, {"referenceID": 29, "context": "Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification.", "startOffset": 0, "endOffset": 120}, {"referenceID": 24, "context": "Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can bene-", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "(2011) propose a recursive neural network (RNN) along sentences\u2019 parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012).", "startOffset": 151, "endOffset": 172}, {"referenceID": 18, "context": "Socher et al. (2011) propose a recursive neural network (RNN) along sentences\u2019 parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities.", "startOffset": 0, "endOffset": 118}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences.", "startOffset": 0, "endOffset": 204}, {"referenceID": 9, "context": "Hashimoto et al. (2013) explicitly weight phrases\u2019 importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8.", "startOffset": 0, "endOffset": 323}, {"referenceID": 0, "context": "In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al.", "startOffset": 211, "endOffset": 257}, {"referenceID": 16, "context": ", 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc.", "startOffset": 93, "endOffset": 113}, {"referenceID": 22, "context": "The dependency parse tree is naturally suitable for relation classification because it focuses on the action and agents in a sentence (Socher et al., 2014).", "startOffset": 134, "endOffset": 155}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al.", "startOffset": 52, "endOffset": 421}, {"referenceID": 12, "context": "Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2014). Concretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1\u20136 as bellow).", "startOffset": 52, "endOffset": 453}, {"referenceID": 11, "context": "Dropout, proposed recently by Hinton et al. (2012), has been very successful on feed-forward networks.", "startOffset": 30, "endOffset": 51}, {"referenceID": 17, "context": "Our implementation is built upon Mou et al. (2015). Section 4.", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": "The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification (Hendrickx et al., 2010).", "startOffset": 87, "endOffset": 111}, {"referenceID": 10, "context": "Hendrickx et al. (2010) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "Neural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification.", "startOffset": 47, "endOffset": 68}, {"referenceID": 19, "context": "Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure.", "startOffset": 0, "endOffset": 21}], "year": 2015, "abstractText": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature.", "creator": "LaTeX with hyperref package"}}}