{"id": "1508.03721", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2015", "title": "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks", "abstract": "this paper aims to compare different regularization strategies to address specifically a common prediction phenomenon, severe overfitting, in embedding - based neural networks for nlp. we chose two widely studied functional neural code models and tasks as our testbed. we tried several frequently applied or newly proposed regularization modelling strategies, primarily including penalizing weights ( embeddings excluded ), regression penalizing embeddings, re - embedding words, and dropout. we also emphasized on incremental inverse hyperparameter tuning, and combining different regularizations. the same results consequently provide a closer picture on tuning of hyperparameters for neural nlp models.", "histories": [["v1", "Sat, 15 Aug 2015 11:16:39 GMT  (113kb,D)", "http://arxiv.org/abs/1508.03721v1", "EMNLP '15"]], "COMMENTS": "EMNLP '15", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["hao peng", "lili mou", "ge li", "yunchuan chen", "yangyang lu", "zhi jin"], "accepted": true, "id": "1508.03721"}, "pdf": {"name": "1508.03721.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks", "authors": ["Hao Peng", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "emails": ["doublepower.mou}@gmail.com,{lige,", "zhijin}@sei.pku.edu.cn", "chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": null, "text": "This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models."}, {"heading": "1 Introduction", "text": "Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters.\n\u2217Equal contribution. \u2020Corresponding author.\nA curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from those in other scenarios like image recognition. Further, selecting hyperparameters to pursue the best performance by validation is extremely timeconsuming, as suggested in Collobert et al. (2011). Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing. Specifically, we focus on the following research questions in this paper. RQ 1: How do different regularization strategies\ntypically behave in embedding-based neural networks?\nRQ 2: Can regularization coefficients be tuned incrementally during training so as to ease the burden of hyperparameter tuning? RQ 3: What is the effect of combining different regularization strategies?\nIn this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivastava et al., 2014). We analyzed these regularization methods by two widely studied models and tasks. We also emphasized on incremental hyperparameter tuning and the combination of different regularization methods.\nOur experiments provide some interesting results: (1) Regularizations do help generalization, but their effect depends largely on the datasets\u2019 size. (2) Penalizing `2-norm of embeddings helps optimization as well, improving training accuracy unexpectedly. (3) Incremental hyperparameter tuning achieves similar performance, indicat-\nar X\niv :1\n50 8.\n03 72\n1v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 5\ning that regularizations mainly serve as a \u201clocal\u201d effect. (4) Dropout performs slightly worse than `2 penalty in our experiments; however, provided very small `2 penalty, dropping out hidden units and penalizing `2-norm are generally complementary. (5) The newly proposed re-embedding words method is not effective in our experiments."}, {"heading": "2 Tasks, Models, and Setup", "text": "Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8.1 The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other.\nRegarding the neural model, we applied Collobert\u2019s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer.\nExperiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral.\nWe used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children\u2019s information to their parent\u2019s; the root vector is finally used for sentiment classification.\nExperimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due\n1 http://www.aclweb.org/anthology/S10-1006 2 http://nlp.stanford.edu/sentiment/\nto the recursive, and thus chaotic nature). Therefore, we applied power decay (Senior et al., 2013) with power equal to \u22121. For each strategy, we tried a large range of regularization coefficients, 10\u22129, \u00b7 \u00b7 \u00b7 , 10\u22122, extensively from underfitting to no effect with granularity 10x. We ran the model 5 times with different initializations. We used mini-batch stochastic gradient descent; gradients are computed by standard backpropagation. For source code, please refer to our project website.3\nIt needs to be noticed that, the goal of this paper is not to outperform or reproduce state-of-the-art results. Instead, we would like to have a fair comparison. The testbed of our work is two widely studied models and tasks, which were not chosen on purpose. During the experiments, we tried to make the comparison as fair as possible. Therefore, we think that the results of this work can be generalized to similar scenarios."}, {"heading": "3 Regularization Strategies", "text": "In this section, we describe four regularization strategies used in our experiment. \u2022 Penalizing `2-norm of weights. Let E be the\ncross-entropy error for classification, and R be a regularization term. The overall cost function is J = E + \u03bbR, where \u03bb is the coefficient. In this case, R = \u2016W\u20162, and the coefficient is denoted as \u03bbW . \u2022 Penalizing `2-norm of embeddings. Some\nstudies do not distinguish embeddings or connectional weights for regularization (Tai et al., 2015). However, we would like to analyze their effect separately, for embeddings are sparse in use. Let \u03a6 denote embeddings; then we have R = \u2016\u03a6\u20162. \u2022 Re-embedding words (Labutov and Lipson,\n2013). Suppose \u03a60 denotes the original embeddings trained on a large corpus, and \u03a6 denotes the embeddings fine-tuned during supervised training. We would like to penalize the norm of the difference between \u03a60 and \u03a6, i.e.,R = \u2016\u03a60\u2212\u03a6\u20162. In the limit of penalty to infinity, the model is mathematically equivalent to \u201cfrozen embeddings,\u201d where word vectors are used as surface features. \u2022 Dropout (Srivastava et al., 2014). In this\nstrategy, each neural node is set to 0 with a predefined dropout probability p during training; when testing, all nodes are used, with activation multiplied by 1\u2212 p.\n3 https://sites.google.com/site/regembeddingnn/"}, {"heading": "4 Individual Regularization Behaviors", "text": "This section compares the behavior of each strategy. We first conducted both experiments without regularization, achieving accuracies of 54.02\u00b1 0.84%, 41.47\u00b12.85%, respectively. Then we plot in Figure 1 learning curves when each regularization strategy is applied individually. We report training and validation accuracies through out this paper. The main findings are as follows. \u2022 Penalizing `2-norm of weights helps gener-\nalization; the effect depends largely on the size of training set. Experiment I contains 7,000 training samples and the improvement is 6.98%; Experiment II contains more than 150k samples, and the improvement is only 2.07%. Such results are consistent with other machine learning models.\n\u2022 Penalizing `2-norm of embeddings unexpectedly helps optimization (improves training accuracy). One plausible explanation is that since embeddings are trained on a large corpus by unsupervised methods, they tend to settle down to large values and may not perfectly agree with the tasks of interest. `2 penalty pushes the embeddings towards small values and thus helps optimization. Regarding validation accuracy, Experiment I is improved by 6.89%, whereas Experiment II has no significant difference.\n\u2022 Re-embedding words does not improve generalization. Particularly, in Experiment II, the ultimate accuracy is improved by 0.44, which is not large. Further, too much penalty hurts the models in both experiments. In the limit \u03bbreembed to infinity, re-embedding words is mathematically equivalent to using embeddings as surface features, that is, freezing embeddings. Such strategy is sometimes applied in the literature like Hu et al. (2014), but is not favorable as suggested by the experiment.\n\u2022 Dropout helps generalization. Under the best settings, the eventual accuracy is improved by 3.12% and 1.76%, respectively. In our experiments, dropout alone is not as useful as `2 penalty. However, other studies report that dropout is very effective (Irsoy and Cardie, 2014). Our results are not consistent; different dimensionality may contribute to this disagreement, but more experiments are needed to confirm the hypothesis."}, {"heading": "5 Incremental Hyperparameter Tuning", "text": "The above experiments show that regularization generally helps prevent overfitting. To pursue the best performance, we need to try out different hyperparameters through validation. Unfortunately, training deep neural networks is time-consuming, preventing full grid search from being a practical technique. Things will get easier if we can incrementally tune hyperparameters, that is, to train the model without regularization first, and then add penalty.\nIn this section, we study whether `2 penalty of weights and embeddings can be tuned incrementally. We exclude the dropout strategy because its does not make much sense to incrementally drop out hidden units. Besides, from this section, we only focus on Experiment I due to time and space limit.\nBefore continuing, we may envision several possibilities on how regularization works. \u2022 (On initial effects) As `2-norm prevents pa-\nrameters from growing large, adding it at early stages may cause parameters settling down to local optima. If this is the case, delayed penalty would help parameters get over local optima, leading to better performance. \u2022 (On eventual effects) `2 penalty lifts er-\nror surface of large weights. Adding such penalty may cause parameters settling down to (a) almost the same catchment basin, or (b) different basins. In case (a), when the penalty is added does not matter much. In case (b), however, it makes difference, because parameters would have already gravitated to catchment basins of larger values before regularization is added, which means incremental hyperparameter tuning would be ineffective.\nTo verify the above conjectures, we design four settings: adding penalty (1) at the beginning, (2) before overfitting at epoch 2, (3) at peak performance (epoch 5), and (4) after overfitting (validation accuracy drops) at epoch 10.\nFigure 2 plots the learning curves regarding penalizing weights and embeddings, respectively; baseline (without regularization) is also included.\nFor both weights and embeddings, all settings yield similar ultimate validation accuracies. This shows `2 regularization mainly serves as a \u201clocal\u201d effect\u2014it changes the error surface, but parameters tend to settle down to a same catchment basin. We notice a recent report also shows local optima\nmay not play an important role in training neural networks, if the effect of parameter symmetry is ruled out (Breuel, 2015).\nWe also observe that regularization helps generalization as soon as it is added (Figure 2a), and that regularizing embeddings helps optimization also right after the penalty is applied (Figure 2b)."}, {"heading": "6 Combination of Regularizations", "text": "We are further curious about the behaviors when different regularization methods are combined.\nTable 1 shows that combining `2-norm of weights and embeddings results in a further accuracy improvement of 3\u20134 percents from applying\neither single one of them. In a certain range of coefficients, weights and embeddings are complementary: given one hyperparameter, we can tune the other to achieve a result among highest ones.\nSuch compensation is also observed in penalizing `2-norm versus dropout (Table 2)\u2014although the peak performance is obtained by pure `2 regularization, applying dropout with small `2 penalty also achieves a similar accuracy. The dropout rate is not very sensitive, provided it is small."}, {"heading": "7 Discussion", "text": "In this paper, we systematically compared four regularization strategies for embedding-based neural networks in NLP. Based on the experimental results, we answer our research questions as follows. (1) Regularization methods (except reembedding words) basically help generalization. Penalizing `2-norm of embeddings unexpectedly helps optimization as well. Regularization performance depends largely on the dataset\u2019s size. (2) `2 penalty mainly acts as a local effect; hyperparameters can be tuned incrementally. (3) Combining `2-norm of weights and biases (dropout and `2 penalty) further improves generalization; their coefficients are mostly complementary within a certain range. These empirical results of regularization strategies shed some light on tuning neural models for NLP."}, {"heading": "Acknowledgments", "text": "This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant No. 61232015. We would also like to thank Hao Jia and Ran Jia."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The effects of hyperparameters on SGD training of neural networks. arXiv preprint arXiv:1508.02788", "author": ["T. Breuel"], "venue": null, "citeRegEx": "Breuel.,? \\Q2015\\E", "shortCiteRegEx": "Breuel.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "A. Mohamed", "G. Hinton"], "venue": "In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["M. Miwa", "Y. Tsuruoka", "T. Chikayama"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "C. Cardie"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Re-embedding words", "author": ["Labutov", "Lipson2013] Igor Labutov", "H. Lipson"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Labutov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Labutov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Y. Bengio"], "venue": "In Proceedings of International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Tree-based convolution: A new neural architecture for sentence modeling", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["Senior et al.2013] Andrew Senior", "G. Heigold", "M. Ranzato", "K. Yang"], "venue": "In Proceedings of 2013 IEEE International Conference on Acoustics,", "citeRegEx": "Senior et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Senior et al\\.", "year": 2013}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "R. Socher", "D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "In Proceedings of Computational Linguistics", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Zhao et al.2015] Han Zhao", "Z. Lu", "P. Poupart"], "venue": "In Proceedings of Intenational Joint Conference in Artificial Intelligence", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013).", "startOffset": 72, "endOffset": 118}, {"referenceID": 4, "context": "Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013).", "startOffset": 72, "endOffset": 118}, {"referenceID": 0, "context": "In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al.", "startOffset": 86, "endOffset": 154}, {"referenceID": 15, "context": ", 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al.", "startOffset": 200, "endOffset": 239}, {"referenceID": 13, "context": ", 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al.", "startOffset": 200, "endOffset": 239}, {"referenceID": 20, "context": ", 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc.", "startOffset": 33, "endOffset": 69}, {"referenceID": 19, "context": ", 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc.", "startOffset": 33, "endOffset": 69}, {"referenceID": 0, "context": "In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning.", "startOffset": 87, "endOffset": 579}, {"referenceID": 2, "context": "consuming, as suggested in Collobert et al. (2011). Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing.", "startOffset": 27, "endOffset": 51}, {"referenceID": 17, "context": "In this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivastava et al., 2014).", "startOffset": 226, "endOffset": 251}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 5, "context": ", Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model", "startOffset": 2, "endOffset": 68}, {"referenceID": 15, "context": "The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral.", "startOffset": 47, "endOffset": 68}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014).", "startOffset": 65, "endOffset": 133}, {"referenceID": 15, "context": "We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children\u2019s information to their parent\u2019s; the root vector is finally used for sentiment classification.", "startOffset": 65, "endOffset": 158}, {"referenceID": 21, "context": "Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008).", "startOffset": 49, "endOffset": 68}, {"referenceID": 21, "context": "Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.", "startOffset": 49, "endOffset": 157}, {"referenceID": 14, "context": "fore, we applied power decay (Senior et al., 2013) with power equal to \u22121.", "startOffset": 29, "endOffset": 50}, {"referenceID": 17, "context": "\u2022 Dropout (Srivastava et al., 2014).", "startOffset": 10, "endOffset": 35}, {"referenceID": 6, "context": "Such strategy is sometimes applied in the literature like Hu et al. (2014), but is not favorable as suggested by the experiment.", "startOffset": 58, "endOffset": 75}, {"referenceID": 1, "context": "may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out (Breuel, 2015).", "startOffset": 109, "endOffset": 123}], "year": 2015, "abstractText": "This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.", "creator": "LaTeX with hyperref package"}}}