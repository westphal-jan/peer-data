{"id": "1508.03826", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution", "abstract": "most existing word embedding methods automatically can be categorized into neural embedding models and matrix factorization ( mf ) - component based methods. however sometimes some models may are opaque to probabilistic code interpretation, and mf - based methods, typically solved using singular value dependency decomposition ( svd ), some may incur loss of corpus information. in addition, it is desirable to incorporate unique global latent characteristic factors, such as topics, sentiments or specific writing styles, into the word embedding model. since generative models provide a principled way up to theoretically incorporate latent factors, initially we propose a generative word gap embedding model, which is easy to actually interpret, effective and can serve as a basis marker of more sophisticated latent factor models. the similarity model inference reduces considerably to a low rank weighted positive semidefinite approximation problem. its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable above and vastly avoids the information loss in svd. in experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than a other mf - based methods.", "histories": [["v1", "Sun, 16 Aug 2015 14:12:17 GMT  (146kb,D)", "http://arxiv.org/abs/1508.03826v1", "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures"]], "COMMENTS": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["shaohua li", "jun zhu", "chunyan miao"], "accepted": true, "id": "1508.03826"}, "pdf": {"name": "1508.03826.pdf", "metadata": {"source": "CRF", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution", "authors": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "emails": ["lish0018@ntu.edu.sg,", "dcszj@tsinghua.edu.cn,", "ascymiao@ntu.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "The task of word embedding is to model the distribution of a word and its context words using their corresponding vectors in a Euclidean space. Then by doing regression on the relevant statistics derived from a corpus, a set of vectors are recovered which best fit these statistics. These vectors, commonly referred to as the embeddings, capture semantic/syntactic regularities between the words.\nThe core of a word embedding method is the link function that connects the input \u2014 the embeddings, with the output \u2014 certain corpus statistics.\nBased on the link function, the objective function is developed. The reasonableness of the link function impacts the quality of the obtained embeddings, and different link functions are amenable to different optimization algorithms, with different scalability. Based on the forms of the link function and the optimization techniques, most methods can be divided into two classes: the traditional neural embedding models, and more recent low rank matrix factorization methods.\nThe neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2006). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known \u201cword2vec\u201d, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks.\nLow-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, Levy and Goldberg (2014) find that \u201cword2vec\u201d is essentially doing stochastic weighted factorization of the word-context pointwise mutual information (PMI) matrix. They then\nar X\niv :1\n50 8.\n03 82\n6v 1\n[ cs\n.C L\n] 1\n6 A\nug 2\n01 5\nfactorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a constraint on the embeddings in a small text window. The maximum likelihood estimate of the embeddings within this text window approximately reduces to a squared norm objective.\nThere are two limitations in current word embedding methods. The first limitation is, all MFbased methods map words and their context words to two different sets of embeddings, and then employ Singular Value Decomposition (SVD) to obtain a low rank approximation of the word-context matrix M . As SVD factorizes M>M , some information in M is lost, and the learned embeddings may not capture the most significant regularities inM . Appendix A gives a toy example on which SVD does not work properly.\nThe second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although (Stratos et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not\nonly easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information.\nBased on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual capturing nonlinear or noisy correlations, and the unigram priors. To reduce overfitting, we put Gaussian priors on embeddings and residuals, and apply Jelinek-Mercer Smoothing to bigrams. Furthermore, to model the probability of a sequence of words, we assume that the contributions of more than one context word approximately add up. Thereby a generative model of documents is constructed, parameterized by embeddings and residuals. The learning objective is to maximize the corpus likelihood, which reduces to a weighted low-rank positive semidefinite (PSD) approximation problem of the PMI matrix. A Block Coordinate Descent algorithm is adopted to find an approximate solution. This algorithm is based on Eigendecomposition, which avoids information loss in SVD, but brings challenges to scalability. We then exploit the sparsity of the weight matrix and implement an efficient online blockwise regression algorithm. On seven benchmark datasets covering similarity and analogy tasks, our method achieves competitive and stable performance.\nThe source code of this method is provided at https://github.com/askerlee/topicvec."}, {"heading": "2 Notations and Definitions", "text": "Throughout the paper, we always use a uppercase bold letter asS,V to denote a matrix or set, a lowercase bold letter as vwi to denote a vector, a normal uppercase letter as N,W to denote a scalar constant, and a normal lowercase letter as si, wi to denote a scalar variable.\nSuppose a vocabulary S = {s1, \u00b7 \u00b7 \u00b7 , sW} consists of all the words, where W is the vocabulary size. We further suppose s1, \u00b7 \u00b7 \u00b7 , sW are sorted in decending order of the frequency, i.e. s1 is most frequent, and sW is least frequent. A document di is a sequence of words di = (wi1, \u00b7 \u00b7 \u00b7 , wiLi), wij \u2208 S. A corpus is a collec-\ntion of M documents D = {d1, \u00b7 \u00b7 \u00b7 , dM}. In the vocabulary, each word si is mapped to a vector vsi in N -dimensional Euclidean space.\nIn a document, a sequence of words is referred to as a text window, denoted by wi, \u00b7 \u00b7 \u00b7 , wi+l, or wi:wi+l in shorthand. A text window of chosen size c before a word wi defines the context of wi as wi\u2212c, \u00b7 \u00b7 \u00b7 , wi\u22121. Here wi is referred to as the focus word. Each context word wi\u2212j and the focus word wi comprise a bigram wi\u2212j , wi.\nThe Pointwise Mutual Information between two words si, sj is defined as\nPMI(si, sj) = log P (si, sj)\nP (si)P (sj) ."}, {"heading": "3 Link Function of Text", "text": "In this section, we formulate the probability of a sequence of words as a function of their embeddings. We start from the link function of bigrams, which is the building blocks of a long sequence. Then this link function is extended to a text window with c context words, as a first-order approximation of the actual probability."}, {"heading": "3.1 Link Function of Bigrams", "text": "We generalize the link function of \u201cword2vec\u201d and \u201cGloVe\u201d to the following:\nP (si, sj) = exp { v>sjvsi + asisj } P (si)P (sj) (1)\nThe rationale for (1) originates from the idea of the Product of Experts in (Hinton, 2002). Suppose different types of semantic/syntactic regularities between si and sj are encoded in different dimensions of vsi ,vsj . As exp{v>sjvsi} =\u220f l exp{vsi,l \u00b7 vsj ,l}, this means the effects of different regularities on the probability are combined\nby multiplying together. If si and sj are independent, their joint probability should be P (si)P (sj). In the presence of correlations, the actual joint probability P (si, sj) would be a scaling of it. The scale factor reflects how much si and sj are positively or negatively correlated. Within the scale factor, v>sjvsi captures linear interactions between si and sj , the residual asisj captures nonlinear or noisy interactions. In applications, only v>sjvsi is of interest. Hence the bigger magnitude v>sjvsi is of relative to asisj , the better.\nNote that we do not assume asisj = asjsi . This provides the flexibility P (si, sj) 6= P (sj , si), agreeing with the asymmetry of bigrams in natural languages. At the same time, v>sjvsi imposes a symmetric part between P (si, sj) and P (sj , si).\n(1) is equivalent to P (sj |si)=exp { v>sjvsi + asisj+ logP (sj) } , (2)\nlog P (sj |si) P (sj) = v>sjvsi + asisj . (3)\n(3) of all bigrams is represented in matrix form:\nV >V +A = G, (4)\nwhereG is the PMI matrix."}, {"heading": "3.1.1 Gaussian Priors on Embeddings", "text": "When (1) is employed on the regression of empirical bigram probabilities, a practical issue arises: more and more bigrams have zero frequency as the constituting words become less frequent. A zero-frequency bigram does not necessarily imply negative correlation between the two constituting words; it could simply result from missing data. But in this case, even after smoothing, (1) will force v>sjvsi + asisj to be a big negative number, making vsi overly long. The increased magnitude of embeddings is a sign of overfitting.\nTo reduce overfitting of embeddings of infrequent words, we assign a Spherical Gaussian prior N (0, 12\u00b5i I) to vsi : P (vsi) \u223c exp{\u2212\u00b5i\u2016vsi\u20162}, where the hyperparameter \u00b5i increases as the frequency of si decreases."}, {"heading": "3.1.2 Gaussian Priors on Residuals", "text": "We wish v>sjvsi in (1) captures as much correlations between si and sj as possible. Thus the smaller asisj is, the better. In addition, the more frequent si, sj is in the corpus, the less noise there is in their empirical distribution, and thus the residual asisj should be more heavily penalized.\nTo this end, we penalize the residual asisj by f(P\u0303(si, sj))a2sisj , where f(\u00b7) is a nonnegative monotonic transformation, referred to as the weighting function. Let hij denote P\u0303 (si, sj), then the total penalty of all residuals are the square of the weighted Frobenius norm ofA:\u2211\nsi,sj\u2208S f(hij)a\n2 sisj = \u2016A\u2016 2 f(H). (5)\nBy referring to \u201cGloVe\u201d, we use the following weighting function, and find it performs well:\nf(hij) =  \u221a hij Ccut \u221a hij < Ccut, i 6= j 1 \u221a hij \u2265 Ccut, i 6= j\n0 i = j\n,\nwhere Ccut is chosen to cut the most frequent 0.02% of the bigrams off at 1. When si = sj , two identical words usually have much smaller probability to collocate. Hence P\u0303 (si, si) does not reflect the true correlation of a word to itself, and should not put constraints to the embeddings. We eliminate their effects by setting f(hii) to 0.\nIf the domain of A is the whole space RW\u00d7W , then this penalty is equivalent to a Gaussian prior N (\n0, 12f(hij) ) on each asisj . The variances of the Gaussians are determined by the bigram empirical probability matrixH ."}, {"heading": "3.1.3 Jelinek-Mercer Smoothing of Bigrams", "text": "As another measure to reduce the impact of missing data, we apply the commonly used JelinekMercer Smoothing (Zhai and Lafferty, 2004) to smooth the empirical conditional probability P\u0303 (sj |si) by the unigram probability P\u0303 (sj) as: P\u0303smoothed(sj |si) = (1\u2212\u03ba)P\u0303 (sj |si)+\u03baP (sj). (6) Accordingly, the smoothed bigram empirical joint probability is defined as\nP\u0303 (si, sj) = (1\u2212\u03ba)P\u0303 (si, sj)+\u03baP (si)P (sj). (7) In practice, we find \u03ba = 0.02 yields good results. When \u03ba \u2265 0.04, the obtained embeddings begin to degrade with \u03ba, indicating that smoothing distorts the true bigram distributions."}, {"heading": "3.2 Link Function of a Text Window", "text": "In the previous subsection, a regression link function of bigram probabilities is established. In this section, we adopt a first-order approximation based on Information Theory, and extend the link function to a longer sequence w0, \u00b7 \u00b7 \u00b7 , wc\u22121, wc.\nDecomposing a distribution conditioned on n random variables as the conditional distributions\non its subsets roots deeply in Information Theory. This is an intricate problem because there could be both (pointwise) redundant information and (pointwise) synergistic information among the conditioning variables (Williams and Beer, 2010). They are both functions of the PMI. Based on an analysis of the complementing roles of these two types of pointwise information, we assume they are approximately equal and cancel each other when computing the pointwise interaction information. See Appendix B for a detailed discussion.\nFollowing the above assumption, we have PMI(w2;w0, w1) \u2248 PMI(w2;w0)+PMI(w2;w1):\nlog P(w0, w1|w2) P(w0, w1) \u2248logP(w0|w2) P(w0) +log P(w1|w2) P(w1) .\nPlugging (1) and (3) into the above, we obtain\nP (w0, w1, w2) \u2248 exp { 2\u2211 i,j=0 i 6=j (v>wivwj + awiwj ) + 2\u2211 i=0 logP (wi) } .\nWe extend the above assumption to that the pointwise interaction information is still close to 0 within a longer text window. Accordingly the above equation extends to a context of size c > 2:\nP (w0, \u00b7 \u00b7 \u00b7 , wc) \u2248 exp { c\u2211 i,j=0 i 6=j (v>wivwj + awiwj ) + c\u2211 i=0 logP (wi) } .\nFrom it derives the conditional distribution of wc, given its context w0, \u00b7 \u00b7 \u00b7 , wc\u22121:\nP (wc | w0 : wc\u22121)= P (w0, \u00b7 \u00b7 \u00b7 , wc) P (w0, \u00b7 \u00b7 \u00b7 , wc\u22121)\n\u2248P (wc) exp { v>wc c\u22121\u2211 i=0 vwi + c\u22121\u2211 i=0 awiwc } . (8)"}, {"heading": "4 Generative Process and Likelihood", "text": "We proceed to assume the text is generated from a Markov chain of order c, i.e., a word only depends on words within its context of size c. Given the hyperparameter \u00b5 = (\u00b51, \u00b7 \u00b7 \u00b7, \u00b5W ), the generative process of the whole corpus is:\n1. For each word si, draw the embedding vsi from N (0, 12\u00b5i I); 2. For each bigram si, sj , draw the residual\nasisj from N ( 0, 12f(hij) ) ;\n3. For each document di, for the j-th word, draw word wij from S with probability P (wij | wi,j\u2212c : wi,j\u22121) defined by (8).\nThe above generative process for a document d is presented as a graphical model in Figure 1.\nBased on this generative process, the probability of a document di can be derived as follows, given the embeddings and residuals V ,A:\nP (di|V ,A)\n= Li\u220f j=1 P (wij) exp { v>wij j\u22121\u2211 k=j\u2212c vwik+ j\u22121\u2211 k=j\u2212c awikwij } .\nThe complete-data likelihood of the corpus is:\np(D,V ,A)\n= W\u220f i=1 N (0, I 2\u00b5i ) W,W\u220f i,j=1 N ( 0, 1 2f(hij) ) M\u220f i=1 p(di|V,A)\n= 1 Z(H,\u00b5) exp { \u2212 W,W\u2211 i,j=1 f(hi,j)a 2 sisj\u2212 W\u2211 i=1 \u00b5i\u2016vsi\u20162 }\n\u00b7 M,Li\u220f i,j=1 P (wij) exp { v>wij j\u22121\u2211 k=j\u2212c vwik+ j\u22121\u2211 k=j\u2212c awikwij } ,\nwhere Z(H,\u00b5) is the normalizing constant. Taking the logarithm of both sides of p(D,A,V ) yields\nlog p(D,V ,A)\n=C0 \u2212 logZ(H,\u00b5)\u2212 \u2016A\u20162f(H)\u2212 W\u2211 i=1 \u00b5i\u2016vsi\u20162\n+ M,Li\u2211 i,j=1 { v>wij j\u22121\u2211 k=j\u2212c vwik+ j\u22121\u2211 k=j\u2212c awikwij } , (9)\nwhere C0 = \u2211M,Li i,j=1 logP (wij) is constant."}, {"heading": "5 Learning Algorithm", "text": ""}, {"heading": "5.1 Learning Objective", "text": "The learning objective is to find the embeddings V that maximize the corpus log-likelihood (9).\nLet xij denote the (smoothed) frequency of bigram si, sj in the corpus. Then (9) is sorted as:\nlog p(D,V ,A)\n=C0 \u2212 logZ(H,\u00b5)\u2212 \u2016A\u20162f(H) \u2212 W\u2211 i=1 \u00b5i\u2016vsi\u20162\n+ W,W\u2211 i,j=1 xij(v > sivsj + asisj ). (10) As the corpus size increases,\u2211W,W i,j=1 xij(v > sivsj+asisj ) will dominate the parameter prior terms. Then we can ignore the prior terms when maximizing (10).\nmax \u2211\nxij(v > sivsj+asisj ) = (\u2211\nxij\n) \u00b7max \u2211 P\u0303smoothed(si, sj) logP (si, sj).\nAs both {P\u0303smoothed(si, sj)} and {P (si, sj)} sum to 1, the above sum is maximized when P (si, sj) = P\u0303smoothed(si, sj).\nThe maximum likelihood estimator is then:\nP (sj |si) = P\u0303smoothed(sj |si),\nv>sivsj + asisj = log P\u0303smoothed(sj |si)\nP (sj) . (11)\nWriting (11) in matrix form: B\u2217 = ( P\u0303smoothed(sj |si) ) si,sj\u2208S\nG\u2217 = logB\u2217 \u2212 logu\u2297 (1 \u00b7 \u00b7 \u00b7 1), (12) where \u201c\u2297\u201d is the outer product.\nNow we fix the values of v>sivsj + asisj at the above optimal. The corpus likelihood becomes log p(D,V ,A) =C1 \u2212 \u2016A\u20162f(H) \u2212 W\u2211 i=1 \u00b5i\u2016vsi\u20162,\nsubject to V >V +A = G\u2217, (13) where C1 = C0 + \u2211 xij log P\u0303smoothed(si, sj) \u2212 logZ(H,\u00b5) is constant."}, {"heading": "5.2 Learning V as Low Rank PSD Approximation", "text": "OnceG\u2217 has been estimated from the corpus using (12), we seek V that maximizes (13). This is to find the maximum a posteriori (MAP) estimates of V ,A that satisfy V >V +A = G\u2217. Applying this constraint to (13), we obtain\nAlgorithm 1 BCD algorithm for finding a unregularized rank-N weighted PSD approximant. Input: matrixG\u2217, weight matrixW = f(H), iteration number T , rank N\nRandomly initializeX(0) for t = 1, \u00b7 \u00b7 \u00b7 , T do Gt = W \u25e6G\u2217 + (1\u2212W ) \u25e6X(t\u22121) X(t) = PSD Approximate(Gt, N) end for \u03bb,Q = Eigen Decomposition(X(T )) V \u2217 = diag(\u03bb 1 2 [1:N ]) \u00b7Q>[1:N ]\nOutput: V \u2217\narg max V log p(D,V ,A)\n= arg min V \u2016G\u2217\u2212V >V \u2016f(H) + W\u2211 i=1 \u00b5i\u2016vsi\u20162. (14)\nLet X = V >V . Then X is positive semidefinite of rank N . Finding V that minimizes (14) is equivalent to finding a rank-N weighted positive semidefinite approximant X ofG\u2217, subject to Tikhonov regularization. This problem does not admit an analytic solution, and can only be solved using local optimization methods.\nFirst we consider a simpler case where all the words in the vocabulary are enough frequent, and thus Tikhonov regularization is unnecessary. In this case, we set \u2200\u00b5i = 0, and (14) becomes an unregularized optimization problem. We adopt the Block Coordinate Descent (BCD) algorithm1 in (Srebro et al., 2003) to approach this problem. The original algorithm is to find a generic rank-N matrix for a weighted approximation problem, and we tailor it by constraining the matrix within the positive semidefinite manifold.\nWe summarize our learning algorithm in Algorithm 1. Here \u201c\u25e6\u201d is the entry-wise product. We suppose the eigenvalues \u03bb returned by Eigen Decomposition(X) are in descending order. Q>[1:N ] extracts the 1 to N rows fromQ>.\nOne key issue is how to initialize X . Srebro et al. (2003) suggest to set X(0)=G\u2217, and point out that X(0) = 0 is far from a local optimum, thus requires more iterations. However we find G\u2217 is also far from a local optimum, and this setting converges slowly too. Setting X(0) = G\u2217/2 usually\n1It is referred to as an Expectation-Maximization algorithm by the original authors, but we think this is a misnomer.\nyields a satisfactory solution in a few iterations. The subroutine PSD Approximate() computes the unweighted nearest rank-N PSD approximation, measured in F-norm (Higham, 1988)."}, {"heading": "5.3 Online Blockwise Regression of V", "text": "In Algorithm 1, the essential subroutine PSD Approximate() does eigendecomposition on Gt, which is dense due to the logarithm transformation. Eigendecomposition on a W \u00d7W dense matrix requires O(W 2) space and O(W 3) time, difficult to scale up to a large vocabulary. In addition, the majority of words in the vocabulary are infrequent, and Tikhonov regularization is necessary for them.\nIt is observed that, as words become less frequent, fewer and fewer words appear around them to form bigrams. Remind that the vocabulary S = {s1, \u00b7 \u00b7 \u00b7 , sW } are sorted in decending order of the frequency, hence the lower-right blocks of H and f(H) are very sparse, and cause these blocks in (14) to contribute much less penalty relative to other regions. Therefore these blocks could be ignored when doing regression, without sacrificing too much accuracy. This intuition leads to the following online blockwise regression.\nThe basic idea is to select a small set (e.g. 30,000) of the most frequent words as the core words, and partition the remaining noncore words into sets of moderate sizes. Bigrams consisting of two core words are referred to as core bigrams, which correspond to the top-left blocks of G and f(H). The embeddings of core words are learned approximately using Algorithm 1, on the top-left blocks of G and f(H). Then we fix the embeddings of core words, and find the embeddings of each set of noncore words in turn. After ignoring the lower-right regions of G and f(H) which correspond to bigrams of two noncore words, the quadratic terms of noncore embeddings are ignored. Consequently, finding these embeddings becomes a weighted ridge regression problem, which can be solved efficiently in closedform. Finally we combine all embeddings to get the embeddings of the whole vocabulary. The details are as follows:\n1. Partition S into K consecutive groups S1, \u00b7 \u00b7 \u00b7 ,Sk. Take K = 3 as an example. The first group is core words;\n2. Accordingly partitionG into K \u00d7K blocks,\nin this example as  G11 G12 G13G21 G22 G23 G31 G32 G33  . Partition f(H),A in the same way. G11, f(H)11,A11 correspond to core bi-\ngrams. Partition V into ( \ufe38\ufe37\ufe37\ufe38 S1 V 1 \ufe38\ufe37\ufe37\ufe38 S2 V 2 \ufe38\ufe37\ufe37\ufe38 S3 V 3 ) ;\n3. Solve V >1V 1 +A11 = G11 using Algorithm 1, and obtain core embeddings V \u22171; 4. Set V 1 = V \u22171, and find V \u2217 2 that minimizes\nthe total penalty of the 12-th and 21-th blocks of residuals (the 22-th block is ignored due to its high sparsity):\narg min V 2\n\u2016G12 \u2212 V >1V 2\u20162f(H)12\n+ \u2016G21 \u2212 V >2V 1\u20162f(H)21 + \u2211 si\u2208S2 \u00b5i\u2016vsi\u20162\n= arg min V 2 \u2016G12\u2212V >1V 2\u20162f\u0304(H)12+ \u2211 si\u2208S2 \u00b5i\u2016vsi\u20162,\nwhere f\u0304(H)12 = f(H)12 + f(H)>21; G12 = ( G12 \u25e6 f(H)12 + G>21 \u25e6 f(H)>21 ) / ( f(H)12 + f(H) > 21 ) is the weighted average ofG12 andG>21, \u201c\u25e6\u201d and \u201c/\u201d are elementwise product and division, respectively. The columns in V 2 are independent, thus for each vsi , it is a separate weighted ridge regression problem, whose solution is (Holland, 1973): v\u2217si=(V > 1 diag(f\u0304 i)V 1+\u00b5iI)\n\u22121V >1 diag(f\u0304 i)g\u0304i, where f\u0304 i and g\u0304i are columns corresponding to si in f\u0304(H)12 andG12, respectively;\n5. For any other set of noncore words Sk, find V \u2217k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks;\n6. Combine all subsets of embeddings to form V \u2217. Here V \u2217 = (V \u22171,V \u2217 2,V \u2217 3)."}, {"heading": "6 Experimental Results", "text": "We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets."}, {"heading": "6.1 Experimental Setup", "text": "Our own method is referred to as PSD. The competitors include:\n\u2022 (Mikolov et al., 2013b): word2vec2, or SGNS in some literature;\n2https://code.google.com/p/word2vec/\n\u2022 (Levy and Goldberg, 2014): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords;\n\u2022 (Pennington et al., 2014): GloVe3; \u2022 (Stratos et al., 2015): Singular4, which does\nSVD-based CCA on the weighted bigram frequency matrix;\n\u2022 (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings.\nAll models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters.\nThe embedding sets PSD-Reg-180K and PSDUnreg-180K were trained using our online blockwise regression. Both sets contain the embeddings of the most frequent 180,000 words, based on 25,000 core words. PSD-Unreg-180K was traind with all \u00b5i = 0, i.e. disabling Tikhonov regularization. PSD-Reg-180K was trained with\n\u00b5i =  2 i \u2208 [25001, 80000] 4 i \u2208 [80001, 130000] 8 i \u2208 [130001, 180000] , i.e. increased regularization as the sparsity increases. To contrast with the batch learning performance, the performance of PSD-25K is listed, which contains the core embeddings only. PSD-25K took advantages that it contains much less false candidate words, and some test tuples (generally harder ones) were not evaluated due to missing words, thus its scores are not comparable to others.\nSparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters.\nThe benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013)\u2019s Rare Words is not included, as many rare words are cut off at the frequency 100, making more than 1/3 of test pairs invalid.\nWord Similarity There are 5 datasets: WordSim Similarity (WS Sim) and WordSim Relatedness (WS Rel) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012)\u2019s MEN dataset;\n3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding\nRadinsky et al. (2011)\u2019s Mechanical Turk dataset; and (Hill et al., 2014)\u2019s SimLex-999 dataset. The embeddings were evaluated by the Spearman\u2019s rank correlation with the human ratings.\nWord Analogy The two datasets are MSR\u2019s analogy dataset (Mikolov et al., 2013c), containing 8000 questions, and Google\u2019s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy et al. (2014)."}, {"heading": "6.2 Results", "text": "Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparameters. This suggests their performance is unstable. The new embeddings yielded by Sparse systematically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015).\nOur method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still better than other MF-based methods. By comparing to PSD-Unreg-180K, we see Tikhonov regularization brings 1-4% performance boost across tasks. In addition, on similarity tasks, online blockwise regression only degrades slightly compared to batch factorization. Their performance gaps on analogy tasks were wider, but this might be explained by the fact that some hard cases were not counted in PSD-25K\u2019s evaluation,\ndue to its limited vocabulary."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, inspired by the link functions in previous works, with the support from Information Theory, we propose a new link function of a text window, parameterized by the embeddings of words and the residuals of bigrams. Based on the link function, we establish a generative model of documents. The learning objective is to find a set of embeddings maximizing their posterior likelihood given the corpus. This objective is reduced to weighted low-rank positive-semidefinite approximation, subject to Tikhonov regularization. Then we adopt a Block Coordinate Descent algorithm, jointly with an online blockwise regression algorithm to find an approximate solution. On seven benchmark sets, the learned embeddings show competitive and stable performance.\nIn the future work, we will incorporate global latent factors into this generative model, such as topics, sentiments, or writing styles, and develop more elaborate models of documents. Through learning such latent factors, important summary information of documents would be acquired, which are useful in various applications."}, {"heading": "Acknowledgments", "text": "We thank Omer Levy, Thomas Mach, Peilin Zhao, Mingkui Tan, Zhiqiang Xu and Chunlin Wu for their helpful discussions and insights. This research is supported by the National Research Foundation, Prime Minister\u2019s Office, Singapore under its IDM Futures Funding Initiative and administered by the Interactive and Digital Media Programme Office."}, {"heading": "Appendix A Possible Trap in SVD", "text": "SupposeM is the bigram matrix of interest. SVD embeddings are derived from the low rank approximation ofM>M , by keeping the largest singular values/vectors. When some of these singular values correspond to negative eigenvalues, undesirable correlations might be captured. The following is an example of approximating a PMI matrix.\nA vocabulary consists of 3 words s1, s2, s3. Two corpora derive two PMI matrices:\nM (1) = (\n1.4 0.8 0 0.8 2.6 0 0 0 2\n) , M (2) = ( 0.2 \u22121.6 0 \u22121.6 \u22122.2 0\n0 0 2\n) .\nThey have identical left singular matrix and singular values (3, 2, 1), but their eigenvalues are (3, 2, 1) and (\u22123, 2, 1), respectively.\nIn a rank-2 approximation, the largest two singular values/vectors are kept, and M (1) and M (2) yield identical SVD embeddings V = ( 0.45 0.89 00 0 1 ) (the rows may be scaled depending on the algorithm, without affecting the validity of the following conclusion). The embeddings of s1 and s2 (columns 1 and 2 of V ) point at the same direction, suggesting they are positively correlated. However as M (2)1,2 = M (2) 2,1 = \u22121.6 < 0, they are actually negatively correlated in the second corpus. This inconsistency is because the principal eigenvalue of M (2) is negative, and yet the corresponding singular value/vector is kept.\nWhen using eigendecomposition, the largest two positive eigenvalues/eigenvectors are kept. M (1) yields the same embeddings V . M (2)\nyields V (2) = (\u22120.89 0.45 0\n0 0 1.41\n) , which correctly\npreserves the negative correlation between s1, s2."}, {"heading": "Appendix B Information Theory", "text": "Redundant information refers to the reduced uncertainty by knowing the value of any one of the conditioning variables (hence redundant). Synergistic information is the reduced uncertainty ascribed to knowing all the values of conditioning variables, that cannot be reduced by knowing the value of any variable alone (hence synergistic).\nThe mutual information I(y;xi) and the redundant information Rdn(y;x1, x2) are defined as:\nI(y;xi) = EP (xi,y)[log P (y|xi) P (y) ]\nRdn(y;x1, x2) = EP (y) [ min x1,x2 EP (xi|y)[log P (y|xi) P (y) ] ] The synergistic information Syn(y;x1, x2) is\ndefined as the PI-function in (Williams and Beer, 2010), skipped here.\nThe interaction information Int(x1, x2, y) measures the relative strength of Rdn(y;x1, x2) and Syn(y;x1, x2) (Timme et al., 2014):\nInt(x1, x2, y)\n=Syn(y;x1, x2)\u2212 Rdn(y;x1, x2) =I(y;x1, x2)\u2212 I(y;x1)\u2212 I(y;x2)\n=EP (x1,x2,y)[log P (x1)P (x2)P (y)P (x1, x2, y)\nP (x1, x2)P (x1, y)P (x2, y) ]\nFigure 2 shows the relationship of different information among 3 random variables y, x1, x2 (based on Fig.1 in (Williams and Beer, 2010)).\nPMI is the pointwise counterpart of mutual information I . Similarly, all the above concepts have their pointwise counterparts, obtained by dropping the expectation operator. Specifically, the pointwise interaction information is defined as PInt(x1, x2, y) = PMI(y;x1, x2)\u2212 PMI(y;x1)\u2212 PMI(y;x2) = log\nP (x1)P (x2)P (y)P (x1,x2,y) P (x1,x2)P (x1,y)P (x2,y) . If we know PInt(x1, x2, y), we can recover PMI(y;x1, x2) from the mutual information over the variable subsets, and then recover the joint distribution P (x1, x2, y).\nAs the pointwise redundant information PRdn(y;x1, x2) and the pointwise synergistic information PSyn(y;x1, x2) are both higherorder interaction terms, their magnitudes are usually much smaller than the PMI terms. We assume they are approximately equal, and thus cancel each other when computing PInt. Given this, PInt is always 0. In the case of three words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to PMI(w2;w0, w1) = PMI(w2;w0)+PMI(w2;w1)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Most existing word embedding methods<lb>can be categorized into Neural Embedding<lb>Models and Matrix Factorization (MF)-<lb>based methods. However some mod-<lb>els are opaque to probabilistic interpre-<lb>tation, and MF-based methods, typically<lb>solved using Singular Value Decomposi-<lb>tion (SVD), may incur loss of corpus in-<lb>formation. In addition, it is desirable to<lb>incorporate global latent factors, such as<lb>topics, sentiments or writing styles, into<lb>the word embedding model. Since gen-<lb>erative models provide a principled way<lb>to incorporate latent factors, we propose a<lb>generative word embedding model, which<lb>is easy to interpret, and can serve as a<lb>basis of more sophisticated latent factor<lb>models. The model inference reduces to<lb>a low rank weighted positive semidefinite<lb>approximation problem. Its optimization<lb>is approached by eigendecomposition on a<lb>submatrix, followed by online blockwise<lb>regression, which is scalable and avoids<lb>the information loss in SVD. In experi-<lb>ments on 7 common benchmark datasets,<lb>our vectors are competitive to word2vec,<lb>and better than other MF-based methods.", "creator": "LaTeX with hyperref package"}}}