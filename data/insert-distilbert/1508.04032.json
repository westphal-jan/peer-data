{"id": "1508.04032", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Variable Elimination in the Fourier Domain", "abstract": "probabilistic inference modelling is a key computational challenge in statistical machine learning laboratory and artificial formal intelligence. the ability to represent complex high dimensional probability distributions based in a compact form is the most important insight in the field of graphical models.", "histories": [["v1", "Mon, 17 Aug 2015 14:04:07 GMT  (167kb,D)", "http://arxiv.org/abs/1508.04032v1", null], ["v2", "Wed, 22 Jun 2016 03:18:10 GMT  (4104kb,D)", "http://arxiv.org/abs/1508.04032v2", "Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yexiang xue", "stefano ermon", "ronan le bras", "carla p gomes", "bart selman"], "accepted": true, "id": "1508.04032"}, "pdf": {"name": "1508.04032.pdf", "metadata": {"source": "CRF", "title": "Variable Elimination in Fourier Domain", "authors": ["Yexiang Xue", "Stefano Ermon", "Ronan Lebras", "Carla P. Gomes", "Bart Selman"], "emails": ["yexiang@cs.cornell.edu", "ermon@cs.stanford.edu", "selman}@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic inference is a key computational challenge in statistical machine learning and artificial intelligence. Inference methods have an enormous number of applications, from learning models to making predictions and informing decision-making using statistical models of data. Unfortunately, the problem is computationally intractable, and standard exact inference algorithms, such as variable elimination and junction tree algorithms have worst-case exponential complexity.\nThe ability to represent complex high dimensional probability distributions in a compact form is the most important insight in the field of graphical models. The fundamental idea is to exploit (conditional) independencies between the variables to achieve compact factored representations, where a complex global model is represented as a product of simpler, local models. Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].\nCompact representations are also key for the development of efficient inference algorithms, including message-passing ones. Efficient algorithms can be developed when messages representing the interaction among many variables can be decomposed or approximated with the product of several smaller messages, each involving a subset of the original variables. Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].\nConditional independence (and related factorizations) is not the only type of structure that can be exploited to achieve compactness. For example, consider the regression tree in Figure 1. No two variables in the probability distribution in Figure 1 are independent with each other. The probability\nar X\niv :1\n50 8.\n04 03\n2v 1\n[ cs\n.A I]\n1 7\nA ug\ndistribution cannot be represented by the product of simpler terms and requires a full probability table with 27 = 128 entries to be represented exactly. Nevertheless, this table can be described exactly by 8 simple decision rules, each corresponding to a path from the root to a leaf in the tree.\nIn this paper, we explore a novel way to exploit compact representations of high-dimensional probability tables in (approximate) probabilistic inference algorithms. Our approach is based on a (discrete) Fourier representation of the tables, which can be interpreted as a change of basis. Crucially, tables that are dense in the canonical basis can have a sparse Fourier representation. Under certain conditions, probability tables can be represented (or well approximated) using a small number of Fourier coefficients. The Fourier representation has found numerous recent applications in PAC learning [18, 15, 1, 2], but these ideas have not been fully exploited in the fields of probabilistic inference and graphical models.\nIn general, a general factor over n Boolean variables requires O(2n) entries to be specified, and similarly the corresponding Fourier representation is dense in general, i.e., it has O(2n) non-zero coefficients. However, a rather surprising fact which was first discovered by Linial [14] is that factors corresponding to fairly general classes of logical forms admit a compact Fourier representation. Linial discovered that formulas in Conjunctive Normal Form (CNF) and Disjunctive Normal Form (DNF) with bounded width (the number of variables in each clause) have compact Fourier representations.\nIn this paper, we introduce a novel approach for using approximate Fourier representations in the field of probabilistic inference. We generalize the work of Linial to the probability distributions (the weighted case where the entries are not necessarily 0 or 1), showing that a large class of probabilistic graphical models have compact Fourier representation, if they are composed with factors with bounded domain size and discriminative weights. This includes interesting graphical models, such as Markov Logic Networks [21] with discriminative weights. The proof extends the famous Hastad\u2019s Switching Lemma[9] to the weighted case. At a high level, a compact Fourier representation often means the weighted probabilistic distribution can be captured by a small set of critical decision rules. Hence, this notion is closely related to decision trees with bounded depth.\nSparse (low-degree) Fourier representations provide an entirely new way of approximating a probability distribution. We demonstrate the power of this idea by applying it to the variable elimination algorithm and comparing the result with bucket representation and various approximate inference algorithms on a set of benchmarks, and obtain very encouraging results."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Inference In Graphical Models", "text": "We consider a Boolean graphical model of N = |V | boolean variables, where V = {xi|i \u2208 V }. We use bold typed variables to represent a vector of variables. For example, the vector of all boolean variables x is defined as x = (x1, x2, . . . , xN )T . Moreover, we use xS to represent the image of vector x projected onto a subset of variables: xS = (xi1 , xi2 , . . . , xik) T where xi1 , . . . , xik \u2208 S. A\nprobabilistic graphical model is defined as:\nPr(x) = 1\nZ f(x) =\n1\nZ K\u220f i=1 \u03c8i(xSi).\nwhere each \u03c8i : {\u22121, 1}|Si| \u2192 R+ is called a factor, and is a function that depends on a subset of variables in Si. Z = \u2211 x \u220fK i=1 \u03c8i(xSi) is the normalization factor, and is often called the partition function. In this paper, we will use\u22121 and 1 to represent false and true for Boolean functions.In this paper we consider two key probabilistic inference tasks: the computation of the partition function Z (PR) and marginal probabilities Pr(e) = 1Z \u2211 x\u223ce f(x) (Marginal) .\nVariable Elimination Algorithm is an exact algorithm to compute marginals and the partition function for general graphical models. It starts with a variable ordering \u03c0. In each iteration, it eliminates one variable by multiplying all factors involving that variable, and then summing that variable out. When all variables are eliminated, the factor remaining is a singleton, whose value corresponds to the partition function. The complexity of the VE algorithm depends on the size of largest factors generated during the elimination process, and is known to be exponential in the tree-width [7].\nDetcher proposed the Mini-bucket Elimination Algorithm [4], which dynamically decomposes and approximates factors (when the domain of a product exceeds a threshold) with the product of smaller factors during the elimination process."}, {"heading": "2.2 Hadmard-Fourier Transformation", "text": "Hadmard-Fourier transformation has recently attracted a lot of attention in PAC Learning Theory. Table 1 provides an example where a function \u03c6(x, y) is transformed into its Fourier representation. The transformation works by writing \u03c6(x, y) using interpolation, then re-arranging the terms to get a canonical term. The example can be generalized, and it can be shown that any function defined on a Boolean hypercube can be equivalently represented in Fourier space: Definition 1. (Hadmard-Fourier Transformation) Every f : {\u22121, 1}n \u2192 R can be uniquely expressed as a multilinear polynomial,\nf(x) = \u2211 S\u2286[n] cS \u220f i\u2208S xi.\nwhere each cS \u2208 R. Following standard notation, we will write f\u0302(S) to denote the coefficient cS and \u03c7S(x) for the function \u220f i\u2208S xi, and call f(x) = \u2211 S\u2286[n] f\u0302(S)\u03c7S(x) the Hadmard-Fourier\nexpansion of f . Note \u03c7\u2205 = 1. We also call f\u0302(S) a k-degree coefficient of f iff |S| = k.\nWe re-iterate some classical results on Fourier expansion. First, as with the classical (inverse) Fast Fourier Transformation (FFT) in the continuous domain, there are similar divide-and-conquer algorithms (FFT and invFFT) which connect the contingency table representation of f with its Fourier\nrepresentation. Both FFT and invFFT run in time O(n \u00b7 2n) for a function involving n variables. In fact, the vector of all function values and the vector of Fourier coefficients are connected by a 2n-by-2n matrix Hn, which is often called the n-th Hadmard-Fourier matrix. In addition, we have the Parseval\u2019s identity for Boolean Functions as well: Ex[f(x)2] = \u2211 S f\u0302(S) 2."}, {"heading": "3 Low Degree Concentration of Fourier Coefficients", "text": "Fourier expansion replaces the complete contingency table representation of a weighted function with its Fourier coefficients. For a function with nBoolean variables, the complete contingency table requires 2n entries, and so does the full Fourier expansion. Interestingly, many natural functions can be approximated well with only a few Fourier coefficients. This raises a natural question: what type of functions can be well approximated by a compact Fourier expansion?\nWe first discuss which functions can be represented exactly in Fourier domain with coefficients up to degree d. To answer this question, we show a tight connection between Fourier representations with bounded degree with decision trees with bounded depth. We start by defining a weighted decision tree. A decision tree for a weighted function f : {\u22121, 1}n \u2192 R+ is a tree in which each inner node is labelled with one variable, and two out-going edges, one labelled with \u22121, and other one with 1. The leaf nodes are labelled with non-negative real values. When evaluating the value on an input x = x1x2 . . . xn, we start from the root node, and follow the corresponding out-going edges by inspecting the value of one variable at each step, until we reach one of the leaf nodes. The value at the leaf node is the output for f(x). The depth of the decision tree is defined as the longest path from the root node to one of the leaf nodes. Figure 1 provides a decision tree representation for a weighted Boolean function. One classical result [18] states that if a function can be captured by a decision tree with depth d, then it can be represented with Fourier coefficients up to degree d: Theorem 1. Suppose f : {\u22121, 1}n \u2192 R can be represented by a decision tree of depth d, then all the coefficients whose degree is larger than d is zero in f \u2019s Fourier expansion: f\u0302(S) = 0 for all S such that |S| > d.\nWe can also provide the converse of Theorem 1: Theorem 2. Suppose f : {\u22121, 1}n \u2192 R can be represented by a Fourier expansion whose coefficients are up to degree d, then f can be represented by the sum of several decision trees, each of which has depth at most d.\nFrom Theorem 1 and Theorem 2, we can see a tight connection between the Fourier expansion and decision trees. Notice that the Fourier representation complements the classical way of approximating weighted functions in compact forms exploiting (conditional) independencies. To see this, suppose there is a decision tree of the same structure as in Figure 1, but has depth d. According to Theorem 1, it can be represented exactly with Fourier coefficients up to degree d. In this example, the number of non-zero Fourier coefficients is O(22d). Nonetheless, no two variables in figure 1 are independent with each other. Therefore, it\u2019s not possible to decompose this factor into a product of smaller factors (exploiting independencies). Notice that the full contingency table representation of this factor has O(22 d ) entries.\nIf we are willing to accept an approximate representation, low degree Fourier coefficients can capture an even wider class of functions. We follow the standard notion of -concentration: Definition 2. The Fourier spectrum of f : {\u22121, 1}n \u2192 R is -concentrated on degree up to k if and only ifW>k[f ] = \u2211 S\u2286[n],|S|>k f\u0302(S) 2 < .\nWe say a CNF (DNF) formula has bounded width d if and only if every clause (term) of the CNF (DNF) has at most d literals. Linial [14] proved the following key result: Theorem 3 (Linial). Suppose f : {\u22121, 1}n \u2192 {\u22121, 1} is computable by a DNF (or CNF) of width w, then f \u2019s Fourier spectrum is -concentrated on degree up to O(w log(1/ )).\nLinial\u2019s result demonstrates the power of Fourier representations, since bounded width CNF\u2019s (or DNF\u2019s) include a very rich class of functions. Interestingly, the bound does not depend on the number of clauses, even though the clause-variable ratio is believed to characterize the hardness of satisfiability problems.\nAs a contribution of this paper, we extend Linial\u2019s results to a class of weighted probabilistic graphical models, which are contractive with gap 1 \u2212 \u03b7 and have bounded width w. To our knowledge, our extension from the deterministic case to the probabilistic case is novel.\nDefinition 3. Suppose f(x) : {\u22121, 1}n \u2192 R+ is a weighted function, we say f(x) has bounded widthw iff |x| \u2264 w. We say f(x) is contractive if with gap 1\u2212\u03b7 if and only if (1) for all x, f(x) \u2264 1; (2) maxx f(x) = 1; (3) if f(x0) < 1, then f(x0) \u2264 \u03b7.\nThe first and second conditions are mild restrictions. For a graphical model, we can always rescale each factor properly to ensure its range is within [0, 1] and the largest element is 1. The approximation bound we are going to prove depends on the gap 1\u2212\u03b7. Ideally, we want \u03b7 to be small. The class of contractive functions with gap 1 \u2212 \u03b7 still captures a wide class of interesting graphical models. For example, it captures Markov Logic Networks [21], when the weight of each clause is large. The main theorem we are going to prove is as follows:\nTheorem 4. (Main) Suppose f(x) = \u220fm i=1 fi(xi), in which every fi is contractive function with width w and gap 1 \u2212 \u03b7, then f \u2019s Fourier spectrum is -concentrated on degree up to O(w log(1/ ) log\u03b7 ) when \u03b7 < 1 and O(w log(1/ )) when \u03b7 = 1.\nThe proof of theorem 4 relies on the notion of random restriction and our extension to the Hastad\u2019s Switching Lemma[9] to the class of weighted functions defined above.\nDefinition 4. Let f(x) : {\u22121, 1}n \u2192 R and J be subset of all the variables x1, . . . , xn. Let z be an assignment to remaining variables J = {\u22121, 1}n \\ J . Define f |J|z : {\u22121, 1}J \u2192 R to be the restricted function of f on J by setting all the remaining variables in J according to z.\nDefinition 5. (\u03b4-random restriction) For a function f(x) : {\u22121, 1}n \u2192 R, a \u03b4-random restriction of f is defined as f |J|z, when elements in J is selected randomly with probability \u03b4, and z is formed by randomly setting variables in J to either \u22121 or 1. We also say J |z is a \u03b4-random restrict set.\nWith these definitions, we proved our weighted extension to the Hastad\u2019s Switching Lemma: Lemma 1. (Weighted Hastad\u2019s Switching Lemma) Suppose f(x) = \u220fm i=1 fi(xi), in which every fi is contractive function with width w and gap 1\u2212 \u03b7. Suppose J |z is a \u03b4-random restrict set, then\nPr ( \u2203 decision tree h with depth t, ||h\u2212 fJ|z||\u221e \u2264 \u03b3 ) \u2265 1\u2212 1\n2\n( \u03b4\n1\u2212 \u03b4 8uw\n)t .\nin which u = dlog\u03b7 \u03b3e+ 1 if \u03b7 < 1 or u = 1 if \u03b7 = 1 and ||.||\u221e means max |.|.\nThe formal proof of Lemma 1 is based on a clever generalization of the proof by Razborov for the unweighted case [20], and is deferred to the supplementary materials. At a high level, the general idea of the proof is as follows. We call J |z fix factor fi into a \u201ccompressing\u201d branch, if under such restriction, ||fi|J|z||\u221e \u2264 \u03b7. Call J |z a bad restriction set, if there are no decision tree h with depth t, such that ||h \u2212 f |J|z||\u221e \u2264 \u03b3. Intuitively, if a random restriction set is bad, it can at most fix u different factors to their \u201ccompressing\u201d branches, since otherwise, the u + 1 factor will render ||f |J|z||\u221e \u2264 \u03b3, and a trivial all-zero decision tree h0 satisfies ||h0 \u2212 f |J|z||\u221e \u2264 \u03b3. Because it is highly unlikely that a random restrict set only fixes at most u factors to their \u201ccompressing\u201d branches but leave other factors open, we can conclude that there cannot be too many bad restrictions, in which case we obtain a bound on the probability of encountering a bad restriction set.\nLemma 2. Suppose f(x) : {\u22121, 1}n \u2192 R and |f(x)| \u2264 1. J |z is a \u03b4-random restrict set. t \u2208 N+, \u03b3 > 0 and let 0 = Pr{\u00ac\u2203 decision tree h with depth t such that ||f |J|z \u2212 h||\u221e \u2264 \u03b3}, then the Fourier spectrum of f is 4 ( 0 + (1\u2212 0)\u03b32 ) -concentrated on degree up to 2t/\u03b4. Proof. We first bound EJ|z [\u2211 S\u2286[n],|S|>t f\u0302 |J|z(S)2 ] . According to Lemma 1, with probability 1\u2212 0, there is a decision tree h with depth t such that ||f |J|z(x)\u2212 h(x)||\u221e \u2264 \u03b3. In this scenario,\u2211 S\u2286[n],|S|>t f\u0302 |J|z(S)2 = \u2211 S\u2286[n],|S|>t ( f\u0302 |J|z(S)\u2212 h\u0302(S) )2 . (1)\nThis is because due to Theorem 1, h\u0302(S) = 0 for all S such that |S| > t. Because |f |J|z(x)\u2212h(x)| \u2264 \u03b3 for all x, hence the right side of Equation 1 must satisfy\u2211\nS\u2286[n],|S|>t\n( f\u0302 |J|z(S)\u2212 h\u0302(S) )2 \u2264 \u2211 S\u2286[n] ( f\u0302 |J|z(S)\u2212 h\u0302(S) )2 = E [( f |J|z(x)\u2212 h(x) )2] \u2264 \u03b32. (2)\nThe second to the last equality of Equation 2 is due to the Parseval\u2019s Identity. With probability 0, there are no decision trees close to f |J|z. However, because |f |J|z| \u2264 1, we must have\u2211 S\u2286[n],|S|>t f\u0302 |J|z(S)2 \u2264 1. Summarizing these two points, we have:\nEJ|z  \u2211 S\u2286[n],|S|>t f\u0302 |J|z(S)2  \u2264 (1\u2212 0)\u03b32 + 0.\nUsing a known result EJ|z [ f\u0302 |J|z(S)2 ] = \u2211 U\u2286[n] Pr{U \u2229 J = S} \u00b7 f\u0302(U)2, we have:\nEJ|z  \u2211 S\u2286[n],|S|>t f\u0302 |J|z(S)2  = \u2211 S\u2286[n],|S|>t EJ|z [ f\u0302 |J|z(S)2 ] = \u2211 U\u2286[n] Pr{|U \u2229 J | > t} \u00b7 f\u0302(U)2.\nThe distribution of random variable |U \u2229J | is Binomial(|U |, \u03b4). When |U | \u2265 2t/\u03b4, this variable has mean at least 2t, using Chernoff bound, Pr{|U \u2229 J | \u2264 t} \u2264 (2/e)t < 3/4. Therefore,\n(1\u2212 0)\u03b32 + 0 \u2265 \u2211 U\u2286[n] Pr{|U \u2229 J | > t} \u00b7 f\u0302(U)2 \u2265 \u2211 U\u2286[n],|U |\u22652t/\u03b4 Pr{|U \u2229 J | > t} \u00b7 f\u0302(U)2\n\u2265 \u2211\nU\u2286[n],|U |\u22652t/\u03b4\n( 1\u2212 3\n4\n) \u00b7 f\u0302(U)2.\nWe get our claim \u2211 |U |\u22652t/\u03b4 f\u0302(U) 2 \u2264 4((1\u2212 0)\u03b32 + 0). Proof. Now we are ready to prove Theorem 4. Firstly suppose \u03b7 < 1, choose \u03b3 = \u221a /8, which ensures 4(1\u2212 0)\u03b32 \u2264 1/2 \u00b7 . Next choose \u03b4 = 1/(16uw + 1), t = C log(1/ ), which ensures\n0 = 1\n2\n( \u03b4\n1\u2212 \u03b4 8uw\n)t = 1\n2 C .\nChoose C large enough, such that 4 \u00b7 1/2 \u00b7 C \u2264 1/2 \u00b7 . Now we have 4((1\u2212 0)\u03b32 + 0) \u2264 . At the same time, 2t/\u03b4 = C log(1/ )(16uw + 1) = O(w log(1/ ) log\u03b7 ).\n\u03b7 = 1 case is similar to the classical CNF (or DNF) case."}, {"heading": "4 Variable Elimination in Fourier Domain", "text": "We propose an inference algorithm which works like the classic Variable Eliminiation (VE) Algorithm, except for passing messages represented in the Fourier domain.\nThe classical VE algorithm consists of two basic steps \u2013 the multiplication step and the elimination step. The multiplication step takes f and g, and returns f \u00b7 g, while the elimination step sums out one variable xi from f by returning \u2211 xi f . Hence, the success of VE procedure in Fourier domain depends on efficient algorithms to carry out the aforementioned two steps. A naive approach is to transform the representation back to the value domain, carry out the two steps there, then transform it back to Fourier space. While correct, this strategy would eliminate all the benefits of Fourier representations.\nLuckily, the elimination step can be carried out in Fourier domain in a rather straightforward way:\nTheorem 5. Suppose f has a Fourier expansion: f(x) = \u2211 S\u2286[n] f\u0302(S)\u03c7S(x), x0 \u2208 x is a variable.\nThen the Fourier expansion for f \u2032 = \u2211 x0 f is: \u2211 S\u2286[n] f\u0302 \u2032(S)\u03c7S(x), where f\u0302 \u2032(S) = 2f\u0302(S) if x0 6\u2208 S and f\u0302 \u2032(S) = 0 if x0 \u2208 S.\nFrom Theorem 5, one only needs a linear scan of all the Fourier coefficients of f in order to compute the Fourier expansion for \u2211 x0 f . Suppose f has m non-zero coefficients in its Fourier representation, then this linear scan takes time O(m).\nThere are several ways to implement the multiplication step. The first option is to use school book multiplication. To multiply functions f and g directly in Fourier domain, one multiplies every pair of their Fourier coefficients, and then combines similar terms. If f and g have mf and mg terms in their Fourier representations respectively, this operation takes time O(mfmg). Because we are working on models in which exact inference is intractable, sometimes we need truncate the Fourier representation to prevent a exponential explosion. We find it is a good idea to keep Fourier coefficients with large absolute values, compared with always keeping ones with low degree. Even though we proved results that most Fourier weights are concentrated on low degree coefficients, keeping coefficients with large absolute values give us extra flexibility, especially when the whole graphical model is dominated by a few key variables.\nAs a second option, one can convert f and g to their value domain, multiply corresponding entries in the their value domain, and then convert the result back to the Fourier domain. Suppose the union of the domains of f and g has n variables, then the conversion between the two domains dominates the complexity, which is O(n \u00b7 2n). Nonetheless, when f and g are relatively dense, this method could have a better time complexity than the school book multiplication. In our implementation, we trade the complexity between the aforementioned two options, and always use the one with lower time complexity."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Weight Concentration on Low Degree Coefficients", "text": "We first validate our theoretical results on the sparsity of Fourier representations and the weight concentration on low-degree coefficients. We choose to evaluate our results on random weighted 3-SAT instances with 20 variables. We choose small instances because we need compute the full Fourier spectrum. The weighted 3-SAT instances is specified by a CNF and a weight \u03b7. Each factor corresponds to a clause in the CNF. When the clause is satisfied, the corresponding factor evaluates to 1, otherwise evaluates to \u03b7. For each setting, we random generate 100 instances. For each instance, we compute the squared sum weight at each degree: Wk[f ] = \u2211 S\u2286[n],|S|=k f\u0302(S)\n2. Figure 2 shows the median value of the squared sum weight over 100 instances. As seen from the figure, the weights are concentrated on low degree coefficients, regardless of \u03b7."}, {"heading": "5.2 The Performance When Applying to Variable Elimination", "text": "We integrate the Fourier representation into the variable elimination algorithm, and evaluate its performance as an approximate probabilistic inference scheme to estimate the partition function of undirected graphical models. Our main comparison is against Mini-Bucket Elimination, since the two algorithms are of the same type, with the only difference being the way in which the messages are approximated. We obtained the source code from the author of Mini-Bucket Elimination, which includes sophisticated heuristics for splitting factors. The versions we obtained are used for Maximum A Posteriori Estimation (MAP). We augment this version to compute the partition function by replacing the maximization operators by summation operators. We also compare our VE algorithm with MCMC and Loopy Belief Propagation. We implemented the classical Ogata-Tanemura scheme [19] to estimate the partition function in MCMC. We use the implementation in LibDAI [17] for belief propagation.\nWe first compare on small instances for which we can compute ground truth using Ace [3]. To do this, we run on 15-by-15 Ising models with mixed coupling strengths and various field strengths. We run 20 instances for each coupling strength. For a fair comparison, we fix the size of the messages for both Fourier VE and Mini-bucket to 1,024. Figure 3 shows the result. Clearly the Fourier VE Algorithm outperforms the MCMC and the Mini-bucket Elimination. It also outperforms the Belief Propagation when the field strength is relatively strong.\nNext we evaluate their performance on a synthetically generated benchmark. For one instance of this benchmark, we randomly generate factors of size 3 with low coupling weights. We then add a backdoor structure to each instance, by enforcing coupling factors of size 3 in which the 3 variables of the factor must take the same value. For these instances, we can compute the expected value of the partition function and compare it with the output of the algorithms. We report the results on Figure 4. Here the message size for Fourier VE is 1,048,576. However, when doing multiplications, we will shrink the message size to 1,024, since the complexity of school book multiplication is quadratic in the number of coefficients. The Mini-bucket approach is not reported, as it performs very poorly on these instances. These results show that the Fourier approach outperforms both mcmc and bp, and suggest that it can perform arbitrarily better than both approaches as the size of the backdoor increases."}, {"heading": "6 Conclusion", "text": "In this paper, we explore a novel way to exploit compact representations of high-dimensional probability distributions in approximate probabilistic inference algorithms. Our approach is based on discrete Fourier Representation of weighted Boolean Functions, complementing the classical method to exploit conditional independence between the variables. We show that a large class of weighted probabilistic graphical models have a compact Fourier representation. This theoretical result opens up an entirely new way of approximating a probability distribution. We demonstrate the significance of this approach by applying it to the variable elimination algorithm and comparing the results with the bucket representation and other approximate inference algorithms, obtaining very encouraging results."}, {"heading": "7 Supplementary Materials", "text": ""}, {"heading": "7.1 Proof of Lemma 1", "text": "Let Rln be the collection of restrictions on n Boolean variables x1, . . . , xn. Each restriction in R l n leaves a set of l variables J = {xi1 , . . . , xil} open, while fix all other variables xi 6\u2208 J to either -1 or 1. It is easy to see that the size of Rln is given by:\n|Rln| = ( n\nl\n) \u00b7 2n\u2212l. (3)\nFor a restriction J |z \u2208 Rln, call J |z bad if and only if for all decision tree h with depth t, there exists at least one input xJ , such that |h(xJ) \u2212 f |J|z(xJ)| > \u03b3. Let Bln be the set of all bad restrictions, ie: Bln = {J |z \u2208 Rln : J |z is bad}. To prove the lemma, it is sufficient to prove that\n|Bln| |Rln| \u2264 1 2\n( l\nn\u2212 l 8uw\n)t . (4)\nIn the proof that follows, for every bad restriction \u03c1 \u2208 Bln, we establish a bijection between \u03c1 and (\u03be, s), in which \u03be is a restriction in Rl\u2212tn and s is a certificate from a witness set A. In this case, the number of distinct \u03c1\u2019s is bounded by the number of (\u03be, s) pairs:\n|Bln| \u2264 |Rl\u2212tn | \u00b7 |A|. (5)\nFor a restriction \u03c1, we establish the canonical decision tree for f |\u03c1 under precision \u03b3 as follows:\n1. We start with a fixed order for the variables and another fixed order for the factors. 2. If f |\u03c1 is a constant function, or ||f |\u03c1||\u221e \u2264 \u03b3, stop. 3. Otherwise, under restriction \u03c1, some factors evaluate to fixed values (all variables in these\nfactors are fixed or there are free variables, but all assignments to these free variables lead to value 1), while other factors do not. Examine the factors according to the fixed factor order until reaching the first factor that still does not evaluate to a fixed value.\n4. Expand on the open variables of this factor, under the fixed variable order specified in step 1. The result will be a tree (The root branch is for the first open variable. The branches in the next level is for the second open variable, etc).\n5. Each leaf of this tree corresponds to f |\u03c1\u03c01 , in which \u03c01 is a value restriction for all open variables of the factor. Recursively apply step 2 to 5 for function f |\u03c1\u03c01 , until the condition in step 2 holds. Then attach the resulting tree to this leaf.\nFigure 5 provides a graphical demonstration of a canonical decision tree.\nNow suppose restriction \u03c1 is bad. By definition, for any decision tree of depth t, there exists at least one input x, such that |h(x) \u2212 f |\u03c1(x)| > \u03b3. The canonical decision tree is no exception. Therefore, there must be a path l in the canonical decision tree of f |\u03c1, which has more than t variables. Furthermore, these t variables can be split into k (1 \u2264 k \u2264 t) segments, each of which corresponds to one factor. Let fi (i \u2208 {1, . . . , k}) be these factors, and let \u03c0i be the assignments of the free variables for fi in path l. Now for each factor fi, by the definition of the canonical decision tree, under the restriction \u03c1\u03c01 . . . \u03c0i\u22121, fi|\u03c1\u03c01 . . . \u03c0i\u22121 must have a branch whose value is no greater than \u03b7 (otherwise fi|\u03c1\u03c01 . . . \u03c0i\u22121 all evaluates to 1). We call this branch the \u201ccompressing\u201d branch for factor fi|\u03c1\u03c01 . . . \u03c0i\u22121. Let the variable assignment which leads to this compressing branch for fi|\u03c1\u03c01 . . . \u03c0i\u22121 be \u03c3i. Let \u03c3 = \u03c31 . . . \u03c3k. Then we map the bad restriction \u03c1 to \u03c1\u03c3 and an auxiliary advice string that we are going to describe.\nIt is self-explanatory that we can map from any bad restriction \u03c1 to \u03c1\u03c3. The auxiliary advice is used to establish the backward mapping, ie, the mapping from \u03c1\u03c3 to \u03c1. When we look at the result of f |\u03c1\u03c3, we will notice that at least one factor is set to its compressing branch (because we set f1 to its compressing branch in the forward mapping). Now there could be other factors set at their compressing branches (because of \u03c1), but an important observation is that: the number of factors at\ntheir compressing branches cannot exceed u = dlog\u03b7 \u03b3e + 1, because otherwise, the other u \u2212 1 factors already render ||f |\u03c1||\u221e \u2264 \u03b3, and the canonical decision tree should stop on expanding this branch. We therefore could record the index number of f1 out of the all factors which are fixed at their compressing branches in the auxiliary advice string, so we can find f1 in the backward mapping. Notice that this index number will be between 1 and u, so it takes log u bits to store it.\nNow with the auxiliary information, we can identify which factor is f1. The next task is to identify which variables in f1 are fixed by \u03c1, and which are fixed by \u03c31. Moreover, if one variable is fixed by \u03c31, we would like to know its correct values in \u03c01. To do this, we introduce additional auxiliary information: for each factor fi, suppose it has ri free variables under restriction fi|\u03c1\u03c01 . . . \u03c0i\u22121, we use ri integers to mark the indecies of these free variables. Notice that every integer of this type is between 1 and w (therefore can be stored in logw bits), since each fi is with width at most w. Also, it requires t integers of this type in total to keep this information, because we have t free variables in total for f1, . . . , fk.\nNotice that it is not sufficient to keep these integers. We further need k \u2212 1 separators, which tell which integer belongs to which factor fi. Aligning these integers in a line, we need k\u2212 1 separators to break the line into k segments. These separators can be represented by t\u22121 bits, in which the i-th bit is 1 if and only if there is a separator between the i-th and (i+1)-th integer (we have t integers at most). With these two pieces of information, we are able to know the locations of free variables set by \u03c3i for each factor fi.\nWe further need to know the values for each variable in \u03c0i. Therefore, we add in another t-bit string, each bit is either 0 or 1. 0 means the assignment of the corresponding variable in \u03c0i is the same as the one in \u03c3i, 1 means the opposite.\nWith all this auxiliary information, we can start from \u03c1\u03c3, find the first factor f1, further identify which variables are set by \u03c31 in f1, and set its correct values in \u03c01. Then we start with f |\u03c01, we can find \u03c02 in the same process, and continue. Finally, we will find all variables in \u03c3 and back up the original restriction \u03c1.\nNow to count the length of the auxiliary information, the total length is t log u + t logw + 2t \u2212 1 bits. Therefore, we can have a one-to-one mapping between elements in Bln and R l\u2212t n \u00d7A, in which the size of A is bounded by 2t log u+t logw+2t\u22121 = (uw)t \u00b7 22t\u22121.\nIn all,\n|Bln| |Rln|\n\u2264 ( n l\u2212t ) 2n\u2212l+t(uw)t \u00b7 22t\u22121(\nn l\n) 2n\u2212l\n(6)\n=\n( n l\u2212t ) 1 2 (8uw)\nt( n l ) (7) =\nl(l \u2212 1) . . . (l \u2212 t+ 1) (n\u2212 l + 1) . . . (n\u2212 l + t) 1 2 (8uw)t (8)\n\u2264 1 2\n( l\nn\u2212 l 8uw\n)t . (9)"}, {"heading": "7.2 Proof of Theorem 2", "text": "For each term in the Fourier expansion whose degree is less than or equal to d, we can treat this term as a weighted function involving less than or equal to d variables. Therefore, it can be represented by a decision tree, in which each path of the tree involves no more than d variables (therefore the tree is at most at the depth of d). Because f is represented as the sum over a set of Fourier terms up to degree d, it can be also represented as the sum of the corresponding decision trees."}, {"heading": "7.3 Proof of Theorem 5", "text": "Let the Fourier expansion of f be: f(x) = \u2211 S f\u0302(S)\u03c7S(x), we have:\nf \u2032(x \\ x0) = f(x \\ x0, x0 = +1) + f(x \\ x0, x0 = \u22121) (10) = \u2211\nS:x0\u2208S f\u0302(S) \u00b7 \u03c7S\\x0(x \\ x0) \u00b7 1 + \u2211 S:x0\u2208S f\u0302(S) \u00b7 \u03c7S\\x0(x \\ x0) \u00b7 (\u22121) (11)\n+ \u2211\nS:x0 6\u2208S\nf\u0302(S) \u00b7 \u03c7S\\x0(x \\ x0) + \u2211\nS:x0 6\u2208S\nf\u0302(S) \u00b7 \u03c7S\\x0(x \\ x0) (12)\n= \u2211\nS:x0 6\u2208S\n2 \u00b7 f\u0302(S) \u00b7 \u03c7S\\x0(x \\ x0). (13)"}], "references": [{"title": "On Learning Monotone Boolean Functions", "author": ["Avirim Blum", "Carl Burch", "John Langford"], "venue": "In FOCS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "On sparse, spectral and other parameterizations of binary probabilistic models", "author": ["David Buchman", "Mark W. Schmidt", "Shakir Mohamed", "David Poole", "Nando de Freitas"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A knowledge compilation", "author": ["Adnan Darwiche", "Pierre Marquis"], "venue": "map. J. Artif. Int. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Mini-buckets: A general scheme for generating approximations in automated reasoning", "author": ["Rina Dechter"], "venue": "In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Mini-bucket elimination with moment matching", "author": ["Natalia Flerova", "Er Ihler", "Rina Dechter", "Lars Otten"], "venue": "NIPS Workshop DISCML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Recursive decomposition for nonconvex optimization", "author": ["Abram L. Friesen", "Pedro Domingos"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A complete anytime algorithm for treewidth", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Structured message passing", "author": ["Vibhav Gogate", "Pedro M. Domingos"], "venue": "In UAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Computational Limitations of Small-depth Circuits", "author": ["Johan H\u00e5stad"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1987}, {"title": "On the partition function and random maximum aposteriori perturbations", "author": ["Tamir Hazan", "Tommi S. Jaakkola"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Join-graph based costshifting schemes", "author": ["Alexander T. Ihler", "Natalia Flerova", "Rina Dechter", "Lars Otten"], "venue": "In UAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Amazon.com recommendations: Item-to-item collaborative filtering", "author": ["Greg Linden", "Brent Smith", "Jeremy York"], "venue": "IEEE Internet Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Constant depth circuits, fourier transform, and learnability", "author": ["Nathan Linial", "Yishay Mansour", "Noam Nisan"], "venue": "J. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Learning Boolean functions via the Fourier transform. advances in neural computation and learning, 0:1\u201328", "author": ["Yishay Mansour"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Join-graph propagation algorithms", "author": ["Robert Mateescu", "Kalev Kask", "Vibhav Gogate", "Rina Dechter"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "libDAI: A free and open source C++ library for discrete approximate inference in graphical models", "author": ["Joris M. Mooij"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Some topics in analysis of boolean functions", "author": ["Ryan O\u2019Donnell"], "venue": "Proceedings of the fourtieth annual ACM symposium on Theory of computing - STOC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Estimation of interaction potentials of spatial point patterns through the maximum likelihood procedure", "author": ["Yosihiko Ogata", "Masaharu Tanemura"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1981}, {"title": "Bounded arithmetic and lower bounds in boolean complexity", "author": ["Alexander A. Razborov"], "venue": "In Feasible Mathematics II, Progress in Computer Science and Applied Logic", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Tightening lp relaxations for map using message passing", "author": ["David Sontag", "Talya Meltzer", "Amir Globerson", "Tommi Jaakkola", "Yair Weiss"], "venue": "In UAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].", "startOffset": 195, "endOffset": 210}, {"referenceID": 11, "context": "Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].", "startOffset": 195, "endOffset": 210}, {"referenceID": 12, "context": "Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].", "startOffset": 195, "endOffset": 210}, {"referenceID": 20, "context": "Similar ideas have been considered in the analysis of Boolean functions and logical forms [4], as well as in physics with low rank tensor decompositions and matrix product states representations [6, 12, 13, 22].", "startOffset": 195, "endOffset": 210}, {"referenceID": 3, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 4, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 15, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 7, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 2, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 10, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 9, "context": "Numerous approximate and exact inference algorithms are based on this idea [4, 5, 16, 8, 23, 3, 11, 10].", "startOffset": 75, "endOffset": 103}, {"referenceID": 17, "context": "The Fourier representation has found numerous recent applications in PAC learning [18, 15, 1, 2], but these ideas have not been fully exploited in the fields of probabilistic inference and graphical models.", "startOffset": 82, "endOffset": 96}, {"referenceID": 14, "context": "The Fourier representation has found numerous recent applications in PAC learning [18, 15, 1, 2], but these ideas have not been fully exploited in the fields of probabilistic inference and graphical models.", "startOffset": 82, "endOffset": 96}, {"referenceID": 0, "context": "The Fourier representation has found numerous recent applications in PAC learning [18, 15, 1, 2], but these ideas have not been fully exploited in the fields of probabilistic inference and graphical models.", "startOffset": 82, "endOffset": 96}, {"referenceID": 1, "context": "The Fourier representation has found numerous recent applications in PAC learning [18, 15, 1, 2], but these ideas have not been fully exploited in the fields of probabilistic inference and graphical models.", "startOffset": 82, "endOffset": 96}, {"referenceID": 13, "context": "However, a rather surprising fact which was first discovered by Linial [14] is that factors corresponding to fairly general classes of logical forms admit a compact Fourier representation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "The proof extends the famous Hastad\u2019s Switching Lemma[9] to the weighted case.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "The complexity of the VE algorithm depends on the size of largest factors generated during the elimination process, and is known to be exponential in the tree-width [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Detcher proposed the Mini-bucket Elimination Algorithm [4], which dynamically decomposes and approximates factors (when the domain of a product exceeds a threshold) with the product of smaller factors during the elimination process.", "startOffset": 55, "endOffset": 58}, {"referenceID": 17, "context": "One classical result [18] states that if a function can be captured by a decision tree with depth d, then it can be represented with Fourier coefficients up to degree d: Theorem 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Linial [14] proved the following key result: Theorem 3 (Linial).", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "For a graphical model, we can always rescale each factor properly to ensure its range is within [0, 1] and the largest element is 1.", "startOffset": 96, "endOffset": 102}, {"referenceID": 8, "context": "The proof of theorem 4 relies on the notion of random restriction and our extension to the Hastad\u2019s Switching Lemma[9] to the class of weighted functions defined above.", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "The formal proof of Lemma 1 is based on a clever generalization of the proof by Razborov for the unweighted case [20], and is deferred to the supplementary materials.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "We implemented the classical Ogata-Tanemura scheme [19] to estimate the partition function in MCMC.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "We use the implementation in LibDAI [17] for belief propagation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "We first compare on small instances for which we can compute ground truth using Ace [3].", "startOffset": 84, "endOffset": 87}], "year": 2017, "abstractText": "Probabilistic inference is a key computational challenge in statistical machine learning and artificial intelligence. The ability to represent complex high dimensional probability distributions in a compact form is the most important insight in the field of graphical models. In this paper, we explore a novel way to exploit compact representations of highdimensional probability distributions in approximate probabilistic inference algorithms. Our approach is based on discrete Fourier Representation of weighted Boolean Functions, complementing the classical method to exploit conditional independence between the variables. We show that a large class of probabilistic graphical models have a compact Fourier representation. This theoretical result opens up an entirely new way of approximating a probability distribution. We demonstrate the significance of this approach by applying it to the variable elimination algorithm and comparing the results with the bucket representation and other approximate inference algorithms, obtaining very encouraging results.", "creator": "LaTeX with hyperref package"}}}