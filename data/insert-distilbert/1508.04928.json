{"id": "1508.04928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis", "abstract": "analysis of sequential event data has been recognized firstly as one of the essential tools in unified data modeling and analysis field. in this paper, after the examination of its technical requirements and issues to model complex but practical situation, we propose a new sequential data model, dubbed duration and interval hidden markov model ( di - hmm ), that efficiently represents \" state duration \" and \" state interval \" outputs of data events. this has significant implications to play an important role in representing practical time - series linear sequential decision data. typically this eventually provides an online efficient and flexible sequential analog data retrieval. numerical experiments on separating synthetic and real data demonstrate increasing the efficiency and accuracy of the proposed di - hmm.", "histories": [["v1", "Thu, 20 Aug 2015 09:09:45 GMT  (1420kb,D)", "http://arxiv.org/abs/1508.04928v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hiromi narimatsu", "hiroyuki kasai"], "accepted": false, "id": "1508.04928"}, "pdf": {"name": "1508.04928.pdf", "metadata": {"source": "CRF", "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis", "authors": ["Hiromi Narimatsu", "Hiroyuki Kasai"], "emails": ["narimatsu@appnet.is.uec.ac.jp", "kasai@is.uec.ac.jp"], "sections": [{"heading": null, "text": "Keywords\u2014Sequential data analysis; HMM; HSMM; State duration; State interval\nI. INTRODUCTION\nIn the context of social science research, the analysis of sequential event data has been studied extensively. These studies have explored broad technical areas that range from biological data analysis to speech recognition, image classification, human behavior recognition, and time-series data analysis. Esmaeili et al. [1] categorize three types of sequential pattern after theoretical investigation for large amounts of data. Lewis et al. [2] propose a sequential algorithm using special queries to train text classifiers. Song et al. [3] propose a sequential clustering algorithm for gene data. More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6]. Those devices enable users to record all of their experiences such as what is viewed, what is heard, and what is noticed. Nevertheless, although collecting all observed data has become much easier, it remains difficult to immediately find the data that we want to access because the amount of time series data is extremely huge. In case of life log data application, for example, it must be much easy to exactly retrieve information of particular places or dates if rich and comprehensive meta-data are sufficiently attached to every piece of datum to be identified. However, if a query is very ambiguous like retrieving a situation similar to the current situation, it must be surely challenging to obtain meaningful results at the end. Thus, finding such similar sequential patterns from vast sequential data using a target pattern extracted from the current situation is of crucial importance. This is of interest in the present paper.\nFinding similar sequential patterns needs to discriminate particular sequential patterns from many partial groups of patterns. There are some useful methods for detecting similar partial patterns from sequential data. One traditional but representative method is Dynamic Programming (DP) matching algorithm that is typically used for speech and natural language processing. However, because practical sequential data always contain time misalignments of events, the sequential pattern detector must support a naive extraction mechanism, namely the algorithm needs to extract the sequential patterns that have with not only the precisely same duration and same interval of events, but also those with the slightly different length of duration and/or the slightly different interval. An alternative modeling category is a statistical model of which representatives are Support Vector Machine (SVM) and Hidden Markov Model (HMM). SVM is introduced in the area of statistical learning and is used for nonlinear classifications such as image classification. Although SVM is powerful to classify data based on the similarity of each feature, it is not specialized for sequential data. Meanwhile, HMM is specialized to deal with sequential data by exploiting transition probability between states, i.e., events. Consequently, we dedicate solely to HMM and its extended methods in this study.\nThe primary contributions of our work are two-fold: (a) we advocate that the support of both \u201cstate duration\u201d and \u201cstate interval\u201d is of great significance to represent practical sequential data based on an analysis about the feature and structure of sequential data, then extracts requirements for its modeling. Next, (b) we propose a new sequential model by extending Hidden semi-Markov Model (HSMM) to support both of them efficiently. Regarding (a), We especially address the generalization of the requirements, and emphasize the importance of handling event order, continuous \u201cduration\u201d of an event, and discontinuous \u201cinterval\u201d time between two events. Hereinafter, we define the event continuous duration as \u201cstate duration\u201d, and define the discontinuous interval with no observation as \u201cstate interval\u201d, respectively because an event is treated as a state in HMM. Then, with respect to (b), after assessment of the extended HMM methods in the literature against the requirements, we show that all the existing models cannot treat both the state duration and the state interval simultaneously. Nevertheless, we also show that HSMM, one of the extended HMM methods, can handle the state duration, and is an appropriate baseline to be extended to meet all the demands. Subsequently, in the present paper, we propose an extended model of HSMM that accommodates the state ar X iv :1\n50 8.\n04 92\n8v 1\n[ cs\n.A I]\n2 0\nA ug\n2 01\nduration as well as the state interval.\nThe rest of the paper is organized as follows. The next section introduces the related work of sequential data analysis. Section III analyzes the model requirement, and assesses adequacy of extended HMM methods against the requirement. Section V explains the proposed method, Duration and Interval HMM (DI-HMM), and Section VI performs numerical evaluations. Finally, Section VII presents a summary of this paper and describes future work."}, {"heading": "II. RELATED WORK", "text": "This section presents an explanation of related work for sequential data analysis. For sequential pattern matching and detection, DP matching [7] extracts similar sequential patterns from different two sequential patterns. It was firstly used for acoustic speech recognition, but has been now widely applied to various fields such as biological sequences of DNA sequences. For sequential pattern classification, SVM [8] and Probabilistic Graphical Models [9] are proposed. SVM is one of the classification algorithms which produces a model using feature attributes extracted from training data, and calculates the distance between the training data and test data using these attributes. More recently, however, SVM has been extended to handle sequential data classification such as speech recognition and handwriting recognition. Shimodaira et al. propose an extended SVM which enables frame-synchronous recognition of sequential pattern [10]. Probabilistic Graphical Model with directed graph or undirected graph is the typical model to represent sequential patterns. Safari proposes a new model of Deep Learning for sequential pattern recognition [11]. Lastly, HMM [12][13] is a statistical tool for modeling sequence of observations. HMM is regarded as one kind of probabilistic Graphical Model. While HMM is used for many applications, for example, speech recognition, handwriting recognition and activity recognition, most of the extensions of HMM are proposed specialized for individual application data.\nConsidering extraction of not only the exactly same sequential pattern but also the similar sequential pattern of time duration and time interval of events, a statistical-based representational capability is preferably required. To this point of view, although pattern matching algorithms like DP matching concern sequential oder of events, they do not address finding such similar sequential patterns due to the lack of statistical modeling capability. Furthermore, because most of statistical modeling algorithms such as SVM mainly consider feature similarities, they do not also directly take into account the structure of sequential data. Hence, they do not handle temporal order and ambiguity of events to find such similar sequential patterns efficiently. On the other hand, HMM, a statistical model, is powerful to treat the sequential patterns by exploiting probability of event orders by transition probability between states. Therefore, we conclude that HMM is the most suitable model to treat the sequential data, and has potential capability to describe temporal ambiguity to find the similar sequential patterns. As a result, we particularly examine HMM hereafter in this paper.\nMany extended HMM methods have been proposed for respective application data. Some of the extended HMM is specialized for sequential data. Xue et al. propose transition-\nemitting HMMs (TE-HMMs) and state-emitting HMMs (SEHMMs) for treating discontinuous symbols [14]. Their studies are for an off-line handwriting word recognition, and the observation data include discontinuous and continuous parts between characters when writing cursive letters. They address such discontinuous and continuous features, and extend HMM to treat both of them. Bengio et al. propose IO-HMM for gesture recognition that maps input sequences to output sequences during learning whereas the original HMM learns only output sequence distributions [15]. IO-HMM supports a new function of maximum likelihood and parameters for calculating the maximum likelihood extracted from training data which is the pair of input/output sequences. IO-HMM is a hybrid model of generative and discriminative models to treat output probability estimation for both of input sequences and observations. Salzenstein et al. deal with a statistical model based on Fuzzy Markov random chains for image segmentation in the context of stationary and non-stationary data [16]. The model handles multispectral data and estimation of hyperparameters in non-stationary context. Yu et al. propose Explicit-Duration Hidden Markov Model [17]. Addressing state interval between state transition, a new forward-backward algorithm to estimate model parameters is proposed. The model treats the difference of state durations in all the states. Beal et al. propose HMMselftrans that is an extended model of EDM [18]. Furthermore, Yu et al. [19] and Murphy et al. [20] propose HSMM which is a basic model of EDM. Their new model treats the state duration and the number of observations being produced while staying in the state. HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23]. Hence, these application data are especially sequential. Although many extended HMM methods exist, they lack some capabilities to efficiently handle sequential data as the next section explains, more specifically, the capability to handle both the state duration and the state interval between events."}, {"heading": "III. SEQUENTIAL DATA ANALYSIS", "text": "This section presents a description of the model requirements for sequential data analysis, and presents comparison of the requirements and the satisfaction of each extended HMM."}, {"heading": "A. Notations", "text": "The symbols and the marks are defined a priori in this section. First, taking a certain time as t, we consider the sequence of which period is 1 \u2264 t \u2264 T . The observation at time t is represented as ot, and the observation sequence starting at time t = t1 to t = t2 is represented as ot1:t2 = ot1 , \u00b7 \u00b7 \u00b7 , ot2 . The length of ot1:t2 , i.e., |ot1:t2 |, is t2\u2212t1+1. Then, the target observation sequence, which has to be assigned a meaningful label to, starting at time t=1 and ending at t = T is represented as o1:T = o1, \u00b7 \u00b7 \u00b7 , oT , and the set of observable values is V = {v1, \u00b7 \u00b7 \u00b7 , vK}. Next, an elemental state is denoted as selm, and each state selm has a different length of time units defined as delm. In addition, each has one hidden state that belongs to the set of hidden states denoted as S = {S1, \u00b7 \u00b7 \u00b7 , SM}, where the size of the set, i.e., |S|, is M . Furthermore, selm is denoted alternatively by using its starting time t1 and its ending time t2, as st1:t2 . In this case, the time length of st1:t2 is equals to t2\u2212 t1 + 1. On the other hand, a state sequence is\ngenerated from multiple elemental states. This state sequence is represented as st1:t2 in a bold font by specifying its starting time t1 and its ending time t2. The n-th constitutional state in st1:t2 is denoted as sn of which duration is dn. For instance, the first state is represented as s1; the duration of s1 is d1. In particular, the state sequence corresponding to the entire period to be considered, namely from time t = 1 to t = T , is denoted as s1:T = s1, \u00b7 \u00b7 \u00b7 , sN , where N is the total number of si. It should be noted that the definition of the state sequence is similar to that of the observation sequence, but the number of constitutional states, i.e., |s1:T |, is not equals to T because each state may have a different time length as explained above."}, {"heading": "B. Requirement for Model Description", "text": "This section presents discussion of the requirements for the sequential data model by using time-series data: representative data of sequential data, as shown in Figure 1. Suppose that two different sequences where successive states, i.e., events, are observed from two different sensors. A state is represented as a block of which width represents its continuous state duration. Furthermore, those events are not continuously observed, that is, a discontinuous interval time between two observed states may exist. Thus, the length of this unobserved period is represented by distance between two successive blocks. On the other hand, the gray-colored state in Figure 1 illustrates the extracted states sequence, s1:T , that forms one group. The group of state sequence, s1:T , in this figure consists of four states: s1, s2, s3, and s4.\nTaking into account of the example above, we now investigate the requirements for the sequential data model. First, it goes without saying that the states in Figure 1 are observed in a prescribed order. This must be described in the model (R1). Next, in some cases, several states can be observed in a partially overlapped manner as s2 and s3. In other words, multiple states might occur simultaneously at a certain period. Therefore, such overlapped multiple states must be represented in the model (R2). Furthermore, the time lengths of respective states mutually differ. This requires the model to express a \u2018state duration\u2019 in a model (R3). Finally, for the case in which two states do not occur in a series without time gap, a vacant time between one state and another state that is not involved in the group of sequence might exist between two states. Additionally, the time length of this vacant time can be variable. Therefore, the \u2018state interval\u2019 between two states in the model must be described (R4). Consequently, we conclude the sequential data model needs to describe the items below.\nR1 State order\nR2 Staying multiple states in a certain time\nR3 State duration\nR4 State interval\nIt should be noted that R2 has a different characteristic from other items because R1, R3, and R4 are the requirements for a single sequence whereas R2 is particular for multiple sequences. Therefore, we specially examine requirements R1, R3, and R4 in this study. The examination of R2 shall be left for advanced research to be undertaken in future work."}, {"heading": "C. Requirement Assessment for Extended HMM Methods", "text": "This section assesses whether HMM and extended HMM methods meet the requirements analyzed in the previous section. Table I presents a comparison of the conventional HMM methods from the viewpoints of the model requirements. The basic HMM represents the order of the states, and all the extended HMM methods inherit this capability. IO-HMM treats the state duration but the target of estimation is different. Therefore, it does not satisfy the model requirements. HSMM is proposed to model the remaining time length to stay in the same state. In addition, HMM-selftrans and EDM are the extended models of HSMM. These methods satisfy the same requirements: the state order and the state duration, but do not support the state interval. As a result of requirement assessment, we find that no model can accommodate both the state duration and the state interval simultaneously. Nevertheless, HSMM expresses the state duration, and this capability is not supported by any other methods. Therefore, we conclude that HSMM is the best starting model for extension to our new model. The next subsection explains the general HSMM."}, {"heading": "IV. HIDDEN SEMI-MARKOV MODEL (HSMM)", "text": "HSMM is an extended model of conducting HMM using a semi-Markov chain with a variable staying duration for each state [17]. The crucial difference between HMM and HSMM is the number of observations per state. HSMM treats the duration of staying at one state by introducing an additional parameter specialized for describing the state duration when calculating the transition and emission probabilities. Figure 2 illustrates the concept of HSMM to handle the state duration in each state when s1, s2, and sN are the super state nodes. Suppose that the state duration of s1 is d1, and {o1, o2, \u00b7 \u00b7 \u00b7 , od1} is the set of emitted observations from s1 during d1. After the state duration time d1 is expired, s1 is transmitted to the next state s2. Thus, HSMM handles the state duration time in each state sn by dn.\nHere, let Sm\u2032 and Sm be hidden states, and Dm\u2032 and Dm be the lengths of time spent in states Sm\u2032 and Sm , respectively.\nTherefore, Sm\u2032 and Sm may happen more than once in a sequence, but we use the same Dm\u2032 and Dm regardless of the number of occurrences in the following equations. The number of observations from the state Sm is determined by the the duration Dm. Thus, the time length of the observation sequence is calculated as T = \u03a3Nn=1dn. Then, the transition probability from Sm\u2032 with duration Dm\u2032 to Sm with duration Dm is represented as a(Sm\u2032 ,Dm\u2032 )(Sm,Dm). This is defined as\na(Sm\u2032 ,Dm\u2032 )(Sm,Dm) , P [st:t+Dm\u22121 =Sm|st\u2212Dm\u2032 :t\u22121 =Sm\u2032 ]. (1)\nThe emission probability bSm,Dm(ot:t+Dm\u22121) is denoted as\nbSm,Dm(ot:t+Dm\u22121) , P [ot:t+Dm\u22121|st:t+Dm\u22121 = Sm]. (2)\nFinally, the set of HSMM parameters, \u03bb, is defined as\n\u03bb , {a(Sm\u2032 ,Dm\u2032 )(Sm,Dm), bSm,Dm(vk1:kDm ), \u03c0Sm\u2032 ,Dm}, (3)\nwhere \u03c0Sm\u2032 ,Dm is the initial distribution of the state Sm\u2032 , and vk1:kDm represents the sequence of the observable values of size Dm. This is vk1 , \u00b7 \u00b7 \u00b7, vkDm \u2208 V \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 V , where vkn is a n-th observable value, and kn(\u2264 K) is the corresponding index in V . For the estimation of the likelihood probability, we use an extended Viterbi algorithm [24] because it is the most popular algorithm for estimating the maximum likelihood. The forward variable in the algorithm is defined as\n\u03b4t(Sm, Dm) , max s1:t\u2212d P [s1:t\u2212Dm , st\u2212Dm+1:t = Sm,o1:Dm |\u03bb]\n= max Sm\u2032\u2208S\\{Sm},Dm\u2032 {\u03b4t\u2212Dm(Sm\u2032 , Dm\u2032) \u00b7\na(Sm\u2032 ,Dm\u2032 )(Sm,Dm) \u00b7 bSm,Dm(ot\u2212Dm:t\u22121},(4)\nwhere, \u03b4t(Sm, Dm) is the maximum likelihood that the partial state sequence ends at t in the state Sm of duration Dm."}, {"heading": "V. DURATION AND INTERVAL HMM (DI-HMM)", "text": ""}, {"heading": "A. Motivation", "text": "HSMM introduced in the previous section handles the state duration. Thus, HSMM is often called an explicit duration model. However, because Sm\u2032 moves to Sm at the expiration of duration time Dm\u2032 , HSMM cannot describe the state interval between two states as it is. The easiest way to improve HSMM to handle both the state duration and the state interval is to introduce special state, called interval state, describing the time interval between two states. However, it is well known that introducing such an interval state between two states degrades the accuracy of discrimination [25].\nTo tackle this problem, we extend the conventional HSMM by newly introducing state interval probability to each transition probability between two states. This present paper calls this new extended model as Duration and Interval Hidden Markov Model (DI-HMM). The concept of DI-HMM is illustrated in Figure 3. Although the structure of DI-HMM is similar to HSMM described in Figure 2, state interval probability is newly added to HMM as Lm\u2032,m as illustrated in the figure. While the start time of Sm is the next to the end time of Sm\u2032 in HSMM, Sm starts after the Lm\u2032,m length of time passes. The time length of the observation sequence; T varies due to its dependency on the length of the durations and the intervals, leading to T = \u03a3Nn=1(dn + ln\u22121,n). Then, the state interval is described by the state interval probability. Exploiting the state interval probability, the proposed model handles the state interval in the extended HSMM. The proposed method uses DI-HMM for training a model, and Viterbi algorithm is used for recognition of a test data."}, {"heading": "B. Training Sequential Data Model", "text": "The details of DI-HMM is elaborated using example data as shown in Figure 4. The slash line patterned blocks represent the data sequence of training set; ln\u22121,n is the time difference between the end of sn\u22121 and the beginning of sn. Furthermore, Lm\u2032,m, which is not represented in Figure 4, represents the time difference between the end of Sm\u2032 and the beginning of Sm when the next state of Sm\u2032 is Sm.\nFirst, the time interval probability density distribution is expressed by adopting the Gaussian distribution, p(Lm\u2032,m), as\np(Lm\u2032,m) = 1\u221a 2\u03c0\u03c32 e\u2212 (x\u2212\u00b5)2 2\u03c32 , (5)\nwhere \u03c3 donates the variance of state interval Lm\u2032,m, and \u00b5 is the mean of Lm\u2032,m. Then, the set of parameters used in DI-HMM is defined as\n\u03bb , {a(Sm\u2032 ,Dm\u2032 )(Sm,Dm), b(Sm,Dm)(o1:Dm), \u03c0Sm\u2032 ,Dm , p(Lm\u2032,m)}. (6)\nThe transition and emission probabilities are defined as being equal to HSMM. The difference between HSMM and DI-HMM is to consider the parameter of p(Lm\u2032,m). The reason why the Gaussian distribution is adopted as the interval distribution is that it simply expresses the density distribution,\nand the parameters are not required to change for each probability. But, other distributions and functions for our proposed algorithm could be adopted.\nThe range of x might influence either memory consumption and/or computational complexity to generate the model. There might be no x value suitable for the observation values due to the range limitation of x if p(Lm\u2032,m) is generated in a training period. However, if the parameter p(Lm\u2032,m) is generated every time an observation is fed to the algorithm, the calculation cost can be much higher. Our motivation to introduce the interval distribution to HSMM is, as explained earlier, to find the similar part of sequential data including the interval and also to discriminate between the target part and the similar part. Therefore, even if the probability of Lm\u2032,m is presumed to zero around the skirts of the distribution, no particular problem arises. Consequently, we introduce the boundary of the probability value \u03b8pt to determine the edge of the skirt of p(Lm\u2032,m). On generating the p(Lm\u2032,m), the calculation is terminated when the probability value goes less than \u03b8pt."}, {"heading": "C. Probability Estimation for Recognition", "text": "The Viterbi algorithm is used to estimate the probability of a model as [26]. Then, the pair of the model and the probability described are stored as the candidate, and finally the maximum likelihood estimate is calculated for each state for each model.\nFirst, we calculate the probability of Lm\u2032,m, i.e., p(Lm\u2032,m), assigning Lm\u2032,m to the new parameter distribution p(Lm\u2032,m) beforehand. If Lm\u2032,m is out of the range for the p(Lm\u2032,m), the time interval probability is determined as\np(Lm\u2032,m) = min 1\u2264m\u2032\u2264M,1\u2264m\u2264M\np(Lm\u2032,m)\u00d7 c. (7)\nTherein, c is 0 \u2264 c \u2264 1. Then, the forward variable for estimating the maximum likelihood is calculated as\n\u03b4t(Sm, Dm) , max s1:t\u2212Dm P [s1:t\u2212Dm , st\u2212Dm\u2032+1:t=Sm,o1:Dm |\u03bb]\n= max Sm\u2032\u2208S\\{Sm},Dm\u2032 {\u03b4t\u2212Dm(Sm\u2032 , Dm\u2032)\n\u00b7a(Sm\u2032 ,Dm\u2032 )(Sm,Dm) \u00b7 bSm\u2032 ,Dm\u2032 (ot\u2212Dm\u2032+1:t) \u00b7p(Lm\u2032,m)} . (8)\nThe parameter of p(Lm\u2032,m) is the same as the one introduced into Eq. (4) in Section IV. The state interval probability is calculated at the same time as calculating the parameter of the likelihood using the transition probability recursively.\nThe difference between HSMM and DI-HMM is the capability of handling the time interval between states as explained earlier. The interval probability in DI-HMM is integrated for introducing the time interval between states to calculate\nthe likelihood. This calculation might cause additional calculation cost. Hence, it is necessary to evaluate the model for calculation cost. In addition, the observation distributions bSm,Dm(o1:Dm) can be parametric or non-parametric. In this proposal, the relation of the state duration and the state interval is not represented in a model. For that reason, bSm,Dm(o1:Dm) is handled as non-parametric, discrete, and independent of the state durations. Then, p(Lm\u2032,m) is also discrete and independent of the state duration and the transition probability."}, {"heading": "VI. NUMERICAL EXPERIMENTS", "text": "This section presents comparisons of the following items between DI-HMM and HSMM: section A describes the discrimination performance, section B describes the recognition performance, and section C describes the calculation time of training and recognition of DI-HMM. Figure 5 portrays an example of how to generate synthetic data for the evaluation. First, the number of states N , the minimum value of the duration dmin, the maximum value of duration dmax, the minimum value of the state interval lmin and the max value of the state interval lmax are given as initial parameters. The length of sequence T is also given in the evaluation. In the example of Figure 5, the number of states and durations are N , and the number of intervals is N \u2212 1. The lengths of each duration and each state interval are incremented one by one. Then, all those lengths are combined with round robin."}, {"heading": "A. Discrimination Performance", "text": "First, we generate 200 different sequences by fixing T = 14, N = {3, 4}, dmin = 1, dmax = 10, lmin = 1, and lmax = 4. Figure 6 shows the generated example data. Each row represents one sequence of data. The gray blocks are observed states, and the length of the states represents the duration. To evaluate the discrimination performance, we compare the likelihoods calculated using Eq. (8) for the test data against each training datum. Discrimination means that the likelihood for each training data is different from one another.\nFigure 7 exhibits the results of Data 5, 10, and 15, extracted by the sequential data presented in Figure 6. The x-axis shows each training datum, and the y-axis shows the likelihood for each test datum. From this figure, the likelihoods of HSMM indicate around 0.5 for each training datum; Data 5, 10, and 15. This means that all the test data have similar data model, and they cannot be discriminated by HSMM. On the other hand, in DI-HMM case, each result has a peak value at the corresponding training data. This means that DI-HMM can discriminate those data, and DI-HMM can discriminate the differences of both the state duration and the state interval.\nFigure 8 shows the discrimination performance against each different dataset when changing the number of training data from 1 to 6. The x-axis shows the number of training data; the y-axis shows the Error Rate of Discrimination (ERD). From the result, DI-HMM discriminates between the entire sequences by checking the difference of the state interval length for the most part. Even if the amount of training data is only one, the performance is sufficiently high because the ERD is close to 0.1 while the ERD of HSMM is around 0.6 constantly. Therefore, DI-HMM has powerful discriminative capability to recognize differences among sequences."}, {"heading": "B. Recognition Performance", "text": "The results of the discrimination performance show that the likelihood of DI-HMM can give the maximum value at the data that has the same duration and the same state interval in all training data. However, in a practical field, robustness to a slight time delay or time extension of the same labeled data is required. For instance, in case of music performance, the consecutive time lengths of one sound must be slightly different when the same rhythm is played by two different\ninstruments or played by two players. Therefore, we evaluate the recognition performance when individual sound has such a time delay and/or a different consecutive time.\nTo evaluate the impact caused by the time difference in the data assigned to the same label, we use music sound data played by different instruments. Figure 9 presents an example of the rhythm scores. The two waveforms are the monophonic waveform of sound played by an organ and a drum. The waveform of the organ has longer length of notes than the drum has. In this experiment, the music data for the evaluation are generated using the following steps:\na Divide an input waveform into bars, which are small pieces of music containing a fixed number of beats.\nb The length of the period when sounds exist corresponds to the state duration time, whereas the length of the period when no sound exists corresponds to the state interval time.\nc The observation sequence consists of the sound \u201con\u201d symbol and \u201coff\u201d symbol.\nFor this experiment, we use the bass part of \u201cWe Wish You A Merry Christmas\u201d that consists of 57 bars. First, we prepare six kinds of music sound which are played by different instruments with different lengths of notes. Table II shows each of differences in the experimental data set. In the experiment, Data 1, 2, and 3 are used as training data; and Data 4, Data 5, and Data 6 are used as test data.\nFor the training phase, the first bars of Data 1, 2, and 3 are trained for the model labeled as \u201c1\u201d. For the recog-\nnition phase, the probability of each model is calculated. The recognition result (the estimated label) is obtained from the label of the model with the maximum probability. We evaluate the recognition accuracy based on f -measure that is calculated by 2 \u00b7 recall \u00b7 precision/(recall + precision), where precision = TP/PP , and recall = TP/AP . Here, when the Predicted Positive (PP ) is the number of models whose likelihood calculated using Eq. (4) or Eq. (8) is the maximum in all models, True Positive (TP ) is the number of collected models in PP ; and Actually Positive (AP ) is the number of labeled models. We prepare and evaluate two patterns of data by changing the length of bars. The observation sequence in the first pattern consists of a sound pattern of one bar. Whereas each observation sequence consists of a sound pattern combining two continuous bars in the second pattern. In the first case, we obtain 57 labeled models, and extract 40 bars of which their rhythms are different. Thus, these 40 bars finally are used for the experiment. At the training phase, 40 models are generated by all 120 (= 40 bars \u00d7 3 data) training data. Then, at the recognition phase, another 120 data are tested. Similarly, as for the second case, the number of the labeled models is 26, and the number of training data is 78 (= 26 bars \u00d7 3 data). The number of test data is 78.\nFigure 10 and Figure 11 show the recognition results in the first and second patterns, where red bar graphs show the results of DI-HMM, and blue bar graphs show those of HSMM. From these figures, all scores of precision, recall, and f-measure of DI-HMM indicate higher values than those of HSMM. Therefore, our proposed model is effective for recognition of the rhythm pattern of music by taking into account various instruments. Furtheremore, comparing the results between Figure 10 and Figure 11, the recognition accuracy for the data of which sequence consists of two bars is worse than that for the data of which each sequence consists of one bar. This result can be explained as follows; the various lengths of durations and intervals are trained for the same duration and interval probability between the same two states when the length of the sequence is longer and when the same symbol appears many times in a sequence. However, the symbols of the observation sequence differ in practical data like music data or some life event data. Hence, the quantities of symbols indicate various different values. Consequently, when various symbols are included in a sequence, the recognition accuracy will get increased even if the sequence is longer."}, {"heading": "C. Calculation time of Training and Recognition", "text": "For calculation time evaluation, we generate 35 sequences, fixing dmin = dmax = 2, lmin = 1, lmax = 10, and T is not fixed a priori. Using the generated data, we compare training time and recognition time while changing the number\nof training data. The results of training time and recognition time are shown in Figure 12 and Figure 13, respectively. The x-axis shows the number of training data, and the y-axis shows the calculation time for training/recognition. The red line is the result of DI-HMM, and the blue line is the result of HSMM. The slopes of the recognition time and the training time in DI-HMM are steeper than those of HSMM. Therefore, the introduction of the interval probability onto HSMM is expected to pose additional calculation cost. However, it does not severely affect the total amount of calculation. Meanwhile, as shown in the previous sections, the recognition performances and the discrimination performances of DI-HMM give superior results than HSMM does. Therefore, we conclude that our proposed DI-HMM is very effective for the sequential data analysis that is originally motivated in this paper."}, {"heading": "VII. SUMMARY AND FUTURE WORK", "text": "As described herein, we investigated the requirements for sequential data analysis by focusing on the structure and feature of sequential data. Then, we have proposed Duration and Interval HMM (DI-HMM) by introducing the interval probability onto HSMM in order to handle both the state duration and the state interval. We evaluated the discrimination performance, recognition performance, and measured the calculation time for training and recognition by the computational simulation. For the evaluation of discrimination performance, DI-HMM can discriminate between different sequences with fewer training data. The error rate of discrimination is less than\n0.1 if we train more than two sequences selectively. Therefore, DI-HMM is powerful to find even the sequence that is not included in the training data. This feature allows us to easily add new labels into existing databases of the training data. Furthermore, the evaluation results obtained using the sound data show that DI-HMM gives higher performances for rhythm pattern recognition than HSMM does by taking into account the slight time delay. Therefore, we can say that DI-HMM supports temporal order as well as temporal ambiguity of events to find similar sequential patterns efficiently. However, from the evaluation of the calculation time, the proposed method requires additional time to treat the interval. This revealed the fact that the more additional time might be needed when the number of training data increases. Future studies will be conducted to compare our proposed method with a further new different method which introduces the interval state node to HSMM, and to evaluate the training-recognition time and memory consumption. Additionally, we shall improve DI-HMM to reduce such calculation costs to facilitate its application as an online system."}], "references": [{"title": "Finding sequential patterns from large sequence data", "author": ["M. Esmaeili", "F. Gabor"], "venue": "Informational Journal of Computer Science Issues (IJSC), vol. 7, no. 1, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding sequential patterns from large sequence data", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proc. of ACM the 17th annual international conference on Research and Development in information retrieval (ACM SIGIR), 1994, pp. 3\u201312.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A sequential clustering algorithm with application to gene expression data", "author": ["J. Song", "D.L. Nicolae"], "venue": "Journal of the Korean Statistical Society, vol. 38, no. 2, pp. 175\u2013184, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Data mining for wearable sensors in health monitoring systems: A review of recent trends and challenges", "author": ["H. Banaee", "M. Ahmed", "A. Loutfi"], "venue": "Sensors 2013, vol. 13, no. 12, pp. 17 472\u201317 500, Dec. 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential bayesian estimation with censored data for multi-sensor systems", "author": ["Y. Zheng", "R. Niu", "P.K. Varshney"], "venue": "IEEE Trans. on Sygnal Processing, vol. 62, no. 16, May 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and recognizing the hierarchical and sequential structure of human activities", "author": ["H. Cheng"], "venue": "Ph.D. dissertation, Carnegie Mellon University, Dec. 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A dynamic programming approach to continuous speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Proc. of 7th ICA 1971, vol. 11, no. C13, 1971.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1971}, {"title": "Support vector machines for pattern classification", "author": ["S. Abe"], "venue": "Springer Science and Business Media, July 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian updating in recursive graphical models by local computations", "author": ["F.V. Jensen", "S. Lauritzen", "K.G. Olsen"], "venue": "Computational Statistics and Data Analysis, vol. 4, pp. 269\u2013282, 1990.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Dynamic time-alignment kernel in support vector machine", "author": ["H. Shimodaira", "K. Noma", "M. Nakai", "S. Sagayama"], "venue": "Advances in Neural Information Processing Systems, Cambridge, MA, MIT Press, 2002.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep learning for sequential pattern recognition", "author": ["P. Safari"], "venue": "Master\u2019s thesis, Faculty of Electrical Engineering and Information Technology, Technical University Munich, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical inference for probabilistic functions of finite state markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Statist, vol. 37, pp. 1554\u20131563, 1966.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1966}, {"title": "Hidden markov models", "author": ["S.R. Eddy"], "venue": "Elsevier Current Opinion in Structural Biology, vol. 6, no. 3, pp. 277\u2013427, June 1996.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Hidden morkov models combining discrete symbols and continuous attributes in handwriting recognition", "author": ["H. Xue", "V. Govindaraju"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 458\u2013462, Mar. 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "An input-output HMM architecture", "author": ["Y. Bengio", "P. Frasconi"], "venue": "Advances in Neural Information Processing Systems, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Non-stationary fuzzy markov chain", "author": ["F. Salzenstein", "C. Collet", "S. Lecam", "M. Hatt"], "venue": "Pattern Recognition Letters, vol. 28, no. 16, pp. 2201\u2013 2208, Dec. 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient forward-backward algorithm for an explicit duration hidden markov model", "author": ["S.Z. Yu", "H. Kobayashi"], "venue": "IEEE Signal Processing Letters, vol. 10, no. 1, pp. 11\u201314, Jan. 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "The infinite hidden markov model", "author": ["M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen"], "venue": "Machine Learning, MIT Press, pp. 29\u2013245, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden semi-markov models", "author": ["S.Z. Yu"], "venue": "Elsevier Artificial Intelligence, vol. 174, pp. 215\u2013243, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Hidden semi-markov models (HSMMs)", "author": ["K.P. Murphy"], "venue": "Nov. 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "A hidden semi-markov model with missing data and multiple observation sequences for mobility tracking", "author": ["S.Z. Yu", "H. Kobayashi"], "venue": "Elsevier Science B.V. Signal Processing, vol. 83, pp. 235\u2013250, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling acoustic transitions in speech by modified hidden markov models with state duration and state duration-dependent observation probabilities", "author": ["Y.K. Park", "C.K. UN", "O.W. Kwon"], "venue": "IEEE Trans. on Speech and Audio Processing, vol. 4, no. 5, pp. 289\u2013392, Sep. 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Map speaker adaptation of state duration distribution for speech recognition", "author": ["Y.B. Yoma", "J.S. Sanchez"], "venue": "Trans. on IEEE Speech and Audio Processing, vol. 10, no. 7, pp. 443\u2013450, Oct. 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The viterbi algorithm", "author": ["G. Forney"], "venue": "Proc. of IEEE, vol. 61, no. 3, pp. 268\u2013278, Mar. 1973.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1973}, {"title": "Hierarchical multi-channel hidden semi markov models", "author": ["P. Natarajan", "R. Nevaia"], "venue": "Proc. of the 3rd International Joint Conrefence on Artificial Intelligence (IJCAI 07), pp. 2562\u20132567, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Dynamic bayesian networks: Representation, inference and learning", "author": ["K. Murphy"], "venue": "Ph.D. dissertation, Dept. Computer Science, UC Berkeley, 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "[1] categorize three types of sequential pattern after theoretical investigation for large amounts of data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] propose a sequential algorithm using special queries to train text classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] propose a sequential clustering algorithm for gene data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 5, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 212, "endOffset": 215}, {"referenceID": 6, "context": "For sequential pattern matching and detection, DP matching [7] extracts similar sequential patterns from different two sequential patterns.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "For sequential pattern classification, SVM [8] and Probabilistic Graphical Models [9] are proposed.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "For sequential pattern classification, SVM [8] and Probabilistic Graphical Models [9] are proposed.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "propose an extended SVM which enables frame-synchronous recognition of sequential pattern [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Safari proposes a new model of Deep Learning for sequential pattern recognition [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "Lastly, HMM [12][13] is a statistical tool for modeling sequence of observations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Lastly, HMM [12][13] is a statistical tool for modeling sequence of observations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "propose transitionemitting HMMs (TE-HMMs) and state-emitting HMMs (SEHMMs) for treating discontinuous symbols [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "propose IO-HMM for gesture recognition that maps input sequences to output sequences during learning whereas the original HMM learns only output sequence distributions [15].", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "deal with a statistical model based on Fuzzy Markov random chains for image segmentation in the context of stationary and non-stationary data [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "propose Explicit-Duration Hidden Markov Model [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "propose HMMselftrans that is an extended model of EDM [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "[19] and Murphy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] propose HSMM which is a basic model of EDM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "HMM [13]", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "IO-HMM [15]", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "HSMM [19][20] X", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "HSMM [19][20] X", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "HMM-selftrans [14] X", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "EDM [17] X", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "HSMM is an extended model of conducting HMM using a semi-Markov chain with a variable staying duration for each state [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "For the estimation of the likelihood probability, we use an extended Viterbi algorithm [24] because it is the most popular algorithm for estimating the maximum likelihood.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "However, it is well known that introducing such an interval state between two states degrades the accuracy of discrimination [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "The Viterbi algorithm is used to estimate the probability of a model as [26].", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "Analysis of sequential event data has been recognized as one of the essential tools in data modeling and analysis field. In this paper, after the examination of its technical requirements and issues to model complex but practical situation, we propose a new sequential data model, dubbed Duration and Interval Hidden Markov Model (DI-HMM), that efficiently represents \u201cstate duration\u201d and \u201cstate interval\u201d of data events. This has significant implications to play an important role in representing practical time-series sequential data. This eventually provides an efficient and flexible sequential data retrieval. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed DI-HMM. Keywords\u2014Sequential data analysis; HMM; HSMM; State duration; State interval", "creator": "LaTeX with hyperref package"}}}