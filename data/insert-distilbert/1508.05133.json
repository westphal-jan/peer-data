{"id": "1508.05133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "abstract": "contemporary deep neural networks exhibit impressive results on practical reliability problems. these networks generalize well although their inherent capacity may extend significantly beyond the number of training task examples. we analyze this computational behavior in the context of deep, infinite structured neural networks. we still show that deep infinite layers are naturally aligned tight with gaussian transformation processes and appropriate kernel methods, assign and devise stochastic kernels that encode the information of these networks. we show that stability results apply despite the size, offering an explanation for their empirical success.", "histories": [["v1", "Thu, 20 Aug 2015 21:35:52 GMT  (49kb,D)", "http://arxiv.org/abs/1508.05133v1", null], ["v2", "Wed, 2 Sep 2015 18:27:36 GMT  (51kb,D)", "http://arxiv.org/abs/1508.05133v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["tamir hazan", "tommi jaakkola"], "accepted": false, "id": "1508.05133"}, "pdf": {"name": "1508.05133.pdf", "metadata": {"source": "CRF", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "authors": ["Tamir Hazan", "Tommi Jaakkola"], "emails": ["tamir.hazan@gmail.com", "tommi@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks have become widely adopted for tasks ranging from image labeling in computer vision to parsing and machine translation in natural language processing. The networks in these tasks usually consist of an input layer, several semistructured intermediate layers and an output layer. Surprisingly, as large, complex models, they appear easier to learn at scale, rendering state of the art performance (e.g., [?]) with increasing amounts of data and computation. The setting poses new questions for learning since the number of parameters in these models, mostly residing in the deep layers, may be substantially larger than what could be supported by the training examples. Many expect such networks to overfit while in practice they (often) do not, and their decision boundaries appear smooth. Our work suggests an explanation for this behavior based on deep and infinitely wide networks where the number of parameters is uncountably infinite.\nNeural networks with a single infinite intermediate layer have been considered by various works. [?] show these networks are universal approximators and [?, ?, ?] explore their properties in the context of Gaussian processes and kernel methods. Unfortunately, since these networks interact linearly with the input layer, they are limited in\nar X\niv :1\n50 8.\n05 13\n3v 1\n[ cs\n.L G\ntheir representation power. Moreover, the recent success of neural networks seems to rely on deep architecture while current infinite networks only encode the information of a single layer. Lastly, these works do not explain why learning the likelihood of infinite networks does not overfit, and the decision boundary of the learned network is simple.\nIn our work we extend the framework of kernel methods for infinite networks to multiple layers. We introduce stochastic kernels that are derived from Gaussian processes and encode the information of two infinite layers. We also provide a generalization bound for these networks, based on stability of regularized loss minimization, and attribute the simplicity of the learned deep infinite network to the fast convergence of algorithms on our learning framework.\nWe begin by introducing infinite neural networks with a single intermediate layer. We relate their learning units to integrals over functions in the Euclidean space with respect to the Gaussian distribution, as well as describe their connections to kernel functions. We subsequently construct the second layer and relate its learning units to expectations with respect to a Gaussian process. These expectations form stochastic kernel functions that encode the multilayer and infinitely wide neural network. Finally, we analyze the generalization properties of these networks and introduce a method to incorporate localities and non-linearities such as those arising from convolutional neural networks."}, {"heading": "2 Background", "text": "Neural networks form a successful framework for classification that imitate the activation function of neurons. Finite neural networks are usually described by a layered graph, see Figure 1. Its input layer consists of nodes that receive the input signal x \u2208 Rd. Its subsequent layers consist of parameters that encode the classification pro-\ncess: its intermediate layers consist of activation nodes. Each activation node rely on its parameter to produce a linear response f(\u3008x,w\u3009) according to its parameters w. The function f(t) is called an activation function or a transfer function and it introduces non-linearities to the network. Transfer functions imitate the neuron behavior, activating its value whenever its linear input \u3008x,w\u3009 is high enough. There are various forms of non-linear transfer functions, e.g., step and rectified linear functions. Recently, the rectified linear function ReLU(t) = max(0, t) was successfully used in neural networks as it carries the neuron signal better [?]. Another popular transfer function is the step function step(t) = 1[t \u2265 0] that attains the value one it t \u2265 0 and zero otherwise. The output of a network with a single intermediate layer linearly weights the activations f(\u3008x,w1\u3009), ..., f(\u3008x,wk\u3009) with the output parameters u1, ..., uk. Its classification is determined by the sign of \u2211k i=1 uif(\u3008x,wi\u3009).\nA classical result by Hornik asserts that networks with one intermediate layer are universal approximators when the number of activation units k tends to infinity [?]. Consequently, neural networks have been studied in the infinite setting [?, ?, ?, ?, ?, ?]. In this setting there are infinitely many transfer functions f(\u3008w, x\u3009) each of them is indexed by w. Summing over infinitely many transfer functions is formalized by integrating over possible w. Formally, we replace \u2211 i uif(\u3008x,wi\u3009)\nwith \u222b u(w)f(\u3008x,w\u3009)d\u00b5(w). The measure \u00b5(w) may be any probability distribution over Rd as long as this integral is finite, e.g, the Gaussian distribution d\u00b5(w) = (2\u03c0)\u2212d/2 exp(\u2212\u2016w\u20162/2).\nWhen taking a discriminative approach, one learns a neural network that best describes the training data S = {(x1, y1), ..., (xm, ym)}, where xi is a data instance (e.g., an image or a sentence) and yi is its semantic label. While learning an infinite network with a single intermediate layer, one needs to consider compact ways to represent the function u(w). Kernel methods can be used for this task while representing the classifier by its dual [?]. Particularly, the network\u2019s output is an inner product between u(w) and an input-dependent function \u03c6x(w) = f(\u3008x,w\u3009)\n\u3008u, \u03c6x\u3009\u00b5 = \u222b u(w)f(\u3008x,w\u3009)d\u00b5(w). (1)\nSince u(w) is trained over a finite space of feature functions it can be restricted without loss of generality to the linear span of the training feature functions \u03c6x1(w), ..., \u03c6xm(w), namely u(w) = \u2211m i=1 \u03b1i\u03c6xi(w) for some real valued numbers \u03b11, ..., \u03b1m. Therefore, when evaluating the output value of the network \u3008u, \u03c6xj \u3009\u00b5 it suffices to compute the kernel entries\nkf (xi, xj) = \u3008\u03c6xi , \u03c6xj \u3009\u00b5 = \u222b f(\u3008xi, w\u3009)f(\u3008xj , w\u3009)d\u00b5(w).\nVarious works have already computed the kernel function for different transfer functions with respect to the Gaussian measure, including the rectified linear and the sign function [?, ?]. In all these cases, the kernel has an analytic form although the features \u03c6x(w) are not finite vectors but functions overRd. Explicitly, let \u03c1i,j = \u3008xi, xj\u3009/\u2016xi\u2016\u2016xj\u2016\nthen\nkReLU(xi, xj) = \u2016xi\u2016\u2016xj\u2016 \u03c0 sin ( arccos(\u03c1i,j) ) + (\u03c0 \u2212 arccos(\u03c1i,j))\u03c1i,j .\nkstep(xi, xj) = \u03c0 \u2212 arccos(\u03c1i,j).\nWhenever the measure is not Gaussian there is no analytic solution for the different kernels. Nevertheless, whenever the probability density function d\u00b5(w) is log-concave (i.e., log(d\u00b5(w)) is a concave function) then \u3008x,w\u3009 is a log-concave function thus the sample complexity of the kernel function decays exponentially with the number of samples."}, {"heading": "3 Stochastic kernels for deep and infinitely wide neural networks", "text": "A deep learning architecture considers multiple intermediate layers. Deep architectures have proven successful as they allow to express non-linearities easily. Unfortunately, when considering multiple infinite layers there are difficulties to represent the network parameters. Such difficulties do not appear when considering finite layers since all parameters in all layers are vectors. However, when considering infinite layers, the parameters are functions (in the second intermediate layer) functions of functions (in the subsequent layer) and so on, see Figure 1. In the following we present the framework of learning with multiple intermediate layers. For the clarity of presentation we describe two intermediate layers. We refer to these networks as deep networks to differentiate them from the known networks with a single infinite layer.\nThe main challenge in working with deep infinite networks is to establish the space in which the deep neurons exist. The neurons of the second intermediate layer take as input the functions \u03c6x(w), which are the output of the first intermediate layer, i.e., \u03c6x(w) for any w \u2208 Rd. Therefore, each neuron in the second layer is a function u : Rd \u2192 R that weights its input values (which are \u03c6x(w) for any w \u2208 Rd) in a linear manner \u3008u, \u03c6x\u3009\u00b5. The output of each such neuron is the activation of the transfer function \u03c8x(u) = f(\u3008\u03c6x, u\u3009\u00b5). Therefore, the output layer of deep infinite network needs to take all its inputs, i.e., \u03c8x(u) for any function u(\u00b7), and weight their activation by v(u). Thus the output layer computes the linear function \u3008v, \u03c8x\u3009\u03bd . with respect the a measure \u03bd. Next, we determine the measure space of v(u) in terms of stochastic processes.\nIt is natural to consider the activation of neurons in the second intermediate layer \u3008\u03c6x, u\u3009\u00b5 with respect to the measure \u00b5(w) using probabilistic terms. This linear function is the covariance of two random variables \u3008\u03c6x, u\u3009\u00b5 = Ew\u223c\u00b5 [ \u03c6x(w)u(w)]. Sim-\nilarly, the activation of the output neuron is \u3008v, \u03c8x\u3009\u03bd = Eu\u223c\u03bd [ \u03c8x(u)v(u)]. With this perspective, the functions u : Rd \u2192 R are chosen randomly according to the measure \u03bd. Equivalently, \u03bd is a stochastic process. In our work we restrict ourselves to a Gaussian process, a stochastic process for which any finite collection of random variables u(w1), ..., u(wk) has a multivariate Gaussian distribution. A Gaussian process is completely determined by its first and second order statistics. The\nmean function \u00b5(w) of a Gaussian process is Eu\u223c\u03bd [u(w)]. Its covariance function C(w1, w2) = Eu\u223c\u03bd [u(w1), u(w2)]. We consider Gaussian process with zero mean function and a general covariance function, thus we denote \u03bd = GP (C).\nTo learn a deep infinite network that linearly separates the training examples it suffices use the stochastic kernel function:\nk (2) f (xi, xj) = \u3008\u03c8xi , \u03c8xj \u3009\u03bd = Eu\u223cGP (C) [ f(\u3008\u03c6xi , u\u3009)f(\u3008\u03c6xj , u\u3009) ] (2)\nRecall that the first layer responses are the transfer functions \u03c6xi(w) = f(\u3008w, xi\u3009), \u03c6xj (w) = f(\u3008w, xj\u3009). Thus a stochastic kernel for deep infinite network averages non-linearities while considering their covariances.\nAlthough the Gaussian process has infinitely many random variables, its unique properties allows to compute the stochastic kernel function analytically.\nTheorem 1. k\n(2) f (xi, xj) = E(z1,z2)\u223cN(0,\u03a3) [ f(z1)f(z2) ] z = (z1, z2) is a bivariate Gaussian random variable with zero mean and covariance matrix \u03a3:\n\u03a3 = Ew1,w2\n( f(\u3008w1, xi\u3009)C(w1, w2)f(\u3008w2, xi\u3009) f(\u3008w1, xi\u3009)C(w1, w2)f(\u3008w2, xj\u3009)\nf(\u3008w1, xi\u3009)C(w1, w2)f(\u3008w2, xj\u3009) f(\u3008w1, xj\u3009)C(w1, w2)f(\u3008w2, xj\u3009)\n) (3)\nw1, w2 are chosen independently from a d\u2212dimensional multivariate Gaussian with zero mean and unit covariance, i.e., N(0, I).\nProof. z1 = \u3008\u03c6xi , u\u3009 is a Gaussian random variable with zero mean1 Similarly, z2 = \u3008\u03c6xj , u\u3009 is a Gaussian random variable and both z1, z2 are jointly Gaussian. Thus z = (z1, z2) is a bivariate Gaussian random variable with zero mean and some covariance matrix \u03a3. The expected value of a Gaussian process reduces to\nEu\u223cGP (C) [ f(\u3008\u03c6xi , u\u3009)f(\u3008\u03c6xj , u\u3009) ] = E(z1,z2)\u223cN(0,\u03a3) [ f(z1)f(z2) ] .\nThe covariance matrix of \u03a3 is a 2 \u00d7 2 matrix whose (r, s) entry is Ez\u2208N(0,\u03a3)E[zrzs]. Recall that z1 = Ew[\u03c6xi(w)u(w)] and that \u03a311 = E[z 2 1 ], then\n\u03a311 = Eu [ Ew1 [\u03c6xi(w1)u(w1)] \u00b7 Ew2 [\u03c6xi(w2)u(w2)] ] = Ew1,w2 [ \u03c6xi(w1)\u03c6xj (w2)Eu[u(w1)u(w2)] ] = Ew1,w2 [ \u03c6xi(w1)\u03c6xj (w2)C(w1, w2) ] .\nWe used Fubini\u2019s theorem to change the order of integration. The values of \u03a3r,s then follow in the same manner, while recalling that \u03c6xi(w1) = f(\u3008xi, w1\u3009) and \u03c6xj (w2) = f(\u3008xj , w2\u3009) .\n1This is a classical result and can be shown by working with the Riemann-Stieltjes integral, decomposing it to finite sums. Since any finite instantiation of a Gaussian process is a multivariate Gaussian random variable with zero mean, the Riemann-Stieltjes sum is also a Gaussian random variable, thus the limit (using the characteristic function) is also Gaussian.\nAn important family of Gaussian processes is described by shift-invariant covariance functions, namely C(w1, w2) = c(w1 \u2212 w2). Bochner\u2019s theorem represents such covariance functions as Ew,b[g(\u3008w1, w\u3009 + b)g(\u3008w2, w\u3009 + b)], where w is drawn from a distribution \u03c1 over Rd, b is drawn from the uniform distribution over [0, 2\u03c0] and g(t) = \u221a 2 cos(t) [?]. Whenever \u03c1 is known we are able to efficiently compute a stochastic kernel for deep infinite networks and shift-invariant covariance functions:\nCorollary 1. Let C(w1, w2) = c(w1 \u2212 w2) be a shift-invariant covariance function and let \u03c1 be its corresponding measure derived by Bochner\u2019s theorem. Consider the 6\u00d7 6 covariance matrix\n\u03a3\u0302 =  \u2016xi\u20162 \u3008xi, w\u3009 \u3008xi, xj\u3009\u3008xi, w\u3009 \u2016w\u20162 \u3008w, xj\u3009 \u3008xi, xj\u3009 \u3008xj , w\u3009 \u2016xj\u20162 \u2297 ( 1 0 0 1 ) .\nA \u2297 B the tensor product of two matrices. Let g(t) = \u221a\n2 cos(t) and assume that b is drawn form the uniform distribution over [0, 2\u03c0] and w is drawn according to \u03c1 and z\u0302 \u223c N(0, \u03a3\u0302) is a multivariate Gaussian. Then the covariance matrix \u03a3 of the stochastic kernel for deep infinite neural networks k(2)f (xi, xj) = E(z1,z2)\u223cN(0,\u03a3)[f(z1)f(z2)] is\n\u03a3 = Ew,b,z\u0302 ( f(z\u03021)f(z\u03022)g(z\u03023 + b)g(z\u03024 + b) f(z\u03021)f(z\u03026)g(z\u03023 + b)g(z\u03024 + b) f(z\u03021)f(z\u03026)g(z\u03023 + b)g(z\u03024 + b) f(z\u03025)f(z\u03026)g(z\u03023 + b)g(z\u03024 + b) )\nProof. The entries of the covariance matrix of the stochastic kernel are derived in Equation (3) using \u03a3r,s = hr,s(z\u03021, ..., z\u03026) where z1 = \u3008w1, xi\u3009, z2 = \u3008w2, xi\u3009, z3 = \u3008w1, w\u3009, z4 = \u3008w2, w\u3009, z5 = \u3008w1, xj\u3009, z6 = \u3008w2, xj\u3009. Since w1, w2 \u223c N(0, I) are independent then z\u0302 is a multivariate Gaussian with zero mean and its distribution is fully determined by its covariance matrix \u03a3\u0302. The corollary then follows by direct computation of the covariance matrix, e.g.,, \u03a3\u03021,3 = Ez\u0302[z\u03021z\u03023] = \u2211 r,sEw1 [w1,rw1,sxi,rws] =\u2211\nr,s xi,rws \u00b7 Ew1 [w1,rw1,s] and Ew1 [w1,rw1,s] = 1[r = s] is the indicator function that equals one if r = s and zero otherwise.\nThe ability to realize the measure \u03c1 that is suggested by Bochner\u2019s theorem determines the validity of this approach. Bochner\u2019s theorem relates a shift-invariance function c(w1\u2212w2) to Fourier transform, thus \u03c1 can be recovered by its inverse-transform. There are some special covariance functions for which this measure is known. For example, the covariance function C(w1, w2) = exp(\u2212\u2016w1 \u2212 w2\u20161) that relates to the Ornstein-Uhlenbeck Gaussian process can be computed using the Cauchy distribution d\u03c1(w) = \u220fd i=1(\u03c0(1 + w 2 i )) \u22121. Whenever the covariance function defines a a squared exponential Gaussian process, C(w1, w2) = \u03b2 exp(\u2212\u2016w1 \u2212w2\u201622), the stochastic kernel for deep neural networks can be computed analytically. This follows from the observation that the Gaussian process couples the independent d\u2212dimensional Gaussians random variables w1, w2 to a 2d\u2212dimensional Gaussian variable w = (w1, w2) with correlation \u03b1:\nCorollary 2. Consider the covariance functionC(w1, w2) = (1+2\u03b1)1+d/2 exp(\u2212\u03b1\u2016w1\u2212 w2\u20162/2). Consider the 4\u00d7 4 covariance matrix\n\u03a3\u0302 = ( \u2016xi\u20162 \u3008xi, xj\u3009 \u3008xi, xj\u3009 \u2016xj\u20162 ) \u2297 ( 1 + \u03b1 \u03b1 \u03b1 1 + \u03b1 ) Then the covariance matrix \u03a3 of the stochastic kernel for deep neural network k(2)f (xi, xj) = E(z1,z2)\u223cN(0,\u03a3)[f(z1)f(z2)] is\n\u03a3 = Ez\u0302\u223cN(0,\u03a3\u0302) ( f(z\u03021)f(z\u03022) f(z\u03021)f(z\u03024) f(z\u03021)f(z\u03024) f(z\u03023)f(z\u03024) )\nMoreover, k(2)ReLU(xi, xj) and k (2) step(xi, xj) have analytic forms.\nProof. Considering Equation (3) we note that gI(w1)gI(w2)C(w1, w2) = (1+2\u03b1)g\u03a3\u0302(w), where gI(wi) is the d\u2212dimensional Gaussian density function N(0, I) and g\u03a3\u0302(w) is the 2d\u2212dimensional Gaussian density function N(0, \u03a3\u0303). We denote by A \u2297 B the tensor product of two matrices, thus\n(1 + 2\u03b1)\u03a3\u0303 = ( 1 + \u03b1 \u03b1 \u03b1 1 + \u03b1 ) \u2297 Id\u00d7d\nThe form of \u03a3\u0302 is attained when setting z\u03021 = \u3008w1, xi\u3009, z\u03022 = \u3008w2, xi\u3009, z\u03023 = \u3008w1, xj\u3009, z\u03024 = \u3008w2, xj\u3009. With this notation, the form of \u03a3 is a direct consequence of Equation (3).\nTo compute the entries of \u03a3 when f(t) = ReLU(t) we recall that whenever z\u20321, z \u2032 2 \u2208 N(0,\u03a3\u2032) with \u03a3\u203211 = \u03c321 , \u03a3\u203212 = \u03a3\u203221 = \u03c1\u03c31\u03c32, \u03a3\u203222 = \u03c322 thenEz\u20321,z\u0302\u20322 [f(z \u2032 1)f(z \u2032 2)] = h(\u03c31, \u03c32, \u03c1) and h(\u03c31, \u03c32, \u03c1) = \u03c31\u03c32\u03c0 sin(arccos(\u03c1))+\u03c1(\u03c0\u2212arccos(\u03c1)). Then \u03a3ReLU =( h( \u221a 1 + \u03b1\u2016xi\u2016, \u221a 1 + \u03b1\u2016xi\u2016, \u03b11+\u03b1 ) h( \u221a 1 + \u03b1\u2016xi\u2016, \u221a 1 + \u03b1\u2016xj\u2016, \u03b11+\u03b1 \u00b7 \u3008xi,xj\u3009 \u2016xi|\u2016xj\u2016 )\nh( \u221a 1 + \u03b1\u2016xi\u2016, \u221a\n1 + \u03b1\u2016xj\u2016, \u03b11+\u03b1 \u00b7 \u3008xi,xj\u3009 \u2016xi|\u2016xj\u2016 ) h(\n\u221a 1 + \u03b1\u2016xj\u2016, \u221a 1 + \u03b1\u2016xj\u2016, \u03b11+\u03b1 )\n)\nThus, k(2)ReLU(xi, xj) = E(z1,z2)\u223cN(0,\u03a3)[f(z1)f(z2)] is a recursive application of h(\u00b7) with the appropriate parameters.\nTo compute the entries of \u03a3 when f(t) = 1[t \u2265 0] we recall that whenever z\u20321, z\u20322 \u2208 N(0,\u03a3\u2032) with \u03a3\u203211 = \u03c3 2 1 , \u03a3 \u2032 12 = \u03a3 \u2032 21 = \u03c1\u03c31\u03c32, \u03a3 \u2032 22 = \u03c3 2 2 then Ez\u20321,z\u0302\u20322 [f(z \u2032 1)f(z \u2032 2)] = h(\u03c1) = \u03c0 \u2212 arccos(\u03c1). Then\n\u03a3step =\n( h( \u03b11+\u03b1 ) h( \u03b1 1+\u03b1 \u00b7 \u3008xi,xj\u3009 \u2016xi|\u2016xj\u2016 )\nh( \u03b11+\u03b1 \u00b7 \u3008xi,xj\u3009 \u2016xi|\u2016xj\u2016 ) h( \u03b1 1+\u03b1 )\n)\nAs before, k(2)step(xi, xj) = E(z1,z2)\u223cN(0,\u03a3)[f(z1)f(z2)] is a recursive application of h(\u00b7) with the appropriate parameters.\nDeep neural networks are usually applied to multiclass problems, where there are more than two labels to classify. Thus the label space resides in a discrete set y \u2208\n{1, ...,K}. For notational convenience we focus above on binary classification, where y \u2208 {\u22121, 1} is determined by the sign of the output layer \u3008v, \u03c6x\u3009. In multiclass setting, a data-instance x can belong to any of theK classes. A standard extension of the above setting to multiclass learning is to introduce K decision boundaries v1(u), ..., vk(u). Multiclass prediction is performed by choosing the decision which is most certain, i.e., arg maxi\u3008vi, \u03c8x\u3009. In the next section we describe the generalization properties of deep infinite neural networks in the multiclass setting."}, {"heading": "4 Deep infinite networks, generalization and experimental validation", "text": "The practice of neural networks proves that they do not overfit, even when the number of learned parameters is orders of magnitude larger than the number of training examples. In the following we address this scenario while suggesting some insight for why infinite networks generalize well. We show that generalization is mostly dependent on the expressive power of the output layer, which is regularized by its capacity. Consider a multiclass deep infinite network v1(u), ..., vk(u) that classifies the functions \u03c8xi(u) according to the most certain linear response function yv(x) = arg maxi\u3008vi, \u03c8x\u3009. Since each decision function vi(u) interacts linearly with the training data, it must be a finite sum of these functions, i.e., vi(u) = \u2211m j=1 \u03b1i,j\u03c8xj (u). Therefore, as long as the functions \u03c8xi(u) are simple (e.g., truncated linear functions in the case of ReLU units), the capacity of the deep infinite network is limited by the size of the training data. Whenever there are stronger guarantees on the data, i.e., that the training data is separable with a margin, they translate to a stronger regularization on vk(u) that is derived from the passive-aggressive learner [?]. To be more precise, we say that the data is separable when there are functions v\u22171(u), ..., v \u2217 k(u) that classifies correctly any data instance. Formally, for any data-label pair (x, y) there holds y = yv\u2217(x). These data-label pairs are separated with a margin if y = yv\u2217(x) and \u3008v\u2217y , \u03c8x\u3009 \u2265 1 + maxi6=y\u3008v\u2217i , \u03c8x\u3009. In this setting, the kernel version of the passiveaggressive algorithm ensures that vi(u) = \u2211t j=1 \u03b1i,j\u03c8xj (u), where t \u2264 R2 \u2211 i \u2016v\u2217i \u20162 and \u2016\u03c8x\u20162 \u2264 R. Thus, whenever there is a separation with margin and the training size m t the passive-aggressive analysis ensures that the deep learner has restricted capacity thus a simple form.\nUnfortunately, the separable setting rarely exists in practice. Nevertheless, deep learners perform well in the non-separable setting. Usually deep learning schemes use the logistic regression framework, that maximizes the conditional probability of the training data S = {(x1, y1), ..., (xm, ym)}. The conditional probability follows the Gibbs distribution pv(y|x) = exp(\u3008vy, \u03c8x\u3009)/Z(v) where Z(v) = \u2211 i exp(\u3008vi, \u03c8x\u3009) is the partition function. Thus the parameters of the network are learned by the optimization program:\nvS = arg max v\n1\nm m\u2211 i=1 log pv(yi|xi) + \u03bbm 2 \u2211 j \u2016vj\u20162 (4)\nAs this is an infinite convex program it is appealing to consider its dual. The dual\nprogram is min\u03b1 \u2211 i,k \u03b1i,k log\u03b1i,k + \u2211m i=1 \u2016vi(\u03b1)\u20162/2\u03bbm where v(\u03b1) = \u2211 i(\u03c8xi \u2212\n( \u2211 k \u03b1i,k\u03c8xk)) and \u2211 k \u03b1i,k = 1. The dual program is smooth and strongly convex, therefore enjoys rapid convergence, i.e., with O(log(1/ )) updates to the elements \u03b1 a dual exponentiated coordinate descent achieves an \u2212optimal dual solution [?]. Although this algorithm achieves a good primal solution in practice, it does not guarantee that v(\u03b1) is an \u2212optimal primal solution as well. Recently, many efficient algorithms were devised to achieve both dual and primal guarantee with O(log(1/ )) steps (cf. [?]). These algorithms aggregate data points \u03c8xi to their separators v(u) therefore after a small number of steps a good, yet simple separator is reached. Said differently, although different separators may exist around vS(u) the algorithm outputs a fairly simple separator as it is regularized by an early stopping criterion.\nConsidering the learning problem in Equation (4) as a loss minimization task, it measures the average log-loss given training data. By the above, the empirical risk minimizer vS is simple, i.e., it consists of O(log(1/ )) functions \u03c8x(u). We turn to show that this simple empirical risk minimizer also generalizes well, it achieves a similar log-loss even when the data-label pairs are sampled from their true distribution in the world.\nTheorem 2. Assume that \u2016\u03c6x\u2016 \u2264 1 and that the training examples are sampled independently from the data-label generating distribution (x, y) \u223c D. Denote log-risk by LD(v) = E(x,y)\u223cDpv(y|x) and the empirical risk by LS(v) = 1m \u2211m i=1 log pv(yi|xi). Consider vS as defined in Equation (4), then |LD(vS)\u2212 LS(vS)| \u2264 1/m\u03bbm.\nProof. Generalization by stability for convex and Lipschitz loss functions with strongly convex regularizer was established in [?, ?, ?]. Although the technical details are obscured in some of these results, we rely on their derivations (specifically [?] Theorem 22 and [?] Theorem 2). The benefit of working with stability is that its basic concepts, convexity and Lipschitz continuity, readily generalize to infinite spaces. To apply generalization via stability to multiclass logistic regression we note that \u2212 log pv(y|x) is convex. Also, it is 1\u2212Lipschitz since its gradient is uniformly bounded by 1 whenever \u2016\u03c8x\u2016 \u2264 1.\nThe regularization ratio \u03bbm is chosen such that m\u03bbm goes to zero as m tends to infinity. The important conclusion of the above theorem is that infinite models does not necessarily overfit, as long as the infinite model interacts in a constrained manner with the data. In our case the infinite model is constrained by convexity and Lipschitz continuity. These two properties stabilize the learning procedure while ensuring that small changes in v do not change the prediction by much.\nNext, we turn to experimentally validate our framework. The effectiveness of infinite network with a single infinite layer using the kernels kReLU(xi, xj) was already demonstrated by [?, ?, ?]. Thus in the following we show that our stochastic kernels k(2)ReLU(xi, xj) with a squared exponential Gaussian process improves upon kReLU(xi, xj). We run our kernels over MNIST digit database. This dataset is the standard entry point of neural networks and kernel methods.\nOur stochastic kernel k(2)ReLU(xi, xj) was able to separate the training data completely with only 50 iterations, while kReLU(xi, xj) that encodes a single infinite layer\ndid not (it nearly separated all examples). This validates the assertion that the stochastic kernel is more expressive than the single layer kernel. As for test results, the average error over all digits is 1.7% for the stochastic kernel and 1.9% for the single layer kernel. Although the improvement is modest in terms of the overall success rate (only 0.2%) it might be insightful to compare it to the possible gain over the errors of the single layer kernel, namely 0.2/1.9 which is about 10% gain. Lastly, since our kernel function is computed analytically, it is trained as fast as any kernel method."}, {"heading": "5 Non-linearities", "text": "The infinite layers, presented in Section 2 and Section 3, are limited in their expressive power. In the first intermediate layer, the inner product \u3008x,w\u3009 is performed for any w \u2208 Rd, while each parameter w acts globally on all input entries x \u2208 Rd linearly. Similarly, in the second layer, the function u(w) acts linearly and globally on every \u03c6x(w) = f(\u3008w, x\u3009). These interactions ignore spatial information in the vectors x or the feature function \u03c6x(w), spatial information that is important in computer vision and language processing applications. Current deep learning architectures exploit spatial information using convolutions. These convolutions are applied to patches in an image, or equivalently to overlapping subsets of the data instance x, and recursively to their functions. These operations introduce important aspects of non-linearity and locality. Our approach can be extended to deal with such operations, thus able to increase the expressiveness of our approach to various non-linearities.\nTo describe a convolution-based operation in the first intermediate layer, we transform the data instance x \u2208 Rd to subsets of its elements x(1), ...., x(P ) \u2208 Rd1 , where x(p) \u2282 x. For each such subset we learn infinitely many responses w \u2208 Rd1 , while each response outputs \u03c6x,p(w) = f(\u3008w, x(p)\u3009). Thus, \u03c6x = (\u03c6x,1, ..., \u03c6x,P ) is a P\u2212dimensional function, \u03c6x : Rd1 \u2192 RP . Note that in Section 2 the feature function \u03c6x mapped Rd to R.\nConvolution based operations in the second intermediate layer may also be applied. The feature function \u03c6x is transformed to subsets of its elements \u03c6 (1) x , ..., \u03c6 (Q) x where \u03c6 (q) x \u2282 \u03c6x, i.e., \u03c6(q)x : Rd1 \u2192 Rd2 that is attained by restricting to \u03c6x(w) to some of its coordinates. Each of these subsets is weighted by u : Rd1 \u2192 Rd2 and its resulting response is \u03c8x,q(u) = f(\u3008u, \u03c6x(q)\u3009), while \u3008u, \u03c6x(q)\u3009 = Ew[\u3008u(w), \u03c6qx(w)\u3009] and the latter inner product \u3008u(w), \u03c6qx(w)\u3009 is between two vectors in Rd2 .\nThe above two constructions show how to integrate convolution-type non-linearities in deep infinite networks. The appropriate kernels follow a straight forward derivation of these higher dimension constructions."}, {"heading": "6 Related work", "text": "Neural networks, kernel methods and Gaussian processes have had a significant impact on the machine learning community and a full exposition of these methods can be found in machine learning textbooks on neural networks [?], kernel methods [?] and Gaussian processes [?].\nNeural networks are attracting a considerable attention in the last few years. Their practical success is unmatched in several machine learning applications (e.g., [?]). In recent years it was possible to construct deep learning architectures with considerable number of parameters that is significantly larger than the number of training examples. Surprisingly, these networks avoid overfitting. Several machine learning theories were devised to explain how deep networks avoid overfitting based on dropouts (e.g., [?, ?]). Our approach is different since we represent neural networks with significant amount of parameters as infinite networks with multiple layers. We encode the neurons responses in functions, while each layer increases the complexity of its functions, namely the first layer consists of functions over the Euclidean space and the second layer consists of Gaussian processes. We avoid overfitting since our algorithm achieves an almost optimal solution with a few steps, thus our resulting classifier is simple to represent and regularized by early stopping. We provide a generalization bound for our classifier based on stability [?, ?, ?].\nInfinite neural networks were introduced by [?, ?] in the context of Bayesian learning. They analyze the predictive probability of a neural network with an infinitely wide intermediate layer. In particular, when the transfer function is bounded, this predictive probability converges to a Gaussian process. When resolving the covariance function of this process, [?] realized the kernel kerf(xi, xj) along with other kernel functions. This work differs from ours in a few respects. First, our work does not consider the predictive probability of labels given data but rather we aim at maximizing the likelihood of infinitely wide layers, a task that initially was supposed to overfit and generalize poorly [?]. We establish the prediction power of our approach using stability. Second, we build on multiple intermediate layers while trying to analyze the success of deep learning architectures, as opposed to [?, ?]. Lastly, our work considers Gaussian processes differently than [?]. We use Gaussian process to define a measure over our second (e.g., deep) intermediate layer.\nMore recently, researchers explored different algorithms to learn infinite neural networks [?]. This work formulates learning an infinite network as an infinite convex program and devise an incremental algorithm that is based on its dual representation. [?] suggest to optimize an infinite networks with a single layer using randomization to decrease the computational complexity of the learning algorithm. Our work addresses other properties of learning infinite networks, mainly Gaussian processes for constructing multiple layers and analyze how infinite networks avoid overfitting.\nKernel methods for infinite neural networks are further explored in [?, ?]. These works introduce the kernels kstep(xi, xj), kReLU(xi, xj) along with other kernels thus augment the works of [?, ?]. Moreover, they introduce kernel composition approach to simulate deep architecture. Our work differs in the way we address and analyze deep architectures of infinite networks. We construct deep layers that use as input their previous layer using Gaussian processes. Connections between kernel methods and Gaussian processes were left as an open problem in [?]. We also introduce a way to incorporate non-linearities and invariances such as convolutional neural networks in our framework, another open problem raised by [?]. In addition, we analyze why our networks avoid overfitting. [?] demonstrate the effectiveness of these kernels in language processing.\nIn our work we provide unbiased estimate to our kernels in the second layer us-\ning Bochner\u2019s theorem. These kernels consider a shift invariant covariance function. [?] suggest the same estimator for kernel functions in the context of random features in kernel methods. [?] suggested improved methods to reduce the variance of these estimates. Such estimators were recently used within kernel methods to match deep learning results in language processing [?, ?]."}, {"heading": "7 Discussion", "text": "Deep neural networks are successful in machine learning applications although the number of their parameters is orders of magnitude larger than the number of training examples. In this work we explain this behavior using deep infinite neural networks. We construct stochastic kernels that rely on Gaussian processes to encode such networks. We also explain how to introduce locality and non-linearity to such networks, similarly to the ones introduced by convolution neural networks. Lastly, we provide generalization bounds and regularity conditions that explain why these networks do not overfit. We present our framework with only two intermediate layers mainly for simplicity. It can be extended to any depth but the higher layers may not use nonlinearities. The problem of finding analytic forms of stochastic kernels that encode arbitrarily deep layers with non-linearities is largely open.\nThe work combines mostly separate areas in machine learning, including kernel methods, neural networks and Gaussian processes. As such, there are many direction that still need to be explored. Importantly, which non-linearities are significant in deep infinite networks and whether they can be learned from data. What probabilities best fit this framework and are there other properties of stochastic processes, besides of covariance, that control learning?"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empir-", "creator": "LaTeX with hyperref package"}}}