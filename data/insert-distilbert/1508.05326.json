{"id": "1508.05326", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "A large annotated corpus for learning natural language inference", "abstract": "understanding entailment and contradiction directly is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable speech testing model ground goal for the development of semantic representations. however, machine learning research in this area has been dramatically limited by noticing the lack of large - dimensions scale resources. today to address this, we introduce the stanford natural language inference matching corpus, a new, freely available collection of labeled sentence pairs, written by humans is doing a most novel grounded recall task based on image captioning. at 570k pairs, yet it is two orders of magnitude, larger than all other grammar resources of its type. additionally this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network - based model to perform competitively on natural language inference benchmarks for the first time.", "histories": [["v1", "Fri, 21 Aug 2015 16:17:01 GMT  (270kb,D)", "http://arxiv.org/abs/1508.05326v1", "To appear at EMNLP 2015. The data will be posted shortly before the conference (the week of 14 Sep) atthis http URL"]], "COMMENTS": "To appear at EMNLP 2015. The data will be posted shortly before the conference (the week of 14 Sep) atthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "gabor angeli", "christopher potts", "christopher d manning"], "accepted": true, "id": "1508.05326"}, "pdf": {"name": "1508.05326.pdf", "metadata": {"source": "CRF", "title": "A large annotated corpus for learning natural language inference", "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "angeli@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.\nNLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground\nfor approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a).\nOur ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality.\nTo address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence. At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type. And, in contrast to many such resources, all of its sentences and labels were written by humans in a grounded, naturalistic context. In a separate validation phase, we collected four additional judgments for each label for 56,941 of the examples. Of these, 98% of cases emerge with a threeannotator consensus, and 58% see a unanimous consensus from all five annotators.\nIn this paper, we use this corpus to evaluate\nar X\niv :1\n50 8.\n05 32\n6v 1\n[ cs\n.C L\n] 2\n1 A\nug 2\n01 5\na variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art."}, {"heading": "2 A new corpus for NLI", "text": "To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, \u00a76). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-\n1http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool\nplementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject\u2013verb\u2013 object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases.\nExisting resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 \u00a74.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then we will only ever find contradictions between sentences that make broad universal assertions, but if we opt to assume coreference, new counterintuitive predictions emerge. For example, Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contradiction, rather than neutral, under this assumption.\nEntity coreference presents a similar kind of indeterminacy, as in the pair A tourist visited New\nYork and A tourist visited the city. Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral. This kind of indeterminacy of label can be resolved only once the questions of coreference are resolved.\nWith SNLI, we sought to address the issues of size, quality, and indeterminacy. To do this, we employed a crowdsourcing framework with the following crucial innovations. First, the examples were grounded in specific scenarios, and the premise and hypothesis sentences in each example were constrained to describe that scenario from the same perspective, which helps greatly in controlling event and entity coreference.2 Second, the prompt gave participants the freedom to produce entirely novel sentences within the task setting, which led to richer examples than we see with the more proscribed string-editing techniques of earlier approaches, without sacrificing consistency. Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data, and at identifying areas of inferential uncertainty."}, {"heading": "2.1 Data collection", "text": "We used Amazon Mechanical Turk for data collection. In each individual task (each HIT), a worker was presented with premise scene descriptions from a pre-existing corpus, and asked to supply hypotheses for each of our three labels\u2014 entailment, neutral, and contradiction\u2014forcing the data to be balanced among these classes.\nThe instructions that we provided to the workers are shown in Figure 1. Below the instructions were three fields for each of three requested sentences, corresponding to our entailment, neutral, and contradiction labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page. That FAQ grew over the course of data collection. It warned about disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and\n2 Issues of coreference are not completely solved, but greatly mitigated. For example, with the premise sentence A dog is lying in the grass, a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog.\ncomplexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sentences), and reviewed logistical issues around payment timing. About 2,500 workers contributed.\nFor the premises, we used captions from the Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.3 The captions were not authored by the photographers who took the source images, and they tend to contain relatively literal scene descriptions that are suited to our approach, rather than those typically associated with personal photographs (as in their example: Our trip to the Olympic Peninsula). In order to ensure that the label for each sentence pair can be recovered solely based on the available text, we did not use the images at all during corpus collection.\nTable 2 reports some key statistics about the collected corpus, and Figure 2 shows the distributions of sentence lengths for both our source hypotheses and our newly collected premises. We observed that while premise sentences varied considerably in length, hypothesis sentences tended to be as\n3 We additionally include about 4k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus (under construction; visualgenome.org). These examples appear only in the training set, and have pair identifiers prefixed with vg in our corpus.\nData set sizes: Training pairs 550,152 Development pairs 10,000 Test pairs 10,000\nSentence length: Premise mean token count 14.1 Hypothesis mean token count 8.3\nParser output: Premise \u2018S\u2019-rooted parses 74.0% Hypothesis \u2018S\u2019-rooted parses 88.9% Distinct words (ignoring case) 37,026\nTable 2: Key statistics for the raw sentence pairs in SNLI. Since the two halves of each pair were collected separately, we report some statistics for both.\nshort as possible while still providing enough information to yield a clear judgment, clustering at around seven words. We also observed that the bulk of the sentences from both sources were syntactically complete rather than fragments, and the frequency with which the parser produces a parse rooted with an \u2018S\u2019 (sentence) node attests to this."}, {"heading": "2.2 Data validation", "text": "In order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we performed an additional round of validation for about 10% of our data. This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we presented workers with pairs of sentences in batches of five, and asked them to choose a single label for each pair. We supplied each pair to four annotators, yielding five labels per pair including the label used by the original author. The instructions were similar to the instructions for initial data collection shown in Figure 1, and linked to a similar FAQ. Though we initially used a very restrictive qualification (based on past approval rate) to select workers for the validation task, we nonetheless discovered (and deleted) some instances of random guessing in an early batch of work, and subsequently instituted a fully closed qualification restricted to about 30 trusted workers.\nFor each pair that we validated, we assigned a gold label. If any one of the three labels was chosen by at least three of the five annotators, it was\nLHS RHS 0 0 0 1 1 39 1 2 42 1011 1/2/00 3 156 7980 3 4 1095 29471 4 5 3882 61196 5 6 12120 74094 6 7 26514 93600 7 8 37434 85851 8 9 44028 61359 9 10 49245 46711 10 11 50919 33241 11 12 48363 22844 12 13 43314 15994 13 14 38121 11047 14 15 33183 7601 5 16 27621 5312 16 17 23250 3732 17 18 20247 2631 18 19 18513 1878 19 20 16386 1325 20 21 13746 911 21 22 12066 642 22 23 9183 449 23 24 7131 357 24\n25 6198 217 25\n26 5007 168 26\n27 3963 138 27\n28 3438 84 28\n29 2631 67 29\n30 1959 46 30 31 1956 26 31 32 1434 31 32 33 1086 23 33 34 912 16 34 35 897 19 35 36 774 8 36 37 453 12 37 38 618 4 38 39 291 5 39 40 330 2 40 41 249 4 41 42 180 2 42 43 225 1 43 44 162 1 44 45 108 1 48 46 87 1 51 47 60 2 55 48 36 1 56 49 90 1 60 50 21 1 62 51 66 52 51 53 36 54 24 55 63 56 18 57 15 58 6 59 27 60 6 61 3 62 3 63 3 64 6 65 3 66 3 67 6 68 6 69 18 70 15 71 3 72 15 73 3 75 15 79 3 82 15\n0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000\n0 5 10 15 20 25 30 35 40\nN um\nbe r\nof se\nnt en\nce s\nSentence length (tokens)\nPremise Hypothesis\nFigure 2: The distribution of sentence length.\nchosen as the gold label. If there was no such consensus, which occurred in about 2% of cases, we assigned the placeholder label \u2018-\u2019. While these unlabeled examples are included in the corpus distribution, they are unlikely to be helpful for the standard NLI classification task, and we do not include them in either training or evaluation in the experiments that we discuss in this paper.\nThe results of this validation process are summarized in Table 3. Nearly all of the examples received a majority label, indicating broad consensus about the nature of the data and categories. The gold-labeled examples are very nearly evenly distributed across the three labels. The Fleiss \u03ba scores (computed over every example with a full five annotations) are likely to be conservative given our large and unevenly distributed pool of annotators, but they still provide insights about the levels of disagreement across the three semantic classes. This disagreement likely reflects not just the limitations of large crowdsourcing efforts but also the uncertainty inherent in naturalistic NLI. Regardless, the overall rate of agreement is extremely high, suggesting that the corpus is sufficiently high quality to pose a challenging but realistic machine learning task."}, {"heading": "2.3 The distributed corpus", "text": "Table 1 shows a set of randomly chosen validated examples from the development set with their labels. Qualitatively, we find the data that we collected draws fairly extensively on commonsense knowledge, and that hypothesis and premise sentences often differ structurally in significant ways, suggesting that there is room for improvement beyond superficial word alignment models. We also find the sentences that we collected to be largely\nfluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted.\nThe corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/\nPartition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated.\nParses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions."}, {"heading": "3 Our data as a platform for evaluation", "text": "The most immediate application for our corpus is in developing models for the task of NLI. In par-\nticular, since it is dramatically larger than any existing corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task. Our ability to evaluate standard classifier-base NLI models, however, was limited to those which were designed to scale to SNLI\u2019s size without modification, so a more complete comparison of approaches will have to wait for future work. In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models."}, {"heading": "3.1 Excitement Open Platform models", "text": "The first class of models is from the Excitement Open Platform (EOP, Pado\u0301 et al. 2014; Magnini et al. 2014)\u2014an open source platform for RTE research. EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP\u2019s full suite of lexical resources.\nOur initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models\nwas separately trained on the training set of each corpus. All models are evaluated only on 2-class entailment. To convert 3-class problems like SICK and SNLI to this setting, all instances of contradiction and unknown are converted to nonentailment. This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words. In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007)."}, {"heading": "3.2 Lexicalized Classifier", "text": "Unlike the RTE datasets, SNLI\u2019s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding. Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized:\n1. The BLEU score of the hypothesis with respect to the premise, using an n-gram length between 1 and 4.\n2. The length difference between the hypothesis and the premise, as a real-valued feature.\n3. The overlap between words in the premise and hypothesis, both as an absolute count and a percentage of possible overlap, and both over all words and over just nouns, verbs, adjectives, and adverbs.\n4. An indicator for every unigram and bigram in the hypothesis.\n5. Cross-unigrams: for every pair of words across the premise and hypothesis which share a POS tag, an indicator feature over the two words.\n6. Cross-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an indicator feature over the two bigrams.\nWe report results in Table 5, along with ablation studies for removing the cross-bigram features (leaving only the cross-unigram feature) and\nfor removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features. The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification. A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis.\nIt is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model."}, {"heading": "3.3 Sentence embeddings and NLI", "text": "SNLI is suitably large and diverse to make it possible to train neural network models that produce distributed representations of sentence meaning. In this section, we compare the performance of three such models on the corpus. To focus specifically on the strengths of these models at producing informative sentence representations, we use sentence embedding as an intermediate step in the NLI classification task: each model must produce a vector representation of each of the two sentences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which predicts the label for the pair. This choice allows us to focus on existing models for sentence embedding, and it allows us to evaluate the ability of those models to learn useful representations of meaning (which may be independently useful for subsequent tasks), at the cost of excluding from con-\nsideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level.\nOur neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself.\nWe test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997).\nThe word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient \u03bb for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen-\ntence embedding models (though not to its internal connections) with a fixed dropout rate. All models were implemented in a common framework for this paper, and the implementations will be made available at publication time.\nThe results are shown in Table 6. The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier\u2014 while the sum of words model can use pretrained word embeddings to better handle rare words, it lacks even the rudimentary sensitivity to word order that the lexicalized model\u2019s bigram features provide. Of the two RNN models, the LSTM\u2019s more robust ability to learn long-term dependencies serves it well, giving it a substantial advantage over the plain RNN, and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set (LSTM performance near the stopping iteration varies by up to 0.5% between evaluation steps). While the lexicalized model fits the training set almost perfectly, the gap between train and test set accuracy is relatively small for all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive."}, {"heading": "3.4 Analysis and discussion", "text": "Figure 4 shows a learning curve for the LSTM and the lexicalized and unlexicalized feature-based models. It shows that the large size of the corpus is crucial to both the LSTM and the lexicalized model, and suggests that additional data would yield still better performance for both. In addition, though the LSTM and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets.\nWe were struck by the speed with which the lexicalized classifier outperforms its unlexicalized\n33.33 40.08 47.63 57.72 67.42 71.88 73.95 76.78 78.22\ncounterpart. With only 100 training examples, the cross-bigram classifier is already performing better. Empirically, we find that the top weighted features for the classifier trained on 100 examples tend to be high precision entailments; e.g., playing \u2192 outside (most scenes are outdoors), a banana \u2192 person eating. If relatively few spurious entailments get high weight\u2014as it appears is the case\u2014 then it makes sense that, when these do fire, they boost accuracy in identifying entailments.\nThere are revealing patterns in the errors common to all the models considered here. Despite the large size of the training corpus and the distributional information captured by GloVe initialization, many lexical relationships are still misanalyzed, leading to incorrect predictions of independent, even for pairs that are common in the training corpus like beach/surf and sprinter/runner. Semantic mistakes at the phrasal level (e.g., predicting contradiction for A male is placing an order in a deli/A man buying a sandwich at a deli) indicate that additional attention to compositional semantics would pay off. However, many of the persistent problems run deeper, to inferences that depend on world knowledge and contextspecific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lexicalized classifier and the LSTM predict neutral. In other cases, the models\u2019 attempts to shortcut\nthis kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can\u2019t stand the ocean, presumably because of distributional associations between throws and can\u2019t stand.\nAnalysis of the models\u2019 predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner, 2004). Our annotators generally followed suit, writing sentences that, while structurally diverse, share topic/focus (theme/rheme) structure with their premises. This promotes a coherent, situation-specific construal of each sentence pair. This is information that our models can easily take advantage of, but it can lead them astray. For instance, all of them stumble with the amusingly simple case A woman prepares ingredients for a bowl of soup/A soup bowl prepares a woman, in which prior expectations about parallelism are not met. Another headline example of this type is A man wearing padded arm protection is being bitten by a German shepherd dog/A man bit a dog, which all the models wrongly diagnose as entailment, though the sentences report two very different stories. A model with access to explicit information about syntactic or semantic structure should perform better on cases like these."}, {"heading": "4 Transfer learning with SICK", "text": "To the extent that successfully training a neural network model like our LSTM on SNLI forces that model to encode broadly accurate representations of English scene descriptions and to build an entailment classifier over those relations, we should expect it to be readily possible to adapt the trained model for use on other NLI tasks. In this section, we evaluate on the SICK entailment task using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results.\nTo perform transfer, we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model, which is trained from that point only on the training portion of SICK. The only newly initialized parameters are\nsoftmax layer parameters and the embeddings for words that appear in SICK, but not in SNLI (which are populated with GloVe embeddings as above). We use the same model hyperparameters that were used to train the original model, with the exception of the L2 regularization strength, which is re-tuned. We additionally transfer the accumulators that are used by AdaDelta to set the learning rates. This lowers the starting learning rates, and is intended to ensure that the model does not learn too quickly in its first few epochs after transfer and destroy the knowledge accumulated in the pre-transfer phase of training.\nThe results are shown in Table 7. Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labeling more neutral examples as contradictions than correctly, possibly as a result of subtle differences in how the labeling task was presented. In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling. This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks.\nWe attempted to apply this same transfer evaluation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training configuration yielded competitive performance. Further research on effective transfer learning on small data sets with neural models might facilitate improvements here."}, {"heading": "5 Conclusion", "text": "Natural languages are powerful vehicles for reasoning, and nearly all questions about meaningfulness in language can be reduced to questions of entailment and contradiction in context. This suggests that NLI is an ideal testing ground for theories of semantic representation, and that training for NLI tasks can provide rich domain-general semantic representations. To date, however, it has not been possible to fully realize this potential due to the limited nature of existing NLI resources. This paper sought to remedy this with a new, largescale, naturalistic corpus of sentence pairs labeled for entailment, contradiction, and independence. We used this corpus to evaluate a range of models, and found that both simple lexicalized models and neural network models perform well, and that the representations learned by a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset. We hope that SNLI presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge support from a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA875013-2-0040, the National Science Foundation under grant no. IIS 1159679, and the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR, or the US government. We also thank our many excellent Mechanical Turk contributors."}], "references": [{"title": "Recognising textual entailment with logical inference", "author": ["Johan Bos", "Katja Markert."], "venue": "Proc. EMNLP.", "citeRegEx": "Bos and Markert.,? 2005", "shortCiteRegEx": "Bos and Markert.", "year": 2005}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "VerbOcean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "Proc. EMNLP.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Entailment, intensionality and text understanding", "author": ["Cleo Condoravdi", "Dick Crouch", "Valeria de Paiva", "Reinhard Stolle", "Daniel G. Bobrow."], "venue": "Proc. of the HLT-NAACL 2003 Workshop on Text Meaning.", "citeRegEx": "Condoravdi et al\\.,? 2003", "shortCiteRegEx": "Condoravdi et al\\.", "year": 2003}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Finding contradictions in text", "author": ["Marie-Catherine de Marneffe", "Anna N. Rafferty", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Marneffe et al\\.,? 2008", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Brown corpus manual", "author": ["W. Nelson Francis", "Henry Kucera."], "venue": "Brown University.", "citeRegEx": "Francis and Kucera.,? 1979", "shortCiteRegEx": "Francis and Kucera.", "year": 1979}, {"title": "A natural logic inference system", "author": ["Yaroslav Fyodorov", "Yoad Winter", "Nissim Francez."], "venue": "Proc. of the 2nd Workshop on Inference in Computational Semantics.", "citeRegEx": "Fyodorov et al\\.,? 2000", "shortCiteRegEx": "Fyodorov et al\\.", "year": 2000}, {"title": "The third PASCAL recognizing textual entailment challenge", "author": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan."], "venue": "Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing.", "citeRegEx": "Giampiccolo et al\\.,? 2007", "shortCiteRegEx": "Giampiccolo et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Semantic Theory", "author": ["Jerrold J. Katz."], "venue": "Harper & Row, New York.", "citeRegEx": "Katz.,? 1972", "shortCiteRegEx": "Katz.", "year": 1972}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "On our best behaviour", "author": ["Hector J. Levesque."], "venue": "Proc. AAAI.", "citeRegEx": "Levesque.,? 2013", "shortCiteRegEx": "Levesque.", "year": 2013}, {"title": "Focused entailment graphs for open IE propositions", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger."], "venue": "Proc. CoNLL.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D Manning."], "venue": "Proc. of the Eighth International Conference on Computational Semantics.", "citeRegEx": "MacCartney and Manning.,? 2009", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "The Excitement Open Platform for textual inferences", "author": ["Bernardo Magnini", "Roberto Zanoli", "Ido Dagan", "Kathrin Eichler", "G\u00fcnter Neumann", "Tae-Gil Noh", "Sebastian Pado", "Asher Stern", "Omer Levy."], "venue": "Proc. ACL.", "citeRegEx": "Magnini et al\\.,? 2014", "shortCiteRegEx": "Magnini et al\\.", "year": 2014}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and tex", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proc. LREC.", "citeRegEx": "Marelli et al\\.,? 2014b", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "WordNet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Design and realization of a modular architecture for textual entailment", "author": ["Sebastian Pad\u00f3", "Tae-Gil Noh", "Asher Stern", "Rui Wang", "Roberto Zanoli."], "venue": "Journal of Natural Language Engineering.", "citeRegEx": "Pad\u00f3 et al\\.,? 2014", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2014}, {"title": "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Johan Bos", "Malvina Nissim", "Charley Beller", "Ben Van Durme", "Chris Callison-Burch"], "venue": "In Proc. ACL", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Direct transfer of learned information among neural networks", "author": ["Lorien Y Pratt", "Jack Mostow", "Candace A Kamm", "Ace A Kamm."], "venue": "Proc. AAAI.", "citeRegEx": "Pratt et al\\.,? 1991", "shortCiteRegEx": "Pratt et al\\.", "year": 1991}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proc. EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A brief history of natural logic", "author": ["Johan van Benthem."], "venue": "M. Chakraborty, B. L\u00f6we, M. Nath Mitra, and S. Sarukki, editors, Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal. College Publications.", "citeRegEx": "Benthem.,? 2008", "shortCiteRegEx": "Benthem.", "year": 2008}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida I. Wang", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Recognizing textual entailment using sentence similarity based on dependency tree skeletons", "author": ["Rui Wang", "G\u00fcnter Neumann."], "venue": "ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.", "citeRegEx": "Wang and Neumann.,? 2007", "shortCiteRegEx": "Wang and Neumann.", "year": 2007}, {"title": "Information structure and non-canonical syntax", "author": ["Gregory Ward", "Betty Birner."], "venue": "Laurence R. Horn and Gregory Ward, editors, Handbook of Pragmatics, pages 153\u2013174. Blackwell, Oxford.", "citeRegEx": "Ward and Birner.,? 2004", "shortCiteRegEx": "Ward and Birner.", "year": 2004}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015a", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "Proc. ICLR.", "citeRegEx": "Weston et al\\.,? 2015b", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Understanding natural language", "author": ["Terry Winograd."], "venue": "Cognitive Psychology, 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "TACL, 2:67\u2013", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts.", "startOffset": 109, "endOffset": 140}, {"referenceID": 7, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 3, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 0, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 4, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 15, "context": "Thus, natural language inference (NLI) \u2014 characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) \u2014 is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "startOffset": 107, "endOffset": 228}, {"referenceID": 1, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 31, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 30, "context": "they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).", "startOffset": 101, "endOffset": 166}, {"referenceID": 33, "context": "The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-", "startOffset": 36, "endOffset": 56}, {"referenceID": 21, "context": "(2014) introduce a large corpus of semi-automatically annotated entailment examples between subject\u2013verb\u2013 object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) in-", "startOffset": 181, "endOffset": 203}, {"referenceID": 14, "context": "sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject\u2013verb\u2013 object relation triples, and the second release of the Paraphrase Database (Pavlick et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 18, "context": "determinacy concerning the correct semantic label (de Marneffe et al. 2008 \u00a74.3; Marelli et al. 2014b).", "startOffset": 50, "endOffset": 102}, {"referenceID": 33, "context": "Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions.", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": "The first class of models is from the Excitement Open Platform (EOP, Pad\u00f3 et al. 2014; Magnini et al. 2014)\u2014an open source platform for RTE research.", "startOffset": 63, "endOffset": 107}, {"referenceID": 8, "context": "We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007).", "startOffset": 138, "endOffset": 164}, {"referenceID": 19, "context": "In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes", "startOffset": 138, "endOffset": 152}, {"referenceID": 2, "context": "In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes", "startOffset": 167, "endOffset": 195}, {"referenceID": 28, "context": "use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007).", "startOffset": 69, "endOffset": 93}, {"referenceID": 27, "context": "A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis.", "startOffset": 30, "endOffset": 54}, {"referenceID": 9, "context": "In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997).", "startOffset": 98, "endOffset": 132}, {"referenceID": 34, "context": "All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving.", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "We applied L2 regularization to all models, manually tuning the strength coefficient \u03bb for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the senSentence model Train Test", "startOffset": 130, "endOffset": 155}, {"referenceID": 32, "context": "Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).", "startOffset": 70, "endOffset": 102}, {"referenceID": 13, "context": "Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).", "startOffset": 70, "endOffset": 102}, {"referenceID": 29, "context": "(Ward and Birner, 2004).", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "In this section, we evaluate on the SICK entailment task using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results.", "startOffset": 97, "endOffset": 117}, {"referenceID": 12, "context": "6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling.", "startOffset": 3, "endOffset": 30}], "year": 2015, "abstractText": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "creator": "LaTeX with hyperref package"}}}