{"id": "1508.05902", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2015", "title": "A Framework for Comparing Groups of Documents", "abstract": "we present a general analytic framework for comparing multiple groups weights of documents. a bipartite graph model is proposed where document groups are represented simply as one node set and furthermore the comparison resolution criteria are represented as the other the node set. using this model, we present basic algorithms to extract insights into similarities each and differences among the document groups. finally, we demonstrate the versatility of our framework through an optimization analysis of nsf funding programs for basic research.", "histories": [["v1", "Mon, 24 Aug 2015 18:14:34 GMT  (314kb)", "http://arxiv.org/abs/1508.05902v1", "6 pages; 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15)"]], "COMMENTS": "6 pages; 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15)", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["arun maiya"], "accepted": true, "id": "1508.05902"}, "pdf": {"name": "1508.05902.pdf", "metadata": {"source": "CRF", "title": "A Framework for Comparing Groups of Documents", "authors": ["Arun S. Maiya"], "emails": ["amaiya@ida.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n05 90\n2v 1\n[ cs\n.C L\n] 2\n4 A\nug 2\n01 5"}, {"heading": "1 Introduction and Motivation", "text": "Given multiple sets (or groups) of documents, it is often necessary to compare the groups to identify similarities and differences along different dimensions. In this work, we present a general framework to perform such comparisons for extraction of important insights. Indeed, many real-world tasks can be framed as a problem of comparing two or more groups of documents. Here, we provide two motivating examples.\n1. Program Reviews. To better direct research efforts, funding organizations such as the National Science Foundation (NSF), the National Institutes of Health (NIH), and the Department of Defense (DoD), are often in the position of reviewing research programs via their artifacts (e.g., grant abstracts, published papers, and other research descriptions). Such reviews might involve identifying overlaps across different programs, which may indicate a duplication of effort. It may also involve the identification of unique, emerging, or diminishing topics. A \u201cdocument group\u201d here could be defined either as a particular research program that funds many organizations, the totality of funded research conducted by a specific organization, or all research associated with a particular time period (e.g., fiscal year). In all cases, the objective is to draw comparisons between groups by comparing the document sets associated with them.\n2. Intelligence. In the areas of defense and intelligence, document sets are sometimes obtained from dif-\nferent sources or entities. For instance, the U.S. Armed Forces sometimes seize documents during raids of terrorist strongholds.1 Similarities between two document sets (each captured from a different source) can potentially be used to infer a non-obvious association between the sources.\nOf course, there are numerous additional examples across many domains (e.g., comparing different news sources, comparing the reviews for several products, etc.). Given the abundance of real-world applications as illustrated above, it is surprising, then, that there are no existing general-purpose approaches for drawing such comparisons. While there is some previous work on the comparison of document sets (referred to as comparative text mining), these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria. Moreover, much of the work in the area focuses largely on the summarization of shared or unshared topics among document groups (e.g., Wan et al. (2011), Huang et al. (2011), Campr and Jez\u030cek (2013), Wang et al. (2012), Zhai et al. (2004)). That is, the problem of drawing multi-faceted comparisons among the groups themselves is not typically addressed. This, then, motivates our development of a general-purpose model for comparisons of document sets along arbitrary dimensions. We use this model for the identification of similarities, differences, trends, and anomalies among large groups of documents. We begin by formally describing our model."}, {"heading": "2 Our Formal Model for Comparing Document Groups", "text": "As input, we are given several groups of documents, and our task is to compare them. We now formally define these document groups and the criteria used to compare them. Let D = {d1, d2, . . . , dN} be a document collection comprising the totality of documents under consideration, where N is the size. Let DP be a partition of D representing the document groups.\n1See Document Exploitation (DOCEX) at http://en.wikipedia.org for more information.\nDefinition 1 A document group is a subset DPi \u2208 DP (where index i \u2208 {1 . . . |DP |}).\nEach document group in DP , for instance, might represent articles associated with either a particular organization (e.g., university), a research funding source (e.g., NSF or DARPA program), or a time period (e.g., a fiscal year). Document groups are compared using comparison criteria, DC , a family of subsets of D.\nDefinition 2 A comparison criterion is a subset DCi \u2208 DC (where index i \u2208 {1 . . . |DC |}).\nIntuitively, each subset of DC represents a set of documents sharing some attribute. Our model allows great flexibility in how DC is defined. For instance, DC might be defined by the named entities mentioned within documents (e.g., each subset contains documents that mention a particular person or organization of interest). For the present work, we defineDC by topics discovered using latent Dirichlet allocation or LDA (Blei et al., 2003).\nLDA Topics as Comparison Criteria. Probabilistic topic modeling algorithms like LDA discover latent themes (i.e., topics) in document collections. By using these discovered topics as the comparison criteria, we can compare arbitrary groups of documents by the themes and subject areas comprising them. Let K be the number of topics or themes in D. Each document in D is composed of a sequence of words: di = \u3008si1, si2, . . . , siNi\u3009, where Ni is the number of words in di and i \u2208 {1 . . .N}. V = \u22c3N i=1 f(di) is the vocabulary of D, where f(\u00b7) takes a sequence of elements and returns a set. LDA takes K and D (including its components such as V ) as input and produces two matrices as output, one of which is \u03b8. The matrix \u03b8 \u2208 RN\u00d7K is the document-topic distribution matrix and shows the distribution of topics within each document. Each row of the matrix represents a probability distribution. DC is constructed using K subsets of documents, each of which represent a set of documents pertaining largely to the same topic. That is, for t \u2208 {1 . . .K} and i \u2208 {1 . . .N}, each subset DCt \u2208 DC is comprised of all documents di where t = argmaxx \u03b8ix.\n2 Having defined the document groups DP and the comparison criteria DC , we now construct a bipartite graph model used to perform comparisons.\nA Bipartite Graph Model. Our objective is to compare the document groups in DP based on DC . We do so by representing DP and DC as a weighted bipartite graph, G = (P,C,E,w), where P and C are disjoint sets of nodes, E is the edge set, and w : E \u2192 Z+ are the edge weights. Each subset of DP is represented as a node in P , and each subset of DC is rep-\n2 D C is also a partition of D, when defined in this way.\nresented as a node in C. Let \u03b1 : P \u2192 DP and \u03b2 : C \u2192 DC be functions that map nodes to the document subsets that they represent. Then, the edge set E is {(u, v) | u \u2208 P, v \u2208 C,\u03b1(u) \u2229 \u03b2(v) 6= \u2205}, and the edge weight for any two nodes u \u2208 P and v \u2208 C is w((u, v)) = |\u03b1(u) \u2229 \u03b2(v)|. Concisely, each weighted edge in G between a document group (in P ) and a topic (in C) represents the number of documents shared among the two sets. Figure 1 shows a toy illustration of the model. Each node in P is shown in black and represents a subset of DP (i.e., a document group). Each node in C is shown in gray and represents a subset of DC (i.e., a document cluster pertaining primarily to the same topic). Each edge represents the intersection of the two subsets it connects. In the next section, we will describe basic algorithms on such bipartite graphs capable of yielding important insights into the similarities and differences among document groups."}, {"heading": "3 Basic Algorithms Using the Model", "text": "We focus on three basic operations in this work.\nNode Entropy. Let ~w be a vector of weights for all edges incident to some node v \u2208 E. The entropy H of v is: H(v) = \u2212\u2211i pi log|~w|(pi), where pi = wi\u2211\nj wj\nand i, j \u2208 {1 . . . |~w|}. A similar formulation was employed in Eagle et al. (2010). Intuitively, if v \u2208 P , H(v) measures the extent to which the document group is concentrated around a small number of topics (lower values of H(v) mean more concentrated). Similarly, if v \u2208 C, it is the extent to which a topic is concentrated around a small number of document groups.\nNode Similarity. Given a graph G, there are many ways to measure the similarity of two nodes based on their connections. Such measures can be used to infer similarity (and dissimilarity) among document groups. However, existing methods are not well-suited for the task of document group comparison. The well-\nknown SimRank algorithm (Jeh and Widom, 2002) ignores edge weights, and neither SimRank nor its extension, SimRank++ (Antonellis et al., 2008), scale to larger graphs. SimRank++ and ASCOS (Chen and Giles, 2013) do incorporate edge weights but in ways that are not appropriate for document group comparisons. For instance, both SimRank++ and ASCOS incorporate magnitude in the similarity computation. Consider the case where document groups are defined as research labs. ASCOS and SimRank++ will measure large research labs and small research labs as less similar when in fact they may publish nearly identical lines of research. Finally, under these existing methods, document groups sharing zero topics in common could still be considered similar, which is undesirable here. For these reasons, we formulate similarity as follows. Let NG(\u00b7) be a function that returns the neighbors of a given node in G. Given two nodesu, v \u2208 P , let Lu,v = NG(u)\u222aNG(v) and let x : I \u2192 Lu,v be the indexing function for Lu,v.3 We construct two vectors, ~a and~b, where ak = w(u, x(k)), bk = w(v, x(k)), and k \u2208 I . Each vector is essentially a sequence of weights for edges between u, v \u2208 P and each node in Lu,v. Similarity of two nodes is measured using the cosine similarity of their corresponding sequences, ~a\u00b7 ~b\n\u2016~a\u2016\u2016~b\u2016 , which we compute using a func-\ntion sim(\u00b7, \u00b7). Thus, document groups are considered more similar when they have similar sets of topics in similar proportions. As we will show later, this simple solution, based on item-based collaborative filtering (Sarwar et al., 2001), is surprisingly effective at inferring similarity among document groups in G.\nNode Clusters. Identifying clusters of related nodes in the bipartite graph G can show how document groups form larger classes. However, we find that G is typically fairly dense. For these reasons, partitioning of the one-mode projection of G and other standard bipartite graph clustering techniques (e.g., Dhillion (2001) and Sun et al. (2009)) are rendered less effective. We instead employ a different tack and exploit the node similarities computed earlier. We transform G into a new weighted graph GP = (P,EP , wsim) where EP = {(u, v) | u, v \u2208 P, sim(u, v) > \u03be}, \u03be is a predefined threshold, andwsim is the edge weight function (i.e., wsim = sim). Thus, GP is the similarity graph of document groups. \u03be = 0.5 was used as the threshold for our analyses. To find clusters in GP , we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008). Modularity measures the fraction of edges falling within clusters as compared to the expected fraction if edges were distributed evenly in the graph (Newman, 2006). The algorithm initially assigns each node to its own cluster.\n3 I is the index set of Lu,v .\nAt each iteration, in a local and greedy fashion, nodes are re-assigned to clusters with which they achieve the highest modularity."}, {"heading": "4 Example Analysis: NSF Grants", "text": "As a realistic and informative case study, we utilize our model to characterize funding programs of the National Science Foundation (NSF). This corpus consists of 132,372 grant abstracts describing awards for basic research and other support funded by the NSF between the years 1990 and 2002 (Bache and Lichman, 2013).4 Each award is associated with both a program element (i.e., funding source) and a date. We define document groups in two ways: by program element and by calendar year. For comparison criteria, we used topics discovered with the MALLET implementation of LDA (McCallum, 2002) using K = 400 as the number of topics and 200 as the number of iterations. All other parameters were left as defaults. The NSF corpus possesses unique properties that lend themselves to experimental evaluation. For instance, program elements are not only associated with specific sets of research topics but are named based on the content of the program. This provides a measure of ground truth against which we can validate our model. We structure our analyses around specific questions, which now follow.\nWhich NSF programs are focused on specific areas and which are not? When defining document groups as program elements (i.e., each NSF program is a node inP ), node entropy can be used to answer this question. Table 1 shows examples of program elements most and least associated with specific topics, as measured by entropy. For example, the program 1311 Linguistics (low entropy) is largely focused on a single linguistics topic (labeled by LDA with words such as \u201clanguage,\u201d \u201clanguages,\u201d and \u201clinguistic\u201d). By contrast, the Australia program (high entropy) was designed to support US-Australia cooperative research across many fields, as correctly inferred by our model.\nWhich research areas are growing/emerging? When defining document groups as calendar years (instead of program elements), low entropy nodes in C are topics concentrated around certain years. Concentrations in\n4Data for years 1989 and 2003 in this publicly available corpus were partially missing and omitted in some analyses.\nlater years indicate growth. The LDA-discovered topic nanotechnology is among the lowest entropy topics (i.e., an outlier topic with respect to entropy). As shown in Figure 2, the number of nanotechnology grants drastically increased in proportion through 2002. This result is consistent with history, as the National Nanotechnology Initiative was proposed in the late 1990s to promote nanotechnology R&D.5 One could also measure such trends using budget allocations by incorporating the award amounts into the edge weights of G.\nGiven an NSF program, to which other programs is it most similar? As described in Section 3, when each node in P represents an NSF program, our model can easily identify the programs most similar to a given program. For instance, Table 2 shows the top three most similar programs to both the Theoretical Physics and Ecology programs. Results agree with intuition. For each NSF program, we identified the top n most similar programs ranked by our sim(\u00b7, \u00b7) function, where n \u2208 {3, 6, 9}. These programs were manually judged for relatedness, and the Mean Average Precision (MAP), a standard performance metric for ranking tasks in information retrieval, was computed. We were unsuccessful in evaluating alternative weighted similarity measures mentioned in Section 3 due to their aforementioned issues with scalability and the size of the NSF dataset. (For instance, the implementations of ASCOS (Antonellis et al., 2008) and SimRank (Jeh and Widom, 2002) that we considered are available here.6) Recall that our sim(\u00b7, \u00b7) function is based on measuring the cosine similarity between two weight vectors, ~a and ~b, generated from our bipartite graph model. As a baseline for comparison, we evaluated two additional similarity implementations using these weight vectors. The first measures the similarity between weight vectors using weighted Jaccard similarity, which is \u2211 k min(ak,bk)\u2211\nk max(ak,bk) (denoted as\n5See National Nanotechnology Initiative at http://en.wikipedia.org for more information.\n6See networkx addon project at http://github.com/hhchen1105/.\nWtd. Jaccard). The second measure is implemented by taking the Spearman\u2019s rank correlation coefficient of ~a and~b (denoted as Rank). Figure 3 shows the Mean Average Precision (MAP) for each method and each value of n. With the exception of the difference between Cosine and Wtd. Jaccard for MAP@3, all other performance differentials were statistically significant, based on a one-way ANOVA and post-hoc Tukey HSD at a 5% significance level. This, then, provides some validation for our choice.\nHow do NSF programs join together to form larger program categories? As mentioned, by using the similarity graph GP constructed from G, clusters of related NSF programs can be discovered. Figure 4, for instance, shows a discovered cluster of NSF programs all related to the field of neuroscience. Each NSF program (i.e., node) is composed of many documents.\nWhich pairs of grants are the most similar in the research they describe? Although the focus of this paper is on drawing comparisons among groups of documents, it is often necessary to draw comparisons among individual documents, as well. For instance, in the case of this NSF corpus, one may wish to identify pairs of grants from different programs describing\nhighly similar lines of research. One common approach to this is to exploit the low-dimensional representations of documents returned by LDA (Blei et al., 2003). Any given document di \u2208 D (where i \u2208 {1 . . .N}) can be represented by a K-dimensional probability vector of topic proportions given by \u03b8i\u2217, the ith row of the document-topic matrix \u03b8. The similarity between any two documents, then, can be measured using the distance between their corresponding probability vectors (i.e., probability distributions). We quantify the similarity between probability vectors using the complement of Hellinger distance: HS(dx, dy) = 1 \u2212 1\u221a 2 \u221a \u2211K i=1( \u221a \u03b8xi \u2212 \u221a\n\u03b8yi)2, where x, y \u2208 {1 . . .N}. Unfortunately, identifying the set of most similar document pairs in this way can be computationally expensive, as the number of pairwise comparisons scales quadratically with the size of the corpus. For the moderately-sized NSF corpus, this amounts to well over 8 billion comparisons. To address this issue, our bipartite graph model can be exploited as a blocking heuristic using either the document groups or the comparison criteria. In the latter case, one can limit the pairwise comparisons to only those documents that reside in the same subset of DC . For the former case, node similarity can be used. Instead of comparing each document with every other document, we can limit the comparisons to only those document groups of interest that are deemed similar by our model. As an illustrative example, out of the 665 different NSF programs covering these 132, 372 grant abstracts, the program 1271 Computational Mathematics and the program 2865 Numeric, Symbolic, and Geometric Computation are inferred as being highly similar by our model. Thus, we can limit the pairwise comparisons to only such document groups that are similar and likely to contain similar documents. In the case of these two programs, the following two grants are easily identified as being the most similar with a Hellinger similarity (HS) score of 0.73 (only text snippets are shown due to space constraints):\nGrant #1 Program: 1271 Computational Mathematics\nTitle: Analyses of Structured Computational Problems and Parallel Iterative Algorithms.\nAbstract: The main objectives of the research planned is the analysis of large scale\nstructured computational problems and of the convergence of parallel iterative methods for solving linear systems and applications of these techniques to the solution of large sparse and dense structured systems of linear equations\nGrant #2 Program: 2865 Numeric, Symbolic, and Geometric Computation\nTitle: Sparse Matrix Algorithms on Distributed Memory Multiprocessors.\nAbstract: The design, analysis, and implementation of algorithms for the solution of sparse matrix problems on distributed memory multiprocessors will be investigated. The development of these parallel sparse matrix algorithms should have an impact of challenging large-scale computational problems in several scientific, econometric, and engineering disciplines.\nSome key terms in each grant are manually highlighted in bold. As can be seen, despite some differences in terminology, the two lines of research are related, as matrices (studied in Grant #2) are used to compactly represent and work with systems of linear equations (studied in Grant #1). That is, despite such differences in terminology (e.g., \u201cmatrix\u201d vs. \u201clinear systems\u201d, \u201cparallel\u201d vs. \u201cdistributed\u201d), document similarity can still be accurately inferred by taking the Hellinger similarity of the LDA-derived low-dimensional representations for the two documents. In this way, by exploiting the group-level similarities inferred by our model in combination with such document-level similarities, we can more effectively \u201czero in\u201d on such highly related document pairs."}, {"heading": "5 Conclusion", "text": "We have presented a bipartite graph model for drawing comparisons among large groups of documents. We showed how basic algorithms using the model can identify trends and anomalies among the document groups. As an example analysis, we demonstrated how our model can be used to better characterize and evaluate NSF research programs. For future work, we plan on employing alternative comparison criteria in our model such as those derived from named entity recognition and paraphrase detection."}], "references": [{"title": "Simrank++: Query Rewriting Through Link Analysis of the Click Graph", "author": ["Hector G. Molina", "Chi C. Chang"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "Antonellis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Antonellis et al\\.", "year": 2008}, {"title": "Latent Dirichlet Allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Fast unfolding of communities in large networks", "author": ["Jean-Loup Guillaume", "Renaud Lambiotte", "Etienne Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "Blondel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2008}, {"title": "Topic Models for Comparative Summarization", "author": ["Campr", "Je\u017eek2013] Michal Campr", "Karel Je\u017eek"], "venue": "Text, Speech, and Dialogue,", "citeRegEx": "Campr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Campr et al\\.", "year": 2013}, {"title": "ASCOS: An Asymmetric Network Structure COntext Similarity Measure", "author": ["Chen", "Giles2013] Hung H. Chen", "C. Lee Giles"], "venue": "In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Min-", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Comparative News Summarization Using Linear Programming", "author": ["Xiaojun Wan", "Jianguo Xiao"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "SimRank: a measure of structural-context similarity", "author": ["Jeh", "Widom2002] Glen Jeh", "Jennifer Widom"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Jeh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jeh et al\\.", "year": 2002}, {"title": "MALLET: A Machine Learning for Language Toolkit", "author": ["Andrew K. McCallum"], "venue": null, "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Modularity and community structure in networks", "author": ["M.E.J. Newman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Newman.,? \\Q2006\\E", "shortCiteRegEx": "Newman.", "year": 2006}, {"title": "Itembased Collaborative Filtering Recommendation Algorithms", "author": ["Sarwar et al.2001] Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 10th International Conference on World Wide Web,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "Ranking-based Clustering of Heterogeneous Information Networks with Star Network Schema", "author": ["Sun et al.2009] Yizhou Sun", "Yintao Yu", "Jiawei Han"], "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Summarizing the Differences in Multilingual News", "author": ["Wan et al.2011] Xiaojun Wan", "Houping Jia", "Shanshan Huang", "Jianguo Xiao"], "venue": "In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Wan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2011}, {"title": "Comparative Document Summarization via Discriminative Sentence Selection", "author": ["Wang et al.2012] Dingding Wang", "Shenghuo Zhu", "Tao Li", "Yihong Gong"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A Cross-collection Mixture Model for Comparative Text Mining", "author": ["Zhai et al.2004] ChengXiang Zhai", "Atulya Velivelli", "Bei Yu"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Zhai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": ", Wan et al. (2011), Huang et al.", "startOffset": 2, "endOffset": 20}, {"referenceID": 5, "context": "(2011), Huang et al. (2011), Campr and Je\u017eek (2013), Wang et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "(2011), Huang et al. (2011), Campr and Je\u017eek (2013), Wang et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 5, "context": "(2011), Huang et al. (2011), Campr and Je\u017eek (2013), Wang et al. (2012), Zhai et al.", "startOffset": 8, "endOffset": 72}, {"referenceID": 5, "context": "(2011), Huang et al. (2011), Campr and Je\u017eek (2013), Wang et al. (2012), Zhai et al. (2004)).", "startOffset": 8, "endOffset": 92}, {"referenceID": 1, "context": "For the present work, we defineD by topics discovered using latent Dirichlet allocation or LDA (Blei et al., 2003).", "startOffset": 95, "endOffset": 114}, {"referenceID": 0, "context": "known SimRank algorithm (Jeh and Widom, 2002) ignores edge weights, and neither SimRank nor its extension, SimRank++ (Antonellis et al., 2008), scale to larger graphs.", "startOffset": 117, "endOffset": 142}, {"referenceID": 9, "context": "As we will show later, this simple solution, based on item-based collaborative filtering (Sarwar et al., 2001), is surprisingly effective at inferring similarity among document groups in G.", "startOffset": 89, "endOffset": 110}, {"referenceID": 2, "context": "To find clusters in G , we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008).", "startOffset": 109, "endOffset": 131}, {"referenceID": 8, "context": "Modularity measures the fraction of edges falling within clusters as compared to the expected fraction if edges were distributed evenly in the graph (Newman, 2006).", "startOffset": 149, "endOffset": 163}, {"referenceID": 8, "context": ", Dhillion (2001) and Sun et al. (2009)) are rendered less effective.", "startOffset": 22, "endOffset": 40}, {"referenceID": 7, "context": "For comparison criteria, we used topics discovered with the MALLET implementation of LDA (McCallum, 2002) using K = 400 as the number of topics and 200 as the number of iterations.", "startOffset": 89, "endOffset": 105}, {"referenceID": 0, "context": "(For instance, the implementations of ASCOS (Antonellis et al., 2008) and SimRank (Jeh and Widom, 2002) that we considered are available here.", "startOffset": 44, "endOffset": 69}, {"referenceID": 1, "context": "One common approach to this is to exploit the low-dimensional representations of documents returned by LDA (Blei et al., 2003).", "startOffset": 107, "endOffset": 126}], "year": 2015, "abstractText": "We present a general framework for comparing multiple groups of documents. A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set. Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups. Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research.", "creator": "LaTeX with hyperref package"}}}