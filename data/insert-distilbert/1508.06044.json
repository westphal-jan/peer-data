{"id": "1508.06044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2015", "title": "Visualizing NLP annotations for Crowdsourcing", "abstract": "visualizing nlp annotation is useful for the collection of training data for the statistical nlp approaches. existing toolkits either provide potentially limited visual aid, or introduce comprehensive operators faster to realize sophisticated linguistic rules. workers only must be well trained to use them. their audience thus can hardly be scaled comparing to large amounts of busy non - expert crowdsourced workers. in this paper, assume we present through crowdanno, a visualization toolkit to allow unemployed crowd - sourced workers to annotate two general categories of nlp problems : clustering and parsing. workers can additionally finish the tasks with simplified operators in an interactive interface, and fix errors conveniently. user studies show our toolkit infrastructure is very friendly to nlp non - experts, and allow them to produce high quality labels for several sophisticated problems. we release our source code and toolkit to spur future research.", "histories": [["v1", "Tue, 25 Aug 2015 06:34:00 GMT  (867kb,D)", "http://arxiv.org/abs/1508.06044v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hanchuan li", "haichen shen", "shengliang xu", "congle zhang"], "accepted": false, "id": "1508.06044"}, "pdf": {"name": "1508.06044.pdf", "metadata": {"source": "CRF", "title": "Visualizing NLP annotations for Crowdsourcing", "authors": ["Hanchuan Li", "Haichen Shen", "Shengliang Xu", "Congle Zhang"], "emails": ["hanchuan@cs.washington.edu", "haichen@cs.washington.edu", "shengliang@cs.washington.edu", "clzhang@cs.washington.edu"], "sections": [{"heading": null, "text": "Visualizing NLP annotation is useful for the collection of training data for the statistical NLP approaches. Existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic rules. Workers must be well trained to use them. Their audience thus can hardly be scaled to large amounts of non-expert crowdsourced workers. In this paper, we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to annotate two general categories of NLP problems: clustering and parsing. Workers can finish the tasks with simplified operators in an interactive interface, and fix errors conveniently. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems. We release our source code and toolkit to spur future research."}, {"heading": "1 Introduction", "text": "Statistical machine learning approaches has made great successes in research disciplines such as parsing (Klein and Manning, 2003), information extractions (Banko et al., 2007), question answering (Kwok et al., 2001). Significant advances have been made in recent years on inferrring latent labels for sequence observations using neural networks (Socher et al., 2011; Huang and Rao, 2014) and kenerl methods (Zhao et al., 2012). Yet offthe-shelf models learned from training data of one particular domain (usually newswire) would often underperform at present tasks, whose data could come from other domains such as social media and biomedical data (Wu et al., 2014). To fully exploit the power of statistical approaches, it is useful to\nquickly collect plentiful in-domain training data in a short period of time.\nWith the advent of crowd-sourcing platforms (e.g. Amazon Mechanical Turk1 and Odesk 2), it becomes realistic to quickly hire a large group of crowd-sourced workers with a fair amount of cost. For example, there are over 500,000 workers in AMT and in average, they are more educated than the United States population. But when the NLP data is annotated by unevenly-trained nonexpert workers, errors are rampant. We believe they happen for at least three reasons: (1) sophisticated linguistic practices, for example, the guidelines for penn treebank are more than 300 pages. It is impossible for non-experts to catch all the details; (2) comprehensive operators caused by sophisticated rules, for example, to generate a parsed tree, annotators must identify the POS of each tree node from a set of hundreds tags; (3) many NLP problems are structured prediction problem, when the labels depend on each other, each decision requires deep lookahead and backtracking.\nTherefore, it is not surprising that the outputs of the crowd-sourced workers are far from oracles. During annotations, they are having limited NLP knowledge, are blind about the dependencies of the data, but having excessive amount of options to choose. To alleviate the problem, we believe a good visualization toolkit for crowdsourced workers could dramatically increase their productivity and accuracy. Unfortunately, existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic practices and restrict their pool of workers. Instead of anticipating the workers to be NLP experts, we aim to allow ordinary people to annotate the training corpus. For this, the toolkit is designed with several principles in mind: (1) simplified operators: comprehensive operators for\n1http://www.mturk.com 2http://www.odesk.com\nar X\niv :1\n50 8.\n06 04\n4v 1\n[ cs\n.C L\n] 2\n5 A\nug 2\n01 5\ncrowd workers often mean longer education time and lower accuracy. We thus only allow users to click and draw over the data. The tradeoff between expressiveness and quantity is worthwhile because we can hire crowd-sourced workers to generate more training data with less errors. (2) interactive interface for deep lookahead and backtracking, workers should efficiently read the dependency of the data in some interactive way; (3) convenient trial and error, for structured prediction, labels are related to each other while human beings have to annotate them in some serialized manner. So it is inevitable that workers would frequently fix their errors; (4) generalization, many toolkits only specialize on one problem because they must provide comprehensive and specialized operators. As oppose to them, our toolkit with simplified operators is consequently easier to cover a broader range of NLP tasks.\nIn this paper, we present CROWDANNO, a visualization toolkit for crowd-sourced NLP annotation. The key observation is that many NLP problems turn out to be clustering and parsing. On one hand, NLP system is often asked to identify the relationship between objects, which could be words (e.g. paraphrasing), mentions (e.g. coreference), sentences (e.g. summarization), documents (e.g. sentiment analysis), etc. All these tasks could be treated as some practice of clustering or formulated as a special form of reinforcement learning (Huang et al., 2012). On the other hand, many NLP applications rely on the results of parsing. For example, relation extraction (Hoffmann et al., 2011) approaches use the dependency features generated from the parsed trees; some syntax-based translation method translate between trees of different languages. CROWDANNO allows crowd-sourced workers to generate the clustering graphs and the parsing trees. They can annotate the training data with simplified operators to avoid tedious education; they can backtrack their labels in an interactive interface to better read the data; they can easily edit their previous actions to quickly fix the errors. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems.\nThe rest of this paper is organized as follows. In Section 2 we demonstrate the overview of our toolkit. Section 3 and 4 presents the implementations for clustering and parsing annotation respectively. In Section 5 we introduce the evalu-\nation metrics, and present the experimental results of our toolkit with respect to several baselines. In section 6 we discuss the related work of our toolkit. Finally we conclude our work and present the discussion."}, {"heading": "2 Toolkit Overview", "text": "In this section, we first introduce the design decisions and the overview of the toolkits. Then we present the user interface and the major functions.\nThe toolkit is designed and implemented as a web application, since it is easiest for crowd workers to visit.\nThe users of our visualization toolkit include annotation gatherers and crowd-source workers. In general, annotation gatherers (NLP experts and well-trained programmers) use our toolkit to collect training data from crowd-source workers for their NLP systems. We believe annotation gatherers know their NLP problems better than anyone else, so we leave them to implement a html page to display their problem. The html page would call the APIs to pass data (e.g. objects to cluster) to the toolkits. The core component then is to visualize the problem, as well as allowing workers to interact with the objects. Our toolkit keeps the taskvisualization transparent to maximize the flexibility of our toolkit. Figure 1 shows the workflow of our toolkit."}, {"heading": "2.1 Cluster Labeling", "text": "We use co-reference as the running example for clustering visualization. Figure 2 shows the user interface. The left sides (about 40%) displays the NLP task. One simple way to display the coreference task, as shown in the figure, is to highlight the mentions and to give them unique IDs. The right side of the interface is for the interaction\npurpose, where workers generate the clusters interactively. Each token has a corresponding node in the operation area. To bridge the connection between the text display and graphic chart, we label each token and its corresponding node with the same index.\nOur main manipulation method for clustering two nodes together is dragging nodes and merging into one group. It is the most intuitive operation for one without any instructions to merge two nodes. We will explain the select and drag, link add and remove, and color scheme in the following paragraphs."}, {"heading": "SELECT AND DRAG", "text": "Since we have two separate parts, one for displaying the text, the other for cluster and group manipulation, a proper selection must be supported in order to reduce user\u2019s eye movement between two parts. We adopt three mechanism to address this issue. First, if user click on one token, the corresponding node will be highlighted in red color. Similarly when user starts to drag a node, the token is also highlighted in red background. Second, to accelerate tracing from node to the actual text, once user start to drag a node, the display section will scroll to a proper position to let user easily read the context of this token. Third, as shown in Figure 3, when drag event starts, an abbreviation text will be displayed under each node so that user can have a brief concept of what each node represents."}, {"heading": "LINK ADD AND REMOVE", "text": "Our tool does not allow user explicitly add a link between two nodes. Instead the way to add a link is by dragging a node close enough to the targeted node. Figure 4 shows the process of adding a link. When user starts to drag one node, a shadow circle will be shown around each node. This shadow\ncircle indicates the effective area of adding a link. Once a node is close enough to this shadow circle, a temporary link (in red) will be immediately added to preview the result of this operation (shown in Figure 4(b)). After user confirms the operation by dropping down the node, a permanent link will be added. Due to the transitivity property of clustering task, CROWDANNO will automatically bundle two groups together and assign same color for all the nodes in this new group.\nThe operation that removes a link is as easy as clicking on the link, shown in Figure 5. When the mouse hovers over the link, the link color turns to red to indicate that the link will be removed (Figure 5(a)). After the click, link will then be removed. If the group is no longer connected after the removement of that link, the system will find out two connected components and separate the original group into two new groups, e.g., Figure 5(b)."}, {"heading": "COLOR SCHEME", "text": "The token and its corresponding node in the graphic sides have the same color. Any change in the color of the node is also reflected in the corresponding token in the text display part (Figure 2). The consistency in color schemes improves the connection between text and graph part.\nThe default color is grey. If a node (or token) does not belong to any group, we will use the default color as filling color (or background color). We will assign a new color when a group is generated. If two groups merge together, we will keep\nthe color of group being merged to as the color of new merged larger group (Figure 4). If a group is splited into two, the majority component keeps the color the origin group."}, {"heading": "GROUP POSITIONING", "text": "To improve the usability and precision of dragging and adding link operations, and to utilize the space effectively, a proper distance should exist between nodes and nodes, and between groups and groups. We use d3 Force model and calibrate the gravity parameter to enforce a proper gap among the nodes. CROWDANNO also calculate a proper central point for each group, and impose a force to every nodes inside this group towards that direction. Therefore, groups won\u2019t be overlapped in a small region."}, {"heading": "2.2 Tree Construction", "text": "The tree annotation task is to construct or edit a tree hierarchy from a sentence. This task is very common in many sentence based NLP problems such as POS tagging."}, {"heading": "INTERFACE", "text": "The tree annotation task interface consists of two parts. We use syntactic parsing as the running example to show how our toolkit can build a parsed tree. Figure 6 shows the general user interface.\nThe left part is the sentence input data list. Our\ntool supports multiple sentence annotation in the same time. Users can select from a list of sentence files at the bottom of the left panel. The sentence files are in plain text format. Each sentence file is simply a list of sentences, each of which is a line. The words are separated by white space characters. The sentence list in a given input file will be shown on the Input section. To the right of each sentence, there is a small download sign, on which a click triggers a download of the current tree built for that sentence.\nThe right part is the tree editing/construction area. A given input sentence will be splitted into word nodes vertically. The reason that we organize the sentence word nodes in this way instead of horizontally is that each word now always occuppies the same unit height. It\u2019s more space consistent visually and also more efficient in space usage. Our tool provides three operations for manipulating the tree construction task."}, {"heading": "NODE ADDITION (GROUPING)", "text": "Our tool provides a simple line-intersect operation to group an existing set of sub-trees as in Figure 7. A new node which serves as the root of all grouped sub-trees is added as a result. To the best of our knowledge, we are the first to propose this visualization operation on tree structures."}, {"heading": "NODE DELETION (UNGROUPING)", "text": "As the couter-operation of node addition, our\ntool also provides a node deletion operation also by line intersect. If a single line is cut, our tool considers it as a node deletion. The node at the child side of the cut line will be deleted unless the node is a leaf node. Figure 8 gives the illustration."}, {"heading": "NODE FOLDING", "text": "The above two tree editing operations are enough for building arbitray trees in most NLP tasks. In addition to them, our tool also provides a node folding operation. It is a change of view operation rather than an editing operation. Given any non-leaf node, a click on the node will folds the whole sub tree and represented by a colored leaf node with the all the words in the sub-tree displayed. Figure 9 gives an illustration.\nSince we are targeting sentence based tree bui-\nliding tasks, the tree layout is designed that the node order is always kept as the input order, which is the sentence order, after any of the operations."}, {"heading": "3 Evaluation", "text": "Our goal is to evaluate whether our visualization toolkit is helpful for non-experts of NLP to annotate clustering and parsing training data."}, {"heading": "3.1 Setup", "text": "For clustering, we use co-reference as our running example; for parsing, we ask workers to build syntactic tree for sentences according to their English skills. They don\u2019t need to tag any of the nodes. We designed the user studies as follows: \u2022 Participants: We asked 6 graduate students\naged 21 to 26 years old to participant in our user study. 3 are native English speakers, the other 3 learned English as a second language. None of them are NLP experts. We divide them into native group and foreign group. \u2022 Instructions: each participant is given two an-\nswer examples one on parsing and one on clustering. And given 5 minute to learn the exam-\nple and ask whatever question they have. \u2022 Clustering: each of them are given 10 articles\nfor co-reference with 5 using our visualization tool and 5 conducted using Excel in the traditional way (where they will cluster expressions by putting them into the same row). The orders of the articles and the tools are random. \u2022 Parsing: each of them are given 10 sentences\nfor tree parsing with 5 using our visualization tool and 5 using text editor to put parenthesis in the traditional way. For example (My dog) (also likes) (eating sausage).)\nThe order of the articles, the sentences and the toolkit they use are randomly shuffled.\nWe compare the time efficiency and accuracy to generate training data with and without the toolkit. For clustering, time efficiency is defined as the seconds worker used for each correct co-reference cluster. Accuracy is computed by purity. Let Nij be the number of mentions in cluster i that belong to entity j, and let Nj = \u2211 iNij . Then the purity of a cluster is pi = maxj pij . The overall purity is \u2211 i Ni N pi. Purity range between 0 (bad) and 1 (good). Table 2 shows the average time, purity for 5 workers. It is clear that using our toolkit dramatically improve the efficiency and accuracy. Purity ranges between 0 (bad) and 1 (good). Table 2 shows the average time, purity for 5 workers. It is clear that using our toolkit dramatically improve the efficiency and accuracy.\nFor parsing, time efficiency is defined as the seconds worker used for each token, to compensate the time cost over long sentences.\nOur clustering tool achieved an average 1.0s/15.2% decrease in time consumption and at the same time an average 6.6% increase of purity on clustering result. Our tree parsing tool achieved an average of 1.9s/27.8% decrease in time consumption for parsing. We manually checked the quality of these tree and find out that the qualities with and without the toolkit is quite comparable. It is reasonable because we provide workers text edits with the function of parenthesis matching, so people can easily figure out the errors of parenthesis in the sentences. It shows that people are more intolerable about the errors in parsing, even with text editors. But it is clear that they spent much more time on each tree. During crowd-sourcing, time cost equals money spending. It shows that our toolkit is still valuable."}, {"heading": "4 Related Work", "text": "Many NLP tasks require large amount of high quality training data. Manual annotation for such training data is well-known for its tedium. To generate a comprehensive annotated training set requires much human effort. Annotators are also prone to make mistakes during the long and tedious annotating process. Researchers are trying to address these problems by two means: 1) building specialized annotating tools to ease the annotating process in the hope of improving efficiency as well as reducing the error rates; 2) adopting crowdsourcing to scale up annotating.\nSpecialized annotating tools. Facing one of the biggest common problems, many NLP researchers have developed a number of tools for annotating training corpora along the history of NLP research. At first, before the blossom of the web, tools are generally built as local programs such as the WordFreak linguistic annotation tool (Morton and LaCivita, 2003) and the UAM CorpusTool for text and image annotation (O\u2019Donnell, 2008). These tools are very restricted because they cannot scale. Web-based annotation tools are developed later in order to scale up the annotating process for specific domains, such as anaphora (Stu\u0308hrenberg et al., 2007) and financial reports (Wang, 2015). However these tools typically only use very basic HTML based techniques to provide very limited visual aids for the annotating process. Most related in scope is (Yan and Webster, 2012) which provides a collaborative tool to assist annotators in tagging of complex Chinese and multilingual linguistic data. It visualizes a tree model that represents the complex relations across different linguistic elements to reduce the learning curve. Besides it proposes a web-based collaborative annotation approach to meet the large amount of data. Their tool only focuses on a specific area that is complex multilingual linguistic data, whereas our work is trying to address how to generate a visualization model for general data sets.\nCrowdsoursing in NLP. Crowdsourcing (Howe, 2006) is a popular and fast growing research area. There have been a lot of studies on understanging what it is and what it can do. For instance, (Quinn and Bederson, 2009) categorizes crowdsourcing into seven genres: Mechanized Labor, Game with a Purpose (GWAP), Widom of Crowds, Crowdsourcing, Dual-Purpose Work, Grand Serarch, Human-based Genetic Algorithms and Knowledge Collection from Volunteer Con-\ntributors. Other works, such as (Abekawa et al., 2010) and (Irvine and Klementiev, 2010), develops a specific tool and verifies the feasibility and benefit of crowdsourcing. It is generally convinced that crowdsourcing is of great beneficial if the tasks are easy to conduct by the workers and the tasks are independent.\nBecause of the high labor requirements in typical NLP training tasks, there also have been some work considering using crowdsoursing in many NLP tasks. For example, Grady et al. generated a data set on document relevance to search queries for information retrieval (Grady and Lease, 2010); Negri et al. built a cross-lingual textual corpora (Negri et al., 2011); Finin et al. collected simple named entity annotations using Amazon MTurk and Crowd-Flower (Finin et al., 2010). Also there are some researchers observed the hardness of collecting high quality data and did some studies on improving that, such as (Hsueh et al., 2009)( how annotations should be selected to maximize quality), and (Lease, 2011) (quality control in crowdsoursing by machine learning).\nDifferent from previous studies, we seek to improve crowdsoursing annotating quality by greatly lower the usability barrier through the proposed visualized toolkit rather than trying to cleaning up the data generated by the crowdsoursing process."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we present CROWDANNO, a toolkit for crowd-sourced NLP annotation. We visualize two important categories of NLP problems: clustering and parsing. By providing simplified operators and interactive interface, we allow crowdsourced workers to generate high quality training data for NLP problem. In our evaluation, we let non-expert student to test our toolkit. The results\nare very promising. Because of the time limitation, we have not yet let the crowd sourced workers to really test our toolkit. In future, we would deploy the toolkit on AMT to collect real training data for NLP problems."}], "references": [{"title": "Community-based construction of draft and final translation corpus through a translation hosting site minna no hon\u2019yaku (mnh)", "author": ["Masao Utiyama", "Eiichiro Sumita", "Kyo Kageura"], "venue": "In LREC. Citeseer", "citeRegEx": "Abekawa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abekawa et al\\.", "year": 2010}, {"title": "Open information extraction for the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Annotating named entities in twitter data with crowdsourcing", "author": ["Finin et al.2010] Tim Finin", "Will Murnane", "Anand Karandikar", "Nicholas Keller", "Justin Martineau", "Mark Dredze"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating", "citeRegEx": "Finin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Finin et al\\.", "year": 2010}, {"title": "Crowdsourcing document relevance assessment with mechanical turk", "author": ["Grady", "Lease2010] Catherine Grady", "Matthew Lease"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Grady et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grady et al\\.", "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "The rise of crowdsourcing", "author": ["Jeff Howe"], "venue": "Wired magazine,", "citeRegEx": "Howe.,? \\Q2006\\E", "shortCiteRegEx": "Howe.", "year": 2006}, {"title": "Data quality from crowdsourcing: A study of annotation selection criteria", "author": ["Hsueh et al.2009] Pei-Yun Hsueh", "Prem Melville", "Vikas Sindhwani"], "venue": "In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Process-", "citeRegEx": "Hsueh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsueh et al\\.", "year": 2009}, {"title": "Neurons as monte carlo samplers: Bayesian inference and learning in spiking networks", "author": ["Huang", "Rao2014] Yanping Huang", "Rajesh P Rao"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "How prior probability influences decision making: A unifying probabilistic model", "author": ["Y. Huang", "A.L. Friesen", "T.D. Hanks", "M.N. Shadlen", "R.P.N. Rao"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Using mechanical turk to annotate lexicons for less commonly used languages", "author": ["Irvine", "Klementiev2010] Ann Irvine", "Alexandre Klementiev"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Ama-", "citeRegEx": "Irvine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2010}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsVolume", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Scaling question answering to the web", "author": ["Kwok et al.2001] Cody Kwok", "Oren Etzioni", "Daniel S Weld"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Kwok et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kwok et al\\.", "year": 2001}, {"title": "On quality control and machine learning in crowdsourcing", "author": ["Matthew Lease"], "venue": "In Human Computation", "citeRegEx": "Lease.,? \\Q2011\\E", "shortCiteRegEx": "Lease.", "year": 2011}, {"title": "Wordfreak: An open tool for linguistic annotation", "author": ["Morton", "LaCivita2003] Thomas Morton", "Jeremy LaCivita"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Morton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Morton et al\\.", "year": 2003}, {"title": "Divide and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora", "author": ["Negri et al.2011] Matteo Negri", "Luisa Bentivogli", "Yashar Mehdad", "Danilo Giampiccolo", "Alessandro Marchetti"], "venue": "In Proceedings of the Conference", "citeRegEx": "Negri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Negri et al\\.", "year": 2011}, {"title": "Demonstration of the uam corpustool for text and image annotation", "author": ["Mick O\u2019Donnell"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session,", "citeRegEx": "O.Donnell.,? \\Q2008\\E", "shortCiteRegEx": "O.Donnell.", "year": 2008}, {"title": "A taxonomy of distributed human computation. Human-Computer Interaction Lab Tech Report, University of Maryland", "author": ["Quinn", "Bederson2009] Alexander J Quinn", "Benjamin B Bederson"], "venue": null, "citeRegEx": "Quinn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quinn et al\\.", "year": 2009}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning (ICML-11),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Web-based annotation of anaphoric relations and lexical chains", "author": ["Daniela Goecke", "Nils Diewald", "Alexander Mehler", "Irene Cramer"], "venue": "In Proceedings of the Linguistic Annotation Workshop,", "citeRegEx": "St\u00fchrenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "St\u00fchrenberg et al\\.", "year": 2007}, {"title": "Information retrieval using markov decision process", "author": ["Nan Wang"], "venue": "International Journal of Computer Systems,", "citeRegEx": "Wang.,? \\Q2015\\E", "shortCiteRegEx": "Wang.", "year": 2015}, {"title": "Learning to rank the severity of unrepaired cleft lip nasal deformity on 3d mesh data", "author": ["Wu et al.2014] Jia Wu", "Raymond Tse", "Linda G Shapiro"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Collaborative annotation and visualization of functional and discourse structures", "author": ["Yan", "Webster2012] Hengbin Yan", "Jonathan Webster"], "venue": "In Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing,", "citeRegEx": "Yan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2012}, {"title": "An adaptive kernel width update for correntropy", "author": ["Zhao et al.2012] Songlin Zhao", "Badong Chen", "Jose C Principe"], "venue": "In Neural Networks (IJCNN), The 2012 International Joint Conference on,", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Statistical machine learning approaches has made great successes in research disciplines such as parsing (Klein and Manning, 2003), information extractions (Banko et al., 2007), question answering (Kwok et al.", "startOffset": 156, "endOffset": 176}, {"referenceID": 11, "context": ", 2007), question answering (Kwok et al., 2001).", "startOffset": 28, "endOffset": 47}, {"referenceID": 17, "context": "Significant advances have been made in recent years on inferrring latent labels for sequence observations using neural networks (Socher et al., 2011; Huang and Rao, 2014) and kenerl methods (Zhao et al.", "startOffset": 128, "endOffset": 170}, {"referenceID": 22, "context": ", 2011; Huang and Rao, 2014) and kenerl methods (Zhao et al., 2012).", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "Yet offthe-shelf models learned from training data of one particular domain (usually newswire) would often underperform at present tasks, whose data could come from other domains such as social media and biomedical data (Wu et al., 2014).", "startOffset": 220, "endOffset": 237}, {"referenceID": 8, "context": "All these tasks could be treated as some practice of clustering or formulated as a special form of reinforcement learning (Huang et al., 2012).", "startOffset": 122, "endOffset": 142}, {"referenceID": 15, "context": "tools are generally built as local programs such as the WordFreak linguistic annotation tool (Morton and LaCivita, 2003) and the UAM CorpusTool for text and image annotation (O\u2019Donnell, 2008).", "startOffset": 174, "endOffset": 191}, {"referenceID": 18, "context": "specific domains, such as anaphora (St\u00fchrenberg et al., 2007) and financial reports (Wang, 2015).", "startOffset": 35, "endOffset": 61}, {"referenceID": 19, "context": ", 2007) and financial reports (Wang, 2015).", "startOffset": 30, "endOffset": 42}, {"referenceID": 5, "context": "Crowdsourcing (Howe, 2006) is a popular and fast growing research area.", "startOffset": 14, "endOffset": 26}, {"referenceID": 14, "context": "built a cross-lingual textual corpora (Negri et al., 2011); Finin et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 2, "context": "collected simple named entity annotations using Amazon MTurk and Crowd-Flower (Finin et al., 2010).", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "Also there are some researchers observed the hardness of collecting high quality data and did some studies on improving that, such as (Hsueh et al., 2009)( how annotations should be selected to maximize quality), and (Lease, 2011) (quality control in crowdsoursing by machine learning).", "startOffset": 134, "endOffset": 154}, {"referenceID": 12, "context": ", 2009)( how annotations should be selected to maximize quality), and (Lease, 2011) (quality control in crowdsoursing by machine learning).", "startOffset": 70, "endOffset": 83}], "year": 2015, "abstractText": "Visualizing NLP annotation is useful for the collection of training data for the statistical NLP approaches. Existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic rules. Workers must be well trained to use them. Their audience thus can hardly be scaled to large amounts of non-expert crowdsourced workers. In this paper, we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to annotate two general categories of NLP problems: clustering and parsing. Workers can finish the tasks with simplified operators in an interactive interface, and fix errors conveniently. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems. We release our source code and toolkit to spur future research.", "creator": "LaTeX with hyperref package"}}}