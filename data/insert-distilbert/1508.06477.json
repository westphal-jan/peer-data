{"id": "1508.06477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization", "abstract": "several fast sparsity - constrained statistics algorithms such as orthogonal matching pursuit or the frank - rosen wolfe algorithm with such sparsity constraints work by iteratively selecting a novel atom to add to the current generalized non - zero symmetric set of variables. this selection step is usually performed by algorithms computing the lower gradient and smoothing then by looking for the gradient component with maximal absolute entry. this step can be computationally expensive especially for large - scale and high - dimensional data. in this work, we aim equally at accelerating these slow sparsity - constrained optimization algorithms by exploiting the key observation that, for these algorithms to work, one only immediately needs the coordinate of the gradient's top entry. hence, we introduce algorithms based on greedy methods and randomization approaches that aim at anticipating cheaply physically estimating the gradient and creating its top entry. another consideration of our contribution concepts is to cast the problem of finding the best backward gradient entry as a best ranked arm identification in a multi - armed bandit problem. owing to this additional novel insight, then we are able eventually to provide a bandit - based algorithm that directly estimates the top entry in uniformly a very efficient way. theoretical results stating that the resulting inexact frank - wolfe or orthogonal matching pursuit algorithms act, with high probability, similarly to their exact versions are therefore also likewise given. eventually we have carried out several experiments showing that the greedy deterministic and the bandit approaches we propose can achieve an acceleration of an order of magnitude while being as efficient as the exact gradient when used in algorithms ) such include as", "histories": [["v1", "Wed, 26 Aug 2015 13:01:36 GMT  (139kb)", "https://arxiv.org/abs/1508.06477v1", null], ["v2", "Mon, 22 Aug 2016 07:50:48 GMT  (227kb,D)", "http://arxiv.org/abs/1508.06477v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["a rakotomamonjy", "s ko\\c{c}o", "liva ralaivola"], "accepted": false, "id": "1508.06477"}, "pdf": {"name": "1508.06477.pdf", "metadata": {"source": "CRF", "title": "Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization", "authors": ["A. Rakotomamonjy", "S. Ko\u00e7o", "L. Ralaivola"], "emails": ["alain.rakoto@insa-rouen.fr", "sokol.koco@gmail.com", "liva.ralaivola@lif.univ-"], "sections": [{"heading": null, "text": "Index Terms\u2014Sparsity, Orthogonal Matching Pursuit, FrankWolfe algorithm, Greedy methods, Best arm identification.\nI. INTRODUCTION\nOver the last decade, there has been a large interest in inference problems featuring data of very high-dimension and a small number of observations. Such problems occur in a wide variety of application domains ranging from computational biology and text mining to information retrieval and finance. In order to learn from these datasets, statistical models are frequently designed so as to feature some sparsity properties. Hence, fueled by the large interest induced by these application domains, an important amount of research work in the machine learning, statistics and signal processing communities, has been devoted to sparse learning and as such, many algorithms\nAR is with University of Rouen, LITIS Lab. Most of this work has been carried while he was visiting LIF at Aix-Marseille University. Email alain.rakoto@insa-rouen.fr\nSK is with University of Rouen, LITIS LAB. Email: sokol.koco@gmail.com\nLR is with Aix-Marseille University, LIF. Email: liva.ralaivola@lif.univmrs.fr\nThis work is supported by Agence Nationale de la Recherche, project GRETA 12-BS02-004-01.\nManuscript received August2015; revised XXX.\nhave been developed for yielding models that use only few dimensions of the data. To obtain these models, one typically needs to solve a problem of the form\nmin w\nL(w) subject to \u2016w\u20160 \u2264 K (1)\nwhere L(w) is a smooth convex objective function that measures a goodness of fit of the model, \u2016w\u20160 is the `0 pseudonorm that counts the number of non-zero components of the vector w and K is a parameter that controls the sparsity level.\nOne usual approach that has been widely considered is the use of a convex and continuous surrogate of the `0 pseudonorm, namely an `1-norm. The resulting problem is the wellknown Lasso problem [1] and a large variety of algorithms for its resolution exist, ranging from homotopy methods [2] to the Frank-Wolfe (FW) algorithm [3]. In the same flavour, non-convex continuous penalties are also common solutions for relaxing the `0 pseudo-norm [4], [5]. Another possible approach is to consider greedy methods that provide local optimal solution to the problem (1). In this last context, a flurry of algorithms have been proposed, the most popular ones being the Matching Pursuit (MP) and the Orthogonal Matching Pursuit (OMP) algorithms [6], [7], [8]. One common point of the aforementioned algorithms for solving problem (1) is that they require, at each iteration, the computation of the objective function\u2019s gradient. For large-scale and highdimensional setting, computing the gradient at each iteration may be very time-consuming.\nStochastic gradient descent (SGD) algorithms are now classical methods for avoiding the computation of the full gradient in large-scale learning problems [9], [10]. Most of these works have been devoted to smooth composite optimization although some efforts addressing `1-regularized problems exist [11]. Recently, these SGD algorithms have been further accelerated through the introduction of variance reduction methods for gradient estimation [12]. In the context of problem (1) where a non-smooth non-convex constraint appears, very few works have envisaged the use of stochastic optimization. Nguyen et al. [13] have proposed stochastic versions of a gradient pursuit algorithm. Following a similar direction, by exploiting inexact gradient information, we address the problem of accelerating sparsity-inducing algorithms such as Matching Pursuit, Orthogonal Matching Pursuit and the Frank-Wolfe algorithm with an `1 ball constraint. However, unlike stochastic gradient approaches, the acceleration we propose leverages on the fact that at each iteration, these algorithms seek for the gradient\u2019s component that has the largest (absolute) value. ar X\niv :1\n50 8.\n06 47\n7v 2\n[ cs\n.L G\n] 2\n2 A\nug 2\n01 6\n2 Hence, our main contribution in this paper is to propose novel algorithms that allow to efficiently find this top entry of the gradient. By doing so, our objective is to design novel efficient versions of MP, OMP and FW algorithms while keeping intact all the properties of these algorithms for sparse approximation. Indeed, this becomes possible if at each iteration of MP, OMP or FW, our inexact gradient-based estimation of the component with largest value is the same as the one obtained with the exact gradient.\nWe propose two approaches, the first one based on a greedy deterministic algorithm and the second one based on a randomized method, whose aim is to build an inexact estimation of the gradient so that its top entry in the same as the exact one. Next, by casting the problem as a best arm identification multiarmed bandit problem [14], we are able to derive an algorithm that directly estimates the best component of the gradient. Interestingly, these algorithms are supported by theoretical evidences that with high probability, they are able to retrieve the correct component of the gradient. As a consequence, we show that MP, OMP and FW algorithms employing these approaches for spotting the correct component of the gradient behave as their exact counter part with high probability.\nThis paper is an extended version of the conference paper [15]. It provides full details on the context of the problem and proposes a novel key contribution based on multi-arm bandits. In addition, it gives enlightning insights compared to related works. Extended experimental analysis also strengthen the results compared to the conference version. The remainder of the paper is organized as follows. Section II presents sparsityconstrained algorithms, formalizes our problem and introduces the key observation on which we build our acceleration strategy. Section III provides the different algorithms we propose for efficiently estimating the extreme gradient component of interest. A discussion with related works is given in Section IV. Experimental results are depicted in Section V while Section VI concludes the work and presents different outlooks."}, {"heading": "II. SPARSE LEARNING ALGORITHM WITH EXTREME GRADIENT COMPONENT", "text": "In this section, we introduce the sparse learning problem we are interested in, as well as some algorithms that are frequently used for sparse learning. We then point out the prominent common trait of those algorithms, namely the extreme gradient component property, and discuss how its estimation can be employed for accelerating some classes of sparse learning algorithms."}, {"heading": "A. Framework", "text": "Consider the problem where we want to estimate a relation between a set of n samples gathered in a vector y \u2208 Rn and the matrix X \u2208 Rn\u00d7d. In a sparse signal approximation problem, X would be a matrix whose columns are the elements of a dictionary and y the target signal, while in a machine learning problem, the i-th row of matrix X, formalized as x>i , xi \u2208 Rd, depicts the features of the i-th example and yi is the label or target associated to that example. In the sequel, we denote as xi,j the entry of X at the i-th row and j-th column.\nAlgorithm 1 Gradient Pursuit Algorithm 1: set k = 0, initialize w0 = 0 2: for k=0,1, \u00b7 \u00b7 \u00b7 do 3: i? = arg maxi |\u2207L(wk)|i 4: \u0393k = \u0393k \u222a {i?} 5: wk+1 = arg minL(w) over \u0393k with w\u0393Ck = 0 6: end for\nOur objective is to learn the relation between y and X through a linear model of the data denoted Xw by looking for the vector w that solves problem (1) when the objective function is of the form\nL(w) = n\u2211 i `(yi, g(w >xi)),\nwhere ` is an individual loss function that measures the discrepancy between a true value y and its estimation, and g(\u00b7) is a given differentiable function that depicts the (potential) non-linear dependence of the loss to w. Typically, ` might be the least-square error function, which leads to L(w) = 12\u2016y \u2212 Xw\u2016 2 2 or the logistic loss and we have\nL(w) = \u2211n i=1 log(1 + exp{\u2212yix>i w}). In the sequel, we present two algorithms that solve problem (1) by a greedy method and by a continuous and a convex relaxation of the `0 pseudo-norm, respectively."}, {"heading": "B. Algorithms", "text": "1) Gradient Pursuit: This algorithm is a generalization of the greedy algorithm known as Orthogonal Matching Pursuit (OMP) to generic loss functions [16]. It can be directly applied to our problem given in Equation (1). Similarly to OMP, the gradient pursuit algorithm is a greedy iterative procedure, which, at each iteration, selects the largest absolute coordinate of the loss gradient. This coordinate is added to the set of already selected elements and the new iterate is obtained as the solution of the original optimization problem restricted to these selected elements while keeping the other ones at 0. The detailed procedure is given in Algorithm 1.\nWhile conceptually simple, this algorithm comes with theoretical guarantees on its ability to recover the exact underlying sparsity pattern of the model, for different types of loss functions [7], [17], [18].\nSeveral variations of this algorithm have been proposed ranging from methods exploring new pursuits directions instead of the gradient [16], to methods making only slight changes to the original one that have strongly impacted the ability of the algorithm to recover signals. For instance, the CoSaMP [19] and GraSP [20] algorithms select the top 2K entries in the absolute gradient, K being the desired sparsity pattern, optimize over these entries and the already selected K ones, and prune the resulting estimate to be K-sparse. In addition to being efficient, these algorithms output sparse vectors whose distances from the true sparse optimum are bounded.\n3 Algorithm 2 Frank-Wolfe Algorithm 1: set k = 0, initialize w0 = 0 2: for k=0,1, \u00b7 \u00b7 \u00b7 do 3: sk = arg mins\u2208C s\n>\u2207L(wk) 4: dk = sk \u2212wk 5: set, linesearch or optimize \u03b3k \u2208 [0; 1] 6: wk+1 = wk + \u03b3kdk 7: end for\n2) Frank-Wolfe algorithm: The Frank-Wolfe (FW) algorithm aims at solving the following problem\nmin w\u2208C L(w) (2)\nwith w \u2208 Rd, L a convex and differentiable function with Lipschitz gradient and C a compact subset of Rd. The FW algorithm, given in Algorithm 2, is a straightforward procedure that solves problem 2 by first iteratively looking for a search direction and then updating the current iterate. The search direction sk is obtained from the following convex optimization problem (line 3 Alg 2)\nsk = arg min s\u2208C\ns>\u2207L(wk), (3)\nwhich may be efficiently solved, depending on the constraint set C. For achieving sparsity, we typically choose C as a `1-norm ball, e.g. C = {w : \u2016w\u20161 \u2264 1}, which turns (3) into a linear program. Despite its simplistic nature, the FW algorithm has been shown to be linearly convergent [21], [3]. Interestingly, it can also be shown that convergence is preserved as long as the following condition holds\ns>k\u2207L(wk) \u2264 min s\u2208C s>\u2207L(wk) + , (4)\nwhere depends on the smoothness of L and the step-size \u03b3k. In general cases where the minimization problem (3) is expensive to solve, this condition suggests that an approximate solution sk may be sufficient, provided that the true gradient is available. Similarly, if an inexact gradient information \u2207\u0302L(wk) is available, convergence is still guaranteed under the condition that s>k \u2207\u0302L(wk) \u2264 mins\u2208C s>\u2207L(wk) + , and some conditions relating \u2207\u0302L(wk) and \u2207L(wk). For instance, if C is a unit-norm ball associated with some norm \u2016 \u00b7 \u2016 and sk a minimizer of mins\u2208C s>\u2207\u0302L(wk), i.e. sk = arg mins\u2208C s\n>\u2207\u0302L(wk), then in order to ensure convergence, it is sufficient to have sk so that [3]\n\u2016\u2207\u0302L(wk)\u2212\u2207L(wk)\u2016? \u2264 , (5)\nwhere \u2016 \u00b7 \u2016? is the conjugate norm associated with \u2016 \u00b7 \u2016. Interestingly, the above equation provides a guarantee on the convergence on an inexact gradient Frank-Wolfe algorithm, but unfortunately, the precision needed on the inexact gradient is given with respect to the exact one. However, while condition (5) is impractical, it conveys the important idea that inexact gradient computation may be sufficient for solving sparsityconstrained optimization problems."}, {"heading": "C. Leveraging from the extreme gradient component estimation", "text": "As stated above, the gradient pursuit algorithm needs to solve at each iteration the following problem j? = arg maxj |\u2207L(wk)|j , i.e., the goal is to find at each iteration the gradient\u2019s component with the largest absolute value.\nIn the Frank-Wolfe algorithm, similar situations where finding sk corresponds to looking for an extreme component of the (absolute) gradient, occur. For instance, when the constraint set is the `1-norm ball C1 = {w \u2208 Rd : \u2016w\u20161 \u2264 1} or the positive simplex constraint C2 = {w \u2208 Rd : \u2016w\u20161 = 1, wj \u2265 0} and we denote\ns? = arg min s\u2208C1 s>\u2207L(wk) and j? = arg max j |\u2207L(wk)|j | ,\nor\ns? = arg min s\u2208C2 s>\u2207L(wk) and j? = arg min j \u2207L(wk)|j ,\nthen s? = ej? , with ej? the j?-th canonical vector of Rd. Hence, as in Matching Pursuit, OMP or the gradient pursuit algorithm, for specific choices of C, we are not interested in the full gradient, nor in its extreme values, but only on the coordinate of the gradient component with the smallest, the largest (absolute) value.\nOur objective in this paper is to leverage on these observations for accelerating sparsity-inducing algorithms which look for one extreme component of the gradient. In particular, we are interested in situations where the gradient is expensive to compute and we aim at providing computation-efficient algorithms that produce either approximated gradients whose extreme component of interest is the same as the exact gradient\u2019s one, or identify the extreme component without computing the whole (exact) gradient."}, {"heading": "III. LOOKING FOR THE EXTREME GRADIENT COMPONENT", "text": "This section formalizes the problem of identifying the extreme gradient component and provides different algorithms for its resolution. We first introduce a greedy approach, then a randomized one and finally exhibit how this problem can be cast as best arm identification in a multi-armed bandit framework."}, {"heading": "A. The problem", "text": "As mentioned in Section II, we are interested in learning problems where the objective function is of the form\u2211\ni\n`(yi, g(w >xi)).\nThe gradient of our objective function is thus given by \u2207L(w) = \u2211 i `\u2032(yi, g(w >xi)))g \u2032(w>xi)xi = X >r (6)\nwhere r \u2208 Rn is the vector whose i-th component is `\u2032(yi, g(w >xi)))g \u2032(w>xi). This particular form entails that the gradient may be computed iteratively. Indeed, the sum \u2207L(w) = \u2211n i=1 xiri is invariant to the order according to which index i decribes the set [1, ..., n]. This makes possible\n4 Algorithm 3 Greedy deterministic algorithm to compute \u2207\u0302L Input: : r, {\u2016xi\u2016}, X\n1: [values, indices]=sort {|ri|\u2016xi\u2016}i in decreasing order 2: \u2207\u0302Lt = 0, t = 0 3: repeat 4: i =indices(t+ 1) 5: \u2207\u0302Lt+1 = \u2207\u0302Lt + xiri 6: t = t+ 1 7: until stopping criterion over \u2207\u0302Lt+1 is met\nOutput: \u2207\u0302Lt+1\nthe computation, at each iteration t, of an approximate gradient \u2207\u0302Lt. Denote It the first t indices used for computing the sum and it+1 the (t + 1)-th index, then, at iteration t, \u2207\u0302Lt = \u2211 i\u2208It xiri and\n\u2207\u0302Lt+1 = \u2207\u0302Lt + xit+1rit+1 . (7)\nAccording to this framework, our objective is then to find an efficient way for computing an approximate gradient \u2207\u0302Lt for which the desired extreme entry is equal to the one of the exact gradient. Note that the computational cost of the exact gradient X>r is about O(nd) and a naive computation of the residual r has the same cost, which leads to a global complexity of O(2nd). However, because w is typically a sparse vector in Gradient Pursuit or in the Frank-Wolfe algorithm, we compute the residual vector r as r = y \u2212 X\u2126w\u2126 where \u2126 are the indices of the non-zero elements of the k-sparse vector w. Hence, computing r has only a cost of O(nk) and it does not require a full pass on all the elements of X. The main objective of our contributions is to estimate the extreme entry of X>r with algorithms having a complexity lower than O(nd). We propose and discuss, in the sequel, three different approaches."}, {"heading": "B. Greedy deterministic approach", "text": "The first approach we propose is a greedy approach which, at iteration t, looks for the best index i so that rixi optimizes a criterion depending on \u2207\u0302Lt and \u2207L.\nLet It be the set of indices of the examples chosen in the first t iterations for computing \u2207\u0302Lt. At iteration t+1, our goal is to find the example i? that is the solution of the following problem:\ni? = argmini\u2208{1,...,n}\\It \u2016\u2207L\u2212 \u2207\u0302Lt \u2212 xiri\u2016, (8)\nwith \u2207\u0302Lt+1 = \u2207\u0302Lt + xiri. The solution of this problem is thus the vector product rixi that has minimal norm difference compared to the current gradient estimation residual \u2207L \u2212 \u2207\u0302Lt. While simple, this solution cannot be computed because \u2207L is not accessible. Hence, we have to resort to an approximation based on the following equivalent problem:\ni? = argmaxi\u2208{1,...,n}\\It \u2212\u2016\u2207L\u2212 \u2207\u0302Lt+1\u2016 (9) = argmaxi\u2208{1,...,n}\\It \u2016\u2207L\u2212 \u2207\u0302Lt\u2016 \u2212 \u2016\u2207L\u2212 \u2207\u0302Lt+1\u2016\nwhere we use the fact that \u2016\u2207L\u2212\u2207\u0302Lt\u2016 does not depend on i. By upper bounding the above objective value, one can derive\nAlgorithm 4 Computing \u2207\u0302L as an empirical average Input: : r, \u2016xi\u2016,X\n1: build p \u2208 \u2206\u2217n e.g pi \u221d 1n or pi \u221d |ri|\u2016xi\u2016 2: repeat 3: random draw i \u2208 1, \u00b7 \u00b7 \u00b7 , n according to p 4: \u2207\u0302Lt+1 = \u2207\u0302Lt + 1pixiri 5: t = t+ 1 6: until stopping criterion over \u2207\u0302Lt+1 is met\nOutput: \u2207\u0302Lt+1\nthe best choice of rixi that achieves the largest variation of the gradient estimation norm residual. Indeed, we have\n\u2016\u2207L\u2212 \u2207\u0302Lt\u2016 \u2212 \u2016\u2207L\u2212 \u2207\u0302Lt+1\u2016 \u2264 \u2016 \u2212 \u2207\u0302Lt + \u2207\u0302Lt+1\u2016\nThe right-hand side of this equation can be further simplified\n\u2016\u2207\u0302Lt+1 \u2212 \u2207\u0302Lt\u2016 = \u2016\u2207\u0302Lt \u2212 xiri \u2212 \u2207\u0302Lt\u2016 = \u2016xiri\u2016 = \u2016xi\u2016|ri|. (10)\nEquation (10) suggests that the index i that should be chosen at iteration t + 1 is the one with the largest absolute residual weighted by its example norm. Simply put, in the first iteration, the algorithm chooses the index that leads to the largest value \u2016xi\u2016|ri| , in the second one, it selects the second best, and so forth. That is, the examples are considered in a decreasing order with respect to the weighted value of their residuals. Pseudo-code of the approach is given in Algorithm 3.\nNote that for this method, at iteration t = n we recover the exact gradient,\n\u2207\u0302Ln = \u2207L.\nHence, when t = n, we are assured to retrieve the correct extreme entry of the gradient at the expense of extra computations needed for running this greedy approach compared to a plain computation of the full gradient. On the other hand, if we stop this gradient estimation procedure before t = n, we save computational efforts at the risk of missing the correct extreme entry."}, {"heading": "C. Matrix-Vector Product as Expectations", "text": "The problem of finding the extreme component of the gradient can also be addressed from the point of view of randomization, as described in Algorithm 4.\nThe approach consists in considering the computation of X>r as an the expectation of a given random variable. Recall that X is composed of the vectors {x>i }ni=1 and xi \u2208 Rd. Hence, the matrix-vector product X>r can be rewritten:\nX>r = n\u2211 i=1 rixi. (11)\nFrom now on, given some integer n, \u2206\u2217n denotes the interior of the probabilistic simplex of size n:\n\u2206\u2217n . = { p = [p1 \u00b7 \u00b7 \u00b7 pn] : n\u2211 i=1 pi = 1, pi > 0, i = 1, . . . , n } .\n(12)\n5 For any element p = (pi)i\u2208[n] \u2208 \u2206\u2217n we introduce a random vector C that takes value in the set\nC .= {ci . = rixi/pi : i = 1, . . . , n}\nso that P(C = ci) = pi. This way,\nEC = n\u2211 i=1 pici = n\u2211 i=1 pirixi/pi = n\u2211 i=1 rixi = X >r (13)\nHence, if C1, \u00b7 \u00b7 \u00b7 , Cs are independent copies of C and C\u0302s is defined as: C\u0302s .= 1s \u2211s i=1 Ci then EC\u0302s = X>r. C\u0302s is thus an estimator of the matrix-vector product that we are interested in i.e the gradient of our objective function.\nAccording to the above, a relevant approach for estimating the extreme component of the gradient is to randomly sample s copies of C, to average them and then to look for the extreme component of this estimated gradient.\nInterestingly, this approach based on randomized matrix multiplication can be related to our deterministic approach. Indeed, a result given by [22] (Lemma 4) says that the element p \u2208 \u2206\u2217n that minimizes E[\u2016X>r\u2212 C\u0302s\u20162F ] is such that\npi \u221d |ri|\u2016xi\u2016. (14)\nIt thus suggests that vectors ci of large values of |ri|\u2016xi\u2016 have higher probability to be sampled. This resonates with the greedy deterministic approach in which vectors xiri are accumulated in the order of the decreasing values of |ri|\u2016xi\u2016."}, {"heading": "D. Best arm identification and multi-armed bandits", "text": "The two preceding approaches seek at approximating the gradient, at a given level of accuracy that has yet to be defined, and then at evaluating the coordinate of its extreme component.\nYet, the problem of finding the extreme component coordinate of a gradient vector obtained from matrix-vector multiplication can also be posed as the problem of finding the best arm in a multi-armed bandit problem. In a nutshell, given a slot machine with multiple arms, the goal in the bandit problem is to find the arm that maximizes the expected reward or minimizes the loss. For this, an iterative procedure is used, where at each step, a forecaster selects an arm, based on his previous actions, and receives a reward or observes a loss. Depending on how the reward is obtained, the problem can be stochastic (the reward/loss is drawn from a probability distribution) or non-stochastic. Bubeck et al. [14] propose an extensive review of these methods for various settings.\nWe cast our problem of finding the extreme gradient component as a best arm identification problem as follows. In the remainder of this section, we suppose that we look for a minimum gradient component and thus we look for the arm with minimal loss instead of maximal reward. We consider that the arms are the components of the gradient (we have d arms) and at each pull of a given arm, we observe a loss that is built from a term of the gradient matrix-vector multiplication X>r, as made clear in the sequel.\nIn a stochastic setting, we consider a similar framework as the one we described in Section III-C. We model the loss\nobtained from the k-th pull of arm j \u2208 [1, \u00b7 \u00b7 \u00b7 , d] as a random variable V , independent of k, that takes value in the set\n{vj,i . = rixi,j/pi : i = 1, . . . , n}\nso that P(V = vj,i) = pi. From this definition, the expected loss of arm i is\nEV = n\u2211 i=1 pivj,i = n\u2211 i=1 rixi,j = (X >r)j ,\nwhich is the j-th component of our gradient vector. In this setting, given a certain number of pulls, one pull providing a realization of the random variable V of the chosen arm, the objective of a best arm identification algorithm is to provide recommendation about the arm with minimal expected loss, which in our case is the coordinate of smallest value in the d-dimensional vector X>r.\nSeveral algorithms for identifying this best arm have been provided in the literature. Most of them are built around an empirical average loss statistic. This latter can be computed, after s pulls of an arm j, as V\u0302j,s = 1s \u2211s t=1 vj,i(k,j), where i(k, j) is the index i drawn at the k-th pull for arm j. Some of the most interesting algorithms are the successive reject [23] and successive halving [24] algorithms which, given a fixed budget of pulls, iteratively discard after some predefined number of pulls (say s) the worst arm or the worst-half arms, respectively, according to the values {V\u0302j,s}dj=1. These approaches are relatively simple and the successive halving approach is depicted in detail in Algorithm 5. They directly provide some guarantees on the probability to correctly estimate the extreme component of the gradient. For instance, for a fixed budget of pulls T , under some minor and easily satisfied conditions on {vj,i}, the successive halving algorithm correctly identifies the minimum gradient component with probability at least 1 \u2212 3 log2 d \u00b7 exp(\u2212 T8H2 log2 d ) where H2 is a problem-dependent constant (the larger H2 is, the harder the problem is).\nHowever, these two algorithms have the drawbacks to work on individual entries xi,j of the matrix X. Hence, overload due to single memory accesses compared to those needed for accessing chunks of memory may hinder the computational gain obtained by identifying the component with minimal gradient value without computing the full gradient.\nFor the successive halving algorithm, one way of overcoming this issue is to consider non i.i.d sampling of the arm\u2019s loss. As such, we consider that at each iteration, the losses generated by pulling the remaining arms come from the same component j of the residual. This approach has the advantage of working on the full vector xiri, allowing thus efficient memory caching, instead of on individual elements of X.\nLet As\u2032 and A denote the sets of components i that have been drawn after s\u2032 and the subsequent s\u2020 pulls such that s = s\u2032 + s\u2020 respectively, then the loss for arm j after s pulls can be defined as\nV\u0302j,s = V\u0302j,s\u2032 + \u2211 i\u2208A rixi,j pi , (15)\nwhere V\u0302j,0 = 0 and the set A is the same for all arms. In practice, as implemented in the numerical simulations we\n6 Algorithm 5 Successive Halving to find the minimum gradient component\n1: inputs: X, r, set T the budget of pulls 2: Initialize S0 = [d ] 3: V\u0302j,0 = 0,\u2200j \u2208 S0 4: for `= 0,1, \u00b7 \u00b7 \u00b7 , dlog2(d)e -1 do 5: Pull each arm in S` for r` = b T|S`|dlog2(n)ec additional\ntimes and compute resulting loss (non-iid) V\u0302\u00b7,R` in Equation (15) or (non-stochastic) v\u00b7,R` in Equation (16) with R` = \u2211` u=0 ru\n6: Sort arms in S` by increasing value of losses 7: Define S`+1 as the b|S`|/2c indices of arms with smallest values 8: end for 9: output: index of the best arm\nprovide, each component i of A is randomly chosen according to a uniform distribution over 1, \u00b7 \u00b7 \u00b7 , n. The non-iidness of the stochastic process comes from that the arm losses are dependent through the set A. However, this does not hinder the fact that empirical loss V\u0302j,s is still a relevant estimation of the expected loss of arm j.\nNon-stochastic best arm identification has been barely studied and only a very recent work has addressed this problem [25]. In this latter work, the main hypothesis about the non-stochasticity of the loss is that they are assumed to be generated before the game starts. This is exactly our situation since before each estimation of the minimum gradient component, all losses are given by xi,jri and thus can be computed beforehand. In this non-stochastic setting [25], the framework is that the k-th pull of an arm j provides a loss vj,k and the objective of the bandit algorithm is to identify arg minj limk\u2192\u221e vj,k, assuming that such limits exist for all j. Again we can fit our problem of finding the extreme gradient component (here the minimum) into this framework by defining the loss for a given arm at pull k as\nvj,k =  0 if k = 0vj,k\u22121 + r\u03c4kx\u03c4k,j if 1 \u2264 k \u2264 n vj,k\u22121 if k > n\n(16)\nwhere \u03c4 is a predefined or random permutation of the rows of the vector r and the matrix X, and \u03c4k its k-th entry. In practice, we choose \u03c4 to be the same for all the arms for computational reasons as explained above, but in theory this is not necessary [25]. According to this loss definition, we have\nlim k\u2192\u221e vj,k = n\u2211 k=1 r\u03c4kx\u03c4k,j = n\u2211 k=1 rkxk,j = (X >r)j .\nHence, an algorithm that recommends the best arm after a given number of pulls, will return the index of the minimum component in our gradient. Interestingly, the algorithm proposed by Jamieson et al. [25] for solving the non-stochastic best arm identification problem is also the one used in the stochastic setting namely the successive halving algorithm (Alg. 5). This algorithm can be shown to work as is despite the dependence between arm losses. Indeed, each round-robin\npull of the surviving arms can have dependent values, and as long as the algorithm does not adapt to the observed losses during the middle of a round-robin pull [25].\nAs already mentioned, for a fixed budget T of pulls, this successive halving bandit algorithm comes with theoretical guarantee in its ability to identify the best arm. In the stochastic case, the probability of success depends on the number of arms, the number of pulls T and on a parameter denoting the hardness of the problem (see Theorem 4.1 in [24]). In the nonstochastic case, the budget of pulls needed for guaranteeing the correct recovery of the best arm essentially depends on a function \u03b3j(k) such that |vj,k\u2212 limk\u2192\u221e vj,k| \u2264 \u03b3j(k) and on a parameter denoting the gap between any of (X>r)j and the smallest component of X>r which is not accessible unless we compute the exact gradient.\nOne may note the strong resemblance between the matrixvector product approximation as given in (13) and the non-iid bandit strategy, as in this latter setting, we consider the full vector xiri to compute V\u0302\u00b7,s for all remaining arms. This noniid strategy can also be related to the non-stochastic setting if we choose \u03c4 as a random permutation of 1, . . . , n. In addition, in the non-stochastic bandit setting, we can recover the greedy deterministic approach if we assume that the permutation \u03c4 defines a re-ordering of \u2016xi\u2016|ri| in decreasing order, then the accumulation given in Equation (16) is exactly the one given in the greedy deterministic approach. This is the choice of \u03c4 we have considered in our experiments. Multi-armed bandit framework and the gradient approximation approaches use thus similar ways for computing the criteria used for estimating the best arm. The main difference resides in the fact that with multi-armed bandit, one is directly provided with the estimation of the best arm."}, {"heading": "E. Stopping criteria", "text": "In the greedy deterministic and randomized methods introduced in this section, we have no clues on how many elements rixi have to be accumulated in order to achieve a sufficient approximation of the gradient or in the multiarmed bandit approach how many pulls we need to draw. Here, we discuss two possible stopping criteria for the non-bandit algorithms: one that holds for any approach and a second one that holds only for the Frank-Wolfe algorithm in the deterministic sampling case. Discussion on the budgets that needs to be allocated to the bandit problem is also provided.\n1) Stability condition: For the sake of simplicity, we limit the exposition to the search of the smallest component of the gradient, although the approach can be generalized to other cases.\nDenote by j? the coordinate such that j? = arg minj \u2207L(wk)|j and let Ts be the maximal number of iterations or samplings allowed for computing the inexact gradient (for instance, in the greedy deterministic approach, Ts = n). Our objective is to estimate j? with the fewest number t of iterations. For this to be possible, we make the hypothesis that there exists an iteration t1, t1 \u2264 Ts, and\nj? = arg min j \u2207\u0302Lt(wk)|j \u2200t : t1 \u2264 t \u2264 Ts\n7 in other words, we suppose that starting from a given number of iterations t1, the gradient approximation is sufficiently accurate so that the updates of the gradient will leave the minimum coordinate unchanged. Formally, this condition means that \u2200t \u2208 [t1, \u00b7 \u00b7 \u00b7 , n], we have [ \u2207\u0302Lt+\nTs\u2212t\u2211 u=0 Ut+u ] j? \u2264 [ \u2207\u0302Lt+ Ts\u2212t\u2211 u=0 Ut+u ] j \u2200j \u2208 [1, \u00b7 \u00b7 \u00b7 , d].\nwhere each Ut+i = ri(t+u)xi(t+u), i(t+ u) being an index of samples that depends how the greedy or randomized strategy considered. However, checking the above condition is as expensive as computing the full gradient, thus we propose an estimation of j? based on an approximation of this inequality, by truncating the sum to few iterations on each side. Basically, this consists in evaluating j? at each iteration and checking whether this index has changed over the last Ns iterations. We refer to this criterion as the stability criterion, parametrized by Ns.\n2) Error bound criterion: In Section II-B2, we discussed that the convergence of the Frank-Wolfe inexact algorithm can be guaranteed as long as the norm difference between the approximate gradient and the exact one could be upperbounded by some quantity . Formally, this means that the iterations over gradient approximation can be stopped as soon as \u2016\u2207\u0302Lt \u2212\u2207L\u2016\u221e \u2264 , where depends on the curvature of the function L(w) [3]. In practice, the criterion \u2016\u2207\u0302Lt\u2212\u2207L\u2016\u221e cannot be computed as it depends on the exact gradient but it can be upper-bounded by a term that is accessible. For the greedy deterministic approach, by norm equivalence, we have\n\u2016\u2207\u0302Lt \u2212\u2207L\u2016\u221e = \u2225\u2225\u2225\u2211 i6\u2208It xiri \u2225\u2225\u2225 \u221e \u2264 \u2211 i 6\u2208It \u2016xi\u2016\u221e|ri| \u2264 (17)\nHence if the norms {\u2016xi\u2016\u221e} have been precomputed beforehand, this criterion can be easily evaluated at each gradient update iteration.\n3) Pull budget for the bandit: In multi-armed bandit algorithms, one typically specifies the number of pulls T available for estimating the best arm. As such, T can be considered as hyperparameter of the algorithm. A possible strategy for removing the dependency of the bandit algorithm to this pull budget, is to use the doubling trick [25], which consists in running the algorithm with a small value of T and then repeatedly doubling it until some stopping criterion is met. The algorithm can then be adapted so as to exploit the loss computations that have already been carried from iteration to iteration. However, this strategy needs a stopping criterion for the doubling trick. According to Theorem 1 in [25], there exists a lower bound of pulls for which the algorithm is guaranteed to return the best arm. Hence, the following heuristic can be considered: if T \u2032 and 2T \u2032 number of pulls return the same best arm, then we conjecture that the proposed arm is indeed the best one. One can note that this idea is similar to the above-described stability criterion. While this strategy is appealing, in the experiment, we have just fixed the budget of pulls T to a fixed predefined value."}, {"heading": "IV. DISCUSSION", "text": "This section provides comments and discussions of the approaches we proposed compared to existing works."}, {"heading": "A. Relation and gains compared to OMP and variants", "text": "Several recent works on sparse approximation have proposed theoretically founded algorithms. These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28]. Most of these algorithms make use of the top absolute entry of the gradient vector at each iteration. The work presented in this paper is strongly related to these ones as we share the same quest for the top entry. Indeed, the proposed methodology provides tools that can be applied to many sparse approximation algorithms including the aforementioned ones. What makes our work novel and compelling is that at each iteration, the gradient is computed with as few information as possible. If the stopping criterion for estimating this gradient is based on a maximal number of samples \u2014 e.g. we are interested in constructing the best approximation of the gradient from only 20% of the samples\u2014, our approach can be interpreted as a method for computing the gradient on a limited budget. Hence, the proposed method allows to obtain a gain in the computational time needed for the estimation of the gradient. On the downside, if other stopping criteria are used (alone or jointly with the budget criterion), this gain may be partly impaired by further computations needed for their estimation. As an example, the stability criterion induces a O(d) overhead at each iteration due to the max computation."}, {"heading": "B. Relation with other stochastic MP/FW approaches", "text": "Some prior works from the literature are related to the approaches we have proposed in the present paper. Chen et al. [29] have recently introduced a stochastic version of a Matching Pursuit algorithm in a Bayesian context. Their principal contribution was to define some prior distribution over each component of the vector w and then to sample over this distribution so as to estimate w. In their approach, the sparsity pattern related to Matching Pursuit is controlled by the prior distribution which is assumed to be a mixture of two distributions one of which induces sparsity. While this approach is indeed stochastic, it strongly differs from ours in the way stochasticity is in play. As we will discuss in the next subsection, our framework is more related to stochastic gradient than to the stochastic sampling of Chen et al.\nStronger similarities with our work appear in the work of Peel et al. [30]. Indeed, they propose to accelerate the atom selection procedure by randomly selecting a subset of atoms as well as a subset of example for computing X>r. This idea is also the base of our work. However, an essential difference appears as we do not select a subset of atoms. By doing so, we are ensured not to discard the top entry of X>r and thus we can guarantee for instance that our bandit approaches are able to retrieve this top entry with high probability given enough budget of pulls.\nStochastic variants of the Frank-Wolfe algorithm have been recently proposed by Lacoste-julien et al. [31] and Ouyang\n8 et al. [32]. These works are mostly tailored for solving largescale SVM optimization problem and do not focus on sparsity."}, {"heading": "C. About stochasticity", "text": "The randomization approach for approximating the gradient, introduced in Section III-C, involves random sampling of the columns. In the extreme situation where only a single column i is sampled, we thus have \u2207\u0302L = xiri, and the method we propose boils down to a stochastic gradient method. In the context of sparse greedy approximation, the first work devoted to stochastic gradient approximation has been recently released [13]. Nguyen et al. [13] show that their stochastic version of the iterative hard thresholding algorithm, or the gradient matching pursuit algorithm which aim at greedily solving a sparse approximation problem with arbitrary loss functions, behave properly in expectation.\nThe randomized approach we propose in this work goes beyond the stochastic gradient method for greedy approximation, since it also provides a novel approach for computing stochastic gradient. Indeed, we differ from their setting in several important aspects: \u2022 First, in our stochastic gradient approximation, we always\nconsider a number of samples larger than 1. As such, we are essentially using a stochastic mini-batch gradient. \u2022 Second, the size of the mini-batch is variable (depending on the stopping criterion considered) and it depends on some heuristics that estimates on the fly the ability of the approximate gradient to retrieve the top entry of the true gradient. \u2022 Finally, one important component of our approach is the importance sampling used in the stochastic minibatch sampling. This component theoretically helps in reducing the error of the gradient estimation [33]. In the context of matrix multiplication approximation we used for developing the randomized approach in Section III-C, theoretical results of Drineas et al. [22] have also shown there exists an importance sampling that minimizes the expectation of the Frobenius norm of the matrix multiplication approximation. Our experiments corroborate these results showing that, compared to a uniform sampling, importance sampling clearly enhances the efficiency in retrieving the top entry of the true gradient.\nAll these differences make our randomized algorithm not only clearly distinguishable from stochastic gradient approaches, but also harder to analyze. We thus defer for further work the theorical analyses of such a stochastic adaptive-size minibatch gradient coupled with an importance sampling approach."}, {"heading": "D. Theoretical considerations", "text": "Although a complete theoretical analysis of the algorithm is out of the scope of this paper, an interesting property deserves to be mentioned here. Note that, unlike stochastic gradient approaches, our algorithm is built upon inexact gradients that hopefully have the same minimum component as the true gradient. If this latter fact occurs along the iterations of the\nFW or OMP algorithms, then all the properties (e.g linear convergence, exact recovery property. . . ) of these algorithms apply. Based on the probability of recovering an exact minimum component of the gradient at each iteration, we show below a bound on the probability of our algorithm to recover at a given iteration K of OMP or FW, the same sequence of minimum components as the one obtained with exact gradient.\nSuppose that at each iteration t of OMP or FW, our algorithm for estimating the minimum component correctly identifies this component with a probability at least 1\u2212Pt and there exists P\u0304 so that Pt > P\u0304 , \u2200t \u2264 K, then the probability of identifying the correct sequence of minimum component is at least 1\u2212KP\u0304 . We get this thanks to the following reasoning. Denote B(t) = {I1 = i1e, I2 = i2e, . . . , It = ite} the event that our algorithm outputs the exact sequence of minimum components up to iteration t, It and ite being the coordinates retrieved with the inexact and exact gradient. Similarly, we note A(t) = {It = ite} the event of retrieving, at iteration t, the correct component of the gradient. We assume that P(B(0)) = 1. Formally, we are interested in lower-bounding the probability of B(K). By definition, we have\nP(B(K)) = P(A(K)|B(K \u2212 1))P(B(K \u2212 1))\n= P(B(0)) K\u220f t=1 P(A(t)|B(t\u2212 1)).\nNote that this equation captures all the time dependencies that occur during the FW or the OMP algorithm. Since P(A(t)|B(t\u2212 1)) \u2265 1\u2212 Pt, we have\nP(B(K)) \u2265 K\u220f t=1 (1\u2212 Pt) \u2265 (1\u2212 P\u0304 )K \u2265 (1\u2212KP\u0304 )\nwhere in the last inequality, we used the fact that (1\u2212u)K \u2265 (1\u2212Ku) for 0 \u2264 u \u2264 1.\nFor instance, in the successive halving algorithm, we have Pt = 3 log2 d\u00b7exp ( \u2212 T8H2(t)log2d ) , where H2(t) is a iterationdependent constant [24] and T the number of pulls. Thus, if we define H\u0304 so that \u2200t, H\u0304 \u2265 H2(t), we have P\u0304 = 3 log2 d \u00b7 exp ( \u2212 T\n8H\u0304log2d\n) and\nP (B(K)) \u2265 1\u2212 3K log2 d \u00b7 exp ( \u2212 T\n8H\u0304log2d\n) .\nWe can see that the probability of our OMP or FW having the same behaviour as their exact counterpart decreases with the number K of iterations and the number d of dimensions of the problems and increases with the number of pulls. By rephrasing this last equation, we also get the following property. For \u03b4 \u2208 [0, 1], if the number T of pulls is set so that at each iteration t,\nT \u2265 ( log log2 d\n\u03b4 + log\nK\n\u03b4 + log\n3\n\u03b4\n) 8H\u0304 log2 d\nthen, when using the successive halving algorithm for retrieving the extremum gradient component, an inexact OMP or FW algorithm behaves like the exact OMP or FW with probability 1\u2212\u03b4. This last property is another emphasis on the strength of our inexact gradient method compared to stochastic gradient\n9 descent approaches as it shows that with high probability, all the theoretical properties of OMP or FW (e.g convergence, exact recovery of sparse signal) apply."}, {"heading": "V. NUMERICAL EXPERIMENTS", "text": "In this section, we describe the experimental studies we have carried out for illustrating the computational benefits of using inexact gradient for sparsity-constraint optimization problems."}, {"heading": "A. Experimental setting", "text": "In order to illustrate the benefit of using inexact gradient for sparse learning or sparse approximation, we have set up a simple sparse approximation problem which focuses on the computational gain, and for which a sparse signal has to be recovered by the Frank-Wolfe, OMP or CoSaMP algorithm.\nNote that sparse approximations are mostly used for approximation problems on overcomplete dictionary. This is the case in our experiments, where the dimension d of the learning problem is in most cases larger than the number n of samples. We believe that if the signal or the image at hand to be approximated can be fairly approximated by representations for which fast transforms are available, then it is better (and faster) to indeed used this representation and the fast transform. Sparse approximation problems as considered in the sequel, mostly occur in overcomplete dictionary learning problems. In such a situation, as the dictionary is data-driven, we believe that the approach we propose is relevant.\nThe target sparse signals are built as follows. For a given value of the dictionary size d and a number k of active elements in the dictionary, the true coefficient vector w? is obtained as follows. The k non-zero positions are chosen randomly, and their values are drawn from a zero-mean unit variance Gaussian distribution, to which we added \u00b10.1 according to the sign of the values. The columns of the regression design matrix X \u2208 Rn\u00d7d are drawn uniformly from the surface of a unit hypersphere of dimension n. Finally, the target vector is obtained as y = Xw? + e, where e is a random noise vector drawn from a Gaussian distribution with zero-mean and variance \u03c32e determined from a given signalto-noise as \u03c32e = 1 n\u2016Xw\n?\u20162 \u00b7 10\u2212SNR/10. Unless specified, the SNR ratio has been set to 3. For each setting, the results are averaged over 20 trials, and X, w? and e are resampled at each trial.\nThe criteria used for evaluating and comparing the proposed approaches are the running time of the algorithms and their ability to recover the true non-zero elements of w?. The latter is computed through the F-measure between the support of the true coefficient vector w? and the estimated one w\u0302:\nF-meas = 2 |supp\u03b3(w?) \u222a supp\u03b3(w\u0302)| |supp\u03b3(w?)|+ |supp\u03b3(w\u0302)|\nwhere, supp\u03b3(w) = {j : |wj | > \u03b3} is the support of vector w and \u03b3 is a threshold used to neglect some non-zero coefficients that may be obliterated by the noise. In all our experiments, we have set \u03b3 = 0.001 which is small compared to the minimal absolute value of a non-zero coefficient (0.1).\nAll the algorithms (Frank-Wolfe, OMP and CoSaMP) and exact and inexact gradient codes have been written in Matlab except for the successive reject bandit which as been written in C and transformed in a mex file. All computations have been run on each single core of an Intel Xeon E5-2630 processor clocked at 2.4 GHz in a Linux machine with 144 Gb of memory."}, {"heading": "B. Sparse learning using a Frank-Wolfe algorithm", "text": "For this experiment, the constraint set C is the `1 unit-ball and the loss function is L(w) = 12\u2016y\u2212Xw\u2016 2 2. Our objectives are two-folded: 1) analyze the capability of inexact gradient approaches to\nrecover the true support and compare them to the FW algorithm, 2) compare the two stopping criteria for computing the inexact gradient : the stability condition and the error bound condition.\nIn the latter, while this error bound condition provides an adaptive condition for stopping \u2014 recall that the parameter in Equation (17) is determined automatically through the data and the related curvature of the loss function \u2014, the stability condition needs a user-defined parameter Ns for stopping the accumulation of the partial gradient. In the same spirit, we use a fixed pre-defined budget of pulls in the best-arm identification problem. This budget is given as a ratio of n\u00d7d. The exact gradient is computed using the accumulation strategy as given in Equation (7) so as to make all running times comparable. The maximum number of iterations for FW is set to 5000.\nFigure 1 presents the results obtained for n = 2000 samples, d = 4000 dictionary elements and k = 50 active atoms. We depict the running time and recovery abilities of the FrankWolfe algorithm with an exact gradient (exact), a greedy deterministic gradient sampling computation with a stability stopping criterion (deterministic) and an error bound stopping criterion (grad upb), the randomized approach with an uniform sampling (uniform), and with a best probability sampling as given in Equation (14) (best), the successive reject bandit (succ), the non-iid successive halving approach with losses computed as given in Equation (15) (SuccHalvSame) with a random uniform sampling and the non-stochastic successive halving approach with losses computed as given in Equation (16) (SuccHalvNonStoch) using a permutation \u03c4 that defines a decreasing ordering of the \u2016xi\u2016|ri|.\nThe figures depict the performances with respect to the stopping condition parameter Ns of the stability criterion (the first value in the bracket) and the sampling budget of the bandit approach (n\u00d7d10 z where z is the second value in the bracket). First, we can note that the deterministic approach used with any stopping criterion and the non-stochastic successive halving approaches are able to perfectly recover the exact support of the true vector w?, regardless of the considered stopping criterion\u2019s value. Randomized approaches with uniform and best probability sampling nearly achieve perfect recovery with an average F-measure of 0.975 with the stability criterion Ns equal to 5 for the uniform approach. When Ns increases, the\n10\nperformances of these two approaches also increase but still fail to achieve perfect recovery.\nFrom a running time point of view, the proposed approaches based on greedy deterministic and randomized sampling strategies with stability criterion and the successive halving strategies are faster than the exact FW approach, the plain successive reject method acting on single entry of {xj,irj} and the deterministic method with the error bound condition. For instance, the greedy deterministic approach (green curve) achieves a gain in running time of a factor 2 with respect to the exact Frank-Wolfe algorithm. Interestingly for the greedy deterministic approach and the successive halving approaches, this gain is achieved without compromise on the recovery performance. For the randomized strategies, increasing the stability parameter Ns leads to a very slight increase of running time, hence for these methods, a trade-off can eventually be found. When comparing bandit approaches, one can note the substantial gain in performances that can be obtained by the halving strategy, the non-iid and non-stochastic strategies compared to successive reject. We conjecture that this higher computational running time of the successive reject algorithm is essentially due to computational overhead needed for accessing each single matrix entry Xi,j in memory while all other methods use slices of this matrix (through the samples xi) and thus they can leverage on the chunk of memory access. Best performances jointly in recovery and running times are achieved by the greedy deterministic and the non-stochastic successive halving approaches.\nWhen comparing the stability and the error bound stopping criteria, the latter one is rather inefficient. While grounded on theoretical analysis, this bound is loose enough to be noninformative. Indeed, a careful inspection shows that the error bound criterion accumulates about 5 times more elements rixi than the stability one before triggering. In addition, other computational overheads necessary for the bound estimation, make the approach just as efficient as the exact Frank-Wolfe algorithm.\nIn summary, from this experiment, we can conclude that the non-stochastic bandit approach is the most efficient one. It can\nachieve a gain in computation of about an order of magnitude (the left most point in the Figure 1\u2019s right panel) without compromising accuracy. The greedy deterministic approach with stability criterion performs also very well but it is slightly less efficient. We can remark that these two best methods both use the same strategy of gradient accumulation based on decreasing ordering of \u2016xi\u2016|ri|."}, {"heading": "C. Sparse Approximation with OMP", "text": "Here, we evaluate the usefulness of using inexact gradient in a greedy framework like OMP. The toy problem is similar to the one used above except that we analyze the performance of the algorithm for an increasing number k of active atoms and two sizes of dictionary matrix X have been considered.\nThe same ways for computing the inexact gradient are evaluated and compared in terms of efficiency and correctness to the true gradient in an OMP algorithm. For all sampling approaches, the stopping criterion for gradient accumulation is based on the stability criterion with the parameter Ns adaptively set at 2% of the number n of samples. For the successive reject bandit approach, the sampling budget has been limited to 20% of the number of entries (which is n \u00b7 d in the matrix X. In all cases, the stopping criterion for the OMP algorithm is based on a fixed number of iterations and this number is the desired sparsity k.\nResults are reported in Figure 2. They globally follow the same trend as those obtained for the Frank-Wolfe algorithm. First, note that in terms of support recovery, when the number of active atoms is small, the greedy deterministic approach performs better than the randomized sampling strategies. Bandit approaches perform similar to the greedy deterministic method. As the number of active atoms increases, the bandit approaches succeed better in recovering the extreme component of the gradient while the deterministic approach is slightly less accurate. Note that for any value of k, the randomized strategies suffer more than the other strategies for recovering the true vector w? support. From a running time point of view, again, we note that the deterministic and noniid successive halving bandit approaches seem to be the most\n11\nefficient methods. The gain in running time compared to the exact gradient OMP is slight but significant while it is larger when comparing with the successive reject algorithm."}, {"heading": "D. Sparse Approximation with CoSaMP", "text": "To the best of our knowledge, there are very few greedy algorithms that are able to leverage from stochastic gradient. One of these algorithms has been introduced in [13]. In this experiment, we want to evaluate the efficiency gain achieved by our inexact gradient approach compared to this stochastic greedy algorithm. Our objective is to show that the approach we propose is empirically significantly faster than a pure stochastic gradient approach. For the different versions of the CoSaMP algorithm, we have set the stopping criterion as follows. For the CoSaMP with exact gradient approach, which serves only as a baseline for computing the exact solution, the number of iteration is set to the level of sparsity k of the target signal. A tolerance of the residual norm is also used as a stopping criterion which should be below 10\u22123. Next, for the stochastic and the inexact gradient CoSaMP versions, the algorithms were stopped when the norm of the residual (y\u2212Xw) became smaller than 1.001 times the one obtained by the exact CoSaMP or when a maximal number of iterations. Regarding gradient accumulation, the stopping criterion we choose is based on the stability condition with the parameter NS set dynamically at 2% of the number of samples. For the bandit approaches, we have fixed the budget of pulls at\n0.2n\u00d7 d. Note that for the CoSaMP algorithm, we do not look for the top entry of the gradient vector but for the top 2k entries as such, we have thus straightforwardly adapted the successive halving algorithm to handle such a situation.\nFigure 3 presents the observed results. Regarding support recovery, we remark that all approaches achieve performances similar to the exact CoSaMP. When few active atoms are in play, we can note that sometimes, the stochastic approach of [13] fails to recover the support of w?. This occurs seldom but it happens regardless of the dictionary size we have experimented with.\nFrom a running time perspective, the results show that the proposed approaches are highly more efficient than the exact gradient approach and interestingly, they are faster than a pure gradient stochastic approach. One or two orders of magnitude can be gained depending on the level of sparsity of the signal to be recovered. This observation clearly depicts the trade-off that occurs in sparsity-constrained optimization problems in which the gradient computation and an approximation problem on a limited number of atoms are the major computational burdens (Lines 3 and 5 of Algo 1). Indeed in a stochastic gradient approach, inexact gradient computations are very cheap but more approximation problems to be solved may be needed for achieving a desired accuracy. In the approaches we propose, the inexact gradient computation is slightly more expensive but we somehow \u201censure\u201d that it provides the correct information\n12\nneeded by the CoSaMP algorithm. Hence, our approaches need less approximation problems to be solved, making them more efficient than a stochastic gradient approach.\nWhen comparing the efficiency of the proposed algorithms, the approach based on non-stochastic successive halving and the greedy deterministic approach are the most efficient especially as the number of active atoms grows.\nIn the second experiment, for the successive halving algorithms, we analyze the effect of the pull budget on the running time and on the recovery performance. We consider the following setting of the problem: n = 5000, d = 10000 and k = 50. Results are given in Figure 4. We note that a budget ratio varying from 0.05 and 0.7 allows a good compromise between ability of recovering the true vector and gain in computational time, particularly for the nonstochastic successive halving method. As the budget of pulls decreases, both algorithms fail more frequently in recovery and in addition, the computational gain substantially reduces. This experiment suggests that one should not be too greedy and should allow sufficient amount of pulls. A budget ratio of 0.2 or 0.3 for the bandit algorithms seems to be a good rule of thumb according to our experience.\nOur last experiment with CoSaMP demonstrates how the\nrunning time and the support recovery performance behave with an increasing number n of samples and afterwards with an increasing number of dictionary elements d. We have restricted our comparison to the exact and stochastic CoSaMP, and the CoSaMP variants based on the successive halving bandit algorithms and the greedy deterministic one (which are the most efficient among those we propose). The experimental setup, the stopping criterion for the CoSaMP algorithm as well as the stopping criterion for the gradient accumulation and pull budget are the same as above. Results are depicted in Figure 5. As a sanity check, we note that recovery performances are almost similar for all algorithms with slightly worse performances for the stochastic CoSaMP and the non-iid bandit algorithm based CoSaMP.\nThe computational time results show that all algorithms globally follow the same trend as the number of dictionary atoms or the number of samples increase. Recall that the computational complexity for the gradient computation is O(nd). For the bandit approaches, we use a fixed budget of pulls dependent on nd to compute the inexact gradient. Similarly, for the greedy deterministic approach, the number of accumulation (and the stability criterion) is proportional to the number n of samples and thus the gradient computation is a constant factor of nd. Hence, our findings, illustrated on\n13\nFigure 5, are somewhat natural since the main differences of running time essentially come from a constant factor. This factor is highly dependent on the problem but according to our numerical experiments, a ten-fold factor computational gain can be expected in many cases."}, {"heading": "E. Application to audio data", "text": "We have compared the efficiency of the approaches we propose on a real signal processing application. The audio dataset we use is the one considered by Yaghoobi et al. [34]. This dataset is composed of an audio sample recorded from a BBC radio session which plays classical music. From that audio sample, 8192 pieces of signal have been extracted, each being composed of 1024 time samples. Details about the dataset can be found in [34]. From this dataset, we have learned 2048 dictionary atoms using the approach described in [35]. Our objective is to perform sparse approximation of each of the 8192 audio pieces over the 2048 dictionary atoms using CoSaMP and we want to evaluate the running time and the approximation quality of a CoSaMP algorithm using an exact gradient computation (Exact), a stochastic gradient CoSaMP algorithm (Stoch k) and the CoSaMP variants with inexact gradient computations as we propose. The approximation error is measured as \u2016y\u2212y\u0302\u20162\u2016y\u20162 where y and y\u0302 are respectively the true audio piece and its CoSaMP-based approximation. We thus want to validate that our approaches achieve similar approximation performance than CoSaMP while being faster. For all algorithms, the number of CoSaMP iterations is fixed to the sparsity pattern, here fixed to k = 10. Note that for the stochastic gradient approach, we have also considered a version with more iterations (Stoch 3k) . Results are gathered in Table I and they are obtained as the averaged performance when approximating all the 8192 pieces of audio signal in the dataset. We can see that the inexact approaches we introduce lead to the best compromise between approximation error and running time. For instance, our successive halving algorithms achieve similar approximation errors than the exact CoSaMP but they are 3 times faster. At the contrary, the stochastic\ngradient CoSaMP approaches are efficient but lack in properly approximating target audio pieces."}, {"heading": "F. Benchmark classification problems", "text": "We have also benchmarked our algorithms on real-world high-dimensional learning classification problems. These datasets are frequently used for evaluating sparse learning problems [36], [4] and more details about them can be found in these papers. Here, we considered CoSaMP as a learning algorithm and our objective is to validate the fact that the approaches we propose for computing approximate gradient are able to speed up computation time while achieving the same level of accuracy as the exact gradient. For the approximate gradient computations, we have considered the stability criterion with NS = ntrain20 (ntrain being the number of training examples) for the deterministic and randomized approaches, and we have set the budget as 0.2ntrain \u00d7 d for the bandit approaches.\nThe protocol we have set up is the following. Training and test sets are obtained by randomly splitting the dataset in a 80% \u2212 20% fold. For model selection, the training set is further split in two sets of equal size. The parameter we have cross-validated is the number of non-zero elements k in w. It has been selected among 10, 50, 100, 250 so as to maximize the accuracy on the validation set. This value of k\n14\nhas also been used as the maximal number of iterations for all the algorithms except for the stochastic ones. For these, we have reported accuracies and running times for a number of maximal iterations of k and 3k.\nResults averaged over 20 replicas of the training and test sets are reported in Table II. Stochastic approaches fail in learning a relevant decision function. We can note that our deterministic and randomized approaches are more efficient than the exact CoSaMP but are less accurate. On the other hand, our bandit approaches achieve nearly similar accuracy to CoSaMP while being at least 30 times faster."}, {"heading": "VI. CONCLUSIONS", "text": "The methodologies proposed in this paper aim at accelerating sparsity-constrained optimization algorithms. This is made possible thanks to the key observation that, at each iteration, only the component of the gradient with smallest or largest entry is needed, instead of the full gradient. By exploiting this insight, we proposed greedy algorithms, randomized approaches and bandit-based best arm identification methods for estimating efficiently this top entry. Our experimental results show that the bandit and the greedy approaches seem to be the most efficient methods for this estimation. Interestingly, the\nbandit approaches come with guarantees that, given a sufficient number of draws, this top entry can be retrieved with highprobability.\nFuture works will be geared towards gaining further theoretical understandings on the good behaviour of the greedy approach, linking the number of iterations needed for the Frank-Wolfe algorithm to converge, with the quality of the gradient approximation in the greedy and randomized approaches, analyzing the role of the importance sampling in the randomized methods. In addition, we plan to explore how this work can be extended to an online and/or distributed computation setting."}], "references": [{"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression (with discussion)", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Revisiting frank-wolfe : Projection free sparse convex optimization", "author": ["M. Jaggi"], "venue": "Proceedings og the International Conference on Machine Learning, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonconvex regularizations for feature selection in ranking with sparse svm", "author": ["L. Laporte", "R. Flamary", "S. Canu", "S. Dejean", "J. Mothe"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 6, pp. 1118\u20131130, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Matching pursuit with time-frequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Trans Signal Processing, vol. 41, no. 12, pp. 3397\u2013 3415, 1993.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Orthogonal matching pursuit : Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "Proc. of the 27th Annual Asilomar Conference on Signals, Systems and Computers, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Embedding prior knowledge within compressed sensing by neural networks", "author": ["D. Merhej", "C. Diab", "M. Khalil", "R. Prost"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 10, pp. 1638\u20131649, Oct 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "Proceedings of the twentyfirst international conference on Machine learning. ACM, 2004, pp. 116\u2013123.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Pegasos : Primal estimated subgradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proceedings of the International Conference on Machine Learning, 2007, pp. 807\u2013814.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic methods for l 1-regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 1865\u20131892, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1865}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 315\u2013323.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear convergence of stochastic iterative greedy algorithms with sparse constraints", "author": ["N Nguyen", "Deanna Needell", "T. Woolf"], "venue": "http://arxiv.org/abs/1407.0088, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "Algorithmic Learning Theory. Springer, 2009, pp. 23\u201337.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "More efficient sparsity-inducing algorithms using inexact gradient", "author": ["Alain Rakotomamonjy", "Sokol Ko\u00e7o", "Liva Ralaivola"], "venue": "Signal Processing Conference (EUSIPCO), 2015 23rd European. IEEE, 2015, pp. 709\u2013713.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient pursuits", "author": ["Thomas Blumensath", "Michael E Davies"], "venue": "Signal Processing, IEEE Transactions on, vol. 56, no. 6, pp. 2370\u20132382, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Orthogonal matching pursuit for sparse quantile regression", "author": ["Aleksandr Aravkin", "Aurelie Lozano", "Ronny Luss", "Prabhajan Kambadur"], "venue": " Data Mining (ICDM), 2014 IEEE International Conference on. IEEE, 2014, pp. 11\u201319.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Group orthogonal matching pursuit for logistic regression", "author": ["Aur\u00e9lie C Lozano", "Grzegorz Swirszcz", "Naoki Abe"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 452\u2013460.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J. Tropp"], "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy sparsity-constrained optimization", "author": ["Sohail Bahmani", "Bhiksha Raj", "Petros T Boufounos"], "venue": "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 807\u2013841, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Some comments on wolfe\u2019s away step", "author": ["J. Gu\u00e9lat", "P. Marcotte"], "venue": "Mathematical Programming, vol. 35, no. 1, 1986.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1986}, {"title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication", "author": ["P. Drineas", "R. Kannan", "M. Mahoney"], "venue": "SIAM Journal on Computing, vol. 36, no. 1, pp. 132\u2013157, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Best arm identification in multi-armed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "COLT-23th Conference on Learning Theory- 2010, 2010, pp. 13\u201320.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Zohar Karnin", "Tomer Koren", "Oren Somekh"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1238\u20131246.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-stochastic best arm identification and hyperparameter optimization", "author": ["K. Jamieson", "A. Talwalkar"], "venue": "Proceedings of the 19th International Workshop on Artificial Intelligence and Statistic, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J. Tropp", "A. Gilbert"], "venue": "IEEE Trans. Information Theory, vol. 53, no. 12, pp. 4655\u20134666, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for simultaneous sparse approximation. part I: Greedy pursuit", "author": ["J. Tropp", "A. Gilbert", "M. Strauss"], "venue": "Signal Processing, vol. 86, pp. 572\u2013588, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["S. Shalev-Shwartz", "N. Srebro", "T. Zhang"], "venue": "Siam Journal on Optimization, vol. 20, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic matching pursuit for bayesian variable selection", "author": ["R.-B. Chen", "C.-H. Chu", "T.-Y. Lai", "Y. Wu"], "venue": "Statistics and Computing, vol. 21, no. 2, pp. 247\u2013259, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Matching pursuit with stochastic selection", "author": ["Thomas Peel", "Valentin Emiya", "Liva Ralaivola", "Sandrine Anthoine"], "venue": "Signal Processing Conference (EUSIPCO), 2012 Proceedings of the 20th European. IEEE, 2012, pp. 879\u2013883.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Block-  16 coordinate frank-wolfe optimization for structural svms", "author": ["S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher"], "venue": "Proceedings of the International Conference on Machine Learning, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast stochastic frank-wolfe algorithms for nonlinear svms", "author": ["Hua Ouyang", "Alexander G Gray"], "venue": "SDM. SIAM, 2010, pp. 245\u2013256.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic optimization with importance sampling", "author": ["P Zhao", "T Zhang"], "venue": "http://arxiv.org/abs/1401.2753, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing, vol. 57, no. 6, pp. 2178\u20132191, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Direct optimization of the dictionary learning problem", "author": ["A. Rakotomamonjy"], "venue": "IEEE Trans. on Signal Processing, vol. 61, no. 12, pp. 5495\u2013 5506, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems", "author": ["P. Gong", "C. Zhang", "Z. Lu", "J. Huang", "J. Ye"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, Jun. 2013, pp. 37\u201345.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The resulting problem is the wellknown Lasso problem [1] and a large variety of algorithms for its resolution exist, ranging from homotopy methods [2] to the Frank-Wolfe (FW) algorithm [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "The resulting problem is the wellknown Lasso problem [1] and a large variety of algorithms for its resolution exist, ranging from homotopy methods [2] to the Frank-Wolfe (FW) algorithm [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "The resulting problem is the wellknown Lasso problem [1] and a large variety of algorithms for its resolution exist, ranging from homotopy methods [2] to the Frank-Wolfe (FW) algorithm [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 3, "context": "In the same flavour, non-convex continuous penalties are also common solutions for relaxing the `0 pseudo-norm [4], [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "In this last context, a flurry of algorithms have been proposed, the most popular ones being the Matching Pursuit (MP) and the Orthogonal Matching Pursuit (OMP) algorithms [6], [7], [8].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "In this last context, a flurry of algorithms have been proposed, the most popular ones being the Matching Pursuit (MP) and the Orthogonal Matching Pursuit (OMP) algorithms [6], [7], [8].", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "In this last context, a flurry of algorithms have been proposed, the most popular ones being the Matching Pursuit (MP) and the Orthogonal Matching Pursuit (OMP) algorithms [6], [7], [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 7, "context": "Stochastic gradient descent (SGD) algorithms are now classical methods for avoiding the computation of the full gradient in large-scale learning problems [9], [10].", "startOffset": 154, "endOffset": 157}, {"referenceID": 8, "context": "Stochastic gradient descent (SGD) algorithms are now classical methods for avoiding the computation of the full gradient in large-scale learning problems [9], [10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "Most of these works have been devoted to smooth composite optimization although some efforts addressing `1-regularized problems exist [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "Recently, these SGD algorithms have been further accelerated through the introduction of variance reduction methods for gradient estimation [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "[13] have proposed stochastic versions of a gradient pursuit algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Next, by casting the problem as a best arm identification multiarmed bandit problem [14], we are able to derive an algorithm that directly estimates the best component of the gradient.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "This paper is an extended version of the conference paper [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "1) Gradient Pursuit: This algorithm is a generalization of the greedy algorithm known as Orthogonal Matching Pursuit (OMP) to generic loss functions [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "While conceptually simple, this algorithm comes with theoretical guarantees on its ability to recover the exact underlying sparsity pattern of the model, for different types of loss functions [7], [17], [18].", "startOffset": 192, "endOffset": 195}, {"referenceID": 15, "context": "While conceptually simple, this algorithm comes with theoretical guarantees on its ability to recover the exact underlying sparsity pattern of the model, for different types of loss functions [7], [17], [18].", "startOffset": 197, "endOffset": 201}, {"referenceID": 16, "context": "While conceptually simple, this algorithm comes with theoretical guarantees on its ability to recover the exact underlying sparsity pattern of the model, for different types of loss functions [7], [17], [18].", "startOffset": 203, "endOffset": 207}, {"referenceID": 14, "context": "Several variations of this algorithm have been proposed ranging from methods exploring new pursuits directions instead of the gradient [16], to methods making only slight changes to the original one that have strongly impacted the ability of the algorithm to recover signals.", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "For instance, the CoSaMP [19] and GraSP [20] algorithms select the top 2K entries in the absolute gradient, K being the desired sparsity pattern, optimize over these entries and the already selected K ones, and prune the resulting estimate to be K-sparse.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "For instance, the CoSaMP [19] and GraSP [20] algorithms select the top 2K entries in the absolute gradient, K being the desired sparsity pattern, optimize over these entries and the already selected K ones, and prune the resulting estimate to be K-sparse.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "Despite its simplistic nature, the FW algorithm has been shown to be linearly convergent [21], [3].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Despite its simplistic nature, the FW algorithm has been shown to be linearly convergent [21], [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "sk = arg mins\u2208C s \u2207\u0302L(wk), then in order to ensure convergence, it is sufficient to have sk so that [3]", "startOffset": 100, "endOffset": 103}, {"referenceID": 20, "context": "Indeed, a result given by [22] (Lemma 4) says that the element p \u2208 \u2206n that minimizes E[\u2016X>r\u2212 \u0108\u2016F ] is such that", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "[14] propose an extensive review of these methods for various settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Some of the most interesting algorithms are the successive reject [23] and successive halving [24] algorithms which, given a fixed budget of pulls, iteratively discard after some predefined number of pulls (say s) the worst arm or the worst-half arms, respectively, according to the values {V\u0302j,s}j=1.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Some of the most interesting algorithms are the successive reject [23] and successive halving [24] algorithms which, given a fixed budget of pulls, iteratively discard after some predefined number of pulls (say s) the worst arm or the worst-half arms, respectively, according to the values {V\u0302j,s}j=1.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Non-stochastic best arm identification has been barely studied and only a very recent work has addressed this problem [25].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "In this non-stochastic setting [25], the framework is that the k-th pull of an arm j provides a loss vj,k and the objective of the bandit algorithm is to identify arg minj limk\u2192\u221e vj,k, assuming that such limits exist for all j.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "In practice, we choose \u03c4 to be the same for all the arms for computational reasons as explained above, but in theory this is not necessary [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "[25] for solving the non-stochastic best arm identification problem is also the one used in the stochastic setting namely the successive halving algorithm (Alg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Indeed, each round-robin pull of the surviving arms can have dependent values, and as long as the algorithm does not adapt to the observed losses during the middle of a round-robin pull [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "1 in [24]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "Formally, this means that the iterations over gradient approximation can be stopped as soon as \u2016\u2207\u0302Lt \u2212\u2207L\u2016\u221e \u2264 , where depends on the curvature of the function L(w) [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 23, "context": "A possible strategy for removing the dependency of the bandit algorithm to this pull budget, is to use the doubling trick [25], which consists in running the algorithm with a small value of T and then repeatedly doubling it until some stopping criterion is met.", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "According to Theorem 1 in [25], there exists a lower bound of pulls for which the algorithm is guaranteed to return the best arm.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 24, "endOffset": 27}, {"referenceID": 24, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "These works include OMP [7], [26], greedy pursuit [16], [27], CoSaMP [19] and several others like [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "[29] have recently introduced a stochastic version of a Matching Pursuit algorithm in a Bayesian context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] and Ouyang", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In the context of sparse greedy approximation, the first work devoted to stochastic gradient approximation has been recently released [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "[13] show that their stochastic version of the iterative hard thresholding algorithm, or the gradient matching pursuit algorithm which aim at greedily solving a sparse approximation problem with arbitrary loss functions, behave properly in expectation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "This component theoretically helps in reducing the error of the gradient estimation [33].", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "[22] have also shown there exists an importance sampling that minimizes the expectation of the Frobenius norm of the matrix multiplication approximation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Pt = 3 log2 d\u00b7exp ( \u2212 T 8H2(t)log2d ) , where H2(t) is a iterationdependent constant [24] and T the number of pulls.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "For \u03b4 \u2208 [0, 1], if the number T of pulls is set so that at each iteration t,", "startOffset": 8, "endOffset": 14}, {"referenceID": 11, "context": "One of these algorithms has been introduced in [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "When few active atoms are in play, we can note that sometimes, the stochastic approach of [13] fails to recover the support of w.", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Details about the dataset can be found in [34].", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "From this dataset, we have learned 2048 dictionary atoms using the approach described in [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 34, "context": "These datasets are frequently used for evaluating sparse learning problems [36], [4] and more details about them can be found in these papers.", "startOffset": 75, "endOffset": 79}], "year": 2016, "abstractText": "Several sparsity-constrained algorithms such as Orthogonal Matching Pursuit or the Frank-Wolfe algorithm with sparsity constraints work by iteratively selecting a novel atom to add to the current non-zero set of variables. This selection step is usually performed by computing the gradient and then by looking for the gradient component with maximal absolute entry. This step can be computationally expensive especially for large-scale and high-dimensional data. In this work, we aim at accelerating these sparsity-constrained optimization algorithms by exploiting the key observation that, for these algorithms to work, one only needs the coordinate of the gradient\u2019s top entry. Hence, we introduce algorithms based on greedy methods and randomization approaches that aim at cheaply estimating the gradient and its top entry. Another of our contribution is to cast the problem of finding the best gradient entry as a best arm identification in a multi-armed bandit problem. Owing to this novel insight, we are able to provide a bandit-based algorithm that directly estimates the top entry in a very efficient way. Theoretical observations stating that the resulting inexact FrankWolfe or Orthogonal Matching Pursuit algorithms act, with high probability, similarly to their exact versions are also given. We have carried out several experiments showing that the greedy deterministic and the bandit approaches we propose can achieve an acceleration of an order of magnitude while being as efficient as the exact gradient when used in algorithms such as OMP, Frank-Wolfe or CoSaMP.", "creator": "LaTeX with hyperref package"}}}