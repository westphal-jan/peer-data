{"id": "1508.06585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Towards universal neural nets: Gibbs machines and ACE", "abstract": "we study a class description of neural communication nets - gibbs machines - which are a type of circular variational auto - encoders, designed for gradual learning. they offer readers an universal platform for incrementally adding newly learned features, including physical symmetries retained in this space / time. combining all them with classifiers gives rise to a brand of universal generative neural nets - stochastic auto - classifier - encoders ( ace ). cyclic ace inhibitors preserve the non - gaussian and clustering nature of real - life data and have state - of - the - art performance, rendered both practical for classification and density estimation for the mnist data generator set.", "histories": [["v1", "Wed, 26 Aug 2015 17:43:08 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v1", null], ["v2", "Sat, 5 Sep 2015 21:49:06 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v2", null], ["v3", "Tue, 10 Nov 2015 03:35:59 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v3", null], ["v4", "Fri, 8 Apr 2016 22:11:23 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v4", null], ["v5", "Thu, 30 Jun 2016 06:26:34 GMT  (651kb,D)", "http://arxiv.org/abs/1508.06585v5", "v5: added thermodynamic identities and variational error estimation; expanded references"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["galin georgiev"], "accepted": false, "id": "1508.06585"}, "pdf": {"name": "1508.06585.pdf", "metadata": {"source": "META", "title": "Towards universal neural nets: Gibbs machines and ACE", "authors": ["Galin Georgiev"], "emails": ["GALIN.GEORGIEV@GAMMADYNAMICS.COM"], "sections": [{"heading": "1. Introduction.", "text": ""}, {"heading": "1.1. Universality.", "text": "We buck the recent trend of building highly specialized neural nets by exploring nets which accomplish multiple tasks without compromising performance. An universal net can be tentatively described as one which, among other things: i) works for a variety of applications, i.e. visual recognition/reconstruction, speech recognition/reconstruction, natural language processing, etc; ii) performs various tasks: classification, generation, probability density estimation, etc; iii) is self-contained, i.e., does not use specialized external machine learning methods; iv) is biologically plausible."}, {"heading": "1.2. Probabilistic and quantum viewpoint.", "text": "The input of a neural net is typically a set of P observations {x\u00b5}P\u00b5=1, which can be represented mathematically as row-vectors in the space spanned by observables {xi}Ni=1, e.g., the N pixels on a screen. The net is then asked to perform classification, estimation, generation, etc, tasks on it. In generative nets, this is accomplished by randomly generating L latent observations {z(\u03ba)\u00b5 }L\u03ba=1 for every observation x\u00b5. This induced \u201cuncertainty\u201d of the \u00b5-th\nstate is modeled by a model conditional density p(z|x\u00b5). It is the copy-cat, in imaginary time/space, of the (squared) wave function from quantum mechanics1, and fully describes the \u00b5-th conditional state ( x\u00b5, {z(\u03ba)\u00b5 }L\u03ba=1 ) . In statistical mechanics parlance, the latents are fluctuating microscopic variables, while the macroscopic observables are obtained from them via some aggregation. In the absence of physical time, observations are thus interpreted as partial equilibria of independent small parts of the expanded (by a factor of L) original data set.\nThe quality of the model conditional density, or more generally - the model joint density p(z,x\u00b5) - is judged by the \u201cdistance\u201d from the implied marginal density p(x) =\u222b p(x, z)dz to the empirical marginal density r(x). This distance is called cross-entropy or negative log-likelihood and denoted by\u2212 logL(r||p) := E(\u2212 log p(x))r(x), where E()r() is an expectation with respect to r(). Its minimization is the ultimate goal."}, {"heading": "1.3. Equilibrium setting. Gibbs machines.", "text": "Einstein originated the theory of equilibrium i.e. small fluctuations in early 20-th century (Einstein, 2006). He used an exponential model density pExp() in space and derived the Brownian diffusion i.e., Gaussian model density pG() in space/time. They are special cases of a broad class of densities - Gibbs or exponential densities - which form the foundation of classic statistical mechanics. Gibbs densities are variational maximum-entropy densities and hence optimal for modeling equilibria.\nWe argue in sub-sections 2.3, 3.2 that Gibbs densities are also optimal for modeling fully-generative equilibrium nets, and call the nets using them Gibbs machines. They were inspired by the first fully-generative nets - the variational auto-encoders (VAE) (Kingma & Welling, 2014), (Rezende et al., 2014) - and employ the same upper bound (3.4) for the negative log-likelihood (because of its univer-\n1 Strictly speaking, we will employ unbounded densities and hence stochastic analysis formalism and its centerpiece - the diffusion equation. But they are formally equivalent to the quantummechanical formalism and its centerpiece - the Schrodinger equation - in imaginary time/space coordinates.\nar X\niv :1\n50 8.\n06 58\n5v 1\n[ cs\n.C V\n] 2\n6 A\nug 2\n01 5\nsality for fully-generative equilibrium nets, section 3.2). Like their physics counterparts, Gibbs machines offer a platform for mimicking the gradual nature of learning: already learned symmetry statistics like space/time symmetries, can be added incrementally and accelerate learning, sub-sections 1.5, 2.3, 3.3."}, {"heading": "1.4. The curse of Gaussianization.", "text": "Unlike physics and Brownian particles, human data is decidedly non-equilibrium in nature and exhibits large fluctuations and non-Gaussian behavior. Unfortunately, some of the key features of modern neural nets, like non-linear activation functions and dropout (Srivastava et al., 2014), come at the high price of Gaussianizing the data set.\nQuantifying non-Gaussianity and \u201cdistance\u201d from equilibrium is not easy when dealing with large number of observables N and observations P . Fortunately, there is a one-dimensional proxy for non-Gaussianity of a multidimensional data set: the non-Gaussianity of the negative Gaussian log-likelihoods {\u2212 log pG(z\u00b5)}\u00b5. Here, \u2212 log pG(z\u00b5) = 12z\u00b5C(z)\n\u22121z\u00b5 +const, with a multivariate Gaussian NNlat(0,C(z)) as model density and C(z) the empirical covariance2. These negative observation entropies, as interpreted by Einstein, are related to singular value decomposition and are central in theory of fluctuations (Landau & Lifshitz, 1980), chapter 12. Their second moment E ( (log pG(z\u00b5)) 2 ) r\nis proportional to the multivariate kurtosis measuring the \u201cfatness\u201d of the density.\nThe right quantile-quantile (Q-Q) plot in Figure 1 shows the Gaussianization effect of non-linearities and dropout for the MNIST data set (LeCun et al., 2015). Nonlinearities Gaussianize because they are compressive in nature and \u201crectify\u201d the unlikely (with large negative loglikelihoods) observations, which we refer to as intricates, see Figure 3. Dropout Gaussianizes because it drops latent variables and hence decreases the kurtosis."}, {"heading": "1.4.1. NON-GENERATIVE ACE.", "text": "It is precisely the intricates, which - because of their extreme non-Gaussianity, see Figure 2 - are ideal candidates for \u201cfeature vectors\u201d in classification tasks (Hyvarinen et al., 2009), section 7.9, 7.10. Their conjugates are then the \u201creceptive fields\u201d or \u201cfeature detectors\u201d3 - see open\n2If the latent observations {z\u00b5}P\u00b5=1 come from an Nlatdimensional Gaussian distribution, the density of these negative Gaussian log-likelihoods is proportional to the familiar F (Nlat, P \u2212 Nlat) density (Mardia et al., 1979), sections 1.8, 3.5. For the typical case P \u2212Nlat \u2192\u221e, it is proportional to the chi-squared density \u03c72Nlat(), which in turn converges to a rescaled GaussianN (0, 1), as Nlat \u2192\u221e.\n3Recall that, for a given row-vector observation x\u00b5, its conjugate is x\u030c\u00b5 = x\u00b5C\u22121, where C is the covariance matrix. Up\nproblem 1 in section 5. We show on the top (respectively, bottom) plot in Figure 3 the most (respectively, least) 30 intricate images in MNIST, in descending order of Gaussian negative log-likelihood \u2212 log pG(x\u00b5) \u2265 0, from the class corresponding to the digit 8.\nIn order to preserve the non-Gaussianity of the data, and improve performance significantly along the way, we will combine classifiers with auto-encoders - hence the name auto-classifier-encoder (ACE). Auto-encoders have a reconstruction error in their negative log-likelihood and thus force the net to be more faithful to the raw data. ACE si-\nto a constant, the Gaussian negative log-likelihood is thus the inner product of an observation and its conjugate, in the standard Euclidean metric: \u2212 log pG(x\u00b5) = 12 < x\u030c\u00b5,x\u00b5 >.\nmultaneously classifies and reconstructs, assuming an independence between the two, and hence additivity of the respective negative log-likelihoods:\n\u2212logLACE = \u2212logLAE \u2212 logLC . (1.1)\nIn its first - non-generative - installment, ACE can do with a standard classifier and a shallow auto-encoder in the dual space of observations. It still beats handily the peers in its class, Figure 7, right."}, {"heading": "1.4.2. NON-GAUSSIAN DENSITIES.", "text": "In real-life data sets, the number of observations P \u2192 \u221e, while the dimension of observables N is fixed. An universal net will hence tend to work better when the dimension of latent layers Nlat \u2265 N , i.e. have the so-called overcomplete representation (Coates et al., 2011). When Nlat >> N , for any given N -dimensional observation x\u00b5, only a small number of latents {z\u00b5j}Nlatj=1 deviate significantly from zero, as on the left plot of Figure 2. For these sparse representations, sampling from high-entropy Gaussian-like densities, as on the right plot of Figure 2, is flawed. Sampling instead from \u201cfat-tail\u201d densities offers a significant performance improvement for MNIST, Figure 8, right. As in mathematical finance, stochastic volatility and jumps are arguably the first natural source of nonGaussianity, and are almost fully-tractable. The q-Gibbs machines offer another venue, sub-section 2.3."}, {"heading": "1.4.3. GENERATIVE ACE.", "text": "An even greater issue for current nets is the spontaneous \u201cclumping\u201d or clusterization which is prevalent in reallife data sets. Statistical mechanics deals with it by introducing higher-hierarchy densities which are conditional on low-hierarchy densities. Clusterization aggregates the low-\nhierarchy partial equilibria - the observations - into higherhierarchy partial equilibria - clusters, sub-section 1.2.\nTo mimic this universal phenomenon, in its second, generative, installment, ACE combines a classifier and a generative auto-encoder in the same space of observables, in a brand of an auto-encoder supervision, Figure 4. ACE generalizes the classic idea of using separate decoders for separate classes, (Hinton et al., 1995). In training, the conditional latent density p(z|x\u00b5) from sub-section 1.2 is generalized to p(z|x\u00b5, c\u00b5), where c\u00b5 is the class or label of the \u00b5-th observation. Since, of course, classification labels can not be used on the testing set, the sampling during testing is from a mixture of densities, with class probabilities supplied by the classifier (hence the dashed line in Figure 4). The ACE is universal in the sense of subsection 1.1 and achieves state-of-the-art performance, both as a classifier and a density estimator, Figure 8. For its relation with information geometry, see open problem 6 in section 5."}, {"heading": "1.5. Symmetries in the latent manifold.", "text": "When the dimensionality Nlat of the ACE latent layer is low, traversing the latent dimension in some uniform fashion describes the latent manifold for a given class. Figure 5 shows the dominant dimension for each of the 10 classes in MNIST. This so-called manifold learning by modern feed-forward nets was pioneered by the contractive autoencoders (CAE) (Rifai et al., 2012). A symmetry in our context is, loosely speaking, a one-dimensional parametric transformation, which leaves the log-likelihood unchanged. In probabilistic terms, this is equivalent to the existence of a one-parametric density, from which \u201csymmetric\u201d observations are sampled, see (1.2) below.\nNets currently learn symmetries from the training data, after it is artificially augmented, e.g. by adding rotated, translated, rescaled, etc, images, in the case of visual recogni-\ntion. But once a symmetry is learned, it does not make sense to re-learn it for every new data set.\nWe hence propose the reverse approach: add the symmetry explicitly to the latent layer, alongside its Noether invariant, (Gelfand & Fomin, 1963). As an example, consider translational symmetries in a two-dimensional system with coordinates (z(h), z(v)) \u2208 R2. They imply the conservation of the horizontal and vertical momenta (\u2212i}\u2202/\u2202z(h),\u2212i}\u2202/\u2202z(v)) = (p(h), p(v)) \u2208 R2 and a quantum mechanical wave function \u223c e i }p (h)(z(h)\u2212h)+ i}p (v)(z(v)\u2212v), where (h, v) are offsets, (Landau & Lifshitz, 1977), section 15. After switching to imaginary time/space as in sub-section 1.2, and setting } = 1, the corresponding conditional model density for a given observation/state \u00b5 is:\np(z|x\u00b5) \u223c e \u22122p(h)\u00b5 |z (h)\u2212h\u00b5|\u22122p(v)\u00b5 |z (v)\u2212v\u00b5|, (1.2)\ni.e. a two-dimensional Laplacian which fits 4 in the Gibbs machine paradigm (2.3). We demonstrate in sub-section 3.3 how to build-in translational, scaling and rotational\n4 Technically, Laplacian is not in the exponential class, but it is a sum of two exponential densities in the domains (\u2212\u221e, \u00b5), [\u00b5,\u221e) defined by its mean \u00b5, and those densities are in the exponential class in their respective domains. Laplacian is biologically-plausible because it is a bi-product of squaring Gaussians.\nsymmetry in a net, by computing the symmetry statistics like {h\u00b5, v\u00b5} explicitly and estimating the respective invariants i.e. momenta with the rest of the net parameters."}, {"heading": "2. Theoretical background.", "text": ""}, {"heading": "2.1. Conditional densities.", "text": "As argued in sub-section 1.2, the conditional density p(z|x\u00b5) is the copy-cat of the wave function of the \u00b5-th conditional state in quantum mechanics, and is thus central in the generative neural net formalism.\nFor discrete data, the minimization of the negative loglikelihood is equivalent to minimizing the Kullback-Leibler divergence D(r||p) = \u2211 x r(x) log r(x) p(x) between empirical r() and model p() densities, since \u2212 logL(r||p) = S(r) + D(r||p), where S(r) = E(\u2212 log r)r is the entropy of r(). Latents are not a priori given but rather sampled from a closed-form conditional model density p(z|x). The optimization target hence is the mixed empirical density r(x, z) = 1P \u2211 \u00b5 \u03b4(x \u2212x\u00b5)p(z|x\u00b5) (Kulhavy\u0300, 1996), section 2.3;(Cover & Thomas, 2006), problem 3.12. The marginal empirical densities are r(x) = 1 P \u2211 \u00b5 \u03b4(x \u2212 x\u00b5), r(z) = 1 P \u2211 \u00b5 p(z|x\u00b5) and hence, the negative log-likelihood is an arithmetic average5 across ob-\n5This decomposition does not imply independence of obser-\nservations\u2212L(r||p) = \u2212 1P \u2211 \u00b5 log p(x\u00b5). From the Bayes identity, we have in terms of the joint density:\n\u2212 logL(r||p) = E(\u2212 log p(x))r(x) = = E(\u2212 log p(x, z))r(x,z) + E(log p(z|x))r(x,z), (2.1)\nor equivalently, using the explicit form of r(x, z), for the \u00b5-th observation alone:\n\u2212 log p(x\u00b5) = D(p(z|x\u00b5)||p(x\u00b5, z)) = = E(\u2212 log p(x\u00b5, z))p(z|x\u00b5) \u2212 S(p(z|x\u00b5)), (2.2)\nwhere S(p(z|x\u00b5)) = E(\u2212 log p(z|x\u00b5))p(z|x\u00b5) is the entropy of the model distribution conditional on a given visible observation x\u00b5. If we sample the latent observables only once per observation, the right-hand side reduces to the familiar \u2212 log p(x\u00b5, z\u00b5) + log p(z\u00b5|x\u00b5)."}, {"heading": "2.2. Conditional independence.", "text": "The hidden/latent observables z = {zj}Nlatj=1 are conditionally independent if, for a given observation x\u00b5, one has p(z|x\u00b5) = \u220fNlat j=1 p(zj |x\u00b5). From the independence bound\nof entropy S(p(z|x\u00b5)) \u2264 \u2211Nlat j=1 S(p(zj |x\u00b5)), (Cover & Thomas, 2006), Chapter 2, conditional independence minimizes the negative entropy term on the right-hand side of (2.2). Everything else being equal, conditional independence is hence optimal for nets."}, {"heading": "2.3. Gibbs and q-Gibbs model densities.", "text": "There is a broad class of probability density families - Gibbs or exponential families - which dominate the choices of model densities, both in physics and neural nets. This class includes a sufficiently large number of density families: Gaussian, Bernoulli, exponential, gamma, etc. Their general closed form is:\np\u03bb(z) = p(z)\nZ e\u2212\n\u2211M s=1 \u03bbsMs(z), (2.3)\nwhere p(z) is an arbitrary prior density, \u03bb = {\u03bbs} are Lagrange multipliers, Mj(z) are so-called sufficient statistics, and Z = \u222b p\u03bb(z)dz is the normalizing partition function. The sufficient statistics in physics form a complete set of state variables like energy, momenta, number of particles, etc, fully describing the \u00b5-th conditional state, sub-section 1.2, see (Landau & Lifshitz, 1980), sections 28,34,35,110. In probability, the sufficient statistics are typically monomials like M1(z) = z, M2(z) = z2, etc, whose expectations form the moments m1, m2, etc, of a given multi-dimensional density. As proposed in subsection 1.5, one can add to the list the symmetry statistics, see subsection 3.3 for details.\nvations: the latent variables can in general contain information from more than one observation, as for example in the case of time series auto-regression.\nThe Gibbs class is special because it is the variational maximum entropy class: it is the unique functional form which maximizes the entropy Sp(f), computed with respect to the reference measure p(), across the universe {f(z)} of all densities with fixed expectations of the sufficient statistics E(Ms(z))f = ms, s = 1, ...,M , see (Cover & Thomas, 2006), chapter 12. The Lagrange multipliers\u03bb= \u03bb(m) are computed so as to satisfy these constraints and are hence functions of the vector m = {ms}Ms=1.\nThe Gibbs class is special in even stronger sense: it is a minimum divergence class. For an arbitrary prior density p(z), the Kullback-Leibler divergence D(p\u03bb(z)||p(z))) minimizes the divergence D(f(z)||p(z)) across the universe {f(z)} of all densities with fixed expectations of the sufficient statistics E(Ms(z))f = ms, s = 1, ...,M . This follows from the variational Pythagorean theorem (Kulhavy\u0300, 1996), section 3.3:\nD(f(z)||p(z)) = D(f(z)||p\u03bb(z)) +D(p\u03bb(z)||p(z)) \u2265 \u2265 D(p\u03bb(z)||p(z)). (2.4)\nIn our context, for a given observation x\u00b5, choosing a wave function from the Gibbs type is equivalent to choosing a conditional model density:\np(z|x\u00b5) = p(z)\nZ e\u2212\n\u2211M s=1 \u03bbs(m(x\u00b5))Ms(z). (2.5)\nWe made explicit the indirect dependence of \u03bbs on x\u00b5 via the constraints vector m = m(x\u00b5). As we will see in sub-section 3.2, minimizing the divergence D(f(z)||p(z)) across an unknown a priori family of conditional distributions p(z|x\u00b5) = f(z), is crucial for the quality of a generative net. The minimum divergence property (2.4) implies that we are always better off choosing p(z|x\u00b5) from a Gibbs class, as in (2.5), hence the name Gibbs machines.\nIn practice, in order for p(z|x\u00b5) to be tractable, p(z) has to be from a specific parametric family within the Gibbs class: then both p(z) and p(z|x\u00b5) will be tractable and in the same family, e.g. Gaussian, exponential, etc.\nExcept for the symmetry statistics, sub-sections 1.5, 3.3, the conditional moments m = m(x\u00b5) for the \u00b5-th quantum state are free parameters. Together with the Lagrange multipliers of the symmetry statistics, they can be thought of as quantum numbers distinguishing the partial equilibrium states from one another, section 1.2. These quantum numbers are added to the rest of the free net parameters, to be optimized by standard methods, like back-propagation/ stochastic gradient descent, etc.\nGibbs densities are only a special case (for q = 1) of the broad class of q-Gibbs (or q-exponential) densities. The corresponding nonextensive statistical mechanics (Tsallis, 2009), describes more adequately long-range-interacting\nmany-body systems like typical human-generated data sets. Many of the properties of the exponential class remain true for the q-exponential class (Amari & Ohara, 2011), see open problem 4 in section 5."}, {"heading": "3. Application to neural nets.", "text": ""}, {"heading": "3.1. Perturbative nets.", "text": "Nets which rely on the decomposition (2.2) for parameter estimation, do not contain the pure marginal model density p(z), and are therefore not fully-generative but only perturbative.\nThe most successful family of perturbative nets to date have been the Boltzmann machines and their multiple reincarnations. Assuming completeness of the state variables, Boltzmann machines adopt as joint model density the Boltzmann density, a special case of the Gibbs equilibrium density from sub-section 2.3: pB(x, z) = 1 Z e \u2212 1T E(x,z), with a single sufficient statistics - a bi-linear energy function E(, ), a trivial prior and temperature T = 1. It is not tractable because the partition function Z = \u222b p(x, z)dxdz can not be computed in closed form. On the other hand, for discrete data, the conditional density of the restricted Boltzmann machines (RBM) is tractable and is the familiar Fermi density pF (z|x\u00b5) = 1/(1+ eE(x\u00b5,z)). The intractable joint density term in (2.2) is handled by approximations of its gradients like contrastive divergence (Hinton, 2002), to avoid brute-force Markov Chain Monte Carlo (MCMC) methods.\nDue to the simple bi-linear shape of the log-likelihood i.e. energy, RBM-s have latent variables conditionally independent on the visible variables (and vice versa). Despite their limitations when handling non-binary data, deep Boltzmann machines (Salakhutdinov & Hinton, 2009) are arguably still the dominant universal nets: They perform well both as classifiers (Srivastava et al., 2014) and probability density estimators (Salakhutdinov & Murray, 2008)."}, {"heading": "3.2. Fully-generative nets. Gibbs machines.", "text": "In order to have a fully-generative net, one needs to choose a marginal model density p(z) and sample directly from it, unencumbered by observations {x\u00b5}. We will hence distinguish two separate regimes of operation:\n- creative: Latent observables z = {zj}Nlatj=1 are sampled from a closed-form model density p(z), unencumbered by observations {x\u00b5}. A closed-form model density p(x|z) is also chosen, which defines a tractable joint density p(x, z) = p(z)p(x|z). Unfortunately, the implied posterior conditional q(z|x) := p(x, z)/ \u222b p(x, z)dz is generally intractable.\n- non-creative: Latent observables are sampled from a\nclosed-form model conditional density p(z|x) with observations {x\u00b5} attached to the net, as is common in training, validation or testing. The same closed-form densities as above are used but of course p(z|x) 6= q(z|x). The empirical densities are the same as in sub-section 2.1.\nExpanding the joint density from (2.1) in the other Bayesian direction, one gets for the negative log-likelihood in non-creative regime:\n\u2212 logL(r||p) = E(\u2212 log p(x|z))r(x,z)\u2212 E(\u2212 log p(z))r(z) + E(log q(z|x))r(x,z), (3.1)\nor, equivalently, using the explicit form of r(x, z), for the \u00b5-th observation:\n\u2212 log p(x\u00b5) = E(\u2212 log p(x\u00b5|z))p(z|x\u00b5)\u2212 E(\u2212 log p(z))p(z|x\u00b5) + E(log q(z|x\u00b5))p(z|x\u00b5). (3.2)\nAdding and subtracting S(p(z|x\u00b5)), this can be re-written as\n\u2212 log p(x\u00b5) = reconstruction error\ufe37 \ufe38\ufe38 \ufe37\n\u2212 logL(p(z|x\u00b5)||p(x\u00b5|z))+ +D(p(z|x\u00b5)||p(z))\ufe38 \ufe37\ufe37 \ufe38\ngenerative error \u2212D(p(z|x\u00b5)||q(z|x\u00b5))\ufe38 \ufe37\ufe37 \ufe38 variational error . (3.3)\nThe reconstruction error measures the negative likelihood of getting x\u00b5 back, after the transformations and randomness inside the net. The generative error is the divergence between the generative densities in the non-creative and creative regimes. Crucially, it can be interpreted as the hypotenuse in the variational Pythagorean theorem (2.4). Minimizing the variational error is hardly ever possible for real-life data sets, because of the intractability of q(z|x\u00b5), see open problem 7 in section 5. Dropping it yields an upper bound for the negative log-likelihood6:\n\u2212 log p(x\u00b5) \u2264 U(\u2212 log p(x\u00b5)) := \u2212 logL(p(z|x\u00b5)||p(x\u00b5|z))\ufe38 \ufe37\ufe37 \ufe38\nreconstruction error\n+D(p(z|x\u00b5)||p(z))\ufe38 \ufe37\ufe37 \ufe38 generative error . (3.4)\nIt is clear from the above derivation that (3.3), (3.4) are universal for fully-generative nets and were first used in the first fully-generative nets, the VAE-s (Kingma & Welling, 2014), (Rezende et al., 2014). The VAE-s owe their name to the variational error term and were introduced in the context of very general sampling densities. We refer to all nets with a generative error as Gibbs machines because, from the variational Pythagorean theorem (2.4), sampling densities (2.5) from the Gibbs class minimize it. While the\n6This is an expanded version of the textbook variational inequality (Cover & Thomas, 2006), Exercise 8.6, hence the name variational error.\nvariational error is due to an approximation, the variational principle from which the Gibbs class is derived, is fundamental to classic statistical mechanics.\nWithout offering a rigorous proof, we believe that the argument from sub-section 2.2 can be generalized to this context: For a given reconstruction error, the generative error in (3.4) is minimized when the latent variables are conditionally independent. For Gaussian multi-variate sampling, this follows from the explicit form of the generative error (Gil et al., 2013), table 3, and Hadamard\u2019s inequality (Cover & Thomas, 2006), chapter 8."}, {"heading": "3.3. Latent symmetry statistics. Momenta.", "text": "We will show for brevity how to build translational, scaling and rotational invariance only in a two-dimensional visual recognition model. Generalizations are straightforward.\nEvery observable i.e. pixel xi , i = 1, ..., N , can be assigned a horizontal hi and a vertical vi integer coordinates on the screen, e.g., hi, vi \u2208 {1, ..., \u221a N}. In these coordinates, a row-observation x\u00b5 = {x\u00b5i}Ni=1 becomes a matrixobservation {x\u00b5,hi,vi} and a net layer of size N becomes a layer of size \u221a N \u00d7 \u221a N . The center of mass (h\u00b5, v\u00b5):\nh\u00b5 := \u2211N i=1 x\u00b5ihi\u2211 i x\u00b5i , v\u00b5 := \u2211N i=1 x\u00b5ivi\u2211 i x\u00b5i . (3.5)\nof every observation defines latent symmetry statistics h = {h\u00b5}P\u00b5=1 and v = {v\u00b5}P\u00b5=1, see sub-sections 1.5, 2.3. Without loss of generality, we assumed here that x\u00b5i \u2265 0, hence 0 \u2264 h\u00b5, v\u00b5 \u2264 \u221a N . In the coordinate system centered by (h\u00b5, v\u00b5), we have for every \u00b5 the new coordinates:\n(h\u0302\u00b5i, v\u0302\u00b5i) := (hi \u2212 h\u00b5, vi \u2212 v\u00b5), (3.6)\n\u2212 \u221a N \u2264 h\u0302\u00b5i, v\u0302\u00b5i \u2264 \u221a N . In this coordinate system, every pixel has polar coordinates r\u00b5i := \u221a h\u03022\u00b5i + v\u0302 2 \u00b5i, \u03d5\u00b5i\n:= atan2(v\u0302\u00b5i, h\u0302\u00b5i) \u2208 (\u2212\u03c0, \u03c0], hence the new symmetry statistics: scale r = {r\u00b5}P\u00b5=1 and angle \u03d5 = {\u03d5\u00b5}P\u00b5=1:\nr\u00b5 :=\n\u2211N i=1 x\u00b5ir\u00b5i\u2211\ni x\u00b5i , \u03d5\u00b5 :=\n\u2211N i=1 x\u00b5i\u03d5\u00b5i\u2211\ni x\u00b5i , (3.7)\n0 < r\u00b5 \u2264 \u221a 2N , \u2212\u03c0 < \u03d5\u00b5 \u2264 \u03c0. In order to un-scale and un-rotate an image, change coordinates one more time to:\n(h\u0303\u00b5i, v\u0303\u00b5i) := \u221a N ( h\u0302\u00b5i r\u00b5 cos(\u2212\u03d5\u00b5), v\u0302\u00b5i r\u00b5 sin(\u2212\u03d5\u00b5) ) ,\n(3.8)\n\u2212M \u2264 h\u0303\u00b5i, v\u0303\u00b5i \u2264 M , for some constant M > \u221a N/2 depending on the smallest7 scale min \u00b5 r\u00b5. After rounding,\n7The scale r\u00b5 needs to have a lower bound, in order to ensure that M is of the same order of magnitude as \u221a N .\nwe thus have for any observation \u00b5, an order-preserving injective mapping of indexes:\n{1, ..., N} \u2192 {\u2212M, ...,M} \u2297 {\u2212M, ...,M} i\u2192 (h\u0303\u00b5i, v\u0303\u00b5i). (3.9)\nThe (2M+1\u2212 \u221a N)2 indexes which are not in the mapping image correspond to observables which are identically zero for that observation. In the coordinates (3.8), a net layer of size N becomes a layer of size (2M + 1)\u00d7 (2M + 1).\nIn summary: i) for auto-encoders, apply (3.9) at the input and its inverse at the output of the net; ii) for classifiers, apply (3.9) at the input only; iii) for fully-generative nets, in addition, include the symmetry statistics h,v, r,\u03d5 in the latent layer. Their prior model density p() can be assumed equal to their respective parametrized posterior p(|x\u00b5), hence they are dropped from the generative error in (3.4).\nWhen sampling the symmetry statistics from independent Laplacians as in (1.2) e.g., the respective density means are set to be h\u00b5, v\u00b5, r\u00b5, \u03d5\u00b5 from (3.5), (3.7). The density scales \u03c3h\u00b5, \u03c3 v \u00b5, \u03c3 r \u00b5, \u03c3 \u03d5 \u00b5 on the other hand are free parameters, and can in principle be optimized in the non-creative regime, alongside the rest of the net parameters, sub-section 2.3. As argued in sub-section 1.5, the inverted scales are the scaled momenta. In the creative regime, when sampling e.g. from h alone, one will get horizontally shifted identical replicas. See open problem 2, section 5."}, {"heading": "4. Experimental results.", "text": ""}, {"heading": "4.1. Non-generative ACE.", "text": "The motivation for the non-generative ACE comes from the Einstein observation entropies {\u2212 log pG(z\u00b5)}\u00b5 in subsection 1.4 and their relation to singular value decomposition (SVD). Recall that the SVD of the B \u00d7 N data matrix X with B observations and N observables is X = V\u039bWT . The B \u00d7 B matrix VVT is i) a projection mapping; ii) its diagonals are up to a constant the negative Gaussian log-likelihoods {\u2212 log pG(z\u00b5)}\u00b5; and iii) it is invariant on X, i.e. X = VVTX.\nLets us consider a shallow auto-encoder, Figure 6, left, and its dual in the space of observations, Figure 6, right. It can be shown that for tied weights V(2) = V(1)T , in the absence of non-linearities, the optimal B\u00d7Nlat hidden layer solution Ho on the left is Ho = V(2) = V (Georgiev, 2015b). The divergence of VVTX from X is thus the reconstruction error in the dual space and minimizing it gets us closer to the optimal Ho. If we treat the first hidden layer H of a classifier as the rescaled dual weight matrix\nTowards universal neural nets: Gibbs machines and ACE. \u221a BV(2), we arrive at the dual reconstruction error :\n\u2212 logL\u2032recon = 1\nB N\u2211 i=1 E(\u2212 log\u03d5(HHTxi/B))xi , (4.1)\nfor a given column-vector observable xi = {x\u00b5i}B\u00b5=1 and sigmoid non-linearity \u03d5(), see Appendix A.\nThe non-generative ACE has as minimization target the composite negative log-likelihood (1.1), with \u2212 logLAE replaced by the dual reconstruction error (4.1). The orthogonality VTV = INlat of V implies the need for an additional batch normalization, similar to (Ioffe & Szegedy, 2015), see Appendix A.\nThe best known results for the test classification error of feed-forward, non-convolutional nets, without artificial data augmentation, are in the 0.9-1% handle (Srivastava et al., 2014), table 2. As shown on the right of Figure 7, the non-generative ACE offers a 20-30% improvement."}, {"heading": "4.2. Generative ACE.", "text": "The architecture is in Figure 4, the minimization target is the ACE negative log-likelihood (1.1), with \u2212 logLAE replaced by the upper bound (3.4). Laplacian sampling density is used in training and the mixed Laplacian in testing, with the explicit formulas for the generative error in Appendix B. The generative ACE produces similarly outstanding classification results as the non-generative ACE on the regular MNIST data set, Figure 8, left. Even without tweaking hyper-parameters, it also produces outstanding results for the density estimation of the binarized MNIST data set, Figure 8, right. An upper bound for the negative log-likelihood in the 86-87 handle is in the ballpark of the best non-recurrent nets, (Gregor et al., 2015), table 2."}, {"heading": "5. Open problems.", "text": ""}, {"heading": "Acknowledgments", "text": "We appreciate motivating discussions with Ivaylo Popov and Nikola Toshev. Credit goes to Christine Haas for coining the terms intricates and creative/non-creative regimes.\nAppendices"}, {"heading": "A. Software and implementation.", "text": "ACE code used for cited experiments is in (Georgiev, 2015a). All cited nets are implemented on a single GeForce GTX 970 GPU and the Theano platform (Bastien et al., 2012), tapping into Theano-Lights standard net implementations (Popov, 2015). Optimizer is ADAM (Kingma & Ba, 2014) stochastic gradient descent back-propagation. Specific hyper-parameters are in the text. We used only two standard sets of hyper-parameters, one for the classifier branch and one for the auto-encoder branch and have not done hyper-parameter optimizations.\nFor reconstruction error of a binarized \u00b5-th rowobservation x\u00b5 = {x\u00b5i}Ni=1 and its reconstruction x\u0302\u00b5, we use the standard binary cross-entropy (Bastien et al., 2012): E(\u2212 log x\u0302\u00b5)x\u00b5 = = \u2211 i(\u2212x\u00b5i log x\u0302\u00b5i \u2212(1 \u2212 x\u00b5i)(1 \u2212 log x\u0302\u00b5i)), with x\u0302\u00b5 the image of a sigmoid non-linearity \u03d5. The batch negative log-likelihood is \u2212 logLrecon = 1 B \u2211B \u00b5=1 E(\u2212 log x\u0302\u00b5)x\u00b5 . In the space of observations, as on right plot of Figure 6, the dual reconstruction error is the same binary cross-entropy E(\u2212 log x\u0302i)xi , but for the ith observable xi = {x\u00b5i}B\u00b5=1 and a sum over \u00b5 instead of i. The batch negative log-likelihood is \u2212 logL\u2032recon = 1B \u2211N i=1 E(\u2212 log x\u0302i)xi , with a normalization factor conforming to the space of observables.\nThe non-linearities are tanh() in the auto-encoder branch and two-unit maxout (Goodfellow et al., 2013) in the classifier branch. Weight matrices of size P \u00d7 N are initialized as random Gaussian matrices, normalized by the order of magnitude of their largest eigen-value \u221a P + \u221a N . As discussed in sub-section 4.1, hidden observables in the first and last hidden layer of classifiers are batch-normalized i.e. de-meaned and divided by their second moment. Unlike (Ioffe & Szegedy, 2015), batch normalization is enforced identically in both the train and test set, hence test results depend slightly on the test batch."}, {"heading": "B. Generative error formulas.", "text": "When sampling from a Laplacian, in order to have an unity variance in the prior, we choose for the independent onedimensional latents a prior p(z) = pLap(z; 0, \u221a 0.5), where pLap(z;\u00b5, b) = exp(\u2212|z \u2212 \u00b5|/b)/(2b) is the standard Laplacian density with mean \u00b5 and scale b. In order to have zero generative error when (\u00b5, \u03c3)\u2192 (0, 1), we parametrize the conditional posterior as p(z|.) = pLap(z;\u00b5, \u03c3 \u221a 0.5). The generative error in (3.4) equals: \u2212 log \u03c3 +|\u00b5|/ \u221a 0.5\n+\u03c3exp ( \u2212|\u00b5|/(\u03c3 \u221a 0.5) ) \u22121, see (Gil et al., 2013), table 3.\nFor the divergence between a mixture prior \u2211 s \u03b1sps(z)\nand a mixture posterior \u2211 s \u03b1sps(z|.) with the same\nweights {\u03b1s}s, we use the upper bound \u2211 s \u03b1s D (ps(z|.)||ps(z)) implied by the log sum inequality (Cover & Thomas, 2006). For improvements, see open problem 5 in section 5."}], "references": [{"title": "Methods of Information Geometry, volume 191 of Translations of Mathematical monographs", "author": ["S. Amari", "H. Nagaoka"], "venue": null, "citeRegEx": "Amari and Nagaoka,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka", "year": 2000}, {"title": "Geometry of qexponential family of probability distributions. Entropy", "author": ["Amari", "Shun-ichi", "Ohara", "Atsumi"], "venue": null, "citeRegEx": "Amari et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Amari et al\\.", "year": 2011}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "On Boltzmanns principle and some immediate consequences thereof", "author": ["Einstein", "Albert"], "venue": "Einstein, 19052005,", "citeRegEx": "Einstein and Albert.,? \\Q2005\\E", "shortCiteRegEx": "Einstein and Albert.", "year": 2005}, {"title": "ACE, 2015a. URL https://github. com/galinngeorgiev/ACE", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "Duality in neural nets., 2015b", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "R\u00e9nyi divergence measures for commonly used univariate continuous distributions", "author": ["M Gil", "F Alajaji", "T. Linder"], "venue": "Information Sciences,", "citeRegEx": "Gil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gil et al\\.", "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Approximating the Kullback Leibler divergence between Gaussian mixture models", "author": ["J.R. Hershey", "P.A. Olsen"], "venue": "In ICASSP,", "citeRegEx": "Hershey and Olsen,? \\Q2007\\E", "shortCiteRegEx": "Hershey and Olsen", "year": 2007}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Recognizing handwritten digits using mixtures of linear models", "author": ["Hinton", "Geoffrey E", "Revow", "Michael", "Dayan", "Peter"], "venue": "In AINIPS,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Natural Image Statistics: A probabilistic approach to early computational vision", "author": ["Hyvarinen", "Aapo", "Hurri", "Jamo", "Hoyer", "Patrick O"], "venue": null, "citeRegEx": "Hyvarinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyvarinen et al\\.", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "URL http://arxiv.org/", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "URL http://arXiv. org/abs/1412.6980", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Recursive Nonlinear Estimation: A Geometric Approach. Lecture Notes in Control And Information Sciences", "author": ["R. Kulhav\u1ef3"], "venue": null, "citeRegEx": "Kulhav\u1ef3,? \\Q1996\\E", "shortCiteRegEx": "Kulhav\u1ef3", "year": 1996}, {"title": "Quantim Mechanics, Nonrelativistic theory, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": null, "citeRegEx": "Landau and Lifshitz,? \\Q1977\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1977}, {"title": "Statistical Physics, Part 1, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": "Elsevier Science,", "citeRegEx": "Landau and Lifshitz,? \\Q1980\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1980}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "URL https:// github.com/Ivaylo-Popov", "author": ["Popov", "Ivaylo"], "venue": "Theano-Lights,", "citeRegEx": "Popov and Ivaylo.,? \\Q2015\\E", "shortCiteRegEx": "Popov and Ivaylo.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In JMLR,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Deep Boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World ", "author": ["Tsallis", "Constantino"], "venue": null, "citeRegEx": "Tsallis and Constantino.,? \\Q2009\\E", "shortCiteRegEx": "Tsallis and Constantino.", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "They were inspired by the first fully-generative nets - the variational auto-encoders (VAE) (Kingma & Welling, 2014), (Rezende et al., 2014) - and employ the same upper bound (3.", "startOffset": 118, "endOffset": 140}, {"referenceID": 24, "context": "Unfortunately, some of the key features of modern neural nets, like non-linear activation functions and dropout (Srivastava et al., 2014), come at the high price of Gaussianizing the data set.", "startOffset": 112, "endOffset": 137}, {"referenceID": 11, "context": "It is precisely the intricates, which - because of their extreme non-Gaussianity, see Figure 2 - are ideal candidates for \u201cfeature vectors\u201d in classification tasks (Hyvarinen et al., 2009), section 7.", "startOffset": 164, "endOffset": 188}, {"referenceID": 18, "context": "If the latent observations {z\u03bc}\u03bc=1 come from an Nlatdimensional Gaussian distribution, the density of these negative Gaussian log-likelihoods is proportional to the familiar F (Nlat, P \u2212 Nlat) density (Mardia et al., 1979), sections 1.", "startOffset": 201, "endOffset": 222}, {"referenceID": 2, "context": "have the so-called overcomplete representation (Coates et al., 2011).", "startOffset": 47, "endOffset": 68}, {"referenceID": 10, "context": "ACE generalizes the classic idea of using separate decoders for separate classes, (Hinton et al., 1995).", "startOffset": 82, "endOffset": 103}, {"referenceID": 21, "context": "This so-called manifold learning by modern feed-forward nets was pioneered by the contractive autoencoders (CAE) (Rifai et al., 2012).", "startOffset": 113, "endOffset": 133}, {"referenceID": 15, "context": "The optimization target hence is the mixed empirical density r(x, z) = 1 P \u2211 \u03bc \u03b4(x \u2212x\u03bc)p(z|x\u03bc) (Kulhav\u1ef3, 1996), section 2.", "startOffset": 95, "endOffset": 110}, {"referenceID": 15, "context": "This follows from the variational Pythagorean theorem (Kulhav\u1ef3, 1996), section 3.", "startOffset": 54, "endOffset": 69}, {"referenceID": 24, "context": "Despite their limitations when handling non-binary data, deep Boltzmann machines (Salakhutdinov & Hinton, 2009) are arguably still the dominant universal nets: They perform well both as classifiers (Srivastava et al., 2014) and probability density estimators (Salakhutdinov & Murray, 2008).", "startOffset": 198, "endOffset": 223}, {"referenceID": 20, "context": "4) are universal for fully-generative nets and were first used in the first fully-generative nets, the VAE-s (Kingma & Welling, 2014), (Rezende et al., 2014).", "startOffset": 135, "endOffset": 157}, {"referenceID": 6, "context": "For Gaussian multi-variate sampling, this follows from the explicit form of the generative error (Gil et al., 2013), table 3, and Hadamard\u2019s inequality (Cover & Thomas, 2006), chapter 8.", "startOffset": 97, "endOffset": 115}, {"referenceID": 24, "context": "9-1% handle (Srivastava et al., 2014), table 2.", "startOffset": 12, "endOffset": 37}, {"referenceID": 7, "context": "An upper bound for the negative log-likelihood in the 86-87 handle is in the ballpark of the best non-recurrent nets, (Gregor et al., 2015), table 2.", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "4, directly as feature detectors, in lieu of artificially computed Independent Component Analysis (ICA) features (Hyvarinen et al., 2009).", "startOffset": 113, "endOffset": 137}, {"referenceID": 6, "context": "5) ) \u22121, see (Gil et al., 2013), table 3.", "startOffset": 13, "endOffset": 31}], "year": 2017, "abstractText": "We study a class of neural nets Gibbs machines which are a type of variational auto-encoders, designed for gradual learning. They offer an universal platform for incrementally adding newly learned features, including physical symmetries in space/time. Combining them with classifiers gives rise to a brand of universal generative neural nets stochastic auto-classifier-encoders (ACE). ACE preserve the non-Gaussian and clustering nature of real-life data and have state-ofthe-art performance, both for classification and density estimation for the MNIST data set.", "creator": "LaTeX with hyperref package"}}}