{"id": "1509.00498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2015", "title": "Sensor-Type Classification in Buildings", "abstract": "many sensors / meters are deployed in commercial buildings to confidently monitor and optimize their performance. however, because sensor metadata is inconsistent across buildings, common software - based automated solutions are tightly coupled to the sensor xml metadata conventions ( i. e. schemas definition and naming ) for each service building. running the same software across buildings requires significant integration effort.", "histories": [["v1", "Tue, 1 Sep 2015 20:46:19 GMT  (1488kb)", "http://arxiv.org/abs/1509.00498v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dezhi hong", "jorge ortiz", "arka bhattacharya", "kamin whitehouse"], "accepted": false, "id": "1509.00498"}, "pdf": {"name": "1509.00498.pdf", "metadata": {"source": "CRF", "title": "Sensor-Type Classification in Buildings", "authors": ["Dezhi Hong", "Jorge Ortiz", "Arka Bhattacharya", "Kamin Whitehouse"], "emails": ["whitehouse}@virginia.edu,", "jjortiz@us.ibm.com,", "arka@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n00 49\n8v 1\n[ cs\n.L G\n] 1\nS ep\n2 01\nMetadata normalization is critical for scaling the deployment process and allows us to decouple building-specific conventions from the code written for building applications. It also allows us to deal with missing metadata. One important aspect of normalization is to differentiate sensors by the type of phenomena being observed. In this paper, we propose a general, simple, yet effective classification scheme to differentiate sensors in buildings by type. We perform ensemble learning on data collected from over 2000 sensor streams in two buildings. Our approach is able to achieve more than 92% accuracy for classification within buildings and more than 82% accuracy for across buildings. We also introduce a method for identifying potential misclassified streams. This is important because it allows us to identify opportunities to attain more input from experts \u2013 input that could help improve classification accuracy when ground truth is unavailable. We show that by adjusting a threshold value we are able to identify at least 30% of the misclassified instances.\nCategories and Subject Descriptors C.3 [Special-Purpose And Application-Based Systems]: Real-time and embedded systems\nGeneral Terms Performance, Experimentation, Verification\nKeywords Sensor Type, Random Forest, Classification"}, {"heading": "1. INTRODUCTION", "text": "Commercial buildings are sites of large sensor/meter deployments used to monitor and optimize their performance. With the recent interest in reducing building energy consumption and increasing their efficiency, it is important to consider ways to quickly bootstrap a set of building data streams into an analytical pipeline, such as overall building efficiency or comfort-assessment analytics and control. However, because sensor metadata is inconsistent across buildings, software-based solutions are tightly coupled to the sensor metadata conventions (i.e. schemas and naming) for each building. Running the same software across buildings requires significant integration effort.\nCurrent \u2018point\u2019 naming conventions and unsystematic recording of metadata form a bottleneck in deployment scalability for analytics jobs. A \u2018point\u2019 refers to a physical location where a sensor is taking measurements. Each building vendor uses their own naming scheme and unique variants of each scheme are implemented from building to building; variations exist even across buildings that have contracted the same vendor. In addition, expanded descriptive information about the point is sometimes unavailable \u2013 so determining their meaning is painfully slow or impossible. Because these are conventions carried out by humans, they are inconsistent within and across building data sets. This makes the integration process laborious for building experts and a non-starter for non experts. The process is fundamentally unscalable.\nConsider a simple analysis program, which has the ability to identify anomalous readings from a specific kind of sensor. To execute this job, the process organizes each sensor by type and location, generates the distribution of readings across them, and identifies broken sensors where some fraction of their readings are above some threshold value on the distribution. The identification step in the process is the most challenging because of the problems described. Ideally, the program would search for points the way you search for web pages in a search engine \u2013 using semantically meaningful terminology.\nPoint names contain set of codes that are semantically meaningful to the building manager of a specific building. For example, the point BLDA1R435__ART is constructed as a concatenation of such codes. The name of the building (first 4 characters), the air handling unit identifier (the fifth character), the room number (R435), and the type ART (area\nroom temperature) \u2013 which indicates that this are measurement is produced by a temperature sensor. In addition to point names, there may be some descriptive metadata. The description for this point (if it exists) could describe that this is a \u201ctemperature sensor in room 435\u201d. However, since point names do not follow the exact same structure within and across buildings (and certainly do not follow the same convention across vendors) no single approach could solve the normalization problem. A suite of approaches is necessary.\nMetadata normalization is critical for scaling the software deployment process. It allows us to decouple building-specific conventions from the code written for building applications. Normalization allows us to boost existing metadata, correct incorrect metadata, or generate common metadata when it is missing altogether. One such component in the normalization suite should differentiate sensor feeds by type. For example, we should be able to differentiate between sensors measuring temperature from sensors measuring pressure. In addition, we should be able to use what we learn from one building and apply it to another. This is especially useful in cases where similar stream types are labeled differently, labeled incorrectly, or not labeled at all.\nNormalization would allow us to quickly run jobs across many sites by enabling wide searchability of points across many buildings at once. In order to meaningfully deal with disparate building streams in a scalable fashion the streams should be searchable across various properties, such as building name, room location, and type. Moreover, we assert that wide searchability is necessary for achieving scalability. By providing a tool for searching across building streams, we minimize the deployment time for applications; allowing them to be used in all buildings, not just a single one.\nOne of the important aspect of the sensor meta/data that we can leverage are the actual patterns in the readings themselves. Deep inspection of features in the data can yield meaningful results about the type of data that it is and can help us with the label normalization problem. This paper examines this path using standard machine learning approaches. We observe that statistical features over small time windows can be used to identify the stream type. Moreover, we show that the classification of stream-type can be achieved using an ensemble of classifiers which is known to outperform a single classifier.\nWe conduct a comprehensive study on the data collected from over 2000 sensors in two separate buildings on two campuses. Our main contributions are:\n\u2022 We propose a simple, general yet effective feature extraction scheme to achieve sensor type classification in the context of commercial buildings.\n\u2022 We formulate an approach to identifying potential misclassified sensor streams (in terms of the type classes) when no ground truth labels are available.\n\u2022 We evaluate our classification technique using data from over 2000 sensor series of 6 types in two buildings on two campuses, and our technique is able to achieve\naround 92% and 98% accuracy when doing classification within each building, and around 82% accuracy when inferring type information across buildings.\n\u2022 We also evaluate our solution to misclassification identification and the results demonstrate that we are able to identify at least 30% percent of the target population by choosing an optimal threshold for decision.\nWe believe this is an important study given the recent trends in the penetration of the internet of things into our homes and environments. Studies show that normalization is an especially pernicious and widely ubiquitous problem in embedded systems, with only 7% of data tagged and only 1% analyzed [15]. Our technique can be used to unify that data across many deployment and enable broad search and exploration of new applications. For example, sensing device names for the internet of things are likely to follow similar conventions with very little context. We argue that unification through boosting will be necessary in this broader domain."}, {"heading": "2. METHODOLOGY", "text": "In this section, we describe the design and construction of the feature-vector we use to characterize sensor type. We explain what it captures, fundamentally and, hence, why it works so well for building sensor data. Then, we discuss the classification technique we apply and give a detailed description of the training and testing process. Finally, we articulate a solution for identifying potentially misclassified streams, when no type-label ground truth is available."}, {"heading": "2.1 Feature Extraction", "text": "Raw sensor time series1 usually contain millions of readings which are too general to be useful for type classification. We need to distill the information embedded in the reading patterns. A signal in the time domain trends the amplitude of a sensor reading and different types of sensor generally occupy distinct amplitude bins, as demonstrated in Figure 1. We can characterize the amplitude distribution of a signal in the time domain by using the percentiles of the value distribution.To identify outliers in the distribution, we pick the 50th percentile value (also known as the median) as a discriminator, which is more robust to outliers skew than the average.\nNaturally, sensor reading value-ranges may overlap. For example, during a rainy season, the humidity in an office can reach the range of 60-70 (percent) which is the same as typical temperature sensor readings (Fahrenheit). If you do not consider measurement units, the distributions for each of the two types look similar. Simply relying on percentiles is not sufficient for differentiating sensor types. Figure 2 demonstrates this point. To capture their difference we need to include the variance of the signal in our feature-vector.\nWhen we extract features from a raw sensor readings, the original trace can span hours, days, or weeks, and the trend can vary significantly, even from hour to hour. Extracting certain features, such as percentiles, and variances over\n1In this paper, we use the term\u201ctrace\u201d, \u201creadings\u201d and\u201ctime series\u201d interchangeably.\nthe entire sensor trace might miss short term dynamics thus missing discriminating characteristics. In contrast, computing features over short time windows can produce too much noise. Too many feature variables typically degrades classifier performance. To succinctly summarize the dynamics of sensor traces, we apply feature extraction to every time window of fixed length on the original trace and compute the statistics of the accumulated features from windowed slices as the final feature set.\nUpon close inspection of the traces, we notice that the shortterm dynamics of the phenomena being measured, could be used to differentiate them. Therefore, the distribution of short-term summary statistics can be used discriminate traces by type. We construct our feature vector as follows: first, each single sensor signal is segmented into N non-overlapping 45-minute long windows (we will discuss the decision of window length in later section). Second, within each time window, we compute the median and variance of the signal, producing a vector of medians and a vector of variances after the window slides over an entire trace:\nMED = {median1, median2, ..., medianN}\nV AR = {variance1, variance2, ..., varianceN}\nWhere N is the number of time windows. The vector MED and V AR reflect short term changes but not all the intermediate values are useful for classification. Finally, we compute\na statistical summary of the two vectors. For each vector we compute the minimum, maximum, median and variance, resulting in a feature vector with eight variables:\nF = {min(MED), max(MED),median(MED), var(MED),\nmin(V AR),max(V AR),median(V AR), var(V AR)}\nAnd F is the feature vector for each sensor trace used in our classification process."}, {"heading": "2.2 Classification", "text": "In general, ensemble learning methods obtain better predictive performance than any of the constituent learning methods as discussed in [19, 22], if the following assumptions hold [8]: 1) the probability of a correct classification by each individual classifier is greater than 0.5 and 2) the errors in predictions of the base-level classifiers are independent. Random forests [4] have been widely used and outperform a single tree classifier. They are also faster [29] in training and testing compared to traditional classifiers such as SVM. The notion of randomized trees was first introduced in [2] and further well developed in [4].\nRandom forests construct a multiple of classification trees. To classify an unlabeled object, we construct the feature vector and \u201cfeed\u201d the vector down each of the trees in the forest. Each tree gives a classification and we use it as a \u201cvote\u201d for that class. The forest chooses the class having the most votes over all the trees in the forest. The process proceeds as follows:\n1. Sample N instances at random with replacement2, from the original data set. These samples will be the training set for growing this particular tree.\n2. Specify M feature variables at random out of the total feature vector when growing each node of a tree. And the best split (measured by the information gain) on\n2An element may appear multiple times in the sample set.\nthese M is used to split the node. The value of M is constant during the forest growing.\n3. Each tree is grown to the largest extent possible without pruning.\nThe randomness of this ensemble learning method occurs in the first two steps. We set N equal the number of instances in the original training set, M equal the square root of the number of original feature variables, and the number of the trees in the forest be 50. Usually these parameters are optimized through cross-validation and we refer interested readers to [4] for further deduction and proof of random forest.\nAll the instances in our data set are labeled with ground truth class. To train a random forest, we split the original set into two subsets, one for building a forest and one for testing the accuracy of the classifier. After the forest is built from training set, we learn the posterior probabilities of each class c at each leaf node l for each tree t: suppose that T is the set of all trees, C is the set of all classes and L is the set of all leaves for a given tree t \u2208 T . In the training stage the posterior probabilities Pt,l(Y (i) = c)) for each class c \u2208 C at each leaf l \u2208 L, are learned for each tree t \u2208 T . These probabilities are calculated as the ratio of the number of instance i of class c that reach l to the total number of instances that reach l. Y (i) is the class label for instance i. To classify an instance in the testing stage, the feature vector of an instance is passed down each tree until reaching a leaf node, which gives a probability distribution. All the posterior probabilities accumulated from each tree are then averaged and the argmax is taken as the class of the instance. Note that instead of letting each tree vote for on class as described in the original paper [4], we combine the results from classifiers for an instance by averaging their probabilistic predictions, in order to facilitate our technique used to help identify potential misclassified instances as described in the following section."}, {"heading": "2.3 Quantify Classification Uncertainty", "text": "Being able to measure the confidence of prediction results and to identify potential misclassifications, is vital to a learning process. It is trivial to identify misclassification when ground truth is available, but in many real-world cases ground truth is unavailable. Quantifying classification uncertainty can help identify potential misclassified instances and presents an opportunity to solicit the user for feedback that we can use to improve our results. To quantify the uncertainty of classifications in our learning process, we use the posterior probabilities learned in the random forest.\nWith the learned posterior probabilities for each class, at each leaf in each tree, we can compute the average probabilities for each class as follows:\nP\u0304 (Y (i) = c) =\n\u2211 t Pt,l(Y (i) = c)\n|T \u2032 | , t \u2208 T\n\u2032\nWhere T \u2032\nis the collection of trees in the forest where Pt,l(Y (i) = c) 6= 0, and | \u00b7 | denotes the cardinality of a set. Given these averaged probabilities for each class, the forest produces a vector of class probabilities for each new instance as:\nPr = {P\u0304 (Y (i) = c)}, c \u2208 C\nSuppose we have a probability vector Pr1 = {0.9, 0, 0, 0, 0, 0.1} for instance i1 and another vector Pr2 = {0.3, 0.25, 0.1, 0, 0.15, 0.2} for instance i2. Both i1 and i2 will be assigned to the same class according to the class probability distribution, but the assignment of i2 is less confident compared to that of i1 because its predicted class probability has a less\u201cconcentrated\u201d distribution. To measure the degree of \u201cuncertainty\u201d in classification of one instance, we compute the entropy of its class probability yielded by the forest. We rank the classification results and filter out the instances for further manual inspection whose entropy are above a threshold. Inspection can help eliminate misclassifications."}, {"heading": "3. EVALUATION", "text": "To demonstrate the effectiveness of our methodology, we evaluate our classification technique in two different scenarios: a) intra-building, that is, the training and testing data for classification is taken from the same building, and b) inter-building, where the training and testing instances are from two distinct buildings. We also discuss how the amount of training instances and the window size of segment affect the performance of classification. At last, we analyze the results of our solution for identifying potential misclassifications."}, {"heading": "3.1 Taxonomy", "text": "Most of the sensing points in the building can be classified into 6 general types, which we use in our work. In this paper, we consider 6 types of sensors: CO2, humidity, room temperature, setpoint, air flow volume, other temperature. Room temperature includes only sensors that measure the air temperature of rooms (as \u201croom temp\u201d in Table 1) and other temperature (as \u201cother temp\u201d in Table 1) incorporates all other temperature measurements involved in an air ventilation system (illustrated in Figure3 3) such as supply air/return air/mixed air temperature and chilled or hot supply water/return water temperature. For set points, we assign only one general type which includes all set points for every actuator configured in the building."}, {"heading": "3.2 Experimental Setup", "text": "We collected a week\u2019s worth of data from two separate buildings on two campuses. One is from the Rice Hall at the 3Included with permission from the authors of [3].\nUniversity of Virginia, where the sense points report to a database [25] anywhere between every 10 seconds to every 10 minutes. The other building is the Sutardja Dai Hall (SDH) at UC Berkeley, where the deployed sensors [12, 1] transmit to an archiver [5] periodically from anywhere between every 5 seconds to every 10 minutes. The number for each type of sensor in each building is summarized in Table 1 and the type ground truth for each sense point is manually expanded based on the metadata in the database.\nAll of our learning and classification processes are implemented based on the scikit-learn [23] library, which is an open-source machine learning package implemented mostly in Python providing a rich set of APIs."}, {"heading": "3.3 Baseline and Metrics", "text": "As a baseline to compare our proposed approach against, we adopt a simple feature extraction scheme for each trace F = {med, var}, where med and var is simply the median and variance computed over the entire trace.\nFor classification, we measure the averaged cross-validation accuracy in two different scenarios (intra- and inter- buildings). In the intra-building case, the data from a single building is split into training and testing sets, where the results illustrate how accurately the type information can be inferred using local within-building information. For interbuilding case, the experiment performs training and testing across buildings, i.e, train the classifier on the data from building A and test it on building B. This set of experiments tests how well we can apply the classification boundaries from one building and apply it to another.\nFor identifying potential misclassifications, we choose the true-positive rate (TPR, also known as recall), false-positive rate (FPR, also known as fall-out) and positive predictive value (PPV, also known as precision) as metrics to evaluate the performance of our entropy-based approach when making different choice of threshold. Particularly, under our misclassification identification context, a true-positive (TP) is when an instance considered to be misclassified is really misclassified while a false-positive (FP) is when an instance considered to be misclassified is instead a correct classification."}, {"heading": "3.4 Classification Accuracy", "text": "We run the two sets of experiments described above, i.e, the intra- and inter- building tests, to examine the effectiveness of feature design and measure how well the classifier performs. The classification results are summarized from\nTable 2-5. In the table, each row is specific to a type and\neach column is the percentage of the full data set that was used for training. Each cell shows two values. The value without parentheses is the average classification accuracy for the richer feature-vector. The value in parentheses is the average classification accuracy for the approach described in Section 3.3. These are compared throughout the table. The last column summarizes \u201cleave-one-out\u201dcross-validation4 results for each approach.\n3.4.1 Intra Building Performance From the last column in Table 2 and 3, we see that type classification in a single building achieves accuracy of \u223c92% and \u223c98% on Rice Hall and SDH respectively, for leave-oneout (LOO) cross validation. The accuracy for the baseline is also shown in the table (in parentheses). The only type we have difficulty differentiating is \u201cother temp\u201d, which includes temperature measurements for air and water in the ventilation system, and particularly, the return air temperature measurements (as illustrated in Figure 3) are almost identical to the ones measuring air temperature in rooms because what the return duct exhausts is mostly the air from a room.\n3.4.2 Inter Building Performance This set of experiments illustrates how accurately we can learn the type information of one building based on the knowledge from another building. The overall classification accuracy achieved for the two buildings by training on the entire data set (train on SDH for Rice and train on Rice for SDH) is around 82%, as seen from the last columns in Table 4 and 5. Particularly, we see that the accuracy for \u201cother temperature\u201d in Rice is abnormal compared to the rest of the results. The issue with classifying \u201cother temperature\u201d in Rice is that there are many sensing points measuring the temperature of supply and return cold/hot water utilized in the ventilation system, which are absent in the Berkeley building as a training set. Therefore the feature of these traces cannot be learned from SDH and causes problems in classifying these traces."}, {"heading": "3.5 Learning Bootstrapping", "text": "We also experiment with different amount of training instances to examine how that affects the classification accuracy, which gives some insight on how many instances are needed to bootstrap the classification process. In Table 2- 5, the last columns demonstrate how accurately we can do classification on average. There also remains the question of how many number of instances we need to bootstrap the learning process in both of the intra- and inter- building cases. To examine the impact of number of instances on classification accuracy, we use different percentage of the original data set as training set, i.e, 5%, 10%, 20%, 33%, 50%, and the results are presented in the first five columns in each table. For each percentage of training instances used, we apply stratified sampling5 on the original set and the remaining instances are used as testing set. We repeat the same percentage 1/percentage times to reduce random errors and get\n4In LOO cross validation, each training set takes all the instances except one with the test set being the sample held out. 5The sampled set contains the same percentage of samples of each class as the original complete set.\nEach table shows the averaged classification accuracy of experiments where different percentage of the complete set is used as training set (denoted as \u2018X%\u2019). In the percentage analysis, each percentage is repeated 1/percentage times and the averaged accuracy is presented. LOO cross validation accuracy is also shown for the intra-building test case. On average, our solution outperforms the baseline approach (shown in parentheses).\nan averaged accuracy for that percentage. We can clearly see a trend that more training instances yield better classification results in all cases. However, we can also notice that after the training set includes about 20% of the complete set (which is \u223c120 instances and \u223c260 instances for Rice and SDH respectively) the accuracy doesn\u2019t increase too much even reaching 100% of the complete set. This indicates that we don\u2019t need too many instances to bootstrap the learning process within or across buildings to accomplish sensor type classification tasks."}, {"heading": "3.6 Window Size Sensitivity", "text": "All the classification results, thus far, were obtained using features extracted in 45-minute window slices on the original sensor traces. We study how different windows sizes affect the classification performance. Figure 4 shows those results. The intra case performs LOO cross validation while inter case runs 10-fold cross validation. For the intra-building case, the classification is not sensitive to different window sizes as seen in the figure: basically, accuracy stays almost the same for both buildings because within the same building, as long as we can capture the short term characteristics\nof sensor dynamics in the windowed time slots, the size of the time window doesn\u2019t make too much difference. However, for the inter-building case, the time window size matters in the way that usually the local micro-climate in one building can be quite different from another, we need to \u201ctune\u201d this common short term window to capture the dynamics that can be used to learn type-related information across buildings. Therefore, in order to achieve decent type classification accuracy across buildings, (i.e, use information from one building to help classify the traces in another building), we still need to optimize the size of time window, which is 45 minutes in our case. This \u201ctuning\u201d is significant for the learning process across buildings and is straightforward to perform."}, {"heading": "3.7 Identifying Potential Misclassifications", "text": "As we discussed in early section, being able to quantify the confidence in classifications and identify misclassified instances in our sensor type classification is vital to improving the overall accuracy considering that in many cases our technique is used there will be the absence of ground truth. As an intermediate step to identify potentially misclassified instances, we propose to quantify the \u201cuncertainty\u201d of classification with an entropy-based approach described in Section 3.3. Figure 5 shows the CDF of class probability entropy of classification in the intra- and inter- building scenarios. We see that the collection of correct classifications (in solid lines) has a distinct distribution from the collection of misclassification (in dotted lines). Based on such distinction in the distribution, we can choose a certain entropy value as a threshold and filter out all the classified instances whose class probability entropy are greater than the threshold outputted by the forest. Figure 6 gives a summary of the performance of our entropy-based approach to identifying potential misclassification. Here are some definitions needed to understand the statistics:\nS1: the set of instance whose class probability entropy is greater than the threshold.\nS2: the set of instance falling in S1 that is misclassified in the classification process.\nS3: the set of instance falling in S1 that is correctly classified in the classification process.\nS4: the set of instance that is misclassified in the classification process.\nS5: the set of instance that is correctly classified in the classification process.\nAnd the TPR, FPR and PPV are defined as:\nTPR = |S2|\n|S4| , FPR =\n|S3| |S5| , PPV = |S2| |S1| ,\nWhere | \u00b7 | is the cardinality of a set. We see that as the threshold value increases, both of the TPR (recall) and FPR (fall-out) decrease while the PPV (precision) keeps increasing.\nIn our case, a smaller threshold essentially leads to a larger population of instances being filtered out as potential misclassification \u201ccandidates\u201d, which helps identify more real misclassified instances. However, the more candidates we filter our, the more instances we need to manually inspect, which inevitably leads to a lower precision of the identification process. So we want to strike a balance between achieving a high recall rate as well as maintaining a high precision. As a result, based on the observation from Figure 6, we suggest picking a threshold value somewhere between 0.4 and 0.45 is appropriate. To note, we have 50 and 13 misclassified instances for Rice and SDH respectively for the intra-building testing case. In the intra-building case, such a threshold (0.4-0.45) helps identify \u223c30% of the misclassified instances for Rice and \u223c50% for SDH while resulting in that \u223c70% and \u223c50% of the instances being manual inspected are actually correct classifications, for Rice and SDH respectively. As for the inter-building case, our approach is able to identify \u223c75% of the misclassified instances for both Rice and SDH with an overhead of \u223c40% and \u223c70% in the candidate inspection, for Rice and SDH respectively."}, {"heading": "4. DISCUSSION", "text": "There are several aspects of our work that we left out or did not have time to explore more deeply. First we go over the expansion of type classes and how we could increase coverage of sensor types in future work. We discuss how we could improve classification accuracy by looking for data sources outside the building data sets. We also discuss why principal component analysis is an aspect that we did not explore in depth and how the principal components can change from building to building. Finally, we explain how our misclassification identifier could be used to improve classification results."}, {"heading": "4.1 Extension of Taxonomy and Class Scope", "text": "Our taxonomy covers 5 specific and one general sensor type. We could extend the class scope to include more sensor types and make our technique more versatile. There are many types of sensors in modern buildings and the sensing fabric in smart buildings continue to diversify, e.g., occupancy sensors, light sensors, etc. We also want to build a deeper taxonomy for certain types. For instance, there are set points for very different actuators. Temperature set-points drive the HVAC system, while the air quality set-point drives the\nfilters and air mixers. Being able to differentiate between these can help enable general control applications in buildings."}, {"heading": "4.2 Improvement on Classification Accuracy", "text": "The learning and classification processes in our work relies only on a set of general features. However, we wish to explore how using external or domain-specific knowledge could help improve the classification accuracy. For instance, if we know the humidity in rooms will increase due to a rain forecast, then we could search for traces with increases in average in reading values as external knowledge to help identify humidity traces."}, {"heading": "4.3 Feature Importance and Selection", "text": "In our study, we did not delve into the the importance of features (i.e. principle component analysis) because the feature vector contains only eight variables. Therefore, doing classification in a hyperspace of only eight dimensions is not computationally expensive \u2013 even if some of the vector elements carry redundant information. More importantly, that\nselecting the set of principle features for each building results in using a different feature set (as demonstrated in Table 6) per building. This makes classification across buildings impossible. Still, evaluating the principle components and uncovering overlap is important for obtaining optimal classification performance for intra-building tasks and single-type analysis."}, {"heading": "4.4 Reducing Misclassification Iteratively", "text": "In cases where no ground truth labels are available, an entropybased approach can be used in an iterative manner to improve classification results. In each iteration, only a few examples (on the top of the entropy-based \u201cuncertainty\u201d ranking list) are inspected and corrected, and the corrected instances could be added to the training set. The training and classification process is repeated until some criteria is satisfied. We expect the number of examples needed for manual inspection will be dramatically reduced in each iteration and overall, compared to a one-time inspection of candidates filtered by some threshold value. Such an interactive,\nsupervised learning process can produce better classification results and reduce the human labeling effort needed."}, {"heading": "5. RELATED WORK", "text": "To the best of our knowledge, we are the first to approach the problem of sensor-type classification of physical data in buildings. We describe the closest, related work in different problem domains and describe work that uses the random forest as a tool.\nThere has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27]. The goal of [14] is to classify audios into categories such as speech, music, background sound and silence using support vector machines, and the work in [18] addresses the same problem with a HMM-based statistical model. Examples of music genre (i.e, jazz, pop and so forth) classification are [10, 16], which use GMM with EM algorithm and logistic regression respectively. And commonly used features for these audio-related classification work are MFCC, zero crossing rate, energy/power and spectral/temporal statistics. For video type classification, texture and color-based features are used to classify videos into classes including cartoon, commercial, news and so on with decision tree [7] and neural network [17]. Query categorization has also been researched, [11] exploits a rule-based classifier while [9] uses a Markov random walk model. There is also work on human activity classification in general cases [21] (i.e, running, walking and sitting) and home setting [27] (i.e, sleeping, toiletting and showering) using accelerometer data with voting-based classifier and HMM with conditional random fields respectively. In contrast, our work is focused on sensor type classification using ensemble learning technique.\nRandom forests have been applied in many different areas [6, 26, 13, 20]. [6] uses a gene as a feature to classify microarray data. [26] uses the intensity of hundreds of measured metabolites from medical subjects as features, to classify the subjects into groups of normal, diseased and diseased with drug treatment. Random forests have also been used in [13] to classify objects in images with image-relevant features. In the area of remote sensing, [20] utilizes user-defined parameters as features to classify land cover types. In our work, we use simple and general statistical feature-set for type classification.\nThere is work leveraging percentile-based features in time series data for different classification purposes. Tarzia [24] et. al use a certain percentile in the audio spectrum to classify the current room location. Wang [28] et. al utilize percentile-based features in audio to characterize occupancy and noise levels. For comparison, we use percentile statistics\nin sensor time series as part of our feature-set to differentiate between different sensor types in commercial buildings."}, {"heading": "6. CONCLUSION", "text": "We describe a general, simple yet effective feature extraction design in support of sensor type classification with time series data. By experimenting with over 2000 streams from two buildings on two campuses, our technique, which leverages an ensemble learning method, is able to achieve an accuracy more than 92% and 82% for testing within building and across buildings, respectively. We also discuss that how to choose the window size applied to a slice of the original time series and how the number of training instances affects classification accuracy. In general, around 100 instances are enough to bootstrap the learning process in the case of 6 types of sensors. Another important contribution of our paper is a probability-based solution for identifying potentially misclassified instances. With the use of probabilities produced by the random forest, in both of the intraand inter- building learning cases, we are able to identify at least 30% of the misclassifications.\nOur technique can act as a tool for metadata construction for building sensors. For cases where type information of sensors is missing, our technique can help infer and generate the type metadata. In cases where metadata is available in an inconsistent manner within/across buildings, our solution can be used to verify type information and unify the naming schema across platforms in different buildings. Questions remain about how broadly we can expand our taxonomy and further study the scalability of our technique."}, {"heading": "7. REFERENCES", "text": "[1] American Society of Heating, Refrigerating and\nAir-Conditioning Engineers. ASHRAE Standard 135-1995: BACnet. ASHRAE, Inc., 1995.\n[2] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Comput., 9(7):1545\u20131588, Oct. 1997.\n[3] B. Balaji, J. Xu, A. Nwokafor, R. Gupta, and Y. Agarwal. Sentinel: Occupancy based hvac actuation using existing wifi infrastructure within commercial buildings. In Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems, SenSys \u201913, pages 17:1\u201317:14, New York, NY, USA, 2013. ACM.\n[4] L. Breiman. Random forests. Mach. Learn., 45(1):5\u201332, Oct. 2001.\n[5] S. Dawson-Haggerty, X. Jiang, G. Tolle, J. Ortiz, and D. Culler. sMAP: a simple measurement and actuation profile for physical information. In Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems, SenSys \u201910, 2010.\n[6] R. Diaz-Uriarte and S. Alvarez de Andres. Gene\nselection and classification of microarray data using random forest. BMC Bioinformatics, 7(1):3, 2006.\n[7] R. Glasberg, S. Schmiedeke, M. Mocigemba, and T. Sikora. New real-time approaches for video-genre-classification using high-level descriptors and a set of classifiers. In Semantic Computing, 2008 IEEE International Conference on, pages 120\u2013127, Aug 2008.\n[8] L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Trans. Pattern Anal. Mach. Intell., 12(10):993\u20131001, Oct. 1990.\n[9] J. Hu, G. Wang, F. Lochovsky, J.-t. Sun, and Z. Chen. Understanding user\u2019s query intent with wikipedia. In Proceedings of the 18th International Conference on World Wide Web, WWW \u201909, pages 471\u2013480, New York, NY, USA, 2009. ACM.\n[10] D.-N. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H. Cai. Music type classification by spectral contrast feature. In Multimedia and Expo, 2002. ICME \u201902. Proceedings. 2002 IEEE International Conference on, volume 1, pages 113\u2013116 vol.1, 2002.\n[11] I.-H. Kang and G. Kim. Query type classification for web document retrieval. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR \u201903, pages 64\u201371, New York, NY, USA, 2003. ACM.\n[12] Korea Electronics Technology Institute. http://www.keti.re.kr/.\n[13] R. Lefort, R. Fablet, and J.-M. Boucher. Weakly supervised classification of objects in images using soft random forests. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV\u201910, pages 185\u2013198, Berlin, Heidelberg, 2010. Springer-Verlag.\n[14] L. Lu, H.-J. Zhang, and S. Z. Li. Content-based audio classification and segmentation by using support vector machines. Multimedia Systems, 8(6):482\u2013492, 2003.\n[15] Mary Meeker. Internet Trends 2014 - Code Conference, 2014.\n[16] F. Moerchen, I. Mierswa, and A. Ultsch. Understandable models of music collections based on exhaustive feature generation with temporal statistics. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 882\u2013891, New York, NY, USA, 2006. ACM.\n[17] M. Montagnuolo and A. Messina. Parallel neural networks for multimodal video genre classification. Multimedia Tools and Applications, 41(1):125\u2013159, 2009.\n[18] T. L. Nwe and H. Li. Broadcast news segmentation by audio type analysis. In Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP \u201905). IEEE International Conference on, volume 2, pages ii/1065\u2013ii/1068 Vol. 2, March 2005.\n[19] D. Opitz and R. Maclin. Popular ensemble methods: An empirical study. Journal of Artificial Intelligence Research, 11:169\u2013198, 1999.\n[20] M. Pal. Random forest classifier for remote sensing\nclassification. International Journal of Remote Sensing, 26(1):217\u2013222, 2005.\n[21] N. Ravi, N. D, P. Mysore, and M. L. Littman. Activity recognition from accelerometer data. In In Proceedings of the Seventeenth Conference on Innovative Applications of Artificial Intelligence(IAAI, pages 1541\u20131546. AAAI Press, 2005.\n[22] L. Rokach. Ensemble-based classifiers. Artif. Intell. Rev., 33(1-2):1\u201339, Feb. 2010.\n[23] Scikit-learn. http://scikit-learn.org/.\n[24] S. P. Tarzia, P. A. Dinda, R. P. Dick, and G. Memik. Indoor localization without infrastructure using the acoustic background spectrum. In Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services, MobiSys \u201911, pages 155\u2013168, New York, NY, USA, 2011. ACM.\n[25] Trane Inc. http://www.trane.com/.\n[26] Y. Truong, X. Lin, and C. Beecher. Learning a complex metabolomic dataset using random forests and support vector machines. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, pages 835\u2013840, New York, NY, USA, 2004. ACM.\n[27] T. van Kasteren, A. Noulas, G. Englebienne, and B. Kro\u0308se. Accurate activity recognition in a home setting. In Proceedings of the 10th International Conference on Ubiquitous Computing, UbiComp \u201908, pages 1\u20139, New York, NY, USA, 2008. ACM.\n[28] H. Wang, D. Lymberopoulos, and J. Liu. Local business ambience characterization through mobile audio sensing. In Proceedings of the 23rd International Conference on World Wide Web, WWW \u201914, pages 293\u2013304, Republic and Canton of Geneva, Switzerland, 2014. International World Wide Web Conferences Steering Committee.\n[29] J. Winn and J. Shotton. The layout consistent random field for recognizing and segmenting partially occluded objects. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1, CVPR \u201906, pages 37\u201344, Washington, DC, USA, 2006. IEEE Computer Society."}], "references": [{"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural Comput., 9(7):1545\u20131588, Oct.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Sentinel: Occupancy based hvac actuation using existing wifi infrastructure within commercial buildings", "author": ["B. Balaji", "J. Xu", "A. Nwokafor", "R. Gupta", "Y. Agarwal"], "venue": "Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems, SenSys \u201913, pages 17:1\u201317:14, New York, NY, USA,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Mach. Learn., 45(1):5\u201332, Oct.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "sMAP: a simple measurement and actuation profile for physical information", "author": ["S. Dawson-Haggerty", "X. Jiang", "G. Tolle", "J. Ortiz", "D. Culler"], "venue": "Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems, SenSys \u201910,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Gene  selection and classification of microarray data using random forest", "author": ["R. Diaz-Uriarte", "S. Alvarez de Andres"], "venue": "BMC Bioinformatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "New real-time approaches for video-genre-classification using high-level descriptors and a set of classifiers", "author": ["R. Glasberg", "S. Schmiedeke", "M. Mocigemba", "T. Sikora"], "venue": "Semantic Computing, 2008 IEEE International Conference on, pages 120\u2013127, Aug", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 12(10):993\u20131001, Oct.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Understanding user\u2019s query intent with wikipedia", "author": ["J. Hu", "G. Wang", "F. Lochovsky", "J.-t. Sun", "Z. Chen"], "venue": "In Proceedings of the 18th International Conference on World Wide Web, WWW", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Music type classification by spectral contrast feature", "author": ["D.-N. Jiang", "L. Lu", "H.-J. Zhang", "J.-H. Tao", "L.-H. Cai"], "venue": "Multimedia and Expo, 2002. ICME \u201902. Proceedings. 2002 IEEE International Conference on, volume 1, pages 113\u2013116 vol.1,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Query type classification for web document retrieval", "author": ["I.-H. Kang", "G. Kim"], "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR \u201903, pages 64\u201371, New York, NY, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Weakly supervised classification of objects in images using soft random forests", "author": ["R. Lefort", "R. Fablet", "J.-M. Boucher"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV\u201910, pages 185\u2013198, Berlin, Heidelberg,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Content-based audio classification and segmentation by using support vector machines", "author": ["L. Lu", "H.-J. Zhang", "S.Z. Li"], "venue": "Multimedia Systems, 8(6):482\u2013492,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Understandable models of music collections based on exhaustive feature generation with temporal statistics", "author": ["F. Moerchen", "I. Mierswa", "A. Ultsch"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 882\u2013891, New York, NY, USA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel neural networks for multimodal video genre classification", "author": ["M. Montagnuolo", "A. Messina"], "venue": "Multimedia Tools and Applications, 41(1):125\u2013159,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Broadcast news segmentation by audio type analysis", "author": ["T.L. Nwe", "H. Li"], "venue": "Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP \u201905). IEEE International Conference on, volume 2, pages ii/1065\u2013ii/1068 Vol. 2, March", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Popular ensemble methods: An empirical study", "author": ["D. Opitz", "R. Maclin"], "venue": "Journal of Artificial Intelligence Research, 11:169\u2013198,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Random forest classifier for remote sensing  classification", "author": ["M. Pal"], "venue": "International Journal of Remote Sensing, 26(1):217\u2013222,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Activity recognition from accelerometer data", "author": ["N. Ravi", "N. D", "P. Mysore", "M.L. Littman"], "venue": "Proceedings of the Seventeenth Conference on Innovative Applications of Artificial Intelligence(IAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artif. Intell. Rev., 33(1-2):1\u201339, Feb.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Indoor localization without infrastructure using the acoustic background spectrum", "author": ["S.P. Tarzia", "P.A. Dinda", "R.P. Dick", "G. Memik"], "venue": "Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services, MobiSys \u201911, pages 155\u2013168, New York, NY, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning a complex metabolomic dataset using random forests and support vector machines", "author": ["Y. Truong", "X. Lin", "C. Beecher"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, pages 835\u2013840, New York, NY, USA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Accurate activity recognition in a home setting", "author": ["T. van Kasteren", "A. Noulas", "G. Englebienne", "B. Kr\u00f6se"], "venue": "In Proceedings of the 10th International Conference on Ubiquitous Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Local business ambience characterization through mobile audio sensing", "author": ["H. Wang", "D. Lymberopoulos", "J. Liu"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, WWW \u201914, pages 293\u2013304, Republic and Canton of Geneva, Switzerland,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "The layout consistent random field for recognizing and segmenting partially occluded objects", "author": ["J. Winn", "J. Shotton"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1, CVPR \u201906, pages 37\u201344, Washington, DC, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 15, "context": "In general, ensemble learning methods obtain better predictive performance than any of the constituent learning methods as discussed in [19, 22], if the following assumptions hold [8]: 1) the probability of a correct classification by each individual classifier is greater than 0.", "startOffset": 136, "endOffset": 144}, {"referenceID": 18, "context": "In general, ensemble learning methods obtain better predictive performance than any of the constituent learning methods as discussed in [19, 22], if the following assumptions hold [8]: 1) the probability of a correct classification by each individual classifier is greater than 0.", "startOffset": 136, "endOffset": 144}, {"referenceID": 6, "context": "In general, ensemble learning methods obtain better predictive performance than any of the constituent learning methods as discussed in [19, 22], if the following assumptions hold [8]: 1) the probability of a correct classification by each individual classifier is greater than 0.", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "Random forests [4] have been widely used and outperform a single tree classifier.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "They are also faster [29] in training and testing compared to traditional classifiers such as SVM.", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "The notion of randomized trees was first introduced in [2] and further well developed in [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "The notion of randomized trees was first introduced in [2] and further well developed in [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "Usually these parameters are optimized through cross-validation and we refer interested readers to [4] for further deduction and proof of random forest.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Note that instead of letting each tree vote for on class as described in the original paper [4], we combine the results from classifiers for an instance by averaging their probabilistic predictions, in order to facilitate our technique used to help identify potential misclassified instances as described in the following section.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "Included with permission from the authors of [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "The other building is the Sutardja Dai Hall (SDH) at UC Berkeley, where the deployed sensors [12, 1] transmit to an archiver [5] periodically from anywhere between every 5 seconds to every 10 minutes.", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 81, "endOffset": 89}, {"referenceID": 8, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 97, "endOffset": 105}, {"referenceID": 12, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 97, "endOffset": 105}, {"referenceID": 5, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 113, "endOffset": 120}, {"referenceID": 13, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 132, "endOffset": 139}, {"referenceID": 7, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 132, "endOffset": 139}, {"referenceID": 17, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 21, "context": "There has been much research work on type classification in the context of audio [14, 18], music [10, 16], video [7, 17], web query [11, 9] and human activity [21, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 11, "context": "The goal of [14] is to classify audios into categories such as speech, music, background sound and silence using support vector machines, and the work in [18] addresses the same problem with a HMM-based statistical model.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "The goal of [14] is to classify audios into categories such as speech, music, background sound and silence using support vector machines, and the work in [18] addresses the same problem with a HMM-based statistical model.", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "e, jazz, pop and so forth) classification are [10, 16], which use GMM with EM algorithm and logistic regression respectively.", "startOffset": 46, "endOffset": 54}, {"referenceID": 12, "context": "e, jazz, pop and so forth) classification are [10, 16], which use GMM with EM algorithm and logistic regression respectively.", "startOffset": 46, "endOffset": 54}, {"referenceID": 5, "context": "For video type classification, texture and color-based features are used to classify videos into classes including cartoon, commercial, news and so on with decision tree [7] and neural network [17].", "startOffset": 170, "endOffset": 173}, {"referenceID": 13, "context": "For video type classification, texture and color-based features are used to classify videos into classes including cartoon, commercial, news and so on with decision tree [7] and neural network [17].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "Query categorization has also been researched, [11] exploits a rule-based classifier while [9] uses a Markov random walk model.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Query categorization has also been researched, [11] exploits a rule-based classifier while [9] uses a Markov random walk model.", "startOffset": 91, "endOffset": 94}, {"referenceID": 17, "context": "There is also work on human activity classification in general cases [21] (i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "e, running, walking and sitting) and home setting [27] (i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Random forests have been applied in many different areas [6, 26, 13, 20].", "startOffset": 57, "endOffset": 72}, {"referenceID": 20, "context": "Random forests have been applied in many different areas [6, 26, 13, 20].", "startOffset": 57, "endOffset": 72}, {"referenceID": 10, "context": "Random forests have been applied in many different areas [6, 26, 13, 20].", "startOffset": 57, "endOffset": 72}, {"referenceID": 16, "context": "Random forests have been applied in many different areas [6, 26, 13, 20].", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "[6] uses a gene as a feature to classify microarray data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[26] uses the intensity of hundreds of measured metabolites from medical subjects as features, to classify the subjects into groups of normal, diseased and diseased with drug treatment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Random forests have also been used in [13] to classify objects in images with image-relevant features.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "In the area of remote sensing, [20] utilizes user-defined parameters as features to classify land cover types.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "Tarzia [24] et.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "Wang [28] et.", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "Many sensors/meters are deployed in commercial buildings to monitor and optimize their performance. However, because sensor metadata is inconsistent across buildings, softwarebased solutions are tightly coupled to the sensor metadata conventions (i.e. schemas and naming) for each building. Running the same software across buildings requires significant integration effort. Metadata normalization is critical for scaling the deployment process and allows us to decouple building-specific conventions from the code written for building applications. It also allows us to deal with missing metadata. One important aspect of normalization is to differentiate sensors by the type of phenomena being observed. In this paper, we propose a general, simple, yet effective classification scheme to differentiate sensors in buildings by type. We perform ensemble learning on data collected from over 2000 sensor streams in two buildings. Our approach is able to achieve more than 92% accuracy for classification within buildings and more than 82% accuracy for across buildings. We also introduce a method for identifying potential misclassified streams. This is important because it allows us to identify opportunities to attain more input from experts \u2013 input that could help improve classification accuracy when ground truth is unavailable. We show that by adjusting a threshold value we are able to identify at least 30% of the misclassified instances.", "creator": "LaTeX with hyperref package"}}}