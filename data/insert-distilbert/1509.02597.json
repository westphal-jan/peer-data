{"id": "1509.02597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2015", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis", "abstract": "aiming broadly at best solving large - scale learning problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers ( admm ). by fully formulating the intermediate learning problem resulting as a consensus problem, the admm can be used to positively solve the consensus problem in a fully parallel fashion over a computer sized network model with a large star topology. however, traditional constraint synchronized computation does not scale well with the problem size, as the speed of the algorithm is actually limited by the slowest workers. this is particularly unlikely true in a heterogeneous network such where apparently the computing queue nodes experience different computation and communication delay delays. in this cited paper, we merely propose an asynchronous distributed admm ( ad - amm ) which can effectively improve the time efficiency of coherent distributed optimization. often our main interest lies this in analyzing the convergence conditions of implemented the ad - admm, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. specifically, by considering general and possibly non - convex cost decision functions, we show that the ad - metric admm is guaranteed to converge to the transition set of karush - bound kuhn - tucker ( kkt ) points as ever long as the algorithm for parameters are chosen appropriately according to the network delay. we further illustrate that the asynchrony of the admm has to be handled with care, as requiring slightly modifying the implementation of the ad - admm can jeopardize the algorithm convergence, even under a standard convex setting.", "histories": [["v1", "Wed, 9 Sep 2015 01:45:08 GMT  (396kb)", "https://arxiv.org/abs/1509.02597v1", "28 pages, submitted for publication"], ["v2", "Fri, 19 Feb 2016 06:02:38 GMT  (399kb)", "http://arxiv.org/abs/1509.02597v2", "37 pages"]], "COMMENTS": "28 pages, submitted for publication", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SY", "authors": ["tsung-hui chang", "mingyi hong", "wei-cheng liao", "xiangfeng wang"], "accepted": false, "id": "1509.02597"}, "pdf": {"name": "1509.02597.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis", "authors": ["Tsung-Hui Chang", "Mingyi Hong", "Wei-Cheng Liao", "Xiangfeng Wang"], "emails": ["tsunghui.chang@ieee.org.", "mingyi@iastate.edu", "liaox146@umn.edu", "xfwang@sei.ecnu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n02 59\n7v 2\n[ cs\n.D C\n] 1\n9 Fe\nb 20\nKeywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization\nPart of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, Grant No. 61571385. Mingyi Hong is supported by NFS Grant No. CCF-1526078 , and AFOSR, Grant No. 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan No. 15YF1403400 and NSFC No. 11501210.\n\u22c6Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172. E-mail: tsunghui.chang@ieee.org.\n\u2020Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu\n\u2020Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: liaox146@umn.edu\n\u2021Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, School of Computer Science and Software Engineering, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn\nFebruary 22, 2016 DRAFT\n2 I. INTRODUCTION"}, {"heading": "A. Background", "text": "Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]\u2013[4]. In this work, we are interested in developing distributed optimization methods for solving the following optimization problem\nmin x\u2208Rn\nN\u2211\ni=1\nfi(x) + h(x), (1)\nwhere each fi : Rn \u2192 R is a (smooth) cost function; h : Rn \u2192 R\u222a{\u221e} is a convex ( proper and lower semi-continuous) but possibly non-smooth regularization function. The latter is used to impose desired structures on the solution (e.g., sparsity) and/or used to enforce certain constraints. Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8]. In this paper, we focus on solving large-scale instances of these learning problems with either a large number of training samples or a large number of features (n is large) [3]. These are typical data-intensive machine learning scenarios in which the data sets are often distributedly located in a few computing nodes. Traditional centralized optimization methods, therefore, fails to scale well due to their inability to handle distributed data sets and computing resources.\nOur goal is to develop efficient distributed optimization algorithms over a computer network with a star topology, in which a master node coordinates the computation of a set of distributed workers (see Figure 1 for illustration). Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16]. For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method. In these works, the distributed workers iteratively calculate the gradients related to their local data, while the master collects such information from the workers to perform SGD, PG or BCD updates.\nHowever, when scaling up these distributed algorithms, node synchronization becomes an important issue. Specifically, under the synchronous protocol, the master is triggered at each iteration only if it receives the required information from all the distributed workers. On the one hand, such synchronization is beneficial to make the algorithms well behaved; on the other hand, however, the speed of the algorithms\nFebruary 22, 2016 DRAFT\nwould be limited by the \u201cslowest\u201d worker especially when the workers have different computation and communication delays. To address such dilemma, a few recent works [10]\u2013[14] have introduced \u201casynchrony\u201d into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information. The asynchronous updates would cause \u201cdelayed\u201d gradient information. A few algorithmic tricks such as delay-dependent step-size selection have been introduced to ensure that the staled gradient information does not destroy the stability of the algorithm. In practice, such asynchrony does make a big difference. As has been consistently reported in [10]\u2013[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers."}, {"heading": "B. Related Works", "text": "A different approach for distributed and parallel optimization is based on the alternating direction method of multipliers (ADMM) [9, Section 7.1.1]. In the distributed ADMM, the original learning problem is partitioned into N subproblems, each containing a subset of training samples or the learning parameters. At each iteration, the workers solve the subproblems and send the up-to-date variable information to the master, who summarizes this information and broadcasts the result to the workers. In this way, a given large-scale learning problem can be solved in a parallel and distributed fashion. Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.\nRecently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14]. Specifically, reference [19] has considered a version of AD-ADMM with bounded delay assumption and studied its theoretical and numerical performances. However, only convex cases are considered in [19]. Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only. This type of distributed optimization schemes, however, may not fully utilize the\nFebruary 22, 2016 DRAFT\n4 computation powers of distributed nodes. Besides, due to inexact update, such schemes usually require more iterations to converge and thus may have higher communication overhead. References [21]\u2013[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks. These works consider network topologies beyond the star network, but their definition of asynchrony is different from what we propose here. Specifically, the asynchrony in [21] lies in that, at each iteration, the nodes are randomly activated to perform variable update. The method presented in [22] further allows that the communications between nodes can succeed or fail randomly. It is shown in [22] that such asynchronous ADMM can converge in a probability-one sense, provided that the nodes and communication links satisfy certain statistical assumption. Reference [23] has considered an asynchronous dual ADMM method. The asynchrony is in the sense that the nodes are partitioned into groups based on certain coloring scheme and only one group of nodes update variable in each iteration."}, {"heading": "C. Contributions", "text": "In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting. Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.\nTheoretically, we show that, for general and possibly non-convex problems in the form of (1), the AD-ADMM converges to the set of KKT points if the algorithm parameters are chosen appropriately according to the maximum network delay. Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems. Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24]. To the best of our knowledge, except the inexact version in [20], this is the first time that the distributed ADMM is rigorously shown to be convergent for non-convex problems under the asynchronous protocol. Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only. Furthermore, we demonstrate that the asynchrony of ADMM has to be handled with care \u2013 as a slight modification of the algorithm may\n1In contrast to the conference paper [1], the current paper presents detailed proofs of theorems and more simulation results.\nFebruary 22, 2016 DRAFT\n5 lead to completely different convergence conditions and even destroy the convergence of ADMM for convex problems. Some numerical results are presented to support our theoretical claims.\nIn the companion paper [25], the linear convergence conditions of the AD-ADMM is further analyzed. In addition, the numerical performance of the AD-ADMM is examined by solving a large-scale LR problem on a high-performance computer cluster.\nSynopsis: Section II presents the applications of problem (1) and reviews the distributed ADMM in [9]. The proposed AD-ADMM and its convergence conditions are presented in Section III. Comparison of the proposed AD-ADMM with an alternative scheme is presented in Section IV. Some simulation results are presented in Section V. Finally, concluding remarks are given in Section VI.\nII. APPLICATIONS AND DISTRIBUTED ADMM"}, {"heading": "A. Applications", "text": "We target at solving problem (1) over a star computer network (cluster) with one master node and N workers/slaves, as illustrated in Figure 1. Such distributed optimization approach is extremely useful in modern big data applications [3]. For example, let us consider the following regularized empirical risk minimization problem [7]\nmin w\u2208Rn\nm\u2211\nj=1\n\u2113(aTj w, yj) + \u2126(w), (2)\nwhere m is the number of training samples and \u2113(aTj w, yj) is a loss function (e.g., regression or classification error) that depends on the training sample aj \u2208 Rn, label yj and the parameter vector w \u2208 Rn. Here, n denotes the dimension of the parameters (features); \u2126(w) is an appropriate convex regularizer. Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few. Obviously, solving (2) can be challenging when the number of training samples is very large. In that case, it is natural to split the training samples across the computer cluster and resort to a distributed optimization approach. Suppose that the m training samples are uniformly distributed and stored by the N workers, with each node i getting qi = xm/Ny samples. By defining fi(w) , \u2211iqi\nj=(i\u22121)qi+1 \u2113(aTj w, yj),\ni = 1, . . . , N , and h(w) , \u2126(w), it is clear that (2) is an instance of (1).\nWhen the number of training samples is moderate but the dimension of the parameters is very large (n \u226b m), problem (2) is also challenging to solve. By [9, Section 7.3], one can instead consider the Lagrangian dual problem of (2) provided that (2) has zero duality gap. Specifically, let the training matrix A , [a1, . . . ,am]T \u2208 Rm\u00d7n be partitioned as A = [A1, . . . ,AN ], and let the parameter vector\nFebruary 22, 2016 DRAFT\n6 w be partitioned conformally as w = [wT1 , . . . ,w T N ]\nT ; moreover, assume that \u2126 is separable as \u2126(w) = \u2211N\ni=1 \u2126i(wi). Then, following [9, Section 7.3], one can obtain the dual problem of (2) as\nmin \u03bd\u2208Rm\nN\u2211\ni=1\n\u2126\u2217i (A T i \u03bd) + \u03a6 \u2217(\u03bd), (3)\nwhere \u03bd , [\u03bd1, . . . , \u03bdm]T is a dual variable, \u03a6\u2217(\u03bd) = \u2211m j=1 \u2113 \u2217(\u03bdj , yj), and \u2113\u2217 and \u2126\u2217i are respectively the conjugate functions of \u2113 and \u2126i. Note that (3) is equivalent to splitting the n parameters across the N workers. Clearly, problem (3) is an instance of (1).\nIt is interesting to mention that many emerging problems in smart power grid can also be formulated as problem (1); see, for example, the power state estimation problem considered in [27] is solved by employing the distributed ADMM. The energy management problems (i.e., demand response) in [28]\u2013[30] can potentially be handled by the distributed ADMM as well."}, {"heading": "B. Distributed ADMM", "text": "In this section, we present the distributed ADMM [4], [9] for solving problem (1). Let us consider the\nfollowing consensus formulation of problem (1)\nmin x0,xi\u2208Rn, i=1,...,N\nN\u2211\ni=1\nfi(xi) + h(x0) (4a)\ns.t. xi = x0, \u2200i = 1, . . . , N. (4b)\nIn (4), the N + 1 variables xi, i = 0, 1, . . . , N , are subject to the consensus constraint in (4b), i.e., x0 = x1 = \u00b7 \u00b7 \u00b7 = xN . Thus, problem (4) is equivalent to (1). It has been shown that such a consensus problem can be efficiently solved by the ADMM [9]. To describe this method, let \u03bb \u2208 Rn denote the Lagrange dual variable associated with constraint (4b) and define the following augmented Lagrangian function\nL\u03c1(x,x0,\u03bb) = N\u2211\ni=1\nfi(xi) + h(x0)\n+\nN\u2211\ni=1\n\u03bb T i (xi \u2212 x0) +\n\u03c1 2\nN\u2211\ni=1\n\u2016xi \u2212 x0\u20162, (5)\nwhere x , [xT1 , . . . ,x T N ] T , \u03bb , [\u03bbT1 , . . . ,\u03bb T N ] T and \u03c1 > 0 is a penalty parameter. According to [4], the standard synchronous ADMM iteratively updates the primal variables xi, i = 0, 1, . . . , N, by minimizing (5) in a (one-round) Gauss-Seidel fashion, followed by updating the dual variable \u03bb using an approximate gradient ascent method. The ADMM algorithm for solving (4) is presented in Algorithm 1,\nFebruary 22, 2016 DRAFT\n7 Algorithm 1 (Synchronous) Distributed ADMM for (4) [9]\n1: Given initial variables x0 and \u03bb0; set x00 = x 0 and k = 0. 2: repeat 3: update\nx k+1 0 =arg min\nx0\u2208Rn\n{ h(x0)\u2212 xT0 \u2211N i=1 \u03bb k i\n+ \u03c12 \u2211N i=1 \u2016xki \u2212 x0\u20162 } , (6)\nx k+1 i =arg min\nxi\u2208Rn fi(xi) + x\nT i \u03bb k i + \u03c1 2\u2016xi \u2212 xk+10 \u20162,\n\u2200 i = 1, . . . , N, (7)\n\u03bb k+1 i = \u03bb k i + \u03c1(x k+1 i \u2212 xk+10 ), \u2200 i = 1, . . . , N. (8)\n4: set k \u2190 k + 1. 5: until a predefined stopping criterion is satisfied.\nAs seen, Algorithm 1 is naturally implementable over the star computer network illustrated in Figure 1. Specifically, the master node takes charge of optimizing x0 by (6), and each worker i is responsible for optimizing (xi,\u03bbi) by (7)-(8). Through exchanging the up-to-date x0 and (xi,\u03bbi) between the master and the workers, Algorithm 1 solves problem (1) in a fully distributed and parallel manner. Convergence properties of the distributed ADMM have been extensively studied; see, e.g., [9], [18], [31]\u2013[33]. Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi\u2019s. For non-convex and smooth fi\u2019s, the work [18] shows that Algorithm 1 can converge to the set of KKT points with a O(1/ \u221a k) rate as long as \u03c1 is large enough.\nHowever, Algorithm 1 is a synchronous algorithm, where the operations of the master and the workers are \u201clocked\u201d with each other. Specifically, to optimize x0 at each iteration, the master has to wait until receiving all the up-to-date variables (xi,\u03bbi), i = 1, . . . , N , from the workers. Since the workers may\nFebruary 22, 2016 DRAFT\n8 have different computation and communication delays2, the pace of the optimization would be determined by the \u201cslowest\u201d worker. As an example illustrated in Figure 2(a), the master updates x0 only when it has received the variable information for the four workers at every iteration. As a result, under such synchronous protocol, the master and speedy workers (e.g., workers 1 and 3 in Figure 2) would spend most of the time idling, and thus the parallel computational resources cannot be fully utilized.\nIII. ASYNCHRONOUS DISTRIBUTED ADMM"}, {"heading": "A. Algorithm Description", "text": "In this section, we present an AD-ADMM. The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers. Instead, the master updates x0 whenever it receives (xi,\u03bbi) from a partial set of the workers. For example, in Figure 2(b), the master updates x0 whenever it receives the variable information from at least two workers. This implies that none of the workers have to be synchronized with each other and the master does not need to wait for the slowest worker either. As illustrated in Figure 2(b), with the lock removed, both the master and speedy workers can update their variables more frequently.\nLet us denote k \u2265 0 as the iteration number of the master (i.e., the number of times for which the master updates x0), and denote Ak \u2286 V , {1, . . . , N} as the index subset of workers from which the master receives variable information during iteration k (for example, in Figure 2(b), A0 = {1, 3} and\n2In a heterogeneous network, the workers can have different computational powers, or the data sets can be non-uniformly distributed across the network. Thus, the workers can require different computational times in solving the local subproblems. Besides, the communication delays can also be different, e.g., due to probabilistic communication failures and message retransmission.\nFebruary 22, 2016 DRAFT\n9 A1 = {1, 2})3. We say that worker i is \u201carrived\u201d at iteration k if i \u2208 Ak and \u201cunarrived\u201d otherwise. Clearly, unbounded delay will jeopardize the algorithm convergence. Therefore throughout this paper, we will assume that the asynchronous delay in the network is bounded. In particular, we follow the popular partially asynchronous model [4] and assume:\nAssumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay. For all i \u2208 V and iteration k \u2265 0, it must be that i \u2208 Ak \u222a Ak\u22121 \u00b7 \u00b7 \u00b7 \u222a Amax{k\u2212\u03c4+1,\u22121}.\nAssumption 1 implies that every worker i is arrived at least once within the period [k \u2212 \u03c4 + 1, k]. In another word, the variable information (xi,\u03bbi) used by the master must be at most \u03c4 iterations old. To guarantee the bounded delay, at every iteration the master should wait for the workers who have been inactive for \u03c4 \u2212 1 iterations, if such workers exist. Note that, when \u03c4 = 1, one has i \u2208 Ak for all i \u2208 V (i.e., Ak = V), which corresponds to the synchronous case and the master always waits for all the workers at every iteration.\nIn Algorithm 2, we present the proposed AD-ADMM, which specifies respectively the steps for the master and the distributed workers. Here, Ack denotes the complementary set of Ak, i.e., Ak \u2229 Ack = \u2205 and Ak \u222a Ack = V . Algorithm 2 has five notable differences compared with Algorithm 1. First, the master is required to update {(xi,\u03bbi)}i\u2208V , and such update is only performed for those variables with i \u2208 Ak. Second, x0 is updated by solving a problem with an additional proximal term \u03b32\u2016x0 \u2212 xk0\u20162, where \u03b3 > 0 is a penalty parameter (cf. (12)). Adding such proximal term is crucial in making the algorithm well-behaved in the asynchronous setting. As will be seen in the next section, a proper choice of \u03b3 guarantees the convergence of Algorithm 2. Third, the variables di\u2019s are introduced to count the delays of the workers. If worker i is arrived at the current iteration, then di is set to zero; otherwise, di is increased by one. So, to ensure Assumption 1 hold all the time, in Step 4 of Algorithm of the Master, the master waits if there exists at least one worker whose di \u2265 \u03c4 \u2212 1. Fourth, in addition to the bounded delay, we assume that the master proceeds to update the variables only if there are at least A \u2265 1 arrived workers, i.e., |Ak| \u2265 A for all k [19]. Note that when A = N , the algorithm reduces to the synchronous distributed ADMM. Fifth, in Step 6 of Algorithm of the Master, the master sends the up-to-date x0 only to the arrived workers.\nWe emphasize again that both the master and fast workers in the AD-ADMM can have less idle time and update more frequently than its synchronous counterpart. As illustrated in Figure 2, during the\n3Without loss of generality, we let A\u22121 = V , as seen from Figure 2.\nFebruary 22, 2016 DRAFT\n10\nsame period of time, the synchronous algorithm only completes two updates whereas the asynchronous algorithm updates six times already. On the flip side, the asynchronous algorithm introduces delayed variable information and thereby requires a larger number of iterations to reach the same solution accuracy than its synchronous counterpart. In practice we observe that the benefit of improved update frequency can outweigh the cost of increased number of iterations, and as a result the asynchronous algorithm can still converge faster in time. This is particularly true when the workers have different computation and communication delays and when the computation and communication delays of the master for solving (12) is much shorter than the computation and communication delays of the workers for updating (13) and (14)4; e.g., see Figure 2. Detailed numerical results will be reported in Section V of the companion paper [25]."}, {"heading": "B. Convergence Analysis", "text": "In this subsection, we analyze the convergence conditions of Algorithm 2. We first make the following\nstandard assumption on problem (1) (or equivalently problem (4)):\nAssumption 2 Each function fi is twice differentiable and its gradient \u2207fi is Lipschitz continuous with a Lipschitz constant L > 0; the function h is proper convex (lower semi-continuous, but not necessarily smooth) and dom(h) (the domain of h) is compact. Moreover, problem (1) is bounded below, i.e., F \u22c6 > \u2212\u221e where F \u22c6 denotes the optimal objective value of problem (1).\nNotably, we do not assume any convexity on fi\u2019s. Indeed, we will show that the AD-ADMM can converge to the set of KKT points even for non-convex fi\u2019s. Our main result is formally stated below.\nTheorem 1 Suppose that Assumption 1 and Assumption 2 hold true. Moreover, assume that there exists a constant S \u2208 [1, N ] such that |Ak| < S for all k and that\n\u221e > L\u03c1(x0,x00,\u03bb0)\u2212 F \u22c6 \u2265 0, (15) \u03c1 > (1 + L+ L2) + \u221a (1 + L+ L2)2 + 8L2\n2 , (16)\n\u03b3 > S(1 + \u03c12)(\u03c4 \u2212 1)2 \u2212N\u03c1\n2 . (17)\n4Note that, for many practical cases (such as h(x0) = \u2016x0\u20161) for which (12) has a closed-form solution, the computation delay of the master is negligible. For high-performance computer clusters connected by large-bandwidth fiber links, the communication delays between the master and the workers can also be short. However, for cases in which the computation and communication delays of the master is significant, the AD-ADMM could be less time efficient than the synchronous ADMM due to the increased number of iterations.\nFebruary 22, 2016 DRAFT\n11\nThen, ({xki }Ni=1,xk0 , {\u03bbki }Ni=1) generated by (9), (10) and (12) are bounded and have limit points which satisfy KKT conditions of problem (4).\nTheorem 1 implies that the AD-ADMM is guaranteed to converge to the set of KKT points as long as the penalty parameters \u03c1 and \u03b3 are sufficiently large. Since 1/\u03b3 can be viewed as the step size of x0, (17) indicates that the master should be more cautious in moving x0 if the network allows a longer delay \u03c4 . In particular, the value \u03b3 in the worst case should increase with the order of \u03c42. When \u03c4 = 1 (the synchronous case), \u03b3 = \u2212(N\u03c1)/2 < 0 and thus the proximal term \u03b32\u2016x0 \u2212 xk0\u20162 can be removed from (12). On the other hand, we also see from (17) that \u03b3 should increase with N if \u03c4 > 1 is fixed5. This is because in the worst case the more workers, the more outdated information introduced in the network. Finally, we should mention that a large \u03c1 may be essential for the AD-ADMM to converge properly, especially for non-convex problems, as we demonstrate via simulations in Section V.\nLet us compare Theorem 1 with the results in [19], [22]. First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems. Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense. Therefore it is possible, at least theoretically, that a realization of the algorithm fails to converge despite satisfying the conditions given in [19]. On the contrary, our convergence results hold deterministically.\nNote that for non-convex fi\u2019s, subproblem (13) is not necessarily convex. However, given \u03c1 \u2265 L in (16) and twice differentiability of fi (Assumption 2), subproblem (13) becomes a (strongly) convex problem6 and hence is globally solvable. When fi\u2019s are all convex functions, Theorem 1 reduces to the following corollary.\nCorollary 1 Assume that fi\u2019s are all convex functions. Under the same premises of Theorem 1, and for \u03b3 satisfying (17) and\n\u03c1 \u2265 (1 + L 2) +\n\u221a (1 + L2)2 + 8L2\n2 , (18)\n({xki }Ni=1,xk0 , {\u03bbki }Ni=1) generated by (9), (10) and (12) are bounded and have limit points which satisfy KKT conditions of problem (4).\n5Note that, for a fixed \u03c4 , S should increase with N . 6By [34, Lemma 1.2.2], the minimum eigenvalue of the Hessian matrix of fi(xi) is no smaller than \u2212L. Thus, for \u03c1 > L,\nsubproblem (13) is a strongly convex problem.\nFebruary 22, 2016 DRAFT\n12"}, {"heading": "C. Proof of Theorem 1 and Corollary 1", "text": "Let us write Algorithm 2 from the master\u2019s point of view. Define k\u0304i as the last iteration number before iteration k for which worker i \u2208 Ak is arrived7, i.e., i \u2208 Ak\u0304i . Then Algorithm 2 from the master\u2019s point of view is as follows: for master iteration k = 0, 1, . . . ,\nx k+1 i =    argmin xi { fi(xi) + x T i \u03bb k\u0304i+1 i + \u03c12\u2016xi \u2212 x k\u0304i+1 0 \u20162 } , \u2200i \u2208 Ak\nx k i \u2200i \u2208 Ack\n, (19)\n\u03bb k+1 i =\n{ \u03bb k\u0304i+1 i + \u03c1(x k+1 i \u2212 xk\u0304i+10 ) \u2200i \u2208 Ak\n\u03bb k i \u2200i \u2208 Ack\n, (20)\nx k+1 0 =arg min\nx0\u2208Rn\n{ h(x0)\u2212 xT0 \u2211N i=1 \u03bb k+1 i\n+ \u03c12 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162 + \u03b32 \u2016x0 \u2212 xk0\u20162 } .\nNow it is relatively easy to see that the master updates x0 using the delayed (xi,\u03bbi)i\u2208Ak and the old (xi,\u03bbi)i\u2208Ack . Under Assumption 1, it must hold\nmax{k \u2212 \u03c4,\u22121} \u2264 k\u0304i < k \u2200 k \u2265 0. (21)\nMoreover, by the definition of k\u0304i it holds that i /\u2208 Ak\u22121 \u222a \u00b7 \u00b7 \u00b7 \u222a Ak\u0304i+1, therefore we have that\n\u03bb k\u0304i+1 i = \u03bb k\u0304i+2 i = \u00b7 \u00b7 \u00b7 = \u03bbki , \u2200 i \u2208 Ak. (22)\nBy applying (22) to (19) and (20) (replacing \u03bbk\u0304i+1i with \u03bb k i ), we rewrite the master-point-of-view algorithm in Algorithm 3.\nInspired by [18], our analysis for Theorem 1 investigates how the augmented Lagrangian function, i.e.,\nL\u03c1(xk,xk0 ,\u03bbk) = N\u2211\ni=1\nfi(x k i ) + h(x k 0) +\nN\u2211\ni=1\n(\u03bbki ) T (xki \u2212 xk0)\n+ \u03c1\n2\nN\u2211\ni=1\n\u2016xki \u2212 xk0\u20162 (26)\nevolves with the iteration number k, where xk , [(xk1) T , . . . , (xkN ) T ]T and \u03bbk , [(\u03bbk1) T , . . . , (\u03bbkN ) T ]T . The following lemma is one of the keys to prove Theorem 1.\n7Note that k\u0304i = \u22121 for k = 0 and k\u0304i \u2265 \u22121 for k \u2265 0\nFebruary 22, 2016 DRAFT\n13\nLemma 1 Suppose that Assumption 2 holds and \u03c1 \u2265 L. Then, it holds that\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(xk,xk0 ,\u03bbk)\n\u2264 \u22122\u03b3 +N\u03c1 2 \u2016xk+10 \u2212 xk0\u20162\n+\n( 1\n\u03c1 +\n1 2\n) \u2211\ni\u2208Ak\n\u2016\u03bbk+1i \u2212 \u03bbki \u20162\n+ 1 + \u03c12\n2\n\u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212xk0\u20162\n+ (1\u2212 \u03c1) + L\n2\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212xki \u20162. (27)\nProof: See Appendix A.\nEquation (27) shows that L\u03c1(xk,xk0 ,\u03bbk) is not necessarily decreasing due to the error terms \u2211 i\u2208Ak \u2016\u03bbk+1i \u2212\n\u03bb k i \u20162 and \u2211 i\u2208Ak\n\u2016xk\u0304i+10 \u2212xk0\u20162. Next we bound the sizes of these two terms. First consider\n\u2211 i\u2208Ak \u2016\u03bbk+1i \u2212\u03bbki \u20162. Note from (24) and the optimality condition of (23) that, \u2200 i \u2208 Ak,\n0 = \u2207fi(xk+1i ) + \u03bbki + \u03c1(xk+1i \u2212 xk\u0304i+10 )\n= \u2207fi(xk+1i ) + \u03bbk+1i . (28)\nFor any i \u2208 Ack, denote k\u0303i < k as the last iteration number for which worker i is arrived. Then, i \u2208 Ak\u0303i and thus \u2207fi(xk\u0303i+1i ) + \u03bbk\u0303i+1i = 0. Since xk\u0303i+1i = xk\u0303i+2i = \u00b7 \u00b7 \u00b7 = xki = xk+1i and \u03bbk\u0303i+1i = \u03bbk\u0303i+2i = \u00b7 \u00b7 \u00b7 = \u03bbki = \u03bbk+1i , we obtain that \u2207fi(xk+1i ) + \u03bbk+1i = 0 \u2200i \u2208 Ack. Therefore, we conclude that\n\u2207fi(xk+1i ) + \u03bbk+1i = 0, \u2200 i \u2208 V and \u2200 k. (29)\nBy (29) and the Lipschitz continuity of \u2207fi (Assumption 2), we can bound\n\u2016\u03bbk+1i \u2212 \u03bbki \u20162 \u2264 \u2016\u2207fi(xk+1i )\u2212\u2207fi(xki )\u20162\n\u2264 L2\u2016xk+1i \u2212 xki \u20162, \u2200 i \u2208 V. (30)\nBy applying (30), we can further write (27) as\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\n\u2264 L\u03c1(xk,xk0 ,\u03bbk) + ( 1 + \u03c12\n2\n) \u2211\ni\u2208Ak\n\u2016xk0 \u2212 xk\u0304i+10 \u20162\n\u2212 ( 2\u03b3 +N\u03c1\n2\n) \u2016xk+10 \u2212 xk0\u20162\n+\n( L+ L2 + (1\u2212 \u03c1)\n2 +\nL2\n\u03c1\n) \u2211\ni\u2208Ak\n\u2016xk+1i \u2212 xki \u20162. (31)\nFebruary 22, 2016 DRAFT\n14\nFrom (31), one can observe that the error term (1+\u03c1 2 2 ) \u2211 i\u2208Ak \u2016xk0 \u2212 xk\u0304i+10 \u20162 is present due to the\nasynchrony of the network. The next lemma bounds this error term:\nLemma 2 Suppose that Assumption 1 holds and assume that |Ak| < S for all k, for some constant S \u2208 [1, N ]. Then, it holds that\nk\u2211\nj=0\n\u2211\ni\u2208Aj\n\u2016xj0 \u2212 x j\u0304i+1 0 \u20162 \u2264 S(\u03c4 \u2212 1)2\nk\u22121\u2211\nj=0\n\u2016xj+10 \u2212 x j 0\u20162. (32)\nProof: See Appendix B.\nThe last lemma shows that L\u03c1(xk,xk0,\u03bbk) is bounded below:\nLemma 3 Under Assumption 2 and for \u03c1 \u2265 L, it holds that\nL\u03c1(xk+1,xk+10 ,\u03bbk+1) \u2265 F \u22c6 > \u2212\u221e. (33)\nProof: See Appendix C.\nGiven the three lemmas above, we are ready to prove Theorem 1.\nProof of Theorem 1: Note that any KKT point ({x\u22c6i }Ni=1,x\u22c60, {\u03bb\u22c6i }Ni=1) of problem (4) satisfies the following conditions\n\u2207fi(x\u22c6i ) + \u03bb\u22c6i = 0, \u2200 i \u2208 V, (34a) s \u22c6 0 \u2212 \u2211N i=1 \u03bb \u22c6 i = 0, (34b) x \u22c6 i = x \u22c6 0, \u2200 i \u2208 V, (34c)\nwhere s\u22c60 \u2208 \u2202h(x\u22c60) denotes a subgradient of h at x\u22c60 and \u2202h(x\u22c60) is the subdifferential of h at x\u22c60. Since (34) also implies\nN\u2211\ni=1\n\u2207fi(x\u22c6)+s\u22c60 = 0, (35)\nwhere x\u22c6 , x\u22c60 = \u00b7 \u00b7 \u00b7 = x\u22c6N , x\u22c6 is also a stationary point of the original problem (1).\nFebruary 22, 2016 DRAFT\n15\nTo prove the desired result, we take a telescoping sum of (31), which yields\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(x0,x00,\u03bb0)\n\u2264 ( 1 + \u03c12\n2\n) k\u2211\nj=0\n\u2211\ni\u2208Aj\n\u2016xj0 \u2212 x j\u0304i+1 0 \u20162\n+\n( L+ L2 + (1\u2212 \u03c1)\n2 +\nL2\n\u03c1\n) k\u2211\nj=0\n\u2211\ni\u2208Aj\n\u2016xj+1i \u2212 x j i\u20162\n\u2212 ( 2\u03b3 +N\u03c1\n2\n) k\u2211\nj=0\n\u2016xj+10 \u2212 xj0\u20162. (36)\nBy substituting (32) in Lemma 2 into (36), we obtain ( 2\u03b3 +N\u03c1\u2212 S(1 + \u03c12)(\u03c4 \u2212 1)2\n2\n) k\u22121\u2211\nj=0\n\u2016xj+10 \u2212 x j 0\u20162\n+\n( (1\u2212 \u03c1)\u2212 (L+ L2)\n2 \u2212 L\n2\n\u03c1\n) k\u2211\nj=0\nN\u2211\ni=1\n\u2016xj+1i \u2212 x j i\u20162\n\u2264 L\u03c1(x0,x00,\u03bb0)\u2212 L\u03c1(xk+1,xk+10 ,\u03bbk+1) = (L\u03c1(x0,x00,\u03bb0)\u2212 F \u22c6)\u2212 (L\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 F \u22c6) \u2264 L\u03c1(x0,x00,\u03bb0)\u2212 F \u22c6 < \u221e, (37)\nwhere the second inequality is obtained by applying Lemma 3, and the last strict inequality is due to Assumption 2 where the optimal value F \u22c6 is assumed to be lower bounded.\nThen, (16) and (17) imply that the left hand side (LHS) of (37) is positive and increasing with k. Since\nthe RHS of (37) is finite, we must have, as k \u2192 \u221e,\nx k+1 0 \u2212 xk0 \u2192 0, xk+1i \u2212 xki \u2192 0, \u2200 i \u2208 V. (38)\nGiven (30), (38) infers\n\u03bb k+1 i \u2212 \u03bbki \u2192 0, \u2200i \u2208 V. (39)\nWe use (38) and (39) to show that every limit point of ({xki }Ni=1,xk0 , {\u03bbki }Ni=1) is a KKT point of problem (4). Firstly, by applying (39) to (24) and by (38), one obtains xk+10 \u2212 xk+1i \u2192 0 \u2200i \u2208 Ak. For i \u2208 Ack, note that i \u2208 Ak\u0303i (see the definition of k\u0303i above (29)) and thus, by (24),\n\u03bb k\u0303i+1 i = \u03bb k\u0303i i + \u03c1(x k\u0303i+1 i \u2212 x (k\u0303i)i+1 0 ),\nFebruary 22, 2016 DRAFT\n16\nwhere (k\u0303i)i denotes the last iteration number before iteration k\u0303i for which worker i is arrived. Moreover, since xk\u0303i+1i = x k\u0303i+2 i = \u00b7 \u00b7 \u00b7 = xki = xk+1i \u2200i \u2208 Ack, and by (24), (38) and (39), we have \u2200i \u2208 Ack,\n\u2016xk+10 \u2212 xk+1i \u2016 = \u2016xk+10 \u2212 xk\u0303i+1i \u2016\n= \u2016xk+10 \u2212 x (k\u0303i)i+1 0 + x (k\u0303i)i+1 0 \u2212 xk\u0303i+1i \u2016 \u2264 \u2016xk+10 \u2212 x (k\u0303i)i+1 0 \u2016+ 1\n\u03c1 \u2016\u03bbk\u0303i+1i \u2212 \u03bbk\u0303ii \u2016\n\u2192 0. (40)\nSo we conclude\nx k+1 0 \u2212 xk+1i \u2192 0 \u2200i \u2208 V. (41)\nSecondly, the optimality condition of (25) gives\ns k+1 0 \u2212\nN\u2211\ni=1\n\u03bb k+1 i \u2212 \u03c1\nN\u2211\ni=1\n(xk+1i \u2212 xk+10 )\n+ \u03b3(xk+10 \u2212 xk0) = 0, (42)\nfor some sk+10 \u2208 \u2202h(xk+10 ). By applying (41) and (38) to (42), we obtain that\ns k+1 0 \u2212\nN\u2211\ni=1\n\u03bb k+1 i \u2192 0. (43)\nEquations (29), (41) and (43) imply that ({xki }Ni=1,xk0 , {\u03bbki }Ni=1) asymptotically satisfy the KKT conditions in (34).\nLastly, let us show that ({xki }Ni=1,xk0 , {\u03bbki }Ni=1) is bounded and has limit points. Since dom(h) is compact and xk0 \u2208 dom(h), xk0 is a bounded sequence and thus has limit points. From (41), xki , i \u2208 V , are bounded and have limit points. Moreover, by (29), \u03bbki , i \u2208 V , are bounded and have limit points as well. In summary, ({xki }Ni=1,xk0 , {\u03bbki }Ni=1) converges to the set of KKT points of problem (4) .\nProof of Corollary 1: The proof exactly follows that of Theorem 1. The only difference is that the\ncoefficient of the term (1\u2212\u03c1)+L2 \u2211 i\u2208Ak \u2016xk+1i \u2212xki \u20162 in (27) reduces from (1\u2212\u03c1)+L 2 to (1\u2212\u03c1) 2 ; see the footnote in Appendix A.\nIV. COMPARISON WITH AN ALTERNATIVE SCHEME\nIn Algorithm 2, the workers compute (xi,\u03bbi), i \u2208 V , and the master is in charge of computing x0. While such distributed implementation is intuitive and natural, one may wonder whether there exist\nFebruary 22, 2016 DRAFT\n17\nother valid implementations, and if so, how they compare with Algorithm 2. To shed some light on this question, we consider in this section an alternative scheme in Algorithm 4.\nAlgorithm 4 differs from Algorithm 2 in that the master handles not only the update of x0 but also that of {\u03bbi}i\u2208V ; so the workers only updates {xi}. In essence, in a synchronous network, Algorithm 2 and Algorithm 4 are equivalent up to a change of update order8 and have the same convergence conditions. However, intriguingly, in an asynchronous network, the two algorithms may require distinct convergence conditions and behave very differently in practice. To analyze the convergence of Algorithm 4, we make the following assumption.\nAssumption 3 Each function fi is strongly convex with modulus \u03c32 > 0 and the function h is convex.\nUnder the strong convexity assumption, we are able to show the following convergence result for Algorithm 4.\nTheorem 2 Suppose that Assumption 1 and Assumption 3 hold true. Moreover, let \u03b3 = 0 and\n0 < \u03c1 \u2264 \u03c3 2\n(5\u03c4 \u2212 3)max{2\u03c4, 3(\u03c4 \u2212 1)} , (48)\nand define x\u0304ki = 1 k \u2211k \u2113=1 x k i \u2200i = 0, 1, . . . , N , where ({xki }Ni=1,xk0) are generated by (44) and (45). Then, it holds that \u2223\u2223\u2223\u2223 [ N\u2211\ni=1\nfi(x\u0304 k i ) + h(x\u0304 k 0)\n] \u2212 F \u22c6 \u2223\u2223\u2223\u2223+ N\u2211\ni=1\n\u2016x\u0304ki \u2212 x\u0304k0\u2016\n\u2264 (2 + \u03b4\u03bb)C k\n(49)\nfor all k, where C < \u221e is a finite constant and \u03b4\u03bb , max{\u2016\u03bb\u22c61\u2016, . . . , \u2016\u03bb\u22c6N\u2016}, in which {\u03bb\u22c6i } denote the optimal dual variables of (4).\nThe proof is presented in Appendix D. Theorem 2 somehow implies that Algorithm 4 may require stronger convergence conditions than Algorithm 2 in the asynchronous network, as fi\u2019s are assumed to be strongly convex. Besides, different from Theorem 1 where \u03c1 is advised to be large for Algorithm 2, Theorem 2 indicates that \u03c1 needs to be small for Algorithm 4. Since \u03c1 is the step size of the dual gradient ascent in (46), (48) implies that the master should move \u03bbi\u2019s slowly when \u03c4 is large. Such insight is reminiscent of the recent convergence results for multi-block ADMM in [33].\nInterestingly and surprisingly, our numerical results to be presented shortly suggest that the strongly\nconvex fi\u2019s and a small \u03c1 are necessary for the convergence of Algorithm 4.\n8Algorithm 2 under the synchronous protocol is the same as Algorithm 1 with the order of (6) and (7) interchanged.\nFebruary 22, 2016 DRAFT\n18\nV. SIMULATION RESULTS\nThe main purpose of this section is to examine the convergence behavior of the AD-ADMM with respect to the master\u2019s iteration number k. So, the simulation results to be presented are obtained by implementing Algorithm 3 on a desktop computer. First, we present the simulation results of the ADADMM for solving the non-convex sparse PCA problem. Second, we consider the LASSO problem and compare Algorithm 4 with Algorithm 2."}, {"heading": "A. Example 1: Sparse PCA", "text": "Theorem 1 has shown that the AD-ADMM can converge for non-convex problems. To verify this point,\nlet us consider the following sparse PCA problem [8]\nmin w\u2208Rn\n\u2212 N\u2211\nj=1\nw T B T j Bjw+\u03b8\u2016w\u20161, (50)\nwhere Bj \u2208 Rm\u00d7n, \u2200j = 1, . . . , N, and \u03b8 > 0 is a regularization parameter. The sparse PCA problem above is not a convex problem. We display in Figure 3 the convergence performance of the AD-ADMM for solving (50). In the simulations, each matrix Bj \u2208 Rn is a 1000 \u00d7 500 sparse random matrix with approximately 5000 non-zero entries; \u03b8 is set to 0.1 and N = 32. The penalty parameter \u03c1 is set to \u03c1 = \u03b2maxj=1,...,N \u03bbmax(B T j Bj) and \u03b3 = 0. To simulate an asynchronous scenario, at each iteration, half of the workers are assumed to have a probability 0.1 to be arrived independently, and half of the workers are assumed to have a probability 0.8 to be \u201carrived\u201d independently. At each iteration, the master proceeds to update the variables as long as there is at least one arrived worker, i.e., A = 1. The accuracy is defined as\naccuracy = |L\u03c1(xk,xk0,\u03bbk)\u2212 F\u0302 |\nF\u0302 (51)\nwhere F\u0302 denotes the optimal objective value for the synchronous case (\u03c4 = 1) which is obtained by running the distributed ADMM (with \u03b2 = 3) for 10000 iterations (it is found in the experiments that the AD-ADMM converges to the same KKT point for different values of \u03c4 ). One can observe from Figure 3 that the AD-ADMM (with \u03b2 = 3) indeed converges properly even though (50) is a non-convex problem.\nInterestingly, we note that for the example considered here, the AD-ADMM with \u03b3 = 0 works well for different values of \u03c4 , even though Theorem 1 suggests that \u03b3 should be a larger value in the worst-case. However, we do observe from Figure 3 that if one sets \u03b2 = 1.5 (i.e., a smaller value of \u03c1), then the AD-ADMM diverges even in the synchronous case (\u03c4 = 1). This implies that the claim of a large enough \u03c1 is necessary for the non-convex sparse PCA problem.\nFebruary 22, 2016 DRAFT\n19"}, {"heading": "B. Example 2: LASSO", "text": "In this example, we compare the convergence performance of Algorithm 4 with Algorithm 2. We\nconsider the following LASSO problem\nmin w\u2208Rn\nN\u2211\ni=1\n\u2016Aiw \u2212 bi\u20162 + \u03b8\u2016w\u20161, (52)\nwhere Ai \u2208 Rm\u00d7n, bi \u2208 Rm, i = 1, . . . , N , and \u03b8 > 0. The elements of Ai\u2019s are randomly generated following the Gaussian distribution with zero mean and unit variance, i.e., \u223c N (0, 1); each bi is generated by bi = Aiw0+\u03bdi where w0 \u2208 Rn is an n\u00d71 sparse random vector with approximately 0.05n non-zero entries and \u03bdi is a noise vector with entries following N (0, 0.01). A star network with 16 (N = 16) workers is considered. To simulate an asynchronous scenario, at each iteration, half of the workers are assumed to have a probability 0.1 to be arrived independently, 4 workers are assumed to have a probability 0.3 to be arrived independently, and the remaining 4 workers are assumed to have a probability 0.8 to be arrived independently.\nFigure 4(a) and Figure 4(b) respectively display the convergence curves (accuracy versus iteration number) of Algorithm 2 and Algorithm 4 for solving (52) with N = 16, m = 200, n = 100 and \u03b8 = 0.1. The accuracy is defined as\naccuracy = |L\u03c1(xk,xk0 ,\u03bbk)\u2212 F \u22c6|\nF \u22c6 (53)\nwhere F \u22c6 denotes the optimal objective value of problem (52). One can see from Figure 4(a) that Algorithm 2 (with \u03c1 = 500, \u03b3 = 0) converges well for various values of delay \u03c4 . From Figure 4(b),\nFebruary 22, 2016 DRAFT\n20\none can observe that, under the synchronous setting (i.e., \u03c4 = 1), Algorithm 4 (with \u03c1 = 500) exhibits a similar behavior as Algorithm 2 in Figure 4(a). However, under the asynchronous setting of \u03c4 = 3, Algorithm 4 (with \u03c1 = 500) diverges as shown in Figure 4(b); Algorithm 4 can become convergent if one decrease \u03c1 to 10. Analogously, for \u03c4 = 10, one has to further reduce \u03c1 to 1 in order to have Algorithm 4 convergent. However, the convergence speed of Algorithm 4 with \u03c1 = 1 is much slower when comparing to Algorithm 2 in Figure 4(a).\nFigure 4(c) and Figure 4(d) show the comparison results of Algorithm 2 and Algorithm 4 for solving\nFebruary 22, 2016 DRAFT\n21\n(52) with n increased to 1000. Note that, given m = 200 and n = 1000, the cost functions fi(wi) , \u2016Aiwi \u2212 bi\u20162 in (52) are no longer strongly convex. One can observe from Figure 4(c) that Algorithm 2 (with \u03c1 = 500, \u03b3 = 0) still converges properly for various values of \u03c4 . However, as one can see from Figure 4(d), Algorithm 4 always diverges for various values of \u03c1 even when the delay \u03c4 is as small as two. As a result, the strong convexity assumed in Theorem 2 may also be necessary in practice. We conclude from these simulation results that Algorithm 2 significantly outperforms Algorithm 4 in the asynchronous network, even though the two have the same convergence behaviors in the synchronous network.\nVI. CONCLUDING REMARKS\nIn this paper, we have proposed the AD-ADMM (Algorithm 2) aiming at solving large-scale instances of problem (1) over a star computer network. Under the partially asynchronous model, we have shown (in Theorem 1) that the AD-ADMM can deterministically converge to the set of KKT points of problem (4), even in the absence of convexity of fi\u2019s. We have also compared the AD-ADMM (Algorithm 2) with an alternative asynchronous implementation (Algorithm 4), and illustrated the interesting fact that a slight modification of the algorithm can significantly change the algorithm convergence conditions/behaviors in the asynchronous setting.\nFrom the presented simulation results, we have observed that the AD-ADMM may exhibit linear convergence for some structured instances of problem (1). The conditions under which linear convergence can be achieved are presented in the companion paper [25]. Numerical results which demonstrate the time efficiency of the proposed AD-ADMM on a high performance computer cluster are also presented in [25].\nAPPENDIX A\nPROOF OF LEMMA 1\nNotice that\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(xk,xk0 ,\u03bbk)\n= L\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(xk+1,xk0 ,\u03bbk+1)\n+ L\u03c1(xk+1,xk0 ,\u03bbk+1)\u2212 L\u03c1(xk+1,xk0 ,\u03bbk) + L\u03c1(xk+1,xk0 ,\u03bbk)\u2212 L\u03c1(xk,xk0 ,\u03bbk). (A.1)\nFebruary 22, 2016 DRAFT\n22\nWe bound the three pairs of the differences on the right hand side (RHS) of (A.1) as follows. Firstly, since \u2212xT0 \u2211N i=1 \u03bb k+1 i + \u03c1 2 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162 + \u03b32\u2016x0 \u2212 xk0\u20162 in (25) is strongly convex with respect to (w.r.t.) x0 with modulus \u03b3 +N\u03c1, by [34, Definition 2.1.2], we have ( \u2212 (xk0)T N\u2211\ni=1\n\u03bb k+1 i +\n\u03c1 2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk0\u20162 )\n\u2212 ( \u2212 (xk+10 )T N\u2211\ni=1\n\u03bb k+1 i\n+ \u03c1\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk+10 \u20162 + \u03b3\n2 \u2016xk+10 \u2212 xk0\u20162\n)\n\u2265 ( \u2212 N\u2211\ni=1\n\u03bb k+1 i + \u03c1\nN\u2211\ni=1\n(xk+10 \u2212 xk+1i )\n+ \u03b3(xk+10 \u2212 xk0) )T (xk0 \u2212 xk+10 ) + \u03b3 +N\u03c1\n2 \u2016xk+10 \u2212 xk0\u20162. (A.2)\nBy the optimality condition of (25) and the convexity of h, we respectively have ( s k+1 0 \u2212 N\u2211\ni=1\n\u03bb k+1 i + \u03c1\nN\u2211\ni=1\n(xk+10 \u2212 xk+1i )\n+ \u03b3(xk+10 \u2212 xk0) )T (xk0 \u2212 xk+10 ) \u2265 0, (A.3)\nh(xk0) \u2265 h(xk+10 ) + (sk+10 )T (xk0 \u2212 xk+10 ). (A.4)\nBy subsequently applying (A.3) and (A.4) to (A.2), we obtain ( h(xk0)\u2212 (xk0)T N\u2211\ni=1\n\u03bb k+1 i +\n\u03c1 2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk0\u20162 )\n\u2212 ( h(xk+10 )\u2212 (xk+10 )T N\u2211\ni=1\n\u03bb k+1 i\n+ \u03c1\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk+10 \u20162 + \u03b3\n2 \u2016xk+10 \u2212 xk0\u20162\n)\n\u2265 \u03b3 +N\u03c1 2 \u2016xk+10 \u2212 xk0\u20162, (A.5)\nthat is,\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(xk+1,xk0 ,\u03bbk+1)\n\u2264 \u22122\u03b3 +N\u03c1 2 \u2016xk+10 \u2212 xk0\u20162. (A.6)\nFebruary 22, 2016 DRAFT\n23\nSecondly, it directly follows from (26) that\nL\u03c1(xk+1,xk0 ,\u03bbk+1)\u2212 L\u03c1(xk+1,xk0 ,\u03bbk)\n=\nN\u2211\ni=1\n(\u03bbk+1i \u2212 \u03bbki )T (xk+1i \u2212 xk0)\n= \u2211\ni\u2208Ak\n(\u03bbk+1i \u2212 \u03bbki )T (xk+1i \u2212 xk\u0304i+10 )\n+ \u2211\ni\u2208Ak\n(\u03bbk+1i \u2212 \u03bbki )T (xk\u0304i+10 \u2212 xk0)\n= 1\n\u03c1\n\u2211\ni\u2208Ak\n\u2016\u03bbk+1i \u2212 \u03bbki \u20162\n+ \u2211\ni\u2208Ak\n(\u03bbk+1i \u2212 \u03bbki )T (xk\u0304i+10 \u2212 xk0), (A.7)\nwhere the second equality is due to the fact that \u03bbk+1i = \u03bb k i \u2200i \u2208 Ack and the last equality is obtained by applying\n\u03bb k+1 i = \u03bb k i + \u03c1(x k+1 i \u2212 xk\u0304i+10 ) \u2200i \u2208 Ak (A.8)\nas shown in (24).\nThirdly, define Li(xi,xk0 ,\u03bbk) = fi(xi) + xTi \u03bbki + \u03c12\u2016xi \u2212 xk0\u20162 and assume that \u03c1 \u2265 L. Since, by [34, Lemma 1.2.2], the minimum eigenvalue of the Hessian matrix of fi(xi) is no smaller than \u2212L, Li(xi,xk0 ,\u03bbk) is strongly convex w.r.t. xi and the convexity parameter is given by \u03c1\u2212L \u2265 0 9. Therefore, one has\nLi(xki ,xk0 ,\u03bbk) \u2265 Li(xk+1i ,xk0 ,\u03bbk)\n+ (\u2207fi(xk+1i ) + \u03bbki + \u03c1(xk+1i \u2212 xk0))T (xki \u2212 xk+1i ) + \u03c1\u2212 L 2 \u2016xk+1i \u2212 xki \u20162. (A.9)\nAlso, by the optimality condition of (23), one has, \u2200i \u2208 Ak,\n0 = \u2207fi(xk+1i ) + \u03bbki + \u03c1(xk+1i \u2212 xk\u0304i+10 ) (A.10)\n= (\u2207fi(xk+1i ) + \u03bbki + \u03c1(xk+1i \u2212 xk0))\n+ \u03c1(xk0 \u2212 xk\u0304i+10 ). (A.11)\n9When fi is a convex function, the minimum eigenvalue of the Hessian matrix of fi(xi) is zero. So, the convexity parameter\nof Li(xi,\u03bbk,xk0) is \u03c1 instead.\nFebruary 22, 2016 DRAFT\n24\nBy substituting (A.11) into (A.9) and by (26), we have\nL\u03c1(xk+1,xk0 ,\u03bbk)\u2212 L\u03c1(xk,xk0 ,\u03bbk)\n=\nN\u2211\ni=1\n(Li(xk+1i ,\u03bbk,xk0)\u2212 Li(xki ,\u03bbk,xk0))\n= \u2211\ni\u2208Ak\n(Li(xk+1i ,\u03bbk,xk0)\u2212 Li(xki ,\u03bbk,xk0))\n\u2264 \u2212\u03c1\u2212 L 2\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 xki \u20162\n+ \u03c1 \u2211\ni\u2208Ak\n(xk\u0304i+10 \u2212 xk0)T (xk+1i \u2212 xki ), (A.12)\nwhere the second equality is due to xk+1i = x k i \u2200i \u2208 Ack from (23).\nAfter substituting (A.6), (A.7) and (A.12) into (A.1), we obtain\nL\u03c1(xk+1,xk+10 ,\u03bbk+1)\u2212 L\u03c1(xk,xk0 ,\u03bbk)\n\u2264 \u22122\u03b3 +N\u03c1 2 \u2016xk+10 \u2212 xk0\u20162 + 1 \u03c1\n\u2211\ni\u2208Ak\n\u2016\u03bbk+1i \u2212 \u03bbki \u20162\n\u2212 \u03c1\u2212 L 2\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212xki \u20162\n+ \u2211\ni\u2208Ak\n(\u03bbk+1i \u2212 \u03bbki )T (xk\u0304i+10 \u2212 xk0)\n+\u03c1 \u2211\ni\u2208Ak\n(xk\u0304i+10 \u2212xk0)T (xk+1i \u2212 xki ). (A.13)\nRecall the Young\u2019s inequality, i.e.,\na T b \u2264 1 2\u03b4 \u2016a\u20162 + \u03b4 2 \u2016b\u20162, (A.14)\nfor any a, b and \u03b4 > 0, and apply it to the fourth and fifth terms in the RHS of (A.13) with \u03b4 = 1 and \u03b4 = 1/\u03c1 for some \u01eb > 0, respectively. Then (27) is obtained.\nFebruary 22, 2016 DRAFT\n25\nAPPENDIX B\nPROOF OF LEMMA 2\nIt is easy to show that\nk\u2211\nj=0\n\u2211\ni\u2208Aj\n\u2016xj0 \u2212 x j\u0304i+1 0 \u20162 =\nk\u2211\nj=0\n\u2211\ni\u2208Aj\n\u2016 j\u22121\u2211\n\u2113=j\u0304i+1\n(x\u21130 \u2212 x\u2113+10 )\u20162\n\u2264 k\u2211\nj=0\n\u2211\ni\u2208Aj\n(j \u2212 j\u0304i \u2212 1) j\u22121\u2211\n\u2113=j\u0304i+1\n\u2016x\u21130 \u2212 x\u2113+10 \u20162\n\u2264 k\u2211\nj=0\n\u2211\ni\u2208Aj\n(\u03c4 \u2212 1) j\u22121\u2211\n\u2113=j\u2212\u03c4+1\n\u2016x\u21130 \u2212 x\u2113+10 \u20162\n\u2264 S(\u03c4 \u2212 1) k\u2211\nj=0\nj\u22121\u2211\n\u2113=j\u2212\u03c4+1\n\u2016x\u21130 \u2212 x\u2113+10 \u20162 (A.15)\nwhere, in the second inequality, we have applied the fact of j \u2212 \u03c4 \u2264 j\u0304i < j from (21); in the last inequality, we have applied the assumption of |Ak| < S for all k. Notice that, in the summation \u2211k\nj=0 \u2211j\u22121 \u2113=j\u2212\u03c4+1 \u2016x\u21130 \u2212 x\u2113+10 \u20162, each \u2016x j 0 \u2212 xj+10 \u20162, where j = 0, . . . , k \u2212 1, appears no more than\n\u03c4 \u2212 1 times. Thus, one can upper bound k\u2211\nj=0\nj\u22121\u2211\n\u2113=j\u2212\u03c4+1\n\u2016x\u21130 \u2212 x\u2113+10 \u20162 \u2264 (\u03c4 \u2212 1) k\u22121\u2211\nj=0\n\u2016xj+10 \u2212 xj0\u20162, (A.16)\nwhich, combined with (A.15), yields (32).\nAPPENDIX C\nPROOF OF LEMMA 3\nThe proof is similar to [18, Lemma 2.3]. We present the proof here for completeness. By recalling\nequation (29) and applying it to (26), one obtains\nL\u03c1(xk+1,xk+10 ,\u03bbk+1) = h(xk+10 ) + N\u2211\ni=1\nfi(x k+1 i )\n\u2212 N\u2211\ni=1\n(\u2207fi(xk+1i ))T (xk+1i \u2212 xk+10 ) + \u03c1\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk+10 \u20162. (A.17)\nAs \u2207fi is Lipschitz continuous under Assumption 2, the descent lemma [36, Proposition A.24] holds\nfi(x k+1 0 ) \u2264 fi(xk+1i ) + (\u2207fi(xk+1i ))T (xk+10 \u2212 xk+1i )\n+ L\n2 \u2016xk+1i \u2212 xk+10 \u20162 \u2200 i = 1, . . . , N. (A.18)\nFebruary 22, 2016 DRAFT\n26\nBy combining (A.17) and (A.18), one can lower bound L\u03c1(xk+1,xk+10 ,\u03bbk+1) as\nL\u03c1(xk+1,xk+10 ,\u03bbk+1) \u2265 h(xk+10 ) + N\u2211\ni=1\nfi(x k+1 0 )\n+ \u03c1\u2212 L 2\nN\u2211\ni=1\n\u2016xk+1i \u2212 xk+10 \u20162, (A.19)\nwhich implies (33) given \u03c1 \u2265 L and under Assumption 2.\nAPPENDIX D\nPROOF OF THEOREM 2\nFor ease of analysis, we equivalently write Algorithm 4 as follows: For iteration k = 0, 1, . . . ,\nx k+1 i =\n{ arg min xi fi(xi) + x T i \u03bb k\u0304i+1 i + \u03c1 2\u2016xi \u2212 x k\u0304i+1 0 \u20162, \u2200i \u2208 Ak\nx k i \u2200i \u2208 Ack\n, (A.20)\nx k+1 0 = arg min\nx0\nh(x0)\u2212 xT0 \u2211N i=1 \u03bb k i + \u03c1 2 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162, (A.21)\n\u03bb k+1 i = \u03bb k i + \u03c1(x k+1 i \u2212 xk+10 ) \u2200i \u2208 V. (A.22)\nHere, k\u0304i is the last iteration number for which the master node receives message from worker i \u2208 Ak before iteration k. For i \u2208 Ack, let us denote k\u0303i (k\u2212\u03c4 < k\u0303i < k) as the last iteration number for which the master node receives message from worker i before iteration k, and further denote k\u0302i (k\u0303i \u2212 \u03c4 \u2264 k\u0302i < k\u0303i) as the last iteration number for which the master node receives message from worker i before iteration k\u0303i. Then, by (A.20), it must be\nx k\u0303i+1 i = arg min\nxi\nfi(xi) + x T i \u03bb k\u0302i+1 i + \u03c1 2\u2016xi \u2212 x k\u0302i+1 0 \u20162 \u2200i \u2208 Ack, (A.23)\nx k+1 i = x k\u0303i+1 i , (A.24)\nwhere the second equation is due to xk\u0303i+1i = x k\u0303i+2 i = \u00b7 \u00b7 \u00b7 = xki = xk+1i \u2200i \u2208 Ack.\nLet us consider the following update steps\nx k+1 i =\n{ arg min xi \u03b1fi(xi) + x T i \u03bb\u0303 k\u0304i+1 i + \u03b2 2 \u2016xi \u2212 x k\u0304i+1 0 \u20162, \u2200i \u2208 Ak\narg min xi\n\u03b1fi(xi) + x T i \u03bb\u0303 k\u0302i+1 i + \u03b2 2 \u2016xi \u2212 x k\u0302i+1 0 \u20162 \u2200i \u2208 Ack\n, (A.25)\nx k+1 0 = arg min\nx0\n\u03b1h(x0)\u2212 xT0 \u2211N i=1 \u03bb\u0303 k i + \u03b2 2 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162, (A.26)\n\u03bb\u0303 k+1 i = \u03bb\u0303 k i + \u03b2(x k+1 i \u2212 xk+10 ) \u2200i \u2208 V, (A.27)\nwhere \u03b1, \u03b2 > 0. One can verify that (A.25)-(A.27) are equivalent to (A.20)-(A.22) and (A.23)-(A.24) if one considers the change of variables \u03bbi = \u03bb\u0303i/\u03b1 and \u03c1 = \u03b2/\u03b1.\nFebruary 22, 2016 DRAFT\n27\nWe first consider the optimality condition of (A.25) for i \u2208 Ak:\n0 \u2265 \u03b1\u2202fi(xk+1i )T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k\u0304i+1i + \u03b2(xk+1i \u2212 xk\u0304i+10 ))T (xk+1i \u2212 x\u22c6i )\n= \u03b1\u2202fi(x k+1 i ) T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k+1i )T (xk+1i \u2212 x\u22c6i )\n+ (\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) + \u03b2(xk+10 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ), (A.28)\nwhere we have applied (A.27) to obtain the equality. Since, under Assumption 3, fi is strongly convex, one has\n\u03b1fi(x \u22c6 i ) \u2265 \u03b1fi(xk+1i ) + \u03b1\u2202fi(xk+1i )T (x\u22c6i \u2212 xk+1i ) +\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162. (A.29)\nCombining (A.28) and (A.29) gives rise to\n\u03b1fi(x k+1 i )\u2212 \u03b1fi(x\u22c6i ) + \u03bb\u0303Ti (xk+1i \u2212 x\u22c6i ) +\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n+ (\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) + \u03b2(xk+10 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) \u2264 0 \u2200i \u2208 Ak. (A.30)\nOn the other hand, consider the optimality condition of (A.25) for i \u2208 Ack:\n0 \u2265 \u03b1\u2207fi(xk+1i )T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k\u0302i+1i + \u03b2(xk+1i \u2212 xk\u0302i+10 ))T (xk+1i \u2212 x\u22c6i )\n= \u03b1\u2207fi(xk+1i )T (xk+1i \u2212 x\u22c6i )\n+ (\u03bb\u0303k\u0302i+1i + \u03bb\u0303 k\u0303i+1 i \u2212 \u03bb\u0303k\u0303ii \u2212 \u03b2(xk\u0303i+1i \u2212 xk\u0303i+10 ) + \u03b2(xk\u0303i+1i \u2212 xk\u0302i+10 ))T (xk+1i \u2212 x\u22c6i )\n= \u03b1\u2207fi(xk+1i )T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k\u0303i+1i )T (xk+1i \u2212 x\u22c6i )\n+ (\u03bb\u0303k\u0302i+1i \u2212 \u03bb\u0303k\u0303i)T (xk+1i \u2212 x\u22c6i ) + \u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 )T (xk+1i \u2212 x\u22c6i ), (A.31)\nwhere (A.27) with k = k\u0303i and (A.24) are used to obtain the first equality. By combining (A.29) with (A.31), one obtains\n\u03b1fi(x k+1 i )\u2212 \u03b1fi(x\u22c6i ) + \u03bb\u0303Ti (xk+1i \u2212 x\u22c6i ) +\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n+ (\u03bb\u0303k\u0303i+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212 x\u22c6i ) + (\u03bb\u0303k\u0302i+1i \u2212 \u03bb\u0303k\u0303i)T (xk+1i \u2212 x\u22c6i ) + \u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 )T (xk+1i \u2212 x\u22c6i ) \u2264 0 \u2200i \u2208 Ack. (A.32)\nBy summing (A.30) for all i \u2208 Ak and (A.32) for all i \u2208 Ack and further summing the resultant two\nFebruary 22, 2016 DRAFT\n28\nterms, we obtain that\n\u03b1\nN\u2211\ni=1\nfi(x k+1 i )\u2212 \u03b1\nN\u2211\ni=1\nfi(x \u22c6 i ) +\nN\u2211\ni=1\n\u03bb\u0303 T i (x k+1 i \u2212 x\u22c6i ) +\nN\u2211\ni=1\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n+ \u2211\ni\u2208Ak\n(\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack (\u03bb\u0303k\u0303i+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212 x\u22c6i ) \ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ \u2211\ni\u2208Ak\n(\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack\n(\u03bb\u0303k\u0302i+1i \u2212 \u03bb\u0303k\u0303i)T (xk+1i \u2212 x\u22c6i )\n+ \u2211\ni\u2208Ak\n\u03b2(xk+10 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack \u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 )T (xk+1i \u2212 x\u22c6i ) \ufe38 \ufe37\ufe37 \ufe38\n(b)\n\u2264 0. (A.33)\nThe term (a) in (A.33), after adding and subtracting \u2211\ni\u2208Ack (\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212x\u22c6i ), can be written as\n(a) =\nN\u2211\ni=1\n(\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack\n(\u03bb\u0303k\u0303i+1i \u2212 \u03bb\u0303k+1i )T (xk+1i \u2212 x\u22c6i ). (A.34)\nThe term (b) in (A.33) can be expressed as\n(b) = \u2211\ni\u2208Ak\n\u03b2(xk+10 \u2212 xk0 + xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack\n\u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 )T (xk+1i \u2212 x\u22c6i )\n=\nN\u2211\ni=1\n\u03b2(xk+10 \u2212 xk0)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ack\n\u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 \u2212 xk+10 + xk0)T (xk+1i \u2212 x\u22c6i )\n+ \u2211\ni\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ). (A.35)\nNote that, by applying (A.27) and the fact of x\u22c6i = x \u22c6 0 \u2200i \u2208 V , one can write\nN\u2211\ni=1\n\u03b2(xk+10 \u2212 xk0)T (xk+1i \u2212 x\u22c6i ) = N\u2211\ni=1\n\u03b2(xk+10 \u2212 xk0)T (xk+1i \u2212 xk+10 + xk+10 \u2212 x\u22c6i )\n=\nN\u2211\ni=1\n(xk+10 \u2212 xk0)T (\u03bb\u0303k+1i \u2212 \u03bb\u0303ki ) +N\u03b2(xk+10 \u2212 xk0)T (xk+10 \u2212 x\u22c60).\n(A.36)\nSo, The term (b) in (A.35) is given by\n(b) =\nN\u2211\ni=1\n(xk+10 \u2212 xk0)T (\u03bb\u0303k+1i \u2212 \u03bb\u0303ki ) +N\u03b2(xk+10 \u2212 xk0)T (xk+10 \u2212 x\u22c60)\n+ \u2211\ni\u2208Ack\n\u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 \u2212 xk+10 + xk0)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ).\n(A.37)\nFebruary 22, 2016 DRAFT\n29\nIt can be shown that N\u2211\ni=1\n(xk+10 \u2212 xk0)T (\u03bb\u0303k+1i \u2212 \u03bb\u0303ki ) \u2265 0. (A.38)\nTo see this, consider the optimality condition of (A.26): \u2200x0 \u2208 Rn,\n0 \u2265 \u03b1h(xk+10 )\u2212 \u03b1h(x0)\u2212 N\u2211\ni=1\n(\u03bb\u0303ki + \u03b2(x k+1 i \u2212 xk+10 ))T (xk+10 \u2212 x0)\n= \u03b1h(xk+10 )\u2212 \u03b1h(x0)\u2212 N\u2211\ni=1\n(\u03bb\u0303k+1i ) T (xk+10 \u2212 x0), (A.39)\nwhere the equality is due to (A.27). By letting x0 = xk0 in (A.39) and also considering (A.39) for iteration k and x0 = x k+1 0 , we have\n0 \u2265 \u03b1h(xk+10 )\u2212 \u03b1h(xk0)\u2212 N\u2211\ni=1\n(\u03bb\u0303k+1i ) T (xk+10 \u2212 xk0),\n0 \u2265 \u03b1h(xk0)\u2212 \u03b1h(xk+10 )\u2212 N\u2211\ni=1\n(\u03bb\u0303ki ) T (xk0 \u2212 xk+10 ), (A.40)\nrespectively. By summing the above two equations, we obtain (A.38). Moreover, by letting x0 = x\u22c6i = x \u22c6 0 in (A.39), we have\n\u03b1h(xk+10 )\u2212 \u03b1h(x\u22c60)\u2212 N\u2211\ni=1\n\u03bb\u0303 T i (x k+1 0 \u2212 x\u22c6i )\u2212\nN\u2211\ni=1\n(\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (xk+10 \u2212 x\u22c6i ) \u2264 0. (A.41)\nBy summing (A.41) and (A.33) followed by applying (A.34), (A.37) and (A.38), one obtains\n\u03b1\nN\u2211\ni=1\nfi(x k+1 i ) + \u03b1h(x k+1 0 )\u2212 \u03b1\nN\u2211\ni=1\nfi(x \u22c6 i )\u2212 \u03b1h(x\u22c60) +\nN\u2211\ni=1\n\u03bb\u0303 T i (x k+1 i \u2212 xk+10 ) +\nN\u2211\ni=1\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n+ 1\n\u03b2\nN\u2211\ni=1\n(\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (\u03bb\u0303k+1i \u2212 \u03bb\u0303ki ) +N\u03b2(xk+10 \u2212 xk0)T (xk+10 \u2212 x\u22c60)\n+ \u2211\ni\u2208Ack\n(\u03bb\u0303k\u0303i+1i \u2212 \u03bb\u0303k+1i + \u03bb\u0303k\u0302i+1i \u2212 \u03bb\u0303k\u0303i)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ak\n(\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i )\n+ \u2211\ni\u2208Ack\n\u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 \u2212 xk+10 + xk0)T (xk+1i \u2212 x\u22c6i ) + \u2211\ni\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) \u2264 0,\n(A.42)\nwhere the seventh term in the LHS is obtained by applying (A.27).\nFebruary 22, 2016 DRAFT\n30\nWe sum (A.42) for k = 0, . . . ,K \u2212 1 and take the average, which yields\n\u03b1 K\nK\u22121\u2211\nk=0\n[ N\u2211\ni=1\nfi(x k+1 i ) + h(x k+1 0 )\n] \u2212 \u03b1 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + 1\nK\nK\u22121\u2211\nk=0\nN\u2211\ni=1\n\u03bb\u0303 T i (x k+1 i \u2212 xk+10 )\n+ 1\n\u03b2K\nK\u22121\u2211\nk=0\nN\u2211\ni=1 (\u03bb\u0303k+1i \u2212 \u03bb\u0303i)T (\u03bb\u0303k+1i \u2212 \u03bb\u0303ki ) \ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ N\u03b2\nK\nK\u22121\u2211\nk=0 (xk+10 \u2212 xk0)T (xk+10 \u2212 x\u22c60) \ufe38 \ufe37\ufe37 \ufe38\n(b)\n\u2264 \u2212 1 K\nK\u22121\u2211\nk=0\nN\u2211\ni=1\n\u03b1\u03c32\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n+ 1\nK\nK\u22121\u2211\nk=0\n( \u2212 \u2211\ni\u2208Ack\n(\u03bb\u0303k\u0303i+1i \u2212 \u03bb\u0303k+1i + \u03bb\u0303k\u0302i+1i \u2212 \u03bb\u0303k\u0303i)T (xk+1i \u2212 x\u22c6i )\u2212 \u2211\ni\u2208Ak\n(\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) )\n\ufe38 \ufe37\ufe37 \ufe38 (c)\n+ 1\nK\nK\u22121\u2211\nk=0\n( \u2212 \u2211\ni\u2208Ack\n\u03b2(xk\u0303i+10 \u2212 xk\u0302i+10 \u2212 xk+10 + xk0)T (xk+1i \u2212 x\u22c6i )\u2212 \u2211\ni\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) )\n\ufe38 \ufe37\ufe37 \ufe38 (d)\n.\n(A.43)\nIt is easy to see that term (a)\n(a) = 1\n2\nK\u22121\u2211\nk=0\n( \u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303i\u20162 \u2212 \u2016\u03bb\u0303ki \u2212 \u03bb\u0303i\u20162 + \u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162 )\n= 1\n2 \u2016\u03bb\u0303Ki \u2212 \u03bb\u0303i\u20162 \u2212\n1 2 \u2016\u03bb\u03030i \u2212 \u03bb\u0303i\u20162 + 1 2\nK\u22121\u2211\nk=0\n\u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162, (A.44)\nand similarly, term (b)\n(b) = 1\n2\nK\u22121\u2211\nk=0\n( \u2016xk+10 \u2212 x\u22c60\u20162 \u2212 \u2016xk0 \u2212 x\u22c60\u20162 + \u2016xk+10 \u2212 xk0\u20162 )\n= 1\n2 \u2016xK0 \u2212 x\u22c60\u20162 \u2212\n1 2 \u2016x00 \u2212 x\u22c60\u20162 + 1 2\nK\u22121\u2211\nk=0\n\u2016xk+10 \u2212 xk0\u20162. (A.45)\nFebruary 22, 2016 DRAFT\n31\nNotice that one can bound the term \u2211K\u22121\nk=0 \u2211 i\u2208Ak\n(\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) in (c) as follows K\u22121\u2211\nk=0\n\u2211\ni\u2208Ak\n(\u03bb\u0303k\u0304i+1i \u2212 \u03bb\u0303ki )T (xk+1i \u2212 x\u22c6i ) = K\u22121\u2211\nk=0\n\u2211\ni\u2208Ak\nk\u22121\u2211\n\u2113=k\u0304i+1\n(\u03bb\u0303\u2113i \u2212 \u03bb\u0303\u2113+1i )T (xk+1i \u2212 x\u22c6i )\n\u2264 K\u22121\u2211\nk=0\n\u2211\ni\u2208Ak\nk\u22121\u2211\n\u2113=k\u2212\u03c4+1\n\u2016\u03bb\u0303\u2113i \u2212 \u03bb\u0303\u2113+1i \u2016 \u00b7 \u2016xk+1i \u2212 x\u22c6i \u2016\n\u2264 N\u2211\ni=1\nK\u22121\u2211\nk=0\nk\u22121\u2211\n\u2113=k\u2212\u03c4+1\n( 1\n2\u03b22 \u2016\u03bb\u0303\u2113i \u2212 \u03bb\u0303\u2113+1i \u20162 +\n\u03b22\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n) (A.46)\n\u2264 N\u2211\ni=1\nK\u22121\u2211\nk=0\n( \u03c4 \u2212 1 2\u03b22 \u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162 + (\u03c4 \u2212 1)\u03b22 2 \u2016xk+1i \u2212 x\u22c6i \u20162 ) , (A.47)\nwhere the second inequality is obtained by applying the Young\u2019s inequality:\na T b \u2264 1 2\u03b4 \u2016a\u20162 + \u03b4 2 \u2016b\u20162 (A.48)\nfor any a, b and \u03b4 > 0; the last inequality is caused by the fact that the term \u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162 for each k does not appear more than \u03c4 \u2212 1 times in the RHS of (A.46). By applying a similar idea to the first term of (c) and by (A.47), one eventually can bound (c) as follows\n(c) \u2264 3(\u03c4 \u2212 1) 2\u03b22\nN\u2211\ni=1\nK\u22121\u2211\nk=0\n\u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162 + 3(\u03c4 \u2212 1)\u03b22\n2\nN\u2211\ni=1\nK\u22121\u2211\nk=0\n\u2016xk+1i \u2212 x\u22c6i \u20162. (A.49)\nSimilarly, the term \u2211K\u22121\nk=0 \u2211 i\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) in (d) can be upper bounded as follows K\u22121\u2211\nk=0\n\u2211\ni\u2208Ak\n\u03b2(xk0 \u2212 xk\u0304i+10 )T (xk+1i \u2212 x\u22c6i ) \u2264 K\u22121\u2211\nk=0\n\u2211\ni\u2208Ak\nk\u22121\u2211\n\u2113=k\u2212\u03c4+1\n\u03b2\u2016xk0 \u2212 xk\u0304i+10 \u2016 \u00b7 \u2016xk+1i \u2212 x\u22c6i \u2016\n\u2264 N\u2211\ni=1\nK\u22121\u2211\nk=0\nk\u22121\u2211\n\u2113=k\u2212\u03c4+1\n( 1\n2 \u2016xk+10 \u2212 xk0\u20162 +\n\u03b22\n2 \u2016xk+1i \u2212 x\u22c6i \u20162\n) (A.50)\n\u2264 N\u2211\ni=1\nK\u22121\u2211\nk=0\n( \u03c4 \u2212 1 2 \u2016xk+10 \u2212 xk0\u20162 + (\u03c4 \u2212 1)\u03b22 2 \u2016xk+1i \u2212 x\u22c6i \u20162 ) . (A.51)\nBy applying a similar idea to the first term of (d) and by (A.51), one can bound (d) as follows\n(d) \u2264 N\u2211\ni=1\nK\u22121\u2211\nk=0\n( \u03c4\u2016xk+10 \u2212 xk0\u20162 + \u03c4\u03b22\u2016xk+1i \u2212 x\u22c6i \u20162 ) . (A.52)\nFebruary 22, 2016 DRAFT\n32\nAfter substituting (A.44), (A.45), (A.49) and (A.52) into (A.43), we obtain that\n\u03b1\n[ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 \u03b1 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + N\u2211\ni=1\n\u03bb\u0303 T i (x\u0304 K i \u2212 x\u0304K0 )\n\u2264 \u03b1 K\nK\u22121\u2211\nk=0\n[ N\u2211\ni=1\nfi(x k+1 i ) + h(x k+1 0 )\n] \u2212 \u03b1 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + 1\nK\nK\u22121\u2211\nk=0\nN\u2211\ni=1\n\u03bb\u0303 T i (x k+1 i \u2212 xk+10 )\n\u2264 1 2\u03b2K\nN\u2211\ni=1\n\u2016\u03bb\u03030i \u2212 \u03bb\u0303i\u20162 \u2212 1\n2\u03b2K\nN\u2211\ni=1\n\u2016\u03bb\u0303Ki \u2212 \u03bb\u0303i\u20162 + N\u03b2\n2K \u2016x00 \u2212 x\u22c60\u20162 \u2212\nN\u03b2 2K \u2016xK0 \u2212 x\u22c60\u20162\n+ ( 3(\u03c4 \u2212 1) 2K\u03b22 \u2212 1 2\u03b2K ) N\u2211\ni=1\nK\u22121\u2211\nk=0\n\u2016\u03bb\u0303k+1i \u2212 \u03bb\u0303ki \u20162 + ( N\u03c4\nK \u2212 N\u03b2 2K\n)K\u22121\u2211\nk=0\n\u2016xk+10 \u2212 xk0\u20162\n+ 1\nK\nK\u22121\u2211\nk=0\nN\u2211\ni=1\n( 3(\u03c4 \u2212 1)\u03b22 + 2\u03c4\u03b22 \u2212 \u03b1\u03c32\n2\n) \u2016xk+1i \u2212 x\u22c6i \u20162 (A.53)\nwhere the first inequality is by the convexity of fi\u2019s and h.\nAccording to (A.53), by choosing\n\u03b2 \u2265 max{2\u03c4, 3(\u03c4 \u2212 1)}, \u03b1 \u2265 (5\u03c4 \u2212 3)\u03b2 2\n\u03c32 , (A.54)\nand recalling that \u03bbi = \u03bb\u0303i/\u03b1 and \u03c1 = \u03b2/\u03b1, one can obtain [ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + N\u2211\ni=1\n\u03bb T i (x\u0304 K i \u2212 x\u0304K0 )\n\u2264 1 2\u03c1K\nN\u2211\ni=1\n\u2016\u03bb0i \u2212 \u03bbi\u20162 + N\u03c1\n2K \u2016x00 \u2212 x\u22c60\u20162. (A.55)\nNote that (A.54) is equivalent to\n\u03c1 = \u03b2/\u03b1 \u2264 \u03c3 2 (5\u03c4 \u2212 3)\u03b2 \u2264 \u03c32 (5\u03c4 \u2212 3)max{2\u03c4, 3(\u03c4 \u2212 1)} . (A.56)\nNow, let \u03bbi = \u03bb\u22c6i + x\u0304\nK i \u2212x\u0304 K 0\n\u2016x\u0304Ki \u2212x\u0304 K 0 \u2016 \u2200i \u2208 V in (A.57), and note that, by the duality theory [37],\n[ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + N\u2211\ni=1\n(\u03bb\u22c6i ) T (x\u0304Ki \u2212 x\u0304K0 ) \u2265 0.\nThus, we obtain that\nN\u2211\ni=1\n\u2016x\u0304Ki \u2212 x\u0304K0 \u2016 \u2264 1\nK\n[ 1\n2\u03c1 max \u2016a\u2016\u22641\n{ N\u2211\ni=1\n\u2016\u03bb0i \u2212 \u03bb\u22c6i + a\u20162 } + N\u03c1\n2 \u2016x00 \u2212 x\u22c60\u20162\n] ,\nC1 K . (A.57)\nFebruary 22, 2016 DRAFT\n33\nOn the other hand, let \u03bbi = \u03bb\u22c6i in (A.57), and note that, [ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0)\n] + N\u2211\ni=1\n(\u03bb\u22c6i ) T (x\u0304Ki \u2212 x\u0304K0 )\n\u2265 \u2223\u2223\u2223\u2223 [ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0) ]\u2223\u2223\u2223\u2223\u2212 \u03b4\u03bb N\u2211\ni=1\n\u2016x\u0304Ki \u2212 x\u0304K0 \u2016 (A.58)\nwhere \u03b4\u03bb , max{\u2016\u03bb\u22c61\u2016, . . . , \u2016\u03bb\u22c6N\u2016}. Thus, we obtain that \u2223\u2223\u2223\u2223 [ N\u2211\ni=1\nfi(x\u0304 K i ) + h(x\u0304 K 0 )\n] \u2212 [ N\u2211\ni=1\nfi(x \u22c6 i ) + h(x \u22c6 0) ]\u2223\u2223\u2223\u2223\n\u2264 \u03b4\u03bbC1 K + 1 2\u03c1K\nN\u2211\ni=1\n\u2016\u03bb0i \u2212 \u03bb\u22c6i \u20162 + N\u03c1\n2K \u2016x00 \u2212 x\u22c60\u20162 = \u03b4\u03bbC1 +C2 K . (A.59)\nFinally, combining (A.57) and (A.59) gives rise to (52).\nREFERENCES\n[1] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, \u201cAsynchronous distributed alternating direction method of multipliers:\nAlgorithm and convergence analysis,\u201d submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016.\n[2] V. Cevher, S. Becker, and M. Schmidt, \u201cConvex optimization for big data,\u201d IEEE Signal Process. Mag., pp. 32\u201343, Sept.\n2014.\n[3] R. Bekkerman, M. Bilenko, and J. Langford, Scaling up Machine Learning- Parallel and Distributed Approaches.\nCambridge University Press, 2012.\n[4] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and distributed computation: Numerical methods. Upper Saddle River, NJ,\nUSA: Prentice-Hall, Inc., 1989.\n[5] R. Tibshirani, \u201cRegression shrinkage and selection via the LASSO,\u201d J. Roy. Stat. Soc. B, vol. 58, pp. 267\u2013288, 1996. [6] J. Liu, J. Chen, and J. Ye, \u201cLarge-scale sparse logistic regression,\u201d in Proc. ACM Int. Conf. on Knowledge Discovery and\nData Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.\n[7] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\nNew York, NY, USA: Springer-Verlag, 2001.\n[8] P. Richta\u0301rik, M. Taka\u0301c\u030c, and S. D. Ahipasaoglu, \u201cAlternating maximization: Unifying framework for 8 sparse PCA\nformulations and efficient parallel codes,\u201d [Online] http://arxiv.org/abs/1212.4137.\n[9] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \u201cDistributed optimization and statistical learning via the alternating\ndirection method of multipliers,\u201d Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.\n[10] F. Niu, B. Recht, C. Re, and S. J. Wright, \u201cHogwild!: A lock-free approach to parallelizing stochastic gradient descent,\u201d\nProc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.\n[11] A. Agarwal and J. C. Duchi, \u201cDistributed delayed stochastic optimization,\u201d Proc. Advances in Neural Information Processing\nSystems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.\n[12] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, \u201cParameter server for distributed machine learning,\u201d\n[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf.\nFebruary 22, 2016 DRAFT\n34\n[13] M. Li, D. G. Andersen, and A. Smola, \u201cDistributed delayed proximal gradient methods,\u201d [Online] http://www.cs.cmu.edu/\n\u223cmuli/file/ddp.pdf.\n[14] J. Liu and S. J. Wright, \u201cAsynchronous stochastic coordinate descent: Parallelism and convergence properties,\u201d SIAM J.\nOptim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.\n[15] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang, \u201cParallel successive convex approximation for nonsmooth nonconvex\noptimization,\u201d in the Proceedings of the Neural Information Processing (NIPS), 2014.\n[16] G. Scutari, F. Facchinei, P. Song, D. P. Palomar, and J.-S. Pang, \u201cDecomposition by partial linearization: Parallel\noptimization of multi-agent systems,\u201d IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.\n[17] A. Daneshmand, F. Facchinei, V. Kungurtsev, and G. Scutari, \u201cHybrid random/deterministic parallel algorithms for\nnonconvex big data optimization,\u201d to appear in IEEE Trans. on Signal Processing [Online] http://www.eng.buffalo.edu/ \u223cgesualdo/Papers/DanFaccKungTSPsub14.pdf.\n[18] M. Hong, Z.-Q. Luo, and M. Razaviyayn, \u201cConvergence analysis of alternating direction method of multipliers for a family\nof nonconvex problems,\u201d to appear in SIAM J. Opt.; available on http://arxiv.org/pdf/1410.1390.pdf.\n[19] R. Zhang and J. T. Kwok, \u201cAsynchronous distributed ADMM for consensus optimization,\u201d in Proc. 31th ICML, , 2014.,\nBeijing, China, June 21-26, 2014, pp. 1\u20139.\n[20] M. Hong, \u201cA distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM based approach,\u201d\ntechnical report; available on http://arxiv.org/pdf/1412.6058.\n[21] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem, \u201cAsynchronous distributed optimization using a randomized alternating\ndirection method of multipliers,\u201d in Proc. IEEE CDC, Florence, Italy, Dec. 10-13, 2013, pp. 3671\u20133676.\n[22] E. Wei and A. Ozdaglar, \u201cOn the O(1/K) convergence of asynchronous distributed alternating direction method of\nmultipliers,\u201d available on arxiv.org.\n[23] J. F. C. Mota, J. M. F. Xavier, P. M. Q. Aguiar, and M. Puschel, \u201cD-ADMM: A communication-efficient distributed\nalgorithm for separable optimization,\u201d IEEE. Trans. Signal Process., vol. 60, no. 10, pp. 2718\u20132723, May 2013.\n[24] Q. Ling, Y. Xu, W. Yin, and Z. Wen, \u201cDecentralized low-rank matrix completion,\u201d in Proc. IEEE ICASSP, Kyoto, Japan,\nMarch 25-30, 2012, pp. 2925\u20132928.\n[25] T.-H. Chang, W.-C. Liao, M. Hong, and X. Wang, \u201cAsynchronous distributed ADMM for large-scale optimization- Part\nII: Linear convergence analysis and numerical performance,\u201d submitted for publication.\n[26] R. Tibshirani and M. Saunders, \u201cSparisty and smoothness via the fused lasso,\u201d J. R. Statist. Soc. B, vol. 67, no. 1, pp.\n91\u2013108, 2005.\n[27] J. Zhang, S. Nabavi, A. Chakrabortty, and Y. Xin, \u201cConvergence analysis of ADMM based power system mode estimation\nunder asynchronous wide-area communication delays,\u201d in Proc. IEEE PES General Meeting, Denver, CO, USA, July 26-30, 2015, pp. 1\u20135.\n[28] T.-H. Chang, A. Nedic\u0301, and A. Scaglione, \u201cDistributed constrained optimization by consensus-based primal-dual perturba-\ntion method,\u201d IEEE. Trans. Auto. Control., vol. 59, no. 6, pp. 1524\u20131538, June 2014.\n[29] J.-Y. Joo and M. Ilic, \u201cMulti-layered optimization of demand resources using Lagrange dual decomposition,\u201d IEEE Trans.\nSmart Grid, vol. 4, no. 4, pp. 2081\u20132088, Dec 2013.\n[30] E. Dall\u2019Anese, H. Zhu, and G. B. Giannakis, \u201cDistributed optimal power flow for smart microgrids,\u201d IEEE Trans. Smart\nGrid, vol. 4, no. 3, pp. 1464\u20131475, Sept. 2013.\n[31] B. He and X. Yuan, \u201cOn the o(1/n) convergence rate of Douglas-Rachford alternating direction method,\u201d SIAM J. Num.\nAnal., vol. 50, 2012.\nFebruary 22, 2016 DRAFT\n35\n[32] W. Deng and W. Yin, \u201cOn the global and linear convergence of the generalized alternating direction method of multipliers,\u201d\nRice CAAM technical report 12-14, 2012.\n[33] M. Hong and Z.-Q. Luo, \u201cOn the linear convergence of the alternating direction method of multipliers,\u201d available on\narxiv.org.\n[34] Y. Nesterov, Introductory lectures on convex optimization: A basic course. Kluwer Academic Publishers, 2004. [35] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, \u201cElectronic companion for \u201cAsynchronous distributed ADMM for\nlarge-scale optimization- Part I: Algorithm and convergence analysis,\u201d available on http://arxiv.org.\n[36] D. P. Bertsekas, Nonlinear Programming: 2nd Ed. Cambridge, Massachusetts: Athena Scientific, 2003. [37] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, UK: Cambridge University Press, 2004.\nFebruary 22, 2016 DRAFT\n36\nAlgorithm 2 Asynchronous Distributed ADMM for (4).\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = \u00b7 \u00b7 \u00b7 = dN = 0; 3: repeat 4: wait until receiving {x\u0302i, \u03bb\u0302i}i\u2208Ak from workers i \u2208 Ak such that |Ak| \u2265 A and di < \u03c4 \u2212 1 \u2200i \u2208 Ack. 5: update\nx k+1 i = { x\u0302i \u2200i \u2208 Ak x k i \u2200i \u2208 Ack , (9)\n\u03bb k+1 i = { \u03bb\u0302i \u2200i \u2208 Ak \u03bb k i \u2200i \u2208 Ack , (10)\ndi = { 0 \u2200i \u2208 Ak di + 1 \u2200i \u2208 Ack , (11)\nx k+1 0 =arg min\nx0\u2208Rn\n{ h(x0)\u2212 xT0 \u2211N i=1 \u03bb k+1 i\n+ \u03c12 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162 + \u03b32\u2016x0 \u2212 xk0\u20162 } , (12)\n6: broadcast xk+10 to the workers in Ak. 7: set k \u2190 k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial \u03bb0 and set ki = 0. 3: repeat 4: wait until receiving x\u03020 from the master node. 5: update\nx ki+1 i = arg min\nxi\u2208Rn fi(xi) + x\nT i \u03bb ki i + \u03c1 2\u2016xi \u2212 x\u03020\u20162, (13)\n\u03bb ki+1 i = \u03bb ki i + \u03c1(x ki+1 i \u2212 x\u03020). (14)\n6: send (xki+1i ,\u03bb ki+1 i ) to the master node. 7: set ki \u2190 ki + 1. 8: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT\n37\nAlgorithm 3 Asynchronous distributed ADMM from the master\u2019s point of view.\n1: Given initial variables x0 and \u03bb0; set x00 = x 0 and k = 0. 2: repeat 3: update\nx k+1 i =    arg min xi\u2208Rn { fi(xi) + x T i \u03bb k i + \u03c12\u2016xi \u2212 x k\u0304i+1 0 \u20162 } , \u2200i \u2208 Ak\nx k i \u2200i \u2208 Ack\n, (23)\n\u03bb k+1 i =\n{ \u03bb k i + \u03c1(x k+1 i \u2212 xk\u0304i+10 ) \u2200i \u2208 Ak\n\u03bb k i \u2200i \u2208 Ack\n, (24)\nx k+1 0 =arg min\nx0\u2208Rn\n{ h(x0)\u2212 xT0 \u2211N i=1 \u03bb k+1 i\n+ \u03c12 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162 + \u03b32\u2016x0 \u2212 xk0\u20162 } . (25)\n4: set k \u2190 k + 1. 5: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT\n38\nAlgorithm 4 An Alternative Implementation of Asynchronous Distributed ADMM.\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = \u00b7 \u00b7 \u00b7 = dN = 0; 3: repeat 4: wait until receiving {x\u0302i, \u03bb\u0302i}i\u2208Ak from workers i \u2208 Ak such that |Ak| \u2265 A and di < \u03c4 \u2212 1 \u2200i \u2208 Ack. 5: update\nx k+1 i = { x\u0302i \u2200i \u2208 Ak x k i \u2200i \u2208 Ack , (44)\ndi = { 0 \u2200i \u2208 Ak di + 1 \u2200i \u2208 Ack ,\nx k+1 0 =arg min\nx0\u2208Rn\n{ h(x0)\u2212 xT0 \u2211N i=1 \u03bb k i\n+ \u03c12 \u2211N i=1 \u2016xk+1i \u2212 x0\u20162 + \u03b32\u2016x0 \u2212 xk0\u20162 } , (45)\n\u03bb k+1 i = \u03bb k i + \u03c1(x ki+1 i \u2212 xk+10 ) \u2200i \u2208 V. (46)\n6: broadcast xk+10 and {\u03bbk+1i }i\u2208Ak to the workers in Ak. 7: set k \u2190 k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial \u03bb0 and set ki = 0. 3: repeat 4: wait until receiving (x\u03020, \u03bb\u0302i) from the master node. 5: update\nx ki+1 i = arg min\nxi\u2208Rn fi(xi) + x\nT i \u03bb\u0302i + \u03c1 2\u2016xi \u2212 x\u03020\u20162, (47)\n6: send xki+1i to the master node. 7: set ki \u2190 ki + 1. 8: until a predefined stopping criterion is satisfied.\nFebruary 22, 2016 DRAFT"}], "references": [{"title": "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Convex optimization for big data", "author": ["V. Cevher", "S. Becker", "M. Schmidt"], "venue": "IEEE Signal Process. Mag., pp. 32\u201343, Sept. 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling up Machine Learning- Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Parallel and distributed computation: Numerical methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B, vol. 58, pp. 267\u2013288, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "S.D. Ahipasaoglu"], "venue": "[Online] http://arxiv.org/abs/1212.4137.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1212}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter server for distributed machine learning", "author": ["M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf. February 22, 2016  DRAFT  34", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed delayed proximal gradient methods", "author": ["M. Li", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/ \u223cmuli/file/ddp.pdf.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM J. Optim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel successive convex approximation for nonsmooth nonconvex optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang"], "venue": "the Proceedings of the Neural Information Processing (NIPS), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Decomposition by partial linearization: Parallel optimization of multi-agent systems", "author": ["G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid random/deterministic parallel algorithms for nonconvex big data optimization", "author": ["A. Daneshmand", "F. Facchinei", "V. Kungurtsev", "G. Scutari"], "venue": "to appear in IEEE Trans. on Signal Processing [Online] http://www.eng.buffalo.edu/ \u223cgesualdo/Papers/DanFaccKungTSPsub14.pdf.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 0}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "to appear in SIAM J. Opt.; available on http://arxiv.org/pdf/1410.1390.pdf.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1390}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J.T. Kwok"], "venue": "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1\u20139.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM based approach", "author": ["M. Hong"], "venue": "technical report; available on http://arxiv.org/pdf/1412.6058.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1412}, {"title": "Asynchronous distributed optimization using a randomized alternating direction method of multipliers", "author": ["F. Iutzeler", "P. Bianchi", "P. Ciblat", "W. Hachem"], "venue": "Proc. IEEE CDC, Florence, Italy, Dec. 10-13, 2013, pp. 3671\u20133676.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "On the O(1/K) convergence of asynchronous distributed alternating direction method of multipliers", "author": ["E. Wei", "A. Ozdaglar"], "venue": "available on arxiv.org.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 0}, {"title": "D-ADMM: A communication-efficient distributed algorithm for separable optimization", "author": ["J.F.C. Mota", "J.M.F. Xavier", "P.M.Q. Aguiar", "M. Puschel"], "venue": "IEEE. Trans. Signal Process., vol. 60, no. 10, pp. 2718\u20132723, May 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Decentralized low-rank matrix completion", "author": ["Q. Ling", "Y. Xu", "W. Yin", "Z. Wen"], "venue": "Proc. IEEE ICASSP, Kyoto, Japan, March 25-30, 2012, pp. 2925\u20132928.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous distributed ADMM for large-scale optimization- Part II: Linear convergence analysis and numerical performance", "author": ["T.-H. Chang", "W.-C. Liao", "M. Hong", "X. Wang"], "venue": "submitted for publication.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 0}, {"title": "Sparisty and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders"], "venue": "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91\u2013108, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Convergence analysis of ADMM based power system mode estimation under asynchronous wide-area communication delays", "author": ["J. Zhang", "S. Nabavi", "A. Chakrabortty", "Y. Xin"], "venue": "Proc. IEEE PES General Meeting, Denver, CO, USA, July 26-30, 2015, pp. 1\u20135.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed constrained optimization by consensus-based primal-dual perturbation method", "author": ["T.-H. Chang", "A. Nedi\u0107", "A. Scaglione"], "venue": "IEEE. Trans. Auto. Control., vol. 59, no. 6, pp. 1524\u20131538, June 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-layered optimization of demand resources using Lagrange dual decomposition", "author": ["J.-Y. Joo", "M. Ilic"], "venue": "IEEE Trans. Smart Grid, vol. 4, no. 4, pp. 2081\u20132088, Dec 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed optimal power flow for smart microgrids", "author": ["E. Dall\u2019Anese", "H. Zhu", "G.B. Giannakis"], "venue": "IEEE Trans. Smart Grid, vol. 4, no. 3, pp. 1464\u20131475, Sept. 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Num. Anal., vol. 50, 2012. February 22, 2016  DRAFT  35", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice CAAM technical report 12-14, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "available on arxiv.org.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 0}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Electronic companion for \u201cAsynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "available on http://arxiv.org.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 0}, {"title": "Nonlinear Programming: 2nd Ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]\u2013[4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 3, "context": "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]\u2013[4].", "startOffset": 239, "endOffset": 242}, {"referenceID": 4, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "In this paper, we focus on solving large-scale instances of these learning problems with either a large number of training samples or a large number of features (n is large) [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 146, "endOffset": 149}, {"referenceID": 15, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 218, "endOffset": 222}, {"referenceID": 9, "context": "To address such dilemma, a few recent works [10]\u2013[14] have introduced \u201casynchrony\u201d into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "To address such dilemma, a few recent works [10]\u2013[14] have introduced \u201casynchrony\u201d into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "As has been consistently reported in [10]\u2013[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "As has been consistently reported in [10]\u2013[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 43, "endOffset": 46}, {"referenceID": 17, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "Specifically, reference [19] has considered a version of AD-ADMM with bounded delay assumption and studied its theoretical and numerical performances.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "However, only convex cases are considered in [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "References [21]\u2013[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "References [21]\u2013[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "Specifically, the asynchrony in [21] lies in that, at each iteration, the nodes are randomly activated to perform variable update.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "The method presented in [22] further allows that the communications between nodes can succeed or fail randomly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "It is shown in [22] that such asynchronous ADMM can converge in a probability-one sense, provided that the nodes and communication links satisfy certain statistical assumption.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Reference [23] has considered an asynchronous dual ADMM method.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].", "startOffset": 186, "endOffset": 189}, {"referenceID": 23, "context": "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].", "startOffset": 224, "endOffset": 228}, {"referenceID": 19, "context": "To the best of our knowledge, except the inexact version in [20], this is the first time that the distributed ADMM is rigorously shown to be convergent for non-convex problems under the asynchronous protocol.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "Furthermore, we demonstrate that the asynchrony of ADMM has to be handled with care \u2013 as a slight modification of the algorithm may In contrast to the conference paper [1], the current paper presents detailed proofs of theorems and more simulation results.", "startOffset": 168, "endOffset": 171}, {"referenceID": 24, "context": "In the companion paper [25], the linear convergence conditions of the AD-ADMM is further analyzed.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Synopsis: Section II presents the applications of problem (1) and reviews the distributed ADMM in [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Such distributed optimization approach is extremely useful in modern big data applications [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For example, let us consider the following regularized empirical risk minimization problem [7] min w\u2208R m \u2211", "startOffset": 91, "endOffset": 94}, {"referenceID": 25, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 179, "endOffset": 182}, {"referenceID": 26, "context": "It is interesting to mention that many emerging problems in smart power grid can also be formulated as problem (1); see, for example, the power state estimation problem considered in [27] is solved by employing the distributed ADMM.", "startOffset": 183, "endOffset": 187}, {"referenceID": 27, "context": ", demand response) in [28]\u2013[30] can potentially be handled by the distributed ADMM as well.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": ", demand response) in [28]\u2013[30] can potentially be handled by the distributed ADMM as well.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "It has been shown that such a consensus problem can be efficiently solved by the ADMM [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "According to [4], the standard synchronous ADMM iteratively updates the primal variables xi, i = 0, 1, .", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Algorithm 1 (Synchronous) Distributed ADMM for (4) [9] 1: Given initial variables x0 and \u03bb0; set x0 = x 0 and k = 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi\u2019s.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi\u2019s.", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "For non-convex and smooth fi\u2019s, the work [18] shows that Algorithm 1 can converge to the set of KKT points with a O(1/ \u221a k) rate as long as \u03c1 is large enough.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "In particular, we follow the popular partially asynchronous model [4] and assume: Assumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay.", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": ", |Ak| \u2265 A for all k [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Detailed numerical results will be reported in Section V of the companion paper [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "Let us compare Theorem 1 with the results in [19], [22].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Let us compare Theorem 1 with the results in [19], [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Therefore it is possible, at least theoretically, that a realization of the algorithm fails to converge despite satisfying the conditions given in [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "Inspired by [18], our analysis for Theorem 1 investigates how the augmented Lagrangian function, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Such insight is reminiscent of the recent convergence results for multi-block ADMM in [33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "To verify this point, let us consider the following sparse PCA problem [8] min w\u2208R \u2212 N \u2211", "startOffset": 71, "endOffset": 74}, {"referenceID": 24, "context": "The conditions under which linear convergence can be achieved are presented in the companion paper [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Numerical results which demonstrate the time efficiency of the proposed AD-ADMM on a high performance computer cluster are also presented in [25].", "startOffset": 141, "endOffset": 145}], "year": 2016, "abstractText": "Aiming at solving large-scale optimization problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers (ADMM). By formulating the optimization problem as a consensus problem, the ADMM can be used to solve the consensus problem in a fully parallel fashion over a computer network with a star topology. However, traditional synchronized computation does not scale well with the problem size, as the speed of the algorithm is limited by the slowest workers. This is particularly true in a heterogeneous network where the computing nodes experience different computation and communication delays. In this paper, we propose an asynchronous distributed ADMM (AD-ADMM) which can effectively improve the time efficiency of distributed optimization. Our main interest lies in analyzing the convergence conditions of the AD-ADMM, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. Specifically, by considering general and possibly non-convex cost functions, we show that the AD-ADMM is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen appropriately according to the network delay. We further illustrate that the asynchrony of the ADMM has to be handled with care, as slightly modifying the implementation of the AD-ADMM can jeopardize the algorithm convergence, even under the standard convex setting. Keywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, Grant No. 61571385. Mingyi Hong is supported by NFS Grant No. CCF-1526078 , and AFOSR, Grant No. 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan No. 15YF1403400 and NSFC No. 11501210. Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172. E-mail: tsunghui.chang@ieee.org. Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: liaox146@umn.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, School of Computer Science and Software Engineering, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn February 22, 2016 DRAFT", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}