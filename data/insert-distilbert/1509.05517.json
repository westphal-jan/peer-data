{"id": "1509.05517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium Free/Open-Source Machine Translation Platform", "abstract": "this paper describes a free / open - source implementation of \u00ab the linux light sliding - window ( lsw ) part - of - speech tagger for the apertium free / open - source machine translation platform. firstly, the mechanism and training implementation process of the tagger are reviewed, and a new method for incorporating linguistic ambiguity rules is proposed. secondly, similar experiments are conducted to jointly compare the performances of the ml tagger under different 3d window settings, with or without apertium - style \" forbid \" rules, with or without constraint grammar, and also somewhat with respect to the broadly traditional hmm tagger in apertium.", "histories": [["v1", "Fri, 18 Sep 2015 06:56:38 GMT  (559kb,D)", "http://arxiv.org/abs/1509.05517v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gang chen", "mikel l forcada"], "accepted": false, "id": "1509.05517"}, "pdf": {"name": "1509.05517.pdf", "metadata": {"source": "CRF", "title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium Free/Open-Source Machine Translation Platform", "authors": ["Gang Chen", "Mikel L. Forcada"], "emails": ["pkuchengang@gmail.com,", "mlf@dlsi.ua.es"], "sections": [{"heading": "1. Introduction", "text": "Apertium1 is a shallow-transfer rule-based free/opensource machine translation platform. This paper reports a free/open-source implementation of the light sliding window (LSW) PoS tagger (Sa\u0301nchez-Villamil et al., 2005), and compares its performance with that of the original firstorder HMM tagger in Apertium (Tyers et al., 2010; Sheikh and Sa\u0301nchez-Mart\u0131\u0301nez, 2009; Cutting et al., 1992). Section 2 reviews the mechanism of the LSW tagger and proposes a method to improve its tagging accuracy by incorporating linguistic rules, Section 3 shows the experimental results and discusses them, and finally, in Section 4, the paper ends with some conclusions and future plans."}, {"heading": "2. Methods", "text": "The main difference between the LSW and HMM PoS taggers is that the LSW PoS tagger makes local decisions about the PoS tag of each word which are based on the ambiguity class (set of PoS tags) of words in a fixed-length context around the problem word, while HMM makes this decision by efficiently considering all possible disambiguations of all words in the sentence, by using a probabilistic model based on a multiplicative chain of transition and emission probabilities. In terms of model complexity, LSW is simpler than HMM, while, on the other hand, the number of parameters of LSW could be larger than that of HMM, which may have a crucial influence on the tagging performance as training material may not be sufficient to estimate them adequately. The LSW tagger is an improved version of the sliding window (SW) PoS tagger (Sa\u0301nchez-Villamil et al., 2004), and the main goal of the LSW tagger is to reduce the parameters of a SW tagger, by using approximations for the parameter estimation, without a significant loss in accuracy. Therefore, we briefly describe the SW tagger first, and then the LSW tagger.\n1The Apertium machine translation engine, linguistic data for various language pairs, and documentation can be downloaded from http://www.apertium.org."}, {"heading": "2.1. The SW tagger", "text": ""}, {"heading": "2.1.1. Overview", "text": "Let \u0393 = {\u03b31, \u03b32, . . . , \u03b3|\u0393|} be the tag set, and W = {w1, w2, . . . } be the words to be tagged. A partition of W is established so that wi \u2261 wj if and only if both are assigned the same subset of tags, where each class of the partition is called an ambiguity class. Let \u03a3 = {\u03c31, \u03c32, . . . , \u03c3|\u03a3|} be the collection of ambiguity classes, where each \u03c3i is an ambiguity class. Let T : \u03a3 \u2192 2\u0393 be the function returning the collection T (\u03c3) of PoS tags for an ambiguity class \u03c3. The PoS tagging problem may be formulated as follows: given a text w[1]w[2] . . . w[L] \u2208 W+, each word w[t] is assigned (using a lexicon and a morphological analyzer) an ambiguity class \u03c3[t] \u2208 \u03a3 to obtain the ambiguously tagged text \u03c3[1]\u03c3[2] . . . \u03c3[t] \u2208 \u03a3+; the task of a PoS tagger is to obtain a tag sequence \u03b3[1]\u03b3[2] . . . \u03b3[t] \u2208 \u0393+ as correct as possible, that is, the one that maximizes the probability of that tag sequence given the word sequence:\n\u03b3\u2217[1] . . . \u03b3\u2217[L] = argmax \u03b3[t]\u2208T (\u03b3[t]) P (\u03b3[1] . . . \u03b3[L]|\u03c3[1] . . . \u03c3[L]) (1) The core idea of SW PoS tagging is to use the ambiguity classes of neighboring words to approximate the dependencies locally: P (\u03b3[1] . . . \u03b3[L]|\u03c3[1] . . . \u03c3[L]) = t=L\u220f t=1 p(\u03b3[t]|C(\u2212)\u03c3[t]C(+)) (2) where t = 1 . . . L, C(\u2212) is the left context of length N(\u2212) (e.g. if N(\u2212) = 1, then C(\u2212) = \u03b3[t \u2212 1]), and C(+) is the left context of length N(+)."}, {"heading": "2.1.2. Unsupervised parameter estimation", "text": "Let p(\u03b3|C(\u2212)\u03c3C(+)) be the probability of a tag \u03b3 appearing between the context C(\u2212) and C(+). The most probable tag \u03b3\u2217[t] is selected as the one with the highest probability by the formula:\n\u03b3\u2217[t] = argmax \u03b3\u2208T (\u03c3[t]) p(\u03b3|C(\u2212)\u03c3C+)) (3)\nar X\niv :1\n50 9.\n05 51\n7v 1\n[ cs\n.C L\n] 1\n8 Se\np 20\n15\nEstimating the parameters from a tagged corpus would be straightforward, but estimating from an untagged corpus requires an iterative process. Let n\u0303C(\u2212)\u03b3C(+) (a simpler and interchangeable representation for p(\u03b3|C(\u2212)\u03c3C(+)) ) be the effective number of times (count) that \u03b3 appears between the context C(\u2212) and C(+). Following the steps in (Sa\u0301nchez-Villamil et al., 2004), we can estimate n\u0303C(\u2212)\u03b3C(+) iteratively by:\nn\u0303 [k] C(\u2212)\u03b3C(+) =\nn\u0303 [k\u22121] C(\u2212)\u03b3C(+) \u2211 \u03c3:\u03b3\u2208T (\u03c3) nC(\u2212)\u03c3C(+)  \u2211 \u03b3\u2032\u2208T (\u03c3) n\u0303 [k\u22121] C(\u2212)\u03b3\u2032C(+) \u22121 (4)\nA recommended initial value could be obtained by assuming that all the tags \u03b3 in \u03c3 are equally probable."}, {"heading": "2.2. The LSW tagger", "text": ""}, {"heading": "2.2.1. Overview", "text": "The SW tagger tags a word by looking at the ambiguity classes of neighboring words, and has therefore a number of parameters in O(|\u03a3|N(\u2212)+N(+) |\u0393|). The LSW tagger (Sa\u0301nchez-Villamil et al., 2005) tags a word by looking at the possible tags of neighboring words, and therefore it has a number of parameters inO(|\u0393|N(\u2212)+N(+)+1). Usually the tag set size |\u0393| is significantly smaller than the combinational ambiguity class size |\u03a3|. In this way, the number parameters is effectively reduced. The LSW approximates the best tag as follows:\n\u03b3\u2217 = argmax \u03b3\u2208T (\u03c3[t])\u2211\nE(\u2212)\u2208T \u2032(C(\u2212)[t]) E(+)\u2208T \u2032(C(+)[t])\np(E(\u2212)\u03b3E(+)|C(\u2212)[t]\u03b3[t]C(+)[t]) (5)\nwhere T \u2032 : \u03a3\u2217 \u2192 2\u0393\u2217 , an extension of T , returns the set of tag sequences for an ambiguity sequence; E(\u2212) and E(+) are the left and right tag sequence respectively."}, {"heading": "2.2.2. Unsupervised parameter estimation", "text": "Following a procedure similar to that for the SW tagger, we can derive an iterative process to train the LSW tagger.\nn\u0303 [k] E(\u2212)\u03b3E(+) = n\u0303 [k\u22121] E(\u2212)\u03b3E(+)\n\u2211 \u03c3:\u03b3\u2208T (\u03c3)\nC(\u2212):E(\u2212)\u2208T \u2032(C(\u2212)) C(+):E(+)\u2208T \u2032(C(+))\nnE(\u2212)\u03c3E(+)  \u2211\n\u03b3\u2032\u2208T (\u03c3) C(\u2212):E(\u2212)\u2208T\n\u2032(C(\u2212)) C(+):E(+)\u2208T \u2032(C(+))\nn\u0303 [k\u22121] E(\u2212)\u03b3\u2032E(+)  \u22121 (6)\nwhere n\u0303E(\u2212)\u03b3E(+) is the effective number of times (count) that \u03b3 appears between the context of tags E(\u2212) and E(+). Similarly to the initialization step in the SW tagger, a recommended initial value can be obtained by assuming that all the tag sequencesE(\u2212)\u03b3E(+) in the windowC(\u2212)\u03c3C(+) are equally probable."}, {"heading": "2.3. LSW with forbid and enforce rules", "text": "There are forbid and enforce rules for sequences of two PoS tags in the current implementation of the Apertium PoS tagger. They were successfully applied in the original HMM tagger in Apertium, with a significant improvement in accuracy (Sheikh and Sa\u0301nchez-Mart\u0131\u0301nez, 2009), simply by making the corresponding transition probabilities equal to zero. The SW tagger could not make use of forbid and enforce rules because of the fact that it works with ambiguity classes, while on the other hand, the LSW tagger can easily incorporate them as it works directly with PoS tags The rules can be introduced right after the initialization step. For a tag sequence in the parameter space, if any consecutive two tags match a forbid rule or fail to match an enforce rule, the underlying parameter n\u0303E(\u2212)\u03b3E(+) will be given a starting value of zero. In this way, for an LSW tagger with rules, the initial value could be given as follows,\nn\u0303 [0] E(\u2212)\u03b3E(+) = { 0 if E(\u2212)\u03b3E(+) is not valid, \u039b otherwise (7)\nwhere\n\u039b = \u2211\n\u03c3:\u03b3\u2208T (\u03c3) C(\u2212):E(\u2212)\u2208T\n\u2032(C(\u2212)) C(+):E(+)\u2208T \u2032(C(+))\nnE(\u2212)\u03c3E(+) 1\n|V \u2032(C(\u2212)\u03c3C(+))| (8)\nwhere, the validity of E(\u2212)\u03b3E(+) is determined by forbid and enforce rules, and the function V \u2032 returns the collection of valid (enforced or not forbidden) tag sequences contained in the ambiguity class sequence C(\u2212)\u03c3C(+)."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Training data and test set", "text": "The experiments are conducted on three languages: Spanish (apertium-en-es-0.8.0), Catalan (apertium-es-ca-1.1.0), and English (apertium-en-es-0.8.0). We obtain the training data for Spanish and English by sampling text from the Europarl corpus (Koehn, 2005), and for Catalan by sampling text from the Catalan Wikipedia. The statistics on the training data and test data are shown in Table 1. Test data for Catalan and Spanish come from apertium-es-ca-1.1.0. It is worth noting that the English test set has been built by mapping the results form the TnT (Brants, 2000) tagger as an approximation."}, {"heading": "3.2. The LSW tagger vs. the SW tagger", "text": "We firstly study whether there is a difference between the LSW tagger and the SW tagger, keeping all other settings the same. Then we study whether rules can help improve the accuracy for the LSW tagger. \u201cAccuracy\u201d in the graph refers to the tagging precision of a tagger on the handtagged test set. Figure 1 shows that rules help significantly for improving accuracy, and that the SW tagger behaves similarly to the LSW tagger without rules, which is consistent with the conclusion in (Sa\u0301nchez-Villamil et al., 2005)."}, {"heading": "3.3. Different window settings for the LSW tagger", "text": "We study the performances of the LSW tagger with different window settings, and of the HMM tagger, on the three languages, as shown in Figure 2. We can see that the HMM tagger performs best among all the taggers, especially when there is enough training data. However, when training data is limited, the LSW taggers learn faster (need less words to learn) and more stably than the HMM tagger. Among all the LSW taggers, the LSW(-1, +1), i.e. left context 1 and right context 1, performs best. When there are enough training data, the performances of the HMM tagger and the LSW(-1, +1) tagger are quite close. Note that under some window settings, the performances of the LSW taggers even decrease as more training lines\nwere added, e.g. LSW(-1) and LSW(-2, -1) for Spanish and Catalan. This is an unexpected phenomenon, and the reason for it would require further investigation."}, {"heading": "3.4. Using Constraint Grammar rules to support the HMM and LSW", "text": "We also tested whether the use of Constraint Grammar (CG) rules helps to improve the accuracy obtained by both HMM and LSW taggers, along the lines suggested in (Hulden and Francom, 2012). For that, we used the CG rules already present in Apertium packages apertium-eo-es-0.8.2 for Spanish and apertium-eo-ca-0.8.2 for Catalan respectively (a CG module is integrated in many Apertium language pairs). Figure 3 shows that CG helps almost in all settings. It is also shown that CG rules help the two taggers in different situations: for the HMM tagger, the positive contribution of CG rules is larger when training data is limited than when training data is relatively enough; while for the LSW tagger, the trend is almost the opposite, that CG rules contribute even more when training data is relatively enough. Note that the logical approach would be to use CG rules both for reducing ambiguity for the training corpus (denoted as cgTrain in Figure 3) and for reducing ambiguity right after morphological analyzer and before the PoS tagger (denoted as cgTag in Figure 3); the results are however almost indistinguishable from those obtained applying CG in either step."}, {"heading": "4. Discussion and future work", "text": "We reviewed the mechanism and unsupervised parameter estimation methods for both the SW and LSW taggers. Compared with previous work (Sa\u0301nchez-Villamil et al., 2004; Sa\u0301nchez-Villamil et al., 2005), firstly, we proposed a method for incorporating the forbid and enforce rules already used for HMM taggers in Apertium into the LSW tagger; and secondly, the implementation is the first time that the LSW tagger is integrated into a real machine translation system (Apertium), and at the same time, its code is free/open-source. We also conducted experiments to compare the performances of the LSW tagger with different settings, and with respect to the original HMM tagger. Firstly, the HMM tagger performs slightly better than the LSW(-1, +1) tagger when there is enough training data, while the LSW(-1, +1) tagger learns faster and is more stable when training data is limited. Secondly, the LSW(-1, +1) tagger performs best\namong all the other window settings, and better than the SW(-1, +1) tagger, which behaves similarly with LSW(-1, +1)-No-Rules. Thirdly, we have found that the use of CG rule sets already existing in some Apertium taggers helps significantly to improve accuracy based both on the HMM and LSW taggers, and that for the HMM tagger CG rules help more when training data is limited, while for the LSW tagger CG rules help more when training data is relatively enough. The reason why the performance of the LSW tagger under some window settings worsens as more training lines are added also requires more efforts to study. Source code is available through the Apertium Subversion repository2 under a free/open-source license.\nAcknowledgements: Support from Google Summer of Code (summer scholarship for Gang Chen) and from the Spanish Ministry of Economy and Competitiveness through grant TIN2012-32615 are gratefully acknowledged. The authors also thank Francis M. Tyers and Jim O\u2019Regan for useful comments."}, {"heading": "5. References", "text": "T. Brants. 2000. TnT: a statistical part-of-speech tagger. In\nProceedings of the sixth conference on Applied natural language processing, pages 224\u2013231. D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the third conference on Applied natural language processing, pages 133\u2013140. M. Hulden and J. Francom. 2012. Boosting statistical tagger accuracy with simple rule-based grammars. In N. Calzolari, K. Choukri, T. Declerck, M. Ugur Dogan, B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, editors, LREC, pages 2114\u20132117. European Language Resources Association (ELRA). P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79\u201386, Phuket, Thailand. AAMT, AAMT. E. Sa\u0301nchez-Villamil, M. L. Forcada, and R. C. Carrasco. 2004. Unsupervised training of a finite-state slidingwindow part-of-speech tagger. In Advances in Natural Language Processing, pages 454\u2013463. Springer. E. Sa\u0301nchez-Villamil, M. L. Forcada, and R. C. Carrasco. 2005. Parameter reduction in unsupervisedly trained sliding-window part-of-speech taggers. In Proceedings of Recent Advances in Natural Language Processing, Borovets, Bulgaria, September, 2005. Z. M. A. W. Sheikh and F. Sa\u0301nchez-Mart\u0131\u0301nez. 2009. Parameter reduction in unsupervisedly trained slidingwindow part-of-speech taggers. In Proceedings of the First International Workshop on Free/Open-Source RuleBased Machine Translation, pages 67\u201374. Universidad de Alicante. Departamento de Lenguajes y Sistemas Informa\u0301ticos. F. M. Tyers, F. Sa\u0301nchez-Mart\u0131\u0301nez, S. Ortiz-Rojas, and M. L. Forcada. 2010. Free/open-source resources in the Aper-\n2https://svn.code.sf.net/p/apertium/svn/ branches/apertium-swpost\ntium platform for machine translation research and development. The Prague Bulletin of Mathematical Linguistics, 93:67\u201376."}], "references": [{"title": "TnT: a statistical part-of-speech tagger", "author": ["T. Brants."], "venue": "Proceedings of the sixth conference on Applied natural language processing, pages 224\u2013231.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "A practical part-of-speech tagger", "author": ["D. Cutting", "J. Kupiec", "J. Pedersen", "P. Sibun."], "venue": "Proceedings of the third conference on Applied natural language processing, pages 133\u2013140.", "citeRegEx": "Cutting et al\\.,? 1992", "shortCiteRegEx": "Cutting et al\\.", "year": 1992}, {"title": "Boosting statistical tagger accuracy with simple rule-based grammars", "author": ["M. Hulden", "J. Francom."], "venue": "N. Calzolari, K. Choukri, T. Declerck, M. Ugur Dogan, B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, editors, LREC, pages 2114\u20132117. European Language Re-", "citeRegEx": "Hulden and Francom.,? 2012", "shortCiteRegEx": "Hulden and Francom.", "year": 2012}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["P. Koehn."], "venue": "Conference Proceedings: the tenth Machine Translation Summit, pages 79\u201386, Phuket, Thailand. AAMT, AAMT.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Unsupervised training of a finite-state slidingwindow part-of-speech tagger", "author": ["E. S\u00e1nchez-Villamil", "M.L. Forcada", "R.C. Carrasco."], "venue": "Advances in Natural Language Processing, pages 454\u2013463. Springer.", "citeRegEx": "S\u00e1nchez.Villamil et al\\.,? 2004", "shortCiteRegEx": "S\u00e1nchez.Villamil et al\\.", "year": 2004}, {"title": "Parameter reduction in unsupervisedly trained sliding-window part-of-speech taggers", "author": ["E. S\u00e1nchez-Villamil", "M.L. Forcada", "R.C. Carrasco."], "venue": "Proceedings of Recent Advances in Natural Language Processing, Borovets, Bulgaria, September, 2005.", "citeRegEx": "S\u00e1nchez.Villamil et al\\.,? 2005", "shortCiteRegEx": "S\u00e1nchez.Villamil et al\\.", "year": 2005}, {"title": "Parameter reduction in unsupervisedly trained slidingwindow part-of-speech taggers", "author": ["Z.M.A.W. Sheikh", "F. S\u00e1nchez-Mart\u0131\u0301nez"], "venue": "In Proceedings of the First International Workshop on Free/Open-Source RuleBased Machine Translation,", "citeRegEx": "Sheikh and S\u00e1nchez.Mart\u0131\u0301nez.,? \\Q2009\\E", "shortCiteRegEx": "Sheikh and S\u00e1nchez.Mart\u0131\u0301nez.", "year": 2009}, {"title": "Free/open-source resources in the Aperhttps://svn.code.sf.net/p/apertium/svn", "author": ["F.M. Tyers", "F. S\u00e1nchez-Mart\u0131\u0301nez", "S. Ortiz-Rojas", "M.L. Forcada"], "venue": null, "citeRegEx": "Tyers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "This paper reports a free/open-source implementation of the light sliding window (LSW) PoS tagger (S\u00e1nchez-Villamil et al., 2005), and compares its performance with that of the original firstorder HMM tagger in Apertium (Tyers et al.", "startOffset": 98, "endOffset": 129}, {"referenceID": 7, "context": ", 2005), and compares its performance with that of the original firstorder HMM tagger in Apertium (Tyers et al., 2010; Sheikh and S\u00e1nchez-Mart\u0131\u0301nez, 2009; Cutting et al., 1992).", "startOffset": 98, "endOffset": 176}, {"referenceID": 6, "context": ", 2005), and compares its performance with that of the original firstorder HMM tagger in Apertium (Tyers et al., 2010; Sheikh and S\u00e1nchez-Mart\u0131\u0301nez, 2009; Cutting et al., 1992).", "startOffset": 98, "endOffset": 176}, {"referenceID": 1, "context": ", 2005), and compares its performance with that of the original firstorder HMM tagger in Apertium (Tyers et al., 2010; Sheikh and S\u00e1nchez-Mart\u0131\u0301nez, 2009; Cutting et al., 1992).", "startOffset": 98, "endOffset": 176}, {"referenceID": 4, "context": "The LSW tagger is an improved version of the sliding window (SW) PoS tagger (S\u00e1nchez-Villamil et al., 2004), and the main goal of the LSW tagger is to reduce the parameters of a SW tagger, by using approximations for the parameter estimation, without a significant loss in accuracy.", "startOffset": 76, "endOffset": 107}, {"referenceID": 4, "context": "Following the steps in (S\u00e1nchez-Villamil et al., 2004), we can estimate \u00f1C(\u2212)\u03b3C(+) iteratively by:", "startOffset": 23, "endOffset": 54}, {"referenceID": 5, "context": "The LSW tagger (S\u00e1nchez-Villamil et al., 2005) tags a word by looking at the possible tags of neighboring words, and therefore it has a number of parameters inO(|\u0393|(\u2212)(+)).", "startOffset": 15, "endOffset": 46}, {"referenceID": 6, "context": "They were successfully applied in the original HMM tagger in Apertium, with a significant improvement in accuracy (Sheikh and S\u00e1nchez-Mart\u0131\u0301nez, 2009), simply by making the corresponding transition probabilities equal to zero.", "startOffset": 114, "endOffset": 150}, {"referenceID": 3, "context": "We obtain the training data for Spanish and English by sampling text from the Europarl corpus (Koehn, 2005), and for Catalan by sampling text from the Catalan Wikipedia.", "startOffset": 94, "endOffset": 107}, {"referenceID": 0, "context": "It is worth noting that the English test set has been built by mapping the results form the TnT (Brants, 2000) tagger as an approximation.", "startOffset": 96, "endOffset": 110}, {"referenceID": 5, "context": "Figure 1 shows that rules help significantly for improving accuracy, and that the SW tagger behaves similarly to the LSW tagger without rules, which is consistent with the conclusion in (S\u00e1nchez-Villamil et al., 2005).", "startOffset": 186, "endOffset": 217}, {"referenceID": 2, "context": "We also tested whether the use of Constraint Grammar (CG) rules helps to improve the accuracy obtained by both HMM and LSW taggers, along the lines suggested in (Hulden and Francom, 2012).", "startOffset": 161, "endOffset": 187}, {"referenceID": 4, "context": "Compared with previous work (S\u00e1nchez-Villamil et al., 2004; S\u00e1nchez-Villamil et al., 2005), firstly, we proposed a method for incorporating the forbid and enforce rules already used for HMM taggers in Apertium into the LSW tagger; and secondly, the implementation is the first time that the LSW tagger is integrated into a real machine translation system (Apertium), and at the same time, its code is free/open-source.", "startOffset": 28, "endOffset": 90}, {"referenceID": 5, "context": "Compared with previous work (S\u00e1nchez-Villamil et al., 2004; S\u00e1nchez-Villamil et al., 2005), firstly, we proposed a method for incorporating the forbid and enforce rules already used for HMM taggers in Apertium into the LSW tagger; and secondly, the implementation is the first time that the LSW tagger is integrated into a real machine translation system (Apertium), and at the same time, its code is free/open-source.", "startOffset": 28, "endOffset": 90}], "year": 2015, "abstractText": "This paper describes a free/open-source implementation of the light sliding-window (LSW) part-of-speech tagger for the Apertium free/open-source machine translation platform. Firstly, the mechanism and training process of the tagger are reviewed, and a new method for incorporating linguistic rules is proposed. Secondly, experiments are conducted to compare the performances of the tagger under different window settings, with or without Apertium-style \u201cforbid\u201d rules, with or without Constraint Grammar, and also with respect to the traditional HMM tagger in Apertium.", "creator": "LaTeX with hyperref package"}}}