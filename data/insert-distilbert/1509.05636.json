{"id": "1509.05636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "Visual Generalized Coordinates", "abstract": "an open problem in robotics is that of using vision to identify a robot's own personal body and the world around it. many models attempt to recover the traditional c - space parameters. instead, we propose an unspecified alternative c - space by deriving generalized coordinates from $ n $ images of the robot. we show that the space of such images is bijective to the motion space, so these images lie on a manifold $ \\ mathcal { v } $ homeomorphic to the canonical c - space. we now approximate this manifold as a set of $ n $ neighbourhood tangent spaces that result in a graph, toward which we call the visual roadmap ( vrm ). easily given a uniquely new robot image, we perform inverse motion kinematics visually by interpolating between nearby images in the image space. obstacles are projected onto the vrm in $ o ( n ) $ time by superimposition of images, leading to the identification of collision poses. the relative edges joining the free nodes can now be checked with a visual local planner, and free - motion space motions computed in $ o ( nlogn ) $ time. achieving this enables us to plan paths in the image space for a robot manipulator with historically unknown link geometries, dof, kinematics, obstacles, and camera pose. we sketch the proofs for the main theoretical ideas, identify the assumptions, and demonstrate the approach for both articulated and mobile robots. we also investigate the feasibility of the process by investigating various metrics and image sampling densities, and demonstrate it on simulated and real robots.", "histories": [["v1", "Fri, 18 Sep 2015 14:17:57 GMT  (4099kb,D)", "http://arxiv.org/abs/1509.05636v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["m seetha ramaiah", "amitabha mukerjee", "arindam chakraborty", "sadbodh sharma"], "accepted": false, "id": "1509.05636"}, "pdf": {"name": "1509.05636.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["M Seetha Ramaiah", "Amitabha Mukerjee", "Arindam Chakraborty", "Sadbodh Sharma"], "emails": ["sadbodh}@iitk.ac.in"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nHumans and animals routinely use prior sensorimotor experience to build motor models, and use vision for gross motor tasks in novel environments. Achieving similar abilities, without having to calibrate a robot\u2019s own body structure, or estimate exact 3-D positions, is a touchstone problem for robotics (e.g. see [3] ch.9). Such an approach would enable a robot to work in less controlled environments, as is being increasingly demanded in social and interactive applications for robots.\nThere have been two methods for approaching this problem - either based on learning a body schema [4]\u2013[9], or by fitting a canonical robot model [10]. Body schema approaches have not scaled up to full scale robotic models or used for global motion planning, and robot model regression requires intrusive structures on the robot [11] and even then it cannot sense the environment.\nAnother approach, visual servoing attempts to estimate the motion needed for small changes in image features. However, visual servoing models cannot construct models spanning large changes in robot pose, since the pseudo-inverse of the image Jacobian can be computed only over small motions. Recently, global motion planning algorithms have been pro-\n\u2217 This work was supported by the Research-I foundation. 1,2 Department of Computer Science & Engineering; 3,4 Center for Mechatronics; Indian Institute of Technology Kanpur. {msram1, amit2, arindamc3, sadbodh4}@iitk.ac.in\nposed by stitching together local visual servos [12], but these require that the goal be constantly visible.\nar X\niv :1\n50 9.\n05 63\n6v 1\n[ cs\n.R O\n] 1\n8 Se\np 20\n15\nA. Visual Generalized Coordinates\nThe notion of Configuration Space is fundamental to conceptualizing multi-body motion. The configuration of a system with d degrees of freedom can usually be specified in terms of d independent parameters, known as generalized coordinates (GC). Thus, for a planar robot arm with two links, as in fig. 1a, the canonical choice for GC is to use the joint angles (\u03b81, \u03b82). However, this is only one of many (potentially infinite) choices of coordinates, each resulting in a different C-space. GCs need not specify joint angles or any motion parameter - they just need to uniquely specify the pose. One of our main aims is to show that an alternate GC can be learned from the robot\u2019s appearance alone, i.e. from a set of images. These visual coordinates are homeomorphic to the canonical coordinates - as in fig. 1d, where we note that the image manifold (the Visual Configuration Space, VCS) is a torus, just like the canonical (\u03b81, \u03b82) manifold. This is particularly notable since the image dimensionality \u2248 3 \u00d7 105, so the image space is enormous; yet the images that can show robot poses lie on this tiny two-dimensional subspace. This can be explained by noting that the probability of a random image being a robot image is vanishingly small. The Visual Manifold theorem below formalizes these claims.\nSuch a system would have many advantages. For example, if an unknown robot is given to us, we can learn its VCS by observing a random set of poses as it moves. In fact, this idea draws inspiration from proposals for how an infant learns to use its limbs [13], [14]. Now if a task is specified visually - such as an object to grasp - the pose desired for executing it may be easier to specify in terms of its image, or that of its gripper (the set of gripper images also form an equivalent manifold). Given a novel goal image, the system can interpolate between nearby known poses to reach the desired pose (inverse kinematics). Assuming we have a controller that can repeat previously seen poses, the robot can now reach for the object by traversing a set of landmark images. Obstacles introduced now can be superimposed on the set of images to identify collision configurations. Given a set of possibly multiple cameras, this enables the system to find paths avoiding obstacles. Parts of its body or workspace that is accessed repeatedly can have a finer model (by sampling more images from this part of the workspace). The system can cluster task trajectories to learn action schemas etc. One could then \u201cimagine\u201d the consequence of a motor command, and compare these quickly to find discrepancies [15]. All this is done without any knowledge of robot kinematics or shape, its environment, or even the camera poses.\nThe model proposed here has some constraints. It requires that the set of cameras be able to see the robot in all its poses. Thus, it is more suited for articulated robot arms, though it would also work for a mobile robot seen from a roof camera. Also, it requires that every pose of the robot must be visually distinguishable - i.e. different poses should look different from at least one of the camera views.\nOur main contributions are a) to show that configuration spaces based on robot images, and not joint angles, exist; b)\npresent a sampling-based algorithm that takes a large set of robot pose images, and constructs piecewise approximations in terms of local neighbourhoods on the image manifold. c) demonstrate how such a visual C-space can be equivalently used to find a roadmap and identify poses (inverse kinematics), and plan motions.\nSection II gives an overview of the algorithm. Section III presents a theoretical analysis proving the existence of the VCS (Visual Manifold Theorem) and that obstacles in the workspace can be modelled via image superposition (Visual Collision Theorem). Section IV constructs a discrete model of the image manifold V by stitching together neighbouring images into a graph that we call the Visual Roadmap (VRM). This is analogous to roadmaps used in sampling based motion planning [1], [16]. During the VRM construction, only immovable parts of the environment are present; obstacles etc can be introduced later. For motion planning purposes, the robot foreground in each image is obtained by removing the fixed background; these are superimposed on an obstacle image to identify the collision states. Note that this is equivalent to modelling the obstacle as the convex hull of the visibility cones (Fig. 5). Each edge between neighbouring free space nodes is now tested using one of several visual local planners (section IV-B). Now, given images for the Start and Goal poses of the robot, one can add edges from these to the nearest safe neighbours in the VRM, and find a path on the graph. Section V presents an empirical analysis of the various choices for image space metrics and local planners, and section VI shows some demos on real robots."}, {"heading": "II. ALGORITHM OVERVIEW AND INVERSE KINEMATICS", "text": "Going from a configuration q \u2208 Q to the workspace robot shape and its inverse - known as forward and inverse kinematics - traditionally involves careful assignment of coordinate frames and complex transformations between these. In the Visual Generalized Coordinates approach, once we have the VRM, inverse kinematics can be computed for any desired pose, presented as an image x. To do this, we find the images nearest to x, and interpolate between these on the local chart on V . Now, to plan motions, traditional methods require explicit knowledge of obstacle geometry as well as a simulator for testing collisions. Both these are replaced by visual intersection\nFig. 2 shows an overview of the VRM algorithm, demonstrated on a simulated mobile robot. The idea of the Visual Roadmap is an analog to the Probabilistic Roadmap, in that it is a graph {V,E} where V is the set of images sampling the entire workspace, and E the edges connecting local neighbours. This is the heart of this work, where the conventional configuration description Q (e.g. the joint angle space) is replaced by a completely different GC based on images. The latent space V here is discovered from images, and is homeomorphic to Q.\nA. Visual Roadmap\nDiscovering Visual Generalized Coordinates requires us to find the neighbours of an image in image space. This requires an image metric - e.g. Fig. 2 uses a simple euclidean metric. An alternate metric may be to evaluate the swept volume between two poses. This can often be effectively approximated by the maximum distance covered by any point [17]. (see section IV-B.2. Poorer metrics may corrupt the neighbourhood and call for much denser samples. Thus, we find that track distance based metrics, or Hausdorff measures, outperform the Euclidean metrics and require order of magnitude less samples for the same results V-B.\nImage space neighbourhoods are then used to construct a local-PCA based nonlinear manifold [18]. The graph based on the neighbourhoods is the VRM. We observe that there can be situations where multiple robot poses look alike to the imaging system (see fig. 4). A critical assumption underlying our approach is that each pose is distinguishable at least from one camera. This is the Visual distinguishability assumption. In practice, most robots already meet this criteria.\nWe thus show that the system can discover a compact nonmetric model, that retains the structure of the conventional Configuration Space Q, but one that is derived solely based on a dense latent space discovery. The discovered lowerdimensional space V can be mapped to the robot image space I and to the traditional C-spaceQ. We also observe that these mappings are the visual analogues for forward and inverse kinematics as in traditional robotics.\nIII. VISUAL CONFIGURATION SPACE\nIn order to understand the idea of the Visual Configuration Space, let us consider the space of images of a robot I (e.g. for the 2-DOF robot of fig. 1). The input Images are high\ndimensional - if each image is 640 \u00d7 480 (approximately 105) pixels, then I \u2282 R3\u00d7105 . However, given an image x in I, it can be altered as many ways as the degrees of freedom d, without the resulting image leaving I. So the intrinsic dimensionality of I is 2. Further, as the links keep rotating, in the end the image sequence returns to the original image. Thus the topology of I is not euclidean (R2), but a d-torus (S1 \u00d7 S1 for the 2-DOF robot). Also, the image space changes smoothly as it moves, and so the mapping is diffeomorphic. Thus if V is a smooth dense latent space for I, then every image neighbourhood in I maps to a neighbourhood in V , and these maps change smoothly as one moves through the poses of the robot.\nWhile these properties hold for the continuous image space, in practice we work with a representative sample X = {x1...xn} \u2282 I. There are a number of manifold discovery algorithms that one could use (e.g. Isomap [2]). However, such methods have difficulty in introducing new data points and in interpolating local data, so we avoid computing the manifold altogether, and restrict ourselves to a piecewise algorithm, as in [18], [19].\nWe now establish the conditions under which the space of all images of the robot would also form a homologous manifold.\nA. Visual Distinguishability\nIn general, the imaging transformation \u03c6 is not invertible - i.e. the 3D positions are not recoverable from the image. It is only because the image is being generated under motion constraints, that one can find a map from the image space to a unique low-dimensional map. However, this does not hold in\nall situations (fig. 4); hence we require that in practice, there be some colour textures on the robot body, or a restricted range of motion, that permits distinguishability of all robot poses. This is the visual distinguishability assumption.\nLet Rq be the set of all points of the workspace occupied by the robot (its volume) in configuration q, and let R(Q) = {Rq : q \u2208 Q} be the set of all robot shapes. Let \u03c6 : Q \u2192 R(Q) and \u03c8 : R(Q) \u2192 I be the functions that map a configuration to a shape and a shape to an image respectively. Then the visual distinguishability assumption requires that the function \u03c8 \u25e6 \u03c6 : Q \u2192 I be a bijection as illustrated in fig. 3.\nThe imaging transformation \u03c8\u25e6\u03c6 maps each configuration q to an image Iq projected by the boundary \u03b4Rq of shape Rq . If the visual distinguishability assumption holds, then both \u03c6 and \u03c6\u22121 exist and are continuous, because small changes in the robot configuration lead to small changes in its shape and the corresponding images and vice versa. So, whenever Q is a manifold, I is also a manifold of the same dimension (i.e., for a d DOF robot the image space is a ddimensional manifold). This is the manifold on which the Visual Roadmap (VRM) is constructed.\nB. Visual Manifold Theorem\nDefinition 1. A Smoothly Moving Piece-wise Rigid body (SMPR) is any system with a smooth map from its configuration space to its shape space.\nLemma 1. For a d-DOF SMPR, the configuration space is a d-dimensional topological manifold.\nDefinition 2. A visually distinguishable system is one for which the visual distinguishability assumption holds.\nHence, for a visually distinguishable SMPR, \u03c8\u25e6\u03c6 : Q \u2192 I is a homeomorphism.\nTheorem 1. Whenever Q is a manifold, I is a manifold of the same dimension.\nProof. The imaging transformation \u03c8 \u25e6\u03c6 maps each configuration q to an image Iq projected by the boundary \u03b4Rq of shape Rq . If the visual distinguishability assumption holds, then both \u03c6\u22121 and \u03c8\u22121 exist, and the image space I is homeomorphic to the configuration space Q. Hence I constitutes a manifold of the same dimension as that of Q, whenever Q is a manifold.\nFig. 9 row 1(c) shows a plot of the residual variance against degrees of freedom for a 2-DOF SCARA arm; the model is clearly captured by a manifold of intrinsic dimensionality two."}, {"heading": "C. Difficulties with Manifold discovery algorithms", "text": "For robots which involve a motion with an S1 topology, the C-space and hence the VRM space is not globally Euclidean. For example, the C-space of a freely-rotating 2-DOF articulated robot is S1 \u00d7 S1 = T2, which is a torus [20]. Traditional nonlinear dimensionality reduction (NLDR) algorithms (e.g. [2]) assume that the target space\nfor dimensionality reduction is a euclidean space (a subspace of Rn). This means that a d-torus manifold, which is ddimensional, cannot be globally mapped to an Rd space, with which it is locally homeomorphic. Another practical difficulty with NLDR algorithms is that it is very challenging to add new points to the manifold without recomputing the entire structure.\nAt the same time, the global non-linear coordinate is little more than a convenience, and does not materially affect the modelling, which can be done in a piecewise linear manner. Thus, we avoid computing global coordinates altogether, and use the local neighbourhood graphs for planning global paths and local tangent spaces, discovered using Principal Component Analysis (PCA), for checking the safety of edges (local planner). These local tangent spaces, in theory, correspond to charts which when stitched together form an atlas for the image manifold.\nWe next describe how obstacles are mapped on the VCS for collision detection."}, {"heading": "D. Collision Detection in VCS", "text": "In the imaging process, robot and obstacle are mapped to a bundle of rays converging on the camera optical center (figure 4).\nLet CRi be the bundle subtended at camera optical center CO by the robot in configuration q(i), CA be the bundle subtended at CO by the obstacle A and IRi, IA be the image regions corresponding to the robot and the obstacle.\nLemma 2. If CRi \u2229C A = \u2205 then A \u2229R(q(i)) = \u2205.\nThus, robot configurations for which the bundles do not intersect with the obstacle bundle are guaranteed to be in the free space F . Note that the converse is not true.\nLemma 3. CA \u2229C R = \u2205 iff IA \u2229I R = \u2205.\nTheorem 2. (Visual Collision Theorem) For a robot in a given pose q(i), if IRi \u2229 IA = \u2205, then q(i) \u2208 F .\nWe note that the above is a necessary condition, but it is often rather conservative. Indeed, the inverse condition defines occlusion situations: where A\u2229R = \u2205 but CR\u2229C A is non-null. This limitation is a result of the information loss in the imaging process. These can be cause particular difficulties for articulated arms. In such cases, one may use multiple cameras; since the Visual Collision Theorem holds for all cameras, we may define any space as free if CR \u2229C A = \u2205 in at least one view. In this situation, both robot and obstacle are less conservatively modelled as the intersection of multiple cones.\nIn general, for non-orthographic projections, the higher the ratio of camera distance/focal length, the tighter the bound. (e.g the Scara robot arm in section VI-A).\nIV. VISUAL ROADMAP AND MOTION PLANNING ALGORITHMS\nA colour image sample X \u2208 Rp\u00d7n where each r \u00d7 c\u00d7 3 RGB image is represented as a p-dimensional vector (p = 3rc) of intensities. We assume that the images are captured against a fixed background, which can be eliminated, so that in the foreground images, a pixel is non-zero if and only if it belongs to the robot. Image xi \u2208 X corresponds to configuration qi \u2208 Q. Let d(xi, xj) be a suitable metric (e.g. Euclidean distance between the image vectors), and let N (x) be the set of k-nearest neighbours of x.\nNext, we construct a graph G(V,E) over the n nodes so that vi \u2208 V corresponds to image xi (or configuration qi). We add an edge between two nodes vi and vj if either xi \u2208 N (xj) or xj \u2208 N (xi) and assign edge weight d(xi, xj). We call this graph the Visual Roadmap (VRM).\nA. VRM with Static Obstacles\nFor handling obstacles, we take the background subtracted images, and test this intersection with the obstacle image. Non-empty overlaps imply that the configuration is not free and we remove the corresponding node and its incident edges from G. If b \u2208 Rp is the obstacle image vector, then the set of nodes to be removed from G is Vcollision = {vi : xi \u2217 b 6= 0}, where \u2217 denotes entry-wise product (Hadamard product) and 0 is the zero-vector. Thus, we obtain a modified\ngraph G\u2032(V \u2032, E\u2032) in which every node represents a free configuration. However, the edges may still touch some part of the obstacle in an intermediate pose. Guaranteeing edgesafety is the problem of local planner below. Note that this process applies to any number of static obstacles."}, {"heading": "B. Local Planner in VRM", "text": "We say that an edge (u, v) of G is safe, if the geodesic from u to v on the configuration manifold does not contain any image that overalps with an obstacle. The geodesic is approximated by the shortest path on G. Given that the nodes V are in the free space, we need to guarantee that every edge is also safe. We describe three local planners that work with robot images and can be used on visual roadmaps. To make sure that an edge is safe, these methods construct a new image that in estimates the swept volume of the robot in the workspace and check this image for collision. Figure 6 shows examples of images generated by these local planners.\n1) Interpolation on the Local Tangent Space (LTS): For each edge (u, v) \u2208 E, let X(u,v) = {xq : q \u2208 N (u)\u2229N (v)} be the p\u00d7m matrix of images corresponding to the intersection of neighbours of u and neighbours of v (including u and v), where m is the cardinality of X(u,v). To see if (u, v) is safe, we interpolate the intermediate images on the tangent space spanned by X(u,v), obtained using PCA. The target dimension is the number of degrees of freedom d, and PCA maps X(u,v) to a Y (u,v) (d\u00d7m). In addition to Y (u,v), PCA also gives a p\u00d7 d orthonormal matrix W (u,v) such that X(u,v) = W (u,v)Y (u,v) or Y (u,v) = W (u,v) T\nX(u,v). We then interpolate between yu and yv to construct y(\u03b1) = \u03b1 \u2217 yu + (1 \u2212 \u03b1) \u2217 yv for various values of \u03b1 \u2208 (0, 1). For each \u03b1, the image x(\u03b1) = Wy(\u03b1) must be in free space. In practice, the resulting image is a poor interpolation, and rejects many valid edges; however, the probability of an edge being unsafe after being passed by the local planner is low (i.e. it is conservative).\nThe image obtained by a linear interpolation on the local tangent space (LTS) is a weighted sum of the images in X(u,v). Thus, for collision detection purposes, it is sufficient to look at the superimposition of images in X(u,v). This achieves the same effect as the PCA based method described above, but avoids the PCA computation.\n2) Ideal Tracked Points (ITP): Here we assume that a set of points on the robot body can be tracked in all poses (including occlusions). Then to see if an edge (u, v) is safe, we join each pair of corresponding tracked-points to create a new image. This image (i.e. the set of trajectories of the tracked points) are used for collision detection.\n3) Join of Nearest Shi-Tomasi features (JNST): In practice, occlusion precludes the tracking of any set of points on the robot body. Here we propose an approximation based on high-contrast points known as the Shi-Tomasi features [21]. We assume that each link of the robot can be separated and that the Shi-Tomasi features are computed on each link. Here we do not know the correspondences between points in the two images. The Join of Nearest Shi-Tomasi features approach (JNST) involves associating each feature point on each link in u with the nearest feature point in the corresponding link in v. We do the same in both directions and insert the joins on the image."}, {"heading": "C. Start and Goal states", "text": "For motion planning on the VRM, we need to map the source (s) and target (t) images onto the VRM G. We first ensure that the poses s, t themselves are in free space. We then add these to G and connect them with their k-nearest neighbours in X . We then run a local planner on the new edges and find the shortest path between s and t as before. Adding a new node (image) to the graph is a computation that requires O(nk) distance computation steps for finding. Time for distance calculation depends on the metric used. This approach again, is almost identical to traditional roadmap methods [20], except that the tests are all visual."}, {"heading": "V. EMPIRICAL ANALYSIS : METRICS AND LOCAL PLANNERS", "text": "Factors affecting the quality of paths in VRM include sampling density, the metric used, and the local planner. We now present an empirical study of these aspects (fig 7) on a planar 3-link simulated arm and a set of obstacles similar to those in figure 6."}, {"heading": "A. Gold Standard Local Planner", "text": "In the traditional Configuration Space, two configurations are assumed to be joined by a linear join between them. To see if an edge (u, v) is actually safe, we generate intermediate pose images by interpolating joint angle vectors at an resolution. We observe that a linear interpolation in joint angle space need not be the same as an interpoloation on visual C-space, but we assume the difference would be fairly small for a reasonable sampling density. If all these images are collision free, we treat (u, v) to be safe. The performance of the local planners is evaluated relative to this gold standard local planner. Results reported here use = 1\u25e6."}, {"heading": "B. Effect of Sampling Density and Distance Metric", "text": "Plots in figure 7a clearly suggest that the sampling density (i.e., the number of images used to construct the visual\nroadmap) heavily affects the fraction of unsafe edges and hence the quality of paths. The more dense the sample is, the better the paths.\nWe present the effect of several representations of the configuration space along with appropriate distance metric for each case. Table I lists the different representations and the corresponding distance metric used to compute neighbourhoods.\nTo find the distance between two images we just flatten all the channels of each image into a single vector and use the standard Euclidean (L2) distance on the resulting vectors. In our experiments we used 30,000 (100x100x3) dimensional vectors for image distance.\nRandom projections (RP [22], [23]) is a dimensionality reduction method that preserves L2 distances. In our experiments we projected the 30,000 dimensional image vectors onto 2000 Gaussian random unit vectors to obtain a 2000 dimensional representation of each image. The experiments show that the L2 distance of RP vectors does almost as well as that on the image vectors. Since the distance computation is done on much smaller vectors, the graph construction gets much faster while preserving the neighbourhoods.\nDistance between two joint angle vectors is computed as the sum of shortest circular-distance (i.e., treating 0 and 2\u03c0 to be the same angle) between individual components. This is in some sense the geodesic distance between the two vectors.\nThe ideal tracked point (ITP) L2 distance between two configurations is computed as the L2 distance between the vectors obtained by concatenating all the tracked point coordinates of each configuration.\nFinally, the Hausdorff distance between two configurations is computed as the sum of Hausdorff distances between the sets of Shi-Tomasi feature points on the corresponding links for the two configurations. Given two sets A and B, Hausdorff distance is defined as\ndH(A,B) = max{sup a\u2208A inf b\u2208B d(a, b), sup b\u2208B inf a\u2208A d(a, b)}.\nAs can be seen from figure 7b and table II, JNST local planner performs almost as well as ITP local planner."}, {"heading": "VI. DEMONSTRATIONS ON REAL ROBOTS", "text": ""}, {"heading": "A. Planar Scara robot", "text": "We now demonstrate the algorithm for a real robot, a Scara 4 DOF arm, in which two revolute joints move the first two links in a plane, so the motion has two degrees of freedom. We observe this robot with an overhead camera. 4000 images are sampled from a video while the robot is moving between random poses throughout its workspace, and the neighbourhood graph is computed. Thereafter, several obstacles are introduced in the workspace and the obstacles are discovered via background subtraction. Note that owing to the motion being planar, a single camera view is quite adequate. A planned path is shown in fig. 8."}, {"heading": "B. CRS A465 robot arm", "text": "Here we have a robot in a 3-D workspace. Clearly, a single camera view will not suffice for identifying collision situations. Hence we construct a joint manifold of multiple views by stitching the corresponding images together, and constructing a joint manifold on the combined image space. The 3-DOF workspace and a path is shown in fig. 9, with the obstacle nodes marked in black."}, {"heading": "VII. CONCLUSION", "text": "In this work, we have introduced a new approach towards the longstanding perceptual robotics problem, which subsumes the problem of body schema learning [4], [5], [7]. Although it has been long known that there may be many kinds of generalized coordinates, so far there have been few attempts in robotics to build on this intuition. The proposed paradigm attempts to develop such a non-traditional GC, and\napproximates the C-space that results from it in terms of a neighbourhood graph on the set of images. We show how such a formulation for tasks such as inverse kinematic or for motion planning.\nUnlike in methods used in robotics today, the Visual Generalized Coordinates approach eliminates several expensive aspects of robot modelling and planning. First, it does not require a humans to create models for robot geometry or kinematics. It does not require precise obstacle shapes and poses, and does not require to calibrate the cameras so that this can be done. There is no need for a precise simulator to test which poses collide with obstacles and which do not. Even the local planner step, based on tracking image points to nearby images, results in a more principled approach than is available presently.\nAnother advantage is for environments that are changing rapidly, e.g. in interaction with humans or other robots. New obstacles are updated in O(n) time, but small motions by another agent require O(m), where there are m nodes near the obstacle boundary.\nThe idea of generalized coordinates originated in Lagrangian dynamics, and here is another direction that needs to be pursued. Differentiating the GC would result in generalized velocities and accelerations and this may give rise to a visual dynamics.\nHowever, there are some significant trade-offs. First, the approach is not complete because the obstacle approximations is conservative, and there may exist paths which it cannot find. We observe that humans also face similar constraints where the vision is less informative. Secondly, it is applicable to situations where the entire C-Space is visible. While the algorithms are reasonably efficient in time complexity, the space costs are higher (O(np)), since all landmark images need to be stored. Another constraint is the Visual Distinguishability assumption, but this may not be very serious in practice.\nThe approach presented is only a beginning for discovering generalized coordinates from sensorimotor data. One of the key future steps would be to fuse modalities other than vision into a joint manifold. Thus, if we were to construct a fused visuo-motor manifold, then even if poses that are separated in motion space look similar, they would remain distinguishable. Similarly touch stimuli could be modelled to predict the result of motions or in preparation for finemotor tasks. Such a process would also make the model more robust against noise arising in any single modality. On the whole, while the ideas presented seem promising, and open up many possibilities, much work remains to deploy Visual Generalized Coordinates fully in theory and in practice."}], "references": [{"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Engelberger, Robotics in practice: management and applications of industrial robots", "author": ["F. J"], "venue": "Kogan Page,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1980}, {"title": "L\u2019espace et la geometrie", "author": ["H. Poincare"], "venue": "Science and Hypothesis, 1895, pp. 60\u201371, transl. W. J. Greenstreet, 1905.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1895}, {"title": "Body schema in robotics: a review", "author": ["M. Hoffmann", "H.G. Marques", "A. Hernandez Arieta", "H. Sumioka", "M. Lungarella", "R. Pfeifer"], "venue": "Autonomous Mental Development, IEEE Transactions on, vol. 2, no. 4, pp. 304\u2013324, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Map learning with uninterpreted sensors and effectors", "author": ["D. Pierce", "B. Kuipers"], "venue": "Artificial Intelligence, vol. 92, pp. 169\u2013229, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Perception of the structure of the physical world using unknown multimodal sensors and effectors", "author": ["D. Philipona", "J. O\u2019Regan", "J. Nadal", "O. Coenen"], "venue": "Advances in neural information processing systems, vol. 16, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Cognitive navigation based on nonuniform gabor space sampling, unsupervised growing networks, and reinforcement learning", "author": ["A. Arleo", "F. Smeraldi", "W. Gerstner"], "venue": "Neural Networks, IEEE Transactions on, vol. 15, no. 3, pp. 639\u2013652, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Sensor map discovery for developing robots", "author": ["J. Stober", "L. Fishgold", "B. Kuipers"], "venue": "AAAI Fall Symposium on Manifold Learning and Its Applications, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On-line regression algorithms for learning mechanical models of robots: a survey", "author": ["O. Sigaud", "C. Sala\u00fcn", "V. Padois"], "venue": "Robotics and Autonomous Systems, vol. 59, no. 12, pp. 1115\u20131129, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Approaches to Probabilistic Model Learning for Mobile Manipulation Robots", "author": ["J. Sturm"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Path-planning for visual servoing: a review and issues", "author": ["M. Kazemi", "K. Gupta", "M. Mehrandezh"], "venue": "Visual Servoing via Advanced Numerical Methods, 2010, pp. 189\u2013207.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "An action perspective on motor development", "author": ["C. Von Hofsten"], "venue": "Trends in cognitive sciences, vol. 8, no. 6, pp. 266\u2013272, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Motor development", "author": ["K. Adolph", "S. Berger"], "venue": "Handbook of child psychology, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagination and abstraction of sensorimotor flow: Towards a robot model", "author": ["J. Stening", "H. Jacobsson", "T. Ziemke"], "venue": "Proceedings of the AISB05 Symposium on Next Generation approaches to Machine Consciousness: Imagination, Development, Intersubjectivity, and Embodiment, 2005, pp. 50\u201358.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic roadmaps for path planning in high-dimensional configuration spaces", "author": ["L.E. Kavraki", "P. \u0160vestka", "J.-C. Latombe", "M.H. Overmars"], "venue": "Robotics and Automation, IEEE Transactions on, vol. 12, no. 4, pp. 566\u2013580, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Dimension reduction by local principal component analysis", "author": ["N. Kambhatla", "T.K. Leen"], "venue": "Neural Computation, vol. 9, no. 7, pp. 1493\u2013 1516, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "A better scaled local tangent space alignment algorithm", "author": ["J. Yang", "F. Li", "J. Wang"], "venue": "Neural Networks, 2005. IJCNN\u201905. Proceedings. 2005 IEEE International Joint Conference on, vol. 2, 2005, pp. 1006\u2013 1011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Principles of Robot Motion: Theory, Algorithms, and Implementations", "author": ["H. Choset", "K.M. Lynch", "S. Hutchinson", "G.A. Kantor", "W. Burgard", "L.E. Kavraki", "S. Thrun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "1994 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201994), 1994, pp. 593 \u2013 600.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 245\u2013250.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Experiments with random projection", "author": ["S. Dasgupta"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 2000, pp. 143\u2013151.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 1, "context": "see [3] ch.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "There have been two methods for approaching this problem - either based on learning a body schema [4]\u2013[9], or by fitting a canonical robot model [10].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "There have been two methods for approaching this problem - either based on learning a body schema [4]\u2013[9], or by fitting a canonical robot model [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "There have been two methods for approaching this problem - either based on learning a body schema [4]\u2013[9], or by fitting a canonical robot model [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "Body schema approaches have not scaled up to full scale robotic models or used for global motion planning, and robot model regression requires intrusive structures on the robot [11] and even then", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "(d) The image manifold V , visualized here in R using the Isomap algorithm [2], shows that the robot images - from a 570\u00d7570-dimensional image space - lie on the 2-D surface of a torus.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "posed by stitching together local visual servos [12], but these require that the goal be constantly visible.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "In fact, this idea draws inspiration from proposals for how an infant learns to use its limbs [13], [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "In fact, this idea draws inspiration from proposals for how an infant learns to use its limbs [13], [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "One could then \u201cimagine\u201d the consequence of a motor command, and compare these quickly to find discrepancies [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "This can often be effectively approximated by the maximum distance covered by any point [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "Image space neighbourhoods are then used to construct a local-PCA based nonlinear manifold [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Isomap [2]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "However, such methods have difficulty in introducing new data points and in interpolating local data, so we avoid computing the manifold altogether, and restrict ourselves to a piecewise algorithm, as in [18], [19].", "startOffset": 204, "endOffset": 208}, {"referenceID": 16, "context": "However, such methods have difficulty in introducing new data points and in interpolating local data, so we avoid computing the manifold altogether, and restrict ourselves to a piecewise algorithm, as in [18], [19].", "startOffset": 210, "endOffset": 214}, {"referenceID": 17, "context": "For example, the C-space of a freely-rotating 2-DOF articulated robot is S \u00d7 S = T, which is a torus [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "[2]) assume that the target space for dimensionality reduction is a euclidean space (a subspace of R).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Here we propose an approximation based on high-contrast points known as the Shi-Tomasi features [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "This approach again, is almost identical to traditional roadmap methods [20], except that the tests are all visual.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Random projections (RP [22], [23]) is a dimensionality reduction method that preserves L2 distances.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Random projections (RP [22], [23]) is a dimensionality reduction method that preserves L2 distances.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "In this work, we have introduced a new approach towards the longstanding perceptual robotics problem, which subsumes the problem of body schema learning [4], [5], [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "In this work, we have introduced a new approach towards the longstanding perceptual robotics problem, which subsumes the problem of body schema learning [4], [5], [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "In this work, we have introduced a new approach towards the longstanding perceptual robotics problem, which subsumes the problem of body schema learning [4], [5], [7].", "startOffset": 163, "endOffset": 166}], "year": 2015, "abstractText": "An open problem in robotics is that of using vision to identify a robot\u2019s own body and the world around it. Many models attempt to recover the traditional C-space parameters. Instead, we propose an alternative C-space by deriving generalized coordinates from n images of the robot. We show that the space of such images is bijective to the motion space, so these images lie on a manifold V homeomorphic to the canonical C-space. We now approximate this manifold as a set of n neighbourhood tangent spaces that result in a graph, which we call the Visual Roadmap (VRM). Given a new robot image, we perform inverse kinematics visually by interpolating between nearby images in the image space. Obstacles are projected onto the VRM in O(n) time by superimposition of images, leading to the identification of collision poses. The edges joining the free nodes can now be checked with a visual local planner, and free-space motions computed in O(nlogn) time. This enables us to plan paths in the image space for a robot manipulator with unknown link geometries, DOF, kinematics, obstacles, and camera pose. We sketch the proofs for the main theoretical ideas, identify the assumptions, and demonstrate the approach for both articulated and mobile robots. We also investigate the feasibility of the process by investigating various metrics and image sampling densities, and demonstrate it on simulated and real robots.", "creator": "LaTeX with hyperref package"}}}