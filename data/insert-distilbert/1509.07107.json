{"id": "1509.07107", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "On The Direct Maximization of Quadratic Weighted Kappa", "abstract": "therefore in recent years, quadratic weighted kappa has mostly been growing especially in popularity in the machine learning community as an evaluation assignment metric in domains where the overall target labels to be predicted are drawn from integer ratings, usually obtained from human experts. for example, it was the jury metric of choice in several recent, similarly high profile machine learning contests hosted on kaggle : www. kaggle. com / c / asap - aes, & www. kaggle. com / c / asap - sas, www. kaggle. org com / c / \\ diabeticretinopathy - detection. yet, little is understood precisely about discovering the nature involved of this metric, its internal underlying experimental mathematical properties, where it fits reasonably among other common evaluation metrics such as mean squared error ( mse ) and correlation, possibly or if it can be manually optimized analytically, and if so, remember how. much of this is due to the cumbersome way that this metric is commonly geographically defined. in this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ quite this alternate form to address arguably the above issues.", "histories": [["v1", "Wed, 23 Sep 2015 19:39:39 GMT  (209kb,D)", "http://arxiv.org/abs/1509.07107v1", "preliminary draft"], ["v2", "Tue, 29 Sep 2015 21:30:43 GMT  (199kb,D)", "http://arxiv.org/abs/1509.07107v2", "preliminary draft"], ["v3", "Sun, 6 Dec 2015 15:16:19 GMT  (0kb,I)", "http://arxiv.org/abs/1509.07107v3", "realized some inaccuracies, and some sloppy reasoning. Need some time to fix"]], "COMMENTS": "preliminary draft", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david vaughn", "derek justice"], "accepted": false, "id": "1509.07107"}, "pdf": {"name": "1509.07107.pdf", "metadata": {"source": "CRF", "title": "On The Direct Maximization of Quadratic Weighted Kappa", "authors": ["David Vaughn", "Derek Justice"], "emails": ["DVAUGHN@MEASINC.COM", "DJUSTICE@MEASINC.COM"], "sections": [{"heading": null, "text": "Yet, little is understood about the nature of this metric, its underlying mathematical properties, where it fits among other common evaluation metrics such as mean squared error (MSE) and correlation, or if it can be optimized analytically, and if so, how. Much of this is due to the cumbersome way that this metric is commonly defined.\nIn this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ this alternate form to address the above issues."}, {"heading": "1. Preliminaries", "text": "Although first developed in the statistical community as a measure of inter-rater agreement, \u03ba has more recently become a popular performance metric in supervised machine learning, specifically in situations where the target (dependent) variable y is a discrete, interval variable (usually drawn from non-negative integers) such as is common in most human rating scales (e.g.\u201con a scale from 1 to 10\u201d).\nThis differs from the ordinal regression setting, where there only exists an ordering over labels, but no intrinsic or constant length interval between them. Some have argued that the use of quadratic weighted kappa as a metric in the domain of human ratings imposes the erroneous assumption of \u201cequal intervals\u201d where there should be none (how this assumption is expressed in the metric itself will be made\nclear in the following section). For example, when rating student essays on a scale from 1 to 5, the difference between a 1 and a 2 may not be equal to the difference between a 4 and a 5. While this may or may not be true in certain cases, we will not be concerned with that here."}, {"heading": "1.1. Standard Definition", "text": "Quadratic weighted kappa, which we write \u03ba to distinguish from linear weighted kappa, was originally developed as a measure of inter-rater agreement. In this scenario, there are two raters, A and B, each associated with a vector of n integer ratings a,b \u2208 Ln\u00d71 where L = {1, 2, \u00b7 \u00b7 \u00b7 , `} is a finite set of ` possible values. We seek to quantify the level of agreement between a and b. In order to compute \u03ba(a,b), it is customary to start by computing frequency tables. The observed confusion matrix U = (ui,j) \u2208 N`\u00d7` is first computed as:\nui,j =\nn\u2211\nk=1\nI(ak = i) \u00b7 I(bk = j)\nNext, the expected confusion matrix V = (vi,j) \u2208 R`\u00d7` is computed by assuming there is no correlation between raters. Under this assumption, we can simply compute V as the outer product between the two rater\u2019s observed marginal distributions, normalized so that V has the same sum as U ( i.e. \u2211` i,j=1 vi,j = \u2211` i,j=1 ui,j ) :\nV = (U \u00b7 e)\u2297 (U> \u00b7 e)\nn\nwhere e denotes the all ones vector. Finally, a matrix of weights W = (wi,j) \u2208 R`\u00d7` is defined as:\nwi,j = (i\u2212 j)2 (`\u2212 1)2\nGiven U, V, and W, it is customary to define \u03ba as:\n\u03ba(a,b) = 1\u2212 \u2211` i,j=1 ui,j \u00b7 wi,j\u2211` i,j=1 vi,j \u00b7 wi,j = 1\u2212 \u3008U,W\u3009F\u3008V,W\u3009 F (1)\nwhere \u3008\u00b7, \u00b7\u3009 F denotes the Frobenius inner product.\nar X\niv :1\n50 9.\n07 10\n7v 1\n[ cs\n.L G\n] 2\n3 Se\np 20\n15"}, {"heading": "1.2. Alternate Form", "text": "Assume a data matrix X = [x>1 , \u00b7 \u00b7 \u00b7 ,x>n ]> \u2208 Rn\u00d7d of n points in d dimensional space, and a vector y \u2208 Ln\u00d71 of n integer labels where L = {1, 2, \u00b7 \u00b7 \u00b7 , `} is a finite set of possible labels. We assume there is some true functional relationship yi = f(xi) between each point xi and it\u2019s label yi. The goal is to find a function y\u0302i = f\u0302(xi) which approximates the true function as closely as possible. Using this notation, we can begin to rewrite the standard definition of \u03ba by first noting that the numerator simply represents a standard sum of squared errors:\n\u3008U,W\u3009 F =\nn\u2211\nk=1\n(yk \u2212 f\u0302(xk))2 (`\u2212 1)2 = \u2016y \u2212 y\u0302\u20162 (`\u2212 1)2\nNext, we note that the denominator can be similarly rewritten in terms of y and y\u0302:\n\u3008V,W\u3009 F = \u2016y\u20162 \u2212 2n (y>e)(y\u0302>e) + \u2016y\u0302\u20162\n(`\u2212 1)2\nBy combining these expressions we can derive a simplified expression for \u03ba purely in terms of y and y\u0302, without the need for contingency tables:\n\u03ba(y, y\u0302) = 1\u2212 \u2016y \u2212 y\u0302\u2016 2\n\u2016y\u20162 \u2212 2n (y>e)(y\u0302>e) + \u2016y\u0302\u20162\n= 1\u2212 \u2016y\u2016 2 \u2212 2y>y\u0302 + \u2016y\u0302\u20162\n\u2016y\u20162 \u2212 2n (y>e)(y\u0302>e) + \u2016y\u0302\u20162\n= 2[y \u2212 e(y>en )] >y\u0302\n\u2016y\u20162 \u2212 2n (y>e)(y\u0302>e) + \u2016y\u0302\u20162\nIf we drop the integer constraint on labels then we can assume that y has been centered, which allows further simplification:\n\u03ba(y, y\u0302) = 2\u3008y \u00b7 y\u0302\u3009\n\u2016y\u20162 + \u2016y\u0302\u20162 (2)\nThroughout the rest of this paper we will assume (without loss of generality) that y is centered."}, {"heading": "2. Linear Model", "text": "We consider a simple linear model for y = f(X):\nf(X) = X\u03b1\nand so we seek the function parameter \u03b1\u0302 that maximizes \u03ba(\u03b1), where\n\u03ba(\u03b1) = 2\u3008y \u00b7X\u03b1\u3009\n\u2016y\u20162 + \u2016X\u03b1\u20162\nWe will rely on the fact that \u03ba(\u03b1) is strictly quasiconcave (see appendix for proof), since it is known that any local maximum of a strictly quasiconcave function f is also a global maximum (Avriel, 2003). And so we can apply the method of Lagrange multipliers to find a local maximum, and conclude that this is the global maximum as well. To begin, we can use the variable substitution \u03b3 = \u2016y\u20162 + \u2016X\u03b1\u20162 to write this as a constrained optimization problem:\nmax \u03b1,\u03b3\n2 \u03b3 \u3008y \u00b7X\u03b1\u3009\nsubject to \u03b3 = \u2016y\u20162 + \u2016X\u03b1\u20162\nFrom here we can define the Lagrangian:\nL(\u03b1, \u03b3, \u03bb) = 2\n\u03b3 \u3008y \u00b7X\u03b1\u3009+ \u03bb(\u03b3 \u2212 \u2016y\u20162 \u2212 \u2016X\u03b1\u20162)\n= 2\n\u03b3 y>X\u03b1+ \u03bb(\u03b3 \u2212 y>y \u2212\u03b1>X>X\u03b1)\nWe solve by first differentiating w.r.t. \u03b3 and setting equal to zero:\n\u2202L\n\u2202\u03b3 \u2223\u2223\u2223\u2223 \u03b1\u0302,\u03b3\u0302,\u03bb\u0302\n= \u2212 2 \u03b3\u03022 y>X\u03b1\u0302+ \u03bb\u0302 = 0\n\u03bb\u0302 = 2\n\u03b3\u03022 y>X\u03b1\u0302 (3)\nThen w.r.t. \u03b1:\n\u2202L\n\u2202\u03b1 \u2223\u2223\u2223\u2223 \u03b1\u0302,\u03b3\u0302,\u03bb\u0302\n= 2\n\u03b3\u0302 X>y \u2212 2\u03bb\u0302X>X\u03b1\u0302 = 0\nSubstituting in the previous expression (3) for \u03bb\u0302 yields:\nX>y\n\u03b3\u0302 \u2212 2y\n>X\u03b1\u0302 \u03b3\u03022 X>X\u03b1\u0302 = 0\n2y>X\u03b1\u0302\n\u03b3\u0302 X>X\u03b1\u0302 = X>y\nNow substituting back in for \u03b3\u0302 gives a surprising result:\n2y>X\u03b1\u0302\n\u2016y\u20162 + \u2016X\u03b1\u0302\u20162X >X\u03b1\u0302 = X>y\n\u03ba\u0302X>X\u03b1\u0302 = X>y\n\u03b1\u0302 = 1\n\u03ba\u0302 (X>X) \u22121 X>y (4)\nwhere we have simplified by recognizing the expression for \u03ba\u0302 = \u03ba(\u03b1\u0302). In addition, we recognize the \u201cleast-squares\u201d solution \u03b1\u0302`s = (X >X) \u22121 X>y, the well studied minimizer of the quantity \u2016y \u2212X\u03b1\u2016, a.k.a. the norm of the residual. This means that \u03b1\u0302 (which we will write \u03b1\u0302\u03ba when necessary to avoid confusion) and the least-squares solution \u03b1\u0302`s are related via \u03ba\u0302 by a surprisingly simple formula:\n\u03b1\u0302\u03ba = 1\n\u03ba\u0302 \u03b1\u0302`s (5)\nIn order to obtain a closed form solution for \u03b1\u0302\u03ba we must solve for \u03ba\u0302 solely in terms of X and y. We employ another useful tool from least-squares regression, the \u201chat\u201d matrix H = X(X>X)\u22121X>. The hat matrix allows the least-squares labels y\u0302`s to be expressed solely in terms of the original labels y:\ny\u0302`s = X\u03b1\u0302`s = Hy (6)\nUsing the well-known fact that the hat matrix is both symmetric and idempotent (H>H = H2 = H), we can establish the following identity:\n\u2016y\u0302`s\u20162 = (Hy)>(Hy) = y>H>Hy\n= y>Hy\n= y>X\u03b1\u0302`s = \u3008y \u00b7X\u03b1\u0302`s\u3009 (7)\nNow we can take the derived expression for \u03b1\u0302\u03ba from (5) and substitute it back into the original expression for \u03ba and apply the above identity (7) to solve for \u03ba\u0302:\n\u03ba\u0302 = \u03ba(\u03b1\u0302\u03ba) = 2\u3008y \u00b7X\u03b1\u0302\u03ba\u3009\n\u2016y\u20162 + \u2016X\u03b1\u0302\u03ba\u20162 = 2\u3008y \u00b7 1\u03ba\u0302X\u03b1\u0302`s\u3009\n\u2016y\u20162 + \u2016 1\u03ba\u0302X\u03b1\u0302`s\u20162\n= 2 \u03ba\u0302 \u3008y \u00b7X\u03b1\u0302`s\u3009\n\u2016y\u20162 + 1\u03ba\u03022 \u2016X\u03b1\u0302`s\u20162\n= 2\u03ba\u0302\u2016y\u0302`s\u20162\n\u03ba\u03022\u2016y\u20162 + \u2016y\u0302`s\u20162 \u03ba\u03022\u2016y\u20162 + \u2016y\u0302`s\u20162 = 2\u2016y\u0302`s\u20162\n\u03ba\u0302 = \u2016y\u0302`s\u2016 \u2016y\u2016 (8)\nFinally we can express \u03b1\u0302 solely in terms of X and y:\n\u03b1\u0302\u03ba = 1\n\u03ba\u0302 \u03b1\u0302`s = \u2016y\u2016 \u2016y\u0302`s\u2016 \u03b1\u0302`s = \u2016y\u2016 \u2016y\u0302`s\u2016 (X>X) \u22121 X>y\n=\n(\u221a y>y\ny>X(X>X) \u22121 X>y\n) (X>X) \u22121 X>y\n=\n(\u221a y>y\ny>Hy\n) (X>X) \u22121 X>y (9)"}, {"heading": "2.1. Regularization", "text": "It is often useful to include a regularization term in the optimization to account for cases where X>X is singular or ill-conditioned, which could lead to a numerically unstable (or non-existent) solution. In addition, it provides a convenient mechanism for manually tuning the bias/variance trade-off. It is easy to accommodate a norm penalty term within the current framework, yielding a slightly modified optimization problem:\nmax \u03b1,\u03b3\n2 \u03b3 \u3008y \u00b7X\u03b1\u3009 \u2212 \u00b5\u2016\u03b1\u20162\nsubject to \u03b3 = \u2016y\u20162 + \u2016X\u03b1\u20162\nwhere \u00b5 controls the magnitude of the penalty, and is usually set experimentally (e.g. with cross-validation). This leads to a similarly modified Lagrangian:\nL(\u03b1, \u03b3, \u03bb) = 2\n\u03b3 y>X\u03b1\u2212 \u00b5\u03b1>\u03b1+ \u03bb(\u03b3 \u2212 y>y \u2212\u03b1>X>X\u03b1)\nProceeding in the same manner as the last section, differentiating w.r.t. \u03b3 and setting equal to zero yields the same result (3) for \u03bb\u0302. Differentiating w.r.t. \u03b1 yields a slightly different result:\n\u2202L\n\u2202\u03b1 \u2223\u2223\u2223\u2223 \u03b1\u0302,\u03b3\u0302,\u03bb\u0302\n= 2\n\u03b3\u0302 X>y \u2212 2\u03bb\u0302X>X\u03b1\u0302\u2212 2\u00b5\u03b1\u0302 = 0\nAgain, substituting in for \u03bb\u0302 and manipulating the result, we end up with:\n\u03ba\u0302X>X\u03b1\u0302+ \u00b5\u03b3\u0302\u03b1\u0302 = X>y\n\u03b1\u0302 = 1\n\u03ba\u0302 (X>X+\n\u00b5\u03b3\u0302\n\u03ba\u0302 I) \u22121 X>y\nIn order to obtain a closed form solution for \u03b1\u0302 all that is important to recognize is that we once again know the optimal solution to within a factor \u03d5 = 1\u03ba\u0302 , just as in the regularized case. The only difference is that now, instead of \u03b1\u0302 just being within a scalar multiple of the least-squares solution,\n\u03b1\u0302 is now within a scalar multiple of the regularized least squares solution, i.e. the ridge regression solution \u03b1\u0302rr. As \u00b5 is just a regularization tuning parameter, it can simply absorb the \u03ba\u0302 and \u03b3\u0302. We make the substitution \u03c3 = \u00b5\u03b3\u0302\u03ba\u0302 to make this more clear:\n\u03b1\u0302\u03ba = 1\n\u03ba\u0302 (X>X+ \u03c3I) \u22121 X>y =\n1 \u03ba\u0302 \u03b1\u0302rr (10)\nJust as in the previous case, we can take the derived expression for \u03b1\u0302\u03ba from (10) and substitute it back into the original expression for \u03ba to solve for \u03ba\u0302:\n\u03ba\u0302 = \u03ba(\u03b1\u0302\u03ba) = 2\u3008y \u00b7X\u03b1\u0302\u03ba\u3009\n\u2016y\u20162 + \u2016X\u03b1\u0302\u03ba\u20162 = 2\u3008y \u00b7 1\u03ba\u0302X\u03b1\u0302rr\u3009\n\u2016y\u20162 + \u2016 1\u03ba\u0302X\u03b1\u0302rr\u20162\n= 2 \u03ba\u0302 \u3008y \u00b7X\u03b1\u0302rr\u3009\n\u2016y\u20162 + 1\u03ba\u03022 \u2016X\u03b1\u0302rr\u20162 \u03ba\u03022\u2016y\u20162 + \u2016X\u03b1\u0302rr\u20162 = 2\u3008y \u00b7X\u03b1\u0302rr\u3009\n\u03ba\u0302 =\n\u221a 2\u3008y \u00b7X\u03b1\u0302rr\u3009 \u2212 \u2016X\u03b1\u0302rr\u20162\n\u2016y\u2016 (11)\nIn this case the result does not simplify as nicely as in the non-regularized case, due to the fact that H\u03c3 (where H\u03c3 = X(X>X + \u03c3I) \u22121 X> denotes the regularized variant of the \u201chat\u201d matrix) is no longer idempotent. Still, we can use H\u03c3 to write a somewhat simplified expression for \u03ba\u0302:\n\u03ba\u0302 =\n\u221a 2y>H\u03c3y \u2212 y>H2\u03c3y\ny>y (12)\nFinally we can express the regularized solution \u03b1\u0302\u03ba in closed form:\n\u03b1\u0302\u03ba = 1\n\u03ba\u0302 \u03b1\u0302rr\n=\n(\u221a y>y\n2y>H\u03c3y \u2212 y>H2\u03c3y\n) (X>X+ \u03c3I) \u22121 X>y\n(13)"}, {"heading": "3. Link to Correlation", "text": "There is an interesting link between quadratic weighted kappa and correlation. There are many (equivalent) definitions of correlation, but the simplest is just the normalized dot product between two vectors u,v:\n\u03c1(u,v) = \u3008u \u00b7 v\u3009 \u2016u\u2016\u2016v\u2016\nNow returning to the (non-regularized) \u03ba-optimal solution labels y\u0302\u03ba = X\u03b1\u0302\u03ba, we can expand the expression for \u03c1(y, y\u0302\u03ba) and apply (7) and (8) to simplify:\n\u03c1(y, y\u0302\u03ba) = \u3008y \u00b7 y\u0302\u03ba\u3009 \u2016y\u2016\u2016y\u0302\u03ba|\n= \u3008y \u00b7X\u03b1\u0302\u03ba\u3009 \u2016y\u2016\u2016X\u03b1\u0302\u03ba| = 1 \u03ba\u0302 \u3008y \u00b7X\u03b1\u0302`s\u3009 \u2016y\u2016\u2016 1\u03ba\u0302X\u03b1\u0302`s\u2016 = 1 \u03ba\u0302\u2016y\u0302`s\u20162\n1 \u03ba\u0302\u2016y\u2016\u2016y\u0302`s\u2016\n= \u2016y\u0302`s\u2016 \u2016y\u2016 = \u03ba(y, y\u0302\u03ba) = \u03ba\u0302 . (14)\nAnd so while correlation is, in general, not equivalent to quadratic weighted kappa, they do coincide precisely at the point where kappa is maximized."}, {"heading": "Appendix: Proof of Strict Quasiconcavity", "text": "We wish to show that the function\n\u03ba(\u03b1) = 2\u3008y \u00b7X\u03b1\u3009\n\u2016y\u20162 + \u2016X\u03b1\u20162\nis strictly quasiconcave. A function f is said to be strictly quasiconcave if, for any two points u,v and any positive weight \u03b8 < 1, the following is true (Avriel, 2003):\nf(\u03b8u+ (1\u2212 \u03b8)v) > min[f(u), f(v)] (A-1)\nLet us assume, without loss of generality, that min[\u03ba(u), \u03ba(v)] = \u03ba(u), which leads us to write:\n\u03ba(v) \u2265 \u03ba(u) 2\u3008y \u00b7Xv\u3009\n\u2016y\u20162 + \u2016Xv\u20162 \u2265 2\u3008y \u00b7Xu\u3009 \u2016y\u20162 + \u2016Xu\u20162\ny>Xv \u2265 y>Xu (\u2016y\u20162 + \u2016Xv\u20162 \u2016y\u20162 + \u2016Xu\u20162 ) (A-2)\nReturning to the the definition of strictly quasiconcave and expanding the expression for \u03ba(\u03b8u+ (1\u2212 \u03b8)v) gives us:\n\u03ba(\u03b8u+ (1\u2212 \u03b8)v) = 2\u03b8y >Xu+ 2(1\u2212 \u03b8)y>Xv\n\u2016y\u20162 + \u2016\u03b8Xu+ (1\u2212 \u03b8)Xv\u20162\nNow, we know that the function h(z) = \u2016Xz\u20162 is strictly convex (because \u2016Xz\u20162 = z>X>Xz, where X>X 0 since X>X is assumed non-singular). By the definition of strict convexity, this means:\n\u2016\u03b8Xu+ (1\u2212 \u03b8)Xv\u20162 < \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162\nTherefore we can substitute the expression on the right into the previous equation to obtain:\n\u03ba(\u03b8u+ (1\u2212 \u03b8)v) > 2\u03b8y >Xu+ 2(1\u2212 \u03b8)y>Xv\n\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162 (A-3)\nWe can then substitute the expression for y>Xv from the right side of (A-2) into the above, leading to:\n2\u03b8y>Xu+ 2(1\u2212 \u03b8)y>Xv \u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162 \u2265\n2\u03b8y>Xu+ 2(1\u2212 \u03b8)y>Xu ( \u2016y\u20162+\u2016Xv\u20162 \u2016y\u20162+\u2016Xu\u20162 )\n\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162\n= 2y>Xu\n( \u03b8 + (1\u2212 \u03b8) ( \u2016y\u20162+\u2016Xv\u20162 \u2016y\u20162+\u2016Xu\u20162 ))\n\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162\n= 2y>Xu\n( \u03b8(\u2016y\u20162+\u2016Xu\u20162)+(1\u2212\u03b8)(\u2016y\u20162+\u2016Xv\u20162) \u2016y\u20162+\u2016Xu\u20162 )\n\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162\n=\n2y>Xu \u2016y\u20162+\u2016Xu\u20162 (\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162)\n\u2016y\u20162 + \u03b8\u2016Xu\u20162 + (1\u2212 \u03b8)\u2016Xv\u20162 = \u03ba(u) .\nCombining this result with (A-3) we have\n\u03ba(\u03b8u+ (1\u2212 \u03b8)v) > \u03ba(u)\nor, stated another way\n\u03ba(\u03b8u+ (1\u2212 \u03b8)v) > min[\u03ba(u), \u03ba(v)]\nwhich is what we were trying to prove."}], "references": [{"title": "Nonlinear programming: analysis and methods", "author": ["Avriel", "Mordecai"], "venue": "Courier Corporation,", "citeRegEx": "Avriel and Mordecai.,? \\Q2003\\E", "shortCiteRegEx": "Avriel and Mordecai.", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "In recent years, quadratic weighted kappa has been growing in popularity in the machine learning community as an evaluation metric in domains where the target labels to be predicted are drawn from integer ratings, usually obtained from human experts. For example, it was the metric of choice in several recent, high profile machine learning contests hosted on Kaggle : www.kaggle.com/c/asap-aes , www.kaggle.com/c/asap-sas , www.kaggle.com/c/diabeticretinopathy-detection . Yet, little is understood about the nature of this metric, its underlying mathematical properties, where it fits among other common evaluation metrics such as mean squared error (MSE) and correlation, or if it can be optimized analytically, and if so, how. Much of this is due to the cumbersome way that this metric is commonly defined. In this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ this alternate form to address the above issues. 1. Preliminaries Although first developed in the statistical community as a measure of inter-rater agreement, \u03ba has more recently become a popular performance metric in supervised machine learning, specifically in situations where the target (dependent) variable y is a discrete, interval variable (usually drawn from non-negative integers) such as is common in most human rating scales (e.g.\u201con a scale from 1 to 10\u201d). This differs from the ordinal regression setting, where there only exists an ordering over labels, but no intrinsic or constant length interval between them. Some have argued that the use of quadratic weighted kappa as a metric in the domain of human ratings imposes the erroneous assumption of \u201cequal intervals\u201d where there should be none (how this assumption is expressed in the metric itself will be made clear in the following section). For example, when rating student essays on a scale from 1 to 5, the difference between a 1 and a 2 may not be equal to the difference between a 4 and a 5. While this may or may not be true in certain cases, we will not be concerned with that here. 1.1. Standard Definition Quadratic weighted kappa, which we write \u03ba to distinguish from linear weighted kappa, was originally developed as a measure of inter-rater agreement. In this scenario, there are two raters, A and B, each associated with a vector of n integer ratings a,b \u2208 Ln\u00d71 where L = {1, 2, \u00b7 \u00b7 \u00b7 , `} is a finite set of ` possible values. We seek to quantify the level of agreement between a and b. In order to compute \u03ba(a,b), it is customary to start by computing frequency tables. The observed confusion matrix U = (ui,j) \u2208 N`\u00d7` is first computed as:", "creator": "LaTeX with hyperref package"}}}