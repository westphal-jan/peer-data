{"id": "1509.07211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Noise-Robust ASR for the third 'CHiME' Challenge Exploiting Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent Neural Network", "abstract": "in this paper, the lingban entry to the third'chime'selective speech separation and recognition challenge is presented. a time - frequency frequency masking based automatic speech enhancement front - end is partially proposed trying to suppress the environmental noise utilizing multi - channel speaker coherence and spatial cues. the required state - of - the - concept art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state sensor space layer minimum bayes risk vector based discriminative acoustic modeling, and i - vector constraint based acoustic condition modeling, are carefully integrated into the speech recognition back - end. to further improve the system performance by fully adaptive exploiting the advantages of different technologies, the recommended final recognition results criterion are obtained by lattice combination and rescoring. evaluations carried out on the official dataset prove inconsistent the effectiveness of the proposed systems. comparing with the best baseline result, the proposed system obtains consistent improvements with over 57 % relative predicted word error rate reduction on the real - data test accuracy set.", "histories": [["v1", "Thu, 24 Sep 2015 02:16:11 GMT  (95kb,D)", "http://arxiv.org/abs/1509.07211v1", "The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages, 1 figure"]], "COMMENTS": "The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["zaihu pang", "fengyun zhu"], "accepted": false, "id": "1509.07211"}, "pdf": {"name": "1509.07211.pdf", "metadata": {"source": "CRF", "title": "NOISE-ROBUST ASR FOR THE THIRD \u2018CHIME\u2019 CHALLENGE EXPLOITING TIME-FREQUENCY MASKING BASED MULTI-CHANNEL SPEECH ENHANCEMENT AND RECURRENT NEURAL NETWORK", "authors": ["Zaihu Pang", "Fengyun Zhu"], "emails": ["fyzhu}@ling-ban.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 Noise-robust ASR, Multi-channel speech enhancement, Time-frequency masking, Recurrent neural network, CHiME challenge\n1. INTRODUCTION\nFar-field noise-robust automatic speech recognition (ASR) in real-world environments is still a challenging problem. The series of \u2018CHiME\u2019 speech separation and recognition challenges offered a great opportunity for the researchers from the signal processing community and the ASR community to work collaboratively toward this goal. The past \u2018CHiME\u2019 challenges have contributed to the development of several speech enhancement techniques, and novel framework that integrate speech enhancement and ASR [1, 2].\nThe main difference between the current \u2018CHiME\u2019 challenge and the past ones is instead of working on simulated data only, the current challenge focuses on real-world data: multi-channel recording using mobile tablet device in a variety of noisy public environments [3]. How to make use of\nboth the real and simulated data to improve the system performance remains an open question for the current challenge.\nIn this paper, we presented the Lingban entry to the 3rd \u2018CHiME\u2019 speech separation and recognition challenge. A time-frequency masking based speech enhancement frontend is proposed to suppress the environmental noise utilizing multi-channel coherence and spatial cues. An adaptive microphone array self-calibration method is adopted to overcome the problem of microphone mismatch. For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted. Mel-frequency cepstral coefficient (MFCC) based feature-space maximum likelihood linear regression (fMLLR) features are utilized. Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10]. State-space Minimum Bayes Risk (sMBR) discriminative training is conducted on the neural networks based acoustic models [11, 12]. For language and lexicon modeling, to model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13]. 4-gram language model with Kneser-Ney smoothing is adopted in the first-pass decoding. A language model based on jointly trained recurrent neural network and maximum entropy models (RNNME) is adopted for the second-pass rescoring [14, 15]. Finally, to further improve the system performance by fully exploiting the advantages of different technologies, the recognition results are obtained by lattice combination and rescoring.\nEvaluations are carried out on the official dataset. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% and 42% relative word error rate (WER) reduction on real and simulated test set respectively.\nThe rest of this paper is organized as follows. The timefrequency masking based speech enhancement front-end is presented in Section 2. The speech recognition back-end is presented in Section 3. Experiments are presented in Section 4, followed by the conclusions in Section 5.\nar X\niv :1\n50 9.\n07 21\n1v 1\n[ cs\n.S D\n] 2\n4 Se\np 20\n2. SPEECH ENHANCEMENT\nFig.1 shows a schematic diagram of the proposed speech enhancement method. The proposed method is developed on the basis of the speech enhancement baseline of the \u2018CHiME\u2019 challenge. Keeping the analysis-synthesis scheme, the microphone failure detector, the SPR-PHAT based speaker localizer and the MVDR beamformer identical to the baseline, in this study, time-frequency masking based on multi-channel magnitude-squared coherence (MSC) and phase difference measurement (PDM) is introduced. The MSC-based masking is used to suppress the diffused noise, while the PDM-based masking is used to suppress the directional interference not coming from the target direction. The time-frequency masking is performed by filtering the output subband signal of the MVDR beamformer with the said maskers. Since the phase difference based methods are sensitive to the mismatch of microphone phase response, an adaptive microphone array self-calibration method is adopted."}, {"heading": "2.1. MSC based time-frequency masking", "text": "The MSC between two signals x(t) and y(t) is defined as:\nCxy(f) = |Sxy(f)|2\nSxx(f)Syy(f) , (1)\nwhere f is the frequency, Sxy(f) is the cross-spectral density between the two signals, Sxx(f) and Syy(f) are the autospectral density of x and y respectively. The values of the coherence function, which always satisfy 0 \u2264 Cxy(f) \u2264 1, indicates the extent to which the power of y could be predicted from x by a linear system. In multi-channel signal processing, the MSC is an efficient mean of noise reduction [16]. In the case of microphone array with sufficiently large microphone spacing, incoherent (diffused) noise would be indicated by small coherence values, while the directional signals would have large coherence values [17, 18].\nIn this study, MSC between all the microphone pairs (except the channels detected to be failed) are estimated by Welch\u2019s method of periodogram. The MSC-based timefrequency masker is derived by averaging the MSC over all the microphone pairs."}, {"heading": "2.2. PDM based time-frequency masking", "text": "In the past \u2018CHiME\u2019 challenges, phase difference based timefrequency masking noise suppression front-ends was proved to be effective in the case of dual-channel simulated data [19]. After steering the multi-channel signals towards the target direction by delay alignment, inter-channel phase differences could be obtained. Time-frequency bins which has a phase difference not close to zero are less likely to be the ones from the target direction.\nIn the current \u2018CHiME\u2019 challenge, the experimental condition is extended from dual-channel simulated data to 6- channel real-world recordings. For the multi-channel subband input signals, delay alignment is performed by steering the signals towards the target direction given by the speaker localizer. Phase differences between all the microphone pairs (except the 2nd channel and the failed ones) could be calculated for all the frequency bins. The PDM could be obtained by averaging the absolute value of the phase differences over all the microphone pairs. Assuming that the estimated speaker location is correct, the bins dominated by the signal coming from the target direction would have small PDM values.\nSince the PDM values don\u2019t lies within the range of [0, 1], the PDM based time-frequency masker W \u2032P (f, t) is obtained by performing a non-linear transformation on the PDM value WP (f, t), and hard clipped to have a maximum value of 1:\nW \u2032P (f, t) = clip(1\u2212 tanh(WP (f, t)\u2212 \u03b1(f))), (2)\nwhere \u03b1(f) is a bias function, which is introduced to fit the frequency-dependent distribution of the PDM values. \u03b1(f) is empirically determined by dividing the PDM values in the training set into signal and noise, prior to the testing stage. In this study, \u03b1(f) = 0.4 + 0.3f/fs, where fs is the sampling rate, is used."}, {"heading": "2.3. Microphone array self-calibration", "text": "It\u2019s well known that microphone mismatch could cause speech signal cancellation in adaptive beamforming [20]. Similarly, in the context of phase difference based timefrequency masking, the accuracy of the masking function would be degraded by the interference of the unknown phase response of the microphones, and the environment-dependent\nphase response of the transfer function between the speaker and the microphones.\nIn this study, adaptive microphone array self-calibration with recursive configuration is adopted [21]. For each channel, a transfer function is estimated by minimizing the error between the output signal of a delay-and-sum beamformer and the delay aligned microphone signal. To evaluate the effectiveness of the self-calibration in the context of phase difference base time-frequency masking, only the phase response of the transfer function is used, leaving the MVDR beamformer identical to the baseline.\nIn this study, a 2-stage self-calibration scheme is proposed. In the first stage, an off-line calibration is done on the training set, which provides a robust estimation of the microphone phase response, due to the relatively large amount of training data. The parameters of the calibration filters are updated in a batched fashion, using the frames with SNR greater than the utterance-level median SNR. The SNR is estimated using the method provided by the acoustic simulation baseline. In the second stage, after the phase response of the first calibration filter is compensated, an on-line calibration is done for each utterance in the test set, which provides an estimation of the environment-dependent transfer function. The parameters of the calibration filters are updated in a batched fashion, using all the frames in the evaluation utterance. The phase responses of the two calibration filters are compensated before the PDM calculation.\n3. SPEECH RECOGNITION\nIn recent years, the recurrent neural networks (RNNs) based speech recognition systems have brought about noteworthy performance improvements. Specifically for acoustic modeling, the long short-term memory (LSTM) based deep networks have been shown to give the state-of-the-art performance on some of the speech recognition tasks. In the seminal work, Graves et al. [22] proposed to use stacked bidirectional LSTM trained with connectionist temporal classification [23] for phoneme recognition. Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6]. On the other hand, RNN based language models have also shown advantages over the conventional N-gram language models in both perplexity and speech recognition error rate.\nThe speaker variability is also an important issue. Efforts have been made to train acoustic models using speakeradapted features, which can be obtained by speaker normalization techniques such as the vocal tract length normalization (VTLN) and fMLLR [25]. Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly. It should be noticed that the information about channel and\nbackground noise is also encoded by the i-vector. In this study, Mel-scale log-filterbank coefficients (FBANK) features, MFCC-based fMLLR features are used. The ivector was extracted online and concatenated to the acoustic feature. The hybrid approach for acoustic modeling is adopted, in which the neural networks outputs are converted to pseudo likelihood, and used as the state output probability of the HMMs. The networks are trained based on alignments from GMM-HMM systems. Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted. sMBR discriminative training is conducted on the neural networks based acoustic models [11, 12]. To model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13]. The baseline 3-gram language model is replaced by a 4-gram language model with Kneser-Ney smoothing trained on the official training set. An RNNME based language model [14, 15] is adopted for the second-pass rescoring. Finally, system combination is introduced to further improve the system performance by integrating systems with different properties using lattice combination and rescoring.\n4. EXPERIMENTS AND DISCUSSIONS"}, {"heading": "4.1. Experimental setups", "text": "All the experiments were conducted on the official dataset. The GMM-HMM system was trained using Kaldi [28]. All the networks were trained based on the alignment result of the GMM-HMM system with 2824 tied context dependent HMM states. In the training procedure of LSTM networks, the strategy introduced by [29] was applied to scale down the gradients. Besides, since the information from the future frames helps the LSTM networks making better decisions, we also delayed the output HMM state labels by 4 frames. The feedforward DNNs used the concatenated features, which were produced by concatenating the current frame with 7 frames in its left and right context. The inputs to LSTMP networks were only the current frames."}, {"heading": "4.2. Speech enhancement", "text": "Table 1 shows the evaluation results of the proposed speech enhancement methods with the baseline acoustic model (GMM-HMM) and language model (3-gram). Comparing with the baseline using noisy data, the performance of the baseline with speech enhancement front-end is greatly improved on simulated test set, but degraded on real test set. This phenomenon shows that mismatch between enhanced real and simulated data is introduced by the baseline beamforming based speech enhancement process. The system performance on the real test set would be greatly influenced by this mismatch, since relatively large amount of simulated data is used in the training phase.\nBy introducing the MSC based time-frequency masking into the baseline speech enhancement front-end, the WER on real test set is reduced by 10.94%. It proves the effectiveness of the MSC based time-frequency masking. By contrast, the WER on simulated test set is increased by 1.42%. It shows that this method performs more equally on real and simulated data. Hence the said mismatch is weakened. The effectiveness of the PDM based time-frequency masking is also proved by the WER reduction on real test set of 11.13%. While the WER on simulated test set is lower than the MSC based system. Comparing to the MSC based method, although better performance on real test set is achieved, greater mismatch between real and simulated data is introduced by the PDM based method. By using the two kinds of time-frequency masking together, an absolute WER reduction of 12.64% is achieved.\nIt\u2019s also shown that the by introducing phase calibration, further absolute WER reduction of 1.12% and 0.17% is made on the basis of the \u201cPDM\u201d and \u201cMSC+PDM\u201d systems respectively. The mismatch between real and simulated data is further weakened. It should be noticed that, since the time for the challenge is limited, further experiments for the phase calibration is not conducted. In the following experiments, the \u201cMSC+PDM\u201d front-end is used."}, {"heading": "4.3. Lexicon and language modeling", "text": "Table 2 shows the evaluation results of different language models with the proposed front-end (MSC+PDM) and the baseline acoustic model (GMM-HMM). The N-gram language models are used in the first-pass decoding, while the RNNME-based language model is used for the second-pass rescoring.\nExperiment results show that the system performance is improved consistently by introducing pronunciation lexicon with silence probability. The system performance is further improved by introducing the 4-gram language model and RNNME-based language model."}, {"heading": "4.4. Acoustic modeling", "text": "Table 3 shows the evaluation results of different features and different acoustic models with the proposed front-end (MSC+PDM) and language model (4-gram+SP). It should be noted that the RNNME based second-pass rescoring is not used in this evaluation.\nThe maxout deep neural network has 4 hidden layers and each layer has 1000 neurons with a group size of 4. The LSTMP recurrent neural network has a single hidden layer with 1000 neurons and 700 projection units. The 40-dimensional fMLLR features are attained based on the 13-dimensional MFCC features. The fMLLR transformation matrix is estimated using the baseline GMM-HMM. The 50-dimensional i-vector features are extracted on utterance level. The language model used in the sMBR discriminative training is the baseline 3-gram language model.\nExperimental result shows that the LSTMP based method performs better than the maxout based method while using the same features. While using same acoustic modeling methods, fMLLR based feature performs better than Fbank based feature. By combining the fMLLR based feature with\nthe i-vector based feature, the system performance improved consistently. By using sMBR based discriminative training, the system performance improved consistently. The best WER of 15.55% is achieved by the \u201cLSTMP+sMBR\u201d on real data, and the second best acoustic modeling method is \u201cMaxout+dp+sMBR\u201d.\nTable 4 shows the evaluation results of the combined systems. Since the RNNME based second-pass rescoring is not included in the evaluation of acoustic modeling, the RNNME based second-pass rescoring is introduced into the two best-performed acoustic models (Maxout+dp+sMBR, LSTMP+sMBR) with the 4-gram+SP language model in one-pass decoding. Moreover, the lattice combination is performed on the two intergraded systems to achieve a better performance. The front-end used in this evaluation is MSC+PDM. The feature used in this evaluation is fMLLR+ivector.\nExperimental result shows that the \u201cLSTMP+sMBR\u201d and \u201cMaxout+dp+sMBR\u201d systems obtain further improvement by introducing RNNME-based language model second-pass rescoring. Through lattice combination, a final WER of 14.28% is achieved on the real test set."}, {"heading": "4.5. Overall comparison", "text": "Table 5 shows the results of the overall comparison of the proposed systems and the baseline systems:\n\u2022 Baseline I: the GMM-HMM baseline on noisy data; \u2022 Baseline II: the GMM-HMM baseline on enhanced\ndata; \u2022 System I: proposed speech enhancement front-end with\nbaseline acoustic model (GMM-HMM) and language model (3-gram); \u2022 System II: proposed language model with baseline acoustic model (GMM-HMM) on noisy data; \u2022 System III: proposed acoustic model with baseline language model (3-gram) on noisy data; \u2022 System IV: the best-performed proposed system.\nTable 6 shows the WERs for the 4 acoustic environments achieved by the best system.\nExperimental results from the upper part of table 5 show the contribution of the proposed front-end, acoustic model and language model separately. Comparing to the baseline system on enhanced data, by introducing the proposed speech enhancement method, an absolute WER reduction of 12.81% is achieved. Comparing to the baseline systems using noisy data, absolute WER reduction of 11.76% and 5.21% is achieved by the proposed acoustic model and language model respectively. The system IV, which make use of the proposed front-end and back-end, performs the best result. A final WER of 14.28% is achieved on the real test set.\n5. CONCLUSIONS\nIn this paper, we presented the Lingban entry to the 3rd \u2018CHiME\u2019 speech separation and recognition challenge. A time-frequency masking based speech enhancement frontend is proposed. The state-of-the-art recurrent neural networks based acoustic and language modeling methods are adopted. Evaluations are carried out on the official dataset. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative WER reduction.\nSince the front-end and back-end are working separately in the current implementation, a unified end-to-end learning framework for multi-channel noise-robust ASR would be proposed in the future works. Furthermore, the proposed methods should be evaluated on real-application tasks, such as spontaneous speech recognition using general purpose commercial mobile devices with less microphones.\n6. ACKNOWLEDGMENT\nThe authors would like to thank Zhiping Zhang, Xiangang Li, Yi Liu and Tong Fu for their kindly helps.\n7. REFERENCES\n[1] J. Barker, E. Vincent, N. Ma, H. Christensen, and P. Green, \u201cThe PASCAL CHiME speech separation and recognition challenge,\u201d Computer Speech & Language, vol. 27, no. 3, pp. 621\u2013633, May 2013.\n[2] E. Vincent, J. Barker, S. Watanabe, J. Le Roux, F. Nesta, and M. Matassoni, \u201cThe second CHiME speech separation and recognition challenge an overview of challenge systems and outcomes,\u201d in Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013.\n[3] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, \u201cThe third \u2018CHiME\u2019 speech separation and recognition challenge: Dataset, task and baselines,\u201d in Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.\n[4] P. Swietojanski, J. Li, and J. Huang, \u201cInvestigation of maxout networks for speech recognition,\u201d in Proceedings of the 2014 ICASSP, 2014, pp. 7649\u20137653.\n[5] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Proceedings of the 2014 INTERSPEECH, 2014, pp. 338\u2013342.\n[6] X. Li and X. Wu, \u201cConstructing long short-term memory based deep recurrent neural network for large vocabulary speech recognition,\u201d in Proceedings of the 2015 ICASSP, 2015.\n[7] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[8] O. Glembek, L. Burget, P. Matejka, M. Karafia\u0301t, and P. Kenny, \u201cSimplification and optimization of i-vector extraction,\u201d in Proceedings of the 2011 ICASSP, 2011, pp. 4516\u20134519.\n[9] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-vectors,\u201d in Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013, pp. 55\u201359.\n[10] A. Senior and I. Lopez-Moreno, \u201cImproving dnn speaker independence with i-vector inputs,\u201d in Proceedings of the 2014 ICASSP, 2014, pp. 225\u2013229.\n[11] K. Vesely\u0301, A. Ghoshal, L. Burget, and D. Povey, \u201cSequence-discriminative training of deep neural networks,\u201d in Proceedings of the 2013 INTERSPEECH, 2013.\n[12] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga, and M. Mao, \u201cSequence discriminative distributed training of long short-term memory recurrent neural networks,\u201d in Proceedings of the 2014 INTERSPEECH, 2014, pp. 1209\u20131213.\n[13] G. Chen, H. Xu, M. Wu, D. Povey, and S. Khudanpur, \u201cPronunciation and silence probability modeling for ASR,\u201d in Proceedings of the 2015 INTERSPEECH, 2015.\n[14] T. Mikolov, Statistical language models based on neural networks, Ph.D. thesis, Brno University of Technology, 2012.\n[15] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. C\u030cernocky\u0301, \u201cRNNLM - recurrent neural network language modeling toolkit,\u201d in Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 196\u2013201.\n[16] R. Le Bouquin and G. Faucon, \u201cUsing the coherence function for noise reduction,\u201d Communications, Speech and Vision, IEE Proceedings I, vol. 139, no. 3, pp. 276\u2013 280, June 1992.\n[17] M. Brandstein and D. Ward, Eds., Microphone Arrays, Springer-Verlag, New York, NY, 2001.\n[18] M. B. Trawicki and M. T. Johnson, \u201cDistributed multichannel speech enhancement with minimum meansquare error short-time spectral amplitude, log-spectral amplitude, and spectral phase estimation,\u201d Signal Processing, vol. 92, no. 2, pp. 345\u2013356, Feb. 2012.\n[19] Y. Tachioka, S. Watanabe, J. Le Roux, and J. R. Hershey, \u201cDiscriminative methods for noise robust speech recognition: A CHiME Challenge Benchmark,\u201d in The 2nd CHiME Workshop on Machine Listening in Multisource Environments, Vancouver, Canada, June 2013, pp. 19\u2013 24.\n[20] S. Haykin and K. J. R. Liu, Handbook on Array Processing and Sensor Networks, Wiley-IEEE Press, Jan. 2010.\n[21] M. Buck, T. Haulick, and H. Pfleiderer, \u201cSelf-calibrating microphone arrays for speech signal acquisition: A systematic approach,\u201d Signal Processing, vol. 86, no. 6, pp. 1230\u20131238, June 2006.\n[22] A. Graves, A. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proceedings of the 2013 ICASSP, 2013, pp. 6645\u20136649.\n[23] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369\u2013 376.\n[24] J. Geiger, Z. Zhang, F. Weninger, B. Schuller, and G. Rigoll, \u201cRobust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling,\u201d in Proceedings of the 2014 INTERSPEECH, 2014, pp. 631\u2013635.\n[25] F. Seide, G. Li, X. Chen, and D. Yu, \u201cFeature engineering in context-dependent deep neural networks for conversational speech transcription,\u201d in Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 24\u201329.\n[26] D. Yu and L. Deng, Automatic speech recognition - A deep learning approach, Springer-Verlag London, 2015.\n[27] O. Abdel-Hamid and H. Jiang, \u201cFast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code,\u201d in Proceedings of the 2013 ICASSP, 2013, pp. 7942\u20137946.\n[28] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motl\u0131\u0301c\u030cek, Y. Qian, P. Schwarz, J. Silovsky\u0301, G. Stemmer, and K. Vesely\u0301, \u201cThe Kaldi speech recognition toolkit,\u201d in Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.\n[29] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d CoRR, vol. 1211.5063, 2012."}], "references": [{"title": "The PASCAL CHiME speech separation and recognition challenge", "author": ["J. Barker", "E. Vincent", "N. Ma", "H. Christensen", "P. Green"], "venue": "Computer Speech & Language, vol. 27, no. 3, pp. 621\u2013633, May 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "The second CHiME speech separation and recognition challenge an overview of challenge systems and outcomes", "author": ["E. Vincent", "J. Barker", "S. Watanabe", "J. Le Roux", "F. Nesta", "M. Matassoni"], "venue": "Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "The third \u2018CHiME\u2019 speech separation and recognition challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigation of maxout networks for speech recognition", "author": ["P. Swietojanski", "J. Li", "J. Huang"], "venue": "Proceedings of the 2014 ICASSP, 2014, pp. 7649\u20137653.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Proceedings of the 2014 INTERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Constructing long short-term memory based deep recurrent neural network for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Proceedings of the 2015 ICASSP, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Simplification and optimization of i-vector extraction", "author": ["O. Glembek", "L. Burget", "P. Matejka", "M. Karafi\u00e1t", "P. Kenny"], "venue": "Proceedings of the 2011 ICASSP, 2011, pp. 4516\u20134519.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013, pp. 55\u201359.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving dnn speaker independence with i-vector inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "Proceedings of the 2014 ICASSP, 2014, pp. 225\u2013229.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u00fd", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proceedings of the 2013 INTERSPEECH, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["H. Sak", "O. Vinyals", "G. Heigold", "A. Senior", "E. McDermott", "R. Monga", "M. Mao"], "venue": "Proceedings of the 2014 INTERSPEECH, 2014, pp. 1209\u20131213.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation and silence probability modeling for ASR", "author": ["G. Chen", "H. Xu", "M. Wu", "D. Povey", "S. Khudanpur"], "venue": "Proceedings of the 2015 INTERSPEECH, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical language models based on neural networks, Ph.D", "author": ["T. Mikolov"], "venue": "thesis, Brno University of Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "RNNLM - recurrent neural network language modeling toolkit", "author": ["T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. \u010cernock\u00fd"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 196\u2013201.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Using the coherence function for noise reduction", "author": ["R. Le Bouquin", "G. Faucon"], "venue": "Communications, Speech and Vision, IEE Proceedings I, vol. 139, no. 3, pp. 276\u2013 280, June 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Distributed multichannel speech enhancement with minimum meansquare error short-time spectral amplitude, log-spectral amplitude, and spectral phase estimation", "author": ["M.B. Trawicki", "M.T. Johnson"], "venue": "Signal Processing, vol. 92, no. 2, pp. 345\u2013356, Feb. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative methods for noise robust speech recognition: A CHiME Challenge Benchmark", "author": ["Y. Tachioka", "S. Watanabe", "J. Le Roux", "J.R. Hershey"], "venue": "The 2nd CHiME Workshop on Machine Listening in Multisource Environments, Vancouver, Canada, June 2013, pp. 19\u2013 24.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-calibrating microphone arrays for speech signal acquisition: A systematic approach", "author": ["M. Buck", "T. Haulick", "H. Pfleiderer"], "venue": "Signal Processing, vol. 86, no. 6, pp. 1230\u20131238, June 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proceedings of the 2013 ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369\u2013 376.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling", "author": ["J. Geiger", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "Proceedings of the 2014 INTER- SPEECH, 2014, pp. 631\u2013635.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 24\u201329.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic speech recognition - A deep learning", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Fast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proceedings of the 2013 ICASSP, 2013, pp. 7942\u20137946.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131\u0301\u010dek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Stemmer", "K. Vesel\u00fd"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "CoRR, vol. 1211.5063, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "The past \u2018CHiME\u2019 challenges have contributed to the development of several speech enhancement techniques, and novel framework that integrate speech enhancement and ASR [1, 2].", "startOffset": 168, "endOffset": 174}, {"referenceID": 1, "context": "The past \u2018CHiME\u2019 challenges have contributed to the development of several speech enhancement techniques, and novel framework that integrate speech enhancement and ASR [1, 2].", "startOffset": 168, "endOffset": 174}, {"referenceID": 2, "context": "The main difference between the current \u2018CHiME\u2019 challenge and the past ones is instead of working on simulated data only, the current challenge focuses on real-world data: multi-channel recording using mobile tablet device in a variety of noisy public environments [3].", "startOffset": 265, "endOffset": 268}, {"referenceID": 3, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 7, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 8, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 9, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 10, "context": "State-space Minimum Bayes Risk (sMBR) discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "State-space Minimum Bayes Risk (sMBR) discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "For language and lexicon modeling, to model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "A language model based on jointly trained recurrent neural network and maximum entropy models (RNNME) is adopted for the second-pass rescoring [14, 15].", "startOffset": 143, "endOffset": 151}, {"referenceID": 14, "context": "A language model based on jointly trained recurrent neural network and maximum entropy models (RNNME) is adopted for the second-pass rescoring [14, 15].", "startOffset": 143, "endOffset": 151}, {"referenceID": 15, "context": "In multi-channel signal processing, the MSC is an efficient mean of noise reduction [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "In the case of microphone array with sufficiently large microphone spacing, incoherent (diffused) noise would be indicated by small coherence values, while the directional signals would have large coherence values [17, 18].", "startOffset": 214, "endOffset": 222}, {"referenceID": 17, "context": "In the past \u2018CHiME\u2019 challenges, phase difference based timefrequency masking noise suppression front-ends was proved to be effective in the case of dual-channel simulated data [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "Since the PDM values don\u2019t lies within the range of [0, 1], the PDM based time-frequency masker W \u2032 P (f, t) is obtained by performing a non-linear transformation on the PDM value WP (f, t), and hard clipped to have a maximum value of 1:", "startOffset": 52, "endOffset": 58}, {"referenceID": 18, "context": "In this study, adaptive microphone array self-calibration with recursive configuration is adopted [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "[22] proposed to use stacked bidirectional LSTM trained with connectionist temporal classification [23] for phoneme recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] proposed to use stacked bidirectional LSTM trained with connectionist temporal classification [23] for phoneme recognition.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 192, "endOffset": 198}, {"referenceID": 5, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 192, "endOffset": 198}, {"referenceID": 22, "context": "Efforts have been made to train acoustic models using speakeradapted features, which can be obtained by speaker normalization techniques such as the vocal tract length normalization (VTLN) and fMLLR [25].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 165, "endOffset": 172}, {"referenceID": 9, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 165, "endOffset": 172}, {"referenceID": 3, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 77, "endOffset": 83}, {"referenceID": 10, "context": "sMBR discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "sMBR discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "To model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "An RNNME based language model [14, 15] is adopted for the second-pass rescoring.", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "An RNNME based language model [14, 15] is adopted for the second-pass rescoring.", "startOffset": 30, "endOffset": 38}, {"referenceID": 25, "context": "The GMM-HMM system was trained using Kaldi [28].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "In the training procedure of LSTM networks, the strategy introduced by [29] was applied to scale down the gradients.", "startOffset": 71, "endOffset": 75}], "year": 2015, "abstractText": "In this paper, the Lingban entry to the third \u2018CHiME\u2019 speech separation and recognition challenge is presented. A timefrequency masking based speech enhancement front-end is proposed to suppress the environmental noise utilizing multichannel coherence and spatial cues. The state-of-the-art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state space minimum Bayes risk based discriminative acoustic modeling, and i-vector based acoustic condition modeling, are carefully integrated into the speech recognition back-end. To further improve the system performance by fully exploiting the advantages of different technologies, the final recognition results are obtained by lattice combination and rescoring. Evaluations carried out on the official dataset prove the effectiveness of the proposed systems. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative word error rate reduction on the real-data test set.", "creator": "LaTeX with hyperref package"}}}