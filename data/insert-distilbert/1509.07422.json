{"id": "1509.07422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "abstract": "a framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such genera as stochastic gradient descent ( sgd ). the optimization problems change slowly in all the sense that the minimizers change at either represents a consistently fixed or bounded rate. a method based on estimates of the change in the minimizers and properties predictions of the optimization algorithm is introduced - for method adaptively selecting the number of samples needed from the distributions underlying each problem specified in order to systematically ensure that the excess risk, i. e., the resulting expected gap increases between the loss achieved by the approximate minimizer produced by the objective optimization algorithm and the exact minimizer, so does not exceed a target level. experiments with synthetic and real data are used to confirm data that this approach performs accuracy well.", "histories": [["v1", "Thu, 24 Sep 2015 16:19:43 GMT  (276kb)", "http://arxiv.org/abs/1509.07422v1", "submitted to ICASSP 2016, extended version"]], "COMMENTS": "submitted to ICASSP 2016, extended version", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["craig wilson", "venugopal v veeravalli"], "accepted": false, "id": "1509.07422"}, "pdf": {"name": "1509.07422.pdf", "metadata": {"source": "CRF", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "authors": ["Craig Wilson", "Venugopal V. Veeravalli"], "emails": ["wilson60@illinois.edu", "vvv@illinois.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n07 42\n2v 1\n[ cs\n.L G\n] 2\n1 Introduction\nConsider solving a sequence of machine learning problems such as regression or classification by minimizing the expected value of a fixed loss function \u2113(x,z) at each time ns:\nmin x\u2208X\n{ fn(x), Ezn\u223cpn [\u2113(x,zn)] } \u2200n \u2265 1 (1)\nFor regression, zn corresponds to the predictors and response pair at time n and x parameterizes the regression model. For classification zn corresponds to the feature and label pair at time n and x parameterizes the classifier. Although, motivated by regression and classification, our framework works for any loss function \u2113(x,z) that satisfies certain properties discussed later. In the learning context, a task consists of the loss function \u2113(x,z) and the distribution pn, and so our problem can be viewed as learning a sequence of tasks.\nThe problems change slowly at a constant but unknown rate in the sense that\n\u2016x\u2217n \u2212x\u2217n\u22121\u2016= \u03c1 \u2200n \u2265 2 (2)\nwith x\u2217n the minimizer of fn(x). In an extended version of this paper [?], we also consider slow changes at a bounded but unknown rate\n\u2016x\u2217n \u2212x\u2217n\u22121\u2016 \u2264 \u03c1 \u2200n \u2265 2 (3) Under this model, we find approximate minimizers xn of each function fn(x) using Kn samples from distribution pn by applying an optimization algorithm. We evaluate the quality of our approximate minimizers xn through an excess risk criterion \u03b5n, i.e.,\nE [ fn(xn)]\u2212 fn(x\u2217n)\u2264 \u03b5n \u2217This work was supported by the NSF under award CCF 11-11342 through the University of Illinois at Urbana-Champaign.\nwhich is a standard criterion for optimization and learning problems [1]. Our goal is to determine adaptively the number of samples Kn required to achieve a desired excess risk \u03b5 for each n with \u03c1 unknown. As \u03c1 is unknown, we will construct estimates of \u03c1 . Given an estimate of \u03c1 , we determine selection rules for the number of samples Kn to achieve a target excess risk \u03b5 .\n1.1 Related Work\nOur problem has connections with multi-task learning (MTL) and transfer learning. In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks. In transfer learning, knowledge from one source task is transferred to another target task either with or without additional training data for the target task [5]. Multi-task learning could be applied to our problem by running a MTL algorithm each time a new task arrives, while remembering all prior tasks. However, this approach incurs a memory and computational burden. Transfer learning lacks the sequential nature of our problem. For multi-task and transfer learning, there are theoretical guarantees on regret for some algorithms [6].\nWe can also consider the concept drift problem in which we observe a stream of incoming data that potentially changes over time, and the goal is to predict some property of each piece of data as it arrives. After prediction, we incur a loss that is revealed to us. For example, we could observe a feature wn and predict the label yn as in [7]. Some approaches for concept drift use iterative algorithms such as SGD, but without specific models on how the data changes. As a result, only simulation results showing good performance are available. There are also some bandit approaches in which one of a finite number of predictors must be applied to the data as in [8]. For this approach, there are regret guarantees using techniques for analyzing bandit problems.\nAnother relevant model is sequential supervised learning (see [9]) in which we observe a stream of data consisting of feature/label pairs (wn,yn) at time n, with wn being the feature vector and yn being the label. At time n, we want to predict yn given xn. One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn\u2212i,yn\u2212i)}Li=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data. Another approach is to assume that there is an underlying hidden Markov model (HMM) [12]. The label yn represents the hidden state and the pair (wn,yn) represents the observation with yn being a noisy version of yn. HMM inference techniques are used to estimate yn.\n2 Adaptive Sequential Optimization With \u03c1 Known For analysis, we need the following assumptions on our functions fn(x) and the optimization algorithm:\nA.1 For the optimization algorithm under consideration, there is a function b(d0,Kn) such that\nE [ fn(xn)]\u2212 fn(x\u2217n)\u2264 b(d0,Kn)\nwith Kn the number of samples from pn and E\u2016xn(0)\u2212 x\u2217n\u20162 \u2264 d0, where xn(0) is the initial point of the optimization algorithm at time n. Finally, b(d0,Kn) is non-decreasing in d0.\nA.2 Each loss function \u2113(x,z) is differentiable in x. Each fn(x) is strongly convex with parameter m, i.e.,\nfn(y)\u2265 fn(x)+ \u3008\u2207x fn(x),y\u2212x\u3009+ 1 2 m\u2016y\u2212x\u20162\nA.3 diam(X )<+\u221e\nA.4 We can find initial points x1 and x2 that satisfy the excess risk criterion with \u03b51 and \u03b52 known, i.e.,\nE [ fi(xi)]\u2212 fi(x\u2217i )\u2264 \u03b5i i = 1,2\nRemarks: For assumption A.1 , we assume that the bound b(d0,Kn) depends on the number of samples Kn and not the number of iterations. For SGD, generally the number of iterations equals Kn as each sample is used to produce a noisy gradient. In addition, we often set xn(0) = xn\u22121. See Appendix A for a discussion of useful b(d0,Kn) bounds. For assumption A.4 , we can fix Ki and set \u03b5i = b(diam(X )2,Ki) for i = 1,2.\nNow, we examine the case when the change in minimizers, \u03c1 in (2) or (3), is known. For the analysis of the section, whether (2) or (3) holds does not affect the analysis. Later we will estimate \u03c1 and in this case whether (2) or (3) holds matters substantially.\nWe want to find a bound \u03b5n on the excess risk at time n in terms of Kn and \u03c1 , i.e., \u03b5n such thatE[ fn(xn)]\u2212 fn(x\u2217n)\u2264 \u03b5n. The idea is to start with the bounds from assumption A.4 and proceed inductively using the previous \u03b5n\u22121 and \u03c1 from (2). Suppose that \u03b5n\u22121 bounds the excess risk at time n\u2212 1. Using the triangle inequality, strong convexity, and (2) we have\nE\u2016xn\u22121 \u2212x\u2217n\u20162 \u2264 ( \u2016xn\u22121 \u2212x\u2217n\u22121\u2016+ \u2016x\u2217n\u2212x\u2217n\u22121\u2016 )2\n\u2264 ( \u221a\n2 m E [ fn\u22121(xn\u22121)]\u2212 fn\u22121(x\u2217n\u22121)+ \u2016x\u2217n \u2212x\u2217n\u22121\u2016\n)2\n\u2264 ( \u221a\n2\u03b5n\u22121 m +\u03c1\n)2\n(4)\nIn comparison, we could use the estimate diam2(X ) to bound E\u2016xn\u22121 \u2212x\u2217n\u20162 and select Kn. If the bound in (4) is much smaller than diam(X )2, then we need significantly fewer samples Kn to guarantee a desired excess risk. Now, by using the bound b(d0,Kn) from assumption A.1 , we can set\n\u03b5n = b\n\n\n(\n\u221a\n2\u03b5n\u22121 m +\u03c1\n)2\n,Kn\n\n \u2200n \u2265 3\nwhich yields a sequence of bounds on the excess risk. Note that this recursion only relies on the immediate past at time n\u2212 1 through \u03b5n\u22121. To achieve \u03b5n \u2264 \u03b5 for all n, we set\nK1 = min{K \u2265 1 | b ( diam(X )2,K ) \u2264 \u03b5}\nand Kn = K\u2217 for n \u2265 2 with\nK\u2217 = min\n\n\n\nK \u2265 1 \u2223 \u2223 \u2223\n\u2223 \u2223\nb\n\n\n(\n\u221a\n2\u03b5 m +\u03c1\n)2\n,K\n\n\u2264 \u03b5\n\n\n\n(5)\n3 Estimating \u03c1 In practice, we do not know \u03c1 , so we must construct an estimate \u03c1\u0302n using the samples from each distribution pn. We introduce two approaches to estimate \u03c1 at one time step, \u2016x\u2217i \u2212x\u2217i\u22121\u2016, and methods to combine these estimates under assumptions (2) and (3). We show that for our estimate \u03c1\u0302n and appropriately chosen sequences {tn} for all n large enough \u03c1\u0302n + tn \u2265 \u03c1 almost surely. With this property, analysis similar to that in Section 2 holds.\n3.1 Allowed Ways to Choose Kn One of the sources of difficulty in estimating \u03c1 is that we will allow Kn to be selected in a data dependent way, so Kn is itself a random variable. We make the assumption that Kn is selected using only information available at the end of time n\u2212 1. To make this precise we define a filtration of sigma algebras to describe the available information. First, we define the sigma algebra K0 containing all the information on the initial conditions of our algorithm. For example, we may start at a random point x0 and then\nK0 = \u03c3(x0)\nThe sigma algebra K0 may also contain information about K1 and K2. Next, we define the filtration\nKn = \u03c3 ( {zn(k)}Knk=1 ) \u2228Kn\u22121 \u2200n \u2265 1 (6)\nwhere F \u2228G = \u03c3 (F \u222aG )\nis the merge operator for sigma algebras. The sigma algebra Kn contains all the information available to us at the end of time n. We assume that Kn is Kn\u22121-measurable to capture the idea that Kn is chosen only using information available at the end of time n\u2212 1.\n3.2 Estimating One Step Change\nFirst, we estimate the one step changes \u2016x\u2217i \u2212x\u2217i\u22121\u2016 denoted by \u03c1\u0303i. Implicitly, we assume that all one step estimates are capped by diam(X ), since trivially \u2016x\u2217n \u2212x\u2217n\u22121\u2016 \u2264 diam(X ).\n3.2.1 Direct Estimate\nFirst, we construct an estimate \u03c1\u0303i of the one step changes \u2016x\u2217i \u2212x\u2217i\u22121\u2016. Using the triangle inequality and variational inequalities from [13] yields\n\u2016x\u2217i \u2212x\u2217i\u22121\u2016 \u2264 \u2016xi \u2212xi\u22121\u2016+ \u2016xi\u2212x\u2217i \u2016+ \u2016xi\u22121\u2212x\u2217i\u22121\u2016\n\u2264 \u2016xi \u2212xi\u22121\u2016+ 1 m \u2016\u2207x fi(xi)\u2016+ 1 m \u2016\u2207x fi(xi\u22121)\u2016\nWe then approximate \u2016\u2207x fi(xi)\u2016= \u2016Ezi\u223cpi [\u2207x\u2113(xi,zi)]\u2016 by \u2225\n\u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n\u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225\n\u2225\nto yield the following estimate that we call the direct estimate:\n\u03c1\u0303i , \u2016xi \u2212xi\u22121\u2016+ 1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 + 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(xi\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225\n3.2.2 Vector Integral Probability Metric Estimate\nGiven a class of functions F where each f \u2208F maps Z \u2192R, an integral probability metric (IPM) [14] between two distributions p and q is defined to be\n\u03b3F (p,q), sup f\u2208F\n\u2223 \u2223Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)] \u2223 \u2223\nWe consider an extension of this idea, which we call a vector IPM, in which the class of functions F maps Z \u2192 X :\n\u03b3VF (p,q), sup f\u2208F \u2016Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)]\u2016 (7)\nLemma 1 shows that a vector IPM can be used to bound the change in minimizer at time i and follows from variational inequalities in [13] and the assumption that {\u2207x\u2113(x, \u00b7) : x \u2208 X } \u2282 F .\nLemma 1. Assume that {\u2207x\u2113(x, \u00b7) : x \u2208 X } \u2282 F . Then \u2016x\u2217i \u2212x\u2217i\u22121\u2016 \u2264 1m \u03b3VF (pi, pi\u22121).\nProof. By exploiting variational inequalities from [13], we can show that\n\u2016x\u2217i \u2212x\u2217i\u22121\u2016 \u2264 1 m \u2016\u2207x fi(x\u2217i\u22121)\u2212\u2207x fi\u22121(x\u2217i\u22121)\u2016\n= 1 m \u2016Ezi\u223cpi [ \u2207x\u2113(x\u2217i\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ \u2207x\u2113(x\u2217i\u22121,zi\u22121) ] \u2016\nBy assumption {\u2207x\u2113(x\u2217i\u22121, \u00b7) : x \u2208 X } \u2282 F , so \u2016\u2207x fi(x\u2217i\u22121)\u2212\u2207x fi\u22121(x\u2217i\u22121)\u2016 = \u2016Ezi\u223cpi [ \u2113(x\u2217i\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ \u2113(x\u2217i\u22121,zi\u22121) ]\n\u2016 \u2264 sup\nf\u2208F \u2016Ezi\u223cpi [ f (zi)]\u2212Ezi\u22121\u223cpi\u22121 [ f (zi\u22121)]\u2016\n= \u03b3VF (pi, pi\u22121)\nWe cannot compute this vector IPM, since we do not know the distributions pi and pi\u22121. Instead, we plug in the empiricals p\u0302i and p\u0302i\u22121 to yield the estimate 1m \u03b3 V F (p\u0302i, p\u0302i\u22121). This estimate is biased upward, which ensures that \u2016x\u2217i \u2212x\u2217i\u22121\u2016 \u2264 E [ 1 m \u03b3 V F (p\u0302i, p\u0302i\u22121) ]\n. Our estimate is still not in a closed form since there is a supremum over F in the computation of \u03b3V\nF (p\u0302i, p\u0302i\u22121).\nFor the class of functions F = { f \u2223 \u2223 \u2016 f (z)\u2212 f (z\u0303)\u2016 \u2264 r(z, z\u0303) } . (8)\nwe can compute an upper bound \u0393i on \u03b3VF (p\u0302i, p\u0302i\u22121) yielding a computable estimate \u03c1\u0303i = 1 m \u0393i. Set z\u0303i(k) = zi(k) if 1 \u2264 k \u2264 Ki and z\u0303i(k) = zi\u22121(k) if Ki + 1 \u2264 k \u2264 Ki +Ki\u22121. From (7), we have\n\u03b3VF (p\u0302i, p\u0302i\u22121) = sup f\u2208F\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 f (z\u0303i(k))\u2212 1 Ki\u22121 Ki\u22121 \u2211 k=1 f (z\u0303i(Ki + k)) \u2225 \u2225 \u2225 \u2225 \u2225\nWe can relax this supremum by maximizing over the function value f (z\u0303i(k)) denoted by \u03b1k in the following nonconvex quadratically constrained quadratic program (QCQP):\nmaximize\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u03b1k \u2212 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u03b1Ki+k \u2225 \u2225 \u2225 \u2225 \u2225\nsubject to \u2016\u03b1k \u2212\u03b1 j\u2016 \u2264 r(z\u0303i(k), z\u0303i( j)) \u2200k < j\nThe constraints are imposed to ensure that the function values \u03b1k can correspond to a function in F from (8). The value of this QCQP exactly may not equal the vector IPM but at least provides an upper bound. Finally, we note that this QCQP can be converted to its dual form to yield an SDP, which is often easier to solve.\n3.2.3 Comparison of Estimates\nThe direct estimate is easier to compute but may be loose if \u2016xn \u2212x\u2217n\u2016 is large. If \u2016xn \u2212x\u2217n\u2016 is large, then the vector IPM approach is in general tighter. However, the vector IPM is more difficult to compute due to need to solve a QCQP or SDP and check the inclusion conditions in Lemma 1. Also, the number of constraints in the QCQP or SDP grows quadratically in the number of samples.\n3.3 Combining One Step Estimates For Constant Change\nAssuming that \u2016x\u2217i \u2212x\u2217i\u22121\u2016= \u03c1 from (2), we average the one step estimates \u03c1\u0303i to yield a better estimate\n\u03c1\u0302n = 1 n\u2212 1 n \u2211 i=2 \u03c1\u0303i\nof \u03c1 at each time n under (2). To analyze the behavior of our combined estimates, we use sub-Gaussian concentration inequalities detailed in Appendix B. Lemma 22 is of particular importance to our analysis.\n3.3.1 Direct Estimate\nThe difficulty in analyzing the direct estimate comes because in approximating 1m\u2016\u2207 fi(xi)\u2016 by\n1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225\nxi is dependent on all the samples {zi(k)}Kik=1. To illustrate the problem further, consider drawing two independent copies {zi(k)}Kik=1 iid\u223c pi and {z\u0303i(k)}Kik=1 iid\u223c pi of the samples. Suppose that we use the second copy {z\u0303i(k)}Kik=1 to compute xi using our optimization algorithm of choice starting from xi\u22121. Then we approximate 1m\u2016\u2207 fi(xi)\u2016 by\n1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225\nNow, since xi is independent of {zi(k)}Kik=1 the quantity\n1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225\nis the norm of an average of independent random variables conditioned on xi. This allows us to apply standard concentration inequalities for norms of random variables as in [15]. In this section, we argue that re-using the samples {zi(k)}Kik=1 to compute xi is not too far from using a second independent draw {z\u0303i(k)} Ki k=1.\nFor analysis, we need the following additional assumptions:\nB.1 The loss function \u2113(x,z) has uniform Lipschitz continuous gradients in x with modulus L, i.e.\n\u2016\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u0303,z)\u2016 \u2264 L\u2016x\u2212 x\u0303\u2016 \u2200z \u2208 Z\nB.2 Assuming X is d-dimensional, each component j of the gradient error \u2207x\u2113(x,zn)\u2212 fn(x) satisfies\nE\n[\nexp { s(\u2207x\u2113(x,zn)\u2212\u2207 fn(x)) j }\n\u2223 \u2223 \u2223 \u2223 x ] \u2264 exp { 1 2 Cg d2 s2 }\nAssumption B.1 is reasonable if the space Z containing z is compact. Although in practice, the distribution of gradient error could depend on x, we assume that the bound Cg does not depend on x. We can view this as a pessimistic assumption corresponding to choosing the worst case bound as a function of x and the resulting Cg. This is a common assumption for in high probability analysis of optimization algorithms as in [16] for example.\nTo proceed, we first define two other useful estimates for \u03c1 . As discussed before, suppose that we make a second independent draw of samples {z\u0303i(k)}Kik=1 from pi. We use these samples to compute x\u0303i in the same manner as xi starting from xi\u22121 except with {z\u0303i(k)}Kik=1 used in place of {zi(k)} Ki k=1. Then define\n\u03c1\u0303 (2)i , \u2016x\u0303i \u2212 x\u0303i\u22121\u2016+ 1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 + 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(x\u0303i\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225\nThis is the same form as the direct estimate with x\u0303i in place of xi. Next, define\n\u03c1\u0303 (3)i , \u2016x\u0303i \u2212 x\u0303i\u22121\u2016+ 1 m \u2016\u2207 fi(xi)\u2016+ 1 m \u2016\u2207 fi\u22121(xi\u22121)\u2016\nThis is in fact the bound that inspired the direct estimate. We also define the averaged estimates\n\u03c1\u0302 (2)n , 1 n\u2212 1 n \u2211 i=2 \u03c1\u0303 (2)i\nand\n\u03c1\u0302 (3)n , 1 n\u2212 1 n \u2211 i=2 \u03c1\u0303 (3)i\nWe know that \u03c1\u0302 (3)n \u2265 \u03c1 . Thus, if we can control the gap between the pair \u03c1\u0302n and \u03c1\u0302 (2)n and the pair \u03c1\u0302 (2)n and \u03c1\u0302 (3)n , then we can ensure that \u03c1\u0302n plus an appropriate constant upper bounds \u03c1 for all n large enough as desired.\nFirst, we show that \u03c1\u0302 (2)n upper bounds \u03c1 eventually.\nLemma 2. Suppose that the following conditions hold:\n1. B.1 -B.2 hold\n2. The sequence {tn} satisfies \u221e\n\u2211 n=2 exp\n{\n\u2212 (n\u2212 1)m 2t2n\n72Cg\n}\n< \u221e\nThen for all n large enough it holds that \u03c1\u0302 (2)n + C\u0302 (2) n + tn \u2265 \u03c1 almost surely with\nC\u0302(2)n , 1\ndm(n\u2212 1)\n(\n\u221a\nCg K1 + 2 n \u2211 i=1\n\u221a\nCg Ki +\n\u221a\nCg Kn\n)\nProof. First, we have by the triangle equality and reverse triangle inequality\nm|\u03c1\u0303 (2)i \u2212 \u03c1\u0303 (3) i |\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 (\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212\u2016\u2207x fi(x\u0303i)\u2016 ) + (\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(x\u0303i\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212\u2016\u2207x fi\u22121(x\u0303i\u22121)\u2016 )\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2223 \u2223 \u2223\n\u2223 \u2223\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212\u2016\u2207x fi(x\u0303i)\u2016 \u2223 \u2223 \u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2223 \u2223 \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(x\u0303i\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212\u2016\u2207x fi\u22121(x\u0303i\u22121)\u2016 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2225 \u2225 \u2225\n\u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n+\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 (\u2207x\u2113(x\u0303i\u22121,zi\u22121(k))\u2212\u2207x fi\u22121(x\u0303i\u22121)) \u2225 \u2225 \u2225 \u2225 \u2225\nThen by the triangle inequality, we have\n|\u03c1\u0302 (2)n \u2212 \u03c1\u0302 (3)n | \u2264 1 m(n\u2212 1) n \u2211 i=2\n(\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n+\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 (\u2207x\u2113(x\u0303i\u22121,zi\u22121(k))\u2212\u2207x fi\u22121(x\u0303i\u22121)) \u2225 \u2225 \u2225 \u2225 \u2225 )\n\u2264 1 m(n\u2212 1)\n(\u2225\n\u2225 \u2225 \u2225 \u2225 1 K1\nK1\n\u2211 k=1\n(\u2207x\u2113(x\u03031,z1(k))\u2212\u2207x f1(x\u03031)) \u2225 \u2225 \u2225 \u2225\n\u2225\n+2 n\u22121 \u2211 i=2\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225\n+\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Kn Kn \u2211 k=1 (\u2207x\u2113(x\u0303n,zn(k))\u2212\u2207x fn(x\u0303n)) \u2225 \u2225 \u2225 \u2225 \u2225 )\n(9)\nWe will analyze the behavior of this bound on |\u03c1\u0302 (2)i \u2212 \u03c1\u0302 (3) i | using Lemma 22 in Appendix B. Define the filtration\nFi = \u03c3\n(\ni \u22c3\nj=1\n{z j(k)}K jk=1 \u222a i+1 \u22c3\nj=1\n{z\u0303 j(k)}K jk=1\n)\n\u2228K0 i = 0, . . . ,n (10)\nwith K0 from (6). Note that Ki\u22121 \u2282 Fi\u22121, so Ki is Fi\u22121-measurable. In addition, x\u0303i but not xi is Fi\u22121-measurable. Define the random variables\nVi =\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212E [\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 ] i = 1, . . . ,n\nClearly, Vi is Fi-measurable, since Vi is a function of x\u0303i, Ki, and {zi(k)}Kik=1 all of which are Fi-measurable. Conditioned on Fi\u22121, the sum\n1 Ki\nKi\n\u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) (11)\nis a sum of iid random variables. We now work with the conditional measure P{\u00b7 | Fi\u22121} to compute sub-Gaussian norms of (11) define in (24) and (25) of Appendix B. By assumption B.2 , we have\n\u03c42 ( (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) j ) \u2264 Cg d2\nTherefore, applying Lemma 24 yields\nB\n(\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) ) \u2264 \u221a\nCg Ki\ndue to the independence conditioned on Fi\u22121. By applying Lemma 25 from [17] to the conditional distribution P{\u00b7|Fi\u22121}, we have\nP\n{\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n> t\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 } \u2264 2exp { \u2212 t 2 2( \u221a Cg/Ki)2 }\n= 2exp\n{\n\u2212Kit 2\n2Cg\n}\nSince\nE\n[\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 ] \u2265 0,\nwe have\nP\n{\nVi > t\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 }\n= P\n{\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2212E [\u2225 \u2225 \u2225\n\u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 ] > t \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 }\n\u2264 P {\u2225 \u2225 \u2225\n\u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n> t\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 }\n\u2264 2exp { \u2212Kit 2\n2Cg\n}\n\u2264 2exp { \u2212 t 2\n2Cg\n}\nSince E[Vi | Fi\u22121] = 0, we can apply Lemma 26 with c = 1/(2Cg) to yield\nE [ esVi \u2223 \u2223 Fi\u22121 ] \u2264 exp { 1 2 (18Cg)s 2 }\nThis shows that the collection of random variables {Vi}ni=1 and the filtration {Fi}ni=0 satisfies the conditions of Lemma 22. Before applying Lemma 22, we bound the conditional expectations\nE\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121\n\n\nBy a straightforward calculation conditioned on Fi\u22121, we have\nE\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121\n\n\n= 1\nK2i\nKi\n\u2211 k=1\nKi \u2211 j=1 E [\u3008\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x f (x\u0303i),\u2207x\u2113(x\u0303i,zi( j))\u2212\u2207x f (x\u0303i)\u3009 | Fi\u22121]\n= 1\nK2i\nKi\n\u2211 k=1 E [ \u2016\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x f (x\u0303i)\u20162 | Fi\u22121 ]\n(a) =\n1\nK2i\nKi\n\u2211 k=1\nd\n\u2211 q=1 E [ (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x f (x\u0303i))2q | Fi\u22121 ]\n(b) \u2264 1\nK2i\nKi\n\u2211 k=1\nd Cg d2\n\u2264 Cg dKi\nwhere (a) is a decomposition into each component of the vector and (b) follows since a centered sub-Gaussian random variable with parameter Cg/d2 satisfies\nE [ (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x f (x\u0303i))2q | Fi\u22121 ] \u2264 Cg d2\nThen by Jensen\u2019s inequality\nE\n[\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 ] \u2264 \u221a Cg dKi\nDefine the constants\na1 = an = 1\nm(n\u2212 1)\na2 = \u00b7 \u00b7 \u00b7= an\u22121 = 2\nm(n\u2212 1)\nresulting in\n\u2016a\u201622 = 2\nm2(n\u2212 1)\nUsing the bound in (9) and Lemma 22 from Appendix B with this choice of a, it holds that\nP\n{\n|\u03c1\u0302 (2)n \u2212 \u03c1\u0302 (3)n |> n\n\u2211 i=1 ai\n\u221a\nCg dKi + t\n}\n\u2264 P { n\n\u2211 i=1 ai\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225\n> n\n\u2211 i=1 aiE\n[\u2225\n\u2225 \u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n(\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 ] + t }\n= P\n{\nn\n\u2211 i=1 aiVi > t\n}\n\u2264 exp { \u2212m 2(n\u2212 1)t2\n72Cg\n}\nCombining this bound with \u03c1\u0302 (3)n \u2265 \u03c1 yields\n\u221e\n\u2211 n=2 P\n{\n\u03c1\u0302 (2)n < \u03c1 \u2212 n\n\u2211 i=1 ai\n\u221a\nCg dKi\n\u2212 tn } \u2264 \u221e\n\u2211 n=2 P\n{\n\u03c1\u0302 (2)n < \u03c1\u0302 (3) n \u2212\nn\n\u2211 i=1 ai\n\u221a\nCg dKi\n\u2212 tn }\n\u2264 \u221e\n\u2211 n=2 P\n{\n|\u03c1\u0302 (2)n \u2212 \u03c1\u0302 (3)n |> n\n\u2211 i=1 ai\n\u221a\nCg dKi + tn\n}\n\u2264 \u221e\n\u2211 n=2 exp\n{\n\u2212m 2(n\u2212 1)t2n\n72Cg\n}\n< \u221e\nThe result follows from the Borel-Cantelli lemma. Note that as claimed\nC\u0302(2)n = 1\ndm(n\u2212 1)\n(\n\u221a\nCg K1 + 2 n\u22121 \u2211 i=2 \u221a Cg Ki + \u221a Cg Kn\n)\nNext, we show that \u03c1\u0302n upper bounds \u03c1\u0302 (2) n eventually with a general assumption on the optimization algorithm.\nWhen the conditions of Lemmas 2 and 3 are satisfied, it holds that \u03c1\u0302n plus a constant upper bounds \u03c1 .\nLemma 3. Suppose the following conditions hold:\n1. B.1-B.2 hold\n2. There exist bounds E [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ] \u2264C(Ki) i = 1, . . . ,n\n3. The sequence {tn} satisfies \u221e\n\u2211 n=2 exp\n{\n\u2212 (n\u2212 1) 2t2n\n2n ( 1+ Lm )2 diam2(X )\n}\n<+\u221e\nThen for all n large enough it holds that \u03c1\u0302n + C\u0302n + tn \u2265 \u03c1\u0302 (2)n almost surely with\nC\u0302n ,\n( 1+ Lm )\nn\u2212 1\n(\nC(K1)+ 2 n\u22121 \u2211 i=2 C(Ki)+C(Kn)\n)\nProof. We have by the triangle inequality, reverse triangle inequality, and the Lipschitz continuity of \u2207x\u2113(x,z) in x from assumption B.1\n|\u03c1\u0303i \u2212 \u03c1\u0303 (2)i | \u2264 \u2223 \u2223\u2016xi \u2212xi\u22121\u2016\u2212\u2016x\u0303i\u2212 x\u0303i\u22121\u2016 \u2223 \u2223\n+\n\u2223 \u2223 \u2223 \u2223 \u2223 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2223 \u2223 \u2223 \u2223 \u2223\n+\n\u2223 \u2223 \u2223 \u2223 \u2223 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(xi\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2212 1 m \u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 \u2207x\u2113(x\u0303i\u22121,zi\u22121(k)) \u2225 \u2225 \u2225 \u2225 \u2225 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2016(xi \u2212 x\u0303i)\u2212 (xi\u22121 \u2212 x\u0303i\u22121)\u2016\n+ 1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(xi,zi(k))\u2212\u2207x\u2113(x\u0303i,zi(k))) \u2225 \u2225 \u2225 \u2225 \u2225\n+ 1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki\u22121 Ki\u22121 \u2211 k=1 (\u2207x\u2113(xi\u22121,zi\u22121(k))\u2212\u2207x\u2113(x\u0303i\u22121,zi\u22121(k))) \u2225 \u2225 \u2225 \u2225 \u2225\n\u2264 ( 1+ L m ) (\u2016xi \u2212 x\u0303i\u2016+ \u2016xi\u22121\u2212 x\u0303i\u22121\u2016)\nso\n|\u03c1\u0302n \u2212 \u03c1\u0302 (2)n | \u2264 1 n\u2212 1 n \u2211 i=2 |\u03c1\u0303i \u2212 \u03c1\u0303 (2)i |\n\u2264 ( 1+ Lm ) n\u2212 1 n \u2211 i=2 (\u2016xi \u2212 x\u0303i\u2016+ \u2016xi\u22121\u2212 x\u0303i\u22121\u2016)\n=\n( 1+ Lm )\nn\u2212 1\n(\n\u2016x1 \u2212 x\u03031\u2016+ 2 n\u22121 \u2211 i=2 \u2016xi \u2212 x\u0303i\u2016+ \u2016xn\u2212 x\u0303n\u2016 )\nWe will again apply Lemma 22 of Appendix B to analyze this upper bound using the sigma algebra\nFi = \u03c3\n(\ni \u22c3\nj=1\n{z j(k)}K jk=1 \u222a i \u22c3\nj=1\n{z\u0303 j(k)}K jk=1\n)\n\u2228K0 i = 0, . . . ,n (12)\nDefine the random variable Vi = \u2016xi \u2212 x\u0303i\u2016\u2212E [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ]\nClearly, Vi is Fi-measurable. Since \u2212diam(X )\u2264Vi \u2264 diam(X ),\nand E [Vi | Fi\u22121] = 0, we can apply the conditional version Hoeffding\u2019s Lemma from Lemma 23 to yield\nE [ esVi \u2223 \u2223 Fi\u22121 ] \u2264 exp { 1 2 diam2(X )s2 }\nThe collection of random variables {Vi}ni=1 and the filtration {Fi}ni=0 satisfy the conditions of Lemma 22. Before applying Lemma 22, we bound the conditional expectations\nE [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ]\nBy assumption, we have E [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ] \u2264C(Ki) i = 1, . . . ,n\nand so (\n1+ Lm ) n\u2212 1\n(\nE [ \u2016x1 \u2212 x\u03031\u2016 \u2223 \u2223 F0 ] + 2 n\u22121 \u2211 i=2 E [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ] \u2016+E [ \u2016xn \u2212 x\u0303n\u2016 \u2223 \u2223 Fn\u22121 ]\n)\n\u2264 ( 1+ Lm )\nn\u2212 1\n(\nC(K1)+ 2 n\u22121 \u2211 i=2 C(Ki)+C(Kn)\n)\n, C\u0302n\nSet\na1 = an =\n( 1+ Lm )\nn\u2212 1 and\na2 = \u00b7 \u00b7 \u00b7= an\u22121 = ( 1+ Lm )\nn\u2212 1 resulting in\n\u2016a\u201622 = n ( 1+ Lm )2\n(n\u2212 1)2\nApplying our bound in (12) and Lemma 22 with this choice of a yields\nP\n{ |\u03c1\u0302n \u2212 \u03c1\u0302 (2)n |> C\u0302n + t }\n\u2264 P { ( 1+ Lm )\nn\u2212 1\n(\n\u2016x1 \u2212 x\u03031\u2016+ 2 n\u22121 \u2211 i=2 \u2016xi \u2212 x\u0303i\u2016+ \u2016xn\u2212 x\u0303n\u2016 )\n>\n( 1+ Lm )\nn\u2212 1\n(\nE [ \u2016x1 \u2212 x\u03031\u2016 \u2223 \u2223 F0 ] + 2 n\u22121 \u2211 i=2 E [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ] \u2016+E [ \u2016xn \u2212 x\u0303n\u2016 \u2223 \u2223 Fn\u22121 ]\n)\n+ t\n}\n= P\n{\n( 1+ Lm )\nn\u2212 1\n(\nV1 + 2 n\u22121 \u2211 i=2 Vi +Vn\n)\n> t\n}\n= P\n{\nn\n\u2211 i=1 aiVi > t\n}\n\u2264 exp { \u2212 (n\u2212 1) 2t2\n2n ( 1+ Lm )2 diam2(X )\n}\nFinally, we have\n\u221e\n\u2211 n=2 P\n{\n\u03c1\u0302n < \u03c1\u0302 (2) n \u2212 C\u0302n \u2212 tn\n} \u2264 \u221e\n\u2211 n=2 P\n{ |\u03c1\u0302n \u2212 \u03c1\u0302 (2)n |> C\u0302n + tn }\n\u2264 \u221e\n\u2211 n=2 exp\n{\n\u2212 (n\u2212 1) 2t2n\n2n ( 1+ Lm )2 diam2(X )\n}\n<+\u221e\nThe claim follows from the Borel-Cantelli Lemma.\nIf Lemmas 2 and 3 hold for the sequence {tn/2}, then for all n large enough it holds that\n\u03c1\u0302n + C\u0302n + C\u0302 (2) n + tn \u2265 \u03c1\nalmost surely.\nLemma 4. It always holds that\nE [ \u2016xi \u2212 x\u0303i\u2016 \u2223 \u2223 Fi\u22121 ] \u2264 2 \u221a\n1 m b ( diam2(X ),Ki )\nTherefore, the choice\nC(Ki), 2\n\u221a\n2 m b ( diam2(X ),Ki )\nsatisfies the conditions of Lemma 3.\nProof. Using the sigma algebras defined in (12) yields\nE [\u2016xi \u2212 x\u0303i\u2016 | Fi\u22121] \u2264 E [\u2016xi\u2212x\u2217i \u2016 | Fi\u22121]+E [\u2016x\u0303i \u2212x\u2217i \u2016 | Fi\u22121]\n\u2264 E [ \u221a\n2 m ( fi(xi)\u2212 fi(x\u2217i )) | Fi\u22121\n]\n+E\n[\n\u221a\n2 m ( fi(x\u0303i)\u2212 fi(x\u2217i )) | Fi\u22121\n]\n\u2264 \u221a\n2 m E [( fi(xi)\u2212 fi(x\u2217i )) | Fi\u22121]+\n\u221a\n2 m E [( fi(x\u0303i)\u2212 fi(x\u2217i )) | Fi\u22121]\n\u2264 2 \u221a\n2 m b(diam2(X ),Ki)\nwhere the third inequality follows from Jensen\u2019s inequality.\nThis choice of C(Kn)works for any algorithm with the associated b(d0,K). For any particular algorithm, we believe that we can produce tighter bounds independent of diam(X ) by copying the Lyapunov analysis used to analyze SGD as in Appendix A. The analysis becomes algorithm dependent in this case and is omitted.\nFinally, we state an overall theorem for the direct estimate that gives general combined conditions under which \u03c1\u0302n upper bounds \u03c1 .\nTheorem 1. If B.1 -B.2 hold and the sequence {tn} satisfies \u2211\u221en=2 e\u2212Cnt 2 n < \u221e for all C > 0, then for a sequence of constants {Cn} and for all n large enough it holds that \u03c1\u0302n +Cn + tn \u2265 \u03c1 almost surely.\nProof. Combine Lemmas 2 and 3 to yield the result with\nCn = C\u0302n + C\u0302 (2) n\n3.3.2 Vector IPM Estimate\nWe first derive a version of Hoeffding\u2019s inequality that allows for some dependence among the random variables. We use this concentration inequality to analyze \u03c1\u0302n for the IPM estimate. Given an integer W , we construct a cover of {1,2, . . . ,n} by dividing the set into W groups of integers spaced by W , i.e.,\nA j =\n{\nj, j+W, j+ 2W . . . , j+\n\u230a\nn\u2212 j W\n\u230b\nW\n}\nj = 1, . . . ,W (13)\nNote that\n{1,2, . . . ,n}= W \u22c3\nj=1\nA j\nand Ai \u2229A j = /0 for i 6= j. The proof of Lemma 5 is nearly identical to the proof of the extension of Hoeffding\u2019s inequality from [18] with Lemma 22 used instead. We assume that if we refer to a filtration Fi with i < 0, then we implicitly refer to F0.\nLemma 5 (Dependent Hoeffding\u2019s Inequality). Suppose we are given a collection of random variable {Vi}ni=1 and a filtration {F}ni=0 such that\n1. ai \u2264Vi \u2264 bi for constants ai and bi i = 1, . . . ,n\n2. Vi is Fi-measurable i = 1, . . . ,n\n3. Given an integer W and a cover {A j}Wj=1 as in (13) for each j it holds that\nE\n[ V j+iW \u2223 \u2223 \u2223 F j+(i\u22121)W ] = 0 i = 1, . . . ,\n\u230a\nn\u2212 j W\n\u230b\nand E [ V j \u2223 \u2223 \u2223 F0 ] = 0\nThen it holds that\nP\n{\nn\n\u2211 i=1 Vi > t\n}\n\u2264 exp { \u2212 2t 2 W \u2211ni=1(bi \u2212 ai)2 }\nand\nP\n{\nn\n\u2211 i=1\nVi <\u2212t } \u2264 exp { \u2212 2t 2 W \u2211ni=1(bi \u2212 ai)2 }\nProof. Define\nU j ,\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 V j+iW\nfor j = 1, . . . ,W . Let {p j}Wj=1 be a probability distribution on {1, . . . ,W} to be specified later. By Jensen\u2019s inequality, we have\nexp\n{\ns n\n\u2211 i=1 Vi\n}\n= exp\n{\nW \u2211 j=1 p j s p j U j\n}\n\u2264 W\n\u2211 j=1 p j exp\n{\ns p j U j\n}\nThen it holds that\nE\n[\nexp\n{\ns n\n\u2211 i=1 Vi\n}]\n\u2264 W\n\u2211 j=1 p jE\n[\nexp\n{\ns p j U j\n}]\nNow consider one term\nE\n[\nexp\n{\ns p j U j\n}]\n= E\n\n  exp\n\n \n \ns p j\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 V j+iW\n\n \n \n\n \nSince a j+iW \u2264V j+iW \u2264 b j+iW and E [ V j+iW \u2223 \u2223 \u2223 F j+(i\u22121)W ] = 0,\nwe can apply the conditional version Hoeffding\u2019s Lemma from Lemma 23 to yield\nE [ esV j+iW \u2223 \u2223 F j+(i\u22121)W ] \u2264 exp { 1 8 (b j+iW \u2212 a j+iW )2 s2 }\nThen we can apply Lemma 22 to {V j+iW} \u230a n\u2212 j W \u230b i=0 and {F j+iW} \u230a n\u2212 j W \u230b i=0 to yield\nE\n[\nexp\n{\ns p j U j\n}]\n\u2264 exp\n\n \n \ns2\n8p2j\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 (b j+iW \u2212 a j+iW )2\n\n \n \n=\n\u230a\nn\u2212 j W\n\u230b\n\u220f i=0 exp\n{\ns2\n8p2j (b\u03b1 \u2212 a\u03b1)2\n}\nThen we have\nE\n[\nexp\n{\ns n\n\u2211 i=1 Vi\n}]\n\u2264 W\n\u2211 j=1 p j\n\u230a\nn\u2212 j W\n\u230b\n\u220f i=0 exp\n{\ns2\n8p2j (b\u03b1 \u2212 a\u03b1)2\n}\n= W\n\u2211 j=1 p j exp\n{\ns2c j 8p2j\n}\nwith\nc j =\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 (b j+iW \u2212 a j+iW )2\nLet p j = \u221a c j/T and\nT = W\n\u2211 j=1\n\u221a c j.\nTherefore, we have\nE\n[\nexp\n{\ns n\n\u2211 i=1 Vi\n}]\n\u2264 exp { 1 8 T 2s2 }\nApplying the Chernoff bound [19] and optimizing yields\nP\n{\nn\n\u2211 i=1 Vi > t\n}\n\u2264 exp { \u22122t2/T 2 }\nBounding T with Cauchy-Schwarz yields\nT 2 \u2264 ( W\n\u2211 j=1 1\n)(\nW \u2211 j=1 c j\n)\n=W n\n\u2211 i=1 (bi \u2212 ai)2\nand the results follows. The proof for the other tail is nearly identical.\nIf we do not have the condition 3 of Lemma 5, then it holds that\nP\n\n \n \nn\n\u2211 i=1\nVi > W\n\u2211 j=1\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 E [ V j+iW \u2223 \u2223 F j+(i\u22121)W ] + t\n\n \n \n\u2264 exp { \u2212 2t 2 W \u2211ni=1(bi \u2212 ai)2 }\nIf we can bound the conditional expectation\nE [ V j+iW \u2223 \u2223 F j+(i\u22121)W ] \u2264C j+iW ,\nby a F j+(i\u22121)W -measurable random variable, then we have\nP\n{\nn\n\u2211 i=1\nVi > n\n\u2211 i=1 Ci + t\n}\n= P\n\n \n \nn\n\u2211 i=1\nVi > W\n\u2211 j=1\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 C j+iW + t\n\n \n \n\u2264 P\n\n \n \nn\n\u2211 i=1\nVi > W\n\u2211 j=1\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0 E [ V j+iW \u2223 \u2223 F j+(i\u22121)W ] + t\n\n \n \n\u2264 P\n\n \n \nW \u2211 j=1\n\u230a\nn\u2212 j W\n\u230b\n\u2211 i=0\n( V j+iW \u2212E [ V j+iW \u2223 \u2223 F j+(i\u22121)W ]) > t\n\n \n \n\u2264 exp { \u2212 2t 2 W \u2211ni=1(bi \u2212 ai)2 }\nWe have the following lemma characterizing the performance of the IPM estimate.\nLemma 6. For the IPM estimate and any sequence {tn} such that \u221e\n\u2211 n=2 exp\n{ \u2212 nt 2 n\n4diam(X )2\n}\n< \u221e\nfor all n large enough it holds that \u03c1\u0302n + tn \u2265 \u03c1 almost surely.\nProof. Define the random variables Vi = \u03c1\u0303i \u2212E [\u03c1\u0303i | Ki\u22122]\nwith {Ki}ni=1 defined in (6). We have \u2212diam(X )\u2264Vi \u2264 diam(X )\nClearly, Vi is Ki-measurable and E[Vi | Ki\u22122] = 0. Now, we can apply Lemma 5 with W = 2 to yield\nP\n{\nn\n\u2211 i=1\nVi <\u2212nt } \u2264 exp { \u2212 2(nt) 2\n(2) ( 4ndiam2(X ) )\n}\n= exp\n{\n\u2212 nt 2\n4diam2(X )\n}\nNone of the random variables {zi(k)}Kik=1 and {zi\u22121(k)} Ki\u22121 k=1 are Ki\u22122 measurable. Also, regardless of how many\nsamples Ki and Ki\u22121 are taken, the IPM estimate is biased upward. Thus, it holds that\nE [\u03c1\u0303i | Ki\u22122]\u2265 \u03c1\nTherefore, it follows that\nP{\u03c1\u0302n < \u03c1 \u2212 t} \u2264 P { n\n\u2211 i=1\n\u03c1\u0303i < n\n\u2211 i=1\nE [\u03c1\u0303i | Ki\u22122]\u2212 nt }\n= P\n{\nn\n\u2211 i=1\nVi <\u2212nt }\n\u2264 exp { \u2212 nt 2\n4diam2(X )\n}\nNote that we pay a price of two in the exponent due to \u03c1\u0303i and \u03c1\u0303i\u22121 both depending on the samples from pi\u22121. Since\n\u221e\n\u2211 n=2 exp\n{\n\u2212 nt 2 n\n4diam(X )2\n}\n< \u221e\nit follows that \u221e\n\u2211 n=2 P{\u03c1\u0302n + t < \u03c1}<+\u221e,\nThis in turn guarantees by way of the Borel-Cantelli Lemma that for n large enough\n\u03c1\u0302n + tn \u2265 \u03c1\nalmost surely.\n3.4 Combining One Step Estimates For Bounded Change\nWe now look at estimating \u03c1 in the case that \u2016x\u2217n \u2212x\u2217n\u22121\u2016 \u2264 \u03c1 .\nWe set \u03c1i , \u2016x\u2217i \u2212x\u2217i\u22121\u2016\nB.3 Assume that we have estimators h\u0302W : RW \u2192 R such that\n1. E[h\u0302W (\u03c1 j, . . . ,\u03c1 j\u2212W+1)]\u2265 \u03c1 for all j \u2265 1 and W \u2265 1 2. For any random variables {\u03c1\u0303i} such that E[\u03c1\u0303i]\u2265 E[\u03c1i], we have\nE [ h\u0302W (\u03c1\u0303 j, . . . , \u03c1\u0303 j\u2212W+1) ] \u2265 E [ h\u0302W (\u03c1 j, . . . ,\u03c1 j\u2212W+1) ]\nFor example, if \u03c1i iid\u223c Unif[0,\u03c1 ], then\nh\u0302W (\u03c1i,\u03c1i+1, . . . ,\u03c1i+W\u22121) = W + 1\nW max{\u03c1i,\u03c1i+1, . . . ,\u03c1i+W\u22121}\nis an estimator of \u03c1 with the required properties. Also, note that the two conditions on the estimator in B.3 imply that\nE [ h\u0302W (\u03c1\u0303 j, . . . , \u03c1\u0303 j\u2212W+1) ] \u2265 E [ h\u0302W (\u03c1 j, . . . ,\u03c1 j\u2212W+1) ] \u2265 \u03c1\nGiven an estimator satisfying assumption B.3 , we compute\n\u03c1\u0303 (i) = h\u0302W (\u03c1\u0303i, \u03c1\u0303i\u22121, . . . , \u03c1\u0303i\u2212W+1)\nand set\n\u03c1\u0302n = 1 n\u2212 1 n \u2211 i=2 \u03c1\u0303 (i) = 1 n\u2212 1 n \u2211 i=2 h\u0302min{W,i\u22121}(\u03c1\u0303i, \u03c1\u0303i\u22121, . . . , \u03c1\u0303max{i\u2212W+1,2}) (14)\nWe have\nE[\u03c1\u0302n] = 1 n\u2212 1 n \u2211 i=2 E[\u03c1\u0303 (i)]\u2265 \u03c1\nLemma 7 (IPM Single Step Estimates). For the estimator in (14) computed using the IPM estimate for \u03c1\u0303i and any sequence {tn} such that\n\u221e\n\u2211 n=2 exp\n{\n\u2212 2(n\u2212 1)t 2 n\n(W + 1)diam(X )2\n}\n< \u221e\nit holds that for all n large enough \u03c1\u0302n + tn \u2265 \u03c1 almost surely.\nProof. We copy the proof of Lemma 6 with W +1 in place of 2 and note that \u03c1\u0303 (i) and \u03c1\u0303 ( j) with |i\u2212 j|>W +1 do not depend on the same samples. Lemma 5 and some simple algebra yields\nP{\u03c1\u0302n < \u03c1 \u2212 t} \u2264 exp { \u2212 2(n\u2212 1)t 2\n(W + 1)diam(X )2\n}\nWe pay a price of W + 1 in the denominator of the exponent due to the dependence of the \u03c1\u0303 (i). By the Borel-Cantelli Lemma, for all n large enough it holds that \u03c1\u0302n + tn \u2265 \u03c1 almost surely as long as\n\u221e\n\u2211 n=2 exp\n{\n\u2212 2(n\u2212 1)t 2 n\n(W + 1)diam(X )2\n}\n< \u221e\nTo analyze the direct estimate, we need the following assumption\nB.4 Suppose that there exists absolute constants {bi}Wi=1 for any fixed W such that\n|h\u0302W (p1, . . . , pW )\u2212 h\u0302W (q1, . . . ,qW )| \u2264 W\n\u2211 i=1 bi|pi \u2212 qi| \u2200p,q \u2208RW\u22650\nFor the uniform case, we have \u2223\n\u2223 \u2223 W + 1 W max{p1, . . . , pW}\u2212 W + 1 W max{q1, . . . ,qW} \u2223 \u2223 \u2223 \u2264 W + 1 W max{|p1 \u2212 q1|, . . . , |pW \u2212 qW |}\n\u2264 W + 1 W\nW\n\u2211 i=1 |pi \u2212 qi|\nso\nb1 = \u00b7 \u00b7 \u00b7= bW = W + 1\nW Under assumption B.4 , we can then show that\n\u03c1\u0302n = 1 n\u2212W n \u2211 i=W+1 \u03c1\u0303 (i)\neventually upper bounds \u03c1 by copying the proofs of the lemmas behind Theorem 1.\nLemma 8 (Direct Single Step Estimates). Suppose that the following conditions hold:\n1. B.1 -B.4 hold\n2. The sequence {tn} satisfies\n\u221e\n\u2211 n=W+1 exp\n\n \n \n\u2212 (n\u2212W) 2t2n\n32n ( 1+ Lm )2 ( \u2211Wj=1 b j )2 diam2(X )\n\n \n \n<+\u221e\nand\n\u221e\n\u2211 n=W+1 exp\n\n \n \n\u2212 (n\u2212W) 2m2t2n\n144nCg ( \u2211Wj=1 b j )2\n\n \n \n<+\u221e\n3. There are bounds C(K) such that E [\u2016xi \u2212 x\u0303i\u2016 | Fi\u22121]\u2264C(Ki)\nThen for all n large enough it holds that \u03c1\u0302n +U\u0302n + V\u0302n + tn \u2265 \u03c1 almost surely with\nU\u0302n = 2 ( 1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 C(Ki)\nand\nV\u0302n = 2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 \u221a Cg dKi\nProof. Define \u03c1\u0303 (2)i , \u03c1\u0303 (3) i , \u03c1\u0302 (2) i , and \u03c1\u0302 (3) i as in Lemmas 2 and 3. First, we have\n|\u03c1\u0302n \u2212 \u03c1\u0302 (3)n | \u2264 1 n\u2212W n \u2211 i=W+1 |\u03c1\u0303 (i)\u2212 \u03c1\u0303 (i)3 |\n\u2264 1 n\u2212W\nn\n\u2211 i=W+1\ni\n\u2211 j=i\u2212W+1 b j|\u03c1\u0303 j \u2212 \u03c1\u0303 (3)j |\n\u2264 1 n\u2212W\nn\n\u2211 i=W+1\ni\n\u2211 j=i\u2212W+1\nb j ( |\u03c1\u0303 j \u2212 \u03c1\u0303 (2)j |+ |\u03c1\u0303 (2) j \u2212 \u03c1\u0303 (3) j | )\n\u2264 \u2211 W j=1 b j n\u2212W n \u2211 i=2 ( |\u03c1\u0303i \u2212 \u03c1\u0303 (2)i |+ |\u03c1\u0303 (2) i \u2212 \u03c1\u0303 (3) i | )\nSecond, define Ui , \u2016xi \u2212 x\u0303i\u2016\nand\nVi ,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207 fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225\nThen we have\n|\u03c1\u0303i \u2212 \u03c1\u0303 (2)i | \u2264 \u2016xi \u2212 x\u0303i\u2016+ 1 m\n\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(xi,zi(k))\u2212\u2207x\u2113(x\u0303i,zi(k))) \u2225 \u2225 \u2225 \u2225 \u2225\n\u2264 ( 1+ L m ) (Ui +Ui\u22121)\nand\n|\u03c1\u0303 (2)i \u2212 \u03c1\u0303 (3) i | \u2264 1 m (Vi +Vi\u22121)\nThen it follows that\n|\u03c1\u0302n \u2212 \u03c1\u0302 (3)n | \u2264 \u2211Wj=1 b j n\u2212W n \u2211 i=2 ( |\u03c1\u0303i \u2212 \u03c1\u0303 (2)i |+ |\u03c1\u0303 (2) i \u2212 \u03c1\u0303 (3) i | )\n\u2264 2 ( 1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 Ui + 2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 Vi\nSuppose that 2 (\n1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 E [Ui | Fi\u22121]\u2264 U\u0302n\nand 2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 E [Vi | Fi\u22121]\u2264 V\u0302n\nThen it holds that\nP\n{ |\u03c1\u0302n \u2212 \u03c1\u0302 (3)n |> U\u0302n + V\u0302n + t }\n\u2264 P { 2 ( 1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 Ui + 2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 Vi > U\u0302n + V\u0302n + t } \u2264 P { 2 ( 1+ Lm )\n\u2211Wj=1 b j n\u2212W n \u2211 i=1 Ui > U\u0302n + t 2\n}\n+P\n{\n2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 Vi > V\u0302n + t 2\n}\nWe can apply Lemma 22 to each term to yield\nP\n{\n2 ( 1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 Ui > U\u0302n + t 2\n}\n\u2264 exp\n\n \n \n\u2212 (n\u2212W) 2t2\n32n ( 1+ Lm )2 ( \u2211Wj=1 b j )2 diam2(X )\n\n \n \nand\nP\n{\n2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 Vi > V\u0302n + t 2\n}\n\u2264 exp\n\n \n \n\u2212 (n\u2212W) 2m2t2\n144nCg ( \u2211Wj=1 b j )2\n\n \n \nThen it holds that\nP\n{ |\u03c1\u0302n \u2212 \u03c1\u0302 (3)n |> U\u0302n + V\u0302n + t }\n\u2264 exp\n\n \n \n\u2212 (n\u2212W) 2t2\n32n ( 1+ Lm )2 ( \u2211Wj=1 b j )2 diam2(X )\n\n \n \n+ exp\n\n \n \n\u2212 (n\u2212W) 2m2t2\n144nCg ( \u2211Wj=1 b j )2\n\n \n \nWe have by straightforward computation\nU\u0302n = 2 ( 1+ Lm ) \u2211Wj=1 b j n\u2212W n \u2211 i=1 C(Ki)\nand\nV\u0302n = 2\u2211Wj=1 b j m(n\u2212W) n \u2211 i=1 \u221a Cg dKi\nThen it holds that \u221e\n\u2211 n=W+1 P { \u03c1\u0302n < \u03c1 \u2212U\u0302n \u2212 V\u0302n \u2212 tn }\n\u2264 \u221e\n\u2211 n=W+1 P\n{\n\u03c1\u0302n < \u03c1\u0302 (3) n \u2212U\u0302n \u2212 V\u0302n \u2212 tn\n}\n\u2264 \u221e\n\u2211 n=W+1 P\n{ |\u03c1\u0302n \u2212 \u03c1\u0302 (3)n |> U\u0302n + V\u0302n + tn }\n\u2264 \u221e\n\u2211 n=W+1 exp\n\n \n \n\u2212 (n\u2212W) 2t2n\n32n ( 1+ Lm )2 ( \u2211Wj=1 b j )2 diam2(X )\n\n \n \n+ \u221e\n\u2211 n=W+1 exp\n\n \n \n\u2212 (n\u2212W) 2m2t2n\n144nCg ( \u2211Wj=1 b j )2\n\n \n \n< \u221e\nBy the Borel-Cantelli lemma, it follows that for all n large enough\n\u03c1\u0302n +U\u0302n + V\u0302n + tn \u2264 \u03c1 almost surely.\n3.5 Parameter Estimation\nWe may need to estimate parameters of the functions { fn} such as the strong convexity parameter m to compute b(d0,K). We need the following assumption on our bound:\nD.1 Suppose that our bound b(d0,K,\u03c8) is parameterized by \u03c8 , which depends on properties of the function \u2113(x,z) and the distributions {pn}\u221en=1. Suppose that\n\u03c81 \u2264 \u03c82 \u21d4 b(d0,K,\u03c81)\u2264 b(d0,K,\u03c82)\nD.2 There exists a true set of parameters \u03c8\u2217 such that\n\u03c8n = \u03c8\u2217 \u2200n \u2265 1\nD.3 The spaces X and Z are compact\nD.4 There exists a constant L such that\n\u2016\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u0303,z)\u2016 \u2264 L\u2016x\u2212 x\u0303\u2016\nD.5 Suppose that we know that the parameters \u03c8 \u2208 P with P compact\nD.6 Suppose that \u2207 fn(xn) has Lipschitz continuous gradients with modulus M\nAs a consequence of Assumption D.4 , it follows that there exists a constant G such that there exists a constant G such that \u2016\u2207x\u2113(x,z)\u2016 \u2264 G \u2200x \u2208 X ,z \u2208 Z Satisfying Assumption D.5 is usually easy due to the compactness assumptions in Assumption D.4 .\nIn most cases, we have\n\u03c8 =\n\n   \u2212m M A B\n\n  \nwhere m is the parameter of strong convexity, M is the Lipschitz gradient modulus, and the pair (A,B) controls gradient growth, i.e.,\nE\u2016\u2207x\u2113(x,z)\u20162 \u2264 A+B\u2016x\u2212x\u2217\u20162\nWe parameterize using \u2212m, since smaller m increase the bound b(d0,K). We present several general methods for estimating these parameters, although in practice, problem specific estimators based on the form of the function may offer better performance. As an example, we present problem specific estimates for\n\u2113(x,z) = 1 2 ( y\u2212w\u22a4x )2 + 1 2 \u03bb\u2016x\u20162\nAs in estimating \u03c1 , we produce one time instant estimates m\u0303i, M\u0303i, A\u0303i, and B\u0303i at time i and combine them. We only examine the case under Assumption D.4 , although we could examine an inequality constraints as with estimating \u03c1 . We combine estimates by averaging to yield\n1. m\u0302n = 1n \u2211 n i=1 m\u0303i\n2. M\u0302n = 1n \u2211 n i=1 M\u0303i\n3. A\u0302n = 1n \u2211 n i=1 A\u0303i\n4. B\u0302n = 1n \u2211 n i=1 B\u0303i\n3.5.1 Estimating Strong Convexity Parameter and Lipschitz Gradient Modulus\nWe seek one step estimators m\u0303n and M\u0303n such that\nE[m\u0303n | Kn\u22121]\u2264 m\nand E[M\u0303n | Kn\u22121]\u2265 M\nwith {Kn} defined in (6). Hessian Method: We exploit the fact that\n\u22072 xx fn(x) mI \u2200x \u2208 X\nThis in turn implies that \u03bbmin ( \u22072 xx fn(x) ) \u2265 m \u2200x \u2208 X\nThis suggests that given {zn(k)}Knk=1 we set\nm\u0303n , min x\u2208X \u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n)\nSince \u03bbmin(A) = min\nv:\u2016v\u2016=1 \u3008Av,v\u3009 ,\n\u03bbmin(A) is a concave function of A. Then by Jensen\u2019s inequality, we have\nE[m\u0303n] = E\n[\nmin x\u2208X \u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n)\n\u2223 \u2223 \u2223 \u2223 Kn\u22121\n]\n\u2264 min x\u2208X E\n[\n\u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n)\n\u2223 \u2223 \u2223 \u2223 Kn\u22121\n]\n\u2264 min x\u2208X \u03bbmin\n(\nE\n[\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n\u2223 \u2223 \u2223 \u2223 Kn\u22121\n])\n= min x\u2208X\n\u03bbmin ( \u22072 xx fn(x) )\n= m\nSimilarly, we can set\nM\u0303n , max x\u2208X \u03bbmax\n(\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n)\nSince \u03bbmax(A) = max\nv:\u2016v\u2016=1 \u3008Av,v\u3009 ,\n\u03bbmax(A) is a convex function of A. By Jensen\u2019s inequality, it holds that\nE[M\u0303n | Kn\u22121]\u2265 M\nGradient Method To Compute m\u0303n: To actually minimize over x, we can use gradient descent. To apply gradient descent, we use eigenvalue perturbation results [20]. Suppose that we have a base matrix T0 with eigenvectors v0i and eigenvalues \u03bb0i. We want to find the eigenvectors vi and eigenvalues \u03bbi of a perturbed matrix T :\nT0v0i = \u03bb0iv0i Tvi = \u03bbivi\nIn particular, we want to relate \u03bb0i to \u03bbi. With \u03b4T , T \u2212T0,\nwe have \u03b4\u03bbi = v\u22a40i (\u03b4T )v0i\nand \u2202\u03bbi \u2202Ti j = v0i(i)v0 j(2\u2212 \u03b4i j)\nSuppose we are given a matrix-valued function T (x) with\nT (x)v(x) = \u03bbmin(x)v(x)\nThen it holds that\n\u2207x\u03bbmin (T (x)) = \u2211 i, j \u2202\u03bbmin \u2202Ti j \u2207xTi j(x)\n= \u2211 i, j vi(x)v j(x)(2\u2212 \u03b4i j)\u2207xTi j(x)\nThen we can use gradient descent to solve\nmin x\u2208X \u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1 \u2207x\u2113(x,zn(k))\n)\nStarting from any x(0), we can compute\nx(p) = \u03a0X\n[ x(p\u2212 1)\u2212 \u00b5\u2207x\u03bbmin (\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x,zn(k))\n)]\np = 1, . . . ,P\nand set\nm\u0302n , \u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1 \u22072 xx \u2113(x(P),zn(k))\n)\n(15)\nHeuristic Method: For any two points x and y, we have by strong convexity\nfn(y)\u2265 fn(x)+ \u3008\u2207 fn(x),y\u2212x\u3009+ 1 2 m\u2016y\u2212x\u20162\nSuppose that we have N points x(1), . . . ,x(N). Then we know that for any two distinct points xi and x j\nm \u2264 fn(x(i))\u2212 fn(x( j))\u2212\u3008\u2207 fn(x( j)),x(i)\u2212x( j)\u30091 2\u2016x(i)\u2212x( j)\u20162\nThis suggests the estimator\nm\u0302n , min i6= j\n1 Kn \u2211 Kn k=1 \u2113(x(i),zn(k))\u2212 1Kn \u2211 Kn k=1 \u2113(x( j),zn(k))\u2212\n\u2329\n1 Kn \u2211 Kn k=1 \u2207x\u2113(x( j),zn(k)),x(i)\u2212x( j)\n\u232a\n1 2\u2016x(i)\u2212x( j)\u20162\n(16)\nfor the strong convexity parameter. Then we have\nE[m\u0302n]\n= E\n\nmin i6= j\n1 Kn \u2211 Kn k=1 \u2113(x(i),zn(k))\u2212 1Kn \u2211 Kn k=1 \u2113(x( j),zn(k))\u2212\n\u2329\n1 Kn \u2211 Kn k=1 \u2207x\u2113(x( j),zn(k)),x(i)\u2212x( j)\n\u232a\n1 2\u2016x(i)\u2212x( j)\u20162\n\n\n\u2264 min i6= j E\n\n\n1 Kn \u2211 Kn k=1 \u2113(x(i),zn(k))\u2212 1Kn \u2211 Kn k=1 \u2113(x( j),zn(k))\u2212\n\u2329\n1 Kn \u2211 Kn k=1 \u2207x\u2113(x( j),zn(k)),x(i)\u2212x( j)\n\u232a\n1 2\u2016x(i)\u2212x( j)\u20162\n\n\n\u2264 min i6= j fn(x(i))\u2212 fn(x( j))\u2212\u3008\u2207 fn(x( j)),x(i)\u2212x( j)\u3009 1 2\u2016x(i)\u2212x( j)\u20162\nIt is difficult to compare this estimator to m exactly. All we can say is that\nm \u2264 min i6= j fn(x(i))\u2212 fn(x( j))\u2212\u3008\u2207 fn(x( j)),x(i)\u2212x( j)\u3009 1 2\u2016x(i)\u2212x( j)\u20162\nas well. In practice, this method produces estimates close to m. Similarly, we can set\nM\u0302n , max i6= j\n1 Kn \u2211 Kn k=1 \u2113(x(i),zn(k))\u2212 1Kn \u2211 Kn k=1 \u2113(x( j),zn(k))\u2212\n\u2329\n1 Kn \u2211 Kn k=1 \u2207x\u2113(x( j),zn(k)),x(i)\u2212x( j)\n\u232a\n1 2\u2016x(i)\u2212x( j)\u20162\n(17)\nProblem Specific: For the penalized quadratic, we have\n\u22072 xx \u2113(x,z) = \u03bbI+ww\u22a4\nso \u22072\nxx fn(x) = \u03bbI+E[wnw\u22a4n ]\nThis suggests the simple closed-form estimates\nm\u0303n = \u03bb +\u03bbmin\n(\n1 Kn\nKn\n\u2211 k=1\nwn(k)wn(k) \u22a4 )\nand\nM\u0303n = \u03bb +\u03bbmax\n(\n1 Kn\nKn\n\u2211 k=1\nwn(k)wn(k) \u22a4 )\nAgain, by Jensen\u2019s inequality, it holds that E[m\u0303n | Kn\u22121]\u2264 m\nand E[M\u0303n | Kn\u22121]\u2265 M\nCombining Estimates: We now look at combining the single time instant estimates of the strong convexity parameter and the Lipschitz gradient modulus.\nLemma 9. Choose tn such that for all C > 0 it holds that\n\u221e\n\u2211 n=1\ne\u2212Cnt 2 n <+\u221e\nThen for all n large enough it holds that\n1. m\u0302n \u2212 tn \u2264 m\n2. M\u0302n + tn \u2265 M\nalmost surely.\nProof. By the compactness of the space P containing \u03c8 , we can apply the dependent version of Hoeffding\u2019s lemma (Lemma 23) to yield\nE [ esm\u0303i \u2223 \u2223 Ki\u22121 ] \u2264 exp { 1 2 \u03c32ms 2 }\nand\nE\n[\nesM\u0303i \u2223 \u2223 Ki\u22121 ] \u2264 exp { 1 2 \u03c32Ms 2 }\nfor some constants \u03c32m and \u03c32M derived from Hoeffding\u2019s lemma. Then applying Lemma 22, it follows that\nP\n{\nm\u0302n > 1 n\nn\n\u2211 i=1\nE[m\u0303i | Ki\u22121]+ tn } \u2264 exp { \u2212 nt 2 n\n2\u03c32m\n}\nWe know that 1 n n \u2211 i=1 E[m\u0303i | Ki\u22121]> m\nso it follows that\nP{m\u0302n > m+ tn} \u2264 exp { \u2212 nt 2 n\n2\u03c32m\n}\nSimilarly, for the Lipschitz gradient modulus, it holds that\nP { M\u0302n < M\u2212 tn } \u2264 exp { \u2212 nt 2 n\n2\u03c32M\n}\nAs before, we have \u221e\n\u2211 n=1\nP{m\u0302n > m+ tn} \u2264 \u221e\n\u2211 n=1 exp\n{\n\u2212 nt 2 n\n2\u03c32m\n}\n<+\u221e\nand \u221e\n\u2211 n=1 P { M\u0302n < M\u2212 tn }\n\u2264 \u221e\n\u2211 n=1 exp\n{\n\u2212 nt 2 n\n2\u03c32M\n}\n<+\u221e\nto ensure that almost surely for all n large enough it holds that\nm\u0302n \u2212 tn \u2264 m\nand M\u0302n + tn \u2265 m\nFor Lemma 9, we need tn to decay no faster that O(n\u22121/2).\n3.5.2 Estimating Gradient Parameters\nFrom Assumption D.6 , it holds that\nE\u2016\u2207x\u2113(x,z)\u20162 = E\u2016\u2207x\u2113(x\u2217,z)+ (\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u2217,z))\u20162\n\u2264 2E\u2016\u2207x\u2113(x\u2217,z)\u20162 + 2E\u2016\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u2217,z)\u20162 \u2264 2E\u2016\u2207x\u2113(x\u2217,z)\u20162 + 2M2\u2016x\u2212x\u2217\u20162\nThus, we can set B = 2M2\nand A = 2E\u2016\u2207x\u2113(x\u2217,z)\u20162\nThis suggests that given an estimate M\u0303n for M, we set\nB\u0303n = 2M\u03032n\nThen by Jensen\u2019s inequality, we have\nE[B\u0303n | Kn\u22121] = 2E[M\u03032n | Kn\u22121] \u2265 2 ( E[B\u0303n | Kn\u22121] )2\n\u2265 2M2\n= B\nLemma 10. Choose tn such that for all C > 0 it holds that \u221e\n\u2211 n=1\ne\u2212Cnt 2 n <+\u221e\nThen for all n large enough it holds that B\u0302n + tn \u2265 B\nalmost surely.\nProof. By identical reasoning for the strong convexity and Lipschitz continuous gradients, it holds that\nP { B\u0302n < B\u2212 tn } \u2264 exp { \u2212 nt 2 n\n2\u03c32B\n}\nSince we have \u221e\n\u2211 n=1 exp\n{\n\u2212 nt 2 n\n2\u03c32B\n}\n<+\u221e\nfor all n large enough it holds that B\u0302n + tn \u2265 B\nalmost surely.\nTo estimate A, consider using a point x to approximate x\u2217. It holds that\nE\u2016\u2207x\u2113(x\u2217,z)\u20162 = E\u2016\u2207x\u2113(x,z)+ (\u2207x\u2113(x\u2217,z)\u2212\u2207x\u2113(x,z))\u20162\n\u2264 2E\u2016\u2207x\u2113(x,z)\u20162 + 2E\u2016\u2207x\u2113(x\u2217,z)\u2212\u2207x\u2113(x,z)\u20162 \u2264 2E\u2016\u2207x\u2113(x,z)\u20162 + 2M2E\u2016x\u2212x\u2217\u20162 \u2264 2E\u2016\u2207x\u2113(x,z)\u20162 + 2 (\nM m\n)2\n\u2016\u2207 f (x)\u20162\n\u2264 2E\u2016\u2207x\u2113(x,z)\u20162 + 2 ( M m\n)2\n\u2016\u2207 f (x)\u20162\nThis suggests the estimate\nA\u0303n(x) = 2\nKn\nKn\n\u2211 k=1\n\u2016\u2207x\u2113(x,zn(k))\u20162 + 4 ( M\u0303n\u22121 + tn\u22121 m\u0303n\u22121 \u2212 tn\u22121\n)2\u2225 \u2225\n\u2225 \u2225 1 Kn\nKn\n\u2211 k=1\n\u2207x\u2113(x,zn(k)) \u2225 \u2225 \u2225\n\u2225\n2\nLemma 11. For any x possibly random but not a function of {zn(k)}Knk=1 and all n large enough, it holds that\nE[A\u0303n | Kn\u22121]\u2265 A\nProof. For any x possibly random but not a function of {zn(k)}Knk=1, it holds that\nE[A\u0303n | Kn\u22121]\n= E\n[\n2 Kn\nKn\n\u2211 k=1\n\u2016\u2207x\u2113(x,zn(k))\u20162 + 4 ( M\u0303n\u22121 + tn\u22121 m\u0303n\u22121 \u2212 tn\u22121\n)2\u2225 \u2225\n\u2225 \u2225 1 Kn\nKn\n\u2211 k=1\n\u2207x\u2113(x,zn(k)) \u2225 \u2225 \u2225\n\u2225\n2 \u2223 \u2223\n\u2223 \u2223 Kn\u22121\n]\n= E\n[\n2 Kn\nKn\n\u2211 k=1\n\u2016\u2207x\u2113(x,zn(k))\u20162 \u2223 \u2223 \u2223\n\u2223\nKn\u22121\n]\n+ 4\n(\nM\u0303n\u22121 + tn\u22121 m\u0303n\u22121 \u2212 tn\u22121\n)2\nE\n[\n\u2225 \u2225 \u2225 \u2225 1 Kn Kn \u2211 k=1 \u2207x\u2113(x,zn(k)) \u2225 \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 Kn\u22121\n]\n\u2265 2E\u2016\u2207x\u2113(x,zn)\u20162 + 4 ( M\u0303n\u22121 + tn\u22121 m\u0303n\u22121 \u2212 tn\u22121\n)2\n\u2016\u2207 fn(x)\u20162\nThe last inequality uses Jensen\u2019s inequality. Then by our prior analysis, almost surely for all n sufficiently large it holds that\nM\u0303n\u22121 + tn\u22121 m\u0303n\u22121 \u2212 tn\u22121 \u2265 M m\nand so for all n sufficiently large\nE[A\u0303n | Kn\u22121] \u2265 2E\u2016\u2207x\u2113(x,zn)\u20162 + 4 ( M m\n)2\n\u2016\u2207 fn(x)\u20162\n= 2E\u2016\u2207x\u2113(x\u2217n,zn)\u20162 = A\nTherefore, for all n sufficiently large (dependent on estimation of m and M), it holds that\nE[A\u0303n | Kn\u22121]\u2265 A\nCombining Estimates for A: In practice, we use A\u0303n(xn), which complicates the analysis due to the fact that xn is computed using the same samples {zn(k)}Knk=1.\nLemma 12. Choose tn such that for all C > 0 it holds that\n\u221e\n\u2211 n=1\ne\u2212Cnt 2 n <+\u221e\nThen for all n large enough it holds that A\u0302n + tn \u2265 A\nalmost surely.\nProof. Consider the following three estimates of A all computed with knowledge of m and M and x\u0303n as in Lemma 2:\nA\u0303(2)i = 2 Ki\nKi\n\u2211 k=1\n\u2016\u2207x\u2113(xi,zi(k))\u20162 + 4 ( M m )2\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 2\nA\u0303(3)i = 2 Ki\nKi\n\u2211 k=1\n\u2016\u2207x\u2113(x\u0303i,zi(k))\u20162 + 4 ( M m )2\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 2\nA\u0303(4)i = 2E\u2016\u2207x\u2113(x\u0303i,zi)\u20162 + 4 ( M m\n)2\n\u2016\u2207 fi(x\u0303i)\u20162\nDefine the averaged estimates\nA\u0302(2)n = 1 n\nn\n\u2211 i=1 A\u0303(2)i\nA\u0302(3)n = 1 n\nn\n\u2211 i=1 A\u0303(3)i\nA\u0302(4)n = 1 n\nn\n\u2211 i=1 A\u0303(4)i\nWe always have A\u0303(4)i \u2265 A so A\u0302(4)n \u2265 A First, we show that A\u0302(2)n is close to A (3) n . We have\n|A\u0303(2)i \u2212 A\u0303 (3) i |\n\u2264 2 \u2223 \u2223 \u2223\n\u2223 1 Ki\nKi\n\u2211 k=1\n( \u2016\u2207x\u2113(xi,zi(k))\u20162 \u2212\u2016\u2207x\u2113(x\u0303i,zi(k))\u20162 )\n\u2223 \u2223 \u2223 \u2223\n+ 4\n(\nM m\n)2 \u2223 \u2223\n\u2223 \u2223\n\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225 \u2225 2 \u2212 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 \u2207x\u2113(x\u0303i,zi(k)) \u2225 \u2225 \u2225 \u2225 2\u2223 \u2223 \u2223 \u2223\n\u2264 4G 1 Ki\nKi\n\u2211 k=1\n\u2016\u2207x\u2113(xi,zi(k))\u2212\u2207x\u2113(x\u0303i,zi(k))\u2016+ 8G ( M m )2\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(xi,zi(k))\u2212\u2207x\u2113(x\u0303i,zi(k))) \u2225 \u2225 \u2225 \u2225 2\n\u2264 ( 4+ 8 (\nM m\n)2 )\nGM\u2016xi \u2212 x\u0303i\u2016\nyielding\n|A\u0302(2)n \u2212 A\u0302(3)n | \u2264 ( 4+ 8 (\nM m\n)2 )\nGM\n(\n1 n\nn\n\u2211 i=1\n\u2016xi \u2212 x\u0303i\u2016 )\nSecond, we have\n|A\u0302(3)n \u2212 A\u0302(4)n |\n\u2264 \u2223 \u2223 \u2223\n\u2223 1 n\nn\n\u2211 i=1\n(\n2 Ki\nKi\n\u2211 k=1\n( \u2016\u2207x\u2113(x\u0303i,zi(k))\u20162 \u2212E [ \u2016\u2207x\u2113(x\u0303i,zi)\u20162 | Fn\u22121 ])\n)\n\u2223 \u2223 \u2223 \u2223\n+ 8\n(\nM m\n)2\nG 1 n\nn\n\u2211 i=1\n\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207 fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\nCombining both inequalities, we know that\n|A\u0302(2)n \u2212 A\u0302(4)n |\n\u2264 ( 4+ 8 (\nM m\n)2 )\nGM\n(\n1 n\nn\n\u2211 i=1\n\u2016xi \u2212 x\u0303i\u2016 )\n+\n\u2223 \u2223 \u2223 \u2223 1 n n \u2211 i=1\n(\n2 Ki\nKi\n\u2211 k=1\n( \u2016\u2207x\u2113(x\u0303i,zi(k))\u20162 \u2212E [ \u2016\u2207x\u2113(x\u0303i,zi)\u20162 | Fn\u22121 ])\n)\n\u2223 \u2223 \u2223 \u2223\n+ 8\n(\nM m\n)2\nG 1 n\nn\n\u2211 i=1\n\u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207x\u2113(x\u0303i,zi(k))\u2212\u2207 fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225\nThe first and third terms in this bound can be controlled by the analysis of the direct estimate and the second term by Lemma (22). This shows that\nP\n{\nA\u0302(2)n < A\u2212 1 n\nn\n\u2211 i=1 Ci\u221a Ki\n\u2212 tn }\n\u2264 P { A\u0302(2)n < A\u0302 (4) n \u2212\n1 n\nn\n\u2211 i=1 Ci\u221a Ki\n\u2212 tn }\n\u2264 P {\n|A\u0302(2)n \u2212 A\u0302(4)n |> 1 n\nn\n\u2211 i=1 Ci\u221a Ki tn\n}\n\u2264 2exp { \u2212 nt 2 n\n2\u03c32A2\n}\nSince \u221e\n\u2211 n=1 P\n{\nA\u0302(2)n < A\u2212 1 n\nn\n\u2211 i=1 Ci\u221a Ki\n\u2212 tn } \u2264 \u221e\n\u2211 n=1 C exp\n{ \u2212 nt 2 n\n2\u03c32A2\n}\n<+\u221e\nalmost surely for all n large enough, it holds that\nA\u0302(2)n + 1 n\nn\n\u2211 i=1 Ci\u221a Ki + tn \u2265 A\nIn addition, we have\nA\u0302(2)n + 1 n\nn\n\u2211 i=1 Ci\u221a Ki + 2tn \u2265 A\nThere exists a random variable N\u0303 such that\nn \u2265 N\u0303 \u21d2 Mn + tn mn \u2212 tn \u2265 M m\nThen for n \u2265 N\u0303, it holds that\nA\u0302n \u2212 A\u0302(2)n\n= 4 n\nn\n\u2211 i=1\n[\n(\nM\u0302i\u22121 + ti\u22121 m\u0302i\u22121 \u2212 ti\u22121\n)2\n\u2212 ( M m\n)2 ] \u2225\n\u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n\u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225\n\u2225\n2\n\u2265 4 n N\u0303\u22121 \u2211 i=1\n[\n(\nM\u0302i\u22121 + ti\u22121 m\u0302i\u22121 \u2212 ti\u22121\n)2\n\u2212 ( M m\n)2 ] \u2225\n\u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n\u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225\n\u2225\n2\nSince our choice of tn can decay only as fast as C/ \u221a n, it follows that\n4 n N\u0303\u22121 \u2211 i=1\n[\n(\nM\u0302i\u22121 + ti\u22121 m\u0302i\u22121 \u2212 ti\u22121\n)2\n\u2212 ( M m\n)2 ] \u2225\n\u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n\u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225\n\u2225\n2\n\u2212 tn < 0\nfor all n large enough. This implies that\nA\u0302n + 1 n\nn\n\u2211 i=1 Ci\u221a Ki + tn\n\u2265 A\u0302n \u2212 (\n4 n N\u0303\u22121 \u2211 i=1\n[\n(\nM m\n)2\n\u2212 ( M\u0302i\u22121 + ti\u22121 m\u0302i\u22121 + ti\u22121\n)2 ] \u2225\n\u2225 \u2225 \u2225 1 Ki\nKi\n\u2211 k=1\n\u2207x\u2113(xi,zi(k)) \u2225 \u2225 \u2225\n\u2225\n2 \u2212 tn )\n+ 1 n\nn\n\u2211 i=1 Ci\u221a Ki + tn\n\u2265 A\u0302(2)n + 1 n\nn\n\u2211 i=1 Ci\u221a Ki + 2tn\n\u2265 A\nfor n large enough.\nUsing these estimates, we have constructed estimates \u03c8\u0302n such that for all n large enough it holds that\n\u03c8\u0302n +Cn + tn1\u2265 \u03c8\u2217\nfor appropriate constants Cn almost surely. Therefore, by assumption for all n large enough it holds that\nb(d0,K,\u03c8\u2217)\u2264 b(d0,K, \u03c8\u0302n + tn)\n3.5.3 Effect on \u03c1 Estimation\nOur analysis of estimating \u03c1 assumes that we know the parameters of the function and in particular the strong convexity parameter m. We now argue that the effect of using estimated parameters instead is minimal. This happens because we know that for all n large enough it holds that\n\u03c8\u0302n \u2265 \u03c8\u2217\nalmost surely.\nLemma 13. We want to estimate a non-negative parameter \u03c6\u2217 by producing a sequence of estimates \u03c6i for all i \u2265 1 and averaging to produce\n\u03c6\u0302n = 1 n\nn\n\u2211 i=1 \u03c6i\nwhere the estimates \u03c6i are dependent on an auxiliary sequence \u03c8i in the sense that \u03c6i(\u03c8i). Suppose that the following conditions hold:\n1. Suppose that there exists a random variable N\u0303 such that n \u2265 N\u0303 implies that \u03c8\u0302n \u2265 \u03c8\u2217\n2. E[\u03c6i(\u03c8\u2217)]\u2265 \u03c6\u2217\nThen it follows that\nliminf n\u2192\u221e E\n[\n1 n\nn\n\u2211 i=1 \u03c6i\n]\n\u2265 \u03c6\u2217\nProof. It holds that\n1 n\nn\n\u2211 i=1 \u03c6i = 1 n N\u0303\u22121 \u2211 i=1 \u03c6i(\u03c8i)+ 1 n n \u2211 i=N\u0303 \u03c6i(\u03c8i)\n\u2265 1 n N\u0303\u22121 \u2211 i=1 \u03c6i(\u03c8i)+ 1 n n \u2211 i=N\u0303 \u03c6i(\u03c8\u2217i ) (18)\nTherefore, it follows that\nliminf n\u2192\u221e E\n[\n1 n\nn\n\u2211 i=1 \u03c6i\n]\n\u2265 liminf n\u2192\u221e E\n[\n1 n\nn\n\u2211 i=N\u0303 \u03c6i(\u03c8\u2217i )\n]\n\u2265 \u03c6\u2217\nWe can extend all the concentration inequalities for estimating \u03c1 as well by extending the inequality in (18) to yield\n1 n\nn\n\u2211 i=1 \u03c6i = 1 n N\u0303\u22121 \u2211 i=1 \u03c6i(\u03c8i)+ 1 n n \u2211 i=N\u0303 \u03c6i(\u03c8i)\n\u2265 1 n N\u0303\u22121 \u2211 i=1 \u03c6i(\u03c8i)+ 1 n n \u2211 i=N\u0303 \u03c6i(\u03c8\u2217i )\n\u2265 1 n N\u0303\u22121 \u2211 i=1 (\u03c6i(\u03c8i)\u2212\u03c6i(\u03c8\u2217))+ 1 n n \u2211 i=1 \u03c6i(\u03c8\u2217i )\n= 1 n\nn\n\u2211 i=1 \u03c6i(\u03c8\u2217i )+ o(1)\nBefore, we have analyzed 1 n n \u2211 i=1 \u03c6i(\u03c8\u2217i )\nso for large enough n, we recover previous results, since the o(1) term goes to 0.\n4 Adaptive Sequential Optimization With \u03c1 Unknown We now examine the case with \u03c1 unknown. We extend the work of Section 2 using the estimates of \u03c1 in Section 3. Our analysis depends on the following crucial assumption:\nC.1 For appropriate sequences {tn}, for all n sufficiently large it holds that \u03c1\u0302n + tn \u2265 \u03c1 almost surely.\nC.2 b(d0,Kn) factors as b(d0,Kn) = \u03b1(Kn)d0 +\u03b2 (Kn)\nWe have demonstrated that assumption C.1 that holds for the direct and IPM estimates of \u03c1 under (2) and (3). Note that whether we assume (2) or (3) does not matter for analysis.\n4.1 General Condition on Kn We start with a general result showing that for any choice of Kn such that Kn \u2265 K\u2217 for all n large enough the excess risk is controlled in the sense that\nlimsup n\u2192\u221e\n(E[ fn(xn)]\u2212 fn(x\u2217n))\u2264 \u03b5\nWe then apply this result to two different selection rules for Kn. Consider the function\n\u03c6K(v) = \u03b1(K)\n(\n\u221a\n2 m v+\u03c1\n)2\n+\u03b2 (K)\nderived from assumption C.2. Note that as a function of v, \u03c6K(v) is clearly increasing and strictly concave. First, suppose that we select K\u2217 defined in (5). Then by definition it holds that\n\u03c6K\u2217(\u03b5)\u2264 \u03b5\nWe study fixed points of the function \u03c6K\u2217(v):\nLemma 14. The function \u03c6K\u2217(v) has a unique positive fixed point v\u0304 with\n1. v\u0304 = \u03c6K\u2217(v\u0304)\u2264 \u03b5\n2. \u03c6 \u2032K\u2217(v\u0304)< 1\nProof. We have \u03c6K\u2217(0) = \u03b1(K\u2217)\u03c12 +\u03b2 (K\u2217)> 0\nSince lim v\u21920 \u03c6K\u2217(v) = \u03c6K\u2217(0)\nand \u03c6K\u2217(0)> 0, there exists a positive a sufficiently small that\n\u03c6K\u2217(a)> a\nNext, expanding \u03c6K(v) yields\n\u03c6K(v) = 2 m\n\u03b1(K)v+ 2\u03b1(K)\u03c1 \u221a\n2 m \u221a v+\u03b1(K)\u03c12 +\u03b2 (K)\nSince \u03c6K\u2217(\u03b5)\u2264 \u03b5 , we obviously must have 2m \u03b1(K\u2217)\u2264 1. Suppose that\n2 m \u03b1(K\u2217) = 1\nThen it holds that \u03c6K\u2217(\u03b5) = \u03b5 + \u221a 2m\u03c1 \u221a \u03b5 +\nm 2 \u03c12 +\u03b2 (K)> \u03b5\nThis is a contradiction, so it holds that 2 m \u03b1(K\u2217)< 1\nIt is thus readily apparent that v\u2212\u03c6K\u2217(v)\u2192 \u221e\nas v \u2192 \u221e. Therefore, there exists a point b > a such that\n\u03c6K\u2217(b)< b\nIt is easy to check that \u03c6K\u2217(v) is increasing and strictly concave. Therefore, we can apply Theorem 3.3 from [21] to conclude that there exists a unique, positive fixed point v\u0304 of \u03c6K\u2217(v).\nNext, suppose that \u03c6 \u2032K\u2217(v\u0304)> 1. Then by Taylor\u2019s Theorem for v > v\u0304 sufficiently close to v\u0304, we have\n\u03c6K\u2217(v)> v\nHowever, we know that as v \u2192 \u221e, it holds that v\u2212\u03c6K\u2217(v)\u2192 \u221e. By the Intermediate Value Theorem, this implies that there is another fixed point on [v,\u221e). This is a contradiction, since v\u0304 is the unique, positive fixed point. Therefore, it holds that \u03c6 \u2032K\u2217(v\u0304)\u2264 1. Now, suppose that \u03c6 \u2032K\u2217(v\u0304) = 1. Since \u03c6K\u2217(v) is strictly concave, its derivative is decreasing [22]. Therefore, on [0, v\u0304), it holds that\n\u03c6 \u2032K\u2217(v)> 1\nThis implies that\n\u03c6K\u2217(v\u0304) = \u03c6K\u2217(0)+ \u222b v\u0304\n0 \u03c6 \u2032K\u2217(v)dx\n\u2265 \u03c6K\u2217(0)+ v\u0304 > v\u0304\nThis is a contradiction, so it must be that \u03c6 \u2032K\u2217(v\u0304)< 1.\nAs a simple consequence of the concavity of \u03c6K\u2217(v), we can study a fixed point iteration involving \u03c6K(v). Define the n-fold composition mapping\n\u03c6 (n)K (v), (\u03c6K \u25e6 \u00b7 \u00b7 \u00b7 \u25e6\u03c6K)(v)\nLemma 15. For any v > 0, it holds that lim n\u2192\u221e \u03c6 (n)K\u2217 (v) = v\u0304\nProof. Following [23], for any fixed point v\u0304, it holds that\n|\u03c6K\u2217(v)\u2212 v\u0304| \u2264 \u03c6 \u2032K\u2217(v\u0304)|v\u2212 v\u0304|\nTherefore, applying the fixed point property repeatedly yields\n|\u03c6 (n)K\u2217 (v)\u2212 v\u0304| \u2264 (\u03c6 \u2032K\u2217(v\u0304))n|v\u2212 v\u0304|\nBy Lemma 14, it holds that \u03c6 \u2032K\u2217(v\u0304)< 1\nand so the result follows.\nNow, we show that we appropriately control the excess risk when we estimate \u03c1 . The extension of this argument to the case when we also estimate function parameters \u03c8 is straightforward. If we have\np({zn(k)}Knk=1 | xn\u22121,Kn) = Kn\n\u220f k=1 pn(zn(k))\nthen\nE [ fn(xn) | xn\u22121,Kn]\u2212 fn(x\u2217n)\u2264 b\n\n\n(\n\u221a\n2 m ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) +\u03c1\n)2\n,Kn\n\n\nTherefore, it holds that\nE [ fn(xn)]\u2212 fn(x\u2217n)\u2264 E\n\nb\n\n\n(\n\u221a\n2 m ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) +\u03c1\n)2\n,Kn\n\n\n\n\nSuppose that we set K\u221e = \u03c3 ({Kn}\u221en=1 \u222a{\u03c1\u0302n}\u221en=2)\nThis sigma algebra contains all the information about {\u03c1\u0302n} and thus {Kn}. Then, we do not have\np({zn(k)}Knk=1 | K\u221e) = Kn\n\u220f k=1 pn(zn(k))\nsince Kn+1,Kn+2, . . . are a function of {Kn}Knk=1. We do not even have\nE [ fn(xn) | K\u221e]\u2212 fn(x\u2217n)\u2264 b\n\n\n(\n\u221a\n2 m ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) +\u03c1\n)2\n,Kn\n\n\nHowever, we would expect that this is not too far from true. Conceptually, we consider running our approach twice on independent samples. The first run determines the required number of samples {Kn}\u221en=1. We then run our process for a second run with these fixed choices of {Kn}\u221en=1and independent samples as in Figure 1. For the second run, it is true that\np({z(2)n (k)}Knk=1 | K\u221e) = Kn\n\u220f k=1\npn(z (2) n (k))\nand\nE\n[\nfn(x (2) n ) | K\u221e\n]\n\u2212 fn(x\u2217n)\u2264 b\n\n\n(\n\u221a\n2 m ( fn\u22121(x (2) n\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) +\u03c1\n)2\n,Kn\n\n\nIn practice, we do not need to run our process twice. This is only a proof technique. Now, for the second run the recursion\n\u03b5(2)n = b\n\n\n(\n\u221a\n2 m \u03b5(2)n\u22121 +\u03c1\n)2\n,Kn\n\n \u2200n \u2265 3 (19)\nwith \u03b51 and \u03b52 from Assumption A.4 bounds the excess risk of the second run\nE[ fn(x (2) n ) | K\u221e]\u2212 fn(x\u2217n)\u2264 \u03b5 (2) n\nThen it follows that E[ fn(x (2) n )]\u2212 fn(x\u2217n)\u2264 E[\u03b5 (2) n ]\nWe now argue that E[\u03b5(2)n ] also bounds the excess risk of the first run.\nLemma 16. For the first run, it holds that\nE[ fn(xn)]\u2212 fn(x\u2217n)\u2264 E[\u03b5 (2) n ]\nProof. We proceed by induction. For n = 1,2, we know that\nE[ fn(xn)]\u2212 fn(x\u2217n)\u2264 E[\u03b5 (2) n ]\nby definition. Next, suppose that\nE[ fn\u22121(xn\u22121)]\u2212 fn\u22121(x\u2217n\u22121)\u2264 E[\u03b5 (2) n\u22121]\nWe have\nE[ fn(xn)]\u2212 fn(x\u2217n)\u2264 E [ \u03b1(Kn) (\u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)+\u03c1 )2 +\u03b2 (Kn) ]\nso it holds that\nE[\u03b5(2)n ]\u2212 (E[ fn(xn)]\u2212 fn(x\u2217n))\n\u2265 E [ \u03b1(Kn) ( \u221a \u03b5(2)n\u22121 +\u03c1 )2 \u2212\u03b1(Kn) (\u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)+\u03c1 )2 ]\n= E [ \u03b1(Kn) ( \u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) )]\n+E\n[ 2\u03c1\u03b1(Kn) ( \u221a \u03b5(2)n\u22121 \u2212 \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )]\nBy the Monotone Convergence Theorem, it holds that\nE\n[ \u03b1(Kn) ( \u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) )]\n= lim q\u2192\u221e E\n[ max{\u03b1(Kn),1/q} ( \u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) )]\n\u2265 liminf q\u2192\u221e 1 q E [ \u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) ] \u2265 0\nwhere the last line follows, since by hypothesis\nE[ fn\u22121(xn\u22121)]\u2212 fn\u22121(x\u2217n\u22121)\u2264 E[\u03b5 (2) n\u22121]\nSimilarly, it holds that\nE\n[ 2\u03c1\u03b1(Kn) ( \u221a \u03b5(2)n\u22121 \u2212 \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )]\n= E\n\n  2\u03c1\u03b1(Kn)\n\u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )\n\u221a\n\u03b5(2)n\u22121 + \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)\n\n \n= lim q\u2192\u221e E\n\n  2\u03c1 max{\u03b1(Kn),1/q}\n\u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )\n\u221a\n\u03b5(2)n\u22121 + \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)\n\n \n\u2265 limsup q\u2192\u221e 2\u03c1 q E\n\n \n\u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )\n\u221a\n\u03b5(2)n\u22121 + \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)\n\n \n\u2265 limsup q\u2192\u221e 2\u03c1 q lim \u03c4\u2192\u221e E\n\n \n\u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) )\n\u221a\n\u03b5(2)n\u22121 + \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) 1\n{ \u221a \u03b5(2)n\u22121+ \u221a fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121)\u2264\u03c4}\n\n \n\u2265 limsup q\u2192\u221e 2\u03c1 q limsup \u03c4\u2192\u221e 1 \u03c4 E [ \u03b5(2)n\u22121 \u2212 ( fn\u22121(xn\u22121)\u2212 fn\u22121(x\u2217n\u22121) ) ]\n\u2265 0\nTherefore, we conclude that E[ fn(xn)]\u2212 fn(x\u2217n)\u2264 E[\u03b5 (2) n ]\nTheorem 2. Under assumptions C.1 - C.2 and with Kn \u2265 K\u2217 for all n large enough almost surely with K\u2217 from (20), we have\nlimsupn\u2192\u221e (E[ fn(xn)]\u2212 fn(x\u2217n))\u2264 \u03b5\nProof. Let v\u0304 be the fixed point associated with \u03c6K\u2217(v) from Lemma 14. We know that\nv\u0304 = \u03c6K\u2217(v\u0304)\u2264 \u03b5\nand \u03c6 (n)K\u2217 (v)\u2192 v\u0304 \u2264 \u03b5 with v\u0304 \u2264 \u03b5 . Since we have Kn \u2265 K\u2217 for all n large enough almost surely, there exists a random variable N\u0303 such that\nn \u2265 N\u0303 \u21d2 Kn \u2265 K\u2217\nThen we have almost surely\nlimsup n\u2192\u221e \u03b5(2)n \u2264 limsup n\u2192\u221e (\u03c6Kn \u25e6 \u00b7 \u00b7 \u00b7 \u25e6\u03c6KN\u0303 )(\u03b5N\u0303\u22121)\n\u2264 limsup n\u2192\u221e\n\u03c6 (n\u2212N\u0303+1)K\u2217 (\u03b5N\u0303\u22121)\n= v\u0304\n\u2264 \u03b5\nFinally, applying Lemma 19 and Fatou\u2019s lemma yields\nlimsup n\u2192\u221e (E[ fn(xn)]\u2212 fn(x\u2217n)) \u2264 limsup n\u2192\u221e E\n[ \u03b5(2)n ]\n\u2264 E [\nlimsup n\u2192\u221e\n\u03b5(2)n ]\n\u2264 \u03b5\n4.2 Update Past Excess Risk Bounds\nWe first consider updating all past excess risk bounds as we go. At time n, we plug-in \u03c1\u0302n\u22121 + tn\u22121 in place of \u03c1 and follow the analysis of Section 2. Define for i = 1, . . . ,n\n\u03b5\u0302(n)i = b\n\n\n(\n\u221a\n2 m \u03b5\u0302(n)i\u22121 +(\u03c1\u0302n\u22121+ tn\u22121)\n)2\n,Ki\n\n\nIf it holds that \u03c1\u0302n\u22121 + tn\u22121 \u2265 \u03c1 , then E [ fn(xn)]\u2212 fn(x\u2217n)\u2264 \u03b5\u0302 (i) n for i = 1, . . . ,n. Assumption C.1 guarantees that this holds for all n large enough almost surely. We can thus set Kn equal to the smallest K such that\nb\n\n\n(\n\u221a\n2 m max{\u03b5\u0302(n\u22121)n\u22121 ,\u03b5}+(\u03c1\u0302n\u22121 + tn\u22121) )2 ,K\n\n\u2264 \u03b5\nfor all n \u2265 3 to achieve excess risk \u03b5 . The maximum in this definition ensures that when \u03c1\u0302n\u22121 + tn\u22121 \u2265 \u03c1 , Kn \u2265 K\u2217 with K\u2217 from (5). We can therefore apply Theorem 2.\n4.3 Do Not Update Past Excess Risk Bounds\nUpdating all past estimates of the excess risk bounds from time 1 up to n imposes a computational and memory burden. Suppose that for all n \u2265 3 we set\nKn = min\n\n\n\nK \u2265 1 \u2223 \u2223 \u2223\n\u2223 \u2223\nb\n\n\n(\n\u221a\n2\u03b5 m +(\u03c1\u0302n\u22121+ tn\u22121)\n)2\n,K\n\n\u2264 \u03b5\n\n\n\n(20)\nThis is the same form as the choice in (5) with \u03c1\u0302n\u22121+ tn\u22121 in place of \u03c1 . Due to assumption C.1 , for all n large enough it holds that \u03c1\u0302n + tn \u2265 \u03c1 almost surely. Then by the monotonicity assumption in A.1 , for all n large enough we pick Kn \u2265 K\u2217 almost surely. We can therefore apply Theorem 2.\n5 Experiments\nWe focus on two regression applications for synthetic and real data as well as two classification applications for synthetic and real data. For the synthetic regression problem, we can explicitly compute \u03c1 and x\u2217n and exactly evaluate the performance of our method. It is straightforward to check that all requirements in A.1 -A.4 are satisfied for the problems considered in this section. We apply the do not update past excess risk choice of Kn here.\n5.1 Synthetic Regression\nConsider a regression problem with synthetic data using the penalized quadratic loss\n\u2113(x,z) = 1 2 ( y\u2212w\u22a4x )2 + 1 2 \u03bb\u2016x\u20162\nwith z = (w,y) \u2208Rd+1. The distribution of zn is zero mean Gaussian with covariance matrix [\n\u03c32 w I rwn,yn\nr\u22a4 wn,yn \u03c3 2 yn\n]\nUnder these assumptions, we can analytically compute minimizers x\u2217n of fn(x) = Ezn\u223cpn [\u2113(x,zn)]. We change only rwn,yn and \u03c32yn appropriately to ensure that \u2016x\u2217n \u2212x\u2217n\u22121\u2016 = \u03c1 holds for all n. We find approximate minimizers using SGD with \u03bb = 0.1. We estimate \u03c1 using the direct estimate.\nWe let n range from 1 to 20 with \u03c1 = 1, a target excess risk \u03b5 = 0.1, and Kn from (20). We average over twenty runs of our algorithm. Figure 2 shows \u03c1\u0302n, our estimate of \u03c1 , which is above \u03c1 in general. Figure 3 shows the number of samples Kn, which settles down. We can exactly compute fn(xn)\u2212 fn(x\u2217n), and so by averaging over the twenty runs of our algorithm, we can estimate the excess risk (denoted \u201csample average estimate\u201d). Figure 4 shows this estimate of the excess risk, the target excess risk, and our bound on the excess risk from Section 4.3. We achieve at least our targeted excess risk\nFigure 2: \u03c1 Estimate n 2 4 6 8 10 12 14 16 18 20\nn 2 4 6 8 10 12 14 16 18 20\nE xc\nes s\nR is\nk\n0\n0.05\n0.1\n0.15\n0.2\n0.25 Direct Estimate Sample Average Estimate\nFigure 4: Excess Risk\n5.2 Panel Study on Income Dynamics Income - Regression\nThe Panel Study of Income Dynamics (PSID) surveyed individuals every year to gather demographic and income data annually from 1981-1997 [24]. We want to predict an individual\u2019s annual income (y) from several demographic features (w) including age, education, work experience, etc. chosen based on previous economic studies in [25]. The\nidea of this problem conceptually is to rerun the survey process and determine how many samples we would need if we wanted to solve this regression problem to within a desired excess risk criterion \u03b5 .\nWe use the same loss function, direct estimate for \u03c1 , and minimization algorithm as the synthetic regression problem. The income is adjusted for inflation to 1997 dollars with mean $20,294. We average over twenty runs of our algorithm by resampling without replacement [26]. We compare to taking an equivalent number of samples up front. Figure 5 shows the test losses over time evaluated over twenty percent of the available samples. The test loss for our approach is substantially less than taking the same number of samples up front. The square root of the average test loss over this time period for our approach and all samples up front are $1153\u00b1 352 and $2805\u00b1 424 respectively in 1997 dollars.\n5.3 Synthetic Classification\nConsider a binary classification problem using \u2113(x,z) = 12 (1\u2212 y(w\u22a4x))2++ 12 \u03bb\u2016x\u20162 with z = (w,y) \u2208 Rd \u00d7R and (y)+ = max{y,0}. This is a smoothed version of the hinge loss used in support vector machines (SVM) [26]. We suppose that at time n, the two classes have features drawn from a Gaussian distribution with covariance matrix \u03c32I but different means \u00b5 (1)n and \u00b5 (2) n , i.e., wn | {yn = i} \u223c N (\u00b5 (i)n ,\u03c32I). The class means move slowly over uniformly spaced points on a unit sphere in Rd as in Figure 6 to ensure that (2) holds. We find approximate minimizers using SGD with \u03bb = 0.1. We estimate \u03c1 using the direct estimate with tn \u221d 1/n3/8.\nWe let n range from 1 to 20 and target a excess risk \u03b5 = 0.1. We average over twenty runs of our algorithm. As a comparison, if our algorithm takes {Kn}20n=1 samples, then we consider taking \u221120n=1 Kn samples up front at n = 1. This is what we would do if we assumed that our problem is not time varying. Figure 7 shows \u03c1\u0302n, our estimate of \u03c1 . Figure 8 shows the average test loss for both sampling strategies. To compute test loss we draw Tn additional samples\n{ztestn (k)}Tnk=1 from pn and compute 1Tn \u2211 Tn k=1 \u2113(xn,z test n (k)). We see that our approach achieves substantially smaller test loss than taking all samples up front.\nn 2 4 6 8 10 12 14 16 18 20\n\u03c1\n1.8\n1.9\n2\n2.1\n2.2 Direct Estimate\nFigure 7: \u03c1 Estimate n 2 4 6 8 10 12 14 16 18 20\nT es\nt L os\ns\n0.2\n0.4\n0.6\n0.8\n1 All Samples Up Front Direct Estimate\nFigure 8: Test Loss\n5.4 General Social Survey - Classification\nThe General Social Survey (GSS) surveyed individuals every year to gather socio-economic data annually from 1981- 2013 [27]. We want to predict an individual\u2019s marital status (y) from several demographic features (w) including age, education, etc. We model this as a binary classification problem using loss\n\u2113(x,z) = 1 2 (1\u2212 y(w\u22a4x))2++ 1 2 \u03bb\u2016x\u20162\nwith z = (w,y) \u2208 Rd \u00d7R and (y)+ = max{y,0}. This is a smoothed version of the hinge loss used in support vector machines [26]. We find approximate minimizers using SGD with \u03bb = 0.1. Figure 9 shows the test loss. We see that our approach achieves smaller test loss than taking all samples up front. We also plot receiver operating characteristics (ROC) [26] to characterize the performance of our classifiers. In particular we plot the ROC for 1974 in Figure 10 and the ROC for 2012 in Figure 11. By examining the ROC, we see that taking all samples up front is much better in 1974 but much worse in 2012.\n6 Conclusion\nWe introduced a framework for adaptively solving a sequence of optimization problems with applications to machine learning. We developed estimates of the change in the minimizers used to determine the number of samples Kn needed to achieve a target excess risk \u03b5 . Experiments with synthetic and real data demonstrate that this approach is effective.\nReferences\n[1] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of Machine Learning, The MIT Press, 2012.\n[2] A. Agarwal, H. Daum\u00e9, and S. Gerber, \u201cLearning multiple tasks using manifold regularization.,\u201d in NIPS, 2011, pp. 46\u201354.\n[3] T. Evgeniou and M. Pontil, \u201cRegularized multi\u2013task learning,\u201d in Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA, 2004, KDD \u201904, pp. 109\u2013117, ACM.\n[4] Y. Zhang and D. Yeung, \u201cA convex formulation for learning task relationships in multi-task learning,\u201d CoRR, vol. abs/1203.3536, 2012.\n[5] S. Pan and Q. Yang, \u201cA survey on transfer learning,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, Oct 2010.\nFalse Positive 0 0.2 0.4 0.6 0.8 1\nT ru\ne P\nos iti\nve\n0\n0.2\n0.4\n0.6\n0.8\n1\nDirect Estimate All Samples Up Front\nFigure 10: ROC for 1974 False Positive 0 0.2 0.4 0.6 0.8 1\nT ru\ne P\nos iti\nve\n0\n0.2\n0.4\n0.6\n0.8\n1\nAll Samples Up Front Direct Estimate\nFigure 11: ROC for 2012\n[6] A. Agarwal, A. Rakhlin, and P. Bartlett, \u201cMatrix regularization techniques for online multitask learning,\u201d Tech. Rep. UCB/EECS-2008-138, EECS Department, University of California, Berkeley, Oct 2008.\n[7] Z. Towfic, J. Chu, and A. Sayed, \u201cOnline distirubted online classifcation in the midst of concept drifts,\u201d Neurocomputing, vol. 112, pp. 138\u2013152, 2013.\n[8] C. Tekin, L. Canzian, and M. van der Schaar, \u201cContext adaptive big data stream mining,\u201d in Allerton Conference, 2014, pp. 46\u201354.\n[9] T. Dietterich, \u201cMachine learning for sequential data: A review,\u201d in Structural, Syntactic, and Statistical Pattern Recognition, 2002, pp. 15\u201330.\n[10] T. Fawcett and F. Provost, \u201cAdaptive fraud detection.,\u201d Data Min. Knowl. Discov., vol. 1, no. 3, pp. 291\u2013316, 1997.\n[11] N. Qian and T. Sejnowski, \u201cPredicting the secondary structure of globular proteins using neural network models,\u201d Journal of Molecular Biology, vol. 202, pp. 865\u2013884, Aug 1988.\n[12] Y. Bengio and P. Frasconi, \u201cInput-output HMM\u2019s for sequence processing,\u201d IEEE Transactions on Neural Networks, vol. 7(5), pp. 1231\u20131249, 1996.\n[13] A. Dontchev and R. Rockafellar, Implicit Functions and Solution Mappings: A View from Variational Analysis, Springer, New York, New York, 2009.\n[14] B. Sriperumbudur, \u201cOn the empirical estimation of integral probability metrics,\u201d Electronic Journal of Statistics, pp. 1550\u20131599, 2012.\n[15] R. Veryshin, \u201cIntroduction to non-asymptotic analysis of random matrices,\u201d Tech. Rep., University of Michigan, 2012.\n[16] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, \u201cStochastic approximation approach to stochastic programming,\u201d SIAM Journal on Optimization, vol. 19, pp. 1574\u20131609, 2009.\n[17] V.V Buldygin and E.D. Pechuk, \u201cInequalities for the distributions of functionals of sub-gaussian vectors,\u201d Theor. Probability and Math. Statist., pp. 25\u201336, 2010.\n[18] S. Janson, \u201cLarge deviations for sums of partly dependent random variables,\u201d Random Structures Algorithms, vol. 24, pp. 234\u2013248, 2004.\n[19] S. Boucheron, G. Lugosi, and P. Massart, Concentration Inequalities: A Nonasymptotic Theory of Independence, Oxford University Press, 2013.\n[20] L. Trefethen, Numerical Linear Algebra, SIAM, 1997.\n[21] J. Kennan, \u201cUniqueness of positive fixed points for increasing concave functions on rn: An elementary result,\u201d Review of Economic Dynamics, vol. 4, pp. 893\u00e2A\u0306S\u0327899, 2001.\n[22] Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Cambridge University Press, New York, NY, USA, 2004.\n[23] A. Granas and J. Dugundji, Fixed Point Theory, Springer-Verlag, 2003.\n[24] \u201cPanel study of income dynamics: public use dataset,\u201d Survey Research Center, 2015.\n[25] S. Jenkins and P. Van Kerm, \u201cTrends in income inequality, pro-poor income growth, and income mobility,\u201d Oxford Economic Papers, vol. 58, no. 3, pp. 531\u2013548, 2006.\n[26] T. Hastie, R. Tibshirani, and J.H. Friedman, The elements of statistical learning: data mining, inference, and prediction: with 200 full-color illustrations, New York: Springer-Verlag, 2001.\n[27] \u201cGeneral social survey,\u201d National Opinion Research Center, 2015.\n[28] F. Bach and E. Moulines, \u201cNon-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,\u201d in Advances in Neural Information Processing Systems (NIPS), Spain, 2011.\n[29] D. Bertsekas, Nonlinear Programming, Athena Scientific, 1999.\n[30] L\u00e9on Bottou, \u201cOnline learning and stochastic approximations,\u201d 1998.\n[31] A. Nedic and S. Lee, \u201cAnalysis of mirror descent for strongly convex functions,\u201d ArXiV, 2013.\n[32] Yu. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course, Kluwer Academic Publishers, Norwell, Massachusetts, USA, 2004.\n[33] R. Antonini and Y. Kozachenko, \u201cA note on the asymptotic behavior of sequences of generalized subgaussian random vectors,\u201d Random Op. and Stoch. Equ., vol. 13, pp. 39\u201352, 2005.\nA Examples of b(d0,K):\nFor this section, we drop the n index for convenience. The bounds of this form depend on the strong convexity parameter m and an assumption on how the gradients grow. In general, we assume that\nEz\u223cp\u2016\u2207x\u2113(x,z)\u20162 \u2264 A+B\u2016x\u2212x\u2217\u20162\nThe base algorithm we look at is SGD. First, we generate iterates x(0), . . . ,x(K) through SGD as follows:\nx(\u2113+ 1) = \u03a0X [x(\u2113)\u2212 \u00b5(\u2113+ 1)\u2207x\u2113(x(\u2113),z(\u2113))] \u2113= 0, . . . ,K \u2212 1\nwith x(0) fixed. We then combine the iterates to yield a final approximate minimizer\nx\u0304(K) = \u03c6(x(0), . . . ,x(K))\nFor our choice of \u03c6 , we look at two cases:\n1. No iterate averaging, i.e., \u03c6(x(0), . . . ,x(K)) = x(K)\n2. Iterate averaging, i.e, for a convex combination {\u03bb (\u2113)}K\u2113=0\n\u03c6(x(0), . . . ,x(K)) = K\n\u2211 \u2113=0 \u03bb (\u2113)x(\u2113)\nDefine d(\u2113), \u2016x(\u2113)\u2212x\u2217\u20162 (21)\nFirst we bound E[d(\u2113)] in Lemma 17.\nLemma 17. Suppose that the function f (x) has Lipschitz continuous gradients. Then it holds that\nE[d(\u2113)]\u2264 \u2113\n\u220f k=1\n(1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))+ \u2113\n\u2211 k=1\n\u2113\n\u220f i=k+1 (1\u2212 2m\u00b5(i)+B\u00b52(i))\u00b52(k)\nProof. Following the standard SGD analysis (see [16]), it holds that\nd(\u2113) \u2264 \u2016x(\u2113\u2212 1)\u2212x\u2217\u2212 \u00b5(\u2113)\u2207x\u2113(x(\u2113\u2212 1),z(\u2113))\u20162\n\u2264 d(\u2113\u2212 1)\u2212 2\u00b5(\u2113)\u3008x(\u2113\u2212 1)\u2212x\u2217,\u2207x\u2113(x(\u2113\u2212 1),z(\u2113))\u3009+ \u00b52(\u2113)\u2016\u2207x\u2113(x(\u2113\u2212 1),z(\u2113))\u20162\nThen it follows that\nE[d(\u2113) | x(\u2113\u2212 1)] \u2264 d(\u2113\u2212 1)\u2212 2\u00b5(\u2113)\u3008x(\u2113\u2212 1)\u2212x\u2217,\u2207 f (x(\u2113\u2212 1))\u3009+ \u00b52(\u2113)E[\u2016\u2207x\u2113(x(\u2113\u2212 1),z(\u2113))\u20162 | x(\u2113\u2212 1)] \u2264 (1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))d(\u2113\u2212 1)+ \u00b52(\u2113\u2212 1)A\nand E[d(\u2113)]\u2264 (1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))E[d(\u2113\u2212 1)]+ \u00b52(\u2113\u2212 1)A\nSince B > m, we have\n2m\u00b5 \u2212B\u00b52 \u2264 2 \u221a\nB 2 \u00b5\n(\n1\u2212 \u221a\nB 2 \u00b5\n)\n\u2264 2 1 4 = 1 2\nand so\n1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113)\u2265 1\u2212 1 2 = 1 2\nSince this quantity is non-negative, we can unwind this recursion to yield\nE[d(\u2113)]\u2264 \u2113\n\u220f k=1\n(1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))+ \u2113\n\u2211 k=1\n\u2113\n\u220f i=k+1 (1\u2212 2m\u00b5(i)+B\u00b52(i))\u00b52(k)\nThe bound in Lemma 17 can be further bounded into a closed form as follows from [28]: Define\n\u03d5\u03b2 (t) =\n{\nt\u03b2\u22121 \u03b2 , if \u03b2 6= 0 log(t), if \u03b2 = 0\nThen with \u00b5(\u2113) =C\u2113\u2212\u03b1 , it holds that\nE[d(\u2113)]\u2264 { 2exp { 2BC2\u03d51\u22122\u03b1(\u2113) } exp { \u2212mC4 \u21131\u2212\u03b1 }( E[d(0)]+ AB ) + 2ACm\u2113\u03b1 , if 0 \u2264 \u03b1 < 1 exp{BC2}\n\u2113mC\n( E[d(0)]+ AB )\n+AC2 \u03d5mC/2\u22121(\u2113)\n\u2113mC/2 , if \u03b1 = 1\nNote that this bound is a closed form but is substantially looser than Lemma 17. In the case that the functions in question have Lipschitz continuous gradients, we introduce a bound on the excess risk using Lemma 17. This case corresponds to choosing\n\u03c6(x(0), . . . ,x(K)) = x(K)\nLemma 18. With arbitrary step sizes and assuming that f (x) has Lipschitz continuous gradients with modulus M, it holds that\nE[ f (x)]\u2212 f (x\u2217)\u2264 1 2 ME[d(K)]\nand therefore, we set\nb(d0,K) = 1 2 M\n(\nK\n\u220f \u2113=1\n(1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))+ K\n\u2211 \u2113=1\nK\n\u220f i=\u2113+1\n(1\u2212 2m\u00b5(i)+B\u00b52(i))\u00b52(\u2113) )\nProof. Using the descent lemma from [29], it holds that\nE[ f (x)]\u2212 f (x\u2217)\u2264 1 2 ME[d(K)]\nPlugging in the bound from Lemma 17 yields the bound b(d0,K).\nNext, we introduce a bound inspired by [30] for the case where \u03c6(x(0), . . . ,x(K)) corresponds to forming a convex combination of the iterates.\nLemma 19. With a constant step size and averaging with\n\u03bb (\u2113) = { \u03b3(\u2113) \u2211K\u03c4=1 \u03b3(\u03c4) , if \u2113 > 0\n0, if \u2113= 0\nwhere \u03b3(\u2113) = (1\u2212m\u00b5 +B\u00b52)\u2212\u2113\nit holds that\nb(d0,K) = d0\n2\u00b5 \u2211K\u2113=0 \u03b3(\u2113) +\n1 2 A\u00b5\nProof. By strong convexity, it holds that\n\u2212\u3008x(\u2113\u2212 1)\u2212x\u2217,\u2207 f (x(\u2113\u2212 1))\u3009 \u2264 \u2212m\u2016x(\u2113\u2212 1)\u2212x\u2217\u20162 \u2212 ( f (x(\u2113\u2212 1))\u2212 f (x\u2217))\nFollowing the Lyapunov-style analysis of Lemma 17, it holds that\nE[d(\u2113)]\u2264 (1\u2212m\u00b5 +B\u00b52)E[d(\u2113\u2212 1)]\u2212 2\u00b5 (E[ f (x(\u2113\u2212 1))]\u2212 f (x\u2217))+A\u00b52\nRearranging, using the telescoping sum, and using convexity, it holds that\nE[ f (x)]\u2212 f (x\u2217)\u2264 d0 2\u00b5 \u2211K\u03c4=0 \u03b3(\u03c4) + 1 2 A\u00b5\nIf we set \u00b5 = 1\u221a K , then it holds that\nb(d0,K) = O\n(\n1\u221a K\n)\nfor Lemma 19. We consider an extension of the averaging scheme in [31]. The bound in this paper only works with B = 0, so we extend it slightly to handle B > 0.\nLemma 20. Consider the choice of step sizes given by\n\u00b5(\u2113) = 1\nm\u2113 \u2200\u2113\u2265 1\nThen\nb(d0,K) = 1 2 d(0)+ 1 2 (K + 1)A+ 1 2 B\u2211 K \u2113=0 \u03b3(\u2113)\n1+ 12 m(K + 1)(K+ 2)\nwhere E[d(\u2113)]\u2264 \u03b3(\u2113)\nNote that we can use the bound in Lemma 17 here.\nProof. We have using Lyapunov style analysis\nE[d(\u2113)]\u2264 (1\u2212 2m\u00b5(\u2113)+B\u00b52(\u2113))E[d(\u2113\u2212 1)]\u2212 2\u00b5(\u2113)(E[ f (x(\u2113))]\u2212 f (x\u2217))+A\u00b52(\u2113)\nThen we have\n1 \u00b52(\u2113) E[d(\u2113)]\u2264 ( 1\u2212 2m\u00b5(\u2113) \u00b52(\u2113) +B ) E[d(\u2113\u2212 1)]\u2212 2 \u00b5(\u2113) (E[ f (x(\u2113))]\u2212 f (x\u2217)+A\nIt holds that\n1\u2212 2m\u00b5(\u2113) \u00b52(\u2113) \u2212 1 \u00b52(\u2113\u2212 1) = 1 \u00b52(\u2113) \u2212 2m 1 \u00b5(\u2113) \u2212 1 \u00b52(\u2113\u2212 1)\n= \u21132 C2 \u2212 2m\u2113 C \u2212 (\u2113\u2212 1) 2 C2 = 2(mC\u2212 1)L\u2212 1\nC2\nAs long as we have\nmC\u2212 1 \u2264 1 \u21d4 C \u2264 2 m\nthen we get\n1 \u00b52(\u2113) E[d(\u2113)]\u2212 1 \u00b52(\u2113\u2212 1)E[d(\u2113\u2212 1)]\u2264 BE[d(\u2113\u2212 1)]\u2212 2 \u00b5(\u2113) (E[ f (x(\u2113))]\u2212 f (x\u2217)+A\nSumming an rearranging yields\nK\n\u2211 \u2113=0\n1 \u00b5(\u2113) (E[ f (x(\u2113))]\u2212 f (x\u2217))\u2264 1 2 d(0)+ 1 2 (K + 1)A+ 1 2 B K\n\u2211 \u2113=0 E[d(\u2113)]\nwith \u00b5(0) = 1 by convention. With the weights\n\u03b3(\u2113) = 1 \u00b5(\u2113)\n\u2211\u2113j=0 1\u00b5( j)\nwe have\nE[ f (x\u0304(K))]\u2212 f (x\u2217)\u2264 1 2 d(0)+ 1 2(K + 1)A+ 1 2 B\u2211 K \u2113=0E[d(\u2113)]\n\u2211K\u03c4=0 1\u00b5(\u03c4) Then it holds that\nK\n\u2211 \u03c4=0\n= 1+ K\n\u2211 \u03c4=1 m\u03c4 = 1+ 1 2 m(K + 1)(K + 2)\nso\nE[ f (x\u0304(K))]\u2212 f (x\u2217)\u2264 1 2 d(0)+ 1 2(K + 1)A+ 1 2 B\u2211 K \u2113=0E[d(\u2113)]\n1+ 12 m(K + 1)(K+ 2)\nFor the choice of step sizes in Lemma 20 from Lemma 17, it holds that\nE[d(\u2113)] = O\n(\n1 \u2113\n)\nSince K\n\u2211 \u2113=1 1 \u2113 = O (logK)\nit holds that\nE[ f (x\u0304(K))]\u2212 f (x\u2217) = O ( d(0) K2 + log(K) K2 + 1 K )\nNote that a rate of O( 1K ) is minimax optimal for stochastic minimization of a strongly convex function [32]. Next, we look at a special case of averaging for functions such that\nE\u2016\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u0303,z)\u2212\u22072xx\u2113(x\u0303,z)(x\u2212 x\u0303)\u20162 = 0\nfrom [28]. For example, quadratics satisfy this condition.\nLemma 21. Assuming that\nE\u2016\u2207x\u2113(x,z)\u2212\u2207x\u2113(x\u0303,z)\u2212\u22072xx\u2113(x\u0303,z)(x\u2212 x\u0303)\u20162 = 0,\nwe select step sizes \u00b5(\u2113) =C\u2113\u2212\u03b1\nwith \u03b1 > 1/2, and\n\u03bb (\u2113) =\n{\n1 K , if \u2113 > 0 0, if \u2113= 0\nit holds that\n( E[d\u0304(K)] )1/2\n\u2264 1 m1/2 K\u22121 \u2211 k=1 \u2223 \u2223 \u2223 \u2223 1 \u00b5(k+ 1) \u2212 1 \u00b5(k) \u2223 \u2223 \u2223 \u2223 (E[d(k)])1/2 + 1 m1/2\u00b5(1) (E[d(0)])1/2 +\n1\nm1/2\u00b5(K) (E[d(K)])1/2\n+\n\u221a\nA mK +\n\u221a\n2B mK2\nK\n\u2211 k=1 E[d(k\u2212 1)]\nwith d\u0304(K) = \u2016x\u0304(K)\u2212x\u2217\u20162. If in addition f has Lipschitz continuous gradients with modulus M, then it holds that\nE[ f (x\u0304(K))]\u2212 f (x\u2217)\u2264 1 2 ME[d\u0304(K)]\nProof. Suppose that we set\nx\u0304(K) = 1 n\nK\n\u2211 k=1 x(k)\nThen it holds that\n\u22072 xx f (x\u2217)(x(k)\u2212x\u2217) = \u2207x\u2113(x(k\u2212 1),z(k\u2212 1))\u2212\u2207x\u2113(x\u2217,z(k\u2212 1)) + [\n\u22072 xx f (x\u2217)\u2212\u22072 xx \u2113(x\u2217,z(k\u2212 1)) ] (x(k\u2212 1)\u2212x\u2217)\nyielding\n\u22072 xx f (x\u2217)(x\u0304(k)\u2212x\u2217) = 1 K\nK\n\u2211 k=1\n\u2207x\u2113(x(k\u2212 1),z(k\u2212 1))\u2212 1 K\nK\n\u2211 k=1\n\u2207x\u2113(x\u2217,z(k\u2212 1))\n+ 1 K\nK\n\u2211 k=1\n[\n\u22072 xx f (x\u2217)\u2212\u22072 xx \u2113(x\u2217,z(k\u2212 1)) ] (x(k\u2212 1)\u2212x\u2217)\nFirst, we have\n1 K\nK\n\u2211 k=1\n\u2207x\u2113(x(k\u2212 1),z(k\u2212 1)) = 1 K\nK\n\u2211 k=1 \u2207x\u2113(x(\u2113\u2212 1),z(\u2113\u2212 1))\n= 1 K\nK\n\u2211 k=1\n1 \u00b5(k) (x(\u2113\u2212 1)\u2212x(\u2113))\n= 1 K\nK\n\u2211 k=1\n1 \u00b5(k) (x(\u2113\u2212 1)\u2212x\u2217)\u2212 1 K\nK\n\u2211 k=1\n1 \u00b5(k) (x(\u2113)\u2212x\u2217)\n= 1 K K\u22121 \u2211 k=1 ( 1 \u00b5(k+ 1) \u2212 1 \u00b5(k) ) (x(\u2113)\u2212x\u2217)+ 1 \u00b5(1) (x(0)\u2212x\u2217)\n\u2212 1 \u00b5(K) (x(K)\u2212x\u2217)\nSecond, we have\nE\n\u2225 \u2225 \u2225 \u2225 1 K K\n\u2211 k=1\n\u2207x\u2113(x\u2217,z(k\u2212 1)) \u2225 \u2225 \u2225\n\u2225\n2\n= 1\nK2\nK\n\u2211 k=1\nE\u2016\u2207x\u2113(x\u2217,z(k\u2212 1))\u20162\n\u2264 A n2\nThird, we have\nE\n\u2225 \u2225 \u2225 \u2225 1 K K \u2211 k=1 [ \u22072 xx f (x\u2217)\u2212\u22072 xx \u2113(x\u2217,z(k\u2212 1)) ] (x(k\u2212 1)\u2212x\u2217) \u2225 \u2225 \u2225 \u2225 2 \u2264 2B K2 K \u2211 k=1 E[d(k\u2212 1)]\nCombining these bounds with Minkowski\u2019s inequality yields\n( mE[d\u0304(K)] )1/2\n\u2264 ( E\u2016\u22072 xx f (x\u2217)(x\u0304(K)\u2212x\u2217)\u20162 )1/2 \u2264 K\u22121 \u2211 k=1 \u2223 \u2223 \u2223 \u2223 1 \u00b5(k+ 1) \u2212 1 \u00b5(k) \u2223 \u2223 \u2223 \u2223 (E[d(k)])1/2 + 1 \u00b5(1) (E[d(0)])1/2 + 1 \u00b5(K) (E[d(K)])1/2\n+\n\u221a\nA K +\n\u221a\n2B K2\nK\n\u2211 k=1 E[d(k\u2212 1)]\nThen we have (\nE[d\u0304(K)] )1/2\n\u2264 1 m1/2 K\u22121 \u2211 k=1 \u2223 \u2223 \u2223 \u2223 1 \u00b5(k+ 1) \u2212 1 \u00b5(k) \u2223 \u2223 \u2223 \u2223 (E[d(k)])1/2 + 1 m1/2\u00b5(1) (E[d(0)])1/2 +\n1\nm1/2\u00b5(K) (E[d(K)])1/2\n+\n\u221a\nA mK +\n\u221a\n2B mK2\nK\n\u2211 k=1 E[d(k\u2212 1)]\nThis decays at rate O ( 1 K ) as long as \u00b5(\u2113) =C\u2113\u2212\u03b1 with 12 \u2264 \u03b1 \u2264 1.\nB Useful Concentration Inequalities\nFor our analysis of both the direct and IPM estimates, we need the following key technical lemma from [33]. This lemma controls the concentration of sums of random variables that are sub-Gaussian conditioned on a particular filtration {Fi}ni=0. Such a collection of random variables is referred to as a sub-Gaussian martingale sequence. We include the proof for completeness.\nLemma 22 (Theorem 7.5 of [33]). Suppose we have a collection of random variables {Vi}ni=1 and a filtration {Fi}ni=0 such that for each random variable Vi it holds that\n1. E [ esVi \u2223 \u2223 Fi\u22121 ] \u2264 e 12 \u03c3 2i s2 with \u03c32i a constant\n2. Vi is Fi-measurable\nThen for every a \u2208Rn it holds that\nP\n{\nn\n\u2211 i=1 aiVi > t\n}\n\u2264 exp { \u2212 t 2\n2\u03bd\n}\n\u2200t > 0\nand\nP\n{\nn\n\u2211 i=1\naiVi <\u2212t } \u2264 exp { \u2212 t 2\n2\u03bd\n}\n\u2200t > 0\nwith\n\u03bd = n\n\u2211 i=1 \u03c32i a 2 i\nProof. We bound the moment generating function of \u2211ni=1 aiVi by induction. As a base case, we have\nE [ esa1V1 ] = E [ E [ esa1V1 \u2223 \u2223\n\u2223 F0\n]]\n\u2264 e 12 \u03c3 21 a21s2\nAssume for induction that we have\nE\n[\nexp\n{\ns j\n\u2211 i=1 aiVi\n}] \u2264 exp {\n1 2\n(\nj\n\u2211 i=1 \u03c32i a 2 i\n) s2 }\nThen we have\nE\n[\nexp\n{\nj+1 \u2211 i=1 aiVi\n}]\n= E\n[\nexp\n{\ns j\n\u2211 i=1 aiVi\n}\nesa j+1X j+1\n]\n= E\n[\nE\n[\nexp\n{\ns j\n\u2211 i=1 aiVi\n}\nesa j+1X j+1 \u2223 \u2223\n\u2223 F j+1\n]]\n(a) = E\n[\nexp\n{\ns j\n\u2211 i=1 aiVi\n}\nE\n[ esa j+1X j+1 \u2223 \u2223\n\u2223 F j+1\n]\n]\n(b) \u2264 E\n[\nexp\n{\ns j\n\u2211 i=1 aiVi\n}]\ne 1 2 \u03c3 2 j+1a 2 j+1s 2\n(c) \u2264 exp\n{\n1 2\n(\nj+1 \u2211 i=1 \u03c32i a 2 i\n) s2 }\nwhere (a) follows since \u2211 ji=1 aiVi is F j measurable, (b) follows since\nE\n[ esa j+1X j+1 \u2223 \u2223\n\u2223 F j+1\n]\n\u2264 e 1 2 \u03c3 2 j+1a 2 j+1s 2 ,\nand (c) is the inductive assumption. This proves that\nE\n[\nexp\n{\ns n\n\u2211 i=1 aiVi\n}] \u2264 exp {\n1 2\n(\nn\n\u2211 i=1 \u03c32i a 2 i\n) s2 } \u2264 exp {\n1 2 \u03bds2 }\nUsing the Chernoff bound [19], we have\nP\n{\nn\n\u2211 i=1 aiVi > t\n} \u2264 e\u2212stE [ exp { s n\n\u2211 i=1 aiVi\n}]\n\u2264 exp { \u2212st + 1 2 \u03bds2 }\nOptimizing the bound over s yields\nP\n{\nn\n\u2211 i=1 aiVi > t\n}\n\u2264 exp { \u2212 t 2\n2\u03bd\n}\nThe proof for the other tail is similar.\nIf the random variables instead satisfy\n1. E [ exp { s ( Vi \u2212E [ Vi \u2223 \u2223 Fi\u22121 ])} \u2223 \u2223 Fi\u22121 ] \u2264 e 12 \u03c3 2i s2 with \u03c32i a constant 2. Vi is Fi-measurable\nthen Lemma 22 can be applied to {Vi\u2212E [ Vi \u2223 \u2223 Fi\u22121 ] }ni=1 to yield\nP\n{\nn\n\u2211 i=1\naiVi > n\n\u2211 i=1 aiE [ Vi \u2223 \u2223 Fi\u22121 ] + t\n}\n\u2264 exp { \u2212 t 2\n2\u03bd\n}\nIf we can upper bound the conditional expectations\nE [ Vi \u2223 \u2223 Fi\u22121 ] \u2264Ci, by Fi\u22121-measurable random variables Ci, then we have\nP\n{\nn\n\u2211 i=1\naiVi > n\n\u2211 i=1 aiCi + t\n} \u2264 P { n\n\u2211 i=1\naiVi > n\n\u2211 i=1 aiE [ Vi \u2223 \u2223 Fi\u22121 ] + t\n}\n\u2264 exp { \u2212 t 2\n2\u03bd\n}\nFor our analysis, we generally cannot compute E [ Vi \u2223 \u2223 Fi\u22121 ]\n, but we can find \u201cnice\u201d Ci. To find \u03c32i for use in Lemma 22, we frequently use the following conditional version of Hoeffding\u2019s Lemma.\nLemma 23 (Conditional Hoeffding\u2019s Lemma). If a random variable V and a sigma algebra F satisfy a \u2264V \u2264 b and E[V |F ] = 0, then\nE [ esV | F ] \u2264 exp { 1 8 (b\u2212 a)2s2 }\nProof. We follow standard proof of Hoeffding\u2019s Lemma from [19]. Since esx is convex, it follows that\nesx \u2264 b\u2212 x b\u2212 ae sa + x\u2212 a b\u2212 ae sb a \u2264 x \u2264 b\nTherefore, taking the conditional expectation with respect to F yields\nE [ esV \u2223 \u2223 F ] \u2264 b\u2212E [V | F ] b\u2212 a e sa + E [V | F ]\u2212 a b\u2212 a e sb (22)\nLet h = s(b\u2212 a), p =\u2212 ab\u2212a , and L(h) =\u2212hp+ log(1\u2212 p+ peh). Then we have\neL(h) = b\nb\u2212 ae sa + \u2212a b\u2212 ae sb\n= b\u2212E [V | F ]\nb\u2212 a e sa + E [V | F ]\u2212 a b\u2212 a e sb (23)\nsince E [V | F ] = 0. Since L(h) = L\u2032(h) = 0 and L\u2032\u2032(h)\u2264 14 ,, it holds that L(h)\u2264 18 (b\u2212 a)2s2. Combining this bound on L(h) with (22) and (23) yields the result.\nBefore proceeding with our analysis, we need to introduce a few useful concentration inequalities for sub-Gaussian vector-valued random variables. First, for a scalar random variable \u03be , define the sub-Gaussian norm\n\u03c4(\u03be ) = inf { a > 0\n\u2223 \u2223 \u2223 \u2223 E[es\u03be ]\u2264 e 12 a2s2 \u2200s \u2265 0 }\n(24)\nClearly, if \u03c4(\u03be )<+\u221e, then \u03be is sub-Gaussian. Second, for a random vector v in Rd , define\nB(v) = d\n\u2211 i=1 \u03c4((v)i) (25)\nwhere (v)i is the ith component of v. We define v to be sub-Gaussian if B(v)<+\u221e. Of crucial importance in our analysis is analyzing the norm of an average of vector-valued sub-Gaussian random variables. The following lemma describes how to control the sub-Gaussian norm in such a situation.\nLemma 24. Suppose that {vi}Ki=1 is a collection of independent sub-Gaussian random variables in Rd . Then it holds that\nB\n(\n1 K\nK\n\u2211 i=1 vi\n)\n\u2264 1 K\nd\n\u2211 j=1\n\u221a\nK\n\u2211 i=1 \u03c42((vi) j)\nIf in addition the random variables {vi}Ki=1 satisfy\nmax i=1,...,K max j=1,...,d\n\u03c42((vi) j)\u2264 \u03c42\nthen it holds that\nB\n(\n1 K\nK\n\u2211 i=1 vi\n)\n\u2264 \u03c4d\u221a K\nProof. We analyze one component of the sum 1K \u2211 K i=1vi. It holds that\nE\n\nexp\n\n\n\ns\n(\n1 K\nK\n\u2211 i=1 vi\n)\nj\n\n\n\n\n = E\n[\nexp\n{\ns K\nK\n\u2211 i=1 (vi) j\n}]\n= K\n\u220f i=1 E\n[ exp { s\nK (vi) j\n}]\n\u2264 K\n\u220f i=1 exp\n{\n1 2 1 K2 \u03c42((vi) j)s2 }\n= exp\n{\n1 2\n(\n1 K2\nK\n\u2211 i=1 \u03c42((vi) j)\n) s2 }\nThis implies that\n\u03c4\n\n\n(\n1 K\nK\n\u2211 i=1 vi\n)\nj\n\n\u2264 1 K\n\u221a\nK\n\u2211 i=1 \u03c42((vi) j)\nand so\nB\n(\n1 K\nK\n\u2211 i=1 vi\n)\n\u2264 1 K\nd\n\u2211 j=1\n\u221a\nK\n\u2211 i=1 \u03c42((vi) j)\nFinally, if \u03c42((vi) j)\u2264 \u03c42, then we have\nB\n(\n1 K\nK\n\u2211 i=1 vi\n)\n\u2264 1 K\nd\n\u2211 j=1\n\u221a\nK\n\u2211 i=1 \u03c42((vi) j)\n\u2264 d K\n\u221a\nK\n\u2211 i=1 \u03c42\n= \u03c4d\u221a\nK\nExample 3.2 from [17], a consequence of Theorem 3.1 in [17], is useful for the concentration of the norm of sub-Gaussian vector random variables.\nLemma 25 (Example 3.2 of [17]). If v is a random vector in Rd with B(v)<+\u221e, then\nP{\u2016v\u2016> t} \u2264 2exp { \u2212 t 2\n2B2(v)\n}\nFinally, we will also need to deal with dependent random variables that are sub-Gaussian with respect to a particular filtration.\nLemma 26. Suppose that a random variable V and a sigma algebra F satisfies\n1. E [V | F ] = 0\n2. P { |V |> t \u2223 \u2223 F } \u2264 2e\u2212ct2 with c a constant. Then it holds that\nE[esV \u2223 \u2223 F ] \u2264 exp { 1 2 ( 9 c ) s2 }\nfor all s \u2265 0.\nProof. Adapted from the characterization of sub-Gaussian random variables in [15]. First, we have for any a < c that\nE\n[ eaV 2 \u2223 \u2223 \u2223 F ] \u2264 1+ \u222b \u221e\n0 2ateat 2 P{|V |> t | F}dt\n\u2264 1+ \u222b \u221e\n0 2ate\u2212(c\u2212a)t 2 dt\n= 1+ 2a\nc\u2212 a\nSetting a = c3 yields the bound\nE\n[ eaV 2 \u2223 \u2223 \u2223 F ] \u2264 2\nSince E [V | F ] = 0, by a Taylor expansion we have\nE [ esV \u2223 \u2223 F ] = 1+ \u222b \u221e\n0 (1\u2212 y)E\n[ (sV )2eysV \u2223 \u2223 \u2223 F ] dy\n\u2264 ( 1+ s2\na\n)\ne s2 2a\n\u2264 exp { 5s2\n2a\n}\n= exp\n{\n1 2\n(\n9 c\n) s2 }"}], "references": [{"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": "The MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "and S", "author": ["A. Agarwal", "H. Daum\u00e9"], "venue": "Gerber, \u201cLearning multiple tasks using manifold regularization.,\u201d in NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularized multi\u2013task learning,", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "A convex formulation for learning task relationships in multi-task learning,", "author": ["Y. Zhang", "D. Yeung"], "venue": "CoRR, vol. abs/1203.3536,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A survey on transfer learning,", "author": ["S. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Matrix regularization techniques for online multitask learning,", "author": ["A. Agarwal", "A. Rakhlin", "P. Bartlett"], "venue": "Tech. Rep. UCB/EECS-2008-138, EECS Department,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "and A", "author": ["Z. Towfic", "J. Chu"], "venue": "Sayed, \u201cOnline distirubted online classifcation in the midst of concept drifts,\u201d Neurocomputing, vol. 112, pp. 138\u2013152", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "and M", "author": ["C. Tekin", "L. Canzian"], "venue": "van der Schaar, \u201cContext adaptive big data stream mining,\u201d in Allerton Conference", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning for sequential data: A review,", "author": ["T. Dietterich"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Adaptive fraud detection.,", "author": ["T. Fawcett", "F. Provost"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Predicting the secondary structure of globular proteins using neural network models,", "author": ["N. Qian", "T. Sejnowski"], "venue": "Journal of Molecular Biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "Input-output HMM\u2019s for sequence processing,", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Implicit Functions and Solution Mappings: A View from Variational Analysis", "author": ["A. Dontchev", "R. Rockafellar"], "venue": "Springer, New York, New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On the empirical estimation of integral probability metrics,", "author": ["B. Sriperumbudur"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Introduction to non-asymptotic analysis of random matrices,", "author": ["R. Veryshin"], "venue": "Tech. Rep., University of Michigan,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "and A", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan"], "venue": "Shapiro, \u201cStochastic approximation approach to stochastic programming,\u201d SIAM Journal on Optimization, vol. 19, pp. 1574\u20131609", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Pechuk, \u201cInequalities for the distributions of functionals of sub-gaussian vectors,", "author": ["E.D.V.V Buldygin"], "venue": "Theor. Probability and Math. Statist., pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Large deviations for sums of partly dependent random variables,", "author": ["S. Janson"], "venue": "Random Structures Algorithms,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Oxford University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical Linear Algebra", "author": ["L. Trefethen"], "venue": "SIAM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Uniqueness of positive fixed points for increasing concave functions on rn: An elementary result,", "author": ["J. Kennan"], "venue": "Review of Economic Dynamics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fixed Point Theory", "author": ["A. Granas", "J. Dugundji"], "venue": "Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Trends in income inequality", "author": ["S. Jenkins", "P. Van Kerm"], "venue": "pro-poor income growth, and income mobility,\u201d Oxford Economic Papers, vol. 58, no. 3, pp. 531\u2013548", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "The elements of statistical learning: data mining", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": "inference, and prediction: with 200 full-color illustrations, New York: Springer-Verlag", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Online learning and stochastic approximations,", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Analysis of mirror descent for strongly convex functions,", "author": ["A. Nedic", "S. Lee"], "venue": "ArXiV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Yu. Nesterov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "which is a standard criterion for optimization and learning problems [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "In transfer learning, knowledge from one source task is transferred to another target task either with or without additional training data for the target task [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "For multi-task and transfer learning, there are theoretical guarantees on regret for some algorithms [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "For example, we could observe a feature wn and predict the label yn as in [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "There are also some bandit approaches in which one of a finite number of predictors must be applied to the data as in [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Another relevant model is sequential supervised learning (see [9]) in which we observe a stream of data consisting of feature/label pairs (wn,yn) at time n, with wn being the feature vector and yn being the label.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn\u2212i,yn\u2212i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn\u2212i,yn\u2212i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "Another approach is to assume that there is an underlying hidden Markov model (HMM) [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Using the triangle inequality and variational inequalities from [13] yields \u2016xi \u2212xi\u22121\u2016 \u2264 \u2016xi \u2212xi\u22121\u2016+ \u2016xi\u2212xi \u2016+ \u2016xi\u22121\u2212xi\u22121\u2016 \u2264 \u2016xi \u2212xi\u22121\u2016+ 1 m \u2016\u2207x fi(xi)\u2016+ 1 m \u2016\u2207x fi(xi\u22121)\u2016 We then approximate \u2016\u2207x fi(xi)\u2016= \u2016Ezi\u223cpi [\u2207xl(xi,zi)]\u2016 by", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "2 Vector Integral Probability Metric Estimate Given a class of functions F where each f \u2208F maps Z \u2192R, an integral probability metric (IPM) [14] between two distributions p and q is defined to be \u03b3F (p,q), sup f\u2208F \u2223 Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)] \u2223", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "We consider an extension of this idea, which we call a vector IPM, in which the class of functions F maps Z \u2192 X : \u03b3V F (p,q), sup f\u2208F \u2016Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)]\u2016 (7) Lemma 1 shows that a vector IPM can be used to bound the change in minimizer at time i and follows from variational inequalities in [13] and the assumption that {\u2207xl(x, \u00b7) : x \u2208 X } \u2282 F .", "startOffset": 300, "endOffset": 304}, {"referenceID": 12, "context": "By exploiting variational inequalities from [13], we can show that \u2016xi \u2212xi\u22121\u2016 \u2264 1 m \u2016\u2207x fi(xi\u22121)\u2212\u2207x fi\u22121(xi\u22121)\u2016 = 1 m \u2016Ezi\u223cpi [ \u2207xl(xi\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ \u2207xl(xi\u22121,zi\u22121) ] \u2016 By assumption {\u2207xl(xi\u22121, \u00b7) : x \u2208 X } \u2282 F , so \u2016\u2207x fi(xi\u22121)\u2212\u2207x fi\u22121(xi\u22121)\u2016 = \u2016Ezi\u223cpi [ l(xi\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ l(xi\u22121,zi\u22121) ] \u2016 \u2264 sup f\u2208F \u2016Ezi\u223cpi [ f (zi)]\u2212Ezi\u22121\u223cpi\u22121 [ f (zi\u22121)]\u2016 = \u03b3 F (pi, pi\u22121) We cannot compute this vector IPM, since we do not know the distributions pi and pi\u22121.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "This allows us to apply standard concentration inequalities for norms of random variables as in [15].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "This is a common assumption for in high probability analysis of optimization algorithms as in [16] for example.", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "By applying Lemma 25 from [17] to the conditional distribution P{\u00b7|Fi\u22121}, we have P {\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207xl(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 > t \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 }", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "The proof of Lemma 5 is nearly identical to the proof of the extension of Hoeffding\u2019s inequality from [18] with Lemma 22 used instead.", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "Applying the Chernoff bound [19] and optimizing yields P { n \u2211 i=1 Vi > t }", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "To apply gradient descent, we use eigenvalue perturbation results [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "3 from [21] to conclude that there exists a unique, positive fixed point v\u0304 of \u03c6K\u2217(v).", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Following [23], for any fixed point v\u0304, it holds that |\u03c6K\u2217(v)\u2212 v\u0304| \u2264 \u03c6 \u2032 K\u2217(v\u0304)|v\u2212 v\u0304| Therefore, applying the fixed point property repeatedly yields |\u03c6 (n) K\u2217 (v)\u2212 v\u0304| \u2264 (\u03c6 \u2032 K\u2217(v\u0304))|v\u2212 v\u0304| By Lemma 14, it holds that \u03c6 \u2032 K\u2217(v\u0304)< 1 and so the result follows.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "chosen based on previous economic studies in [25].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "We average over twenty runs of our algorithm by resampling without replacement [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "This is a smoothed version of the hinge loss used in support vector machines (SVM) [26].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well.", "creator": "LaTeX with hyperref package"}}}