{"id": "1509.07450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "A 128 channel Extreme Learning Machine based Neural Decoder for Brain Machine Interfaces", "abstract": "currently, state - of - the - world art brain motor intention decoding algorithms built in brain - machine interfaces are mostly implemented on a pc software and directly consume significant amount directly of motive power. yet a parallel machine learning co - computer processor operating in 0. 35um cmos for brain motor intention decoding in brain - machine interfaces is continuously presented demonstrated in this paper. using extreme learning machine algorithm generators and low - power analog processing, it achieves an energy efficiency of 290 gmacs / w at a classification rate limit of 50 hz. the learning in second stage and corresponding digitally supervised stored coefficients are used to increase robustness of the core analog processor. the mri chip is verified with neural data recorded in parallel monkey finger movements experiment, achieving a decoding accuracy of 99. 3 % for movement type. the same co - processor is also used frequently to decode time of movement from asynchronous neural spikes. with time - delayed feature dimension enhancement, the classification accuracy can be increased by 5 % with limited number of input channels. further, a sparsity promoting training scheme enables reduction of number of programmable computation weights by ~ 2x.", "histories": [["v1", "Tue, 22 Sep 2015 06:30:16 GMT  (3980kb)", "http://arxiv.org/abs/1509.07450v1", "13 pages, accepted by IEEE Transactions on Biomedical Circuits and Systems, 2015"], ["v2", "Sun, 27 Sep 2015 07:39:03 GMT  (3982kb)", "http://arxiv.org/abs/1509.07450v2", "13 pages, 17 figures, accepted by IEEE Transactions on Biomedical Circuits and Systems, 2015"]], "COMMENTS": "13 pages, accepted by IEEE Transactions on Biomedical Circuits and Systems, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["yi chen", "enyi yao", "arindam basu"], "accepted": false, "id": "1509.07450"}, "pdf": {"name": "1509.07450.pdf", "metadata": {"source": "CRF", "title": "A 128 channel Extreme Learning Machine based Neural Decoder for Brain Machine Interfaces", "authors": ["Yi Chen", "Enyi Yao"], "emails": ["ychen3@ntu.edu.sg,", "arindam.basu@ntu.edu.sg)."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n07 45\n0v 1\n[ cs\n.L G\n] 2\n2 Se\np 20\n15 1 A 128 channel Extreme Learning Machine based Neural Decoder for Brain Machine Interfaces Yi Chen, Student Member, IEEE, Enyi Yao, Student Member, IEEE, Arindam Basu, Member, IEEE\nAbstract\u2014Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning co-processor in 0.35\u00b5m CMOS for motor intention decoding in brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 3.45 pJ/MAC at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same co-processor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by \u2248 2X .\nIndex Terms\u2014Neural Decoding, Motor Intention, BrainMachine Interfaces, VLSI, Extreme Learning Machine, Machine Learning, Neural Network, Portable, Implant\nI. INTRODUCTION\nBrain-machine interfaces (BMI) are becoming increasingly popular over the last decade and open up the possibility of neural prosthetic devices for patients with paralysis or in locked-in state. As depicted in Fig. 1, typical implanted BMI consists of a neural recording IC to amplify, digitize and transmit the neural action potentials (AP) recorded by the microelectrode array (MEA). Significant effort has been dedicated to develop energy efficient neural recording channel in recent years for long-term operation of the implanted devices [1] [2] [3] [4]. Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11]. However, in order to produce an actuation command (e.g. for a prosthetic arm), the subsequent step of motor intention decoding is required to map the spike train patterns acquired in the neural recording to the motor intention of the subjects.\nThough various elaborate models and methods of motor intention decoding have been developed in past decades with the goal of achieving high decoding performance [12] [13] [14], the state-of-the art neural signal decoding are mainly conducted on PC consuming a considerable amount of power and making it impractical for the long-term use. With onchip real-time motor intention decoding, the size and the power consumption of the computing device can be reduced\nYi Chen, Enyi Yao, and Arindam Basu are with Centre of Excellence in IC Design (VIRTUS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798 (e-mail:ychen3@ntu.edu.sg, arindam.basu@ntu.edu.sg).\nA F E A D C DSP\nTx\nRx\nA F E A D C D S P\nTx\nRx\nTraditional design\nEnvisioned design\nMLCP\nML\nActuator\nTx/Rx\nActuator\nTx/Rx\nSkull\nSkull\nFig. 1: Comparison of envisioned and traditional implanted BMI: The envisioned system uses a machine learning co-processor (MLCP) along with the DSP used in traditional neural implants to estimate motor intentions from neural recordings thus providing data compression. Traditional systems perform such decoding outside the implant and use bulky computers.\no1\n1 j L\nx1 xD\n\u03b2kj\nH1 Hj HL\nwij\nELM\noM\nMovement\nOnset G(tk)\nMovement type S(tk):\nlabel of maximum output\nMoving average of input spikes\noM+1 \u03b8\n(a) (b)\no1\n1 j L\nx1 xD\n\u03b2kj\nH1 Hj HL\nwij\ng(\u00b7)\ng(\u00b7)\ng(\u00b7)\noC\nMLCP\nDSP\nFig. 2: Algorithm: (a) The architecture of the Extreme Learning Machine (ELM) with one nonlinear hidden layer and linear output layer. (b) Use of ELM in neural decoding for classifying movement type and onset time of movement.\neffectively and the solution becomes truly portable. Furthermore, integrating the neural decoding algorithm with the neural recording device is also desired to reduce the wireless data transmission rate and make the implanted BMI solution scalable as required in the future [15]. Until now, very few attempts have been made to give a solution for this problem. A low-power motor intention architecture using analog computing is proposed in [16], featuring an active filtering with massive parallel computing through low power analog filters and memories. However, no measurement results are published to support the silicon viability of the architecture. A more recent work proposes a universal computing architecture for neural signal decoding [17]. The architecture is implemented on a FPGA with a power consumption of 537 \u00b5W.\nIn this paper, we present a machine learning co-processor (MLCP) achieving low-power operation through massive par-\n2 allelism, sub-threshold analog processing and careful choice of algorithm. Figure 1 contrasts our approach with traditional approaches: our MLCP acts in conjunction with the digital signal processor (DSP) already present in implants (for spike sorting, detection and packetizing) to provide the decoded outputs. The bulk of processing is done on the MLCP while simple digital functions are performed on the DSP. Compared to traditional designs that perform the decoding outside the implant, our envisioned system that provides opportunity for huge data compression by integrating the decoder in the implant. The MLCP is characterized by measurement and the decoding performance of the proposed design is verified with data acquired in individuate finger movements experiment of monkeys. Some initial results of this work were presented in [18]. Here, we present more detailed theory, experimental results including decoding time of movement, new sparsity promoting training and also discuss scalability of this architecture."}, {"heading": "II. PROPOSED DESIGN: ALGORITHM", "text": ""}, {"heading": "A. Extreme Learning Machine", "text": "1) Network Architecture: The machine learning algorithm used in this design is Extreme Learning Machine (ELM) proposed in [19]. As depicted in Fig. 2(a), The ELM is essentially a single hidden-layer feed-forward network (SLFN). The k-th output of the network (1\u2264 k \u2264 C) can be expressed as follows,\nok =\nL \u2211\ni\n\u03b2kig (wi,x, bi) =\nL \u2211\ni\n\u03b2kihi = h T\u03b2k,\nwi,x \u2208 R D;\u03b2ki, bi \u2208 R;h,\u03b2k \u2208 R L\n(1)\nwhere x denotes the input feature vector, L is the number of hidden neurons, h is the output of the hidden layer, bi is the bias for each hidden layer node, wi and \u03b2ki are input and output weights respectively. A non-linear activation function g() is needed for non-linear classification. A special case of the nonlinear function is the additive node defined by hi = g ( w T i x+ bi )\n. The above equation can be compactly written for all classes as o = h\u03b2, o = [o1, o2, ..oc] where \u03b2 = [\u03b21,\u03b22...\u03b2C ] denotes the L\u00d7C matrix of output weights.\nWhile the output ok can be directly used in regression, for classification tasks the input is categorized as the k-th class if ok is the largest output. Formally, we can define the classification output as an integer class label s given by s = argmaxkok, 1 \u2264 k \u2264 C. Intuitively, we can think of the first layer as creating a set of random basis functions while the second layer chooses how to combine these functions to match a desired target. Of course, if we could choose the basis functions through training as well, we would need less number of such functions. But the penalty to be paid is longer training times. More details about the algorithm can be found in [19], [20].\n2) Training Methods: The special property of the ELM is that w can be random numbers from any continuous probability distribution and remains unchanged after initiation [19], while only \u03b2 needs to be trained and stored with high resolution. Therefore the training of this SLFN reduces to\nfinding a least-square solution of \u03b2 given the desired target values in a training set. We will next show two methods of training\u2013the conventional one (T1) for improved generalization as well as a second method (T2) that promotes sparsity. For simplicity, we show the solution of weights for one output ok\u2013the same method can be extended to other output weights as well and can be represented in a compact matrix equation [19].\nSuppose there are p training samples\u2013then we can create a p\u00d7L hidden layer output matrix H where each row of H has the hidden layer neuron output for each training sample. Let Tk \u2208 Rp be the vector of target values for the p samples. With these inputs, the two training methods are shown in Fig. 3. The step for L2 norm minimization can be solved directly with the solution given by \u03b2k = H \u2020 Tk where H\u2020 is the Moore-Penrose generalized inverse of the matrix H. Hence, training can happen quickly in this case. The L1 norm minimization step in T2 however has to be performed using standard optimization algorithms like LARS [21]. Thus T2 provides reduced hardware complexity due to reduction in the number of hidden neurons at the cost of increased training time."}, {"heading": "B. Neural Decoding", "text": "The neural decoding algorithm we use is inspired by the method in [22]. We replace the committee of ANN in their work with ELM in our case. Three specific advantages of the ELM for this application are (1) the fixed random input weights can be realized by a current mirror array exploiting fabrication mismatch of the CMOS process; (2) one-step training that is necessary for quick weight update to address change in input statistics and (3) the hidden layer outputs h can be reused for multiple operations on the same input data x. In this case, we have reused h to classify both the onset time and type of movement. One disadvantage of the ELM algorithm is the usage of 1.5\u2212 3X hidden neurons compared to fully tuned architectures (e.g. SVM, AdaBoost) since the hidden nodes in ELM only create random projections that are not fine tuned [23]. However, implementing random weights results in more than 10X savings over fully tunable weights making this architecture more lucrative overall. Next, we give an overview of the decoding algorithm while the reader is pointed to [22] for more details.\n1) Movement type and Onset time Decoding: Figure 2(b) depicts how the ELM is used in neural decoding. Even though the input is an asynchronous spike train, the ELM produces classification outputs at a fixed rate of once every Ts seconds. The input x is created from the firing rate of spike trains p(t) = \u2211\nts \u03b4(t \u2212 ts) of biological neurons by\nfinding the moving average over a duration Tw. Hence, we can define the firing rate ri of i-th neuron at time instant tk as ri(tk) = \u222b tk\ntk\u2212Tw p(t) where Ts = 20 ms and Tw = 100\nms following [22]. Finally, x(tk) = [r1(tk), r2(tk), ...rq(tk)] where there are q biological neurons in the recording (d = q). As shown in Fig. 2(b), we have C = M +1 output neurons in this case where there are M movement types to be decoded. The M + 1-th neuron is used to decode the onset time of\n3T1\nH\nS1: Create hidden layer matrix\nfor all p training samples\nT2\nLpR \u00b4\u00ceH p\nk R\u00ceT\n22 min kkk\nk\nbgb b +T-H\nS2: Least square optimization with L2 regularization:\nLpR \u00b4\u00ceH p\nk R\u00ceT\nS3: Prune the hidden layer neurons\nwith zero output weights\nLpR \u00a2\u00b4\u00ceH*\np\nk R\u00ceT\nL\nk R\u00ceb\nL k R\u00ce *b\nL k R \u00a2 \u00ceb\nH\nS1: Create hidden layer matrix\nfor all p training samples\nS2: Least square optimization with L1 regularization:\nS4: Least square optimization with L2 regularization:\n12 min kkk\nk\nbgb b +T-H\n22 *min kkk k bgb b +T-H\nFig. 3: Training methods for ELM: T1 is the conventionally used training method to improve generalization by minimizing norm of weights as well as training error. T2 uses an additional step of sparsifying output weights to reduce the required hardware.\nmovement. For decoding type of movement, we can directly use the method described in the earlier section II-A for M - class classifier to get the predicted output class at time tk as s(tk) = argmaxpop(tk), 1 \u2264 p \u2264 M .\nFor decoding movement onset time, we further create a binary classifier that reuses the same hidden layer but adds an extra output neuron. Similar to [22], this output is trained for regression\u2013the target is a trapezoidal fuzzy membership function which gradually rises from 0 to 1 representing the gradual evolution of biological neural activity. This output oM+1 is thresholded to produce the final output G(tk) at time tk as:\nG(tk) =\n{\n1, ifoM+1(tk) > \u03b8 0, otherwise (2)\nwhere \u03b8 is a threshold optimized as a hyperparameter. Moreover, to reduce spurious classification and produce a continuous output, the primary output G(tk) is processed to create Gtrack(tk) that is high only if G is high for at least \u03bb times over the last \u03c4 time points. Further, to reduce false positives, another detection is prohibited for Tr ms after a valid one. The final decoded output, F (tk) is obtained by a simple combination of the two classifiers as F (tk) = Gtrack(tk)\u00d7 s(tk).\n2) Time delay based dimension increase (TDBDI): A common problem in long-term neural recording is the loss of information from electrodes over time due to tissue reactions such as gliosis, meningitis or mechanical failures [24]. Hence, initially functional electrodes may not provide information later on. To retain the quality of decoding, we propose a method commonly used in time series analysis\u2013the use of information from earlier time points [25]. In the context of neural decoding, it means that we use more information from the dynamics of neural activity in functional electrodes in\nplace of lost information from the instantaneous values of activity in previously functional electrodes. So if we use p\u22121 previous values from the n functional electrodes, the new feature vector is given by:\nx(tk) = [r1(tk), r1(tk\u22121), r1(tk\u22122), r1(tk\u2212p+1),\nr2(tk), r2(tk\u22121)...rn(tk\u2212p+1)] (3)\nwhere the input dimension of the ELM is given by D = n\u00d7p. This is a novel algorithmic feature in our work compared to [22]."}, {"heading": "III. PROPOSED DESIGN: HARDWARE IMPLEMENTATION", "text": "Fig.1 shows a typical usage scenario for our MLCP where it works in tandem with the DSP and performs the intention decoding. The DSP only needs to send very simple control signals to the MLCP and performs the calculation of the second stage of ELM (multiplication by learned weights \u03b2). The input to the MLCP comes from spike sorting that can be performed on the DSP [10]. In some cases, spike sorting may not be needed and spike detection may be the only required pre-processing [24]."}, {"heading": "A. Architecture", "text": "Details and timing of the MLCP are shown in Fig. 4. We map input and hidden layers of ELM into the MLCP fabricated in AMS-0.35\u00b5m CMOS process, where high computation efficiency is achieved by exploiting fabrication mismatch abundantly found in analog devices, while the output layer that requires precision and tunability (tough to attain analog designs) can be implemented on the DSP. Since the number of computations in first stage far outnumbers those in the second (as long as D >> C), such a system partition still retains\n4 RN_cnt S P K\nA < 6 :0 >\nCCO1\nCNT1\nh1\n1 -t o -1 2 8 D e M u x\nWinCNT1 DAC1\nInput Processing Current Mirror\nArray\nReference\nSPI\nMLCP\nCLK_out C<13:0>\nIin1\nCCO2\nh2\nIin2\nCNT2\nCCOL\nhL\nIinL\nCNTL\nWinCNT2 DAC2 WinCNTD DACD I D A C ,2\nI D A C ,1\nI D A C ,D\nColumn Scanner\nDSP Timing & Control ELM output Stage\nWireless T/Rx\nNEU S D L < 2 :0 > R N _ in C L K _ in\nRandom weight\n(wij)\nHidden layer output\nIn p u t v e c to r (x )\nCM\nCM\nCM\n(a)\nCLK_out\nSPK\nA<6:0>\nRN_in\nCLK_in\nNEU\nC<0:13>\nRN_cnt\n(b)\nFig. 4: The diagram (a) and the timing (b) of MLCP based neural decoder.\nthe power efficiency of the analog design. Up to 128 input channels and 128 hidden layer nodes are supported by the MLCP, with each input channel embedding an input processing circuit that extracts input feature from the incoming spike trains. As mentioned in the earlier section, we extract a moving average of the spike count as the input feature of interest.\nOn receiving a spike from the neural amplifier array (after spike detection and/or spike sorting), the DSP sends a pulse via SPK and 7-bit channel address (A\u30086 : 0\u3009) to the DEMUX in the MLCP for row-decoding. Each row of the MLCP has a 6-bit window counter (WinCNT ) to count the total number of input spikes in a moving window with length of 5ts and a moving step of ts. The length of ts, normally set to 20 ms, is determined by the period of CLK in. The counter value in j-th row is converted into input feature current IDACj for the ELM, corresponding to the input xj in Fig. 2. Furthermore, a 1-bit control signal (Sext\u3008j\u3009) stored in each row determines whether the j-th row\u2019s input to the moving window circuit is an external spike count or a delayed spike count from the previous channel. The delay length can be selected from among 5 delay\nsteps ranging from 20 ms to 100 ms, based on SDL\u30082 : 0\u3009. This is how the TDBDI feature described earlier is implemented in the MLCP.\nInput feature current from each row is further mirrored into all hidden-layer nodes by a current mirror array. Hence, ratios of the current mirrors are essentially the input weights, and are inherently random due to fabrication mismatch of the transistors even when identical value is used in the design. We use sub-threshold current mirror to achieve very low power consumption, resulting in wij = e \u2206Vt,ij\nUT with UT denoting thermal voltage and \u2206Vt,ij denoting the threshold voltage mismatch between input transistor on j-th row and mirror transistor on i-th column of that row. This is similar to the concepts described in [26] [27]. The input weights are lognormal distributed since \u2206Vt,ij follows a normal distribution. We therefore realize random input weights in a very low \u2018cost\u2019 way that requires only one transistor per weight. It is the fixed random input weights of the ELM that makes this unique design possible. A capacitance CM=400 fF on each row sets the SNR of the mirroring to 43 dB.\n5 4b CNT 4b reg\n6 b re g\nSext<1>=0Di<3:0>\nM U X 5 to 1 D o < 3 :0 >\n6b current\nDAC\n4b reg\n4b reg\n4b reg\n4b reg\n4b reg\nDAC1WinCNT1\nDi<3:0>\n4b CNT 4b reg\n6 b re g\nSext<2>=1\nM U X 5 to 1 D o < 3 :0 >\n6b current\nDAC\n4b reg\n4b reg\n4b reg\n4b reg\n4b reg\nDAC2WinCNT2\nI D A C 2\nI D A C 1\nSPK1 Dn\nDn-5\nQn-1\n0\n1\nSDL<2:0>\nSDL<2:0>\n(a)\nD5D5\n_\nIDAC\nIref\nD4 D4\n_ D0\nD0\n_ DVDD\n(b)\nDVDD\nvmem Iin\nN E U\nCint\nM1\nI l e a k\nCf\nvf M2 M3\nV lk\nvo\nINV1 INV2 INV3\n(c)\nFig. 5: Sub-block circuit diagrams: (a) Input processing circuit to take moving average of incoming spikes; (b) Current-mode DAC to convert average value to input x for the current mirror; (c) Neuronbased CCO to implement hidden node non-linearity and convert to digital.\nThe hidden layer node is implemented by a current controlled oscillator (CCO) driving a 14-bit counter with a 3- bit programmable stop value fmax to implement a saturating nonlinearity in the activation function g(). The advantage of choosing this nonlinearity is that it can be digitally set and also some neurons can be configured to be linear as well to achieve good performance in linearly separable problems [28]. The computation of hidden layer nodes is activated by setting NEU high. The output of CCO is a pulse frequency modulated signal with the frequency proportional to total input current. The counter outputs are latched and serially read using the CLK OUT signal when NEU is low with CCO disabled to save power. The output weights, \u03b2, are stored on the DSP where the final output ok is calculated. Thus the\nMLCP\nMCU\nWireless\nT/Rx\n5.1 cm\n7 .4 c m\nPEU Photo\nCurrent\nMirror\nArray W in d o w C o u n te r C ir c u it s\nCCOs & Output\nCounters\nReference\n& SPI\n4 .9 5 m m\n4.95 mm\nMLCP Die Photo\nFig. 6: Die photo and test board: The die photo of MLCP fabricated in 0.35\u00b5m CMOS process and the portable external unit (PEU) integrating MLCP with MCU and battery.\nMLCP performs the bulk of MACs (D\u00d7L) while the DSP only performs C\u00d7L MACs of the output layer. It should be noted here, the output of hidden layer neurons changes with power supply voltage due to sensitivity of the CCO frequency to power supply variation, leading to degradation of the decoding accuracy. However, since power supply variation is a commonmode component to all CCOs, normalization methods can be applied in post-processing (see Section IV-E4) to the hidden layer outputs to reduce the effect introduced by power supply variation."}, {"heading": "B. Sub-circuit: Input processing", "text": "Fig. 5 shows diagrams of the circuit blocks in the MLCP. Fig. 5(a) shows two adjacent input processing circuits with WinCNT1 configured to receive an external spike train by setting Sext\u30081\u3009=0 and WinCNT2 configured as time delay based channel by setting Sext\u30082\u3009=1. The corresponding signal flows are also depicted in the figure by red dash lines. The moving window counter is realized by (1) counting spike in a sub-window in a length of ts; (2) storing sub-window counter value in a delay chain made of shift registers; and (3) adding and subtracting previous 6-b output value with corresponding sub-window counter values in the delay chain to get new 6-b output value of WinCNT . This calculation can be represented as:\nQn \u30085 : 0\u3009 = Qn\u22121 \u30085 : 0\u3009+Dn \u30083 : 0\u3009 \u2212Dn\u22125 \u30083 : 0\u3009 , (4)\nwhere Qn \u30085 : 0\u3009 and Dn \u30083 : 0\u3009 are 6-b output value and 4-b sub-window counter value at time instance n respectively. All registers in the input processing circuits toggle at the rising edge of CLK in. The advantage of this structure is that the delay chain for sub-window counter value is reused in the proposed TDBDI feature, leading to a compact design.\nA compact, 6-bit MOS ladder based current mode DAC, as shown in Fig. 5(b), splits a reference current Iref (6-bit programmable in range of 1 nA to 63 nA) according to the WinCNT output value to generate the input feature current IDAC to the current mirrors."}, {"heading": "C. Sub-circuit: Current Controlled Oscillator", "text": "The diagram of the current controlled oscillator (CCO) is depicted in Fig. 5(c). The capacitance of Cint = 400 fF\n6 173 174 175 0 20 40 60 80 100\ncounter value\nMedian counter value: 174\n710 711 712 713 714 0\n20\n40\n60\n80\n100\ncounter value\nMedian counter value: 712\n2150 2151 2152 2153 2154 2155 2156 2157 0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\ncounter value\nMedian counter value: 2154\n\u03bc=174\n\u03c3=0\n\u03bc=712.1\n\u03c3=0.48\n\u03bc=2153.6\n\u03c3=1.39\nFig. 8: Jitter performance: The variation in the counter output for a fixed value of input current is observed for 100 trials and plotted as a histogram for (a) low, (b) medium and (c) high input currents. The measured jitter is < 0.1%\nsets oscillation frequency of this relaxation oscillator based on summed input current while Cf = 100 fF provides hysteresis through positive feedback. When NEU is pulled high, pFET M2 is turned off. M1 is used to set the leakage term bi in equation 1 and can be set to 0 for most cases. Iin from the current mirrors starts to discharge vmem until it crosses the threshold voltage of the INV1, leading to transition of all inverters. Then, vmem is pulled down very quickly through a positive feedback loop formed by Cf . At the same time, M3 turns on, charging vmem towards DVDD until it crosses the threshold voltage of INV1 from low to high and the cycle\nrepeats. Neglecting higher order effects, the time for each cycle of the CCO operation is determined by the sum of the charging and discharging time constant of vmem, and can be expressed by:\nTCCO = Cf \u00d7DVDD\nIin +\nCf \u00d7DVDD\n(Irst \u2212 Iin) , (5)\nwhere Irst is the charging current when M3 is on. Normally Irst >> Iin reducing equation 5 to:\nfCCO = 1\nTCCO \u2248 Iin Cf \u00d7DVDD . (6)"}, {"heading": "IV. MEASUREMENT RESULTS", "text": ""}, {"heading": "A. MLCP Characterization", "text": "This section presents the measurement results from the MLCP fabricated in 0.35\u00b5m CMOS process. To test the circuit, we have integrated it with a microcontroller unit or MCU (TI MSP430F5529) to act as the DSP. Though we have not integrated it with an implant yet, this setup does allow us to realistically assess performance of the MLCP with prerecorded neural data as shown later. Moreover, the designed board is entirely portable with its own power supply and wireless TX-RX module (TI CC2500). Hence, it can be used as a portable external unit (PEU) for neural implant systems\n7 0 1 2 3 4 5 6 0 500 1000 1500 2000 2500 3000\nNormalized Random Weights\nOutput Channel Address\nIn p\nu t\nC h\nan n\nel A\nd d\nre ss\nOutput Frequency (kHz)\n20 40 60 80 100 120\n120\n100\n80\n60\n40 20 100\n200\n300\n-60 -40 -20 0 20 40 60 0\n500\n1000\n1500\n2000\n2500\nDV t (mV)\n(a)\n(b) (c)\nFig. 10: The random input weights: (a) Measured mismatch map of the CCO frequencies; (b) Distribution of input weights and (c) \u2206Vt,ij . These values are measured by reading the output counter values when a fixed input value is given one row at a time.\nas well. As shown in Fig. 6, the MLCP has a die area of 4.95\u00d7 4.95 mm2 and the PEU measures 7.4cm\u00d75.1cm.\nFor the characterization results shown next, we use AVDD=2.1 V powering the reference circuits to generate bias currents and DVDD=0.6 V for the rest. Figure 7(a) verifies operation of the input processing by probing output of the window counter, with frequency of CLK in and input spike train being 20 Hz and 630 Hz respectively. The output, as labeled by Q \u30085 : 0\u3009, increases from 0 to 63 within 100 ms in the left half of Fig. 7(a). The TDBDI feature is shown in the right half of Fig. 7 based on setting of SDL \u30082 : 0\u3009=001 when Sext=1. It adds a delay of 40 ms to the Q \u30085 : 0\u3009, comparing with waveforms in the left half. Measured charging and discharging dynamics of the CCO based neuron are shown in Fig. 7(b) by probing a buffered version of membrane voltage vmem. Measured transfer curves of the 128 CCOs in a chip is plotted in Fig. 7(c), by varying input spike frequency from 0 to 630 Hz. Here, the saturation of the count is not shown\u2013\nwhen implemented, it stops the count at the preset value. The noise of the whole circuit is also characterized in terms of jitter at the output of the CCO. The variance in the counter value is measured for the same input current over 100 trials. This experiment is repeated for three different current values spanning most of the counting range. The results of this experiment, shown in Fig. 8, demonstrate percentage jitter less than 0.1% for the entire counting range.\nNext, we show characterization results for the input DAC channels. Since it is not possible to separately measure output current of the DAC, we measure the output of the CCO to infer the linearity of the DAC. This is reasonable since the linearity and the noise performance of the CCO is better than the 6 bit resolution of the DAC. Figure 9 plots the measured differential non-linearity (DNL) of 64 randomly selected input DACs. The worst case DNL is approximately \u00b13 LSB. While this DNL can be part of the non-linearity g(wi,x,bi) in the general case, it makes the implementation of the additive node less accurate.\nVariation in transfer curves of the CCO array is a result of random mismatch from various aspects of the circuits, mainly current mirror array, which is expected and desired in this design. By applying the same input spike frequency of 320 Hz to each row individually, a mismatch map of the CCO frequencies is generated with Iref = 32 nA, as presented in Fig. 10(a), by reading out the quantized frequency values in the output counters. These frequencies are normalized to the median frequency and plotted in Fig. 10(b) and (c) to show conformance to the log-normal distribution as expected. The underlying random variable of \u2206Vt,ij has a normal distribution with mean\u22480 and standard deviation\u224816.5 mV. Totally eight sample dies are characterized with mean and standard deviation of \u2206Vt,ij in all chips listed in Tab. I."}, {"heading": "B. Experiment", "text": "The neural data used to verify the decoding performance of the proposed design is acquired in a monkey finger movement experiment described in detail in [22]. In the experiment, the monkey puts its right hand into a pistol-grip manipulandum with one finger placed in one slot of the manipulandum. The monkey is trained to perform flexion or extension of the individual finger and wrist according to the given visual instruction. A single-unit recording device is implanted into the M1 cortex of the monkey, enabling the real-time recording of single unit spike train during the experiment. The entire data set includes neural data recorded from three different monkeys\u2013Monkey C, G and K, performing 12 types of individuated movements labeled by the moving finger number and by the first letter of the moving direction. Furthermore, all the trials are aligned such that the onset of the movement happens at 1 s. Therefore the ELM can be trained according to the given label and the onset moment."}, {"heading": "C. Neural Decoding Performance", "text": "We have tested the MLCP based PEU using the data set mentioned above. A multiple-output ELM with number of classes C = 12 is trained to identify the movement type of the\ntrial. An additional output is used to decode the onset time of movement. During training, the pre-recorded input spikes from biological neurons in M1 are sent to the MLCP the counter values of H are wirelessly transmitted to a PC where fmax and \u03b2 are calculated and communicated back. This process already includes non-idealities in the analog processor such as DNL of input DAC, non-linearity in CCO and early effect induced current addition errors\u2013hence, the learning takes these effects into account and corrects for them appropriately. Then, the MLCP can run autonomously during testing phase.\nWe present decoding results in a format similar to [22] for easy comparison wherever possible. For the first set of experiments, we use the normal training method T1 described in section II-A2. As shown in Fig. 11(a) with n = D = 30, the decoding accuracy of the 12 types of movements (the flexion and extension of the fingers and wrist in one hand) increases as L is increased, with a mean accuracy of 94.8% at L = 60. This trend is expected [19] since more number of random projections should allow better separation of the classes till the point when the amount of extra information for a new projection is negligible. Based on this result, we fix L = 60 for the rest of the experiments unless stated otherwise.\nNext, we explore the variation in performance as number of available neural channels (n) (or equivalently M1 neurons in this case) reduces while keeping L fixed at 60. Fig. 11(b) shows that an increase in accuracy from 85.4% to 91.7% can be obtained at n = 15, by using delayed samples as added features (TDBDI). Here, we have used only one earlier sample\u2013hence, p = 2 and the effective input dimension of the ELM is D = 2 \u00d7 n. With n = 40, L = 60 and p = 2, a decoding accuracy of 99.3% can be achieved. Next, to check\nthe robustness of the earlier result, the same experiment is performed using several different datasets, including individuated finger movement data from Monkey K, C and G and combined finger movement from Monkey K (12 individuate movements and 6 types of simultaneous movement of two fingers). The results of the MLCP with increasing M1 neurons, as shown in Fig. 11(c), is consistent with software result in [22]. The trend of increasing performance with more M1 neurons is expected\n0 0.02 0.04 0.06 0.08 0.1 0.12 0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate\nT ru\ne P\no si\nti v e\nR at\ne\nl =2, Tr=0 ms l =6, Tr=0 ms l =2, Tr=140 ms l =6, Tr=140 ms\u03b8=1.0\n\u03b8=0.6\n\u03b8=0.3\n(b)\nFig. 13: Measured movement onset decoding results: (a) A segment of 40 channel input spike trains is shown with real-time decoding output deciding when a movement onset happens and which tpye is this onset. (b) ROC curves of onset decoding.\n0 20 40 60 80 100 120\n0.4\n0.6\n0.8\n1\nNumber of Hidden Layer Neurons (L)\nD ec\nod in\ng A\ncc ur\nac y\nT1\nT2\nFig. 14: Advantage of Sparsity promoting training T2: The sparsity promoting method chooses best random projections and can reduce required number of hidden neurons by around 50%.\nsince it provides more information. The performance of the proposed MLCP is also robust across eight sample chips, as presented in Fig. 11(d) for the same experiment as in the last two cases.\nThe hidden layer output matrix H is reused to decode the onset time of finger movement using the regression capacity of the ELM. As mentioned earlier, only one more output node is added to the ELM. The trapezoidal membership function described in section II-B and shown in Fig. 13(a) is set to 1 around the time of 1 s to indicate the onset and set to 0 where there is definitely no movement. Figure 12 illustrates the finite state machine in the MCU to implement the post-processing\ndescribed in section II-B to obtain Gtrack from the primary output G. Optimal values of \u03bb = 6 and Tr = 140 ms can be found from the ROC curve shown in Fig. 13(b). The nature of the ROC curves are again very similar to the ones in [22]. With H reused, we achieve real-time combined decoding by detecting when there is a movement in the trial and labeling the predicted movement type when a movement onset is detected. This is illustrated by a snapshot of the developed GUI in Fig. 13(a), where three 2-s trials are shown with 40-channel input spike trains recorded from M1 region printed at the bottom part of the figure. Primary, post-processed output and predicted movement type are also shown in the top half of the figure. Lastly we show the benefits of the sparsity promoting training method, T2 described in section II-A2. To show the benefit of this method, we compare with the first experiment shown earlier in Fig. 11(a) where n = D = 30 and the number of hidden layer neurons L is varied to see its effect on performance. It can be seen that for the method T2, the decoding accuracy increases to approximately the maximum value of 94.8% attained by the method T1 for much fewer number of hidden layer neurons (L \u2248 30). This is possible because the sparsity promoting step of minimizing L1 norm of output weights chooses the most relevant random projections in the hidden layer. Thus, the new method T2 can reduce power dissipation by approximately 50% due to reduction in number of hidden layer neurons."}, {"heading": "D. Power Dissipation", "text": "Finally, we report the power consumption of the proposed MLCP for the 40 input channels, 60 hidden layer nodes, 12- class classification problem. The current drawn from analog and digital power supply pins were measured using a Keithley picoammeter. The power breakup is shown in Fig. 15. At the lowest value of AVDD=1.2V and DVDD=0.6V needed for robust operation, the total power dissipated is 414 nW with 54 nW from DVDD and 360 nW from AVDD. Performing 40 \u00d7 60 MAC in the current mirror array at 50 Hz rate of classification, the MLCP provides a 3.45 pJ/MAC and 8.3 nJ/classify performance. It is clear that the efficiency is limited by the fixed analog power that is amortized across the L hidden layer neurons and D\u00d7L current mirror multipliers. The\n10\nTABLE II: Comparison Table\nJSSC 2013 [29] JSSC 2007 [30] JSSC 2013 [31] ISSCC 2014 [32] This Work Technology 0.13 \u00b5m 0.5 \u00b5m 0.13 \u00b5m 0.13 \u00b5m 0.35 \u00b5m\nSupply voltage 0.85 V 4 V 1.2 V (digital) 3 V 0.6 V (digital) 1 V (analog) 1.2 V (analog)\nDesign style Digital Analog floating Mixed mode Analog floating Mixed mode gate gate\nAlgorithm SVM SVM Fuzzy logic Deep learning feature ELM feature with TDBDI Application EEG/ECG analysis Speech Recognition Image processing Autonomous sensing Neural Implant\nPower dissipation 136.5 \u00b5W 0.84 \u00b5W 57 mW 11.4 \u00b5W 0.4 \u00b5W Max input dimension 4001 14 14 8 1281\nEnergy efficiency 631 pJ/MAC2 0.8 pJ/MAC 1.4 pJ/MAC3 1 pJ/op4 5.2/1.46 pJ/MAC 5\nResolution 16 b 4.5 b 6 b 7.5 b 7/14 b6\nClassification rate 0.5-2 Hz 40 Hz 5 MHz 8.3 kHz 50 Hz 1 can be further extended by reusing input channels at the expense of classification rate 2 assuming 1000 support vectors 3 1024 6-bit multiply at 10 MHz consumes 14 mW. 4 The operations are much simpler than a MAC. 5 5.2 pJ/MAC includes both analog and digital power for D = 40, L = 60 and C = 12. In reality, analog power is amortized across all multiplies\nand the peak efficiency of 1.46 pJ/MAC is attainable for D = L = 128 for the same value of C. See section IV-D for details. 6 Each multiply is 7 bit accurate due to SNR limitation while the output quantization in the CCO-ADC has 14 bits for dynamic range.\nfundamental limit of this architecture is the power dissipation of the CCO and current mirror array which is limited to 0.45 pJ/MAC.\nIn contrast, recently reported 16-bit digital multipliers consume 16 \u2212 70 pJ/MAC [33] [34] [35] [36] where we ignore the power consumed by the adder for simplicity. We have also implemented near threshold digital array multipliers in 65nm CMOS operating at 0.65 V that resulted in energy efficiency of 11 pJ/MAC confirming the much lower energy attainable by analog solutions over digital ones. Moreover, implementing the MLCP computations in digital domain would incur further energy cost due to memory access (for getting the weight values) and clocking which are ignored here.\nSince we implement the operation of second stage in digital domain, we need C \u00d7 L multiplications per classification. For the case of L = 60 and C = 12 described above and energy cost of 11 pJ/MAC for digital multiplies, the total energy cost of second stage operation is 7.92 nJ/classify. Hence, the total energy/classification becomes 16.22 nJ and the combined energy/operation increases to 5.2 pJ/MAC. For peak energy efficiency, we consider D = 128, L = 128 and C = 12 resulting in a net energy/computation of 1.46 pJ/MAC including both stages."}, {"heading": "E. Discussion", "text": "1) Comparison: Our MLCP is compared with other recently reported machine learning systems in Table II. Compared to the digital implementation of SVM in [29], our implementation achieves far less energy per MAC due to the analog implementation. [30], [31] and [32] achieve good energy efficiency similar to our method by using analog computing. [31] uses a multiplying DAC (MDAC) to perform the multiplication by weights\u2013however, they have only 6 bit resolution in the multiply and also the MDAC occupies much larger area than the single transistor we use for multiplications. [30] and [32] use analog floating-gate transistors for the multiplication. Compared to these, our single transistor multiplier takes lesser area (no capacitors that are needed in floating-\nWinCNT\nWinCNT\nC C\nO C\nN T\nC C\nO C\nN T\n25 \u03bcm\n24 \u03bcm\n0.4\u00d70.35 \u03bcm 2\nFig. 16: Array Layout: The area of the current IC is limited by the pitch of the CCO and WINCNT circuits even though the actual area of the current mirrors (0.4\u00d7 0.35\u00b5m2) are very small.\ngates), does not require high voltages for programming charge and allow for digital correction of errors due to the digital output.\n2) Area Limits: Using a single transistor for multiplication in the first layer should provide area benefits over other schemes. The current layout (Fig. 16) was done due to its simple connection patterns and is not optimized. It can be seen that the actual area of a unit transistor in the array (0.4\u00d7 0.35\u00b5m2) is much less than the area of an unit cell in the layout which is limited by the pitch of the CCO and the window counter circuits. Moving to a highly scaled process or folding the placement of the output CCO layer to be parallel to the input window counter circuits would enable large reduction (\u2248 80X) in the area of the current mirror array. The ultimate limit in terms of area for this architecture stems from the area of capacitors\u2013for this 128 input, 128 output architecture, the total capacitor area is 0.132mm2.\n3) Data rate requirements: When used in an implant with offline training, the MLCP can reduce transmission data rate drastically. First, for direct transmission of 100 channel data sampled at 20kHz with 10 bit resolution, the required data rate is 20Mbps. This massive data rate can be reduced partially by including spike sorting [11]. In this case, assuming 8 bit address encoding a maximum of 256 biological neurons each firing at a rate fbio, the data rate to be transmitted for\n11\na conventional implant without neural decoder is given by Rconv = 8\u00d7 256\u00d7 fbio. As an example, with fbio = 100Hz, Rconv = 204kbps. This can be reduced even further by integrating the decoder as proposed here. For the proposed case, the output of the decoder is obtained at a rate fdeco. During regular operation after training, the data rate for C classes is given by Rprop,test = fdeco \u00d7 \u2308log2(C)\u2309. As an example, for the case described in section IV-C with fdeco = 50 Hz and C = 13, Rprop,test = 200 bps. This example, shows the potential for thousand fold data rate reductions over spike sorting by integrating the decoder in the implant.\nFrom the viewpoint of power dissipation, the analog front end and spike detection can be accomplished within a power budget of 1\u00b5W per channel [37] [38] [5]. Assuming a transmission energy of \u2248 50pJ/bit from recently reported wireless transmitters for implants [39]\u2013[41], the power dissipation for raw data rates of 200kbps/channel and compressed data rates of 2kbps/channel after spike sorting are 10\u00b5W and 0.1\u00b5W respectively. Hence, the power for wireless transmission is a bottleneck for systems transmitting raw data. For systems with spike sorting in the implant, this power dissipation is not a bottleneck. However, the power/channel needed for the spike sorter is about 5\u00b5W. In comparison, if our decoder operates directly on the spike detector output, it can provide compression at a power budget of < 0.01\u00b5W/channel. This would result in a total power dissipation/channel of \u2248 1\u00b5W in our case compared to \u2248 6\u00b5W in the case of spike sorting\u2013 a 6X reduction. There is a lot of evidence that the decoding algorithms can work on the spike detector output [24]; in fact, it is believed that this will make the system more robust for long term use. This will be a subject of our future studies.\nEven if the decoder is explanted, a MCU cannot provide sufficient throughput to support advanced decoding algorithms while FPGA based systems consume a large baseline power. A custom MLCP based solution provides an attractive combination of low-power and high throughput operation when paired with a flexible MCU for control flow.\n4) Normalization for Increased Robustness: The variation of temperature is not a big concern in the case of implantable electronics since body temperature is well regulated. However, variation of power supply voltage can be a concern. A normalization method can be applied to the hidden layer output for reducing its variation due to power supply fluctuation, at the cost of additional computation. The normalization proposed here can be expressed by:\nhj,norm = hj\n\u2211L j=0 hj/ \u2211D i=0 xi\n. (7)\nThe rationale behind the proposed normalization is that the effect of power supply fluctuation on the hidden layer output can be modelled as multiplication factor in hidden layer output equation. As analyzed before, the output of the jth hidden layer node can be formulated as: hj =\nIin,j Cf\u00d7V DD tcnt, where\nIin,j is the input current of the jth hidden layer node and tcnt is counting window length. Since Iin,j is proportional to the strength of input vector x = [x1, x2...xD], we can model the relation between the input vector and hidden layer output as: hj = Kj\u03b1(T, V DD) \u2211D i=0 xi, where the variation part\nDVDD (V) 2.5\n2.5\n3.5\n4.5\n1 2 DVDD (V) 1.5 2.5\n2.5\n3\n3.5\n4\n4.5 x=8\n(a) (b)\nx=10\n150\n200\n250\n300\n350\n400\n450\n500\n550\n600\n650\nh i\nN o rm\nal iz\ned h i\n150\n200\n250\n300\n350\n400\n450\n500\n550\n600\n650\nh i\nN o rm\nal iz\ned h i\n1 21.5\n3\n4\nhi Normalized hi\nhi Normalized hi\nBlue lines are original hidden layer output from SPICE simulation, while green dashed lines are normalized output in both (a) and (b). The input x in (a) and (b) are 8 and 10 respectively.\nFig. 17: Normalization to reduce variation:\nis a multiplicative term \u03b1(T, V DD), and Kj lumps up the constant part of the path gain from input to jth hidden layer output. It is reasonable to assume that \u03b1(T, V DD) is the same across different nodes, since fluctuation of power supply is a global effect on chip scale. Hence, it can be cancelled by the proposed normalization as:\nhj,norm = hj\n\u2211L j=0 hj/ \u2211D i=0 xi\n= Kj\u03b1(T, V DD)\n\u2211D\ni=0 xi \u2211L\nj=0(Kj\u03b1(T, V DD) \u2211D i=0 xi)/ \u2211D i=0 xi\n= Kj\n\u2211L\nj=0 Kj\nD \u2211\ni=0\nxi.\n(8)\nSimulation results are presented here to verify the proposed method of normalization. The original hidden layer outputs (L = 3) are obtained by SPICE simulations where DVDD is swept from 0.6 V to 2.5 V and input x (D = 1) changes from 8 to 10. Original and normalized values of one of the hidden layer outputs are compared in Fig. 17. As can be observed here, the normalized output (in green dashed lines) varies significantly less due to variation of DVDD than the original output (in blue solid lines). The hardware cost for this normalization is D + L additions and L divisions. Assuming similar costs for division and multiplication, the normalization does not incur much overhead if C >> 1 since L \u00d7 C multiplications are required by the second stage anyway.\n5) Considerations for Long Term Implants: When using this MLCP based decoder in long term implants, we have to consider issues of parameter drift over several time scales. Over long term of days, aging of the circuits in MLCP or probe impedance change due to gliosis and scarring may change performance. This is typically countered by retraining the decoder every day [24]. Such retraining has allowed decoders to operate at similar level of performance over years. Over shorter time scales, any variation not sufficiently quenched by the normalization method described earlier can be explicitly calibrated by having digital multiplication of coefficients for every input and output channel. These can be determined periodically by injecting calibration inputs and observing the output of the CCO.\nAnother type of training\u2013referred to as decoder retraining [42], [43] are needed to take into account change in neural\n12\nstatistics during closed loop experiments. The training done here may be thought of as open loop training for initialization of coefficents of second stage of ELM. Next, the experiment has to be redone with closed loop feedback and new training data set has to be generated for retraining the second layer weights. After several such iterations, the final set of weights of second layer will be obtained."}, {"heading": "V. CONCLUSION", "text": "We presented a MLCP in 0.35\u00b5m CMOS with a die area of 4.95\u00d74.95mm2 and a 7.4cm\u00d75.1cm PEU based on the proposed MLCP that achieves real-time motor intention decoding in an efficient way. Implementing the ELM algorithm, the MLCP utilizes massive parallel low power analog computing and hardware reuse, achieving a power consumption of 0.4 \u00b5W at 50 Hz classification rate, resulting in an energy efficiency of 3.45 pJ/MAC. Learning in the second stage also compensates for non-idealities in the analog processor. Furthermore, It includes time-delayed sample based dimension increase feature for enhancing decoding performance when number of recorded neurons are limited. A sparsity promoting training method is shown to reduce the number of hidden layer neurons and output weights by \u2248 50%. We demonstrated the operation of the IC for decoding individuated finger movements using recordings of M1 neurons. However, the ELM algorithm used in the decoder is quite general and has been shown to be an universal approximator and equivalent to SVM or multi-layer perceptrons [20]. Hence, our MLCP can also be used for other decoding applications requiring regression or classification computations. Higher dimensions of inputs and hidden layers can be handled by making a larger IC and also by reusing the same hidden layer several times. In either case, power dissipation increases but not energy/compute. Higher input dimensions can be accommodated at same power by reducing the bias current input of the splitter DACs in input channels [27]. Increase of hidden layer neurons however do incur a proportional power increase. Given that the power requirement of the current decoder is > 100X lower than the AFE, we can easily extend it to handle many more input and output channels."}, {"heading": "VI. ACKNOWLEDGEMENT", "text": "The authors would like to thank Dr. Nitish Thakor for providing neural recording data."}], "references": [{"title": "A low-power integrated circuits for a wireless 100-electrode neural recording system", "author": ["R.R. Harrison", "P.T. Watkins", "R.J. Kier", "R.O. Lovejoy", "D.J. Black", "B. Greger", "F. Solzbacher"], "venue": "IEEE Journal of Solid- State Circuits, vol. 42, no. 1, pp. 123\u2013133, Jan. 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Low-power circuits for brain-machine interfaces", "author": ["R. Sarpeshkar", "W. Wattanapanitch", "S.K. Arfin", "B.I. Rapoport", "S. Mandal", "Michael W. Baker", "M.S. Fee", "S. Musallam", "R.A. Adersen"], "venue": "IEEE Transactions on Biomedical Circuits and Systems, vol. 2, no. 3, pp. 173\u2013183, Sept. 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "The 128-channel fully differnetial digital integrated neural recording and stimulation interface", "author": ["F. Shahrokhi", "K. Abdelhalim", "D. Serletis", "P.L. Carlen", "R. Genov"], "venue": "IEEE Transactions on Biomedical Circuits and Systems, vol. 4, no. 3, pp. 149\u2013161, Jun. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "A Digitally Assisted, Signal Folding Neural Recording Amplifier", "author": ["Y. Chen", "A. Basu", "L. Liu", "X. Zou", "R. Rajkumar", "G. Dawe", "M. Je"], "venue": "IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 8, pp. 528\u2013542, August 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A 1 V, Compact, Current-Mode Neural Spike Detector with Detection Probability Estimator in 65 nm CMOS", "author": ["Yao Enyi", "A. Basu"], "venue": "IEEE ISCAS, May 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A micro-power neural spike detector and feature extractor in .13um CMOS", "author": ["J. Holleman", "A. Mishra", "C. Diorio", "B. Otis"], "venue": "IEEE Custom Integrated Circuits Conference (CICC), 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "VLSI architecture of NEO spike detection with noise shaping filter and feature extraction using informative samples", "author": ["L. Hoang", "Y. Zhi", "W. Liu"], "venue": "IEEE EMBC, Sept. 2009, pp. 978\u2013981.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An Ultra Low-Power CMOS Automatic Action Potential Detector", "author": ["B. Gosselin", "M. Sawan"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 17, no. 4, pp. 346\u2013353, Aug. 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A biomedical multiprocessor SoC for closed-loop neuroprosthetic applications", "author": ["Tung-Chien Chen", "Kuanfu Chen", "Zhi Yang", "K. Cockerham", "Wentai Liu"], "venue": "Solid-State Circuits Conference - Digest of Technical Papers, 2009. ISSCC 2009. IEEE International, Feb 2009, pp. 434\u2013 435,435a.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A 130-uW, 64-Channel Neural Spike-Sorting DSP chip", "author": ["V. Karkare", "S. Gibson", "D. Markovic"], "venue": "IEEE Journal of Solid-State Circuits, vol. 46, no. 5, pp. 1214\u201322, May 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A 75-uW, 16-Channel Neural Spike-Sorting Processor With Unsupervised Clustering", "author": ["V. Karkare", "S. Gibson", "D. Markovic"], "venue": "IEEE Journal of Solid-State Circuits, vol. 48, no. 9, pp. 2230\u20138, Sept 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Decoding individuated finger movements using volumeconstrained neuronal ensembles in the M1 hand area", "author": ["S. Acharya", "F. Tenore", "V. Aggarwal", "R. Etienne-Cummings", "M. Schieber", "N. Thakor"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 16, pp. 15\u201323, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "A brain-machine interface enables bimanual arm movements in monkeys", "author": ["P. Ifft", "S. Shokur", "Z. Li", "M. Lebedev", "M. Nicolelis"], "venue": "Science: Translational Medicine, vol. 5, pp. 1\u201313, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Reach and grasp by people with tetraplegia using a neurally controlled robotic arm", "author": ["L. Hochberg", "D. Bacher", "B. Jarosiewicz", "N. Masse", "J. Simeral", "J. Vogel", "S. Haddain", "J. Liu", "S. Cash", "P. der Smagt", "J. Donoghue"], "venue": "Nature, vol. 485, pp. 372\u2013375, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "How advances in neural recording affect data analysis", "author": ["I.H. Stevenson", "K.P. Kording"], "venue": "Nature Neuroscience, vol. 14, pp. 139\u2013142, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "A biomimetic adaptive algorithm and low-power architecture for implantable neural decoders", "author": ["B. Rapoport", "W. Wattanapanitch", "H. Penagos", "S. Musallam", "R. Andersen", "R. Sarpeshkar"], "venue": "31st Annual International Conference of the IEEE EMBS, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient universal computing architectures for decoding neural activity", "author": ["B. Rapoport", "L. Turicchian", "W. Wattanapanitch", "T. Davidson", "R. Sarpeshkar"], "venue": "PLoS ONE, vol. 7, pp. e42492, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A 128 Channel 290 GMACs/W Machine Learning Based Co-Processor for Intention Decoding in Brain Machine Interfaces", "author": ["Chen Yi", "Y. Enyi", "A. Basu"], "venue": "IEEE ISCAS, May 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme Learning Machines: Theory and Applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, vol. 70, pp. 489\u2013501, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["Guang-Bin Huang", "Hongming Zhou", "Xiaojian Ding", "Rui Zhang"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 42, no. 2, pp. 513\u2013529, April 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of Statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Asynchronous decoding of dexterous finger movements using M1 neurons", "author": ["V. Aggarwal", "S. Acharya", "F. Tenore", "H. Shin", "R. Etienne-Cummings", "M. Schieber", "N. Thakor"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 16, pp. 3\u201314, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "Proceedings of Neural Information Processing Systems (NIPS), 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Information Systems Opportunities in Brain-Machine Interface DecodersInformation Systems Opportunities in Brain-Machine Interface Decoders", "author": ["J.C. Kao", "S.D. Stavisky", "D. Sussillo", "P. Nuyujukian", "K.V. Shenoy"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 666\u2013682, May 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term time series pprediction using OP-ELM", "author": ["A. Grigorievskiy", "Y. Miche", "A. Ventela", "E. Severin", "A. Lendasse"], "venue": "Neural Networks, vol. 51, pp. 50\u201356, March 2014.  13", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Silicon Spiking Neurons for Hardware Implementation of Extreme Learning Machines", "author": ["A. Basu", "S. Shuo", "H. Zhou", "M. Lim", "G. Huang"], "venue": "Neurocomputing, vol. 102, pp. 125\u2013134, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Computation using Mismatch: Neuromorphic Extreme Learning Machines", "author": ["Y. Enyi", "S. Hussain", "A. Basu"], "venue": "IEEE Biomedical Circuits and Systems Conference (BioCAS), Rotterdam, 2013, pp. 294\u20137.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Op-elm: Optimally pruned extreme learning machine", "author": ["Yoan Miche", "A. Sorjamaa", "P. Bas", "O. Simula", "C. Jutten", "A. Lendasse"], "venue": "Neural Networks, IEEE Transactions on, vol. 21, no. 1, pp. 158\u2013162, Jan 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "A low-power processor with configurable embedded machine-learning accelerators for high-order and adaptive analysis of medical-sensor signals", "author": ["Kyong Ho Lee", "N. Verma"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 48, no. 7, pp. 1625\u20131637, July 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Sub-microwatt analog vlsi trainable pattern classifier", "author": ["S. Chakrabartty", "G. Cauwenberghs"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 42, no. 5, pp. 1169\u20131179, May 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A 57 mw 12.5 uj/epoch embedded mixed-mode neuro-fuzzy processor for mobile real-time object recognition", "author": ["Jinwook Oh", "Gyeonghoon Kim", "Byeong-Gyu Nam", "Hoi-Jun Yoo"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 48, no. 11, pp. 2894\u20132907, Nov 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "A 1TOPS/W Analog Deep Machine-Learning Engine with Floating-Gate Storage in 0.13um CMOS", "author": ["J. Lu", "S. Young", "I. Arel", "J. Holleman"], "venue": "ISSCC Dig. Tech. Papers, 2014, pp. 504\u20135.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "A New Redundant Binary Booth Encoding for Fast 2n-Bit Multiplier Design", "author": ["Y. He", "C-H. Chang"], "venue": "IEEE Transactions on Circuits and Systems-I, vol. 56, no. 6, pp. 1192\u20131201, June 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "A Micropower Low-Voltage Multiplier With Reduced Spurious Switching", "author": ["K-S Chong", "B-H Gwee", "J.S. Chang"], "venue": "IEEE Transactions on VLSI, vol. 13, no. 2, pp. 255\u201365, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Razor Based Programmable Truncated Multiply and Accumulate, Energy-Reduction for Efficient Digital Signal Processing", "author": ["M. La Guia de Solaz", "R. Conway"], "venue": "IEEE Transactions on VLSI, vol. 23, no. 1, pp. 189\u201393, Jan 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Truncated Binary Multipliers With Variable Correction and Minimum Mean Square Error", "author": ["N. Petra", "D. De Caro", "V. Garofalo", "E. Napoli", "A.G.M. Strollo"], "venue": "IEEE Transactions on Circuits and Systems-I, vol. 57, no. 6, pp. 1312\u201325, Jun 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A 0.45V 100-channel neural-recording IC with sub-uW/channel consumption in 0.18um CMOS", "author": ["D. Han", "Y. Zheng", "R. Rajkumar", "G. Dawe", "M. Je"], "venue": "IEEE International Solid-State Circuits Conference, 2013, pp. 290\u2013291.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A 0.7 V, 40 nW Compact, Current-Mode Neural Spike Detector in 65 nm CMOS", "author": ["Y. Enyi", "Chen Yi", "Arindam Basu"], "venue": "IEEE Transactions on Biomedical Circuits and Systems, Early Access 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "A 2.4 GHz ULP reconfigurable asymmetric transceiver for single-chip wireless neural recording IC", "author": ["J. Tan", "W.S. Liu", "C.H. Heng", "Y. Lian"], "venue": "IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 4, pp. 497\u2013509, Aug 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "A 50-Mb/s CMOS QPSK/O-QPSK transmitter employing injection locking for direct modulation", "author": ["S.X. Diao", "Y.J. Zheng", "Y. Gao", "S.J. Cheng", "X.J. Yuan", "M.Y. Je"], "venue": "IEEE Transactions on Microwave Theory and Techniques, vol. 60, no. 1, pp. 120\u2013130, Jan 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "A 128-Channel 6 mW Wireless Neural Recording IC With Spike Feature Extraction and UWB Transmitter", "author": ["M. Chae", "Z. Yang", "M. Yuce", "L. Hong", "W. Liu"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 17, no. 4, pp. 312\u2013321, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Intention estimation in brain machine interfaces", "author": ["J.M. Fan", "P. Nuyujukian", "J.C. Kao et. al."], "venue": "Journal of Neuroengg., vol. 11, no. 1, 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Closedloop decoder adaptation on intermediate time-scales facilitates rapid BMI performance improvements independent of decoder initialization conditions", "author": ["A.L. Orsborn", "S. Dangi", "H.G. Moorman", "J.M. Camena"], "venue": "IEEE Trans. Neural Syst. Rehabil. Engg., vol. 20, no. 4, pp. 468\u2013477, July 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Significant effort has been dedicated to develop energy efficient neural recording channel in recent years for long-term operation of the implanted devices [1] [2] [3] [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "Significant effort has been dedicated to develop energy efficient neural recording channel in recent years for long-term operation of the implanted devices [1] [2] [3] [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "Significant effort has been dedicated to develop energy efficient neural recording channel in recent years for long-term operation of the implanted devices [1] [2] [3] [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "Significant effort has been dedicated to develop energy efficient neural recording channel in recent years for long-term operation of the implanted devices [1] [2] [3] [4].", "startOffset": 168, "endOffset": 171}, {"referenceID": 4, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "Some recent solutions have also integrated the AP detection [5] [6] [7] [8] and spike sorting features [9] [10] [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Though various elaborate models and methods of motor intention decoding have been developed in past decades with the goal of achieving high decoding performance [12] [13] [14], the state-of-the art neural signal decoding are mainly conducted on PC consuming a considerable amount of power and making it impractical for the long-term use.", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "Though various elaborate models and methods of motor intention decoding have been developed in past decades with the goal of achieving high decoding performance [12] [13] [14], the state-of-the art neural signal decoding are mainly conducted on PC consuming a considerable amount of power and making it impractical for the long-term use.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Though various elaborate models and methods of motor intention decoding have been developed in past decades with the goal of achieving high decoding performance [12] [13] [14], the state-of-the art neural signal decoding are mainly conducted on PC consuming a considerable amount of power and making it impractical for the long-term use.", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "Furthermore, integrating the neural decoding algorithm with the neural recording device is also desired to reduce the wireless data transmission rate and make the implanted BMI solution scalable as required in the future [15].", "startOffset": 221, "endOffset": 225}, {"referenceID": 15, "context": "A low-power motor intention architecture using analog computing is proposed in [16], featuring an active filtering with massive parallel computing through low power analog filters and memories.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "A more recent work proposes a universal computing architecture for neural signal decoding [17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Some initial results of this work were presented in [18].", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Extreme Learning Machine 1) Network Architecture: The machine learning algorithm used in this design is Extreme Learning Machine (ELM) proposed in [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "More details about the algorithm can be found in [19], [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "More details about the algorithm can be found in [19], [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "2) Training Methods: The special property of the ELM is that w can be random numbers from any continuous probability distribution and remains unchanged after initiation [19], while only \u03b2 needs to be trained and stored with high resolution.", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "For simplicity, we show the solution of weights for one output ok\u2013the same method can be extended to other output weights as well and can be represented in a compact matrix equation [19].", "startOffset": 182, "endOffset": 186}, {"referenceID": 20, "context": "The L1 norm minimization step in T2 however has to be performed using standard optimization algorithms like LARS [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "Neural Decoding The neural decoding algorithm we use is inspired by the method in [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "SVM, AdaBoost) since the hidden nodes in ELM only create random projections that are not fine tuned [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Next, we give an overview of the decoding algorithm while the reader is pointed to [22] for more details.", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Hence, we can define the firing rate ri of i-th neuron at time instant tk as ri(tk) = \u222b tk tk\u2212Tw p(t) where Ts = 20 ms and Tw = 100 ms following [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "Similar to [22], this output is trained for regression\u2013the target is a trapezoidal fuzzy membership function which gradually rises from 0 to 1 representing the gradual evolution of biological neural activity.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "2) Time delay based dimension increase (TDBDI): A common problem in long-term neural recording is the loss of information from electrodes over time due to tissue reactions such as gliosis, meningitis or mechanical failures [24].", "startOffset": 223, "endOffset": 227}, {"referenceID": 24, "context": "To retain the quality of decoding, we propose a method commonly used in time series analysis\u2013the use of information from earlier time points [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 21, "context": "This is a novel algorithmic feature in our work compared to [22].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "The input to the MLCP comes from spike sorting that can be performed on the DSP [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "In some cases, spike sorting may not be needed and spike detection may be the only required pre-processing [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "This is similar to the concepts described in [26] [27].", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "This is similar to the concepts described in [26] [27].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "The advantage of choosing this nonlinearity is that it can be digitally set and also some neurons can be configured to be linear as well to achieve good performance in linearly separable problems [28].", "startOffset": 196, "endOffset": 200}, {"referenceID": 21, "context": "Experiment The neural data used to verify the decoding performance of the proposed design is acquired in a monkey finger movement experiment described in detail in [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "We present decoding results in a format similar to [22] for easy comparison wherever possible.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "This trend is expected [19] since more number of random projections should allow better separation of the classes till the point when the amount of extra information for a new projection is negligible.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "11(c), is consistent with software result in [22].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "The nature of the ROC curves are again very similar to the ones in [22].", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "JSSC 2013 [29] JSSC 2007 [30] JSSC 2013 [31] ISSCC 2014 [32] This Work Technology 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "JSSC 2013 [29] JSSC 2007 [30] JSSC 2013 [31] ISSCC 2014 [32] This Work Technology 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 30, "context": "JSSC 2013 [29] JSSC 2007 [30] JSSC 2013 [31] ISSCC 2014 [32] This Work Technology 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "JSSC 2013 [29] JSSC 2007 [30] JSSC 2013 [31] ISSCC 2014 [32] This Work Technology 0.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "In contrast, recently reported 16-bit digital multipliers consume 16 \u2212 70 pJ/MAC [33] [34] [35] [36] where we ignore the power consumed by the adder for simplicity.", "startOffset": 81, "endOffset": 85}, {"referenceID": 33, "context": "In contrast, recently reported 16-bit digital multipliers consume 16 \u2212 70 pJ/MAC [33] [34] [35] [36] where we ignore the power consumed by the adder for simplicity.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "In contrast, recently reported 16-bit digital multipliers consume 16 \u2212 70 pJ/MAC [33] [34] [35] [36] where we ignore the power consumed by the adder for simplicity.", "startOffset": 91, "endOffset": 95}, {"referenceID": 35, "context": "In contrast, recently reported 16-bit digital multipliers consume 16 \u2212 70 pJ/MAC [33] [34] [35] [36] where we ignore the power consumed by the adder for simplicity.", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": "Compared to the digital implementation of SVM in [29], our implementation achieves far less energy per MAC due to the analog implementation.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "[30], [31] and [32] achieve good energy efficiency similar to our method by using analog computing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[30], [31] and [32] achieve good energy efficiency similar to our method by using analog computing.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "[30], [31] and [32] achieve good energy efficiency similar to our method by using analog computing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "[31] uses a multiplying DAC (MDAC) to perform the multiplication by weights\u2013however, they have only 6 bit resolution in the multiply and also the MDAC occupies much larger area than the single transistor we use for multiplications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] and [32] use analog floating-gate transistors for the multiplication.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[30] and [32] use analog floating-gate transistors for the multiplication.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "This massive data rate can be reduced partially by including spike sorting [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "From the viewpoint of power dissipation, the analog front end and spike detection can be accomplished within a power budget of 1\u03bcW per channel [37] [38] [5].", "startOffset": 143, "endOffset": 147}, {"referenceID": 37, "context": "From the viewpoint of power dissipation, the analog front end and spike detection can be accomplished within a power budget of 1\u03bcW per channel [37] [38] [5].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "From the viewpoint of power dissipation, the analog front end and spike detection can be accomplished within a power budget of 1\u03bcW per channel [37] [38] [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 38, "context": "Assuming a transmission energy of \u2248 50pJ/bit from recently reported wireless transmitters for implants [39]\u2013[41], the power dissipation for raw data rates of 200kbps/channel and compressed data rates of 2kbps/channel after spike sorting are 10\u03bcW and 0.", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "Assuming a transmission energy of \u2248 50pJ/bit from recently reported wireless transmitters for implants [39]\u2013[41], the power dissipation for raw data rates of 200kbps/channel and compressed data rates of 2kbps/channel after spike sorting are 10\u03bcW and 0.", "startOffset": 108, "endOffset": 112}, {"referenceID": 23, "context": "There is a lot of evidence that the decoding algorithms can work on the spike detector output [24]; in fact, it is believed that this will make the system more robust for long term use.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "This is typically countered by retraining the decoder every day [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 41, "context": "Another type of training\u2013referred to as decoder retraining [42], [43] are needed to take into account change in neural", "startOffset": 59, "endOffset": 63}, {"referenceID": 42, "context": "Another type of training\u2013referred to as decoder retraining [42], [43] are needed to take into account change in neural", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "However, the ELM algorithm used in the decoder is quite general and has been shown to be an universal approximator and equivalent to SVM or multi-layer perceptrons [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "Higher input dimensions can be accommodated at same power by reducing the bias current input of the splitter DACs in input channels [27].", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning co-processor in 0.35\u03bcm CMOS for motor intention decoding in brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 3.45 pJ/MAC at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same co-processor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by \u2248 2X .", "creator": "LaTeX with hyperref package"}}}