{"id": "1509.08112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Feature Selection for classification of hyperspectral data by minimizing a tight bound on the VC dimension", "abstract": "hyperspectral data consists of large number of features areas which require sophisticated analysis to manually be extracted. a popular approach to reduce computational sample cost, facilitate continuous information density representation processing and accelerate knowledge discovery is to intentionally eliminate bands that don't improve the classification and analysis selection methods being applied. in particular, algorithms that perform band elimination should be designed to take advantage entirely of the specifics of the classification method being used. this paper employs a recently proposed filter - feature - selection algorithm based on minimizing a tight bound on the vc dimension. we have successfully applied this algorithm to broadly determine a reasonable discrete subset of bands without any user - defined stopping criteria on widely used hyperspectral images and demonstrate that this method outperforms state - of - the - art encryption methods in terms of both sparsity of feature set sizes as also well as accuracy of software classification. \\ and end { abstract }", "histories": [["v1", "Sun, 27 Sep 2015 17:36:18 GMT  (537kb,D)", "http://arxiv.org/abs/1509.08112v1", "basic papers are onthis http URL"]], "COMMENTS": "basic papers are onthis http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["phool preet", "sanjit singh batra", "jayadeva"], "accepted": false, "id": "1509.08112"}, "pdf": {"name": "1509.08112.pdf", "metadata": {"source": "CRF", "title": "Feature Selection for classification of hyperspectral data by minimizing a tight bound on the VC dimension", "authors": ["Phool Preet", "Sanjit Singh Batra"], "emails": ["jayadeva@ee.iitd.ac.in"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn recent years, hyperspectral image analysis has gained widespread importance among the remote sensing community. Hyperspectral sensors capture image data over hundreds of contiguous spectral channels (termed as bands), covering a broad spectrum of wavelengths(0.4-2.5 \u00b5m). Each hyperspectral image\u2019s scene is represented as an image cube. Hyperspectral data is increasingly becoming a valuable tool in many areas such as agriculture, mineralogy, surveillance, chemical imaging and automatic target recognition. A common task in such applications is to classify hyperspectral images. The abundance of information provided by hyperspectral data can be leveraged to enhance the ability to classify and recognize materials. However, the high dimensionality of hyperspectral data presents several obstacles. Firstly, it increases the computational complexity of classification. Further, it has been noted that highly correlated features have a negative impact on classification accuracy [1]. Another quandary often observed in the classification of hyperspectral data is the Hughes effect, which posits that in the presence of a limited number of training samples, the addition of features may have a considerable negative impact on the accuracy of classification. Therefore, dimensionality reduction is often employed in hyperspectral data analysis to reduce computational complexity and improve classification accuracy.\nDimensionality reduction methods can be divided into two broad categories: feature extraction and feature selection. Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4]. Feature selection methods, on the other hand, attempt to identify a subset of features that contain the fundamental characteristics of the data. Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].\nThis paper explores the application of a novel feature selection method based on minimizing a tight bound on the VC dimension [8], on hyperspectral data analysis. We present a comparison with various state-of-the-art feature selection methods on benchmark hyperspectral datasets. We used the Support Vector Machine (SVM) classifier [9] to assess the classification accuracy, following feature selection. Rest of the paper is organized as follows. Section II briefly describes the related work and background. In section III, the various feature selection methods used in this paper are described. Section IV describes the datasets used, the experimental setup and the results obtained on benchmark hyperspectral datasets."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "Dimensionality reduction prior to classification is advantageous in hyperspectral data analysis because the dimensionality of the input space greatly affects the performance of many supervised classification methods [7]. Further, there is a high likelihood\nar X\niv :1\n50 9.\n08 11\n2v 1\n[ cs\n.L G\n] 2\n7 Se\nof redundancy in the features and it is possible that some features contain less discriminatory information than others. Moreover, the high-dimensionality imposes requirements for storage space and computational load. The analysis in [1] supports this line of reasoning and suggests that feature selection may be a valuable procedure in preprocessing hyperspectral data for classification by the widely used SVM classifier.\nIn hyperspectral image analysis, feature selection is preferred over feature extraction for dimensionality reduction [1], [10]. Feature extraction methods involve transforming the data and hence, crucial and critical information may be compromised and distorted. In contrast, feature selection methods strive to discover a subset of features which capture the fundamental characteristics of the data, while possessing sufficient capacity to discriminate between classes. Hence, they have the advantage of preserving the relevant original information of the data.\nThere are various studies which establish the usefulness of feature selection in hyperspectral data classification. [1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14]. In [6], a band prioritization scheme based on Principal Component Analysis (PCA) and classification criterion is presented. Mutual information is a widely used quantity in various feature selection methods. In a general setting, features are ranked based on the mutual information between the spectral bands and the reference map(also known as the ground truth). In [7], mutual information is computed using the estimated reference map obtained by using available a priori knowledge about the spectral signature of frequently-encountered materials.\nRecently, Brown et al [15] have presented a framework for unifying many information based feature selection selection methods. Based on their results and suggestions we have chosen the set of feature selection methods that they recommend outperform others, in various situations, which are elaborated in the next section for the purposes of our analysis. In [8] a feature selection method based on minimization of a tight bound on the VC dimension is presented. This paper presents the first application of this novel method to hyperspectral data analysis."}, {"heading": "III. FEATURE SELECTION METHODS", "text": ""}, {"heading": "A. PCA based Feature Selection", "text": "Principal Component Analysis (PCA) is one of the most extensively used feature selection method. It transforms the data in such a way that the projections of the transformed data(termed as the principal components) exhibit maximal variance. Chein et al. [6] presents a band prioritization method based on Principal Component Analysis. For our experiments, we consider the features obtained from PCA to be the eigenvectors sorted by their corresponding eigenvalues.\nB. Information Theoretic Feature Selection\nFeature selection techniques can be broadly divided into two categories: classifier-dependent(wrapper and embedded methods) and classifier-independent (filter methods). Wrapper methods rank the features based on the accuracy of a particular classifier. They have the disadvantage of being computationally expensive and classifier-specific. Embedded methods exploit the structure of particular classes of learning models to guide the feature selection process. In contrast, Filter methods evaluate statistics of the data, independent of any classifier and define a heuristic scoring criterion(relevance index). This scoring criterion is a measure of how useful a feature can be, when input to a classifier.\nMRMR: The Minimum-Redundancy Maximum-Relevance criterion was proposed by Peng et al. [13]. Let X be our feature vector and Y is training label then mRMR criterion is given by\nJmrmr(Xk) = I(Xk;Y )\u2212 1 |S| \u2211 j\u2208S I(Xk;Xj) (1)\nI(\u00b7; \u00b7) denotes the mutual information and S is subset of selected features. Feature Xk is ranked on the basis of mutual information between Xk and training labels in order to maximize the relevance and also on the basis of the mutual information between Xk and already selected features Xj (where j \u2208 S) in order to minimize the redundancy.\nJMI: Yang et al. in [16] proposed Joint Mutual Information.\nJjmi(Xk) = \u2211 j\u2208S I(XkXj ;Y ) (2)\nIt is defined as the mutual information between the training labels and a joint random variable XkXj . It ranks the features Xk on the basis of how complimentary it is with already selected features Xj .\nCMIM: Conditional Mutual Information Maximization is another information theoretic criterion that was proposed by Fleuret [17].\nJcmim(Xk) = argmaxk{min j\u2208S [I(Xk;Y |Xj)]} (3)\nThe feature which maximizes the criterion in equation 3 at each stafe is selected as the next candidate feature. As a result, this criterion selects the feature that carries most information about Y and also considers whether this information has been captured by any of the already selected features."}, {"heading": "C. RELIEF", "text": "Relief is a feature weight based algorithm statistical feature selection method proposed by Kira and Rendell [18]. Relief detects those features which are statistically relevant to the target concept. The algorithm starts with a weight vector W initialized by zeros. At each iteration, the algorithm takes the feature vector Xk belonging to a random instance and the feature vectors of the instance closest to Xk, from each class. The closest same-class instance is termed as a near-hit and the closest different-class instance is called a near-miss. The weight vector is then updated using equation 4.\nWi =Wi \u2212 (xi \u2212 nearHit)2 + (xi \u2212 nearMiss)2 (4)\nThus the weight of any given feature decreases if it differs from that feature\u2019s value in nearby instances of the same class more than nearby instances of the other class, and increases in the converse case. Features are selected if their relevance is greater than a defined threshold. Features are then ranked on the basis of their relevance."}, {"heading": "D. Feature Selection by VC Dimension Minimization", "text": "In order to perform feature selection via MCM, we solve the following linear programming problem:\nMin w,b,h h+ C \u00b7 M\u2211 i=1 qi (5)\nh \u2265 yi \u00b7 [wTxi + b] + qi, i = 1, 2, ...,M (6) yi \u00b7 [wTxi + b] + qi \u2265 1, i = 1, 2, ...,M (7)\nqi \u2265 0, i = 1, 2, ...,M. (8)\nwhere xi, i = 1, 2, ...,M are the input data points and yi, i = 1, 2, ...,M are the corresponding target labels.\nThe classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22]. Here, the choice of C allows a tradeoff between the complexity (machine capacity) of the classifier and the classification error.\nOnce w and b have been determined, to obtain a feature ranking, features are sorted in descending order based on the value of \u2223\u2223wj\u2223\u2223 for each feature j = 1, 2...D."}, {"heading": "IV. EXPERIMENTAL SETUP AND RESULTS", "text": "To assess the classification accuracy for the multi-class datasets in this paper, we use the \u201done-vs-rest\u201d strategy. Each class is classified using the data belonging to rest of the classes as negative training samples. A Support Vector Machine classifier [23] with an RBF kernel is used for classification. The box-constraint parameter of SVM, C, is set to a high value to give more emphasis on correct classification; the width of the Gaussian kernel is selected empirically.\nTo assess the ability of the different methods to pick out the best features in the scarcity of training data, we evaluate classification results for a fixed test/train ratio while varying the number of features output by the different methods. Number of bands selected using different methods are 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 15, 20, 25, 30, 35, 40, 45 and 50.\nFurther, to asses the impact of the availability labeled data for training the model, we also evaluate the results for varying test/train ratios, while fixing the number of features. Different test/train ratio chosen for the experiment are 0.7, 0.75, 0.8, 0.85, 0.90 and 0.95.\nIn the one-vs-rest strategy, data often become highly unbalanced and hence accuracy(percentage of correctly classified points) alone is not a good metric of classification performance. Hence, we measure the Matthews Correlation Coefficient (MCC) for\neach class and computed the weighted MCC, where the weight of a class is derived from the fraction of training samples present in the one-vs-rest class split for that particular class. Matthews Correlation Coefficient is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. Matthews Correlation Coefficient is given by equation (9)\nmcc = tp \u2217 tn\u2212 fp \u2217 fn\n(tp+ fp) \u2217 (tp+ fn) \u2217 (tn+ fp) \u2217 (tn+ fn) (9)\nwhere tp (true positive) is the number of correctly classified positive samples, fp (false positive) is the number of negative samples classified as positive samples. tn (true negative) is the correctly classified negative samples and fn (false negative) is the number of positive samples classified as negative samples. MCC is computed for each class and weighted average is calculated using the number of samples in each class.\nA. Indian Pines Data-set\nThis scene was acquired by the AVIRIS sensor. Indian Pines is a 145\u00d7145 scene containing 224 spectral reflectance bands in the wavelength range 0.4\u20132.5 10\u22126 meters. The Indian Pines scene contains two-thirds agriculture, and one-third forest or other natural perennial vegetation. A random band along with ground truth is shown in Figure 1. The ground truth available is designated into sixteen classes. The corrected Indian Pines data-set contains 200 bands, obtained after removing bands covering the region of water absorption: (104-108), (150-163), 220.\nTable I gives the details of classes, number of samples and number of training and testing points for Indian Pines data-set.\nFigure 2 shows the plot of number of bands vs classification accuracy (Matthews Correlation Coefficient) for Indian Pines data-set. This plot was generated using test/train ratio of 0.90 (Table I). Plot of test-train ratio vs classification accuracy generated using first 15 bands is also shown in this figure.\nTable II reports the indices of the top 15 selected bands by different feature selection methods."}, {"heading": "B. Salinas", "text": "This scene was also gathered by the AVIRIS sensor and contains 224 bands. In this dataset, 20 water absorption bands [108-112], [154-167] and 224 have been removed during preprocessing. A random band along with the ground truth is shown in Figure 3. It includes vegetables, bare soils, and vineyard fields. Salinas\u2019 groundtruth consists of 16 classes. The dataset is available online [?].\nTable IV lists the different classes, number of samples and number of training and testing points in the Salinas dataset corresponding to test/train ratio of 0.90.\nNumber of bands vs classification accuracy plot is given in figure 4. Plot of test-train ratio vs classification accuracy generated using first 15 bands is also shown in this figure."}, {"heading": "C. Botswana Data-Set", "text": "The Botswana dataset was acquired by the Hyperion sensor at 30m pixel resolution over a 7.7 km strip in 242 bands covering the 400-2500 nm portion of the spectrum in 10nm windows. Uncalibrated and noisy bands that cover water absorption features were removed, and the remaining 145 bands were included as candidate features [?]. This dataset consists of observations from 14 identified classes representing the land cover types in seasonal swamps, occasional swamps, and drier woodlands. A random band along with ground truth for Botswana data-set is shown in Figure 5.\nTable VII gives the listing of number of samples and number of training and testing points in Botswana data-set corresponding to test train ratio 0.90.\nNumber of bands vs classification accuracy plot is given in figure 6. Plot of test-train ratio vs classification accuracy generated using first 15 bands is also shown in this figure."}, {"heading": "V. CONCLUSION", "text": "This paper applies a recently proposed filter feature selection method based on minimizing a tight bound on the VC dimension to the task of hyperspectral image classification. We demonstrate that this feature selection method significantly outperforms state-of-the-art methods in terms classification accuracy that is suitably measured in the presence of a large number of classes. The superior results obtained over different datasets and across a variety of metrics suggests that the proposed method should be the method of choice for this problem. It has not escaped our attention that this method can also be applied to a variety of other high-dimensional classification tasks; we are working on developing modifications of this method for the same."}], "references": [{"title": "Feature selection for classification of hyperspectral data by SVM.", "author": ["Pal", "Mahesh", "Giles M. Foody"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 48.5", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Unsupervised hyperspectral image analysis with projection pursuit.", "author": ["Ifarraguerri", "Agustin", "Chein-I. Chang"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 38, no", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Efficient hierarchical-PCA dimension reduction for hyperspectral imagery.", "author": ["Agarwal", "Abhishek", "Tarek El-Ghazawi", "Hesham El-Askary", "Jacquline Le-Moigne"], "venue": "In Signal Processing and Information Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Unsupervised band selection for hyperspectral imagery classification without manual band removal.", "author": ["Jia", "Sen", "Zhen Ji", "Yuntao Qian", "Linlin Shen"], "venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of 5,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Constrained band selection for hyperspectral imagery.", "author": ["Chang", "Chein-I", "Su Wang"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 44, no", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A joint band prioritization and band-decorrelation approach to band selection for hyperspectral image classification.", "author": ["Chang", "Chein-I", "Qian Du", "Tzu-Lung Sun", "Mark LG Althouse"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 37, no", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Band selection for hyperspectral image classification using mutual information.", "author": ["Guo", "Baofeng", "Steve R. Gunn", "R.I. Damper", "J.D.B. Nelson"], "venue": "Geoscience and Remote Sensing Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Feature Selection through Minimization of the VC dimension.", "author": ["Jayadeva", "Batra", "Sanjit S", "Siddharth Sabharwal"], "venue": "arXiv preprint arXiv:1410.7372", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Classification of hyperspectral remote sensing images with support vector machines.", "author": ["Melgani", "Farid", "Lorenzo Bruzzone"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 42, no", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Clustering-based hyperspectral band selection using information measures.", "author": ["Martnez-Us", "Adolfo", "Filiberto Pla", "Jos Martnez Sotoca", "Pedro Garca-Sevilla"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 45, no", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Gene selection for cancer classification using support vector machines.", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Feature subset selection: a correlation based filter approach.", "author": ["Hall", "Mark A", "Lloyd A. Smith"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy.", "author": ["Peng", "Hanchuan", "Fulmi Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 27,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Random forests.", "author": ["Breiman", "Leo"], "venue": "Machine learning 45,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection.", "author": ["Brown", "Gavin", "Adam Pocock", "Ming-Jie Zhao", "Mikel Lujn"], "venue": "The Journal of Machine Learning Research 13,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Data Visualization and Feature Selection: New Algorithms for Nongaussian Data.", "author": ["Yang", "Howard Hua", "John E. Moody"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Fast binary feature selection with conditional mutual information.", "author": ["Fleuret", "Franois"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "The feature selection problem: Traditional methods and a new algorithm.", "author": ["Kira", "Kenji", "Larry A. Rendell"], "venue": "In AAAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Learning a hyperplane regressor through a tight bound on the VC dimension.", "author": ["Jayadeva", "Chandra", "Suresh", "Sanjit S. Batra", "Siddarth Sabharwal"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning a hyperplane regressor by minimizing an exact bound on the VC dimension.", "author": ["Jayadeva", "Chandra", "Suresh", "Siddarth Sabharwal", "Sanjit S. Batra"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "LIBSVM: A library for support vector machines.", "author": ["Chang", "Chih-Chung", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Further, it has been noted that highly correlated features have a negative impact on classification accuracy [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 246, "endOffset": 249}, {"referenceID": 6, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 279, "endOffset": 282}, {"referenceID": 7, "context": "This paper explores the application of a novel feature selection method based on minimizing a tight bound on the VC dimension [8], on hyperspectral data analysis.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "We used the Support Vector Machine (SVM) classifier [9] to assess the classification accuracy, following feature selection.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "BACKGROUND AND RELATED WORK Dimensionality reduction prior to classification is advantageous in hyperspectral data analysis because the dimensionality of the input space greatly affects the performance of many supervised classification methods [7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 0, "context": "The analysis in [1] supports this line of reasoning and suggests that feature selection may be a valuable procedure in preprocessing hyperspectral data for classification by the widely used SVM classifier.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "In hyperspectral image analysis, feature selection is preferred over feature extraction for dimensionality reduction [1], [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "In hyperspectral image analysis, feature selection is preferred over feature extraction for dimensionality reduction [1], [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 261, "endOffset": 265}, {"referenceID": 5, "context": "In [6], a band prioritization scheme based on Principal Component Analysis (PCA) and classification criterion is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], mutual information is computed using the estimated reference map obtained by using available a priori knowledge about the spectral signature of frequently-encountered materials.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "Recently, Brown et al [15] have presented a framework for unifying many information based feature selection selection methods.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "In [8] a feature selection method based on minimization of a tight bound on the VC dimension is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[6] presents a band prioritization method based on Principal Component Analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "in [16] proposed Joint Mutual Information.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "CMIM: Conditional Mutual Information Maximization is another information theoretic criterion that was proposed by Fleuret [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "RELIEF Relief is a feature weight based algorithm statistical feature selection method proposed by Kira and Rendell [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 169, "endOffset": 172}, {"referenceID": 18, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "A Support Vector Machine classifier [23] with an RBF kernel is used for classification.", "startOffset": 36, "endOffset": 40}], "year": 2015, "abstractText": "Hyperspectral data consists of large number of features which require sophisticated analysis to be extracted. A popular approach to reduce computational cost, facilitate information representation and accelerate knowledge discovery is to eliminate bands that do not improve the classification and analysis methods being applied. In particular, algorithms that perform band elimination should be designed to take advantage of the specifics of the classification method being used. This paper employs a recently proposed filterfeature-selection algorithm based on minimizing a tight bound on the VC dimension. We have successfully applied this algorithm to determine a reasonable subset of bands without any user-defined stopping criteria on widely used hyperspectral images and demonstrate that this method outperforms state-of-the-art methods in terms of both sparsity of feature set as well as accuracy of classification.", "creator": "TeX"}}}