{"id": "1509.08792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "An intelligent extension of Variable Neighbourhood Search for labelling graph problems", "abstract": "in this paper we describe an extension of the variable neighbourhood minimum search ( vns ) which integrates the basic vns with other complementary approaches from cognitive machine learning, statistics and experimental algorithmic, in order to produce high - quality job performance goals and to completely automate the resulting optimization strategy. the resulting : intelligent vns has been successfully externally applied to a couple of optimization problems where the solution space consists of the subsets of a finite reference set. these problems are the labelled spanning tree and forest cover problems that are just formulated on an undirected labelled butterfly graph ; a graph where in each edge space has a label in a finite set of labels by l. the problems consist on selecting the subset of labels such that the subgraph generated by these labels has an optimal spanning finite tree or forest, hence respectively. these various problems have inspired several applications in the real - world, where one aims to locally ensure connectivity by means of homogeneous connections.", "histories": [["v1", "Sun, 27 Sep 2015 22:12:42 GMT  (13kb)", "http://arxiv.org/abs/1509.08792v1", "MIC 2015: The XI Metaheuristics International Conference, 3 pages, Agadir, June 7-10, 2015"]], "COMMENTS": "MIC 2015: The XI Metaheuristics International Conference, 3 pages, Agadir, June 7-10, 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sergio consoli", "jos\\`e", "r\\`es moreno p\\`erez"], "accepted": false, "id": "1509.08792"}, "pdf": {"name": "1509.08792.pdf", "metadata": {"source": "CRF", "title": "An intelligent extension of Variable Neighbourhood Search for labelling graph problems", "authors": ["Sergio Consoli", "Jos\u00e9 Andr\u00e9s Moreno P\u00e9rez"], "emails": ["sergio.consoli@istc.cnr.it", "jamoreno@ull.edu.es"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n08 79\n2v 1\n[ cs\n.A I]\n2 7\nSe p\n20 15"}, {"heading": "1 Introduction", "text": "In this paper we scratch an Intelligent Variable Neighbourhood Search (Int-VNS) aimed to achieve further improvements of a successful VNS for the Minimum Labelling Spanning Tree (MLST) and the k-Labelled Spanning Forest (kLSF) problems. This approach integrates the basic VNS with other complementary intelligence tools and has been shown a promising strategy in [3] for the MLST problem and in [5] for the kLSF problem. The approach could be easily adapted to other optimization problems where the space solution consists of the subsets of a reference set; like the feature subset selection or some location problems. First we introduced a local search mechanism that is inserted at top of the basic VNS to get the Complementary Variable Neighbourhood Search (Co-VNS). Then we insert a probability-based constructive method and a reactive setting of the size of shaking process."}, {"heading": "2 The Labelled Spanning Tree and Forest problems", "text": "A labelled graph G = (V,E,L) consists of an undirected graph where V is its set of nodes and E is the set of edges that are labelled on the set L of labels. In this paper we consider two problems defined on a labelled graph: the MLST and the kLSF problems. The MLST problem [2] consists on, given a labelled input graph G = (V,E,L), to get the spanning tree with the minimum number of labels; i.e., to find the labelled spanning tree T \u2217 = (V,E\u2217, L\u2217) of the input graph that minimizes the size of label set |L\u2217|. The kLSF problem [1] is defined as follows. Given a labelled input graph G = (V,E,L) and an integer positive value k\u0304, to find a labelled spanning forest F \u2217 = (V,E\u2217, L\u2217) of the input graph having the minimum number of connected components with the upper bound k\u0304 for the number of labels to use, i.e. min|Comp(G\u2217)| with |L\u2217| \u2264 k\u0304. Given the subset of labels L\u2217 \u2286 L, the labelled subgraph G\u2217 = (V,E\u2217, L\u2217) may contain cycles, but they can arbitrarily break each of them by eliminating edges in polynomial time until a forest or a tree is obtained. Therefore in both problems, the matter is to find the optimal set of labels L\u2217. Since a MLST solution would be a solution also to the kLSF problem if the obtained solution tree would not violate the limit k\u0304 on the used number of labels, it is easily deductable that the two problems are deeply correlated. The NP-hardness of the MLST and kLSF problems was stated in [2] and in [1] respectively. Therefore any practical solution approach to both problems requires heuristics [1, 4].\nAgadir, June 7-10, 2015"}, {"heading": "3 Complementary Variable Neighbourhood Search", "text": "The first extension of the VNS metaheuristic that we introduced for these problems is a local search mechanism that is inserted at top of the basic VNS [4]. The resulting local search method is referred to as Complementary Variable Neighbourhood Search (Co-VNS) [5, 3]. Given a labelled graph G = (V,E,L) with n vertices, m edges, and \u2113 labels, Co-VNS replaces iteratively each incumbent solution L\u2217 with another solution selected from the complementary space of L\u2217 defined as the sets of labels that are not contained in L\u2217; L \\ L\u2217. The iterative process of extraction of a complementary solution helps to escape the algorithm from possible traps in local minima, since the complementary solution lies in a very different zone of the search space with respect to the incumbent solution. This process yields an immediate peak of diversification of the whole local search procedure. To get a complementary solution, Co-VNS uses a greedy heuristic as constructive method in the complementary space of the current solution. For the MLST and kLSF problems the greedy heuristic is the Maximum Vertex Covering Algorithm (MVCA) [2] applied to the subgraph of G with labels in L \\L\u2217. Note that Co-VNS stops if either the set of unused labels contained in the complementary space is empty (L \\L\u2217 = \u2205) or a final feasible solution is produced. Successively, the basic VNS is applied in order to improve the resulting solution.\nAt the starting point of VNS, it is required to define a suitable series of neighbourhood structures of size qmax. In order to impose a neighbourhood structure on the solution space S we use the Hamming distance between two solutions L1, L2 \u2208 S given by \u03c1(L1, L2) = |L1\u2206L2| where L1\u2206L2 consists of labels that are in one of the solutions but not in the other. VNS starts from an initial solution L\u2217 with q increasing iteratively from 1 up to the maximum neighborhood size, qmax. The basic idea of VNS to change the neighbourhood structure when the search is trapped at a local minimum, is implemented by the shaking phase. It consists of the random selection of another point in the neighbourhood Nq(L\u2217) of the current solution L\u2217. Given L\u2217, we consider its qth neighbourhood Nq(L\u2217) comprised by sets having a Hamming distance from L\u2217 equal to q labels, where q = 1, 2, . . . , qmax. In order to construct the neighbourhood of a solution L\u2217, the algorithm proceeds with the deletion of q labels from L\u2217."}, {"heading": "4 Intelligent Variable Neighbourhood Search", "text": "The proposed intelligent metaheuristic (Int-VNS) is built from Co-VNS, with the insertion of a probabilitybased local search as constructive method to get the complementary space solutions. In particular, this local search is a modification of greedy heuristic, obtained by introducing a probabilistic choice on the next label to be added into incomplete solutions. By allowing worse components to be added to incomplete solutions, this probabilistic constructive heuristic produces a further increase on the diversification of the optimization process. The construction criterion is as follows. The procedure starts from an initial solution and iteratively selects at random a candidate move. If this move leads to a solution having a better objective function value than the current solution, then this move is accepted unconditionally; otherwise the move is accepted with a probability that depends on the deterioration, \u2206, of the objective function value. This construction criterion takes inspiration from Simulated Annealing (SA) [7]. However, the probabilistic local search works with partial solutions which are iteratively extended with additional components until complete solutions emerge. In the probabilistic local search, the acceptance probability of a worse component into a partial solution is evaluated according to the usual SA criterion by the Boltzmann function exp(\u2212\u2206/T ), where the temperature parameter T controls the dynamics of the search. Initially the value of T is large, so allowing many worse moves to be accepted, and is gradually reduced by the following geometric cooling law: Tj+1 = \u03b1 \u00b7 Tj , where T0 = |BestL| and \u03b1 = 1/|BestL| \u2208 [0, 1], with BestL being the current best solution. This cooling law is very fast and produces a good balance between intensification and diversification capabilities. In addition, this cooling schedule does not requires any intervention from the user regarding the setting of its parameters, as it is guided automatically by the best solution BestL. Therefore the whole optimization process is able to react in response to the search algorithm\u2019s behavior and to adapt its setting on-line according to the instance of the problem under evaluation [7]. The probabilistic local search has the purpose of allowing\nAgadir, June 7-10, 2015\nalso the inclusion of less promising labels to incomplete solutions. Probability values assigned to each label are decreasing in the quality of the solution they give. In this way, at each step, labels with a better quality will have a higher probability of being selected; the progressive reduction of the temperature in the adaptive cooling law produces, step by step, an increasing of this diversity in probabilities.\nAt the beginning of Int-VNS, the algorithm generates an initial feasible solution at random, that is the first current best solution BestL, and set parameter qmax to the number of labels of the initial solution (qmax \u2190 |BestL|). Then the Complementary procedure is applied to BestL to obtain a solution L\u2217 from the complementary space of BestL by means of the probabilistic local search. The Complementary procedure stops if either a feasible solution L\u2217 is obtained, or the set of unused labels contained in the complementary space is empty producing a final infeasible solution. Subsequently, the shaking phase used for the basic VNS is applied to the resulting solution L\u2217. It consists of the random selection of a point L\u2032 in the neighbourhood Nq(L\u2217) of the current solution L\u2217, as in Co-VNS. The successive local search corresponds also to that of Co-VNS. However, since either Co-VNS, or the deletion of labels in the shaking phase, can produce an incomplete solution, the first step of the local search consists of including additional labels in the current solution in order to restore feasibility, if needed. The addition of labels at this step is according to the probabilistic procedure. Then, the local search tries to drop labels in L\u2032, and then to add further labels following the greedy rule, until k\u0304 labels emerge. At this step, if no improvements are obtained the neighbourhood structure is increased (q \u2190 q+1) producing progressively a larger diversification. Otherwise, the algorithm moves L\u2217 to solution L\u2032 restarting the search with the smallest neighbourhood (q \u2190 1). This iterative process is repeated until the maximum size of the shaking phase, qmax, is reached. The resulting local minimum L\u2217 is compared to the current best solution BestL, which is updated in case of improvement (BestL \u2190 L\u2217). At this point a reactive setting for the parameter qmax is used [6]. In case of an improved solution, qmax is decreased (qmax \u2190 max(qmax \u2212 1; |BestL|/2)) in order to raise the intensification factor of the search process. Conversely, in case of none improvement, the maximum size of the shaking is increased (qmax \u2190 min(qmax+1; 2 \u00b7 |BestL|)) in order to enlarge the diversification factor of the algorithm. In each case, the adaptive setting of qmax is bounded to lie in the interval between |BestL|/2 and 2 \u00b7 |BestL| to avoid a lack of balance between intensification and diversification factors. The algorithm proceeds with the same procedure until the user termination conditions are satisfied, producing at the end the best solution to date, BestL, as output."}, {"heading": "5 Summary and Outlook", "text": "The achieved optimization strategy seems to be highly promising for both labelling graph problems. Ongoing investigation consists in statistical comparisons of the resulting strategy against the best algorithms in the literature for these problems, in order to quantify and qualify the improvements obtained. Further investigation will deal with the application of this strategy to other problems."}], "references": [{"title": "The k-labeled spanning forest problem", "author": ["R. Cerulli", "A. Fink", "M. Gentili", "A. Raiconi"], "venue": "Procedia - Social and Behavioral Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The minimum labelling spanning trees", "author": ["R.S. Chang", "S.J. Leu"], "venue": "Information Processing Letters,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Solving the minimum labelling spanning tree problem by intelligent optimization", "author": ["S. Consoli", "N. Mladenovi\u0107", "J.A. Moreno-P\u00e9rez"], "venue": "Applied Soft Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Variable neighbourhood search for the k-labelled spanning forest problem", "author": ["S. Consoli", "J.A. Moreno-P\u00e9rez"], "venue": "Electronic Notes in Discrete Mathematics, to appear,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Intelligent variable neighbourhood search for the minimum labelling spanning tree problem", "author": ["S. Consoli", "J.A. Moreno-P\u00e9rez", "N. Mladenovi\u0107"], "venue": "Electronic Notes in Discrete Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "An adaptive variable neighborhood search algorithm for a vehicle routing problem arising in small package shipping", "author": ["A. Stenger", "D. Vigo", "S. Enz", "M. Schwind"], "venue": "Transportation Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A theoretical study on the behavior of simulated annealing leading to a new cooling schedule", "author": ["E. Triki", "Y. Collette", "P. Siarry"], "venue": "European Journal of Operational Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "This approach integrates the basic VNS with other complementary intelligence tools and has been shown a promising strategy in [3] for the MLST problem and in [5] for the kLSF problem.", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "This approach integrates the basic VNS with other complementary intelligence tools and has been shown a promising strategy in [3] for the MLST problem and in [5] for the kLSF problem.", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "The MLST problem [2] consists on, given a labelled input graph G = (V,E,L), to get the spanning tree with the minimum number of labels; i.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "The kLSF problem [1] is defined as follows.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "The NP-hardness of the MLST and kLSF problems was stated in [2] and in [1] respectively.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "The NP-hardness of the MLST and kLSF problems was stated in [2] and in [1] respectively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Therefore any practical solution approach to both problems requires heuristics [1, 4].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "Therefore any practical solution approach to both problems requires heuristics [1, 4].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "The first extension of the VNS metaheuristic that we introduced for these problems is a local search mechanism that is inserted at top of the basic VNS [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "The resulting local search method is referred to as Complementary Variable Neighbourhood Search (Co-VNS) [5, 3].", "startOffset": 105, "endOffset": 111}, {"referenceID": 2, "context": "The resulting local search method is referred to as Complementary Variable Neighbourhood Search (Co-VNS) [5, 3].", "startOffset": 105, "endOffset": 111}, {"referenceID": 1, "context": "For the MLST and kLSF problems the greedy heuristic is the Maximum Vertex Covering Algorithm (MVCA) [2] applied to the subgraph of G with labels in L \\L.", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "This construction criterion takes inspiration from Simulated Annealing (SA) [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Initially the value of T is large, so allowing many worse moves to be accepted, and is gradually reduced by the following geometric cooling law: Tj+1 = \u03b1 \u00b7 Tj , where T0 = |BestL| and \u03b1 = 1/|BestL| \u2208 [0, 1], with BestL being the current best solution.", "startOffset": 200, "endOffset": 206}, {"referenceID": 6, "context": "Therefore the whole optimization process is able to react in response to the search algorithm\u2019s behavior and to adapt its setting on-line according to the instance of the problem under evaluation [7].", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "At this point a reactive setting for the parameter qmax is used [6].", "startOffset": 64, "endOffset": 67}], "year": 2015, "abstractText": "In this paper we describe an extension of the Variable Neighbourhood Search (VNS) which integrates the basic VNS with other complementary approaches from machine learning, statistics and experimental algorithmic, in order to produce high-quality performance and to completely automate the resulting optimization strategy. The resulting intelligent VNS has been successfully applied to a couple of optimization problems where the solution space consists of the subsets of a finite reference set. These problems are the labelled spanning tree and forest problems that are formulated on an undirected labelled graph; a graph where each edge has a label in a finite set of labels L. The problems consist on selecting the subset of labels such that the subgraph generated by these labels has an optimal spanning tree or forest, respectively. These problems have several applications in the real-world, where one aims to ensure connectivity by means of homogeneous connections.", "creator": "LaTeX with hyperref package"}}}