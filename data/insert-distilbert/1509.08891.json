{"id": "1509.08891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "The Computational Principles of Learning Ability", "abstract": "it has has been quite a long time since ai researchers in the field of computer science stop talking about simulating human intelligence or trying to explain how brain works. recently, represented internally by deep learning techniques, the field of machine learning is experiencing unprecedented experimental prosperity and some applications with near human - level evaluation performance bring researchers without confidence to imply that their approaches are the promising candidate for finally understanding the mechanism of human brain. however apart from several ancient philological criteria and some imaginary black bottom box tests ( turing test, chinese room ) there is no computational level explanation, definition or criteria about intelligence or any of its components. base on the common sense that learning ability is one critical component of intelligence and inspect from the viewpoint of mapping relations, initially this paper presents two laws apart which critically explains what is the \" learning ability \" as we familiar with and under what conditions a mapping relation can be acknowledged as \" learning model \".", "histories": [["v1", "Wed, 23 Sep 2015 04:25:44 GMT  (641kb,D)", "http://arxiv.org/abs/1509.08891v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hao wu"], "accepted": false, "id": "1509.08891"}, "pdf": {"name": "1509.08891.pdf", "metadata": {"source": "CRF", "title": "The Computational Principles of Learning Ability", "authors": ["Wu Hao"], "emails": ["wuhao29@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Except for many philosophical discriptions and the famous black box test \u201cTuring test\u201d, there is no clear definition of intelligence and it is well accepted that the ability of \u201cthinking\u201d is difficult to define[9]. On the other hand, for machine learning researchers and artificial intelligence experts, once a problem is solved, the solution as a computational model, seems to have nothing to do with intelligence, it seems like only a problem to be solved is related to the understanding of intelligences[6]. Therefore, researchers are trapped in a paradox as shown in figure 1.\nar X\niv :1\n50 9.\n08 89\n1v 1\n[ cs\n.A I]\n2 3\nSe p\nSince all possible automated solutions implemented by computer systems are basically different computational models[2], it seems like there will be no computational model which could be acknowledged as possessing true intelligence forever. One possible solution of breaking this paradox is to find one or a set of criteria which can be used for white box testing of all computational models, as shown in figure 2.\nInstead of giving criteria for intelligences, based on the understanding that the learning ability is a critical component of intelligence, this paper proposes two laws for a computational model to be a learning model. With the help of these two laws, computational models can be classified as \u201cLearning Model\u201d and\n\u201cNon-Learning Model\u201d (figure 3), these two laws also provide a computational explanation about what the \u201cLearning Ability\u201d is.\nBased on the viewpoint that has been missed by previous researchers, this paper focuses on discussing the behaviour of a model, or more specifically the behaviour of individual configurations \u03b8 \u2208 \u0398 of a given dynamic system M(x,\u0398) (\u0398 is the parameter set and \u03b8 is any of its element, the detailed discussion of dynamic systems is in the explanatory part of section three). The \u201cLearning Ability\u201d discussed in this paper is a behaviour of a mapping relation, so when being used in the term \u201cLearning Model\u201d and \u201cMachine Learning\u201d (Learning Theory), the word \u201cLearning\u201d carries different meanings respectively.\nIt is a common sense that without learning ability a creature cannot be called as an intelligent creature. And what being learned is nothing but information, the next section gives six definitions about relations between information and mapping relation. And only by expressing some common philosophical understanding about information formally is further inference possible, these six definitions play key roles in understanding these two laws and corresponding corollaries. Section three introduces two laws that explain the fundamental mechanism of our intuitive feeling of learning ability and three corresponding corollaries that proves the existence of a common learning model which satisfies these two laws. These two laws and corollaries also illustrate several core properties of this common learning model."}, {"heading": "2 Definitions", "text": ""}, {"heading": "2.1 Definition I", "text": "For a given mapping relation M : D \u2192 O, an element of the range ( o \u2208 O) and each element in its corresponding domain\n({e | e \u2208 D,M(e) = o}) are global and local information respectively.\nWhen we sit on our chair and look outside the window, we could see birds, butterflies, clouds, and trees. When we take a deep breath, we could taste the sweet smell of the freshly cut grass. Although we have no direct access to the world other than through our sensors [5], we can always rely on different kinds of apparatus to discover the world, in fact all apparatus can be regarded as extension of our biological sensors. But what if something cannot be detected by all means? Is it necessary to insist on its existence? The question has been answered perfectly by Carl Sagan\u2019s famous story \u201cThe Dragon In My Garage\u201d[8]. And the following notion has been well known and accepted for decades.\nThere is no observation independent reality.\nThis statement illustrates the relation of two kind of information:\n1. Observation \u21d2 a set of appearances\n2. Reality \u21d2 being defined by a set of appearances\nAny apparatus being used to detect the world can be regarded as creating a mapping relation from appearances (subset of apparatus\u2019 domain) to the reality (elements in the range) and biological systems can also be regarded as one kind of apparatus which includes us.\nThe term \u201creality\u201d means the existence of certain concept which could be an object or an abstract concept such as the existence of gravity or electromagnetic wave. Therefore the term \u201creality\u201d will be replaced by \u201cconcept\u201d in the following sections."}, {"heading": "2.2 Definition II", "text": "For a given mapping relation M : D \u2192 O, one subset of the domain and each element of the corresponding subset of the range are local\nrepresentation and global representations which define the same concept respectively.\nA set of appearances are detected by an apparatus M (the domain of a mapping relation M). All these appearances indicate the existence of a concept, so the local representation of this concept is defined as the subset X and the global representation of this concept is defined as each element of the subset Y. Therefore all local information about a concept is its only local representation and the global information of the concept is equivalent to its global representation.\nIn summary, definition I and definition II indicate that:\n1. These four expressions are equivalent: A concept\u2261A set of appearances\u2261A set of local information (definition one)\u2261A local representation (definition two)\n2. These three expressions are equivalent: A concept\u2261A piece of global information\u2261A global representation"}, {"heading": "2.3 Definition III", "text": "For a given mapping relation M : D \u2192 O and subset X \u2282 O, we have Y = {y | y = M(x), x \u2208 X}, so that:\n1. \u2200y \u2208 Y subject to a constraint c\u03b1 which is defined as a pair < Y,Ri >, Ri is the i-ary relation;\n2. \u2200x \u2208 Xsubject to a constraint c\u03b2 which is defined as a pair < X,Rj >, Rj is the j-ary relation;\n3. if Ri and Rj are different, then each element of Y is a invariant representation of X.\nTogether definition I and II indicate that there could be more than one global representation for one concept, therefore in order to guarantee the certainty of information, all global representations are supposed to be subject to at least one constraint, or in other words, it is necessary to find the constraint that all global representations are subject to so that these global information can be recognised or be harvested. If all global representations of a concept and all appearances of this concept are only subject to different constraints, then global representation of this concept is its invariant representation."}, {"heading": "2.4 Definition IV", "text": "For a given mapping relation M : D \u2192 O, it defines the type of global information as M .\nAs the notion being introduced in definition I :\nThere is no observation independent reality\nThe existence of a certain concept depends on whether it is observable. Furthermore the nature of the concept depends on the method of observation.\nDefinition IV guarantees that it is the appearances and the way these appearances are being processed that decide not only the existence but also the nature of the concept, because the appearances (local information) form a subset of the domain, and the way these local information are being processed (mapping relation) gives the global representation (global information) of a system(the concept). This definition is also a generalisation of our daily life experiences.\nVisual detection enables us to tell different kinds of trees. And it is hard to tell the differences of these realities by only touching them. Because with the change of the domain, the appearances of these realities do not carry enough information for telling differences within each kind, but we can still tell the differences between plant and animal by touching them.\nEven when facing the same domain, different mapping relations will give different types of information. One typical example is the camera where the domain is provided by the CCD array, and different functions of the camera will give different types of information, such as the focusing information being used to adjust the lens and the information being recorded as photos."}, {"heading": "2.5 Definition V", "text": "For two given mapping relations: Ml : S1 \u2192 S2 and Mh : S3 \u2192 S4, if S1 \u222a S2 \u2282 S3 then \u2200s \u2208 S2 is homologous global information with\nrespect to Mh."}, {"heading": "2.6 Definition VI", "text": "For two given mapping relations: Ml : S1 \u2192 S2 and Mh : S3 \u2192 S4, if S2 \u2282 S3 then \u2200s \u2208 S4 is first order global information with respect to\nMl.\nApparently, all elements in S4 is beyond the field of vision of mapping relation Ml, actually if Mh is one to one mapping relation between S2 = S3 and S4, it is also true that all elements in S2 is beyond the field of vision of mapping relation Ml. Therefore for a given mapping relation M : D \u2192 O, all elements in its range is first order global information with respect to M itself. And together with definition II we know that these three concepts are equivalent: a global information, a global representation, a first order global information.\nUsually a mapping relation is the composition of several mapping relations, it could be combination of different approaches of processing information from the domain. Definitions V and VI describe the relation between different level of process."}, {"heading": "2.7 Explanatory Comment", "text": "This section explains in what sense these definitions shall be required to be understood. Our daily experience are further processed products of basic information and two trivial facts of information are usually being ignored, yet they are the most notable, because these two facts lead to a unique viewpoint which will enable us to understand the relationship between the appearance and the reality, and the role of constraint. The understanding of these concepts is critical for establishing the theoretical framework which can be used to explain what learning is, analyse the learning ability of a given model and construct learning models with different levels of ability.\n1. The concept of global and local are comparative.\nMore specifically, it means the existence of a concept depends on its appearances, and this concept itself could also be one of many appearances which define a higher level concept. This can be explained by a simple mental experiment. Imagine you were sleeping on the backseat of a minivan, the shaking caused by the speed bump wakes you up. You do not know how long you have slept and you watch the scenery passing outside the window with your sleepy eyes. Suddenly you realised that you are approaching \u201cSome Place\u201d. What is included in the passing landscape depends on their appearance, it could be a cottage, a church or a supermarket. And all these rapidly passing views are appearances which enable you to recognise that you are near this \u201csome place\u201d, and this \u201csome place\u201d is a higher level concept. Therefore, it is necessary to give definitions which can be used to describe this hierarchical relationship of different information.\n2. Certainty plays a critical role in representing information, in fact without certainty there will be no information. Even when people are measuring the uncertainty of a system ( entropy), information of the number of possible states and the probability of being in each state are still necessary and must be expressed in a certain way:\nIf the precise value of i and Pi cannot be obtained, we still need a certain number to represent the information of a certain within bounds If there is no information about i and Pi at all, there is still a piece of notation which represent the definition of entropy; and if there was nothing in figure 7 then there will be no information at all. Therefore it is reasonable to say that one piece of information must be carried by at least one certain state and one certain state can be used to represent at least one piece of information.\nHowever, certainty itself only guarantees the existence of information, whether the information can be recognised relies on another critical factor: constraint. This relation between certainty and constraint is usually being ignored because most of the time in our daily life, the acquisition and recognition of information occur one after another immediately and the harvested information directly affects all of our sensation and perception. Occasionally people could have the chance to realise the existence of this relation, such as when we are hearing people speaking another language for the first time. Because of being subject to different constraint, for example, it is hard for most readers of this article to understand Chinese. And we have not known exactly what the constraint is for any language, although we rely on at least one of these constraints everyday.\nRelying on variance hypothesises of different constraints, researchers try to mimic unknown constraints of different languages or the world we are seeing, the development of this field brings various NLP and classification applications, some with great performance. Almost all researchers in this field claim that they\nare interested in knowing how the brain might work and imply their applications are promising candidates that could lead us on the course to the answer. Therefore, one question to which we are all interested to know the answer is \u201cAre these applications qualified to be prefixed with intelligent?\u201d. The following is an imaginary experiment which illustrates one of many possible key factors of this question:\nSuppose people were able to create today\u2019s state-of-the-art on-line translator (such as Skye on-line translator) in the beginning of this millennium and keep it running. Could this system be able to translate \u201cselfies\u201d into Chinese now?\nIf the system could, does this mean that the system can be labeled as \u201cintelligent\u201d indisputably? After all, at the time the system was created, the words \u2018selfies\u2019 had not been invented and the system somehow managed to learn the \u201cmeaning\u201d of this word, therefore it is possible that the system could be as smart as human. Now the focus of the argument about \u201cintelligence\u201d switches to exactly how this information was learned by the system. What if the system was linked to a dictionary which was periodically updated by human? what if the system asks end users to report mistakes and automate the correction process? Actually, no matter what strategies are being used, it must be some kind of computation, but exactly what kind of computation would match our understanding of learning? The answer to this question is: \u201cThere is no criterion of learning ability for computational models yet\u201d or in other words, we have no formal definition of learning for computational model. Therefore the judgement of learning ability for computational model cannot be made, this is also part of the reason why, as Rodney Brooks complained[7]:\n\u201cEvery time we figure out a piece of it, it stops being magical; we say, \u2019Oh, that\u2019s just a computation\u201d\nUsually human do not start talking before one-year-old, we cannot recognise individuals of different species or even different races unless we have seen enough, we train dogs to help us because we know dogs are smart animal, meanwhile we also like funny videos which record dog confused by \u201cinvisible doors\u201d. It is highly unlikely that the effort of understanding what is intelligence would bring us C3PO directly, the lack of a criterion for learning ability of computation model would make us ignore some computational models which could be a building block of higher level learning ability or intelligence, only because their current performance is not as good as some other computation model which might have no such potential.\nAnd only with the help of such criteria could we carry out research to help us understand the nature of learning, analyse the level of learning ability quantitatively, construct computational model with different levels of learning ability, and eventually provide people the research materials for studying what is intelligence, and the intelligence of us. Based on definitions about information given in this section, next section will discuss two laws for a mapping relation to be a learning model and the corresponding corollaries."}, {"heading": "3 Axiom and The Laws of Learning", "text": ""}, {"heading": "3.1 Axiom", "text": "Without mapping relation there will be no acquisition of any information.\nThis axiom comes from interpreting the statement at the beginning of section two:\nThere is no observation independent reality.\nIn other words, this axiom indicates that mapping relation is necessary for defining concepts. And if a mapping relation ML : D \u2192 O is said to be a learning model which is able to learn from its domain D, it must follow these two laws."}, {"heading": "3.2 Law I", "text": "\u2200XS \u2282 D,YS = ML(XS),\u2203XN \u2282 D \\XS : YN = ML(XN ), YN \u2229 YS = \u2205\nFor any subset XS of the domain D, and the corresponding subset of the range is YS , there exist a set XN which is a subset of the complement set of XS , so that we have a subset YN which is disjoint with YS .\nThis law would be best understood by assuming that there is a mapping relation Mnon which does not obey law one. Then there will be four\n1 possible scenarios as follows:\nIn the three scenarios in figure 8, XS and XN contain different local information, however the mapping relation Mnon will map them to the same set of global\n1Scenario two and three are equivalent.\nrepresentations, therefore as a detector Mnon fails to detect different concepts in the domain. In other words, mapping relations which do not obey law one are basically information black holes which could allow the possible information of the existence of many concepts to devolve into the same state [1, p.43].\nIn the scenario in figure 9, for guaranteeing the certainty property of information, elements of YS and YN will subject to the same constraint\n2, therefore the possible existences of many concepts still devolve into the same state as in the previous three scenarios.\nIn summary, law I guarantees that the possible existence of a new concept will not be missed.\n2Trying to avoid using same constraint by labelling elements as described in above four scenarios will violate law II, and at this stage this violation seems inevitable. Formal mathematical discussion about this issue is part of the future works."}, {"heading": "3.3 Law II", "text": "The training process3 of ML(\u03b1 \u2282 \u039b) : D \u2192 O should not depend on any of ML\u2019s first order global information MH which follows law I\nand law II as well.\nThe intuitive explanation of law II is: a learning model should be able to define the existence of new concept all by itself. This explanation ends all similar arguments like \u201cShould a NLP system linked with a dictionary be label as \u2019Intelligence\u2019?\u201d. Furthermore, together with law one, origins of information (mapping relations) being used for a composite mapping relation can be classified into two categories, learning model and non-learning model(memory system), so that further analysis could be possible. Actually whenever the construction process of a mapping relation is inferred by some given knowledge about the desirable learning results, it will almost always limit the learning ability. Topics related to law two are core problems which need further study.4"}, {"heading": "3.4 Corollary I", "text": "Given mapping relation ML : D \u2192 O(O \u2282 Rn, D \u2282 Rm), if ML follows law one, then there exist a family of functions H : Rn \u2192 R which can be used to harvest information being contained in the range of ML."}, {"heading": "3.4.1 Lemma One", "text": "Given D \u2282 Rn: \u2200Xj \u2282 D and \u2200r \u2208 R, \u2203\u03a6(x | x \u2208 Xj) = r. For any subset Xj of a n-dimensional real number domain D and any given real number r, there exist an equality constraint \u03a6(x), so that \u03a6(x | x \u2208 Xj) = r.\nProof: For a given subset Xj \u2282 D, there exists an equation5:\nG(t) = (t\u2212 x1) \u2022 (t\u2212 x2) \u2022 ... \u2022 (t\u2212 xi) (1)\nso that for any x \u2208 Xj , we have G(x) = 0. And for a given real number r, there exists:\n\u03a6(t) = G(t) + r (2)\nso that for any x \u2208 Xj , we have \u03a6(x) = r.\nLemma one shows that for a given subset Xj of a n-dimensional real number space, there exists a family of equality constraints: \u03a6Xj(t, r | Xj), so that \u2200x \u2208 Xj and \u2200r \u2208 R we have \u03a6rXj(x | r,Xj) = r.\n3The notation \u039b represents the parameter space of mapping relation ML. 4 In this paper, this law is the only discussion which involves dynamicity of a mapping relation or usually being mentioned as \u201cLearning Process\u201d. 5Solution of this equation could not be an element of Xj , this equation \u201cG(t)\u201d is just an example which proofs the existence of equality constraint \u03a6.\nThe constraint family \u03a6Xj(t, r | Xj) can also be expressed as a column that includes infinitely many constraints:\nFurther more, constraint family \u03a6(t, r,Xk) is defined by the power set of the domain D (\u2200Xk \u2208 P(D)), each member of this family is \u03a6rXk(x) = r and the constraint family \u03a6(t, r,Xk) can also be expressed wit a matrix as shown in figure 11.\nProof of Corollary One\nKnown that mapping relation ML : D \u2192 O(O \u2282 Rn, D \u2282 Rm) satisfies law I, so that for possible learning result Y1 there exists an equality constraint \u03a6r1Y1 , and for a new learning result Y2, there exists infinity many equality constraints \u03a6r26=r1Y2 , so for a new learning result Yi, there always exist an equality constraint \u03a6 ri Yi\nwhich is defined by Yi and a real number ri (ri 6= r1, ri 6= r2). Then the constraint that all possible learning results follow can be expressed as:\nBecause Y1,Y2 to Yi are disjoint sets(law I ) and the corresponding formula \u03a6riYI (y) equals a unique real number, therefore H(y) is by definition a function. And there could be infinite many possible H(y). Thus H is the harvesting function which can harvest information from a mapping relation which follows law I."}, {"heading": "3.5 Corollary II", "text": "The composition of mapping relations ML which follows law I and its corresponding harvesting function H still follows law I.\nProof: Denote Fl = H \u25e6ML. Because ML follows law one then: \u2200XS \u2282 D,\u2203XN \u2282 D \\XS so that YS = ML(XS), YN = ML(XN ) and YN \u2229 YS = \u03c6 Then according to the definition of (H): H(y | y \u2208 YS) 6= H(y | y \u2208 YN ). Therefore we know \u2200XS \u2282 D,\u2203XN \u2282 D \\XS : FL(XS) 6= FL(XN )\nCorollary two indicates it is possible to represent different concepts of information type ML by using different real number."}, {"heading": "3.6 Corollary III", "text": "The composition of mapping relations ML and any of its harvesting function H is equivalent to a common constraint VC and\na mapping relation \u201dL\u201d which also satisfies law I.\nProof: Real numbers can be represent by parallel hyperplanes defined by a vector set VC . And because of corollary II we know: \u2200XS \u2282 D,\u2203XN \u2282 D \\XS : FL(XS) 6= FL(XN ) (corollary two) Therefore L(XS) \u2229 L(XN ) = \u03c6 (satisfies law I )\nThis corollary carries double meaning:\n1. Linear separability is the common constraint for all mapping relations that satisfy law one could convert to.\n2. Without the harvesting function, the learning result of mapping relation ML cannot be recognised, so the mapping relation FL (as shown in figure\n13) is supposed to be the mapping relation which could provide useful information eventually. However, in spite of the existence of infinitely many possible H, it is almost impossible to locate a suitable harvesting function without violating law two. Corollary three shows that VC is independent of any first order global information of FL (prior knowledge about the undetected concepts)6, therefore it has been so far the only known family of implementable harvesting functions7 and it cooperate only with mapping relation \u201cL\u201d. This corollary also explains researchers intuitive preference for linear separability."}, {"heading": "3.7 Explanatory Comment", "text": "This section explains in what sense these two laws and three corollaries above shall be required to be understood. For understanding the essence of these laws and corollaries, it is necessary to experience the detail of what information can be gained and what cannot from the viewpoint of being a mapping relation rather than being the creator of a mapping relation.\n1. Firstly, the definition of a dynamic system shall be generalised 8 as follows: A state space D (domain D \u2282 Rm), a set of parameters \u039b, and a rule M that specifies how each state will be mapped to another state space O (range O \u2282 Rn) with the changing of parameters. The rule M is a mapping relation whose domain is D\u00d7\u039b and whose codomain is D. Therefore, M : D \u00d7 \u039b \u2192 O(D \u2282 Rm, O \u2282 Rn). Mapping relation M takes two input M(x, \u03b1), where x \u2208 D \u2282 Rm is the outcome of each observation and \u03b1 \u2208 \u039b is a possible configuration of this mapping relation. The mapping relation discussed in this paper is referred as a certain configuration of a dynamic system M\u03b1(x | \u03b1 \u2208 \u039b), the dynamicity related topic is off-discussion in this paper9.\n2. For mapping relation ML : D \u2192 O, (D \u2282 Rm, O \u2282 Rn), m, n and the range of each dimension are assumed to be very large number, to infinity ideally 10. The reason of this assumption is straightforward. It is apparently that a domain D \u2282 R3 is less likely to contain less information than only 2 of its dimension are recorded and it is also less likely to contain less information than only the integer part of all dimensions are recorded. And the direct consequence of having a domain which contains only a small amount of information is that there will be not enough information to define possibly different concepts. An intuitive example of\n6It is a common sense that intelligent creatures are able to learn concepts from completely unfamiliar environment.\n7VC is a set of vectors. 8Not restricted in the dynamicity of time 9The only discussion that related to the dynamic property is law two.\n10The scenario shown in figure 14 illustrates the reason why the range of each dimension are ideally to be infinity.\nthis scenario would be when a person with a high degree of myopia accidentally loses his glasses. Since the domain is supposed to be big enough to carry large amounts of information, it is reasonable to assume that the dimensionality of the range is high enough to contain enough parallel hyperplanes, and the range of each dimensionality seems not very important at this stage, but it is directly related to the further analysis of mapping relation L. It is worth mentioning that shrinking the range of each dimensionality is clearly a strategy that enable a non-learning model which does not satisfy law one behaves like it is able to learn the domain.\nAs shown in figure 14 (picture 1), the function F \u2217 is defined on domain D1, and does not satisfy law one appearantly. But by shrinking the range of the only dimension of its domain, as shown in 14 (picture 2), now F\u2217 behaves like it satisfies law one almost perfectly. This strategy is usually being known as normalisation process in the field of machine learning, by doing so, application usually will be fed with pre-prepared data and can behave like it is able to define new concepts based on the appearances of the domain. However this pre-prepared process usually involves extensive knowledge or operation about the input and finial output of the application. Therefore, a learning model is expected to define concepts directly from the observation of high dimensional space with big range(ideally infinite) of each dimension.\n3. Law one together with corollary three indicate that harvest function VC could utilise global representations for all possible concepts without the existence of L. This implication seems irrational at first glance, however by assuming the well accepted learning model: human, is learning model as described in corollary three VC \u25e6 L, the correctness of this implication is undeniable. This implication equivalent to:\nPeople can see everything they could possible see in future any time.\nMost people do not realise this fact in their daily life, but this is the foundation of creative activities such as painting or sculpture 11. It is not hard to image that a painter would still be able to create masterpiece once he lose his sight and for normal people who do not master this painting skill the experience of dreaming strange-looking creatures is not unusual. In this case, when VC and L were being constructed, all dimensions of the domain contain same type of information and the appearances of a concept is fairly straightforward provided. What if the domain consists of different types of information? We know that people who merely remember the answer of certain examination or all past examinations will not be acknowledged as having learned the concept of the corresponding subject, on the other hand people who learned concepts of a certain subject cannot only give answers to every possible related questions, but also can see facts that cannot be seen by people who merely remember those answers. This is an example of non-learning model(memory system) and learning model that are related to high-level concept. In this case a high-level concept will be defined based on different type of information and apparently how different types of information being clustered will largely affect whether higher level concept can be effectively learned or not, and this belongs to the discussion of intelligence which beyond the scope of this paper 12.\n4. The generalised definition of Machine Learning Problem is: \u201cFinding desired dependence using a limited number of\nobservations \u201d[10]\nAnd this general description consists of three components[10] as shown in figure 15.\n11Dynamic property of a system is off-discussion here. 12From the viewpoint of a blind person, the description of the painting of \u201cThe Last Supper\u201d is full of articulation problem, equivalently in the teaching-learning process, for student who has no corresponding basic knowledge, the introduction of some concept would be also full of articulation problem. For most people, intelligence is about how different type of information can be related so that higher level concepts could be learned.\n\u2022 An generator (G) of vector x \u2208 Rn, x is a variable which represents possible outcome of independent observations and presumably x follows an unknown distribution F (X).\n\u2022 A supervisor S that takes x as input and returns an output value y. The mechanism of the supervisor F (y | x) is also unknown.\n\u2022 A learning machine that manipulates a family of functions Loss(y, y\u0302 = f(x,\u0398)).\nAll solutions of machine learning problems directly depend on people\u2019s hypothesis of f(x,\u0398) 13. Based on current observation and by implementing some training algorithm, the goal of training process is to find a desirable \u03b8 so that when facing new observation results the difference between y\u0302 and y can be minimised.\nIn this paper, a mapping relation is referred as a possible configuration of a dynamic system:M\u03b1(x | \u03b1 \u2208 \u039b), therefore if we treat the hypothesis f(x,\u0398) as a dynamic system, then any configuration of f : f\u03b8(x | \u03b8 \u2208 \u0398) is within the discussion of this paper. And from the viewpoint of a certain configuration f\u03b8(x | \u03b8 \u2208 \u0398), there is no difference among various machine learning concepts such as on-line, off-line, supervised, non-supervised, semi-supervised and so no. So law I, II and the following corollaries are eligible to be applied on the hypothesis f(x,\u0398) without regarding how a desirable result shall be obtained.\nThe necessary conditions for f(x,\u0398) to be a learning model are law I and law II, therefore our hypothesis about observation is the most critical part which affects whether f(x,\u0398) could be a learning model or not. When applying a probabilistic model to solving the object recognition problem, the hypothesis is based on our belief that there are unknown statistical laws which represent different concepts. Assume mapping relation Fhypo is able to mimic the unknown statistical law of the appearance of a cup perfectly, then the mapping relation we get is as follows:\n13 \u0398 is the parameter space of function f.\nAt this stage, Fhypo(Cup | observation; \u03b8 \u2208 \u0398) is able to give the probability of being a cup of each observation. What if we want Fhypo to give probabilities of being a dog and a cup of each observation respectively? Being different from our hypothesis in regression problem, there is no real number naturally related to different objects. Therefore we usually choose another way of constructing our hypothesis of observation as shown in figure 17.\nFhypo : R n \u2192 R\u00d7 I (I is the indicator set)\nHowever the introduction of this \u201cIndicator Set\u201d breaks law II directly.\nThe size of this indicator set is the first order global information that represents the information of the amount of objects which Fhypo can effectively identify, this information is given by us (the creator of this model) and without this information the model cannot reach its best performance after the training process, and the worst case is that the training process will not converge forever[10]. This example indicates that people should careful construct our hypothesis for getting a learning model, once too much information about the dataset was adopted, the corresponding training process is nothing but a high performance database(non-learning model) constructing program.\n5. Although hypothesis f(x,\u0398) being used for classification problems may not satisfy law II and I, the corresponding loss function Loss(y, f(x,\u0398))[10] are almost always learning models: Loss : X \u2282 D \u00d7 \u03b8 \u2192 R. The term \u201cLearning\u201d in machine learning problem means learning a desirable \u03b8 \u2208 \u0398. The term \u201cLearning\u201d in this paper means the ability of a mapping relation. Furthermore, in the field of classical machine learning, by manipulating Loss(\u0398 | X,Y ) based on a set of observation X,Y , our goal is to minimise the loss r \u2208 R during the training process. Hopefully after training, the loss Loss(y, f(D \\X | \u03b8)) will still be very small. It is obvious that knowledge about D (first order global information) is the key factor, because it is necessary for our hypothesis to have equivalent performance on current observation X and possible future observation D \\X.\n6. Since the existence of concepts are defined based on different appearances, the notion \u201dright\u201d or \u201dwrong\u201d is redundant in this situation. More precisely, for a learning model ML : D \u2192 O talking about whether the mapping relation x \u2192 y(x \u2208 X \u2282 D; y \u2208 Y = M(x)) is correct or not is meaningless, in other words, any criterion which can be used to test this mapping relation only provides first order global information with respect to ML. For example, When explaining the object recognition problem using the theoretical framework proposed in this paper, two seemingly counter-intuitive deductions are dataset separation and dataset merge problems.\n\u2022 Dataset separation problem: When dataset generated by one object is separated, there could be two different sets of invariant representations.\n\u2022 Dataset merge problem: When two datasets of different objects are merged together, there could be a new set of invariant representations.\nAbove description is the common intuitive understanding of these two problems, but there are two mistakes about this understanding:\n\u2022 The information generators is defined by the mapping relation, so these two problems are equivalent.\n\u2022 Since there is no observation independent concept, solely talking about the existence of certain concept is meaningless and this example shown in figure 18 is often known as over-fitting problem in the field of machine learning. However, from the viewpoint of a learning model there is no notion of \u201cright\u201d or \u201cwrong\u201d, therefore it is not a problem, it is a phenomenon which can be used to verify the validity of future implementation based on this theoretical framework.\nUp to this stage it has been considered to give a summary which explains relations among each sections of this paper, so that readers could have a better understanding of this paper.\nFirstly, by giving definitions of local and global information (representations), the intuitive understanding about the relation between observation and the reality can be explained in the framework of mapping relation. Based on this formalised expression of information, law I indicates that for a mapping relation to be a learning model it at least should not be a information black-hole. Law II confines possible information being used in constructing the mapping\nrelation so that from a mapping relations point of view it will not be able to utilise information which goes beyond its field of vision (first order global information).Corollary I proves the existence of a set of equivalent constraints which can harvest global information from a learning model and all possible sets of equivalent constraints are functions. Corollary II proves a learning model ML, together with any of its possible harvest function H still satisfies law I and different concepts of any type of information ML can be represented by different real numbers although there are infinite many possible choices and it could be impossible to find any of them. Corollary III proves it is possible to have a harvest function VC which is independent from any possible first order global information and all learning model ML has a corresponding family of mapping relation L that are also learning models."}, {"heading": "4 Conclusion and Future Works", "text": "Focusing on achieving functional similarities has been the basic guideline of top-down approaches since always and various machine learning techniques has been developed under this guideline. However, without a fundamental interpretation about what a given ability is, there could be dramatic differences between constructing a near-this-ability-performance system and constructing a havingthis-ability-performance system. It is just like researchers might eventually build a sky-facing Columbiad space gun which could shot three people to the Moon or even the Mars, but this should not bring researchers any confidence to claim that the space gun is the promising technique for interstellar travel. This paper gives two laws which are necessary conditions for any mapping relations to be acknowledged to have learning ability (to be a learning model) and illustrate these following facts:\n\u2022 \u201cLearning\u201d is the ability of identifying the existence of new concept.\n\u2022 A mapping relation ML will be acknowledged as a learning model of information ML only when it is able to possess this \u201clearning\u201d ability with no help from other learning models.\n\u2022 If the mathematical expression (model) of our hypothesis of observation is not being constructed carefully, information provided by us (the creator of the model) can exceed the vision of the model very easily and causes the model to be a non-learning model, therefore further development based on this model for achieving the human-level-learning ability can lead to only inevitable failure. For example, using labels to represent our observation in typical classification problems will directly leads our hypothesises of observation to failure of satisfying law II and I.\n\u2022 There exists a common learning model that utilises the power of hyperplanes.\nBy inspecting from the viewpoint of mapping relations and treating them equally (human, animal, apparatus), these key ideas of this paper can be appreciated more clearly.\nBecause the discussion of this paper is confined in certain configurations \u03b8 of a dynamic system f(x,\u0398), so there are two major future works: What is the family of L(x,\u0398); how to get a certain configuration of common learning model L(\u03b8 \u2208 \u0398) and how these parameters change dynamically. Further research on these two issues will address following questions:\n\u2022 Under what circumstances is hierarchy structure14 of a learning model necessary?\n\u2022 In a typical machine learning problem, the learning result is a \u03b8 \u2208 \u0398, however, for a learning model, compared with obtaining desirable \u03b8, a more valuable question is \u201chow a learning model encoding the unknown constraint of local representations of all concepts in the domain?\u201d.\n\u2022 How could possible existence of concepts depend on the information being contained in the domain quantitatively?\n\u2022 The dependency relation between learning model and non-learning model (memory system).\nThese days, swing into the saddle does not mean people are going on a long journey; juggling on the pavement is usually just for exercise. After the industrial revolution, people keep inventing all different kinds of machines which enable us to exceed the physical limitation of our biological blueprint. Therefore, the ability of invention, or more broadly speaking, intelligence is the proudest property of human and it has not been simulated by any man-made-machine successfully, yet. This dissertation is one of many steps to the inevitable future when human might not be the absolutely necessary information resource of automatic systems and we could harvest knowledge which cannot be provided by our learning ability.\n14Currently, there is no explanation about the necessity of having layer structure for both biological and artificial neural networks."}], "references": [{"title": "The Cosmic Compendium: Black Holes", "author": ["R.W. Anderson"], "venue": "LULU Press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The advent of the algorithm: The idea that rules the world", "author": ["David Berlinski"], "venue": "AMC, 10:12,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Deep neural networks rival the representation of primate it cortex for core visual object recognition", "author": ["Charles F Cadieu", "Ha Hong", "Daniel LK Yamins", "Nicolas Pinto", "Diego Ardila", "Ethan A Solomon", "Najib J Majaj", "James J DiCarlo"], "venue": "PLoS computational biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Subjectivity and lifeworld in transcendental phenomenology", "author": ["Sebastian Luft"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Demon-haunted world: science as a candle in the dark", "author": ["Carl Sagan"], "venue": "Ballantine Books,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Computing machinery and intelligence", "author": ["Alan M Turing"], "venue": "Mind, pages 433\u2013", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1950}, {"title": "The nature of statistical learning theory", "author": ["Vladimir Vapnik"], "venue": "springer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}], "referenceMentions": [{"referenceID": 3, "context": "Recently, represented by deep learning techniques, the field of machine learning is experiencing unprecedented prosperity and some applications with near human-level performance bring researchers confidence to imply that their approaches are the promising candidate for understanding the mechanism of human brain[4][3].", "startOffset": 312, "endOffset": 315}, {"referenceID": 2, "context": "Recently, represented by deep learning techniques, the field of machine learning is experiencing unprecedented prosperity and some applications with near human-level performance bring researchers confidence to imply that their approaches are the promising candidate for understanding the mechanism of human brain[4][3].", "startOffset": 315, "endOffset": 318}, {"referenceID": 6, "context": "Except for many philosophical discriptions and the famous black box test \u201cTuring test\u201d, there is no clear definition of intelligence and it is well accepted that the ability of \u201cthinking\u201d is difficult to define[9].", "startOffset": 210, "endOffset": 213}, {"referenceID": 1, "context": "Since all possible automated solutions implemented by computer systems are basically different computational models[2], it seems like there will be no computational model which could be acknowledged as possessing true intelligence forever.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Although we have no direct access to the world other than through our sensors [5], we can always rely on different kinds of apparatus to discover the world, in fact all apparatus can be regarded as extension of our biological sensors.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "But what if something cannot be detected by all means? Is it necessary to insist on its existence? The question has been answered perfectly by Carl Sagan\u2019s famous story \u201cThe Dragon In My Garage\u201d[8].", "startOffset": 194, "endOffset": 197}, {"referenceID": 7, "context": "The generalised definition of Machine Learning Problem is: \u201cFinding desired dependence using a limited number of observations \u201d[10]", "startOffset": 127, "endOffset": 131}, {"referenceID": 7, "context": "And this general description consists of three components[10] as shown in figure 15.", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "Figure 15: General model of learning from observation[10]", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "The size of this indicator set is the first order global information that represents the information of the amount of objects which Fhypo can effectively identify, this information is given by us (the creator of this model) and without this information the model cannot reach its best performance after the training process, and the worst case is that the training process will not converge forever[10].", "startOffset": 398, "endOffset": 402}, {"referenceID": 7, "context": "Although hypothesis f(x,\u0398) being used for classification problems may not satisfy law II and I, the corresponding loss function Loss(y, f(x,\u0398))[10] are almost always learning models: Loss : X \u2282 D \u00d7 \u03b8 \u2192 R.", "startOffset": 143, "endOffset": 147}], "year": 2015, "abstractText": "It has been quite a long time since AI researchers in the field of computer science stop talking about simulating human intelligence or trying to explain how brain works. Recently, represented by deep learning techniques, the field of machine learning is experiencing unprecedented prosperity and some applications with near human-level performance bring researchers confidence to imply that their approaches are the promising candidate for understanding the mechanism of human brain[4][3]. However apart from several ancient philological criteria and some imaginary black box tests (Turing test, Chinese room) there is no computational level explanation, definition or criteria about intelligence or any of its components. Base on the common sense that learning ability is one critical component of intelligence and inspect from the viewpoint of mapping relations, this paper presents two laws which explains what is the \u201clearning ability\u201d as we familiar with and under what conditions a mapping relation can be acknowledged as \u201cLearning Model\u201d.", "creator": "LaTeX with hyperref package"}}}