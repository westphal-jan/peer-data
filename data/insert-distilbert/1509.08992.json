{"id": "1509.08992", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets", "abstract": "inference is typically intractable again in high - treewidth undirected graphical models, making maximum likelihood learning a challenge. one way to overcome this is to restrict parameters to a tractable set, most typically the set of elementary tree - structured parameters. this paper explores an alternative notion of a tractable set, namely a set of \" fast - mixing parameters \" where markov chain monte carlo ( mcmc ) inference can be guaranteed to quickly converge to the stationary distribution. while it is common in practice proposals to approximate the likelihood gradient using samples obtained from mcmc, inherently such linear procedures lack theoretical guarantees. this paper proves that for any exponential family with bounded sufficient statistics, ( not yet just graphical models ) when parameters are constrained to a fast - mixing set, gradient descent with gradients approximated by sampling random will approximate the maximum likelihood solution inside the set with high - probability. when unregularized, to find a solution epsilon - accurate in log - likelihood requires a hypothetical total amount of effort cubic in 1 / epsilon, practically disregarding logarithmic factors. when ridge - regularized, strong convexity allows a solution epsilon - accurate in parameter distance with effort quadratic in 1 / epsilon. both of these provide of a fully - polynomial short time randomized approximation scheme.", "histories": [["v1", "Wed, 30 Sep 2015 01:44:41 GMT  (149kb)", "http://arxiv.org/abs/1509.08992v1", "Advances in Neural Information Processing Systems 2015"], ["v2", "Fri, 30 Oct 2015 07:29:08 GMT  (149kb)", "http://arxiv.org/abs/1509.08992v2", "Advances in Neural Information Processing Systems 2015"]], "COMMENTS": "Advances in Neural Information Processing Systems 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["justin domke"], "accepted": true, "id": "1509.08992"}, "pdf": {"name": "1509.08992.pdf", "metadata": {"source": "CRF", "title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets", "authors": ["Justin Domke"], "emails": ["justin.domke@nicta.com.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n08 99\n2v 1\n[ cs\n.L G\n] 3\n0 Se"}, {"heading": "1 Introduction", "text": "In undirected graphical models, maximum likelihood learning is intractable in general. For example, Jerrum and Sinclair [17] show that evaluation of the partition function (which can easily be computed from the likelihood) for an Ising model is #P-complete, and that even the existence of a fullypolynomial time randomized approximation scheme (FPRAS) for the partition function would imply that RP = NP.\nIf the model is well-specified (meaning that the target distribution falls in the assumed family) then there exist several methods that can efficiently recover correct parameters, among them the pseudolikelihood [3], score matching [16, 22], composite likelihoods [20, 30], Mizrahi et al.\u2019s [23] method based on parallel learning in local clusters of nodes and Abbeel et al.\u2019s [1] method based on matching local probabilities. While often useful, these methods have some drawbacks. First, these methods typically have inferior sample complexity to the likelihood. Second, these all assume a well-specified model. If the target distribution is not in the assumed class, the maximum-likelihood solution will converge to the M-projection (minimum of the KL-divergence), but these estimators do not have similar guarantees. Third, even when these methods succeed, they typically yield a distribution in which inference is still intractable, and so it may be infeasible to actually make use of the learned distribution.\nGiven these issues, a natural approach is to restrict the graphical model parameters to a tractable set \u0398, in which learning and inference can be performed efficiently. The gradient of the likelihood is determined by the marginal distributions, whose difficulty is typically determined by the treewidth of the graph. Thus, probably the most natural tractable family is the set of tree-structured distri-\nbutions, where \u0398 = {\u03b8 : \u2203tree T, \u2200(i, j) 6\u2208 T, \u03b8ij = 0}. The Chow-Liu algorithm [6] provides an efficient method for finding the maximum likelihood parameter vector \u03b8 in this set, by computing the mutual information of all empirical pairwise marginals, and finding the maximum spanning tree. Similarly, Heinemann and Globerson [13] give a method to efficiently learn high-girth models where correlation decay limits the error of approximate inference, though this will not converge to the M-projection when the model is mis-specified.\nThis paper considers a fundamentally different notion of tractability, namely a guarantee that Markov chain Monte Carlo (MCMC) sampling will quickly converge to the stationary distribution. Our fundamental result is that if \u0398 is such a set, and one can project onto \u0398, then there exists a FPRAS for the maximum likelihood solution inside \u0398. While inspired by graphical models, this result works entirely in the exponential family framework, and applies generally to any exponential family with bounded sufficient statistics.\nThe existence of a FPRAS is established by analyzing a common existing strategy for maximum likelihood learning of exponential families, namely gradient descent where MCMC is used to generate samples and approximate the gradient. It is natural to conjecture that, if the Markov chain is fast mixing, is run long enough, and enough gradient descent iterations are used, this will converge to nearly the optimum of the likelihood inside \u0398, with high probability. This paper shows that this is indeed the case. A separate analysis is used for the ridge-regularized case (using strong convexity) and the unregularized case (which is merely convex)."}, {"heading": "2 Setup", "text": "Though notation is introduced when first used, the most important symbols are given here for more reference.\n\u2022 \u03b8 - parameter vector to be learned \u2022 M\u03b8 - Markov chain operator corresponding to \u03b8 \u2022 \u03b8k - estimated parameter vector at k-th gradient descent iteration \u2022 qk = Mv\u03b8k\u22121r - approximate distribution sampled from at iteration k. (v iterations of the\nMarkov chain corresponding to \u03b8k\u22121 from arbitrary starting distribution r.)\n\u2022 \u0398 - constraint set for \u03b8 \u2022 f - negative log-likelihood on training data \u2022 L - Lipschitz constant for the gradient of f . \u2022 \u03b8\u2217 = argmin\u03b8\u2208\u0398 f(\u03b8) - minimizer of likelihood inside of \u0398 \u2022 K - total number of gradient descent steps \u2022 M - total number of samples drawn via MCMC \u2022 N - length of vector x. \u2022 v - number of Markov chain transitions applied for each sample \u2022 C,\u03b1 - parameters determining the mixing rate of the Markov chain. (Equation 3) \u2022 Ra - sufficient statistics norm bound. \u2022 \u01ebf - desired optimization accuracy for f \u2022 \u01eb\u03b8 - desired optimization accuracy for \u03b8 \u2022 \u03b4 - permitted probability of failure to achieve a given approximation accuracy\nThis paper is concerned with an exponential family of the form\np\u03b8(x) = exp(\u03b8 \u00b7 t(x)\u2212A(\u03b8)), where t(x) is a vector of sufficient statistics, and the log-partition function A(\u03b8) ensures normalization. An undirected model can be seen as an exponential family where t consists of indicator functions for each possible configuration of each clique [32]. While such graphical models motivate this work, the results are most naturally stated in terms of an exponential family and apply more generally.\nWe are interested in performing maximum-likelihood learning, i.e. minimizing, for a dataset z1, ..., zD,\nf(\u03b8) = \u2212 1 D\nD \u2211\ni=1\nlog p\u03b8(zi) + \u03bb\n2 \u2016\u03b8\u201622 = A(\u03b8)\u2212 \u03b8 \u00b7 t\u0304+\n\u03bb 2 \u2016\u03b8\u201622, (1)\nwhere we define t\u0304 = 1D \u2211D i=1 t(zi). It is easy to see that the gradient of f takes the form\nf \u2032(\u03b8) = Ep\u03b8 [t(X)]\u2212 t\u0304+ \u03bb\u03b8.\nIf one would like to optimize f using a gradient-based method, computing the expectation of t(X) with respect to p\u03b8 can present a computational challenge. With discrete graphical models, the expected value of t is determined by the marginal distributions of each factor in the graph. Typically, the computational difficulty of computing these marginal distributions is determined by the treewidth of the graph\u2013 if the graph is a tree, (or close to a tree) the marginals can be computed by the junction-tree algorithm [18]. One option, with high treewidth, is to approximate the marginals with a variational method. This can be seen as exactly optimizing a \u201csurrogate likelihood\u201d approximation of Eq. 1 [31].\nAnother common approach is to use Markov chain Monte Carlo (MCMC) to compute a sample {xi}Mi=1 from a distribution close to p\u03b8, and then approximate Ep\u03b8 [t(X)] by (1/M) \u2211M i=1 t(xi). This strategy is widely used, varying in the model type, the sampling algorithm, how samples are initialized, the details of optimization, and so on [10, 25, 27, 24, 7, 33, 11, 2, 29, 5]. Recently, Steinhardt and Liang [28] proposed learning in terms of the stationary distribution obtained from a chain with a nonzero restart probability, which is fast-mixing by design.\nWhile popular, such strategies generally lack theoretical guarantees. If one were able to exactly sample from p\u03b8, this could be understood simply as stochastic gradient descent. But, with MCMC, one can only sample from a distribution approximating p\u03b8, meaning the gradient estimate is not only noisy, but also biased. In general, one can ask how should the step size, number of iterations, number of samples, and number of Markov chain transitions be set to achieve a convergence level.\nThe gradient descent strategy analyzed in this paper, in which one updates a parameter vector \u03b8k using approximate gradients is outlined and shown as a cartoon in Figure 1. Here, and in the rest of the paper, we use pk as a shorthand for p\u03b8k , and we let ek denote the difference between the estimated gradient and the true gradient f \u2032(\u03b8k\u22121). The projection operator is defined by \u03a0\u0398[\u03c6] = argmin\u03b8\u2208\u0398 ||\u03b8 \u2212 \u03c6||2. We assume that the parameter set \u03b8 is constrained to a set \u0398 such that MCMC is guaranteed to mix at a certain rate (Section 3.1). With convexity, this assumption can bound the mean and variance\nof the errors at each iteration, leading to a bound on the sum of errors. With strong convexity, the error of the gradient at each iteration is bounded with high probability. Then, using results due to [26] for projected gradient descent with errors in the gradient, we show a schedule the number of iterations K , the number of samples M , and the number of Markov transitions v such that with high probability,\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f (\u03b8\u2217) \u2264 \u01ebf or \u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 \u01eb\u03b8,\nfor the convex or strongly convex cases, respectively, where \u03b8\u2217 \u2208 argmin\u03b8\u2208\u0398 f(\u03b8). The total number of Markov transitions applied through the entire algorithm, KMv grows as (1/\u01ebf)3 log(1/\u01ebf) for the convex case, (1/\u01eb2\u03b8) log(1/\u01eb 2 \u03b8) for the strongly convex case, and polynomially in all other parameters of the problem."}, {"heading": "3 Background", "text": ""}, {"heading": "3.1 Mixing times and Fast-Mixing Parameter Sets", "text": "This Section discusses some background on mixing times for MCMC. Typically, mixing times are defined in terms of the total-variation distance \u2016p\u2212q\u2016TV = maxA |p(A)\u2212q(A)|, where the maximum ranges over the sample space. For discrete distributions, this can be shown to be equivalent to \u2016p\u2212 q\u2016TV = 12 \u2211\nx |p(x)\u2212 q(x)|. We assume that a sampling algorithm is known, a single iteration of which can be thought of an operator M\u03b8 that transforms some starting distribution into another. The stationary distribution is p\u03b8, i.e. limv\u2192\u221e Mv\u03b8q = p\u03b8 for all q. Informally, a Markov chain will be fast mixing if the total variation distance between the starting distribution and the stationary distribution decays rapidly in the length of the chain. This paper assumes that a convex set \u0398 and constants C and \u03b1 are known such that for all \u03b8 \u2208 \u0398 and all distributions q,\n\u2016Mv\u03b8q \u2212 p\u03b8\u2016TV \u2264 C\u03b1v. (2) This means that the distance between an arbitrary starting distribution q and the stationary distribution p\u03b8 decays geometrically in terms of the number of Markov iterations v. This assumption is justified by the Convergence Theorem [19, Theorem 4.9], which states that if M is irreducible and aperiodic with stationary distribution p, then there exists constants \u03b1 \u2208 (0, 1) and C > 0 such that\nd(v) := sup q\n\u2016Mvq \u2212 p\u2016TV \u2264 C\u03b1v. (3)\nMany results on mixing times in the literature, however, are stated in a less direct form. Given a constant \u01eb, the mixing time is defined by \u03c4(\u01eb) = min{v : d(v) \u2264 \u01eb}. It often happens that bounds on mixing times are stated as something like \u03c4(\u01eb) \u2264 \u2308\na+ b ln 1\u01eb \u2309\nfor some constants a and b. It follows from this that \u2016Mvq \u2212 p\u2016TV \u2264 C\u03b1v with C = exp(a/b) and \u03b1 = exp(\u22121/b). A simple example of a fast-mixing exponential family is the Ising model, defined for x \u2208 {\u22121,+1}N as\np(x|\u03b8) = exp\n\n\n\u2211\n(i,j)\u2208Pairs \u03b8ijxixj +\n\u2211\ni\n\u03b8ixi \u2212A(\u03b8)\n\n .\nA simple result for this model is that, if the maximum degree of any node is \u2206 and |\u03b8ij | \u2264 \u03b2 for all (i, j), then for univariate Gibbs sampling with random updates, \u03c4(\u01eb) \u2264 \u2308 N log(N/\u01eb)1\u2212\u2206tanh(\u03b2)\u2309 [19]. The algorithm discussed in this paper needs the ability to project some parameter vector \u03c6 onto \u0398 to find argmin\u03b8\u2208\u0398 ||\u03b8\u2212\u03c6||2. Projecting a set of arbitrary parameters onto this set of fast-mixing parameters is trivial\u2013 simply set \u03b8ij = \u03b2 for \u03b8ij > \u03b2 and \u03b8ij \u2190 \u2212\u03b2 for \u03b8ij < \u2212\u03b2. For more dense graphs, it is known [12, 9] that, for a matrix norm \u2016\u00b7\u2016 that is the spectral norm \u2016\u00b7\u20162, or induced 1 or infinity norms,\n\u03c4(\u01eb) \u2264 \u2308 N log(N/\u01eb)\n1\u2212 \u2016R(\u03b8)\u2016\n\u2309\n(4)\nwhere Rij(\u03b8) = |\u03b8ij |. Domke and Liu [8] show how to perform this projection for the Ising model when \u2016 \u00b7 \u2016 is the spectral norm \u2016 \u00b7 \u20162 with a convex optimization utilizing the singular value decomposition in each iteration.\nLoosely speaking, the above result shows that univariate Gibbs sampling on the Ising model is fastmixing, as long as the interaction strengths are not too strong. Conversely, Jerrum and Sinclair [17] exhibited an alternative Markov chain for the Ising model that is rapidly mixing for arbitrary interaction strengths, provided the model is ferromagnetic, i.e. that all interaction strengths are positive with \u03b8ij \u2265 0 and that the field is unidirectional. This Markov chain is based on sampling in different \u201csubgraphs world\u201d state-space. Nevertheless, it can be used to estimate derivatives of the Ising model log-partition function with respect to parameters, which allows estimation of the gradient of the log-likelihood. Huber [15] provided a simulation reduction to obtain an Ising model sample from a subgraphs world sample.\nMore generally, Liu and Domke [21] consider a pairwise Markov random field, defined as\np(x|\u03b8) = exp\n\n\n\u2211\ni,j\n\u03b8ij(xi, xj) + \u2211\ni\n\u03b8i(xi)\u2212A(\u03b8)\n\n ,\nand show that, if one defines Rij(\u03b8) = maxa,b,c 12 |\u03b8ij(a, b)\u2212\u03b8ij(a, c)|, then again Equation 4 holds. An algorithm for projecting onto the set \u0398 = {\u03b8 : \u2016R(\u03b8)\u2016 \u2264 c} exists. There are many other mixing-time bounds for different algorithms, and different types of models [19]. The most common algorithms are univariate Gibbs sampling (often called Glauber dynamics in the mixing time literature) and Swendsen-Wang sampling. The Ising model and Potts models are the most common distributions studied, either with a grid or fully-connected graph structure. Often, the motivation for studying these systems is to understand physical systems, or to mathematically characterize phase-transitions in mixing time that occur as interactions strengths vary. As such, many existing bounds assume uniform interaction strengths. For all these reasons, these bounds typically require some adaptation for a learning setting."}, {"heading": "4 Main Results", "text": ""}, {"heading": "4.1 Lipschitz Gradient", "text": "For lack of space, detailed proofs are postponed to the appendix. However, informal proof sketches are provided to give some intuition for results that have longer proofs. Our first main result is that the regularized log-likelihood has a Lipschitz gradient.\nTheorem 1. The regularized log-likelihood gradient is L-Lipschitz with L = 4R22 + \u03bb, i.e.\n\u2016f \u2032(\u03b8)\u2212 f \u2032(\u03c6)\u20162 \u2264 (4R22 + \u03bb)\u2016\u03b8 \u2212 \u03c6\u20162.\nProof sketch. It is easy, by the triangle inequality, that \u2016f \u2032(\u03b8)\u2212f \u2032(\u03c6)\u20162 \u2264 \u2016 dAd\u03b8 \u2212 dAd\u03c6 \u20162+\u03bb\u2016\u03b8\u2212\u03c6\u20162. Next, using the assumption that \u2016t(x)\u20162 \u2264 R2, one can bound that \u2016 dAd\u03b8 \u2212 dAd\u03c6 \u20162 \u2264 2R2\u2016p\u03b8\u2212p\u03c6\u2016TV . Finally, some effort can bound that \u2016p\u03b8 \u2212 p\u03c6\u2016TV \u2264 2R2\u2016\u03b8 \u2212 \u03c6\u20162."}, {"heading": "4.2 Convex convergence", "text": "Now, our first major result is a guarantee on the convergence that is true both in the regularized case where \u03bb > 0 and the unregularized case where \u03bb = 0.\nTheorem 2. With probability at least 1\u2212 \u03b4, at long as M \u2265 3K/ log(1\u03b4 ), Algorithm 1 will satisfy\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 8R 2 2\nKL\n( L\u2016\u03b80 \u2212 \u03b8\u2217\u20162 4R2 + log 1 \u03b4 + K\u221a M +KC\u03b1v )2 .\nProof sketch. First, note that f is convex, since the Hessian of f is the covariance of t(X) when \u03bb = 0 and \u03bb > 0 only adds a quadratic. Now, define the quantity dk = 1M \u2211M m=1 t(X k m) \u2212\nEqk [t(X)] to be the difference between the estimated expected value of t(X) under qk and the true value. An elementary argument can bound the expected value of \u2016dk\u2016, while the Efron-Stein inequality can bounds its variance. Using both of these bounds in Bernstein\u2019s inequality can then show that, with probability 1 \u2212 \u03b4,\n\u2211K k=1 \u2016dk\u2016 \u2264 2R2(K/ \u221a M + log 1\u03b4 ). Finally, we can observe\nthat \u2211K k=1 \u2016ek\u2016 \u2264 \u2211K k=1 \u2016dk\u2016+ \u2211K k=1 \u2016Eqk [t(X)]\u2212Ep\u03b8k [t(X)]\u20162. By the assumption on mixing speed, the last term is bounded by 2KR2C\u03b1v . And so, with probability 1 \u2212 \u03b4, \u2211K k=1 \u2016ek\u2016 \u2264\n2R2(K/ \u221a M + log 1\u03b4 ) + 2KR2C\u03b1\nv. Finally, a result due to Schmidt et al. [26] on the convergence of gradient descent with errors in estimated gradients gives the result.\nIntuitively, this result has the right character. If M grows on the order of K2 and v grows on the order of logK/(\u2212 log\u03b1), then all terms inside the quadratic will be held constant, and so if we set K of the order 1/\u01eb, the sub-optimality will on the order of \u01eb with a total computational effort roughly on the order of (1/\u01eb)3 log(1/\u01eb). The following results pursue this more carefully. Firstly, one can observe that a minimum amount of work must be performed.\nTheorem 3. For a, b, c, \u03b1 > 0, if K,M, v > 0 are set so that 1K (a+ b K\u221a M +Kc\u03b1v)2 \u2264 \u01eb, then\nKMv \u2265 a 4b2 \u01eb3 log ac\u01eb (\u2212 log\u03b1) .\nSince it must be true that a/ \u221a K + b \u221a K/M + \u221a Kc\u03b1v \u2264 \u221a\u01eb, each of these three terms must also be at most \u221a \u01eb, giving lower-bounds on K , M , and v. Multiplying these gives the result.\nNext, an explicit schedule for K, M , and v is possible, in terms of a convex set of parameters \u03b21, \u03b22, \u03b23. Comparing this to the lower-bound above shows that this is not too far from optimal.\nTheorem 4. Suppose that a, b, c, \u03b1 > 0. If \u03b21 + \u03b22 + \u03b23 = 1, \u03b21, \u03b22, \u03b23 > 0, then setting K = a 2\n\u03b22 1 \u01eb , M = ( ab\u03b21\u03b22\u01eb ) 2, v = log ac\u03b21\u03b23\u01eb/(\u2212 log\u03b1) is sufficient to guarantee that 1 K (a + b K\u221a M +\nKc\u03b1v)2 \u2264 \u01eb with a total work of\nKMv = 1\n\u03b241\u03b2 2 2\na4b2\n\u01eb3 log ac\u03b21\u03b23\u01eb (\u2212 log\u03b1) .\nSimply verify that the \u01eb bound holds, and multiply the terms together.\nFor example, setting \u03b21 = 0.66, \u03b22 = 0.33 and \u03b23 = 0.01 gives that KMv \u2248 48.4a 4b2 \u01eb3 log ac \u01eb +5.03 (\u2212 log\u03b1) . Finally, we can give an explicit schedule for K , M , and v, and bound the total amount of work that needs to be performed.\nTheorem 5. If D \u2265 max ( \u2016\u03b80 \u2212 \u03b8\u2217\u20162, 4R2L log 1\u03b4 ) , then for all \u01eb there is a setting of K,M, v such that f( 1K \u2211K k=1 \u03b8k)\u2212 f(\u03b8\u2217) \u2264 \u01ebf with probability 1\u2212 \u03b4 and\nKMv \u2264 32LR 2 2D 4\n\u03b241\u03b2 2 2\u01eb 3 f(1 \u2212 \u03b1)\nlog 4DR2C\n\u03b21\u03b23\u01ebf .\n[Proof sketch] This follows from setting K , M , and v as in Theorem 4 with a = L\u2016\u03b80 \u2212 \u03b8\u2217\u20162/(4R2) + log 1\u03b4 , b = 1, c = C, and \u01eb = \u01ebfL/(8R22)."}, {"heading": "4.3 Strongly Convex Convergence", "text": "This section gives the main result for convergence that is true only in the regularized case where \u03bb > 0. Again, the main difficulty in this proof is showing that the sum of the errors of estimated gradients at each iteration is small. This is done by using a concentration inequality to show that the error of each estimated gradient is small, and then applying a union bound to show that the sum is small. The main result is as follows.\nTheorem 6. When the regularization constant obeys \u03bb > 0, with probability at least 1\u2212\u03b4 Algorithm 1 will satisfy\n\u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 (1\u2212 \u03bb L )K\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + L \u03bb\n( \u221a\nR2 2M\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)\n+ 2R2C\u03b1 v\n)\n.\nProof sketch. When \u03bb = 0, f is convex (as in Theorem 2) and so is strongly convex when \u03bb > 0. The basic proof technique here is to decompose the error in a particular step as \u2016ek+1\u20162 \u2264 \u2016 1M \u2211M i=1 t(x k i )\u2212 Eqk [t(X)]\u20162 + \u2016Eqk [t(X)]\u2212 Ep\u03b8k [t(X)]\u20162. A multidimensional variant of Hoeffding\u2019s inequality can bound the first term, with probability 1 \u2212 \u03b4\u2032 by R2(1 + \u221a 2 log 1\u03b4 )/ \u221a M , while our assumption on mixing speed can bound the second term by 2R2C\u03b1v . Applying this to all iterations using \u03b4\u2032 = \u03b4/K gives that all errors are simultaneously bounded as before. This can then be used in another result due to Schmidt et al. [26] on the convergence of gradient descent with errors in estimated gradients in the strongly convex case.\nA similar proof strategy could be used for the convex case where, rather than directly bounding the sum of the norm of errors of all steps using the Efron-Stein inequality and Bernstein\u2019s bound, one could simply bound the error of each step using a multidimensional Hoeffding-type inequality, and then apply this with probability \u03b4/K to each step. This yields a slightly weaker result than that shown in Theorem 2. The reason for applying a uniform bound on the errors in gradients here is that Schmidt et al.\u2019s bound [26] on the convergence of proximal gradient descent on strongly convex functions depends not just on the sum of the norms of gradient errors, but a non-uniform weighted variant of these.\nAgain, we consider how to set parameters to guarantee that \u03b8K is not too far from \u03b8\u2217 with a minimum amount of work. Firstly, we show a lower-bound.\nTheorem 7. Suppose a, b, c > 0. Then for anyK,M, v such that \u03b3Ka+ b\u221a M\n\u221a\nlog(K/\u03b4)+c\u03b1v \u2264 \u01eb. it must be the case that\nKMv \u2265 b 2 \u01eb2 log a\u01eb log c \u01eb (\u2212 log \u03b3)(\u2212 log\u03b1) log ( log a\u01eb \u03b4(\u2212 log \u03b3) ) .\n[Proof sketch] This is established by noticing that \u03b3Ka, b\u221a M\n\u221a\nlog K\u03b4 , and c\u03b1 v must each be less\nthan \u01eb, giving lower bounds on K , M , and v.\nNext, we can give an explicit schedule that is not too far off from this lower-bound. Theorem 8. Suppose that a, b, c, \u03b1 > 0. If \u03b21 + \u03b22 + \u03b23 = 1, \u03b2i > 0, then setting K =\nlog( a\u03b21\u01eb)/(\u2212 log \u03b3), M = b2\n\u01eb2\u03b22 2\n(\n1 + \u221a 2 log(K/\u03b4) )2 and v = log (\nc \u03b23\u01eb\n)\n/(\u2212 log\u03b1) is sufficient to guarantee that \u03b3Ka+ b\u221a\nM (1 +\n\u221a\n2 log(K/\u03b4)) + c\u03b1v \u2264 \u01eb with a total work of at most\nKMV \u2264 b 2\n\u01eb2\u03b222\nlog (\na \u03b21\u01eb\n) log (\nc \u03b23\u01eb\n)\n(\u2212 log \u03b3)(\u2212 log\u03b1)\n\n1 +\n\u221a\n2 log log( a\u03b21\u01eb )\n\u03b4(\u2212 log \u03b3)\n\n\n2\n.\nFor example, if you choose \u03b22 = 1/ \u221a 2 and \u03b21 = \u03b23 = (1 \u2212 1/ \u221a 2)/2 \u2248 0.1464, then this varies from the lower-bound in Theorem 7 by a factor of two, and a multiplicative factor of 1/\u03b23 \u2248 6.84 inside the logarithmic terms.\nCorollary 9. If we choose K \u2265 L\u03bb log ( \u2016\u03b80\u2212\u03b8\u20162 \u03b21\u01eb ) , M \u2265 L2R2 2\u01eb2\u03b22\n2 \u03bb2\n(\n1 + \u221a 2 log(K/\u03b4) )2\n, and v \u2265 1\n1\u2212\u03b1 log (2LR2C/(\u03b23\u01eb\u03bb)), then \u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 \u01eb\u03b8 with probability at least 1 \u2212 \u03b4, and the total amount of work is bounded by\nKMv \u2264 L 3R2\n2\u01eb2\u03b8\u03b2 2 2\u03bb\n3(1 \u2212 \u03b1) log (\u2016\u03b80 \u2212 \u03b8\u20162\n\u03b21\u01eb\u03b8\n)\n(\n1 +\n\u221a\n2 log\n(\nL \u03bb\u03b4 log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb\u03b8 ))\n)2\n."}, {"heading": "5 Discussion", "text": "An important detail in the previous results is that the convex analysis gives convergence in terms of the regularized log-likelihood, while the strongly-convex analysis gives convergence in terms of the parameter distance. If we drop logarithmic factors, the amount of work necessary for \u01ebf - optimality in the log-likelihood using the convex algorithm is of the order 1/\u01eb3f , while the amount of work necessary for \u01eb\u03b8 - optimality using the strongly convex analysis is of the order 1/\u01eb2\u03b8. Though these quantities are not directly comparable, the standard bounds on sub-optimality for \u03bb-strongly convex functions with L-Lipschitz gradients are that \u03bb\u01eb2\u03b8/2 \u2264 \u01ebf \u2264 L\u01eb2\u03b8/2. Thus, roughly speaking, when regularized for the strongly-convex analysis shows that \u01ebf optimality in the log-likelihood can be achieved with an amount of work only linear in 1/\u01ebf ."}, {"heading": "6 Example", "text": "While this paper claims no significant practical contribution to practice, it is useful to visualize an example. Take an Ising model p(x) \u221d exp(\u2211(i,j)\u2208Pairs \u03b8ijxixj) for xi \u2208 {\u22121, 1} on a 4 \u00d7 4 grid with 5 random vectors as training data. The sufficient statistics are t(x) = {xixj |(i, j) \u2208 Pairs}, and with 24 pairs, \u2016t(x)\u20162 \u2264 R2 = \u221a 24. For a fast-mixing set, constrain |\u03b8ij | \u2264 .2 for all pairs. Since the maximum degree is 4, \u03c4(\u01eb) \u2264 \u2308 N log(N/\u01eb)1\u22124 tanh(.2)\u2309 . Fix \u03bb = 1, \u01eb\u03b8 = 2 and \u03b4 = 0.1. Though the theory above suggests the Lipschitz constant L = 4R22 + \u03bb = 97, a lower value of L = 10 is used, which converged faster in practice (with exact or approximate gradients). Now, one can derive that \u2016\u03b80 \u2212 \u03b8\u2217\u20162 \u2264 D = \u221a\n24\u00d7 (2\u00d7 .2)2, C = log(16) and \u03b1 = exp(\u2212(1 \u2212 4 tanh .2)/16). Applying Corollary 9 with \u03b21 = .01, \u03b22 = .9 and \u03b23 = .1 gives K = 46, M = 1533 and v = 561. Fig. 2 shows the results. In practice, the algorithm finds a solution tighter than the specified \u01eb\u03b8, indicating a degree of conservatism in the theoretical bound."}, {"heading": "7 Conclusions", "text": "This section discusses some weaknesses of the above analysis, and possible directions for future work. Analyzing complexity in terms of the total sampling effort ignores the complexity of projection itself. Since projection only needs to be done K times, this time will often be very small in comparison to sampling time. (This is certainly true in the above example.) However, this might not be the case if the projection algorithm scales super-linearly in the size of the model.\nAnother issue to consider is how the samples are initialized. As far as the proof of correctness goes, the initial distribution r is arbitrary. In the above example, a simple uniform distribution was used. However, one might use the empirical distribution of the training data, which is equivalent to contrastive divergence [5]. It is reasonable to think that this will tend to reduce the mixing time when the p\u03b8 is close to the model generating the data. However, the number of Markov chain transitions v prescribed above is larger than typically used with contrastive divergence, and Algorithm 1 does not reduce the step size over time. While it is common to regularize to encourage fast mixing with contrastive divergence [14, Section 10], this is typically done with simple heuristic penalties. Further, contrastive divergence is often used with hidden variables. Still, this provides a bound for how closely a variant of contrastive divergence could approximate the maximum likelihood solution.\nThe above analysis does not encompass the common strategy for maximum likelihood learning where one maintains a \u201cpool\u201d of samples between iterations, and initializes one Markov chain at each iteration from each element of the pool. The idea is that if the samples at the previous iteration were close to pk\u22121 and pk\u22121 is close to pk, then this provides an initialization close to the current solution. However, the proof technique used here is based on the assumption that the samples xki at each iteration are independent, and so cannot be applied to this strategy."}, {"heading": "Acknowledgements", "text": "Thanks to Ivona Bez\u00e1kov\u00e1, Aaron Defazio, Nishant Mehta, Aditya Menon, Cheng Soon Ong and Christfried Webers. NICTA is funded by the Australian Government through the Dept. of Communications and the Australian Research Council through the ICT Centre of Excellence Program."}, {"heading": "8 Background", "text": ""}, {"heading": "8.1 Optimization", "text": "The main results in this paper rely strongly on the work of Schmidt et al. [26] on the convergence of proximal gradient methods with errors in estimated gradients. The first result used is the following theorem for the convergence of gradient descent on convex functions with errors in the estimated gradients.\nTheorem 10. (Special case of [26, Proposition 1]) Suppose that a function f is convex with an L-Lipshitz gradient (meaning \u2016f \u2032(\u03c6) \u2212 f \u2032(\u03b8)\u20162 \u2264 L\u2016\u03c6\u2212 \u03b8\u20162). If \u0398 is a closed convex set and one iterates\n\u03b8k \u2190 \u03a0\u0398 [ \u03b8k\u22121 \u2212 1\nL (f \u2032(\u03b8k\u22121) + ek)\n]\n,\nthen, defining \u03b8\u2217 \u2208 argmin\u03b8\u2208\u0398 f(\u03b8), for all K \u2265 1, we have, for AK := \u2211K k=1 \u2016ek\u2016 L , that\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 L 2K (\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 2AK)2 .\nThis section will show that this is indeed a special case of .[26] To start with, we simply restate exactly the previous result [26, Proposition 1], with only trivial changes in notation.\nTheorem 11. Assume that:\n\u2022 f is convex and has L-Lipschitz continuous gradient\n\u2022 h is a lower semi-continuous proper convex function.\n\u2022 The function r = f + h attains it\u2019s minimum at a certain \u03b8\u2217 \u2208 Rn.\n\u2022 \u03b8k is an \u01ebk-optimal solution, i.e. that L\n2 \u2016\u03b8k \u2212 y\u20162 + h(\u03b8k) \u2264 \u01ebk + min \u03b8\u2208Rn L 2 \u2016\u03b8 \u2212 y\u20162 + h(\u03b8)\nwhere\ny = \u03b8k\u22121 \u2212 1 L (f \u2032(\u03b8k\u22121) + ek) .\nThen, for all K \u2265 1, one has that\nr\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 r(\u03b8\u2217) \u2264 L 2K ( \u2016\u03b80 \u2212 \u03b8\u2217\u2016+ 2AK + \u221a 2BK\n)2\nwith\nAK =\nK \u2211\nk=1\n(\n\u2016ek\u2016 L +\n\u221a\n2\u01ebk L\n)\n, BK =\nK \u2211\nk=1\n\u01ebk K .\nThe first theorem follows from this one by setting h to be the indicator function for the set \u0398, i.e.\nh(\u03b8) =\n{\n0 \u03b8 \u2208 \u0398 \u221e \u03b8 6\u2208 \u0398\nand assuming that \u01ebk = 0. By the convexity of \u0398, h will be a lower semi-continuous proper convex function. Further, from the fact that \u0398 is closed, r will attain its minimum. Now, we verify that this\nresults in the theorem statement at the start of this section. \u03b8k takes the form\n\u03b8k = arg min \u03b8\u2208Rn\nL 2 \u2016\u03b8 \u2212 y\u20162 + h(\u03b8)\n= argmin \u03b8\u2208\u0398\n\u2016\u03b8 \u2212 y\u2016\n= argmin \u03b8\u2208\u0398\n\u2016\u03b8 \u2212 \u03b8k\u22121 + 1 L (f \u2032(\u03b8k\u22121) + ek) \u2016\n= \u03a0\u0398\n[\n\u03b8k\u22121 \u2212 1 L (f \u2032(\u03b8k\u22121) + ek)\n]\n.\nWe will also use the following result for strongly-convex optimzation. The special case follows from the same construction used above.\nNext, consider the following result on optimization of strongly convex functions, which follows from [26] by a very similar argument.\nTheorem 12. (Special case of [26, Proposition 3]) Suppose that a function f is \u03bb-strongly convex with an L-Lipshitz gradient (meaning \u2016f \u2032(\u03c6) \u2212 f \u2032(\u03b8)\u20162 \u2264 L\u2016\u03c6\u2212 \u03b8\u20162). If \u0398 is a closed convex set and one iterates\n\u03b8k \u2190 \u03a0\u0398 [ \u03b8k\u22121 \u2212 1\nL (f \u2032(\u03b8k\u22121) + ek)\n]\n,\nThen, defining \u03b8\u2217 = argmin\u03b8\u2208\u0398 f(\u03b8), for all K \u2265 1, we have, for A\u0304k = \u2211K k=1(1\u2212 \u03bbL )\u2212k \u2016ek\u2016 L that\n\u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 (1 \u2212 \u03bb\nL )K\n( \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + A\u0304k )\nCorollary 13. Under the same conditions, if \u2016ek\u2016 \u2264 r for all k, then\n\u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 (1\u2212 \u03bb L )K\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + rL \u03bb\nProof. Using the fact that \u2211K k=1 a \u2212k = a\u2212K \u2211K\u22121 k=0 a k \u2264 a\u2212K \u2211\u221ek=0 ak = a \u2212K 1\u2212a , we get that\nA\u0304K \u2264 r K \u2211\nk=1\n(1 \u2212 \u03bb L )\u2212k \u2264 rL \u03bb (1\u2212 \u03bb L )\u2212K ,\nand therefore that\n\u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 (1 \u2212 \u03bb\nL )K\n(\n\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + r L \u03bb (1 \u2212 \u03bb L )\u2212K\n)\n."}, {"heading": "8.2 Concentration Results", "text": "Three concentration inequalities, are stated here for reference. The first is Bernstein\u2019s inequality.\nTheorem 14. (Bernstein\u2019s inequality) SupposeZ1, ..., ZK are independent with mean 0, that |Zk| \u2264 c and that \u03c32i = V[Zi]. Then, if we define \u03c3 2 = 1K \u2211K k=1 \u03c3 2 k,\nP\n[\n1\nK\nK \u2211\nk=1\nZk > \u01eb\n]\n\u2264 exp ( \u2212 K\u01eb 2\n2\u03c32 + 2c\u01eb/3\n)\n.\nThe second is the following Hoeffding-type bound to control the difference between the expected value of t(X) and the estimated value using M samples.\nTheorem 15. If X1, ..., XM are independent variables with mean \u00b5, and \u2016Xi \u2212 \u00b5\u2016 \u2264 c, then for all \u01eb \u2265 0, with probability at least 1\u2212 \u03b4,\n\u2016X\u0304 \u2212 \u00b5\u2016 \u2264 \u221a c\n4M\n(\n1 +\n\u221a\n2 log 1\n\u03b4\n)\n.\nProof. Boucheron et al. [4, Ex. 6.3] show that, under the same conditions as stated, for all s \u2265 \u221av,\nP\n[ \u2016X\u0304 \u2212 \u00b5\u2016 > s M ] \u2264 exp (\n\u2212 (s\u2212 \u221a v)2\n2v\n)\n,\nwhere v = cM4 . We will fix \u03b4, and solve for the appropriate s. If we set \u03b4 = exp(\u2212 (s\u2212\u221av)2 2v ), then we have that s = \u221a 2v log 1\u03b4 + \u221a v, meaning that, with probability at least 1\u2212 \u03b4,\n\u2016X\u0304 \u2212 \u00b5\u2016 \u2264 1 M\n( \u221a\n2 cM\n4 log\n1 \u03b4 +\n\u221a\ncM\n4\n)\n,\nwhich is equivalent to the result with a small amount of manipulation.\nThe third is the Efron-Stein inequality [4, Theorem 3.1].\nTheorem 16. If X = (X1, ..., Xm) is a vector of independent random variables and f(X) is a square-integrable function, then\nV[f(X)] \u2264 1 2\nM \u2211\ni=1\nE\n[\n( (f(X)\u2212 f(X(i)) )2\n]\n,\nwhere X(i) is X with Xi independently re-drawn, i.e.\nX(i) = (X1, ..., Xi\u22121, X \u2032 i\u2032 , Xi+1, ..., Xm)."}, {"heading": "9 Preliminary Results", "text": "A result that we will use several times below is that, for 0 < \u03b1 < 1, \u2212 1log(\u03b1) \u2264 11\u2212\u03b1 . This bound is tight in the limit that \u03b1 \u2192 1. Lemma 17. The difference of two estimated mean vectors is bounded by\n\u2016Eq[t(X)]\u2212 Ep[t(X)]\u20162 \u2264 2R2\u2016q \u2212 p\u2016TV .\nProof. Let the distribution functions of p and q be P and Q, respectively. Then, we have that\n\u2016E p [t(X)]\u2212 E q [t(X)]\u20162 =\n\u2225 \u2225 \u2225 \u2225 \u02c6\nx\nt(x) (dP (x)\u2212 dQ(x)) \u2225 \u2225 \u2225\n\u2225\n2\n\u2264 \u02c6\nx\n|dP (x)\u2212 dQ(x)| \u00b7 \u2016t(x)\u20162.\nUsing the definition of total-variation distance, and the bound that \u2016t(x)\u20162 \u2264 R2 gives the result.\nLemma 18. If 1/a+ 1/b = 1, then the difference of two log-partition functions is bounded by\n|A(\u03b8) \u2212A(\u03c6)| \u2264 Ra\u2016\u03b8 \u2212 \u03c6\u2016b.\nProof. By the Lagrange remainder theorem, there must exist some \u03b3 on the line segment between \u03b8 and \u03c6 such that A(\u03c6) = A(\u03b8)+ (\u03c6\u2212 \u03b8)T\u2207\u03b3A(\u03b3). Thus, applying H\u00f6lder\u2019s inequality, we have that\n|A(\u03c6) \u2212A(\u03b8)| = |(\u03c6\u2212 \u03b8)T\u2207\u03b3A(\u03b3)| \u2264 \u2016\u03c6\u2212 \u03b8\u2016b \u00b7 \u2016\u2207\u03b3A(\u03b3)\u2016a. The result follows from the fact that \u2016\u2207\u03b3A(\u03b3)\u2016a = \u2016Ep\u03b3 t(X)\u2016a \u2264 Ra.\nNext, we observe that the total variation distance between p\u03b8 and p\u03c6 is bounded by the distance between \u03b8 and \u03c6.\nTheorem 19. If 1/a+ 1/b = 1, then the difference of distributions is bounded by\n\u2016p\u03b8 \u2212 p\u03c6\u2016TV \u2264 2Ra\u2016\u03b8 \u2212 \u03c6\u2016b.\nProof. If we assume that p\u03b8 is a density, we can decompose the total-variation distance as\n||p\u03b8 \u2212 p\u03c6||TV\n= 1\n2\n\u02c6\nx\np\u03b8(x)|1 \u2212 p\u03c6(x)\np\u03b8(x) |\n= 1\n2\n\u02c6\nx\np\u03b8(x) |1\u2212 exp ((\u03c6\u2212 \u03b8) \u00b7 t(x)\u2212A(\u03c6) +A(\u03b8))|\n\u2264 1 2\n\u02c6\nx\np\u03b8(x) |1\u2212 exp |(\u03c6\u2212 \u03b8) \u00b7 t(x)\u2212A(\u03c6) +A(\u03b8)|| .\nIf p\u03b8 is a distribution, the analogous expression is true, replacing the integral over x with a sum.\nWe can upper-bound the quantity inside exp by applying H\u00f6lder\u2019s inequality and the previous Lemma as\n|(\u03c6 \u2212 \u03b8) \u00b7 t(x) \u2212A(\u03c6) +A(\u03b8))| \u2264 |(\u03c6\u2212 \u03b8) \u00b7 t(x)|+ |A(\u03c6) \u2212A(\u03b8))| \u2264 2Ra\u2016\u03b8 \u2212 \u03c6\u2016b.\nFrom which we have that\n\u2016p\u03b8 \u2212 p\u03c6\u2016TV \u2264 1\n2 |1\u2212 exp (2Ra\u2016\u03b8 \u2212 \u03c6\u2016b)| .\nIf 2Ra\u2016\u03b8 \u2212 \u03c6\u2016b > 1, the theorem is obviously true, since \u2016 \u00b7 \u2016TV \u2264 1. Suppose instead that that 2Ra\u2016\u03b8\u2212 \u03c6\u2016b \u2264 1. If 0 \u2264 c \u2264 1, then 12 |1\u2212 exp(c)| \u2264 c e\u221212 . Applying this with c = 2Ra\u2016\u03b8\u2212 \u03c6\u2016b gives that ||p\u03b8\u2212p\u03c6||TV \u2264 (e\u22121)R2||\u03b8\u2212\u03c6||b. The result follows from the fact that 2 > (e\u22121)."}, {"heading": "10 Lipschitz Continuity", "text": "This section shows that the ridge-regularized empirical log-likelihood does indeed have a Lipschitz continuous gradient.\nTheorem 20. The regularized log-likelihood function is L-Lipschitz with L = 4R22 + \u03bb, i.e.\n\u2016f \u2032(\u03b8)\u2212 f \u2032(\u03c6)\u20162 \u2264 (4R22 + \u03bb)\u2016\u03b8 \u2212 \u03c6\u20162.\nProof. We start by the definition of the gradient, with\n\u2016f \u2032(\u03b8)\u2212 f \u2032(\u03c6)\u20162 = \u2225 \u2225 \u2225\n\u2225\n(\ndA d\u03b8 \u2212 t\u0304+ \u03bb\u03b8\n) \u2212 ( dA\nd\u03c6 \u2212 t\u0304+ \u03bb\u03c6\n)\u2225\n\u2225 \u2225 \u2225\n2\n= \u2016dA d\u03b8 \u2212 dA d\u03c6 + \u03bb(\u03b8 \u2212 \u03c6)\u20162.\n\u2264 \u2016dA d\u03b8 \u2212 dA d\u03c6 \u20162 + \u03bb\u2016\u03b8 \u2212 \u03c6\u20162.\nNow, looking at the first two terms, we can apply Lemma 17 to get that \u2225\n\u2225 \u2225 \u2225\ndA d\u03b8 \u2212 dA d\u03c6\n\u2225 \u2225 \u2225 \u2225\n2\n= \u2225 \u2225Ep\u03b8 [t(X)]\u2212 Ep\u03c6 [t(X)] \u2225 \u2225\n2\n\u2264 2R2\u2016p\u03b8 \u2212 p\u03c6\u2016TV .\nObserving by Theorem 19 that \u2016p\u03b8 \u2212 p\u03c6\u2016TV \u2264 2R2\u2016\u03b8 \u2212 \u03c6\u20162 gives that\n\u2016f \u2032(\u03b8)\u2212 f \u2032(\u03c6)\u20162 \u2264 4R22\u2016\u03b8 \u2212 \u03c6\u20162 + \u03bb\u2016\u03b8 \u2212 \u03c6\u20162"}, {"heading": "11 Convex Convergence", "text": "This section gives the main result for convergence this is true both in the regularized case where \u03bb > 0 and the unregularized case where \u03bb = 0. The main difficulty in this proof is showing that the sum of the norms of the errors of estimated gradients is small.\nTheorem 21. Assuming that X1, ..., XM are independent and identically distributed with mean \u00b5 and that \u2016Xm\u20162 \u2264 R2, then\nE\n[\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u20162 ]\n\u2264 2R2\u221a M\nProof. Using that E [ Z2 ] = V [Z] +E [Z]2and the fact that the variance is non-negative (Or simply Jensen\u2019s inequality), we have\nE\n[\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u20162 ]2 \u2264 E [\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u201622\n]\n= 1\nM E [ \u2016Xm \u2212 \u00b5\u201622 ]\n\u2264 1 M (2R2) 2 = 4R22 M .\nTaking the square-root gives the result.\nTheorem 22. Assuming that X1, ..., XM are iid with mean \u00b5 and that \u2016Xm\u2016 \u2264 R2, then\nV\n[\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u2016 ] \u2264 2R 2 2\nM .\nProof.\nV\n[\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u2016 ] = V [\n\u2016 1 M\nM \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 ]\n= 1\nM2 V\n[\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 ]\nNow, the Efron-Stein inequality tells us that\nV[f(X1, ..., Xm)] \u2264 1\n2\nM \u2211\nm\u2032=1\nE\n[\n( (f(X)\u2212 f(X(m\u2032)) )2\n]\nwhere X(m \u2032) is X with Xm\u2032 independently re-drawn. Now, we identify f(X1, ..., Xm) = \u2016\u2211Mm=1(Xm \u2212 \u00b5)\u2016 to obtain that\nV\n[\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 ]\n\u2264 1 2\nM \u2211\nm\u2032=1\nE\n\n\n(\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 \u2212 \u2016 M \u2211\nm=1\n(X(m \u2032) m \u2212 \u00b5)\u2016 )2\n\n .\nFurther, since we know that\nM \u2211\nm=1\n(X(m \u2032) m \u2212 \u00b5) = M \u2211\nm=1\n(Xm \u2212 \u00b5) +X(m \u2032) m\u2032 \u2212Xm\u2032 ,\nwe can apply that that (\u2016a+ b\u2016 \u2212 \u2016a\u2016)2 \u2264 \u2016b\u20162 to obtain that (\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 \u2212 \u2016 M \u2211\nm=1\n(X(m \u2032) m \u2212 \u00b5)\u2016 )2 = \u2016X(m \u2032) m\u2032 \u2212Xm\u2032\u20162,\nand so\nV\n[\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 ]\n\u2264 1 2\nM \u2211\nm\u2032=1\nE\n[\n\u2016X(m \u2032) m\u2032 \u2212Xm\u2032\u20162 ] .\nAnd, since we assume that \u2016Xm\u2016 \u2264 R2, \u2016X(m \u2032) m\u2032 \u2212Xm\u2032\u2016 \u2264 2R2, which leads to\nV\n[\n\u2016 M \u2211\nm=1\n(Xm \u2212 \u00b5)\u2016 ] \u2264 2MR22,\nfrom which it follows that\nV\n[\n\u2016 1 M\nM \u2211\nm=1\nXm \u2212 \u00b5\u2016 ] \u2264 2R 2 2\nM .\nTheorem 23. With probability at least 1\u2212 \u03b4, K \u2211\nk=1\n\u2016 1 M\nM \u2211\ni=1\nt(xki )\u2212 Eqk [t(X)]\u20162 \u2264 K\u01eb(\u03b4) + 2R2K\u221a\nM ,\nwhere \u01eb(\u03b4) is the solution to\n\u03b4 = exp\n(\n\u2212 K\u01eb 2\n4R22/M + 4R2\u01eb/3\n)\n. (5)\nProof. Let dk = 1M \u2211M i=1 t(x k i ) \u2212 Eqk [t(X)]. Applying Bernstein\u2019s inequality immediately gives us that\nP\n[\n1\nK\nK \u2211\nk=1\n(\u2016dk\u20162 \u2212 E\u2016dk\u20162) > \u01eb ] \u2264 exp ( \u2212 K\u01eb 2\n2\u03c32 + 2c\u01eb/3\n)\n.\nHere, we can bound \u03c32 by\n\u03c32 = 1\nK\nK \u2211\nk=1\n\u03c32k = 1\nK\nK \u2211\nk=1\nV [\u2016dk\u20162 \u2212 E\u2016dk\u20162] = 1\nK\nK \u2211\nk=1\nV [\u2016dk\u20162] \u2264 2R22 M ,\nwhere the final inequality follows from Theorem 22. We also know that \u2016dk\u2016 \u2264 2R2 = c, from which we get that\nP\n[\n1\nK\nK \u2211\nk=1\n\u2016dk\u20162 \u2212 E[\u2016dk\u20162] > \u01eb ] \u2264 exp ( \u2212 K\u01eb 2\n4R22/M + 4R2\u01eb/3\n)\n.\nSo we have that, with probability 1\u2212 \u03b4\n1\nK\nK \u2211\nk=1\n\u2016dk\u20162 \u2212 E[\u2016dk\u20162] \u2264 \u01eb(\u03b4)\n1\nK\nK \u2211\nk=1\n\u2016dk\u20162 \u2264 \u01eb(\u03b4) + E[\u2016dk\u20162]\n\u2264 \u01eb(\u03b4) + 2R2\u221a M ,\nwhere the final inequality follows from Theorem 21.\nCorollary 24. If M \u2265 3K/ log(1\u03b4 ), then with probability at least 1\u2212 \u03b4, K \u2211\nk=1\n\u2016 1 M\nM \u2211\ni=1\nt(xki )\u2212 Eqk [t(X)]\u20162 \u2264 2R2 ( K\u221a M + log 1 \u03b4 ) .\nProof. Solving Equation 5 for \u01eb yields that\n\u01eb(\u03b4) = 2R2 3K\n\nlog 1\n\u03b4 +\n\u221a\n(\nlog 1\n\u03b4\n)2\n+ 9K log 1\u03b4\nM\n\n .\nNow, suppose that 3KM \u2264 log 1\u03b4 , as assumed here. Then,\n\u01eb(\u03b4) \u2264 2R2 3K\n\nlog 1\n\u03b4 +\n\u221a\n(\nlog 1\n\u03b4\n)2\n+ 3(log 1\n\u03b4 )2\n\n\n\u2264 2R2 3K\n(\nlog 1\n\u03b4 + 2 log(\n1 \u03b4 )\n)\n= 2R2 K log 1 \u03b4 .\nSubstituting this bound into the result of Theorem 23 gives the result.\nNow, we can prove the main result.\nTheorem 25. With probability at least 1\u2212 \u03b4, at long as M \u2265 3K/ log(1\u03b4 ),\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 8R 2 2\nKL\n( L\u2016\u03b80 \u2212 \u03b8\u2217\u20162 4R2 + log 1 \u03b4 + K\u221a M +KC\u03b1v )2 .\nProof. Applying Theorem 10 gives that\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 L 2K (\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 2AK)2 ,\nfor AK = 1L \u2211K k=1 \u2016ek\u2016, where\nek = 1\nM\nM \u2211\ni=1\nt(xk\u22121i )\u2212 t\u0304+ \u03bb\u03b8k\u22121 \u2212 f \u2032(\u03b8k\u22121)\n= 1\nM\nM \u2211\ni=1\nt(xk\u22121i )\u2212 Epk\u22121 [t(X)].\nNow, we know that\nK \u2211\nk=1\n\u2016ek\u2016 \u2264 K \u2211\nk=1\n\u2016 1 M\nM \u2211\ni=1\nt(xk\u22121i )\u2212 Eqk\u22121 [t(X)]\u20162 + K \u2211\nk=1\n\u2016Eqk\u22121 [t(X)]\u2212 Epk\u22121 [t(X)]\u20162.\nWe have by Lemma 17 and the assumption of mixing speed that\n\u2016Eqk\u22121 [t(X)]\u2212 Epk\u22121 [t(X)]\u20162 \u2264 2R2\u2016qk\u22121 \u2212 pk\u22121\u2016TV \u2264 2R2C\u03b1v . Meanwhile, the previous Corollary tells us that, with probability 1\u2212 \u03b4,\nK \u2211\nk=1\n\u2016 1 M\nM \u2211\ni=1\nt(xk\u22121i )\u2212 Eqk\u22121 [t(X)]\u20162 \u2264 2R2 ( K\u221a M + log 1 \u03b4 )\nThus, we have that\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 L 2K\n(\n\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 2\nL\n(\n2R2\n(\nK\u221a M + log 1 \u03b4\n)\n+ 2R2KC\u03b1 v\n))2\n= L\n2K\n(\n\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 4R2 L\n(\nK\u221a M + log 1 \u03b4 +KC\u03b1v\n))2\n= 8R22 KL\n( L\u2016\u03b80 \u2212 \u03b8\u2217\u20162 4R2 + log 1 \u03b4 + K\u221a M +KC\u03b1v )2 .\nNow, what we really want to do is guarantee that f (\n1 K \u2211K k=1 \u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 \u01eb, while ensuring the the total work MKv is not too large. Our analysis will use the following theorem.\nTheorem 26. Suppose that a, b, c, \u03b1 > 0. If \u03b21 + \u03b22 + \u03b23 = 1, \u03b21, \u03b22, \u03b23 > 0, then setting\nK = a2\n\u03b221\u01eb , M = (\nab\n\u03b21\u03b22\u01eb )2, v = log ac\u03b21\u03b23\u01eb (\u2212 log\u03b1)\nis sufficient to guarantee that 1K\n(\na+ b K\u221a M\n+Kc\u03b1v )2 \u2264 \u01eb with a total work of\nKMv = 1\n\u03b241\u03b2 2 2\na4b2\n\u01eb3 log ac\u03b21\u03b23\u01eb (\u2212 log\u03b1) .\nProof. Firstly, we should verify the \u01eb bound. We have that\na+ b K\u221a M +Kc\u03b1v = a+ b a2 \u03b221\u01eb \u03b21\u03b22\u01eb ab + a2 \u03b221\u01eb c \u03b21\u03b23\u01eb ac\n= a+ a \u03b22 \u03b21 + a \u03b23 \u03b21 ,\nand hence that\n1\nK\n(\na+ b K\u221a M\n+Kc\u03b1v )2 = a2\nK\n(\n1 + \u03b22 \u03b21 + \u03b23 \u03b21\n)2\n= 1\nK\na2 \u03b221 (\u03b21 + \u03b22 + \u03b23) 2\n\u2264 \u01eb.\nMultiplying together th terms gives the second part of the result.\nWe can also show that this solution is not too sub-optimal.\nTheorem 27. Suppose that a, b, c, \u03b1 > 0. If K,M, v > 0 are set so that 1 K ( a+ b K\u221a M +Kc\u03b1v )2 \u2264 \u01eb, then\nKMv \u2265 a 4b2 \u01eb3 log ac\u01eb (\u2212 log\u03b1) .\nProof. The starting condition is equivalent to stating that\na\u221a K + b\n\u221a\nK M + \u221a Kc\u03b1v \u2264 \u221a \u01eb.\nSince all terms are positive, clearly each is less than \u221a \u01eb. From this follows that\nK \u2265 a 2\n\u01eb\nM \u2265 b 2a2\n\u01eb2\nv \u2265 log ac \u01eb\n(\u2212 log\u03b1) .\nMultiplying these together gives the result.\nTheorem 28. If D \u2265 max ( \u2016\u03b80 \u2212 \u03b8\u2217\u20162, 4R2L log 1\u03b4 ) , then for all \u01eb there is a setting of KMv such that f (\n1 K \u2211K k=1 \u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 \u01ebf with probability 1\u2212 \u03b4 and\nKMv \u2264 32LR 2 2D 4\n\u03b241\u03b2 2 2\u01eb 3 f (1\u2212 \u03b1)\nlog 4DR2C\n\u03b21\u03b23\u01ebf\n= O ( LR22D 4\n\u01eb3f (1\u2212 \u03b1) log\n1\n\u01ebf\n)\n= O\u0303 ( LR22D 4\n\u01eb3f (1\u2212 \u03b1)\n)\n.\nProof. So, we apply this to the original theorem. Our settings are\nf\n(\n1\nK\nK \u2211\nk=1\n\u03b8k\n)\n\u2212 f(\u03b8\u2217) \u2264 8R 2 2\nKL\n( L\u2016\u03b80 \u2212 \u03b8\u2217\u20162 4R2 + log 1 \u03b4 + K\u221a M +KC\u03b1v )2 .\na = L\u2016\u03b80 \u2212 \u03b8\u2217\u20162\n4R2 + log\n1\n\u03b4\nb = 1\nc = C \u01eb = \u01ebfL\n8R22\nNote that, by the definition of D, a \u2264 LD2R2 and so ac \u2264 LDC 2R2 . Thus, the total amount of work is\nKMv = 1\n\u03b241\u03b2 2 2\na4b2 \u01eb3 log \u03b21\u03b23\u01ebac log\u03b1\n= 1\n\u03b241\u03b2 2 2\na4b2\n\u01eb3 log ac\u03b21\u03b23\u01eb \u2212 log\u03b1\n\u2264 1 \u03b241\u03b2 2 2 1 \u01eb3\n(\nLD\n2R2\n)4 log LDC\u03b21\u03b232R2\u01eb log\u03b1\n= 1\n\u03b241\u03b2 2 2 83R62 \u01eb3fL 3\n(\nLD\n2R2\n)4 log LDC8R2 2\n\u03b21\u03b232R2\u01ebfL\nlog\u03b1\n= 1\n\u03b241\u03b2 2 2 32LD4R22 \u01eb3f log 4DR2C\u03b21\u03b23\u01ebf log\u03b1\n\u2264 32LD 4R22\n\u03b241\u03b2 2 2\u01eb\n3(1\u2212 \u03b1) log 4DR2C \u03b21\u03b23\u01eb ."}, {"heading": "12 Strongly Convex Convergence", "text": "This section gives the main result for convergence this is true both only in the regularized case where \u03bb > 0. Again, the main difficulty in this proof is showing that the sum of the norms of the errors of estimated gradients is small. This proof is relatively easier, as we simply bound all errors to be small with high probability, rather than jointly bounding the sum of errors.\nLemma 29. With probability at least 1\u2212 \u03b4,\n\u2016ek+1\u20162 \u2264 R2\u221a M\n(\n1 +\n\u221a\n2 log 1\n\u03b4\n)\n+ 2R2C\u03b1 v\nProof. Once we have the difference of the distributions, we can go after the error in the gradient estimate. By definition,\n\u2016ek+1\u20162 = \u2016 1\nM\nM \u2211\ni=1\nt(xki )\u2212 Ep\u03b8k [t(X)]\u20162\n\u2264 \u2016 1 M\nM \u2211\ni=1\nt(xki )\u2212 Eqk [t(X)]\u20162\n+ \u2016Eqk [t(X)]\u2212 Ep\u03b8k [t(X)]\u20162.\nConsider the second term. We know by Lemma 17 and the assumption of mixing speed\n\u2016Eqk [t(X)]\u2212 Epk [t(X)]\u20162 \u2264 2R2\u2016qk \u2212 pk\u2016TV \u2264 2R2C\u03b1v. (6)\nNow, consider the first term. We know that Eqk [t(X)] is the expected value of 1 M \u2211M i=1 t(x k i ). We also know that ||t(xki ) \u2212 Eqk [t(X)]|| \u2264 2R2. Thus, we can apply Theorem 15 to get that, with probability 1\u2212 \u03b4,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 M M \u2211\ni=1\nt ( xki ) \u2212 Eqk [t(X)] \u2225 \u2225 \u2225 \u2225\n\u2225 \u2264 R2\u221a M\n(\n1 +\n\u221a\n2 log 1\n\u03b4\n)\n. (7)\nAdding together Equations 6 and 7 gives the result.\nTheorem 30. With probability at least 1\u2212 \u03b4,\n\u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 (1 \u2212 \u03bb L )K\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + L \u03bb\n( \u221a\nR2 2M\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)\n+ 2R2C\u03b1 v\n)\nProof. Apply the previous Lemma to bound bound on \u2016ek+1\u20162 with probability at least 1\u2212\u03b4\u2032 where \u03b4\u2032 = \u03b4/K . Then, plug this into the main optimization result in Corollary 13.\nTheorem 31. Suppose a, b, c > 0. Then for any K,M, v such that \u03b3Ka+ b\u221a M\n\u221a\nlog K\u03b4 + c\u03b1 v \u2264 \u01eb.\nit must be the case that\nKMv \u2265 b 2 \u01eb2 log a\u01eb log c \u01eb (\u2212 log \u03b3)(\u2212 log\u03b1) log ( log a\u01eb \u03b4(\u2212 log \u03b3) )\nProof. Clearly, we must have that each term is at most \u01eb, yielding that\nK \u2265 log \u01eb a\nlog \u03b3\nM \u2265 b 2\n\u01eb2 log\nK \u03b4 \u2265 b\n2\n\u01eb2 log log \u01eba \u03b4 log \u03b3\nv \u2265 log(c/\u01eb) (\u2212 log\u03b1)\nFrom this we obtain that\nKMv \u2265 b 2 \u01eb2 log a\u01eb log(c/\u01eb) (\u2212 log \u03b3)(\u2212 log\u03b1) log ( log a\u01eb \u03b4(\u2212 log \u03b3) ) .\nTheorem 32. Suppose that a, b, c, \u03b1 > 0. If \u03b21 + \u03b22 + \u03b23 = 1, \u03b2i > 0, then setting\nK = log( a\n\u03b21\u01eb )/(\u2212 log \u03b3)\nM = b2\n\u01eb2\u03b222\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)2\nv = log\n(\nc\n\u03b23\u01eb\n)\n/(\u2212 log\u03b1)\nis sufficient to guarantee that \u03b3Ka+ b\u221a M (1 +\n\u221a\n2 log K\u03b4 ) + c\u03b1 v \u2264 \u01eb with a total work of at most\nKMV \u2264 b 2\n\u01eb2\u03b222\nlog (\na \u03b21\u01eb\n) log (\nc \u03b23\u01eb\n)\n(\u2212 log \u03b3)(\u2212 log\u03b1)\n\n1 +\n\u221a\n2 log log( a\u03b21\u01eb)\n\u03b4(\u2212 log \u03b3)\n\n\n2\nProof. We define the errors so that\n\u03b3Ka = \u01eb\u03b21\nb\u221a M (1 +\n\u221a\n2 log K\n\u03b4 ) = \u01eb\u03b22\nc\u03b1v = \u01eb\u03b23.\nSolving, we obtain that\nK = log( a\n\u03b21\u01eb )/(\u2212 log \u03b3)\nM = b2\n\u01eb2\u03b222\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)2\nv = log\n(\nc\n\u03b23\u01eb\n)\n/(\u2212 log\u03b1).\nThis yields that the final amount of work is\nKMv \u2264 log\n(\na \u03b21\u01eb\n) log (\nc \u03b23\u01eb\n)\n(\u2212 log \u03b3)(\u2212 log\u03b1) b2\n\u01eb2\u03b222\n\n1 +\n\u221a\n2 log log( a\u03b21\u01eb)\n\u03b4(\u2212 log \u03b3)\n\n\n2\nRemark 33. For example, you might choose \u03b22 = 12 , \u03b21 = 1 4 and \u03b23 = 1 4 , in which case the total amount of work is bounded by\nKMv \u2264 4b 2 \u01eb2 log\n(\n4a \u01eb\n) log ( 4c \u01eb )\n(\u2212 log \u03b3)(\u2212 log\u03b1)\n\n1 +\n\u221a\n2 log log(4a\u01eb )\n\u03b4(\u2212 log \u03b3)\n\n\n2\n= 4b2\n\u01eb2\n( log ( a \u01eb ) + log 4 ) (log ( 4c \u01eb ) + log 4)\n(\u2212 log \u03b3)(\u2212 log\u03b1)\n(\n1 +\n\u221a\n2 log log(a\u01eb ) + log 4\n\u03b4(\u2212 log \u03b3)\n)2\nOr, if you choose \u03b22 = 1/ \u221a 2 and \u03b21 = \u03b23 = (1\u2212 1/ \u221a 2)/2 \u2248 0.1464, then you get the bound of\nKMV \u2264 2b 2\n\u01eb2\n(log ( a \u01eb )\n+ 1.922)(log (\nc \u03b23\n)\n+ 1.922)\n(\u2212 log \u03b3)(\u2212 log\u03b1)\n(\n1 +\n\u221a\n2 log log(a\u01eb ) + 1.922\n\u03b4(\u2212 log \u03b3)\n)2\nwhich is not too much worse than the lower-bound.\nCorollary 34. If we choose\nK \u2265 L \u03bb log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb )\nM \u2265 L 2R2\n2\u01eb2\u03b222\u03bb 2\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)2\nv \u2265 1 1\u2212 \u03b1 log\n(\n2LR2C\n\u03b23\u01eb\u03bb\n)\nthen \u2016\u03b8K \u2212 \u03b8\u2217\u20162 \u2264 \u01eb with probability at least 1\u2212 \u03b4, and the total amount of work is bounded by\nKMv \u2264 1 \u01eb2\n(\nL\n\u03bb\n)3 R2\n2\u03b222(1\u2212 \u03b1) log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb )\n(\n1 +\n\u221a\n2 log\n(\nL \u03bb\u03b4 log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb ))\n)2\nProof. Apply the previous convergence theory to our setting. We equate\n(1\u2212\u03bb L )K\u2016\u03b80\u2212\u03b8\u2217\u20162+ L \u03bb\n( \u221a\nR2 2M\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)\n+ 2R2C\u03b1 v\n)\n= \u03b3Ka+ b\u221a M (1+\n\u221a\n2 log K\n\u03b4 )+c\u03b1v.\nThis requires the constants\n\u03b3 = 1\u2212 \u03bb L a = \u2016\u03b80 \u2212 \u03b8\u20162\nb = L\n\u03bb\n\u221a\nR2 2\nc = 2LR2C/\u03bb\nThus, we will make the choices\nK = log( a\n\u03b21\u01eb )/(\u2212 log \u03b3)\n\u2264 log(\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb )/(1\u2212 \u03b3)\n= L\n\u03bb log( \u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb )\nM = b2\n\u01eb2\u03b222\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)2\n= L2R2\n2\u01eb2\u03b222\u03bb 2\n(\n1 +\n\u221a\n2 log K\n\u03b4\n)2\nv = log\n(\nc\n\u03b23\u01eb\n)\n/(\u2212 log\u03b1)\n= log\n(\n2LR2C\n\u03b23\u01eb\u03bb\n)\n/(\u2212 log\u03b1)\n\u2264 1 1\u2212 \u03b1 log\n(\n2LR2C\n\u03b23\u01eb\u03bb\n)\nThis means a total amount of work of\nKMv = = L\n\u03bb log( \u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb ) L2R2 2\u01eb2\u03b222\u03bb 2(1 \u2212 \u03b1)\n(\n1 +\n\u221a\n2 log\n(\nL \u03bb\u03b4 log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb ))\n)2\nlog\n(\n2LR2C\n\u03b23\u01eb\u03bb\n)\n= 1\n\u01eb2\n(\nL\n\u03bb\n)3 R2\n2\u03b222(1 \u2212 \u03b1) log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb )\n(\n1 +\n\u221a\n2 log\n(\nL \u03bb\u03b4 log (\u2016\u03b80 \u2212 \u03b8\u20162 \u03b21\u01eb ))\n)2\n."}], "references": [{"title": "Learning factor graphs in polynomial time and sample complexity", "author": ["P. Abbeel", "D. Koller", "A. Ng"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning with blocks composite likelihood and contrastive divergence", "author": ["A. Asuncion", "Q. Liu", "A. Ihler", "P. Smyth"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series D (The Statistician),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Peripi\u00f1\u00e1n", "G. Hinton"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.I. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "Estimation of markov Random field prior parameters using Markov chain Monte Carlo maximum likelihood", "author": ["X. Descombes", "J.Z. Robin Morris", "M. Berthod"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Projecting Ising model parameters for fast mixing", "author": ["J. Domke", "X. Liu"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Matrix norms and rapid mixing for spin systems", "author": ["M.E. Dyer", "L.A. Goldberg", "M. Jerrum"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C. Geyer"], "venue": "In Symposium on the Interface,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Maximum likelihood estimation for spatial models by Markov chain Monte Carlo stochastic approximation", "author": ["M.G. Gu", "Zhu", "H.-T"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["T. Hayes"], "venue": "In FOCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Inferning with high girth graphical models", "author": ["U. Heinemann", "A. Globerson"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Simulation reductions for the ising model", "author": ["M. Huber"], "venue": "Journal of Statistical Theory and Practice,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Polynomial-time approximation algorithms for the ising model", "author": ["M. Jerrum", "A. Sinclair"], "venue": "SIAM Journal on Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Composite likelihood methods", "author": ["B. Lindsay"], "venue": "Contemporary Mathematics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Projecting Markov random field parameters for fast mixing", "author": ["X. Liu", "J. Domke"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood", "author": ["B. Marlin", "N. de Freitas"], "venue": "In UAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Linear and parallel learning of markov random fields", "author": ["Y. Mizrahi", "M. Denil", "N. de Freitas"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A.L. Yuille"], "venue": "In ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "A generative perspective on MRFs in low-level vision", "author": ["U. Schmidt", "Q. Gao", "S. Roth"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Learning fast-mixing models for structured prediction", "author": ["J. Steinhardt", "P. Liang"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "An overview of composite likelihood methods", "author": ["C. Varin", "N. Reid", "D. Firth"], "venue": "Statistica Sinica,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Estimating the \"wrong\" graphical model: Benefits in the computation-limited setting", "author": ["M. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling", "author": ["S.C. Zhu", "Y. Wu", "D. Mumford"], "venue": "International Journal of Computer Vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of \u201cfast-mixing parameters\u201d where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution \u01eb-accurate in log-likelihood requires a total amount of effort cubic in 1/\u01eb, disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution \u01eb-accurate in parameter distance with effort quadratic in 1/\u01eb. Both of these provide of a fully-polynomial time randomized approximation scheme.", "creator": "LaTeX with hyperref package"}}}