{"id": "1509.09030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "abstract": "then two popular approaches for distributed training of svms on big data are parameter averaging and admm. parameter independent averaging procedures is efficient but nevertheless suffers from loss of accuracy with increase in number component of partitions, while while admm in the feature space estimation is accurate but suffers from slow convergence. in this paper, we report a hybrid approach called weighted parameter averaging ( wpa ), which optimizes the regularized hinge loss with respect same to weights on parameters. locally the problem is shown to be same closely as solving svm in a projected space. we also demonstrate an $ o ( \\ frac { 1 } { n } ) $ stability bound approximation on final hypothesis given by wpa, using novel inequality proof techniques. experimental results on a variety of toy and real world generalized datasets jointly show that our approach is significantly more accurate than parameter regression averaging for over high number of partitions. it is also seen how the proposed method enjoys much faster convergence compared to admm in features geographic space.", "histories": [["v1", "Wed, 30 Sep 2015 06:59:31 GMT  (178kb)", "http://arxiv.org/abs/1509.09030v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ayan das", "sourangshu bhattacharya"], "accepted": false, "id": "1509.09030"}, "pdf": {"name": "1509.09030.pdf", "metadata": {"source": "CRF", "title": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "authors": ["Ayan Das"], "emails": ["ayand@cse.iitkgp.ernet.in", "sourangshu@cse.iitkgp.ernet.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n09 03\n0v 1\n[ cs\n.L G\n] 3\n0 Se\nN )\nstability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space."}, {"heading": "1 Introduction", "text": "With the growing popularity of Big Data platforms [1] for various machine learning and data analytics applications [9, 12], distributed training of Support Vector Machines (SVMs)[4] on Big Data platforms have become increasingly important. Big data platforms such as Hadoop [1] provide simple programming abstraction (Map Reduce), scalability and fault tolerance at the cost of distributed iterative computation being slow and expensive [9]. Thus, there is a need for SVM training algorithms which are efficient both in terms of the number of iterations and volume of data communicated per iteration.\nThe problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times. Here, the training data is partitioned into M -nodes, each having L datapoints. Parameter averaging (PA), also called \u201cmixture weights\u201d [9] or \u201cparallelized SGD\u201d [12], suggests solving an appropriate RLM problem on data in each node, and use average of the resultant parameters. Hence, a single distributed iteration is needed. However, as shown in this paper, the accuracy of this approach reduces with increase in number of partitions. Another interesting result described in [9] is a bound of O( 1\nML ) on the stability of the final hypothesis, which results in a bound on deviation\nfrom optimizer of generalization error.\nAnother popular approach for distributed RLM is alternating direction method of multipliers (ADMM) [2, 6]. This approach tries to achieve consensus between parameters at different nodes while optimizing the objective function. It achieves optimal performance irrespective of the number of partitions. However, this approach needs many distributed iterations. Also, number of parameters to be communicated among machines per iteration is same as the dimension of the problem. This can be \u223c millions for some practical datasets, e.g. webspam [3].\nIn this paper, we propose a hybrid approach which uses weighted parameter averaging and proposes to learn the weights in a distributed manner from the data. We propose a novel SVM-like formulation for learning the weights of the weighted parameter averaging (WPA) model. The dual of WPA turns out to be same as SVM dual, with data projected in a lower dimensional space. We propose an ADMM [2] based distributed algorithm (DWPA), and an accelerated version (DWPAacc), for learning the weights.\nAnother contribution is a O( 1 ML ) bound on the stability of final hypothesis leading to a bound on deviation from optimizer of generalization error. This requires a novel proof technique as both the original parameters and the weights are solutions to optimization problems (section 2.4). Empirically, we show that that accuracy of parameter averaging degrades with increase in the number of partitions. Experimental results on real world datasets show that DWPA and DWPAacc achieve better accuracies than PA as the number of partitions increase, while requiring lower number of iterations and time per iteration compared to ADMM."}, {"heading": "2 Distributed Weighted Parameter Averaging (DWPA)", "text": "In this section, we describe the distributed SVM training problem, the proposed solution approach and a distributed algorithm. We describe a bound on stability of the final hypothesis in section 2.4. Note that, we focus on the distributed SVM problem for simplicity. The techniques described here are applicable to other distributed regularized risk minimization problems."}, {"heading": "2.1 Background", "text": "Given a training dataset S = {(xi, yi) : i = 1, \u00b7 \u00b7 \u00b7 ,ML, yi \u2208 {\u22121,+1},xi \u2208 Rd}, the linear SVM problem [4] is given by:\nmin w\n\u03bb\u2016w\u201622 + 1\nm\nML \u2211\ni=1\nloss(w; (xi, yi)), (1)\nwhere, \u03bb is the regularization parameter and the hinge loss is defined as loss(w; (xi, yi)) = max(0, 1 \u2212 yiw Txi). The separating hyperplane is given by the equation wTx + b = 0. Here we include the bias b within w by making the following transformation, w = [ wT , b ]T and\nxi = [ xTi , 1 ]T .\nThe above SVM problem can be posed to be solved in a distributed manner, which is interesting when the volume of training data is too large to be effectively stored and processed on a single computer. Let the dataset be which is partitioned into M partitions (Sm, m = 1, . . . ,M ), each having L datapoints. Hence, S = S1\u222a, . . . ,\u222aSM , where Sm = {(xml, yml)}, l = 1, . . . , L. Under this setting, the SVM problem (Eqn 1), can be stated as:\nmin wm,z\nM \u2211\nm=1\nL \u2211\nl=1\nloss(wm; (xml, yml)) + r(z) (2)\ns.t.wm \u2212 z = 0,m = 1, \u00b7 \u00b7 \u00b7 ,M, l = 1, . . . , L where loss() is as described above and r(z) = \u03bb\u2016z\u20162.This problem is solved in [2] using ADMM (see section 2.3).\nAnother method for solving distributed RLM problems, called parameter averaging (PA), was proposed by Mann et. al. [9], in the context of conditional maximum entropy model. Let w\u0302m = argminw 1\nL \u2211L l=1 loss(w;xml, yml) + \u03bb\u2016w\u2016 2 , m = 1, . . . ,M be the standard SVM solution obtained by training on partition Sm. Mann et al. [9] suggest the approximate final parameter to be the arithmetic mean of the parameters learnt on individual partitions, (w\u0302m). Hence:\nwPA = 1\nM\nM \u2211\nm=1\nw\u0302m (3)\nZinekevich et al. [12] have also suggested a similar approach where w\u0302m\u2019s are learnt using SGD. We tried out this approach for SVM. Note that assumptions regarding differentiability of loss function made in [2] can be relaxed in case of convex loss function with an appropriate definition of bregmann divergence using sub-gradients (see [11], section 2.4). The results (reported in section 3) show that the method fails to perform well as the number of partitions increase. This drawback of the above\nmentioned approach motivated us to propose the weighted parameter averaging method described in the next section."}, {"heading": "2.2 Weighted parameter averaging (WPA)", "text": "The parameter averaging method uses uniform weight of 1 M for each of the M components. One can conceive a more general setting where the final hypothesis is a weighted sum of the parameters obtained on each partition: w =\n\u2211M m=1 \u03b2mw\u0302m, where w\u0302m are as defined above and \u03b2m \u2208 R,m =\n1, . . . ,M . Thus, \u03b2 = [\u03b21, \u00b7 \u00b7 \u00b7 , \u03b2M ]T = [ 1M , . . . , 1 M ] achieves the PA setting. Note that Mann et al. [9] proposed \u03b2 to be in a simplex. However, no scheme was suggested for learning an appropriate \u03b2.\nOur aim is to find the optimal set of weights \u03b2 which attains the lowest regularized loss. Let W\u0302 = [w\u03021, \u00b7 \u00b7 \u00b7 , w\u0302M ], so that w = W\u0302\u03b2. Substituting w in eqn. 1, the regularized loss minimization problem becomes:\nmin \u03b2,\u03be\n\u03bb\u2016W\u0302\u03b2\u20162 + 1\nML\nM \u2211\nm=1\nl \u2211\ni=1\n\u03bemi (4)\nsubject to: ymi(\u03b2TW\u0302Txmi) \u2265 1\u2212 \u03bemi, \u2200i,m \u03bemi \u2265 0, \u2200m = 1, . . . ,M, i = 1, . . . , l\nNote that, here the optimization is only w.r.t. \u03b2 and \u03bem,i. W\u0302 is a pre-computed parameter. Next we can derive the dual formulation by writing the lagrangian and eliminating the primal variables. The Lagrangian is given by:\nL(\u03b2, \u03bemi, \u03b1mi, \u00b5mi) = \u03bb\u2016W\u0302\u03b2\u2016 2 +\n1\nML\n\u2211\nm,i\n\u03bemi + \u2211\nm,i\n\u03b1mi(ymi(\u03b2 TWTxmi)\u2212 1 + \u03bemi)\u2212\n\u2211\nm,i\n\u00b5mi\u03bemi\nDifferentiating the Lagrangian w.r.t. \u03b2 and equating to zero, we get:\n\u03b2 = 1\n2\u03bb (W\u0302TW\u0302)\u22121(\n\u2211\nm,i\n\u03b1miymiW\u0302 Txmi) (5)\nDifferentiating L w.r.t. \u03bemi and equating to zero, \u2200i \u2208 1, \u00b7 \u00b7 \u00b7 , L and \u2200m \u2208 1, \u00b7 \u00b7 \u00b7 ,M , implies 1 ML \u2212 \u03b1mi \u2212 \u00b5mi = 0. Since \u00b5mi \u2265 0 and \u03b1mi \u2265 0, 0 \u2264 \u03b1mi \u2264 1ML . Substituting the value of \u03b2 in the Lagrangian L, we get the dual problem:\nmin \u03b1\nL(\u03b1) = \u2211\nm,i\n\u03b1mi \u2212 1\n4\u03bb\n\u2211\nm,i\n\u2211\nm\u2032,j\n\u03b1mi\u03b1m\u2032jymiym\u2032j(x T miW\u0302(W\u0302 TW\u0302)\u22121W\u0302Txm\u2032j) (6)\nsubject to: 0 \u2264 \u03b1mi \u2264 1\nML \u2200i \u2208 1, \u00b7 \u00b7 \u00b7 , L,m \u2208 1, \u00b7 \u00b7 \u00b7 ,M\nNote that this is equivalent to solving SVM using the projected datapoint (Hxmi, ymi), instead of (xmi, ymi), where H = W\u0302(W\u0302TW\u0302)\u22121W\u0302T , which is the projection on column space of W\u0302. Hence the performance of the method is expected to depend on size and orientation of the column space of W\u0302. Next, we describe distributed algorithms for learning \u03b2."}, {"heading": "2.3 Distributed algorithms for WPA using ADMM", "text": "In the distributed setting, we assume the presence of a central (master) computer which stores and updates the final hypothesis. The partitions of training set S1, . . . ,SM are distributed to M (slave) computers, where the local optimizations are performed. The master needs to communicate to slaves and vice versa. However, no communication between slaves is necessary. Thus, the underlying networks has a star topology, which is also easily implemented using Big data platforms like Hadoop [1].\nLet \u03b3m, for m = 1, \u00b7 \u00b7 \u00b7 ,M be the weight values at the M different nodes and \u03b2 be the value of the weights at the central server. The formulation given in eqn. 4 can be written as:\nmin \u03b3m,\u03b2\n1\nML\nM \u2211\nm=1\nL \u2211\nl=1\nloss(W\u0302\u03b3m;xml, yml) + r(\u03b2) (7)\ns.t. \u03b3m \u2212 \u03b2 = 0, m = 1, \u00b7 \u00b7 \u00b7 ,M, where r(\u03b2) = \u03bb\u2016W\u0302\u03b2\u20162. The augmented lagrangian for the above problem is: L(\u03b3m,\u03b2,\u03bb) = 1 ML \u2211M m=1 \u2211L l=1 loss(W\u0302\u03b3m;xml, yml)+r(\u03b2)+ \u2211M i=1 \u03c1 2 \u2016\u03b3m\u2212\u03b2\u2016 2+ \u2211M i=1\u03c8 T m(\u03b3m\u2212\u03b2), where\n\u03c8m is the lagrange multiplier vector corresponding to mth constraint. Let Am \u2208 RL\u00d7d = \u2212diag(ym)XmW\u0302 . Using results from [2], the ADMM updates for solving the above problem can derived as:\n\u03b3k+1m := argmin \u03b3 (loss(Ai\u03b3) + (\u03c1/2)\u2016\u03b3ml \u2212 \u03b2 k + ukm\u2016 2 2) (8)\n\u03b2k+1 := argmin \u03b2 (r(\u03b2) + (M\u03c1/2)\u2016\u03b2 \u2212 \u03b3k+1 \u2212 uk\u201622) (9)\nuk+1m = u k m + \u03b3 k+1 m \u2212 \u03b2 k+1. (10)\nwhere, um = 1\u03c1\u03c8m, \u03b3 = 1 M \u2211M m=1 \u03b3m and u = 1 M \u2211M m=1 um and the superscript k denotes the iteration counts. Algorithm 1 describes the full procedure.\nAlgorithm 1: Distributed Weighted Parameter Averaging (DWPA) input : Partitioned datasets Sm, SVM parameter learnt for each partition w\u0302m, \u2200m = 1, \u00b7 \u00b7 \u00b7 ,M output: Optimal weight vector \u03b2\n1 Initialize \u03b2 = 1,\u03b3m = 1,um = 1, \u2200m \u2208 {1, \u00b7 \u00b7 \u00b7 ,M}; 2 while k < T do /* Executed on slaves */ 3 for m \u2190 1 to M do 4 \u03b3km := argmin\u03b3m(1 T (Am\u03b3m + 1)+ + \u03c1/2\u2016\u03b3 k\u22121 m \u2212 \u03b2 k\u22121 \u2212 uk\u22121m \u2016 2 2) 5 end\n/* Executed on master */\n6 \u03b2k := 1 2\u03bb (W\u0302T W\u0302 +M\u03c1Im) \u22121M\u03c1(\u03b3k + uk\u22121) 7 for m \u2190 1 to M do 8 ukm = u k\u22121 m + \u03b3 k m \u2212 \u03b2 k\n9 end 10 end\nA heuristic called overrelaxation [2] is ofter used for improving the convergence rate of ADMM. For overrelaxation, the updates for \u03b2k (line 6 and ukm (line 8) are obtained by replacing \u03b3\nk with \u03b3\u0302km = \u03b1 \u00d7 \u03b3 k m + (1 \u2212 \u03b1) \u00d7 \u03b2\nk\u22121, in algorithm 1. We implemented this heuristic for both DSVM and DWPA. We call them accelarated DSVM (DSVMacc) and accelarated DWPA (DWPAacc)."}, {"heading": "2.4 Bound on stability of WPA", "text": "In this section, we derive a bound of O( 1 ML ) on stability of the final hypothesis returned by WPA algorithm described in eqn. 4. A similar bound was derived by Mann et al. [9] on the stability of PA. This leads to a O( 1\u221a\nML ) bound on deviation from optimizer of generalization error (see [9],\ntheorem 2).\nLet S = {S1, \u00b7 \u00b7 \u00b7 , SM} and S\u2032 = {S\u20321, \u00b7 \u00b7 \u00b7 , S \u2032 M} be two datasets with M partitions and L datapoints per partition, differing in only one datapoint. Hence, Sm = {zm1, \u00b7 \u00b7 \u00b7 , zmL} and S\u2032m = {z \u2032 m1, \u00b7 \u00b7 \u00b7 , z \u2032 mL}, where zml = (xml, yml) and z \u2032 ml = (x \u2032 ml, y \u2032 ml). Further, S1 = S\u20321, \u00b7 \u00b7 \u00b7 , SM\u22121 = S \u2032 M\u22121, and SM and S \u2032 M differs at single point zML and z \u2032 ML. Also, let \u2016x\u2016 \u2264 R, \u2200x. Moreover, let W\u0302 = [w\u0302S1 , \u00b7 \u00b7 \u00b7 , w\u0302SM ] and W\u0302 \u2032 = [w\u0302S\u20321 , \u00b7 \u00b7 \u00b7 , w\u0302S\u2032M ] where, w\u0302Si = argminw \u03bb\u2016w\u2016 2 + 1\nL\n\u2211\ni\u2208Si max(0, 1 \u2212 yw Tx).We also assume \u2016W\u0302\u2016F = \u2016W\u0302\u2032\u2016F = 1.\nHence, \u2016w\u0302m\u20162 = \u2016w\u0302\u2032m\u2016 2 = 1 M , \u2200m \u2208 {1, \u00b7 \u00b7 \u00b7 ,M}.\nWe also define the following quantities:\n\u03b2 = argmin \u03b2\n\u03bb\u2016W\u0302\u03b2\u20162 + 1\nML\nM\u2211\ni=1\n\u2211\nz\u2208Si\nmax(0, 1\u2212 y(W\u0302\u03b2)Tx)\n\u03b2 \u2032 = argmin \u03b2 \u03bb\u2016W\u0302 \u2032\u03b2\u20162 +\n1\nML\nM\u2211\ni=1\n\u2211\nz\u2032\u2208S\u2032 i\nmax(0, 1\u2212 y\u2032(W\u0302\u03b2)Tx\u2032)\n\u03b2\u0303 = argmin \u03b2\n\u03bb\u2016W\u0302 \u2032\u03b2\u20162 + 1\nML\nM\u2211\ni=1\n\u2211\nz\u2208Si\nmax(0, 1\u2212 y(W\u0302 \u2032\u03b2)Tx)\nAlso, let \u03b8 = W\u0302\u03b2, \u03b8\u2032 = W\u0302 \u2032\u03b2\u2032 and \u03b8\u0303 = W\u0302 \u2032\u03b2\u0303.\nWe are interested in deriving a bound on \u2016\u03b8 \u2212 \u03b8\u2032\u2016, which decompose as: \u2016\u03b8 \u2212 \u03b8\u2032\u2016 \u2264 \u2016\u03b8 \u2212 \u03b8\u0303\u2016 + \u2016\u03b8\u0303 \u2212 \u03b8\u2032\u2016. Intuitively, the first term captures the change from W\u0302 to W\u0302 \u2032 and second term captures change in dataset. Lemma 2.2, shows that \u2016\u03b8\u0303\u2212\u03b8\u2032\u2016 is O( 1\nML ). Showing bound on \u2016\u03b8\u2212 \u03b8\u0303\u2016 requires\nbounds on \u2016\u03b2 \u2212 \u03b2\u0303\u2016 (lemma 2.3) and \u2016W\u0302 \u2212 W\u0302 \u2032\u2016 (lemma 2.1). The final proof is given in Theorem 2.4.\nLemma 2.1. \u2016W\u0302 \u2212 W\u0302 \u2032\u2016 = O( 1 ML )\nProof (sketch): Since w\u0302m = w\u0302\u2032m,m = 1, . . . ,M \u2212 1, it suffices to show that \u2016w\u0302M \u2212 w\u0302 \u2032 M\u2016 = O( 1 ML ). Since, w\u0302 and w\u0302\u2032 are scaled as \u2016w\u0302m\u20162 = \u2016w\u0302\u2032m\u2016 2 = 1 M it suffices to show thatM\u2016w\u0302\u2212w\u0302\u2032\u2016 = O( 1 L ). This result is analogous to theorem 1 of [9]. This can be proved using a special definition of bregmann divergence shown in appendix A.\nLemma 2.2. \u2016\u03b8\u0303 \u2212 \u03b8\u2032\u2016 = O( 1 ML )\nProof (sketch): This can be shown using similar technique as proof in appendix B using \u2016 \u00b7 \u2016K , where, K = W\u0302 \u2032 T W\u0302 \u2032 instead of the Euclidean norm.\nLemma 2.3. \u2016\u03b2 \u2212 \u03b2\u0303\u2016 = O( 1 ML )\nProof: Let FW (\u03b2) = GW (\u03b2) + LW (\u03b2) and FW \u2032 (\u03b2\u0303) = GW \u2032 (\u03b2\u0303) + LW \u2032(\u03b2\u0303). Using a similar definition of Bregmann divergence as in appendix B and its positivity:\nBG W\u0302 (\u03b2\u0303\u2016\u03b2) +BG \u02c6 W \u2032 (\u03b2\u2016\u03b2\u0303) \u2264 BF W\u0302 (\u03b2\u0303\u2016\u03b2) +BF \u02c6 W \u2032 (\u03b2\u2016\u03b2\u0303) (11)\nThe left hand side of the inequality 11, is given by;\nBG W\u0302 (\u03b2\u0303\u2016\u03b2) +BG \u02c6 W \u2032 (\u03b2\u2016\u03b2\u0303) = \u03bb\u2016\u03b2\u0303 \u2212 \u03b2\u2016T (\u2016W\u0302T W\u0302 + W\u0302 \u2032\nT W\u0302 \u2032\u2016)\u2016\u03b2\u0303 \u2212 \u03b2\u2016\n\u2264 \u03bb\u2016\u03b2 \u2212 \u03b2\u0303\u20162K\u2032 , where, K \u2032 = W\u0302T W\u0302 + W\u0302 \u2032\nT W\u0302 \u2032\nNow we solve the right hand side of inequality 11, BF\nW\u0302 (\u03b2\u0303\u2016\u03b2) +BF \u02c6 W \u2032 (\u03b2\u2016\u03b2\u0303) = F W\u0302 (\u03b2\u0303)\u2212 F W\u0302 (\u03b2) + F W\u0302 \u2032 (\u03b2) + F W\u0302 \u2032 (\u03b2\u0303)\n=\u03bb[\u2016W\u0302 \u03b2\u0303\u20162 \u2212 \u2016W\u0302\u03b2\u20162 + \u2016W\u0302 \u2032\u03b2\u20162 \u2212 \u2016W\u0302 \u2032\u03b2\u0303\u20162]+ (12)\n[L W\u0302 (\u03b2\u2032)\u2212 L W\u0302 (\u03b2) + L W\u0302 \u2032 (\u03b2)\u2212 L W\u0302 \u2032 (\u03b2\u2032)] = R+ L\nFrom 12, we have, L = LW\u0302 (\u03b2 \u2032)\u2212 LW\u0302 (\u03b2) + LW\u0302 \u2032(\u03b2)\u2212 LW\u0302 \u2032(\u03b2 \u2032)\n= 1\nML\nM,L\u2211\nm,l=1\n[max(0, 1\u2212 yml(W\u0302 \u03b2\u0303) T xml)\u2212max(0, 1\u2212 yml(W\u0302\u03b2) T xml)+\nmax(0, 1\u2212 yml(W\u0302 \u2032\u03b2) T xml)\u2212max(0, 1\u2212 yml(W\u0302 \u2032\u03b2\u0303) T xml)]\n\u2264 1\nML\nM,L\u2211\nm,l=1\nmax(0, yml((W\u0302 \u2032 \u2212 W\u0302 )(\u03b2 \u2212 \u03b2\u0303)) T xml) \u2264\n1\nML\nM,L\u2211\nm,l=1\n|yml((W\u0302 \u2032 \u2212 W\u0302 )(\u03b2 \u2212 \u03b2\u0303)) T xml|\n\u2264 R\nML \u2016(\u03b2 \u2212 \u03b2\u0303))\u2016\nWhere, first two inequalities use: max(a, 0) \u2212max(b, 0) \u2264 max(a \u2212 b, 0), and the last step uses lemma 2.1.\nFor the part R of the 12 involving regularizers:\nR = \u03bb[\u2016W\u0302 \u03b2\u0303\u20162 \u2212 \u2016W\u0302\u03b2\u20162 + \u2016W\u0302 \u2032\u03b2\u20162 \u2212 \u2016W\u0302 \u2032\u03b2\u0303\u20162] = \u03bb(\u03b2\u0303 + \u03b2)T (W\u0302T W\u0302 \u2212 W\u0302 \u2032 T W\u0302 \u2032)(\u03b2\u0303 \u2212 \u03b2)\n\u2264 \u03bb\u2016\u03b2\u0303 + \u03b2\u2016\u2016W\u0302\u2016\u2016(W\u0302 \u2212 W\u0302 \u2032)\u2016\u2016\u03b2\u0303 \u2212 \u03b2\u2016+ \u2016\u03b2\u0303 + \u03b2\u2016\u2016W\u0302 \u2032\u2016\u2016(W\u0302 \u2212 W\u0302 \u2032)\u2016\u2016\u03b2\u0303 \u2212 \u03b2\u2016\n\u2264 4\u03bbR\nML \u2016\u03b2 \u2212 \u03b2\u0303\u2016\nwhere for the last step, we use the constant bound \u2016\u03b2\u2016 = \u03bbR on \u03b2 obtained from its expression of in 5. Therefore, from the left hand side and right hand side of the inequality 11, we have:\n\u03bb\u2016\u03b2 \u2212 \u03b2\u0303\u201622 \u2264 \u03bb\n\u03c3min \u2016\u03b2 \u2212 \u03b2\u0303\u20162K\u2032 \u2264\n4\u03bbR ML \u2016\u03b2 \u2212 \u03b2\u0303\u2016+ R ML \u2016(\u03b2 \u2212 \u03b2\u0303))\u2016 (13)\nwhere, \u03c3min is smallest eigenvalue of K \u2032. This implies;\u2016\u03b2 \u2212 \u03b2\u0303\u2016 is O( 1ML ).\nTheorem 2.4. \u2016\u03b8 \u2212 \u03b8\u2032\u2016 is of the order of O( 1 ML ).\nProof: The steps involved in the proof are as follows; \u2016\u03b8 \u2212 \u03b8\u2032\u2016 \u2264 \u2016\u03b8 \u2212 \u03b8\u0303\u2016+ \u2016\u03b8\u0303 \u2212 \u03b8\u2032\u2016 (14)\nFrom lemma 2.2, \u2016\u2016\u03b8 \u2212 \u03b8\u0303\u2016 is O( 1 ML ).\n\u2016\u03b8\u0303 \u2212 \u03b8\u2032\u2016 \u2264 1\n2 (\u2016(W\u0302 \u2212 W\u0302 \u2032)(\u03b2 + \u03b2\u0303)\u2016+ \u2016(W\u0302 + W\u0302 \u2032)(\u03b2 \u2212 \u03b2\u0303)\u2016)\n\u2264 1\n2 ((\u2016(W\u0302 \u2212 W\u0302 \u2032\u2016)(\u2016\u03b2 + \u03b2\u0303\u2016) + (\u2016W\u0302 + W\u0302 \u2032\u2016)(\u2016\u03b2 \u2212 \u03b2\u0303\u2016)) (15)\nWe have already, shown that, we have a constant bound on \u2016\u03b2+ \u03b2\u0303\u2016 and \u2016W\u0302 + W\u0302 \u2032\u2016F , since, norms of \u03b2, \u03b2\u0303, W\u0302 and W\u0302 \u2032 are bounded. Also both \u2016\u03b2 \u2212 \u03b2\u0303\u2016 and \u2016(W\u0302 \u2212 W\u0302 \u2032\u2016 are O( 1\nML ).\nHence, from 15, we have the required result."}, {"heading": "3 Experimental Results", "text": "In this section, we experimentally analyze and compare the methods proposed here, distributed weighted parameter averaging (DWPA) and accelerated DWPA (DWPAacc) described in section 2.3, with parameter averaging (PA) [9], Distributed SVM (DSVM) using ADMM, and accelerated DSVM (DSVMacc) [2]. For our experimentation, we have implemented all the above mentioned algorithms in Matlab [10]. We have used the liblinear library [5] to obtain the SVM parameters corresponding each partition. Optimization problems which arise as subproblems in ADMM has been solved using CVX [7], [8].\nWe used both toy datasets (section 3.1) and real world datasets (described in table 1) for our experiments. Real world datasets were obtained from LIBSVM website [3]. Samples for real world datasets were selected randomly. The datasets were selected to have various ranges of feature count and sparsity. Section 3.1 describes a specially construc"}, {"heading": "3.1 Results on toy dataset", "text": "The main purpose of the toy dataset was to visually observe the effect of change in the number of partitions on the final hypothesis for various algorithms. Datapoints are generated from a 2 dimensional mixture of gaussians. In figure 1, the red and blue dots indicate the datapoints from two different classes. The upper red blob contains only 20% of red points. Hence as the number of partitions increase, many partitions will not have any data points from upper blob. For these partitions, the separating hyperplane passes throught the upper red blob. These cause the average hyperplane to pass through upper red blob, thus decreasing the accuracy. This effect is visible in the left plot of figure 1. This effect is mitigated in weighted parameter averaging as the weights learnt for the hyperplanes passing through upper red blob are lesser. This is shown in middle plot of figure 1. Finally, the right plot of figure 1 shows the resultant decrease in accuracy for PA with increase in number of partitions.\nBias of a learning algorithm is E[|w\u2212w\u2217 |], where w and w\u2217 are minimizers of regularized loss and generalization error, and the expectation is over all samples of fixed size say N . An criticism against PA is the lack of bound on bias [9]. In table 2, we compare bias of PA, WPA and SVM as a function of N . Data samples were generated from the same toy distribution as above. w\u2217 was computed by training on a large sample size and ensuring that training set error and test set error are very close. w was computed 100 times by randomly sampling from the distribution. The average of |w \u2212 w \u2217 | is reported in table 2. We observe that bias of PA is indeed much higher than SVM or WPA."}, {"heading": "3.2 Comparison of Accuracies", "text": "In this section, we compare accuracies obtained by various algorithms on real world datasets, with increase in number of paritions. Figure 2 reports test set accuracies for PA, WPA and SVM on three real world datasets with varying size of partitions. It is clear that performance of PA degrades dramatically as the number of parition increases. Thus, the effect demonstrated in section 3.1 is also observed on real world datasets.\nWe also observe that performance of WPA improves with increase in number of paritions. This is due to fact that dimension of space on which xml\u2019s are projected using H (section 2.2) increases, thus reducing the information loss caused by projection. Finally, as expected WPA performs slightly worse than SVM."}, {"heading": "3.3 Convergence Analysis and time comparison", "text": "In this section, we compare the convergence properties of DSVM, DSVMacc, DWPA, and DWPAacc. Here we report results on real-sim due to lack of space. Results on other real world datasets are provided in appendix C. Top row of figure 3 shows variation of primal residual (disagreement between parameters on various partitions) with iterations. It is clear that DWPA and DWPAacc\nshow much lesser disagreement compared to DSVM and DSVMacc, thus showing faster convergence. Bottom row fo figure 3 shows variation of test set accuracy with iterations. The same behaviour is apparent here, with testset accuracy of DWPA and DWPAacc converging much faster than DSVM and DSVMacc. One of the reasons is also that DWPA has an obvious good starting point of \u03b2 = [ 1\nM , . . . , 1 M ] corresponding to PA.\nTable 3 reports the average time taken by DWPA and DSVM for completing one iteration as a function of number of paritions. It is clear that DWPA takes much lesser time due to much smaller number of variables in the local optimization problem (Feature dimensions for DSVM, number of paritions for DWPA). There is slight increase in time per iteration with increase in number of paritions due to increase in number of variables."}, {"heading": "4 Conclusion", "text": "We propose a novel approach for training SVM in a distributed manner by learning an optimal set of weights for combining the SVM parameters independently learnt on partitions of the entire dataset. Experimental results show that our method is much more accurate than parameter averaging and is much faster than training SVM in feature space. Moreover, our method reaches an accuracy close to that of SVM trained in feature space in a much shorter time. We propose a novel proof to show that the stability final SVM parameter learnt using DWPA is O( 1\nML ). Also, our method requires much\nless network band-width as compared to DSVM when the number of features for a given dataset is very large as compared to the number of partitions, which is the usual scenerio for Big Data."}, {"heading": "Appendix A", "text": "Theorem 4.1. For any two arbitrary training samples of size L differing by one sample point, the stability bound that holds for the parameter vectors returned by support vector machine is:\n\u2016\u2206w\u2016 is O( 1\nML ). (16)\nProof. Suppose we have two training datasets S = (z1, \u00b7 \u00b7 \u00b7 , zL\u22121, zL) and S\u2032 = (z1, \u00b7 \u00b7 \u00b7 , zL\u22121, z \u2032 L), where z = (x, y) \u2208 X \u00d7 Y , such that X \u2282 R\nd and Y = {\u22121,+1}. The two sets differ at a single data point: zL = (xi, yi) and z\u2032L = (x \u2032 i, y \u2032 i). Let BF be the Bregman divergence associated with a convex and non-diffentiable function F defined for all x, y by; BF (x, y) = F (x) \u2212 F (y)\u2212 < gy, (x\u2212 y) >, where g \u2208 \u2202Fy and \u2202Fy is the set of subdifferentials of F at y. Since, the minima is achieved at a point y if 0 \u2208 \u2202Fy . We define g as follows;\ng =\n{\n0 if 0 \u2208 \u2202Fy h subject to, h \u2208 \u2202Fy\n(17)\nLet Ls : x \u2192 \u2211L i=1 Hzi(x), where, Hzi(x) = max(0, 1 \u2212 yw Tx) denote the loss function and G : x \u2192 \u03bb\u2016x\u20162 denote the regularizer corresponding to the SVM problem. Clearly, the function, FS = G + LS , is the objective function for SVM. LS is convex and non-differentiable while G is convex and differentiable. Since, Bregman divergence is non-negative (BF \u2265 0),\nBFS = BG +BLS \u2265 BG (18)\nBF S\u2032 \u2265 BG\u2032 . (19)\nThus, BG(w \u2032\u2016w) +BG(w\u2016w \u2032) \u2264 BFS (w \u2032\u2016w) +BFS\u2032 (w\u2016w \u2032). (20)\nIf w and w\u2032 are minimizers for of BFS and BFS\u2032 , then, gS(w) = gS\u2032(w \u2032) = 0 and\nBFS (w \u2032\u2016w) + BF S\u2032 (w\u2016w\u2032) = FS(w \u2032)\u2212 FS(w) + FS\u2032(w) \u2212 FS\u2032(w \u2032)\n= 1\nL [HzL(w\n\u2032)\u2212HzL(w) +Hz\u2032L(w)\u2212Hz\u2032L(w \u2032)]\n\u2264 \u2212 1\nL [gzL \u00b7 (w\n\u2032)(w \u2212w\u2032) + gz\u2032L \u00b7 (w)(w \u2032 \u2212w)]\n= \u2212 1\nL [gz\u2032L(w)\u2212 gzL(w\n\u2032)] \u00b7 (w\u2032 \u2212w)\n= \u2212 1\nL [gz\u2032L(w)\u2212 gzL(w\n\u2032)] \u00b7 (\u2206w) (21)\nFrom definition, BG((w \u2032)\u2016w) +BG((w)\u2016w \u2032) = \u03bb\u2016\u2206w\u20162. (22)\nHence, from derivation 21 and equation 22 and Cauchy-Schwarz inequality we have,\n\u03bb\u2016w\u2016 \u2264 1\nL \u2016gz\u2032L(w)\u2212 gzL(w\n\u2032)\u2016 \u2264 1\nL [\u2016gz\u2032L(w)\u2016 + \u2016gzL(w\n\u2032)\u2016] (23)\nBy definition, Hzi(w) = max(0, 1\u2212 yiw Txi) and gzi(w) \u2208 \u2202Hzi(w). Therefore, \u2202Hzi(w) \u2264 \u2016yixi\u2016\n\u21d2\u2202Hzi(w) \u2264 \u2016xi\u2016\n\u21d2\u2016gzi(w)\u2016 \u2264 \u2016xi\u2016.\nIf we assume that the feature vectors are bounded i.e., there exists a positive integer R > 0 such that for all training instances (x, y) \u2208 X \u00d7 Y , \u2016x\u2016 \u2264 R, then we may state that,\n\u2016w\u2016 \u2264 R\n\u03bbL (24)\nSince, w is normalized and scaled by 1 M . So, the bound on \u2016\u2206w\u2016 in our case, is O( 1 ML )."}, {"heading": "Appendix B", "text": "Theorem 4.2. \u2016\u03b8\u2032 \u2212 \u03b8\u0303\u2016 is of the order of O( 1 ML ).\nProof: From definitions we have; \u2016\u03b8\u2032 \u2212 \u03b8\u0303\u20162 = \u2016W\u0302 \u2032\u03b2\u2032 \u2212 W\u0302 \u2032\u03b2\u0303\u20162\n\u2264 \u2016\u03b2\u2032 \u2212 \u03b2\u0303\u20162K , where ,K = W\u0302 \u2032 T W\u0302 \u2032 (25)\nSince, we have a lower bound on \u2016\u03b2\u2032 \u2212 \u03b2\u0303\u20162 \u2264 1 \u03c3min \u2016\u03b2\u2032 \u2212 \u03b2\u0303\u20162K , where \u03c3min is the minimum eigenvalue of K .\nHence, we need to prove an upper bound on \u2016\u03b2\u2032 \u2212 \u03b2\u0303\u2016.\nFrom the reasoning of theorem 2.1, we have; BG(\u03b2 \u2032\u2016\u03b2\u0303) +BG(\u03b2\u0303\u2016\u03b2 \u2032) \u2264 BFS (\u03b2 \u2032\u2016\u03b2\u0303) +BF \u2032 S (\u03b2\u0303\u2016\u03b2\u2032) (26)\nFrom the left hand side of the equation we have;\nBG(\u03b2 \u2032\u2016\u03b2\u0303) +BG(\u03b2\u0303\u2016\u03b2 \u2032) = (\u03b2\u0303 + \u03b2\u2032)T (W\u0302 \u2032 T W\u0302 \u2032)(\u03b2\u2032 \u2212 \u03b2\u0303) (27)\nFrom the right hand side of the equation, using the similar reasoning as that used for \u2016\u03b2 \u2212 \u03b2\u0303\u2016, we have; BFS (\u03b2 \u2032\u2016\u03b2\u0303) +BF \u2032 S (\u03b2\u0303\u2016\u03b2\u2032) = LS(\u03b2 \u2032)\u2212 LS(\u03b2\u0303) + LS\u2032(\u03b2\u0303)\u2212 LS\u2032(\u03b2)\n\u2264 1\nML [max(0, (W\u0302 \u2032\u03b2\u2032)T (yz\u2032 m xz\u2032 m \u2212 yzmxzm)\u2212 (W\u0302 \u2032\u03b2\u0303)T (yz\u2032 m xz\u2032 m \u2212 yzmxzm)]\n\u2264 1\nML |W\u0302 \u2032(\u03b2\u0303 \u2212 \u03b2\u2032)(yz\u2032 m xz\u2032 m \u2212 yzmxzm)|\n\u2264 2R\nML \u2016\u03b2\u0303 \u2212 \u03b2\u2032\u2016 (28)\nEquating, left hand side and right hand side of the equation, we get; \u2016\u03b2\u2032\u2212 \u03b2\u0303\u2016 is O( 1 ML ), and hence, from 25, we get, \u2016\u03b8\u2032 \u2212 \u03b8\u0303\u2016 is O( 1 ML ).\nAppendix C"}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "LIB- LINEAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Consensus-based distributed support vector machines", "author": ["Pedro A. Forero", "Alfonso Cano", "Georgios B. Giannakis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["Michael Grant", "Stephen Boyd"], "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.0 beta", "author": ["Michael Grant", "Stephen Boyd"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["Gideon Mann", "Ryan McDonald", "Mehryar Mohri", "Nathan Silberman", "Dan Walker"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction With the growing popularity of Big Data platforms [1] for various machine learning and data analytics applications [9, 12], distributed training of Support Vector Machines (SVMs)[4] on Big Data platforms have become increasingly important.", "startOffset": 130, "endOffset": 137}, {"referenceID": 6, "context": "Big data platforms such as Hadoop [1] provide simple programming abstraction (Map Reduce), scalability and fault tolerance at the cost of distributed iterative computation being slow and expensive [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "Parameter averaging (PA), also called \u201cmixture weights\u201d [9] or \u201cparallelized SGD\u201d [12], suggests solving an appropriate RLM problem on data in each node, and use average of the resultant parameters.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Another interesting result described in [9] is a bound of O( 1 ML ) on the stability of the final hypothesis, which results in a bound on deviation from optimizer of generalization error.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "Another popular approach for distributed RLM is alternating direction method of multipliers (ADMM) [2, 6].", "startOffset": 99, "endOffset": 105}, {"referenceID": 3, "context": "Another popular approach for distributed RLM is alternating direction method of multipliers (ADMM) [2, 6].", "startOffset": 99, "endOffset": 105}, {"referenceID": 1, "context": "webspam [3].", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "We propose an ADMM [2] based distributed algorithm (DWPA), and an accelerated version (DWPAacc), for learning the weights.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "This problem is solved in [2] using ADMM (see section 2.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "[9], in the context of conditional maximum entropy model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] suggest the approximate final parameter to be the arithmetic mean of the parameters learnt on individual partitions, (\u0175m).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that assumptions regarding differentiability of loss function made in [2] can be relaxed in case of convex loss function with an appropriate definition of bregmann divergence using sub-gradients (see [11], section 2.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Note that assumptions regarding differentiability of loss function made in [2] can be relaxed in case of convex loss function with an appropriate definition of bregmann divergence using sub-gradients (see [11], section 2.", "startOffset": 205, "endOffset": 209}, {"referenceID": 6, "context": "[9] proposed \u03b2 to be in a simplex.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Using results from [2], the ADMM updates for solving the above problem can derived as: \u03b3 m := argmin \u03b3 (loss(Ai\u03b3) + (\u03c1/2)\u2016\u03b3ml \u2212 \u03b2 k + ukm\u2016 2 2) (8) \u03b2 := argmin \u03b2 (r(\u03b2) + (M\u03c1/2)\u2016\u03b2 \u2212 \u03b3 \u2212 uk\u201622) (9) u m = u k m + \u03b3 k+1 m \u2212 \u03b2 .", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "A heuristic called overrelaxation [2] is ofter used for improving the convergence rate of ADMM.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[9] on the stability of PA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This leads to a O( 1 \u221a ML ) bound on deviation from optimizer of generalization error (see [9], theorem 2).", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "This result is analogous to theorem 1 of [9].", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "3, with parameter averaging (PA) [9], Distributed SVM (DSVM) using ADMM, and accelerated DSVM (DSVMacc) [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "3, with parameter averaging (PA) [9], Distributed SVM (DSVM) using ADMM, and accelerated DSVM (DSVMacc) [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "We have used the liblinear library [5] to obtain the SVM parameters corresponding each partition.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Optimization problems which arise as subproblems in ADMM has been solved using CVX [7], [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "Optimization problems which arise as subproblems in ADMM has been solved using CVX [7], [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Real world datasets were obtained from LIBSVM website [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "An criticism against PA is the lack of bound on bias [9].", "startOffset": 53, "endOffset": 56}], "year": 2014, "abstractText": "Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an O( 1 N ) stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space.", "creator": "gnuplot 4.4 patchlevel 3"}}}