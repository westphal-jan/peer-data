{"id": "1510.01784", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "abstract": "modern recommender systems model people and items by discovering or ` teasing slipping apart'the underlying dimensions that encode the properties of items and users'preferences toward them. critically, such dimensions are uncovered based on user feedback, often in implicit form ( such include as purchase histories, browsing consumption logs, etc. ) ; in financial addition, some recommender systems make use of advisory side information, such as product attributes, temporal information, or review text. however one important feature that is typically ignored by existing personalized recommendation scales and ranking methods is the visual appearance of the items being considered. in this paper we propose a scalable factorization model to incorporate visual signals into predictors of people's opinions, which we apply fundamentally to a selection of large, real - world datasets. we make use of visual features extracted from product images using ( pre - trained ) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in surrounding people's feedback. this not only leads to significantly more accurate personalized ranking assessment methods, but also helps to primarily alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people's opinions.", "histories": [["v1", "Tue, 6 Oct 2015 23:46:15 GMT  (4989kb,D)", "http://arxiv.org/abs/1510.01784v1", "AAAI'16"]], "COMMENTS": "AAAI'16", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["ruining he", "julian mcauley"], "accepted": true, "id": "1510.01784"}, "pdf": {"name": "1510.01784.pdf", "metadata": {"source": "CRF", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "authors": ["Ruining He", "Julian McAuley"], "emails": ["r4he@ucsd.edu", "jmcauley@ucsd.edu"], "sections": [{"heading": "Introduction", "text": "Modern Recommender Systems (RSs) provide personalized suggestions by learning from historical feedback and uncovering the preferences of users and the properties of the items they consume. Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al., 2007; Lu et al., 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).\nThe \u2018historical feedback\u2019 used to train such systems may come in the form of explicit feedback such as star ratings, or implicit feedback such as purchase histories, bookmarks, browsing logs, search patterns, mouse activities etc. (Yi et al., 2014). In order to model user feedback in large, realworld datasets, Matrix Factorization (MF) approaches have\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbeen proposed to uncover the most relevant latent dimensions in both explicit and implicit feedback settings (Bell, Koren, and Volinsky, 2007; Hu, Koren, and Volinsky, 2008; Pan et al., 2008; Rendle et al., 2009). Despite the great success, they suffer from cold start issues due to the sparsity of real-world datasets.\nVisual personalized ranking. Although a variety of sources of data have been used to build hybrid models to make cold start or context-aware recommendations (Schein et al., 2002), from text (Bao, Fang, and Zhang, 2014), to a user\u2019s physical location (Qiao et al., 2014), to the season or temperature (Brown, Bovey, and Chen, 1997), here we are interested in incorporating the visual appearance of the items into the preference predictor, a source of data which is typically neglected by existing RSs. One wouldn\u2019t buy a t-shirt from Amazon without seeing the item in question, and therefore we argue that this important signal should not be ignored when building a system to recommend such products.\nBuilding on the success of Matrix Factorization methods at uncovering the latent dimensions/factors of people\u2019s behavior, our goal here is to ask whether it is possible to uncover the visual dimensions that are relevant to people\u2019s opinions, and if so, whether such \u2018visual preference\u2019 models shall lead to improved performance at tasks like personalized ranking. Answering these questions requires us to develop scalable methods and representations that are capable of handling millions of user actions, in addition to large volumes of visual data (e.g. product images) about the content they consume.\nIn this paper, we develop models that incorporate visual features for the task of personalized ranking on implicit feedback datasets. By learning the visual dimensions people consider when selecting products we will be able to alleviate cold start issues, help explain recommendations in terms of visual signals, and produce personalized rankings that are more consistent with users\u2019 preferences. Methodologically we model visual aspects of items by using representations of product images derived from a (pre-trained) deep network (Jia et al., 2014), on top of which we fit an additional layer that uncovers both visual and latent dimensions that are relevant to users\u2019 opinions. Although incorporating complex and domain-specific features often requires some amount of manual engineering, we found that visual features are read-\nar X\niv :1\n51 0.\n01 78\n4v 1\n[ cs\n.I R\n] 6\nO ct\n2 01\n5\nily available out-of-the-box that are suitable for our task. Experimentally our model exhibits significant performance improvements on real-world datasets like Amazon clothing, especially when addressing item cold start problems. Specifically, our main contributions are listed as follows: \u2022 We introduce a Matrix Factorization approach that incor-\nporates visual signals into predictors of people\u2019s opinions while scaling to large datasets.\n\u2022 Derivation and analysis of a Bayesian Personalized Ranking (BPR) based training procedure, which is suitable to uncover visual factors.\n\u2022 Experiments on large and novel real-world datasets revealing our method\u2019s effectiveness, as well as visualizations of the visual rating space we uncover."}, {"heading": "Related Work", "text": "Matrix Factorization (MF) methods relate users and items by uncovering latent dimensions such that users have similar representations to items they rate highly, and are the basis of many state-of-the-art recommendation approaches. (e.g. Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges.\nPoint-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix.\nIn contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data.\nOthers have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these\nworks have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here.\nExploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture. Another method was proposed by Kalantidis, Kennedy, and Li (2013), which accepts a query image and uses segmentation to detect clothing classes before retrieving visually similar products from each of the detected classes. McAuley et al. (2015) use visual features extracted from CNNs and learn a visual similarity metric to identify visually complementary items to a query image. In contrast to our method, the above works focus on visual retrieval, which differs from recommendation in that such methods aren\u2019t personalized to users based on historical feedback, nor do they take into account other factors besides visual dimensions, both of which are essential for a method to be successful at addressing one-class personalized ranking tasks. Thus it is the combination of visual and historical user feedback data that distinguishes our approach from prior work.\nVisual Features. Recently, high-level visual features from Deep Convolutional Neural Networks (\u2018Deep CNNs\u2019) have seen successes in tasks like object detection (Russakovsky et al., 2014), photographic style annotations (Karayev et al., 2014), and aesthetic quality categorization (Lu et al., 2014), among others. Furthermore, recent transfer learning studies have demonstrated that CNNs trained on one large dataset (e.g. ImageNet) can be generalized to extract CNN features for other datasets, and outperform stateof-the-art approaches on these new datasets for different visual tasks (Donahue et al., 2014; Razavian et al., 2014). These successes demonstrate the highly generic and descriptive ability of CNN features for visual tasks and persuade us to exploit them for our recommendation task.\nVBPR: Visual Bayesian Personalized Ranking In this section, we build our visual personalized ranking model (VBPR) to uncover visual and latent (non-visual) dimensions simultaneously. We first formulate the task in question and introduce our Matrix Factorization based predictor function. Then we develop our training procedure using a Bayesian Personalized Ranking (BPR) framework. The notation we use throughout this paper is summarized in Table 1."}, {"heading": "Problem Formulation", "text": "Here we focus on scenarios where the ranking has to be learned from users\u2019 implicit feedback (e.g. purchase histories). Letting U and I denote the set of users and items respectively, each user u is associated with an item set I+u about which u has expressed explicit positive feedback. In addition, a single image is available for each item i \u2208 I. Using only the above data, our objective is to generate for each user u a personalized ranking of those items about which they haven\u2019t yet provided feedback (i.e. I \\ I+u )."}, {"heading": "Preference Predictor", "text": "Our preference predictor is built on top of Matrix Factorization (MF), which is state-of-the-art for rating prediction as well as modeling implicit feedback, whose basic formulation assumes the following model to predict the preference of a user u toward an item i (Koren and Bell, 2011):\nx\u0302u,i = \u03b1+ \u03b2u + \u03b2i + \u03b3 T u \u03b3i, (1)\nwhere \u03b1 is global offset, \u03b2u and \u03b2i are user/item bias terms, and \u03b3u and \u03b3i are K-dimensional vectors describing latent factors of user u and item i (respectively). The inner product \u03b3Tu \u03b3i then encodes the \u2018compatibility\u2019 between the user u and the item i, i.e., the extent to which the user\u2019s latent \u2018preferences\u2019 are aligned with the products\u2019 \u2018properties\u2019.\nAlthough theoretically latent factors are able to uncover any relevant dimensions, one major problem it suffers from is the existence of \u2018cold\u2019 (or \u2018cool\u2019) items in the system, about which there are too few associated observations to estimate their latent dimensions. Using explicit features can alleviate this problem by providing an auxiliary signal in such situations. In particular, we propose to partition rating dimensions into visual factors and latent (non-visual) factors, as shown in Figure 1. Our extended predictor takes the form\nx\u0302u,i = \u03b1+ \u03b2u + \u03b2i + \u03b3 T u \u03b3i + \u03b8 T u \u03b8i, (2)\nwhere \u03b1, \u03b2, and \u03b3 are as in Eq. 1. \u03b8u and \u03b8i are newly introduced D-dimensional visual factors whose inner product models the visual interaction between u and i, i.e., the extent to which the user u is attracted to each of D visual dimensions. Note that we still use K to represent the number of latent dimensions of our model.\nOne naive way to implement the above model would be to directly use Deep CNN features fi of item i as \u03b8i in the above equation. However, this would present issues due to the high dimensionality of the features in question, for example the features we use have 4096 dimensions. Dimensionality reduction techniques like PCA pose a possible solution, with the potential downside that we would lose much of the expressive power of the original features to explain users\u2019 behavior. Instead, we propose to learn an embedding\nItem Visual Factors\nkernel which linearly transforms such high-dimensional features into a much lower-dimensional (say 20 or so) \u2018visual rating\u2019 space: \u03b8i = Efi (3) Here E is a D \u00d7 F matrix embedding Deep CNN feature space (F -dimensional) into visual space (D-dimensional), where fi is the original visual feature vector for item i. The numerical values of the projected dimensions can then be interpreted as the extent to which an item exhibits a particular visual rating facet. This embedding is efficient in the sense that all items share the same embedding matrix which significantly reduces the number of parameters to learn.\nNext, we introduce a visual bias term \u03b2\u2032 whose inner product with fi models users\u2019 overall opinion toward the visual appearance of a given item. In summary, our final prediction model is\nx\u0302u,i = \u03b1+ \u03b2u + \u03b2i + \u03b3 T u \u03b3i + \u03b8 T u (Efi) + \u03b2 \u2032T fi. (4)"}, {"heading": "Model Learning Using BPR", "text": "Bayesian Personalized Ranking (BPR) is a pairwise ranking optimization framework which adopts stochastic gradient ascent as the training procedure. A training set DS consists of triples of the form (u, i, j), where u denotes the user together with an item i about which they expressed positive feedback, and a non-observed item j:\nDS = {(u, i, j)|u \u2208 U \u2227 i \u2208 I+u \u2227 j \u2208 I \\ I+u }. (5) Following the notation in Rendle et al. (2009), \u0398 is the parameter vector and x\u0302uij(\u0398) denotes an arbitrary function of \u0398 that parameterises the relationship between the components of the triple (u, i, j). The following optimization criterion is used for personalized ranking (BPR-OPT):\u2211\n(u,i,j)\u2208DS\nln\u03c3(x\u0302uij)\u2212 \u03bb\u0398||\u0398||2 (6)\nwhere \u03c3 is the logistic (sigmoid) function and \u03bb\u0398 is a modelspecific regularization hyperparameter.\nWhen using Matrix Factorization as the preference predictor (i.e., BPR-MF), x\u0302uij is defined as\nx\u0302uij = x\u0302u,i \u2212 x\u0302u,j , 1 (7) where x\u0302u,i and x\u0302u,j are defined by Eq. 1. BPR-MF can be learned efficiently using stochastic gradient ascent. First a\n1Note that \u03b1 and \u03b2u in the preference predictor are canceled out in Eq. 7, therefore are removed from the set of parameters.\ntriple (u, i, j) is sampled from DS and then the learning algorithm updates parameters in the following fashion:\n\u0398\u2190 \u0398 + \u03b7 \u00b7 (\u03c3(\u2212x\u0302uij) \u2202x\u0302uij \u2202\u0398 \u2212 \u03bb\u0398\u0398), (8)\nwhere \u03b7 is the learning rate. One merit of our model is that it can be learned efficiently using such a sampling procedure with minor adjustments. In our case, x\u0302uij is also defined by Eq. 7 but we instead use Eq. 4 as the predictor function for x\u0302u,i and x\u0302u,j in Eq. 7. Compared to BPR-MF, there are now two sets of parameters to be updated: (a) the non-visual parameters, and (b) the newly-introduced visual parameters. Non-visual parameters can be updated in the same form as BPR-MF (therefore are suppressed for brevity), while visual parameters are updated according to:\n\u03b8u \u2190 \u03b8u + \u03b7 \u00b7 (\u03c3(\u2212x\u0302uij)E(fi \u2212 fj)\u2212 \u03bb\u0398\u03b8u), \u03b2\u2032 \u2190 \u03b2\u2032 + \u03b7 \u00b7 (\u03c3(\u2212x\u0302uij)(fi \u2212 fj)\u2212 \u03bb\u03b2\u03b2\u2032),\nE\u2190 E + \u03b7 \u00b7 (\u03c3(\u2212x\u0302uij)\u03b8u(fi \u2212 fj)T \u2212 \u03bbEE).\nNote that our method introduces an additional hyperparameter \u03bbE to regularize the embedding matrix E. We sample users uniformly to optimize the average AUC across all users to be described in detail later. All hyperparameters are tuned using a validation set as we describe in our experimental section later."}, {"heading": "Scalability", "text": "The efficiency of the underlying BPR-MF makes our models similarly scalable. Specifically, BPR-MF requires O(K) to finish updating the parameters for each sampled triple (u, i, j). In our case we need to update the visual parameters as well. In particular, updating \u03b8u takesO(D\u00d7F ) = O(D), \u03b2\u2032 takesO(F ), and E takesO(D\u00d7F ) = O(D), where F is the dimension of CNN features (fixed to 4096 in our case). Therefore the total time complexity of our model for updating each triple is O(K +D) (i.e. O(K) +O(D \u00d7 F )), i.e., linear in the number of dimensions. Note that visual feature vectors (fi) from Deep CNNs are sparse, which significantly reduces the above worst-case running time."}, {"heading": "Experiments", "text": "In this section, we perform experiments on multiple realworld datasets. These datasets include a variety of settings where visual appearance is expected to play a role in consumers\u2019 decision-making process."}, {"heading": "Datasets", "text": "The first group of datasets are from Amazon.com introduced by McAuley et al. (2015). We consider two large categories where visual features have already been demonstrated to be meaningful, namely Women\u2019s and Men\u2019s Clothing. We also consider Cell Phones & Accessories, where we expect visual characteristics to play a smaller but possibly still significant role. We take users\u2019 review histories as implicit feedback and use one image per item to extract visual features.\nWe also introduce a new dataset from Tradesy.com, a second-hand clothing trading community. It discloses users\u2019 purchase histories and \u2018thumbs-up\u2019, which we use together as positive feedback. Note that recommendation in this setting inherently involves cold start prediction due to the \u2018oneoff\u2019 trading characteristic of second-hand markets. Thus to design a meaningful recommender system for such a dataset it is critical that visual information be considered.\nWe process each dataset by extracting implicit feedback and visual features as already described. We discard users u where |I+u | < 5. Table 2 shows statistics of our datasets, all of which shall be made available at publication time."}, {"heading": "Visual Features", "text": "For each item i in the above datasets, we collect one product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever, and Hinton (2012). The architecture has 5 convolutional layers followed by 3 fully-connected layers, and has been pre-trained on 1.2 million ImageNet (ILSVRC2010) images. In our experiments, we take the output of the second fully-connected layer (i.e. FC7), to obtain an F = 4096 dimensional visual feature vector fi."}, {"heading": "Evaluation Methodology", "text": "We split our data into training/validation/test sets by selecting for each user u a random item to be used for validation Vu and another for testing Tu. All remaining data is used for training. The predicted ranking is evaluated on Tu with the widely used metric AUC (Area Under the ROC curve):\nAUC = 1 |U| \u2211 u 1 |E(u)| \u2211 (i,j)\u2208E(u) \u03b4(x\u0302u,i > x\u0302u,j) (9)\nwhere the set of evaluation pairs for user u is defined as\nE(u) = {(i, j)|(u, i) \u2208 Tu\u2227(u, j) /\u2208 (Pu\u222aVu\u222aTu)}, (10)\nand \u03b4(b) is an indicator function that returns 1 iff b is true . In all cases we report the performance on the test set T for the hyperparameters that led to the best performance on the validation set V ."}, {"heading": "Baselines", "text": "Matrix Factorization (MF) methods are known to have stateof-the-art performance for implicit feedback datasets. Since there are no comparable visual-aware MF methods, we mainly compare against state-of-the-art MF models, in addition to a recently proposed content-based method.\nTable 3: AUC on the test set T (#factors = 20). The best performing method on each dataset is boldfaced.\nDataset Setting (a) (b) (c) (d) (e) (f) improvementRAND MP IBR MM-MF BPR-MF VBPR f vs. best f vs. e\nAmazon Women All Items 0.4997 0.5772 0.7163 0.7127 0.7020 0.7834 9.4% 11.6%Cold Start 0.5031 0.3159 0.6673 0.5489 0.5281 0.6813 2.1% 29.0%\nAmazon Men All Items 0.4992 0.5726 0.7185 0.7179 0.7100 0.7841 9.1% 10.4%Cold Start 0.4986 0.3214 0.6787 0.5666 0.5512 0.6898 1.6% 25.1%\nAmazon Phones All Items 0.5063 0.7163 0.7397 0.7956 0.7918 0.8052 1.2% 1.7%Cold Start 0.5014 0.3393 0.6319 0.5570 0.5346 0.6056 -4.2% 13.3%\nTradesy.com All Items 0.5003 0.5085 N/A 0.6097 0.6198 0.7829 26.3% 26.3%Cold Start 0.4972 0.3721 N/A 0.5172 0.5241 0.7594 44.9% 44.9%\n\u2022 Random (RAND): This baseline ranks items randomly for all users.\n\u2022 Most Popular (MP): This baseline ranks items according to their popularity and is non-personalized.\n\u2022 MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on xuij and trained using SGA as in BPR-MF.\n\u2022 BPR-MF: This pairwise method was introduced by Rendle et al. (2009) and is the state-of-the-art of personalized ranking for implicit feedback datasets.\nWe also include a \u2018content-based\u2019 baseline for comparison against another method which makes use of visual data, though which differs in terms of problem setting and data (it does not make use of feedback but rather graphs encoding relationships between items as input):\n\u2022 Image-based Recommendation (IBR): Introduced by McAuley et al. (2015), it learns a visual space and retrieves stylistically similar items to a query image. Prediction is then performed by nearest-neighbor search in the learned visual space.\nThough significantly different from the pairwise methods considered above, for comparison we also compared against a point-wise method, WRMF (Hu, Koren, and Volinsky, 2008).\nMost baselines are from MyMediaLite (Gantner et al., 2011). For fair comparison, we use the same total number of dimensions for all MF based methods. In our model, visual and non-visual dimensions are fixed to a fifty-fifty split for simplicity, though further tuning may yield better performance. All experiments were performed on a standard desktop machine with 4 physical cores and 32GB main memory. Reproducibility. All hyperparameters are tuned to perform the best on the validation set. On Amazon, regularization hyperparamter \u03bb\u0398 = 10 works the best for BPR-MF, MM-MF and VBPR in most cases. While on Tradesy.com, \u03bb\u0398 = 0.1 is set for BPR-MF and VBPR and \u03bb\u0398 = 1 for MM-MF. \u03bbE is always set to 0 for VBPR. For IBR, the rank of the Mahalanobis transform is set to 100, which is reported to perform very well on Amazon data. All of our code and datasets shall be made available at publication time so that our experimental evaluation is completely reproducible."}, {"heading": "Performance", "text": "Results in terms of the average AUC on different datasets are shown in Table 3 (all with 20 total factors). For each dataset, we report the average AUC on the full test set T (denoted by \u2018All Items\u2019), as well as a subset of T which only consists of items that had fewer than five positive feedback instances in the training set (i.e., cold start). These cold start items account for around 60% of the test set for the two Amazon datasets, and 80% for Tradesy.com; this means for such sparse real-world datasets, a model must address the inherent cold start nature of the problem and recommend items accurately in order to achieve acceptable performance. The main findings from Table 3 are summarized as follows:\n1. Building on top of BPR-MF, VBPR on average improves on BPR-MF by over 12% for all items, and more than 28% for cold start. This demonstrates the significant benefits of incorporating CNN features into our ranking task.\n2. As expected, IBR outperforms BPR-MF & MM-MF in cold start settings where pure MF methods have trouble learning meaningful factors. Moreover, IBR loses to MF methods for warm start since it is not trained on historical user feedback.\n3. By combining the strengths of both MF and content-based methods, VBPR outperforms all baselines in most cases.\n4. Our method exhibits particularly large improvements on Tradesy.com, since it is an inherently cold start dataset due to the \u2018one-off\u2019 nature of trades.\n5. Visual features show greater benefits on clothing than cellphone datasets. Presumably this is because visual factors play a smaller (though still significant) role when selecting cellphones as compared to clothing.\n6. Popularity-based methods are particularly ineffective here, as cold items are inherently \u2018unpopular\u2019. Finally, we found that pairwise methods indeed out-\nperform point-wise methods (WRMF in our case) on our datasets, consistent with our analysis in Related Work. We found that on average, VBPR beats WRMF by 14.3% for all items and 20.3% for cold start items. Sensitivity. As shown in Figure 2, MM-MF, BPR-MF, and VBPR perform better as the number of factors increases, which demonstrates the ability of pairwise methods to avoid overfitting. Results for other Amazon categories are similar and suppressed for brevity. Training Efficiency. In Figure 3 we demonstrate the AUC (on the test set) with increasing training iterations. Generally speaking, our proposed model takes longer to converge than MM-MF and BPR-MF, though still requires only around 3.5 hours to train to convergence on our largest dataset (Women\u2019s Clothing).\nVisualizing Visual Space VBPR maps items to a low-dimensional \u2018visual space,\u2019 such that items with similar styles (in terms of how users eval-\nuate them) are mapped to nearby locations. We visualize this space (for Women\u2019s Clothing) in Figure 4. We make the following two observations: (1) although our visual features are extracted from a CNN pre-trained on a different dataset, by using the embedding we are nevertheless able to learn a \u2018visual\u2019 transition (loosely) across different subcategories, which confirms the expressive power of the extracted features; and (2) VBPR not only helps learn the hidden taxonomy, but also more importantly discovers the most relevant underlying visual dimensions and maps items and users into the uncovered space."}, {"heading": "Conclusion & Future Work", "text": "Visual decision factors influence many of the choices people make, from the clothes they wear to their interactions with each other. In this paper, we investigated the usefulness of visual features for personalized ranking tasks on implicit feedback datasets. We proposed a scalable method that incorporates visual features extracted from product images into Matrix Factorization, in order to uncover the \u2018visual dimensions\u2019 that most influence people\u2019s behavior. Our model is trained with Bayesian Personalized Ranking (BPR) using stochastic gradient ascent. Experimental results on multiple large real-world datasets demonstrate that we can significantly outperform state-of-the-art ranking techniques and alleviate cold start issues.\nAs part of future work, we will further extend our model with temporal dynamics to account for the drifting of fashion tastes over time. Additionally, we are also interested in investigating the efficacy of our proposed method in the setting of explicit feedback."}], "references": [{"title": "Topicmf: Simultaneously exploiting ratings and reviews for recommendation", "author": ["Y. Bao", "H. Fang", "J. Zhang"], "venue": "AAAI.", "citeRegEx": "Bao et al\\.,? 2014", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "The bellkor solution to the netflix prize", "author": ["R.M. Bell", "Y. Koren", "C. Volinsky"], "venue": null, "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "KDDCup.", "citeRegEx": "Bennett and Lanning,? 2007", "shortCiteRegEx": "Bennett and Lanning", "year": 2007}, {"title": "Context-aware applications: from the laboratory to the marketplace", "author": ["P. Brown", "J. Bovey", "X. Chen"], "venue": "IEEE Wireless Communications.", "citeRegEx": "Brown et al\\.,? 1997", "shortCiteRegEx": "Brown et al\\.", "year": 1997}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "WWW.", "citeRegEx": "Das et al\\.,? 2007", "shortCiteRegEx": "Das et al\\.", "year": 2007}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Mymedialite: A free recommender system library", "author": ["Z. Gantner", "S. Rendle", "C. Freudenthaler", "L. SchmidtThieme"], "venue": "RecSys.", "citeRegEx": "Gantner et al\\.,? 2011", "shortCiteRegEx": "Gantner et al\\.", "year": 2011}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM. IEEE.", "citeRegEx": "Hu et al\\.,? 2008", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Large scale visual recommendations from street fashion images", "author": ["V. Jagadeesh", "R. Piramuthu", "A. Bhardwaj", "W. Di", "N. Sundaresan"], "venue": "SIGKDD.", "citeRegEx": "Jagadeesh et al\\.,? 2014", "shortCiteRegEx": "Jagadeesh et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "MM.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos", "author": ["Y. Kalantidis", "L. Kennedy", "L.-J. Li"], "venue": "ICMR.", "citeRegEx": "Kalantidis et al\\.,? 2013", "shortCiteRegEx": "Kalantidis et al\\.", "year": 2013}, {"title": "Supercharging recommender systems using taxonomies for learning user purchase behavior", "author": ["B. Kanagal", "A. Ahmed", "S. Pandey", "V. Josifovski", "J. Yuan", "L. Garcia-Pueyo"], "venue": "VLDB Endowment.", "citeRegEx": "Kanagal et al\\.,? 2012", "shortCiteRegEx": "Kanagal et al\\.", "year": 2012}, {"title": "Recognizing image style", "author": ["S. Karayev", "M. Trentacoste", "H. Han", "A. Agarwala", "T. Darrell", "A. Hertzmann", "H. Winnemoeller"], "venue": "BMVC.", "citeRegEx": "Karayev et al\\.,? 2014", "shortCiteRegEx": "Karayev et al\\.", "year": 2014}, {"title": "Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy", "author": ["N. Koenigstein", "G. Dror", "Y. Koren"], "venue": "RecSys.", "citeRegEx": "Koenigstein et al\\.,? 2011", "shortCiteRegEx": "Koenigstein et al\\.", "year": 2011}, {"title": "Advances in collaborative filtering", "author": ["Y. Koren", "R. Bell"], "venue": "Recommender systems handbook. Springer.", "citeRegEx": "Koren and Bell,? 2011", "shortCiteRegEx": "Koren and Bell", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Multi-relational matrix factorization using bayesian personalized ranking for social network data", "author": ["A. Krohn-Grimberghe", "L. Drumond", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "WSDM.", "citeRegEx": "Krohn.Grimberghe et al\\.,? 2012", "shortCiteRegEx": "Krohn.Grimberghe et al\\.", "year": 2012}, {"title": "RAPID: rating pictorial aesthetics using deep learning", "author": ["X. Lu", "Z. Lin", "H. Jin", "J. Yang", "J.Z. Wang"], "venue": "MM.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Content-based collaborative filtering for news topic recommendation", "author": ["Z. Lu", "Z. Dou", "J. Lian", "X. Xie", "Q. Yang"], "venue": "AAAI.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Image-based recommendations on styles and substitutes", "author": ["J.J. McAuley", "C. Targett", "Q. Shi", "A. van den Hengel"], "venue": null, "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Gbpr: Group preference based bayesian personalized ranking for one-class collaborative filtering", "author": ["W. Pan", "L. Chen"], "venue": "IJCAI.", "citeRegEx": "Pan and Chen,? 2013", "shortCiteRegEx": "Pan and Chen", "year": 2013}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "ICDM.", "citeRegEx": "Pan et al\\.,? 2008", "shortCiteRegEx": "Pan et al\\.", "year": 2008}, {"title": "Combining heterogenous social and geographical information for event recommendation", "author": ["Z. Qiao", "P. Zhang", "Y. Cao", "C. Zhou", "L. Guo", "B. Fang"], "venue": "AAAI.", "citeRegEx": "Qiao et al\\.,? 2014", "shortCiteRegEx": "Qiao et al\\.", "year": 2014}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CVPR Workshops.", "citeRegEx": "Razavian et al\\.,? 2014", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. SchmidtThieme"], "venue": "UAI.", "citeRegEx": "Rendle et al\\.,? 2009", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M.S. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "CoRR.", "citeRegEx": "Russakovsky et al\\.,? 2014", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A. Schein", "A. Popescul", "L. Ungar", "D. Pennock"], "venue": "SIGIR.", "citeRegEx": "Schein et al\\.,? 2002", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "Neuroaesthetics in fashion: Modeling the perception of fashionability", "author": ["E. Simo-Serra", "S. Fidler", "F. Moreno-Noguer", "R. Urtasun"], "venue": "CVPR.", "citeRegEx": "Simo.Serra et al\\.,? 2014", "shortCiteRegEx": "Simo.Serra et al\\.", "year": 2014}, {"title": "Semrec: A semantic enhancement framework for tag based recommendation", "author": ["G. Xu", "Y. Gu", "P. Dolog", "Y. Zhang", "M. Kitsuregawa"], "venue": "AAAI.", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Beyond clicks: dwell time for personalization", "author": ["X. Yi", "L. Hong", "E. Zhong", "N.N. Liu", "S. Rajan"], "venue": "RecSys.", "citeRegEx": "Yi et al\\.,? 2014", "shortCiteRegEx": "Yi et al\\.", "year": 2014}, {"title": "Leveraging social connections to improve personalized ranking for collaborative filtering", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "CIKM.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Userrec: A user recommendation framework in social tagging systems", "author": ["T.C. Zhou", "H. Ma", "M.R. Lyu", "I. King"], "venue": "AAAI.", "citeRegEx": "Zhou et al\\.,? 2010", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Social recommendation using low-rank semidefinite program", "author": ["J. Zhu", "H. Ma", "C. Chen", "J. Bu"], "venue": "AAAI.", "citeRegEx": "Zhu et al\\.,? 2011", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al.", "startOffset": 136, "endOffset": 199}, {"referenceID": 4, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al., 2007; Lu et al., 2015), to tags and even other users (Xu et al.", "startOffset": 238, "endOffset": 273}, {"referenceID": 18, "context": "Such systems play a central role in helping people discover items of personal interest from huge corpora, ranging from movies and music (Bennett and Lanning, 2007; Koenigstein, Dror, and Koren, 2011), to research articles, news and books (Das et al., 2007; Lu et al., 2015), to tags and even other users (Xu et al.", "startOffset": 238, "endOffset": 273}, {"referenceID": 28, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 31, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 32, "context": ", 2015), to tags and even other users (Xu et al., 2011; Zhou et al., 2010; Zhu et al., 2011).", "startOffset": 38, "endOffset": 92}, {"referenceID": 29, "context": "(Yi et al., 2014).", "startOffset": 0, "endOffset": 17}, {"referenceID": 21, "context": "been proposed to uncover the most relevant latent dimensions in both explicit and implicit feedback settings (Bell, Koren, and Volinsky, 2007; Hu, Koren, and Volinsky, 2008; Pan et al., 2008; Rendle et al., 2009).", "startOffset": 109, "endOffset": 212}, {"referenceID": 24, "context": "been proposed to uncover the most relevant latent dimensions in both explicit and implicit feedback settings (Bell, Koren, and Volinsky, 2007; Hu, Koren, and Volinsky, 2008; Pan et al., 2008; Rendle et al., 2009).", "startOffset": 109, "endOffset": 212}, {"referenceID": 26, "context": "Although a variety of sources of data have been used to build hybrid models to make cold start or context-aware recommendations (Schein et al., 2002), from text (Bao, Fang, and Zhang, 2014), to a user\u2019s physical location (Qiao et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 22, "context": ", 2002), from text (Bao, Fang, and Zhang, 2014), to a user\u2019s physical location (Qiao et al., 2014), to the season or temperature (Brown, Bovey, and Chen, 1997), here we are interested in incorporating the visual appearance of the items into the preference predictor, a source of data which is typically neglected by existing RSs.", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "Methodologically we model visual aspects of items by using representations of product images derived from a (pre-trained) deep network (Jia et al., 2014), on top of which we fit an additional layer that uncovers both visual and latent dimensions that are relevant to users\u2019 opinions.", "startOffset": 135, "endOffset": 153}, {"referenceID": 16, "context": "More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014).", "startOffset": 102, "endOffset": 184}, {"referenceID": 20, "context": "More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014).", "startOffset": 102, "endOffset": 184}, {"referenceID": 11, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 18, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 22, "context": "Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014).", "startOffset": 169, "endOffset": 255}, {"referenceID": 25, "context": "Recently, high-level visual features from Deep Convolutional Neural Networks (\u2018Deep CNNs\u2019) have seen successes in tasks like object detection (Russakovsky et al., 2014), photographic style annotations (Karayev et al.", "startOffset": 142, "endOffset": 168}, {"referenceID": 12, "context": ", 2014), photographic style annotations (Karayev et al., 2014), and aesthetic quality categorization (Lu et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 17, "context": ", 2014), and aesthetic quality categorization (Lu et al., 2014), among others.", "startOffset": 46, "endOffset": 63}, {"referenceID": 5, "context": "ImageNet) can be generalized to extract CNN features for other datasets, and outperform stateof-the-art approaches on these new datasets for different visual tasks (Donahue et al., 2014; Razavian et al., 2014).", "startOffset": 164, "endOffset": 209}, {"referenceID": 23, "context": "ImageNet) can be generalized to extract CNN features for other datasets, and outperform stateof-the-art approaches on these new datasets for different visual tasks (Donahue et al., 2014; Razavian et al., 2014).", "startOffset": 164, "endOffset": 209}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)).", "startOffset": 34, "endOffset": 83}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al.", "startOffset": 34, "endOffset": 614}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix.", "startOffset": 34, "endOffset": 766}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.", "startOffset": 34, "endOffset": 1207}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements.", "startOffset": 34, "endOffset": 2315}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture.", "startOffset": 34, "endOffset": 2427}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture. Another method was proposed by Kalantidis, Kennedy, and Li (2013), which accepts a query image and uses segmentation to detect clothing classes before retrieving visually similar products from each of the detected classes.", "startOffset": 34, "endOffset": 2614}, {"referenceID": 2, "context": "Bell, Koren, and Volinsky (2007); Bennett and Lanning (2007); Rendle et al. (2009)). When it comes to personalized ranking from implicit feedback, traditional MF approaches are challenged by the ambiguity of interpreting \u2018non-observed\u2019 feedback. In recent years, point-wise and pairwise methods have been successful at adapting MF to address such challenges. Point-wise methods assume non-observed feedback to be inherently negative to some degree. They approximate the task with regression which for each user-item pair predicts its affinity score and then ranks items accordingly. Hu, Koren, and Volinsky (2008) associate different \u2018confidence levels\u2019 to positive and non-observed feedback and then factorize the resulting weighted matrix, while Pan et al. (2008) sample non-observed feedback as negative instances and factorize a similar weighted matrix. In contrast to point-wise methods, pairwise methods are based on a weaker but possibly more realistic assumption that positive feedback must only be \u2018more preferable\u2019 than non-observed feedback. Such methods directly optimize the ranking of the feedback and are to our knowledge state-of-the-art for implicit feedback datasets. Rendle et al. (2009) propose a generalized Bayesian Personalized Ranking (BPR) framework and experimentally show that BPRMF (i.e., with MF as the underlying predictor) outperforms a variety of competitive baselines. More recently BPR-MF has been extended to accommodate both users\u2019 feedback and their social relations (Krohn-Grimberghe et al., 2012; Pan and Chen, 2013; Zhao, McAuley, and King, 2014). Our goal here is complementary as we aim to incorporate visual signals into BPR-MF, which presents a quite different set of challenges compared with other sources of data. Others have developed content-based and hybrid models that make use of a variety of information sources, including text (and context), taxonomies, and user demographics (Bao, Fang, and Zhang, 2014; Kanagal et al., 2012; Lu et al., 2015; Qiao et al., 2014). However, to our knowledge none of these works have incorporated visual signals into models of users\u2019 preferences and uncover visual dimensions as we do here. Exploiting visual signals for the purpose of \u2018in-style\u2019 image retrieval has been previously proposed. For example, Simo-Serra et al. (2014) predict the fashionability of a person in a photograph and suggest subtle improvements. Jagadeesh et al. (2014) use a street fashion dataset with detailed annotations to identify accessories whose style is consistent with a picture. Another method was proposed by Kalantidis, Kennedy, and Li (2013), which accepts a query image and uses segmentation to detect clothing classes before retrieving visually similar products from each of the detected classes. McAuley et al. (2015) use visual features extracted from CNNs and learn a visual similarity metric to identify visually complementary items to a query image.", "startOffset": 34, "endOffset": 2793}, {"referenceID": 14, "context": "Our preference predictor is built on top of Matrix Factorization (MF), which is state-of-the-art for rating prediction as well as modeling implicit feedback, whose basic formulation assumes the following model to predict the preference of a user u toward an item i (Koren and Bell, 2011):", "startOffset": 265, "endOffset": 287}, {"referenceID": 24, "context": "Following the notation in Rendle et al. (2009), \u0398 is the parameter vector and x\u0302uij(\u0398) denotes an arbitrary function of \u0398 that parameterises the relationship between the components of the triple (u, i, j).", "startOffset": 26, "endOffset": 47}, {"referenceID": 19, "context": "com introduced by McAuley et al. (2015). We consider two large categories where visual features have already been demonstrated to be meaningful, namely Women\u2019s and Men\u2019s Clothing.", "startOffset": 18, "endOffset": 40}, {"referenceID": 9, "context": "For each item i in the above datasets, we collect one product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever, and Hinton (2012).", "startOffset": 131, "endOffset": 149}, {"referenceID": 9, "context": "For each item i in the above datasets, we collect one product image and extract visual features fi using the Caffe reference model (Jia et al., 2014), which implements the CNN architecture proposed by Krizhevsky, Sutskever, and Hinton (2012). The architecture has 5 convolutional layers followed by 3 fully-connected layers, and has been pre-trained on 1.", "startOffset": 132, "endOffset": 242}, {"referenceID": 6, "context": "\u2022 MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on xuij and trained using SGA as in BPR-MF.", "startOffset": 34, "endOffset": 56}, {"referenceID": 6, "context": "\u2022 MM-MF: A pairwise MF model from Gantner et al. (2011), which is optimized for a hinge ranking loss on xuij and trained using SGA as in BPR-MF. \u2022 BPR-MF: This pairwise method was introduced by Rendle et al. (2009) and is the state-of-the-art of personalized ranking for implicit feedback datasets.", "startOffset": 34, "endOffset": 215}, {"referenceID": 19, "context": "\u2022 Image-based Recommendation (IBR): Introduced by McAuley et al. (2015), it learns a visual space and retrieves stylistically similar items to a query image.", "startOffset": 50, "endOffset": 72}, {"referenceID": 6, "context": "Most baselines are from MyMediaLite (Gantner et al., 2011).", "startOffset": 36, "endOffset": 58}], "year": 2015, "abstractText": "Modern recommender systems model people and items by discovering or \u2018teasing apart\u2019 the underlying dimensions that encode the properties of items and users\u2019 preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text. However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people\u2019s opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people\u2019s feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people\u2019s opinions.", "creator": "LaTeX with hyperref package"}}}