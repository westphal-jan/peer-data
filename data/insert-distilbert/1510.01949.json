{"id": "1510.01949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", "abstract": "prominences and boundaries indices are the essential constituents of prosodic structure in speech. they provide for means to chunk the entire speech stream into linguistically relevant information units by providing them with relative saliences and demarcating them within coherent utterance structures. prominences and boundaries have both been currently widely used in both basic research research on prosody morphology as well as in text - to - speech synthesis. however, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. then here we present an unsupervised unified account for roughly estimating and representing prosodic prominences and boundaries using a scale - space analysis based on continuous wavelet transform. the methods are evaluated and compared to earlier work using the boston university radio news corpus. the results show that the collectively proposed method is comparable thus with the best published supervised annotation methods.", "histories": [["v1", "Wed, 7 Oct 2015 14:08:13 GMT  (1916kb,D)", "http://arxiv.org/abs/1510.01949v1", "22 pages, 5 figures"]], "COMMENTS": "22 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["antti suni", "daniel aalto", "martti vainio"], "accepted": false, "id": "1510.01949"}, "pdf": {"name": "1510.01949.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", "authors": ["Antti Suni", "Daniel Aalto", "Martti Vainioa"], "emails": [], "sections": [{"heading": null, "text": "Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in textto-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.\nKeywords: phonetics, prosody, speech synthesis, wavelets"}, {"heading": "1. Introduction", "text": "Two of the most primary features of speech prosody have to do with chunking speech into linguistically relevant units above the segment and the relative salience of the given units; that is, boundaries and prominences, respectively. These two aspects are present in every utterance and are central to any representation of speech prosody; moreover, they give rise to a hierarchy. Ideally they would be represented with a uniform methodology that would take into account both the production and the perceptual aspects of the speech signals. Such a system would be beneficial to both basic speech research and speech technology, especially speech synthesis. On the other hand, to be useful for data oriented\n\u2217Corresponding author\nar X\niv :1\n51 0.\n01 94\n9v 1\n[ cs\n.C L\n] 7\nresearch and technology, the system should strive towards being unsupervised as opposed to annotation systems that rely on humans. Ideally the system would still behave in a human-like fashion, while avoiding the subjectiveness and variability caused by the blend of top-down and bottom-up influences involved in the interpretation of linguistic speech signals.\nIn this paper we present a hierarchical, time-frequency scale-space analysis of prosodic signals (e.g., fundamental frequency, energy, duration) based on the continuous wavelet transform (CWT). The presented algorithms can be used to analyse and annotate speech signals in an entirely unsupervised fashion. The work stems from a need to annotate speech corpora automatically for text-tospeech synthesis (TTS) [1] and the subject matter is mainly examined from that point of view. However, the presented representations should be of interest to anyone working on speech prosody.\nWavelets extend the classical Fourier theory by replacing a fixed window with a family of scaled windows resulting in scalograms, resembling the spectrogram commonly used for analysing speech signals. The most interesting aspect of wavelet analysis with respect to speech is that it resembles the perceptual hierarchical structures related to prosody. In scalograms speech sounds, syllables, (phonological) words, and phrases can be localised precisely in both time and frequency (scale). This would be considerably more difficult to achieve with traditional spectrograms. Furthermore, the wavelets give natural means to discretise and operationalise the continuous prosodic signals.\nFigure 1 depicts the hierarchical nature of speech as captured in a timefrequency scale-space by CWT of the signal envelope of a typical English utterance. The upper part contains the formant structure (which is not visible due to the rectification of the signal) as well as the fundamental frequency in terms of separate glottal pulses. Underneath the f0 scale are the separate speech segments followed by (prominent) syllables, as well as prosodic words. The lower part including the syllables and prosodic words depicts the suprasegmental and prosodic structure which has typically not been represented in e.g., the melfrequency cepstral coefficient (MFCC) based features in both ASR and TTS.\nSpoken language is organised hierarchically both structurally and phonetically: words belong to phrases and are built up from syllables which are further divisible into phonemes which stand for the actual speech sounds when the structures are realised as speech. This has many non-obvious effects on the speech signal that need to be modelled. The assumption of hierarchical structure combined with new deep learning algorithms has lead to recent breakthroughs in automatic speech recognition [2]. In synthesis the assumption has played a key role for considerably longer. The prosodic hierarchy has been central in TTS since 1970\u2019s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure. Few systems go above single utterances (which typically represent sentence in written form), but some take the paragraph sized units as a basis of production [5].\nThe hierarchical utterance structure serves as a basis for modelling the prosody, e.g., speech melody, timing, and stress structure of the synthetic speech. Controlling prosody in synthesis has been based on a number of different\ntheoretical approaches stemming from both phonological considerations as well as phonetic ones. The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9]. These models are sequential in nature and the hierarchical structure is only implicated in certain features of the models. The more phonetically oriented hierarchical models are based on the assumption that prosody \u2013 especially intonation \u2013 is truly hierarchical in a super-positional and parallel fashion.\nActual models capturing the superpositional nature of intonation were first proposed in [10] by O\u0308hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands. The accent commands produce faster changes which are superposed on a slowly varying phrase contours. Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16]. Superpositional models attempt to capture both the chunking of speech into phrases as well the highlighting of words within an utterance. Typically smaller scale changes, caused by e.g., the modulation of the airflow (and consequently the f0) by the closing of the vocal tract during certain consonants, are not modelled.\nProminence is a functional phonological phenomenon that signals syntagmatic relations of units within an utterance by highlighting some parts of the speech signal while attenuating others. Thus, for instance, some of syllables\nwithin a word stand out as stressed [17]. At the level of words prominence relations can signal how important the speaker considers each word in relation to others in the same utterance. These often information based relations range from simple phrasal structures (e.g., prime minister, yellow car) to relating utterances to each other in discourse as in the case of contrastive focus (e.g., \u201dWhere did you leave your car? No, we WALKED here.\u201d). Although prominence probably functions in a continuous fashion, it is relatively easily categorised in e.g, four levels where the first level stands for words that are not stressed in any fashion prosodically to moderately stressed and stressed and finally words that are emphasised (as the word WALKED in the example above). These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20]. In sum, prominence functions to structure utterances in a hierarchical fashion that directs the listener\u2019s attention in a way which enables the understanding of the message in an optimal manner. However, prominent units \u2013 be they words or syllables \u2013 do not by themselves demarcate the speech signal but are accompanied by boundaries that chunk the prominent and non-prominent units into larger ones: syllables to (phonological) words, words to phrases, and so forth. Prominence and boundary estimation have been treated as separate problems stemming from different sources in the speech signals.\nAs functional \u2013 rather than a formal \u2013 prosodic phenomena prominences and boundaries lend themselves optimally to statistical modelling. The actual signalling of prosody in terms of speech parameters is extremely complex and context sensitive \u2013 the form follows function in a complex fashion. As onedimensional features, prominence and boundary values provide for a means to reduce the representational complexity of speech annotations in an advantageous way. In a synthesis system it occurs at a juncture that is relevant in terms of both representations and data scarcity. The complex feature set that is known to effect the prosody of speech can be reduced to a few categories or a single continuum from dozens of context sensitive features, such as e.g, part-of-speech and whatever can be computed from the input text. Taken this way, both word prominence and boundaries can be viewed as abstract phonological functions that impact the phonetic realisation of the speech signal predictably and that can show considerable phonetic variation in its manifestation. They are essential constituents of the utterance structure, whereas features like part-of-speech or information content (which are typically used for predicting prosody) are not.\nWord prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25]. In principle English should require a more detailed modelling scheme with explicit knowledge about the intonational forms. The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19]. Typically a prominent word is accompanied with a f0 movement, the stressed syllable is longer in duration, and its intensity is higher. However, estimating prominences automatically is not straight-forward and a multitude of differenct estimation algorithms have\nbeen suggested (see Section 3 for more detail). Statistical speech synthesis requires relatively little data as opposed to unitselection based synthesis. However, labelling even small amounts of speech \u2013 especially by experts \u2013 is prohibitively time consuming. In order to be practicable the labelling of any feature in the synthesis training data should be preferably attainable with automatic and unsupervised means.\nIn what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work). The main insight in this methodology is that both prominences and boundaries can be treated as arising from the same sources in the (prosodic) speech signals and estimated with exactly the same methods. These methods, then, provide for a uniform representation for prosody that is useful in both speech synthesis and basic phonetic research. These representations are purely computational and thus objective. It is \u2013 however \u2013 interesting to see how the proposed methods relate to annotations provided by humans as well as earlier attempts at the problem (Section 3)."}, {"heading": "2. Methods", "text": "Wavelets are used in a great variety of applications for effectively compressing and denoising signals, to represent the hierarchical properties of multidimensional signals like polychromatic visual patterns in image retrieval, and to model optical signal processing of visual neural fields [30, 31]. In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38]. A recent summary of wavelets in speech technology can be found in [39].\nFigure 2 shows a CWT of the f0 contour of a Finnish utterance \u201cAluetta, jossa puhetta tutkivat eri tieteenalat kohtaavat toisensa on perinteisesti kutsuttu fonetiikaksi\u201d, (The area where the sciences interested in speech meet each other has been traditionally called phonetics.). The lower pane shows the (interpolated) contour itself as well as orthographic words (word boundaries are shown as vertical lines in both panes). The upper pane shows the wavelet transform as well as eight separated scales (grey lines) ranging from segmentally influenced perturbation or microprosody (lowest scale) to utterance level phrase structure (the highest level). The potentially prominent peaks in the signal occurring during most content words are clearly visible in the scalogram.\nThe time-scale analysis allows for not only locating the relevant features in the signal but also estimating their relative salience, i.e., their prominence. The relative prominences of the different words are visible as positive local extrema (red in Fig. 2). There are several ways to estimate word prominences from a CWT. Suni et al. [27] and Vainio et al. [28] used amplitude of the word prosody scale which was chosen from a discrete set of scales with ratio 2 between ascending scales as the one with the number of local maxima as close to the number of words in the corpus as possible. A more sophisticated way is\npresented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41]. This method was shown to be on par with human estimated prominence values (on a four degree scale). However, the method still suffers from the fact that not all prominent words are identified and \u2013 more importantly \u2013 some words are estimated as prominent whereas they should be seen as non-prominent parts of either another phonological word or a phrase.\nFigure 3 shows an f0 contour of an English utterance (\u201cSometimes the players play in festivities to enliven the atmosphere.\u201d) analysed with CWT. The analyses provide both an accurate measure for the locations of the prominent features in the signal as well as their magnitudes. All in all, the CWT based analysis can be used for a fully automatic labelling of speech corpora for synthesis. The synthesis, however, cannot produce a full CWT at run time; neither does it make sense to use the full transform for training. That is, the CWT needs to be partitioned into meaningful scales for both training and producing the contours.\nIn earlier work, wavelets have been used in speech synthesis context mainly for parameter estimation [42, 43, 44] but never as a full modelling paradigm. In the HMM based synthesis framework, decomposition of f0 to its explicit hierarchical components during acoustic modelling has been investigated in [45, 46]. These approaches rely on exposing the training data to a level-dependent subset of questions for separating the layers of the prosodic hierarchy. The layers can then be modelled separately as individual streams [45], or jointly with adaptive training methods [46].\nIn the current study we have extended the CWT based analysis by using two-dimensional tagging of prosodic structure; in addition to the LoMA based prominence we use a boundary value of each word in order to 1) better represent the hierarchical structure of the signal, and 2) to disambiguate those prominence\nestimates that are estimated to be similarly prominent by the LoMA estimation alone. This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49]. The boundary value for each word can be estimated by e.g, following the lines of minimum amplitude at word boundaries (blue areas in Figure 3). The combination of word prominence and boundary values \u2013 together with the traditional text based utterance structure \u2013 are enough to represent the sound structure of any utterance. These utterance structures can be further modified by other functional features such as whether the utterance is a question or a statement by simply adding the feature to the top-level of the tree.\nThe above described scheme reduces the complexity of the symbolic representation of speech at a juncture that optimises the learning of the actual phonetic features derived from the speech events \u2013 be they parameter tracks or something else, such as e.g., articulatory gestures.\nIn the remaining part of the section we describe the main steps for analysing and annotating prominences and boundaries in a fully automatic and unsupervised fashion using the CWT and LoMA on composite prosodic signal based on fundamental frequency, intensity, and timing."}, {"heading": "2.1. Wavelet decomposition", "text": "The basis for the modeling of hierarchies in speech signals is provided by continuous wavelet transform (CWT). The continuous theory is explained in detail by Daubechies and the theory is applied to time series as by Torrence and Compo [50, 51]. The CWT is a decomposition of a signal in scales which can be\nsummed up to yield the original signal approximately. To define the transform, let s be a one-dimensional signal with real values and finite energy. Given a scale \u03c3 > 0 and a temporal translation \u03c4 , the continuous wavelet transform can be defined as Ws(\u03c3, \u03c4) = \u03c3\u22121/2s \u2217 \u03c8\u03c4,\u03c3 where \u2217 denotes the convolution and \u03c8\u03c4,\u03c3 is the Mexican hat mother wavelet translated by \u03c4 and dilated by \u03c3. Although the Mexican hat mother wavelet has infinite support, the values decay exponentially fast far away from the origin and the mother wavelet effectively acts on a support of seven units.\nThe sampling rate of a digital signal determines the finest temporal scales available for the analysis. In the statistical speech synthesis context a 5 ms fixed window size is used for acoustical parameters. Every real signal also has finite length and the coarsest scales become obsolete. The onset and offset of the signal can create artifacts propagating to the wavelet image and here these effects are counteracted by continuing the signal periodically.\nThe original signal s can be reconstructed approximately from the original signal using a finite number of wavelet scales with\ns(t) \u2248 c N\u2211 j=0 a\u2212j/2Ws(a0a j , t)\nwhere a0 > 0 is the finest (smallest) scale, a > 1 defines the spacing between chosen scales, N > 1 is the number of scales included, and c is a constant. Throughout this work, a = \u221a 2."}, {"heading": "2.2. Lines of maximum amplitude", "text": "The Mexican hat mother wavelet belongs to a family of Gaussian wavelets. These wavelets seem to give a suitable compromise between temporal and frequency selectivity in the time-frequency representation of the prosodic signals.\nImportantly, the Gaussian wavelets give trees that allow for full reconstruction of the original signal. Visually, the trees look stable and consistent with Mexican hat mother wavelets.\nInstead of a full tree representation of the prosody (as depicted in the Fig. 3.), a reduced tree representation is used here. Towards this purpose, lines of maximum amplitude (LoMA) are defined recursively by connecting local maxima across scales. First, let t1,0, t2,0,. . ., tMW ,0 be the time points where the local maxima occurred in the finest scale (\u03c3 = a0) in descending order, Ws(a0, t1,0) \u2265 . . . \u2264 Ws(a0, tMW ,0). Then the point ti,0, i = 1, . . . ,MW is connected to the nearest local maximum (the mother candidate) to the right at the scale a0a if the derivative along the scale at ti,0 is positive, the distance to the mother candidate is at most 200 ms, and the mother candidate was not connected to a child earlier. If the derivative was negative, the search was done to the left. For consecutive levels, the ordering is based on the cumulative weighted sum of the local maximum together with its descendants: for a local maximum in ti,j , j > 0, at level a0a\nj , with descendants in ti0,0, . . . , tij ,j at levels a0, . . . , a0a j respectively, the cumulative weighted sum is\nWs(a0, ti0,0) + . . .+ log(j + 1)a \u2212j/2Ws(a0a j , tij ,j).\nWithout the logarithmic term in the above sum, the formula resembles a lot the reconstruction of the original signal. Since the local maxima often are close to each other, the logarithmic term plays a crucial role in giving more weight to the higher levels of hierarchy. Observe that the number of local maxima decrease with increasing scales, every local maximum has at most one parent, and every parent has exactly one child. Finally, the points connected as children and parents form lines of maximum amplitude (LoMA) and the strength of such a line is the weighted sum of all the elements included in the line.\nThe lines of minimum amplitude (LomA) of a signal s are defined as the lines of maximum amplitude of \u2212s. The positive and negative lines are then used for estimating prominence and boundary magnitudes, respectively. An example of LoMA analysis is shown in Figure 4."}, {"heading": "2.3. Preprocessing of the signals", "text": "The acoustic signal reflects the physiological control actions behind speech communication. Emphasised words are often louder, higher pitch, and longer as a result of more production effort, higher fundamental frequency, and prolonged duration. For analysing the acoustic patterns, the abrupt changes in f0 or gain, due to e.g. closures in the vocal tract during stops, create strong hierarchical structures in the wavelet image that might not be part of the auditory gestalt [52]. Because of the more continuous underlying articulatory gestures and because of the seemingly more continuous percepts, the acoustic signals are \u201cfilled in\u201d for the portions where signal cannot be found (for f0) or where it is very weak (gain). In addition, a continuous (with respect to the time) representation for duration is derived. Although inspired by the physiology of vocal and auditory apparatuses, the aim of these transformations is not to model these\nsystems but to make the algorithm more comparable to the other phenomenological approaches to describe the key prosodic patterns."}, {"heading": "2.3.1. Intensity", "text": "Intensity variations in the speech signal are primarily caused by (deliberate and random) fluctuations of subglottal pressure and the degree of hyperarticulation (especially in fricatives). As a proxy to the articulatory effort, the gain of the acoustical signal is transformed by iteratively interpolating the silent gaps.\nLet \u03c6 be the Gaussian kernel and g the original gain signal (i.e. a logarithm of the amplitude). A family of scaling functions, {\u03c6i}i is obtained by dilating and scaling \u03c6 with constants \u03bbi = w (i\u2212n)/n max w \u2212i/n min , i = 0, 1, 2, . . . , n, where wmax is the maximum smoothing window size, wmin is the minimum window, and n is the size of the family. The g is recursively smoothed. For i = 0, a pointwise maximum is taken by g0 = max{g, g \u2217\u03c60} where \u2217 denotes the convolution. For i > 0, gi = max{g, gi\u22121 \u2217 \u03c6i}. This results in the preprocessed gain g = gn shown in the top pane of Figure 5."}, {"heading": "2.3.2. Fundamental frequency", "text": "The auditory pitch of the voiced sounds is closely related to the lowest eigen resonances of the vocal folds. However, during unvoiced speech segments, the association between the acoustic signal and the eigen resonances of the vocal folds break apart. Importantly, even during the silent periods there are control actions to the vocal folds that impact the f0 once the vibration its reinitiated either by adducting the vocal folds or by restoring the airflow through vocal tract. In addition to the internal state of the larynx, the frequency of the glottal pulsing is influenced by the subglottal pressure. Not surprisingly then, the f0 and intensity are strongly correlated. To estimate the state of the f0\ncontrol during unvoiced portions, an algorithm is proposed where the surface f0 values are left unchanged for the voiced passages and the underlying state of the vocal folds is estimated by interpolation for unvoiced passages.\nThe gap filling for the unvoiced portions of fundamental frequency signal s is similar to that for the gain. First, the signal is decomposed in voiced and unvoiced portions by defining the set V of time points where the speech signal is voiced.\nIn practice, the voicedness of a time point is defined using the GlottHMM [53] analysis which applies low-frequency energy and zero-crossings thresholds for voicing decision. Then, using the same smoothing family as before, the smoothed s is defined iteratively: for i = 0, s0 = s\u03c7V +max{s, s\u2217\u03c60}\u03c7V C where \u03c7A is the characteristic function of a set A and A\nC denotes the complement of the set A. The analogous recursive formula then is\nsi = s\u03c7V + max{s, si\u22121 \u2217 \u03c6i}\u03c7V C\nresulting in the preprocessed fundamental frequency. Finally, to remove perturbation around gaps, the iterated signal sn is smoothed using the same iterated maximisation algorithm as for the gain.\nTo find suitable parameters in the above algorithms, two test utterances were used. These values were used: wmax = 100 ms, wmin = 1, for both gain and f0; n = 100 for for gain, n = 200 for f0; and for final smoothing of f0 wmax = 25 ms and n = 50.\nObserve that the repeated convolutions and maximums do not let the signals grow in an unlimited way. Instead, every point converges and the resulting (maximal) function has comparable energy to the original which can be seen by iterating a result of Hardy and Littlewood [54], (for modern approach, see Theorem 2.19 in [55])."}, {"heading": "2.3.3. Duration", "text": "The duration of a phonological unit varies as a function of its position within an utterance. For instance the speech rate often changes across boundaries and accented words are longer. Due to a lack of signal based speech rate estimators, the duration signal has to be based on analytical linguistic units rather than the raw signal. To quantify the duration, a relation between acoustical (continuous) duration and a suitable discrete linguistic unit is needed. A natural candidate could be a syllable but here an orthographic word is chosen instead as the syllable boundaries might not be easy to derive from text without supervision. To apply the wavelet analysis to the duration, it is expanded to a continuous time dependent variable which ideally would reflect the local duration of the linguistic units. For the current experiment provided word alignments were used. The word boundaries, x0, x1, . . . , xNw , where Nw is the number of orthographic words within a given speech signal, and the associated durations di = xi\u2212xi\u22121, i = 1, . . . , Nw, are computed. These points {(xi\u22121 + di/2, di)} are connected using cubic splines to yield a duration signal d defined for every time instant from x0 to xNw with the same sampling rate as for fundamental frequency and\ngain. When annotated pauses and breaths between words occurred, these were not taken into consideration, i.e. these gaps were not interpolated."}, {"heading": "2.4. Annotation", "text": "The annotation of accentsand breaks (prominences and boundaries) is based on wavelet decompositionof the fundamental frequency, gain, and duration signals.These three acoustic signalswere normalized to have unit varianceand then summed to yield aprosodic signal s.The finest scale to be analysed was defined asbeing one octave below the rate of occurence ofortographic words. To normalize the speech rate, the finest scalewas selected for each utterance separatelythrough finding the word scale aWwhich is the ratio of word count and utterance duration.\nThe finest scale was one octave below word scale, i.e. a0 = 1/2aW , and the coarsest scale was three octaves higher, i.e. 8a0. If no LoMA took place during the word, the accent strength was set to zero.\nThe prosodic breaks manifest mostly on larger scales, sothe word scale was taken as the finest scale a0 = aW .The coarsest scale was again three octaves higher.Instead of local maxima and theassociated LoMA, the local minima were used as a basisfor the break annotation. To approximate the local speech rate,the time derivative of the continuous duration wasused instead of the continuous duration.Then, the combined signal of scaled f0, gain, and duration derivativeweresubject to LomA analysis."}, {"heading": "3. Experimental Results", "text": "As stated in the introduction, a solid method for prosody annotation would be very welcome in speech synthesis field, where recent development has concentrated on acoustic modelling side [56]. The motivation is especially strong in building speech synthesizers for low-resourced languages, where neither linguistically nor prosodically annotated corpora are available [1]. In this chapter, we asses the utility of the proposed CWT-LoMA representation of prosody on the tasks of unsupervised annotation of prosodic prominences and boundaries. Although this hierarchical method does lend itself naturally to multi-level prosody annotation [28], here, we restrict ourselves to binary detection task, in order to produce comparable results with previous studies. Furthermore, in TTS binary prosodic labels can be a useful simplification, as it better facilites text based prediction.\nPrevious work on unsupervised prosody annotation has focused on accent or prominence. For example, Ananthakrishnan & Narayanan [57] performed twoclass unsupervised clustering on syllable level acoustic features combined with lexical and syntactic features, achieving accent detection accuracy of 78% using Boston University Radio News Corpus (BURNC). In a similar vein, Mehrabani et al. [58] annotated a corpus with four level prominence scale by K-means clustering on foot-level acoustic features, achieving improved synthesis quality compared to a rule-based prominence model. Using more analytic approach,\nTambourini [59] derived a continuous prominence function, using expert knowledge to weight various acoustic correlates of prominence, achieving 80% accuracy on syllable prominence detection on TIMIT corpus. Word prominence was annotated by Vainio & Suni [60] with similar method, using prosodic features generated by parametric synthesis build without prominence labels as a powerful normalizing method. An ambitious approach was presented by Kalinli & Narayanan [61], extracting multi-scale auditory features insipired on the processing stages in the human auditory system, combined to an auditory salience map. They achieved prominent word detection accuracy of 78% with F-score of 0.82 on BURNC, which , to our knowledge, is the best reported unsupervised result on this corpus to date.\nWhereas text-based break prediction literature is abundant due to its importance in TTS, unsupervised acoustic boundary annotation has received less interest. This probably stems from the fact that both acoustic pauses, which can be obtained reliably by HMM forced alignment, and punctuation yield high baseline accuracy on major boundaries, and for TTS purposes, this has been considered satisfactory. For example in BURNC, intonational phrase boundaries can be predicted by silence alone with 88% accuracy, though with only 45% recall, and traditional acoustic features offer little improvent over this trivial baseline [62]. In terms of combining text and acoustic evidence, Ananthakrishnan & Narayanan [57] obtained 81% accuracy in combined intermediate and intonational boundary detection with two class k-means model."}, {"heading": "3.1. Corpus", "text": "We perform the evaluation of our prominence and boundary detection method on Boston Radio News corpus [63], chosen for high quality prosodic labeling and comparability with several previous methods also evaluated on BURNC. The corpus consists of about two and a half hours of news stories read by 6 speakers with manual Tone and Break Index annotations. The ToBi labelling scheme was originally developed for transcribing speech melody [64], thus high (H), low (L) and complex accent types are employed (H*, L*, L*+H, L+H*, H+ !H*), concerned with syllable level shape and peak alignment. Prosodic boundaries are annotated with boundary tones (L-, H-,L \u2013 L%, L \u2013 H%, H \u2013 H%, H \u2013 L%), again signalling the shape of melody. Break strength is annotated in the form of break indices ranging from zero (clitized) to four (intonational phrase boundary). For the boundary detection task, we consider a word boundary as a prosodic boundary if the last syllable of a preceding word is marked with break index three (intermediate phrase break) or four (intonational phrase break). Prominence, on the other hand, has not been directly annotated and for the current experiment, we make a simplifying assumption that word is prominent if any of it\u2019s syllables carries an accent. These binary boundary and prominence categories are consistent with previous prosodic event detection studies [65, 66]. Almost all of the annotated data were used for the experiment, totalling 442 stories or 29774 words. Three stories from speaker f2b, used for setting values of free parameters were excluded as well as few cases were syllable and word alignments did not match. Word level break and prominence labels were derived\nby combining the provided, time aligned syllable and word labels. Manually corrected alignments were used when available."}, {"heading": "3.2. Features and Processing", "text": "The proposed method was evaluated using standard prosodic features; f0, energy and word duration, as well as all combinations of those. Raw f0 and energy parameters were analyzed from 16 kHz speech signals with GlottHMM analysis-synthesis framework [53] with five millisecond frame shift. The method uses Iterative-adaptive inverse filtering to separate the contributions of vocal tract and voice source, and performs f0 analysis on the source signal with autocorrelation method. Log energy is calculated from the whole signal. Pitch range was set separately for male and female speakers, 70\u2013300 Hz and 120\u2013400 Hz, respectively. Obtained f0 and energy parameters were interpolated using peak preserving method and word durations were transformed to continuous signals as described in section 2.3. Labeled pauses and breaths were not considered in the duration transfom on the y-axis. When evaluating the performance of combinations of prosodic features, the individual parameters were normalized utterance-wise to zero mean, unity variance, and summed prior to the wavelet analysis, with no weight adjustments, after which the composite prosodic signal was again normalized.\nThe signal was then used as such in wavelet analysis, without any feature extraction step. Continuous wavelet transform was performed using the second derivative of gaussian (Mexican hat) wavelet, with a half octave scale separation. Scale corresponding to word level was estimated individually for each paragraph in order to normalise speech rate differences. Lines of maximum and minimum amplitude were then estimated from the scalogram. Strongest peak LoMA of each word was assigned as the prominence value of the word and strongest valley LomA between each two word\u2019s strongest peak LoMA as a boundary value. If word contained no peak LoMA, valley LomA was searched between the midpoints of adjacent words. Further, if either peak LoMA or valley LomA was not found, prominence or boundary value was set to zero respectively. To verify the utility of hierarchical modelling and rule out the possibility that improvements were achieved only due to feature engineering, we also calculated word maximum (to represent prominence) and minimum between midpoints of adjacent words (to represent boundary) from raw prosodic signal to be used as a baseline. In order to compare the predicted continuous prominence and boundary values against manual labels, the values were converted to binary form by searching for an optimal value for separating the two classes in terms of classification accuracy, using 10% of the manual labels. Although continuous values could be used as such in other applications, it might be argued that this step weakens our claim for unsupervision for the current task. Thus, for the best configurations, we also report results based on dividing the prominence and boundary distributions to two classes by unsupervised k-means clustering."}, {"heading": "3.3. Results", "text": "We report results on CWT-LoMA analysis of f0 (f0) energy (en) and duration (dur) and their combinations on prominence and boundary detection. The performance of gap-filling on energy is evaluated separately, and whenever the gap-filling improves the performance for prominence or boundary annotation, it is used for energy in combined features as well. Boundaries were defined as manual break indices of either 3 or 4; prominence if any syllable of a word carries an accent. Results are presented in word level, in terms of percentage of correct detections, i.e. accuracy, as well as precision, recall and F-score. As baselines, we report the majority class, predictions derieved from best combination signal without wavelet analysis, as well as current state-of-the-art unsupervised and supervised acoustic results. Note that these results are only roughly comparable, as there are minor differences in data selection. Results are presented in Table 1. Strictly unsupervised results using two class k-means clustering on the prediction distributions using all acoustic features were 84.0% accuracy and 0.86 F-score for prominence and 85.5%, 0.73 for boundary detection respectively.\nExamining the results of individual acoustic features, we note similar perfomance for f0 and energy in both tasks, and word duration not far behind. f0 appears more important for prominence detection, which is expected as reference labeling concerned pitch accents. Filling the unvoiced gaps of energy signal helps in boundary detection, but not in the accent detection task, perhaps due to syllable level features of the signal being smoothed too much. Interestingly, combining f0 and energy yields only modest improvement, whereas combining either with duration provides substantial gain; accuracy increases approximately 3% in accent detection and 4% in boundary detection. Though a na\u0308\u0131ve feature,\nword duration may capture both lengthening effects as well as lexical information, separating most of the function and content words, and disambiguating the alignment of LoMA. Combining all features provides best results, but not by significant margin. Comparison of the detection estimates from raw combined signal to ones provided by CWT-LoMA confirm the importance of hierarchical modelling with solid advantage in both tasks.\nCompared to previous methods, our results improve opon unsupervised state-of-the-art by a significant margin, and at least match the accuracy of acoustic-based supervised methods. The results are not far from performance of supervised methods using acoustic, lexical, and syntactic evidence, where reported accuracies for both word level prominence and boundary detection range from 84% to 87% [65, 66]."}, {"heading": "4. Discussion and Conclusions", "text": "In contrast to most published work on speech prosody, the results here show that prosodic structure can \u2013 and probably should \u2013 be studied and represented in a unified framework comprising all relevant signal variables at the same time. For statistical speech synthesis we are now in a position where we can perform full annotation and modelling of prosody in a unified framework. Although, we have presented the methods in the service of speech synthesis, the results are interesting by themselves. That is, they show that prominences and boundaries can be viewed as manifestations of the same underlying speech production process. This has, of course, many theoretical implications. As foremost is the fact that the suprasegmental variables used (f0, energy envelope, duration) seem to work seamlessly to the same end, which is to signal the hierarchical and parallel structure of the linguistic signals. The role of signal energy as a reliable determinant of prosodic structure is interesting, but not altogether surprising [68]. On the one hand, it diminishes the role of f0, while on the other hand, it also provides it with more freedom for other (post-lexical) prosodic functions that are not strictly related to the hierarchical structure.\nAs mentioned above, the methods and representations brought forward in this study have been designed to be feasible in a broader scientific spectrum keeping in mind their psychological plausibility. Although the wavelet representation of prosody has a strong correspondence with the manual annotations of the evaluation corpus (highlighting their relationship with perception), the neural computations performed by the auditory system might differ considerably in contributing to the percepts underlying the accent and break judgments of the labellers. In particular, the scheme for iteratively filling the gaps in the acoustic signals is not a plausible algorithm for neural processing. However, the assumed temporal integration model to explain silent gap detection gives similar \u201cfilling\u201d behaviour as the current processing of gain signal. Importantly, the parameters and particular formulas were only inspired by the known auditory processes but chosen based on performance on a few test sentences. In the proposed accent and boundary annotation the wavelet analysis is performed to a few one-dimensional signals. However, a neurally more plausible approach\nwould be a truly multidimensional representation of speech signal similar to the multi-scale visual analyses [31]. Crucially, the wavelet trees relate the accents and boundaries together phonetically hinting at a unified mechanism, in both production and perception, between the phonetic realisation of these primary concepts of prosodic phonology."}, {"heading": "Acknowledgements", "text": "The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007\u20132013) under grant agreement no 287678 (Simple4All) and the Academy of Finland (project no 1265610 (the MIND programme)). We would also like to thank Juraj S\u030cimko for his insight regarding this manuscript. Special thanks go to Paavo Alku and Tuomo Raitio for the GlottHMM collaboration."}], "references": [{"title": "Machine learning paradigms for speech recognition: An overview", "author": ["L. Deng", "X. Li"], "venue": "IEEE Transactions on Audio, Speech & Language Processing 21 (5) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "From text to speech with srs", "author": ["S.R. Hertz"], "venue": "The Journal of the Acoustical Society of America 72 (4) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "The delta rule development system for speech synthesis from text", "author": ["S.R. Hertz", "J. Kadin", "K.J. Karplus"], "venue": "Proceedings of the IEEE 73 (11) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1985}, {"title": "Developing a Finnish concept-to-speech system", "author": ["M. Vainio", "A. Suni", "P. Sirjola"], "venue": "in: M. Langemets, P. Penjam (Eds.), Proceedings of the Second Baltic Conference on HUMAN LANGUAGE TECHNOLOGIES, Tallinn", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Autosegmental and metrical phonology", "author": ["J.A. Goldsmith"], "venue": "Vol. 11, Blackwell Oxford", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Three dimensional phonology", "author": ["M. Halle", "J.-R. Vergnaud"], "venue": "Journal of linguistic research 1 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1980}, {"title": "Review of text-to-speech conversion for english", "author": ["D.H. Klatt"], "venue": "The Journal of the Acoustical Society of America 82 (3) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Word and sentence intonation: A quantitative model", "author": ["S. \u00d6hman"], "venue": "Speech Transmission Laboratory, Department of Speech Communication, Royal Institute of Technology", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "A generative model for the prosody of connected speech in japanese", "author": ["H. Fujisaki", "H. Sudo"], "venue": "Annual Report of Engineering Research Institute 30 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1971}, {"title": "Analysis of voice fundamental frequency contours for declarative sentences of Japanese", "author": ["H. Fujisaki", "K. Hirose"], "venue": "Journal of the Acoustical Society of Japan (E) 5 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1984}, {"title": "Sfc: a trainable prosodic model", "author": ["G. Bailly", "B. Holm"], "venue": "Speech Communication 46 (3) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A", "author": ["G.K. Anumanchipalli", "L.C. Oliveira"], "venue": "W. Black, A statistical phrase/accent model for intonation modeling., in: INTERSPEECH", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "C", "author": ["G. Kochanski"], "venue": "Shih, Stem-ml: language-independent prosody description., in: INTERSPEECH", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Prosody modeling with soft templates", "author": ["G. Kochanski", "C. Shih"], "venue": "Speech Communication 39 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Syllable prominence: A matter of vocal effort", "author": ["A. Eriksson", "G.C. Thunberg", "H. Traunm\u00fcller"], "venue": "phonetic distinctness and top-down processing, in: Proc. European Conf. on Speech Communication and Technology Aalborg, September 2001, Vol. 1", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Signal-based and expectation-based factors in the perception of prosodic prominence", "author": ["J. Cole", "Y. Mo", "M. Hasegawa-Johnson"], "venue": "Laboratory Phonology 1 (2) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "New method for delexicalization and its application to prosodic tagging for textto-speech synthesis", "author": ["M. Vainio", "A. Suni", "T. Raitio", "J. Nurminen", "J. J\u00e4rvikivi", "P. Alku"], "venue": "in: Interspeech, Brighton, UK", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Obtaining prominence judgments from n\u00e4\u0131ve listeners\u2013influence of rating scales, linguistic levels and normalisation", "author": ["D. Arnold", "P. Wagner", "B. M\u00f6bius"], "venue": "Proceedings of Interspeech", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Rule-based prosody prediction for German text-to-speech synthesis", "author": ["S. Becker", "M. Schr\u00f6der", "W.J. Barry"], "venue": "in: Proceedings of Speech Prosody, Vol. 2006", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "The GlottHMM speech synthesis entry for Blizzard Challenge 2010", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2010 Workshop, Kyoto, Japan", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "The GlottHMM entry for Blizzard Challenge 2011: Utilizing source unit selection in HMM-based speech synthesis for improved excitation generation", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2011 Workshop, Florence, Italy", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "The GlottHMM entry for Blizzard Challenge 2012 \u2013 Hybrid approach", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2012 Workshop, Portland, Oregon", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["L. Badino", "R.A. Clark"], "venue": "Wester, Towards hierarchical prosodic prominence generation in tts synthesis., in: INTERSPEECH", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Tonal features", "author": ["M. Vainio", "J. J\u00e4rvikivi"], "venue": "intensity, and word order in the perception of prominence, Journal of Phonetics 34 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Wavelets for intonation modeling in HMM speech synthesis", "author": ["A. Suni", "D. Aalto", "T. Raitio", "P. Alku", "M. Vainio"], "venue": "in: 8th ISCA Speech Synthesis Workshop (SSW8), Barcelona, Spain", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Continuous wavelet transform for analysis of speech prosody", "author": ["M. Vainio", "A. Suni", "D. Aalto"], "venue": "TRASP 2013-Tools and Resources for the Analysys of Speech Prosody, An Interspeech 2013 satellite event, August 30", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Emphasis", "author": ["M. Vainio", "A. Suni", "D. Aalto"], "venue": "word prominence, and continuous wavelet transform in the control of hmm based synthesis, in: Hirose, Tao (Eds.), Speech Prosody in Speech Synthesis - Modeling, realizing, converting prosody for high quality and flexible speech synthesis, Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "The image processing handbook", "author": ["J.C. Russ", "R.P. Woods"], "venue": "Journal of Computer Assisted Tomography 19 (6) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "A geometric model for the functional circuits of the visual front-end, in: Brain-Inspired", "author": ["B.M. ter Haar Romeny"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Basilar membrane motion", "author": ["G. Zweig"], "venue": "in: Cold Spring Harbor Symposia on Quantitative Biology, Vol. 40, Cold Spring Harbor Laboratory Press", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1976}, {"title": "Auditory representations of acoustic signals", "author": ["X. Yang", "K. Wang", "S.A. Shamma"], "venue": "Information Theory, IEEE Transactions on 38 (2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "Modern methods of speech processing", "author": ["R. Ramachandran", "R. Mammone"], "venue": "Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Speech Time-Frequency Representation", "author": ["M.D. Riley"], "venue": "Vol. 63, Springer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1989}, {"title": "Signal processing in the cochlea: The structure equations", "author": ["H. Reimann"], "venue": "J. Math. Neuroscience 1 (5) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Cortical oscillations and speech processing: emerging computational principles and operations", "author": ["A.-L. Giraud", "D. Poeppel"], "venue": "Nature neuroscience 15 (4) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of Wavelets in Speech Processing", "author": ["M.H. Farouk"], "venue": "Springer", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Access Online via Elsevier", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Decomposition of functions into wavelets of constant shape", "author": ["A. Grossman", "J. Morlet"], "venue": "and related transforms, Mathematics and Physics: Lectures on Recent Results 11 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1985}, {"title": "M", "author": ["H. Kruschke"], "venue": "Lenz, Estimation of the parameters of the quantitative intonation model with continuous wavelet analysis., in: INTERSPEECH", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimating phrase curves in the general superpositional intonation", "author": ["J.P. v. Santen", "T. Mishra", "E. Klabbers"], "venue": "Fifth ISCA Workshop on Speech Synthesis,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "F", "author": ["M. Lei", "Y.-J. Wu"], "venue": "K. Soong, Z.-H. Ling, L.-R. Dai, A hierarchical f0 modeling method for hmm-based speech synthesis., in: INTERSPEECH", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "N", "author": ["H. Zen"], "venue": "Braunschweiler, Context-dependent additive log f 0 model for hmm-based speech synthesis., in: INTERSPEECH", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Pitch accent in context: Predicting intonational prominence from text", "author": ["J. Hirschberg"], "venue": "Artificial Intelligence 63 (1-2) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1993}, {"title": "An introduction to text-to-speech synthesis", "author": ["T. Dutoit"], "venue": "Vol. 3, Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1997}, {"title": "Text-to-speech synthesis", "author": ["P. Taylor"], "venue": "Cambridge University Press", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to wavelet analysis", "author": ["C. Torrence", "G.P. Compo"], "venue": "Bulletin of the American Meteorological society 79 (1) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Voiceless intervals and perceptual completion in f0 contours: Evidence from scaling perception in american english", "author": ["J. Barnes", "A. Brugos", "N. Veilleux", "S. Shattuck-Hufnagel"], "venue": "Proc. 16th ICPhS, Hong Kong, China ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Hmm-based speech synthesis utilizing glottal inverse filtering", "author": ["T. Raitio", "A. Suni", "J. Yamagishi", "H. Pulakka", "J. Nurminen", "M. Vainio", "P. Alku"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on 19 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "A maximal theorem with function-theoretic applications", "author": ["G.H. Hardy", "J.E. Littlewood"], "venue": "Acta Mathematica 54 (1) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1930}, {"title": "Geometry of sets and measures in Euclidean spaces: fractals and rectifiability", "author": ["P. Mattila"], "venue": "no. 44, Cambridge University Press", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1999}, {"title": "K", "author": ["H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black"], "venue": "Tokuda, The hmm-based speech synthesis system (hts) version 2.0, in: Proc. of Sixth ISCA Workshop on Speech Synthesis", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Combining acoustic", "author": ["S. Ananthakrishnan", "S.S. Narayanan"], "venue": "lexical, and syntactic evidence for automatic unsupervised prosody labeling, in: Proceedings of InterSpeech, Pittsburgh, PA", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised prominence prediction for speech synthesis", "author": ["M. Mehrabani", "T. Mishra", "A. Conkie"], "venue": "in: INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association, Lyon, France, August 25-29, 2013", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "An automatic system for detecting prosodic prominence in american english continuous speech", "author": ["F. Tamburini", "C. Caini"], "venue": "International Journal of Speech Technology 8 (1) ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "Accent and prominence in Finnish speech synthesis", "author": ["M. Vainio", "A. Suni", "P. Sirjola"], "venue": "in: G. Kokkinakis, N. Fakotakis, E. Dermatos, R. Potapova (Eds.), Proceedings of the 10th International Conference on Speech and Computer ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech", "author": ["O. Kalinli", "S.S. Narayanan"], "venue": "in: INTERSPEECH 2007, 8th Annual Conference of the International Speech Communication Association, Antwerp, Belgium, August 27-31, 2007", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic detection and classification of prosodic events", "author": ["A. Rosenberg"], "venue": "Ph.D. thesis, Columbia University ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "The boston university radio news corpus", "author": ["P.S.-H.S. Ostendorf", "M. Price"], "venue": "Tech. rep. ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2005}, {"title": "J", "author": ["K.E.A. Silverman", "M.E. Beckman", "J.F. Pitrelli", "M. Ostendorf", "C.W. Wightman", "P. Price", "J.B. Pierrehumbert"], "venue": "Hirschberg, Tobi: a standard for labeling english prosody., in: ICSLP, ISCA", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1992}, {"title": "Automatic prosodic event detection using acoustic", "author": ["S. Ananthakrishnan", "S. Narayanan"], "venue": "lexical, and syntactic evidence, Audio, Speech, and Language Processing, IEEE Transactions on 16 (1) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Transactions on Audio", "author": ["O. Kalinli", "S.S. Narayanan"], "venue": "Speech and Language Processing 17 (5) ", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting pitch accents at the word", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "syllable and vowel level, in: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short \u201909, Association for Computational Linguistics", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2009}, {"title": "Loudness predicts prominence: Fundamental frequency lends little", "author": ["G. Kochanski", "E. Grabe", "J. Coleman", "B. Rosner"], "venue": "The Journal of the Acoustical Society of America 118 (2) ", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The assumption of hierarchical structure combined with new deep learning algorithms has lead to recent breakthroughs in automatic speech recognition [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "The prosodic hierarchy has been central in TTS since 1970\u2019s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure.", "startOffset": 60, "endOffset": 66}, {"referenceID": 2, "context": "The prosodic hierarchy has been central in TTS since 1970\u2019s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure.", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "Few systems go above single utterances (which typically represent sentence in written form), but some take the paragraph sized units as a basis of production [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 149, "endOffset": 157}, {"referenceID": 9, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 149, "endOffset": 157}, {"referenceID": 10, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 11, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 13, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 14, "context": "within a word stand out as stressed [17].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 16, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 17, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 18, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 19, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 20, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 21, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 22, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 23, "context": "The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19].", "startOffset": 193, "endOffset": 201}, {"referenceID": 16, "context": "The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19].", "startOffset": 193, "endOffset": 201}, {"referenceID": 24, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 25, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 26, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 27, "context": "Wavelets are used in a great variety of applications for effectively compressing and denoising signals, to represent the hierarchical properties of multidimensional signals like polychromatic visual patterns in image retrieval, and to model optical signal processing of visual neural fields [30, 31].", "startOffset": 291, "endOffset": 299}, {"referenceID": 28, "context": "Wavelets are used in a great variety of applications for effectively compressing and denoising signals, to represent the hierarchical properties of multidimensional signals like polychromatic visual patterns in image retrieval, and to model optical signal processing of visual neural fields [30, 31].", "startOffset": 291, "endOffset": 299}, {"referenceID": 29, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 30, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 31, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 32, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 33, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 34, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 35, "context": "A recent summary of wavelets in speech technology can be found in [39].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "[27] and Vainio et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] used amplitude of the word prosody scale which was chosen from a discrete set of scales with ratio 2 between ascending scales as the one with the number of local maxima as close to the number of words in the corpus as possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 32, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 37, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 38, "context": "In earlier work, wavelets have been used in speech synthesis context mainly for parameter estimation [42, 43, 44] but never as a full modelling paradigm.", "startOffset": 101, "endOffset": 113}, {"referenceID": 39, "context": "In earlier work, wavelets have been used in speech synthesis context mainly for parameter estimation [42, 43, 44] but never as a full modelling paradigm.", "startOffset": 101, "endOffset": 113}, {"referenceID": 40, "context": "In the HMM based synthesis framework, decomposition of f0 to its explicit hierarchical components during acoustic modelling has been investigated in [45, 46].", "startOffset": 149, "endOffset": 157}, {"referenceID": 41, "context": "In the HMM based synthesis framework, decomposition of f0 to its explicit hierarchical components during acoustic modelling has been investigated in [45, 46].", "startOffset": 149, "endOffset": 157}, {"referenceID": 40, "context": "The layers can then be modelled separately as individual streams [45], or jointly with adaptive training methods [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "The layers can then be modelled separately as individual streams [45], or jointly with adaptive training methods [46].", "startOffset": 113, "endOffset": 117}, {"referenceID": 42, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 43, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 44, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 45, "context": "The continuous theory is explained in detail by Daubechies and the theory is applied to time series as by Torrence and Compo [50, 51].", "startOffset": 125, "endOffset": 133}, {"referenceID": 46, "context": "closures in the vocal tract during stops, create strong hierarchical structures in the wavelet image that might not be part of the auditory gestalt [52].", "startOffset": 148, "endOffset": 152}, {"referenceID": 47, "context": "In practice, the voicedness of a time point is defined using the GlottHMM [53] analysis which applies low-frequency energy and zero-crossings thresholds for voicing decision.", "startOffset": 74, "endOffset": 78}, {"referenceID": 48, "context": "Instead, every point converges and the resulting (maximal) function has comparable energy to the original which can be seen by iterating a result of Hardy and Littlewood [54], (for modern approach, see Theorem 2.", "startOffset": 170, "endOffset": 174}, {"referenceID": 49, "context": "19 in [55]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 50, "context": "As stated in the introduction, a solid method for prosody annotation would be very welcome in speech synthesis field, where recent development has concentrated on acoustic modelling side [56].", "startOffset": 187, "endOffset": 191}, {"referenceID": 25, "context": "Although this hierarchical method does lend itself naturally to multi-level prosody annotation [28], here, we restrict ourselves to binary detection task, in order to produce comparable results with previous studies.", "startOffset": 95, "endOffset": 99}, {"referenceID": 51, "context": "For example, Ananthakrishnan & Narayanan [57] performed twoclass unsupervised clustering on syllable level acoustic features combined with lexical and syntactic features, achieving accent detection accuracy of 78% using Boston University Radio News Corpus (BURNC).", "startOffset": 41, "endOffset": 45}, {"referenceID": 52, "context": "[58] annotated a corpus with four level prominence scale by K-means clustering on foot-level acoustic features, achieving improved synthesis quality compared to a rule-based prominence model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Tambourini [59] derived a continuous prominence function, using expert knowledge to weight various acoustic correlates of prominence, achieving 80% accuracy on syllable prominence detection on TIMIT corpus.", "startOffset": 11, "endOffset": 15}, {"referenceID": 54, "context": "Word prominence was annotated by Vainio & Suni [60] with similar method, using prosodic features generated by parametric synthesis build without prominence labels as a powerful normalizing method.", "startOffset": 47, "endOffset": 51}, {"referenceID": 55, "context": "An ambitious approach was presented by Kalinli & Narayanan [61], extracting multi-scale auditory features insipired on the processing stages in the human auditory system, combined to an auditory salience map.", "startOffset": 59, "endOffset": 63}, {"referenceID": 56, "context": "For example in BURNC, intonational phrase boundaries can be predicted by silence alone with 88% accuracy, though with only 45% recall, and traditional acoustic features offer little improvent over this trivial baseline [62].", "startOffset": 219, "endOffset": 223}, {"referenceID": 51, "context": "In terms of combining text and acoustic evidence, Ananthakrishnan & Narayanan [57] obtained 81% accuracy in combined intermediate and intonational boundary detection with two class k-means model.", "startOffset": 78, "endOffset": 82}, {"referenceID": 57, "context": "We perform the evaluation of our prominence and boundary detection method on Boston Radio News corpus [63], chosen for high quality prosodic labeling and comparability with several previous methods also evaluated on BURNC.", "startOffset": 102, "endOffset": 106}, {"referenceID": 58, "context": "The ToBi labelling scheme was originally developed for transcribing speech melody [64], thus high (H), low (L) and complex accent types are employed (H*, L*, L*+H, L+H*, H+ !H*), concerned with syllable level shape and peak alignment.", "startOffset": 82, "endOffset": 86}, {"referenceID": 59, "context": "These binary boundary and prominence categories are consistent with previous prosodic event detection studies [65, 66].", "startOffset": 110, "endOffset": 118}, {"referenceID": 60, "context": "These binary boundary and prominence categories are consistent with previous prosodic event detection studies [65, 66].", "startOffset": 110, "endOffset": 118}, {"referenceID": 47, "context": "Raw f0 and energy parameters were analyzed from 16 kHz speech signals with GlottHMM analysis-synthesis framework [53] with five millisecond frame shift.", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "1Kalinli & Narayanan [61],2Rosenberg & Hirchberg [67],3Ananthakrishnan & al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 61, "context": "1Kalinli & Narayanan [61],2Rosenberg & Hirchberg [67],3Ananthakrishnan & al.", "startOffset": 49, "endOffset": 53}, {"referenceID": 51, "context": "[57],4Ananthakrishnan & al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "The results are not far from performance of supervised methods using acoustic, lexical, and syntactic evidence, where reported accuracies for both word level prominence and boundary detection range from 84% to 87% [65, 66].", "startOffset": 214, "endOffset": 222}, {"referenceID": 60, "context": "The results are not far from performance of supervised methods using acoustic, lexical, and syntactic evidence, where reported accuracies for both word level prominence and boundary detection range from 84% to 87% [65, 66].", "startOffset": 214, "endOffset": 222}, {"referenceID": 62, "context": "The role of signal energy as a reliable determinant of prosodic structure is interesting, but not altogether surprising [68].", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "would be a truly multidimensional representation of speech signal similar to the multi-scale visual analyses [31].", "startOffset": 109, "endOffset": 113}], "year": 2015, "abstractText": "Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in textto-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.", "creator": "LaTeX with hyperref package"}}}