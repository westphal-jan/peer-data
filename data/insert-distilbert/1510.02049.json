{"id": "1510.02049", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Assisting Composition of Email Responses: a Topic Prediction Approach", "abstract": "we propose an approach for helping agents globally compose email replies customized to customer requests. to enable that, we concurrently use lda to extract latent topics from a collection of email exchanges. we then use these latent topics to label our contact data, gradually obtaining a so - called \" silver standard \" topic labelling. we exploit this labelled set to train a classifier to : ( i ) predict the topic distribution options of the entire agent'current s email response, based on features of the customer's email ; and ( ii ) predict the topic distribution of the next sentence in the agent's reply, based on the average customer's email features and on features of the appropriate agent's current sentence. the experimental results on a typically large email collection from a online contact center established in the tele - com domain show that the proposed ap - proach is effective in predicting the best topic of the agent'priority s overall next sentence. in 80 % of the 97 cases, the correct given topic is present among 50 the original top five recommended topics ( out of fifty possible ones ). this shows the potential of interpreting this method to be applied in an interactive setting, where the opposing agent is presented a chosen small list inclusive of likely topics he to choose from for the next sentence.", "histories": [["v1", "Wed, 7 Oct 2015 18:08:45 GMT  (99kb,D)", "http://arxiv.org/abs/1510.02049v1", "8 pages, 5 figures"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["spandana gella", "marc dymetman", "jean michel renders", "sriram venkatapathy"], "accepted": false, "id": "1510.02049"}, "pdf": {"name": "1510.02049.pdf", "metadata": {"source": "CRF", "title": "Assisting Composition of Email Responses: a Topic Prediction Approach", "authors": ["Spandana Gella", "Marc Dymetman", "Jean Michel Renders", "Sriram Venkatapathy"], "emails": ["S.Gella@ed.ac.uk", "Marc.Dymetman@xrce.xerox.com", "Jean-Michel.Renders@xrce.xerox.com", "vesriram@amazon.com"], "sections": [{"heading": null, "text": "The experimental results on a large email collection from a contact center in the telecom domain show that the proposed approach is effective in predicting the best topic of the agent\u2019s next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence."}, {"heading": "1 Introduction", "text": "The focus of the work presented in this paper is to develop models that can help a person reply to an email query. This is very relevant in the customer care situation where agents frequently have to reply to similar queries from different customers. Of course, those queries that are similar solicit replies\n\u2217email:S.Gella@ed.ac.uk \u2020email:Marc.Dymetman@xrce.xerox.com \u2021email:Jean-Michel.Renders@xrce.xerox.com \u00a7email:vesriram@amazon.com\nthat are similar as well, sharing similar topic structures and vocabulary. Hence, providing suggestions to the agent with respect to the topic structure as well as to the content, in an interactive manner, can help in the effective composition of the email reply.\nTypically, in customer care centres, in order to help the agents send replies to similar queries, the agents have access to a repository of canned responses. In response to a query from the customer, the agent searches among appropriate canned responses, makes appropriate modifications to the text, fills in information and then sends the reply. This process is both inflexible, as well as time consuming, specially in cases where the customer query is slightly different from one of the expected queries.\nIn what we are proposing here, the goal is to provide topic and content suggestions to the agent in a non-intrusive manner. This means that the agent can ignore any suggestion that is irrelevant to him/her during the composition of the message.\nThere are two types of suggestions that we are targeting:\n1. Topic prediction of the entire email response that needs to be composed.\nThis can be useful for automatically suggesting an appropriate canned response to the agent. If such a document is available, then this can help the agent in planning the response.\n2. Topic prediction of the next sentence in the reply.\nThis can be useful for interactively presenting the topics for the next sentence (and the corresponding representative sentence or phrases), which the agent can choose or ignore while composing the reply.\nWe show that, with the methods described in\nar X\niv :1\n51 0.\n02 04\n9v 1\n[ cs\n.C L\n] 7\nO ct\n2 01\n5\nthis paper, topic prediction of the entire email response as well as of the next sentence, can be made with reasonably high accuracy, making these predictions potentially useful in a scenario of interactive composition. We note however that these methods could not lead to a fully automatic composition, because, as all techniques that rely only on such textual training data, they do not have access to knowledge bases or similar external resources that the agent needs to consult in order to provide detailed and specific answers.\nWe evaluated our methods on a set of email exchanges in the Telecom domain in a customer care scenario. Three kinds of experiments were conducted:\n1. Investigating the influence of the customer\u2019s email on the word-level perplexity of the agent\u2019s response, to validate and quantify our basic assumption that the context given by the customer\u2019s query strongly conditions the agent\u2019s response. The details are presented in Section 5.\n2. Predicting the topics in the agent\u2019s response given the customer\u2019s query. See Section 6.1 for details.\n3. Predicting the topic(s) in the agent next sentence given the context of his/her previous sentences (in addition to the customer\u2019s query). See Section 6.2 for details."}, {"heading": "2 Related Work", "text": "Most works addressing \u201cemail text analytics\u201d in the past few years have been on classification and summarization. Email classification has proven to be useful in many standard applications such as spam detection and filtering high-priority messages. Research themes such as summarization and question answering came into focus because of the need of better interpreting the overwhelming amount of emails/messages generated with the advent of email groups and discussion forums. One of the earlier contributions to email summarization was the work by (Muresan et al., 2001) whereas (Rambow et al., 2004) extended it to email threads; Scheffer et al. (2004) on the other hand proposed semi-supervised classification techniques for question answering in the context of such threads.\nSome of the recent classification and summarization techniques have been based on\u201cspeech\nacts\u201d or \u201cdialog acts\u201d such as proposing a meeting, requesting information (Searle, 1976; Bunt, 2011). Several email studies including summarizing of email threads (Oya and Carenini, 2014) or classification of emails (Cohen et al., 2004) involve dialog-act based analysis. There has been very little work so far on customer-agent related email threads. Some of these works include identification of emotional emails related to customer dissatisfaction/frustration (Gupta et al., 2013), as well as learning possible patterns/phrases for textual re-use in email responses (Lamontagne and Lapalme, 2004). (Chen and Rudnicky, 2014) is a recent work that attempts to generate emails based on a two-stage process where a structural template is first produced and then a topic-specific language model is used for producing textual realizations of the different slots in the template (see also (Oh and Rudnicky, 2002) for an earlier work using a similar language model based approach).\nThere have been a number of works that have addressed the problem of discovering the latent structure of topics in the related area of spoken and chat conversations. Recently (Zhai and Williams, 2014) have addressed this problem using HMMbased methods handling dialog state transitions and topic content simultaneously; this work differs from ours in several respects. First, the nature of the data is not the same, short alternating conversational utterances in their case, large single email responses in ours. Second, the focus of (Zhai and Williams, 2014) is on the discovery of latent topics (and conversational states) based on existing dialog texts (speech transcripts or chats), using HMM-based techniques different from our LDA approach. Finally, in our case, in addition to discovering the latent structure of existing emails, we also actually predict which topics will likely be employed in the forthcoming agent\u2019s response, which is not attempted in their paper."}, {"heading": "3 Dataset", "text": "The dataset that we used for our study is a collection of emails from the technical support team of a major telecom company in the UK. The dataset contains 54.7k email threads collected from the UK region during Jan 2013\u2212May 2014. Usually, an email thread is started by a customer reporting a problem or seeking information, followed by an agent\u2019s response suggesting fixes or asking for more details to fix the reported problem. These\nthreads continue until the problem is solved or the customer is satisfied with the agent\u2019s response. An example email conversation between a customer and an agent is given in the left column of Table 3. Usually, customer emails are in free form while agent replies have a moderately systematic structure. On average, there are 8 emails in a thread.\nFor our study, we just considered the first two emails in a thread, namely the original customer\u2019s query and the corresponding agent\u2019s reply. We have limited our experiments to emails which have at least 10 words for customer emails and 20 words for agent replies. This resulted in 48.3k email threads out of which we used 80% for training and 20% for testing. The statistics concerning the number of documents, as well as their average length in tokens (words) and sentences are given in Table 1."}, {"heading": "4 Extracting Topics and Building a \u201cSilver Standard\u201d based on LDA", "text": "As we have explained, we focus on two main tasks:\n\u2022 Task T1: predict the likely overall topics of the whole agent\u2019s reply based on the knowledge of the customer\u2019s email ;\n\u2022 Task T2: when an agent is writing an email, predict the likely topics of the next sentence, based on the initial query and the additional knowledge of the previous sentences.\nHowever, our training data are not annotated at the level of topics. In order to synthesize such an annotation, we use a popular unsupervised technique \u2013 Latent Dirichlet Allocation (Blei et al., 2003) \u2013 for modeling the topic space of various views of the collection. There are potentially three ways for extracting topics in our set of conversations. In the first setting, we keep customer and agent emails separated, identifying distinct topic\nmodels for each collection: a document is a customer email in the first collection, and an agent email in the second collection. We denote byMNC and MMA the topic models trained on customer emails (using N topics) and agent emails (using M topics) respectively. In the second setting, we concatenate customer and agent emails and identify a unique, common set of topics: so, here, we are considering a collection of documents, where each document is the concatenation of the customer\u2019s email and of the agent\u2019s reply. The resulting model will be noted asMMCA, where M is the number of topics in the model. In the third setting, instead of considering the whole email as a document, we take each sentence of the email as a separate document when building the topic models. The model is called MMS . In our case, we built this model from the agent reply messages only, in order to specialize the sentence-level topic model on the agent \u201c\u2018style\u201d and vocabulary.\nAs outcome of the topic extraction process on the training sets, we have both the topic distribution over the training documents and the word distribution for each topic. Once trained, from these word distributions and the model priors, we can infer a topic distribution for each word of a given test document (this document being an entire email or a single sentence), and by aggregating these individual distributions, a global topic distribution can be derived for the whole test document. We consider these assignments of topic distributions as providing a silver-standard annotation of the documents (a proxy to a supervised \u201cgold standard\u201d). These ascribed annotations will subsequently be used for training our prediction models (using the silver-standard annotations of the training set) and evaluating them (using the silver-standard annotations of the test set). Additionally these topic assignments will not only be used as labels to be predicted, but also as additional features to represent and summarize at the semantic level the content of the customer\u2019s query (for task T1) and the content of the previous agent sentences for task T2 (see details in Section 6).\nWe denote a pair of emails of the form customer-query / agent-reply by (Ci, Ai) and the application of a topic modelMnt to Ci (resp. Ai) by \u03c4nt (Ci) (resp. \u03c4 n t (Ai)), with n the number of topics and t the type of model (t \u2208 C,A,CA, S), as described here above. The quantity \u03c4nt is a probability distribution over n topics and we define the\ndominant topic Dnt of a test sample as the topic with the highest probability in this distribution.\nIn Table 2, we present a sample of topics learned using the SENTENCES model (MMS ). In the table, we describe each topic with its top ranked words and phrases.\nIn Table 3, we present an example of a query/reply email pair, as well as the corresponding Top-3 highest probability topics and their most representative words/phrases, both for the customer and the agent parts (MMCA model, with M = 50).\nFinally, it should be noted that the SENTENCES model (MMS ) gives rise to more peaked topic distributions compared to the other models. We consider that a distribution is peaked when the probability of its dominant topic is more than 0.5 (that is, more than the aggregated probability of all competing topics) and at least twice larger than that of the second topic. Over the test set, more than 90% of sentences exhibit a peaked distribution when considering the \u03c4MS (Ai) topic distribution (see Figure 1). This will motivate the use of the dominant topic instead of the whole distribution when solving the task T2, as explained in Section 6.2."}, {"heading": "5 Influence of the Customer Query on the word-level perplexity", "text": "We examine the influence of knowing the context of the customer\u2019s query on the content of the agent\u2019s email. In order to do that, we consider the set of test agent emails and compare the perplexity of the language model based on MMA versus the one identified with MMCA. Recall that the model MMA infers the probability distribution (\u03c4MA (Ai))\nover topics by exploiting only the agent emails from the training set, while the modelMMCA infers a probability distribution (\u03c4MCA(Ai)) over topics by exploiting both the customer queries and the agent replies in the training set.\nThe perplexity scores are computed using the following formulas:\nperplexity(A|MMA ) = exp( \u2212 \u2211d\ni=1 logL(Ai)\u2211d i=1NAi )\nperplexity(A|C,MMCA) = exp( \u2212 \u2211d i=1 log L(Ci+Ai)\nL(Ci)\u2211d i=1NAi )\n(1)\nIn these equations, the test set is ((C1, A1), (C2, A2), . . . , (Cd, Ad)), with A = (A1, . . . , Ad) and C = (C1, . . . , Cd), d is the number of agent emails in the test set and NAi is the total number of words in Ai. The term L(Ai) (resp. L(Ci), L(Ci + Ai)) is the likelihood of the sequence of words in Ai (resp. Ci, Ci + Ai), as given by the LDA model MMA (resp.MMCA,MMCA).\nIn Figure 2, we present the perplexity scores of the two models. We see that the model which uses the customer\u2019s email as context has lower perplexity scores, as could be expected intuitively. This indicates that a generative LDA model has the potential to use the context C to directly improve the prediction of the words in A. However, in this paper, instead of directly trying to predict words (which is strongly connected with the design of the user interface, for instance in the form of semiautomatic word completion), we will focus on the different, but related, problem of predicting the most relevant topics in a given context. As topics could be rather easily associated with canned responses (sentences, paragraphs or whole emails), predicting the most relevant topics amounts to recommending the most adequate responses."}, {"heading": "6 Predicting Relevant Topics of the Agent Response", "text": ""}, {"heading": "6.1 Topic prediction for the overall agent\u2019s email", "text": "In this section, we focus on Task T1, namely predicting the topic distribution of the agent response using only the contextual information : the customer query Ci and its topic distribution \u03c4MCA(Ci). The choice of the MMCA model rather than MNC is motivated by the considerations described in the previous section (Section 5). Note that theMMCA\nmodel is used both to compute synthetic semantic features (\u03c4MCA(Ci)) and to provide a silver standard for the topic prediction (\u03c4MCA(Ai)). The predictor can be written as:\n\u03c4\u0302M (Ai) = f(\u03c9(Ci), \u03c4 M CA(Ci)) (2)\nwhere \u03c9(Ci) represents the bag-of-words of the customer query.\nLearning the mapping shown in Equation 2 could be considered as a structured output learning problem. For solving it, we use an extension of logistic regression that can be trained with soft labels (the silver standard annotations given by \u03c4MCA(Ai)), adopting the Kullback-Leibler divergence between \u03c4\u0302M (Ai) and \u03c4MCA(Ai) as loss function and using a simple Stochastic Descent Gradient algorithm to optimize this loss function. Recall that the silver-standard labels are used both for building the predictor (from the training set) and for assessing the quality of the predicted topic distribution on the test set."}, {"heading": "6.2 Topic prediction for each sentence of the agent\u2019s email", "text": "To solve our second task (Task T2), namely predicting the topic distribution of the next sentence\nof an agent\u2019s response, we use the words of the customer query Ci, its topic distribution \u03c4MCA(Ci), the words of the current sentence and the topic distribution of the current sentence \u03c4MS (Ai,j). Note that we are making some kind of Markovian assumption for the agent-side content: we consider that the current sentence and its topic distribution is sufficient to predict the topic of the next sentence, given the customer query context. Noting Ai,j the jth sentence in the agent email Ai, we then build the predictor as:\n\u03c4\u0302MS (Ai,j+1) =\nf(\u03c9(Ci), \u03c4 M CA(Ci), \u03c9(Ai,j), \u03c4 M S (Ai,j), j))\n(3)\nwhere j is the sentence position (index), and where \u03c9(Ci) and \u03c9(Ai) represent the bag-ofwords of the customer and agent emails, respectively.\nIn practice, as we mentioned in Section 4, the topic distributions for sentences using the MMS models are highly peaked at the \u201cdominant\u201d topic. So, it makes sense to use DMS (Ai,j), the dominant topic of the distribution \u03c4MS (Ai,j) instead of the whole distribution. In the same vein, instead of trying to predict the whole topic distribution of the next sentence, it is reasonable to predict only what will be its dominant topic. So, a variant of equation 3 is:\nD\u0302MS (Ai,j+1) =\nfD(\u03c9(Ci), \u03c4 M CA(Ci), \u03c9(Ai,j), D M S (Ai,j), j))\n(4)\nWe use the standard multiclass logistic regression for modeling the function shown in equation 4. To be more precise, we build M different predictors, one for each possible value of the dominant topic of the current sentence DMS (Ai,j). Moreover, for j=0, i.e. for the first sentence, we build a family of simpler \u201cdegenerated\u201d models, in the following form:\nD\u0302MS (Ai,1) = fD(\u03c9(Ci), \u03c4 M CA(Ci)) (5)"}, {"heading": "7 Evaluation", "text": "In this section, we present experimental results, showing how our proposed methods perform in predicting topics of the agent\u2019s response. For learning the LDA topic models (as described in section 4), we have used MALLET (McCallum, 2002) toolkit, with the standard (default) setting.\nWe evaluate our methods using three metrics:\n1. Bhattacharya coefficient (Bhattacharya, 1943)\nHere, we evaluate how close the predicted topic distribution is to the silver-standard topic distribution. For Task T1, we compare \u03c4\u0302M (Ai) with \u03c4MCA(Ai) for each agent email Ai of the test set. For Task T2, we compare \u03c4\u0302MS (Ai,j+1) with \u03c4 M S (Ai,j+1) for each sentence (j + 1) of the agent emails Ai of the test set. We have also computed more commonly used measures such as KL divergence and they strongly correlate our findings with our Bhattacharya coefficient scores.\n2. Text ranking measure (for Task T1)\nInstead of directly comparing the probability distributions, we also try to measure how useful is the predicted probability distribution in discriminating the correct agent\u2019s response in comparison to a set of k-1 randomly introduced responses from the training set. The k possible answers are ranked according to the Bhattacharya coefficient between their silverstandard topic distribution and the one predicted from the customer\u2019s query email following equation 2. We consider here the average Recall@1 measure, i.e. the average number of times where the correct response is ranked first.\n3. Dominant topic prediction accuracy (for Task T2)\nHere, we examine whether the dominant topic of the silver-standard annotation for the next sentence belongs to the top-K predicted topics. A high accuracy is necessary for ensuring effective topical suggestions for interactive response composition, which is the primary motivation of our work."}, {"heading": "7.1 Topic prediction of agent email", "text": "In Figure 3, we present the average Bhattacharya coefficient over all test emails for different numbers of topics (M ) using theMMCA model.1 This figure illustrates the trade-off between the difficulty of the task and the usefulness of the model: a higher number of topics corresponds to a more\n1For our purposes, the Bhattacharya coefficient is preferable to other measures of distribution \u201cdistance\u201d such as KLdivergence, because it is upper-bounded by 1, and allows easier comparison across distributions of varying dimensions.\nfine-grained analysis of the content, with potentially better predictive usefulness, but at the same time it is harder to reach a given level of performance (as measured by the Bhattacharya coefficient) than with a small number of topics. For comparison, we also show a baseline where the prediction of the agent\u2019s email topic distribution is simply a copy of the customer\u2019s email topic distribution, with a much lower performance.\nFigure 4 gives the evolution of the average Recall@1 measure for k = 5 when the number of topics is changed. In this case, the baseline (a simple random guess) would have given an average Recall@1 equal to 20%. The best performance (Recall@1 = 52.5%) is reached when M=50.\nWe have also assessed the average Recall@1\nscore with varying k andM fixed to 50: see Figure 5. We see that the text ranking based on the topic distribution prediction is always much higher than the baseline scores."}, {"heading": "7.2 Topic Prediction of Next Sentence", "text": "We compare the dominant topic prediction accuracy of the proposed approach with other baseline approaches. The baseline approaches that we examined are:\n\u2022 Uniform: we assign uniform distribution of topics for every sentence in the test set and compare it with silver standard. In other words, we perform a completely random ranking of the topics.\n\u2022 Average: we assign the same topic distribution for every sentence of the test set; this topic distribution is the global average topic distribution and can be directly derived from the hyper-parameter \u03b1 in LDA models by \u201cnormalizing\u201d the values of the \u03b1 vector (note that, in our case, this hyper-parameter is learned from the training data).\nTable 4 gives the dominant topic prediction accuracy for the \u201cnext topic prediction\u201d task, with K=1 (i.e. the relative number of times where the predicted dominant topic corresponds to the dominant topic given by the silver-standard annotation). These values are averaged over all sentences of the test set, irrespective of their position. For this, we have used the predictors given by equation 4 and fixed the number of topics M to 50. Note that the standard multi-class logistic regression outputs a probability distribution over the topics, so that we can compute the Bhattacharya coefficient between\nthis predicted distribution and the silver-standard distribution as well: this information is also given in Table 4. To see the relative impact of each type of input features, the table gives the performance for specific subsets of features: the \u03c9(x) notation represents the bag-of-word feature vector of text entity x, while \u03c4(x) represents the topic distribution vector of text entity x.\nWe see that the topic prediction accuracy for the next sentence is 0.471, which is much higher than both baselines (Uniform and Average). We can also see that the topic distribution vectors of the current sentence and the customer\u2019s query give a higher prediction accuracy (0.450) than using the bag of words features of the context (0.416). When the position of the next sentence is used as a feature, it improves the results indicating that certain topics are more likely to occur at particular positions in the email than at others. The same trend is also seen when we compute the Bhattacharya coefficient (BC) where the predicted topic distribution has a much higher BC than both the uniform distribution as well as the Average distribution.\nThese results (presented in table 4) illustrate that about half of the predicted topics match with the actual topic (based on silver-standard annotations), which is a significant accuracy in an interactive composition scenario. In Table 5, we show the dominant topic prediction accuracies in the top-K predicted topics, with different values of K. It can be seen that, in an interactive composition scenario where the agent is presented with 5 recommended topics, the agent will be able to recognize the relevant topic in more than 80% of the cases. When the agent is presented with 2 recommended topics, the agent can choose the right topic in 62.5% of the cases. These results are obtained through a combination of all the features, with the topic distribution vectors of the context (current sentence and customer\u2019s query) playing an impor-\ntant role."}, {"heading": "8 Conclusion and Future Work", "text": "We have presented new unsupervised models for discovering discourse structures of email replies in customer-agent oriented email systems, and have evaluated their predictive ability on a real-world Contact-Center email dataset, at the global and local levels. Our experiments indicate the potential of these techniques for an interactive scenario where the agent is guided in the selection of whole emails or individual sentences based on predicted topics.\nStill, numerous interesting avenues could be investigated further. One natural extension of our work would be to consider Multi-dimensional LDA models in the sense of (Paul and Dredze, 2013), which are able to detect topics along different semantic aspects, which would be very useful for disentangling several dimensions that we currently do not distinguish, such as: which issue is being talked about, what is the device concerned, at what stage of a conversation we are, and so on.\nAnother extension would be to examine prediction models that go beyond the Markovian assumption, by exploiting topic dependencies at a longer distance than one sentence.\nThe intended application of this work requires developing a user interface, which has implications on the models (level of granularity, number of topics, ...), as well as going beyond the identification of topics towards the interactive generation of actual texts. Ultimately, we want to be able to automatically recognize what is the most relevant response in a given context, not only because it was often given by agents in similar contexts, but because this response will lead to the fastest and more efficient way of solving the customer\u2019s problem: we should then couple and solve jointly the topic modelling/prediction task with a sequential optimization problem."}], "references": [{"title": "On a measure of divergence between two statistical populations defined by their probability distributions", "author": ["Anil Bhattacharya"], "venue": "Bulletin of the Calcutta Mathematical Society,", "citeRegEx": "Bhattacharya.,? \\Q1943\\E", "shortCiteRegEx": "Bhattacharya.", "year": 1943}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The semantics of dialogue acts", "author": ["Harry Bunt"], "venue": "In Proceedings of the Ninth International Conference on Computational Semantics,", "citeRegEx": "Bunt.,? \\Q2011\\E", "shortCiteRegEx": "Bunt.", "year": 2011}, {"title": "Two-stage stochastic natural language generation for email synthesis by modeling sender style and topic structure", "author": ["Chen", "Rudnicky2014] Yun-Nung Chen", "Alexander Rudnicky"], "venue": "In Proceedings of the 8th International Natural", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning to classify email into \u201cSpeech Acts", "author": ["Vitor R. Carvalho", "Tom M. Mitchell"], "venue": "In EMNLP,", "citeRegEx": "Cohen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2004}, {"title": "Emotion detection in email customer care", "author": ["Gupta et al.2013] Narendra Gupta", "Mazin Gilbert", "Giuseppe Di Fabbrizio"], "venue": "Computational Intelligence,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Textual reuse for email response", "author": ["Lamontagne", "Lapalme2004] Luc Lamontagne", "Guy Lapalme"], "venue": "In Advances in Case-Based Reasoning,", "citeRegEx": "Lamontagne et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lamontagne et al\\.", "year": 2004}, {"title": "Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu", "author": ["Andrew Kachites McCallum"], "venue": null, "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Combining linguistic and machine learning techniques for email summarization", "author": ["Evelyne Tzoukermann", "Judith L Klavans"], "venue": "In Proceedings of the 2001 workshop on Computational Natural Lan-", "citeRegEx": "Muresan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Muresan et al\\.", "year": 2001}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Oh", "Rudnicky2002] Alice H. Oh", "Alexander I. Rudnicky"], "venue": "Computer Speech and Language,", "citeRegEx": "Oh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2002}, {"title": "Extractive summarization and dialogue act modeling on email threads: An integrated probabilistic approach", "author": ["Oya", "Carenini2014] Tatsuro Oya", "Giuseppe Carenini"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group", "citeRegEx": "Oya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oya et al\\.", "year": 2014}, {"title": "Drug extraction from the web: Summarizing drug experiences with multi-dimensional topic models", "author": ["Paul", "Dredze2013] J. Michael Paul", "Mark Dredze"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Associ-", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "Summarizing email threads", "author": ["Rambow et al.2004] Owen Rambow", "Lokesh Shrestha", "John Chen", "Chirsty Lauridsen"], "venue": "In Proceedings of HLT-NAACL 2004: Short Papers,", "citeRegEx": "Rambow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rambow et al\\.", "year": 2004}, {"title": "Email answering assistance by semi-supervised text classification", "author": ["Tobias Scheffer"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Scheffer.,? \\Q2004\\E", "shortCiteRegEx": "Scheffer.", "year": 2004}, {"title": "A classification of illocutionary acts", "author": ["John R Searle"], "venue": "Language in society,", "citeRegEx": "Searle.,? \\Q1976\\E", "shortCiteRegEx": "Searle.", "year": 1976}, {"title": "Discovering latent structure in task-oriented dialogues", "author": ["Zhai", "Williams2014] Ke Zhai", "Jason D. Williams"], "venue": "ACL", "citeRegEx": "Zhai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "One of the earlier contributions to email summarization was the work by (Muresan et al., 2001) whereas (Rambow et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 12, "context": ", 2001) whereas (Rambow et al., 2004) extended it to email threads; Scheffer et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "Some of the recent classification and summarization techniques have been based on\u201cspeech acts\u201d or \u201cdialog acts\u201d such as proposing a meeting, requesting information (Searle, 1976; Bunt, 2011).", "startOffset": 164, "endOffset": 190}, {"referenceID": 2, "context": "Some of the recent classification and summarization techniques have been based on\u201cspeech acts\u201d or \u201cdialog acts\u201d such as proposing a meeting, requesting information (Searle, 1976; Bunt, 2011).", "startOffset": 164, "endOffset": 190}, {"referenceID": 4, "context": "Several email studies including summarizing of email threads (Oya and Carenini, 2014) or classification of emails (Cohen et al., 2004) involve dialog-act based analysis.", "startOffset": 114, "endOffset": 134}, {"referenceID": 5, "context": "Some of these works include identification of emotional emails related to customer dissatisfaction/frustration (Gupta et al., 2013), as well as learning possible patterns/phrases for textual re-use in email responses (Lamontagne and Lapalme, 2004).", "startOffset": 111, "endOffset": 131}, {"referenceID": 5, "context": "One of the earlier contributions to email summarization was the work by (Muresan et al., 2001) whereas (Rambow et al., 2004) extended it to email threads; Scheffer et al. (2004) on the other hand proposed semi-supervised classification techniques for question answering in the context of such threads.", "startOffset": 73, "endOffset": 178}, {"referenceID": 1, "context": "In order to synthesize such an annotation, we use a popular unsupervised technique \u2013 Latent Dirichlet Allocation (Blei et al., 2003) \u2013 for modeling the topic space of various views of the collection.", "startOffset": 113, "endOffset": 132}, {"referenceID": 7, "context": "For learning the LDA topic models (as described in section 4), we have used MALLET (McCallum, 2002) toolkit, with the standard (default) setting.", "startOffset": 83, "endOffset": 99}, {"referenceID": 0, "context": "Bhattacharya coefficient (Bhattacharya, 1943)", "startOffset": 25, "endOffset": 45}], "year": 2015, "abstractText": "We propose an approach for helping agents compose email replies to customer requests. To enable that, we use LDA to extract latent topics from a collection of email exchanges. We then use these latent topics to label our data, obtaining a so-called \u201csilver standard\u201d topic labelling. We exploit this labelled set to train a classifier to: (i) predict the topic distribution of the entire agent\u2019s email response, based on features of the customer\u2019s email; and (ii) predict the topic distribution of the next sentence in the agent\u2019s reply, based on the customer\u2019s email features and on features of the agent\u2019s current sentence. The experimental results on a large email collection from a contact center in the telecom domain show that the proposed approach is effective in predicting the best topic of the agent\u2019s next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence.", "creator": "LaTeX with hyperref package"}}}