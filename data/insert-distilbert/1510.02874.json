{"id": "1510.02874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "TSEB: More Efficient Thompson Sampling for Policy Learning", "abstract": "in model - based solution approaches to the problem of learning in pursuing an unknown environment, exploring to learn the model parameters takes a toll on the regret. the optimal performance with respect to regret or pac bounds is achievable, if the algorithm exploits with particular respect to reward or explores with respect to the model discipline parameters, respectively. in this paper, finally we propose tseb, a thompson sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter pac guarantees, while being cautious on the regret as well. the proposed approach maintains distributions leverage over approximately the model parameters which are successively refined with more experience. at any given time, the agent solves a model sampled from this distribution, and the sampled improved reward distribution is skewed by an exploration bonus in order to constantly generate more informative exploration. the policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. we provide a fuller detailed analysis of the pac guarantees, and convergence of the proposed approach. we show that our adaptive exploration bonus encourages the additional exploration required for better pac bounds inflicted on the algorithm. we provide empirical analysis on two different simulated domains.", "histories": [["v1", "Sat, 10 Oct 2015 04:16:08 GMT  (732kb,D)", "http://arxiv.org/abs/1510.02874v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["p prasanna", "sarath chandar", "balaraman ravindran"], "accepted": false, "id": "1510.02874"}, "pdf": {"name": "1510.02874.pdf", "metadata": {"source": "CRF", "title": "TSEB: More Efficient Thompson Sampling for Policy Learning", "authors": ["Prasanna P Sarath Chandar", "Balaraman Ravindran"], "emails": ["pp1403@gmail.com", "apsarathchandar@gmail.com", "ravi@cse.iitm.ac.in"], "sections": [{"heading": null, "text": "In model-based solution approaches to the problem of learning in an unknown environment, exploring to learn the model parameters takes a toll on the regret. The optimal performance with respect to regret or PAC bounds is achievable, if the algorithm exploits with respect to reward or explores with respect to the model parameters, respectively. In this paper, we propose TSEB, a Thompson Sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter PAC guarantees, while being cautious on the regret as well. The proposed approach maintains distributions over the model parameters which are successively refined with more experience. At any given time, the agent solves a model sampled from this distribution, and the sampled reward distribution is skewed by an exploration bonus in order to generate more informative exploration. The policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. We provide a detailed analysis of the PAC guarantees, and convergence of the proposed approach. We show that our adaptive exploration bonus encourages the additional exploration required for better PAC bounds on the algorithm. We provide empirical analysis on two different simulated domains."}, {"heading": "1 INTRODUCTION", "text": "In the standard Reinforcement Learning (RL) framework, the environment with which the agent interacts is modeled as a Markov Decision Process (MDP) and the goal of the agent is to learn a policy such that the cumulative reward it receives is maximized over a finite or an infinite horizon. If the parameters of the MDP are known, then the learning process is straight forward, and the optimal policy can be\nlearnt by traditional DP-methods (Sutton and Barto, 1998). However, in any real life application, the parameters of the MDP are not known a priori. In such a scenario, the agent can try to directly learn the policy that maximizes the return (model-free learning) or the agent can try to estimate the parameters of the MDP and learn a policy based on the learnt MDP (model-based learning).\nRecently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014). In modelbased RL the goal of the agent is two-fold. First, it should estimate the true parameters of the model. Second, it should also behave optimally during the phase of learning the model parameters. This is yet another instance of exploration-exploitation dilemma in Reinforcement Learning. The agent has to explore to learn the model parameters, but trying to be explorative in improving the belief over the model parameters reduces the performance, i.e., sum of cumulative rewards over a certain number of timesteps. In this approach, the belief over the parameters of the model gets updated as and when the agent receives a sample, (st, at, st+1, rt+1), where st is the state the agent is at time t, at is the action the agent took at time t and st+1 and rt+1 are next state and its corresponding reward. As the number of samples increases, the belief converges to the true parameters of the MDP.\nAmong model based methods, Bayesian approaches are particularly attractive due to their amenability for theoretical analysis, and the convenient posterior update rule. Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al., 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015). The Bayesian RL approach proposed in (Strens, 2000) is an episodic way of incrementally converging to the true parameters of the model. The learning happens in phases, where after each phase, the agent estimates a posterior distribution over the parameters and samples a model for the next episode. The agent solves for an optimal policy with the sampled parameters and uses it to generate trajectories in that episode followed by updating the\nar X\niv :1\n51 0.\n02 87\n4v 1\n[ cs\n.L G\n] 1\n0 O\nct 2\n01 5\nposterior. This approach of posterior sampling is known as Thompson sampling (Thompson, 1933). The structure of the Bayesian learning as discussed provides a non-zero probability mass over the true model parameters guaranteeing convergence.\nEver since Chapelle and Li (Chapelle and Li, 2011) discussed the efficacy of TS approaches for reinforcement learning, there have been concerted attempts made to achieve a better theoretical understanding of such approaches. Apart from the results in the bandit setting, Thompson sampling approach for full RL has been shown to work well in practice and has been shown to be regret optimal (Gopalan and Mannor, 2015). However, there are no PAC guarantees in the literature for the Thompson sampling approach. To achieve PAC guarantees we need to encourage more aggressive exploration than enjoined by the basic TS approach and one way to do that is to use an exploration bonus. For e.g., (Kolter and Ng, 2009) proposed Bayesian Exploration Bonus (BEB) algorithm which added a constant exploration bonus to the problem of solving for an optimal policy in an unknown environment. (Kolter and Ng, 2009) computes a point estimate of the MDP and solves for the optimal policy in every episode. This improves on an another exploration bonus approach, MBIE-EB (Strehl and Littman, 2008a), in terms of the PAC bounds. Note that in general, adding exploration bonus to the learning agent results in better performance with respect to PAC but may not result in a regret optimal algorithm.\nThe primary contribution of our work is TSEB (Thompson Sampling with Exploration Bonus), a Thompson Sampling algorithm that uses an adaptive exploration bonus. As with usual TS approaches, TSEB also maintains a distribution over the parameter space. But the sampling strategy employs an adaptive exploration bonus - when a model is sampled from the distribution in each phase, the rewards of the sampled model are modified. This exploration bonus at that state is related to the current uncertainty in the parameter estimates for the state and this leads to more informative trajectories being generated in each episode. The exploration in most Thompson sampling approaches lead to optimal regret. We show that TSEB encourages additional exploration required for better PAC bounds. To our knowledge this is the first work in the literature to provide PAC guarantees for TS. We empirically show that by appropriately tuning a trade-off parameter we can improve performance with respect to regret as well.\nMajor contributions of this work are,\n\u2022 Introducing an adaptive, value based, exploration bonus to aid model based learning agent.\n\u2022 Providing a tighter PAC guarantee for TS, with exploration bonus.\n\u2022 Theoretically showing the convergence of the algorithm.\n\u2022 Empirically showing the inadequacy of TS to be PAC optimal.\nThe rest of the paper is organized as follows. Section 2 describes the preliminaries. We describe the TSEB algorithm in Section 3 followed by theoretical analysis of TSEB in section 4. Section 5 discusses about the regret guarantees of TSEB. In section 6, we experimentally analyse TSEB in 2 domains. Section 7 discusses the related work, and section 8 concludes the paper."}, {"heading": "2 PRELIMINARIES", "text": "In reinforcement learning, a learning agent interacts with a world modeled as a MDP, <S,A,R,T,\u03b3>. The MDP description consists of, S, the set of all states, A, the set of all actions, R, the reward function, R: SxA\u2192 R, T, the transition function, T : SxAxS \u2192 [0, 1] and the discount factor \u03b3 \u2208 (0, 1). The agent has to learn an optimal action mapping, \u03c0\u2217 : S \u2192 A that maximizes the cumulative reward over a finite or infinite horizon,H . When the model parameters are known, the optimal policy, \u03c0\u2217, can be obtained by solving the MDP using classical DP-techniques like valueiteration, policy iteration or by optimization methods.\nFurther in the discussion we will use a metric to bound the distance between the value function of the sampled and the true MDP. The metric is inspired from the homomorphism literature (Ravindran and Barto, 2004). Consider two different MDPs M1 and M2. Let the max norm over the difference in their rewards be Kr, and in transition be Kp, and the range of the rewards in M1 be \u03b4r. The similarity between the problem of homomorphism and the problem of estimating the closeness of sampled MDP to true MDP is subtle. The structure, state space, S and action space, A, remains the same and the R2(s), reward function in the true MDP, can be approximated with the samples obtained from the world. Thus, with the given descriptions the difference in the values of the state in M1 and M2, v1 and v2, which we define as the f-function, can be bounded by the following expression (Ravindran and Barto, 2004),\n||v1 \u2212 v2||\u221e = 2\n1\u2212 \u03b3\n[ Kr + \u03b3\n1\u2212 \u03b3 \u03b4rKp\n] (1)\nIf we have a Dirichlet distribution governing the transition, Kp can be bounded by 1n(s,a) (Sorg et al., 2010), where n(s, a) is the number of times the state-action pair was observed. We refer Eqn. 1 as f(Kr, \u03b3). The \u03b4r is upper bounded by 2 in a normalized bounded rewards setting (rt \u2208 [-1,1]),\nf(Kr, \u03b3) = 2\n1\u2212 \u03b3 Kr + 2\u03b3 (1\u2212 \u03b3) min\ns\u2208S,a\u2208A n(s, a)\n (2)\nKr is estimated from the difference between the expected reward sampled in an episode e for an arbitrary state s to the empirical mean of the rewards sampled with samples obtained till episode e\u22121 and time-step t\u22121 in that episode. This measure, f-function, provides a measure of variance of the sampled MDP. This is an unbiased estimate of the distance from the true reward.\nLet R\u0302n be the sample average constructed with n samples, across the episodes. As n\u2192\u221e,\n1\nn \u221e\u2211 n R\u0302n(s, a)\u2212 E[R(s, a)]\u2192 0 (3)\nThe above expression shows that the computed sample average is an unbiased estimate of the expectation of the random variable R(s, a), reward of state s."}, {"heading": "3 TSEB ALGORITHM", "text": "TSEB is an episodic approach, where the incrementally sampled model grows closer to the true model. Solving the converged model gives us a near optimal policy. From the problem as posed, it is intuitive to understand that the agent has to learn the true model to converge to an optimal policy, \u03c0\u2217. TSEB has a modified Bellman update that considers the exploration bonus. The reward, thus, is a convex combination of the reward obtained from the sampled world, and the exploration bonus computed for that state \u03c1(s) (or \u03c1(s, a)) in that episode. The Bellman update will be,\nV (s) = \u03bbR(s, a) + (1\u2212\u03bb)\u03c1(s, a) +\u03b3 \u2211 s\u2032 P (s\u2032|s, a)V (s\u2032)\n(4)\n\u03c1(s, a) = fs(Kr, \u03b3)\nn(s, a) (5)\nwhere fs(kr, \u03b3) provides an upper bound on the difference in the value of the state s between the true and sampled MDP. n(s,a) is the number of times s was visited.\nThe fs term is similar to the f(Kr, \u03b3) defined in the preliminary section except that instead of ||.||\u221e it will be computed for that particular state, s ( min\ns n(s, a) gets replaced\nby the n(s,a)).\nfs(Kr, \u03b3) = 2\n1\u2212 \u03b3\n[ Kr +\n2\u03b3\n(1\u2212 \u03b3)n(s, a)\n] (6)\nThe algorithm follows a greedy policy and takes the max action with respect to the modified Bellman update. As \u03c1(s, a) decays with every visit, the agent explores the state space adaptively. The updates from the sampled trajectories help the distribution to narrow its belief- reducing the variance over the distribution. As the agent samples from those states that are useful, by following a greedy policy,\nAlgorithm 1: Thompson Sampling with Exploration Bonus (TSEB) Input: Parameter Space \u0398, prior over \u0398, action space A, state space S, and \u03b3. Define: E . Number of episodes.\nTe . Number of time-steps in episode e. \u03c1 . Exploration Bonus. ret . reward at time step t in episode e. r . reward obtained by taking an action. Ve . Value function in episode e. Re . Reward function sampled for episode e. Pe . Transistion function sampled for episode e.\nOutput: policy \u03c0 for e in range (E) do\nM\u03b8 \u2190Sample Re and Pe from posterior Ve \u2190 Solve for \u03c0\u2217(M\u03b8) for t in range (Te) do\n\u03c0(set )\u2190 maxa\u2208A((\u03bbR(set , a) + (1\u2212 \u03bb)\u03c1(s\u2032) +\u03b3 \u2211 s\u2032 Pe(s\n\u2032|s, a)Ve(s\u2032|s, a)) r\u2190take action(\u03c0(set )) n(set )\u2190 n(set )+1 n(set , s e t\u22121)\u2190 n(set , set\u22121)+1 r(set )\u2190r(set )+ 1n(set ) [r \u2212 r(s e t )] \u03c1(set )\u2190 \u03c1(set )+f(s e t )\nn(set )\nend Update the posterior: \u03c0t+1(d\u03b8) \u221d p(St, At, Rt, S\u2032t)\u03c0(d\u03b8)\nend\nmore often after a few episodes, the sampled MDP might not be close to the true MDP, or cannot be guaranteed. But, the parameters of the better rewarding states will be close to the optimal, thus providing an -optimal policy. Note that TSEB learns optimal policy for states which are useful and the notion of useful states evolve over the episodes. Thus, even though TSEB does not learn optimal policy for the true MDP, it learns a near-optimal policy which is more close to the optimal policy in states that will be often visited by the agent.\nThe linear decay of the exploration bonus makes the exploration bonus to become insignificant either when the parameters are closer to the true MDP or if the number of visits is large.\nTSEB, unlike the most other previous algorithms (Except (Sorg et al., 2010)) uses the uncertainty in the estimates to structure the exploration bonus. This helps us model real world systems, which have inherent uncertainty. Exploiting the inherent uncertainty in the events to decide on exploration is the key feature of our work. The exploration bonus entails the PAC guarantees of the algorithm. Further theoretical analysis shows us that the bound is indeed tighter than in (Sorg et al., 2010). The exploration bonus is com-\nputed here even more cleverly, thus avoiding integrating over the parameters, compared to (Sorg et al., 2010). Also there is a principal difference with (Kolter and Ng, 2009), wherein the exploration of the agent is concentrated around the uncertain region and the uncertainty is not assumed to be uniform over the world. This formulation can also be extended to provide exploration bonus a priori. Apart from the prior over the model parameters, prior over the exploration bonus helps in learning the model faster and better. We don\u2019t analyze a priori exploration bonus in this work."}, {"heading": "4 THEORETICAL ANALYSIS OF TSEB", "text": "PAC analysis provides an upper bound on the number of sub-optimal steps of an asymptotic agent that is required for the algorithm to converge to an -optimal solution with probability 1 \u2212 \u03b4. Let us assume the algorithm requires a set of samples, M. Let each sample be (st, at, st+1, rt+1). Though, TS theory gives us regret guarantees at O(logT) (Gopalan and Mannor, 2015), where T is the number of time-steps, we don\u2019t have a notion of PAC-bound for TS. This is primarily because the algorithm has to be explorative to learn the model parameters for it to be PACoptimal. The conundrum here is, if the algorithm is explorative its regret will be worse. Hence, the greedy action selection doesn\u2019t let the agent to be explorative. In TSEB with the addition of exploration bonus and thereby skewing the sampled MDP in a way that would make the exploration as part of rewards, the agent can still be greedy with the action selection and provide a sample guarantee. The linear decay of exploration bonus helps the agent converge to the optimal policy like in the TS setting.\nTheorem 4.1. Exploration using the defined exploration bonus , \u03c1, leads to a monotonic convergence of sampled parameters to the optimal parameters.\nProof. This theorem essentially says that adding exploration does not affect the monotonic convergence of Thompson Sampling. To prove this theorem, it is sufficient to show that the exploration bonus leads to a monotonic convergence of the f-function. The f -function defined in Eqn. 6 for every episode, by way of doing posterior sampling, decreases with samples. As the f -function is an unbiased estimate of the difference in the value of the true and the sampled MDP (Eqn. 1), the decrease in the f-function indicates that the sampled model parameters are closer to the true MDP. The f-function will converge to an \u2032 such that \u2032 \u2265 0. For any number of samples further, the sampled MDP lies within the -ball of the true MDP. To bound the saturation point, let us estimate the rate of change of f with respect to timestep, \u2206f ,\n\u2206f \u2264 2 1\u2212 \u03b3\n[ \u2206Kr + \u03b3\n1\u2212 \u03b3 c\n1\nn(s, a)\n] (7)\nSince the rewards are bounded, rate of change of range can be bounded by a constant c, 0 < c \u2264 2. At optimum, the first order derivative vanishes. Hence,\n\u2206Kr \u2264 c\u03b3\n(1\u2212 \u03b3)n(s, a) (8)\nThe sum over differences across all the states gives an upper bound on the true distance between the sampled and actual MDP. Hence, in an episode this is,\u2211\ns,a\u2208S\u00d7A \u2206Kr = \u2211 s,a\u2208S\u00d7A\nc\u03b3\n(1\u2212 \u03b3)n(s, a) (9)\nNow, with S, the cardinality of the set of states, and A, the cardinality of set of actions, this can be upper bounded by,\u2211\n\u2206Kr \u2264 SAc\u03b3\n(1\u2212 \u03b3)nmin(s, a) (10)\nLet the sum over differences be denoted by \u03c4 , then,\n\u03c4 \u2264 SAc\u03b3 (1\u2212 \u03b3)nmin(s, a)\n(11)\nEqn. 11 states that the mins\u2208Sn(s,a) is inversely related to \u03c4 . And, \u03c4 is directly proportional to f . As we don\u2019t discard the samples, the nmin(s, a) increases monotonically thus letting the \u03c4 to decrease monotonically. The saturation of the upper-bound on the distance, f-function, provides a formal guarantee of the convergence of TSEB.\nPAC-MDP: An RL algorithm is said to be PAC-MDP, if for any MDP, M, > 0, 0 < \u03b4 < 1, the sample complexity of the algorithm is bounded by some function f that is polynomial in S, A, 1/ , 1/\u03b4, and 11\u2212\u03b3 , with probability at least 1-\u03b4.\nTheorem 4.2. After M = O ( SAf0(Kr,\u03b3)\n2\n) steps, TSEB\nconverges to an -optimal value function with probability 1-\u03b4.\nOutline of the Proof. Let p(u) be the expected probability of selecting an action that will lead the agent to an unexplored state. Let us define a positive non-zero number, T , which is the number of time steps in each episodes such that the expected number of visits to unexplored states is at least 1. Let there be a finite positive integer k, the number of times a state has to be visited for its exploration bonus to become insignificant implying that the state has been explored.\nWith the above notations, the number of episodes required to converge to the true MDP parameters will be kSA, where S and A are the cardinality of state and action sets. We define the samples required for the algorithm to be optimal as kSAT. The expression has k, and T which are not\nknown. The expressions obtained in this section, for the sample complexity, maps to k and T indirectly.\nBy showing the ||.||\u221e of difference between the true MDP and the sampled MDP monotonically decreases with every sample and f-function provides a finite length converging sequence, we can compute the total sample complexity, M . We consider variance-based concentration measure, as it is more applicable for deriving the bounds for TSEB, and provides a sharper concentration measure than the Chernoff-Bound used in the analysis of UCB like algorithms. Let X1, X2, ..., Xn be independent random variables with E[Xi] = \u00b5 and V ar[Xi] = \u03c32. Then for >0,\nP\n( 1\nn n\u2211 i=1 Xi \u2265 \u00b5+\n) \u2264 e \u2212 2 4\u03c32 (12)\nThis is an extension of the Chernoff bounds (Chernoff, 1952) in a known variance setting.\nProof. From Variance bounds definition, for a sample of reward sequence from a single state, (R)i. Let E[Ri] = R\u2217, and V ar[Ri] = \u03c32\nP\n( 1\nn n\u2211 i=1 Ri \u2265 R\u2217 +\n) = e \u2212 2 4\u03c32 = \u03b4 (13)\n\u03c32 = 2\n4 log 1\u03b4 (14)\nThe above equation expresses the relation between ( , \u03b4) and (\u03c32). \u03c32 in the above equation is the summation of differences in value of states between the true and sampled MDP. This can be upper bounded by S \u2217A \u2217 f(Kr, \u03b3).\nFurther we need to establish that the exploration bonus decays with the variance of the model parameters. By definition, the exploration bonus \u03c1(s, a) is a cumulative sum of differences between the sampled state parameters and an unbiased estimate of the parameters. This,\n\u03c1(s, a) = 1\nn \u2211 n |E\u0302[\u03b8s,a]\u2212 \u03b8\u2032s,a| (15)\nwhere, \u03b8s,a is the augmented notation for the state\u2019s parameter, E\u0302[\u03b8s,a] is an unbiased estimate of the mean, and \u03b8\u2032s,a is the sampled parameter.\nThe variance of the estimate will be 2-norm of the estimate above, Eqn. 15, and we look at cumulative 1-norm. The difference occurs in the magnitude of the convergence rate, but the point of convergence remains the same. Hence, we use a variance based complexity bound to upper bound the number of samples.\nFor a tighter , variance has to be smaller. This implies that f(Kr, \u03b3) has to be smaller. As repeated sampling of trajectory decreases the variance, this is set as an adaptive exploration bonus to the agent. Now, with n being the number of visits to a state s, and fo(Kr, \u03b3) being the expected initial distance of the sampled MDP from the true MDP with respect to the prior,\nf0(Kr, \u03b3)\nn \u2265\n2\n4 log 1\u03b4 (16)\nThe upper bound on the number of visits to an individual state is given by,\nn \u2264 4f0(Kr, \u03b3) 2 log 1\n\u03b4 (17)\nThe total sample complexity, M, for ( , \u03b4) guarantee on the converged MDP, with S and A being the cardinalities of set of states and actions, is bounded by,\nM \u2264 4SAf0(Kr, \u03b3) 2 log 1\n\u03b4 (18)\nM = O ( SAf0(Kr, \u03b3)\n2\n) (19)\nEqn. 19 shows that the upper bound on the sample complexity is dependent on the initial estimates of the model. M , the total sample complexity is adaptive, as it is a function of the distance between the sampled MDP and the true MDP. The bound, hence, is adaptive and theoretically better than the earlier bounds on sample complexity for a PACMDP."}, {"heading": "5 ARGUMENTS ON REGRET", "text": "The discussion so far elucidates the PAC guarantees offered by the TSEB algorithm. The claim of the algorithm being not going worse in regret has not been addressed so far. Following a greedy policy from the sampled MDP is not very different from the TS approach. The parameters sampled in every episode grow closer to the true model as discussed empirically and theoretically in earlier corresponding sections. As the TSEB agent acts greedily with the sampled model parameters and the model parameters converge, the agent after a certain number of episodes will be acting optimally with the true parameters, \u03b8\u2217. Because, the greedy policy in the M\u03b8\u2217 will be an optimal policy \u03c0\u2217.\nThe exploration bonus, a linearly decaying component in the modified Bellman update, will become insignificant even if it doesn\u2019t become zero. This ensures that TSEB behaves like pure Thompson Sampling after sufficient exploration. Let us define regret at any arbitrary step i, \u2206i, as\n\u2206i = \u00b5 \u2217 \u2212 \u00b5i (20)\nWhere, \u00b5\u2217 is the expected reward by taking the optimal action and \u00b5i is the average reward obtained at a step i. The expected regret E(R),\nE(R) = \u2211 i \u2206iE(T i) (21)\nwhere, T i is the number of sub-optimal steps in an episode and \u2206i, the expected regret in episode i, different from the previous definition. From previous sections, we can observe that the Tni is a converging sequence and so is \u2206i, because of the greedy policy that is mandated in the TS algorithm.\nFrom the algorithm, it is clear that it behaves like the true Thompson Sampling algorithm after a point when the exploration bonuses becomes numerically insignificant. The sub-optimal steps taken by the agent falls into the two cases,\n\u2022 When the sampled parameters are off from the true model parameters and the agent takes a greedy action.\n\u2022 Taking an action that is not the optimal action with respect to the sampled MDP.(May be due to the uncertainty in the action selection.)\nThe recent work on regret in parameterized MDP (Gopalan and Mannor, 2015) is a major contribution to the regret analysis of the full RL Thompson sampling approach. The arguments for the regret analysis of TSEB can be done similar to the TSMDP, but varies in the additive constant term. The big-oh notation of the regret makes it insignificant and hence the same analysis holds."}, {"heading": "6 EMPIRICAL ANALYSIS", "text": "In this section, we experimentally analyze the performance of TSEB. We run experiments in two simulated domains, Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015). The aim of the experiments is to experimentally validate the claim of convergence of the belief and analyze the algorithm under different values of the trade-off parameter, \u03bb \u2208[0,1]."}, {"heading": "6.1 CHAIN WORLD", "text": "The chain domain has 5 states and 2 actions a and b. The agent can take both the actions from any state. With probability 0.2 the agent takes the opposite action than the one selected. The transitions and reward are shown in the figure. The first state has a stochastic reward, from a Gaussian N (0.2, 0.5). The optimal policy with \u03b3 = 0.8 is to take action a in all the states. The algorithm is experimented on different values of the trade-off parameter (Table 2). The analysis shows better performance (cumulative sum of rewards) on every non-zero value. This is intuitive, because when \u03bb is 0 the algorithm behaves only to reduce the variance and ignores the rewards obtained in the world. This behavior is expected. But, the performance increases with increase in \u03bb and decreases after 0.5. The performance has high variance and is inconsistent when \u03bb = 1; this is the TS case. \u03bb = 0.1 has the maximum cumulative reward in this case.\nFurther we analyze the convergence of the model parameters in Fig.2(a) and Fig.2(b). We plot the f-function against the Episodes. The graphs explain that the posterior sampling with exploration bonus converges faster. When \u03bb = 0, the plot shows that the algorithm converges to an inferior model. The inferiority in the model corresponds to the higher f-value. The f-value for \u03bb = 0.5 converges to a much better model. This can be argued because the agent considers both the variance in the model parameters as well as the reward obtained in the true world to be maximized, thus converging to a better model.\nThompson sampling, which is a special case when \u03bb=1, keeps oscillating and doesn\u2019t converge. This is because of the lack of exploration. The graph relates the distance between the sampled MDP and true MDP to the number of samples. As the agent in TS set up acts greedily, the exploration of the agent is poor. The agent has to explore to converge to the true model parameters. TS, being regret optimal always chooses the greedy action and doesn\u2019t explore the state-space well. Hence, the poor PAC-guarantees of TS is experimentally validated. Similarly, the better PAC guarantees that can be obtained by inducing exploration bonus is validated as well.\nFigure 2(c) shows the average regret for all three cases. We see that \u03bb = 0.5 which results in better average reward (from Table 2) is not doing worse in regret when compared to pure TS setting (\u03bb = 1)."}, {"heading": "6.2 QUEUING WORLD", "text": "We analyse the TSEB algorithm with different \u03bb values (Table 3) in the Queuing world defined in (Gopalan and Mannor, 2015). The states of the MDP is simply the number of packets in the queue at any given time, i.e.,S={0,1,2,...,50}. At any given time, one of 2 actions: Action 1 (SLOW service) and Action 2 (FAST service) may be chosen, i.e., A={1,2}. Applying SLOW (resp. FAST) service results in serving one packet from the queue with probability 0. 3 (resp. 0.8) if it is not empty, i.e., the service model is Bernoulli(\u00b5i) where \u00b5i is the packet processing probability under service type i= 1,2. Actions 1 and 2 incur a per-instant cost of 0 and -0.25 units respectively. In addition to this cost, there is a holding cost of -0.1 per packet in the queue at all times. The system gains a reward of +1 units whenever a packet is served from the queue.\nThe comparison in Table. 3 shows the cumulative reward for different settings. \u03bb = 1.0, the regret optimal case, outperformed the others. This is because the model didn\u2019t have much variance in the parameter, so the learning was\nfaster. Hence, the regret optimal way was better than the rest.\nThe two worlds provide two different scenarios: one in which the difference between the performance with different \u03bb values is large (Chain Domain), two in which the difference is less (Queuing Domain). Both the experiments suggest that the combination of exploration bonus and the true rewards in the MDP provides a better performance. As the Chain domain doesn\u2019t offer negative rewards to the agent, exploration as well does pay off well for the agent. But relying only on exploration bonus, \u03bb=0, doesn\u2019t let it converge to the optimal policy. Hence, the reason the agent accumulates better reward when it is not being exploration centric. On the other hand, in the Queuing domain, the agent receives negative rewards as well, this doesn\u2019t aid the agent being over explorative, and the variance in the model parameters are less as well. Hence it accumulates better cumulative reward when it is regret optimal. These two experiments suggest a heuristic to tune \u03bb. \u03bb can be dynamically adapted with respect to unbiased variance in the reward parameter estimate. We leave this as a future work."}, {"heading": "7 RELATED WORK", "text": "Optimism in the face of uncertainty, is an appreciated approach and reasonably widely applied in practice. The approach over-estimates the state-value or action-value estimates with some heuristic to aid in exploration of the agent. In (Kaelbling, 1990), an algorithm proposed as Interval estimation Q-learning (IEQ), the action with the highest upper bound on the underlying Q-value gets chosen. This work also asserts that the gradual decay of the over-estimation lets the agent converge to the optimal policy. This has been followed in approaches as early as UCB(Auer et al., 2002), where the empirical mean, \u00b5\u0302i, of an arm i is over-estimated by the confidence interval of the estimated mean. And, for solving an MDP, the UCRL (Auer and Ortner, 2006) takes an approach inspired by the UCB technique for over estimation to aid exploration. This provides a logarithmic regret bounds in an MDP setting. In an unknown environment setting, the variance based approach to over estimate the value of a state to aid in exploration was proposed in (Sorg et al., 2010), but it is not a TS approach.\nQuite a few approaches have addressed the sample complexity issue in RL. But, while being sample efficient the regret gets worse. And, hence PSRL is better when regret optimal learning is needed. Also, the theoretical guarantees of TS have not been analyzed until recently (Agrawal and Goyal, 2011). Similar guarantees, though, were not extended to the PAC setting. Recently, (Gopalan and Mannor, 2015) gave a regret analysis of TS in full MDP setting that is logarithmic in T , the time-steps. (Russo and Roy, 2014) highlighted an information-theoretic analysis of TS, giving\na better regret bound, considering the entropy of the actiondistribution. In the last decade, parameter estimation was extended for the MDP setting; an episodic way of solving for the model estimation in unknown environment (Strens, 2000).\nMore recently, (Kolter and Ng, 2009) proposed Bayesian Exploration Policy (BEB) algorithm which added a constant exploration bonus to the standard (non-Thompson sampling based) Bayesian RL. This is improved upon the MBIE-EB (Strehl and Littman, 2008b), an interval based exploration bonus algorithm, by increasing the decay rate. (Kolter and Ng, 2009) states that the Bayesian approach cannot have a PAC solution if it doesn\u2019t encode an exploration bonus. So, BEB first proposed a bound on the samples, which is the first PAC-analysis of the Bayesian RL. In line of (Strens, 2000) BOSS, Best of Sampled Sets (Asmuth et al., 2009) that samples multiple models and merges them. The framework then runs trajectories on the derived MDP. It has a constant B, the number of visits for the agent to know a state\u2019s parameters. Note that TSEB can be extended to BOSS setting by sampling multiple MDPs, and following TSEB exploration bonus.\nFrom the literature, it is evident that quite a few approaches were looked at in solving for an optimal policy. The most recent of them include computing the mean MDP (Kolter and Ng, 2009) and ML MDP (Sorg et al., 2010). As the two approaches compute a point estimate, it is theoretically very likely that the probability mass over the true model parameters becomes zero or converges to a very bad estimate in certain cases. The TS approach on the other hand is a pure Bayesian technique that keeps updating the belief and samples a new MDP from the updated samples. This, though converges, is only regret optima, so provides a very bad PAC-estimate. Thus, we showed that adding a better exploration bonus, can make the traditional TS sample efficient and converges to a PAC-MDP."}, {"heading": "8 CONCLUSION", "text": "In this work we propose TSEB - a Thompson sampling approach to model-based RL that uses an adaptive exploration bonus. This is the first TS variant that provides a PAC bound. We introduced a trade-off parameter that controls how much the exploration bonus influences the policy learnt on a sampled MDP. Tuning this parameter allows us to achieve better empirical performance with respect to the regret as well. While this work provides initial intuition into the PAC analysis of TS, more work needs to be done to establish a theory of useful exploration bonus and performance guarantees. Extending the model estimation to a non-parameterized setting, devoid of tight constraints over the parameter space, will also be an useful extension that will be applicable to a wide range of problems."}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "arXiv preprint arXiv:1111.1797.", "citeRegEx": "Agrawal and Goyal,? 2011", "shortCiteRegEx": "Agrawal and Goyal", "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "30th International Conference on Machine Learning (ICML).", "citeRegEx": "Agrawal and Goyal,? 2013", "shortCiteRegEx": "Agrawal and Goyal", "year": 2013}, {"title": "A bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "Uncertainty in Artificial Intelligence.", "citeRegEx": "Asmuth et al\\.,? 2009", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3):235\u2013256.", "citeRegEx": "Auer et al\\.,? 2002", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P. Auer", "R. Ortner"], "venue": "Advances in Neural Information Processing Systems, pages 49\u201356.", "citeRegEx": "Auer and Ortner,? 2006", "shortCiteRegEx": "Auer and Ortner", "year": 2006}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems, pages 2249\u20132257.", "citeRegEx": "Chapelle and Li,? 2011", "shortCiteRegEx": "Chapelle and Li", "year": 2011}, {"title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations", "author": ["H. Chernoff"], "venue": "Ann. Math. Statist., 23(4):493\u2013507.", "citeRegEx": "Chernoff,? 1952", "shortCiteRegEx": "Chernoff", "year": 1952}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 861\u2013898.", "citeRegEx": "Gopalan and Mannor,? 2015", "shortCiteRegEx": "Gopalan and Mannor", "year": 2015}, {"title": "Thompson sampling for complex bandit problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "arXiv preprint arXiv:1311.0466.", "citeRegEx": "Gopalan et al\\.,? 2013", "shortCiteRegEx": "Gopalan et al\\.", "year": 2013}, {"title": "Learning in embedded systems", "author": ["L.P. Kaelbling"], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kaelbling,? 1990", "shortCiteRegEx": "Kaelbling", "year": 1990}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 513\u2013520. ACM.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Approximate homomorphisms: A framework for nonexact minimization in markov decision processes", "author": ["B. Ravindran", "A. Barto"], "venue": "Proceedings of the 5th International Conference on Knowledge Based Computer Systems.", "citeRegEx": "Ravindran and Barto,? 2004", "shortCiteRegEx": "Ravindran and Barto", "year": 2004}, {"title": "An information-theoretic analysis of thompson sampling", "author": ["D. Russo", "B.V. Roy"], "venue": "CoRR, abs/1403.5341.", "citeRegEx": "Russo and Roy,? 2014", "shortCiteRegEx": "Russo and Roy", "year": 2014}, {"title": "Variancebased rewards for approximate bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R.L. Lewis"], "venue": null, "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "An analysis of model-based interval estimation for markov decision", "author": ["A.L. Strehl", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl and Littman,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309\u20131331.", "citeRegEx": "Strehl and Littman,? 2008b", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "A bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "ICML, pages 943\u2013950.", "citeRegEx": "Strens,? 2000", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, pages 285\u2013294.", "citeRegEx": "Thompson,? 1933", "shortCiteRegEx": "Thompson", "year": 1933}], "referenceMentions": [{"referenceID": 17, "context": "If the parameters of the MDP are known, then the learning process is straight forward, and the optimal policy can be learnt by traditional DP-methods (Sutton and Barto, 1998).", "startOffset": 150, "endOffset": 174}, {"referenceID": 16, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 10, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 13, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 12, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 18, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al.", "startOffset": 67, "endOffset": 83}, {"referenceID": 5, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al., 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 133, "endOffset": 209}, {"referenceID": 8, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al., 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 133, "endOffset": 209}, {"referenceID": 16, "context": ", 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 41, "endOffset": 81}, {"referenceID": 7, "context": ", 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 41, "endOffset": 81}, {"referenceID": 16, "context": "The Bayesian RL approach proposed in (Strens, 2000) is an episodic way of incrementally converging to the true parameters of the model.", "startOffset": 37, "endOffset": 51}, {"referenceID": 18, "context": "This approach of posterior sampling is known as Thompson sampling (Thompson, 1933).", "startOffset": 66, "endOffset": 82}, {"referenceID": 5, "context": "Ever since Chapelle and Li (Chapelle and Li, 2011) discussed the efficacy of TS approaches for reinforcement learning, there have been concerted attempts made to achieve a better theoretical understanding of such approaches.", "startOffset": 27, "endOffset": 50}, {"referenceID": 7, "context": "Apart from the results in the bandit setting, Thompson sampling approach for full RL has been shown to work well in practice and has been shown to be regret optimal (Gopalan and Mannor, 2015).", "startOffset": 165, "endOffset": 191}, {"referenceID": 10, "context": ", (Kolter and Ng, 2009) proposed Bayesian Exploration Bonus (BEB) algorithm which added a constant exploration bonus to the problem of solving for an optimal policy in an unknown environment.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "(Kolter and Ng, 2009) computes a point estimate of the MDP and solves for the optimal policy in every episode.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "The metric is inspired from the homomorphism literature (Ravindran and Barto, 2004).", "startOffset": 56, "endOffset": 83}, {"referenceID": 11, "context": "Thus, with the given descriptions the difference in the values of the state in M1 and M2, v1 and v2, which we define as the f-function, can be bounded by the following expression (Ravindran and Barto, 2004),", "startOffset": 179, "endOffset": 206}, {"referenceID": 13, "context": "If we have a Dirichlet distribution governing the transition, Kp can be bounded by 1 n(s,a) (Sorg et al., 2010), where n(s, a) is the number of times the state-action pair was observed.", "startOffset": 92, "endOffset": 111}, {"referenceID": 13, "context": "TSEB, unlike the most other previous algorithms (Except (Sorg et al., 2010)) uses the uncertainty in the estimates to structure the exploration bonus.", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "Further theoretical analysis shows us that the bound is indeed tighter than in (Sorg et al., 2010).", "startOffset": 79, "endOffset": 98}, {"referenceID": 13, "context": "puted here even more cleverly, thus avoiding integrating over the parameters, compared to (Sorg et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 10, "context": "Also there is a principal difference with (Kolter and Ng, 2009), wherein the exploration of the agent is concentrated around the uncertain region and the uncertainty is not assumed to be uniform over the world.", "startOffset": 42, "endOffset": 63}, {"referenceID": 7, "context": "Though, TS theory gives us regret guarantees at O(logT) (Gopalan and Mannor, 2015), where T is the number of time-steps, we don\u2019t have a notion of PAC-bound for TS.", "startOffset": 56, "endOffset": 82}, {"referenceID": 6, "context": "This is an extension of the Chernoff bounds (Chernoff, 1952) in a known variance setting.", "startOffset": 44, "endOffset": 60}, {"referenceID": 10, "context": "BEB (Kolter and Ng, 2009) O ( SAH 2 log SA \u03b4 )", "startOffset": 4, "endOffset": 25}, {"referenceID": 13, "context": "Variance Based (Sorg et al., 2010) O ( \u03b3SA \u03b4 2(1\u2212\u03b3)2 )", "startOffset": 15, "endOffset": 34}, {"referenceID": 7, "context": "The recent work on regret in parameterized MDP (Gopalan and Mannor, 2015) is a major contribution to the regret analysis of the full RL Thompson sampling approach.", "startOffset": 47, "endOffset": 73}, {"referenceID": 10, "context": "We run experiments in two simulated domains, Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015).", "startOffset": 57, "endOffset": 78}, {"referenceID": 7, "context": "We run experiments in two simulated domains, Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015).", "startOffset": 98, "endOffset": 124}, {"referenceID": 7, "context": "We analyse the TSEB algorithm with different \u03bb values (Table 3) in the Queuing world defined in (Gopalan and Mannor, 2015).", "startOffset": 96, "endOffset": 122}, {"referenceID": 9, "context": "In (Kaelbling, 1990), an algorithm proposed as Interval estimation Q-learning (IEQ), the action with the highest upper bound on the underlying Q-value gets chosen.", "startOffset": 3, "endOffset": 20}, {"referenceID": 3, "context": "This has been followed in approaches as early as UCB(Auer et al., 2002), where the empirical mean, \u03bc\u0302i, of an arm i is over-estimated by the confidence interval of the estimated mean.", "startOffset": 52, "endOffset": 71}, {"referenceID": 4, "context": "And, for solving an MDP, the UCRL (Auer and Ortner, 2006) takes an approach inspired by the UCB technique for over estimation to aid exploration.", "startOffset": 34, "endOffset": 57}, {"referenceID": 13, "context": "In an unknown environment setting, the variance based approach to over estimate the value of a state to aid in exploration was proposed in (Sorg et al., 2010), but it is not a TS approach.", "startOffset": 139, "endOffset": 158}, {"referenceID": 0, "context": "Also, the theoretical guarantees of TS have not been analyzed until recently (Agrawal and Goyal, 2011).", "startOffset": 77, "endOffset": 102}, {"referenceID": 7, "context": "Recently, (Gopalan and Mannor, 2015) gave a regret analysis of TS in full MDP setting that is logarithmic in T , the time-steps.", "startOffset": 10, "endOffset": 36}, {"referenceID": 12, "context": "(Russo and Roy, 2014) highlighted an information-theoretic analysis of TS, giving a better regret bound, considering the entropy of the actiondistribution.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In the last decade, parameter estimation was extended for the MDP setting; an episodic way of solving for the model estimation in unknown environment (Strens, 2000).", "startOffset": 150, "endOffset": 164}, {"referenceID": 10, "context": "More recently, (Kolter and Ng, 2009) proposed Bayesian Exploration Policy (BEB) algorithm which added a constant exploration bonus to the standard (non-Thompson sampling based) Bayesian RL.", "startOffset": 15, "endOffset": 36}, {"referenceID": 15, "context": "This is improved upon the MBIE-EB (Strehl and Littman, 2008b), an interval based exploration bonus algorithm, by increasing the decay rate.", "startOffset": 34, "endOffset": 61}, {"referenceID": 10, "context": "(Kolter and Ng, 2009) states that the Bayesian approach cannot have a PAC solution if it doesn\u2019t encode an exploration bonus.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In line of (Strens, 2000) BOSS, Best of Sampled Sets (Asmuth et al.", "startOffset": 11, "endOffset": 25}, {"referenceID": 2, "context": "In line of (Strens, 2000) BOSS, Best of Sampled Sets (Asmuth et al., 2009) that samples multiple models and merges them.", "startOffset": 53, "endOffset": 74}, {"referenceID": 10, "context": "The most recent of them include computing the mean MDP (Kolter and Ng, 2009) and ML MDP (Sorg et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 13, "context": "The most recent of them include computing the mean MDP (Kolter and Ng, 2009) and ML MDP (Sorg et al., 2010).", "startOffset": 88, "endOffset": 107}], "year": 2015, "abstractText": "In model-based solution approaches to the problem of learning in an unknown environment, exploring to learn the model parameters takes a toll on the regret. The optimal performance with respect to regret or PAC bounds is achievable, if the algorithm exploits with respect to reward or explores with respect to the model parameters, respectively. In this paper, we propose TSEB, a Thompson Sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter PAC guarantees, while being cautious on the regret as well. The proposed approach maintains distributions over the model parameters which are successively refined with more experience. At any given time, the agent solves a model sampled from this distribution, and the sampled reward distribution is skewed by an exploration bonus in order to generate more informative exploration. The policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. We provide a detailed analysis of the PAC guarantees, and convergence of the proposed approach. We show that our adaptive exploration bonus encourages the additional exploration required for better PAC bounds on the algorithm. We provide empirical analysis on two different simulated domains.", "creator": "LaTeX with hyperref package"}}}