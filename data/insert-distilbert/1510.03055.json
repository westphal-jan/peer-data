{"id": "1510.03055", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2015", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "abstract": "sequence - to - sequence neural network programming models for generation of conversational responses tend consistently to generate safe, commonplace responses ( e. g., \\ textit { ~ i \u2665 don't know } ) regardless of of the input. we suggest that the traditional objective function, i. e., the likelihood variable of output ( responses ) given input ( messages ) is unsuited to everyday response generation tasks. than instead we propose simulations using maximum mutual information ( mmi ) as objective function in neural models. experimental results demonstrate that the proposed objective fisher function produces more diverse, interesting, unexpected and appropriate responses, yielding prefer substantive gains in \\ bleu sum scores on two conversational datasets.", "histories": [["v1", "Sun, 11 Oct 2015 14:04:57 GMT  (27kb)", "http://arxiv.org/abs/1510.03055v1", null], ["v2", "Thu, 7 Jan 2016 06:59:19 GMT  (270kb)", "http://arxiv.org/abs/1510.03055v2", null], ["v3", "Fri, 10 Jun 2016 22:03:28 GMT  (32kb)", "http://arxiv.org/abs/1510.03055v3", "In. Proc of NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "michel galley", "chris brockett", "jianfeng gao", "bill dolan"], "accepted": true, "id": "1510.03055"}, "pdf": {"name": "1510.03055.pdf", "metadata": {"source": "CRF", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "authors": ["Jiwei Li", "Michel Galley Chris Brockett", "Jianfeng Gao Bill Dolan"], "emails": ["jiweil@stanford.edu", "mgalley@microsoft.com", "chrisbkt@microsoft.com", "jfgao@microsoft.com", "billdol@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n03 05\n5v 1\n[ cs\n.C L\n] 1\n1 O\nSequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don\u2019t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets."}, {"heading": "1 Introduction", "text": "Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet traditional hand-crafted dialog (Levin et al., 2000; Young et al., 2010) poses major challenges for scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). SEQ2SEQ models offer the promise\n* The entirety of this work was conducted at Microsoft.\nof scalability and language-independence, together with the capacity to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way that was not possible with conventional SMT approaches (Ritter et al., 2011).\nAn engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015). Table 1 illustrates this phenomenon, showing top outputs from SEQ2SEQ models in terms of average log likelihood per token.1 All the top-ranked responses are generic, along the lines of I don\u2019t know or Oh my god! Responses that seem more meaningful or specific can also be found in the N-best lists, but these rank much lower. In part at least, this behavior can be ascribed to the relative frequency of generic responses like I don\u2019t know in conversational datasets, in contrast with the relative sparsity of other, more contentful or specific alternative responses.2 It appears that by optimizing for the likelihood of outputs/targets/responses given inputs/sources/messages, neural models assign high\n1We trained a 4-layer SEQ2SEQ neural model with 1,000 dimensional units from 20 million conversation pairs in the OpenSubtitles database (OSDb).\n2In our training dataset from the OpenSubtitles database (OSDb), 0.45% sentences contain the sequence I don\u2019t know, a high rate considering huge diversity of conversational content in the dataset.\nprobability to \u201csafe\u201d responses. This objective function common in related tasks such as machine translation may thus be unsuited to generation tasks involving intrinsically diverse valid outputs.\nThe question is how to overcome the neural models\u2019 predilection for the commonplace. Intuitively, we want to capture not only the dependency of responses on messages, but also the inverse, the likelihood that a message will be provided to a given response. Whereas the sequence I don\u2019t know is of high probability in response to most question-related messages, the reverse will generally not be true.\nWe propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures\nthe mutual dependence between inputs and outputs. In this work, we present practical training and decoding strategies for neural generation models that use MMI as objective function. We demonstrate that using MMI results in a clear decrease in the proportion of generic response sequences produced by models trained and evaluated on two datasets\u2013a large set of Twitter conversations and a subset of the OpenSubtitles movie database\u2013generating correspondingly more varied and interesting outputs. We also find a significant performance boost from the proposed models as measured by BLEU (Papineni et al., 2002)."}, {"heading": "2 Related work", "text": "Earlier efforts to incorporate statistical methods into dialog systems typically relied on one of two approaches. The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011). This approach is both expensive and difficult to extend to open-domain scenarios. The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).\nA third line of investigation\u2013and the one that we adopt in this paper\u2013was first introduced by Ritter et al. (2011), who framed the response generation task as a statistical machine translation (SMT) problem in which message-response phrase alignments are learned in an unsupervised manner from large volumes of conversational data. Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation. Sordoni et al. (2015) extend the system of Ritter et al. (2011) by re-ranking the N-best list of a phrasal SMT-based conversation system with a neural language model that incorporates the prior conversational context. There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015). These SEQ2SEQ models\nare Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that are able to implicitly capture compositionality and longspan dependencies."}, {"heading": "3 Sequence-to-Sequence Models", "text": "Given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as it, ft and ot. We distinguish e and h where et denotes the vector for an individual text unit (for example, a word or sentence) at time step t while ht denotes the vector computed by LSTM model at time t by combining et and ht\u22121. ct is the cell state vector at time t, and \u03c3 denotes the sigmoid function. Then, the vector representation ht for each time step t is given by:\nit = \u03c3(Wi \u00b7 [ht\u22121, et]) (1)\nft = \u03c3(Wf \u00b7 [ht\u22121, et]) (2)\not = \u03c3(Wo \u00b7 [ht\u22121, et]) (3)\nlt = tanh(Wl \u00b7 [ht\u22121, et]) (4)\nct = ft \u00b7 ct\u22121 + it \u00b7 lt (5)\nhst = ot \u00b7 tanh(ct) (6)\nwhere Wi, Wf , Wo, Wl \u2208 RK\u00d72K . In SEQ2SEQ generation tasks, each input X is paired with a sequence of outputs to predict: Y = {y1, y2, ..., ynY }. The LSTM defines a distribution over outputs and sequentially predicts tokens using a softmax function:\np(Y |X) =\nny \u220f\nt=1\np(yt|x1, x2, ..., xt, y1, y2, ..., yt\u22121)\n=\nny \u220f\nt=1\nexp(f(ht\u22121, eyt)) \u2211\ny\u2032 exp(f(ht\u22121, ey\u2032))\nwhere f(ht\u22121, eyt) denotes the activation function between ht\u22121 and eyt , where ht\u22121 is the representation output from the LSTM at time t\u2212 1. Each sentence concludes with a special end-of-sentence symbol EOS. Commonly, the input and output each use different LSTMs with separate sets of compositional parameters to capture different compositional patterns.\nDuring decoding, the algorithm terminates when an EOS token is predicted. At each time step, either a greedy approach or beam search can be adopted\nfor word prediction. Greedy search selects the token with the largest conditional probability, the embedding of which is then combined with preceding output to predict the token at the next step."}, {"heading": "4 MMI Models", "text": ""}, {"heading": "4.1 Notation", "text": "In the response generation task, let S denote an input message sequence (source) S = {s1, s2, ..., sNS} where NS denotes the number of words in S. Let T (target) denote a sequence in response to source sequence S, where T = {t1, t2, ..., tNT , EOS}, NT is the length of the response (terminated by an EOS token) and w denotes a word token that is associated with a K dimensional distinct word embedding ew. V denotes vocabulary size."}, {"heading": "4.2 Objective Function", "text": "The standard objective function for sequence-tosequence models is the log-likelihood of target T given source S, which at test time yields the optimization problem:\nT\u0302 = argmax T\n{ log p(T |S) }\n(7)\nAs discussed in the introduction, we surmise that this formulation leads to generic responses being generated, since it only selects for targets given sources, not the converse. To remedy this, we substitute Maximum Mutual Information (MMI) as objective function. In MMI, parameters are chosen to maximize (pairwise) mutual information between the source S and the target T :\nlog p(S, T )\np(S)p(T ) (8)\nIt thus avoids favoring responses that unconditionally enjoy high probability, and instead biases towards those responses that are specific to the given input. The MMI objective can written as follows:3\nT\u0302 = argmax T\n{ log p(T |S)\u2212 log p(T ) }\nWe use a generalization of the MMI objective which introduces a hyperparameter \u03bb that controls how much to penalize generic responses:\nT\u0302 = argmax T\n{ log p(T |S)\u2212 \u03bb log p(T ) }\n(9)\n3Note: log p(S,T ) p(S)p(T ) = log p(T |S) p(T ) = log p(T |S)\u2212log p(T )\nAn alternate formulation of the MMI objective uses Bayes\u2019 theorem:\nlog p(T ) = log p(T |S) + log p(S)\u2212 log p(S|T )\nwhich lets us rewrite Equation 9 as follows:\nT\u0302 = argmax T\n{\n(1\u2212 \u03bb) log p(T |S)\n+ \u03bb log p(S|T )\u2212 \u03bb log p(S) }\n= argmax T\n{ (1\u2212 \u03bb) log p(T |S) + \u03bb log p(S|T ) }\n(10) This weighted MMI objective function can thus be viewed as representing a tradeoff between sources given targets (i.e., p(S|T )) and targets given sources (i.e., p(T |S)).\nAlthough the MMI optimization criterion has been comprehensively studied for other tasks, such as acoustic modeling in speech recognition (Huang et al., 2001), adapting MMI to SEQ2SEQ training is empirically nontrivial. Moreover, we would like to be able to adjust the value \u03bb in Equation 9 without repeatedly training neural network models from scratch, which would otherwise be extremely timeconsuming. Accordingly, we did not train a joint model (log p(T |S)\u2212\u03bb log p(T )), but instead trained maximum likelihood models. The MMI criterion was used only during testing."}, {"heading": "4.3 Practical Considerations", "text": "Responses can be generated either from Equation 9, i.e., p(T |S) \u2212 \u03bbp(T ) or Equation 10 i.e., (1 \u2212 \u03bb)p(T |S) + \u03bbp(S|T ). However, these strategies are difficult to apply directly to decoding since they can lead to ungrammatical responses (under Equation 9) or make decoding intractable (under Equation 10). In the rest of this section, we will discuss these issues and explain how we resolve them in practice.\n4.3.1 p(T |S)\u2212 \u03bbp(T )\nThe second term (i.e., \u2212\u03bbp(T )) functions as an anti-language model. It penalizes not only highfrequency, generic responses, but also fluent ones and thus can lead to ungrammatical outputs. In theory, this issue should not arise when \u03bb is less than 1, since ungrammatical sentences should always be more severely penalized by the first term of the equation, i.e., log p(T |S). In practice, however,\nwe found that the model tends to select ungrammatical outputs that escaped being penalized by p(T |S).\nSolution Let LT be the length of target T . p(T ) in Equation 9 can be written as:\np(T ) =\nLt \u220f\ni=1\np(ti|t1, t2, ..., ti\u22121) (11)\nWe replace the language model p(T ) with U(T ), which adapts the standard language model by multiplying by a weight g(i) that is decremented monotonically as the index of the current token i increases:\nU(T ) =\nLt \u220f\ni=1\np(ti|t1, t2, ..., tI) \u00b7 g(i) (12)\nThe underlying intuition here is as follows: First, neural decoding combines the previously built representation with the word predicted at the current step. As decoding proceeds, the influence of the initial input on decoding (i.e., the source sentence representation) diminishes as additional previouslypredicted words are encoded in the vector representations.4 In other words, the first words to be predicted significantly determine the remainder of the sentence. Penalizing words predicted early on by the language model contributes more to the diversity of the sentence than it does to words predicted later. Second, as the influence of the input on decoding declines, the influence of the language model comes to dominate. We have observed that ungrammatical segments tend to appear in the latter part of the sentences, especially in long sentences.\nWe adopt the most straightforward form of g(i) by by setting up a threshold (\u03b3) by penalizing the first \u03b3 words where5\ng(i) =\n{\n1 if i \u2264 \u03b3 0 if i > \u03b3 (13)\nThe objective Equation 9 can therefore be rewritten as:\nlog p(T |S)\u2212 \u03bb logU(T ) (14)\nwhere direct decoding is tractable. 4Attention models (Xu et al., 2015) may offer some promise of addressing this issue. 5We experimented with a smooth decay in g(i) rather than a stepwise function, but this did not yield better performance.\n4.3.2 (1\u2212 \u03bb)p(T |S) + \u03bbp(S|T )\nDirect decoding from the above form is intractable, as the second part (i.e., p(S|T )) requires completion of target generation before p(S|T ) can be effectively computed. Due to the enormous search space for target T , exploring all possibilities is infeasible.\nFor practical reasons, then, we turn to an approximation approach that involves first generating Nbest lists given the first part of objective function, i.e., standard SEQ2SEQ model p(T |S). Then we rerank the N-best lists using the second term of the objective function. Since N-best lists produced by SEQ2SEQ models are generally grammatical, the final selected options are likely to be well-formed. Model reranking has obvious drawbacks. It results in non-globally-optimal solutions by first emphasizing standard SEQ2SEQ objectives. Moreover, it relies heavily on the system\u2019s success in generating a sufficiently diverse N-best set, requiring that a long list of N-best lists be generated for each message.\nDespite these challenges, these two objectives work well in practice, significantly improving both the interestingness and the diversity of responses."}, {"heading": "4.4 Training", "text": "Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015). We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000. Other training details are given below, broadly following Sutskever et al. (2014).\n\u2022 LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.08, 0.08]. \u2022 Stochastic gradient decent is implemented using a fixed learning rate of 0.1. \u2022 Batch size is set to 256. \u2022 Gradient clipping is adopted by scaling gradi-\nents when the norm exceeded a threshold of 1. Our implementation on a single GPU processes at a speed of approximately 600-1200 tokens per second.6\n6Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores.\nThe p(S|T ) model described in Section 4.3.1 was trained using the same model as that of p(T |S), with messages (S) and responses (T) interchanged."}, {"heading": "4.5 Decoding", "text": "4.5.1 p(T |S)\u2212 \u03bbU(T )\nAs described in Section 4.3.1, decoding using this model can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decoding. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target T as follows:\nScore(T ) = p(T |S)\u2212 \u03bbU(T ) + \u03b3LT (15)\nwhere LT denotes the length of the target and \u03b3 denotes associated weight. We optimize the length weight using grid search on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size 200. We set a maximum length of 20 for generated candidates. N-best lists are then constructed so that sentences generated with an EOS token at each decoding time step are stored as decoding proceeds.\n4.5.2 (1\u2212 \u03bb)p(T |S) + \u03bbp(S|T )\nWe generate N-best lists based on P (T |S) and then rerank the list by linearly combining p(T |S) with \u03bbp(S|T ) and \u03b3LT . We use grid search to tune the combination weights of \u03bb and \u03b3 on the development set."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "Twitter Conversation Triple Dataset We used an extension of the dataset described in Sordoni et al. (2015), which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month period from June through August 2012. For the purposes of our experiments, we limited context to the turn in the conversation immediately preceding the message. In our LSTM models, we used a simple input model in which contexts and messages are concatenated to form the source input.\nFor tuning and evaluation, we used the development dataset (2118 conversations) and the test dataset (2114 examples), augmented using IR to create a multi-reference set, as described by Sordoni et al. (2015). The selection criteria for these two datasets included a component of relevance/interestingness, with the result that dull responses will tend to be penalized in evaluation.\nOpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters. This dataset does not specify which character speaks each subtitle line, which prevents us from inferring speaker turns. Following Vinyals et al. (2015), we make the simplifying assumption that each line of subtitle constitutes a full speaker turn. Our models are trained to predict the current turn given the preceding ones based on the assumption that consecutive turns belong to the same conversation. This introduces a degree of noise, since consecutive lines may not appear in the same conversation or scene, and may not even be spoken by the same character.\nThis limitation potentially renders the OSDb dataset unreliable for evaluation purposes. We therefore used data from the Internet Movie Script Database (IMSDB),7 which explicitly identifies which character speaks each line of the script. This allowed us to identify consecutive message-response pairs spoken by different characters within the same scene. We randomly selected two subsets as development and test datasets, each containing 2,000 pairs, with source and target length restricted to the range of [6,18]."}, {"heading": "5.2 Evaluation", "text": "For parameter tuning and final evaluation, we used BLEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference BLEU. As the IMSDB data is too limited to support extraction of multiple references, only sin-\n7IMSDB (http://www.imsdb.com/) is a relatively small database of around 0.4 million sentences and thus not suitable for open domain dialogue training.\ngle reference BLEU was used in training and evaluating the OSDb models.\nWe did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard SEQ2SEQ model in order diversify the outputs. Instead, we report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2 in Tables 2 and 3)."}, {"heading": "5.3 Results", "text": "Twitter Dataset We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset.\nMachine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood translation probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015).\nMT+neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1-4]-gram matches between context and response and between message and context (context and message match CMM features) are also employed, as in Sordoni et al. (2015).\nMT+neural reranking achieves a BLEU score of 4.44, which corresponds, to the best of our knowledge, to the previous state-of-the-art performance on this Twitter dataset. Note that Machine Translation and MT+neural reranking are trained on a much larger dataset of roughly 50 million examples. A significant performance boost is observed from MMI over standard SEQ2SEQ, both in terms of BLEU score and diversity.\nOpenSubtitles Dataset Our models achieve significantly lower BLEU scores on this dataset than\nthose on Twitter dataset, primarily because the IMSDB data provides only single references for evaluation. We note, however, that standard SEQ2SEQ models yield lower levels of unigram diversity (distinct-1) on the OpenSubtitles dataset than on the Twitter data (0.0056 vs 0.017), which suggests that other factors may be in play. It is likely that movie dialogs may be much more concise and information-rich than typical conversations on Twitter, making it harder to match gold-standard responses and causing the learned models to strongly favor safe, conservative responses.\nTable 3 shows that the p(T |S) \u2212 \u03bbU(T ) model yields a significant performance boost, with a BLEU score increase of up to 36% and a more than 200% jump in unigram diversity. Our interpretation of this huge performance improvement is that the diversity and complexity of input messages lead standard SEQ2SEQ models to generate very conservative responses,8 which fail to match more the interesting reference strings discussed typical of this dataset. This interpretation is also supported by the fact that the (1 \u2212 \u03bb)p(T |S) + \u03bbp(S|T ) model does not produce as significant a performance boost as p(T |S) \u2212 \u03bbU(T ). In the former, N-best lists gen-\n8The strings I don\u2019t know, I don\u2019t know what you are talking about, I don\u2019t think that is a good idea, and Oh my god constitute 32% percent of all generated responses.\nerated using standard SEQ2SEQ models remain conservative and uninteresting, attenuating the impact of latter reranking. An important potential limitation of (1\u2212 \u03bb)p(T |S) + \u03bbp(S|T ) model is thus that its performance hinges on the initial generation of a highly diverse, informative N-best list.\nQualitative Evaluation Table 4 presents the top N-best candidates generated using the p(T |S) \u2212 \u03bbU(T ) model for the messages described in Section 1. We see that MMI models generate significantly\nmore interesting outputs than standard SEQ2SEQ models.\nIn Table 5, we present responses generated by different models. All examples were randomly sampled (without cherry picking). We see that standard SEQ2SEQ models tend to generate reasonable responses to simple messages such as How are you doing? or I love you. As the complexity of the message increases, however, the outputs switch to more conservative and duller forms, such as I don\u2019t know or I don\u2019t know what you are talking about. An occasional answer of this kind might go unnoticed in a natural conversation, but a dialog agent that always produces such responses risks being perceived as uncooperative or even rude. MMI models produce far more diverse and interesting responses, though they can also occasionally generate completely irrelevant outputs, e.g., message: I am losing my grip;\nresponse: I\u2019m the only one in the world. To mitigate this problem, we will need to seek a better trade-off that will permit generation of responses that are both interesting and relevant.\nPhrase-based MT versus Neural Generation Neural models using MMI as objective function outperform MT in BLEU, establishing a new state-ofthe-art result on the Twitter conversational dataset. More than that, they address several limitations inherent in the MT framework. First, neural models are more flexible in leveraging contextual information such as speaker characteristics, specific topics, domain information, and scenarios that are related to the dialogue. Second, these models are more scalable. Instead of relying on a big phrase translation table to memorize individual response pairs, they encode large amounts of contextual information\nusing a low-dimensionality vector so that semantically similar messages lead to similar responses. Finally, neural models allow end-to-end optimization of model parameters, yielding significant performance gains over earlier methods."}, {"heading": "6 Conclusions", "text": "We investigated an issue encountered when applying SEQ2SEQ models to conversational response generation: These models tend to generate safe, commonplace responses (e.g., I don\u2019t know) regardless of the input. Our analysis suggests that the issue is at least in part attributable to the use of the traditional objective function, namely the unidirectional likelihood of output (responses) given input (messages), widely used in Statistical Machine Translation and other machine learning models. To remedy this problem, we have proposed using Maximum Mutual Information (MMI) as the objective function in neural models, in order to capture not only the dependency of responses on messages but also the inverse. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets.\nTo the best of our knowledge, this paper represents the first work to address the issue of output diversity in the neural generation framework. We have focused on the algorithmic dimensions of the problem. Unquestionably numerous other factors such as grounding, persona (of both user and agent), and intent also play a significant role in generating diverse, conversationally interesting outputs, but those must be left for future investigation. The implications of this work extend beyond conversational response generation, since the challenge of producing interesting outputs also arises in other neural generation tasks, including image-description generation and question answering, and potentially any task where mutual correspondences must be modeled."}], "references": [{"title": "Luke, I am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents, pages 13\u201321. Springer.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition", "author": ["L. Bahl", "P. Brown", "P. de Souza", "R. Mercer."], "venue": "pages 49\u201352.", "citeRegEx": "Bahl et al\\.,? 1986", "shortCiteRegEx": "Bahl et al\\.", "year": 1986}, {"title": "IRIS: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proc. of the ACL 2012 System Demonstrations, pages 37\u201342. Association for Computational Linguistics.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "The Acoustic-modeling Problem in Automatic Speech Recognition", "author": ["Peter F. Brown."], "venue": "Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.", "citeRegEx": "Brown.,? 1987", "shortCiteRegEx": "Brown.", "year": 1987}, {"title": "An empirical investigation of sparse log-linear models for improved dialogue act classification", "author": ["Yun-Nung Chen", "Wei Yu Wang", "Alexander Rudnicky."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8317\u2013", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "\u2206BLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of ACL-", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng."], "venue": "Proc. of ACL, pages 699\u2013709, Baltimore, Maryland. Association for Computational Linguistics.", "citeRegEx": "Gao et al\\.,? 2014", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Spoken language processing: A guide to theory, algorithm, and system development", "author": ["Xuedong Huang", "Alex Acero", "Hsiao-Wuen Hon", "Raj Foreword By-Reddy."], "venue": "Prentice Hall.", "citeRegEx": "Huang et al\\.,? 2001", "shortCiteRegEx": "Huang et al\\.", "year": 2001}, {"title": "Open source toolkit", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01057.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proc. of ACL-IJCNLP, pages 11\u201319, Beijing, China.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013361.", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proc. of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, pages 27\u201332. Association for Computational Linguistics.", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William Dolan."], "venue": "Proc. of EMNLP, pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP, pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS \u2013 a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A neural conversa", "author": ["Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A trainable generator for recommendations", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent"], "venue": null, "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Improving spoken dialogue", "author": ["William Yang Wang", "Ron Artstein", "Anton Leuski", "David Traum"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet traditional hand-crafted dialog (Levin et al., 2000; Young et al., 2010) poses major challenges for scalability and domain adaptation.", "startOffset": 164, "endOffset": 204}, {"referenceID": 19, "context": "Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al.", "startOffset": 241, "endOffset": 262}, {"referenceID": 23, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 26, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 22, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 21, "context": ", 2010), or using neural networks to rerank, or directly in the form of sequence-to-sequence models (SEQ2SEQ) (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015).", "startOffset": 110, "endOffset": 213}, {"referenceID": 23, "context": "of scalability and language-independence, together with the capacity to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way that was not possible with conventional SMT approaches (Ritter et al.", "startOffset": 176, "endOffset": 198}, {"referenceID": 20, "context": ", 2015) in a way that was not possible with conventional SMT approaches (Ritter et al., 2011).", "startOffset": 72, "endOffset": 93}, {"referenceID": 23, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 21, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 26, "context": "In practice, however, neural conversation models exhibit a tendency to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don\u2019t know or I\u2019m OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 197, "endOffset": 262}, {"referenceID": 2, "context": "We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs.", "startOffset": 119, "endOffset": 151}, {"referenceID": 4, "context": "We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs.", "startOffset": 119, "endOffset": 151}, {"referenceID": 16, "context": "We also find a significant performance boost from the proposed models as measured by BLEU (Papineni et al., 2002).", "startOffset": 90, "endOffset": 113}, {"referenceID": 11, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 27, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 17, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 28, "context": "The first is stochastic models built on top of hand-coded rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011).", "startOffset": 77, "endOffset": 182}, {"referenceID": 15, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 18, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 3, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 14, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 5, "context": "The second approach also requires handcrafting, attempting to learn generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; 0; Nio et al., 2014; Chen et al., 2013).", "startOffset": 132, "endOffset": 235}, {"referenceID": 24, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 7, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 1, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 13, "context": "Inspired by recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation.", "startOffset": 83, "endOffset": 168}, {"referenceID": 21, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 22, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 26, "context": "There have also been a number of promising attempts to apply direct end-to-end neural encoding-decoding SEQ2SEQ models (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015).", "startOffset": 119, "endOffset": 200}, {"referenceID": 16, "context": "A third line of investigation\u2013and the one that we adopt in this paper\u2013was first introduced by Ritter et al. (2011), who framed the response generation task as a statistical machine translation (SMT) problem in which message-response phrase alignments are learned in an unsupervised manner from large volumes of conversational data.", "startOffset": 94, "endOffset": 115}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation. Sordoni et al. (2015) extend the system of Ritter et al.", "startOffset": 8, "endOffset": 181}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015), other research teams have subsequently attempted to extend these neural techniques to response generation. Sordoni et al. (2015) extend the system of Ritter et al. (2011) by re-ranking the N-best list of a phrasal SMT-based conversation system with a neural language model that incorporates the prior conversational context.", "startOffset": 8, "endOffset": 223}, {"referenceID": 8, "context": "are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that are able to implicitly capture compositionality and longspan dependencies.", "startOffset": 50, "endOffset": 84}, {"referenceID": 9, "context": "Although the MMI optimization criterion has been comprehensively studied for other tasks, such as acoustic modeling in speech recognition (Huang et al., 2001), adapting MMI to SEQ2SEQ training is empirically nontrivial.", "startOffset": 138, "endOffset": 158}, {"referenceID": 24, "context": "Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015).", "startOffset": 96, "endOffset": 159}, {"referenceID": 12, "context": "Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014; Li et al., 2015).", "startOffset": 96, "endOffset": 159}, {"referenceID": 12, "context": ", 2014; Li et al., 2015). We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000. Other training details are given below, broadly following Sutskever et al. (2014). \u2022 LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.", "startOffset": 8, "endOffset": 370}, {"referenceID": 23, "context": "For tuning and evaluation, we used the development dataset (2118 conversations) and the test dataset (2114 examples), augmented using IR to create a multi-reference set, as described by Sordoni et al. (2015). The selection criteria for these two datasets included a component of relevance/interestingness, with the result that dull responses will tend to be penalized in evaluation.", "startOffset": 186, "endOffset": 208}, {"referenceID": 25, "context": "OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters.", "startOffset": 110, "endOffset": 127}, {"referenceID": 25, "context": "OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M-70M scripted lines spoken by movie characters. This dataset does not specify which character speaks each subtitle line, which prevents us from inferring speaker turns. Following Vinyals et al. (2015), we make the simplifying assumption that each line of subtitle constitutes a full speaker turn.", "startOffset": 111, "endOffset": 388}, {"referenceID": 16, "context": "For parameter tuning and final evaluation, we used BLEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al.", "startOffset": 56, "endOffset": 79}, {"referenceID": 6, "context": ", 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015).", "startOffset": 106, "endOffset": 127}, {"referenceID": 20, "context": "Machine Translation is the phrase-based MT system described in (Ritter et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "MT features include commonly used ones in Moses (Koehn et al., 2007), e.", "startOffset": 48, "endOffset": 68}, {"referenceID": 20, "context": ", Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset.", "startOffset": 62, "endOffset": 84}, {"referenceID": 10, "context": "MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood translation probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models.", "startOffset": 49, "endOffset": 243}, {"referenceID": 23, "context": "ditional features to score [1-4]-gram matches between context and response and between message and context (context and message match CMM features) are also employed, as in Sordoni et al. (2015). MT+neural reranking achieves a BLEU score of 4.", "startOffset": 173, "endOffset": 195}], "year": 2015, "abstractText": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don\u2019t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}