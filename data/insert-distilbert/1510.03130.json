{"id": "1510.03130", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2015", "title": "On Correcting Inputs: Inverse Optimization for Online Structured Prediction", "abstract": "algorithm engine designers typically assume that evaluating the input data is correct, and then proceed to find \" optimal \" or \" sub - optimal \" replacement solutions using this input data. however this assumption of truly correct data design does not always hold in practice, especially in the context problem of online learning systems where the objective is to learn appropriate feature weights often given some training samples. such scenarios necessitate the study of inverse optimization problems where one is given an input instance as well as a desired output and the task is to adjust the input data so that the given output is indeed optimal. motivated by learning structured prediction models, in this paper we consider inverse minimum optimization with a margin, i. e., we require the given output to be known better than all other feasible outputs by a desired margin. we consider such inverse level optimization problems for maximum weight matroid basis, matroid intersection, perfect matchings, minimum cost maximum flows, and shortest paths and derive the first known results for such problems with a non - zero margin. the effectiveness of these algorithmic approaches to online learning for structured prediction is also discussed.", "histories": [["v1", "Mon, 12 Oct 2015 03:33:47 GMT  (78kb,D)", "http://arxiv.org/abs/1510.03130v1", "Conference version to appear in FSTTCS, 2015"]], "COMMENTS": "Conference version to appear in FSTTCS, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hal daum\\'e iii", "samir khuller", "manish purohit", "gregory sanders"], "accepted": false, "id": "1510.03130"}, "pdf": {"name": "1510.03130.pdf", "metadata": {"source": "CRF", "title": "On Correcting Inputs: Inverse Optimization for Online Structured Prediction", "authors": ["Samir Khuller", "Manish Purohit", "Gregory Sanders"], "emails": ["gsanders}@cs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "Algorithm designers generally assume that the input data is sacrosanct and correct. Algorithms are then typically run on this input data to compute \u201coptimal\u201d or \u201csub-optimal\u201d solutions quickly whether it be the computation of a maximum spanning tree, a maximum matching, max weight arborescence, or shortest paths. However, with an increasing reliance on automatic methods to collect data, as well as in systems that learn, this assumption does not always hold. The input data can be erroneous (even though it may be approximately correct), and it becomes important to \u201cadjust\u201d the input data to achieve certain desired conditions.\nA simple example can be used to illustrate the main point \u2013 suppose we are given a weighted graph G = (V,E) and a spanning tree T , and told that T should be a maximum weight spanning tree in G. The goal now is to perturb the edge weights of the graph G, minimizing the L2 norm of the perturbation, so that T is indeed the optimal spanning tree. This kind of problem has been\n? Partially supported by NSF grant IIS-1451430.\nar X\niv :1\n51 0.\n03 13\n0v 1\n[ cs\n.L G\n] 1\n2 O\nct 2\nstudied previously in the form of \u201cInverse Optimization\u201d problems. However, we wish to accomplish a stronger goal of making sure that the given tree T is better than every other tree in G by a given margin \u03b4.\nOur initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5]. For concreteness and ease of exposition, we now describe structured prediction in the context of predicting dependency parse trees for natural language sentences. Given an English sentence, its dependency parse is a rooted, directed tree that indicates the dependencies between different words in the sentence as shown in Figure 4. The input sentence can be represented as a complete, directed graph on the words of the sentence that is parameterized by features on the edges. Given a learned model (represented as a vector of parameters), the weight of an edge is computed as the inner product of its feature vector and the model. As linguistic constraints dictate that the required dependency parse must form a rooted, spanning arborescence of the graph, one can use off-the-shelf combinatorial algorithms [6, 7] to find the highest weight arborescence. The learning problem is thus to find a parameter vector such that once the edges are weighted by the inner products, running a combinatorial optimization algorithm would return the desired parse tree. At \u201ctraining time\u201d, we are given a sentence as well as its correct parse tree and the problem that we need to solve is exactly the inverse optimization problem - given the current model and the parse tree, say T , find the minimum perturbation to the model so that the combinatorial optimization algorithm would return T . It is well established in the learning theory literature that achieving a large margin solution enables better generalization [8]. We consider minimizing the L2 norm because of connections to prior work [9]. In particular, for applications in structured prediction, the convergence and error bounds (included in Section 6) require L2 norm minimization.\nIn our work we consider such inverse optimization problems with a margin in a general matroid setting. We consider both the problem of modifying the weights of the elements of a matroid, so that a given basis is a maximum weight\nbasis (with a given margin of \u03b4) and the considerably harder problem of matroid intersection where a given basis of two matroids should have weight higher (by at least \u03b4) than any other set of elements that is a basis in the two matroids. This framework captures two special cases which are useful for structured prediction - namely maximum weight bipartite matching (useful for language translation) and maximum weight arborescence (useful for sentence parsing). We also consider \u03b4-margin inverse optimization problems for a number of other classical combinatorial optimzation problems such as perfect matchings, minimum cost flows and shortest path trees. In addition, we present a generic framework for online learning for structured prediction using the corresponding inverse optimization problem as a subroutine and present convergence and error bounds on this framework."}, {"heading": "1.1 Related Work", "text": "Inverse optimization problems have been widely studied in the Operations Research literature. Most prior work however has focused on minimizing the L1 or L\u221e norms between the weight vectors and, more importantly, do not allow non-zero margin (\u03b4). Heuberger [10] provides an excellent survey of the diverse inverse optimization problems that have been tackled. Both the inverse matroid optimization [11] and matroid intersection [12] have previously been studied in the setting of minimizing the L1 norm and with zero margin. However, they use techniques that are specialized to minimizing the L1 norm of the perturbation and do not extend to minimizing the L2 norm. At the same time, these approaches to do not generalize to the general case of inverse optimization with non-zero margins.\nIn typical global models for structured prediction (for e.g. see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d. By treating the combinatorial problem as a black box, these methods lose the ability to precisely reason about how certain changes to the underlying parameter vector can affect the eventual output. The simplest approach to solving the online structured prediction problem is the structured perceptron [3]. On each example, the structured perceptron makes a prediction based on its current model. If this prediction is incorrect, the algorithm suffers unit loss and updates its parameters with a simple linear update that moves the predictor closer to the truth and further from the current best guess. While empirically successful in a number of problems, this particular update is relatively imprecise: there are typically an exponential number of possible outputs for any given input, and simply promoting the correct one and demoting the models\u2019 current prediction may do very little to move the model as far as it needs to go. An alternative approach is the large margin discriminative approach [8] that seeks to change the parameters as little as possible subject to the constraint that the true output has a higher score than all incorrect outputs. However, such an approach is often computationally infeasible for structured prediction as there are usually an exponential number of potential outputs. McDonald et al. [15] circumvent this infeasibility by using a k-best list of possible outputs and restrict the set of constraints to require that\nthe true output has a higher score than the incorrect outputs on the k-best list. This has been shown to be effective for small values of k on simple parsing tasks [15]. However, for more complex tasks, like machine translation, one needs more complicated update frameworks [16]. In this work we show that the large margin discriminative approach is applicable to a wide range of problems in structured prediction using techniques from inverse combinatorial optimization.\nIn the context of online prediction, the most related work to ours is that of Taskar et al. [4], who also consider structured prediction using inverse bipartite matchings. They define a loss function that measures, against a ground truth matching, the number of mispredicted edges in the found matching. This \u201cHamming distance\u201d style loss function nicely decomposes over the structure of the graph and thereby admits an efficient \u201closs augmented\u201d inference solution, in which correct edges are penalized during learning. (The idea is that if correct edges are penalized, but the model still produces the correct matching, then it has done so with a sufficiently large margin.) This idea only works in the case of decomposable loss functions, or the simpler 0-margin formulation. In comparison, our approach works both for decomposable loss functions as well as \u201czero/one loss\u201d over the entire structure. Furthermore, our approach generalizes to arbitrary matroid intersection problems and minimum cost flows and thus is applicable to a much wider range of structured prediction problems."}, {"heading": "1.2 Contribution and Techniques", "text": "A lot of prior work in the inverse optimization literature formulates the problem as a linear program and then uses strong duality conditions to find the new perturbed weights. However, such techniques cannot be extended to handle a non-zero margin that is required by the application to structured prediction. We formulate inverse optimization to minimize the L2 norm of the perturbations as a quadratic program and use problem specific optimality conditions to determine a concise set of linear constraints that are both necessary and sufficient to guarantee the required margin. In particular, one of the key ingredients is a set of polynomially many linear constraints that ensure that an appropriately defined auxiliary graph does not contain small directed cycles. We note that our formulations can easily be adapted to minimize the L1 norm of the perturbations by simply modifying the objective and using linear programming.\nWe obtain concise formulations for exactly solving \u03b4-margin inverse optimization problems for (i) maximum weight matroid basis, (ii) maximum weight basis in the intersection of two matroids, (iii) shortest s-t path, (iv) shortest path tree, (v) minimum cost maximum flow in a directed graph.\nWe also present convergence results for the generic online learning framework for structured prediction motivating our study.\nThe rest of the paper is organized as follows. In Section 2, we formally define \u03b4-margin inverse optimization. In Sections 3 and 4, we present our results on inverse optimization for matroids, and matroid intersections respectively. In Sections 5, Appendix A, and Appendix B, we discuss inverse optimization for perfect matchings in bipartite graphs, minimum cost flows, and shortest path\ntrees. In Section 6, we describe an online learning framework for structured prediction as an application and the proof of convergence and error bounds for this learning framework are presented in Appendix C. Experimental results for our learning model are presented in Appendix D showing significant improvement over previous techniques."}, {"heading": "2 Problem Description", "text": "As explained in the introduction, we require a given solution to be better than all other feasible solutions by a margin of \u03b4. We now formalize this notion of \u03b4-optimality.\nDefinition 1 (\u03b4-Optimality). For a maximization problem P , let F denote the set of feasible solutions, let w be the weight vector, c(w,A) denote the cost of feasible solution A under weights w, and let \u03b4 \u2265 0 be a scalar. A feasible solution S \u2208 F is called \u03b4-optimal under weights w if and only if\nc(w, S) \u2265 c(w, S\u2032) + \u03b4, \u2200S\u2032( 6= S) \u2208 F .\n\u03b4-optimality for minimization problems is defined similarly. All problems we consider in this work can be classified as \u03b4-margin inverse optimization.\nDefinition 2 (\u03b4-Margin Inverse Optimization). For a given optimization problem P , let F denote the set of feasible solutions, let w be the weight vector, let \u03b4 \u2265 0 be a scalar, and let S \u2208 F be a given feasible solution. \u03b4-Margin Inverse optimization is to find a new weight vector w\u2032 minimizing ||w\u2032\u2212w||2 (L2 norm) such that S is the \u03b4-optimal solution of P under weights w\u2032.\nIn the following sections we consider \u03b4-margin inverse optimization for a number of problems mentioned earlier."}, {"heading": "3 Maximum weight matroid basis", "text": "In order to provide intuition about the type of problems we propose to solve in this paper, we first begin with the simple case of Inverse Matroid Optimization. We recall the definition of a matroid.\nDefinition 3 (Matroid). A matroid is a pair M = (X, I) where X is a ground set of elements and I is a family of subsets of X (called Independent sets) such that - (i) I 6= \u03c6 (ii) (Hereditary) If B \u2208 I, and A \u2286 B, then A \u2208 I. (iii) (Exchange property) If A,B \u2208 I, and |A| < |B|, then there exists some element e \u2208 B \\A such that A \u222a {e} \u2208 I.\nDefinition 4 (Matroid Basis and Circuit). Let M = (X, I) be a matroid. Then any maximal independent set in I is called a basis of the matroid. Conversely, any minimal dependent set is called a circuit.\nFor the inverse problem we are given a matroid M = (X, I), a weight function w on the elements, and a basis B of M. The goal is to find a weight function w\u2032 so that B is the \u03b4-optimal basis of M under the new weights. As it is well known that a spanning tree is a basis of a graphical matroid, this inverse matroid optimization problem directly generalizes the inverse maximum spanning tree problem.\nWe first state a simple optimality condition for a given basis B of a matroid M. An easy generalization of [17] for \u03b4 \u2265 0 gives the following lemma.\nLemma 1. A given basis B of a matroid M is \u03b4-optimal (under weight function w) if and only if for any f /\u2208 B, and each e \u2208 CB(f), w(e) \u2212 w(f) \u2265 \u03b4, where CB(f) denotes the unique circuit in B \u222a {f}.\nWe thus have a set of polynomially many linear constraints that are necessary and sufficient for the given basis B to be \u03b4-optimal. The inverse matroid optimization problem can then be formulated as a linearly constrained quadratic problem as follows -\nmin w\u2032 \u2211 e\u2208X (w\u2032(e)\u2212 w(e))2 subj. to: (1) w\u2032(e)\u2212 w\u2032(f) \u2265 \u03b4, \u2200f /\u2208 B, \u2200e \u2208 CB(f) (2)\nSuch a program with a quadratic objective and linear constraints can be solved in polynomial time and a number of practical solvers such as [18] are available."}, {"heading": "4 Matroid Intersection", "text": "Similar to the case with a single matroid, we need to derive a necessary and sufficient condition for a common basis B of two matroids to be \u03b4-optimal. We can establish such an optimality condition with the help of an exchange graph associated with the basis B and matroids M1 and M2.\nDefinition 5 (Exchange Graph). Given two matroids M1 = (X, I1) and M2 = (X, I2), a weight function w : X \u2192 R+, and a common basis B, an exchange graph is a directed, bipartite graph G = (V,A) with a length function l on edges that is defined as follows.\nV = B \u222aX \\B (3) A = A1 \u222aA2 (4) A1 = {(x, y)|x \u2208 B, y \u2208 X \\B,B \u2212 {x}+ {y} \u2208 I1} (5) A2 = {(y, x)|x \u2208 B, y \u2208 X \\B,B \u2212 {x}+ {y} \u2208 I2} (6)\nl(s) = { w(x) if s = (x, y) \u2208 A1 \u2212w(y) if s = (y, x) \u2208 A2\n(7)\nThe above graph captures the exchange operations that can be performed. An edge (e, f) implies that deleting e and adding f toB preserves independence w.r.t matroid M1 and similarly for the other direction. As the graph is bipartite, every cycle is of even length - a cycle C = (x1, y1, x2, y2, . . . xk, yk, x1) corresponds to constructing a set B\u2032\u2032 = B \u2212 {x1, x2, . . . xk} \u222a {y1, y2, . . . , yk}. Further\nw(B\u2032\u2032) = w(B)\u2212 k\u2211 i=1 w(xi) + k\u2211 i=1 w(yi) = w(B)\u2212 l(C)\nwhere l(C) = \u2211 e\u2208C l(e) is the sum of lengths of edges in the cycle C. We are now in a position to present the \u03b4-optimality condition of B in terms of the exchange graph. Fujishige [19] shows the following lemma for the case of \u03b4 = 0. We include the extended proof for general \u03b4 margin here for completeness. It is important to note that while there are other optimality conditions for matroid intersection such as the weight decomposition theorem by Frank [20], these conditions do not easily generalize for non-zero \u03b4.\nLemma 2 (Matroid Intersection \u03b4-optimality condition). The given common basis B is \u03b4-optimal if and only if the exchange graph G contains no directed cycle C such that \u2211 e\u2208C l(e) \u2264 \u03b4.\nProof. We\u2019ll refer to two well-known lemmas [17] regarding the relationship between bases of a matroid and matchings in the exchange graph. Let G1 = (V,A1) and G2 = (V,A2) be the subgraphs of G induced by the two matroids respectively. Further for B\u2032 \u2282 X, let G(B,B\u2032) denote the subgraph induced on the G by the vertex sets B \\B\u2032 and B\u2032 \\B.\nLemma 3. If B\u2032 is a basis of matroid M1 [M2], then G1(B,B\u2032)[G2(B,B\u2032)] contains a perfect matching. ut\nLemma 4. For B\u2032 \u2286 X, if G1(B,B\u2032)[G2(B,B\u2032)] has a unique perfect matching, then B\u2032 is a basis of M1 [M2]. ut\nSufficiency: This is the easy direction. Let B\u2032 be any common basis other than B. Applying Lemma 3, we know that G(B,B\u2032) has two perfect matchings (one each in G1(B,B \u2032) and G2(B,B \u2032)). Union of these two perfect matchings yields a collection of cycles C. Further, by construction, by traversing these cycles, one can transform B \u2192 B\u2032 and hence, we have w(B\u2032) = w(B) \u2212 \u2211 C\u2208C l(C). Therefore, since we have l(C) > \u03b4 for all cycles, we are guaranteed that w(B\u2032) < w(B)\u2212 \u03b4 as desired.\nNecessity: Ideally, we would like to say that every cycle in G leads to a swapping such that the set so obtained is also independent in both the matroids. This would immediately imply that a cycle of small length would lead to a common basis B\u2032 which is not much smaller than B.\nHowever, the presence of a cycle simply implies the presence of a perfect matching (one in each direction) which may not be unique. For example, Figure 2 shows an instance of an arborescence problem (left), and the associated exchange\ngraph (right). Here G contains a cycle a-x-b-y-a which leads to a new set x, y, c which is not an arborescence.\nIn the previous example, observe that if the cycle a-x-b-y-a were to have small weight, that would imply that at least one of a-y-a or b-x-b cycles too has small weight both of which lead to a feasible solution. This observation motivates us to look at the smallest cycle of weight less than \u03b4 and hope that it does induce an unique perfect matching.\nSuppose that the graph has a cycle having weight less than \u03b4. Let C be the smallest (in terms of number of arcs) such cycle. Look at the graph induced by the vertex set of the cycle. We claim that this induced subgraph has a unique perfect matching (one in each direction). Here we prove the claim for one direction. C being an even cycle trivially contains a perfect matching M from B-side to X\\Bside. Suppose there exists another perfect matching M \u2032. For every edge (x, y) in M \u2032 \\M , the edge along with the path between y and x in C cause a cycle. Further, each such cycle is smaller (number of edges) than C.\nLet M\u0304 denote the matching M with edge directions reversed. The union of M \u2032 and M\u0304 now forms a collection of cycles. Consider any such cycle D. WLOG let the cycle be (x0, y0, x1, y1, . . . , xk, yk, x0) such that the (xi+1, yi) are edges in M (i.e. (yi, xi+1) \u2208 M\u0304) and (xi, yi) \u2208 M \u2032. [All arithmetic is modulo k + 1]. We\u2019ll now be interested in the length of the path between these vertices in the original cycle C. Let Ci denote the cycle formed by the edge (xi, yi) and the path between yi and xi in C. We have,\nl(Ci) = l(C)\u2212 l(Path from xi to yi in C) + l((xi, yi))\nSince (xi, yi\u22121) \u2208M ,\nl(Path from xi to yi in C) = l((xi, yi\u22121)) + l(Path from yi\u22121 to yi in C)\nFurther since by construction l((x, yi)) = l((x, yj))(= \u00b1w(x)), we have\nl(Ci) = l(C)\u2212 l(Path from yi\u22121 to yi in C)\nLet Pi\u22121\u2192i denote this path. Summing over all (xi, yi) edges in D, we get\nk\u2211 i=0 l(Ci) = kl(C)\u2212 (l(Pk\u21920) + l(P0\u21921) + . . .+ l(Pk\u22121\u2192k))\n= kl(C)\u2212 k\u2032l(C)\n\u2191 Since we start from yk, go around the C and reach yk back\n= k\u2032\u2032l(C) < k\u2032\u2032\u03b4\nThe sum of k weights is less than k\u2032\u2032\u03b4 with k\u2032\u2032 < k, which implies\n\u2203Ci, such that l(Ci) < \u03b4\nBut this is a contradiction since C was the smallest cycle having weight less than \u03b4. Hence, the perfect matching M is unique. Similarly, the perfect matching induced by C in the other direction too is unique. Applying Lemma 4 successively on both sides, we know that B\u2032 obtained by exchanging as per C is a common basis for both matroids. Further, we have\nw(B\u2032) = w(B)\u2212 l(C) w(B\u2032) > w(B)\u2212 \u03b4\nHence we have proved that if G has a cycle with small weight, then B is not \u03b4-optimal, thus proving the necessity of the claim."}, {"heading": "4.1 Lower bounding cycles", "text": "In order to use Lemma 2 to solve the inverse matroid intersection problem efficiently using quadratic programming, we need a way to formulate this condition as a polynomial number of linear constraints. We now explore a technique to express the condition that a given graph has no small (of length less than \u03b4) cycles concisely. Say we are given a directed graph G = (V,A) and our task is to assign edge-lengths so that all cycles in G have weight at least \u03b4. Letting the edge-lengths to be variables, the feasible region in this case is unbounded and is defined by a constraint for every cycle in G, i.e. we have the region R1 in m dimensions defined by -\nR1 :\u2211 e\u2208C le \u2265 \u03b4 For all cycles C (8)\nOf course, this formulation has an exponential number of constraints. Although the ellipsoid algorithm can be used to solve the quadratic program in polynomial time, it is often too slow for practical use. We now show that we can obtain a concise extended formulation by adding a few extra variables.\nSuppose we have variables dxy representing the shortest distance between vertices x and y. In this case, the graph has no cycle of weight less than \u03b4 if and only if dxx \u2265 \u03b4 for all vertices x (assume dxx = \u221e, if x is not in any cycle).\nConsider the region R2 in m+ n 2 dimensions.\nR2 :\ndxy \u2264 l(xy) For all (x, y) \u2208 A (9) dxz \u2264 dxy + l(yz) For all x, z \u2208 V and y s.t. (y, z) \u2208 A (10) dxx \u2265 \u03b4 For all x \u2208 V (11)\nConstraints (9) and (10) enforce triangle inequality, and (11) enforce the condition that all cycles are large. We now prove that optimizing any function of l over R1 is equivalent to optimizing the same over R2.\nLemma 5. R1 is identical to the projection of R2 on the m dimensions corresponding to the edge-lengths.\nProof. R1 \u2286 Projection(R2): Let l : E \u2192 R denote a point in R1. Since the constraints (9) and (10) are always valid for a true distance function, let d : V\u00d7V \u2192 R denote the actual distance function in the graph induced by l. Such a d definitely satisfies constraints (9) and (10). Additionally, for all vertices x belonging to some cycle, since all cycles under l have weight at least \u03b4, we have dxx \u2265 \u03b4. For a vertex x which does not belong to any cycle, one can simply set dxx =\u221e.\nProjection(R2) \u2286 R1: Consider a point in R2. We now have the lengths of edges le as well as some dxy values. Consider any cycle C = (x1, x2, . . . , xk, x1) in the graph. Applying constraint (10) repeatedly we get\ndx1x1 \u2264 l(x1x2) + l(x2x3) + . . .+ l(xk\u22121xk) + l(xkx1) (12)\nand also by constraint (11), we have\ndx1x1 \u2265 \u03b4 (13) Hence we have, l(x1x2) + l(x2x3) + . . . + l(xk\u22121xk) + l(xkx1) \u2265 \u03b4, i.e. \u2211 e\u2208C le \u2265 \u03b4 which means that the le values are feasible in R1.\nHence, optimizing any function of the le variables over R1 is equivalent to optimizing it over R2. However, R2 has only m+mn+n constraints and n\n2 +m variables."}, {"heading": "4.2 Putting it together", "text": "Lemmas 2 and 5 suggest a way to solve the \u03b4-margin inverse matroid intersection problem. As per the requirements of Lemma 2, given the two matroids and the common basis B, construct the exchange graph G = (V,A = A1 \u222a A2). Let w : X \u2192 R+ be the original weight function and let w\u2032 be the new weight function which we desire. If l is the arc lengths ofG, according to the construction of Lemma 2, lxy = w\n\u2032(x) and lyx = \u2212w\u2032(y) where x \u2208 B, y \u2208 S \\ B. Further, the objective that we minimize is the L2 norm of w\u2212w\u2032. We can now add these\nadditional constraints and the objective to the region R2 as per Lemma 5 to obtain the minimum change on the weights of elements so that the exchange graph has no small cycles and hence B is \u03b4\u2212optimal.\nmin w\u2032 \u2211 e\u2208X (w\u2032(e)\u2212 w(e))2 subj. to: (14) lxy = w \u2032(x), \u2200(x, y) \u2208 A1 (15) lyx = \u2212w\u2032(y), \u2200(y, x) \u2208 A2 (16) dxy \u2264 lxy, \u2200(x, y) \u2208 A (17) dxz \u2264 dxy + lyz, \u2200x, z \u2208 V,\u2200(y, z) \u2208 A (18) dxx \u2265 \u03b4, \u2200x \u2208 V (19)"}, {"heading": "4.3 Maximum Weight Arborescence", "text": "Given a directed graph, a r-arborescence (also known as a branching) is the directed analogue of a spanning tree and is defined as a set of edges T spanning all vertices such that every vertex (except r) has exactly one incoming edge in T . It is well known that an arborescence in a directed graph is a basis in the intersection of a graphical matroid and a partition matroid. We analyze the complexity of the above technique for the special case of maximum weight arborescence. Let G denote the graph in question having n vertices and m edges.\nThe exchange graph Gex has a vertex for every edge of G, i.e., nex = m. The bipartition of Gex is such that we have components of size n and m \u2212 n respectively. Hence we have mex = O(mn). As seen in Section 4.1, we use O(n 2 ex) variables and O(mexnex) contraints. Thus, putting it all together, we have a quadratic program with O(m2) variables and O(m2n) constraints.\nThe inverse maximum weight arborescence problem is important as it can used as a subroutine in the online learning for dependency parsing [21]. The dependency parse tree of a sentence can be represented as an arborescence over a graph consisting of every word in the sentence as a node. In Appendix D, we show experimental results for dependency parsing using our framework.\nShortest s\u2212t paths. Given a weighted graph G = (V,E,w), a path P between terminals s and t, and a margin \u03b4, the inverse shortest s-t path problem is to find a minimum perturbation to w (minimizing the L2 norm) so that P is shorter than all other paths between s and t by at least \u03b4 under the new weight function. As shown by [22], the inverse shortest s-t path problem can be reduced to the inverse arborescence problem. Let G\u2032 be G augmented by adding zero weight edges from t to all other vertices. It can be easily observed that P is the shortest s-t path in G if and only if P and a subset of the zero weight edges form the minimum weight s-arborescence of G\u2032. Thus we can use an algorithm for inverse minimum weight arborescence to solve the inverse shortest path problem.?\n? Inverse minimum weight arborescence problem can be solved similar to the inverse maximum weight arborescence problem"}, {"heading": "5 Perfect Matchings in Bipartite Graphs", "text": "For the bipartite maximum weight perfect matching inverse problem, the previous technique yields a quadratic program having O(m2) variables and O(m2) constraints as the exchange graph is sparse. In this section we show that we can in fact obtain more concise formulations. Recall that for a given edge weighted, bipartite graph G = (X \u222a Y,E,w), and a perfect matching M , an alternating cycle is a cycle in G in which edges alternate between those that belong to M and those that do not. An alternating cycle C is called \u03b4-augmenting, if\u2211 e\u2208C\u2229M w(e) < \u2211 e\u2208C\\M w(e)+\u03b4. The following characterization of a \u03b4-optimal perfect matching is well known.\nLemma 6. A perfect matching M is \u03b4-optimal if and only if the graph contains no \u03b4-augmenting cycles.\nThe central idea is to construct a directed graph H on just the nodes of X such that any directed cycle in H will correspond to an alternating cycle in G (w.r.t to the matching M) and vice versa. We construct H = (X,A) to be a directed graph such that (x, z) \u2208 A if and only if \u2203y \u2208 Y such that (x, y) \u2208 M and (y, z) \u2208 E; further let l(x, z) = w(x, y)\u2212w(y, z). Figure 3 shows an example of this construction.\nProposition 1. The auxiliary graph H has a directed cycle of length less than \u03b4 if and only if G has a \u03b4-augmenting alternating cycle.\nProof. If: Let C = (x0, y0, x1, y1, . . . , xk, yk, x0) be a \u03b4-augmenting cycle in G where all (xi, yi) \u2208 M . By construction, H has a cycle C \u2032 = (x0, x1, . . . , xk, x0) and l(C \u2032) = \u2211k i=0(w(xi, yi) \u2212 w(yi, xi+1)) (modulo k + 1) = \u2211 e\u2208C\u2229M w(e) \u2212\u2211\ne\u2208C\\M w(e) < \u03b4.\nOnly If: Let C = (x0, x1, . . . , xk, x0) be a cycle in H with l(C) < \u03b4. By construction, \u2203 cycle C \u2032 = (x0, y0, x1, y1, . . . , xk, yk, x0) in G. Now, l(C) =\n\u2211k i=0(w(xi, yi)\u2212w(yi, xi+1)) (modulo k+1) = \u2211 e\u2208C\u2032\u2229M w(e)\u2212 \u2211 e\u2208C\u2032\\M w(e). Thus C \u2032 is a \u03b4-augmenting cycle in G.\nUsing Lemma 6 and Proposition 1 along with Lemma 5, we can formulate the inverse perfect matching problem as a quadratic program having O(n2) variables and O(mn) constraints."}, {"heading": "6 Application : Online learning for structured prediction", "text": "In this section, we present a framework for online learning using inverse combinatorial optimization. The structured prediction task is to predict a discrete combinatorial structure (such as an arborescence) given a structured input (such as a graph). The learning task is to learn model parameters so that solving a combinatorial optimization problem on the input instance would return the desired output structure. Structured prediction is extensively used in natural language processing tasks such as obtaining parse trees of a sentence, or automatic language translation.\nIn the online learning setting, we are presented with a set of T training samples. These consist of an input xt (for instance, a sentence) and an output yt (for instance, a syntactic analysis of this sentence described as an arborescence on a graph over the words in the sentence [23, 21]). Each edge in this graph is parameterized by a set of F features that, for instance, indicate how likely one word is to be the subject of another. Thus, each training sample is a pair (xt, yt) where xt is a graph parameterized by features on edges, and yt is the desired output sub-structure (such as a spanning tree, or an arborescence, or a matching depending on the application). The task is to learn a vector (of length F ) of parameters \u03b8 such that when edge weights are computed as inner products between the \u03b8 and the edge\u2019s features, the output obtained by computing an optimal sub-structure (spanning tree, etc.) is the desired output with some margin.\nAlgorithm 1 describes the generic online learning framework for structured predcition. It is parameterized by an user-defined loss function `(yt, y\u0302) that specifies the loss incurred by the prediction y\u0302 with respect to the training solution yt. Algorithm 1 is an adaptation of the Passive-Aggressive MIRA algorithm [24] for structured prediction.\nNote that the minimization problem solved for each training sample is exactly \u03b4-inverse optimization where we minimize the perturbations to the feature parameters instead of the edge weights. In this framework, the different inverse optimization problems we considered have applications for different structured predictions. For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].\nSince we have shown that we can efficiently solve the inverse optimization problems for a variety of combinatorial structures, we can extend the error bounds of the MIRA algorithm [24] to work for learning the corresponding\n\u03b81 = 0 for t = 1 to T do\nObtain training example xt, yt w \u2190 weight function s.t. w(e) = \u03b8t \u00b7 fe where fe is feature vector of edge e y\u0302 \u2190 optimal sub-structure for graph xt under weights w Suffer loss \u03b4t = `(yt, y\u0302) Update \u03b8t+1 = argmin\u03b8\u2032 ||\u03b8\u2032 \u2212 \u03b8t||22 such that\nw\u2032 \u2190 weight function s.t. w\u2032(e) = \u03b8\u2032 \u00b7 fe where fe is feature vector of edge e\nyt is the \u03b4t-optimal sub-structure for graph xt under weights w \u2032\nend Return \u03b8T+1\nAlgorithm 1: Generic online learning framework\nstructured prediction models. In this section, we present both convergence results and loss bounds for our generic online learning framework. The proofs for these bounds closely follow those in Crammer\u2019s Ph.D. dissertation [24] and are relegated to Appendix C for clarity and brevity.\nThe statement of the convergence result depends on a set of dual variables obtained from the optimization problem in the \u201cUpdate\u201d step of Algorithm 1. This implicitly encodes constraints over all possible outputs; we denote the dual variable for output y on the tth example by \u03b1ty. We can show that the cumulative sum of these dual variables is bounded by a constant independent of T , which implies convergence of the learning algorithm.\nTheorem 1 (Convergence). Let {(xt, yt)}Tt=1 be a sequence of structured examples. Let \u03b8\u2217 be any vector that separates the data with a positive margin \u03b4\u2217 > 0. Assume the loss function is upper bounded: `(yt, y\u0302) \u2264 A. Then the cumulative sum of coefficients is upper bounded by:\nT\u2211 t=1 \u2211 y\u2208Yt \u03b1ty \u2264 2A ( ||\u03b8\u2217|| \u03b4\u2217 )2 . (20)\nHowever, it is not enough to show that the algorithm converges: it could converge to a useless solution! We wish to show that in the process of learning it does not make too many errors. In particular, we show that Algorithm 1 incurs a total hinge loss bounded by a constant also independent of T , which implies that at some point it has exactly solved the learning problem.\nTheorem 2 (Total Loss). Under the same assumptions as above, assume further that the norm of the examples are bounded by R. Then, the cumulative hinge loss (H\u03b4t) suffered by the algorithm over T trials is bounded by:\nT\u2211 t=1 H\u03b4t(\u03b8t, (xt, yt)) \u2264 8A ( R ||\u03b8\u2217|| \u03b4\u2217 )2 . (21)"}, {"heading": "A Minimum Cost Maximum Flow", "text": "Certain problems in structured prediction can be solved using inverse minimum cost maximum flow problems. Consider, for example, the task of assigning reviewers to papers. Suppose we would like to have R reviewers per paper, and at most P papers assigned per reviewer. Such a scenario can be easily modelled as a generalization of bipartite matching with non-unit supply and demands using a minimum cost maximum flow problem. In order to learn the weights of such an instance (suitability of a reviewer for a paper), we can use structured prediction methods that require solving inverse minimum cost maximum flow problems. Formally,\nDefinition 6 (Inverse Min Cost Max Flow Problem). Given a directed graph G = (V,A), a capacity function on edges c : E \u2192 R+, a cost function w : E \u2192 R, and a feasible maximum flow f : E \u2192 R+, the inverse minimum cost maximum flow problem is to find a new cost function w\u2032 such that ||w\u2212w\u2032||2 (L2 norm) is minimized and f is \u03b4-optimal under w \u2032.\nUsing Lemma 5, we can easily formulate the inverse min cost max flow problem as a convex optimization problem if we can characterize a minimum cost flow in terms of small cycles in an auxiliary graph. Indeed, the following lemma is well-known.\nLemma 7. Given an instance of a minimum cost maximum flow problem G = (V,E, c, w), and a maximum flow f , f is \u03b4-optimal if and only if the residual graph Gf has no cycle having weight less than \u03b4."}, {"heading": "B Shortest Path Trees", "text": "Suppose we are given a directed graph G = (V,E) with a weight function w on edges, a subtree Tspt rooted at r, and a margin \u03b4. The inverse shortest path tree problem is then to minimally modify the edge weights so that Tspt becomes the \u03b4-optimal shortest path tree, i.e., for every vertex v(6= r) in G, the path prescribed by Tspt is the \u03b4-optimal shortest path from r to v.\nTo solve this problem we define variables dv representing distance labels for each vertex and generate the following quadratic program.\nmin w\u2032 \u2211 e\u2208E (w\u2032(e)\u2212 w(e))2 subj. to: (22)\ndr = 0 (23) da + w \u2032(a, b) = db \u2200e = (a, b) \u2208 Tspt (24) da + w \u2032(a, b) \u2265 db + \u03b4 \u2200e = (a, b) /\u2208 Tspt (25)\nIn other words, our claim is that we can require the distance labels to be the length of the unique path in the shortest path tree. For every other edge e = (a, b), we would like the path to b coming via a to be longer by at least \u03b4. We now prove that these conditions are necessary and sufficient.\nProof. Sufficient: Suppose we find a solution to the convex program. The length of the path from r to v via Tspt is exactly dv due to the equality constraints. Consider any other path P \u2032(r, v) = [r = v0, v1, . . . , vk = v] from r to v. The\nlength of this path is exactly \u2211k\u22121 i=0 w \u2032(vi, vi+1) \u2265 \u2211k\u22121 i=0 (dvi+1 \u2212 dvi + \u03b4\u2032) where \u03b4\u2032 = 0 if the edge (vi, vi+1) belongs to Tspt and \u03b4 otherwise. Adding, we see that this is at least dv + \u03b4 since at least one edge on P\n\u2032 does not belong to Tspt. Necessary: Let w\u2032 be a feasible weight function, i.e., Tspt is \u03b4-optimal under w\u2032. Let dv denote the distance of vertex v from r under weights w \u2032. Since Tspt forms a shortest path tree, its easy to see that constraints (23) and (24) are satisfied. Now suppose that an edge e\u2032 = (a, b) /\u2208 Tspt does not satisfy constraint (25). Then we have db + \u03b4 > da + w\n\u2032(a, b). Since there is a path from r to a of length da, we now have a path to b of length shorter than db + \u03b4 which violates \u03b4-optimality. Hence, we have a contradiction."}, {"heading": "C Proofs of Learning Theory Results", "text": "Theorem 1 (Convergence). Let {(xt, yt)}Tt=1 be a sequence of structured examples. Let \u03b8\u2217 be any vector that separates the data with a positive margin \u03b4\u2217 > 0. Assume the loss function is upper bounded: `(yt, y\u0302) \u2264 A. Then the cumulative sum of coefficients is upper bounded by:\nT\u2211 t=1 \u2211 y\u2208Yt \u03b1ty \u2264 2A ( ||\u03b8\u2217|| \u03b4\u2217 )2 . (20)\nProof. Let Lt(\u03b1) denote the lagrangian dual of the optimization problem solved in the update step of Algorithm 1. We have\nLt(\u03b1) = \u22121 2 || \u2211 y\u2208Yt \u03b1ty\u03c6 t \u2206y||2 + \u2211 y\u2208Yt \u03b1ty(\u03b4t \u2212 \u03b8t\u03c6t\u2206y) (26)\nHere \u03c6t\u2206y = \u03c6(xt, yt)\u2212\u03c6(xt, y) is shorthand for the difference in feature vectors, while \u03b4t is the specified margin which is taken to be the current loss incurred. Define \u2206t = ||\u03b8t \u2212 \u03b8\u2217||2 \u2212 ||\u03b8t+1 \u2212 \u03b8\u2217||2. We will establish a bound on the cumulative sum of the dual coeffecients by bounding the sum of \u2206ts above and below. Upper bounding:\nT\u2211 t=1 \u2206t = ||\u03b81 \u2212 \u03b8\u2217||2 \u2212 ||\u03b8T+1 \u2212 \u03b8\u2217||2 (27)\n= ||\u03b8\u2217||2 \u2212 ||\u03b8T+1 \u2212 \u03b8\u2217||2 (28) \u2264 ||\u03b8\u2217||2 (29)\nEquation 28 is obtained by substituting \u03b81 = 0\nLower bounding:\n\u2206t = ||\u03b8t \u2212 \u03b8\u2217||2 \u2212 ||\u03b8t+1 \u2212 \u03b8\u2217||2 (30)\n= ||\u03b8t \u2212 \u03b8\u2217||2 \u2212 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u03b8t +\u2211\ny\n\u03b1ty\u03c6 t \u2206y \u2212 \u03b8\n\u2217 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2\n(31)\n= ||\u03b8t \u2212 \u03b8\u2217||2 \u2212 [ ||\u03b8t \u2212 \u03b8\u2217||2 + \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2211 y \u03b1ty\u03c6 t \u2206y \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2\n+ 2(\u03b8t \u2212 \u03b8\u2217) \u00b7 \u2211 y \u03b1ty\u03c6 t \u2206y\n] (32)\n= \u2212 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2211 y \u03b1ty\u03c6 t \u2206y \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \u2212 2\u03b8t \u00b7 \u2211 y \u03b1ty\u03c6 t \u2206y + 2\u03b8 \u2217 \u00b7 \u2211 y \u03b1ty\u03c6 t \u2206y\n(33)\nSubstituting for Lt(\u03b1) from Eq. (26), we get\n\u2206t = 2\n[ Lt(\u03b1) +\n\u2211 y \u03b8\u2217 \u00b7 \u03b1ty\u03c6 t \u2206y \u2212 \u2211 y \u03b1ty\u03b4t\n] (34)\nAs \u03b1 = 0 is dual feasible and Lt(0) = 0, we have Lt(\u03b1) \u2265 0\n\u2265 2 [\u2211 y \u03b8\u2217 \u00b7 \u03b1ty\u03c6 t \u2206y \u2212 \u2211 y \u03b1ty\u03b4t ] (35)\n\u2265 2 [\u2211 y \u03b1ty\u03b4 \u2217 \u2212 \u2211 y \u03b1ty\u03b4t ] (36)\n\u2265 2 \u2211 y \u03b1ty(\u03b4 \u2217 \u2212 \u03b4t) (37)\n\u2265 2(\u03b4\u2217 \u2212A) \u2211 y \u03b1ty (38)\nIn the last step, we used the bound on the instantaneous loss. In the previous, we used the assumption on the margin achieved by \u03b8\u2217. The rest is algebra.\nSumming over all t we get\nT\u2211 t=1 \u2206t \u2265 2(\u03b4\u2217 \u2212A) T\u2211 t=1 \u2211 y \u03b1ty (39)\nCombining the bounds in Equations (29) and (39)\n2(\u03b4\u2217 \u2212A) \u2211 t \u2211 y \u03b1ty \u2264 ||\u03b8 \u2217||2 (40)\nNow, fix c = 2A\u03b4\u2217 and scale \u03b8 \u2217 and \u03b4\u2217 by c. Rearrange to get the desired bound.\nLemma 8. Under the same assumptions as before, and writing \u03a6t\u2206} to denote a |Yt|\u00d7F matrix whose rows are the feature vectors for all possible outputs, and where p and q are dual (i.e., 1p + 1 q = 1), the optimal dual variables \u03b1 t y satisfy:\n\u03b4t \u2212 \u03b8t \u00b7 \u03c6t\u2206y \u2264 \u2223\u2223\u2223\u2223\u03b1t\u2223\u2223\u2223\u2223 p \u2223\u2223\u2223\u2223\u03a6t\u2206}\u03c6t\u2206y\u2223\u2223\u2223\u2223q (41) Proof. By the enforced \u03b4t - optimality conditions, we know that for all t and y \u2208 Yt -\n\u03b8t+1 \u00b7 \u03c6t\u2206y \u2265 \u03b4t (42)\nSubstituting for \u03b8t+1 in terms of \u03b8t using the dual optimality conditions\n(\u03b8t + \u2211 z\u2208Yt \u03b1tz\u03c6 t \u2206z ) \u00b7 \u03c6 t \u2206y \u2264 \u03b4t (43)\n\u03b4t \u2212 \u03b8t \u00b7 \u03c6t\u2206y \u2264 \u2211 z\u2208Yt \u03b1tz\u03c6 t \u2206z \u00b7 \u03c6 t \u2206y (44)\n= \u03b1t \u00b7\u03a6t\u2206}\u03c6 t \u2206y (45) \u2264 \u2223\u2223\u2223\u2223\u03b1t\u2223\u2223\u2223\u2223 p \u00d7 \u2223\u2223\u2223\u2223\u2223\u2223\u03a6t\u2206}\u03c6t\u2206y \u2223\u2223\u2223\u2223\u2223\u2223\nq (46)\nThe first equality is rewriting things in terms of the matrix \u03a6, the final step is Ho\u0308lder\u2019s inequality.\nTheorem 2 (Total Loss). Under the same assumptions as above, assume further that the norm of the examples are bounded by R. Then, the cumulative hinge loss (H\u03b4t) suffered by the algorithm over T trials is bounded by:\nT\u2211 t=1 H\u03b4t(\u03b8t, (xt, yt)) \u2264 8A ( R ||\u03b8\u2217|| \u03b4\u2217 )2 . (21)\nProof. On a round t with non-zero hinge loss, take the y in Eq (46) that has maximal hinge loss (numerator). Take p = 1 and q =\u221e. Then:\u2211\nt H\u03b4t(\u03b8t, (xt, yt)) \u2264 \u2211 t (\u03b4t \u2212 \u03b8t \u00b7 \u03c6t\u2206y ) (47)\n\u2264 \u2211 t \u2223\u2223\u2223\u2223\u03b1t\u2223\u2223\u2223\u2223 1 \u2223\u2223\u2223\u2223\u03a6t\u2206}\u03c6t\u2206y\u2223\u2223\u2223\u2223\u221e (48) = \u2211 t ( \u2211 z \u2223\u2223\u03b1tz\u2223\u2223)(max z \u03c6t\u2206z \u00b7 \u03c6 t \u2206y) (49)\nUsing Ho\u0308lder\u2019s inequality again with p = q = 2\n\u2264 \u2211 t ( \u2211 z \u2223\u2223\u03b1tz\u2223\u2223)(max z \u2223\u2223\u2223\u2223\u03c6t\u2206z\u2223\u2223\u2223\u22232 \u2223\u2223\u2223\u2223\u03c6t\u2206y\u2223\u2223\u2223\u22232) (50) By assumption the norm is bounded,\n\u2264 \u2211 t ( \u2211 z \u2223\u2223\u03b1tz\u2223\u2223)(max z (2R)(2R)) (51)\n\u2264 4R2 \u2211 t \u2211 z \u2223\u2223\u03b1tz\u2223\u2223 (52) Bounding the cumulative sum using Theorem 1, we get\n\u2264 4R22A ( ||\u03b8\u2217|| \u03b4\u2217 )2 (53)\n= 8A ( R ||\u03b8\u2217|| \u03b4\u2217 )2 (54)"}, {"heading": "D Experimental Analysis", "text": "We perform preliminary experiments to demonstrate the efficacy of the online learning framework. We consider two structured prediction tasks: dependency parsing and word alignment for language translation. In dependency parsing, the input is a sentence, and the goal is to find its dependence parse, i.e. evaluate how words in a sentence relate to one-another, forming a tree starting with an empty root node. Figure 4 shows an example of a dependency parse tree of a sentence. The input sentence is considered as a complete graph with a vertex for each word and each edge parameterized by features. The task now is to learn feature weights so that the maximum weight spanning tree in the graph corresponds to the parse tree of the sentence. In word alignment for language translation, we are given two equivalent sentences in two languages and the task is to identify corresponding words. The input instance in this case is considered to be a complete bipartite graph, and the output would be an assignment (matching). Once again given features on edges, the task is to learn feature weights so that the maximum weight perfect matching in the graph would correspond to the correct word alignment.\nD.1 Maximum Spanning Trees\nAlthough dependency parsing is better modelled by directed arborescences, for the sake of simplicity we consider only spanning trees in directed graphs in our experiments. For these experiments we used the CoNLL shared task English treebank [26] in order to predict undirected dependency arcs in English sentences. Each word only depends on one word, but can have many dependents.\nWe use a 1500-sentence subset of the training data (36k words) and the test data consists of 3800 sentences (90k words). We train for undirected unlabeled dependencies and evaluate in the same manner. We use standard features: words, word suffixes, position, edge length and predicted part of speech tags.\nAn averaged structured perceptron baseline obtains an accuracy of 82.7% on the test data. One-best MIRA achieves an accuracy of 84.2%, which is very close to the performance of a structured SVM trained by stochastic gradient descent (accuracy of 84.4%). Our approach achieves a significant improvement on this of 85.1%.\nD.2 Bipartite Matching\nWe are considering German/English alignment at the word level. Given two equivalent sentences in their respective languages, we want to choose an alignment that best fits the \u201cequality\u201d of the respective words in each sentence.\nOur data is comprised of 217 manually word-aligned sentences from [27], with many-to-many matchings possible. Since we are restricted to one-to-one alignment (matching) ,we enforce this restriction here by pruning extra edges until we have 1-to-1 matchings only.\nThe graph structure is as follows: Given a German sentence of length n and an English sentence of length m, we construct a complete bipartite graph\nG = (Xm, Yn, E), where Xm is the German sentence of length m. Yn is an English sentence of length n. E are the possible alignments for the two sentences, which are currently fully connected. Further, to ensure that the induced matching will be perfect(as required from our problem definition), we add n extra vertices(referred to as dummy vertices) to the German sentence and m dummy vertices to the English sentence, resulting in G\u2032 = (Xm \u222aX \u2032n, Yn \u222a Y \u2032m, E\u2032).\nNext we will describe the structure of E\u2032 in the graph G\u2032. Each vertex of the original G is fully connected with every other vertex in G as before. Each dummy vertex x\u2032i \u2208 X \u2032n is fully connected to each dummy node y\u2032j \u2208 Y \u2032m,\u2200j. In addition, each dummy vertex x\u2032i \u2208 X \u2032n is also connected to its single, respective real word vertex yi \u2208 Yn, and similarly Y \u2032m to Xm. If an alignment for a particular word in Xm or Yn is not present in Mg due to the nature of word alignment being somewhat sparse, we designate the truth edge to be the one connected to its corresponding dummy vertex and add this to Mg \u2282 E, giving us M \u2032g \u2282 E\u2032. This will allow us a perfect matching for any two sentences we are given, fitting this particular problem\u2019s framework.\nEach edge of the graph G\u2032 corresponds to a set of feature values for the corresponding words. These include features such as their Dice Coefficient (computed from Europarl corpus), relative word positioning in the sentence, string match without vowels, and others detailed in [25]. We also created slack features and weights, one for each viable edge in the graph. For example, the feature \u201c4 10\u201d means \u201cthis edge connects node 4 to node 10\u201d. These features are very small valued (the result being that making these features important is expensive), and are used to ensure feasibility during training time for each example presented. The edges corresponding to dummy vertices will only have these slack features, which will make certain this perfect matching problem is feasible. After each example is learned, we immediately forget these slack weights.\nResults One simple baseline we are testing against is the use of only DICE values. This is being used as a sanity-check for other algorithms\u2019 performance. This simple baseline obtained an accuracy of 36.4%. Averaged MIRA performs quite poorly on this dataset with these features, at 13.76%. In our experiments we are only comparing our algorithm with 1-best MIRA, which is unfair as it\nonly constructs the constraints based on the single best matching based on our old weight vector w. Limited to optimizing against one matching appears to be insufficient to learn anything of value. Again however, our algorithm supersedes any k chosen for MIRA, as we ensure our matching beats every single other possible matching. Averaged Perceptron performs better on this dataset, at 40.52%. The variance from run to run was very small compared to the other baselines. Our algorithm obtains an accuracy of 44.0%. This failure of 1-Best MIRA is analogous to a corresponding failure of structured SVMs optimized with cutting plane(30.42%) [5, 28]: when the problem is \u201chard\u201d (in the sense that \u03b8\u2217 has high loss), this approach appears to perform quite poorly in practice [29, 30]."}], "references": [{"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "The use of classifiers in sequential inference", "author": ["V. Punyakanok", "D. Roth"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning structured prediction models: A large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Optimum branchings", "author": ["J. Edmonds"], "venue": "Journal of Research of the National Bureau of Standards", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "On the shortest arborescence of a directed graph", "author": ["Y. Chu", "T. Liu"], "venue": "Science Sinica", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1965}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivenen", "M. Warmuth"], "venue": "In: Symposium on the Theory of Computing (STOC)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Inverse combinatorial optimization: A survey on problems, methods, and results", "author": ["C. Heuberger"], "venue": "Journal of Combinatorial Optimization", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "The base-matroid and inverse combinatorial optimization problems. Discrete applied mathematics", "author": ["M. Dell\u2019Amico", "F. Maffioli", "F. Malucelli"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Inverse matroid intersection problem", "author": ["C. Mao-Cheng", "Y. Li"], "venue": "Mathematical Methods of Operations Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Case-factor diagrams for structured probabilistic modeling", "author": ["D. McAllester", "M. Collins", "F. Pereira"], "venue": "Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Online passiveaggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Online large-margin training of dependency parsers", "author": ["R. McDonald", "K. Crammer", "F. Pereira"], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["D. Chiang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Combinatorial optimization: polyhedra and efficiency", "author": ["A. Schrijver"], "venue": "Volume 24. Springer Verlag", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A primal approach to the independent assignment problem", "author": ["S. Fujishige"], "venue": "Journal of the Operations Research Society of Japan", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1977}, {"title": "A weighted matroid intersection algorithm", "author": ["A. Frank"], "venue": "Journal of Algorithms", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1981}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R. McDonald", "F. Pereira", "K. Ribarov", "J. Hajic"], "venue": "Proceedings of the Joint Conference  on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "A strongly polynomial algorithm for the inverse shortest arborescence problem. Discrete applied mathematics", "author": ["H. Zhiquan", "L. Zhenhong"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "A statistical approach to parsing of czech", "author": ["D. Zeman"], "venue": "Prague Bulletin of Mathematical Linguistics", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Online Learning of Complex Categorical Problems", "author": ["K. Crammer"], "venue": "PhD thesis, Hebrew University of Jerusalem", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "A discriminative matching approach to word alignment", "author": ["B. Taskar", "S. Lacoste-Julien", "D. Klein"], "venue": "Proceedings of EMNLP 2005", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The conll 2007 shared task on dependency parsing", "author": ["J. Nivre", "J. Hall", "J.Nilsson", "A. Chanev", "G. Eryigit", "S. Kubler", "S. Marinov", "E. Marsi"], "venue": "Proceedings of CoNLL-2007", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["C. Callison-Burch", "C. Bannard"], "venue": "Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.N. Yu"], "venue": "Machine Learning Journal", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In: International Conference on Computer Vison (ICCV)", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 1, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 2, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 3, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 4, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 5, "context": "As linguistic constraints dictate that the required dependency parse must form a rooted, spanning arborescence of the graph, one can use off-the-shelf combinatorial algorithms [6, 7] to find the highest weight arborescence.", "startOffset": 176, "endOffset": 182}, {"referenceID": 6, "context": "As linguistic constraints dictate that the required dependency parse must form a rooted, spanning arborescence of the graph, one can use off-the-shelf combinatorial algorithms [6, 7] to find the highest weight arborescence.", "startOffset": 176, "endOffset": 182}, {"referenceID": 7, "context": "It is well established in the learning theory literature that achieving a large margin solution enables better generalization [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "We consider minimizing the L2 norm because of connections to prior work [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Heuberger [10] provides an excellent survey of the diverse inverse optimization problems that have been tackled.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Both the inverse matroid optimization [11] and matroid intersection [12] have previously been studied in the setting of minimizing the L1 norm and with zero margin.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Both the inverse matroid optimization [11] and matroid intersection [12] have previously been studied in the setting of minimizing the L1 norm and with zero margin.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 12, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 13, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 14, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "The simplest approach to solving the online structured prediction problem is the structured perceptron [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "An alternative approach is the large margin discriminative approach [8] that seeks to change the parameters as little as possible subject to the constraint that the true output has a higher score than all incorrect outputs.", "startOffset": 68, "endOffset": 71}, {"referenceID": 14, "context": "[15] circumvent this infeasibility by using a k-best list of possible outputs and restrict the set of constraints to require that", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "This has been shown to be effective for small values of k on simple parsing tasks [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "However, for more complex tasks, like machine translation, one needs more complicated update frameworks [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "[4], who also consider structured prediction using inverse bipartite matchings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "An easy generalization of [17] for \u03b4 \u2265 0 gives the following lemma.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "Fujishige [19] shows the following lemma for the case of \u03b4 = 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "It is important to note that while there are other optimality conditions for matroid intersection such as the weight decomposition theorem by Frank [20], these conditions do not easily generalize for non-zero \u03b4.", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "We\u2019ll refer to two well-known lemmas [17] regarding the relationship between bases of a matroid and matchings in the exchange graph.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "The inverse maximum weight arborescence problem is important as it can used as a subroutine in the online learning for dependency parsing [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "As shown by [22], the inverse shortest s-t path problem can be reduced to the inverse arborescence problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "These consist of an input xt (for instance, a sentence) and an output yt (for instance, a syntactic analysis of this sentence described as an arborescence on a graph over the words in the sentence [23, 21]).", "startOffset": 197, "endOffset": 205}, {"referenceID": 19, "context": "These consist of an input xt (for instance, a sentence) and an output yt (for instance, a syntactic analysis of this sentence described as an arborescence on a graph over the words in the sentence [23, 21]).", "startOffset": 197, "endOffset": 205}, {"referenceID": 22, "context": "Algorithm 1 is an adaptation of the Passive-Aggressive MIRA algorithm [24] for structured prediction.", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 91, "endOffset": 99}, {"referenceID": 19, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 91, "endOffset": 99}, {"referenceID": 23, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "Since we have shown that we can efficiently solve the inverse optimization problems for a variety of combinatorial structures, we can extend the error bounds of the MIRA algorithm [24] to work for learning the corresponding", "startOffset": 180, "endOffset": 184}, {"referenceID": 22, "context": "dissertation [24] and are relegated to Appendix C for clarity and brevity.", "startOffset": 13, "endOffset": 17}], "year": 2015, "abstractText": "Algorithm designers typically assume that the input data is correct, and then proceed to find \u201coptimal\u201d or \u201csub-optimal\u201d solutions using this input data. However this assumption of correct data does not always hold in practice, especially in the context of online learning systems where the objective is to learn appropriate feature weights given some training samples. Such scenarios necessitate the study of inverse optimization problems where one is given an input instance as well as a desired output and the task is to adjust the input data so that the given output is indeed optimal. Motivated by learning structured prediction models, in this paper we consider inverse optimization with a margin, i.e., we require the given output to be better than all other feasible outputs by a desired margin. We consider such inverse optimization problems for maximum weight matroid basis, matroid intersection, perfect matchings, minimum cost maximum flows, and shortest paths and derive the first known results for such problems with a non-zero margin. The effectiveness of these algorithmic approaches to online learning for structured prediction is also discussed.", "creator": "LaTeX with hyperref package"}}}