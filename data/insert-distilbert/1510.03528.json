{"id": "1510.03528", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "$\\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time", "abstract": "we study the improper learning of multi - layer neural networks. suppose that implementing the neural network to be learned has $ k $ its hidden layers and guarantee that the $ \\ ell _ 1 $ - norm of the incoming weights of any neuron complex is currently bounded on by $ / l $. we present a simplified kernel - based method, such : that then with probability at least $ 1 - \\ delta $, it learns a predictor whose generalization error is at most $ \\ epsilon $ worse than that of mapping the neural network. the sample complexity and the time complexity of the presented method are polynomial in the input order dimension and in $ ( 1 / \\ psi epsilon, \\ log ( 1 / \\ * delta ), f ( k, l ) ) $, where $ f ( k, l ) $ is roughly a function depending on $ ( k, l ) $ and on the activation function, independent of the number of neurons. the algorithm that applies to both sigmoid - like activation functions ( and relu - root like activation functions. it implies that any known sufficiently sparse neural network is learnable precisely in strictly polynomial time.", "histories": [["v1", "Tue, 13 Oct 2015 04:36:09 GMT  (290kb)", "http://arxiv.org/abs/1510.03528v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "jason d lee", "michael i jordan"], "accepted": true, "id": "1510.03528"}, "pdf": {"name": "1510.03528.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuchen Zhang", "Jason D. Lee", "Michael I. Jordan"], "emails": ["yuczhang@eecs.berkeley.edu", "jasondlee@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n03 52\n8v 1\n[ cs\n.L G\n] 1\n3 O"}, {"heading": "1 Introduction", "text": "Neural networks have been successfully applied in many areas of artificial intelligence, such as image classification, face recognition, speech recognition and natural language processing. Practical successes have been driven by the rapid growth in the size of data sets and the increasing availability of large-scale parallel and distributed computing platforms. Examples of recent work in this area include [16, 15, 22, 7, 9, 11].\nThe theoretical understanding of learning in neural networks has lagged the practical successes. It is known that any smooth function can be approximated by a network with just one hidden layer [4], but training such a network is NP-hard [6]. In practice, people use optimization algorithms such as stochastic gradient descent (SGD) to train neural networks. Although strong theoretical results are available for SGD in the setting of convex objective functions, there are few such results in the nonconvex setting of neural networks. While it is possible to transform the neural network training problem to a convex optimization problem involving an infinite number of variables [5], the infinitude of variables means that there is no longer a guarantee that the learning algorithm will terminate in polynomial time.\nSeveral recent papers have risen to the challenge of establishing polynomial-time learnability results for neural networks. These papers necessarily (given that the problem is NP-hard) introduce additional assumptions or relaxations. For instance, one may assume that the data is in fact generated by the neural network. Under this assumption, Arora et al. [2] study the recovery of denoising auto-encoders which are represented by multi-layer neural networks. They assume that the toplayer values of the network are randomly generated and all network weights are randomly drawn\nfrom {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations using which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations.\nSedghi and Anandkumar [19] study the supervised learning of neural networks under the assumption that the data distribution has a score function that is known in advance. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. Learning the deeper layers remains as an open problem. In addition, their method assumes that the network weights are randomly drawn from a Bernoulli-Gaussian distribution. More recently, Janzamin et al. [12] propose another algorithm based on the score function that removes the restrictions of Sedghi and Anandkumar [19]. The assumption in this case is that the network weights satisfy a non-degeneracy condition; moreover, the algorithm is only capable of learning neural networks with one hidden layer.\nAnother approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. [18] consider changing the activation function and over-specifying the network to make it easier to train. They show that polynomial networks (e.g., networks whose activation function is quadratic) with sufficient width and depth are as expressive as the sigmoid-activated neural networks. Although a deep polynomial network is still hard to train, they propose training in a superclass\u2014the class of all polynomial functions with bounded degree. As a consequence, there is an improper learning algorithm which achieves a generalization error at most \u01eb worse than that of the best neural network. The time complexity is polynomial in the input dimension d and quasi-polynomial in 1/\u01eb. Since the dependence on d has a large power, the algorithm is not practical unless d is quite small. Livni et al. [18] further show, however, that there is a practical algorithm to directly train the polynomial network if it has one or two hidden layers.\nA recent line of work has focused on understanding the energy landscape of a neural network. After several simplifying assumptions, a neural network can be shown to be a Gaussian field whose critical points can be analyzed using the Kac-Rice formula and properties of the Gaussian Orthogonal Ensemble [3, 10, 8]. The conclusion of these papers is that all critical points with nonnegative eigenvalues tend to have objective value near the global minimum. Thus in such networks if we could find such a point, it would have small objective value and thus small training error. This combined with generalization error bounds would imply finding a neural network with low excess risk. However, there is no provably efficient algorithm for finding a critical point with nonnegative eigenvalues."}, {"heading": "1.1 Our contribution", "text": "In this paper, we propose a practical algorithm called the recursive kernel method for learning multi-layer neural networks, under the framework of improper learning. Our method is inspired by the work of Shalev-Shwartz et al. [20], which shows that for binary classification with the sigmoidal loss, there is a kernel-based method that achieves the same generalization error as the best linear classifier. We extend this method to deeper networks. In particular, we assume that the neural network to be learned takes d-dimensional input. It has k hidden layers and the \u21131norm of the incoming weights of any neuron is bounded by L. Under these assumptions, the\nalgorithm learns a kernel-based predictor whose generalization error is at most \u01eb worse than that of the best neural network. The sample and the time complexity of the algorithm are polynomial in (d, 1/\u01eb, log(1/\u03b4), F (k, L)), where F (k, L) is a function depending on (k, L) and on the activation function, independent of the input dimension or the number of neurons. The theoretical result holds for any data distribution.\nAs concrete examples, we demonstrate that if the activation function is a quadratic function, then F (k, L) is a polynomial function of L. Thus, the algorithm recovers the theoretical guarantee of Livni et al. [18]. We also demonstrate two activation functions, one that approximates the sigmoid function and the other that approximates the ReLU function, under which F (k, L) is finite. Thus, the algorithm also learns neural networks activated by sigmoid-like or ReLU-like functions. For these latter examples, the dependence on L is no longer polynomial. This nonpolynomial dependence is in fact inevitable: Under a hardness assumption in cryptographics and assuming sigmoid-like or ReLU-like activation, we prove that no algorithm running in poly(L) time can improperly learn the neural network.\nThe paper is organized as follow. In Section 2, we formalize the problem and clarify the assumptions that we make for the theoretical analysis. In Section 3, the algorithm and the associated theoretical results are presented. We discuss concrete examples to demonstrate the application of the theory. In Section 4, we present hardness results for the improper learning of neural networks. In Section 5, we report experiments on the MNIST dataset and its variations, demonstrating that in addition to its role in our theoretical analysis the proposed algorithm is comparable in practice with baseline neural network learning methods."}, {"heading": "2 Problem Setup", "text": "We consider a fully-connected neural network N that maps a vector x \u2208 Rd to a real number N (x) via k hidden layers. Let d(p) represent the number of neurons in the p-th layer. Let y\n(p) i represent\nthe output of the i-th neuron in the p-th layer. We define the zero-th layer to be the input vector so that d(0) = d and y(0) = x. The transformation performed by the neural network is defined as follows:\ny (p) i := \u03c3\n( d(p\u22121)\u2211\nj=1\nw (p\u22121) i,j y (p\u22121) j\n) and N (x) := d(k)\u2211\nj=1\nw (k) 1,j y (k) j ,\nwhere w (p\u22121) i,j is the weight of the edge that connects the neuron j on the (p \u2212 1)-th layer to the neuron i on the p-th layer. The activation function \u03c3 : R \u2192 R is a one-dimensional nonlinear function. We will discuss the choice of function \u03c3 later in this section.\nWe assume that the input vector has bounded \u21132-norm and the edge weights have bounded \u21131 or \u21132 norms. The assumptions are formalized as follows.\nAssumption A. The input vector x satisfies \u2016x\u20162 \u2264 1. The neuron edge weights satisfy d\u2211\nj=1\n(w (0) i,j ) 2 \u2264 L2 for all i \u2208 {1, . . . , d}.\nd(p)\u2211\nj=1\n|w(p)i,j | \u2264 L for all (p, i) \u2208 {1, . . . , k} \u00d7 {1, . . . , d(p+1)}.\nLet Nk,L,\u03c3 be the set of k-layer neural networks with activation function \u03c3 that satisfy the edge weight constraints.\nAssumption A implies that for all neurons on the first hidden layer, the \u21132-norm of their incoming weights is bounded by L. For other neurons, the \u21131-norm of their incoming weights is bounded by L. The \u21131-regularization imposes sparsity on the neural network. It is observed in practice that sparse neural networks are capable of learning meaningful representations. For example, the convolution neural network has sparse edges. It has been argued that sparse connectivity is a natural constraint which can lead to improved performance in practice [21].\nIn a prediction task, there is a convex function \u2113 : R \u00d7 R \u2192 R that measures the loss of the prediction. For a feature-label pair (x, y) \u2208 X \u00d7 R, its prediction loss is measured by \u2113(N (x), y). We assume that (x, y) is sampled from an underlying distribution D. The prediction risk of the neural network is defined by E[\u2113(N (x), y)]. Our goal is to learn a predictor f : X \u2192 R, which is not necessarily a neural network, such that\nE[\u2113(f(x), y)] \u2264 arg min N\u2208Nk,L,\u03c3 E[\u2113(N (x), y)] + \u01eb. (1)\nIn other words, we want to learn a predictor whose generalization loss is at most \u01eb worse than that of the best neural network in Nk,L,\u03c3.\nIn practice, both the sigmoid function \u03c3(x) = (1 + e\u2212\u03b2x)\u22121 and the ReLU function \u03c3(x) = max(0, x) are widely used as activation functions for neural networks. We define two classes of activation functions that includes the sigmoid and ReLU respectively.\nDefinition 1 (sigmoid-like activation). A function \u03c3 is called sigmoid-like if it is non-decreasing on (\u2212\u221e,+\u221e) and\nlim x\u2192\u2212\u221e xc\u03c3(x) = 0 and lim x\u2192\u221e xc(1\u2212 \u03c3(x)) = 0\nfor some positive constant c.\nDefinition 2 (ReLU-like activation). A function \u03c3 is called ReLU-like if \u03c3(x)\u2212\u03c3(x\u22121) a sigmoidlike function.\nIntuitively, a sigmoid-like function is a non-decreasing function on [0, 1]. When x \u2192 \u2212\u221e or x \u2192 \u221e, the function value approaches 0 or 1 at a polynomial rate (or faster) in x. A ReLU-like function is a convex function on [0,\u221e). When x \u2192 \u221e, it approaches a linear function with unit slope."}, {"heading": "3 Algorithm and Theoretical Result", "text": "In this section, we present a kernel method which learns a predictor performing as well as the neural network. We begin by recursively defining a sequence of kernels. Let K : RN \u00d7 RN \u2192 R be a function defined by\nK(x, y) := 1\n2\u2212 \u3008x, y\u3009 ,\nwhere both \u2016x\u20162 and \u2016y\u20162 are assumed to be bounded by one. The function K is a kernel function because we can find a mapping \u03c8 : RN \u2192 RN such that K(x, y) = \u3008\u03c8(x), \u03c8(y)\u3009. The function \u03c8 maps an infinite-dimensional vector to an infinite-dimensional vector. We use xi to represent the\nAlgorithm 1: Recursive Kernel Method for Learning Neural Network\nInput: Feature-label pairs {(xi, yi)}ni=1; Loss function \u2113 : R\u00d7 R \u2192 R; Number of hidden layers k; Regularization coefficient B. Solve the following convex optimization problem:\n\u03b1\u0302 = arg min \u03b1\u2208Rn\n1\nn\nn\u2211\nj=1\n\u2113\n( n\u2211\ni=1\n\u03b1iK (k)(xi, xj), yi ) s.t. n\u2211\ni,j=1\n\u03b1i\u03b1jK (k)(xi, xj) \u2264 B2\nwhere K(k) is defined in Eq. (4). Output: Predictor f\u0302n(x) = \u2211n i=1 \u03b1\u0302iK (k)(xi, x).\ni-th coordinate of an infinite-dimensional vector x. The (k1, . . . , kj)-th coordinate of \u03c8(x), where j \u2208 N and k1, . . . , kj \u2208 N, is defined as 2\u2212 j+1 2 xk1 . . . xkj . By this definition, we have\n\u3008\u03c8(x), \u03c8(y)\u3009 = \u221e\u2211\nj=0\n2\u2212(j+1) \u2211\n(k1,...,kj)\u2208Nj\nxk1 . . . xkjyk1 . . . ykj . (2)\nThe inner term on the right-hand side of Eq. (2) can be simplified to\n\u2211\n(k1,...,kj)\u2208Nj\nxk1 . . . xkjyk1 . . . ykj = (\u3008x, y\u3009)j . (3)\nCombining Eqs. (2) and (3) and using the fact that \u3008x, y\u3009 \u2264 1, we have\n\u3008\u03c8(x), \u03c8(y)\u3009 = \u221e\u2211\nj=0\n2\u2212(j+1)(\u3008x, y\u3009)j = 1 2\u2212 \u3008x, y\u3009 = K(x, y),\nwhich verifies that K is a kernel function and \u03c8 is the associated mapping. Since \u03c8 maps from RN to RN and \u2016x\u20162 \u2264 1 implies \u2016\u03c8(x)\u20162 = K(\u03c8(x), \u03c8(x)) \u2264 1, we can recursively define a sequence of mappings\n\u03c8(0)(x) = x and \u03c8(p)(x) = \u03c8(\u03c8(p\u22121)(x)).\nUsing the relation between K and \u03c8, it is easy to verify that the associated kernels are\nK(0)(x, y) = \u3008x, y\u3009 and K(p)(x, y) = 1 2\u2212K(p\u22121)(x, y) , (4)\nwhich satisfy \u3008\u03c8(p)(x), \u03c8(p)(y)\u3009 = K(p)(x, y). Thus, the kernel function K(k)(x, y) can be easily computed from the inner product of x and y."}, {"heading": "3.1 Algorithm", "text": "We are now ready to specify the algorithm to learn the neural network. Suppose that the neural network has k hidden layers. Let Fk represent the Reproducing Kernel Hilbert Space (RKHS)\ninduced by the kernel K(k) and let Fk,B \u2282 Fk be the set of RKHS elements whose norm are bounded by B. Given training examples {(xi, yi)}ni=1, define the predictor\nf\u0302n := arg min f\u2208Fk,B\n1\nn\nn\u2211\ni=1\n\u2113(f(xi), yi).\nAccording to the representer theorem, we can represent f\u0302n by\nf\u0302n(x) =\nn\u2211\ni=1\n\u03b1iK (k)(xi, x) where\nn\u2211\ni,j=1\n\u03b1i\u03b1jK (k)(xi, xj) \u2264 B2, (5)\nComputing the vector \u03b1 is a convex optimization problem in Rn and therefore can be solved in time poly(n, d) using standard optimization tools. We call this algorithm the recursive kernel method and summarize it in Algorithm 1. It is an improper learning algorithm since the learned predictor f\u0302n cannot be represented by a neural network."}, {"heading": "3.2 Main Result", "text": "Applying classical results from learning theory, we can upper bound the Rademacher complexity of Fk,B by \u221a 2B2/n (see, e.g., [13]). Thus, with probability at least 1\u2212 \u03b4, we can upper bound the generalization loss of predictor f\u0302n(x) by\nE[\u2113(f\u0302n(x), y)] \u2264 arg min f\u2208Fk,B E[\u2113(f(x), y)] + \u01eb,\nwhen the sample size n = \u2126(B2 log(1/\u03b4)/\u01eb2). See [20, Theorem 2.2] for the proof of this claim. In order to establish the bound (1), it suffices to show that Nk,L,\u03c3 \u2282 Fk,B where B is a constant that only depends on k and L. The following lemma establishes the claim. See Appendix A for the proof. Lemma 1. Assume that the function \u03c3(x) has a polynomial expansion \u03c3(x) = \u2211\u221e\nj=0 \u03b2jx j. Let\nH(\u03bb) := L \u00b7 \u221a\u2211\u221e\nj=0 2 j+1\u03b22j \u03bb 2j and define H(k)(x) be the degree-k composition of function H, then\nNk,L,\u03c3 \u2282 Fk,H(k)(L).\nUsing Lemma 1 and the above analyses, we obtain the main result of this paper.\nTheorem 1. Let Assumption A be true and define F (k, L) := H(k)(L) where H(k)(L) is specified in Lemma 1. If F (k, L) is finite, then with probability at least 1 \u2212 \u03b4, the predictor defined in Algorithm 1 achieves\nE[\u2113(f\u0302n(x), y)] \u2264 arg min N\u2208Nk,L,\u03c3 E[\u2113(N (x), y)] + \u01eb.\nThe sample complexity is bounded by poly(1/\u01eb, log(1/\u03b4), F (k, L)); the time complexity is bounded by poly(d, 1/\u01eb, log(1/\u03b4), F (k, L))."}, {"heading": "3.3 Examples", "text": "We study several concrete examples where F (k, L) is finite. Our first example is the quadratic activation function:\n\u03c3sq(x) = x 2.\nThis activation function has been studied by Livni et al. [18], who refer to a neural network activated by this function as a polynomial network. In Theorem 1, if the quadratic activation function is employed, we have H(\u03bb) = 2L\u03bb2. As a consequence, we have F (1, L) = 2L2 and more generally F (k, L) \u2264 (2L)2k+1\u22121 by induction. Thus, the sample and the time complexity of Algorithm 1 is a polynomial function of (d, 1/\u01eb, log(1/\u03b4), L) for any constant k.\nNext, we study sigmoid-like or ReLU-like activation functions. We consider a shifted erf function defined as:\n\u03c3erf(x) = 1\n2 (1 + erf(\n\u221a \u03c0x)),\nand a smoothed hinge loss function defined as:\n\u03c3sh(x) =\n\u222b x\n\u2212\u221e\n\u03c3erf(t)dt = \u03c3erf(x) \u00b7 x+ e\u2212\u03c0x\n2\n2\u03c0 .\nIn Figure 1, we compare \u03c3erf and \u03c3sh with the sigmoid function and the ReLU function. It is seen that \u03c3erf is similar to the sigmoid function and \u03c3sh is a smoothed version of ReLU. It is also easy to verify that \u03c3erf is sigmoid-like and \u03c3sh is ReLU-like. The following proposition shows that if either \u03c3erf or \u03c3sh is used as the activation function, the quantity F (k, L) is finite. See Appendix B for the proof.\nProposition 1. For the \u03c3erf function, we have\nH(\u03bb) \u2264 L \u00b7 \u221a 1\n2 + 4\u03bb2(1 + 3e\u03c0\u03bb2e4\u03c0\u03bb2) for any \u03bb \u2265 3.\nFor the \u03c3sh function, we have\nH(\u03bb) \u2264 L \u00b7 \u221a \u03bb2 + 8\u03bb4(1 + 3e\u03c0\u03bb2e4\u03c0\u03bb2) for any \u03bb \u2265 3.\nThus, Theorem 1 implies that the neural network activated by \u03c3erf or \u03c3sh is learnable in polynomial time given any constant (k, L).\nFinally, we demonstrate how the conditions of Assumption A could be modified. Consider a sigmoid-activated network with k hidden layers which satisfies the following:\nd(p)\u2211\nj=1\n|w(p)i,j | \u2264 L for all (p, i) \u2208 {1, . . . , k} \u00d7 {0, . . . , d(p+1)}.\nThis means that the \u21131-norm of all layers is bounded by L. In addition, we assume that the input vector satisfies \u2016x\u2016\u221e \u2264 1. This is in contrast to the condition \u2016x\u20162 \u2264 1 in Assumption A. It was shown by Livni et al. [18, Theorem 4] that this sigmoid network can be approximated by a polynomial network with arbitrarily small approximation error \u01eb. The associated polynomial network has O(k log(Lk+L log(1/\u01eb))) hidden layers, whose \u21131-norms are bounded by eO(L log(1/\u01eb)). If we normalize the input vector x \u2208 Rd by x \u2190 x/ \u221a d and multiple all first-layer weights by \u221a d, the output of the network remains invariant and it satisfies Assumption A. Thus, combining our result for the polynomial network and the above analysis, the sigmoid network can be learned in\npoly ( d(Lk+L log(1/\u01eb)) O(k) , log(1/\u03b4) )\nsample and time complexity. This is a quasi-polynomial dependence on 1/\u01eb for any constant (k, L). Notice that the dimension d comes into the expression."}, {"heading": "4 Hardness Result", "text": "In Section 3.3, we see that the dependence of the time complexity on L is at least exponential for \u03c3erf and \u03c3sh, but it is polynomial for the quadratic activation. It is thus natural to wonder if there is a sigmoid-like or ReLU-like activation function that makes the time complexity a polynomial function of L. In this section, we prove that this is impossible given standard hardness assumptions.\nOur proof relies on the hardness of standard (nonagnostic) PAC learning of intersection of halfspaces given in Klivans and Sherstov [14]. More precisely, let\nH = {x \u2192 sign(wTx\u2212 b\u2212 1/2) : x \u2208 {\u22121, 1}d, b \u2208 N, w \u2208 Nd, |b|+ \u2016w\u20161 \u2264 poly(d)}\nbe the family of halfspace indicator functions mapping X = {\u22121, 1}d to {\u22121, 1}, and let HT be the set of functions taking the form:\nh(x) = { 1 if h1(x) = \u00b7 \u00b7 \u00b7 = hT (x) = 1, \u22121 otherwise. where h1, . . . , hT \u2208 H.\nThus, HT is the set of functions that indicates the intersection of T halfspaces. For any distribution on X , an algorithm A takes a sequence of (x, h\u2217(x)) as input where x is a sample from X and h\u2217 \u2208 HT . The algorithm learns a function h\u0302 such that with probability at least 1\u2212 \u03b4, one has\nP (h\u0302(x) 6= h\u2217(x)) \u2264 \u01eb. (6)\nIf there is such an algorithm A whose sample complexity and time complexity scale as poly(d), then we say that HT is efficiently learnable. Klivans and Sherstov [14] show that HT is not efficiently learnable under a certain cryptographic assumption.\nTheorem 2 (Klivans and Sherstov [14]). If T = d\u03c1 for some constant \u03c1 > 0, then under a certain cryptographic assumption, HT is not efficiently learnable.\nWe use this hardness result to prove the hardness of learning neural networks. In particular, we construct a neural network N such that if there is a learning algorithm computing a predictor f\u0302 such that E[\u2113(f\u0302(x), y)] \u2264 E[\u2113(N (x), y)] + \u01eb, then the error bound (6) is satisfied. Thus, the hardness of learning intersection of halfspaces implies the hardness of learning neural networks. See Appendix C for the proof.\nTheorem 3. Assume the cryptographic assumption of Theorem 2. Let \u03c3 be a sigmoid-like or ReLU-like function and let \u2113(f(x), y) = max(0, 1 \u2212 yf(x)) be the hinge loss. For fixed (\u03b4, \u01eb), there is no algorithm running in poly(L) time that learns a predictor f\u0302 satisfying\nE[\u2113(f\u0302(x), y)] \u2264 arg min N\u2208N1,L,\u03c3 E[\u2113(N (x), y)] + \u01eb with probability at least 1\u2212 \u03b4. (7)\nThe hardness of learning sigmoid-activated and ReLU-activated neural networks has been proved by Livni et al. [18] when \u2113 is the zero-one loss. Theorem 3 presents a more general result, showing that any activation function that is sigmoid-like or ReLU-like leads to the computational hardness, even if the loss function \u2113 is convex."}, {"heading": "5 Experiments", "text": "In this section, we compare the proposed algorithm with several baseline algorithms on the MNIST digit recognition task. Since the basic MNIST digits are relatively easy to classify, we introduce three variations which make the problem more challenging.\nDatasets We use the MNIST handwritten digits dataset and three variations of it. See Figure 2 for the description of these datasets and several exemplary images. All the images are of size 28\u00d7 28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This partitioning is recommended by the source of the data [1].\nAlgorithms For the recursive kernel method, we train one-vs-all SVM classifiers with Algorithm 1. The hyper-parameters are given by k \u2208 {1, 4} and B = 100. All images are pre-processed by the following steps: deskewing, centering and normalization. The deskewing step computes the principal axis of the shape that is closest to the vertical, and shifts the lines so as to make it vertical. It is a common preprocessing step for the kernel method [17]. The centering and normalization steps center the feature vector and scale it to have the unit \u21132-norm.\nWe compare with the following baseline models: multi-class logistic regression, multi-layer perceptron and convolution neural networks. The multi-layer perceptron is a fully connected neural network with a single hidden layer which contains 500 hidden neurons. It covers the networks that can be learned by the method of Janzamin et al. [12]. The convolution neural networks implement the LeNet5 architecture [17]. All baseline models are trained via stochastic gradient descent.\nResults The classification error rates are summarized in Table 1. As the table shows, the recursive kernel method is consistently more accurate than logistic regression and the multi-layer perceptron. On the Basic and the Rotation datasets, the proposed algorithm is comparable with LeNet5. On the other two datasets, LeNet5 wins over other methods by a relatively large margin. It is worth noting that when we choose a greater k, the performance of the proposed algorithm gets better. Recall that a greater k learns a deeper neural network, thus the empirical observation is intuitive.\nAlthough the recursive kernel method doesn\u2019t outperform the LeNet5 model, the experiment demonstrates that it does learn better predictors than fully connected neural networks such as the multi-layer perceptron. The LeNet5 architecture encodes prior knowledge about digit recogniition via the convolution and pooling operations; thus its performance is better than the generic architectures."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented an algorithm and a theoretical analysis for the improper learning of multi-layer neural networks. The proposed method, which is based on a recursively defined kernel, is guaranteed to learn the neural network if it has a constant depth and a constant \u21131-norm. We also present hardness results showing that the time complexity cannot be polynomial in the \u21131-norm bound. We compare the algorithm with several baseline methods on the MNIST dataset and its variations. The algorithm learns better predictors than the full-connected multi-layer perceptron but is outperformed by LeNet5. We view this line of work as a contribution to the ongoing effort to develop learning algorithms for neural networks that are both understandable in theory and useful in practice."}, {"heading": "A Proof of Lemma 1", "text": "Consider an arbitrary neural network N \u2208 Nk,L,\u03c3. Let g(p)i := \u2211d(p) j=1w (p) ji y (p) j represent the input of the neuron i at layer p+1. Note that g (p) i is a function of the input vector x. By this definition, it suffices to show that g (k) 1 \u2208 Fk,H(k)(L).\nWe claim that g (p) i \u2208 Fp,H(p)(L) for any p \u2208 {0, 1, . . . , k} and prove the claim by induction. For\np = 0, we have\ng (0) i (x) =\nd\u2211\nj=1\nw (0) i,j xj = \u3008w (0) i , \u03c8 (0)(x)\u3009.\nThus, g (0) i belongs to the RKHS induced by the kernel K (0). Furthermore, we have \u2016g(0)i \u2016F0 = \u2016w(0)i \u20162 \u2264 L = H(0)(L), which implies g (0) i \u2208 F0,H(0)(L).\nFor p > 0, we assume that the claim holds for p \u2212 1 and we will prove it for p. The definition of g\n(p) i implies\ng (p) i (x) =\nd(p)\u2211\nj=1\nw (p) ji \u03c3 ( g (p\u22121) j (x) ) .\nUsing the inductive hypothesis, we have g (p\u22121) j \u2208 Fp\u22121,H(p\u22121)(L), which implies that g (p\u22121) j (x) = \u3008vj , \u03c8(p\u22121)(x)\u3009 for some vj \u2208 RN, and \u2016vj\u20162 \u2264 H(p\u22121)(L). This implies\ng (p) i (x) =\nd(p)\u2211\nj=1\nw (p) i,j \u03c3(\u3008vj , \u03c8(p\u22121)(x)\u3009). (8)\nLet x(p\u22121) be a shorthand notation of \u03c8(p\u22121)(x). We define vector uj \u2208 RN as follow: the (k1, . . . , kt)th coordinate of uj , where t \u2208 N and k1, . . . , kt \u2208 N+, is equal to 2 t+1 2 \u03b2tvj,k1 . . . vj,kt. By this definition, we have\n\u03c3(\u3008vj , x(p\u22121)\u3009) = \u221e\u2211\nt=0\n\u03b2t(\u3008vj , x(p\u22121)\u3009)t\n= \u221e\u2211\nt=0\n\u03b2t \u2211\n(k1,...,kt)\u2208Nt\nvj,k1 . . . vj,ktx (p\u22121) k1 . . . x (p\u22121) kt\n= \u3008uj , \u03c8(x(p\u22121))\u3009, (9)\nwhere the first equation holds since \u03c3(x) has a polynomial expansion \u03c3(x) = \u2211\u221e\nt=0 \u03b2tx t, the second\nby expanding the inner product, and the third by definition of \u03c8(x) . Combining Eq. (8) and Eq. (9), we have\ng (p) i (x) =\nd(p)\u2211\nj=1\nw (p) i,j \u3008uj , \u03c8(\u03c8(p\u22121)(x))\u3009 =\n\u2329 d(p)\u2211\nj=1\nw (p) ji uj, \u03c8\n(p)(x) \u232a .\nThis implies that g (p) i belongs to the RKHS induced by the kernel K (p).\nFinally, we upper bound the norm of g (p) i . Notice that\n\u2016g(p)i \u2016Fp = \u2225\u2225\u2225 d(p)\u2211\nj=1\nw (p) i,j uj \u2225\u2225\u2225 2 \u2264 d(p)\u2211\ni=1\n|w(p)i,j | \u00b7 \u2016uj\u20162 \u2264 L \u00b7 max j\u2208[d(p)] {\u2016uj\u20162}. (10)\nUsing the definition of uj and the inductive hypothesis, we have\n\u2016uj\u201622 = \u221e\u2211\nt=0\n2t+1\u03b22t \u2211\n(k1,...,kt)\u2208Nt\nv2j,k1v 2 j,k2 \u00b7 \u00b7 \u00b7 v2j,kt\n=\n\u221e\u2211\nt=0\n2t+1\u03b22t \u2016vj\u20162t2 \u2264 \u221e\u2211\nt=0\n2t+1\u03b22t (H (p\u22121)(L))2t. (11)\nCombining inequality (10) and (11), we have \u2016g(p)i \u2016Fp \u2264 H(p)(L), which verifies that g (p) i \u2208 Fp,H(p)(L)."}, {"heading": "B Proof of Proposition 1", "text": "For the \u03c3erf function, the polynomial expansion is\n\u03c3erf(x) = 1\n2 + 1\u221a \u03c0\n\u221e\u2211\nj=0\n(\u22121)j(\u221a\u03c0x)2j+1 j!(2j + 1) .\nTherefore, we have\nH(\u03bb) = L \u00b7 \u221a\u221a\u221a\u221a1 2 + 2 \u03c0 \u221e\u2211\nj=0\n(2\u03c0\u03bb2)2j+1\n(j!)2(2j + 1)2 . (12)\nShalev-Shwartz et al. [20, Corollary C] provide an upper bound on the right-hand side of Eq. (12). In particular, they prove that\n2\n\u03c0\n\u221e\u2211\nj=0\n(2\u03c0\u03bb2)2j+1\n(j!)2(2j + 1)2 \u2264 4\u03bb2(1 + 3e\u03c0\u03bb2e4\u03c0\u03bb2) for any \u03bb \u2265 3. (13)\nPlugging this upper bound to Eq. (12) completes the proof.\nFor the \u03c3sh function, since it is the integral of the \u03c3erf function, its polynomial expansion is\n\u03c3sh(x) = x\n2 + 1\u221a \u03c0\n\u221e\u2211\nj=0\n(\u22121)j(\u221a\u03c0x)2j+1x j!(2j + 1)(2j + 2) ,\nand consequently,\nH(\u03bb) = L \u00b7 \u221a\u221a\u221a\u221a\u03bb2 + 2 \u03c0 \u221e\u2211\nj=0\n(2\u03c0\u03bb2)2j+1(2\u03bb2)\n(j!)2(2j + 1)2(2j + 2)2 . (14)\nWe upper bound the right-hand side of Eq. (14) by\n2\n\u03c0\n\u221e\u2211\nj=0\n(2\u03c0\u03bb2)2j+1(2\u03bb2)\n(j!)2(2j + 1)2(2j + 2)2 \u2264 4\u03bb\n2\n\u03c0\n\u221e\u2211\nj=0\n(2\u03c0\u03bb2)2j+1\n(j!)2(2j + 1)2\n\u2264 8\u03bb4(1 + 3e\u03c0\u03bb2e4\u03c0\u03bb2) for any \u03bb \u2265 3,\nwhere the final inequality holds because of Eq. (13). Plugging this upper bound into Eq. (14) completes the proof."}, {"heading": "C Proof of Theorem 3", "text": "We construct a one-hidden-layer neural network that encodes the intersection of T halfspaces. Suppose that the t-th halfspace is characterized by gt(x) = w T t x\u2212 bt\u2212 1/2. Since both x, wt and bt are composed of integers, we have gt(x) \u2265 1/2 when ht(x) = 1, and gt(x) \u2264 \u22121/2 when ht(x) = \u22121. We extend x to be (x, 1), then extend wt to be (wt, bt), and define\ng\u0303t(x) = \u3008w\u0303t, x\u0303\u3009 where x\u0303 := 1\u221a d+ 1\n(x, 1) and w\u0303 := 2\u03bb \u221a d+ 1(wt, bt),\nwhere \u03bb is a scalar to be specified. According to this definition, we have \u2016x\u0303\u20162 = 1 and \u2016w\u0303\u20162 = poly(d). In addition, we have g\u0303t(x) \u2265 \u03bb when ht(x) = 1, and g\u0303t(x) \u2264 \u2212\u03bb when ht(x) = \u22121.\nSigmoid-like Activation If \u03c3 a is sigmoid-like function, there is a constant c such that\nlim x\u2192\u2212\u221e xc\u03c3(x) = lim x\u2192\u221e xc(1\u2212 \u03c3(x)) = 0.\nThus, there is a sufficiently large constant C such that \u03c3(x) \u2264 x\u2212c for all x \u2264 \u2212C and \u03c3(x) \u2265 1\u2212x\u2212c for all x \u2265 C. Note that the number T of intersecting halfspaces is a polynomial function of dimension d. As a consequence, there is a sufficiently large constant \u03bb \u223c poly(d) such that\n\u03c3(x) \u2265 1\u2212 1 4T for all x > \u03bb and \u03c3(x) \u2264 1 4T for all x \u2264 \u2212\u03bb.\nThus, we have \u03c3(g\u0303t(x)) \u2265 1\u2212 14T if ht(x) = 1 and \u03c3(g\u0303t(x)) \u2264 14T if ht(x) = \u22121. We define the neural network N to be\nN (x) = T\u2211\nt=1\n4 \u03c3(g\u0303t(x))\u2212 (4T \u2212 2). (15)\nIt is easy to verify that N \u2208 N1,L,\u03c3 for some L \u223c poly(d). If h\u2217(x) = 1, then x belongs to the intersection of halfspaces. It implies that \u03c3(g\u0303t(x)) \u2265 1 \u2212 14T for all t \u2208 [T ]. Combining with Eq. (15), we obtain N (x) \u2265 1. On the other hand, if h\u2217(x) = \u22121, then there is some t such that \u03c3(g\u0303t(x)) \u2264 14T . Thus, Eq. (15) implies N (x) \u2264 \u22121. In summary, we have h\u2217(x)N (x) \u2265 1 for any x \u2208 X . As a consequence, we have \u2113(N (x), h\u2217(x)) \u2261 0 where \u2113 is the hinge loss.\nAssume that there is a predictor f\u0302 satisfying the error bound (7). Let h\u0302(x) = sign(f\u0302(x)) be a classifier that judges the intersection of hyperplanes. Since the hinge loss is an upper bound on the zero-one loss, we have\nP (h\u0302(x) 6= h\u2217(x)) = E[I(h\u0302(x) 6= h\u2217(x))] = E[I(sign(f\u0302(x)) 6= h\u2217(x))] \u2264 E[\u2113(f\u0302(x), h\u2217(x))] \u2264 E[\u2113(N (x), h\u2217(x))] + \u01eb = \u01eb,\nwhere the final inequality follows from inequality (7). The last equation holds since \u2113(N (x), h\u2217(x)) \u2261 0. This implies that the associated classifier h\u0302 satisfies the error bound (6). Since h\u0302 cannot be computed in poly(d) time, we conclude that f\u0302 cannot be computed in poly(L) time.\nReLU-like Activation If \u03c3 is a ReLU-like function, then by definition, we have \u03c3\u2032(x) := \u03c3(x)\u2212 \u03c3(x \u2212 1) is a sigmoid-like function. Following the argument for the sigmoid-like activation, if we treat \u03c3\u2032 as the activation function, then the remaining part of the proof will go through without any further modification. This completes the proof for the ReLU-like activation."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We study the improper learning of multi-layer neural networks. Suppose that the neural network<lb>to be learned has k hidden layers and that the l1-norm of the incoming weights of any neuron is<lb>bounded by L. We present a kernel-based method, such that with probability at least 1\u2212 \u03b4, it<lb>learns a predictor whose generalization error is at most \u01eb worse than that of the neural network.<lb>The sample complexity and the time complexity of the presented method are polynomial in the<lb>input dimension and in (1/\u01eb, log(1/\u03b4), F (k, L)), where F (k, L) is a function depending on (k, L)<lb>and on the activation function, independent of the number of neurons. The algorithm applies to<lb>both sigmoid-like activation functions and ReLU-like activation functions. It implies that any<lb>sufficiently sparse neural network is learnable in polynomial time.", "creator": "LaTeX with hyperref package"}}}