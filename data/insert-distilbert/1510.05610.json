{"id": "1510.05610", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2015", "title": "Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues", "abstract": "unfortunately there are various parametric models for analyzing advanced pairwise comparison data, including the bradley - terry - smith luce ( btl ) utility and thurstone models, but their reliance on strong parametric assumptions is limiting. in this work, we later study a flexible residual model for pairwise target comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. this class includes several parametric models including the btl and thurstone correlation models as special cases, but is considerably more general. we provide various examples of models identified in this broader stochastically transitive class for which classical parametric models provide poor fits. despite this greater flexibility, we gradually show that the matrix of probabilities can be also estimated at the same interval rate smoothly as in standard parametric models. on the matters other hand, unlike candidates in the btl and thurstone models, computing the least - squares estimate in generating the stochastically transitive model is non - trivial, and we explore various computationally tractable approximate alternatives. we show that a simple discrete singular value - thresholding simulation algorithm is statistically consistent about but does not achieve that the minimax rate. we then propose and study alternative algorithms that ultimately achieve the minimax rate over interesting sub - classes of defining the full stochastically transitive class. we complement our theoretical model results with thorough numerical simulations.", "histories": [["v1", "Mon, 19 Oct 2015 18:19:16 GMT  (166kb,D)", "http://arxiv.org/abs/1510.05610v1", null], ["v2", "Wed, 21 Oct 2015 17:54:49 GMT  (171kb,D)", "http://arxiv.org/abs/1510.05610v2", null], ["v3", "Wed, 25 May 2016 18:41:40 GMT  (179kb,D)", "http://arxiv.org/abs/1510.05610v3", null], ["v4", "Wed, 28 Sep 2016 02:14:25 GMT  (232kb,D)", "http://arxiv.org/abs/1510.05610v4", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["nihar b shah", "sivaraman balakrishnan", "aditya guntuboyina", "martin j wainwright"], "accepted": true, "id": "1510.05610"}, "pdf": {"name": "1510.05610.pdf", "metadata": {"source": "CRF", "title": "Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues", "authors": ["Nihar B. Shah", "Sivaraman Balakrishnan", "Adityanand Guntuboyina", "Martin J. Wainright"], "emails": ["nihar@eecs.berkeley.edu,", "sbalakri@berkeley.edu,", "aditya@stat.berkeley.edu,", "wainwrig@berkeley.edu."], "sections": [{"heading": "1 Introduction", "text": "Pairwise comparison data is ubiquitous and arises naturally in a variety of applications, including tournament play, voting, online search rankings, and ad placement problems. In rough terms, given a set of n objects, and a collection of possibly inconsistent comparisons between pairs of these objects, the goal is to aggregate these comparisons in order to perform effective statistical inference on various underlying properties of the population. Particular properties of interest include the ordering of the objects and the underlying pairwise comparison probabilities\u2014that is, the probability that object i is preferred to object j in a pairwise comparison. The Bradley-Terry-Luce [BT52, Luc59] and Thurstone [Thu27] models are mainstays in analyzing this type of pairwise comparison data. These models are parametric in nature: more specifically, they assume the existence of an n-dimensional weight vector that measures the quality or strength of each item. The pairwise comparison probabilities are then determined via some fixed (parametric) function of the qualities of the pair of objects. Estimation in these models reduces to estimating the underlying weight vector, and a large body of\nAuthor email addresses: nihar@eecs.berkeley.edu, sbalakri@berkeley.edu, aditya@stat.berkeley.edu, wainwrig@berkeley.edu.\nar X\niv :1\n51 0.\n05 61\n0v 1\n[ st\nat .M\nL ]\n1 9\nO ct\nprior work has focused on these models (see, e.g., [NOS12, HOX14, SBB+15]). These models however, enforce strong relationships on the pairwise comparison probabilities that often fail to hold in real applications. Various papers [DM59, ML65, Tve72, BW97] have provided experimental results in which these parametric modeling assumptions fail to hold.\nOur focus in this paper is on models that have their roots in social science and psychology (e.g., see Fishburn [Fis73] for an overview), where the only coherence assumption made on the pairwise comparison probabilities is that of strong stochastic transitivity, or SST for short. These models include the parametric models as special cases but are considerably more general. The SST model has been validated by several empirical analyses, including those in a long line of work [DM59, ML65, Tve72, BW97]. The conclusion of Ballinger et al. [BW97] is especially strongly worded:\nAll of these parametric c.d.f.s are soundly rejected by our data. However, SST usually survives scrutiny.\nWe are thus provided with strong empirical motivation for studying the fundamental properties of pairwise comparison probabilities satisfying SST.\nIn this paper, we focus on the problem of estimating the matrix of pairwise comparison probabilities\u2014that is, the probability that an item i will beat a second item j in any given comparison. Estimates of these comparison probabilities are useful in various applications. For instance, when the items correspond to players or teams in a sport, the predicted odds of one team beating the other are central to betting and bookmaking operations. In a supermarket or an ad display, an accurate estimate of the probability of a customer preferring one item over another, along with the respective profits for each item, can effectively guide the choice of which product to display. Accurate estimates of the pairwise comparison probabilities can also be used to infer partial or full rankings of the underlying items.\nOur contributions: We begin by studying the performance of optimal methods for estimating matrices in the SST class: our first main result (Theorem 1) characterizes the minimax rate in squared Frobenius norm up to logarithmic factors. This result reveals that even though the SST class of matrices is considerably larger than the classical parametric class, surprisingly, it is possible to estimate any SST matrix at the same rate as the classical parametric family. On the other hand, computing the least-squares solution over the SST class is non-trivial, as a brute force approach entails an exhaustive search over permutations. Accordingly, we turn to studying computationally tractable estimation procedures. Our second main result (Theorem 2) applies to a polynomial-time estimator based on thresholding the singular values of the data matrix. An estimator based on hard-thresholding was studied previously in this context by Chatterjee [Cha14]. We sharpen and generalize this previous analysis, and give a tight characterization of the rate achieved by both hard and soft thresholding estimators. Our third contribution, formalized in Theorems 3 and 4, is to show that for certain interesting subsets of the full SST class, a combination of parametric maximum likelihood [SBB+15] and noisy sorting algorithms [BM08] leads to a tractable two-stage method that achieves the minimax rate. Our fourth contribution is to supplement our minimax lower bound with lower bounds for various known estimators, including those based on thresholding singular values [Cha14], noisy sorting [BM08], and parametric methods [NOS12, HOX14, SBB+15]) based on parametric models. These lower bounds show that none of these tractable estimators achieve the minimax rate uniformly over the entire class. The lower bounds also show that the minimax rates for any of these subclasses is no better than the full SST class.\nRelated work: The literature on ranking and estimation from pairwise comparisons is vast and we refer the reader to various surveys [FV93, Mar96, Cat12] for a more detailed overview. We focus our literature review on papers that are closely related to the contributions of our work. Some recent work [NOS12, HOX14, SBB+15] studies procedures and minimax rates for estimating the latent quality vector that underlie such parametric models. Theorem 4 in this paper provides an extension of these results, in particular by showing that an optimal estimate of the latent quality vector can be used to construct an optimal estimate of the pairwise comparison probabilities. Chatterjee [Cha14] formally introduced the estimation problem considered in this paper, and analyzed an estimator based on singular value thresholding. In Theorem 2, we provide a sharper analysis of this estimator, and show that our upper bound is\u2014in fact\u2014unimprovable.\nIn past work, various authors [KMS07, BM08] have considered the noisy sorting problem, in which the goal is to infer the underlying order under the assumption that each pairwise comparison has a probability of agreeing with the underlying order that is bounded away from 12 by a fixed constant. These works provide polynomial-time algorithms to infer the true underlying order with a certain accuracy. Part of our analysis leverages an algorithm from the paper [BM08]; in particular, we extend their analysis in order to provide guarantees for estimating pairwise comparison probabilities as opposed to estimating the underlying order.\nAs will be clarified in the sequel, the assumption of strong stochastic transitivity, has close connections to the statistical literature on shape constrained inference (e.g., [SS11]), particularly to the problem of bi-variate isotonic regression. In our analysis of the leastsquares estimator, we leverage metric entropy bounds from past work in this area (e.g., [GW07, CGS15])."}, {"heading": "2 Background and problem formulation", "text": "Given a collection of n \u2265 2 items, suppose that we have a mechanism for making noisy comparisons between any pair i 6= j of distinct items. The full set of all possible pairwise comparisons can be described by a probability matrix M\u2217 \u2208 [0, 1]n\u00d7n, in which M\u2217ij is the probability that item i is preferred to item j. The upper and lower halves of the probability matrix M\u2217 are related by the shifted-skew-symmetry condition M\u2217ji = 1\u2212M\u2217ij for all i, j \u2208 [n].1"}, {"heading": "2.1 Estimation of pairwise comparison probabilities", "text": "Given any matrix M\u2217 \u2208 [0, 1]n\u00d7n with M\u2217ij = 1\u2212M\u2217ji for every (i, j), suppose that we observe a random matrix Y \u2208 {0, 1}n\u00d7n with (upper-triangular) independent Bernoulli entries, in particular, with P[Yij = 1] = M\u2217ij for every 1 \u2264 i \u2264 j \u2264 n and Yji = 1 \u2212 Yij . Based on observing Y , our goal in this paper is to recover an accurate estimate, in the squared Frobenius norm, of the full matrix M\u2217.\nOur primary focus in this paper will be on the setting where for n items we observe the outcome of a single pairwise comparison for each pair. We will subsequently also address the more general case when we have partial observations, that is, when each pairwise comparison is observed with a fixed probability.2\n1In other words, the shifted matrix M\u2217 \u2212 1 2\nis skew-symmetric. 2The extension to other observation models, with multiple and/or uneven observations across different pairs\nfor instance, is also interesting but beyond the scope of this work.\nFor future reference, note that we can always write the Bernoulli observation model in the linear form\nY = M\u2217 +W, (1)\nwhere W \u2208 [\u22121, 1]n\u00d7n is a random matrix with independent zero-mean entries for every i \u2265 j given by\nWij \u223c { 1\u2212M\u2217ij with probability M\u2217ij \u2212M\u2217ij with probability 1\u2212M\u2217ij ,\n(2)\nand Wji = \u2212Wij for every i < j. This linearized form of the observation model is convenient for subsequent analysis."}, {"heading": "2.2 Strong stochastic transitivity", "text": "Beyond the previously mentioned constraints on the matrix M\u2217\u2014namely that M\u2217ij \u2208 [0, 1] and that M\u2217ij = 1 \u2212M\u2217ij\u2014more structured and interesting models are obtained by imposing further restrictions on the entries of M\u2217. We now turn to one such condition, known as strong stochastic transitivity (SST), which reflects the natural transitivity of any complete ordering. Formally, suppose that the full collection of items [n] is endowed with a complete ordering \u03c0\u2217. We use the notation \u03c0\u2217(i) \u03c0\u2217(j) to convey that item i is preferred to item j in the total ordering \u03c0\u2217. Consider some triple (i, j, k) such that \u03c0\u2217(i) \u03c0\u2217(j). A matrix M\u2217 satisfies the SST condition if the inequality M\u2217ik \u2265 M\u2217jk holds for every such triple. The intuition underlying this constraint is as follows: since i dominates j in the true underlying order, when we make noisy comparisons, the probability that i is preferred to k should be at least as large as the probability that j is preferred to k. The SST condition was first described3 in the psychology literature (e.g., [Fis73, DM59]).\nThe SST condition is characterized by the existence of a permutation such that the permuted matrix has entries that increase across rows and decrease down columns. More precisely, for a given permutation \u03c0\u2217, let us say that a matrix M is \u03c0\u2217-faithful if for every pair (i, j) such that \u03c0\u2217(i) \u03c0\u2217(j), we have Mik \u2265Mjk for all k \u2208 [n]. With this notion, the set of SST matrices is given by\nCSST = { M \u2208 [0, 1]n\u00d7n | Mba = 1\u2212Mab \u2200 (a, b), and \u2203 perm. \u03c0\u2217 s.t. M is \u03c0\u2217-faithful } . (3)\nNote that the stated inequalities also guarantee that for any pair with \u03c0\u2217(i) \u03c0\u2217(j), we have Mki \u2264 Mkj for all k, which corresponds to a form of column ordering. The class CSST will be our primary focus in this paper."}, {"heading": "2.3 Classical parametric models", "text": "Let us now describe a family of classical parametric models, one which includes BradleyTerry-Luce and Thurstone (Case V) models [BT52, Luc59, Thu27]. As we will see, they can be understood as inducing a relatively small subclass of the SST matrices CSST.\nIn more detail, parametric models are described by a weight vector w\u2217 \u2208 Rn that corresponds to the notional qualities of the n items. Moreover, consider any non-decreasing\n3We note that the psychology literature has also considered what are known as weak and moderate stochastic transitivity conditions. From a statistical standpoint, pairwise comparison probabilities cannot be consistently estimated in a minimax sense under these conditions. We establish this formally in Appendix C.\nfunction F : R 7\u2192 [0, 1] such that F (t) = 1\u2212F (\u2212t) for all t \u2208 R; we refer to any such function F as being valid. Any such pair (F,w\u2217) induces a particular pairwise comparison model in which\nM\u2217ij = F (w \u2217 i \u2212 w\u2217j ) for all pairs (i, j). (4)\nFor each valid choice of F , we define CPAR(F ) = { M \u2208 [0, 1]n\u00d7n |M induced by Equation (4) for some w\u2217 \u2208 Rn } . (5)\nFor any choice of F , it is straightforward to verify that CPAR(F ) is a subset of CSST, meaning that any matrix M induced by the relation (4) satisfies all the constraints defining the set CSST. As particular important examples, we recover the Thurstone model by setting F (t) = \u03a6(t) where \u03a6 is the Gaussian CDF, and the Bradley-Terry-Luce model by setting F (t) = e t\n1+et , corresponding to the sigmoid function.\nRemark: One can impose further constraints on the vector w\u2217 without reducing the size of the class {CPAR(F ), for some valid F}. In particular, since the pairwise probabilities depend only on the differences w\u2217i \u2212 w\u2217j , we can assume without loss of generality that \u3008w\u2217, 1\u3009 = 0. Moreover, since the choice of F can include rescaling its argument, we can also assume that \u2016w\u2217\u2016\u221e \u2264 1. Accordingly, we assume in our subsequent analysis that w\u2217 belongs to the set{ w \u2208 Rn | such that \u3008w, 1\u3009 = 0 and \u2016w\u2016\u221e \u2264 1. } ."}, {"heading": "2.4 Inadequacies of parametric models", "text": "As noted in the introduction, a large body of past work (e.g., [DM59, ML65, Tve72, BW97]) has shown that parametric models, of the form (5) for some choice of F , often provide poor fits to real-world data. What might be a reason for this phenomenon? Roughly, parametric models impose the very restrictive assumption that the choice of an item depends on the value of a single latent factor (as indexed by w\u2217)\u2014e.g., that our preference for cars depends only on their fuel economy, or that the probability that one hockey team beats another depends only on the skills of the goalkeepers.\nThis intuition can be formalized to construct matrices M\u2217 \u2208 CSST that are far away from every valid parametric approximation as summarized in the following result:\nProposition 1. For every n \u2265 4, there exist matrices M\u2217 \u2208 CSST such that\ninf valid F inf M\u2208CPAR(F )\n|||M \u2212M\u2217|||2F \u2265 Kn2, (6)\nfor some universal constant K > 0.\nGiven the interval constraints [0, 1], the Frobenius norm diameter of the class CSST is at most n2, so that the quadratic scaling of the lower bound (6) cannot be sharpened.\nWhat sort of matrices M\u2217 are \u201cbad\u201d in the sense of satisfying a lower bound of the form (6)? Panel (a) of Figure 1 describes the construction of one \u201cbad\u201d matrix M\u2217. To provide some intuition, let us return to the analogy of rating cars. A key property of any parametric model is that if we prefer car 1 to car 2 more than we prefer car 3 to car 4, then we must also prefer car 1 to car 3 more than we prefer car 2 to car 4.4 This condition is\n4This condition follows from the proof of Proposition 1.\npotentially satisfied if there is a single determining factor across all cars, for instance, the fuel economy.\nThis ordering condition is, however, violated by the pairwise matrix M\u2217 from Figure 1(a). In this example, we have M\u221712 = 6 8 > 5 8 = M \u2217 34 and M \u2217 13 = 7 8 < 8 8 = M \u2217 24. Such an occurrence can be explained by a simple two-factor model: suppose the fuel economies of cars 1, 2, 3 and 4 are 20, 18, 12 and 6 kilometers per liter respectively, and the comfort levels of the four cars are also ordered as 1 2 3 4. Suppose that in a pairwise comparison of two cars, if one car is more fuel efficient by at least 10 kilometers per liter, it is always chosen. Otherwise the choice is governed by a parametric choice model acting on the respective comfort levels of the two cars. Observe that while the comparisons between the pairs (1, 2), (3, 4) and (1, 3) of cars can be explained by this parametric model acting on their respective comfort levels, the preference between cars 1 and 4, as well as between cars 2 and 4, is governed by their fuel economies. This two-factor model accounts for the said values of M\u221712, M \u2217 34, M \u2217 24 and M \u2217 13, which violate parametric requirements. While this was a simple hypothetical example, there is a more ubiquitous phenomenon underlying our example. It is often the case that our preferences are decided by comparing items on a multitude of dimensions. In any situation where a single (latent) parameter per item does not adequately explain our preferences, one can expect that the class of parametric models to provide a poor fit to the pairwise preference probabilities.\nThe lower bound on approximation quality guaranteed by Proposition 1 means that any parametric estimator of the matrix M\u2217 should perform poorly. This expectation is confirmed by the simulation results in panel (b) of Figure 1. After generating observations from a \u201cbad matrix\u201d over a range of n, then fit the data set using either the maximum likelihood estimate in the Thurstone model, or the singular value thresholding (SVT) estimator, to be discussed in Section 3.2. For each estimator M\u0302 , we plot the rescaled Frobenius norm error |||M\u0302\u2212M\u2217|||2F\nn2\nversus the sample size. Consistent with the lower bound (6), the error in the Thurstone-based estimator stays bounded above a universal constant. In contrast, the SVT error goes to zero with n, and as our theory in the sequel shows, the rate at which the error decays is at least\nas fast as 1/ \u221a n."}, {"heading": "3 Main results", "text": "Thus far, we have introduced two classes of models for matrices of pairwise comparison probabilities. Our main results provide characterizations of the estimation error associated with different subsets of these classes, using either optimal estimators (that we suspect are not polynomial-time computable), or more computationally efficient estimators that can be computed in polynomial-time."}, {"heading": "3.1 Characterization of the minimax risk", "text": "We begin by providing a result that characterizes the minimax risk in squared Frobenius norm over the class CSST of SST matrices. It involves taking an infimum over the set of all possible estimators, which are measurable functions Y 7\u2192 M\u0303 \u2208 [0, 1]n\u00d7n. Here the data matrix Y \u2208 {0, 1}n\u00d7n is drawn from the observation model (1).\nTheorem 1. There are universal constants 0 < K` < Ku such that\nK` n \u2264 inf M\u0303 sup M\u2217\u2208CSST 1 n2 E[|||M\u0303 \u2212M\u2217|||2F] \u2264 Ku\nlog2(n)\nn , (7)\nwhere the infimum ranges over all measurable functions M\u0303 of the data matrix.\nWe prove this theorem in Section 5.1. The proof of the lower bound is based on extracting a particular subset of CSST such that any matrix in this subset has at least n positions that are unconstrained, apart from having to belong to the interval [12 , 1]. We can thus conclude that estimation of the full matrix is at least as hard as estimating n numbers belonging to the interval [12 , 1] based on a single observation per number, and this leads to an \u2126(n\n\u22121) lower bound, as stated.\nProving an upper bound requires substantially more effort. In particular, we establish it via careful analysis of the constrained least-squares estimator\nM\u0302 \u2208 arg min M\u2208CSST |||Y \u2212M |||2F. (8a)\nIn particular, we prove that there are universal constants (c0, c1, c2) such that, for any matrix M\u2217 \u2208 CSST, this estimator satisfies the high probability bound\nP [ 1 n2 |||M\u0302 \u2212M\u2217|||2F \u2265 c0 log2(n) n ] \u2264 c1e\u2212c2n. (8b)\nSince the entries of M\u0302 and M\u2217 all lie in the interval [0, 1], this tail bound implies the upper bound on the expected mean-squared error stated in Equation (7). Proving the high probability bound (8b) requires sharp control on a quantity known as the localized Gaussian complexity of the class CSST. We use Dudley\u2019s entropy integral in order to derive an upper bound that is sharp up to a logarithmic factor; doing so in turn requires deriving upper bounds on the metric entropy of the class CSST for which we leverage the prior work of Gao and Wellner [GW07].\nWe do not know whether the constrained least-squares estimator (8a) is computable in time polynomial in n, but we suspect not. This complexity is a consequence of the fact that\nthe set CSST is not convex, but is a union of n! convex sets. Given this issue, it becomes interesting to consider the performance of alternative estimators that can be computed in polynomial-time."}, {"heading": "3.2 Sharp analysis of singular value thresholding (SVT)", "text": "The first polynomial-time estimator that we consider is a simple estimator based on thresholding singular values of the observed matrix Y , and reconstructing its truncated SVD. For the full class CSST, Chatterjee [Cha14] analyzed the performance of such an estimator and proved that the squared Frobenius error decays as O(n\u2212 1 4 ) uniformly over CSST. In this section, we prove that its error decays as O(n\u2212 1 2 ), again uniformly over CSST, and moreover, that this upper bound is unimprovable. Let us begin by describing the estimator. Given the observation matrix Y \u2208 Rn\u00d7n, we can write its singular value decomposition as Y = UDV T , where the (n\u00d7n) matrix D is diagonal, whereas the (n\u00d7 n) matrices U and V are orthonormal. Given a threshold level \u03bbn > 0, the soft-thresholded version of a diagonal matrix D is the diagonal matrix T\u03bbn(D) with values\n[T\u03bbn(D)]jj = max{0, Djj \u2212 \u03bbn} for every integer j \u2208 [1, n]. (9)\nWith this notation, the soft singular-value-thresholded (soft-SVT) version of Y is given by T\u03bbn(Y ) = UT\u03bbn(D)V\nT . The following theorem provides a bound on its squared Frobenius error:\nTheorem 2. There are universal positive constants (Ku, c0, c1) such that the soft-SVT estimator M\u0302\u03bbn = T\u03bbn(Y ) with \u03bbn = 2.1 \u221a n satisfies the bound\nP [ 1 n2 |||M\u0302\u03bbn \u2212M\u2217|||2F \u2265 Ku\u221a n ] \u2264 c0e\u2212c1n (10a)\nfor any M\u2217 \u2208 CSST. Moreover, there is a universal constant K` > 0 such that for any choice of \u03bbn, we have\nsup M\u2217\u2208CSST\n1\nn2 |||M\u0302\u03bbn \u2212M\u2217|||2F \u2265 K`\u221a n . (10b)\nA few comments on this result are in order. Since the matrices M\u0302\u03bbn and M \u2217 have entries in the unit interval [0, 1], the normalized squared error 1 n2 |||M\u0302\u03bbn \u2212M\u2217|||2F is at most 1. Consequently, tail bound (10a) guarantees that\nsup M\u2217\u2208CSST\nE[ 1\nn2 |||M\u0302\u03bbn \u2212M\u2217|||2F] \u2264 Ku\u221a n + c0e \u2212c1n \u2264 K \u2032 u\u221a n . (11)\nOn the other hand, the matching lower bound (10b) holds with probability one, meaning that the soft-SVT estimator incurs an error of the order 1/ \u221a n irrespective of the realization of the noise. To be clear, Chatterjee [Cha14] actually analyzed the hard-SVT estimator, which is based on the hard-thresholding operator\n[H\u03bbn(D)]jj = Djj 1{Djj \u2265 \u03bbn}.\nHere 1{\u00b7} denotes the 0-1-valued indicator function. In this setting, the hard-SVT estimator is simply, H\u03bbn(Y ) = UH\u03bbn(D)V T . With essentially the same choice of \u03bbn as above, Chatterjee\nshowed that the estimate H\u03bbn(Y ) has a mean-squared error of O(n\u22121/4). One can verify that the proof of Theorem 2 in our paper goes through for the hard-SVT estimator as well. Consequently the performance of the hard-SVT estimator is of the order \u0398(n\u22121/2), and is identical to that of the soft-thresholded version up to universal constants.\nTogether the upper and lower bounds of Theorem 2 provide a sharp characterization of the behavior of the soft/hard SVT estimators. On the positive side, these are easily implementable estimators that achieve a mean-squared error bounded by O(1/ \u221a n) uniformly over the entire class CSST. On the negative side, this rate is slower than the O(log2 n/n) rate achieved by the least-squares estimator, as in Theorem 1.\nIn conjunction, Theorems 1 and 2 raise a natural open question: is there a polynomialtime estimator that achieves the minimax rate uniformly over the family CSST? We do not know the answer to this question, but the following subsections provide some partial answers by analyzing some polynomial-time estimators that (up to logarithmic factors) achieve the optimal O\u0303(1/n)-rate over some interesting sub-classes of CSST. In the next two sections, we turn to results of this type."}, {"heading": "3.3 Optimal rates for high SNR subclass", "text": "In this section, we describe a multi-step polynomial-time estimator that (up to logarithmic factors) can achieve the optimal O\u0303(1/n) rate over an interesting subclass of CSST. This subset corresponds to matrices M that have a relatively high signal-to-noise ratio (SNR), meaning that no entries of M fall within a certain window of 1/2. More formally, for some \u03b3 \u2208 (0, 12), we define the class\nCHIGH(\u03b3) = {M \u2208 CSST | max(Mij ,Mji) \u2265 1/2 + \u03b3 \u2200 i 6= j} . (12)\nFor any matrix CHIGH(\u03b3), the amount of information contained in each observation is bounded away from zero uniformly in n, as opposed to matrices in which some large subset of entries have values equal (or arbitrarily close) to 12 . In terms of estimation difficulty, this SNR restriction does not make the problem substantially easier: as the following theorem shows, the minimax mean-squared error remains lower bounded by a constant multiple of 1/n. Moreover, we can demonstrate a polynomial-time algorithm that achieves this optimal mean squared error up to logarithmic factors.\nThe following theorem applies to any fixed \u03b3 \u2208 (0, 12 ] independent of n, and involves constants (K`,Ku, c) that may depend on \u03b3 but are independent of n.\nTheorem 3. There is a constant K` > 0 such that\ninf M\u0303 sup M\u2217\u2208CHIGH(\u03b3)\n1 n2 E [ |||M\u0303 \u2212M\u2217|||2F] \u2265 K` n , (13a)\nwhere the infimum ranges over all estimators. Moreover, there is a two-stage estimator M\u0302 , computable in polynomial-time, for which\nP [ 1 n2 |||M\u0302 \u2212M\u2217|||2F \u2265 Ku log 2(n) n ] \u2264 c n2 , (13b)\nvalid for any M\u2217 \u2208 CHIGH(\u03b3).\nAs before, since the ratio 1 n2 |||M\u0302 \u2212M\u2217|||2F is at most 1, so the tail bound (13b) implies that\nsup M\u2217\u2208CHIGH(\u03b3)\nE [ |||M\u0302 \u2212M\u2217|||2F\nn2\n] \u2264 Ku log 2(n)\nn +\nc n2 \u2264 K\n\u2032 u log 2(n)\nn . (14)\nWe provide the proof of this theorem in Section 5.3. As with our proof of the lower bound in Theorem 1, we prove the lower bound by considering the sub-class of matrices that are free only on the two diagonals just above and below the main diagonal. We now provide a brief sketch for the proof of the upper bound (13b) which is based on analyzing the following two-step procedure:\n1. In the first step of algorithm, we find a permutation \u03c0\u0302FAS of the n items that minimizes the total number of disagreements with the observations. (For a given ordering, we say that any pair of items (i, j) are in disagreement with the observation if either i is rated higher than j in the ordering and Yij = 0, or if i is rated lower than j in the ordering and Yij = 1.) The problem of finding such a disagreement-minimizing permutation \u03c0\u0302FAS is commonly known as the minimum feedback arc set (FAS) problem. It is known to be NP-hard in the worst-case [ACN08, Alo06], but our set-up has additional probabilistic structure that allows for polynomial-time solutions with high probability. In particular, we call upon a polynomial-time algorithm due to Braverman and Mossel [BM08] that, under the model (12), is guaranteed to find the exact solution to the FAS problem with high probability. Viewing the FAS permutation \u03c0\u0302FAS as an approximation to the true permutation \u03c0\u2217, the novel technical work in this first step is show that \u03c0\u0302FAS is \u201cgood enough\u201d for Frobenius norm estimation, in the sense that for any M\u2217 \u2208 CHIGH(\u03b3), it satisfies the bound\n1\nn2 |||\u03c0\u2217(M\u2217)\u2212 \u03c0\u0302FAS(M\u2217)|||2F \u2264\nK log n\nn (15a)\nwith high probability. In this statement, for any given permutation \u03c0, we have used \u03c0(M\u2217) to denote the matrix obtained by permuting the rows and columns of M\u2217 by \u03c0. The term 1 n2 |||\u03c0\u2217(M\u2217) \u2212 \u03c0\u0302FAS(M\u2217)|||2F can be viewed in some sense as the bias in estimation incurred from using \u03c0\u0302FAS in place of \u03c0 \u2217.\n2. Define CBISO be the class of matrices M \u2208 [0, 1]n\u00d7n that satisfy the linear constraints Mij = 1 \u2212Mji for all (i, j) \u2208 [n]2, and Mk` \u2265 Mij whenever k \u2264 i and ` \u2265 j. This class corresponds to the subset of matrices CSST that are faithful with respect to the identity permutation. Letting \u03c0\u0302FAS(CBISO) = {\u03c0\u0302FAS(M), M \u2208 CBISO} denote the image of this set under \u03c0\u0302FAS, the second step involves computing the constrained least-squares estimate\nM\u0302 \u2208 arg min M\u2208\u03c0\u0302FAS(CBISO) |||Y \u2212M |||2F. (15b)\nSince the set \u03c0\u0302FAS(CBISO) is a convex polytope, with a number of facets that grows polynomially in n, the constrained quadratic program (15b) can be solved in polynomial-time. The\nfinal step in the proof of Theorem 3 is to show that the estimator M\u0302 also has mean-squared error that is upper bounded by a constant multiple of log 2(n) n with high probability.\nOur analysis shows that for any fixed \u03b3 \u2208 (0, 12 ], the proposed two-step estimator works well for any matrix M\u2217 \u2208 CHIGH(\u03b3). Since this two-step estimator is based on finding a minimum feedback arc set (FAS) in the first step, it is natural to wonder whether an FASbased estimator works well over the full class CSST. Somewhat surprisingly the answer to this question turns out to be negative: we refer the reader to Appendix B for more intuition and details on why the minimal FAS estimator does not perform well over the full class."}, {"heading": "3.4 Optimal rates for parametric subclasses", "text": "Let us now return to the class of parametric models CPAR(F ) introduced earlier in Section 2.3. As shown previously in Proposition 1, this class is much smaller than the class CSST, in the sense that there are models in CSST that cannot be well-approximated by any parametric model. Nonetheless, in terms of minimax rates of estimation, these classes differ only by logarithmic factors. An advantage of the parametric class is that it is possible to achieve the 1/n minimax rate by using a simple, polynomial-time estimator. In particular, for any log concave function F , the maximum likelihood estimate w\u0302ML can be obtained by solving a convex program. This MLE induces a matrix estimate M(w\u0302ML) via Equation (4), and the following result shows that this estimator is minimax-optimal up to constant factors.\nTheorem 4. Suppose that F is strictly increasing, strongly log-concave and twice differentiable. Then there is a constant K` > 0, depending only on F , such that the minimax risk over CPAR(F ) is lower bounded as\ninf M\u0303 sup M\u2217\u2208CPAR(F )\n1\nn2 E[|||M\u0303 \u2212M\u2217|||2F] \u2265 K` n , (16a)\nConversely, there is a constant Ku \u2265 K`, depending only on F , such that the matrix estimate M(w\u0302ML) induced by the MLE satisfies the bound\nsup M\u2217\u2208CPAR(F )\n1\nn2 E[|||M(w\u0302ML)\u2212M\u2217|||2F] \u2264 Ku n . (16b)\nTo be clear, the constants (K`,Ku) in this theorem are independent of n, but they do depend on the specific properties of a given function F . We note that the stated conditions on F are true for many popular parametric models, including (for instance) the Thurstone and BTL models.\nWe provide the proof of Theorem 4 in Section 5.4. The lower bound (16a) is, in fact, stronger than the the lower bound in Theorem 1, since the supremum is taken over a smaller class. The proof of the lower bound in Theorem 1 relies on matrices that cannot be realized by any parametric models, so that we pursue a different route to establish the bound (16a). On the other hand, in order to prove the upper bound (16b), we make use of bounds on the accuracy of the MLE w\u0302ML from our own past work (see the paper [SBB +15])."}, {"heading": "3.5 Extension to partial observations", "text": "We now consider the extension of our results to the setting in which not all entries of Y are observed. Suppose instead that every entry of Y is observed independently with probability pobs. In other words, the set of pairs compared is the set of edges of an Erdo\u030bs-Re\u0301nyi graph G(n, pobs) that has the n items as its vertices. In this setting, we obtain upper bounds by setting Yij = 1 2 whenever the pair (i, j) is not compared, and applying the algorithms studied earlier to the resulting matrix Y .\nTheorem 5. In the partial observation setting described above:\n(a) There are positive universal constants K` and Ku such that the minimax risk is sandwiched as\nK` max { 1 pobsn , 1 n2 } \u2264 inf M\u0303 sup M\u2217\u2208CSST 1 n2 E[|||M\u0302 \u2212M\u2217|||2F] \u2264 Ku min {(log n)2 p2obsn , 1 pobs \u221a n } .\n(17a)\n(b) The soft-SVT estimator M\u0302\u03bbn,pobs with \u03bbn,pobs = 2.1 \u221a n pobs satisfies the bound:\nsup M\u2217\u2208CSST\n1\nn2 E[|||M\u0302\u03bbn,pobs \u2212M\n\u2217|||2F] \u2264 Ku\npobs \u221a n . (17b)\n(c) For a parametric sub-class based on a strongly log-concave and smooth F , there is a\npolynomial-time estimator M\u0302 such that\nsup M\u2217\u2208CPAR(F )\n1\nn2 E[|||M\u0302 \u2212M\u2217|||2F] \u2264 Ku pobsn , valid when pobs \u2265 K (logn) 2 n , (17c)\nwhere K > 0 is a universal constant.\nProving the lower bound in Equation (17a) requires a nuanced construction of a packing set of matrices in CSST that depends on pobs. The upper bound in Equation (17a) follows from a modification of the analysis of the least-squares estimator (8a) in the proof of Theorem 1. Apart from the logarithmic factors, the main difference between the upper and lower bounds is the (pobsn) \u22121 scaling of the lower bound as compared to the (p2obsn) \u22121 scaling of the upper bound. Since the expected number of observed entries scales proportionally with pobs (as opposed to p2obs), we conjecture that the lower bound is tight (up to logarithmic factors).\nThe proof of the upper bound (17c) for the parametric models is based on an extension of corresponding proof from the case of complete observations. In general, we are interested in scalings of pobs, which allow pobs \u2192 0 as n\u2192\u221e. We note that we do not have an analogue of the high-SNR result in the partial observations case since having partial observations reduces the SNR; the noisy-sorting algorithm of Braverman and Mossel [BM08] for the high-SNR case requires a computational complexity of e\u03b3 \u22124 and hence is not computable in time polynomial in n when \u03b3 < (log n)\u2212 1 4 , and this disallows most interesting scalings of pobs with n."}, {"heading": "4 Simulations", "text": "In this section, we present results from simulations to gain a further understanding of the problem at hand, in particular to understand the rates of estimation under specific generative models. We investigate the performance of the soft-SVT estimator (Section 3.2) and the maximum likelihood estimator under the Thurstone model (Section 2.3).5 The output of the SVT estimator need not lie in the set [0, 1]n\u00d7n of matrices; in our implementation, we take a projection of the output of the SVT estimator on this set, which gives a constant factor reduction in the error.\nIn our simulations, we generate the ground truth M\u2217 in the following five ways: \u2022 Uniform: The matrix M\u2217 is generated by drawing ( n 2 ) values independently and uniformly\nat random in [12 , 1] and sorting them in descending order. The items are then inserted above the diagonal of an (n\u00d7n) matrix such that the entries decrease down a column or left along a row. We then make the matrix skew-symmetric and permute the rows and columns.\n5We could not compare the algorithm that underlies Theorem 3, since it is not easily implementable. In particular, it relies on the algorithm due to Braverman and Mossel [BM08] to compute the feedback arc set minimizer. The computational complexity of this algorithm, though polynomial in n, has a very large polynomial degree which precludes it from being implemented for matrices of any reasonable size.\nThe simulations in this section add to the simulation results of Section 2.4 (Figure 1) demonstrating a large class of matrices in the SST class that cannot be represented by any parametric class.\n\u2022 Thurstone: The weight vector M\u2217 is generated by first choosing w\u2217 uniformly at random from the set satisfying \u3008w\u2217, 1\u3009 = 0 and \u2016w\u2217\u2016\u221e \u2264 1, and then setting M\u2217ij = \u03a6(w\u2217i \u2212 w\u2217j ) where \u03a6 is the CDF of the standard normal distribution. The matrix M\u2217 is then generated from w\u2217 via Equation (4).\n\u2022 Bradley-Terry-Luce (BTL): Identical to the Thurstone case, except that F is given by the sigmoid function.\n\u2022 High SNR: A setting studied previously by Braverman and Mossel [BM08], in which the noise is independent of the items being compared. Some global order is fixed over the n items, and the comparison matrix M\u2217 takes the values M\u2217ij = 0.9 = 1\u2212M\u2217ji for every pair (i, j) where i is ranked above j in the underlying ordering. The entries on the diagonal are 0.5.\n\u2022 Independent bands: The matrix M\u2217 is chosen with diagonal entries all equal to 12 . Entries on diagonal band immediately above the diagonal itself are chosen i.i.d. and uniformly at random from [12 , 1]\nn\u22121. The band above is then chosen uniformly at random from the allowable set, and so on. The choice of any entry in this process is only constrained to be upper bounded by 1 and lower bounded by the entries to its left and below. The entries below the diagonal are chosen to make the matrix skew-symmetric.\nFigure 2 depicts the results of the simulations based on observations of the entire matrix Y . Each point is an average across T = 20 trials. The error bars in most cases are too small and hence not visible. We see that the uniform case (Figure 2a) is favorable for both estimators, with the error scaling as O( 1\u221a\nn ). With data generated from the Thurstone model, both\nestimators continue to perform well, and the Thurstone MLE yields an error of the order 1n (Figure 2b). Interestingly, the Thurstone model also fits relatively well when data is generated via the BTL model (Figure 2c). This behavior is likely a result of operating in the near-linear regime of the logistic and the Gaussian CDF where the two curves are similar. In these two parametric settings, the SVT estimator incurs an error strictly worse than order 1n but better than 1\u221a\nn . The Thurstone model, however, yields a poor fit for the model in the high-\nSNR (Figure 2d) and the independent bands (Figure 2e) cases, incurring a constant error as compared to an error scaling as O( 1\u221a\nn ) for the SVT estimator. We recall that the poor\nperformance of the Thurstone estimator was also described previously in Proposition 1. In summary, we see that while the Thurstone MLE estimator gives minimax optimal rates of estimation when the underlying model is parametric, it can be inconsistent when the parametric assumptions are violated. On the other hand, the SVT estimator is robust to violations of parametric assumptions, and while it does not necessarily give minimax-optimal rates, it remains consistent across the entire SST class."}, {"heading": "5 Proofs of main results", "text": "This section is devoted to the proofs of our main results\u2013namely, Theorems 1 through 5. Throughout these and other proofs, we use the notation {c, c\u2032, c0, C, C \u2032} and so on to denote positive constants whose values may change from line to line. In addition, we assume throughout that n is lower bounded by a universal constant so as to avoid degeneracies. For any square matrix A \u2208 Rn\u00d7n, we let {\u03c31(A), . . . , \u03c3n(A)} denote its singular values (ordered from largest to smallest), and similarly, for any symmetric matrix M \u2208 Rn\u00d7n, we let {\u03bb1(M), . . . , \u03bbn(M)} denote its ordered eigenvalues."}, {"heading": "5.1 Proof of Theorem 1", "text": "This section is devoted to the proof of Theorem 1, including both the upper and lower bounds on the minimax risk in squared Frobenius norm."}, {"heading": "5.1.1 Proof of upper bound", "text": "Define the difference \u2206\u0302 = M\u0302\u2212M\u2217 between M\u2217 and the optimal solution M\u0302 to the constrained least-squares problem. Since M\u0302 is optimal andM\u2217 is feasible, we have |||Y \u2212M\u0302 |||2F \u2264 |||Y \u2212M\u2217|||2F, and hence following some algebra, we arrive at the basic inequality\n1 2 |||\u2206\u0302|||2F \u2264 \u3008\u3008\u2206\u0302, W \u3009\u3009,\nwhere W \u2208 Rn\u00d7n is the noise matrix in the observation model (1), and \u3008\u3008A, B\u3009\u3009 = trace(ATB) is the trace inner product.\nWe introduce some additional objects that are useful in our analysis. First, we recall our earlier definition of the class of bivariate isotonic matrices under the identity permutation, CBISO \u2208 [0, 1]n\u00d7n, is the set:\nCBISO : = { M \u2208 [0, 1]n\u00d7n | |Mk` \u2265Mij whenever k \u2264 i and ` \u2265 j } . (18)\nFor a given permutation \u03c0 and matrix M , we let \u03c0(M) denote the matrix obtained by applying \u03c0 to its rows and columns. Now let CDIFF \u2208 [\u22121, 1]n\u00d7n be the set of differences between pairs of matrices in CBISO, that is,\nCDIFF : = { \u03c01(M1)\u2212 \u03c02(M2) | for some M1, M2 \u2208 CBISO, and perm. \u03c01 and \u03c02 } . (19)\nOne can verify that for any M\u2217 \u2208 CSST, we are guaranteed the inclusion\n{M \u2212M\u2217 |M \u2208 CSST, |||M \u2212M\u2217|||F \u2264 t} \u2282 {D \u2208 CDIFF | |||D|||F \u2264 t}.\nNow for each choice of radius t > 0, define the random variable\nZ(t) : = sup D\u2208CDIFF,|||D|||F\u2264t\n\u3008\u3008D, W \u3009\u3009. (20)\nUsing our earlier basic inequality, the Frobenius norm error |||\u2206\u0302|||F then satisfies the bound\n1 2 |||\u2206\u0302|||2F \u2264 \u3008\u3008\u2206\u0302, W \u3009\u3009 \u2264 Z\n( |||\u2206\u0302|||F ) . (21)\nThus, in order to obtain a high probability bound, we need to understand the behavior of the random quantity Z(\u03b4).\nOne can verify that the set CDIFF is star-shaped, meaning that \u03b1D \u2208 CDIFF for every \u03b1 \u2208 [0, 1] and every D \u2208 CDIFF. Using this star-shaped property, we are guaranteed that there is a non-empty set of scalars \u03b4n > 0 satisfying the critical inequality\nE[Z(\u03b4n)] \u2264 \u03b42n 2 . (22)\nOur interest is in the smallest (strictly) positive solution \u03b4n to the critical inequality (22), and moreover, our goal is to show that for every t \u2265 \u03b4n, we have |||\u2206\u0302|||F \u2264 c \u221a t\u03b4n with probability at least 1\u2212 c1e\u2212c2nt\u03b4n . Define a \u201cbad\u201d event At as\nAt = { \u2203\u2206 \u2208 CDIFF | |||\u2206|||F \u2265 \u221a t\u03b4n and \u3008\u3008\u2206, W \u3009\u3009 \u2265 2|||\u2206|||F \u221a t\u03b4n } . (23)\nUsing the star-shaped property of CDIFF, it follows by a rescaling argument that P[At] \u2264 P[Z(\u03b4n) \u2265 2\u03b4n \u221a t\u03b4n] for all t \u2265 \u03b4n.\nThe entries of W lie in [\u22121, 1] and hence are 1-sub-Gaussian, are i.i.d. on and above the diagonal, are zero-mean, and satisfy skew-symmetry. Moreover, the function W 7\u2192 Z(t) is convex and Lipschitz with parameter t. Consequently, by Ledoux\u2019s concentration theorem [Led01, Theorem 5.9], we have\nP [ Z(\u03b4n) \u2265 E[Z(\u03b4n)] + \u221a t\u03b4n\u03b4n ] \u2264 2e\u2212c1t\u03b4n for all t \u2265 \u03b4n.\nBy the definition of \u03b4n, we have E[Z(\u03b4n)] \u2264 \u03b42n \u2264 \u03b4n \u221a t\u03b4n for any t \u2265 \u03b4n, and consequently\nP[At] \u2264 P[Z(\u03b4n) \u2265 2\u03b4n \u221a t\u03b4n ] \u2264 2e\u2212c1t\u03b4n for all t \u2265 \u03b4n.\nConsequently, either |||\u2206\u0302|||F \u2264 \u221a t\u03b4n, or we have |||\u2206\u0302|||F > \u221a t\u03b4n. In the latter case, conditioning on the complement Act , our basic inequality implies that 12 |||\u2206\u0302||| 2 F \u2264 2|||\u2206\u0302|||F \u221a t\u03b4n, and hence\n|||\u2206\u0302|||F \u2264 4 \u221a t\u03b4n with probability at least 1 \u2212 2e\u2212c1n logn. Putting together the pieces yields that\n|||\u2206\u0302|||F \u2264 c0 \u221a t\u03b4n (24)\nwith probability at least 1\u2212 2e\u2212c1t\u03b4n for every t \u2265 \u03b4n. In order to determine a feasible \u03b4n satisfying the critical inequality (22), we need to bound the expectation E[Z(\u03b4n)]. We do using Dudley\u2019s entropy integral and bounding the metric entropies of certain sub-classes of matrices. In particular, the remainder of this section is devoted to proving the following claim:\nLemma 1. There is a universal constant C such that for all t \u2208 [0, 2n], E[Z(t)] \u2264 C { n log2(n) + t \u221a n log n } . (25)\nTaking this lemma as given for the moment, we see that the critical inequality (22) is satisfied for \u03b4n = C \u2032\u221an log n, and hence from our bound (24), we have\n|||\u2206\u0302|||2F n2 \u2264 C \u2032\u2032 log 2(n) n ,\nwith probability at least 1 \u2212 2e\u2212c1n(logn)2 , where C \u2032\u2032 and c1 are positive universal constants. This argument completes the proof of the upper bound.\nIt remains to prove Lemma 1, and we do so by using Dudley\u2019s entropy integral, as well as some auxiliary results on metric entropy. We use the notation logN( ,C, \u03c1) to denote the metric entropy of the class C in the metric \u03c1. Our proof requires the following auxiliary lemma:\nLemma 2. For every > 0, we have the metric entropy bound\nlogN( ,CDIFF, |||.|||F) \u2264 8 n2 2 ( log n )2 + 8n log n.\nSee the end of this section for the proof of this claim. Taking it as given for the moment, let us now prove Lemma 1. Letting BF (t) denote the Frobenius norm ball of radius t, the truncated form of Dudley\u2019s entropy integral inequality yields\nE[Z(t)]] \u2264 c inf \u03b4\u2208[0,n]\n{ n\u03b4 + \u222b t \u03b4 2 \u221a logN( ,CDIFF \u2229 BF (t), |||.|||F)d } \u2264 c { n\u22128 +\n\u222b t 1 2 n\u22129 \u221a logN( ,CDIFF, |||.|||F)d } , (26)\nwhere the second step follows by setting \u03b4 = n\u22129, and making use of the set inclusion (CDIFF \u2229 BF (t)) \u2286 CDIFF. For any \u2265 12n\n\u22129, applying Lemma 2 yields the upper bound\u221a logN( ,CDIFF, |||.|||F) \u2264 c {n log n + \u221a n log n } .\nOver the range \u2265 n\u22129/2, we have log n \u2264 c log n, and hence\u221a logN( ,CDIFF, |||.|||F) \u2264 c {n log n+ \u221a n log n } .\nSubstituting this bound into our earlier inequality (26) yields E[Z(t)] \u2264 c { n\u22128 + ( n log n ) log(nt) + t \u221a n log n } (i)\n\u2264 c {( n log n ) log(n2) + t \u221a n log n } \u2264 c { n log2(n) + t \u221a n log n } ,\nwhere step (i) uses the upper bound t \u2264 2n.\nThe only remaining detail is to prove Lemma 2.\nProof of Lemma 2: We first derive an upper bound on the metric entropy of the class CBISO (defined in (18)). Let F denote the set of all bivariate monotonic functions [0, 1]2 \u2192 [0, 1]2. For any matrix M \u2208 CBISO, choose gM \u2208 F as\ngM (x, y) = Mdn(1\u2212x)e,dnye,\nwhere to handle corner conditions we define M0,i = M1,i and Mi,0 = Mi,1 for all i. Then we have\n\u2016gM\u201622 = \u222b 1 x=0 \u222b 1 y=0 (gM (x, y)) 2dxdy = 1 n2 n\u2211 i=1 n\u2211 j=1 M2i,j = 1 n2 |||M |||2F.\nThus, we have\nlogN( ,CBISO, |||.|||F) \u2264 logN ( n ,F , \u2016.\u20162 ) \u2264 n 2 2 ( log n )2 , (27)\nwhere the last inequality (27) follows from Theorem 1.1. of Gao and Wellner [GW07]. We now bound the metric entropy of CDIFF in terms of the metric entropy of CBISO. For any\n> 0, let C BISO denote an -covering set in CBISO that satisfies the inequality (27). Consider the set\nC DIFF : = {\u03c01(M1)\u2212 \u03c02(M2) | for some permutations \u03c01, \u03c02 and some M1,M2 \u2208 C / \u221a 2 BISO }.\nFor any D \u2208 CDIFF, we can write D = \u03c01(M \u20321) \u2212 \u03c02(M \u20322) for some permutations \u03c01 and \u03c02 and some matrices M \u20321 and M \u2032 2 \u2208 CBISO. We know there exist some M1,M2 \u2208 C / \u221a 2 BISO such that\n|||M \u20321 \u2212M1|||F \u2264 / \u221a 2 and |||M \u20322 \u2212M2|||F \u2264 / \u221a\n2. Then we have \u03c01(M1)\u2212 \u03c02(M2) \u2208 C DIFF, and moreover\n|||D \u2212 (\u03c01(M1)\u2212 \u03c02(M2))|||2F \u2264 |||\u03c01(M1)\u2212 \u03c01(M \u20321)|||2F + |||\u03c02(M2)\u2212 \u03c01(M \u20322)|||2F \u2264 2.\nThus the set C DIFF forms an -covering set for the class CDIFF. One can now count the number of elements in this set to get\nN( ,CDIFF, |||.|||F) \u2264 ( n!N( / \u221a 2,CBISO, |||.|||F) )2 .\nSome straightforward algebraic manipulations yield the claimed result."}, {"heading": "5.1.2 Proof of lower bound", "text": "We now turn to the proof of the lower bound in Theorem 1. We may assume that the correct row/column ordering is fixed and known to be 1 2 \u00b7 \u00b7 \u00b7 n. Here we are using the fact that revealing the knowledge of this ordering cannot make the estimation problem any harder. Recalling the definition (18) of the bivariate isotonic class CBISO, consider the subclass\nC\u2032SST : = {M \u2208 CBISO |Mi,j = 1 when j > i+ 1 and Mi,j = 1\u2212Mj,i when j \u2264 i}\nAny matrix M is this subclass can be identified with the vector q = q(M) \u2208 Rn\u22121 with elements qi : = Mi,i+1. The only constraint imposed on q(M) by the inclusion M \u2208 CSST is that qi \u2208 [12 , 1] for all i = 1, . . . , n\u2212 1.\nIn this way, we have shown that the difficulty of estimating M\u2217 \u2208 C\u2032SST is at least as hard as that of estimating a vector q \u2208 [12 , 1]\nn\u22121 based on observing the random vector Y = {Y1,2, . . . , Yn\u22121,n} with independent coordinates, and such that each Yi,i+1 \u223c Ber(qi). For this problem, it is easy to show that there is a universal constant K` > 0 such that\ninf q\u0302 sup q\u2208[ 1\n2 ,1]n\u22121\nE [ \u2016q\u0302 \u2212 q\u201622 ] \u2265 K`\n2 n,\nwhere the infimum is taken over all measurable functions Y 7\u2192 q\u0302. Putting together the pieces, we have shown that\ninf M\u0302 sup M\u2217\u2208CSST\n1\nn2 E[|||M\u0302 \u2212M\u2217|||2F] \u2265\n2\nn2 inf q\u0302 sup q\u2208[0.5,1]n\u22121 E[\u2016q\u0302 \u2212 q\u201622] \u2265 K` n ,\nas claimed."}, {"heading": "5.2 Proof of Theorem 2", "text": "Recall from (1) that we can write our observation model as Y = M\u2217+W , where W \u2208 Rn\u00d7n is a zero-mean matrix with entries that are drawn independently (except for the skew-symmetry condition) from the interval [\u22121, 1]."}, {"heading": "5.2.1 Proof of upper bound", "text": "Our proof of the upper bound hinges upon the following two lemmas.\nLemma 3. If \u03bbn \u2265 1.01|||W |||op, then\n|||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 c n\u2211 j=1 min { \u03bb2n, \u03c3 2 j (M \u2217) }\nwith probability at least 1\u2212 c1e\u2212c \u2032n, where c, c1 and c \u2032 are positive universal constants.\nOur second lemma is an approximation-theoretic result:\nLemma 4. For any matrix M\u2217 \u2208 CSST and any s \u2208 {1, 2, . . . , n\u2212 1}, we have\n1\nn2 n\u2211 j=s+1 \u03c32j (M \u2217) \u2264 1 s .\nSee the end of this section for the proofs of these two auxiliary results.\nBased on these two lemmas, it is easy to complete the proof of the theorem. The entries of W lie in [\u22121, 1] and hence are 1-sub-Gaussian, are i.i.d. on and above the diagonal, are zero-mean, and satisfy skew-symmetry. Consequently, we may apply Theorem 3.4 of Chatterjee [Cha14], which guarantees that\nP [ |||W |||op > (2 + t) \u221a n ] \u2264 ce\u2212f(t)n,\nwhere c is a universal constant, and the quantity f(t) is strictly positive for each t > 0. Thus, the choice \u03bbn = 2.1 \u221a n guarantees that \u03bbn \u2265 1.01|||W |||op with probability at least 1 \u2212 ce\u2212cn, as is required for applying Lemma 3. Applying this lemma then yields the upper bound\n|||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 c n\u2211 j=1 min { n, \u03c32j (M \u2217) }\nwith probability at least 1\u2212 c1e\u2212c2n. Applying Lemma 4 yields that for any s \u2208 {1, . . . , n},\n1\nn2 |||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 c { s n + 1 s } ,\nwith probability at least 1 \u2212 c1e\u2212c2n. Setting s = d \u221a ne and performing some algebra shows that\nP [ 1 n2 |||T\u03bbn(Y )\u2212M\u2217|||2F > Ku\u221a n ] \u2264 c1e\u2212c2n,\nas claimed. Since 1 n2 |||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 1, we are also guaranteed that\n1\nn2 E[|||T\u03bbn(Y )\u2212M\u2217|||2F] \u2264 Ku\u221a n + c1e \u2212c2n \u2264 K \u2032 u\u221a n .\nProof of Lemma 3 Fix \u03b4 = 0.01. Let b be the number of singular values of M\u2217 above \u03b4\n1+\u03b4\u03bbn, and let M \u2217 b be the version of M \u2217 truncated to its top b singular values. We then have\n|||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 2|||T\u03bbn(Y )\u2212M\u2217b |||2F + 2|||M\u2217b \u2212M\u2217|||2F\n\u2264 2 rank(T\u03bbn(Y )\u2212M\u2217b )|||T\u03bbn(Y )\u2212M\u2217b |||2op + 2 n\u2211\nj=b+1\n\u03c32j (M \u2217).\nWe claim that T\u03bbn(Y ) has rank at most b. Indeed, for any j \u2265 b+ 1, we have\n\u03c3j(Y ) \u2264 \u03c3j(M\u2217) + |||W |||op \u2264 \u03bbn,\nwhere we have used the facts that \u03c3j(M \u2217) \u2264 \u03b41+\u03b4\u03bbn for every j \u2265 b+1 and \u03bbn \u2265 (1+\u03b4)|||W |||op. As a consequence we have \u03c3j(T\u03bbn(Y )) = 0, and hence rank(T\u03bbn(Y ) \u2212M\u2217b ) \u2264 2b. Moreover, we have\n|||T\u03bbn(Y )\u2212M\u2217b |||op \u2264 |||T\u03bbn(Y )\u2212 Y |||op + |||Y \u2212M\u2217|||op + |||M\u2217 \u2212M\u2217b |||op\n\u2264 \u03bbn + |||W |||op + \u03b4\n1 + \u03b4 \u03bbn\n\u2264 2\u03bbn.\nPutting together the pieces, we conclude that\n|||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 16b\u03bb2n + 2 n\u2211\nj=b+1\n\u03c32j (M \u2217)\n(i) \u2264 C n\u2211 j=1 min{\u03c32j (M\u2217), \u03bb2n},\nfor some constant6 C. Here inequality (i) follows since \u03c3j(M \u2217) \u2264 \u03b41+\u03b4\u03bbn whenever j \u2265 b + 1 and \u03c3j(M \u2217) > \u03b41+\u03b4\u03bbn whenever j \u2264 b.\nProof of Lemma 4 In this proof, we make use of a construction due to Chatterjee [Cha14]. For a given matrix M\u2217, we can define the vector t \u2208 Rn of row sums\u2014namely, with entries ti = \u2211n j=1M \u2217 ij for i \u2208 [n]. Using this vector, we can define a rank s approximation M to the\noriginal matrix M\u2217 by grouping the rows according to the vector t according to the following procedure: \u2022 Observing that each ti \u2208 [0, n], let us divide the full interval [0, n] into s groups\u2014say of the\nform [0, n/s), [n/s, 2n/s), . . . [(s \u2212 1)n/s, n]. If ti falls into the interval \u03b1 for some \u03b1 \u2208 [s], we then map row i to the group G\u03b1 of indices. \u2022 For each group G\u03b1, we choose a particular row index k = k(\u03b1) \u2208 G\u03b1 in an arbitrary fashion. For every other row index i \u2208 G\u03b1, we set Mij = Mkj for all j \u2208 [n].\nBy construction, the matrix M has at most s distinct rows, and hence rank at most s. Let us now bound the Frobenius norm error in this rank s approximation. Fixing an arbitrary group index \u03b1 \u2208 [s] and an arbitrary row in i \u2208 G\u03b1, we then have\nn\u2211 j=1 (M\u2217ij \u2212Mij)2 \u2264 n\u2211 j=1 |M\u2217ij \u2212Mij |.\nBy construction, we either have M\u2217ij \u2265 Mij for every j \u2208 [n], or M\u2217ij \u2264 Mij for every j \u2208 [n]. Thus, letting k \u2208 G\u03b1 denote the chosen row, we are guaranteed that\nn\u2211 j=1 |M\u2217ij \u2212Mij | \u2264 |ti \u2212 tk| \u2264 n s ,\nwhere we have used the fact the pair (ti, tk) must lie in an interval of length at most n/s. Putting together the pieces yields the claim."}, {"heading": "5.2.2 Proof of lower bound", "text": "We now turn to the proof of the lower bound in Theorem 2. We split our analysis into two cases, depending on the magnitude of \u03bbn.\nCase 1: First suppose that \u03bbn \u2264 \u221a n 3 . In this case, we consider the matrix M \u2217 : = 1211 T in which all items are equally good, so any comparison is simply a fair coin flip. Let the observation matrix Y \u2208 {0, 1}n\u00d7n be arbitrary. By definition of the singular value thresholding operation, we have |||Y \u2212 T\u03bbn(Y )|||op \u2264 \u03bbn, and hence the SVT estimator M\u0302\u03bbn = T\u03bbn(Y ) has Frobenius norm at most\n|||Y \u2212 M\u0302\u03bbn |||2F \u2264 n\u03bb2n \u2264 n2\n9 .\n6To be clear, the precise value of the constant C is determined by \u03b4, which has been fixed as \u03b4 = 0.01.\nSince M\u2217 \u2208 {12} n\u00d7n and Y \u2208 {0, 1}n\u00d7n, we are guaranteed that |||M\u2217 \u2212 Y |||F = n2 . Applying the triangle inequality yields the lower bound\n|||M\u0302\u03bbn \u2212M\u2217|||F \u2265 |||M\u2217 \u2212 Y |||F \u2212 |||M\u0302\u03bbn \u2212 Y |||F \u2265 n 2 \u2212 n 3 = n 6 .\nCase 2: Otherwise, we may assume that \u03bbn > \u221a n 3 . Consider the matrix M \u2217 \u2208 Rn\u00d7n with entries\n[M\u2217]ij =  1 if i > j 1 2 if i = j\n0 if i < j.\n(28)\nBy construction, the matrix M\u2217 corresponds to the degenerate case of noiseless comparisons. Consider the matrix Y \u2208 Rn\u00d7n generated according to the observation model (1). (To be clear, all of its off-diagonal entries are deterministic, whereas the diagonal is population with i.i.d. Bernoulli variates.) Our proof requires the following auxiliary result regarding the singular values of Y :\nLemma 5. The singular values of the observation matrix Y \u2208 Rn\u00d7n generated by the noiseless comparison matrix M\u2217 satisfy the bounds\nn 4\u03c0(i+ 1) \u2212 1 2 \u2264 \u03c3n\u2212i\u22121(Y ) \u2264\nn\n\u03c0(i\u2212 1) +\n1 2 for all integers i \u2208 [1, n6 \u2212 1].\nWe prove this lemma at the end of this section.\nTaking it as given, we get that \u03c3n\u2212i\u22121(Y ) \u2264 \u221a n 3 for every integer i \u2265 2 \u221a n, and \u03c3n\u2212i(Y ) \u2265\nn 50i for every integer i \u2208 [1, n 25 ]. It follows that\nn\u2211 i=1 (\u03c3i(Y )) 21{\u03c3i(Y ) \u2264 \u221a n 3 } \u2265 n 2 2500 n 25\u2211 i=2 \u221a n 1 i2 \u2265 cn 3 2 ,\nfor some universal constant c > 0. Recalling that \u03bbn \u2265 \u221a n\n3 , we have the lower bound\n|||Y \u2212 M\u0302\u03bbn |||2F \u2265 cn 3 2 . Furthermore, since the observations (apart from the diagonal entries) are noiseless, we have |||Y \u2212M\u2217|||2F \u2264 n4 . Putting the pieces together yields the lower bound\n|||M\u0302\u03bbn \u2212M\u2217|||F \u2265 |||M\u0302\u03bbn \u2212 Y |||F \u2212 |||M\u2217 \u2212 Y |||F \u2265 cn 3 4 \u2212 \u221a n\n2 \u2265 c\u2032n 3 4 ,\nwhere the final step holds when n is large enough (i.e., larger than a universal constant).\nProof of Lemma 5: Instead of working with the original observation matrix Y , it is convenient to work with a transformed version. Define the matrix Y\u0304 = Y \u2212diag(Y ) + In, so that the matrix Y\u0304 is identical to Y except that all its diagonal entries are set to 1. Using this intermediate object, define the (n\u00d7 n) matrix\nY\u0303 : = (Y\u0304 (Y\u0304 )T )\u22121 \u2212 eneTn , (29)\nwhere en denotes the n th standard basis vector. One can verify that this matrix has entries\n[Y\u0303 ]ij =  1 if i = j = 1 or i = j = n 2 if 1 < i = j < n \u22121 if i = j + 1 or i = j \u2212 1 0 otherwise.\nConsequently, it is equal to the graph Laplacian7 of an undirected chain graph on n nodes. Consequently, from standard results in spectral graph theory [BH11], the eigenvalues of Y\u0303 are given by {4 sin2(\u03c0in )} n\u22121 i=0 . Recall the elementary sandwich relationship x 2 \u2264 sinx \u2264 x, valid for every x \u2208 [0, \u03c06 ]. Using this fact, we are guaranteed that\n\u03c02i2\nn2 \u2264 \u03bbi+1(Y\u0303 ) \u2264\n4\u03c02i2\nn2 for all integers i \u2208 [1, n6 ]. (30)\nWe now use this intermediate result to establish the claimed bounds on the singular values of Y . Observe that the matrices Y\u0303 and (Y\u0304 (Y\u0304 )T )\u22121 differ only by the rank one matrix ene T n . Standard results in matrix perturbation theory [Tho76] guarantee that a rank-one perturbation can shift the position (in the large-to-small ordering) of any eigenvalue by at most one. Consequently, the eigenvalues of the matrix (Y\u0304 (Y\u0304 )T )\u22121 must be sandwiched as\n\u03c02(i\u2212 1)2\nn2 \u2264 \u03bbi+1((Y\u0304 (Y\u0304 )T )\u22121) \u2264\n4\u03c02(i+ 1)2\nn2 for all integers i \u2208 [1, n6 \u2212 1].\nIt follows that the singular values of Y\u0304 are sandwiched as\nn\n4\u03c0(i+ 1) \u2264 \u03c3n\u2212i\u22121(Y\u0304 ) \u2264\nn\n\u03c0(i\u2212 1) for all integers i \u2208 [1, n6 \u2212 1].\nObserve that Y\u0304 \u2212 Y is a {0, 12}-valued diagonal matrix, and hence |||Y\u0304 \u2212 Y |||op \u2264 1 2 , which\nimplies that maxi=1,...,n |\u03c3i(Y )\u2212 \u03c3i(Y\u0304 )| \u2264 12 , and hence\nn 4\u03c0(i+ 1) \u2212 1 2 \u2264 \u03c3n\u2212i\u22121(Y ) \u2264\nn\n\u03c0(i\u2212 1) +\n1 2 for all integers i \u2208 [1, n6 \u2212 1],\nas claimed."}, {"heading": "5.3 Proof of Theorem 3", "text": "We now prove our results on the high SNR subclass of CSST, in particular establishing a lower bound and then analyzing the two-stage estimator described in Section 3.3 so as to obtain the upper bound."}, {"heading": "5.3.1 Proof of lower bound", "text": "In order to prove the lower bound, we follow the proof of the lower bound of Theorem 1, with the only difference being that the vector q \u2208 Rn\u22121 is restricted to lie in the interval [12 + \u03b3, 1] n\u22121.\n7In particular, the Laplacian of a graph is given by L = D \u2212 A, where A is the graph adjacency matrix, and D is the diagonal degree matrix."}, {"heading": "5.3.2 Proof of upper bound", "text": "Without loss of generality, assume that the true matrix M\u2217 is associated to the identity permutation. Recall that the second step of our procedure involves performing constrained regression over the set CBISO(\u03c0\u0302FAS). The error in such an estimate is necessarily of two types: the usual estimation error induced by the noise in our samples, and in addition, some form of approximation error that is induced by the difference between \u03c0\u0302FAS and the correct identity permutation.\nIn order to formalize this notion, for any fixed permutation \u03c0, consider the constrained least-squares estimator\nM\u0302\u03c0 \u2208 arg min M\u2208CBISO(\u03c0) |||Y \u2212M |||2F. (31)\nOur first result provides an upper bound on the error matrix \u2206\u0302 = M\u0302\u03c0 \u2212M\u2217 that involves both approximation and estimation error terms.\nLemma 6. There is a universal constant c0 > 0 such that error in the constrained LS estimate (31) satisfies the upper bound\n|||\u2206\u0302|||2F c0 \u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F\ufe38 \ufe37\ufe37 \ufe38\nApprox. error + n log2(n)\ufe38 \ufe37\ufe37 \ufe38 Estimation error\n(32)\nwith probability at least 1\u2212 c1e\u2212c2n.\nThere are two remaining challenges in the proof. Since the second step of our estimator involves the FAS-minimizing permutation \u03c0\u0302FAS, we cannot simply apply Lemma 6 to it directly. (The permutation \u03c0\u0302FAS is random, whereas this lemma applies to any fixed permutation). Consequently, we first need to extend the bound (32) to one that is uniform over a set that includes \u03c0\u0302FAS with high probability. Our second challenge is to upper bound the approximation error term |||M\u2217 \u2212 \u03c0\u0302FAS(M\u2217)|||2F that is induced by using the permutation \u03c0\u0302FAS instead of the correct identity permutation.\nIn order to address these challenges, for any constant c > 0, define the set\n\u03a0\u0302(c) : = {\u03c0 | max i\u2208[n] |i\u2212 \u03c0(i)| \u2264 c log n}.\nThis set corresponds to permutations that are relatively close to the identity permutation in the sup-norm sense. Our second lemma shows that any permutation in \u03a0\u0302(c) is \u201cgood enough\u201d in the sense that the approximation error term in the upper bound (32) is well-controlled:\nLemma 7. For any M\u2217 \u2208 CBISO and any permutation \u03c0 \u2208 \u03a0\u0302(c), we have\n|||M\u2217 \u2212 \u03c0(M\u2217)|||2F \u2264 2c\u2032\u2032n log n, (33)\nwhere c\u2032\u2032 is a positive constant that may depend only on c.\nTaking these two lemmas as given, let us now complete the proof of Theorem 3. (We return to prove these lemmas at the end of this section.) Braverman and Mossel [BM08] showed that for the class CHIGH(\u03b3), there exists a positive constant c\u2014depending on \u03b3 but independent of n\u2014such that\nP [ \u03c0\u0302FAS \u2208 \u03a0\u0302(c) ] \u2265 1\u2212 c3\nn2 . (34)\nFrom the definition of class \u03a0\u0302(c), there is a positive constant c\u2032 (whose value may depend only on c) such that its cardinality is upper bounded as card(\u03a0\u0302(c)) \u2264 n2c\u2032 logn. Consequently, by combining the union bound with Lemma 6 we conclude that, with probability at least 1 \u2212 c\u20321e\u2212c \u2032 2n \u2212 c3 n2 , the error matrix \u2206\u0302FAS : = M\u0302\u03c0\u0302FAS \u2212 M\u2217 satisfies the upper bound (32). Combined with the approximation-theoretic guarantee from Lemma 7, we find that\n|||\u2206\u0302FAS|||2F c0 \u2264 |||M\u2217 \u2212 \u03c0\u0302FAS(M\u2217)|||2F + n log2(n)\n\u2264 c\u2032\u2032n log n+ +n log2(n),\nfrom which the claim follows.\nIt remains to prove the two auxiliary lemmas, and we do so in the following subsections.\nProof of Lemma 6: The proof of this lemma involves a slight generalization of the proof of the upper bound in Theorem 1 (see Section 5.1.1 for this proof). From the optimality of M\u0302\u03c0 and feasibility of \u03c0(M\u2217) for the constrained least-squares program (31), we are guaranteed that |||Y \u2212 M\u0302\u03c0|||2F \u2264 |||Y \u2212 \u03c0(M\u2217)|||2F. Introducing the error matrix \u2206\u0302\u03c0 : = M\u0302\u03c0 \u2212M\u2217, some algebraic manipulations yield the modified basic inequality\n|||\u2206\u0302\u03c0|||2F \u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F + 2\u3008\u3008W, M\u0302\u03c0 \u2212 \u03c0(M\u2217)\u3009\u3009.\nLet us define \u2206\u0302 : = M\u0302\u03c0\u2212\u03c0(M\u2217). Further, for each choice of radius t > 0, recall the definitions of the random variable Z(t) and event At from equations (20) and (23), respectively. With these definitions, we have the upper bound\n|||\u2206\u0302\u03c0|||2F \u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F + 2Z ( |||\u2206\u0302|||F ) . (35)\nLemma 2 proved earlier shows that the inequality E[Z(\u03b4n)] \u2264 \u03b4 2 n 2 is satisfied by \u03b4n = c\n\u221a n log n.\nIn a manner identical to the proof in Section 5.1.1, one can show that P[At] \u2264 P[Z(\u03b4n) \u2265 2\u03b4n \u221a t\u03b4n ] \u2264 2e\u2212c1t\u03b4n for all t \u2265 \u03b4n.\nGiven these results, we break the next step into two cases depending on the magnitude of \u2206\u0302. Case I: Suppose |||\u2206\u0302|||F \u2264 \u221a t\u03b4n. In this case, we have\n|||\u2206\u0302\u03c0|||2F \u2264 2|||M\u2217 \u2212 \u03c0(M\u2217)|||2F + 2|||\u2206\u0302|||2F \u2264 2|||M\u2217 \u2212 \u03c0(M\u2217)|||2F + t\u03b4n.\nCase II: Or otherwise we will have |||\u2206\u0302|||F > \u221a t\u03b4n. Conditioning on the complement Act , our basic inequality (35) implies that\n|||\u2206\u0302\u03c0|||2F \u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F + 4|||\u2206\u0302|||F \u221a t\u03b4n\n\u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F + |||\u2206\u0302|||2F\n8 + 32t\u03b4n,\n\u2264 |||M\u2217 \u2212 \u03c0(M\u2217)|||2F + 2|||\u2206\u0302\u03c0|||2F + 2|||M\u2217 \u2212 \u03c0(M\u2217)|||2F\n8 + 32t\u03b4n,\nwith probability at least 1\u2212 2e\u2212c1t\u03b4n . Finally, setting t = \u03b4n = c \u221a n log(n) in either case and re-arranging yields the bound (32).\nProof of Lemma 7: For any matrix M and any value i, let Mi denote its i th row. Also define the clipping function b : Z \u2192 [n] via b(x) = min{max{1, x}, n}. Using this notation, we have\n|||M\u2217 \u2212 \u03c0(M\u2217)|||2F = n\u2211 i=1 \u2016M\u2217i \u2212M\u2217\u03c0\u22121(i)\u2016 2 2\n\u2264 n\u2211 i=1 max 0\u2264j\u2264c logn {\u2016M\u2217i \u2212M\u2217b(i\u2212j)\u2016 2 2, \u2016M\u2217i \u2212M\u2217b(i+j)\u2016 2 2},\nwhere we have used the definition of the set \u03a0\u0302(c) to obtain the final inequality. Since M\u2217 corresponds to the identity permutation, we have M\u22171 \u2265M\u22172 \u2265 \u00b7 \u00b7 \u00b7 \u2265M\u2217n, where the inequalities are in the pointwise sense. Consequently, we have\n|||M\u2217 \u2212 \u03c0(M\u2217)|||2F \u2264 n\u2211 i=1 max { \u2016M\u2217i \u2212M\u2217b(i\u2212c logn)\u2016 2 2, \u2016M\u2217i \u2212M\u2217b(i+c logn)\u2016 2 2 }\n\u2264 2 n\u2212c logn\u2211 i=1 \u2016M\u2217i \u2212M\u2217i+c logn\u201622.\nOne can verify that the inequality \u2211k\u22121\ni=1 (ai \u2212 ai+1)2 \u2264 (a1 \u2212 ak)2 holds for all ordered sequences of real numbers a1 \u2265 a2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 ak. As stated earlier, the rows of M\u2217 dominate each other pointwise, and hence we conclude that\n|||M\u2217 \u2212 \u03c0(M\u2217)|||2F \u2264 2c log n\u2016M\u22171 \u2212M\u2217n\u201622 \u2264 2cn log n,\nwhich establishes the claim (33)."}, {"heading": "5.4 Proof of Theorem 4", "text": "We now turn to our theorem giving upper and lower bounds on estimating pairwise probability matrices for parametric models. Let us begin with a proof of the claimed lower bound."}, {"heading": "5.4.1 Lower bound", "text": "We prove our lower bound by constructing a set of matrices that are well-separated in Frobenius norm. Using this set, we then use an argument based on Fano\u2019s inequality to lower bound the minimax risk. Underlying our construction of the matrix collection is a collection of Boolean vectors. For any two Boolean vectors b, b\u2032 \u2208 {0, 1}n, let dH(b, b\u2032) = \u2211n j=1 1[bj 6= b\u2032j ] denote their Hamming distance.\nLemma 8. For any fixed \u03b1 \u2208 (0, 1/4), there is a collection of Boolean vectors {b1, . . . , bT } such that\nmin { dH(b j , bk), dH(b j , 0) } \u2265 d\u03b1ne for all distinct j 6= k \u2208 {1, . . . , T}, and (36a)\nT \u2261 T (\u03b1) \u2265 exp ( (n\u2212 1)DKL(2\u03b1\u2016 1 2 ) ) \u2212 1. (36b)\nGiven the collection {bj , j \u2208 [T (\u03b1)]} guaranteed by this lemma, we then define the collection of real vectors {wj , j \u2208 [T (\u03b1)]} via\nwj = \u03b4 ( I \u2212 1\nn 11T\n) bj for each j \u2208 [T (\u03b1)],\nwhere \u03b4 \u2208 (0, 1) is a parameter to be specified later in the proof. By construction, for each index j \u2208 [T (\u03b1)], we have \u30081, wj\u3009 = 0 and \u2016wj\u2016\u221e \u2264 \u03b4. Based on these vectors, we then define the collection of matrices { M j , j \u2208 [T (\u03b1)] } via\n[Mk]ij : = F ([w k]i \u2212 [wk]j).\nBy construction, this collection of matrices is contained within our parametric family. We also claim that they are well-separated in Frobenius norm:\nLemma 9. For any distinct pair j, k \u2208 [T (\u03b1)], we have\n|||M j \u2212Mk|||2F n2 \u2265 \u03b1 2 4 (F (\u03b4)\u2212 F (0))2. (37)\nIn order to apply Fano\u2019s inequality, our second requirement is an upper bound on the mutual information I(Y ; J), where J is a random index uniformly distributed over the set [T ] = {1, . . . , T}. By Jensen\u2019s inequality, we have I(Y ; J) \u2264 1\n(T2)\n\u2211 j 6=kDKL(Pj\u2016Pk), where Pj\ndenotes the distribution of Y when the true underlying matrix is M j . Let us upper bound these KL divergences.\nFor any pair of distinct indices u, v \u2208 [n]2, let xuv be a differencing vector\u2014that is, a vector in components u and v are set equal are 1 and \u22121, respectively, with all remaining components equal to 0. We are then guaranteed that\n\u3008xuv, wj\u3009 = \u03b4\u3008xuv, bj\u3009, and F (\u3008xuv, wj\u3009) \u2208 { F (\u2212\u03b4), F (0), F (\u03b4) } ,\nwhere F (\u03b4) \u2265 F (0) \u2265 F (\u2212\u03b4) by construction. Using these facts, we have\nDKL(Pj\u2016Pk) \u2264 1\n2 \u2211 u,v\u2208[n]\n( F (\u3008xuv, wj\u3009)\u2212 F (\u3008xuv, wk\u3009) )2 min{F (\u3008xuv, wk\u3009), 1\u2212 F (\u3008xuv, wk\u3009)}\n\u2264 1 2 n2\n(F (\u03b4)\u2212 F (\u2212\u03b4))2\nF (\u2212\u03b4)\n\u2264 2n2 (F (\u03b4)\u2212 F (0)) 2\nF (\u2212\u03b4) . (38)\nThis upper bound on the KL divergence (38) and lower bound on the Frobenius norm (37),\nwhen combined with Fano\u2019s inequality, imply that any estimator M\u0302 has its worst-case risk over our family lower bounded as\nsup j\u2208[T (\u03b1)]\n1 n2 E [ |||M\u0302 \u2212M(wj)|||2F ] \u2265 1 8 \u03b12(F (\u03b4)\u2212 F (0))2 ( 1\u2212 2 F (\u2212\u03b4)n 2(F (\u03b4)\u2212 F (0))2 + log 2 n ) .\nChoosing a value of \u03b4 > 0 such that (F (\u03b4)\u2212F (0))2 = F (\u2212\u03b4)20n gives the claimed result. (Such a value of \u03b4 is guaranteed to exist with F (\u2212\u03b4) \u2208 [14 , 1 2 ] given our assumption that F is continuous and strictly increasing.)\nThe only remaining details are to prove Lemmas 8 and 9.\nProof of Lemma 8: The Gilbert-Varshamov bound [Gil52, Var57] guarantees the existence of a collection of vectors {b0, . . . , bT\u0304\u22121} contained with the Boolean hypercube {0, 1}n such that T\u0304 \u2265 2n\u22121 (\u2211d\u03b1ne\u22121 `=0 ( n\u22121 ` ))\u22121 , and\ndH(b j , bk) \u2265 d\u03b1ne for all j 6= k, j, k \u2208 [T\u0304 \u2212 1].\nMoreover, their construction allows loss of generality that the all-zeros vector is a member of the set\u2014say b0 = 0. We are thus guaranteed that dH(b\nj , 0) \u2265 d\u03b1ne for all j \u2208 {1, . . . , T\u0304 \u2212 1}. Since n \u2265 2 and \u03b1 \u2208 (0, 14), we have d\u03b1ne\u22121 n\u22121 \u2264 2\u03b1 \u2264 1 2 . Applying standard bounds on the\ntail of the binomial distribution yields\n1\n2n\u22121 d\u03b1ne\u22121\u2211 `=0 ( n\u2212 1 ` ) \u2264 exp ( \u2212 (n\u2212 1)DKL( d\u03b1ne \u2212 1 n\u2212 1 \u20161 2 ) ) \u2264 exp ( \u2212 (n\u2212 1)DKL(2\u03b1\u2016 1 2 ) ) .\nConsequently, the number of non-zero code words T : = T\u0304 \u2212 1 is at least\nT (\u03b1) : = exp ( (n\u2212 1)DKL(2\u03b1\u2016 1 2 ) ) \u2212 1.\nThus, the collection {b1, . . . , bT } has the desired properties.\nProof of Lemma 9: By definition of the matrix ensemble, we have |||M(wj)\u2212M(wk)|||2F = \u2211\nu,v\u2208[n]\n(F (\u3008xuv, wj\u3009)\u2212 F (\u3008xuv, wk\u3009))2. (39)\nBy construction, the Hamming distances between the triplet of vectors {wj , wk, 0} are lower bounded dH(w\nj , 0) \u2265 \u03b1n, dH(wk, 0) \u2265 \u03b1n and dH(wj , wk) \u2265 \u03b1n. We claim that this implies that\ncard { u 6= v \u2208 [n]2 | \u3008xuv, wj\u3009 6= \u3008xuv, wk\u3009 } \u2265 \u03b1 2\n4 n2. (40)\nTaking this auxiliary claim as given for the moment, applying it to Equation (39) yields the lower bound |||M(w1)\u2212M(w2)|||2F \u2265 14\u03b1 2n2(F (\u03b4)\u2212 F (0))2, as claimed.\nIt remains to prove the auxiliary claim (40). We relabel j = 1 and k = 2 for simplicity in notation. For (y, z) \u2208 {0, 1} \u00d7 {0, 1}, let set Iyz \u2286 [n] denote the set of indices on which w1 takes value y and w2 takes value z. We then split the proof into two cases:\nCase 1: Suppose | I00 \u222a I11 |\u2265 \u03b1n2 . The minimum distance condition dH(w 1, w2) \u2265 \u03b1n implies that | I01 \u222a I10 |\u2265 \u03b1n. For any i \u2208 I00 \u222a I11 and any j \u2208 I01 \u222a I10, it must be that \u3008xuv, w1\u3009 6= \u3008xuv, w2\u3009. Thus there are at least \u03b1 2 2 n 2 such pairs of indices.\nCase 2: Otherwise, we may assume that | I00 \u222a I11 |< \u03b1n2 . This condition, along with the minimum Hamming weight conditions dH(w\n1, 0) \u2265 \u03b1n and dH(w2, 0) \u2265 \u03b1n, gives I10 \u2265 \u03b1n2 and I01 \u2265 \u03b1n2 . For any i \u2208 I01 and any j \u2208 I10, it must be that \u3008xuv, w\n1\u3009 6= \u3008xuv, w2\u3009. Thus there are at least \u03b1 2\n4 n 2 such pairs of indices."}, {"heading": "5.4.2 Upper bound", "text": "In our earlier work [SBB+15, Theorem 2b] we prove that when F is strongly log-concave and twice differentiable, then there is a universal constant Ku such that the maximum likelihood estimator w\u0302ML has mean squared error at most\nsup w\u2217\u2208[\u22121,1]n,\u3008w\u2217, 1\u3009=0\nE[\u2016w\u0302ML \u2212 w\u2217\u201622] \u2264 Ku. (41)\nMoreover, given the log-concavity assumption, the MLE is computable in polynomial-time. Let M(w\u0302ML) and M(w\n\u2217) denote the pairwise comparison matrices induced, via Equation (4), by w\u0302ML and w\n\u2217. It suffices to bound the Frobenius norm |||M(w\u0302ML)\u2212M(w\u2217)|||F. Consider any pair of vectors w1 and w2 that lie in the hypercube [\u22121, 1]n. For any pair\nof indices (i, j) \u2208 [n]2, we have\n((M(w1))ij \u2212 (M(w2))ij)2 = (F (w1i \u2212 w1j )\u2212 F (w2i \u2212 w2j ))2 \u2264 \u03b62((w1i \u2212 w1j )\u2212 (w2i \u2212 w2j ))2,\nwhere we have defined \u03b6 : = max z\u2208[\u22121,1]\nF \u2032(z). Putting together the pieces yields\n|||M(w1)\u2212M(w2)|||2F \u2264 \u03b62(w1 \u2212 w2)T (nI \u2212 11T )(w1 \u2212 w2) = n\u03b62\u2016w1 \u2212 w2\u201622. (42)\nApplying this bound with w1 = w\u0302ML and w 2 = w\u2217 and combining with the bound (41) yields the claim."}, {"heading": "5.5 Proof of Theorem 5", "text": "We now turn to the proof of Theorem 5, which characterizes the behavior of different estimators for the partially observed case."}, {"heading": "5.5.1 Proof of part (a)", "text": "In this section, we prove the lower and upper bounds stated in Equation (17a) in part (a).\nProof of lower bound: We begin by proving the lower bound in Equation (17a). The Gilbert-Varshamov bound [Gil52, Var57] guarantees the existence of a set of vectors {b1, . . . , bT } in the Boolean cube {0, 1} n 2 with cardinality at least T : = 2cn, and such that\ndH(b j , bk) \u2265 d n\n10 e for all distinct pairs j, k \u2208 [T ] : = {1, . . . , T}.\nFixing some \u03b4 \u2208 (0, 14) whose value will be specified later, for each k \u2208 [T ], we define a matrix Mk \u2208 CSST with entries\n[Mk]uv =\n{ 1 2 + \u03b4 if u \u2264 n 2 , [b\nk]u = 1 and v \u2265 n2 1 2 otherwise.\nBy construction, for each distinct pair j, k \u2208 [T ], we have the lower bound\n\u2016M j \u2212Mk\u201622 = 1\n2 n\u03b42\u2016bj \u2212 bk\u201622 \u2265 c0n2\u03b42.\nLet Pj and Pjuv denote (respectively) the distributions of the matrix Y and entry Yuv when the underlying matrix is M j . Since the entries of Y are generated independently, we have DKL(Pj\u2016Pk) =\n\u2211 1\u2264u<v\u2264n DKL(Pjuv\u2016Pkuv). Matrix entry Yuv is generated according to the model\nYuv =  1 w.p. pobsM \u2217 uv\n0 w.p. pobs(1\u2212M\u2217uv) not observed w.p. 1\u2212 pobs,\nso that\nDKL(Pjuv\u2016Pkuv) = pobs ( M juv log\nM juv Mkuv + (1\u2212M juv) log (1\u2212M juv) (1\u2212Mkuv) ) (i) \u2264 pobs { M juv (M juv \u2212Mkuv Mkuv ) + (1\u2212M juv) (Mkuv \u2212M juv 1\u2212Mkuv )}\n= pobs (M juv \u2212Mkuv)2 Mkuv(1\u2212Mkuv) (ii) \u2264 16pobs (M juv \u2212Mkuv)2,\nwhere inequality (i) follows from the fact that log(t) \u2264 t \u2212 1 for all t > 0; and inequality (ii) follows since the numbers {M juv,Mkuv} both lie in the interval [14 , 3 4 ]. Putting together the pieces, we conclude that\nDKL(Pj\u2016Pk) \u2264 c1pobs|||M j \u2212Mk|||2F \u2264 c\u20321 pobsn\u03b42\u2016bj \u2212 bk\u20162 \u2264 c\u20321pobsn2\u03b42.\nThus, applying Fano\u2019s inequality to the packing set {M1, . . . ,MT } yields that any estimator M\u0302 has mean squared error lower bounded by\nsup k\u2208[T ]\n1\nn2 |||M\u0302 \u2212Mk|||2F \u2265 c0\u03b42\n( 1\u2212 c \u2032 1pobsn 2\u03b42 + log 2\ncn\n) .\nFinally, choosing \u03b42 = c22c1pobsn yields the lower bound supk\u2208[T ] 1 n2 |||M\u0302 \u2212Mk|||2F \u2265 c3 1npobs . Note that in order to satisfy the condition \u03b4 \u2264 14 , we must have pobs \u2265 16c2 2c1n .\nOtherwise, if pobs < 16c2 2c1n then simply consider the \u201cbetter\u201d case when pobs = 16c2 2c1n\n; applying the argument above then yields a lower bound of order one, which is the best obtainable in this regime.\nProof of upper bound: In this section, we prove the upper bound of the order O( (logn) 2\np2obsn ).\nThe upper bound of the order O( 1 pobs \u221a n ) follows as a consequence of our proof of part (b).\nGiven our subset of observed matrix entries, suppose that we form a full observation matrix Y \u2208 Rn\u00d7n by assigning any unobserved entry the value 1/2. Define the matrix Y\u0304 : = 1pobsY \u2212 1\u2212pobs 2pobs 11T . One can verify that we can write\nY\u0304 = M\u2217 + 1\npobs W \u2032, (43)\nwhere the noise matrix W \u2032 has entries\nW \u2032uv =  pobs( 1 2 \u2212M \u2217 uv) + 1 2 with probability pobsM \u2217 uv pobs( 1 2 \u2212M \u2217 uv)\u2212 12 with probability pobs(1\u2212M \u2217 uv)\npobs( 1 2 \u2212M \u2217 uv) with probability 1\u2212 pobs.\n(44)\nWe now analyze the error \u2206\u0302 : = M\u0302 \u2212M\u2217 associated with the constrained least-squares estimator applied to the observation matrix Y\u0304 :\nM\u0302 \u2208 arg min M\u2208CSST |||Y\u0304 \u2212M |||2F.\nIn order to do so, we follow the analysis of the fully observed model, given in Section 5.1.1. In particular, the same argument yields the modified basic inequality 12 |||\u2206\u0302||| 2 F \u2264 1pobs \u3008\u3008W\n\u2032, \u2206\u0302\u3009\u3009. Note that all entries of the matrix W \u2032 are zero-mean and lie in the interval [\u22121, 1] and hence are 1-sub-Gaussian; in addition, they are i.i.d. above the diagonal, and satisfy skew-symmetry. Thus, the argument from Section 5.1.1 yields that the upper bounds\n|||\u2206\u0302|||F \u2264 1\npobs\n\u221a n log n, and hence\n1\nn2 |||\u2206\u0302|||2F \u2264\n(log n)2\np2obsn ,\nhold with the stated probability, which completes the proof."}, {"heading": "5.5.2 Proof of part (b)", "text": "In order to prove the bound (17b), we analyze the SVT estimator T\u03bbn,pobs (Y ) with the threshold \u03bbn,pobs = 2.1 \u221a n\npobs . Naturally then, our analysis is similar to that of complete observations\ncase from Section 5.2. Recall our formulation of the problem in terms of the observation matrix Y\u0304 along with the noise matrix W \u2032 from equations (43) and (44). Observe that Lemma 3 and Lemma 4 continue to hold in the partial observations case, but with W replaced by\n1 pobs W \u2032 in the case of Lemma 3. The entries of W \u2032 lie in [\u22121, 1] and hence are 1-sub-Gaussian, are i.i.d. on and above the diagonal, are zero-mean, and satisfy skew-symmetry. As before, Theorem 3.4 of Chatterjee [Cha14] then implies that\nP [ |||W \u2032|||op > (2 + t) \u221a n ] \u2264 c1e\u2212f(t)n\nwhere c1 is a universal constant and f(t) is strictly positive for each t > 0. With our choice \u03bbn,pobs = 2.1 \u221a n pobs , the event {\u03bbn,pobs \u2265 1.01|||W \u2032|||op} holds with probability at least 1\u2212 c1e\u2212c2n. Conditioned on this event, we may apply Lemma 4. Combined with the approximationtheoretic result from Lemma 4, we find that\n1\nn2 |||T\u03bbn(Y )\u2212M\u2217|||2F \u2264 c (s\u03bb2n n2 + 1 s ) with probability at least 1 \u2212 c1e\u2212c2n. Substituting \u03bbn,pobs = 2.1 \u221a n\npobs in this bound and setting\ns = pobs \u221a n gives the claimed result."}, {"heading": "5.5.3 Proof of part (c)", "text": "As in our of proof of the fully observed case from Section 5.4.2, we consider the two-stage estimator based on first computing the MLE w\u0302ML of w\n\u2217, and then constructing the matrix estimate M(w\u0302ML) via Equation (4). Let us now upper bound the mean-squared error associated with this estimator.\nOur observation model can be (re)described in the following way. Consider an Erdo\u030bs-Re\u0301nyi graph on n vertices with each edge drawn independently with a probability pobs. For each edge in this graph, we obtain one observation of the pair of vertices at the end-points of that edge. Let L be the (random) Laplacian matrix of this graph. From Theorem 2(b) from our paper [SBB+15] on estimating parametric models, for this graph, there is a universal constant c1 such that the maximum likelihood estimator w\u0302ML has mean squared error upper bounded as\nE[\u2016w\u0302ML \u2212 w\u2217\u201622 | L] \u2264 c1 n\n\u03bb2(L) ,\nThe estimator is computable in a time polynomial in n.\nLet \u03bb2(L) denote the second largest eigenvalue. Since pobs \u2265 c0 (logn) 2\nn , known results on the eigenvalues of random graphs [Oli09, CR11, KOVB14] imply that\nP [ \u03bb2(L) \u2265 c1npobs ] \u2265 1\u2212 1\nn4 (45)\nfor a universal constant c1 (that may depend on c0). As shown earlier in Equation (42), for any valid score vectors w1, w2, we have |||M(w1) \u2212 M(w2)|||2F \u2264 n\u03b62\u2016w1 \u2212 w2\u201622 where \u03b6 : = maxz\u2208[\u22121,1] F\n\u2032(z). Putting these results together and performing some simple algebraic manipulations leads to the upper bound\n1 n2 E [ |||M(w\u0302ML)\u2212M\u2217|||2F ] \u2264 c3\u03b6 2 npobs ,\nthus proving our claim."}, {"heading": "6 Discussion", "text": "In this paper, we analyzed a flexible model for pairwise comparison data that includes various parametric models, including the BTL and Thurstone models, as special cases. We analyzed various estimators for this broader matrix family, ranging from optimal estimators through to various polynomial-time estimators, including forms of singular value thresholding, as well as multi-stage method based on a noisy sorting routine. All of the results in this paper focused on estimation of the matrix of pairwise comparison probabilities in the Frobenius norm. Estimation of probabilities in other metrics, such as the KL divergence or `1-norm, can be desirable. We suspect that the results given here will be useful in establishing results in these metrics. Establishing the best possible rates for polynomial-time algorithms over the full class CSST is a challenging open problem.\nWe evaluated a computationally efficient estimator based on thresholding the singular values of the observation matrix that is consistent, but achieves a suboptimal rate. In our analysis of this estimator, we have so far been conservative in our choice of the regularization parameter, in that it is a fixed choice. Such a fixed choice has been prescribed in various\ntheoretical works on the soft or hard-thresholded singular values (see, for instance, the papers [Cha14, GD13]). In practice, the entries of the effective noise matrix W have variances that depend on the unknown matrix, and the regularization parameter may be obtained via cross-validation. The effect of allowing a data-dependent choice of the regularization parameter remains to be studied, although we suspect it may improve the minimax risk by a constant factor at best.\nFinally, in some applications, choices can be systematically intransitive, for instance when objects have multiple features and different features dominate different pairwise comparisons. In these situations, the SST assumption may be weakened to one where the underlying pairwise comparison matrix is a mixture of a small number of SST matrices. The results of this work may form building blocks to address this general setting; we defer a detailed analysis to future work.\nAcknowledgments: This work was partially supported by ONR-MURI grant DOD-002888, AFOSR grant FA9550-14-1-0016, NSF grant CIF-31712-23800, and ONR MURI grant N0001411-1-0688. The work of NBS was also supported in part by a Microsoft Research PhD fellowship."}, {"heading": "A Proof of Proposition 1", "text": "We will show that the matrix M\u2217 specified in Figure 1a satisfies the conditions required by the proposition. It is easy to verify that M\u2217 \u2208 CSST, so that it remains to prove the approximation-theoretic lower bound (6). In order to do so, we require the following auxiliary result:\nLemma 10. Consider any matrix M that belongs to CPAR(F ) for a valid function F . Suppose for some collection of four distinct items {i1, . . . , i4}, the matrix M satisfies the inequality Mi1i2 > Mi3i4. Then it must also satisfy the inequality Mi1i3 \u2265Mi2i4.\nWe return to prove this lemma at the end of this section. Taking it as given, let us now proceed to prove the lower bound (6). For any valid F , fix an arbitrary member M of a class CPAR(F ), and let w \u2208 Rn be the underlying weight vector (see the definition (4)).\nPick any item in the set of first n4 items (corresponding to the first n 4 rows of M \u2217) and call this item as \u201c1\u201d; pick an item from the next set of n4 items (rows) and call it item \u201c2\u201d; item \u201c3\u201d from the next set and item \u201c4\u201d from the final set. Our analysis proceeds by developing some relations between the pairwise comparison probabilities for these four items that must hold for every parametric model, that are strongly violated by M\u2217. We divide our analysis into two possible relations between the entries of M . Case I: First suppose that M12 \u2264M34. Since M\u221712 = 6/8 and M\u221734 = 5/8 in our construction, it follows that\n(M12 \u2212M\u221712)2 + (M34 \u2212M\u221734)2 \u2265 1\n256 .\nCase II: Otherwise, we may assume thatM12 < M34. Then Lemma 10 implies thatM13 \u2265M24. Moreover, since M\u221713 = 7/8 and M \u2217 24 = 1 in our construction, it follows that\n(M13 \u2212M\u221713)2 + (M24 \u2212M\u221724)2 \u2265 1\n256 .\nAggregating across these two exhaustive cases, we find that\u2211 (u,v)\u2208{1,2,3,4} (Muv \u2212M\u2217uv)2 \u2265 1 256 .\nSince this bound holds for any arbitrary selection of items from the four sets, we conclude that 1\nn2 |||M \u2212M\u2217|||2F is lower bounded by a universal constant c > 0 as claimed.\nFinally, it is easy to see that upon perturbation of any of the entries of M\u2217 by at most 132\u2014 while still ensuring that the resulting matrix lies in CSST\u2014the aforementioned results continue to hold, albeit with a worse constant. Every element in this class satisfies the claim of this proposition.\nProof of Lemma 10: It remains to prove Lemma 10. Since M belongs to the parametric family, there must exist some valid function F and some vector w that induce M (see Equation (4)). Since F is non-decreasing, the condition Mi1i2 > Mi3i4 implies that\nwi1 \u2212 wi2 > wi3 \u2212 wi4 .\nAdding wi2\u2212wi3 to both sides of this inequality yields wi1\u2212wi3 > wi2\u2212wi4 . Finally, applying the non-decreasing function F to both sides of this inequality gives yields Mi1i3 \u2265 Mi2i4 as claimed, thereby completing the proof."}, {"heading": "B Minimizing feedback arc set over entire SST class", "text": "Our analysis in Theorem 3 shows that the two-step estimator proposed in Section 3.3 works well under the stated bounds on the entries of M\u2217, i.e., for M\u2217 \u2208 CHIGH(\u03b3) for a fixed \u03b3. This two-step estimator is based on finding a minimum feedback arc set (FAS) in the first step. In this section, we investigate the efficacy of estimators based on minimum FAS over the full class CSST. We show that minimizing the FAS does not work well over CSST.\nThe intuition is that although minimizing the feedback arc set appears to minimize disagreements at a global scale, it makes only local decisions: if it is known that items i and j are in adjacent positions, the order among these two items is decided based solely on the outcome of the comparison between items i and j, and is independent of the outcome of the comparisons of i and j with all other items.\nHere is a concrete example to illustrate this property. Suppose n is divisible by 3, and consider the following (n\u00d7 n) block matrix M \u2208 CSST:\nM =  1 2\n1 2 1\n1 2\n1 2\n3 4\n0 14 1 2  , where each block is of size (n3 \u00d7 n 3 ). Let \u03c0\n1 be the identity permutation, and let \u03c02 be the permutation [n3 + 1, . . . , 2n 3 , 1, . . . , n 3 , 2n 3 + 1, . . . , n], that is, \u03c0\n2 swaps the second block of n3 items with the first block. For any permutation \u03c0 of the n items and any M \u2208 CSST, let \u03c0(M) denote the (n\u00d7 n) matrix resulting from permuting both the rows and the columns by \u03c0.\nOne can verify that |||\u03c01(M) \u2212 \u03c02(M)|||2F \u2265 cn2, for some universal constant c > 0. Now suppose an observation Y is generated from \u03c01(M) as per the model (1). Then the distribution\nof the size of the feedback arc set of \u03c01 is identical to the distribution of the size of the feedback arc set of \u03c02. Minimizing FAS will not be able to distinguish between \u03c01(M) and \u03c02(M) at least 50% of the time, and due to this problem an estimator that is based on the minimum FAS result will not perform well over the SST class."}, {"heading": "C Relation to other models", "text": "We put things in perspective to the other models considered in the literature. We begin with two weaker versions of stochastic transitivity that are also investigated in the literature on psychology and social science.\nC.1 Moderate and weak stochastic transitivity\nThe model CSST that we consider is called strong stochastic transitivity in the literature on psychology and social science [Fis73, DM59]. The two other popular (and weaker) models are those of moderate stochastic transitivity CMST defined as\nCMST : = { M \u2208 [0, 1]n\u00d7n | Mik \u2265 min{Mij ,Mjk} for every (i, j, k) satisfying Mij \u2265 12 and Mjk \u2265 1 2 } ,\nand weak stochastic transitivity CWST defined as\nCWST : = { M \u2208 [0, 1]n\u00d7n | Mik \u2265 12 for every (i, j, k) satisfying Mij \u2265 1 2 and Mjk \u2265 1 2 } .\nClearly, we have the inclusions CSST \u2286 CMST \u2286 CWST. In Theorem 1, we prove that the minimax rates of estimation under the strong stochastic transitivity assumption are \u0398\u0303(n\u22121). It turns out, however, that the two weaker transitivity conditions do not permit meaningful estimation.\nProposition 2. There exists a universal constant c > 0 such that under the moderate CMST stochastic transitivity model,\ninf M\u0303 sup M\u2217\u2208CMST\n1\nn2 E[|||M\u0303 \u2212M\u2217|||2F] > c.\nwhere the infimum is taken over all measurable mappings from the observations Y to [0, 1]n\u00d7n. Consequently, for the weak stochastic transitivity model CWST, we also have\ninf M\u0303 sup M\u2217\u2208CWST\n1\nn2 E[|||M\u0303 \u2212M\u2217|||2F] > c,\nThe minimax risk incurred under these two classes is clearly the worst possible (up to a universal constant) since for any two arbitrary matrices M and M \u2032 in [0, 1]n\u00d7n, we have 1 n2 |||M \u2212 M \u2032|||2F \u2264 1. For this reason, in the paper we restrict our analysis to the strong stochastic transitivity condition.\nC.2 Comparison with statistical models\nLet us now investigate relationship of the strong stochastic transitivity model considered in this paper with two other popular models in the literature on statistical learning from\ncomparative data. Perhaps the most popular model in this regard is the class of parametric models CPAR: recall that this class is defined as\nCPAR : = {M |Mij = F (w\u2217i \u2212 w\u2217j ) for some non-decreasing function F : R\u2192 [0, 1], and vector w\u2217 \u2208 Rn}.\nThe parametric class of models assumes that the function F is fixed and known. Statistical estimation under the parametric class is studied in several recent papers [NOS12, HOX14, SBB+15]. The setting where the function F is fixed, but unknown leads to a semi-parametric variant. The results presented in this section also readily apply to the semi-parametric class.\nThe second class is that generated from distributions over complete rankings [Dia89, FJS13, DIS15]. Specifically, every element in this class is generated as the pairwise marginal of an arbitrary probability distribution over all possible permutations of the n items. We denote this class as CFULL.\nThe following result characterizes the relation between the classes.\nProposition 3. Consider any value of n > 10. The parametric class CPAR is a strict subset of the strong stochastic transitivity class CSST. The class CFULL of marginals of a distribution on total rankings is neither a subset nor a superset of either of the classes CSST, CPAR, and CSST\\CPAR.\nThe various relationships in Proposition 3 are depicted pictorially in Figure 3. These relations are derived by first establishing certain conditions that matrices in the classes considered must satisfy, and then constructing matrices that satisfy or violate one or more of these conditions. The conditions on CFULL arise from the observation that the class is the convex hull of all SST matrices that have their non-diagonal elements in {0, 1}; we derive conditions on this convex hull that leads to properties of the CFULL class. To handle the parametric class CPAR, we employ a necessary condition discussed earlier in Section 2.4 and defined formally in Lemma 10. The SST class CSST is characterized using the insights derived throughout the paper.\nC.3 Proof of Proposition 2\nWe will derive an order one lower bound under the moderate stochastic transitivity condition. This result automatically implies the order one lower bound for weak stochastic transitivity.\nThe proof imposes a certain structure on a subset of the entries of M\u2217 in a manner that \u0398(n2) remaining entries are free to take arbitrary values within the interval [12 , 1]. This flexibility then establishes a minimax error of \u0398(1) as claimed.\nLet us suppose M\u2217 corresponds to the identity permutation of the n items, and that this information is public knowledge. Set the entries of M\u2217 above the diagonal in the following manner. For every i \u2208 [n] and every odd j \u2208 [n], set M\u2217ij = 12 . For every i \u2208 [n] and every even j \u2208 [n], set M\u2217ji = 12 . This information is also assumed to be public knowledge. Let S \u2282 [n] 2 denote the set of all entries of M\u2217 above the diagonal whose values were not assigned in the previous step. Let |S| denote the size of set S. The entries below the diagonal are governed by the skew-symmetry constraints.\nWe first argue that every entry in S can take arbitrary values in the interval [12 , 1], and are not constrained by each other under the moderate stochastic transitivity condition. To this end, consider any entry (i, k) \u2208 S. Recall that the moderate stochastic transitivity condition imposes the following set of restrictions in M\u2217ik: for every j, M \u2217 ik \u2265 min{M\u2217ij ,M\u2217jk}. From our earlier construction we have that for every odd value of j, M\u2217ij = 1 2 and hence the restriction simply reduces to M\u2217ik \u2265 1 2 . On the other hand, for every even value of j, our construction gives M\u2217jk = 1 2 , and hence the restriction again reduces to M \u2217 ik \u2265 1 2 . Given the absence of any additional restrictions, the error E[|||M\u0302 \u2212M\u2217|||2F] \u2265 c|S|. Finally, observe that every entry (i, k) where i < k, i is odd and k is even belongs to the set S. It follows that |S| \u2265 n28 , thus proving our claim.\nC.4 Proof of Proposition 3\nThe constructions governing the claimed relations are enumerated in Figure 3 and the details are provided below.\nIt is easy to see that since F is non-decreasing, the parametric class CPAR is contained in the strong stochastic transitivity class CSST. We provide a formal proof of this statement for the sake of completeness. Suppose without loss of generality that w1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 wn. Then we claim that the distribution of pairwise comparisons generated through this model result in a matrix, say M , that lies in the SST model with the ordering 1 2 \u00b7 \u00b7 \u00b7 n. This is because for any i > j > k,\nwi \u2212 wk \u2265 wi \u2212 wj F (wi \u2212 wk) \u2265 F (wi \u2212 wj)\nMik \u2265Mij .\nWe now show the remaining relations with the four constructions indicated in Figure 3. While these constructions target some specific value of n, the results hold for any value n greater than that specific value. To see this, suppose we construct a matrix M for some n = n0, and show that it lies inside (or outside) one of these classes. Consider any n > n0, and define a (n\u00d7n) matrix M \u2032 as having M as the top-left (n0\u00d7n0) block, 12 on the remaining diagonal entries, 1 on the remaining entries above the diagonal and 0 on the remaining entries below the diagonal. This matrix M \u2032 will retain the properties of M in terms of lying inside (or outside, respectively) the claimed class.\nC.4.1 Construction 1\nWe construct a matrix M such that M \u2208 CFULL but M /\u2208 CSST. Let n = 3. Consider the following distribution over permutations of 3 items (1, 2, 3):\nP(1 2 3) = 25 , P(3 1 2) = 15 , P(2 3 1) = 25 .\nThis distribution induces the pairwise marginals\nP(1 2) = 35 , P(2 3) = 45 , P(3 1) = 35 .\nSet Mij = P(i j) for every pair. By definition of the class CFULL, we have M \u2208 CFULL. A necessary condition for a matrix M to belong to the class CSST is that there must exist at least one item, say item i, such that Mij \u2265 12 for every item j. One can verify that the pairwise marginals enumerated above do not satisfy this condition, and hence M /\u2208 CSST.\nC.4.2 Construction 2\nWe construct a matrix M such that M \u2208 CSST \u2229 CFULL but M /\u2208 CPAR. Let n = 4 and consider the following distribution over permutations of 4 items (1, 2, 3, 4):\nP(3 1 2 4) = 1 8 , P(1 2 4 3) = 1 8\nP(2 1 4 3) = 2 8 and P(1 2 3 4) = 4 8 .\nOne can verify that this distribution leads to the following pairwise comparison matrix M (with the ordering of the rows and columns respecting the permutation 1 2 3 4):\nM : = 1\n8  4 6 7 8 2 4 7 8 1 1 4 5 0 0 3 4  . It is easy to see that this matrix M \u2208 CSST, and by construction M \u2208 CFULL. Finally, the proof of Proposition 2 shows that M /\u2208 CPAR, thereby completing the proof.\nC.4.3 Construction 3\nWe construct a matrix M such that M \u2208 CPAR (and hence M \u2208 CSST) but M /\u2208 CFULL. First observe that any total ordering on n items can be represented as an (n\u00d7n) matrix in the SST class such that all its off-diagonal entries take values in {0, 1}. The class CFULL is precisely the convex hull of all such binary SST matrices.\nLetB1, . . . , Bn! denote all (n\u00d7n) matrices in CSST whose off-diagonal elements are restricted to take values in the set {0, 1}. The following lemma derives a property that any matrix in the convex hull of B1, . . . , Bn! must satisfy.\nLemma 11. Consider any M \u2208 CSST, and consider three items i, j, k \u2208 [n] such that M respects the ordering i j k. Suppose Mij = Mjk = 12 and Mik = 1. Further suppose that M can be written as\nM = \u2211 `\u2208[n!] \u03b1`B`, (46)\nwhere \u03b1` \u2265 0 \u2200 ` and \u2211n!\n`=1 \u03b1 ` = 1. Then for any ` \u2208 [n!] such that \u03b1` > 0, it must be that\nB`ij 6= B`jk.\nThe proof of the lemma is provided at the end of this section. Now consider the following (7\u00d7 7) matrix M \u2208 CSST:\nM : =  1 2 1 2 1 1 1 1 1 1 2 1 2 1 2 1 2 1 1 1 0 12 1 2 1 2 1 2 1 1 0 12 1 2 1 2 1 2 1 2 1 0 0 12 1 2 1 2 1 2 1\n0 0 0 12 1 2 1 2 1 2 0 0 0 0 0 12 1 2\n . (47)\nWe will now show via proof by contradiction that M cannot be represented as a convex combination of the matrices B1, . . . , Bn!. We will then show that M \u2208 CPAR. Suppose one can representM as a convex combinationM = \u2211\n`\u2208[n!] \u03b1 `B`, where \u03b11, . . . , \u03b1n!\nare non-negative scalars that sum to one. Consider any ` such that \u03b1` 6= 0. Let B`12 = b \u2208 {0, 1}. Let us derive some more constraints on B`. Successively applying Lemma 11 for the following values of i, j, k implies that B` must necessarily have the form (48) shown below. Here b\u0304 : = 1 \u2212 b and \u2018\u2217\u2019 denotes some arbitrary value that is irrelevant to the discussion at hand.\n\u2022 i = 1, j = 2, k = 3 gives B`23 = b\u0304\n\u2022 i = 1, j = 2, k = 4 gives B`24 = b\u0304\n\u2022 i = 2, j = 3, k = 5 gives B`35 = b\n\u2022 i = 2, j = 4, k = 6 gives B`46 = b\n\u2022 i = 3, j = 5, k = 6 gives B`56 = b\u0304\n\u2022 i = 4, j = 6, k = 7 gives B`67 = b\u0304.\nThus B` must be of the form\nM : =  1 2 b 1 1 1 1 1 1 2 1 2 b\u0304 b\u0304 1 1 1 0 b 12 \u2217 b 1 1 0 b \u2217 12 \u2217 b 1 0 0 b\u0304 \u2217 12 b\u0304 1 0 0 0 b\u0304 b 12 b\u0304\n0 0 0 0 0 b 12\n . (48)\nFinally, applying Lemma 11 with i = 5, j = 6, k = 7 necessitates B`67 = b, which contradicts the necessary condition of (48). We have thus shown that M /\u2208 CFULL.\nWe will now show that the matrix M constructed in (47) is contained in the class CPAR. Consider the following function F : [\u22121, 1]\u2192 [0, 1] in the definition of a parametric class:\nF (x) =  0 if x < \u22120.25 1 2 if \u22120.25 \u2264 x \u2264 0.25 1 if x > 0.25.\nLet n = 7 with w1 = .9, w2 = .7, w3 = .6, w4 = .5, w5 = .4, w6 = .3 and w7 = .1. One can verify that under this construction, the matrix of pairwise comparisons is identical to that in Equation (47). Proof of Lemma 11 In what follows, we will show that \u2211\n`:B`ij=1,B ` jk=1\n\u03b1` = \u2211\n`:B`ij=1,B ` jk=1\n\u03b1` =\n0. The result then follows immediately. Consider some `\u2032 \u2208 [n!] such that \u03b1`\u2032 > 0 and B`\u2032ij = 0. Since Mik = 1, we must have B` \u2032 ik = 1. Given that B `\u2032 represents a total ordering of the n items, that is, B` \u2032 is an SST matrix with boolean-valued its off-diagonal elements, B` \u2032 ij = 0 and B `\u2032 ik = 1 imply that B `\u2032 jk = 1. We have thus shown that B` \u2032 jk = 1 whenever B `\u2032 ij = 0. This result has two consequences. The first\nconsequence is that \u2211\n`:B`ij=0,B ` jk=0\n\u03b1` = 0. The second consequence employs the additional\nfact that Mij = 1 2 and hence \u2211 `:B`ij=0 \u03b1` = 12 , and then gives \u2211 `:B`ij=0,B ` jk=1 \u03b1` = 12 . Building on, we have\n1 2 = Mjk = \u2211 `:B`ij=0,B ` jk=1 \u03b1` + \u2211 `:B`ij=1,B ` jk=1 \u03b1`,\nand hence we have \u2211\n`:B`ij=1,B ` jk=1\n\u03b1` = 0, thus completing the proof.\nC.4.4 Construction 4\nWe construct a matrix M such that M \u2208 CSST but M /\u2208 CFULL and M /\u2208 CPAR. Consider n = 11. Let M2 denote the (4\u00d7 4) matrix of Construction 2 and let M3 denote the (7\u00d7 7) matrix of construction 3. Consider the (11\u00d7 11) matrix M of the form\nM : = [ M2 1 0 M3 ] .\nSince M2 \u2208 CSST and M3 \u2208 CSST, it is easy to see that M \u2208 CSST. Since M2 /\u2208 CPAR and M /\u2208 CFULL, it follows that M /\u2208 CPAR and M /\u2208 CFULL. This construction completes the proof of Proposition 3."}], "references": [{"title": "Aggregating inconsistent information: ranking and clustering", "author": ["N. Ailon", "M. Charikar", "A. Newman"], "venue": "Journal of the ACM (JACM), 55(5):23", "citeRegEx": "ACN08", "shortCiteRegEx": null, "year": 2008}, {"title": "SIAM Journal on Discrete Mathematics", "author": ["N. Alon. Ranking tournaments"], "venue": "20(1):137\u2013142,", "citeRegEx": "Alo06", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectra of graphs", "author": ["A.E. Brouwer", "W.H. Haemers"], "venue": "Springer", "citeRegEx": "BH11", "shortCiteRegEx": null, "year": 2011}, {"title": "Noisy sorting without resampling", "author": ["M. Braverman", "E. Mossel"], "venue": "Proc. ACMSIAM symposium on Discrete algorithms, pages 268\u2013276", "citeRegEx": "BM08", "shortCiteRegEx": null, "year": 2008}, {"title": "Rank analysis of incomplete block designs: I", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "The method of paired comparisons. Biometrika, pages 324\u2013345", "citeRegEx": "BT52", "shortCiteRegEx": null, "year": 1952}, {"title": "Decisions", "author": ["T.P. Ballinger", "N.T. Wilcox"], "venue": "error and heterogeneity. The Economic Journal, 107(443):1090\u20131105", "citeRegEx": "BW97", "shortCiteRegEx": null, "year": 1997}, {"title": "Models for paired comparison data: A review with emphasis on dependent data", "author": ["M. Cattelan"], "venue": "Statistical Science, 27(3):412\u2013433", "citeRegEx": "Cat12", "shortCiteRegEx": null, "year": 2012}, {"title": "On matrix estimation under monotonicity constraints", "author": ["S. Chatterjee", "A. Guntuboyina", "B. Sen"], "venue": "arXiv:1506.03430", "citeRegEx": "CGS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix estimation by universal singular value thresholding", "author": ["S. Chatterjee"], "venue": "The Annals of Statistics, 43(1):177\u2013214", "citeRegEx": "Cha14", "shortCiteRegEx": null, "year": 2014}, {"title": "On the spectra of general random graphs", "author": ["F. Chung", "M. Radcliffe"], "venue": "The electronic journal of combinatorics, 18(1):P215", "citeRegEx": "CR11", "shortCiteRegEx": null, "year": 2011}, {"title": "A generalization of spectral analysis with application to ranked data", "author": ["P. Diaconis"], "venue": "The Annals of Statistics, 17(3):949\u2013979", "citeRegEx": "Dia89", "shortCiteRegEx": null, "year": 1989}, {"title": "A topic modeling approach to ranking", "author": ["W. Ding", "P. Ishwar", "V. Saligrama"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "DIS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Experimental tests of a stochastic decision theory", "author": ["D. Davidson", "J. Marschak"], "venue": "Measurement: Definitions and theories, pages 233\u201369", "citeRegEx": "DM59", "shortCiteRegEx": null, "year": 1959}, {"title": "Binary choice probabilities: on the varieties of stochastic transitivity", "author": ["P.C. Fishburn"], "venue": "Journal of Mathematical psychology, 10(4):327\u2013352", "citeRegEx": "Fis73", "shortCiteRegEx": null, "year": 1973}, {"title": "A nonparametric approach to modeling choice with limited data", "author": ["V.F. Farias", "S. Jagabathula", "D. Shah"], "venue": "Management Science, 59(2):305\u2013322", "citeRegEx": "FJS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability models and statistical analyses for ranking data", "author": ["M.A. Fligner", "J.S. Verducci"], "venue": "volume 80. Springer", "citeRegEx": "FV93", "shortCiteRegEx": null, "year": 1993}, {"title": "Donoho", "author": ["M. Gavish", "D. L"], "venue": "The optimal hard threshold for singular values is 4/sqrt(3),", "citeRegEx": "GD13", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of signalling alphabets", "author": ["E.N. Gilbert"], "venue": "Bell System Technical Journal, 31(3):504\u2013522", "citeRegEx": "Gil52", "shortCiteRegEx": null, "year": 1952}, {"title": "Entropy estimate for high-dimensional monotonic functions", "author": ["F. Gao", "J.A. Wellner"], "venue": "Journal of Multivariate Analysis, 98(9):1751\u20131764", "citeRegEx": "GW07", "shortCiteRegEx": null, "year": 2007}, {"title": "Minimax-optimal inference from partial rankings", "author": ["B. Hajek", "S. Oh", "J. Xu"], "venue": "Advances in Neural Information Processing Systems, pages 1475\u20131483", "citeRegEx": "HOX14", "shortCiteRegEx": null, "year": 2014}, {"title": "How to rank with few errors", "author": ["C. Kenyon-Mathieu", "W. Schudy"], "venue": "Symposium on Theory of computing (STOC), pages 95\u2013103. ACM", "citeRegEx": "KMS07", "shortCiteRegEx": null, "year": 2007}, {"title": "Algebraic connectivity of Erd\u00f6s-R\u00e9nyi graphs near the connectivity threshold", "author": ["T. Kolokolnikov", "B. Osting", "J. Von Brecht"], "venue": "Available online http://www.mathstat.dal.ca/ tkolokol/papers/braxton-james.pdf", "citeRegEx": "KOVB14", "shortCiteRegEx": null, "year": 2014}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI", "citeRegEx": "Led01", "shortCiteRegEx": null, "year": 2001}, {"title": "Individual choice behavior: A theoretical analysis", "author": ["R.D. Luce"], "venue": "New York: Wiley", "citeRegEx": "Luc59", "shortCiteRegEx": null, "year": 1959}, {"title": "Analyzing and modeling rank data", "author": ["J.I. Marden"], "venue": "CRC Press", "citeRegEx": "Mar96", "shortCiteRegEx": null, "year": 1996}, {"title": "Stochastic transitivity and cancellation of preferences between bitter-sweet solutions", "author": ["D.H. McLaughlin", "R.D. Luce"], "venue": "Psychonomic Science, 2(1-12):89\u201390", "citeRegEx": "ML65", "shortCiteRegEx": null, "year": 1965}, {"title": "Iterative ranking from pair-wise comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": "Advances in Neural Information Processing Systems, pages 2474\u20132482", "citeRegEx": "NOS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Concentration of the adjacency matrix and of the Laplacian in random graphs with independent edges", "author": ["R.I. Oliveira"], "venue": "arXiv preprint:0911.0600", "citeRegEx": "Oli09", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence", "author": ["N.B. Shah", "S. Balakrishnan", "J. Bradley", "A. Parekh", "K. Ramchandran", "M. Wainwright"], "venue": "Conference on Artificial Intelligence and Statistics, pages 856\u2013865", "citeRegEx": "SBB+15", "shortCiteRegEx": null, "year": 2015}, {"title": "Constrained statistical inference: Order", "author": ["M.J. Silvapulle", "P.K. Sen"], "venue": "inequality, and shape constraints, volume 912. John Wiley & Sons", "citeRegEx": "SS11", "shortCiteRegEx": null, "year": 2011}, {"title": "The behavior of eigenvalues and singular values under perturbations of restricted rank", "author": ["R. Thompson"], "venue": "Linear Algebra and its Applications, 13(1):69\u201378", "citeRegEx": "Tho76", "shortCiteRegEx": null, "year": 1976}, {"title": "A law of comparative judgment", "author": ["L.L. Thurstone"], "venue": "Psychological Review, 34(4):273", "citeRegEx": "Thu27", "shortCiteRegEx": null, "year": 1927}, {"title": "Elimination by aspects: A theory of choice", "author": ["A. Tversky"], "venue": "Psychological review, 79(4):281", "citeRegEx": "Tve72", "shortCiteRegEx": null, "year": 1972}, {"title": "Estimate of the number of signals in error correcting codes", "author": ["R. Varshamov"], "venue": "Dokl. Akad. Nauk SSSR", "citeRegEx": "Var57", "shortCiteRegEx": null, "year": 1957}], "referenceMentions": [{"referenceID": 31, "context": "The Bradley-Terry-Luce [BT52, Luc59] and Thurstone [Thu27] models are mainstays in analyzing this type of pairwise comparison data.", "startOffset": 51, "endOffset": 58}, {"referenceID": 13, "context": ", see Fishburn [Fis73] for an overview), where the only coherence assumption made on the pairwise comparison probabilities is that of strong stochastic transitivity, or SST for short.", "startOffset": 15, "endOffset": 22}, {"referenceID": 5, "context": "[BW97] is especially strongly worded:", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "An estimator based on hard-thresholding was studied previously in this context by Chatterjee [Cha14].", "startOffset": 93, "endOffset": 100}, {"referenceID": 3, "context": "Our third contribution, formalized in Theorems 3 and 4, is to show that for certain interesting subsets of the full SST class, a combination of parametric maximum likelihood [SBB+15] and noisy sorting algorithms [BM08] leads to a tractable two-stage method that achieves the minimax rate.", "startOffset": 212, "endOffset": 218}, {"referenceID": 8, "context": "Our fourth contribution is to supplement our minimax lower bound with lower bounds for various known estimators, including those based on thresholding singular values [Cha14], noisy sorting [BM08], and parametric methods [NOS12, HOX14, SBB+15]) based on parametric models.", "startOffset": 167, "endOffset": 174}, {"referenceID": 3, "context": "Our fourth contribution is to supplement our minimax lower bound with lower bounds for various known estimators, including those based on thresholding singular values [Cha14], noisy sorting [BM08], and parametric methods [NOS12, HOX14, SBB+15]) based on parametric models.", "startOffset": 190, "endOffset": 196}, {"referenceID": 8, "context": "Chatterjee [Cha14] formally introduced the estimation problem considered in this paper, and analyzed an estimator based on singular value thresholding.", "startOffset": 11, "endOffset": 18}, {"referenceID": 3, "context": "Part of our analysis leverages an algorithm from the paper [BM08]; in particular, we extend their analysis in order to provide guarantees for estimating pairwise comparison probabilities as opposed to estimating the underlying order.", "startOffset": 59, "endOffset": 65}, {"referenceID": 29, "context": ", [SS11]), particularly to the problem of bi-variate isotonic regression.", "startOffset": 2, "endOffset": 8}, {"referenceID": 18, "context": "We use Dudley\u2019s entropy integral in order to derive an upper bound that is sharp up to a logarithmic factor; doing so in turn requires deriving upper bounds on the metric entropy of the class CSST for which we leverage the prior work of Gao and Wellner [GW07].", "startOffset": 253, "endOffset": 259}, {"referenceID": 8, "context": "For the full class CSST, Chatterjee [Cha14] analyzed the performance of such an estimator and proved that the squared Frobenius error decays as O(n\u2212 1 4 ) uniformly over CSST.", "startOffset": 36, "endOffset": 43}, {"referenceID": 8, "context": "To be clear, Chatterjee [Cha14] actually analyzed the hard-SVT estimator, which is based on the hard-thresholding operator", "startOffset": 24, "endOffset": 31}, {"referenceID": 3, "context": "In particular, we call upon a polynomial-time algorithm due to Braverman and Mossel [BM08] that, under the model (12), is guaranteed to find the exact solution to the FAS problem with high probability.", "startOffset": 84, "endOffset": 90}, {"referenceID": 3, "context": "We note that we do not have an analogue of the high-SNR result in the partial observations case since having partial observations reduces the SNR; the noisy-sorting algorithm of Braverman and Mossel [BM08] for the high-SNR case requires a computational complexity of e\u03b3 \u22124 and hence is not computable in time polynomial in n when \u03b3 < (log n)\u2212 1 4 , and this disallows most interesting scalings of pobs with n.", "startOffset": 199, "endOffset": 205}, {"referenceID": 3, "context": "In particular, it relies on the algorithm due to Braverman and Mossel [BM08] to compute the feedback arc set minimizer.", "startOffset": 70, "endOffset": 76}, {"referenceID": 3, "context": "\u2022 High SNR: A setting studied previously by Braverman and Mossel [BM08], in which the noise is independent of the items being compared.", "startOffset": 65, "endOffset": 71}, {"referenceID": 18, "context": "of Gao and Wellner [GW07].", "startOffset": 19, "endOffset": 25}, {"referenceID": 8, "context": "4 of Chatterjee [Cha14], which guarantees that", "startOffset": 16, "endOffset": 23}, {"referenceID": 8, "context": "Proof of Lemma 4 In this proof, we make use of a construction due to Chatterjee [Cha14].", "startOffset": 80, "endOffset": 87}, {"referenceID": 2, "context": "Consequently, from standard results in spectral graph theory [BH11], the eigenvalues of \u1ef8 are given by {4 sin2( n )} n\u22121 i=0 .", "startOffset": 61, "endOffset": 67}, {"referenceID": 30, "context": "Standard results in matrix perturbation theory [Tho76] guarantee that a rank-one perturbation can shift the position (in the large-to-small ordering) of any eigenvalue by at most one.", "startOffset": 47, "endOffset": 54}, {"referenceID": 3, "context": ") Braverman and Mossel [BM08] showed that for the class CHIGH(\u03b3), there exists a positive constant c\u2014depending on \u03b3 but independent of n\u2014such that", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "4 of Chatterjee [Cha14] then implies that", "startOffset": 16, "endOffset": 23}], "year": 2017, "abstractText": "There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes several parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the least-squares estimate in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations.", "creator": "LaTeX with hyperref package"}}}