{"id": "1510.05956", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Optimal Cluster Recovery in the Labeled Stochastic Block Model", "abstract": "we consider the problem of community detection in the labeled independent stochastic block model ( individually labeled sbm ) with a conditional finite number $ k $ distribution of communities of sizes linearly growing graph with the network edge size $ n $. every pair of nodes is typically labeled independently at random, and operation label $ \\ ell $ appears with probability $ p ( i, parameter j, \\ ell ) $ data between two nodes in community $ i $ and $ j $, } respectively. one further observes a realization of these random sequence labels, separately and the objective operation is to reconstruct the communities from this observation. under mild assumptions on the optimal parameters $ r p $, we show that under spectral algorithms, the number of misclassified nodes does not exceed $ s $ with high probability as $ n $ grows large, whenever $ \\ bar { > p } n = \\ omega ( \u2265 1 ) $ ( where $ \\ bar { p } = \\ max _ { i, j, \\ ell \\ ge 1 } p ( i, l j, \\ ell ) $ ), $ s = o ( & n ) $ 10 and $ \\ frac { n : d ( p ) } { \\ log ( n / s ) } & \u30fb gt ; exactly 1 $, where $ d ( p ) $, referred to as the { \\ x it divergence }, is an appropriately defined function of the parameters $ or p = ( log p ( i, j, \\ d ell ), i, j, \\ ell ) $. we further show that $ \\ frac { l n d ( p ) } { \\ log ( n / s ) } & gt ; 1 $ is actually necessary to obtain less variables than $ \u2208 s $ misclassified upstream nodes asymptotically. this establishes the optimality strategy of spectral algorithms, i. ^ e., when $ \\ bar { p } if n = \\ negative omega ( 1 ) $ and $ nd ( p ) = \\ omega ( 1 ) $, furthermore no visibility algorithm currently can perform better in terms of expected misclassified nodes than spectral algorithms.", "histories": [["v1", "Tue, 20 Oct 2015 16:47:27 GMT  (210kb)", "https://arxiv.org/abs/1510.05956v1", "16 pages"], ["v2", "Mon, 26 Oct 2015 01:18:59 GMT  (247kb)", "http://arxiv.org/abs/1510.05956v2", "20 pages"], ["v3", "Mon, 2 Nov 2015 23:50:11 GMT  (392kb)", "http://arxiv.org/abs/1510.05956v3", "32 pages"], ["v4", "Wed, 4 Nov 2015 14:03:41 GMT  (392kb)", "http://arxiv.org/abs/1510.05956v4", "32 pages. arXiv admin note: text overlap witharXiv:1412.7335"], ["v5", "Mon, 21 Dec 2015 01:23:31 GMT  (399kb)", "http://arxiv.org/abs/1510.05956v5", "33 pages. arXiv admin note: text overlap witharXiv:1412.7335"], ["v6", "Sat, 21 May 2016 19:41:08 GMT  (399kb)", "http://arxiv.org/abs/1510.05956v6", "arXiv admin note: text overlap witharXiv:1412.7335"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "math.PR cs.LG cs.SI stat.ML", "authors": ["se-young yun", "alexandre prouti\u00e8re"], "accepted": true, "id": "1510.05956"}, "pdf": {"name": "1510.05956.pdf", "metadata": {"source": "CRF", "title": "Optimal Cluster Recovery in the Labeled Stochastic Block Model", "authors": ["Se-Young Yun", "Alexandre Proutiere"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n05 95\n6v 6\n[ m\nat h.\nPR ]\nClustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters."}, {"heading": "1 Introduction", "text": "Community detection consists in extracting (a few) groups of similar items from a large global population, and has applications in a wide spectrum of disciplines including social sciences, biology, computer science, and statistical physics. The communities or clusters of items are inferred from the observed pair-wise similarities between items, which, most often, are represented by a graph whose vertices are items and edges are pairs of items known to share similar features.\nThe stochastic block model (SBM), introduced three decades ago in [13], constitutes a natural performance benchmark for community detection, and has been, since then, widely studied. In the SBM, the set of items V = {1, . . . , n} are partitioned into K non-overlapping clusters V1, . . . ,VK , that have to be recovered from an observed realization of a random graph. In the latter, an edge between two items belonging to clusters Vi and Vj , respectively, is present with probability p(i, j), independently of other edges. The analyses presented in this paper apply to the SBM, but also to the labeled stochastic block model (LSBM) [12], a more general model to describe the similarities of items. There, the observation of the similarity between two items comes in the form of a label taken from a finite set L = {0, 1, . . . , L}, and label \u2113 is observed between two items in clusters Vi and Vj , respectively, with probability p(i, j, \u2113), independently of other labels. The standard SBM can be seen as a particular instance of its labeled\ncounterpart with two possible labels 0 and 1, and where the edges present (resp. absent) in the SBM correspond to item pairs with label 1 (resp. 0). The problem of cluster recovery under the LSBM consists in inferring the hidden partition V1, . . . ,VK from the observation of the random labels on each pair of items.\nOver the last few years, we have seen remarkable progresses for the problem of cluster recovery under the SBM (see [8] for an exhaustive literature review), highlighting its scientific relevance and richness. Most recent work on the SBM aimed at characterizing the set of parameters (i.e., the probabilities p(i, j) that there exists an edge between nodes in clusters i and j for 1 \u2264 i, j \u2264 K) such that some qualitative recovery objectives can or cannot be met. For sparse scenarios where the average degree of items in the graph is O(1), parameters under which it is possible to extract clusters positively correlated with the true clusters have been identified [5, 20, 18]. When the average degree of the graph is \u03c9(1), one may predict the set of parameters allowing a cluster recovery with a vanishing (as n grows large) proportion of misclassified items [25, 19], but one may also characterize parameters for which an asymptotically exact cluster reconstruction can be achieved [1, 24, 9, 19, 2, 3, 14].\nIn this paper, we address the finer and more challenging question of determining, under the general LSBM, the minimal number of misclassified items given the parameters of the model. Specifically, for any given s = o(n), our goal is to identify the set of parameters such that it is possible to devise a clustering algorithm with at most s misclassified items. Of course, if we achieve this goal, we shall recover all the aforementioned results on the SBM.\nMain results. We focus on the labeled SBM as described above, and where each item is assigned to cluster Vk with probability \u03b1k > 0, independently of other items. We assume w.l.o.g. that \u03b11 \u2264 \u03b12 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b1K . We further assume that \u03b1 = (\u03b11, . . . , \u03b1K) does not depend on the total population of items n. Conditionally on the assignment of items to clusters, the pair or edge (v,w) \u2208 V2 has label \u2113 \u2208 L = {0, 1, . . . , L} with probability p(i, j, \u2113), when v \u2208 Vi and w \u2208 Vj . W.l.o.g., 0 is the most frequent label, i.e., 0 = argmax\u2113 \u2211K i=1 \u2211K j=1 \u03b1i\u03b1jp(i, j, \u2113). Throughout the paper, we typically assume that p\u0304 = o(1) and p\u0304n = \u03c9(1) where p\u0304 = maxi,j,\u2113\u22651 p(i, j, \u2113) denotes the maximum probability of observing a label different than 0. We shall explicitly state whether these assumption are made when deriving our results. In the standard SBM, the second assumption means that the average degree of the corresponding random graph is \u03c9(1). This also means that we can hope to recover clusters with a vanishing proportion of misclassified items. We finally make the following assumption: there exist positive constants \u03b7 and \u03b5 such that for every i, j, k \u2208 [K] = {1, . . . ,K},\n(A1) \u2200\u2113 \u2208 L, p(i, j, \u2113) p(i, k, \u2113)\n\u2264 \u03b7 and (A2) \u2211K k=1 \u2211L \u2113=1(p(i, k, \u2113) \u2212 p(j, k, \u2113))2\np\u03042 \u2265 \u03b5.\n(A2) imposes a certain separation between the clusters. For example, in the standard SBM with two communities, p(1, 1, 1) = p(2, 2, 1) = \u03be, and p(1, 2, 1) = \u03b6 , (A2) is equivalent to 2(\u03be \u2212 \u03b6)2/\u03be2 \u2265 \u01eb. In summary, the LSBM is parametrized by \u03b1 and p = (p(i, j, \u2113))1\u2264i,j\u2264K,0\u2264\u2113\u2264L, and recall that \u03b1 does not depend on n, whereas p does.\nFor the above LSBM, we derive, for any arbitrary s = o(n), a necessary condition under which there exists an algorithm inferring clusters with s misclassified items. We further establish that under this condition, a simple extension of spectral algorithms extract communities with less than s misclassified items. To formalize these results, we introduce the divergence of (\u03b1, p). We denote by p(i) the K \u00d7 (L + 1) matrix whose element on the j-th row and the (\u2113 + 1)-th column is p(i, j, \u2113) and denote by p(i, j) \u2208 [0, 1]L+1 the vector describing the probability distribution of the label of a pair of items in Vi and Vj , respectively. Let PK\u00d7(L+1) denote the set of K \u00d7 (L + 1) matrices such that each row represents a probability distribution. The divergence D(\u03b1, p) of (\u03b1, p) is defined as follows: D(\u03b1, p) =\nmini,j:i 6=j DL+(\u03b1, p(i), p(j)) with\nDL+(\u03b1, p(i), p(j)) = min y\u2208PK\u00d7(L+1) max\n{\nK \u2211\nk=1\n\u03b1kKL(y(k), p(i, k)),\nK \u2211\nk=1\n\u03b1kKL(y(k), p(j, k))\n}\nwhere KL denotes the Kullback-Leibler divergence between two label distributions, i.e., KL(y(k), p(i, k)) =\n\u2211L \u2113=0 y(k, \u2113) log y(k,\u2113) p(i,k,\u2113) . Finally, we denote by \u03b5 \u03c0(n) the number of misclassified items under the clustering algorithm \u03c0, and by E[\u03b5\u03c0(n)] its expectation (with respect to the randomness in the LSBM and in the algorithm).\nWe first derive a tight lower bound on the average number of misclassified items when the latter is o(n). Note that such a bound was unknown even for the SBM [2].\nTheorem 1 Assume that (A1) and (A2) hold, and that p\u0304n = \u03c9(1). Let s = o(n). If there exists a clustering algorithm \u03c0 misclassifying in average less than s items asymptotically, i.e., lim supn\u2192\u221e E[\u03b5\u03c0(n)] s \u2264 1, then the parameters (\u03b1, p) of the LSBM satisfy:\nlim inf n\u2192\u221e\nnD(\u03b1, p) log(n/s) \u2265 1. (1)\nTo state the corresponding positive result (i.e., the existence of an algorithm misclassifying only s items), we make an additional assumption to avoid extremely sparse labels: (A3) there exists a constant \u03ba > 0 such that np(j, i, \u2113) \u2265 (np\u0304)\u03ba for all i, j and \u2113 \u2265 1.\nTheorem 2 Assume that (A1), (A2), and (A3) hold, and that p\u0304 = o(1), p\u0304n = \u03c9(1). Let s = o(n). If the parameters (\u03b1, p) of the LSBM satisfy (1), then the Spectral Partition (SP ) algorithm presented in Section 4 misclassifies at most s items with high probability, i.e., limn\u2192\u221e P[\u03b5SP (n) \u2264 s] = 1.\nThese theorems indicate that under the LSBM with parameters satisfying (A1) and (A2), the number of misclassified items scales at least as n exp(\u2212nD(\u03b1, p)(1 + o(1)) under any clustering algorithm, irrespective of its complexity. They further establish that the Spectral Partition algorithm reaches this fundamental performance limit under the additional condition (A3). We note that the SP algorithm runs in polynomial time, i.e., it requires O(n2p\u0304 log(n)) floating-point operations.\nWe further establish a necessary and sufficient condition on the parameters of the LSBM for the existence of a clustering algorithm recovering the clusters exactly with high probability. Deriving such a condition was also open [2].\nTheorem 3 Assume that (A1) and (A2) hold. If there exists a clustering algorithm that does not misclassify any item with high probability, then the parameters (\u03b1, p) of the LSBM satisfy: lim infn\u2192\u221e nD(\u03b1,p) log(n) \u2265 1. If this condition holds, then under (A3), the SP algorithm recovers the clusters exactly with high probability.\nThe paper is organized as follows. Section 2 presents the related work and example of application of our results. In Section 3, we sketch the proof of Theorem 1, which leverages change-of-measure and coupling arguments. We present in Section 4 the Spectral Partition algorithm, and analyze its performance (we outline the proof of Theorem 2). All results are proved in details in the supplementary material."}, {"heading": "2 Related Work and Applications", "text": ""}, {"heading": "2.1 Related work", "text": "Cluster recovery in the SBM has attracted a lot of attention recently. We summarize below existing results, and compare them to ours. Results are categorized depending on the targeted level of performance. First, we consider the notion of detectability, the lowest level of performance requiring that the extracted clusters are just positively correlated with the true clusters. Second, we look at asymptotically accurate recovery, stating that the proportion of misclassified items vanishes as n grows large. Third, we present existing results regarding exact cluster recovery, which means that no item is misclassified. Finally, we report recent work whose objective, like ours, is to characterize the optimal cluster recovery rate.\nDetectability. Necessary and sufficient conditions for detectability have been studied for the binary symmetric SBM (i.e., L = 1, K = 2, \u03b11 = \u03b12, p(1, 1, 1) = p(2, 2, 1) = \u03be, and p(1, 2, 1) = p(2, 1, 1) = \u03b6). In the sparse regime where \u03be, \u03b6 = o(1), and for the binary symmetric SBM, the main focus has been on identifying the phase transition threshold (a condition on \u03be and \u03b6) for detectability: It was conjectured in [5] that if n(\u03be \u2212 \u03b6) < \u221a\n2n(\u03be + \u03b6) (i.e., under the threshold), no algorithm can perform better than a simple random assignment of items to clusters, and above the threshold, clusters can partially be recovered. The conjecture was recently proved in [20] (necessary condition), and [18] (sufficient condition). The problem of detectability has been also recently studied in [27] for the asymmetric SBM with more than two clusters of possibly different sizes. Interestingly, it is shown that in most cases, the phase transition for detectability disappears.\nThe present paper is not concerned with conditions for detectability. Indeed detectability means that only a strictly positive proportion of items can be correctly classified, whereas here, we impose that the proportion of misclassified items vanishes as n grows large.\nAsymptotically accurate recovery. A necessary and sufficient condition for asymptotically accurate recovery in the SBM (with any number of clusters of different but linearly increasing sizes) has been derived in [25] and [19]. Using our notion of divergence specialized to the SBM, this condition is nD(\u03b1, p) = \u03c9(1). Our results are more precise since the minimal achievable number of misclassified items is characterized, and apply to a broader setting since they are valid for the generic LSBM.\nAsymptotically exact recovery. Conditions for exact cluster recovery in the SBM have been also recently studied. [1, 19, 9] provide a necessary and sufficient condition for asymptotically exact recovery in the binary symmetric SBM. For example, it is shown that when \u03be = a log(n)n and \u03b6 = b log(n) n for a > b,\nclusters can be recovered exactly if and only if a+b2 \u2212 \u221a ab \u2265 1. In [2, 3], the authors consider a more general SBM corresponding to our LSBM with L = 1. They define CH-divergence as:\nD+(\u03b1, p(i), p(j)) = n\nlog(n) max \u03bb\u2208[0,1]\nK \u2211\nk=1\n\u03b1k\n( (1\u2212 \u03bb)p(i, k, 1) + \u03bbp(j, k, 1) \u2212 p(i, k, 1)1\u2212\u03bbp(j, k, 1)\u03bb ) ,\nand show that mini 6=j D+(\u03b1, p(i), p(j)) > 1 is a necessary and sufficient condition for asymptotically exact reconstruction. The following claim, proven in the supplementary material, relates D+ to DL+.\nClaim 4 When p\u0304 = o(1), we have for all i, j:\nDL+(\u03b1, p(i), p(j)) n\u2192\u221e\u223c max\n\u03bb\u2208[0,1]\nL \u2211\n\u2113=1\nK \u2211\nk=1\n\u03b1k\n( (1\u2212 \u03bb)p(i, k, \u2113) + \u03bbp(j, k, \u2113) \u2212 p(i, k, \u2113)1\u2212\u03bbp(j, k, \u2113)\u03bb ) .\nThus, the results in [2, 3] are obtained by applying Theorem 3 and Claim 4. In [14], the authors consider a symmetric labeled SBM where communities are balanced (i.e., \u03b1k =\n1 K for all k) and where label probabilities are simply defined as p(i, i, \u2113) = p(\u2113) for all i and p(i, j, \u2113) = q(\u2113) for all i 6= j. It is shown that nIlog(n) > 1 is necessary and sufficient for asymptotically exact recovery, where I = \u2212 2K log ( \u2211L \u2113=0 \u221a p(\u2113)q(\u2113) ) . We can relate I to D(\u03b1, p):\nClaim 5 In the LSBM with K clusters, if p\u0304 = o(1), and for all i, j, \u2113 such that i 6= j, \u03b1i = 1K , p(i, i, \u2113) = p(\u2113), and p(j, k, \u2113) = q(\u2113), we have: D(\u03b1, p)\nn\u2192\u221e\u223c \u2212 2K log ( \u2211L \u2113=0 \u221a p(\u2113)q(\u2113) ) .\nAgain from this claim, the results derived in [14] are obtained by applying Theorem 3 and Claim 5.\nOptimal recovery rate. In [6, 21], the authors consider the binary SBM in the sparse regime where the average degree of items in the graph is O(1), and identify the minimal number of misclassified items for very specific intra- and inter-cluster edge probabilities \u03be and \u03b6 . Again the sparse regime is out of the scope of the present paper. [26, 8] are concerned with the general SBM corresponding to our LSBM with L = 1, and with regimes where asympotically accurate recovery is possible. The authors first characterize the optimal recovery rate in a minimax framework. More precisely, they consider a (potentially large) set of possible parameters (\u03b1, p), and provide a lower bound on the expected number of misclassified items for the worst parameters in this set. Our lower bound (Theorem 1) is more precise as it is model-specific, i.e., we provide the minimal expected number of misclassified items for a given parameter (\u03b1, p) (and for a more general class of models). Then the authors propose a clustering algorithm, with time complexity O(n3 log(n)), and achieving their minimax recovery rate. In comparison, our algorithm yields an optimal recovery rate O(n2p\u0304 log(n)) for any given parameter (\u03b1, p), exhibits a lower running time, and applies to the generic LSBM."}, {"heading": "2.2 Applications", "text": "We provide here a few examples of application of our results, illustrating their versatility. In all examples, f(n) is a function such that f(n) = \u03c9(1), and a, b are fixed real numbers such that a > b.\nThe binary SBM. Consider the binary SBM where the average item degree is \u0398(f(n)), and represented by a LSBM with parameters L = 1, K = 2, \u03b1 = (\u03b11, 1 \u2212 \u03b11), p(1, 1, 1) = p(2, 2, 1) = af(n)n , and p(1, 2, 1) = p(2, 1, 1) = bf(n)n . From Theorems 1 and 2, the optimal number of misclassified vertices scales as n exp(\u2212g(\u03b11, a, b)f(n)(1 + o(1))) when \u03b11 \u2264 1/2 (w.l.o.g.) and where g(\u03b11, a, b) := max\n\u03bb\u2208[0,1] (1\u2212 \u03b11 \u2212 \u03bb+ 2\u03b11\u03bb)a+ (\u03b11 + \u03bb\u2212 2\u03b1\u03bb)b\u2212 \u03b11a\u03bbb(1\u2212\u03bb) \u2212 (1\u2212 \u03b11)a(1\u2212\u03bb)b\u03bb.\nIt can be easily checked that g(\u03b11, a, b) \u2265 g(1/2, a, b) = 12( \u221a a\u2212 \u221a b)2 (letting \u03bb = 12 ). The worst case is hence obtained when the two clusters are of equal sizes. When f(n) = log(n), we also note that the condition for asymptotically exact recovery is g(\u03b11, a, b) \u2265 1. Recovering a single hidden community. As in [10], consider a random graph model with a hidden community consisting of \u03b1n vertices, edges between vertices belonging the hidden community are present with probability af(n)n , and edges between other pairs are present with probability bf(n) n . This is modeled by a LSBM with parameters K = 2, L = 1, \u03b11 = \u03b1, p(1, 1, 1) = af(n) n , and p(1, 2, 1) = p(2, 1, 1) = p(2, 2, 1) = bf(n)n . The minimal number of misclassified items when searching for the hidden community scales as n exp(\u2212h(\u03b1, a, b)f(n)(1 + o(1))) where\nh(\u03b1, a, b) := \u03b1\n(\na\u2212 (a\u2212 b)1 + log(a\u2212 b)\u2212 log(a log(a/b)) log(a/b)\n)\n.\nWhen f(n) = log(n), the condition for asymptotically exact recovery of the hidden community is h(\u03b1, a, b) \u2265 1. Optimal sampling for community detection under the SBM. Consider a dense binary symmetric SBM with intra- and inter-cluster edge probabilities a and b. In practice, to recover the clusters, one might not be able to observe the entire random graph, but sample its vertex (here item) pairs as considered in [25]. Assume for instance that any pair of vertices is sampled with probability \u03b4f(n)n for some fixed \u03b4 > 0, independently of other pairs. We can model such scenario using a LSBM with three labels, namely \u00d7, 0 and 1, corresponding to the absence of observation (the vertex pair is not sampled), the observation of the absence of an edge and of the presence of an edge, respectively, and with parameters for all i, j \u2208 {1, 2}, p(i, j,\u00d7) = 1 \u2212 \u03b4f(n)n , p(1, 1, 1) = p(2, 2, 1) = a \u03b4f(n) n , and p(1, 2, 1) = p(2, 1, 1) = b \u03b4f(n)n . The minimal number of misclassified vertices scales as n exp(\u2212l(\u03b4, a, b)f(n)(1 + o(1))) where l := \u03b4(1 \u2212 \u221a ab \u2212 \u221a\n(1\u2212 a)(1 \u2212 b)). When f(n) = log(n), the condition for asymptotically exact recovery is l(\u03b1, a+, a\u2212, b+, b\u2212) \u2265 1. Signed networks. Signed networks [16, 23] are used in social sciences to model positive and negative interactions between individuals. These networks can be represented by a LSBM with three possible labels, namely 0, + and -, corresponding to the absence of interaction, positive and negative interaction, respectively. Consider such LSBM with parameters: K = 2, \u03b11 = \u03b12, p(1, 1,+) = p(2, 2,+) = a+f(n) n , p(1, 1,\u2212) = p(2, 2,\u2212) = a\u2212f(n)n , p(1, 2,+) = p(2, 1,+) = b+f(n)\nn , and p(1, 2,\u2212) = p(2, 1,\u2212) = b\u2212f(n)\nn , for some fixed a+, a\u2212, b+, b\u2212 such that a+ > b+ and a\u2212 < b\u2212. The minimal number of misclassified individuals here scales as n exp(\u2212m(\u03b1, a+, a\u2212, b+, b\u2212)f(n)(1 + o(1))) where\nm(\u03b1, a+, a\u2212, b+, b\u2212) := 1\n2\n( ( \u221a a+ \u2212 \u221a b+) 2 + ( \u221a a\u2212 \u2212 \u221a b\u2212) 2 ) .\nWhen f(n) = log(n), the condition for asymptotically exact recovery is l(\u03b1, a+, a\u2212, b+, b\u2212) \u2265 1."}, {"heading": "3 Fundamental Limits: Change of Measures through Coupling", "text": "In this section, we explain the construction of the proof of Theorem 1. The latter relies on an appropriate change-of-measure argument, frequently used to identify upper performance bounds in online stochastic optimization problems [15]. In the following, we refer to \u03a6, defined by parameters (\u03b1, p), as the true stochastic model under which all the observed random labels are generated, and denote by P\u03a6 = P (resp. E\u03a6[\u00b7] = E[\u00b7]) the corresponding probability measure (resp. expectation). In our change-of-measure argument, we construct a second stochastic model \u03a8 (whose corresponding probability measure and expectation are P\u03a8 and E\u03a8[\u00b7], respectively). Using a change of measures from P\u03a6 to P\u03a8, we relate the expected number of misclassified items E\u03a6[\u03b5\u03c0(n)] under any clustering algorithm \u03c0 to the expected (w.r.t. P\u03a8) log-likelihood ratio Q of the observed labels under P\u03a6 and P\u03a8. Specifically, we show that, roughly, log(n/E\u03a6[\u03b5\u03c0(n)]) must be smaller than E\u03a8[Q] for n large enough. Construction of \u03c8. Let (i\u22c6, j\u22c6) = argmini,j:i<j DL+(\u03b1, p(i), p(j)), and let v\u22c6 denote the smallest item index that belongs to cluster i\u22c6 or j\u22c6. If both Vi\u22c6 and Vj\u22c6 are empty, we define v\u22c6 = n. Let q \u2208 PK\u00d7(L+1) such that: D(\u03b1, p) = \u2211Kk=1 \u03b1kKL(q(k), p(i\u22c6, k)) = \u2211K k=1 \u03b1kKL(q(k), p(j\n\u22c6, k)). The existence of such q is proved in Lemma 7 in the supplementary material. Now to define the stochastic model \u03a8, we couple the generation of labels under \u03a6 and \u03a8 as follows.\n1. We first generate the random clusters V1, . . . ,VK under \u03a6, and extract i\u22c6, j\u22c6, and v\u22c6. The clusters generated under \u03a8 are the same as those generated under \u03a6. For any v \u2208 V , we denote by \u03c3(v) the cluster of item v.\n2. For all pairs (v,w) such that v 6= v\u22c6 and w 6= v\u22c6, the labels generated under \u03a8 are the same as those generated under \u03a6, i.e., the label \u2113 is observed on the edge (v,w) with probability p(\u03c3(v), \u03c3(w), \u2113).\n3. Under \u03a8, for any v 6= v\u22c6, the observed label on the edge (v, v\u22c6) under \u03a8 is \u2113 with probability q(\u03c3(v), \u2113).\nLet xv,w denote the label observed for the pair (v,w). We introduce Q, the log-likelihood ratio of the observed labels under P\u03a6 and P\u03a8 as:\nQ = v\u22c6\u22121 \u2211\nv=1\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) +\nn \u2211\nv=v\u22c6+1\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) . (2)\nLet \u03c0 be a clustering algorithm with output (V\u0302k)1\u2264k\u2264K , and let E = \u22c3 1\u2264k\u2264K V\u0302k \\ Vk be the set of misclassified items under \u03c0. Note that in general in our analysis, we always assume without loss of generality that |\u22c31\u2264k\u2264K V\u0302k \\ Vk| \u2264 | \u22c3\n1\u2264k\u2264K V\u0302\u03b3(k) \\ Vk| for any permutation \u03b3, so that the set of misclassified items is indeed E . By definition, \u03b5\u03c0(n) = |E|. Since under \u03a6, items are interchangeable (remember that items are assigned to the various clusters in an i.i.d. manner), we have: nP\u03a6{v \u2208 E} = E\u03a6[\u03b5\n\u03c0(n)] = E[\u03b5\u03c0(n)]. Next, we establish a relationship between E[\u03b5\u03c0(n)] and the distribution of Q under P\u03a8. For any\nfunction f(n), we can prove that: P\u03a8{Q \u2264 f(n)} \u2264 exp(f(n)) E\u03a6[\u03b5 \u03c0(n)]\n(\u03b1i\u22c6+\u03b1j\u22c6)n + \u03b1j\u22c6 \u03b1i\u22c6+\u03b1j\u22c6 . Using this\nresult with f(n) = log (n/E\u03a6[\u03b5\u03c0(n)]) \u2212 log(2/\u03b1i\u22c6), and Chebyshev\u2019s inequality, we deduce that: log (n/E\u03a6[\u03b5 \u03c0(n)])\u2212 log(2/\u03b1i\u22c6) \u2264 E\u03a8[Q] + \u221a\n4 \u03b1i\u22c6 E\u03a8[(Q\u2212 E\u03a8[Q])2], and thus, a necessary condition for E[\u03b5\u03c0(n)] \u2264 s is:\nlog (n/s)\u2212 log(2/\u03b1i\u22c6) \u2264 E\u03a8[Q] + \u221a 4\n\u03b1i\u22c6 E\u03a8[(Q\u2212 E\u03a8[Q])2]. (3)\nAnalysis of Q. In view of (3), we can obtain a necessary condition for E[\u03b5\u03c0(n)] \u2264 s if we evaluate E\u03a8[Q] and E\u03a8[(Q \u2212 E\u03a8[Q])2]. To evaluate E\u03a8[Q], we can first prove that v\u22c6 \u2264 log(n)2 with high probability. From this, we can approximate E\u03a8[Q] by E\u03a8[ \u2211n v=v\u22c6+1 log q(\u03c3(v),xv\u22c6,v)\np(\u03c3(v\u22c6),\u03c3(v),xv\u22c6,v) ], which is\nitself well-approximated by nD(\u03b1, p). More formally, we can show that:\nE\u03a8[Q] \u2264 ( n+ 2 log(\u03b7) log(n)2 ) D(\u03b1, p) + log \u03b7\nn3 . (4)\nSimilarly, we prove that E\u03a8[(Q \u2212 E\u03a8[Q])2] = O(np\u0304), which in view of Lemma 8 (refer to the supplementary material) and assumption (A2), implies that: E\u03a8[(Q\u2212 E\u03a8[Q])2] = o(nD(\u03b1, p)).\nWe complete the proof of Theorem 1 by putting the above arguments together: From (3), (4) and the above analysis of Q, when the expected number of misclassified items is less than s (i.e., E[\u03b5\u03c0(n)] \u2264 s), we must have: lim infn\u2192\u221e nD(\u03b1,p) log(n/s) \u2265 1."}, {"heading": "4 The Spectral Partition Algorithm and its Optimality", "text": "In this section, we sketch the proof of Theorem 2. To this aim, we present the Spectral Partition (SP) algorithm and analyze its performance. The SP algorithm consists in two parts, and its detailed pseudocode is presented at the beginning of the supplementary document (see Algorithm 1).\nThe first part of the algorithm can be interpreted as an initialization for its second part, and consists in applying a spectral decomposition of a n \u00d7 n random matrix A constructed from the observed labels. More precisely, A =\n\u2211L \u2113=1 w\u2113A \u2113, where A\u2113 is the binary matrix identifying the item pairs with\nobserved label \u2113, i.e., for all v,w \u2208 V , A\u2113vw = 1 if and only if (v,w) has label \u2113. The weight w\u2113 for label \u2113 \u2208 {1, . . . , L} is generated uniformly at random in [0, 1], independently of other weights. From the spectral decomposition of A, we estimate the number of communities and provide asymptotically accurate estimates S1, . . . , SK of the hidden clusters asymptotically accurately, i.e., we show that when np\u0304 = \u03c9(1), with high probability, K\u0302 = K and there exists a permutation \u03b3 of {1, . . . ,K} such that 1 n \u2223 \u2223\u222aKk=1Vk \\ S\u03b3(k) \u2223 \u2223 = O ( log(np\u0304)2 np\u0304 ) . This first part of the SP algorithm is adapted from algorithms proposed for the standard SBM in [4, 25] to handle the additional labels in the model without the knowledge of the number K of clusters.\nThe second part is novel, and is critical to ensure the optimality of the SP algorithm. It consists in first constructing an estimate p\u0302 of the true parameters p of the model from the matrices (A\u2113)1\u2264\u2113\u2264L and the estimated clusters S1, . . . , SK provided in the first part of SP. We expect p to be well estimated since S1, . . . , SK are asymptotically accurate. Then our cluster estimates are iteratively improved. We run \u230alog(n)\u230b iterations. Let S(t)1 , . . . , S (t) K denote the clusters estimated after the t-th iteration, initialized with (S (0) 1 , . . . , S (0) K ) = (S1, . . . , SK). The improved clusters S (t+1) 1 , . . . , S (t+1) K are obtained by assigning each item v \u2208 V to the cluster maximizing a log-likelihood formed from p\u0302, S(t)1 , . . . , S (t) K , and the observations (A\u2113)1\u2264\u2113\u2264L: v is assigned to S (t+1) k\u22c6 where k \u22c6 = argmaxk{ \u2211K i=1 \u2211\nw\u2208S(t\u22121)i\n\u2211L \u2113=0 A \u2113 vw log p\u0302(k, i, \u2113)}.\nPart 1: Spectral Decomposition. The spectral decomposition is described in Lines 1 to 4 in Algorithm 1. As usual in spectral methods, the matrix A is first trimmed (to remove lines and columns corresponding to items with too many observed labels \u2013 as they would perturb the spectral analysis). To this aim, we estimate the average number of labels per item, and use this estimate, denoted by p\u0303 in Algorithm 1, as a reference for the trimming process. \u0393 and A\u0393 denote the set of remaining items after trimming, and the corresponding trimmed matrix, respectively.\nIf the number of clusters K is known and if we do not account for time complexity, the two step algorithm in [4] can extract the clusters from A\u0393: first the optimal rank-K approximation A(K) of A\u0393 is derived using the SVD; then, one applies the k-mean algorithm to the columns of A(K) to reconstruct the clusters. The number of misclassified items after this two step algorithm is obtained as follows. Let M \u2113 = E[A\u2113\u0393], and M = \u2211L \u2113=1w\u2113M\n\u2113 (using the same weights as those defining A). Then, M is of rank K . If v and w are in the same cluster, Mv = Mw and if v and w do not belong to the same cluster, from (A2), we must have with high probability: \u2016Mv \u2212Mw\u20162 = \u2126(p\u0304 \u221a n). Thus, the k-mean algorithm misclassifies v only if \u2016A(K)v \u2212Mv\u20162 = \u2126(p\u0304 \u221a n). By leveraging elements of random graph and random matrix theories, we can establish that \u2211\nv \u2016A (k) v \u2212Mv\u201622 = \u2016A(k)\u2212M\u20162F = O(np\u0304) with high probability.\nHence the algorithm misclassifies O(1/p\u0304) items with high probability. Here the number of clusters K is not given a-priori. In this scenario, Algorithm 2 estimates the rank of M using a singular value thresholding procedure. To reduce the complexity of the algorithm, the singular values and singular vectors are obtained using the iterative power method instead of a direct SVD. It is known from [11] that with \u0398(log(n)) iterations, the iterative power method find singular values and the rank-K approximation very accurately. Hence, when np\u0304 = \u03c9(1), we can easily estimate the rank of M by looking at the number of singular values above the threshold \u221a np\u0303 log(np\u0303), since we know from random matrix theory that the (K+1)-th singular value of A\u0393 is much less than \u221a np\u0303 log(np\u0303) with high probability. In the pseudo-code of Algorithm 2, the estimated rank of M is denoted by K\u0303 . The rank-K\u0303 approximation of A\u0393 obtained by the iterative power method is A\u0302 = U\u0302 V\u0302 = U\u0302 U\u0302\u22a4A\u0393. From the columns of A\u0302, we can estimate the number of clusters and classify items. Almost every column\nof A\u0302 is located around the corresponding column of M within a distance 12\n\u221a\nnp\u03032\nlog(np\u0303) , since \u2211 v \u2016A\u0302v \u2212 Mv\u201622 = \u2016A\u0302 \u2212 M\u20162F = O(np\u0304 log(np\u0304)2) with high probability (we rigorously analyze this distance in\nthe supplementary material Section D.2). From this observation, the columns can be categorised into K groups. To find these groups, we randomly pick log(n) reference columns and for each reference column, search all columns within distance \u221a np\u03032\nlog(np\u0303) . Then, with high probability, each cluster has at least one reference column and each reference column can find most of its cluster members. Finally, the K groups are identified using the reference columns. To this aim, we compute the distance of n log(n) column pairs A\u0302v, A\u0302w. Observe that \u2016A\u0302v\u2212 A\u0302w\u20162 = \u2016V\u0302v\u2212 V\u0302w\u20162 for any u, v \u2208 \u0393, since the columns of U\u0302 are orthonormal. Now V\u0302v is of dimension K\u0303, and hence we can identify the groups using O(nK\u0303 log(n)) operations.\nTheorem 6 Assume that (A1) and (A2) hold, and that np\u0304 = \u03c9(1). After Step 4 (spectral decomposition) in the SP algorithm, with high probability, K\u0302 = K and there exists a permutation \u03b3 of {1, . . . ,K} such that: \u2223\n\u2223\u222aKk=1Vk \\ S\u03b3(k) \u2223\n\u2223 = O ( log(np\u0304)2\np\u0304\n)\n.\nPart 2: Successive clusters improvements. Part 2 of the SP algorithm is described in Lines 5 and 6 in Algorithm 1. To analyze the performance of each improvement iteration, we introduce the set of items H as the largest subset of V such that for all v \u2208 H: (H1) e(v,V) \u2264 10\u03b7np\u0304L; (H2) when v \u2208 Vk, \u2211K\ni=1 \u2211L \u2113=0 e(v,Vi, \u2113) log p(k,i,\u2113) p(j,i,\u2113) \u2265 np\u0304 log(np\u0304)4 for all j 6= k; (H3) e(v,V \\ H) \u2264 2 log(np\u0304)2, where for any S \u2282 V and \u2113, e(v, S, \u2113) = \u2211w\u2208S A\u2113vw , and e(v, S) = \u2211L \u2113=1 e(v, S, \u2113). Condition (H1) means that there are not too many observed labels \u2113 \u2265 1 on pairs including v, (H2) means that an item v \u2208 Vk must be classified to Vk when considering the log-likelihood, and (H3) states that v does not share too many labels with items outside H .\nWe then prove that |V \\ H| \u2264 s with high probability when nD(\u03b1, p) \u2212 np\u0304log(np\u0304)3 \u2265 log(n/s) + \u221a\nlog(n/s). This is mainly done using concentration arguments to relate the quantity \u2211K\ni=1 \u2211L \u2113=0 e(v,Vi, \u2113) log p(k,i,\u2113) p(j,i,\u2113) involved in (H2) to nD(\u03b1, p).\nFinally, we establish that if the clusters provided after the first part of the SP algorithm are asymptotically accurate, then after log(n) improvement iterations, there is no misclassified items in H . To that aim, we denote by E(t) the set of misclassified items after the t-th iteration, and show that with high probability, for all t, |E\n(t+1)\u2229H| |E(t)\u2229H| \u2264 1\u221a np\u0304 . This completes the proof of Theorem 2, since after log(n) iterations,\nthe only misclassified items are those in V \\H ."}, {"heading": "A The SP Algorithm", "text": "In this section, we present the Spectral Partition (SP) algorithm. The main pseudo-code of SP is presented in Algorithm 1. The SP algorithm consists in two parts. In the first part, corresponding to Lines 1-4 in the pseudo-code, we apply a spectral decomposition of the matrix A =\n\u2211L \u2113=1 w\u2113A \u2113 constructed from the observed labels. This matrix is first trimmed, and then treated by applying the spectral decomposition algorithm, whose pseudo-code is presented in Algorithm 2. The second part of the SP algorithm, corresponding to Lines 5 and 6 in Algorithm 1, consists in improving the clusters initially identified in the first step.\nAlgorithm 1 Spectral Partition\nInput: Observation matrices A\u2113 for every label \u2113 (A\u2113uv = 1 if \u2113 is observed between u and v). 1. Estimated average degree. p\u0303 \u2190 \u2211L \u2113=1 \u2211 u,v A \u2113 uv n(n\u22121) 2. Random Weights. A \u2190 \u2211L\u2113=1w\u2113A\u2113 where the weights w\u2113\u2019s are i.i.d and uniformly distributed on [0, 1]. 3. Trimming. Construct A\u0393 = (Avw)v,w\u2208\u0393, where \u0393 is the set of nodes obtained after removing \u230an exp(\u2212np\u0303)\u230b nodes having the largest \u2211\u2113 \u2211 w\u2208V A \u2113 vw. 4. Spectral Decomposition. Run Algorithm 2 with input A\u0393, p\u0303, and output (Sk)k=1,...,K\u0302 . 5. Estimated parameters. p\u0302(i, j, \u2113) \u2190 \u2211 u\u2208Si \u2211 v\u2208Sj A\u2113uv\n|Si||Sj | for all 1 \u2264 i, j \u2264 K\u0302 and 0 \u2264 \u2113 \u2264 L. 6. Improvement. S (0) k \u2190 Sk, for all k for t = 1 to log n do S (t) k \u2190 \u2205, for all k\nfor v \u2208 V do Find k\u22c6 = argmax1\u2264k\u2264K\u0302{ \u2211K\u0302 i=1 \u2211\nw\u2208S(t\u22121)i\n\u2211L \u2113=0A \u2113 vw log p\u0302(k, i, \u2113)} (tie broken uniformly at\nrandom) S (t) k\u22c6 \u2190 S (t) k\u22c6 \u222a {v} end for end for V\u0302k \u2190 S(logn)k , for all k Output: (V\u0302k)k=1,...,K\u0302 ."}, {"heading": "B Properties of the divergence D(\u03b1, p) and related quantities", "text": "In this section, we prove the two claims of Section 2, as well as other results on the divergence D(\u03b1, p) that will be instrumental in the proofs of Theorems.\nAlgorithm 2 Spectral decomposition Input: A\u0393, p\u0303 1. Iterative Power Method with singular value thresholding (Initialization) \u03c7 \u2190 n, k \u2190 0, and U\u0302 \u2190 0n\u00d71 while \u03c7 \u2265 \u221anp\u0303 log(np\u0303) do\nk \u2190 k + 1, U0 \u2190 n\u00d7 1 Gaussian random vector (Iterative power method) Ut \u2190 (A\u0393)\u23082 log(n)\u2309U0 (Orthonormalizing Ut) U\u0302k \u2190\nUt\u2212U\u03021:k\u22121(U\u0302\u22a41:k\u22121Ut) \u2016Ut\u2212U\u03021:k\u22121(U\u0302\u22a41:k\u22121Ut)\u20162\n(Estimating the k-th singular value) \u03c7 \u2190 \u2016A\u0393U\u0302k\u20162 end while K\u0303 \u2190 k \u2212 1, V\u0302 \u2190 U\u0302\u22a4\n1:K\u0303 A\u0393\n2. Clustering VR \u2190 a subset of \u0393 obtained by randomly selecting \u2308log(n)\u2309 items of \u0393 Qv \u2190 {w \u2208 \u0393 : \u2016V\u0302w \u2212 V\u0302v\u201622 \u2264 np\u0303 2\nlog(np\u0303)} for all v \u2208 VR (Initialization) S0 \u2190 \u2205, k \u2190 0, and \u03c1 \u2190 |\u0393| while \u03c1 \u2265 log(np\u0303)4p\u0303 do\nk \u2190 k + 1, v\u22c6k \u2190 argmaxv\u2208VR |Qv \\ \u22c3k\u22121 l=0 Sl|, Sk \u2190 Qv\u22c6k \\ \u22c3k\u22121 l=0 Sl and \u03c1 \u2190 |Sk|. end while K\u0302 \u2190 k \u2212 1 for v \u2208 \u0393 \\\u22c3K\u0302k=1 Sk do\nk\u22c6 \u2190 argmink \u2016V\u0302v\u22c6 k \u2212 V\u0302v\u20162, Sk\u22c6 \u2190 Sk\u22c6 \u222a {v}\nend for Output: (Sk)k=1,...,K\u0302 ."}, {"heading": "B.1 Proof of Claim 4", "text": "DL+(p(i), p(j)) is the minimum of the objective function of the following convex optimization problem:\nmin y\u2208PK\u00d7(L+1)\nK \u2211\nk=1\n\u03b1k\n(\nL \u2211\n\u2113=1\ny(k, \u2113) log\n(\ny(k, \u2113)\np(i, k, \u2113)\n) + (1\u2212 L \u2211\n\u2113=1\ny(k, \u2113)) log\n(\n1\u2212\u2211L\u2113=1 y(k, \u2113) 1\u2212\u2211L\u2113=1 p(i, k, \u2113)\n))\ns.t. K \u2211\nk=1\n\u03b1kKL(y(k), p(i, k)) \u2265 K \u2211\nk=1\n\u03b1kKL(y(k), p(j, k)).\n(5)\nNote that we define y(k, 0) = 1 \u2212\u2211L\u2113=1 y(k, \u2113) for all k. Since p\u0304 = o(1), one can easily check that the solution of (5) has to be\n\u2211L \u2113=1 y(k, \u2113) = o(1) for all k. The objective function converges to infinity when\n\u2211L \u2113=1 y(k, \u2113) = \u2126(1), while it has o(p\u0304) when y(k, \u2113) = p(j, k, \u2113) for all k and \u2113. Thus, we consider\n\u2211L \u2113=1 y(k, \u2113) = o(1). The associated Lagrangian is:\ng(y, \u03bb) = K \u2211\nk=1\n\u03b1k\n(\nL \u2211\n\u2113=1\ny(k, \u2113) log\n(\ny(k, \u2113)\np(i, k, \u2113)\n) + (1\u2212 L \u2211\n\u2113=1\ny(k, \u2113)) log\n(\n1\u2212\u2211L\u2113=1 y(k, \u2113) 1\u2212\u2211L\u2113=1 p(i, k, \u2113)\n))\n+\nK \u2211\nk=1\n\u03b1k\u03bb\n(\nL \u2211\n\u2113=1\ny(k, \u2113) log\n(\np(i, k, \u2113)\np(j, k, \u2113)\n) + (1\u2212 L \u2211\n\u2113=1\ny(k, \u2113)) log\n(\n1\u2212\u2211L\u2113=1 p(i, k, \u2113) 1\u2212\u2211L\u2113=1 p(j, k, \u2113)\n))\n.\n(6)\nThe derivative of g(y, \u03bb) w.r.t. y(k, \u2113) is computed as follows:\n\u2202g(y, \u03bb) \u2202y(k, \u2113) =\u03b1k\n(\nlog\n(\ny(k, \u2113)\np(i, k, \u2113)\n) \u2212 log ( 1\u2212\u2211Lm=1 y(k,m) 1\u2212\u2211Lm=1 p(i, k,m) )) +\n\u03b1k\u03bb\n(\nlog\n(\np(i, k, \u2113)\np(j, k, \u2113)\n) \u2212 log ( 1\u2212\u2211Lm=1 p(i, k,m) 1\u2212\u2211Lm=1 p(j, k,m) )) .\nObserve that, since (A1) holds, p\u0304 = o(1) and \u2211L \u2113=1 y(k, \u2113) = o(1), as n grows large, log (\n1\u2212 \u2211L\nm=1 y(k,m)\n1\u2212 \u2211L\nm=1 p(i,k,m)\n)\nand log ( 1\u2212\u2211Lm=1 p(i,k,m) 1\u2212\u2211Lm=1 p(j,k,m) ) converges to 0. Thus, (6) is minimized at\ny(k, \u2113) = p(i, k, \u2113)\n(\np(j, k, \u2113)\np(i, k, \u2113)\n)\u03bb\n(1 + o(1)). (7)\nWhen we put (7) onto (6) and use the approximation limx\u21920 log(1 + x) = x (again using p\u0304 = o(1)),\nmin y\u2208PK\u00d7{0,1} g(y, \u03bb)\n= min y\u2208PK\u00d7{0,1}\nK \u2211\nk=1\nL \u2211\n\u2113=1\n\u03b1k (o(p\u0304)+\n(1\u2212 L \u2211\n\u2113=1\ny(k, \u2113)) log\n(\n1\u2212\u2211L\u2113=1 y(k, \u2113) 1\u2212\u2211L\u2113=1 p(i, k, \u2113)\n)(\n1\u2212\u2211L\u2113=1 p(i, k, \u2113) 1\u2212\u2211L\u2113=1 p(j, k, \u2113)\n)\u03bb \n\n= min y\u2208PK\u00d7{0,1}\nK \u2211\nk=1\nL \u2211\n\u2113=1\n\u03b1k (o(p\u0304)\u2212\nL \u2211\n\u2113=1\ny(k, \u2113)(1 + o(1)) + (1\u2212 \u03bb) L \u2211\n\u2113=1\np(i, k, \u2113)(1 + o(1)) + \u03bb\nL \u2211\n\u2113=1\np(j, k, \u2113)(1 + o(1))\n)\n.\nTherefore, the minimum value of (5) is equivalent to\nmax \u03bb\u2208[0,1]\nK \u2211\nk=1\nL \u2211\n\u2113=1\n\u03b1k\n( (1\u2212 \u03bb)p(i, k, \u2113) + \u03bbp(j, k, \u2113) \u2212 p(i, k, 1)1\u2212\u03bbp(j, k, \u2113)\u03bb ) + o(p\u0304)."}, {"heading": "B.2 Proof of Claim 5", "text": "When p\u0304 = o(1), for all i 6= j, \u03b1i = 1K , p(i, i, \u2113) = p(\u2113), and p(i, j, \u2113) = q(\u2113), from Claim 4,\nDL+(\u03b1, p(i), p(j)) = max \u03bb\u2208[0,1]\nK \u2211\nk=1\nL \u2211\n\u2113=1\n\u03b1k\n( (1\u2212 \u03bb)p(i, k, \u2113) + \u03bbp(j, k, \u2113) \u2212 p(i, k, \u2113)1\u2212\u03bbp(j, k, \u2113)\u03bb )\n= 1\nK max \u03bb\u2208[0,1]\nL \u2211\n\u2113=1\n( p(\u2113) + q(\u2113)\u2212 p(\u2113)1\u2212\u03bbq(\u2113)\u03bb \u2212 p(\u2113)\u03bbq(\u2113)1\u2212\u03bb )\n= 1\nK\nL \u2211\n\u2113=1\n(\np(\u2113) + q(\u2113)\u2212 2 \u221a p(\u2113)q(\u2113) ) . (8)\nNow, since \u221a 1 + x = 1 + x2 (1 + o(1)) and log(1 + x) = x(1 + o(1)) when x = o(1),\n\u2212 2 K log\n(\nL \u2211\n\u2113=0\n\u221a\np(\u2113)q(\u2113)\n)\n= \u2212 2 K log\n(\n\u221a\np(0)q(0) +\nL \u2211\n\u2113=1\n\u221a\np(\u2113)q(\u2113)\n)\n= \u2212 2 K log\n(\n1\u2212 \u2211L \u2113=1 p(\u2113) + q(\u2113)\n2 (1 + o(1)) +\nL \u2211\n\u2113=1\n\u221a\np(\u2113)q(\u2113)\n)\n= 2\nK\n(\n\u2211L \u2113=1 p(\u2113) + q(\u2113)\n2 \u2212\nL \u2211\n\u2113=1\n\u221a\np(\u2113)q(\u2113)\n)\n(1 + o(1)). (9)\nThe claim follows from (8) and (9)."}, {"heading": "B.3 Other properties", "text": "Lemma 7 Let (i\u22c6, j\u22c6) = argmini,j DL+(p(i), p(j)) and i\u22c6 < j\u22c6. Then, there exists q \u2208 PK\u00d7(L+1) such that\nD(\u03b1, p) = K \u2211\nk=1\n\u03b1kKL(q(k), p(i \u22c6, k)) =\nK \u2211\nk=1\n\u03b1kKL(q(k), p(j \u22c6, k)).\nProof. We check by contradiction that such a q exists. Indeed, assume that\nD(\u03b1, p) = K \u2211\nk=1\n\u03b1kKL(q(k), p(i \u22c6, k)) >\nK \u2211\nk=1\n\u03b1kKL(q(k), p(j \u22c6, k)).\nThen there exists k0 such that KL(q(k0), p(i\u22c6, k0)) > KL(q(k0), p(j\u22c6, k0)). Observe that by positivity of the KL divergence, q(k0) 6= p(i\u22c6, k0). Hence by continuity of the KL divergence, we can construct q\u2032 such that q(k) = q\u2032(k) for all k 6= k0, and such that: KL(q(k0), p(i\u22c6, k0)) \u2212 \u01eb < KL(q\u2032(k0), p(i\u22c6, k0)) < KL(q(k0), p(i\u22c6, k0)) and KL(q\u2032(k0), p(j\u22c6, k0)) < KL(q(k0), p(j\u22c6, k0)) + \u01eb for some 0 < \u01eb < (KL(q(k0), p(i\u22c6, k0))\u2212KL(q(k0), p(j\u22c6, k0)))/2. With this choice of q\u2032, we get:\nD(\u03b1, p) >\nK \u2211\nk=1\n\u03b1kKL(q \u2032(k), p(i\u22c6, k)) >\nK \u2211\nk=1\n\u03b1kKL(q \u2032(k), p(j\u22c6, k)),\nwhich contradicts the definition of D(\u03b1, p).\nLemma 8 When p\u0304 = o(1),\nlim n\u2192\u221e D(\u03b1, p) \u2211K\nk=1 \u03b1k 2\n(\n\u2211L \u2113=1(\n\u221a p(i\u22c6, k, \u2113) \u2212 \u221a p(j\u22c6, k, \u2113))2 ) \u2265 1.\nProof. Let (i\u22c6, j\u22c6) = argmini,j DL+(\u03b1, p(i), p(j)) and i\u22c6 < j\u22c6. From Lemma 7, there exists q satisfying that\nD(\u03b1, p) = K \u2211\nk=1\n\u03b1kKL(q(k), p(i \u22c6, k)) =\nK \u2211\nk=1\n\u03b1kKL(q(k), p(j \u22c6, k)).\nThen,\nnD(\u03b1, p) = n\n\u2211K k=1 (\u03b1kKL(q(k), p(i \u22c6, k)) + \u03b1kKL(q(k), p(j \u22c6, k)))\n2\n= \u2212n K \u2211\nk=1\n\u03b1k\nL \u2211\n\u2113=0\nq(k, \u2113) log\n(\n\u221a\np(i\u22c6, k, \u2113)p(j\u22c6, k, \u2113)\nq(k, \u2113)\n)\n\u2265 n K \u2211\nk=1\n\u03b1k\nL \u2211\n\u2113=0\n(\nq(k, \u2113)\u2212 \u221a p(i\u22c6, k, \u2113)p(j\u22c6, k, \u2113) )\n= n K \u2211\nk=1\n\u03b1k\n(\n\u2211L \u2113=1(p(i \u22c6, k, \u2113) + p(j\u22c6, k, \u2113))\n2 \u2212\nL \u2211\n\u2113=1\n\u221a\np(i\u22c6, k, \u2113)p(j\u22c6, k, \u2113)\n)\n(1\u2212 o(1))\n= n\nK \u2211\nk=1\n\u03b1k 2\n(\nL \u2211\n\u2113=1\n( \u221a p(i\u22c6, k, \u2113) \u2212 \u221a p(j\u22c6, k, \u2113))2\n)\n(1\u2212 o(1)) .\nLemma 9 Under condition (A1), when p\u0304 = o(1), lim supn\u2192\u221e D(\u03b1,p) \u03b7p\u0304L \u2264 1.\nProof. From the definition of D(\u03b1, p), for any i 6= j,\nD(\u03b1, p) \u2264 max { K \u2211\nk=1\n\u03b1kKL(p(i, k), p(i, k)),\nK \u2211\nk=1\n\u03b1kKL(p(i, k), p(j, k))\n}\n=\nK \u2211\nk=1\n\u03b1kKL(p(i, k), p(j, k))\n\u2264 K \u2211\nk=1\n\u03b1k\nL \u2211\n\u2113=1\n(p(i, k, \u2113) \u2212 p(j, k, \u2113))2 p(j, k, \u2113) (1 + o(1))\n\u2264 K \u2211\nk=1\n\u03b1k\nL \u2211\n\u2113=1\n\u03b7p\u0304(1 + o(1))\n= \u03b7p\u0304L(1 + o(1)),\nwhere we use log(1 + x) = x(1 + o(1)) when x = o(1)."}, {"heading": "C Proof of Theorem 1", "text": "The proof consists in an appropriate change-of-measure argument. The originality of the proof stems from the fact that the change of measures is obtained by a judicious coupling argument [17]. In the following, we refer to \u03a6 as the true stochastic model under which all the observed random labels are generated, and denote by P\u03a6 = P (resp. E\u03a6[\u00b7] = E[\u00b7]) the corresponding probability measure (resp. expectation). We recall that \u03a6 is defined by the parameters (\u03b1, p), and that under \u03a6, the nodes are first\nattached to the various clusters according to the distribution \u03b1, and the labels between two nodes are then generated using distributions p. The proof consists in constructing a perturbed stochastic model \u03a8 coupling the labels generated under \u03a6 with those generated under \u03a8. We denote by P\u03a8 (resp. E\u03a8[\u00b7] = E[\u00b7]) the probability measure (resp. expectation) under the perturbed model \u03a8. We then relate the proportion of misclassified nodes under any given clustering algorithm \u03c0 to the distribution under P\u03a8 of a quantity Q that resembles the log-likelihood ratio of the observed labels under P\u03a6 and P\u03a8. The analysis of the likelihood ratio finally provides the desired lower bound on the expected misclassified nodes under \u03c0. Next, we detail each step of the proof.\nCoupling and the perturbed stochastic model \u03a8. Let (i\u22c6, j\u22c6) = argmini,j:i<j DL+(p(i), p(j)), and let v\u22c6 denote the smallest node index that belongs to cluster i\u22c6 or j\u22c6. If both Vi\u22c6 and Vj\u22c6 are empty, we define v\u22c6 = n. Let q \u2208 [0, 1]K\u00d7(L+1) satisfy:\nD(\u03b1, p) =\nK \u2211\nk=1\n\u03b1kKL(q(k), p(i \u22c6, k)) =\nK \u2211\nk=1\n\u03b1kKL(q(k), p(j \u22c6, k)).\nThere exists such a q from Lemma 7. Now to define the perturbed stochastic model \u03a8, we couple the generation of labels under \u03a6 and \u03a8 as follows.\n1. We first generate construct the random clusters V1, . . . ,VK under \u03a6, and extract i\u22c6, j\u22c6, and v\u22c6. The clusters generated under \u03a8 are the same as those generated under \u03a6. For any v \u2208 V , we denote by \u03c3(v) the cluster of node v.\n2. For all nodes v,w 6= v\u22c6, the labels generated under \u03a8 are the same as those generated under \u03a6, i.e., the label \u2113 is observed on the edge (v,w) with probability p(\u03c3(v), \u03c3(w), \u2113).\n3. Under \u03a8, for any v 6= v\u22c6, the observed label on the edge (v, v\u22c6) under \u03a8 is \u2113 with probability q(\u03c3(v), \u2113).\nThe log-likelihood ratio and its connection to the expected number of misclassified nodes. Let xv,w denote the label observed on the edge (v,w). We introduce Q, referred to as the pseudo-log-likelihood ratio of the observed labels under P\u03a6 and P\u03a8) as:\nQ = v\u22c6\u22121 \u2211\nv=1\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) +\nn \u2211\nv=v\u22c6+1\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) . (10)\nLet \u03c0 denote a clustering algorithm with output (V\u0302k)1\u2264k\u2264K , and let E = \u22c3 1\u2264k\u2264K V\u0302k \\ Vk be the set of misclassified nodes under \u03c0. Note that in general in our proofs, we always assume without loss of generality that |\u22c31\u2264k\u2264K V\u0302k \\ Vk| \u2264 | \u22c3\n1\u2264k\u2264K V\u0302\u03b3(k) \\ Vk| for any permutation \u03b3, so that the set of misclassified nodes is really E . We denote by \u03b5\u03c0(n) = |E|. Since under \u03a6, nodes are interchangeable (remember that nodes are assigned to the various clusters in an i.i.d. manner), we have:\nnP\u03a6{v \u2208 E} = E\u03a6[\u03b5\u03c0(n)] = E[\u03b5\u03c0(n)].\nNext, we establish a relationship between E[\u03b5\u03c0(n)] and the distribution of Q under P\u03a8. For any function f(n), we have:\nP\u03a8{Q \u2264 f(n)} = P\u03a8{Q \u2264 f(n), v\u22c6 \u2208 E}+ P\u03a8{Q \u2264 f(n), v\u22c6 /\u2208 E}. (11)\nUsing Q, we get:\nP\u03a8{Q \u2264 f(n), v\u22c6 \u2208 E} = \u222b\n{Q\u2264f(n),v\u22c6\u2208E} dP\u03a8\n=\n\u222b\n{Q\u2264f(n),v\u22c6\u2208E} exp(Q)dP\u03a6\n\u2264 exp(f(n))P\u03a6{Q \u2264 f(n), v\u22c6 \u2208 E} \u2264 exp(f(n))P\u03a6{v\u22c6 \u2208 E} \u2264 exp(f(n)) E\u03a6[\u03b5 \u03c0(n)]\n(\u03b1i\u22c6 + \u03b1j\u22c6)n , (12)\nwhere the last inequality is obtained from the fact that we cannot distinguish between v\u22c6 and any other v \u2208 V\u03c3(v\u22c6). Indeed,\nP\u03a6{v\u22c6 \u2208 E} = P\u03a6{v \u2208 E|v \u2208 Vi\u22c6 \u222a Vj\u22c6} =\nP\u03a6{v \u2208 E , v \u2208 Vi\u22c6 \u222a Vj\u22c6} P\u03a6{v \u2208 Vi\u22c6 \u222a Vj\u22c6}\n\u2264 P\u03a6{v \u2208 E} P\u03a6{v \u2208 Vi\u22c6 \u222a Vj\u22c6} = E\u03a6[\u03b5 \u03c0(n)] (\u03b1i\u22c6 + \u03b1j\u22c6)n .\nFurthermore, since under the stochastic model \u03a8, the observed labels do not depend on whether v\u22c6 belongs to cluster i\u22c6 or j\u22c6, we have:\nP\u03a8{v\u22c6 \u2208 V\u0302i\u22c6 |v\u22c6 \u2208 Vi\u22c6} = P\u03a8{v\u22c6 \u2208 V\u0302i\u22c6 |v\u22c6 \u2208 Vj\u22c6} and P\u03a8{v\u22c6 \u2208 V\u0302j\u22c6|v\u22c6 \u2208 Vi\u22c6} = P\u03a8{v\u22c6 \u2208 V\u0302j\u22c6|v\u22c6 \u2208 Vj\u22c6}.\nFinally, since P\u03a8{v\u22c6 \u2208 V\u0302i\u22c6 |v\u22c6 \u2208 Vi\u22c6}+ P\u03a8{v\u22c6 \u2208 V\u0302j\u22c6|v\u22c6 \u2208 Vi\u22c6} \u2264 1, we also have:\nP\u03a8{Q \u2264 f(n), v\u22c6 /\u2208 E} \u2264 P\u03a8{v\u22c6 /\u2208 E} = \u03b1i\u22c6\n\u03b1i\u22c6 + \u03b1j\u22c6 P\u03a8{v\u22c6 \u2208 V\u0302i\u22c6 |v\u22c6 \u2208 Vi\u22c6}+\n\u03b1j\u22c6\n\u03b1i\u22c6 + \u03b1j\u22c6 P\u03a8{v\u22c6 \u2208 V\u0302j\u22c6|v\u22c6 \u2208 Vj\u22c6}\n= \u03b1i\u22c6\n\u03b1i\u22c6 + \u03b1j\u22c6 P\u03a8{v\u22c6 \u2208 V\u0302i\u22c6 |v\u22c6 \u2208 Vi\u22c6}+\n\u03b1j\u22c6\n\u03b1i\u22c6 + \u03b1j\u22c6 P\u03a8{v\u22c6 \u2208 V\u0302j\u22c6|v\u22c6 \u2208 Vi\u22c6}\n\u2264 \u03b1j\u22c6 \u03b1i\u22c6 + \u03b1j\u22c6 . (13)\nCombining (11), (12), and (13), we conclude that:\nP\u03a8{Q \u2264 f(n)} \u2264 exp(f(n)) E\u03a6[\u03b5\n\u03c0(n)]\n(\u03b1i\u22c6 + \u03b1j\u22c6)n +\n\u03b1j\u22c6\n\u03b1i\u22c6 + \u03b1j\u22c6 . (14)\nThe previous equation provides the desired generic relationship between E\u03a6[\u03b5\u03c0(n)] and P\u03a8{Q \u2264 f(n)} from which can deduce a necessary condition for E[\u03b5\u03c0(n)] \u2264 s. Applying (14) with f(n) = log (n/E\u03a6[\u03b5 \u03c0(n)])\u2212 log(2/\u03b1i\u22c6), we have:\nP\u03a8{Q \u2264 log (n/E\u03a6[\u03b5\u03c0(n)])\u2212 log(2/\u03b1i\u22c6)} \u2264 1\u2212 \u03b1i\u22c6 2 < 1\u2212 \u03b1i\u22c6 4 . (15)\nIn addition, from Chebyshev\u2019s inequality,\nP\u03a8\n{ Q \u2264 E\u03a8[Q] + \u221a 4\n\u03b1i\u22c6 E\u03a8[(Q\u2212 E\u03a8[Q])2]\n}\n\u2265 1\u2212 \u03b1i\u22c6 4 . (16)\nFrom (15) and (16), we deduce that:\nlog (n/E\u03a6[\u03b5 \u03c0(n)]) \u2212 log(2/\u03b1i\u22c6) \u2264 E\u03a8[Q] +\n\u221a\n4\n\u03b1i\u22c6 E\u03a8[(Q\u2212 E\u03a8[Q])2],\nand thus, a necessary condition for E[\u03b5\u03c0(n)] \u2264 s is:\nlog (n/s)\u2212 log(2/\u03b1i\u22c6) \u2264 E\u03a8[Q] + \u221a 4\n\u03b1i\u22c6 E\u03a8[(Q\u2212 E\u03a8[Q])2]. (17)\nAnalysis of the log-likelihood ratio. In view of (17), we can obtain a necessary condition for E[\u03b5\u03c0(n)] \u2264 s if we evaluate E\u03a8[Q] and E\u03a8[(Q\u2212 E\u03a8[Q])2]. (i) We first compute E\u03a8[Q]. Note that in view of the definition of v\u22c6, a node whose index is smaller than v\u22c6 cannot be in Vi\u22c6 or Vj\u22c6 , whereas a node whose index v is larger than v\u22c6 can be in any cluster (and the cluster of such a v is drawn according to the distribution \u03b1 independently of other nodes). This slightly complicates the computation of the expectation of the two sums defining Q in (10). To circumvent this problem, we can observe that v\u22c6 is rather small, i.e., less log(n)2 with high probability, and that hence, we can approximate E\u03a8[Q] by E\u03a8[ \u2211n v=v\u22c6+1 log q(\u03c3(v),xv\u22c6,v)\np(\u03c3(v\u22c6),\u03c3(v),xv\u22c6,v) ], which is itself well-approximated by\nnD(\u03b1, p). More formally, since P{v\u22c6 \u2264 m} = 1\u2212 (1\u2212 \u03b1i\u22c6 \u2212 \u03b1j\u22c6)m,\nP{v\u22c6 \u2264 log(n)2} \u2265 1\u2212 1 n4 . (18)\nHence from condition (A1), (18), and the definition of Q,\nE\u03a8[Q] = P{v\u22c6 > log(n)2}E\u03a8[Q|v\u22c6 > log(n)2] + P{v\u22c6 \u2264 log(n)2}E\u03a8[Q|v\u22c6 \u2264 log(n)2] \u2264 log \u03b7\nn3 + E\u03a8[Q|v\u22c6 \u2264 log(n)2]\n\u2264 log \u03b7 n3 + E\u03a8\n[ v\u22c6\u22121 \u2211\nv=1\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) |v\u22c6 \u2264 log(n)2\n]\n+ nD(\u03b1, p)\n\u2264 log \u03b7 n3 + E\u03a8\n\n(v\u22c6 \u2212 1) \u2211\nk/\u2208{i\u22c6,j\u22c6}\n\u03b1kKL(q(k), p(\u03c3(v \u22c6, k)))\n1\u2212 \u03b1i\u22c6 \u2212 \u03b1j\u22c6 |v\u22c6 \u2264 log(n)2\n\n+ nD(\u03b1, p)\n\u2264 ( n+ 2 log(n)2 log \u03b7 ) D(\u03b1, p) + log \u03b7\nn3 , (19)\nwhere the last inequlaity stems from the fact that 2KL(q(i), p(\u03c3(v\u22c6, i))) log \u03b7 \u2265 KL(q(j), p(\u03c3(v\u22c6, j))) for all i and j from condition (A1).\n(ii) To compute E\u03a8[(Q \u2212 E\u03a8[Q])2], we evaluate E\u03a8[(Q \u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6] and E\u03a8[(Q \u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = j\u22c6]. From condition (A1), (18), and the definition of Q,\nE\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6] = P{v\u22c6 \u2264 log(n)2}E\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6, v\u22c6 \u2264 log(n)2]\n+P{v\u22c6 > log(n)2}E\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6, v\u22c6 > log(n)2] \u2264 E\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6, v\u22c6 \u2264 log(n)2]\n+ 1\nn4 E\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = i\u22c6, v\u22c6 > log(n)2]\n= O(np\u0304).\nTo derive the above inequality, we have used:\nE\u03a8\n\n\n(\nn \u2211\nv=v\u22c6+1\n(\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) \u2212D(\u03b1, p)\n)\n)2 |\u03c3(v\u22c6) = i\u22c6  \n= n \u2211\nv=v\u22c6+1\nE\u03a8\n[\n(\nlog q(\u03c3(v), xv\u22c6 ,v)\np(i\u22c6, \u03c3(v), xv\u22c6 ,v) \u2212D(\u03b1, p)\n)2 |\u03c3(v\u22c6) = i\u22c6 ]\n= O(np\u0304) and\nE\u03a8\n\n\n( v\u22c6\u22121 \u2211\nv=1\n(\nlog q(\u03c3(v), xv\u22c6 ,v)\np(\u03c3(v\u22c6), \u03c3(v), xv\u22c6 ,v) \u2212D(\u03b1, p)\n)\n)2 |\u03c3(v\u22c6) = i\u22c6  \n= O(v\u22c6p\u0304+ (v\u22c6p\u0304)2),\nwhere we use (A1) and the fact that every label is generated independently. Using the same approach, we can also conclude that E\u03a8[(Q\u2212 nD(\u03b1, p))2|\u03c3(v\u22c6) = j\u22c6] = O(np\u0304). In summary, we have:\nE\u03a8[(Q\u2212 E\u03a8[Q])2] = O(np\u0304). (20)\nWe are ready to complete the proof of Theorem 1. From (17), (19), (20), and Lemma 8, when the expected number of misclassified nodes is less than s (i.e., E[\u03b5\u03c0(n)] \u2264 s ), we must have:\nlim inf n\u2192\u221e\nnD(\u03b1, p) log (n/s) \u2265 1."}, {"heading": "D Performance of the SP Algorithm \u2013 Proof of Theorem 2", "text": "Notations. We use the standard matrix norm \u2016A\u2016 = sup x:\u2016x\u20162=1 \u2016Ax\u20162. We denote by M \u2113 the expectation of the matrix of A\u2113, i.e., M \u2113u,v = p(i, j, \u2113) when u \u2208 Vi and v \u2208 Vj . Let M = \u2211L \u2113=1 w\u2113M \u2113. We define A\u0393 to denote the adjacency matrix obtained after trimming (Step 3 in Algorithm 1). For any matrix R \u2208 R n\u00d7n, we define the matrix R\u0393 the square matrix formed by the lines and columns of R whose indexes are in \u0393. Hence, we can define A\u2113\u0393, M \u2113 \u0393, and M\u0393 where \u0393 is the set of items obtained after the trimming process (Line 3) in the SP algorithm (when taking the expectation to get for example M\u0393, we condition on \u0393). We introduce the noise matrices X\u2113\u0393 = A \u2113 \u0393 \u2212 M \u2113\u0393 and X\u0393 = \u2211L \u2113=1w\u2113X \u2113 \u0393. We also denote by e(v, S, \u2113) = \u2211\nw\u2208S A \u2113 vw the total number of item pairs with observed label \u2113 including the item v and an\nitem from S and \u00b5(v, S, \u2113) = e(v,S,\u2113)|S| the empirical density of label \u2113. Let e(v, S) = \u2211L\n\u2113=1 e(v, S, \u2113) and \u00b5(v, S) = [\u00b5(v, S, \u2113)]0\u2264\u2113\u2264L. In what follows, e(v,V) is referred to as the degree of item v (the number of observed labels different than 0 of pairs of items including v).\nOutline of the proof. To analyze the performance of the SP algorithm, we first state preliminary lemmas. Lemma 10 is concerned with the concentration of the degree of the various items. Lemma 11 provides an upper bound of the matrix norm of random noise matrix X\u2113\u0393. From these two lemmas, we analyze the performance of the first part of the SP algorithm, and prove Theorem 6. To analyze the second part of the SP algorithm consisting of log(n) improvement iterations, we introduce an appropriate set of items H such that that V \\H is of cardinality less than s with high probability under the condition that\nnD(\u03b1, p)\u2212 np\u0304 log(np\u0304)3 \u2265 log(n/s)+ \u221a log(n/s). We further bound the rate of improvement of our cluster estimates in each iteration when restricted to the set of items H , and deduce that after log(n) iterations, no item in H is misclassified."}, {"heading": "D.1 Preliminary lemmas", "text": "Lemma 10 For every v \u2208 V and c \u2265 1, we have\nP{e(v,V) \u2265 10cnp\u0304L} \u2264 exp(\u221210cnp\u0304L).\nProof. From Markov inequality,\nP{e(v,V) \u2265 10np\u0304L} \u2264 inf \u03b8>0\n\u220fK k=1 E [exp(\u03b8e(v,Vk))]\nexp(\u03b810cnp\u0304L)\n\u2264 inf \u03b8>0\n\u220fK k=1\n( 1 + p\u0304L(exp(\u03b8)\u2212 1) )\u03b1kn\nexp(\u03b810cnp\u0304L)\n\u2264 inf \u03b8>0\n\u220fK k=1\n( exp(p\u0304L(exp(\u03b8)\u2212 1)) )\u03b1kn\nexp(\u03b810cnp\u0304L) \u2264 exp(\u221210cnp\u0304L),\nwhere we derive the last inequality choosing \u03b8 = 2.\nLemma 11 (Lemma 8.5 of [4]) When e(v,V, \u2113) \u2264 \u2206 for all v \u2208 \u0393, with high probability,\n\u2016X\u2113\u0393\u2016 = O( \u221a np\u0304+\u2206).\nThe proof of Lemma 11 relies on arguments used in the spectral analysis of random graphs, see [7] and [4].\nLemma 12 For all v \u2208 Vk and D \u2265 0,\nP\n{(\nK \u2211\ni=1\n|Vi|KL(\u00b5(v,Vi), p(k, i)) \u2265 nD ) \u2229 ( e(v,V) \u2264 10\u03b7np\u0304L ) }\n\u2264 exp ( \u2212nD +KL log(10\u03b7Lnp\u0304) + 100\u03b7 2np\u03042L2\n\u03b11\n)\n.\nProof. Let X be a set of K \u00d7 (L+ 1) matrices such that\nX = { x \u2208 ZK\u00d7(L+1) : K \u2211\ni=1\nL \u2211\n\u2113=1\nxi,\u2113 \u2264 10\u03b7np\u0304L, and L \u2211\n\u2113=0\nxi,\u2113 = |Vi| for all 1 \u2264 i \u2264 K } .\nFor notational simplicity, we use [xi,\u2113|Vi| ] instead of [ xi,\u2113 |Vi| ]0\u2264\u2113\u2264L to represent the probability mass vector on labels defined by xi. With a slight abuse of notation, we denote by e(v) the K \u00d7 (L + 1) matrix whose (i, \u2113) element is e(v,Vi, \u2113). Then, for v \u2208 Vk,\nP\n{(\nK \u2211\ni=1\n|Vi|KL(\u00b5(v,Vi), p(k, i)) \u2265 nD ) \u2229 ( e(v,V) \u2264 10np\u0304L ) }\n= \u2211\nx\u2208X P {e(v) = x}P\n{\nK \u2211\ni=1\n|Vi|KL(\u00b5(v,Vi), p(k, i)) \u2265 nD \u2223 \u2223 \u2223\n\u2223\ne(v) = x\n}\n\u2264 \u2211\nx\u2208X P{e(v) = x}\nexp (\n\u2211K i=1 |Vi|KL([ xi,\u2113 |Vi| ], p(k, i))\n)\nexp(nD)\n\u2264 \u2211\nx\u2208X P{e(v) = x}\n\u220fK i=1 \u220fL \u2113=0\n(\nxi,\u2113 |Vi|p(k,i,\u2113)\n)xi,\u2113\nexp(nD)\n(a) \u2264 1 exp(nD) \u2211\nx\u2208X\nK \u220f\ni=1\n((\n1\u2212 \u2211L\n\u2113=1 xi,\u2113 |Vi|\n)xi,0\nexp(\nL \u2211\n\u2113=1\nxi,\u2113)\n)\n= 1\nexp(nD)\n\u2211\nx\u2208X\nK \u220f\ni=1\nexp\n(\n(|Vi| \u2212 L \u2211\n\u2113=1\nxi,\u2113) log\n(\n1\u2212 \u2211L\n\u2113=1 xi,\u2113 |Vi|\n)\n+\nL \u2211\n\u2113=1\nxi,\u2113\n)\n\u2264 1 exp(nD) \u2211\nx\u2208X\nK \u220f\ni=1\nexp\n(\n( \u2211L \u2113=1 xk,\u2113) 2\n|Vi|\n)\n\u2264 (10\u03b7np\u0304L) KL exp(100\u03b72np\u03042L2/\u03b11)\nexp(nD)\n= exp\n(\n\u2212nD +KL log(10\u03b7Lnp\u0304) + 100\u03b7 2np\u03042L2\n\u03b11\n)\n,\nwhere (a) stems from the following inequality:\nP{e(v,Vi, \u2113) = xi,\u2113 for all i, \u2113}\n\u2264 K \u220f\ni=1\n(\np(k, i, 0)xi,0 L \u220f\n\u2113=1\n(|Vi| xi,\u2113 ) p(k, i, \u2113)xk,\u2113\n)\n\u2264 K \u220f\ni=1\n(\np(k, i, 0)xi,0 L \u220f\n\u2113=1\n( e|Vi| xi,\u2113 )xi,\u2113 p(k, i, \u2113)xi,\u2113\n)\n."}, {"heading": "D.2 Part 1 of the SP algorithm \u2013 Proof of Theorem 6", "text": "Recall that A\u0302 = U\u0302 V\u0302 = U\u0302 U\u0302\u22a4A\u0393 and \u2016A\u0302u\u2212A\u0302v\u2016 = \u2016V\u0302u\u2212V\u0302v\u2016. We can bound the number of misclassified items as follows:\n\u2022 with high probability, we have \u2016A\u0302\u2212M\u0393\u20162F = \u2211\nv\u2208\u0393 \u2016A\u0302v \u2212Mv,\u0393\u201622 = O(np\u0304 log(np\u0304)2); (21)\n\u2022 with high probability, every item pair u and v satisfies that when \u03c3(v) represents the cluster of v and Mv,\u0393 denotes the column vector of M\u0393 on v,\n\u2016Mu,\u0393 \u2212Mv,\u0393\u201622 = \u2126 ( np\u03042 ) when \u03c3(u) 6= \u03c3(v), (22)\nsince every w\u2113 is generated uniformly at random in [0, 1] and (A2) holds;\n\u2022 (22) suggests that if v is misclassified by Algorithm 2, then we should have:\n\u2016A\u0302v \u2212Mv,\u0393\u201622 = \u2126 ( np\u03042 ) ; (23)\n\u2022 from (21) and (23), with high probability, \u2223\n\u2223 \u2223 \u2223 \u2223\nK \u22c3\nk=1\n(Vk \\ Sk) \u2223 \u2223 \u2223 \u2223\n\u2223\n= O\n(\nlog(np\u0304)2\np\u0304\n)\n.\nNext, we prove (21) and (23).\nProof of (21). First observe that from the definition of \u0393,\nP\n{\nmax v\u2208\u0393\ne(v,V) \u2265 10np\u0304L } = P {|{v : e(v,V) \u2265 10np\u0304L}| > \u230an exp(\u2212np\u0303)\u230b}\n\u2264 n exp(\u221210np\u0304L)\u230an exp(\u2212np\u0303)\u230b+ 1 \u2264 exp(\u22125np\u0304L),\nwhere the first inequality stems from Lemma 10 and Markov inequality. Therefore, with high probability,\nmax v\u2208\u0393\ne(v,V) \u2264 10np\u0304L. (24)\nWhen the degrees of items are bounded, the standard matrix norm of each noise matrix X\u2113\u0393 can be bounded using Lemma 11. From (24) and Lemma 11,\n\u2016X\u0393\u2016 \u2264 L \u2211\n\u2113=1\nw\u2113\u2016X\u2113\u0393\u2016\n= L \u2211\n\u2113=1\nO(w\u2113 \u221a np\u0304+ 10np\u0304L)\n= O( \u221a np\u0304). (25)\nLet K\u0303 be the number of columns of U\u0302 . Since A\u0302 is the K\u0303-rank approximation of A\u0393 obtained by the iterative power method with 2 log(n) iterations, from Theorem 9.1 and Theorem 9.2 in [11], with high probability,\n1 2 sk(A\u0393) \u2264 \u2016A\u0393U\u0302k\u2016 \u2264 sk(A\u0393) and \u2016A\u0393(I \u2212 U\u03021:kU\u0302\u22a41:k)\u2016 \u2264 2sk+1(A\u0393). (26)\nSince \u2016A\u0393U\u0302K\u2016 \u2264 sK+1(A\u0393) \u2264 \u2016X\u0393\u2016 = O( \u221a np\u0304) from Lemma 11 and (26), K\u0303 \u2264 K and thus the rank of (A\u0302\u2212M\u0393) is less than 2K . Therefore,\n\u2016A\u0302\u2212M\u0393\u20162F \u2264 2K\u2016A\u0302\u2212M\u0393\u20162\n\u2264 4K ( \u2016A\u0302\u2212A\u0393\u20162 + \u2016A\u0393 \u2212M\u0393\u20162 ) \u2264 O(np\u0304 log(np\u0304)2), (27)\nwhere the last inequality stems from the fact that \u2016A\u0393 \u2212 M\u0393\u2016 = \u2016X\u0393\u2016 = O( \u221a np\u0304) and \u2016A\u0302 \u2212 A\u0393\u2016 \u2264\n2sK\u0303+1(A\u0393) = O( \u221a np\u0304 log(np\u0304)) from (26). Proof of (23). Define the following sets:\nIk = {v \u2208 Vk \u2229 \u0393 : \u2016A\u0302v \u2212Mk\u0393\u20162 \u2264 1\n4\nnp\u03032\nlog(np\u0303) }\nO = {v \u2208 \u0393 : \u2016A\u0302v \u2212Mk\u0393\u20162 \u2265 4 np\u03032\nlog(np\u0303) for all 1 \u2264 k \u2264 K}.\nThese sets are designed so that\n(i) |(\u222aKk=1Ik)\u2229Qv| = 0 for all v \u2208 O\u2229VR, since \u2016A\u0302v \u2212 A\u0302w\u20162 \u2265 12\u2016A\u0302v \u2212Mk\u0393\u20162 \u2212\u2016A\u0302w \u2212Mk\u0393\u20162 > np\u03032\nlog(np\u0303) for all w \u2208 Ik;\n(ii) |\u0393 \\ (\u222aKk=1Ik)| \u2264 \u2016A\u0302\u2212M\u0393\u20162F\nmin v\u2208\u0393\\(\u222aK\nk=1 Ik)\n\u2016A\u0302v\u2212Mk\u0393\u20162 = O\n(\nlog(np\u0304)3\np\u0304\n)\n;\n(iii) Ik \u2282 Qv for all v \u2208 Ik \u2229VR, since \u2016A\u0302v \u2212 A\u0302w\u20162 \u2264 2\u2016A\u0302v \u2212Mk\u0393\u20162 +2\u2016A\u0302w \u2212Mk\u0393\u20162 \u2264 np\u0303 2\nlog(np\u0303) for all w \u2208 Ik;\n(iv) If |Qv \u2229 Ik| \u2265 1, |Qv \u2229 Ij| = 0 for all j 6= k, since \u2016Mk\u0393 \u2212M j \u0393\u2016 = \u2126(np\u03042) is much larger than\nthe radius np\u0303 2 log(np\u0303) = O( np\u03042 log(np\u0304));\nFrom the properties of Ik and O, we state the following results. \u2022 From (i) and (ii), we deduce that\n|Qv| = O ( log(np\u0304)3\np\u0304\n)\nfor all v \u2208 O \u2229 VR, (28)\nsince every w \u2208 (\u222aKk=1Ik) is outside of Qv (i.e., w \u2208 \u0393 \\ (\u222aKk=1Ik) is necessary for w \u2208 Qv);\n\u2022 since \u03b1k is a constant for all k and |\u0393\\(\u222a K k=1Ik)| |\u0393| = o(1) from (ii), with high probability,\n|Ik \u2229 VR| \u2265 1 for all 1 \u2264 k \u2264 K; (29)\n\u2022 The properties (ii), (iii), and (iv) and (29) imply that\n|Qv \\ \u222ak\u22121l=0 Sl| \u2265 mk, \u2203v \u2208 (\u222aKm=1Ik \u2229 VR) \\ (\u222ak\u22121l=0 Sl), (30)\nwhere mk is the k-th largest value among {|I1|, . . . , |IK |} ;\n\u2022 since |Ik| \u2265 |Vk \u2229 (\u0393 \\ O)| \u2265 \u03b1kn(1\u2212 o(1)) from (ii) and (iii),\n|Ik| \u2265 |Vk \u2229 (\u0393 \\ O)| \u2265 \u03b1kn(1\u2212 o(1)). (31)\nThus, we can conclude that K\u0302 = K from (30) and (31) and the property (ii); and from (28), there exists a permutation \u03b3 such that \u2016A\u0302v\u22c6 k \u2212M\u03b3(k)\u0393 \u20162 \u2264 4 np\u0303 2 log(np\u0303) for all k. Hence from (22), \u2016A\u0302v\u2212Mv,\u0393\u20162 = \u2126 ( np\u03042 ) when v is misclassified."}, {"heading": "D.3 Proof of Theorem 2", "text": "From Chernoff bound, with high probability,\n||Vk| \u2212 \u03b1kn| \u2264 \u221a n log(n) for all k. (32)\nIn what follows, we hence just prove the theorem assuming that (32) holds. Let H be the largest set of items v \u2208 V satisfying:\n(H1) e(v,V) \u2264 10\u03b7np\u0304L,\n(H2) When v \u2208 Vk, \u2211K\ni=1 \u2211L \u2113=0 e(v,Vi, \u2113) log p(k,i,\u2113) p(j,i,\u2113) \u2265 np\u0304 log(np\u0304)4 for all j 6= k.\n(H3) e(v,V \\H) \u2264 2 log(np\u0304)2. (H1) regularizes degrees, (H2) means that v \u2208 H is correctly classified when using the log-likelihood estimate, and (H3) means that v does not share too many labels with items outside H .\nThe proof of the theorem follows from the following propositions. The first provides an upper bound of |V \\ H|, and the second provides the rate at which our estimated clusters improve in each iteration when we restrict our attention to items in H .\nProposition 13 When nD(\u03b1, p)\u2212 np\u0304log(np\u0304)3 \u2265 log(n/s)+ \u221a log(n/s), |V \\H| \u2264 s with high probability.\nProposition 14 If |\u22c3Kk=1(S (0) k \\ Vk) \u2229H|+ |V \\H| = O(1/p\u0304), with high probability, the following statement holds |\u22c3Kk=1(S (t+1) k \\ Vk) \u2229H|\n|\u22c3Kk=1(S (t) k \\ Vk) \u2229H|\n\u2264 1\u221a np\u0304 for all t \u2265 0.\nFrom Proposition 14, after log(n) iterations (remember that np\u0304 = \u03c9(1), so when n is large enough 1/ \u221a np\u0304 \u2264 e\u22122), no item in H can be misclassified with high probability. Hence the number of misclassified items cannot exceed |V \\H| \u2264 s, nD(\u03b1, p) \u2212 np\u0304log(np\u0304)3 \u2265 log(n/s) + \u221a\nlog(n/s). The proof is completed by remarking that if the previous condition on D(\u03b1, p) holds, then\n1 \u2264 lim n\u2192\u221e\nnD(\u03b1, p)\u2212 np\u0304 log(np\u0304)3\nlog(n/s) + \u221a log(n/s) = lim n\u2192\u221e nD(\u03b1, p) log(n/s) ,\nwhere we used D(\u03b1, p) = \u2126(p\u0304) from condition (A2) and Lemma 8."}, {"heading": "D.3.1 Proof of Proposition 13 \u2013 Size of V \\H", "text": "We compute the number of items satisfying (H1), (H2), and (H3) in (33), (34), and Lemma 15, respectively.\nNumber of items satisfying (H1): From Lemma 10, we get:\nP{e(v,V) \u2264 10\u03b7np\u0304L} \u2265 1\u2212 exp(\u221210\u03b7np\u0304L). (33)\nNumber of items satisfying (H2): We shall prove that when v satisfies (H1), v satisfies (H2) as well with probability at least\n1\u2212 exp ( \u2212nD(\u03b1, p) + np\u0304 2 log(np\u0304)3 ) . (34)\nTo this aim, we first establish that if v satisfies\nK \u2211\ni=1\n|Vi|KL(\u00b5(v,Vi), p(k, i)) \u2264 ( 1\u2212 log(n) 2\n\u221a n\n)\nnD(\u03b1, p)\u2212 np\u0304 log(np\u0304)4 , (35)\nthen v satisfies (H2). Indeed, assume that (35) holds, then\n(i) \u2211K i=1 \u03b1inKL(\u00b5(v,Vi), p(k, i)) \u2264 ( 1 + log(n) 2 \u221a n ) \u2211K i=1 |Vi|KL(\u00b5(v,Vi), p(k, i)) < nD(\u03b1, p),\nsince ||Vi| \u2212 \u03b1in| \u2264 \u221a n log(n) and (35) holds;\n(ii) \u2211K i=1 \u03b1inKL(\u00b5(v,Vi), p(j, i)) \u2265 nD(\u03b1, p), since max {\n\u2211K i=1 \u03b1iKL(\u00b5(v,Vi), p(j, i)), \u2211K i=1 \u03b1iKL(\u00b5(v,Vi), p(k, i))\n}\n\u2265 D(\u03b1, p) and \u2211K\ni=1 \u03b1iKL(\u00b5(v,Vi), p(k, i)) < D(\u03b1, p);\n(iii) \u2211K i=1 |Vi|KL(\u00b5(v,Vi), p(j, i)) \u2265 ( 1\u2212 log(n)2\u221a n )\nnD(\u03b1, p), from ii) and the fact that ||Vi|\u2212\u03b1in| \u2264\u221a n log(n);\n(iv) from (35) and iii), for all j 6= i, K \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(k, i, \u2113)\np(j, i, \u2113) =\nK \u2211\ni=1\n|Vi| (KL(\u00b5(v,Vi), p(j, i)) \u2212KL(\u00b5(v,Vi), p(k, i)))\n\u2265 np\u0304 log(np\u0304)4 .\nHence v satisfies (H2). It remains to evaluate the probability of the event (35), which is done by applying Lemma 12 and proves (34).\nNumber of items satisfying (H3): From (33), (34), and the Markov inequality, we deduce that with probability at least 1 \u2212 exp ( \u2212 \u221a log(n/s) ) , the number of items that do not satisfy either (H1) or (H2) is\nless than s/3 when nD(\u03b1, p)\u2212 np\u0304log(np\u0304)3 \u2265 log(n/s) + \u221a log(n/s), since\nE{The number of items that do not satisfy either (H1) or (H2)} s/3\n\u2264 n exp(\u221210\u03b7np\u0304L) + n exp\n(\n\u2212nD(\u03b1, p) + np\u0304 2 log(np\u0304)3\n)\ns/3\n\u2264 n s exp\n(\n\u2212nD(\u03b1, p) + np\u0304 log(np\u0304)3\n)\n\u2264 exp ( \u2212 \u221a log(n/s) ) , (36)\nwhere we have used Lemma 9 for the last inequality. Lemma 15 allows us to complete the proof of Proposition.\nLemma 15 When the number of items that do not satisfy either (H1) or (H2) is less than s/3, |V\\H| \u2264 s, with high probability.\nProof. Let e(S, S) = \u2211\nv\u2208S e(S, S). Next we prove the following intermediate claim: there is no subset S \u2282 V such that e(S, S) \u2265 s log(np\u0304)2 and |S| = s with high probability. For any subset S \u2208 V such that |S| = s, by Markov inequality,\nP{e(S, S) \u2265 s log(np\u0304)2} \u2264 inf t\u22650\nE[exp(e(S, S)t)]\nst log(np\u0304)2\n\u2264 inf t\u22650\n\u220fs2/2 i=1 (1 + Lp\u0304 exp(t))\nst log(np\u0304)2\n\u2264 inf t\u22650 exp\n(\ns2Lp\u0304\n2 exp(t)\u2212 st log(np\u0304)2\n)\n\u2264 exp ( \u2212np\u0304s ( log np\u0304\u2212 sL 2n exp( np\u0304 log np\u0304 ) ) ) \u2264 exp (\n\u2212np\u0304s log np\u0304 2\n)\n, (37)\nwhere, in the last two inequalities, we have set t = np\u0304lognp\u0304 and used the fact that: n s \u2265 exp( np\u0304 lognp\u0304), which comes from the assumptions made in the theorem. Since the number of subsets S \u2282 V with size s is (n s ) \u2264 (ens )s, from (37), we deduce:\nE[|{S : e(S, S) \u2265 s log(np\u0304)2 and |S| = s}|] \u2264 (en s )s exp\n(\n\u2212np\u0304s log np\u0304 2\n)\n= exp\n(\n\u2212s(np\u0304 log np\u0304 2 \u2212 log en s )\n)\n\u2264 exp ( \u2212np\u0304s log np\u0304 4 ) .\nTherefore, by Markov inequality, we can conclude that there is no S \u2282 V such that e(S, S) \u2265 s log(np\u0304)2 and |S| = s with high probability.\nTo conclude the proof of the lemma, we build the following sequence of sets. Let Z1 denote the set of items that do not satisfy at least one of (H1) and (H2). Let {Z(t) \u2282 V}1\u2264t\u2264t\u22c6 be generated as follows:\n\u2022 Z(0) = Z1.\n\u2022 For t \u2265 1, Z(t) = Z(t\u2212 1) \u222a {vt} if there exists vt \u2208 V such that e(vt, Z(t\u2212 1)) > 2 log(np\u0304)2 and vt /\u2208 Z(t\u2212 1). If such an item does not exist, the sequence ends.\nThe sequence ends after the construction of Z(t\u22c6). We show that if we assume that the cardinality of items that do not satisfy (H3) is strictly larger than s/2, then one the set of the sequence {Z(t) \u2282 V}1\u2264t\u2264t\u22c6 contradicts the claim we just proved.\nAssume that the number of items do not satisfy (H3) is strictly larger than s/2, then these items will be at some point added to the sets Z(t), and by definition, each of these node contributes with more than 2 log(np\u0304)2 in e(Z(t), Z(t)). Hence if starting from Z1, we add s/2 items not satisfying (H3), we get a set Z(t) of cardinality less than s/3+ s/2 and such that e(Z(t), Z(t)) > s log(np\u0304)2. We can further add arbitrary items to Z(t) so that it becomes of cardinality s, and the obtained set contradicts the claim."}, {"heading": "D.3.2 Proof of Proposition 14", "text": "Recall that {S(t)j }1\u2264j\u2264K is the partition after the t-th improvement iteration. Also recall that with loss of generality, we assume that the set of misclassified items in H after the t-th step is E(t) =\n( \u222ak(S(t)k \\ Vk) )\n\u2229 H (it should be defined through an appropriate permutation \u03b3 of {1, . . . ,K} by E(t) = (\u222ak(S(t)k \\ V\u03b3(k))) \u2229 H , but we omit \u03b3). With this notational convention, we can define E(t)jk = (S (t) j \u2229 Vk) \u2229 H and E(t) = \u22c3 j,k:j 6=k E (t) jk . At each improvement step, items move to the most likely cluster (according to the log-likelihood defined in the SP algorithm). Thus, for all i,\n0 \u2264 \u2211\nj,k:j 6=k\n\u2211\nv\u2208E(t+1) jk\nK \u2211\ni=1\nL \u2211\n\u2113=0\ne(v, S (t) i , \u2113) log\np\u0302(j, i, \u2113)\np\u0302(k, i, \u2113)\n\u2264 \u2211\nj,k:j 6=k\n\u2211\nv\u2208E(t+1) jk\nK \u2211\ni=1\nL \u2211\n\u2113=0\ne(v, S (t) i , \u2113) log\np(j, i, \u2113) p(k, i, \u2113) + |E(t+1)|(np\u0304)1\u2212\u03ba log(np\u0304)3 (38)\n\u2264 \u2211\nj,k:j 6=k\n\u2211\nv\u2208E(t+1) jk\nK \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(j, i, \u2113)\np(k, i, \u2113)\n+ \u2211 w\u2208E(t+1) e(w, E(t)) log(2\u03b7) + 2|E(t+1)|(np\u0304)1\u2212\u03ba log(np\u0304)3 (39)\n\u2264\u2212 np\u0304 log(np\u0304)4 |E(t+1)|+ \u2211 w\u2208E(t+1) e(w, E(t), \u2113) log(2\u03b7) + 2|E(t+1)|(np\u0304)1\u2212\u03ba log(np\u0304)3 (40) \u2264\u2212 np\u0304 log(np\u0304)4 |E(t+1)|+ \u221a |E(t)||E(t+1)|np\u0304 log np\u0304+ 3|E(t+1)|(np\u0304)1\u2212\u03ba log(np\u0304)3. (41)\nTherefore, from the above inequalities, we conclude that\n|E(t+1)| |E(t)| \u2264 log(np\u0304)10 np\u0304 \u2264 1\u221a np\u0304 .\nNext we prove all the steps of the previous analysis. Proof of (38): From log(1 + x) \u2264 x, when p(j, i, \u2113) \u2212 |p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| > 0,\n\u2223 \u2223 \u2223 \u2223 log p\u0302(j, i, \u2113)\np(j, i, \u2113)\n\u2223 \u2223 \u2223 \u2223 \u2264 |p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| p(j, i, \u2113) \u2212 |p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| .\nThus, we just provide an upper bound of |p\u0302(j, i, \u2113)\u2212 p(j, i, \u2113)| to show (38). From the triangle inequality,\n|p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)|\n=\n\u2223 \u2223 \u2223e(S (0) i , S (0) j , \u2113)\u2212 p(j, i, \u2113)|S (0) i ||S (0) j | \u2223 \u2223 \u2223\n|S(0)i ||S (0) j |\n\u2264\n\u2223 \u2223 \u2223e(S (0) i , S (0) j , \u2113)\u2212 E[e(S (0) i , S (0) j , \u2113)] \u2223 \u2223 \u2223 + \u2223 \u2223 \u2223E[e(S (0) i , S (0) j , \u2113)]\u2212 p(j, i, \u2113)|S (0) i ||S (0) j | \u2223 \u2223 \u2223\n|S(0)i ||S (0) j |\n. (42)\nWe first find an upper bound of \u2223 \u2223 \u2223 e(S\n(0) i , S (0) j , \u2113)\u2212 E[e(S (0) i , S (0) j , \u2113)]\n\u2223 \u2223 \u2223 . Let S be the of partitions such\nthat \u2223\n\u2223\u222aKk=1Vk \\ Sk \u2223\n\u2223 \u2264 \u03be = O ( log(np\u0304)2\np\u0304\n)\nfor all {Sk}1\u2264k\u2264K \u2208 S.\nThen,\n|S| \u2264 ( n\n\u03be\n)\nK\u03be\n\u2264 ( ken\n\u03be\n)\u03be\n= exp\n(\nO\n(\nlog(np\u0304)3\np\u0304\n))\n. (43)\nFor all {Sk}1\u2264k\u2264K \u2208 S and for all \u2113 \u2265 1 and 1 \u2264 i, j \u2264 K , e(Si, Sj , \u2113) is the sum of |Si||Sj | (or |Si| 2\n2 when i = j) independent Bernoulli random variables. Since the variance of e(Si, Sj , \u2113) is always less than n2p\u0304, by Chernoff inequality (e.g., Theorem 2.1.3 in [22]), with probability at least 1\u2212 exp ( \u2212\u0398 ( log(np\u0304)4\np\u0304\n))\n,\n|e(Si, Sj , \u2113)\u2212 E[e(Si, Sj , \u2113)]| \u2264 n log(np\u0304)2 for all i, j, \u2113. (44)\nFrom (43) and (44), with high probability,\n|e(Si, Sj , \u2113)\u2212 E[e(Si, Sj , \u2113)]| \u2264 n log(np\u0304)2 for all i, j, \u2113 and {Sk}1\u2264k\u2264K \u2208 S.\nSince {S(0)k }1\u2264k\u2264K \u2208 S , from the above inequality, \u2223\n\u2223 \u2223e(S (0) i , S (0) j , \u2113)\u2212 E[e(S (0) i , S (0) j , \u2113)]\n\u2223 \u2223 \u2223 \u2264 n log(np\u0304)2 for all i, j, \u2113. (45)\nWe now devote to the remaining part of (42). Since |E(0)| = O ( log(np\u0304)2\np\u0304\n)\nfrom Theorem 6,\n\u2223 \u2223 \u2223 E[e(S\n(0) i , S (0) j , \u2113)]\u2212 |S (0) i ||S (0) j |p(i, j, \u2113)\n\u2223 \u2223 \u2223 \u2264 \u03b7|E(0)|np(i, j, \u2113) = O(n log(np\u0304)2). (46)\nFrom (42), (45) and (46), with high probability,\n|p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| = O(log(np\u0304)2/n) for all i, j, \u2113,\nwhich implies that: \u2223\n\u2223 \u2223 \u2223\nlog p\u0302(j, i, \u2113)\np(j, i, \u2113)\n\u2223 \u2223 \u2223 \u2223 \u2264 |p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| p(j, i, \u2113) \u2212 |p\u0302(j, i, \u2113) \u2212 p(j, i, \u2113)| = O ( log(np\u0304)2 np(j, i, \u2113) ) for all i, j, \u2113.\nSince e(v, S(t)i , \u2113) \u2264 e(v,V) \u2264 10\u03b7np\u0304L from (H1) and np(j, i, \u2113) \u2265 (np\u0304)\u03ba from (A3), we deduce that, for all v \u2208 \u0393 and i, j, k,\nL \u2211\n\u2113=0\ne(v, S (t) i , \u2113)\n\u2223 \u2223 \u2223 \u2223 log p\u0302(j, i, \u2113) p\u0302(k, i, \u2113) \u2212 log p(j, i, \u2113) p(k, i, \u2113) \u2223 \u2223 \u2223 \u2223 = O ( log(np\u0304)2(np\u0304)1\u2212\u03ba ) .\nProof of (39): Since log p(j,i,0)p(k,i,0) = O(p\u0304) for all i, j, k and |E(t)| = O(log(np\u0304)2/p\u0304),\nK \u2211\ni=1\nL \u2211\n\u2113=0\ne(v, S (t) i , \u2113) log\np(j, i, \u2113)\np(k, i, \u2113)\n= K \u2211\ni=1\n(\n|S(t)i | log p(j, i, 0)\np(k, i, 0) +\nL \u2211\n\u2113=1\ne(v, S (t) i , \u2113) log\np(j, i, \u2113)p(k, i, 0)\np(k, i, \u2113)p(j, i, 0)\n)\n\u2264 K \u2211\ni=1\n(\n|Vi| log p(j, i, 0)\np(k, i, 0) +\nL \u2211\n\u2113=1\ne(v, S (t) i , \u2113) log\np(j, i, \u2113)p(k, i, 0)\np(k, i, \u2113)p(j, i, 0)\n)\n+ log(np\u0304)3\n\u2264 K \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(j, i, \u2113)\np(k, i, \u2113) +\nK \u2211\ni=1\nL \u2211\n\u2113=1\ne(v,Vi \\ S(t)i , \u2113) log(2\u03b7) + log(np\u0304)3\n= K \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(j, i, \u2113)\np(k, i, \u2113) +\n( e(v, E(t)) + e(v,V \\H) ) log(2\u03b7) + log(np\u0304)3\n\u2264 K \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(j, i, \u2113)\np(k, i, \u2113) + log(2\u03b7)e(v, E(t)) + 2 log(np\u0304)3,\nwhere the last inequality stems from (H3), i.e., from e(v,V \\H) \u2264 2 log(np\u0304)2 when v \u2208 H .\nProof of (40): Since E(t+1) \u2282 H and every v \u2208 H satisfies (H2), every v \u2208 E(i+1)jk satisfies:\nK \u2211\ni=1\nL \u2211\n\u2113=0\ne(v,Vi, \u2113) log p(j, i, \u2113) p(k, i, \u2113) \u2264 \u2212 np\u0304 log(np\u0304)4 .\nProof of (41): Let \u0393\u0304 = {v : e(v,V) \u2264 10\u03b7np\u0304L} and A\u2113 \u0393\u0304 be the trimmed matrix of A\u2113 whose elements in rows and columns corresponding to w /\u2208 \u0393\u0304 are set to 0. \u0393\u0304 is the set of all items that satisfy (H1) and H \u2282 \u0393\u0304. Let X\u0393\u0304 = \u2211L \u2113=1(A \u2113 \u0393\u0304 \u2212M \u2113 \u0393\u0304 ). We have:\n\u2211\nv\u2208E(t+1) (e(v, E(t))\u2212 E[e(v, E(t))]) \u2264 1TE(t) \u00b7X\u0393\u0304 \u00b7 1E(t+1) ,\nwhere 1S is the vector whose v-th component is equal to 1 if v \u2208 S and to 0 otherwise. Since E[e(v, E(t))] \u2264 p\u0304L|E(t)| and \u2016X\u0393\u0304\u20162 \u2264 \u221a np\u0304 log np\u0304 with high probability from Lemma 11,\n\u2211\nv\u2208E(t+1) e(v, E(t)) =\n\u2211\nv\u2208E(t+1)\n( e(v, E(t))\u2212 E[e(v, E(t))] ) + p\u0304L|E(t)||E(t+1)|\n\u2264 \u20161TE(t) \u00b7X\u0393\u0304 \u00b7 1E(t+1)\u20162 + |E (t+1)| log(np\u0304) \u2264 \u20161TE(t)\u20162\u2016X\u0393\u0304\u20162\u20161E(t+1)\u20162 + |E (t+1)| log(np\u0304) \u2264 \u221a |E(t)||E(t+1)|np\u0304 log(np\u0304) + |E(t+1)| log(np\u0304)."}, {"heading": "E Proof of Theorem 3", "text": "The positive result is obtained by applying Theorem 2 to s = 12 . When lim infn\u2192\u221e nD(\u03b1,p) log(n) \u2265 1, SP algorithm find clusters exactly with high probability. Thus, it suffices to show the negative result. We prove the negative part by contradiction. Consider a maximum a posteriori (MAP) estimation with full parameter information. When we observe a labeld information A, the MAP estimates the clusters as follows:\n(S\u0302k)k=1,...,k = arg max (Sk)k=1,..,K\nP {(Sk)k=1,..,K|\u03b1, p,K,A} . (47)\nLet \u03b5MAP denote the number of misclassified nodes by the MAP estimation. From the definition of the MAP estimation, for any clustring algorithm \u03c0, we have\nP {\u03b5\u03c0 \u2265 1} \u2265 P { \u03b5MAP \u2265 1 } . (48)\nThus, in what follows, we show that when lim infn\u2192\u221e nD(\u03b1,p) log(n) < 1, the MAP estimation is failed to find the exact clusters with high probability. We start by Lemma 16 which finds a large deviation inequality for edge connections.\nLemma 16 Let x \u2208 ZK\u00d7(L+1) whose (k, \u2113+ 1) element is xk,\u2113, and such that \u2211L \u2113=0 xk,\u2113 = |Vk| for all"}, {"heading": "1 \u2264 k \u2264 K , \u2211L\u2113=1 xk,\u2113 = \u0398(np\u0304) for all k, and", "text": "K \u2211\nk=1\n|Vk|KL(\u00b5(v,Vk), p(i, k)) = nD when e(v) = x,\nwhere we denote by e(v) the K \u00d7 (L+ 1) matrix whose (k, \u2113+ 1) element is e(v,Vk, \u2113). Then,\nlog (P {e(v) = x}) \u2265 \u2212nD(1 + o(1)) when v \u2208 Vi and D = \u2126(p\u0304).\nProof. When using the convention \u2211b\n\u2113=a as 0 when a > b, we have\nlog (P {e(v) = x})\n=\nK \u2211\nk=1\n((\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog (p(i, k, 0)) +\nL \u2211\n\u2113=1\nlog\n(\np(i, k, \u2113)xk,\u2113 (|Vk| \u2212 \u2211\u2113\u22121 m=1 xk,m\nxk,\u2113\n)\n))\n\u2265 K \u2211\nk=1\n\n\n(\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog (p(i, k, 0)) +\nL \u2211\n\u2113=1\nlog\n\np(i, k, \u2113)xk,\u2113\n(\n|Vk| \u2212 \u2211L m=1 xk,m\n)xk,\u2113\nxk,\u2113!\n\n\n\n\n(a) \u2265 K \u2211\nk=1\n\n\n(\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog (p(i, k, 0)) +\nL \u2211\n\u2113=1\nlog\n\n\n\n p(i, k, \u2113)e xk,\u2113\n|Vk|\u2212 \u2211L m=1 xk,m\n\n\nxk,\u2113\n1\ne \u221a xk,\u2113\n\n\n\n\n(b) =\nK \u2211\nk=1\n\n\n(\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog (p(i, k, 0)) + L \u2211\n\u2113=1\nlog\n\n p(i, k, \u2113)e xk,\u2113\n|Vk|\u2212 \u2211L m=1 xk,m\n\n\nxk,\u2113 \n\u2212 o ( K \u2211\nk=1\nL \u2211\n\u2113=1\nxk,\u2113\n)\n(c) \u2265 K \u2211\nk=1\n(\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog\n(\np(i, k, 0)\n(\n1 +\n\u2211L \u2113=1 xk,\u2113\n|Vk| \u2212 \u2211L \u2113=1 xk,\u2113\n))\n+ K \u2211\nk=1\n\n\nL \u2211\n\u2113=1\nxk,\u2113 log\n\n\np(i, k, \u2113) xk,\u2113\n|Vk|\u2212 \u2211L m=1 xk,m\n\n\n \u2212 o ( K \u2211\nk=1\nL \u2211\n\u2113=1\nxk,\u2113\n)\n=\nK \u2211\nk=1\n(\n|Vk| \u2212 L \u2211\n\u2113=1\nxk,\u2113\n)\nlog\n(\np(i, k, 0)\n(|Vk| \u2212 \u2211L \u2113=1 xk,\u2113)/|Vk|\n)\n+\nK \u2211\nk=1\n(\nL \u2211\n\u2113=1\nxk,\u2113 log\n(\np(i, k, \u2113) xk,\u2113/|Vk|\n)\n)\n+ K \u2211\nk=1\n(\nL \u2211\n\u2113=1\nxk,\u2113 log\n(\n|Vk| \u2212 \u2211L\nm=1 xk,m |Vk|\n)) \u2212 o ( K \u2211\nk=1\nL \u2211\n\u2113=1\nxk,\u2113\n)\n(d) \u2265 \u2212 nD \u2212 o ( K \u2211\nk=1\nL \u2211\n\u2113=1\nxk,\u2113\n)\n(e) \u2265 \u2212 nD(1 + o(1)),\nwhere (a) is obtained from n! \u2264 e\u221an (\nn e )n ; (b) stems from \u2211K k=1 \u2211L \u2113=1 xk,\u2113 = \u03c9(1); to derive (c), we\nuse e \u2211L \u2113=1 xk,\u2113 \u2265 ( 1 + \u2211L \u2113=1 xk,\u2113\n|Vk|\u2212 \u2211L \u2113=1 xk,\u2113\n)|Vk|\u2212 \u2211L\n\u2113=1 xk,\u2113 since e \u2265 (1 + 1/x)x for all x > 0; to prove (d),\nwe use the definition of x and the following inequality:\nL \u2211\n\u2113=1\nxk,\u2113 log\n(\n|Vk| |Vk| \u2212 \u2211L m=1 xk,m\n)\n=\n(\n\u2211L \u2113=1 xk,\u2113\n)2\n|Vk| \u2212 \u2211L \u2113=1 xk,\u2113 (1 + o(1)) = o(\nL \u2211\n\u2113=1\nxk,\u2113);\nand (e) is obtained from the definition of x that \u2211L\n\u2113=1 xk,\u2113 = \u0398(np\u0304) for all k.\nAssume that there exists a constant \u03b7 > 0 such that nD(\u03b1,p)log(n) < 1\u2212 \u03b7. Let (i\u22c6, j\u22c6) = argmini,j:i<j DL+(p(i), p(j)) (i.e., it is the hardest case to discriminate cluster i\u22c6 and cluster j\u22c6). When n \u2192 \u221e, one can easily check using the continuity of the KL divergence that there\nexists x\u22c6 such that when e(v) = x\u22c6,\n\u03b7 2 log n+\nK \u2211\nk=1\n|Vk|KL(\u00b5(v,Vk), p(j\u22c6, k)) < K \u2211\nk=1\n|Vk|KL(\u00b5(v,Vk), p(i\u22c6, k)) and (49)\nK \u2211\nk=1\n|Vk|KL(\u00b5(v,Vk), p(i\u22c6, k)) \u2264 (1\u2212 \u03b7/2) log(n). (50)\nLet Ve = {v \u2208 Vi\u22c6 : e(v) = x\u22c6}. From (50) and Lemma 16, E[|Ve|] \u2265 n\u03b7/4. Thus, from Markov inequality, with probability at least 1\u2212 n\u2212\u03b7/4, Ve is not empty (i.e., |Ve| \u2265 1).\nLet v\u22c6 \u2208 Ve be a node in Ve. We denote by \u03a6 the original partition and define a slightly modified partition \u03a8 as follows:\nV\u0302i\u22c6 = Vi\u22c6 \\ {i\u22c6}, V\u0302j\u22c6 = Vj\u22c6 \u222a {i\u22c6}, and V\u0302k = Vk otherwise.\nThen, \u03a8 is a more likely partition than \u03a6 from (49), i.e.,\nP {\u03a6|\u03b1, p,K,A} \u2265 P {\u03a8|\u03b1, p,K,A} (51)\nwhich means that the MAP estimator does not select the exact partition when Ve is not empty. Therefore, from (48), every clustering algorithm \u03c0 has the error probability that\nE {\u03b5\u03c0 \u2265 1} \u2265 1\u2212 n\u2212\u03b7/4\nwhen there exists a constant \u03b7 > 0 such that nD(\u03b1,p)log(n) < 1\u2212 \u03b7."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label l appears with probability p(i, j, l) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters.", "creator": "LaTeX with hyperref package"}}}