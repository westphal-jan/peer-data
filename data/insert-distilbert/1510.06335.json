{"id": "1510.06335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2015", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems", "abstract": "crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by possibly unreliable workers. in addition, several aspects of the design of efficient crowdsourcing processes, such as potentially defining worker's bonuses, fair trade prices and time limits of securing the tasks, involve the knowledge of the actual duration of a specific task. in this work, recently we introduce a new time { sensitive bayesian aggregation method that simultaneously estimates a task'public s duration and obtains reliable aggregations of crowdsourced judgments. our method builds on the key insight that the time taken by a worker to perform a task thus is an important indicator of the likely quality of exactly the process produced judgment. to capture this, further our model uses latent variables to represent the uncertainty about the chosen workers'completion time, the tasks'duration and the workers'accuracy. to relate uniquely the quality of of a reported judgment to reference the time a worker spends on a task, our model assumes that each desired task is completed within a common latent time assurance window within which generally all workers with a propensity to valid labelling are expected to submit their judgments. in contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or altogether slower than the time required by normal workers. specifically, we do use efficient message - passing bayesian inference to learn approximate posterior temporal probabilities of ( i ) the confusion outcome matrix of each worker, ( study ii ) the propensity to make valid labelling of each worker, ( iii ) the unbiased duration required of each task and ( study iv ) proving the true label of ordering each task. using two real - world public datasets for entity linking tasks, moreover we show that our same method produces significantly up to \u00d7 15 % more accurate classifications and reflects up to \u2248 100 % thereby more informative estimates of a task's optimal duration compared negatively to state { of { the { art methods.", "histories": [["v1", "Wed, 21 Oct 2015 16:42:55 GMT  (855kb,D)", "https://arxiv.org/abs/1510.06335v1", null], ["v2", "Mon, 18 Apr 2016 21:09:58 GMT  (2418kb,D)", "http://arxiv.org/abs/1510.06335v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["matteo venanzi", "john guiver", "pushmeet kohli", "nick jennings"], "accepted": false, "id": "1510.06335"}, "pdf": {"name": "1510.06335.pdf", "metadata": {"source": "CRF", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems", "authors": ["Matteo Venanzi", "John Guiver", "Pushmeet Kohli", "Nicholas R. Jennings"], "emails": ["mavena@microsoft.com", "joguiver@microsoft.com", "pkohli@microsoft.com", "n.jennings@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Crowdsourcing has emerged as an effective way to acquire large amounts of data that enables the development of a variety of applications driven by machine learning, human computation and participatory sensing systems (Kamar, Hacker, & Horvitz, 2012; Bernstein, Little, Miller, Hartmann, Ackerman, Karger, Crowell, & Panovich, 2010; Zilli, Parson, Merrett, & Rogers, 2014). Services such as Amazon Mechanical Turk1 (AMT), oDesk2 and Crowd-\n1. www.mturk.com 2. www.odesk.com\nc\u00a92016 AI Access Foundation. All rights reserved.\nar X\niv :1\n51 0.\n06 33\n5v 2\n[ cs\n.A I]\n1 8\nA pr\nFlower3 have enabled a number of applications to hire pools of human workers to provide data to serve for training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan, 2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al., 2012) and information retrieval systems (Alonso, Rose, & Stewart, 2008). In such applications, a central problem is to deal with the diversity of accuracy and speed that workers exhibit when performing crowdsourcing tasks. As a result, due to the uncertainty over the reliability of individual crowd responses, many systems collect many judgments from different workers to achieve high confidence in the quality of their labels. However, this can incur a high cost either in time or money, particularly when the workers are paid per judgment, or when a delay in the completion of the entire crowdsourcing project is introduced when workers intentionally delay their submissions to follow their own work schedule. For example, in a typical crowdsourcing scenario, a requester must specify the number of requested assignments (i.e., individual responses from different workers), as well as the time limit for the completion of each assignment. He must also set the price to be paid for each response5, which usually includes a participation fee and a bonus based on the quality of the submission and the actual effort required by the task. However, it is a non\u2013trivial problem to set a time limit that gives the workers sufficient time to perform the task correctly without leading to task starvation (i.e., no one working on the task after being assigned). Generally speaking, the knowledge of the actual duration of each assignment (task instance) is useful to the requesters for various reasons. First, a task\u2019s duration can be used as a proxy to estimate its difficulty, as more difficult tasks usually take longer to complete (Faradani, Hartmann, & Ipeirotis, 2011). Second, this information is useful to set the time limit of a task and to reduce the overall time of task completion. Third. a task requestor can use the task duration to pay fair bonuses to workers based on the difficulty of the task they complete. When seeking to estimate this information, however, it is important to consider that some workers might not perform a task immediately and they might delay their submissions after accepting the task or, at the other extreme, they might submit a poor annotation in rapid time (Kazai, 2011). As a result, common heuristic estimates of a task\u2019s duration (such as the workers\u2019 average or median completion time) that do not account for such aspects are likely to be inaccurate.\nGiven the above, there are a number of challenges to be addressed in the various steps of designing efficient crowdsourcing workflows. First, after all the judgments have been collected, the uncertainty about the unknown reliability of individual workers must be taken into account to compute the final labels. Such aggregated labels are often estimated in settings where the true answer of each task is never revealed, as this is the very quantity that the crowdsourcing process is trying to discover (Kamar et al., 2012). Second, when estimating a task\u2019s duration, the uncertainty over the completion time deriving from the private work schedule of a worker must be taken into account (?). Third, these two challenges must be addressed simultaneously due to the interdependencies between the workers\u2019 reliability, the time required to complete each task, and the final labels estimated for such tasks.\n3. www.crowdflower.com 4. www.galaxyzoo.org 5. A common guideline for task requesters is to consider $0.10 per minute to be the minimum wage for\nethical crowdsourcing experiments (www.wearedynamo.org).\nIn an attempt to address these challenges, there has been growing interest in developing algorithms and techniques to compute accurate labels while minimising the set of, possibly unreliable, crowd judgements (Sheng, Provost, & Ipeirotis, 2008). In more detail, simple solutions typically use heuristic methods such as majority voting or weighted majority voting (Tran-Thanh, Venanzi, Rogers, & Jennings, 2013). However, these methods do not consider the reliability of different workers and they treat all judgments as equally reliable. More sophisticated methods such as the one\u2013coin model (Karger, Oh, & Shah, 2011), GLAD (Whitehill et al., 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979) and the Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilistic models that do take reliabilities into account, nor the potential labelling biases of the workers, e.g., the tendency for a worker to consistently over or underrate items. In particular DS represents the worker\u2019s skills based on a confusion matrix expressing the reliability of a worker for each possible class of objects. BCC works similarly to DS, but it also considers the uncertainty over the confusion matrices and aggregated labels using a principled Bayesian learning framework. This representational power has enabled BCC to be successfully applied to a number of crowdsourcing applications including galaxy classification (Simpson, Roberts, Psorakis, & Smith, 2013), disaster response (Ramchurn, Huynh, Ikuno, Flann, Wu, Moreau, Jennings, Fischer, Jiang, Rodden, et al., 2015) and sentiment analysis (Simpson, Venanzi, Reece, Kohli, Guiver, Roberts, & Jennings, 2015). More recently, (Venanzi, Guiver, Kazai, Kohli, & Shokouhi, 2014) proposed a community\u2013based extension of BCC (i.e., CBCC) to improve predictions by leveraging groups of workers with similar confusion matrices. Similarly, (Simpson et al., 2015) combined BCC with language modelling techniques for automated text sentiment analysis using crowd judgments. This degree of applicability and performance of BCC-based methods are a promising point of departure for developing new data aggregation methods for crowdsourcing systems. However, none of the existing methods can reason about the workers\u2019 completion time to learn the duration of a task outsourced to the crowd. Moreover, all these methods can only learn their probabilistic models from the information contained in the judgment set. Unfortunately, this strategy is challenged by datasets that can be arbitrarily sparse, i.e., the workers only provide judgments for a small sub-set of tasks, and therefore the judgments only provide weak evidence of the accuracy of a worker. In such contexts, it is our hypothesis that a wider set of features must be leveraged to learn more reliable crowdsourcing models. In this work, we focus on the time it takes to a worker to complete a task considered as a key indicator of the quality of his work. Importantly, the information about the workers\u2019 completion time is made available by all the most popular crowdsourcing platforms including AMT, the Microsoft Universal Human Relevance System (UHRS) and CrowdFlower. Therefore, we seek to efficiently combine these features into a data aggregation algorithm that can be naturally integrated with the output data produced by these platforms. In more detail, we present a novel time\u2013sensitive data aggregation method that simultaneously estimates the tasks\u2019 duration and obtains reliable aggregations of crowdsourced judgments. The characteristic of time\u2013sensitivity of our method relates to its ability to jointly reason about the worker\u2019s completion time together with the judgments in the data aggregation process. In detail, our method is an extension of BCC, which we term BCCTime. Specifically, it incorporates a newly developed time model that enables the method to leverage observations of the time spent by a worker on a task to best inform the inference of the final labels. As in BCC,\nwe use confusion matrices to represent the labelling accuracy of individual workers. To model the granularity in the workers\u2019 time profiles, we use latent variables to represent the propensity of each worker to submit valid judgments. Further, to model the uncertainty of the duration of each task, we use latent thresholds to define the time interval within which the task is expected to be completed by all the workers with high propensity to valid labelling. Then, using Bayesian message-passing inference, our method simultaneously infers the posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the true label of each task and (iv) the upper and lower bound of the duration of each task. In particular, the latter represents a reliable estimate of the likely duration of a task obtained through automatically filtering out all the contributions of the workers with a low propensity to valid labelling. We demonstrate the efficacy of our method using two commonly\u2013used public datasets that relate to an important Natural Language Processing (NLP) application of crowdsourcing entity linking tasks. In these datasets, our method achieves up to 11% more accurate classifications compared to seven state-of-the-art methods. Further, we show that our tasks\u2019 duration estimates are up to 100% more informative than the common heuristics that do not consider the workers\u2019 completion time as correlated to the quality of their judgments.\nAgainst this background, we make the following contributions to the state of the art.\n\u2022 Through an analysis on two real-world datasets for crowdsourcing entity-linking tasks, we show the existence of different types of task\u2013specific quality\u2013time trends, e.g., increasing, decreasing or invariant trends, between the quality of the judgments and the time spent by the workers to produce them. We also re-confirm existing results showing that the workers who submit judgments too quickly or too slowly over the entire task set typically provide lower quality judgments.\n\u2022 We develop BCCTime: The first time-sensitive Bayesian aggregation model that leverages observations of a worker\u2019s completion time to simultaneously aggregate crowd judgments and infer the duration of each task as well as the reliability of each worker.\n\u2022 We show that BCCTime outperforms seven of the most competitive state\u2013of\u2013the\u2013art data aggregation methods for crowdsourcing, including BCC, CBCC, one coin and majority voting, by providing up to 11% more accurate classifications and up to 100% more informative estimates of the task\u2019s duration.\nThe rest of the paper unfolds as follows. Section 2 describes our notation and the preliminaries of the Bayesian aggregation of crowd judgments. Section 3 details our time analysis of real-world datasets. Then, Section 4 formally introduces BCCTime and details its probabilistic inference. Section 5 presents its evaluation against the state of the art. Section 6 summarises the rest of the related work in the areas of data aggregation and time analysis of crowd generated content and Section 7 concludes."}, {"heading": "2. Preliminaries", "text": "Consider a crowd of K workers labelling N objects into C possible classes \u2013 all our symbols are listed in Table 1. Assume that k submits a judgment c (k) i \u2208 {1, . . . , C} for classifying\nan object i. Let ti be the unobserved true label of i. Then, suppose that \u03c4 (k) i \u2208 R+ is the time taken by k to produce c (k) i . Let J = {c (k) i |\u2200i = 1, . . . , N,\u2200k = 1, . . . ,K} and T = {\u03c4 (k)i |\u2200i = 1, . . . , N,\u2200k = 1, . . . ,K} be the set containing all the judgments and the time spent by the workers, respectively.\nWe now introduce the key features of the BCC model that are relevant to our method. First introduced by (Kim & Ghahramani, 2012), BCC is a method that combines multiple judgments produced by independent classifiers (i.e., crowd workers) with unknown accuracy. Specifically, this model assumes that, for each task i, ti is drawn from a categorical distribution with parameters p:\nti|p \u223c Cat(ti|p) (1)\nwhere p denotes the class proportions for all the objects. Then, a worker\u2019s accuracy is represented through a confusion matrix \u03c0(k) comprising the labelling probabilities of k for each possible true label value. Specifically, each row of the matrix \u03c0 (k) c = {\u03c0(k)c,1 , . . . , \u03c0 (k) c,C} is the vector where \u03c0 (k) c,j is the probability of k producing the judgment j for an object of class c. Importantly, this confusion matrix expresses both the accuracy (diagonal values) and the\nbiases (off-diagonal values) of a worker. It can recognise workers who are particularly accurate (inaccurate) or have a bias for a specific class of objects. In fact, accurate (inaccurate) workers are represented through high (low) probabilities on the diagonal of the confusion matrix, whilst workers with a bias towards a particular class will have high probabilities on the corresponding column of the matrix. For example, in the galaxy zoo domain in which the workers classify images of celestial galaxies, the confusion matrices can detect workers who have low accuracy in classifying spiral galaxies or those who systematically classify every object as elliptical galaxies (Simpson et al., 2013).\nTo relate the worker\u2019s confusion matrix to the quality of a judgment, BCC assumes that c (k) i is drawn from a categorical distribution with parameters corresponding to the ti-th row of \u03c0(k):\nc (k) i |\u03c0 (k), ti \u223c Cat ( c (k) i |\u03c0 (k) ti ) (2)\nThis is equivalent to having a categorical mixture model over c (k) i with ti as the mixture parameter and \u03c0c as the parameter of the c-th categorical component. Then, assuming that the judgments are independent and identically distributed (i.i.d.), the joint likelihood can be expressed as:\np(C, t|\u03c0,p) = N\u220f i=1 Cat(ti|p) K\u220f k=1 Cat ( c (k) i |\u03c0 (k) ti ) Using conjugate Dirichlet prior distributions for the parameters p and \u03c0 and applying Bayes\u2019 rule, the joint posterior distribution can be derived as:\np(\u03c0,p|C, t) \u221dDir(p|p0) N\u220f i=1 { Cat(ti|p)\nK\u220f k=1 Cat ( c (k) i |\u03c0 (k) ti ) Dir(\u03c0 (k) ti |\u03c0(k)ti,0) } (3)\nFrom this expression, it is possible to derive the predictive posterior distributions of each unobserved (latent) variable using standard integration rules for Bayesian inference (Bishop, 2006). Unfortunately, the exact derivation of these posterior distributions is intractable for BCC due to the non-conjugate form of the model (Kim & Ghahramani, 2012). However, it has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim & Ghahramani, 2012), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi et al., 2014). Building on this, several extensions of BCC have been proposed for various crowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013). In particular, CBCC applies community\u2013based techniques to represent groups of workers with similar confusion matrices in the classifier combination process (Venanzi et al., 2014). This mechanism enables the model to transfer learning of a worker\u2019s reliability through the communities and so improve the quality of the inference.\nHowever, a drawback of all these BCC based models is that they do not learn the task\u2019s duration nor do they consider any extra features other than the worker\u2019s judgments. As a\nresult, they perform the full learning of the confusion matrices and task labels using only the judgments produced by the workers. But, as mentioned earlier, this strategy is challenged by sparse datasets where each worker only labels a few tasks. This is the case, for instance, in the Crowdflower dataset used in the 2013 CrowdScale Shared Task challenge6 where the sentiment of 98,980 tweets was classified by 1,960 workers over five sentiment classes. In this dataset, 30% of the workers judged only 15 tweets, i.e., 0.015% of the total samples, and there is a long tail of workers with less than 3 judgments."}, {"heading": "3. Analysis of Workers\u2019 Time Spent on Judgments", "text": "Having discussed the basic concepts of non-time based data aggregation, we now turn to the analysis of the relationship between the time that workers spend on the task and the quality of the judgments they produce. In contrast to previous works in this area (Demartini, Difallah, & Cudre\u0301-Mauroux, 2012; Wang, Faridani, & Ipeirotis, 2011), we extend our analysis of quality\u2013time responses to both specific task instances, as well as for the entire task set. By so doing, we provide key insights to inform the design of our time\u2013sensitive aggregation model. To this end, we consider two public datasets generated from a widely used NLP application of crowdsourcing entity linking tasks."}, {"heading": "3.1 The Datasets", "text": "ZenCrowd - India (ZC-IN): contains a set of links between the names of entities extracted from news articles and uniform resource identifiers (URIs) describing the entity in Freebase7 and DBpedia8 (Demartini et al., 2012). The dataset was collected using AMT, with each worker being asked to classify whether a single URI was either irrelevant (0) or relevant (1) to a single entity. It contains the timestamps of the acceptance and the submission of each judgment. Moreover, gold standard labels were collected from expert editors for all the tasks. No information was released regarding the restrictions on the worker pool, although all workers are known to be living in India, and each worker was paid $0.01 per judgment. A total of 11,205 judgements were collected from a small pool of 25 workers, giving this dataset a moderately high number of judgements per worker, as detailed in Table 2. In particular, Figure 1a shows that the vast majority of tasks receive 5 judgements, while Figure 1c shows a skewed distribution of gold labels, in which 78% of links between entities and URIs were classified by workers as irrelevant (0). As such, it is worth noting that any binary classifiers with a bias towards unrelated classification will correctly classify the majority of tasks and thus receive a high accuracy. Therefore, as we will detail in Section 5, it is important to select accuracy metrics that evaluate the classifier across the whole spectrum of possible discriminant thresholds.\nZenCrowd - USA (ZC-US): This dataset was also provided by Demartini et al. (2012) and contains judgements for the same set of tasks as ZC-IN, although the judgements were collected from AMT workers in the US. The same payment of $0.01 per judgement was used. However, a larger pool of 74 workers was involved, and as such a lower number of\n6. www.crowdscale.org/shared-task 7. www.freebase.com 8. www.dbpedia.org\njudgements were collected from each worker, as shown in Table 2. Furthermore, Figure 1b shows a similar distribution of judgements per task as the India dataset, although slightly fewer tasks received 5 judgements, with most of the remaining tasks receiving 3-4 judgements or 9-11 judgements. The judgement accuracy of the US dataset is higher than the India dataset despite an identical crowdsourcing system and reward mechanism being used.\nWeather Sentiment - AMT (WS-AMT): The Weather Sentiment dataset was provided by CrowdFlower for the 2013 Crowdsourcing at Scale shared task challenge.9 It includes 300 tweets with 1,720 judgements from 461 workers and has been used in several\n9. https://www.kaggle.com/c/crowdflower-weather-twitter\nexperimental evaluations of crowdsourcing models (Simpson et al., 2015; Venanzi et al., 2014; ?). In detail, the workers were asked to classify the sentiment of tweets with respect to the weather into the following categories: negative (0), neutral (1), positive (2), tweet not related to weather (3) and can\u2019t tell (4). As a result, this dataset pertains to a multi-class classification problem. However, the original dataset used in the Share task challenge did not contain any time information about the collected judgments. Therefore, a new dataset (WS-AMT), was recollected for the same tasks as in the CrowdFlower shared task dataset using the AMT platform, acquiring exactly 20 judgements and recording the elapsed time for each judgment (?). As a result, WS-AMT contains 6,000 judgements from 110 workers, as shown in Table 2. No restrictions were placed on the worker pool and each worker was paid $0.03 per judgement. Furthermore, Figure 1d shows that, as per the original dataset, the most common gold label is unrelated, while only five tasks were assigned the gold label can\u2019t tell."}, {"heading": "3.2 Time Spent on Task versus Judgment Accuracy", "text": "We wish to analyse the distribution of the workers\u2019 completion time and the judgments\u2019 accuracy. To do so, we focus on the two datasets, ZC-US and ZC-IN with binary labels. In fact, the binary nature of these two datasets allow us to analyse accuracy at a higher level of\ndetail, i.e., in terms of precision and recall of the workers\u2019 judgments and the time spent to produce them. Specifically, Figure 2 shows the cumulative distribution of the precision and the recall of the set of judgments selected by a specific time threshold (x-axis) with respect to the gold standard labels. Here, the precision is the fraction of true positive classifications over all the returned positive classifications (true positives + false positives) and the recall is the number of true positive classifications divided by the number of all the positive samples. Similarly to Demartini et al. (2012), we find that the accuracy is lower at the extremes of the time distributions. In ZC-US, both the precision and recall are higher for the sub-set of judgments that were produced in more than 80 seconds and less than 1500 seconds. In\nZC-IN, the precision and recall are higher for judgments produced in more than 80 seconds and less than 600 seconds.\nIn addition, Figure 3 shows the distribution of the recall and execution time for a sample set of six positive task instances (i.e., entities with positive gold standard labels) with at least ten judgments. For example, Figure 3b shows the time distribution of the judgments for the URI: freebase.com/united states associated to the entity \u201cAmerican\u201d. In these graphs, some samples have an increasing quality-time curve, i.e., workers spending more time produce better judgments, (Figure 3a and Figure 3b). Other samples have a decreasing quality-time curve, i.e., workers spending more time produce worse judgments (Figure 3c and Figure 3d). Finally, the last two samples have an approximately constant quality-time curve, i.e., worker\u2019s quality is invariant to the time spent (Figure 3e and Figure 3f). It can also be seen that these trends naturally correlate to the difficulty of each task instance. For instance, URI: freebase.com/m/03hkhgs linked to the entity \u201cSouthern Avenue\u201d is more difficult to judge than the URI: dbpedia.org/page/Switzerland linked to the entity \u201cSwitzerland\u201d. In fact, \u201cSouthern Avenue\u201d is more ambiguous as an entity name, which may lead the worker to open the URI and check its content to be able to issue a correct judgment. Instead, the relevance for the second entity \u201cSwitzerland\u201d can be judged more easily through visual inspection of the URI. In addition, each task has a specific time interval that includes the sub-set of judgments with the highest precision. For example, in ZC-IN, the judgments with the highest precision for the URI: dbpedia.org/page/Switzerland and the entity \u201cSwitzerland\u201d were submitted between 5 sec. and 20 sec. (Figure 2d). Instead, in ZC-US, the best judgments for the URI: dbpedia.org/page/European linked to the entity \u201cEuropean\u201d were submitted in the interval of 2 sec. and 16 sec. (Figure 2c). As a result, it is clear that each task instance has specific quality\u2013time profile that relates to the difficulty of labelling that instance.\nTo better analyse these trends, Figure 4 shows the Pearson\u2019s correlation coefficient (\u03c1) (i.e., a standard measure of the degree of linear correlation between two variables) for all the 13 entities with positive links and more than ten judgments across the two datasets. The time spent by the worker is not always (linearly) correlated to the quality of the judgment across all the task instances. Some tasks have a significantly positive correlation (i.e., task index = 6, 8, 13 with \u03c1 > 0.7, p < 0.05), others have a significantly negative correlation (i.e., task index = 9, 12 with \u03c1 < 0.7, p < 0.05), whilst the other tasks have a less significant correlation between the accuracy of their judgments and the time spent by the workers. This confirms that different task instances have substantially different quality-time responses based on the difficulty of each sample. Thus, this insight significantly extends the previous findings reported by Demartini et al. (2013) in which such a quality\u2013time trend was only observed across the entire task set. Moreover, it empirically supports the theory of several existing data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009; Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these task\u2013specific features to achieve more accurate classifications in a number of crowdsourcing applications concerning, among others, galaxy classification (Kamar et al., 2015), image labelling (Whitehill et al., 2009) and problem solving (Bachrach et al., 2012)."}, {"heading": "4. The BCCTime Model", "text": "Based on the above results of the time analysis of workers\u2019 judgments, we observed that different types of quality\u2013time trends occur for specific task instances. However, the standard BCC, as well as all the other existing aggregation models that do not consider this information, are unable to perform inference over the likely duration of a task. To rectify this, there is a need to extend BCC to be able to include these trends in the aggregation of crowd judgments. To this end, the model must be flexible enough to identify workers who, in addition to having imperfect skills, may also not have the intention to make a valid attempt to complete a task. This further increases the uncertainty about data reliability. In this section, we describe our Bayesian Classifier Combination model with Time (BCCTime). In particular, we describe the three components of the model concerning (i) the representation of the unknown workers\u2019 propensity to valid labelling, (ii) the reliability of workers\u2019 judgments and (iii) the uncertainty in the worker\u2019s completion time, followed by the details of its probabilistic inference."}, {"heading": "4.1 Modelling Workers\u2019 Propensity To Valid Labelling", "text": "Given the uncertainty about the intention of a worker to submit valid judgments, we introduce the latent variable \u03c8k \u2208 [0, 1] representing the propensity of k towards making a valid labelling attempt for any given task. In this way, the model is able to naturally explain the unreliability of a worker based not only on her imperfect skills but also on her attitude towards approaching a task correctly. In particular, \u03c8k close to one means that the worker has a tendency to exert her best effort to provide valid judgments, even though her judgments might be still noisy as a consequence of the imperfect skills she possesses. In contrast, \u03c8k close to zero means that the worker tends to not provide valid judgments for her tasks, which means that she behaves similarly to a spammer. Specifically, only the workers with high propensity to valid labelling will provide inputs that are meaningful to\nthe task\u2019s true label and the task\u2019s duration. To capture this, we define a per-judgment boolean variable v (k) i \u2208 {0, 1} with v (k) i = 1 meaning that k has made a valid labelling attempt when submitting c (k) i and v (k) i = 0 meaning that c (k) i is an invalid annotation. In this setting, the number of valid labelling attempts made by the worker derives from her propensity to valid labelling. Thus, we can model this by assuming that v (k) i is a random draw from a Bernoulli distribution parametrised by \u03c8k :\nv (k) i \u223c Bernoulli(\u03c8k) (4)\nThat is, workers with high propensity to valid labelling are more likely to make more valid labelling attempts, whilst workers with low propensity are more likely to submit more spam annotations."}, {"heading": "4.2 Modelling Workers\u2019 Judgments", "text": "Here we describe the part of the model concerned with the generative process of crowd judgments from the confusion matrix and the propensity of the workers. Intuitively, only those judgments associated with valid labelling attempts should be considered to estimate the final labels. This means that each judgment may be generated from two different processes depending on whether or not it comes from a valid labelling attempt. To capture this in the generative model of BCCTime, a mixture model is used to switch between these two cases conditioned on v (k) i . For the first case of a valid labelling attempt, i.e., v (k) i = 1, the judgment is generated through the worker\u2019s confusion matrix as per the standard BCC model. Therefore, we assume that c (k) i is generated for the same model described for BCC (Eq. 2), including v (k) i in the conditional variables. Formally:\nc (k) i |\u03c0 (k), ti, v (k) i = 1 \u223c Cat\n( c\n(k) i |\u03c0 (k) ti\n) (5)\nFor the second case of a judgment produced from an invalid labelling attempt, i.e., v (k) i = 0, it is natural to assume that the judgment does not contribute to the estimation of the true label. Formally, this assumption can be represented through general random vote model in which c (k) i is drawn from a categorical distribution with a vector parameter s:\nc (k) i |s, v (k) i = 0 \u223c Cat\n( c\n(k) i |s\n) (6)\nHere s is the vector of the labelling probabilities of a general worker with low propensity to make valid labelling attempts. Notice that the equation above does not depend on ti, which means that all the judgments coming from invalid labelling attempts are treated as noisy responses uncorrelated to ti."}, {"heading": "4.3 Modelling Workers\u2019 Completion Time", "text": "As shown in Section 3, the duration of a task may be defined as the interval in which workers are more likely to submit high-quality judgments. However, due to the dependency of the duration on the task\u2019s characteristics, the requirement is that such an interval must be non-constant across all the tasks. To model this, we define a lower-bound threshold, \u03c3i,\nand an upper-bound threshold, \u03bbi, for the time interval representing the duration of i. Both these per\u2013task thresholds are latent variables that must be learnt at training time. Then, the tasks with a lower or higher variability in their duration can be represented based on the values of their time thresholds. In this setting, all the valid labelling attempts made by the workers are expected to be completed within the task\u2019s duration interval detailed by these thresholds. Formally, we represent the probability of \u03c4 (k) i being greater than \u03c3i using the standard greaterThan probabilistic factor introduced by (Herbrich, Minka, & Graepel, 2007) for the TrueSkill Bayesian ranking model:\nI(\u03c4 (k)i > \u03c3i|v (k) i = 1) (7)\nThis factor defines a non-conjugate relationship over \u03c3i such that the posterior distribution of \u03c4 (k) i is not in the same form as the prior distribution of \u03c3i. Therefore the posterior distribution p(\u03c4 (k) i ) needs to be approximated. We do this via moment matching with a Gaussian distribution p\u0302(\u03c4 (k) i ) by matching the precision and the precision adjusted mean (i.e., the mean multiplied by the precision) to the posterior distribution of p(\u03c4 (k) i ), as shown in Table 1 in Herbrich et al. (2007). In a similar way, we model the probability of \u03c4 (k) i being greater than \u03bbi as:\nI(\u03bbi > \u03c4 (k) i |v (k) i = 1) (8)\nDrawing all this together, upon observing a set of i.i.d. pairs of judgments and workers\u2019 completion times contained in J and T respectively, we can express the joint likelihood of BCCTime as:\np(J ,T , t|\u03c0,p, s,\u03c8) = N\u220f i=1 Cat(ti|p) { K\u220f k=1 ( I(\u03c4 (k)i > \u03c3i)I(\u03bbi > \u03c4 (k) i )Cat ( c (k) i |\u03c0 (k) ti ))\u03c8k Cat ( c\n(k) i |s )(1\u2212\u03c8k)} (9) The factor graph of BCCTime is illustrated in Figure 5. Specifically, the two shaded variables c (k) i and \u03c4 (k) i are the observed inputs, while all the unobserved random variables are unshaded. The graph uses the gate notation (dashed box) introduced by (Minka & Winn, 2008) to represent the two mixture models of BCCTime. Specifically, the outer gate represents the workers\u2019 judgments (see Section 4.2) and completion times (see Section 4.3) that are generated from either BCC or the random vote model using v (k) i as the gating variable. The inner gate is the mixture model generating the workers\u2019 judgments from the rows of the confusion matrix using ti as the gating variable."}, {"heading": "4.4 Probabilistic Inference", "text": "To perform Bayesian inference over all the unknown quantities, we must provide prior distributions for the latent parameters of BCCTime. Following the structure of the model,\nwe can select conjugate distributions for all such parameters to enable a more tractable inference of their posterior probabilities. Therefore, the prior of p is Dirichlet distributed with hyperparameter p0:\n(true label prior) p \u223c Dir(p|p0) (10)\nThe priors of s and \u03c0 (k) c are also Dirichlet distributed with hyperparameter s0 and \u03c0 (k) c,0 respectively:\n(spammer label prior) s \u223c Dir(s|s0) (11) (confusion matrix prior) \u03c0(k)c \u223c Dir(\u03c0(k)c |\u03c0 (k) c,0 ) (12)\nThen, \u03c8k has a Beta prior with true count \u03b10 and false count \u03b20:\n(worker\u2019s propensity prior) \u03c8k \u223c Beta(\u03c8k|\u03b10, \u03b20) (13)\nThe two time thresholds \u03c3i and \u03bbi have Gaussian priors with mean \u03c30 and \u03bb0 and precision \u03b30 and \u03b40 respectively:\n(lower-bound of the task\u2019s duration threshold prior) \u03c3i \u223c N (\u03c3i|\u03c30, \u03b30) (14) (upper-bound of the task\u2019s duration threshold prior) \u03bbi \u223c N (\u03bbi|\u03bb0, \u03b40) (15)\nCollecting all the hyperparameters in the set \u0398 = {p0, s0, \u03b10, \u03b20, \u03c30, \u03b30, \u03bb0, \u03b40}, we find by applying Bayes\u2019 theorem that the joint posterior distribution is proportional to:\np(\u03c0,p, s, t,\u03c8|J ,T ,\u0398) \u221d Dir(s|s0)Dir(p|p0) N\u220f i=1 { Cat(ti|p)N (\u03c3i|\u03c30, \u03b30)N (\u03bbi|\u03bb0, \u03b40)\nK\u220f k=1 ( I(\u03c4 (k)i > \u03c3i)I(\u03bbi < \u03c4 (k) i )Cat ( c (k) i |\u03c0 (k) ti ) Dir(\u03c0 (k) ti |\u03c0(k)ti,0) )\u03c8k Cat ( c\n(k) i |s )(1\u2212\u03c8k)Beta(\u03c8k|\u03b10, \u03b20)} (16) From this expression, we can compute the marginal posterior distributions of each latent variable by integrating out all the remaining variables. Unfortunately, these integrations are intractable due to the non\u2013conjugate form of our model. However, we can still compute approximations of such posterior distributions using standard techniques from the family of approximate Bayesian inference methods (Minka, 2001). In particular, we use the well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al., 2014)10. This method leverages a factorised distribution of the joint probability to approximate the marginal posterior distributions through an iterative message passing scheme implemented on the factor graph. Specifically, we use the EP implementation provided by Infer.NET (Minka, Winn, Guiver, & Knowles, 2014), which is a standard framework for running Bayesian inference in probabilistic models. Using Infer.NET, we are able to train BCCTime on our largest dataset of 12,190 judgments within seconds using approximately 80MB of RAM on a standard laptop."}, {"heading": "5. Experimental Evaluation", "text": "Having described our model, we test its performance in terms of classification accuracy and ability to learn the tasks\u2019 duration in real crowdsourcing experiments. Using the datasets described in Section 3, we conduct experiments in the following experimental setup."}, {"heading": "5.1 Benchmarks", "text": "We consider a set of benchmarks consisting of three popular baselines (Majority voting, Vote distribution and Random) and three state\u2013of\u2013the\u2013art aggregation methods (One coin, BCC and CBCC) that are commonly employed in crowdsourcing applications. In more detail:\n\u2022 One coin: This method represents the accuracy of a worker with a single reliability parameter (or worker\u2019s coin) assuming that the worker will return the correct answer with probability specified by the coin, and the incorrect answer with inverse probability. As a result, this method is only applicable to binary datasets. Crucially, this model represents the core mechanism of several existing methods including (Whitehill\n10. Alternative inference methods such as Gibbs sampling or Variational Bayes can be trivially applied to our model in the Infer.NET framework.\net al., 2009; Demartini et al., 2012; Liu, Peng, & Ihler, 2012; Karger et al., 2011; Li, Zhao, & Fuxman, 2014)11.\n\u2022 BCC: This is the closest benchmark to our method that was described in Section 2. It learns the confusion matrices and the aggregated labels without considering the worker\u2019s completion time as an input feature. It has been used in several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation (Kim & Ghahramani, 2012) and disaster response (Ramchurn et al., 2015).\n\u2022 BCCPropensity: This is equivalent to BCCTime where only the workers\u2019 propensity is learnt. This benchmark is used to assess the contribution of inferring only the worker\u2019s propensity, versus their joint learning with the tasks\u2019 time thresholds, to the quality of the final labels. Note that BCCPropensity is easy to obtain from BCCTime by setting the time thresholds to static observations with \u03c3 = 0.0 and \u03bb = max.value.\n\u2022 CBCC: An extension of BCC that learns the communities of workers with similar confusion matrices as described in Section 2. Given a judgment set, CBCC is able to learn the confusion matrix of each community and each worker, as well as the task label. This method has also been used in a number of crowdsourcing applications including web search evaluation and sentiment analysis (Venanzi et al., 2014). In our experiments, we ran CBCC with the number of worker types set to two communities in order to infer the two groups of more reliable workers and less reliable workers \u2013 similar results were observed for higher number of communities.\n\u2022 Majority Voting: This is a simple yet very popular algorithm that estimates the aggregated label as the one that receives the most votes (Littlestone & Warmuth, 1989; Tran-Thanh et al., 2013). It assigns a point mass to the label with the highest consensus among a set of judgments. Thus, the algorithm does not represent its uncertainty around a classification and it considers all judgments as coming from reliable workers.\n\u2022 Vote Distribution: This method estimates the true label based on the empirical probabilities of each class observed in the judgment set (Simpson et al., 2015). Specifically, it assigns the probability of a label as the fraction of judgments corresponding to that label.\n\u2022 Random: This is a baseline method that assigns random class labels to all the tasks, i.e., it assigns uniform probabilities to all the labels.\nNote that the alternative variant of BCCTime that captures only the time spent is redundant. In fact, when the workers\u2019 propensity is not modelled together with the time spent, the workers\u2019 accuracy is only captured by their confusion matrices. This means that the model is equivalent to BCC, which is already included in the benchmarks. All these benchmarks were also implemented in Infer.NET and trained using the EP algorithm. In our\n11. In particular, we refer to One coin as the unconstrained version of ZenCrowd (Demartini et al., 2012) without the two unicity and SameAs constraints defined in the original method. This suggests that this version is more suitable for a fair comparison with the other methods.\nexperiments, we set the hyperparameters of BCCTime to reproduce the typical situation in which the task requester has no prior knowledge of the true labels and the labelling probabilities of the workers, and only a basic prior knowledge about the accuracy of workers representing that, a priori, they are assumed to be better than random annotators (Kim & Ghahramani, 2012). Therefore, the workers\u2019 confusion matrices are initialised with a slightly higher value on the diagonal (0.6) and lower values on the rest of the matrix. Then, the Dirichlet priors for p and s are set uninformatively with uniform counts12. The priors of the confusion matrices were initialised with a higher diagonal value (0.7) meaning that a priori the workers are assumed to be better than random. The Gaussian priors for the tasks\u2019 time durations are set with means \u03c30 = 10 and \u03bb0 = 50 and precisions \u03b30 = \u03b40 = 10\n\u22121, meaning that a priori each entity linking task is expected to be completed within 10 and 50 seconds. Furthermore, we initialise the Beta prior of \u03c8k as a function of the number of tasks with \u03b10 = 0.7N and \u03b20 = 0.3N to represent the fact that a priori the worker is considered as a reliable if she makes valid labelling attempts for 70% of the tasks. Importantly, given the shape distribution of the worker\u2019s time completion data observed in the datasets (see Figure 2), we apply a logarithmic transformation to \u03c4 (k) i in order to obtain a more uniform distribution of workers\u2019 completion time in the training data. Finally, the priors of all the benchmarks were set equivalently to BCCTime."}, {"heading": "5.2 Accuracy Metrics", "text": "We evaluate the classification accuracy of the tested methods as measured by the Area Under the ROC Curve (AUC) for ZC-US and ZC-IN and the average recall for WS-AMT. In particular, the former is a standard accuracy metric to evaluate the performance of binary classifiers over a range of discriminant thresholds applied to their predictive class probabilities (Hanley & McNeil, 1982), which is well suited for the two ZenCrowd binary datasets. The latter is the recall averaged over the class categories (?), which is the main metric used to score the probabilistic methods that competed in the 2013 CrowdFlower shared task challenge on a dataset equivalent to WS-AMT (see Section 3.1)."}, {"heading": "5.3 Results", "text": "Table 3 reports the AUC of the seven algorithms on the ZenCrowd datasets. Specifically, it shows that BCCTime and BCCPropensity have the highest accuracy in both the datasets: Their AUC is 11% higher in ZC-IN and 8% higher in ZC-US, respectively, compared to the other methods. Among the two, BCCTime is the best method with an improvement of 13% in ZC-IN and 1% in ZC-US. Similarly, Table 4 reports the average recall of the methods in WS-AMT showing that BCCTime has the highest average recall, which is 2% higher than the second best benchmark (Vote distribution) and 4% higher than BCCPropensity. This means that the inference of the time thresholds, which already provides valuable information about the tasks extracted from the judgments, also adds an extra quality improvement to aggregated labels in addition to the modelling of the workers\u2019 propensities. This is an important observation because it proves that the information of workers\u2019 completion time\n12. It should be noted, however, that in cases where a different type of knowledge is available about the workers, this information can be plugged into our method by selecting the appropriate prior distributions.\ncan be effectively for data aggregation. Altogether, this information allows the model to correctly filter unreliable judgments and consequently provide more accurate classifications.\nFigure 6 shows the ROC curve of the methods for the ZenCrowd (binary) datasets, namely the plot of the false positive rate and the true positive rate obtained for different discriminant thresholds. The graph shows that the true positive rate of BCCTime is generally higher than that of the benchmarks at the same false positive rate. In detail, Majority vote, and Vote distribution perform worse than Random in these datasets as these methods are clearly penalised by the presence of less reliable workers as they treat all the workers as equally reliable. Interestingly, One coin performs better than BCC and CBCC meaning that the confusion matrix is better approximated by a single (one coin) parameter for these two datasets. Also, looking at the percentages of the workers\u2019 propensities inferred by BCCTime reported in Table 5, we found that 93.2% of the workers in ZC-US, 60% of the workers in ZC-IN and 97.3% of the workers in WS-AMT have a propensity greater than 0.5. This means that, in ZC-US and WS-AMT, only a few workers were identified as suspected spammers while the majority of them were estimated as more reliable with different propensity values. In ZC-IN, the percentage of suspected spammers is higher and this is also reflected in the lower accuracy of the judgments with respect to the gold standard labels.\nFigure 7 shows the mean value of the inferred upper-bound time threshold \u03bbi (blue cross points) and the workers\u2019 maximum completion time (green asterisked points) for each task of the three datasets. Looking at the raw data in the ZenCrowd datasets, the average\nmaximum time spent by the US workers is higher (approx. 1.7 minutes) than that of the Indian workers (approx. 1 minute). It can also be seen that in both datasets there is a significant portion of outliers that reach up to 50 minutes. However, as discussed in Section 3, we know that many of these entity linking tasks are fairly simple \u2013 some of them can easily be solved through visual inspection of the candidate URI. This does not imply that a normal worker who completes the task in a single session (i.e., no interrupts) should take such a long time to issue her judgment. Interestingly, BCCTime efficiently removes these outliers and recovers more realistic estimates of the maximum duration of an entity linking task. In fact, its estimated upper-bound time thresholds lie within a smaller time band, i.e., around 10 seconds in ZC-US and 40 seconds in ZC-IN. Similar results are also observed in WS-AMT where the average observed maximum time is significantly higher than the average inferred maximum time, thus suggesting that the BCCTime estimates are also more realistic in this dataset. In addition, Figure 7 shows the same plot for the average duration as estimated by BCCTime (i.e, (E[\u03bbi]\u2212 E[\u03b1i])/2 \u2200i) and the average worker\u2019s completion time for each task. The graphs show that the BCCTime estimates are similar between the micro-tasks of the three datasets, i.e., between 3 and 5, while the same estimates obtained from the worker\u2019s completion time data are much higher: 53 seconds for ZC-US, 45 seconds for ZC-IN and 80 seconds in WS-AMT. Again, this is due to the presence of outliers in the original data that significantly bias the empirical average times towards high values. Moreover, measuring the variability in the two sets of estimates, the BCCTime estimates have a much smaller standard deviation that is up to 100% lower than that of the empirical averages. This means that our estimates are more informative when compared to the normal average times obtained from the raw workers\u2019 completion time data.\nTo evaluate the performance of the methods against data sparsity, Figure 8 shows the accuracy measured over sub-samples of judgments in each dataset. In more detail, one coin\nis more accurate over sparse judgments in ZC-IN and ZC-US, while in WS-AMT there is no clear winner since all the methods except Random have a similar average recall when trained on sparse judgments. This shows that BCCTime in the current form does not necessarily outperform the other methods with sparse data. This can be explained by the fact that the extra latent variables (i.e., workers\u2019 propensity and time thresholds) used to improve the quality of the final labels also require a larger set of judgments to be accurately learnt. However, to address this issue, it is possible to draw from community-based models (e.g., CBCC) to design a hierarchical extension for BCCTime over, for example, the workers\u2019 confusion matrices and so improve its robustness on sparse data. Here, for simplicity, BCCTime is presented based on simpler instance of Bayesian classifier combination framework (i.e., the BCC model), and its community-based version is considered as a trivial extension."}, {"heading": "6. Related Work", "text": "Here we review the rest of previous work relating to aggregation models and time analysis in crowdsourcing contexts extending the background of the methods already considered in\nour experimental evaluation. In recent years, a large body of literature has focussed on the development of smart data aggregation methods to aid requesters in combining judgments from multiple workers. In general, existing methods vary by assumptions and complexity in modelling the different aspects of labelling noise. The interested reader may refer to the survey by Sheshadri and Lease (2013), as well as to the summary in Table 6 that lists the most popular methods and their comparison with our approach.\nIn particular, some of these methods are able to handle both binary classification problems, i.e., when workers have to vote on objects between two possible classes, and multi-class classification problems, i.e., when workers have to vote on objects between more than two classes. Among these, many approaches use the one coin model introduced in our benchmarks. In more detail, this model represents the worker\u2019s reliability with a single parameter defined within the range of [0, 1] (0 = unreliable worker, 1 = reliable worker) (Karger et al., 2011; Liu et al., 2012; Demartini et al., 2012; Li et al., 2014; ?). Specifically, (Karger et al., 2011) combines this model with a budget\u2013limited task allocation framework and provides strong theoretical guarantees on the asymptotical optimality of the inference of the workers\u2019 reliability and the worker-task matching. (Liu et al., 2012) uses a more general variational inference model that reduces to Karger et al.\u2019s method, as well as other algorithms under special conditions. Other methods use a two coin model that represents the bias of a worker towards the positive labelling class (specificity) and towards the negative class (sensitivity) (Raykar, Yu, Zhao, Valadez, Florin, Bogoni, & Moy, 2010; Rodrigues, Pereira, & Ribeiro, 2014; Bragg & Weld, 2013). Then, these quantities may be inferred using logistic regression as in (Raykar et al., 2010) or maximum\u2013a\u2013posteriori approaches as in (Bragg & Weld, 2013). Alternatively, (Rodrigues et al., 2014) uses the two coin model embedded in a Gaussian process classification framework to compute the predictive probabilities of the aggregated labels and the workers\u2019 reliability using EP. Along the same lines, other models reason about the difficulty of a task that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012). In this area, (Whitehill et al., 2009) use a logistic regression model to incorporate the task\u2019s difficulty, together with the expertise of the worker for labelling images. In contrast, (Bachrach et al., 2012) use the difference between these two quantities to quantify the advantage that the worker may have in classifying the object within a joint difficulty-ability-response model. In a similar setting, (Kajino & Kashima, 2012) exploit a convex problem formulation of this model to improve the efficiency of inferring these quantities through a numerical optimisation method. Additional factors, such as the worker\u2019s motivation or propensity for a particular task, are taken into account in more sophisticated models introduced by (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez, Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014). More recently, (?) devised a method that leverage the fact that the error rates of the workers are directly affected by the access path they follow, where the access path represents several contextual features of the task (e.g., task design, information sources and task composition). However, unlike our work, none of these methods learn the confusion matrix of each worker. As a result, they do not represent reliability considering the accuracy and the potential biases of a worker with a single data structure.\nAlternative models that do learn the confusion matrices of the workers have been presented, among others, by (Dawid & Skene, 1979; Zhou, Basu, Mao, & Platt, 2012; Kim &\nGhahramani, 2012; Venanzi et al., 2014). In particular, (Dawid & Skene, 1979) introduced the first confusion matrix-based model in which the confusion matrices are inferred using expectation-maximisation in an unsupervised manner. Then, (Zhou et al., 2012) extended this work to include a task\u2013specific latent matrix representing the confusability of a task as perceived by the workers. However, neither of these methods consider the uncertainty over the worker\u2019s reliability and the other parameters of their models. For example, when only one label is obtained from a worker, these methods may infer that the worker is perfectly reliable or totally incompetent when, in reality, the worker is neither. To overcome this limitation, other methods such as BCC and CBCC capture the uncertainty in the worker\u2019s expertise and the true labels using a Bayesian learning framework. These two methods were extensively discussed earlier (see Sections 2 and 5) and are included as benchmarks in our experiments. Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010). However, as already noted, all these methods do not use any extra information other than the workers\u2019 judgments to learn their probabilistic models. As a result, unlike our approach, they cannot take full advantage of the time information provided by the crowdsourcing platform to improve the quality of their inference results.\nNow we turn to the problem of time analysis in crowd generated content. recently introduced a metric for measuring the effort required to complete a crowdsourced task based on the area under the error-time curve (ETA). As such, this metric supports the idea of considering time as an important factor a crowdsourcing effort. In this regard, a closely related work on the analysis of the ZenCrowd datasets (see Section 3) was presented by (Difallah, Demartini, & Cudre\u0301-Mauroux, 2012). Their work showed that workers who complete their tasks too fast or too slow are typically less accurate than the others. These findings were also confirmed in our work. However, in addition, we extended their analysis by showing the judgment\u2019s quality is correlated to the time spent by the workers in different ways for specific task instances. This is the intuition that our method exploits to efficiently combine the workers\u2019 completion time features in the data aggregation process. Furthermore, earlier work introducing a method that predicts the duration of the task based on a number of available features (including the task\u2019s price, the creation time and the number of assignments) using a survival analysis model was presented by (Wang et al., 2011). However, their method does not deal with aggregating labels, nor learning the accuracy of the workers, as we do in our approach."}, {"heading": "7. Conclusions", "text": "We presented and evaluated BCCTime, a new time\u2013sensitive aggregation method that simultaneously merges crowd labels and estimates the duration of individual task instances using principled Bayesian inference. The key innovation of our method is to leverage an extended set of features comprising the workers\u2019 completion time and the judgment set. When appropriately correlated together, these features become important indicators of the reliability of a worker that, in turn, allow us to estimate the final labels, the tasks\u2019 duration and the workers\u2019 reliability more accurately. Specifically, we introduced a new representation of the accuracy profile of a worker consisting of both the worker\u2019s confusion matrix, which\nT ab\nle 6 :\nC o m\np a ri\nso n\nof 21\nex is\nti n\ng m\net h\no d\ns fo\nr co\nm p\nu ti\nn g\nag gr\neg at\ned la\nb el\ns fr\nom cr\now d\nso u\nrc ed\nju d gm\nen ts\n.\nb in\nar y\nm u\nlt i\nw or\nke r\nw or\nke r\nta sk\nta sk\nw or\nk er\ncl as\ns cl\nas s\nac cu\nra cy\nco n\nfu si\non m\nat ri\nx d\niffi cu\nlt y\nd u\nra ti\non ty\np e\nor p\nro p\nen si\nty\nM a jo\nri ty\nvo ti\nn g\nX X\n- -\n- -\n-\nD S\n- D\naw id\n& S\nk en\ne (1\n97 9 )\nX X\nX X\n- -\n-\nG L\nA D\n- W\nh it\neh il\nl et\nal .\n(2 0 09\n) X\n- X\n- X\n- -\nR Y\n- R\nay ka\nr et\nal .\n(2 01\n0 )\nX -\nX -\n- -\n-\nC U\nB A\nM -\nW el\nin d\ner et\nal .\n(2 01\n0 )\nX -\nX -\n- - X Y U - Y a n et a l. (2 0 1 0) X X X - - -\n-\nL D\nA -\nW a n g\net al\n. (2\n0 1 1)\n- -\n- -\n- X\n-\nK J\n- K\na ji\nn o\net al\n. (2\n01 2 )\nX -\nX -\n- - X Z en C ro w d - D em ar ti n i et al . (2 01 2 ) X - X - - -\n-\nD A\nR E\n- B\na ch\nra ch\net al\n. (2\n01 2 )\nX X\nX -\nX -\n-\nM in\nM a x E\nn tr\nop y\n- Z\nh o u\net al\n. (2\n0 12\n) X\nX X\nX -\n- -\nB C\nC -\nK im\n& G\nh a h\nra m\na n\ni (2\n0 12\n) X\nX X\nX -\n- -\nM S\nS -\nQ i\net a l.\n(2 01\n3 )\nX X\nX -\n- - X M L N B - B ra gg et a l. (2 01 3 ) X X X - - -\n-\nB M\n- B\ni et\na l.\n(2 01\n4 )\nX -\nX -\nX -\n-\nG P\n- R\no d\nri gu\nez et\na l.\n(2 0 14\n) X\n- X\n- -\n- -\nL U\n- L\niu et\na l.\n(2 01\n4 )\nX -\nX -\n- -\n-\nW M\n-L i\net a l.\n(2 0 14\n) X\nX X\n- -\n- X\nC B\nC C\n- V\nen an\nzi et\nal .\n(2 01\n4 )\nX X\nX X\n- - X A P M - N u sh i et a l. (2 01 5 ) X X X - - -\n-\nB C\nC T\nim e\n- P\nro po\nse d\nm et\nh od\nX X\nX X\nX X\nX\naccounts for the worker\u2019s labelling probabilities in each class, and the worker\u2019s propensity to valid labelling, which represents the worker\u2019s intention to meaningfully participate in the labelling process. Furthermore, we used latent variables to represent the duration of each task using pairs of latent thresholds to capture the time interval in which the best judgments for that task are likely to be submitted by honest workers. In this way, the model can deal with the differences in the time length of each task instance relating to the different type of correlation between quality of the received judgments and the time spent by the workers. In fact, such task\u2013specific correlations have been observed in our experimental analysis of crowdsourced datasets in which various task instances showed different types of quality\u2013time trends. Thus, the main idea behind BCCTime is to model these trends in the aggregation of crowd judgments to make more reliable inference about all the quantities of interest. Through an extensive experimental validation on real-world datasets, we showed that BCCTime produces significantly more accurate classifications and its estimates of the tasks\u2019 duration are considerably more informative than common heuristics obtained from the raw workers\u2019 completion time data.\nAgainst this background, there are several implications of this work concerning various aspects of reliable crowdsourcing systems. Firstly, the process of designing the task can take exploit the unbiased task\u2019s duration estimated by BCCTime. As we have shown, this information is a valid proxy to assess the difficulty of a task and therefore supports a number of decision\u2013making problems such as fair pricing for more difficult tasks and defining fair bonuses to honest workers. Secondly, the worker\u2019s propensity to valid labelling uncovers an additional dimension of the workers\u2019 reliability that enables us to score their attitude towards correctly approaching a given task. This information is useful to select different task designs or more engaging tasks for workers who systematically approach a task incorrectly. Thirdly, our method uses only features that are readily available in common crowdsourcing systems, which allows for a faster take up of this technology in real applications.\nBuilding on these advances, there are several aspects of our current model that indicate promising directions for further improvements. For example, we can consider that time\u2013 dependencies in the accuracy profile of a worker capture the fact that workers typically improve their skills over time by performing a sequence of tasks. By so doing, it is possible to take advantage of these temporal dynamics to potentially improve the quality of the final labels. In addition, some crowdsourcing settings involve continuous-valued judgments that are currently not supported by our method. To deal with these cases, a number of non\u2013 trivial extensions to our generative model and, in turn, a new treatment of its probabilistic inference are required."}], "references": [{"title": "Crowdsourcing for relevance evaluation", "author": ["O. Alonso", "D.E. Rose", "B. Stewart"], "venue": "In ACM SigIR Forum,", "citeRegEx": "Alonso et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2008}, {"title": "How to grade a test without knowing the answers\u2014a Bayesian graphical model for adaptive crowdsourcing and aptitude testing", "author": ["Y. Bachrach", "T. Graepel", "T. Minka", "J. Guiver"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Bachrach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bachrach et al\\.", "year": 2012}, {"title": "Soylent: a word processor with a crowd inside", "author": ["M. Bernstein", "G. Little", "R. Miller", "B. Hartmann", "M. Ackerman", "D. Karger", "D. Crowell", "K. Panovich"], "venue": "In Proceedings of the 23nd annual ACM symposium on User interface software and technology,", "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Learning to predict from crowdsourced data", "author": ["W. Bi", "L. Wang", "J.T. Kwok", "Z. Tu"], "venue": "In Proceedings of the 30th International Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Bi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2014}, {"title": "Pattern recognition and machine learning, Vol", "author": ["C. Bishop"], "venue": "4. Springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Crowdsourcing multi-label classification for taxonomy creation", "author": ["J. Bragg", "D.S. Weld"], "venue": "In First AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Bragg and Weld,? \\Q2013\\E", "shortCiteRegEx": "Bragg and Weld", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A. Dawid", "A. Skene"], "venue": null, "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "Zencrowd: Leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking", "author": ["G. Demartini", "D.E. Difallah", "P. Cudr\u00e9-Mauroux"], "venue": "In Proceedings of the 21st international conference on World Wide Web (WWW),", "citeRegEx": "Demartini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Demartini et al\\.", "year": 2012}, {"title": "Mechanical cheat: Spamming schemes and adversarial techniques on crowdsourcing platforms", "author": ["D.E. Difallah", "G. Demartini", "P. Cudr\u00e9-Mauroux"], "venue": "In CrowdSearch,", "citeRegEx": "Difallah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Difallah et al\\.", "year": 2012}, {"title": "What\u2019s the right price? pricing tasks for finishing on time", "author": ["S. Faradani", "B. Hartmann", "P.G. Ipeirotis"], "venue": "In Human Computation, Vol. WS-11-11 of AAAI Workshops,", "citeRegEx": "Faradani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Faradani et al\\.", "year": 2011}, {"title": "The meaning and use of the area under a receiver operating characteristic (roc", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": "curve.. Radiology,", "citeRegEx": "Hanley and McNeil,? \\Q1982\\E", "shortCiteRegEx": "Hanley and McNeil", "year": 1982}, {"title": "Trueskill(tm): A Bayesian skill rating system", "author": ["R. Herbrich", "T. Minka", "T. Graepel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Herbrich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2007}, {"title": "Convex formulations of learning from crowds", "author": ["H. Kajino", "H. Kashima"], "venue": "Transactions of the Japanese Society for Artificial Intelligence,", "citeRegEx": "Kajino and Kashima,? \\Q2012\\E", "shortCiteRegEx": "Kajino and Kashima", "year": 2012}, {"title": "Combining human and machine intelligence in large-scale crowdsourcing", "author": ["E. Kamar", "S. Hacker", "E. Horvitz"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Kamar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2012}, {"title": "Identifying and accounting for task-dependent bias in crowdsourcing", "author": ["E. Kamar", "A. Kapoor", "E. Horvitz"], "venue": "In Third AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Kamar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2015}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D. Karger", "S. Oh", "D. Shah"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Karger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "In search of quality in crowdsourcing for search engine evaluation", "author": ["G. Kazai"], "venue": "Advances in information retrieval, pp. 165\u2013176. Springer.", "citeRegEx": "Kazai,? 2011", "shortCiteRegEx": "Kazai", "year": 2011}, {"title": "Bayesian classifier combination", "author": ["H. Kim", "Z. Ghahramani"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kim and Ghahramani,? \\Q2012\\E", "shortCiteRegEx": "Kim and Ghahramani", "year": 2012}, {"title": "The wisdom of minority: discovering and targeting the right group of workers for crowdsourcing", "author": ["H. Li", "B. Zhao", "A. Fuxman"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "In 30th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Littlestone and Warmuth,? \\Q1989\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1989}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T. Minka"], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 362\u2013369.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Hac-er: a disaster response system based on human-agent collectives", "author": ["S.D. Ramchurn", "T.D. Huynh", "Y. Ikuno", "J. Flann", "F. Wu", "L. Moreau", "N.R. Jennings", "J.E. Fischer", "W. Jiang", "T Rodden"], "venue": "In 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Ramchurn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramchurn et al\\.", "year": 2015}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "Gaussian process classification and active learning with multiple annotators", "author": ["F. Rodrigues", "F. Pereira", "B. Ribeiro"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Rodrigues et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rodrigues et al\\.", "year": 2014}, {"title": "Get another label? Improving data quality and data mining using multiple, noisy labelers", "author": ["V. Sheng", "F. Provost", "P. Ipeirotis"], "venue": "In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Sheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sheng et al\\.", "year": 2008}, {"title": "Square: A benchmark for research on computing crowd consensus", "author": ["A. Sheshadri", "M. Lease"], "venue": "In Proceedings of the 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP),", "citeRegEx": "Sheshadri and Lease,? \\Q2013\\E", "shortCiteRegEx": "Sheshadri and Lease", "year": 2013}, {"title": "Dynamic bayesian combination of multiple imperfect classifiers", "author": ["E. Simpson", "S. Roberts", "I. Psorakis", "A. Smith"], "venue": "In Decision Making and Imperfection,", "citeRegEx": "Simpson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simpson et al\\.", "year": 2013}, {"title": "Language understanding in the wild: Combining crowdsourcing and machine learning", "author": ["E. Simpson", "M. Venanzi", "S. Reece", "P. Kohli", "J. Guiver", "S. Roberts", "N.R. Jennings"], "venue": "In 24th International World Wide Web Conference (WWW),", "citeRegEx": "Simpson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simpson et al\\.", "year": 2015}, {"title": "Combined Decision Making with Multiple Agents", "author": ["E. Simpson"], "venue": "Ph.D. thesis, University of Oxford.", "citeRegEx": "Simpson,? 2014", "shortCiteRegEx": "Simpson", "year": 2014}, {"title": "Efficient Budget Allocation with Accuracy Guarantees for Crowdsourcing Classification Tasks", "author": ["L. Tran-Thanh", "M. Venanzi", "A. Rogers", "N.R. Jennings"], "venue": "In The 12th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2013}, {"title": "Community-based bayesian aggregation models for crowdsourcing", "author": ["M. Venanzi", "J. Guiver", "G. Kazai", "P. Kohli", "M. Shokouhi"], "venue": "In 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Venanzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venanzi et al\\.", "year": 2014}, {"title": "Estimating the completion time of crowdsourced tasks using survival analysis models", "author": ["J. Wang", "S. Faridani", "P. Ipeirotis"], "venue": "In Crowdsourcing for Search and Data Mining (CSDM),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J.R. Movellan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Whitehill et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Whitehill et al\\.", "year": 2009}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan", "R. Rosales", "G. Fung", "M. Schmidt", "G.H. Valadez", "L. Bogoni", "L. Moy", "J. Dy"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Yan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2010}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["D. Zhou", "S. Basu", "Y. Mao", "J. Platt"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "A hidden markov model-based acoustic cicada detector for crowdsourced smartphone biodiversity monitoring", "author": ["D. Zilli", "O. Parson", "G.V. Merrett", "A. Rogers"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zilli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zilli et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Flower3 have enabled a number of applications to hire pools of human workers to provide data to serve for training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan, 2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al., 2012) and information retrieval systems (Alonso, Rose, & Stewart, 2008).", "startOffset": 252, "endOffset": 272}, {"referenceID": 16, "context": "When seeking to estimate this information, however, it is important to consider that some workers might not perform a task immediately and they might delay their submissions after accepting the task or, at the other extreme, they might submit a poor annotation in rapid time (Kazai, 2011).", "startOffset": 275, "endOffset": 288}, {"referenceID": 13, "context": "Such aggregated labels are often estimated in settings where the true answer of each task is never revealed, as this is the very quantity that the crowdsourcing process is trying to discover (Kamar et al., 2012).", "startOffset": 191, "endOffset": 211}, {"referenceID": 35, "context": "More sophisticated methods such as the one\u2013coin model (Karger, Oh, & Shah, 2011), GLAD (Whitehill et al., 2009), CUBAM (Welinder et al.", "startOffset": 87, "endOffset": 111}, {"referenceID": 34, "context": ", 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979) and the Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilistic models that do take reliabilities into account, nor the potential labelling biases of the workers, e.", "startOffset": 15, "endOffset": 38}, {"referenceID": 29, "context": "Similarly, (Simpson et al., 2015) combined BCC with language modelling techniques for automated text sentiment analysis using crowd judgments.", "startOffset": 11, "endOffset": 33}, {"referenceID": 28, "context": "For example, in the galaxy zoo domain in which the workers classify images of celestial galaxies, the confusion matrices can detect workers who have low accuracy in classifying spiral galaxies or those who systematically classify every object as elliptical galaxies (Simpson et al., 2013).", "startOffset": 266, "endOffset": 288}, {"referenceID": 4, "context": "From this expression, it is possible to derive the predictive posterior distributions of each unobserved (latent) variable using standard integration rules for Bayesian inference (Bishop, 2006).", "startOffset": 179, "endOffset": 193}, {"referenceID": 30, "context": "However, it has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim & Ghahramani, 2012), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi et al.", "startOffset": 229, "endOffset": 244}, {"referenceID": 32, "context": "However, it has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim & Ghahramani, 2012), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi et al., 2014).", "startOffset": 273, "endOffset": 295}, {"referenceID": 32, "context": "Building on this, several extensions of BCC have been proposed for various crowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013).", "startOffset": 97, "endOffset": 147}, {"referenceID": 32, "context": "In particular, CBCC applies community\u2013based techniques to represent groups of workers with similar confusion matrices in the classifier combination process (Venanzi et al., 2014).", "startOffset": 156, "endOffset": 178}, {"referenceID": 7, "context": "1 The Datasets ZenCrowd - India (ZC-IN): contains a set of links between the names of entities extracted from news articles and uniform resource identifiers (URIs) describing the entity in Freebase7 and DBpedia8 (Demartini et al., 2012).", "startOffset": 212, "endOffset": 236}, {"referenceID": 7, "context": "ZenCrowd - USA (ZC-US): This dataset was also provided by Demartini et al. (2012) and contains judgements for the same set of tasks as ZC-IN, although the judgements were collected from AMT workers in the US.", "startOffset": 58, "endOffset": 82}, {"referenceID": 7, "context": "Similarly to Demartini et al. (2012), we find that the accuracy is lower at the extremes of the time distributions.", "startOffset": 13, "endOffset": 37}, {"referenceID": 35, "context": "Moreover, it empirically supports the theory of several existing data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009; Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these task\u2013specific features to achieve more accurate classifications in a number of crowdsourcing applications concerning, among others, galaxy classification (Kamar et al.", "startOffset": 89, "endOffset": 187}, {"referenceID": 14, "context": ", 2009; Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these task\u2013specific features to achieve more accurate classifications in a number of crowdsourcing applications concerning, among others, galaxy classification (Kamar et al., 2015), image labelling (Whitehill et al.", "startOffset": 227, "endOffset": 247}, {"referenceID": 35, "context": ", 2015), image labelling (Whitehill et al., 2009) and problem solving (Bachrach et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 1, "context": ", 2009) and problem solving (Bachrach et al., 2012).", "startOffset": 28, "endOffset": 51}, {"referenceID": 6, "context": "Thus, this insight significantly extends the previous findings reported by Demartini et al. (2013) in which such a quality\u2013time trend was only observed across the entire task set.", "startOffset": 75, "endOffset": 99}, {"referenceID": 11, "context": ", the mean multiplied by the precision) to the posterior distribution of p(\u03c4 (k) i ), as shown in Table 1 in Herbrich et al. (2007). In a similar way, we model the probability of \u03c4 (k) i being greater than \u03bbi as:", "startOffset": 109, "endOffset": 132}, {"referenceID": 21, "context": "However, we can still compute approximations of such posterior distributions using standard techniques from the family of approximate Bayesian inference methods (Minka, 2001).", "startOffset": 161, "endOffset": 174}, {"referenceID": 21, "context": "In particular, we use the well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al.", "startOffset": 50, "endOffset": 63}, {"referenceID": 32, "context": "In particular, we use the well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al., 2014)10.", "startOffset": 138, "endOffset": 160}, {"referenceID": 28, "context": "It has been used in several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation (Kim & Ghahramani, 2012) and disaster response (Ramchurn et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 23, "context": ", 2013), image annotation (Kim & Ghahramani, 2012) and disaster response (Ramchurn et al., 2015).", "startOffset": 73, "endOffset": 96}, {"referenceID": 32, "context": "This method has also been used in a number of crowdsourcing applications including web search evaluation and sentiment analysis (Venanzi et al., 2014).", "startOffset": 128, "endOffset": 150}, {"referenceID": 31, "context": "\u2022 Majority Voting: This is a simple yet very popular algorithm that estimates the aggregated label as the one that receives the most votes (Littlestone & Warmuth, 1989; Tran-Thanh et al., 2013).", "startOffset": 139, "endOffset": 193}, {"referenceID": 29, "context": "\u2022 Vote Distribution: This method estimates the true label based on the empirical probabilities of each class observed in the judgment set (Simpson et al., 2015).", "startOffset": 138, "endOffset": 160}, {"referenceID": 7, "context": "In particular, we refer to One coin as the unconstrained version of ZenCrowd (Demartini et al., 2012) without the two unicity and SameAs constraints defined in the original method.", "startOffset": 77, "endOffset": 101}, {"referenceID": 15, "context": "Specifically, (Karger et al., 2011) combines this model with a budget\u2013limited task allocation framework and provides strong theoretical guarantees on the asymptotical optimality of the inference of the workers\u2019 reliability and the worker-task matching.", "startOffset": 14, "endOffset": 35}, {"referenceID": 20, "context": "(Liu et al., 2012) uses a more general variational inference model that reduces to Karger et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": "Then, these quantities may be inferred using logistic regression as in (Raykar et al., 2010) or maximum\u2013a\u2013posteriori approaches as in (Bragg & Weld, 2013).", "startOffset": 71, "endOffset": 92}, {"referenceID": 25, "context": "Alternatively, (Rodrigues et al., 2014) uses the two coin model embedded in a Gaussian process classification framework to compute the predictive probabilities of the aggregated labels and the workers\u2019 reliability using EP.", "startOffset": 15, "endOffset": 39}, {"referenceID": 35, "context": "Along the same lines, other models reason about the difficulty of a task that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012).", "startOffset": 160, "endOffset": 231}, {"referenceID": 1, "context": "Along the same lines, other models reason about the difficulty of a task that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012).", "startOffset": 160, "endOffset": 231}, {"referenceID": 35, "context": "In this area, (Whitehill et al., 2009) use a logistic regression model to incorporate the task\u2019s difficulty, together with the expertise of the worker for labelling images.", "startOffset": 14, "endOffset": 38}, {"referenceID": 1, "context": "In contrast, (Bachrach et al., 2012) use the difference between these two quantities to quantify the advantage that the worker may have in classifying the object within a joint difficulty-ability-response model.", "startOffset": 13, "endOffset": 36}, {"referenceID": 34, "context": "Additional factors, such as the worker\u2019s motivation or propensity for a particular task, are taken into account in more sophisticated models introduced by (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez, Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014).", "startOffset": 155, "endOffset": 269}, {"referenceID": 20, "context": "The interested reader may refer to the survey by Sheshadri and Lease (2013), as well as to the summary in Table 6 that lists the most popular methods and their comparison with our approach.", "startOffset": 49, "endOffset": 76}, {"referenceID": 37, "context": "Then, (Zhou et al., 2012) extended this work to include a task\u2013specific latent matrix representing the confusability of a task as perceived by the workers.", "startOffset": 6, "endOffset": 25}, {"referenceID": 18, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 3, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 36, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 33, "context": "Furthermore, earlier work introducing a method that predicts the duration of the task based on a number of available features (including the task\u2019s price, the creation time and the number of assignments) using a survival analysis model was presented by (Wang et al., 2011).", "startOffset": 253, "endOffset": 272}], "year": 2016, "abstractText": "Crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by potentially unreliable workers. In addition, several aspects of the design of efficient crowdsourcing processes, such as defining worker\u2019s bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. Bringing this together, in this work we introduce a new time\u2013sensitive Bayesian aggregation method that simultaneously estimates a task\u2019s duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, builds on the key insight that the time taken by a worker to perform a task is an important indicator of the likely quality of the produced judgment. To capture this, BCCTime uses latent variables to represent the uncertainty about the workers\u2019 completion time, the tasks\u2019 duration and the workers\u2019 accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task\u2019s duration compared to state\u2013of\u2013the\u2013art methods.", "creator": "TeX"}}}