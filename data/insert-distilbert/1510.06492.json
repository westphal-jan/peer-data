{"id": "1510.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2015", "title": "Generalized Shortest Path Kernel on Graphs", "abstract": "we consider the problem of classifying graphs using graph kernels. we define a uniquely new convex graph graph kernel, called the generalized shortest path decomposition kernel, based on the number and length of shortest paths between nodes. for our example classification problem, we systematically consider the task of classifying random graphs from two well - known families, by the number of clusters they contain. if we verify empirically that the generalized shortest path kernel outperforms matches the original shortest path kernel on a number of datasets. we give a theoretical analysis for explaining our experimental results. in particular, we estimate distributions of the expected feature vectors for the shortest path kernel and the generalized shortest path kernel, and we show thus some evidence explaining why either our graph kernel outperforms the shortest path kernel for our graph efficient classification problem.", "histories": [["v1", "Thu, 22 Oct 2015 05:49:31 GMT  (205kb,D)", "http://arxiv.org/abs/1510.06492v1", "Short version presented at Discovery Science 2015 in Banff"]], "COMMENTS": "Short version presented at Discovery Science 2015 in Banff", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["linus hermansson", "fredrik d johansson", "osamu watanabe"], "accepted": false, "id": "1510.06492"}, "pdf": {"name": "1510.06492.pdf", "metadata": {"source": "CRF", "title": "Generalized Shortest Path Kernel on Graphs", "authors": ["Linus Hermansson", "Fredrik D. Johansson", "Osamu Watanabe"], "emails": ["linus3@is.titech.ac.jp", "watanabe@is.titech.ac.jp", "frejohk@chalmers.se"], "sections": [{"heading": null, "text": "Keywords: Graph Kernel \u00b7 SVM \u00b7 Machine Learning \u00b7 Shortest Path"}, {"heading": "1 Introduction", "text": "Classifying graphs into different classes depending on their structure is a problem that has been studied for a long time and that has many useful applications [1,5,13,14]. By classifying graphs researchers have been able to solve important problems such as to accurately predict the toxicity of chemical compounds [14], classify if human tissue contains cancer or not [1], predict if a particular protein is an enzyme or not [5], and many more.\nIt is generally regarded that the number of self-loop-avoiding paths between all pairs of nodes of a given graph is useful for understanding the structure of the graph [9,15]. Computing the number of such paths between all nodes is however a computationally hard task (usually #P-hard). Counting only the number of shortest paths between node pairs is however possible in polynomial time and such paths at least avoid cycles, which is why some researchers have considered shortest paths a reasonable substitute. When using standard algorithms to compute the shortest paths between node pairs in a graph we also get, as a by-product, the number of such shortest paths between all node pairs. Taking this number of shortest paths into account when analyzing the properties of a graph could provide useful information and is what our approach is built upon. ar X iv :1\n51 0.\n06 49\n2v 1\n[ cs\n.D S]\n2 2\nO ct\n2 01\n5\n2 One popular technique for classifying graphs is by using a support vector machine (SVM) classifier with graph kernels. This approach has proven successful for classifying several types of graphs [4,5,10]. Different graph kernels can however give vastly different results depending on the types of graphs that are being classified. Because of this it is useful to analyze which graph kernels that works well on which types of graphs. Such an analysis contributes to understanding when graph kernels are useful and on which types of graphs one can expect a good result using this approach. In order to classify graphs, graph kernels that consider many different properties have been proposed. Such as graph kernels considering all walks [8], shortest paths [4], small subgraphs [17], global graph properties [11], and many more. Analyzing how these graph kernels perform for particular datasets, gives us the possibility of choosing graph kernels appropriate for the particular types of graphs that we are trying to classify.\nOne particular type of graphs, that appears in many applications, are graphs with a cluster structure. Such graphs appear for instance when considering graphs representing social networks. In this paper, in order to test how well our approach works, we test its performance on the problem of classifying graphs by the number of clusters that they contain. More specifically, we consider two types of models for generating random graphs, the Erdo\u030bs-Re\u0301nyi model [3] and the planted partition model [12], where we use the Erdo\u030bs-Re\u0301nyi model to generate graphs with one cluster and the planted partition model to generate graphs with two clusters (explained in detail in Sect. 4). The example task considered in this paper is to classify whether a given random graph is generated by the Erdo\u030bs-Re\u0301nyi model or by the planted partition model.\nFor this classification problem, we use the standard SVM and compare experimentally the performance of the SVM classifier, with the shortest path (SP) kernel, and with our new generalized shortest path (GSP) kernel. In the experiments we generate datasets with 100 graphs generated according to the Erdo\u030bsRe\u0301nyi model and 100 graphs generated according to the planted partition model. Different datasets use different parameters for the two models. The task is then, for any given dataset, to classify graphs as coming from the Erdo\u030bs-Re\u0301nyi model or the planted partition model, where we consider the supervised machine learning setting with 10-fold cross validation. We show that the SVM classifier that uses our GSP kernel outperforms the SVM classifier that uses the SP kernel, on several datasets.\nNext we give some theoretical analysis of the random feature vectors of the SP kernel and the GSP kernel, for the random graph models used in our experiments. We give an approximate estimation of expected feature vectors for the SP kernel and show that the expected feature vectors are relatively close between graphs with one cluster and graphs with two clusters. We then analyze the distributions of component values of expected feature vectors for the GSP kernel, and we show some evidence that the expected feature vectors have a different structure between graphs with one cluster and graphs with two clusters.\nThe remainder of this paper is organized as follows. In Sect. 2 we introduce notions and notations that are used throughout the paper. Section 3 defines\n3 already existing and new graph kernels. In Sect. 4 we describe the random graph models that we use to generate our datasets. Section 5 contains information about our experiments and experimental results. In Sect. 6 we give an analysis explaining why our GSP kernel outperforms the SP kernel on the used datasets. Section 7 contains our conclusions and suggestions for future work."}, {"heading": "2 Preliminaries", "text": "Here we introduce necessary notions and notation for our technical discussion. Throughout this paper we use symbols G, V , E (with a subscript or a superscript) to denote graphs, sets of nodes, and sets of edges respectively. We fix n and m to denote the number of nodes and edges of considered graphs. By |S| we mean the number of elements of the set S.\nWe are interested in the length and number of shortest paths. In relation to the kernels we use for classifying graphs, we use feature vectors for expressing such information. For any graph G, for any d \u2265 1, let nd denote the number of pairs of nodes of G with a shortest path of length d (in other words, distance d nodes). Then we call a vector vsp = [n1, n2, . . .] a SPI feature vector. On the other hand, for any d, x \u2265 1, we use nd,x to denote the number of pairs of nodes of G that have x number of shortest paths of length d, and we call a vector vgsp = [n1,1, n1,2, . . . , n2,1 . . .] a GSPI feature vector. Note that nd = \u2211 x nd,x. Thus, a GSPI feature vector is a more detailed version of a SPI feature vector. In order to simplify our discussion we often use feature vectors by considering shortest paths from any fixed node of G. We will clarify which version we use in each context. By E[vsp] and E[vgsp] we mean the expected SPI feature vector and the expected GSPI feature vector, for some specified random distribution. Note that the expected feature vectors are equal to [E[nd]]d\u22651 and [E[nd,x]]d\u22651,x\u22651.\nIt should be noted that the SPI and the GSPI feature vectors are computable efficiently. For example, we can use Dijkstra\u2019s algorithm [6] for each node in a given graph, which gives all node pairs\u2019 shortest path length (i.e. a SPI feature vector) in time O(nm + n2 log n). Note that by using Dijkstra\u2019s algorithm to compute the shortest path from a fixed source node to any other node, the algorithm actually needs to compute all shortest paths between the two nodes, to verify that it really has found a shortest path. In many applications, however, we are only interested in obtaining one shortest path for each node pair, meaning that we do not store all other shortest paths for that node pair. It is however possible to store the number of shortest paths between all node pairs, without increasing the running time of the algorithm, meaning that we can compute the GSPI feature vector in the same time as the SPI feature vector. Note that for practical applications, it might be wise to use a binning scheme for the number of shortest paths, where we consider numbers of shortest paths as equal if they are close enough. For example instead of considering the numbers of shortest paths {1, 2...100}, as different. We could consider the intervals {[1, 10], [11, 20]...[91, 100]} as different and consider all the numbers inside a particular interval as equal. Doing this will reduce the dimension of the GSPI\n4 feature vector, which could be useful since the number of shortest paths in a graph might be large for dense graphs.\nWe note that the graph kernels used in this paper can be represented explicitly as inner products of finite dimensional feature vectors. We choose to still refer to them as kernels, because of their relation to other graph kernels."}, {"heading": "3 Shortest Path Kernel and Generalized Shortest Path Kernel", "text": "A graph kernel is a function k(G1, G2) on pairs of graphs, which can be represented as an inner product k(G1, G2) = \u3008\u03c6(G1), \u03c6(G2)\u3009H for some mapping \u03c6(G) to a Hilbert space H, of possibly infinite dimension. In many cases, graph kernels can be thought of as similarity functions on graphs. Graph kernels have been used as tools for using SVM classifiers for graph classification problems [4,5,10].\nThe kernel that we build upon in this paper is the shortest path (SP) kernel, which compares graphs based on the shortest path length of all pairs of nodes [4]. By D(G) we denote the multi set of shortest distances between all node pairs in the graph G. For two given graphs G1 and G2, the SP kernel is then defined as:\nKSP(G1, G2) = \u2211\nd1\u2208D(G1) \u2211 d2\u2208D(G2) k(d1, d2),\nwhere k is a positive definite kernel [4]. One of the most common kernels for k is the indicator function, as used in Borgwardt and Kriegel [4]. This kernel compares shortest distances for equality. Using this choice of k we obtain the following definition of the SP kernel:\nKSPI(G1, G2) = \u2211\nd1\u2208D(G1) \u2211 d2\u2208D(G2) 1 [d1 = d2] . (1)\nWe call this version of the SP kernel the shortest path index (SPI) kernel. It is easy to check that KSPI(G1, G2) is simply the inner product of the SPI feature vectors of G1 and G2.\nWe now introduce our new kernel, the generalized shortest path (GSP) kernel, which is defined by using also the number of shortest paths. For a given graph G, by ND(G) we denote the multi set of numbers of shortest paths between all node pairs of G. Then the GSP kernel is defined as:\nKGSP(G1, G2) = \u2211\nd1\u2208D(G1) \u2211 d2\u2208D(G2) \u2211 t1\u2208ND(G1) \u2211 t2\u2208ND(G2) k(d1, d2, t1, t2),\nwhere k is a positive definite kernel. A natural choice for k would be again a kernel where we consider node pairs as equal if they have the same shortest distance and the same number of shortest paths. Resulting in the following definition, which we call the generalized shortest path index (GSPI) kernel.\nKGSPI(G1, G2) = \u2211\nd1\u2208D(G1) \u2211 d2\u2208D(G2) \u2211 t1\u2208ND(G1) \u2211 t2\u2208ND(G2) 1 [d1 = d2]1 [t1 = t2]\n(2)\n5 It is easy to see that this is equivalent to the inner product of the GSPI feature vectors of G1 and G2."}, {"heading": "4 Random Graph Models", "text": "We investigate the advantage of our GSPI kernel over the SPI kernel for a synthetic random graph classification problem. Our target problem is to distinguish random graphs having two relatively \u201cdense parts\u201d, from simple graphs generated by the Erdo\u030bs-Re\u0301nyi model. Here by \u201cdense part\u201d we mean a subgraph that has more edges in its inside compared with its outside.\nFor any edge density parameter p, 0 < p < 1, the Erdo\u030bs-Re\u0301nyi model (with parameter p) denoted by G(n, p) is to generate a graph G (of n nodes) by putting an edge between each pair of nodes with probability p independently at random. On the other hand, for any p and q, 0 < q < p < 1, the planted partition model [12], denoted by G(n/2, n/2, p, q) is to generate a graph G = (V +\u222aV \u2212, E) (with |V +| = |V \u2212| = n/2) by putting an edge between each pair of nodes u and v again independently at random with probability p if both u and v are in V + (or in V \u2212) and with probability q if u \u2208 V + and v \u2208 V \u2212 (or, u \u2208 V \u2212 and v \u2208 V +).\nThroughout this paper, we use the symbol p1 to denote the edge density parameter for the Erdo\u030bs-Re\u0301nyi model and p2 and q2 to denote the edge density parameters for the planted partition model. We want to have q2 < p2 while keeping the expected number of edges the same for both random graph models (so that one cannot distinguish random graphs by just couting the number of edges). It is easy to check that this requirement is satisfied by setting\np2 = (1 + \u03b10)p1, and q2 = 2p1 \u2212 p2 \u2212 2(p1 \u2212 p2)/n (3)\nfor some constant \u03b10, 0 < \u03b10 < 1. We consider the \u201csparse\u201d situation for our experiments and analysis, and assume that p1 = c0/n for sufficiently large constant c0. Note that we may expect with high probability, that when c0 is large enough, a random graph generated by both models have a large connected component but might not be fully connected [3]. In the rest of the paper, a random graph generated by G(n, p1) is called a one-cluster graph and a random graph generated by G(n/2, n/2, p2, q2) is called a two-cluster graph.\nFor a random graph, the SPI/GSPI feature vectors are random vectors. For each z \u2208 {0, 1}, we use v(z)sp and v(z)gsp to denote random SPI and GSPI feature vectors of a z-cluster graph. We use n\n(z) d and n (z) d,x to denote respectively the\ndth and (d, x)th component of v (z) sp and v (z) gsp. For our experiments and analysis, we consider their expectations E[v (z) sp ] and E[v (z) gsp ], that is, [E[n (z) d ]]d\u22651 and [E[n (z) d,x]]d\u22651,x\u22651. Note that E[n (z) d,x] is the expected number of node pairs that have x number of shortest paths of length d; not to be confused with the expected number of distance d shortest paths.\n6"}, {"heading": "5 Experiments", "text": "In this section we compare the performance of the GSPI kernel with the SPI kernel on datasets where the goal is to classify if a graph is a one-cluster graph or a two-cluster graph."}, {"heading": "5.1 Generating Datasets and Experimental Setup", "text": "All datasets are generated using the models G(n, p1) and G(n/2, n/2, p2, q2), described above. We generate 100 graphs from the two different classes in each dataset. q2 is chosen in such a way that the expected number of edges is the same for both classes of graphs. Note that when p2 = p1, the two-cluster graphs actually become one-cluster graphs where all node pairs are connected with the same probability, meaning that the two classes are indistinguishable. The bigger difference there is between p1 and p2, the more different the one-cluster graphs are compared to the two-cluster graphs. In our experiments we generate graphs where n \u2208 {200, 400, 600, 800, 1000}, np1 = c0 = 40 and p2 \u2208 {1.2p1, 1.3p1, 1.4p1, 1.5p1}. Hence p1 = 0.2 for n = 200, p1 = 0.1 for n = 400 etc.\nIn all experiments we calculate the normalized feature vectors for all graphs. By normalized we mean that each feature vector vsp and vgsp is normalized by its Euclidean norm. This means that the inner product between two feature vectors always is in [0, 1]. We then train an SVM using 10-fold cross validation and evaluate the accuracy of the kernels. We use Pegasos [16] for solving the SVM."}, {"heading": "5.2 Results", "text": "Table 1 shows the accuracy of both kernels, using 10-fold cross validation, on the different datasets. As can be seen neither of the kernels perform very well on the datasets where p2 = 1.2p1. This is because the two-cluster graphs generated in this dataset are almost the same as the one-cluster graphs. As p2 increases compared to p1, the task of classifying the graphs becomes easier. As can be seen in the table the GSPI kernel outperforms the SPI kernel on nearly all datasets. In particular, on datasets where p2 = 1.4p1, the GSPI kernel has an increase in accuracy of over 20% on several datasets. When n = 200 the increase in accuracy is over 40%! Although the shown results are only for datasets where c0 = 40, experiments using other values for c0 gave similar results.\nOne reason that our GSPI kernel is able to classify graphs correctly when the SPI kernel is not, is because the feature vectors of the GSPI kernel, for the two classes, are a lot more different than for the SPI kernel. In Fig. 1 we have plotted the SPI feature vectors, for a fixed node, for both classes of graphs and one particular dataset. By feature vectors for a fixed node we mean that the feature vectors contains information for one fixed node, instead of node pairs, so that for example, nd from vsp = [n1, n2, . . .], contains the number of nodes that are at distance d from one fixed node, instead of the number of node pairs that are at distance d from each other. The feature vector displayed in Fig. 1 is the\n7\naverage feature vector, for any fixed node, and averaged over the 100 randomly generated graphs of each type in the dataset. The dataset showed in the figure is when the graphs were generated with n = 600, the one-cluster graphs used p1 = 0.06667, the two-cluster graphs used p2 = 0.08667 and q2 = 0.04673, this corresponds to, in Table 1, the dataset where n = 600, p2 = 1.3p1, this dataset had an accuracy of 60.5% for the SPI kernel and 67.0% for the GSPI kernel. As can be seen in the figure there is almost no difference at all between the average SPI feature vectors for the two different cases. In Fig. 2 we have plotted the subvectors [n (1) 2,x]x\u22651 of v (1) gsp and [n (2) 2,x]x\u22651 of v (2) gsp, for a fixed node, for the same dataset as in Fig. 1. The vectors contain the number of nodes at distance 2 from the fixed node with x number of shortest paths, for one-cluster graphs and two-cluster graphs respectively. The vectors have been averaged for each node in the graph and also averaged over the 100 randomly generated graphs, for both classes of graphs, in the dataset. As can be seen the distributions of such numbers of nodes are at least distinguishable for several values of x, when comparing the two types of graphs. This motivates why the SVM is able to distinguish the two classes better using the GSPI feature vectors than the SPI feature vectors."}, {"heading": "6 Analysis", "text": "In this section we give some approximated analysis of random feature vectors in order to give theoretical support for our experimental observations. We first show that one-cluster and two-cluster graphs have quite similar SPI feature vectors (as their expectations). Then we next show some evidence that there is a nonnegligible difference in their GSPI feature vectors. Throughout this section, we consider feature vectors defined by considering only paths from any fixed source node s. Thus, for example, n (1) d is the number of nodes at distance d from s in\n8\na one-cluster graph, and n (2) d,x is the number of nodes that have x shortest paths of length d to s in a two-cluster graph. Here we introduce a way to state an approximation. For any functions a and b depending on n, we write a \u2248rel b by which we mean\nb (\n1\u2212 c n\n) < a < b ( 1 + c\nn ) holds for some constant c > 0 and sufficiently large n. We say that a and b are relatively (1 \u00b1 O(1/n))-close if a \u2248rel b holds. Note that this closeness notion is closed under constant number of additions/subtractions and multiplications. For example, if a \u2248rel b holds, then we also have ak \u2248rel bk for any k \u2265 1 that can be regarded as a constant w.r.t. n. In the following we will often use this approximation."}, {"heading": "6.1 Approximate Comparison of SPI Feature Vectors", "text": "We consider relatively small1 distances d so that d can be considered as a small constant w.r.t. n. We show that E[n (1) d ] and E[n (2) d ] are similar in the following sense.\n1 This smallness assumption is for our analysis, and we believe that the situation is more or less the same for any d.\n9\nTheorem 1. For any constant d, we have E[n (1) d ] \u2208 E[n (2) d ](1 \u00b1 2 c0\u22121 ), holds\nwithin our \u2248rel approximation when c0 \u2265 2 + \u221a 3.\nRemark. For deriving this relation we assume a certain independence on the existence of two paths in G; see the argument below for the detail. Note that this difference between E[n (1) d ] and E[n (2) d ] vanishes for large values of c0.\nProof. First consider a one-cluster graph G = (V,E) and analyze E[n (1) d ]. For this analysis, consider any target node t (6= s) of G (we consider this target node to be a fixed node to begin with), and estimate first the probability Fd that there exists at least one path of length d from s to t. Let Au,v be the event that an edge {u, v} exists in G, and let Wd denote the set of all paths (from s to t) expressed by a permutation (v1, . . . , vd\u22121) of nodes in V \\ {s, t}. For each tuple (v1, . . . , vd\u22121) of Wd, the event As,v1 \u2227 Av1,v2 \u2227 . . . \u2227 Avd\u22121,t is called the event that the path (from s to t) specified by (s, v1, . . . , vd\u22121, t) exists in G (or, more simply, the existence of one specific path). Then the probability Fd is expressed by\nFd = Pr  \u2228 (v1,...,vd\u22121)\u2208Wd As,v1 \u2227Av1,v2 \u2227 . . . \u2227Avd\u22121,t  (4)\n10\nClearly, the probability of the existence of one specific path is pd1, and the above probability can be calculated by using the inclusion-exclusion principle. Here we follow the analysis of Fronczak et al. [7] and assume that every specific path exists independently. Note that the number of dependent paths can be big when the length of a path is long, therefore this assumption is only reasonable for short distances d. To simplify the analysis2 we only consider the first term of the inclusion-exclusion principle. That is, we approximate Fd by\nFd \u2248 \u2211\n(v1,...,vd\u22121)\u2208Wd\nPr [ As,v1 \u2227Av1,v2 \u2227 . . . \u2227Avd\u22121,t ] (5)\n= |Wd|pd1 = (n\u2212 2)(n\u2212 3) \u00b7 \u00b7 \u00b7 (n\u2212 (2 + d\u2212 2))pd1 \u2248rel nd\u22121pd1,\nwhere the last approximation relation holds since d is constant. From this approximation, we can approximate the probability fd that t has a shortest path of length d to s. For any d \u2265 1, let Ad be the event that there exists at least one path of length d, or less, between s and t, and let Bd be the event that there exists a shortest path of length d between s and t. Then\nFd \u2264 Pr[Ad] \u2264 d\u2211 i=1 Fi. (6)\nNote that\nd\u2211 i=1 Fi \u2248rel d\u2211 i=1 ni\u22121pi1 = n d\u22121pd1( d\u22121\u2211 i=0 1 (np1)i )\n\u2264 nd\u22121pd1( np1\nnp1 \u2212 1 ) = nd\u22121pd1(1 +\n1\nnp1 \u2212 1 ).\nSince np1 = c0 \u2265 1, it follows that\nnd\u22121pd1(1 + 1\nnp1 \u2212 1 ) = nd\u22121pd1(1 +\n1\nc0 \u2212 1 ).\nWhile Fd \u2248rel nd\u22121pd1. Thus we have within our \u2248rel approximation, that\nnd\u22121pd1 \u2264 Pr[Ad] \u2264 nd\u22121pd1(1 + 1\nc0 \u2212 1 ).\nIt is obvious that fd = Pr[Bd], note also that Ad = Bd \u2228 Ad\u22121 and that the two events Bd and Ad\u22121 are disjoint. Thus, we have Pr[Ad] = Pr[Bd]+Pr[Ad\u22121], which is equivalent to\nnd\u22121pd1 \u2212 nd\u22122pd\u221211 (1 + 1 c0 \u2212 1 ) \u2264 fd \u2264 nd\u22121pd1(1 + 1 c0 \u2212 1 )\u2212 nd\u22122pd\u221211 .\n2 Clearly, this is a rough approximation; nevertheless, it is enough for asymptotic analysis w.r.t. the (1 \u00b1 O(1/n))-closeness. For smaller n, we may use a better approximation from [7], which will be explained in Subsection 6.3.\n11\nSince fd is the probability that there is a shortest path of length d from s to any fixed t, it follows that E[n (1) d ], i.e., the expected number of nodes that have a shortest path of length d to s, can be estimated by\nE[n (1) d ] = (n\u2212 1)fd \u2248rel nfd.\nWhich gives that\nndpd1 \u2212 nd\u22121pd\u221211 (1 + 1\nc0 \u2212 1 ) \u2264 E[n(1)d ] \u2264 n\ndpd1(1 + 1 c0 \u2212 1 )\u2212 nd\u22121pd\u221211 . (7)\nholds within our \u2248rel, approximation. We may rewrite the above equation using the following equalities\nndpd1 \u2212 nd\u22121pd\u221211 (1 + 1 c0 \u2212 1 ) = ndpd1 \u2212 nd\u22121pd\u221211 \u2212 nd\u22121pd\u221211 c0 \u2212 1\n= (ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212 nd\u22121pd\u221211 c0\u22121\nndpd1 \u2212 nd\u22121p d\u22121 1\n)\n= (ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212 1\n(c0 \u2212 1)2 ), (8)\nand\nndpd1(1 + 1 c0 \u2212 1 )\u2212 nd\u22121pd\u221211 = ndpd1 \u2212 nd\u22121p d\u22121 1 + ndpd1 c0 \u2212 1\n= (ndpd1 \u2212 nd\u22121pd\u221211 )(1 + ndpd1 c0\u22121\nndpd1 \u2212 nd\u22121p d\u22121 1\n)\n= (ndpd1 \u2212 nd\u22121pd\u221211 )(1 + c0\n(c0 \u2212 1)2 ). (9)\nSubstituting (8) and (9) into (7) we get\n(ndpd1\u2212nd\u22121pd\u221211 )(1\u2212 1\n(c0 \u2212 1)2 ) \u2264 E[n(1)d ] \u2264 (n\ndpd1\u2212nd\u22121pd\u221211 )(1 + c0\n(c0 \u2212 1)2 ).\n(10) We will later use these bounds to derive the theorem.\nWe now analyze a two-cluster graph and E[n (2) d ]. Let us assume first that s is in V +. Again we fix a target node t to begin with. Here we need to consider the case that the target node t is also in V + and the case that it is in V \u2212. Let F+d and F \u2212 d be the probabilities that t has at least one path of length d to s in the two cases. Then for the first case, the path starts from s \u2208 V + and ends in t \u2208 V +, meaning that the number of times that the path crossed from one cluster to another (either from V + to V \u2212 or V \u2212 to V +) has to be even. Thus the probability of one specific path existing is pd\u2212k2 q k 2 for some even k, 0 \u2264 k \u2264 d. Thus, the first term of the inclusion-exclusion principle (the sum of the probabilities of all possible paths) then becomes\nF+d \u2248rel (n\n2 )d\u22121 d\u2211 even k=0 ( d k ) pd\u2212k2 q k 2 ,\n12\nwhere the number of paths is approximated as before, i.e., |V + \\ {s, t}| \u00b7 (|V + \\ {s, t}| \u2212 1) \u00b7 \u00b7 \u00b7 ((|V + \\ {s, t}| \u2212 (d \u2212 2)) is approximated by (n/2)d\u22121. We can similarly analyze the case where t is in V \u2212 to obtain\nF\u2212d \u2248rel (n\n2 )d\u22121 d\u2211 odd k=1 ( d k ) pd\u2212k2 q k 2 .\nSince both cases (t \u2208 V +, or t \u2208 V \u2212) are equally likely, the average probability of there being a path of length d, between s and t, in a two-cluster graph is\nF+d + F \u2212 d\n2 \u2248rel (n 2 )d\u22121 d\u2211 even k=0 ( d k ) pd\u2212k2 q k 2 2 + (n 2 )d\u22121 d\u2211 odd k=1 ( d k ) pd\u2212k2 q k 2 2\n= (n\n2 )d\u22121 d\u2211 k=0 ( d k ) pd\u2212k2 q k 2 2 = (n 2 )d\u22121 (p2 + q2)d 2 .\nNote here that p2 + q2 \u2248rel 2p1 from our choice of q2 (see (3)). Thus, we have\nF+d + F \u2212 d\n2 \u2248rel (n 2 )d\u22121 (2p1)d 2 = nd\u22121pd1.\nWhich is exactly the same as in the one cluster case, see (5). Thus we have\n(ndpd1\u2212nd\u22121pd\u221211 )(1\u2212 1\n(c0 \u2212 1)2 ) \u2264 E[n(2)d ] \u2264 (n\ndpd1\u2212nd\u22121pd\u221211 )(1 + c0\n(c0 \u2212 1)2 ).\n(11) Using this we now prove the main statement of the theorem, namely that\nE[n (1) d ] \u2208 E[n (2) d ](1\u00b1\n2\nc0 \u2212 1 ).\nTo prove the theorem we need to prove the following two things\nE[n (1) d ] \u2264 E[n (2) d ](1 +\n2\nc0 \u2212 1 ), and (12)\nE[n (1) d ] \u2265 E[n (2) d ](1\u2212\n2\nc0 \u2212 1 ) (13)\nThe proof of (12) can be done by using (10) and (11).\nE[n (1) d ] \u2264 E[n (2) d ] +\n2E[n (2) d ]\nc0 \u2212 1\n\u21d4 (ndpd1 \u2212 nd\u22121pd\u221211 )(1 + c0 (c0 \u2212 1)2 ) \u2264 (ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212\n1\n(c0 \u2212 1)2 )\n+ 2(ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212 1(c0\u22121)2 )\nc0 \u2212 1\n\u21d4 1 + c0 (c0 \u2212 1)2 \u2264 1\u2212 1 (c0 \u2212 1)2 + 2 c0 \u2212 1 \u2212 2 (c0 \u2212 1)3 \u21d4 c0 + 1 c0 \u2212 1 + 2 (c0 \u2212 1)2 \u2264 2. (14)\n13\nWhich holds when c0 \u2265 2 + \u221a\n3 \u2248 3.7. The proof of (13) is similar and shown below.\nE[n (1) d ] \u2265 E[n (2) d ]\u2212\n2E[n (2) d ]\nc0 \u2212 1\n\u21d4 (ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212 1 (c0 \u2212 1)2 ) \u2265 (ndpd1 \u2212 nd\u22121pd\u221211 )(1 + c0 (c0 \u2212 1)2 )\n\u2212 2(ndpd1 \u2212 nd\u22121pd\u221211 )(1\u2212 1(c0\u22121)2 )\nc0 \u2212 1\n\u21d4 1\u2212 1 (c0 \u2212 1)2 \u2265 1 + c0 (c0 \u2212 1)2 \u2212 2 c0 \u2212 1 + 2 (c0 \u2212 1)3 \u21d4 2 \u2265 c0 + 1 c0 \u2212 1 + 2 (c0 \u2212 1)2 . (15)\nWhich again holds when c0 \u2265 2 + \u221a\n3 \u2248 3.7. This completes the proof of the theorem."}, {"heading": "6.2 Heuristic Comparison of GSPI Feature Vectors", "text": "We compare in this section the expected GSPI feature vectors E[v (1) gsp ] and E[v (2) gsp ], that is, [E[n (1) d,x]]d\u22651,x\u22651 and [E[n (2) d,x]]d\u22651,x\u22651, and show evidence that they have some non-negligible difference. Here we focus on the distance d = 2 part of the GSPI feature vectors, i.e., subvectors [E[n (z) 2,x]]x\u22651 for z \u2208 {1, 2}. Since it is not so easy to analyze the distribution of the values E[n (z) 2,1],E[n (z) 2,2], . . ., we introduce some \u201cheuristic\u201d analysis. We begin with a one-cluster graph G, and let V2 denote the set of nodes of G with distance 2 from the source node s. Consider any t in V2, and for any x \u2265 1, we estimate the probability that it has x number of shortest paths of length 2 to s. Let V1 be the set of nodes at distance 1 from s. Recall that G has (n \u2212 1)f1 \u2248rel np1 nodes in V1 on average, and we assume that t has an edge from some node in V1 each of which corresponds to a shotest path of distance 2 from s to t. We now assume for our \u201cheuristic\u201d analysis that |V1| = np1 and that an edge between each of these distance 1 nodes and t exists with probability p1 independently at random. Then x follows the binomial distribution Bin(np1, p1), where by Bin(N, p) we mean a random number of heads that we have when flipping a coin that gives heads with probability p independently N times. Then for each x \u2265 1, E[n(1)2,x], the expected number of nodes of V2 that have x shortest paths of length 2 to s, is estimated by\nE[n (1) 2,x] \u2248 \u2211 t\u2208V2 Pr [ Bin(np1, p1) = x ] = E[n (1) 2 ] \u00b7 Pr [ Bin(np1, p1) = x ] ,\nby assuming that |V2| takes its expected value E[n(1)2 ]. Clearly the distribution of values of vector [E[n\n(1) 2,x]]x\u22651 is proportional to Bin(np1, p1), and it has one\npeak at x (1) peak = np 2 1, since the mean of a binomial distribution, Bin(N, p) is Np.\n14\nConsider now a two-cluster graph G. We assume that our start node s is in V +. For d \u2208 {1, 2}, let V +d and V \u2212 d denote respectively the set of nodes in V + and V \u2212 with distance d from s. Let V2 = V + 2 \u222a V \u2212 2 . Again we assume that V + 1 and V \u22121 have respectively np2/2 and nq2/2 nodes and that the numbers of edges from V +1 and V \u2212 1 to a node in V2 follow binomial distributions. Note that we need to consider two cases here, t \u2208 V +2 and t \u2208 V \u2212 2 . First consider the case that the target node t is in V +2 . In this case there are two types of shortest paths. The first type of paths goes from s to V +1 and then to t \u2208 V + 2 . The second type of shortest path goes from s to V \u22121 and then to t \u2208 V + 2 . Based on this we get\nf\u0303 (2,+) 2,x := Pr\n[ t has x shortest paths ] = Pr [ Bin (n 2 p2, p2 ) + Bin (n 2 q2, q2 ) = x\n] \u2248 Pr [ N (n\n2 p22, \u03c3 2 1\n) + N (n 2 q22 , \u03c3 2 2 ) \u2208 [x\u2212 0.5, x+ 0.5] ] = Pr [ N ( n(p22 + q 2 2)\n2 , \u03c321 + \u03c3 2 2\n) \u2208 [x\u2212 0.5, x+ 0.5] ] ,\nwhere we use the normal distribution N(\u00b5, \u03c32) to approximate each binomial distribution so that we can express their sum by a normal distribution (here we omit specifying \u03c31 and \u03c32). For the second case where t \u2208 V \u22122 , with a similar argument, we derive\nf\u0303 (2,\u2212) 2,x := Pr\n[ t has x shortest paths ] = Pr [ N ( np2q2, \u03c3 2 3 + \u03c3 2 4 ) \u2208 [x\u2212 0.5, x+ 0.5] ] .\nNote that the first case (t \u2208 V +2 ), happens with probability |V + 2 |/(|V + 2 | +\n|V \u22122 |). The second case (t \u2208 V \u2212 2 ), happens with probability |V \u2212 2 |/(|V + 2 |+ |V \u2212 2 |). Then again we may approximate the xth component of the expected feature subvector by\nE[n (2) 2,x] \u2248 E[|V +2 |] E[|V +2 |] + E[|V \u2212 2 |] E[|V +2 |]f\u0303 (2,+) 2,x + E[|V \u22122 |] E[|V +2 |] + E[|V \u2212 2 |] E[|V \u22122 |]f\u0303 (2,\u2212) 2,x .\nWe have now arrived at the key point in our analysis. Note that the distribution of values of vector [E[n (2) 2,x]]x\u22651 follows the mixture of two distributions, namely, N(n(p22 + q 2 2)/2, \u03c3 2 1 + \u03c3 2 2) and N(np2q2, \u03c3 2 3 + \u03c3 2 4), with weights E[|V +2 |]/(E[|V + 2 |] + E[|V \u2212 2 |]) and E[|V \u2212 2 |]/(E[|V + 2 |] + E[|V \u2212 2 |]). Now we estimate the distance between the two peaks x (2,+) peak and x (2,\u2212) peak of these two distributions. Note that the mean of a normal distribution N(\u00b5, \u03c32) is simply \u00b5. Then we have\nx (2,+) peak \u2212 x (2,\u2212) peak =\nn 2 (p22 + q 2 2)\u2212 np2q2 = n 2 (p2 \u2212 q2)2\n\u2248rel n\n2 (p2 \u2212 (2p1 \u2212 p2))2 =\nn 2 (2p2 \u2212 2p1)2\n\u2248rel 2n(p1(1 + \u03b10)\u2212 p1)2 = 2n(p1\u03b10)2 = 2n\u03b120p21\nNote that q2 \u2248rel 2p1 \u2212 p2 holds (from (3)); hence, we have p1 \u2248rel (p2 + q2)/2 \u2265 \u221ap2q2, and we approximately have p21 \u2265 p2q2. By using this, we can bound\n15\nthe difference between these peaks by\nx (2,+) peak \u2212 x (2,\u2212) peak \u2248rel 2n\u03b1 2 0p 2 1 \u2265 2\u03b120x (2,\u2212) peak .\nThat is, these peaks have non-negligible relative difference.\nFrom our heuristic analysis we may conclude that the two vectors [E[n (1) 2,x]]x\u22651 and [E[n (2) 2,x]]x\u22651 have different distributions of their component values. In particular, while the former vector has only one peak, the latter vector has a double peak shape (for large enough \u03b10). Note that this difference does not vanish even when c0 is big. This means that the GSPI feature vectors are different for onecluster graphs and two-clusters graphs, even when c0 is big, which is not the case for the SPI feature vectors, since their difference vanishes when c0 is big. This provides evidence as to why our GSPI kernel performs better than the SPI kernel\nThough this is a heuristic analysis, we can show some examples that our observation is not so different from experimental results. In Fig. 3 we have plotted both our approximated vector of [E[n (2) 2,x]]x\u22651 (actually we have plotted the mixed normal distribution that gives this vector) and the corresponding experimental vector obtained by generating graphs according to our random model. In this figure the double peak shape can clearly be observed, which provides empirical evidence supporting our analysis. This experimental vector is the average vector for each fixed source node in random graphs, which is averaged over 500 randomly generated graphs with the parameters n = 400, p2 = 0.18, and q2 = 0.0204. (For these parameters, we need to use a better approximation of (4) explained in the next subsection to derive the normal distribution of this figure.)"}, {"heading": "6.3 Inclusion-Exclusion Principle", "text": "Throughout the analysis, we have always used the first term of the inclusionexclusion principle to estimate (4). Although this works well for expressing our analytical result, where we consider the case where n is big. When applying the approximation for graphs with a small number of nodes, it might be necessary to consider a better approximation of the inclusion-exclusion principle. For example, we in fact used the approximation from [7] for deriving the mixed normal distributions of Fig. 3. Here for completeness, we state this approximation as a lemma and give its proof that is outlined in [7].\nLemma 1. Let E1, E2, . . . , El be mutually independent events such that Pr[Ei] \u2264 holds for all i, 1 \u2264 i \u2264 l. Then we have\nPr [ l\u22c3 i=1 Ei ] = 1\u2212 exp ( \u2212 l\u2211 i=1 Pr[Ei] ) \u2212Q. (16)\nWhere\n\u2212 l+1\u2211 k=0 (l )k k! + (1 + )l \u2264 Q \u2264 l+1\u2211 k=0 (l )k k! \u2212 (1 + )l. (17)\n16\nRemark. The above bound for the error term Q is slightly weaker than the one in [7], but it is sufficient enough for many situations, in particular for our usage. In our analysis of (4) each Ei corresponds to an event that one specific path exists. Recall that we assumed that all paths exists independently.\nProof. Using the definition of the inclusion-exclusion principle we get\nPr [ l\u22c3 i=1 Ei ] = l\u2211 k=1 (\u22121)k+1S(k), (18)\nwhere each S(k) is defined by S(k) = \u2211\n1\u2264i1<...<ik\u2264l\nPr[Ei1 ]Pr[Ei2 ] \u00b7 \u00b7 \u00b7Pr[Eik ] = \u2211\n1\u2264i1<...<ik\u2264l\nPi1Pi2 \u00b7 \u00b7 \u00b7Pik .\nHere and in the following we denote each probability Pr[Ei] simply by Pi. First we show that\nS(k) = 1\nk! ( l\u2211 i=1 Pi )k \u2212Qk, (19)\nwhere\n0 \u2264 Qk \u2264 ( lk k! \u2212 ( l k )) k. (20)\nTo see this we introduce two index sequence sets \u0393k and \u03a0k defined by\n\u0393k = { (i1, . . . , ik) : ij \u2208 {1, . . . , l} for all j, 1 \u2264 j \u2264 k }, \u03a0k = { (i1, . . . , ik) \u2208 \u0393k : ij 6= ij\u2032 for all j, j\u2032, 1 \u2264 j < j\u2032 \u2264 k }.\nThen it is easy to see that( l\u2211 i=1 Pi )k = \u2211 (i1,...,ik)\u2208\u0393k Pi1 \u00b7 \u00b7 \u00b7Pik , and k!S(k) = \u2211 (i1,...,ik)\u2208\u03a0k Pi1 \u00b7 \u00b7 \u00b7Pik .\nThus, we have\nk!Qk = ( l\u2211 i=1 Pi )k \u2212 k!S(k) = \u2211 (i1,...,ik)\u2208\u0393k\\\u03a0k Pi1 \u00b7 \u00b7 \u00b7Pik\n\u2264 |\u0393k \\\u03a0k| k = ( lk \u2212 l(l \u2212 1) \u00b7 \u00b7 \u00b7 (l \u2212 k + 1) ) k,\nwhich gives bound (20) for Qk. Now from (18) and (19) we have\n1\u2212 Pr [ l\u22c3 i=1 Ei ] = l\u2211 k=0 (\u22121)k k! ( l\u2211 i=1 Pi )k + l\u2211 k=1 (\u22121)k+1Qk.\n17\nHere we note that the sum \u2211l k=0(\u22121)k/k!( \u2211l i=1 Pi) k is the first l + 1 terms of\nthe MacLaurin expansion of exp(\u2212 \u2211l i=1 Pi). Hence, the error term Q of (16) becomes\nQ = \u2212 \u2211 k\u2265l+1 (\u22121)k k! ( l\u2211 i=1 Pi )k + l\u2211 k=1 (\u22121)k+1Qk.\nWe now derive an upper bound for Q.\nQ \u2264 \u2212 \u2211 k\u2265l+1 (\u22121)k k! ( l\u2211 i=1 Pi )k + l\u2211 k=1 Qk\n\u2264 (l ) l+1\n(l + 1)! + l\u2211 k=1 (l )k k! \u2212 l\u2211 k=1 ( l k ) k\n= l+1\u2211 k=0 (l )k k! \u2212 (1 + )l.\nThis proves the upper bound on Q, the proof for the lower bound of Q is completely analogous. Thus, the lemma holds."}, {"heading": "7 Conclusions and Future Work", "text": "We have defined a new graph kernel, based on the number of shortest paths between node pairs in a graph. The feature vectors of the GSP kernel do not take longer time to calculate than the feature vectors of the SP kernel. The reason for this is the fact that the number of shortest paths between node pairs is a by-product of using Dijkstra\u2019s algorithm to get the length of the shortest paths between all node pairs in a graph. The number of shortest paths between node pairs does contain relevant information for certain types of graphs. In particular we showed in our experiments that the GSP kernel, which also uses the number of shortest paths between node pairs, outperformed the SP kernel, which only uses the length of the shortest paths between node pairs, at the task of classifying graphs as containing one or two clusters. We also gave an analysis motivating why the GSP kernel is able to correctly classify the two types of graphs when the SP kernel is not able to do so.\nFuture research could examine the distribution of the random feature vectors vsp and vgsp, for random graphs, that are generated using the planted partition model, and have more than two clusters. Although we have only given experimental results and an analysis for graphs that have either one or two clusters, preliminary experiments show that the GSPI kernel outperforms the SPI kernel on such tasks as e.g. classifying if a random graph contains one or four cluster, if a random graph contains two or four clusters etc. It would be interesting to see which guarantees it is possible to get, in terms of guaranteeing that the vgsp vectors are different and the vsp vectors are similar, when the numbers of clusters are not just one or two.\n18"}], "references": [{"title": "Cell-graph mining for breast tissue modeling and classification", "author": ["C. Bilgin", "C. Demir", "C. Nagi", "B. Yener"], "venue": "In Engineering in Medicine and Biology Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Distance distribution in random graphs and application to network exploration", "author": ["V.D. Blondel", "J.-L. Guillaume", "J.M. Hendrickx", "R.M. Jungers"], "venue": "Physical Review E,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.-P. Kriegel"], "venue": "In Prof. of ICDM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Protein function prediction via graph kernels", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Sch\u00f6nauer", "S. Vishwanathan", "A.J. Smola", "H.P. Kriegel"], "venue": "Bioinformatics, 21(suppl 1):i47\u2013i56,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische mathematik,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1959}, {"title": "Average path length in random networks", "author": ["A. Fronczak", "P. Fronczak", "J.A. Ho lyst"], "venue": "Physical Review E,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P. Flach", "S. Wrobel"], "venue": "Learning Theory and Kernel Machines,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Theoretical and numerical study of fractal dimensionality in self-avoiding walks", "author": ["S. Havlin", "D. Ben-Avraham"], "venue": "Physical Review A,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Entity disambiguation in anonymized graphs using graph kernels", "author": ["L. Hermansson", "T. Kerola", "F. Johansson", "V. Jethava", "D. Dubhashi"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Global graph kernels using geometric embeddings", "author": ["F. Johansson", "V. Jethava", "D. Dubhashi", "C. Bhattacharyya"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Semi-supervised feature selection for graph classification", "author": ["X. Kong", "P.S. Yu"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "An application of boosting to graph classification", "author": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "The complexity of counting self-avoiding walks in subgraphs of two-dimensional grids and hypercubes", "author": ["M. L\u00edskiewicz", "M. Ogihara", "S. Toda"], "venue": "Theoretical Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["N. Shervashidze", "S. Vishwanathan", "T. Petri", "K. Mehlhorn", "K.M. Borgwardt"], "venue": "In Proc. of AISTATS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Classifying graphs into different classes depending on their structure is a problem that has been studied for a long time and that has many useful applications [1,5,13,14].", "startOffset": 160, "endOffset": 171}, {"referenceID": 3, "context": "Classifying graphs into different classes depending on their structure is a problem that has been studied for a long time and that has many useful applications [1,5,13,14].", "startOffset": 160, "endOffset": 171}, {"referenceID": 10, "context": "Classifying graphs into different classes depending on their structure is a problem that has been studied for a long time and that has many useful applications [1,5,13,14].", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "Classifying graphs into different classes depending on their structure is a problem that has been studied for a long time and that has many useful applications [1,5,13,14].", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "By classifying graphs researchers have been able to solve important problems such as to accurately predict the toxicity of chemical compounds [14], classify if human tissue contains cancer or not [1], predict if a particular protein is an enzyme or not [5], and many more.", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "By classifying graphs researchers have been able to solve important problems such as to accurately predict the toxicity of chemical compounds [14], classify if human tissue contains cancer or not [1], predict if a particular protein is an enzyme or not [5], and many more.", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "By classifying graphs researchers have been able to solve important problems such as to accurately predict the toxicity of chemical compounds [14], classify if human tissue contains cancer or not [1], predict if a particular protein is an enzyme or not [5], and many more.", "startOffset": 253, "endOffset": 256}, {"referenceID": 7, "context": "It is generally regarded that the number of self-loop-avoiding paths between all pairs of nodes of a given graph is useful for understanding the structure of the graph [9,15].", "startOffset": 168, "endOffset": 174}, {"referenceID": 12, "context": "It is generally regarded that the number of self-loop-avoiding paths between all pairs of nodes of a given graph is useful for understanding the structure of the graph [9,15].", "startOffset": 168, "endOffset": 174}, {"referenceID": 2, "context": "This approach has proven successful for classifying several types of graphs [4,5,10].", "startOffset": 76, "endOffset": 84}, {"referenceID": 3, "context": "This approach has proven successful for classifying several types of graphs [4,5,10].", "startOffset": 76, "endOffset": 84}, {"referenceID": 8, "context": "This approach has proven successful for classifying several types of graphs [4,5,10].", "startOffset": 76, "endOffset": 84}, {"referenceID": 6, "context": "Such as graph kernels considering all walks [8], shortest paths [4], small subgraphs [17], global graph properties [11], and many more.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "Such as graph kernels considering all walks [8], shortest paths [4], small subgraphs [17], global graph properties [11], and many more.", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "Such as graph kernels considering all walks [8], shortest paths [4], small subgraphs [17], global graph properties [11], and many more.", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Such as graph kernels considering all walks [8], shortest paths [4], small subgraphs [17], global graph properties [11], and many more.", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "For example, we can use Dijkstra\u2019s algorithm [6] for each node in a given graph, which gives all node pairs\u2019 shortest path length (i.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "We could consider the intervals {[1, 10], [11, 20].", "startOffset": 33, "endOffset": 40}, {"referenceID": 8, "context": "We could consider the intervals {[1, 10], [11, 20].", "startOffset": 33, "endOffset": 40}, {"referenceID": 9, "context": "We could consider the intervals {[1, 10], [11, 20].", "startOffset": 42, "endOffset": 50}, {"referenceID": 2, "context": "Graph kernels have been used as tools for using SVM classifiers for graph classification problems [4,5,10].", "startOffset": 98, "endOffset": 106}, {"referenceID": 3, "context": "Graph kernels have been used as tools for using SVM classifiers for graph classification problems [4,5,10].", "startOffset": 98, "endOffset": 106}, {"referenceID": 8, "context": "Graph kernels have been used as tools for using SVM classifiers for graph classification problems [4,5,10].", "startOffset": 98, "endOffset": 106}, {"referenceID": 2, "context": "The kernel that we build upon in this paper is the shortest path (SP) kernel, which compares graphs based on the shortest path length of all pairs of nodes [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "where k is a positive definite kernel [4].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "One of the most common kernels for k is the indicator function, as used in Borgwardt and Kriegel [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "This means that the inner product between two feature vectors always is in [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 13, "context": "We use Pegasos [16] for solving the SVM.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "[7] and assume that every specific path exists independently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For smaller n, we may use a better approximation from [7], which will be explained in Subsection 6.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "For example, we in fact used the approximation from [7] for deriving the mixed normal distributions of Fig.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Here for completeness, we state this approximation as a lemma and give its proof that is outlined in [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "The above bound for the error term Q is slightly weaker than the one in [7], but it is sufficient enough for many situations, in particular for our usage.", "startOffset": 72, "endOffset": 75}], "year": 2015, "abstractText": "We consider the problem of classifying graphs using graph kernels. We define a new graph kernel, called the generalized shortest path kernel, based on the number and length of shortest paths between nodes. For our example classification problem, we consider the task of classifying random graphs from two well-known families, by the number of clusters they contain. We verify empirically that the generalized shortest path kernel outperforms the original shortest path kernel on a number of datasets. We give a theoretical analysis for explaining our experimental results. In particular, we estimate distributions of the expected feature vectors for the shortest path kernel and the generalized shortest path kernel, and we show some evidence explaining why our graph kernel outperforms the shortest path kernel for our graph classification problem.", "creator": "LaTeX with hyperref package"}}}