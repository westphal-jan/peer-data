{"id": "1510.07099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2015", "title": "Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media", "abstract": "in this paper, we recently propose a joint algorithm for the word segmentation on chinese social media. previous work mainly focus on word segmentation for including plain chinese in text, in order to develop a chinese social media processing tool, we need to take the certain main features of social media into account, whose grammatical meaning structure is not rigorous, and the substantial tendency of searches using colloquial and abstract internet terms makes the existing chinese - processing tools inefficient to obtain thus good performance on social media.", "histories": [["v1", "Sat, 24 Oct 2015 02:24:54 GMT  (12kb)", "http://arxiv.org/abs/1510.07099v1", "5 pages, 5 tables"]], "COMMENTS": "5 pages, 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yao yushi", "huang zheng"], "accepted": false, "id": "1510.07099"}, "pdf": {"name": "1510.07099.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yys12345@sjtu.edu.cn", "huangzhengsjtu@126.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n07 09\n9v 1\n[ cs\n.C L\n] 2\n4 O\nct 2\n01 5"}, {"heading": "1 Introduction", "text": "Social media contains vast amounts of information, all of the information can be used to explore micro-blog event and apply sentiment analysis, etc. In order to obtain essential information, first step is text processing, generally we use natural language processing methods to analyze social media and extract information for other social media research. (Xiong el al., 2013)\nAn English sentence uses spaces as the gap between words, but Chinese has no such word boundaries, so Chinese natural language process includes one more step than English: Word Segmentation \u2013 determination of the word boundaries. Word Segmentation is the core step of the entire Chinese language processing, it is directly related\nto the accuracy of the entire Chinese natural language process, in addition, the difficulty of Chinese word segmentation lies in the removal of ambiguity and identifying OOV words. Ambiguity means that a sentence may result in many possible word segmentation results and every kind of segmentation has different semantic meanings, while the meaning of OOV words identification refers to the word which is not included by word dictionary, the most typical example is person\u2019s name, place name and so on."}, {"heading": "2 Related Work", "text": "Currently research on natural language process for social media is still at the starting stage, in the 2006 SIGHAN Chinese word segmentation competition, approaches based on sequence annotation has been widely used. Microsoft used conditional random field and the size of feature window equals one. (Huang and Zhao, 2007) It turns out that features have been simplified, but the performance was still very good. DaLian Science and Technology University built two models, one is based on the use of the word CRF model, the other is based on MMSM model. During our experiments, we used CRF algorithm as well, and it plays the key role when we build the model. Our work mainly depends on the improvement of conditional random field.\nConditional random field (CRF) is a statistical sequence modeling framework which is first introduced into language processing by Lafferty J et al(2001). (Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng H et al(2005) and John Lafferty et al(2001) introduced a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features. (Tseng et al., 2005; Lafferty et al., 2001) The study that is closely re-\nlated to ours is (Zhao et al., 2006), which also used assistant algorithm and added external lexicon, while they just add the output of assistant algorithm to the feature templates. Different from that work, we take not only the relevance between the character and its MMSEG output tag, but also the context feature of these MMSEG output tags as well."}, {"heading": "3 Methods", "text": "There are already a lot of sophisticated algorithms for word segmentation such as: statistical methods(Hidden Markov Model HMM, CRF etc.), lexicon-based algorithms (MMSEG), and rule-based algorithms. CRF performs better than lexicon-based models on OOV rates because CRF introduce additional features, (Collins, 2002) which may be artificially added (Xiong et al., 2009), including character level features and context level features, in addition, CRF also maintains the Markov characteristics of the word, (Wallach, ) thus we can remove word ambiguity by combining more features as well.\nGenerally there are several simple algorithms used in word segmentation. When applying lexicon-based algorithm such as MMSEG, we simply match words according to our lexicon. While for statistical-based algorithm such as CRF, the training set is turned into a Chinese character sequence and the segmentation task can be considered as an annotation task.\nMore specifically, the model trained by CRF assigns each character a word boundary tag when labeling a sentence. Here we use the BMES tag set, B, M and E denote the first, middle and last character of a multi-character word respectively, and S denotes the single character word. (McCallum et al., 2010)\nAs social media contains a large amount of OOV words, and with respect to out-of-vocabulary recognition, lexicon-based algorithm has a strong advantage to identify the correct rate, so when provided a suitable lexicon, lexicon-based algorithm can be combined with CRF algorithm to enhance the performance of CRF in word segmentation of social media. (Qian et al., 2010)\nFinally we choose to use MMSEG to do rough segmentation and take the segmentation result as a new feature during the training process of the CRF segmentation model."}, {"heading": "4 Experiments", "text": "The files involved in the training process include the entire feature template file, training set and lexicon files, in order to get the best results, we adopted a similar approach as coordinates rise, we will divide the experiment into three stages, each stage select one training material to optimize and fix the other two materials. (Razvan and Bunescu, 2008) At each stage, if one training material gets the best performance on test set, it will be chosen to be the final training material that we would use. (Wallach, 2002)"}, {"heading": "4.1 Data and Tool", "text": "The training set of word segmentation is from Backoff 2005. (Tseng et al., 2005) We have to mention that the lexicon used by MMSEG is the Sougou lab internet lexicon published in 2006, which contains a number of high-frequency words under internet environment, and mmseg4j project lexicon file.\nThere are few test sets about social media, so we crawled raw content from Sina weibo and then annotate manually, the contents is from all kinds of areas and users. The size of test set is about 190K, containing about 25 thousands Chinese words. What is worth mention is that our segmentation is the same as MSR\u2019s standard.\nDuring our experiment, we use PCRF to train the segmentation model and test the model on our test sets, PCRF is an open source implementation of (Linear) Conditional Random Fields (CRFs) for segmenting/labeling sequential data. The train file format and feature template of PCRF are compatible with popular CRF implementations, like CRF++."}, {"heading": "4.2 Evaluation Metric", "text": "We evaluate system performance on the individual tasks. For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R), where precision is the percentage of correct words in the system output, and recall is the percentage of words in gold standard annotations that are correctly predicted."}, {"heading": "4.3 Feature Template", "text": "Template file plays the core role in model training. Since CRF is designed to calculate the conditional probability of the sequence annotation de-\nduced by the observed sequence, and this probability can be used to help describe a set of random variables related to the distribution of feature vectors, which are composed by characteristic function derived from feature templates defined by user. (Pietra et al., 1997)\nEach line in the template file of PCRF denotes one template. In each template, special macro %x[row,col] will be used to specify a token in the input data, row specifies the relative position from the current focusing token and col specifies the absolute position of the column.\nThe first stage of the experiment is to determine the feature vectors of our model, (Finkel et al., 2008) the original features are too simple so we change the feature template step by step and test the accuracy on our test set. Table 1 presents the features that get the best performance during our experiments. Our template are composed by three-level feature templates: Character-level, Tag-level and Character-Tag level, the three levels of features introduce the correlation of characters and tags respectively, as well as the correlation of the character and its neighbour tags.\n\u2022 Experiment 1: No new correlation, just the correlation between each character and its neighbours.\n\u2022 Experiment 2: New correlation between the MMSEG tag of each character, and the correlation between each MMSEG tag and its neighbour.\n\u2022 Experiment 3: New trigram correlation among current character, its neighbour and its corresponding MMSEG tag.\n\u2022 Experiment 4: Breaking one trigram correlation in experiment 3 into three bigram correlations, new correlation between the current\ncharacter\u2019s neighbour and the MMSEG tag of current character\n\u2022 Experiment 5: New correlation between the previous(next) two or three character, and the current MMSEG tag\nTable2 shows the results of word segmentation experiments with different feature template files. We found that maybe more feature templates means higher precision and recall. After these experiments we consider adding more correlations, including expanding the width of correlation window to 2 or 3(Experiment 5), but the test results got setback, which indicates that our word segmentation model only need to consider unigram Markov property. Considering too much correlation will have opposite effect on the performance.\nIn the segmentation experiment, the model raised nearly 0.9 percent on both precision and recall. Finally we decided to use segmentation model with the highest F-score."}, {"heading": "4.4 Training Set", "text": "The next stage is to change the training set and find out the influence of the training set on our model. We choose the training sets from MSR and PKU because the text of these two training sets are mainly from newspaper, which is similar to the content of some social media because many social medias are informative, these social medias act like news, the only difference is that\nthey are from Internet and general news is mainly from newspapers.\nTable3 shows the results of word segmentation experiments with different training sets. From the result of experiments we can see that the model trained by the combination of these two training sets did not outperform the model of single training set and the F score got declined conversely, it is confusing indeed, and the main reason is the different segmentation standard of the two training set result in the fact that misleading during the training process, which will be discussed later."}, {"heading": "4.5 Lexicon", "text": "The final stage of our experiment is to choose an appropriate lexicon. We have known that the more words the lexicon contains, the higher ability of distinguishing words that MMSEG has, since our goal is to get high performance on the social media, we should add the Internet lexicon to our original lexicon, and we conducted one more experiment during which we add a lexicon of a particular field just for comparison.\nTable 4 shows the results of word segmentation experiments using different lexicons. The combination of the Internet lexicon has improved the overall performance of word segmentation about 0.1%, but if we just add the the lexicon of a particular field, such as the lexicon of medical science, it will have just a little positive effect on the test set, which is mainly due to the amount of overall coverage of terminology in specific areas is not high enough in our test set."}, {"heading": "4.6 Result Comparison", "text": "For reference we took a comparison between our best experiment result and other segmentation\ntools. We choose the model trained by backoff2005 MSR training set and the internet lexicon from Sougou Lab for comparison.\nWe add a column of closed test results for each tool to the table, LTP-cloud got the second rank in the the Chinese social media segmentation evaluation task in CLP 2012, indicating that the results of its closed test has great authority for social media NLP tasks. These data just represent the performance on a small test set. Our model may have a larger gap between these sophisticated tools on bigger test sets, but the results still guarantees the good adaptability of our model on Chinese social media."}, {"heading": "4.7 Error Analysis", "text": "The most surprising result of our experiments is the combination of MSR and PKU training sets. We compared the training sets of MSR and PKU and finally get the effect of different segmentation standard on the model.\nWe find there are many fundamental differences between the standards. For example, 613 is one word in the MSR training set, but is broken into 6 and 13 as two words in pku\u2019s. In addition, people\u2019s name is also quite a big issue, MSR tends to treat the whole name as a word while PKU takes the first name as a word and the last name as another one. All in all, MSR generates more words than PKU."}, {"heading": "5 Conclusion", "text": "In this paper, our research is mainly focused on the study of word segmentation for Chinese social media. One of the most applicable algorithm is the conditional random field, which is widely used in word segmentation, part of speech tagging and named entity recognition and other aspects. CRF has a large advantage in the labeling issue, and MMSEG algorithm has a comparative advantage over other algorithms on in-vocabulary words, so we use MMSEG to do segmentation first, and use the result of the MMSEG segmentation as a new\nfeature of CRF, in the experiments, we also continue to adjust the template file and exploit more correlation between Chinese characters, after several times of adjustment we got about 1.2% improvement on precision and recall over single CRF model."}], "references": [{"title": "Chinese Segmentation and New Word Detection using Conditional Random Fields", "author": ["F McCallum et al.2010] Peng", "F Feng", "A. McCallum"], "venue": "Proceedings of the 20th international conference on Computational Linguistics:562", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "An Improved Chinese word Segmentation System with Conditional Random Field", "author": ["H Zhao et al.2006] Zhao", "N Huang C", "M. Li"], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "A Conditional Random Field Word Segmenter for Sighan Bakeoff", "author": ["H Tseng et al.2005] Tseng", "P Chang", "G Andrew"], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks", "author": ["X Qian et al.2010] Qian", "Q Zhang", "Y Zhou"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Qian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2010}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Data Mining and Knowledge Discovery", "author": ["J Lafferty et al.2001] Lafferty", "A McCallum", "N. Pereira F C"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Semi-supervised learning of hidden conditional random fields for time-series classification", "author": ["M. Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2013\\E", "shortCiteRegEx": "Kim", "year": 2013}, {"title": "Natural Language Processing (Almost) from Scratch, volume 1", "author": ["J Weston", "L Bottou"], "venue": "Journal of Machine Learning", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Conditional ordinal random fields for structured ordinal-valued label prediction. Data mining and knowledge", "author": ["M. Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Minimum tag error for discriminative training of conditional random fields", "author": ["Y Xiong et al.2009] Xiong", "J Zhu", "H Huang"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2009}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["Alex Kleeman", "Christopher D"], "venue": "In Proceedings of ACL-08:HLT:959\u2013967", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of ICML 2001:282\u2013289", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Civil Transportation Event Extraction from Chinese Microblog", "author": ["J Xiong el al.2013] Xiong", "Y Hao", "Z. Huang"], "venue": "Cloud Computing and Big Data (CloudCom-Asia),", "citeRegEx": "Xiong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2013}, {"title": "Learning with probabilistic features for improved pipeline models", "author": ["Razvan", "Bunescu2008] Razvan C. Bunescu"], "venue": "InProceedings of EMNLP:670\u2013", "citeRegEx": "Razvan and Bunescu.,? \\Q2008\\E", "shortCiteRegEx": "Razvan and Bunescu.", "year": 2008}, {"title": "Chinese word segmentation: A decade review", "author": ["Huang", "Zhao2007] Changning Huang", "Hai Zhao"], "venue": "Journal of Chinese Information", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proceedings of the 16th Conference on Computational linguistics:340\u2013345", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Inducing features of random fields", "author": ["V. Della Pietra", "J. Lafferty"], "venue": "In Proceedings of the 16th Conference on Computational linguistics:380\u2013393", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Inducing features of random fields", "author": ["H. Wallach"], "venue": "In Proc. 6th Annual CLUKResearch Colloquium", "citeRegEx": "Wallach.,? \\Q2002\\E", "shortCiteRegEx": "Wallach.", "year": 2002}, {"title": "Chinese statistical parsing", "author": ["Harper", "Huang2009] Mary Harper", "Zhongqiang Huang"], "venue": null, "citeRegEx": "Harper et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Harper et al\\.", "year": 2009}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "Conditional Random Fields: An introduction", "author": ["M. Wallach] Wallach H"], "venue": "Technical Reports", "citeRegEx": "H,? \\Q2004\\E", "shortCiteRegEx": "H", "year": 2004}, {"title": "Shallow Parsing with Conditional Random Fields", "author": ["Sha", "Pereira2003] F. Sha", "F. Pereira"], "venue": "Proceedings of Human Language Technology,", "citeRegEx": "Sha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "(Collobert et al., 2011) In our approach, we combine CRF and MMSEG algorithm and extend features of traditional CRF algorithm to train the model for word segmentation, We use Internet lexicon in order to improve the performance of our model on Chinese social media.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng H et al(2005) and John Lafferty et al(2001) introduced a conditional random field se-", "startOffset": 1, "endOffset": 155}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng H et al(2005) and John Lafferty et al(2001) introduced a conditional random field se-", "startOffset": 1, "endOffset": 185}, {"referenceID": 1, "context": "lated to ours is (Zhao et al., 2006), which also used assistant algorithm and added external lexicon, while they just add the output of assistant algorithm to the feature templates.", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "CRF performs better than lexicon-based models on OOV rates because CRF introduce additional features, (Collins, 2002) which may be artificially added (Xiong et al.", "startOffset": 102, "endOffset": 117}, {"referenceID": 8, "context": "CRF performs better than lexicon-based models on OOV rates because CRF introduce additional features, (Collins, 2002) which may be artificially added (Xiong et al., 2009), including character level features and context level features, in addition, CRF also maintains the Markov characteristics of the word, (Wallach, ) thus we can remove word ambiguity by combining more features as well.", "startOffset": 150, "endOffset": 170}, {"referenceID": 3, "context": "(Qian et al., 2010)", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "(Razvan and Bunescu, 2008) At each stage, if one training material gets the best performance on test set, it will be chosen to be the final training material that we would use.", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "(Wallach, 2002)", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "(Tseng et al., 2005) We have to mention that the lexicon used by MMSEG is the Sougou lab internet lexicon published in 2006, which contains a number of high-frequency words under internet environment, and mmseg4j project lexicon file.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "(Pietra et al., 1997) Each line in the template file of PCRF denotes one template.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "The first stage of the experiment is to determine the feature vectors of our model, (Finkel et al., 2008) the original features are too simple so we change the feature template step by step and test the accuracy on our test set.", "startOffset": 84, "endOffset": 105}], "year": 2015, "abstractText": "In this paper, we propose a joint algorithm for the word segmentation on Chinese social media. Previous work mainly focus on word segmentation for plain Chinese text, in order to develop a Chinese social media processing tool, we need to take the main features of social media into account, whose grammatical structure is not rigorous, and the tendency of using colloquial and Internet terms makes the existing Chinese-processing tools inefficient to obtain good performance on social media. (Collobert et al., 2011) In our approach, we combine CRF and MMSEG algorithm and extend features of traditional CRF algorithm to train the model for word segmentation, We use Internet lexicon in order to improve the performance of our model on Chinese social media. Our experimental result on Sina Weibo shows that our approach outperforms the stateof-the-art model.", "creator": "LaTeX with hyperref package"}}}