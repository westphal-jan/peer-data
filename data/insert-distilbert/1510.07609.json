{"id": "1510.07609", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction", "abstract": "we study the problem of reducing test - time acquisition costs in existing classification systems. our goal is to learn decision rules that adaptively select sensors fitting for each example as necessary to make a baseline confident prediction. we model our system as a directed acyclic graph ( dag ) where internal nodes precisely correspond to sensor subsets and therefore decision functions at each node uniquely choose whether to acquire a new sensor cluster or classify using the available measurements. this problem can be naturally posed as an empirical risk minimization over training data. rather than jointly optimizing such a highly coupled and non - convex problem arising over replacing all decision nodes, we propose an attribute efficient algorithm motivated by dynamic programming. we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems. our approach ai is computationally efficient yet and currently has proven guarantees reductions of global convergence to the optimal system expected for a fixed model architecture. in addition, we present an extension to map other budgeted learning problems complete with large number of matching sensors to our dag architecture and demonstrate empirical performance bounds exceeding specific state - of - the - art algorithms for data composed of both few and many sensors.", "histories": [["v1", "Mon, 26 Oct 2015 19:40:10 GMT  (76kb,D)", "http://arxiv.org/abs/1510.07609v1", "To appear in NIPS 2015"]], "COMMENTS": "To appear in NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["joseph wang", "kirill trapeznikov", "venkatesh saligrama"], "accepted": true, "id": "1510.07609"}, "pdf": {"name": "1510.07609.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction", "authors": ["Joseph Wang", "Kirill Trapeznikov"], "emails": ["joewang@bu.edu", "kirill.trapeznikov@stresearch.com", "srv@bu.edu"], "sections": [{"heading": "1 Introduction", "text": "Many scenarios involve classification systems constrained by measurement acquisition budget. In this setting, a collection of sensor modalities with varying costs are available to the decision system. Our goal is to learn adaptive decision rules from labeled training data that, when presented with an unseen example, would select the most informative and cost-effective acquisition strategy for this example. In contrast, non-adaptive methods [23] attempt to identify a common sparse subset of sensors that can work well for all data. Our goal is an adaptive method that can classify typical cases using inexpensive sensors while using expensive sensors only for atypical cases.\nWe propose an adaptive sensor acquisition system learned using labeled training examples. The system, modeled as a directed acyclic graph (DAG), is composed of internal nodes, which contain decision functions, and a single sink node (the only node with no outgoing edges), representing the terminal action of stopping and classifying (SC). At each internal node, a decision function routes an example along one of the outgoing edges. Sending an example to another internal node represents acquisition of a previously unacquired sensor, whereas sending an example to the sink node indicates that the example should be classified using the currently acquired set of sensors. The goal is to learn these decision functions such that the expected error of the system is minimized subject to an expected budget constraint.\nFirst, we consider the case where the number of sensors available is small (as in [19, 22, 20]), though the dimensionality of data acquired by each sensor may be large (such as an image taken in different modalities). In this scenario, we construct a DAG that allows for sensors to be acquired in any order and classification to occur with any set of sensors. In this regime, we propose a novel algorithm to learn node decisions in the DAG by emulating dynamic programming (DP). In our approach, we decouple a complex sequential decision problem into a series of tractable cost-sensitive\nar X\niv :1\n51 0.\n07 60\n9v 1\n[ st\nat .M\nL ]\n2 6\nO ct\nlearning subproblems. Cost-sensitive learning (CSL) generalizes multi-decision learning by allowing decision costs to be data dependent [2]. Such reduction enables us to employ computationally efficient CSL algorithms for iteratively learning node functions in the DAG. In our theoretical analysis, we show that, given a fixed DAG architecture, the policy risk learned by our algorithm converges to the Bayes risk as the size of the training set grows.\nNext, we extend our formulation to the case where a large number of sensors exist, but the number of distinct sensor subsets that are necessary for classification is small (as in [24, 11] where the depth of the trees is fixed to 5). For this regime, we present an efficient subset selection algorithm based on sub-modular approximation. We treat each sensor subset as a new \u201csensor,\u201d construct a DAG over unions of these subsets, and apply our DP algorithm. Empirically, we show that our approach outperforms state-of-the-art methods in both small and large scale settings.\nRelated Work: There is an extensive literature on adaptive methods for sensor selection for reducing test-time costs. It arguably originated with detection cascades (see [25, 4] and references therein), a popular method in reducing computation cost in object detection for cases with highly skewed class imbalance and generic features. Computationally cheap features are used at first to filter out negative examples and more expensive features are used in later stages.\nOur technical approach is closely related to Trapeznikov et al. [19] and Wang et al. [22, 20]. Like us they formulate an ERM problem and generalize detection cascades to classifier cascades and trees and handle balanced and/or multiclass scenarios. Trapeznikov et al. [19] propose a similar training scheme for the case of cascades, however restrict their training to cascades and simple decision functions which require alternating optimization to learn. Alternatively, Wang et al. [21, 22, 20] attempt to jointly solve the decision learning problem by formulating a linear upper-bounding surrogate, converting the problem into a linear program (LP).\nConceptually, our work is closely related to Xu et al. [24] and Kusner et al.[11], who introduce Cost-Sensitive Trees of Classifiers (CSTC) and Approximately Submodular Trees of Classifiers (ASTC), respectively, to reducing test time costs. Like our paper they propose a global ERM problem. They solve for the tree structure, internal decision rules and leaf classifiers jointly using alternative minimization techniques. Recently, Kusner et al.[11] propose Approximately Submodular Trees of Classifiers (ASTC), a variation of CSTC which provides robust performance with significantly reduced training time and greedy approximation, respectively. Recently, Nan et al. [14] proposed random forests to efficiently learn budgeted systems using greedy approximation over large data sets.\nThe subject of this paper is broadly related to other adaptive methods in the literature. Generative methods [17, 8, 9, 6] pose the problem as a POMDP, learn conditional probability models, and myopically select features based information gain of unknown features. MDPbased methods [5, 10, 7, 3] encode current observations as state, unused features as action space, and formulate various reward func-\ntions to account for classification error and costs. He et al. [7] apply imitation learning of a greedy policy with a single classification step as actions. Dulac-Arnold et al. [5] and Karayev et al. [10] apply reinforcement learning to solve this MDP. Benbouzid et al.[3] propose classifier cascades with an additional skip action within an MDP framework. Nan et al. [15] consider a nearest neighbor approach to feature selection, with classification confidence driven by the classification margin."}, {"heading": "2 Adaptive Sensor Acquisition by DAG", "text": "In this section, we present our adaptive sensor acquisition DAG that during test-time sequentially decides which sensors should be acquired for every new example entering the system.\nBefore formally describing the system and our learning approach, we first provide a simple illustration for a 3 sensor DAG shown in Fig. 1. The state indicating acquired sensors is represented by a binary vector, with a 0 indicating that a sensor measurement has not been acquired and a 1 representing an acquisition. Consider a new example that enters the system. Initially, it has a state of [0, 0, 0]T (as do all samples during test-time) since no sensors have been acquired. It is routed to the policy function \u03c00, which makes a decision to measure one of the three sensors or to stop and classify. Let us assume that the function \u03c00 routes the example to the state [1, 0, 0]T , indicating that the first sensor is acquired.\nAt this node, the function \u03c01 has to decide whether to acquire the second sensor, acquire the third, or classifying using only the first. If \u03c01 chooses to stop and classify then this example will be classified using only the first sensor.\nSuch decision process is performed for every new example. The system adaptively collects sensors until the policy chooses to stop and classify (we assume that when all sensors have been collected the decision function has no choice but to stop and classify, as shown for \u03c07 in Fig. 1)."}, {"heading": "2.1 Problem Formulation", "text": "A data instance, x \u2208 X , consists of M sensor measurements, x = {x1, x2, . . . , xM}, and belongs to one of L classes indicated by its label y \u2208 Y = {1, 2, . . . L}. Each sensor measurement, xm, is not necessarily a scalar but may instead be multi-dimensional. Let the pair, (x, y), be distributed according to an unknown joint distribution D. Additionally, associated with each sensor measurement xm is an acquisition cost, cm.\nTo model the acquisition process, we define a state space S = {s1, . . . , sK , sSC}. The states {s1, . . . , sK} represent subsets of sensors, and the stop-and-classify state sSC represents the action of stopping and classifying with a current subset. Let Xs correspond to the space of sensor measurements in subset s. We assume that the state space includes all possible sensor subsets1, K = 2M . For example in Fig. 1, the system contains all subsets of 3 sensors. We also introduce the state transition function, T : S \u2192 S , that defines a set of actions that can be taken from the current state. A transition from the current sensor subset to a new subset corresponds to an acquisition of new sensor measurements. A transition to the state sSC corresponds to stopping and classifying using the available information. This terminal state, sSC , has access to a classifier bank which is used to predict the label of an example. Since classification has to operate on any sensor subset, there is one classifier for every sk: fs1 , . . . , fsK such that fs : Xs \u2192 Y . We assume such classifier bank is given and pre-trained. Practically, the classifiers can be either unique for each subset or a missing feature (i.e. sensor) capable classification system as in [13]. We overload notation and use node, subset of sensors, and path leading upto that subset on the DAG interchangeably. In particular we let S denote the collection of subsets of nodes. Each subset is associated with a node on the DAG graph. We refer to each node as a state since it represents the \u201cstate-of-information for an instance arriving at that node.\nNext, we define the loss associated with classifying an example/label pair (x, y) using the sensors in sj as Lsj (x, y) = 1fsj (x)6=y + \u2211 k\u2208sj ck. (1)\nUsing this convention, the loss is the sum of the empirical risk associated with classifier fsj and the cost of the sensors in the subset sj . The expected loss over the data is defined LD(\u03c0) = Ex,y\u223cD [ L\u03c0(x)(x, y) ] . (2)\nOur goal is to find a policy which adaptively selects subsets for examples such that their average loss is minimized min \u03c0\u2208\u03a0 LD(\u03c0), (3)\nwhere \u03c0 : X \u2192 S is a policy selected from a family of policies \u03a0 and \u03c0(x) is the state selected by the policy \u03c0 for example x. We denote the quantity LD as the value of (3) when \u03a0 is the family of all measurable functions. LD is the Bayes cost, representing the minimum possible cost for any function given the distribution of data. In practice, the distribution D is unknown, and instead we are given training examples (x1, y1), . . . , (xn, yn) drawn I.I.D. from D. The problem becomes an empirical risk minimization:\nmin \u03c0\u2208\u03a0 n\u2211 i=1 L\u03c0(xi)(xi, yi). (4)\nRecall that our sensor acquisition system is represented as a DAG. Each node in a graph corresponds to a state (i.e. sensor subset) in S, and the state transition function, T (sj), defines the outgoing edges from every node sj . We refer to the entire edge set in the DAG as E. In such a system, the policy \u03c0 is parameterized by the set of decision functions \u03c01, . . . , \u03c0K at every node in the DAG. Each function, \u03c0j : X \u2192 T (sj), maps an example to a new state (node) from the set specified by outgoing edges. Rather than directly minimizing the empirical risk in (4), first, we define a step-wise cost associated with all edges (sj , sk) \u2208 E\nC(x, y, sj , sk) =\n{\u2211 t\u2208sk\\sj ct if sk 6= sSC\n1fsj (x)6=y otherwise . (5)\n1While enumerating all possible combinations is feasible for small M , for large M this problem becomes intractable. We will overcome this limitation in Section 3 by applying a novel sensor selection algorithm. For now, we remain in the small M regime.\nC(\u00b7) is either the cost of acquiring new sensors or is the classification error induced by classifying with the current subset if sk = sSC . Using this step-wise cost, we define the empirical loss of the system w.r.t a path for an example x:\nR (x, y, \u03c01, ..., \u03c0K) = \u2211\n(sj ,sj+1) \u2208 path(x,\u03c01,...,\u03c0K)\nC (x, y, sj , sj+1) , (6)\nwhere path (x, \u03c01, . . . , \u03c0K) is the path on the DAG induced by the policy functions \u03c01, . . . , \u03c0K for example x. The empirical minimization equivalent to (4) for our DAG system is a sample average over all example specific path losses:\n\u03c0\u22171 , . . . , \u03c0 \u2217 K = argmin\n\u03c01,...,\u03c0K\u2208\u03a0 n\u2211 i=1 R (xi, yi, \u03c01, . . . , \u03c0K) . (7)\nNext, we present a reduction to efficiently learn the functions \u03c01, . . . , \u03c0K that minimize the empirical loss in (7)."}, {"heading": "2.2 Learning Policies in a DAG", "text": "Learning the functions \u03c01, . . . , \u03c0K that minimize the cost in (7) is a highly coupled problem. Learning a decision function \u03c0j is dependent on the other functions in two ways: (a) \u03c0j is dependent on functions at nodes downstream (nodes for which a path exists from \u03c0j), as these determine the cost of each action taken by \u03c0j on an individual example (the cost-to-go), and (b) \u03c0j is dependent on functions at nodes upstream (nodes for which a path exists to \u03c0j), as these determine the distribution of examples that \u03c0j acts on. Consider a policy \u03c0j at a node corresponding to state sj such that all outgoing edges from j lead to leaves. Also, we assume all examples pass through this node \u03c0j (we are ignoring the effect of upstream dependence b). This yields the following important lemma: Lemma 2.1. Given the assumptions above, the problem of minimizing the risk in (6) w.r.t a single policy function, \u03c0j , is equivalent to solving a k-class cost sensitive learning (CSL) problem.2\nProof. Consider the risk in (6) with \u03c0j such that all outgoing edges from j lead to a leaf. Ignoring the effect of other policy functions upstream from j, the risk w.r.t \u03c0j is:\nR(x, y, \u03c0j) = \u2211\nsk\u2208T (sj)\nC(x, y, sj , sk)1\u03c0j(x)=sk \u2192 min \u03c0\u2208\u03a0 n\u2211 i=1 R(xi, yi, \u03c0j).\nMinimizing the risk over training examples yields the optimization problem on the right hand side. This is equivalent to a CSL problem over the space of \u201clabels\u201d T (sj) with costs given by the transition costs C(x, y, sj , sk).\nAlgorithm 1 Graph Reduce Algorithm Input: Data: (xi, yi)ni=1, DAG: (nodes S, edges E, costs C(xi, yi, e),\u2200e \u2208 E), CSL alg: Learn ((x1, ~w1), . . . , (xn, ~wn)))\u2192 \u03c0(\u00b7) while Graph S is NOT empty do\n(1) Choose a node, j \u2208 S, s.t. all children of j are leaf nodes for example i \u2208 {1, . . . , n} do\n(2) Construct the weight vector ~wi of edge costs per action. end for (3) \u03c0j \u2190 Learn ((x1, ~w1), . . . , (xn, ~wn)) (4) Evaluate \u03c0j and update edge costs to node j: C(xi, yi, sn, sj)\u2190 ~wji (\u03c0j(xi)) + C(xi, yi, sn, sj) (5) Remove all outgoing edges from node j in E (6) Remove all disconnected nodes from S.\nend while Output: Policy functions, \u03c01, . . . , \u03c0K\nIn order to learn the policy functions \u03c01, . . . , \u03c0K , we propose Algorithm 1, which iteratively learns policy functions using Lemma 2.1. We solve the CSL problem by using a filter-tree scheme [2] for Learn, which constructs a tree of binary classifiers. Each binary classifier can be trained using regularized risk minimization. For concreteness we define the Learn algorithm as\nLearn ((x1, ~w1), . . . , (xn, ~wn))\n, FilterTree((x1, ~w1), . . . , (xn, ~wn)), (8)\nwhere the binary classifiers in the filter tree are trained using an appropriately regularized calibrated convex loss function. Note that multiple schemes exist that map the CSL problem to binary classification.\nA single iteration of Algorithm 1 proceeds as follows: (1) A node j is chosen whose outgoing edges connect only to leaf nodes. (2) The costs associated with each connected leaf node are found. (3) The policy \u03c0j is trained on the entire\n2We consider the k-class CSL problem formulated by Beygelzimer et al. [2], where an instance of the problem is defined by a distribution D over X \u00d7 [0, inf)k, a space of features and associated costs for predicting each of the k labels for each realization of features. The goal is to learn a function which maps each element of X to a label {1, . . . , k} s.t. the expected cost is minimized.\nset of training data according to these costs by solving a CSL problem. (4) The costs associated with taking the action \u03c0j are computed for each example, and the costs of moving to state j are updated. (5) Outgoing edges from node j are removed (making it a leaf node), and (6) disconnected nodes (that were previously connected to node j) are removed. The algorithm iterates through these steps until all edges have been removed. We denote the policy functions trained on the empirical data using Alg. 1 as \u03c0n1 , . . . , \u03c0 n K ."}, {"heading": "2.3 Analysis", "text": "Our goal is to show that the expected risk of the policy functions \u03c01, . . . , \u03c0K learned by Alg. 1 converge to the Bayes risk. We first state our main result:\nTheorem 2.2. Alg. 1 is universally consistent, that is lim n\u2192\u221e LD(\u03c0n1 , . . . , \u03c0nK)\u2192 LD (9)\nwhere \u03c0n1 , . . . , \u03c0 n K are the policy functions learned using Alg. (1), which in turn uses Learn described by Eq. 8.\nAlg. 1 emulates a dynamic program applied in an empirical setting. Policy functions are decoupled and trained from leaf to root conditioned on the output of descendant nodes.\nTo adapt to the empirical setting, we optimize at each stage over all examples in the training set. The key insight is the fact that universally consistent learners output optimal decisions over subsets of the space of data, that is they are locally optimal. To illustrate this point, consider a standard classification problem. Let X \u2032 \u2282 X be the support (or region) of examples induced by upstream deterministic decisions. d\u2217 and f\u2217, Bayes optimal classifiers w.r.t the full space and subset, respectively, are equal on the reduced support:\nd\u2217(x) = arg min d E [ 1d(x) 6=y|x ] = f\u2217(x) = arg min f E [ 1f(x)6=y|x, x \u2208 X \u2032 \u2282 X ] \u2200 x \u2208 X \u2032.\nFrom this insight, we decouple learning problems while still training a system that converges to the Bayes risk. This can be achieved by training universally consistent CSL algorithms such as filter trees [2] that reduce the problem to binary classification. By learning consistent binary classifiers [1, 18], the risk of the cost-sensitive function can be shown to converge to the Bayes risk [2]. Proof. (Theorem 2.2) The proof can be broken down into two steps. First, we show that training the policy function with no downstream policy functions decouples from other policies. Next, we show that sequentially learning policy functions from leaf to root leads to an optimal policy.\nConsider first the node associated with state sj whose outgoing edges lead to leaves. Alg. 1 trains the policy \u03c0nj over the entire training set using (8). As (8) is a universally consistent algorithm, the \u03c0nj converges to the optimal policy as the data grows:\nlim n\u2192\u221e\nEx,y,\u223cD [ C(x, y, sj , \u03c0 n j (x)) ] \u2192 inf{Ex,y,\u223cD [ C(x, y, sj , \u03c0 \u2217 j (x)) ] |\u03c0\u2217j : X \u2192 S}\nwhere the infimum is over any measurable function \u03c0\u2217j . As this infimum is over any measurable function, we point out that this convergence holds for any realization x \u2208 X\nEy\u223cD(x) [ C(x, y, sj , \u03c0 n j (x))|x ] \u2192 inf{Ey\u223cD(x) [ C(x, y, sj , \u03c0 \u2217 j (x))|x ] |\u03c0\u2217j : X \u2192 S}\nwhere D(x) is the distribution of y conditioned on x. As the outgoing edges of node sj contain only leaves, other policy functions \u03c0n1 , . . . , \u03c0 n j\u22121, \u03c0 n j+1, . . . , \u03c0 n K do not affect the conditional distributionD(x). Instead, they only reduce the support of X observed by \u03c0nj , and therefore \u03c0nj converges to the Bayes optimal function independent of the other policy functions, and therefore the learned policy \u03c0nj is fixed.\nAlg. 1 updates the edge costs (costs-to-go) of edges directed to sj . These costs-to-go do not vary as we train new policy functions in ancestor nodes of sj and these values can be viewed as fixed when training the next policy function, \u03c0nk . The dependence on \u03c0 n j is well captured when learning \u03c0 n k . As \u03c0 n j converges to the Bayes optimal function, the costs-to-go converge to the Bayes optimal values, \u03c0nk is trained on the Bayes optimal costs-to-go as n\u2192\u221e. By recursion, this implies that every decision function is learned on costs-to-go approaching the Bayes optimal, and therefore each function in the learned decision functions approach the point-wise Bayesian optimal decision. Consequently, the learned system approaches the Bayesian optimal system.\nComputational Efficiency: Alg. 1 reduces the problem to solving a series ofO(KM) binary classification problems, where K is the number of nodes in the DAG and M is the number of sensors. Finding each binary classifier is computationally efficient, as it reduces to solving a convex problem with O(n) variables. In contrast, nearly all previous approaches require solving a non-convex problem and resort to alternating optimization [24, 19] or greedy approximation [11]. Alternatively, convex surrogates proposed for the global problem [22, 20] require solving large convex programs with \u03b8(n) variables, even for simple linear decision functions. Furthermore, existing off-the-shelf algorithms cannot be applied to train these systems, often leading to less efficient implementation in practice."}, {"heading": "2.4 Generalization to Other Budgeted Learning Problems", "text": "Although, we presented our algorithm in the context of supervised classification and a uniform linear sensor acquisition cost structure, the above framework holds for a wide range of problems. In particular, any loss-based learning problem can be solved using the proposed DAG approach by generalizing the cost function\nC\u0303(x, y, sj , sk) = { c(x, y, sj , sk) if sk 6= sSC D (x, y, sj) otherwise , (10)\nwhere c(x, y, sj , sk) is the cost of acquiring sensors in sk\\sj for example (x, y) given the current state sj and D (x, y, sj) is some loss associated with applying sensor subset sj to example (x, y). This framework allows for significantly more complex budgeted learning problems to be handled. For example, the sensor acquisition cost, c(x, y, sj , sk), can be object dependent and non-linear, such as increasing acquisition costs as time increases (which can arise in image retrieval problems, where users are less likely to wait as time increases). The cost D (x, y, sj) can include alternative costs such as `2 error in regression, precision error in ranking, or model error in structured learning. As in the supervised learning case, the learning functions and example labels do not need to be explicitly known. Instead, the system requires only empirical performance to be provided, allowing complex decision systems (such as humans) to be characterized or systems learned where the classifiers and labels are sensitive information."}, {"heading": "3 Adaptive Sensor Acquisition in High-Dimensions", "text": "So far, we considered the case where the DAG system allows for any subset of sensors to be acquired, however this is often computationally intractable as the number of nodes in the graph grows exponentially with the number of sensors. In practice, these complete systems are only feasible for data generated from a small set of sensors ( 10 or less).\n3.1 Learning Sensor Subsets\nAlthough constructing an exhaustive DAG for data with a large number of sensors is computationally intractable, in many cases this is unnecessary. Motivated by previous methods [6, 24, 11], we assume that the number of \u201cactive\u201d nodes in the exhaustive graph is small, that is these nodes are either not visited by any examples or all examples that visit the node acquire the same next sensor. Equivalently, this can be viewed as the system needing only a small number of sensor subsets to classify all examples with low acquisition cost.\nRather than attempt to build the entire combinatorially sized graph, we instead use this assumption to first find these \u201cactive\u201d subsets of sensors and construct a DAG to choose between unions of these subsets. The step of finding these sensor subsets can be viewed as a form of feature clustering, with a goal of grouping features that are jointly useful for classification. By doing so, the size of the DAG is reduced from exponential in the number of sensors, 2M , to exponential in a\nmuch smaller user chosen parameter number of subsets, 2t. In experimental results, we limit t = 8, which allows for a diverse subsets of sensors to be found while preserving computational tractability and efficiency.\nOur goal is to learn sensor subsets with high classification performance and low acquisition cost: subsets of sensors with empirically low cost as defined in (1). Ideally, our goal is to jointly learn the subsets which minimize the empirical risk of the entire system as defined in (4), however this presents a computationally intractable problem due\nto the exponential search space. Rather than attempt to solve this difficult problem directly, we minimize classification error over a collection of sensor subsets \u03c31, . . . , \u03c3t subject to a cost constraint on the total number of sensors used. We decouple the problem from the policy learning problem by assuming that each example is classified by the best possible subset. For a constant sensor cost, the problem can be expressed as a set constraint problem:\nmin \u03c31,...,\u03c3t\n1\nN N\u2211 i=1 min j\u2208{1,...,t} [ 1f\u03c3j (xi)6=yi ] such that: t\u2211 j=1 |\u03c3j | \u2264 B \u03b4 , (11)\nwhere B is the total sensor budget over all sensor subsets and \u03b4 is the cost of a single sensor.\nAlthough minimizing this loss is still computationally intractable, consider instead the equivalent problem of maximizing the \u201creward\u201d (the event of a correct classification) of the subsets, defined as\nG = N\u2211 i=1 max j\u2208{1,...,t} [ 1f\u03c3j (xi)=yi ] \u2192 max \u03c31,...,\u03c3t 1 N G(c1, . . . , ct) such that: t\u2211 j=1 |\u03c3j | \u2264 B \u03b4 . (12)\nThis problem is related to the knapsack problem with a non-linear objective. Maximizing the reward in (12) is still a computationally intractable problem, however the reward function is structured to allow for efficient approximation. Lemma 3.1. The objective of the maximization in (12) is sub-modular with respect to the set of subsets, such that adding any new set to the reward yields diminishing returns.\nProof. This follows directly from the fact that maximization over a set of objects is a submodular function.\nTheorem 3.2. Given that the empirical risk of each classifier f\u03c3k is submodular and monotonically decreasing w.r.t. the elements in \u03c3k and uniform sensor costs, the strategy in Alg. 2 is an O(1) approximation of the optimal reward in (12).\nProof. Consider adding a sensor k to any subset \u03c3j . By assumption, the empirical risk of each classifier is monotonically decreasing and therefore the reward is monotonically increasing. Additionally, note that the reward for any training point xi using \u03c3j is less than the reward from using \u03c3j \u222a k and therefore the objective is equal to the objective without replacement of \u03c3j by \u03c3j \u222a k:\nG(c1, . . . , cj\u22121, cj \u222a k, . . . , cK) = G(c1, . . . , cj , cj \u222a k, . . . , cK). As a result, we can view adding a sensor to a subset as adding an entirely new subset without changing the objective in (12). From the above lemma, adding a new subset results in a submodular function, and therefore the reward in (12) is submodular with respect to adding sensors to each subset. Applying a greedy strategy therefore yields a 1\u2212 1e approximation of the optimal strategy [16].\n3.2 Constructing DAG using Sensor Subsets Algorithm 2 Sensor Subset Selection\nInput: Number of Subsets t, Cost Constraint B\u03b4 Output: Feature subsets, \u03c31, . . . , \u03c3t Initialize: \u03c31, . . . , \u03c3t = \u2205 (i, j) = argmaxi\u2208{1,...,t} argmaxj\u2208\u03c3Ci G(\u03c31, ..., \u03c3i \u222a j, ..., \u03c3t) while \u2211T j=1 |\u03c3j | \u2264 C \u03b4 do\n\u03c3i = \u03c3i \u222a j (i, j) = argmaxi\u2208{1,...,t} argmaxj\u2208\u03c3Ci G(\u03c31, ..., \u03c3i\u222aj, ..., \u03c3t)\nend while\nAlg. 2 requires computation of the reward G for only O ( B \u03b4 tM ) sensor subsets, where M is the number of sensors, to return a constant-order approximation to the NP-hard knapsack-type problem. Given the set of sensor subsets \u03c31, . . . , \u03c3t, we can now construct a DAG using all possible unions of these subsets, where each sensor subset \u03c3j is treated as a new single sensor, and apply the small scale system presented in Sec. 2. The result is an efficiently learned system with relatively low complexity yet strong performance/cost trade-off. Additionally, this result can be extended to the case of non-uniform costs, where a simple extension of the greedy algorithm yields a constant-order approximation [12].\nA simple case where three subsets are used is shown in Fig. 2. The three learned subsets of sensors are shown on the bottom left of Fig. 2, and these three subsets are then used to construct the entire DAG in the same fashion as in Fig. 1. At each stage, the state is represented by the union of sensor subsets acquired. Grouping the sensors in this fashion reduces the size of the graph to 8 nodes as opposed to 64 nodes required if any subset of the 6 sensors can be selected. This approach allows us to map high-dimensional adaptive sensor selection problems to small scale DAG in Sec. 2."}, {"heading": "4 Experimental Results", "text": "To demonstrate the performance of our DAG sensor acquisition system, we provide experimental results on data sets previously used in budgeted learning. Three data sets previously used for budget cascades [19, 22] are tested. In these data sets, examples are composed of a small number of sensors (under 4 sensors). To accurately compare performance, we apply the LP approach to learning sensor trees [20] and construct trees containing all subsets of sensors as opposed to the fixed order cascades previously applied [19, 22].\nNext, we examine performance of the DAG system using 3 higher dimensional sets of data previously used to compare budgeted learning performance [11]. In these cases, the dimensionality of the data (between 50 and 400 features) makes exhaustive subset construction computationally infeasible. We greedily construct sensor subsets using Alg. 2, then learn a DAG over all unions of these sensor subsets. We compare performance with CSTC [24] and ASTC [11].\nFor all experiments, we use cost sensitive filter trees [2], where each binary classifier in the tree is learned using logistic regression. Homogeneous polynomials are used as decision functions in the filter trees. For all experiments, uniform sensor cost were were varied in the range [0,M ] achieve systems with different budgets. Performance between the systems is compared by plotting the average number of features acquired during test-time vs. the average test error."}, {"heading": "4.1 Small Sensor Set Experiments", "text": "We compare performance of our trained DAG with that of a complete tree trained using an LP surrogate [20] on the landsat, pima, and letter datasets. To construct each sensor DAG, we include all subsets of sensors (including the empty set) and connect any two nodes differing by a single sensor, with the edge directed from the smaller sensor subset to the larger sensor subset. By including the empty set, no initial sensor needs to be selected. 3rd-order homogeneous polynomials are used for both the classification and system functions in the LP and DAG.\nAs seen in Fig. 3, the systems learned with a DAG outperform the LP tree systems. Additionally, the performance of both of the systems is significantly better than previously reported performance on these data sets for budget cascades [19, 22]. This arises due to both the higher complexity of the classifiers and decision functions as well as the flexibility of sensor acquisition order in the DAG and LP tree compared to cascade structures. For this setting, it appears that the DAG approach is superior approach to LP trees for learning budgeted systems."}, {"heading": "4.2 Large Sensor Set Experiments", "text": "Next, we compare performance of our trained DAG with that of CSTC [24] and ASTC [11] for the MiniBooNE, Forest, and CIFAR datasets. We use the validation data to find the homogeneous polynomial that gives the best classification performance using all features (MiniBooNE: linear, Forest: 2nd order, CIFAR: 3rd order). These polynomial functions are then used for all classification and policy functions. For each data set, Alg. 2 was used to find 7 subsets, with an 8th subset of all features added. An exhaustive DAG was trained over all unions of these 8 subsets.\nFig. 4 shows performance comparing the average cost vs. average error of CSTC, ASTC, and our DAG system. The systems learned with a DAG outperform both CSTC and ASTC on the MiniBooNE and Forest data sets, with comparable performance on CIFAR at low budgets and superior performance at higher budgets."}], "references": [{"title": "Convexity, Classification, and Risk Bounds", "author": ["P. Bartlett", "M. Jordan", "J. Mcauliffe"], "venue": "Journal of American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Multiclass classification with filter trees", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Classifier cascade: Tradeoff between accuracy and feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "O. Chapelle", "D. Kedem"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Datum-wise classification: a sequential approach to sparsity", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Preux", "P. Gallinari"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Imitation learning by coaching", "author": ["H. He", "H. Daume III", "J. Eisner"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Cost-sensitive feature acquisition and classification", "author": ["S. Ji", "L. Carin"], "venue": "Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Prediction-time active feature-value acquisition for cost-effective customer targeting", "author": ["P. Kanani", "P. Melville"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Dynamic feature selection for classification on a budget", "author": ["S. Karayev", "M. Fritz", "T. Darrell"], "venue": "In International Conference on Machine Learning: Workshop on Prediction with Sequential Models,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M. Kusner", "W. Chen", "Q. Zhou", "Z. Xu", "K. Weinberger", "Y. Chen"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Cost-effective outbreak detection in networks", "author": ["J. Leskovec", "A. Krause", "C. Guestrin", "C. Faloutsos", "J. VanBriesen", "N. Glance"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Learning with marginalized corrupted features", "author": ["L. Maaten", "M. Chen", "S. Tyree", "K.Q. Weinberger"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Feature-budgeted random forest", "author": ["F. Nan", "J. Wang", "V. Saligrama"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast margin-based cost-sensitive classification", "author": ["F. Nan", "J. Wang", "K. Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "An analysis of approximations for maximizing submodular set functionsi", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1978}, {"title": "Feature value acquisition in testing: A sequential batch test algorithm", "author": ["V.S. Sheng", "C.X. Ling"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Consistency of support vector machines and other regularized kernel classifiers", "author": ["I. Steinwart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Supervised sequential classification under budget constraints", "author": ["K. Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T.K. Bolukbasi", "Trapeznikov", "V. Saligrama"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "J. Saligrama"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "An lp for sequential learning under budgets", "author": ["J. Wang", "K. Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Z. Xu", "O. Chapelle", "K. Weinberger"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Cost-sensitive tree of classifiers", "author": ["Z. Xu", "M. Kusner", "M. Chen", "K. Weinberger"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A Survey of Recent Advances in Face Detection", "author": ["C. Zhang", "Z. Zhang"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 22, "context": "In contrast, non-adaptive methods [23] attempt to identify a common sparse subset of sensors that can work well for all data.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "First, we consider the case where the number of sensors available is small (as in [19, 22, 20]), though the dimensionality of data acquired by each sensor may be large (such as an image taken in different modalities).", "startOffset": 82, "endOffset": 94}, {"referenceID": 21, "context": "First, we consider the case where the number of sensors available is small (as in [19, 22, 20]), though the dimensionality of data acquired by each sensor may be large (such as an image taken in different modalities).", "startOffset": 82, "endOffset": 94}, {"referenceID": 19, "context": "First, we consider the case where the number of sensors available is small (as in [19, 22, 20]), though the dimensionality of data acquired by each sensor may be large (such as an image taken in different modalities).", "startOffset": 82, "endOffset": 94}, {"referenceID": 1, "context": "Cost-sensitive learning (CSL) generalizes multi-decision learning by allowing decision costs to be data dependent [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 23, "context": "Next, we extend our formulation to the case where a large number of sensors exist, but the number of distinct sensor subsets that are necessary for classification is small (as in [24, 11] where the depth of the trees is fixed to 5).", "startOffset": 179, "endOffset": 187}, {"referenceID": 10, "context": "Next, we extend our formulation to the case where a large number of sensors exist, but the number of distinct sensor subsets that are necessary for classification is small (as in [24, 11] where the depth of the trees is fixed to 5).", "startOffset": 179, "endOffset": 187}, {"referenceID": 24, "context": "It arguably originated with detection cascades (see [25, 4] and references therein), a popular method in reducing computation cost in object detection for cases with highly skewed class imbalance and generic features.", "startOffset": 52, "endOffset": 59}, {"referenceID": 3, "context": "It arguably originated with detection cascades (see [25, 4] and references therein), a popular method in reducing computation cost in object detection for cases with highly skewed class imbalance and generic features.", "startOffset": 52, "endOffset": 59}, {"referenceID": 18, "context": "[19] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[22, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19] propose a similar training scheme for the case of cascades, however restrict their training to cascades and simple decision functions which require alternating optimization to learn.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21, 22, 20] attempt to jointly solve the decision learning problem by formulating a linear upper-bounding surrogate, converting the problem into a linear program (LP).", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "[21, 22, 20] attempt to jointly solve the decision learning problem by formulating a linear upper-bounding surrogate, converting the problem into a linear program (LP).", "startOffset": 0, "endOffset": 12}, {"referenceID": 19, "context": "[21, 22, 20] attempt to jointly solve the decision learning problem by formulating a linear upper-bounding surrogate, converting the problem into a linear program (LP).", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[24] and Kusner et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], who introduce Cost-Sensitive Trees of Classifiers (CSTC) and Approximately Submodular Trees of Classifiers (ASTC), respectively, to reducing test time costs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] propose Approximately Submodular Trees of Classifiers (ASTC), a variation of CSTC which provides robust performance with significantly reduced training time and greedy approximation, respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed random forests to efficiently learn budgeted systems using greedy approximation over large data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Generative methods [17, 8, 9, 6] pose the problem as a POMDP, learn conditional probability models, and myopically select features based information gain of unknown features.", "startOffset": 19, "endOffset": 32}, {"referenceID": 7, "context": "Generative methods [17, 8, 9, 6] pose the problem as a POMDP, learn conditional probability models, and myopically select features based information gain of unknown features.", "startOffset": 19, "endOffset": 32}, {"referenceID": 8, "context": "Generative methods [17, 8, 9, 6] pose the problem as a POMDP, learn conditional probability models, and myopically select features based information gain of unknown features.", "startOffset": 19, "endOffset": 32}, {"referenceID": 5, "context": "Generative methods [17, 8, 9, 6] pose the problem as a POMDP, learn conditional probability models, and myopically select features based information gain of unknown features.", "startOffset": 19, "endOffset": 32}, {"referenceID": 4, "context": "MDPbased methods [5, 10, 7, 3] encode current observations as state, unused features as action space, and formulate various reward functions to account for classification error and costs.", "startOffset": 17, "endOffset": 30}, {"referenceID": 9, "context": "MDPbased methods [5, 10, 7, 3] encode current observations as state, unused features as action space, and formulate various reward functions to account for classification error and costs.", "startOffset": 17, "endOffset": 30}, {"referenceID": 6, "context": "MDPbased methods [5, 10, 7, 3] encode current observations as state, unused features as action space, and formulate various reward functions to account for classification error and costs.", "startOffset": 17, "endOffset": 30}, {"referenceID": 2, "context": "MDPbased methods [5, 10, 7, 3] encode current observations as state, unused features as action space, and formulate various reward functions to account for classification error and costs.", "startOffset": 17, "endOffset": 30}, {"referenceID": 6, "context": "[7] apply imitation learning of a greedy policy with a single classification step as actions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and Karayev et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] apply reinforcement learning to solve this MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] propose classifier cascades with an additional skip action within an MDP framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] consider a nearest neighbor approach to feature selection, with classification confidence driven by the classification margin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Let us assume that the function \u03c00 routes the example to the state [1, 0, 0] , indicating that the first sensor is acquired.", "startOffset": 67, "endOffset": 76}, {"referenceID": 12, "context": "sensor) capable classification system as in [13].", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "We solve the CSL problem by using a filter-tree scheme [2] for Learn, which constructs a tree of binary classifiers.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "[2], where an instance of the problem is defined by a distribution D over X \u00d7 [0, inf), a space of features and associated costs for predicting each of the k labels for each realization of features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This can be achieved by training universally consistent CSL algorithms such as filter trees [2] that reduce the problem to binary classification.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "By learning consistent binary classifiers [1, 18], the risk of the cost-sensitive function can be shown to converge to the Bayes risk [2].", "startOffset": 42, "endOffset": 49}, {"referenceID": 17, "context": "By learning consistent binary classifiers [1, 18], the risk of the cost-sensitive function can be shown to converge to the Bayes risk [2].", "startOffset": 42, "endOffset": 49}, {"referenceID": 1, "context": "By learning consistent binary classifiers [1, 18], the risk of the cost-sensitive function can be shown to converge to the Bayes risk [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 23, "context": "In contrast, nearly all previous approaches require solving a non-convex problem and resort to alternating optimization [24, 19] or greedy approximation [11].", "startOffset": 120, "endOffset": 128}, {"referenceID": 18, "context": "In contrast, nearly all previous approaches require solving a non-convex problem and resort to alternating optimization [24, 19] or greedy approximation [11].", "startOffset": 120, "endOffset": 128}, {"referenceID": 10, "context": "In contrast, nearly all previous approaches require solving a non-convex problem and resort to alternating optimization [24, 19] or greedy approximation [11].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Alternatively, convex surrogates proposed for the global problem [22, 20] require solving large convex programs with \u03b8(n) variables, even for simple linear decision functions.", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "Alternatively, convex surrogates proposed for the global problem [22, 20] require solving large convex programs with \u03b8(n) variables, even for simple linear decision functions.", "startOffset": 65, "endOffset": 73}, {"referenceID": 5, "context": "Motivated by previous methods [6, 24, 11], we assume that the number of \u201cactive\u201d nodes in the exhaustive graph is small, that is these nodes are either not visited by any examples or all examples that visit the node acquire the same next sensor.", "startOffset": 30, "endOffset": 41}, {"referenceID": 23, "context": "Motivated by previous methods [6, 24, 11], we assume that the number of \u201cactive\u201d nodes in the exhaustive graph is small, that is these nodes are either not visited by any examples or all examples that visit the node acquire the same next sensor.", "startOffset": 30, "endOffset": 41}, {"referenceID": 10, "context": "Motivated by previous methods [6, 24, 11], we assume that the number of \u201cactive\u201d nodes in the exhaustive graph is small, that is these nodes are either not visited by any examples or all examples that visit the node acquire the same next sensor.", "startOffset": 30, "endOffset": 41}, {"referenceID": 15, "context": "Applying a greedy strategy therefore yields a 1\u2212 1e approximation of the optimal strategy [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "Additionally, this result can be extended to the case of non-uniform costs, where a simple extension of the greedy algorithm yields a constant-order approximation [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 18, "context": "Three data sets previously used for budget cascades [19, 22] are tested.", "startOffset": 52, "endOffset": 60}, {"referenceID": 21, "context": "Three data sets previously used for budget cascades [19, 22] are tested.", "startOffset": 52, "endOffset": 60}, {"referenceID": 19, "context": "To accurately compare performance, we apply the LP approach to learning sensor trees [20] and construct trees containing all subsets of sensors as opposed to the fixed order cascades previously applied [19, 22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "To accurately compare performance, we apply the LP approach to learning sensor trees [20] and construct trees containing all subsets of sensors as opposed to the fixed order cascades previously applied [19, 22].", "startOffset": 202, "endOffset": 210}, {"referenceID": 21, "context": "To accurately compare performance, we apply the LP approach to learning sensor trees [20] and construct trees containing all subsets of sensors as opposed to the fixed order cascades previously applied [19, 22].", "startOffset": 202, "endOffset": 210}, {"referenceID": 10, "context": "Next, we examine performance of the DAG system using 3 higher dimensional sets of data previously used to compare budgeted learning performance [11].", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "We compare performance with CSTC [24] and ASTC [11].", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "We compare performance with CSTC [24] and ASTC [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "For all experiments, we use cost sensitive filter trees [2], where each binary classifier in the tree is learned using logistic regression.", "startOffset": 56, "endOffset": 59}, {"referenceID": 19, "context": "We compare performance of our trained DAG with that of a complete tree trained using an LP surrogate [20] on the landsat, pima, and letter datasets.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Additionally, the performance of both of the systems is significantly better than previously reported performance on these data sets for budget cascades [19, 22].", "startOffset": 153, "endOffset": 161}, {"referenceID": 21, "context": "Additionally, the performance of both of the systems is significantly better than previously reported performance on these data sets for budget cascades [19, 22].", "startOffset": 153, "endOffset": 161}, {"referenceID": 23, "context": "Next, we compare performance of our trained DAG with that of CSTC [24] and ASTC [11] for the MiniBooNE, Forest, and CIFAR datasets.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "Next, we compare performance of our trained DAG with that of CSTC [24] and ASTC [11] for the MiniBooNE, Forest, and CIFAR datasets.", "startOffset": 80, "endOffset": 84}], "year": 2015, "abstractText": "We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.", "creator": "LaTeX with hyperref package"}}}