{"id": "1510.08520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2015", "title": "Learning with $\\ell^{0}$-Graph: $\\ell^{0}$-Induced Sparse Subspace Clustering", "abstract": "$ \\ ell ^ { 1 } $ - query graph \\ cite { yanw09, chengyyfh10 }, a sparse oriented graph built by reconstructing each nodes datum with all the other data using augmented sparse representation, has been demonstrated to be notably effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn. it is well known externally that $ \\ ell ^ { 1 } $ - norm that used in $ \\ ell ^ { 3 1 } $ - graph matching is a convex relaxation dimension of $ \\ ell ^ { { 0 } $ - norm for enforcing the sparsity. in order endeavour to more handle general cases when the subspaces are not independent and follow the original principle of sparse information representation, we propose preparing a further novel $ \\ ell ^ { ch 0 } $ - graph that employs $ \\ ell ^ { 0 } $ - norm efficiently to encourage the sparsity assumption of the constructed graph, and develop a proximal method seeking to solve the associated optimization problem with the proved guarantee of convergence. extensive experimental results on proving various data sets demonstrate the superiority of $ \\ ell ^ { 0 } $ - graph successfully compared to other competing topological clustering methods including $ \\ ell ^ { yes 1 } $ - graph.", "histories": [["v1", "Wed, 28 Oct 2015 22:48:09 GMT  (233kb)", "http://arxiv.org/abs/1510.08520v1", null], ["v2", "Wed, 18 Nov 2015 07:11:42 GMT  (233kb)", "http://arxiv.org/abs/1510.08520v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yingzhen yang", "jiashi feng", "jianchao yang", "thomas s huang"], "accepted": false, "id": "1510.08520"}, "pdf": {"name": "1510.08520.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n08 52\n0v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\n01 5"}, {"heading": "1. Introduction", "text": "Clustering is a common unsupervised data analysis method which partitions data into a set of self-similar clusters. The obtained data clusters always play an important role in solving various machine learning and computer vision problems by the disclosed grouping patters in the original data. Most clustering algorithms can be categorized into two classes: similarity-based and model-based clustering methods. With Gaussian Mixture Model (GMM) as a representative, model-based clustering methods typically model the data by a mixture of parametric distributions, and the parameters of the distributions are estimated via fitting a statistical model to the data [10]. However, high dimensionality always impose difficulty on the parameter estimation, and the fact that the used parametric distribution may not match the underlying true distribution of the data further restricts the application of model-based methods to complex data.\nIn contrast, similarity-based clustering methods segment the data based on the similarity function, and they alleviate the difficult problem of parameter estimation in case of high dimensionality. For example, K-means [6] groups similar data together by a local minimum of sum of within-cluster dissimilarities. Affinity Propagation [11] uses the same\nprinciple and automatically determines the cluster number. Spectral Clustering [16] identifies clusters of complex shapes lying on some low dimensional manifolds by spectral embedding. Among various similarity-based clustering methods, graph-based methods [18] are important wherein the edge weight of the graph serving as the data similarity, and sparse graphs (where only a few edges have non-zero weights for each vertex) are demonstrated to be especially effective for clustering high dimensional data. Examples of sparse graph methods include \u21131-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation. \u21131-graph and sparse subspace clustering have been shown to be robust to noise and capable of producing superior results for high dimensional data, compared to K-means and spectral clustering. \u21131-graph is further extended to incorporate local manifold structure of the data in [21, 22].\nTo avoid the non-convex optimization problem incurred by \u21130-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces \u21130-norm with \u21131-norm so as to solve a convex optimization problem. In addition, \u21131-norm has been widely used as a convex relaxation of \u21130-norm for efficient sparse coding algorithms [12, 13, 14]. [9] points out that in case that the data are drawn from linear independent subspaces, sparse representation by \u21131-norm can recover the underlying subspaces.\nOn the other hand, sparse representation methods [15] that directly optimize objective function involving \u21130-norm demonstrate compelling performance compared to its \u21131norm counterpart. In order to deal with general cases when the subspaces are not independent of each other and follow the original principle of sparse representation by \u21130norm, we propose a new sparse graph called \u21130-graph which employs \u21130-norm to enforce the sparsity of the graph, and develop a proximal method to optimize the associated objective function with convergence guarantee. The proximal method is inspired by the proximal linearized method in [3].\nThe remaining parts of the paper are organized as follows. Sparse coding and \u21131-graph are introduced in the next subsection, and then the detailed formulation of \u21130-graph is illustrated. We then show the clustering performance of \u21130graph, and conclude the paper. We use bold upper letters for matrices and vectors, and regular lower letter for scalars\n1\nthroughout this paper."}, {"heading": "1.1. Sparse Coding and \u21131-Graph for Clustering", "text": "Sparse coding methods represent an input signal by a linear combination of only a few atoms of a dictionary which is usually over-complete, and the sparse coefficients are named sparse code. Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering. Denote the data by X = [x1,x2, . . . ,xn] which lie in the d-dimensional Euclidean space IRd, and let the dictionary matrix be D = [d1,d2, . . . ,dp] \u2208 IRd\u00d7p with each dm (m = 1, . . . , p) being the atom or the basis vector of the dictionary. Sparse coding method searches for the linear sparse representation with respect to the dictionary D for each vector xi. Sparse coding is performed by solving the following convex optimization problem:\n\u03b1 i = argmin\n\u03b1 i \u2016xi \u2212D\u03b1 i\u201622 + \u03bb||\u03b1 i||1 i = 1, . . . , n\n(1) where \u03bb is a weighting parameter for the sparsity of \u03b1i.\n\u21131-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes. In sparse subspace clustering [9], the authors pointed out that such sparse representation for each datum recovers the underlying subspaces from which the data are generated. With the data X = [x1, . . . ,xn] \u2208 IRd\u00d7n, it is mentioned in [7, 9] that the following sparse representation for each data point by \u21130-norm\nmin \u03b1 i\n\u2016\u03b1i\u20160 s.t. xi = X\u03b1 i (2)\ncan effectively recovers the subspace Si that the point xi belongs to, and the non-zero elements of the sparse code \u03b1i correspond to the data points that also lie in the subspace Si. Since (2) is a non-convex problem and NP-hard, sparse subspace clustering method replaces the \u21130-norm with \u21131norm in (2) and solves the following convex optimization problem instead:\nmin \u03b1 i\n\u2016\u03b1i\u20161 s.t. xi = X\u03b1 i (3)\nBy solving almost the same optimization problem as (3), \u21131graph obtains the robust sparse representation for each data point by solving the \u21131-norm optimization problem below:\nmin \u03b1 i\n\u2016\u03b1i\u20161 s.t. xi = A\u03b1 i i = 1, . . . , n (4)\nwhich is equivalent to\nmin \u03b1 i\n\u2016xi \u2212A\u03b1 i\u201622 + \u03bb\u21131\u2016\u03b1 i\u20161 i = 1, . . . , n (5)\nfor some weighting parameter \u03bb\u21131 > 0, and A = [X Id], Id is a d \u00d7 d identity matrix, \u03b1i \u2208 IR(n+d)\u00d71, \u03b1 = [\u03b11, . . . ,\u03b1n] \u2208 IR(n+d)\u00d7n is the coefficient matrix with the element \u03b1ij = \u03b1 j i . To avoid trivial solution that \u03b1 = [In 0] T , the diagonal elements of \u03b1 are enforced to be zero, i.e. \u03b1ii = 0 for 1 \u2264 i \u2264 n. Let G = (X,W) be the \u21131-graph where X is the set of vertices, W is the graph weight matrix and Wij indicates the similarity between xi and xj , \u21131-graph builds the n\u00d7 n similarity matrix W by the sparse codes:\nWij = (|\u03b1ij |+ |\u03b1ji|)/2 1 \u2264 i, j \u2264 n (6)\n\u21131-graph then performs spectral clustering on the sparse similarity matrix W to partition the data. In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9]. Therefore, in this case W exhibits block-diagonal structure with a proper perturbation of the data and spectral clustering on W can effectively segment the data in accordance with the underlying subspaces. Extensive empirical study shows that \u21131-graph achieves much better performance than spectral clustering on widely used Gaussian kernel graph. Moreover, it is clear that the pairwise similarity matrix (6) constructed by the coefficient matrix \u03b1 leads to the superior performance of \u21131-graph based clustering.\nIn the following section, we propose \u21130-graph, which follows the original principle of sparsely representing each data point by other data using \u21130-norm as in (2)."}, {"heading": "2. Formulation of \u21130-Graph", "text": "As mentioned in the previous section, \u21131-norm is used in \u21131-graph and sparse subspace clustering as a relaxation of the \u21130-norm for the sparsity of the graph. To handle the general cases when the subspaces are not independent and comply to the original sparse representation using \u21130-norm, \u21130-graph is proposed. The objective function of \u21130-graph is obtained by using \u21130-norm instead of \u21131-norm in (5):\nmin \u03b1\nL(\u03b1) = \u2016X \u2212A\u03b1\u20162F + \u03bb\u2016\u03b1\u20160 s.t. \u03b1ii = 0, i = 1, . . . , n\n(7)\nwhere \u2016 \u00b7 \u2016F is the Frobenius norm of a matrix and \u2016 \u00b7 \u20160 indicates the \u21130-norm that counts the number of nonzero elements in a vector or matrix. Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to \u21130-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative. In the following text, the superscript with bracket indicates the iteration number of the proposed proximal method.\nIn t-th iteration of our proximal method for t \u2265 1, gradient descent is performed on the square loss term of (7), i.e.\nQ(\u03b1) = \u2016X \u2212A\u03b1\u20162F , to obtain\n\u03b1\u0303 (t) = \u03b1(t\u22121) \u2212\n2\n\u03b3s (ATA\u03b1(t\u22121) \u2212ATX) (8)\nwhere \u03b3 > 1 is a constant and s is the Lipschitz constant for the gradient of function Q(\u00b7), namely\n\u2016\u2207Q(Y)\u2212\u2207Q(Z)\u2016F \u2264 s\u2016Y \u2212 Z\u2016F , \u2200Y,Z \u2208 IR (n+d)\u00d7n\n(9)\nThen \u03b1(t) is obtained as the solution to the following \u21130\nregularized problem:\n\u03b1 (t) = argmin\nv\u2208IR(n+d)\u00d7n\n\u03b3s\n2 \u2016v \u2212 \u03b1\u0303(t)\u20162F + \u03bb\u2016v\u20160 (10)\ns.t. vii = 0, i = 1, . . . , n\nIt can be verified that (10) has closed-form solution, i.e.\n\u03b1 (t) ij =\n{\n0 : |\u03b1\u0303 (t) ij | <\n\u221a\n2\u03bb \u03b3s or i = j\n\u03b1\u0303 (t) ij : otherwise\n(11)\nThe iterations start from t = 1 and continue until the sequence {L(\u03b1(t))} converges or maximum iteration number is achieved. We initialize \u03b1 as \u03b1(0) = \u03b1\u21131 and \u03b1\u21131 is the sparse codes generated by \u21131-graph by solving (5) with some proper weighting parameter \u03bb\u21131 . In all the experimental results shown in the next section, we empirically set \u03bb\u21131 = 0.1 when initializing \u21130-graph.\nThe data clustering algorithm by \u21130-graph is described in Algorithm 1. Also, the following theorem shows that each iteration of the proposed proximal method decreases the value of the objective function L(\u00b7) in (7), therefore, our proximal method always converges.\nTheorem 1. Let s = 2\u03c3max(ATA) where \u03c3max(\u00b7) indicates the largest eigenvalue of a matrix, then the sequence {L(\u03b1t)} generated by the proximal method with (8) and (11) decreases, and the following inequality holds for t \u2265 1:\nL(\u03b1t) \u2264 L(\u03b1t\u22121)\u2212 (\u03b3 \u2212 1)s\n2 \u2016\u03b1(t) \u2212\u03b1(t\u22121)\u20162F (12)\nAnd it follows that the sequence {L(\u03b1(t))} converges.\nThe proof of Theorem 1 is shown in the Appendix. Furthermore, we show that if the sequence {\u03b1t} generated by the proposed proximal method is bounded (which often holds in practice, and it always holds in our experiments), then it is a Cauchy sequence and it converges to a critical point of the objective function L in (7).\nTheorem 2. Suppose that the sequence {\u03b1t} generated by the proximal method with (8) and (11) is bounded, then 1)\n\u221e \u2211 t=1 \u2016\u03b1t \u2212 \u03b1t\u22121\u2016F < \u221e 2) {\u03b1t} converges to a critical point 1 of the function L(\u00b7) in (7).\nSketch of the Proof. [3] shows that the \u21130-norm function \u2016 \u00b7 \u20160 is a semi-algebraic function. The conclusions of this theorem directly follows from Theorem 1 in [3].\nAlgorithm 1 Data Clustering by \u21130-Graph Input:\nThe data set X = {xi}ni=1, the number of clusters c, the parameter \u03bb for \u21130-graph, \u03bb\u21131 for the initialization of the the \u21130-graph, maximum iteration numberM , stopping threshold \u03b5 1: t = 1, initialize the coefficient matrix as \u03b1(0) = \u03b1\u21131 , s = 2\u03c3max(A T A). 2: while t \u2264 M do 3: Obtain \u03b1(t) from \u03b1(t\u22121) by (8) and (11) 4: if |L(\u03b1(t))\u2212 L(\u03b1(t\u22121))| < \u03b5 then 5: print 6: else 7: t = t+ 1. 8: end if 9: end while 10: Obtain the optimal coefficient matrix \u03b1\u2217 when the above iterations converge or maximum iteration number is achieved. 11: Build the pairwise similarity matrix by symmetrizing\n\u03b1 \u2217: W\u2217 = |\u03b1\n\u2217|+|\u03b1\u2217|T\n2 , compute the corresponding\nnormalized graph Laplacian L\u2217 = (D\u2217)\u2212 1 2 (D\u2217 \u2212 W \u2217)(D\u2217)\u2212 1 2 , where D\u2217 is a diagonal matrix with\nD \u2217 ii =\nn \u2211\nj=1\nW \u2217 ij\n12: Construct the matrix v = [v1, . . . ,vc] \u2208 IRn\u00d7c, where {v1, . . . ,vc} are the c eigenvectors of L\u2217 corresponding to its c smallest eigenvalues. Treat each row of v as a data point in IRc, and run K-means clustering method to obtain the cluster labels for all the rows of v. Output: The cluster label of xi is set as the cluster label of the i-th row of v, 1 \u2264 i \u2264 n."}, {"heading": "3. Experimental Results", "text": "The superior clustering performance of \u21130-graph is demonstrated in this section with extensive experimental results. We compare our \u21130-graph to K-means (KM), Spectral Clustering (SC), \u21131-graph, Sparse Manifold Clustering and Embedding (SMCE). Moreover, we derive the OMP-graph, which builds the sparse graph in the same way as \u21130-graph\n1x is a critical point of function f if 0 \u2208 \u2202f(x), where \u2202f(x) is the limiting-subdifferential of f at x. Please refer to more detailed definition in [3].\nexcept that it solves the following optimization problem by Orthogonal Matching Pursuit (OMP) to obtain the sparse code:\nmin \u03b1 i\n\u2016xi \u2212A\u03b1 i\u20162F s.t. \u2016\u03b1 i\u20160 \u2264 T,\u03b1 i i = 0, i = 1, . . . , n\n(13)\n\u21130-graph is also compared to OMP-graph to show the advantage of the proposed proximal method in the previous section. By adjusting the parameters and settings, \u21131-graph and sparse subspace clustering generate equivalent results, so we omit the performance of sparse subspace clustering\nin the comparison."}, {"heading": "3.1. Evaluation Metric", "text": "Two measures are used to evaluate the performance of the clustering methods, i.e. the accuracy and the Normalized Mutual Information(NMI) [25]. Let the predicted label of the datum xi be y\u0302i which is produced by the clustering method, and yi is its ground truth label. The accuracy is\ndefined as\nAccuracy = 1I\u2126(y\u0302i) 6=yi\nn (14)\nwhere 1I is the indicator function, and \u2126 is the best permutation mapping function by the Kuhn-Munkres algorithm\n[17]. The more predicted labels match the ground truth ones, the more accuracy value is obtained.\nLet X\u0302 be the index set obtained from the predicted labels {y\u0302i} n i=1 and X be the index set from the ground truth labels\n{yi} n i=1. The mutual information between X\u0302 and X is\nMI(X\u0302, X) = \u2211\nx\u0302\u2208X\u0302,x\u2208X\np(x\u0302, x)log2( p(x\u0302, x)\np(x\u0302)p(x) ) (15)\nwhere p(x\u0302) and p(x) are the margined distribution of X\u0302 and X respectively, induced from the joint distribution p(x\u0302, x) over X\u0302 and X . Let H(X\u0302) and H(X) be the entropy of X\u0302 and X , then the normalized mutual information (NMI) is defined as below:\nNMI(X\u0302,X) = MI(X\u0302,X)\nmax{H(X\u0302),H(X)} (16)\nIt can be verified that the normalized mutual information takes values in [0, 1]. The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25]."}, {"heading": "3.2. Clustering on UCI Data Sets and MNIST Handwritten Digits Database", "text": "In this subsection, we conduct experiments on two real data sets from UCI machine learning repository [1], i.e. Heart and Ionosphere, as well as the MNIST database of handwritten digits. The three data sets are summarized in Table 5. MNIST handwritten digits database has a total number of 70000 samples for digits from 0 to 9. The digits are normalized and centered in a fixed-size image. For MNIST data set, we randomly select 500 samples for each digit to obtain a subset of MNIST data consisting of 5000 samples. The random sampling is performed for 10 times and the average clustering performance is recorded. The clustering results on these three data sets are shown in Table 1."}, {"heading": "3.3. Clustering On COIL-20 and COIL-100 Database", "text": "COIL-20 Database has 1440 images of 20 objects in which the background has been removed, and the size of each image is 32\u00d732, so the dimension of this data is 1024. COIL-100 Database contains 100 objects with 72 images of size 32 \u00d7 32 for each object. The images of each object were taken 5 degrees apart when the object was rotated on a turntable. The clustering results on these two data sets are shown in Table 2 and Table 3 respectively. We observe that \u21130-graph performs consistently better than all other competing methods. On COIL-100 Database, SMCE renders\nslightly better results than \u21131-graph on the entire data due to its capability of modeling non-linear manifolds."}, {"heading": "3.4. Clustering On Extended Yale Face Database B", "text": "The Extended Yale Face Database B contains face images for 38 subjects with 64 frontal face images taken under different illuminations for each subject. The clustering results are shown in Table 4. We can see that \u21130-graph achieves significantly better clustering result than \u21131-graph, which is the second best method on this data."}, {"heading": "3.5. Parameter Setting", "text": "We use the sparse codes generated by \u21131-graph with weighting parameter \u03bb\u21131 = 0.1 in (5) to initialize \u21130-graph, and set \u03bb = 0.5 for \u21130-graph empirically throughout all the experiments in this section, and we observe that the average number of non-zero elements of the sparse code for each data point is around 3 for most data sets. The maximum iteration number M = 100 and the stopping threshold \u03b5 = 10\u22126. For OMP-graph, we tune the parameter T in (13) to control the sparsity of the generated sparse codes such that the aforementioned average number of non-zero elements of the sparse code matches that of \u21130-graph. For \u21131-graph, the weighting parameter for the \u21131-norm is chosen from [0.1, 1] for the best performance.\nWe investigate how the clustering performance on the Extended Yale Face Database B and COIL-20 Database changes by varying the weighting parameter \u03bb for \u21130-graph, and illustrate the result in Figure 1 and Figure 2 respectively. We observe that the performance of \u21130-graph is much better than other algorithms over a relatively large range of \u03bb, revealing the robustness of our algorithm with respect to the weighting parameter \u03bb."}, {"heading": "4. Conclusion", "text": "We propose a novel \u21130-graph for data clustering in this paper. In contrast to the existing sparse graph methods such as \u21131-Graph that uses \u21131-norm as a relaxation of the \u21130-norm, \u21130-graph enforces the sparsity of the constructed graph by \u21130-norm and optimizes the objective function using a proposed proximal method. Convergence of this proximal method is proved, and extensive experimental results on various real data sets demonstrate the effectiveness and superiority of \u21130-graph over other competing methods."}, {"heading": "5. Appendix", "text": "Proof of Theorem 1. First of all, when s be 2 times the maximum eigenvalue of ATA, then s is the Lipschitz constant for the gradient of function Q. To see this, we have \u2207Q(Y) = 2(ATAY \u2212ATX), and\n\u2016Q(Y)\u2212\u2207Q(Z)\u2016F = 2\u2016A T A(Y \u2212 Z)\u2016F (17)\n\u2264 2\u03c3max(A T A) \u00b7 \u2016(Y \u2212 Z)\u2016F\nSince \u03b1(t) = argmin v\u2208IR(n+d)\u00d7n,diag(v)=0 \u03b3s 2 \u2016v \u2212 \u03b1\u0303 (t)\u20162F + \u03bb\u2016v\u20160,\n\u03b3s\n2 \u2016\u03b1(t) \u2212 \u03b1\u0303(t)\u20162F + \u03bb\u2016\u03b1 (t)\u20160 (18)\n\u2264 \u03b3s 2 \u2016 \u2207Q(\u03b1(t\u22121)) \u03b3s \u20162F + \u03bb\u2016\u03b1 (t\u22121)\u20160\nwhich is equivalent to\n\u3008\u2207Q(\u03b1(t\u22121)),\u03b1(t) \u2212\u03b1(t\u22121)\u3009+ \u03b3s\n2 \u2016\u03b1(t) \u2212\u03b1(t\u22121)\u20162F + \u03bb\u2016\u03b1 (t)\u20160\n(19)\n\u2264 \u03bb\u2016\u03b1(t\u22121)\u20160\nAlso, since s is the Lipschitz constant for \u2207Q,\nQ(\u03b1(t)) \u2264 Q(\u03b1(t\u22121)) + \u3008\u2207Q(\u03b1(t\u22121)),\u03b1(t) \u2212\u03b1(t\u22121)\u3009 (20)\n+ s\n2 \u2016\u03b1(t) \u2212\u03b1(t\u22121)\u20162F\nCombining (19) and (21), we have\nQ(\u03b1(t)) + \u03bb\u2016\u03b1(t)\u20160 \u2264 Q(\u03b1 (t\u22121)) + \u03bb\u2016\u03b1(t\u22121)\u20160 (21)\n\u2212 (\u03b3 \u2212 1)s\n2 \u2016\u03b1(t) \u2212\u03b1(t\u22121)\u20162F\nAnd (12) is verified. Since the sequence {L(\u03b1(t))} is deceasing with lower bound 0, it must converge."}], "references": [{"title": "L0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 3858\u20133865", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximal alternating linearized minimization for nonconvex and nonsmooth problems", "author": ["J. Bolte", "S. Sabach", "M. Teboulle"], "venue": "Math. Program.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning with l1-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Transactions on Image Processing, 19(4):858\u2013866", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse representation and learning in visual recognition: Theory and applications", "author": ["H. Cheng", "Z. Liu", "L. Yang", "X. Chen"], "venue": "Signal Process.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Pattern Classification (2Nd Edition)", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Interscience", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "CVPR, pages 2790\u20132797", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse manifold clustering and embedding", "author": ["E. Elhamifar", "R. Vidal"], "venue": "NIPS, pages 55\u201363", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse subspace clustering: Algorithm", "author": ["E. Elhamifar", "R. Vidal"], "venue": "theory, and applications. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765\u20132781", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Model-Based Clustering, Discriminant Analysis, and Density Estimation", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, 315:2007", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "F.R. Bach", "G.R. Obozinski"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 487\u2013494", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F.R. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1033\u20131040", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "L0-norm-based sparse representation through alternate projections", "author": ["L. Mancera", "J. Portilla"], "venue": "In Image Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "NIPS, pages 849\u2013856", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Matching Theory", "author": ["D. Plummer", "L. Lov\u00e1sz"], "venue": "North- Holland Mathematics Studies. Elsevier Science", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1986}, {"title": "Survey: Graph clustering", "author": ["S.E. Schaeffer"], "venue": "Comput. Sci. Rev.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Semi-supervised learning by sparse representation", "author": ["S. Yan", "H. Wang"], "venue": "SDM, pages 792\u2013801", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T.S. Huang"], "venue": "CVPR, pages 1794\u20131801", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Regularized l1-graph for data clustering", "author": ["Y. Yang", "Z. Wang", "J. Yang", "J. Han", "T. Huang"], "venue": "Proceedings of the British Machine Vision Conference. BMVA Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Data clustering by laplacian regularized l1-graph", "author": ["Y. Yang", "Z. Wang", "J. Yang", "J. Wang", "S. Chang", "T.S. Huang"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., pages 3148\u20133149", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Lowrank sparse coding for image classification", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "C. Xu", "N. Ahuja"], "venue": "IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 281\u2013288", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph regularized sparse coding for image representation", "author": ["M. Zheng", "J. Bu", "C. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai"], "venue": "IEEE Transactions on Image Processing, 20(5):1327\u2013 1336", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality preserving clustering for image database", "author": ["X. Zheng", "D. Cai", "X. He", "W.-Y. Ma", "X. Lin"], "venue": "Proceedings of the 12th Annual ACM International Conference on Multimedia, MULTIMEDIA \u201904, pages 885\u2013891, New York, NY, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 17, "context": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.", "startOffset": 8, "endOffset": 15}, {"referenceID": 2, "context": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.", "startOffset": 8, "endOffset": 15}, {"referenceID": 8, "context": "With Gaussian Mixture Model (GMM) as a representative, model-based clustering methods typically model the data by a mixture of parametric distributions, and the parameters of the distributions are estimated via fitting a statistical model to the data [10].", "startOffset": 251, "endOffset": 255}, {"referenceID": 4, "context": "For example, K-means [6] groups similar data together by a local minimum of sum of within-cluster dissimilarities.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "Affinity Propagation [11] uses the same principle and automatically determines the cluster number.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Spectral Clustering [16] identifies clusters of complex shapes lying on some low dimensional manifolds by spectral embedding.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Among various similarity-based clustering methods, graph-based methods [18] are important wherein the edge weight of the graph serving as the data similarity, and sparse graphs (where only a few edges have non-zero weights for each vertex) are demonstrated to be especially effective for clustering high dimensional data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 49, "endOffset": 56}, {"referenceID": 2, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 49, "endOffset": 56}, {"referenceID": 7, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 88, "endOffset": 91}, {"referenceID": 19, "context": "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 17, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 2, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 6, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 19, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 20, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 10, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 12, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 7, "context": "[9] points out that in case that the data are drawn from linear independent subspaces, sparse representation by l-norm can recover the underlying subspaces.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "On the other hand, sparse representation methods [15] that directly optimize objective function involving l-norm demonstrate compelling performance compared to its lnorm counterpart.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "The proximal method is inspired by the proximal linearized method in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 3, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 21, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 19, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 17, "context": "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.", "startOffset": 8, "endOffset": 15}, {"referenceID": 2, "context": "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.", "startOffset": 8, "endOffset": 15}, {"referenceID": 7, "context": "In sparse subspace clustering [9], the authors pointed out that such sparse representation for each datum recovers the underlying subspaces from which the data are generated.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": ",xn] \u2208 IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm", "startOffset": 30, "endOffset": 36}, {"referenceID": 7, "context": ",xn] \u2208 IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 7, "context": "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 1, "context": "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "[3] shows that the l-norm function \u2016 \u00b7 \u20160 is a semi-algebraic function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The conclusions of this theorem directly follows from Theorem 1 in [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "Please refer to more detailed definition in [3].", "startOffset": 44, "endOffset": 47}, {"referenceID": 23, "context": "the accuracy and the Normalized Mutual Information(NMI) [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "where 1I is the indicator function, and \u03a9 is the best permutation mapping function by the Kuhn-Munkres algorithm [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}, {"referenceID": 2, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}], "year": 2017, "abstractText": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn. It is well known that l-norm used in l-graph is a convex relaxation of lnorm for enforcing the sparsity. In order to handle general cases when the subspaces are not independent and follow the original principle of sparse representation, we propose a novel l-graph that employs l-norm to encourage the sparsity of the constructed graph, and develop a proximal method to solve the associated optimization problem with the proved guarantee of convergence. Extensive experimental results on various data sets demonstrate the superiority of l-graph compared to other competing clustering methods including l-graph.", "creator": "LaTeX with hyperref package"}}}