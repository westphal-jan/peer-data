{"id": "1511.02909", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Efficient Construction of Local Parametric Reduced Order Models Using Machine Learning Techniques", "abstract": "reduced order models are computationally inexpensive approximations that helped capture the important dynamical characteristics of large, high - fidelity computer models of physical systems. this paper applies machine learning techniques to improve the design of parametric reduced order models. specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, primarily to chose the best two already existing bases for assuming a new parameter configuration from accuracy point of view and to pre - select the optimal dimension of the reduced basis such sufficient as to meet the desired accuracy. by combining available information using bases concatenation and interpolation as well as high - fidelity solutions interpolation processes we are able typically to build accurate reduced order models associated with new parameter settings. generating promising numerical results with calculating a homogeneous viscous burgers model illustrate the potential of machine dynamic learning approaches to help design better reduced order models.", "histories": [["v1", "Mon, 9 Nov 2015 22:12:14 GMT  (581kb,D)", "http://arxiv.org/abs/1511.02909v1", "28 pages, 15 figures, 6 tables"]], "COMMENTS": "28 pages, 15 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["azam moosavi", "razvan stefanescu", "adrian sandu"], "accepted": false, "id": "1511.02909"}, "pdf": {"name": "1511.02909.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Azam Moosavi", "Razvan Stefanescu", "Adrian Sandu"], "emails": ["rstefan@ncsu.edu"], "sections": [{"heading": null, "text": "key words reduced order models, high-fidelity models, data fitting, machine learning, feasible region of parameters, local reduced order models."}, {"heading": "1 Introduction", "text": "Many physical phenomena are described mathematically by partial differential equations (PDE), and, after applying suitable discretization schemes, are simulated on a computer. PDE-based models frequently require calibration and parameter tuning in order to provide realistic simulation results. Recent developments in the field of uncertainty quantification [52, 85, 35, 18] provide the necessary tools for validation of such models even in the context of variability and lack of knowledge on the input parameters. While uncertainty propagation techniques such as Markov chain [45] and perturbation methods [15, 16, 17] can measure the impact of uncertain parameters on some quantities of interest, they often become infeasible due to the large number of model realizations requirement. Similar difficulties are encountered when solving Bayesian inference problems since sampling from posterior distribution is required.\nFor large-scale simulations, the variational [79, 78, 21, 60, 61, 87] and ensemble [30, 96, 48, 43] based inference approaches are widely used in practice. Their efficiency decreases with increasing computational complexity of the underlying physical models. However, increasing model complexity is unavoidable as science fields progress. For example, finer space resolution of the underlying PDE models is one of the most important factors contributing to the one day/decade growth rate of the reliability of atmospheric weather predictions [13, 92].\nThe need for computational efficiency motivated the development of surrogate models such as response surfaces, low resolution, and reduced basis models, in order to facilitate optimization, inference, and uncertainty quantification.\nData fitting or response surface models [85] are constructed using only a data-driven angle. The underlying physics remains unknown and only the input-output behavior of the model is considered. Data fitting can use techniques such as regression, interpolation, radial basis function, Gaussian Processes, artificial neural networks and other supervised machine-learning methods. The latter techniques can automatically detect patterns in data, and one can use them to predict future data under uncertainty in a probabilistic framework [59]. While easy to implement due to the non-intrusive nature, the prediction abilities may suffer since the governing physics is not specifically accounted for.\nLow fidelity models attempt to reduce the computational burden of the high-fidelity models by neglecting some of the physical aspects (e.g., replacing Navier-Stokes and Large Eddy Simulations with inviscid Euler\u2019s equations and Raynolds-Averaged NavierStokes [33, 76, 97], or decreasing the spatial resolution [20, 90]). The additional approximations, however, may considerably degrade the physical solution with only a modest decrease of the computational load.\nReduced basis [67, 9, 34, 65, 74, 24] and Proper Orthogonal Decomposition [49, 56, 42, 57, 58] are two of the popular reduced order modeling (ROM) strategies available in the literature. Data analysis is conducted to extract basis functions from experimental data or detailed simulations of high-dimensional systems (method of snapshots [81, 82, 83]), for subsequent use in Galerkin projections that yield low dimensional dynamical models. While these type of models are physics-based and therefore require intrusive implementations, they are usually faster and more robust than data fitting and low-fidelity models.\nRobustness of ROM in a parametric setting can be achieved by constructing a global basis [40, 68], but this strategy generates large dimensional bases that may lead to slow reduced order models. Local approaches have been designed for parametric or time domains generating local bases for both the state variables [70, 23] and non-linear terms [28, 66]. Here we address the robustness issue of POD reduced order models by charting the parametric domain with feasible regions of local reduced order models where the reduced solutions are accurate within an admissible prescribed threshold. Two essential ingredients are used to construct the parametric map: a database of reduced order models, and a data-fitting probabilistic model for the reduced order model errors. Then an incremental procedure uses the newly created probabilistic model to sample the parametric domain and generates a feasible region where a specific reduced order model provides accurate solutions within a prescribed tolerance. We then use a greedy approach to sweep the parameter domain and cover it with such feasible regions. This methodology is applied to the viscous 1D-Burgers model, and a parametric map for the viscosity parameter is generated for various error thresholds. Once the map is constructed there is no need to run the high-fidelity model again, since for each parameter value \u00b50 there exists a parameter \u00b5, and the associated reduced order model (basis and reduced operators), in whose interval the current value \u00b50 falls; the corresponding reduced solution error is accurately estimated a-priori. The dimension k of the local basis is usually small since it depends only on one high-fidelity model trajectory.\nThe database of reduced order models and the high-fidelity trajectories are used to statistically model the reduced order approximation errors. We solve the resulting non-linear regression problems using Gaussian processes (GP) [84, 55] and artificial neural networks\n(ANN). Specifically, consider the reduced order model of dimension k constructed using the high-fidelity solution computed with parameter value \u00b5. Let \u03b5 be the error of this reduced model when approximating the full solution at parameter configuration \u00b50. We use a GP or ANN approach to model the mapping {\u00b50, \u00b5, k} \u2192 log(\u03b5). Our approach is inspired by the multifidelity correction and ROMES methodologies available in the literature for estimation of surrogate models errors using a global basis. Multifidelity correction [2, 29, 33, 44] has been developed for low-fidelity models in the context of optimization. They simulate the input-output relation \u00b5\u2192 \u03b5, where \u03b5 is the low-fidelity model errors. The ROMES method [25] introduces the concept of error indicators for reduced order models and generalizes the \u2019multifidelity correction\u2019 framework by approximating the mapping \u03c1(\u00b5) \u2192 log(\u03b5). The error indicators \u03c1(\u00b5) include rigorous error bounds and reduced order residual norms, while \u03b5 is the reduced order model error at \u00b5 using a reduced global basis. By estimating the log of the reduced order model error instead of the error itself the probabilistic map exhibits a lower variance as shown by our numerical experiments as well as those in [25].\nThe size of a feasible region directly depends on the location of parameter value \u00b5 within the parametric space. For overlapping feasible regions one can combine the available bases and the high-fidelity model trajectories in an efficient manner to generate more accurate reduced order models. Three different approaches are proposed here, i.e. bases interpolation, bases concatenation, and highfidelity model solution interpolation. Assuming a linear dependence we perform a Lagrangian interpolation of the bases in the matrix space [54], or interpolate their projections onto some local coordinate systems [54, 4]. Following the idea of the spanning ROM introduced in [93], for a new parameter not found in the ROM database, we create a projection basis either by concatenating two of the available bases that generated the higher accurate reduced order solutions for the new configurations, or by interpolating the associated high-fidelity solutions and then extracting the singular vectors.\nFinally, we address the issue of a-priori selection of the reduced basis dimension for a prescribed accuracy of the reduced solution. The standard approach is to analyze the spectrum of the snapshots matrix, and use the largest singular value removed from the expansion to estimate the accuracy level [91]. To take into account the error due to the integration in the reduced space, here we use data fitting models to approximate the mapping {\u00b5, log(\u03b5)} \u2192 k. Numerical results obtained using Gaussian processing and artificial neural networks are very promising.\nThe remainder of the paper is organized as follows. Section 2 reviews the reduced order modeling parametric framework. The problems solved herein are formulated in Section 3. Gaussian processes and artificial neural networks are reviewed in Section 4. The proposed techniques for generating reduced order bases for new parameter configurations are introduced in Section 5. Section 6 presents the details of the data fitting models, constructs the parametric map for the viscous 1D-Burgers model, and analyses the probabilistic model\u2019s performance. Conclusions are drawn in Section 7."}, {"heading": "2 Parametrized Reduced Order Modeling", "text": "Proper Orthogonal Decomposition has been successfully applied in numerous applications such as compressible flow [72], computational fluid dynamics [50, 73, 98], and aerodynamics [12], to mention a few. It can be thought of as a Galerkin approximation in the spatial variable built from functions corresponding to the solution of the physical system at specified time instances. A system reduction strategy for Galerkin models of fluid flows leading to dynamic models of lower order based on a partition in slow, dominant, and fast modes, has been proposed in [64]. Closure models and stabilization strategies for POD of turbulent flows have been investigated in [77, 95].\nIn this paper we consider discrete inner products (Euclidian dot product), though continuous products may be employed as well. Generally, an atmospheric or oceanic computer model is described by the following semi\u2013discrete dynamical system:\ndx(\u00b50, t)\ndt = F(x, t, \u00b50), x(\u00b50, 0) = x0 \u2208 RNstate , \u00b50 \u2208 P\u0303. (1)\nThe input-parameter \u00b50 typically characterizes the physical properties of the flow. For a given parameter configuration \u00b50 we select an ensemble of Nt time instances of the flow x \u00b50 t1 , ...,x \u00b50 tNt \u2208 RNstate , where Nstate is the total number of discrete model variables, and Nt \u2208 N, Nt > 0. The POD method chooses an orthonormal basis U\u00b50 = [u\u00b5 0\ni ]i=1,..,k \u2208 RNstate\u00d7k, k > 0, such that the mean square error between x(\u00b50, ti) and the POD expansion x \u00b50 POD(ti) = U\u00b50 x\u0303(\u00b5 0, ti), x\u0303(\u00b50, ti) \u2208 Rk, is minimized on average. The POD space dimension k Nstate is appropriately chosen to capture the dynamics of the flow. Algorithm 1 describes the reduced order basis construction procedure [88].\nAlgorithm 1 POD basis construction\n1: Compute the singular value decomposition for the snapshots matrix [x\u00b5 0 t1 . . . x \u00b50 tNt ] = U\u0304\u00b50\u03a3\u00b50 V\u0304 T \u00b50 , with the singular vectors matrix\nU\u0304\u00b50 = [u \u00b50 i ]i=1,..,Nstate . 2: Using the singular-values \u03bb1 \u2265 \u03bb2 \u2265 ...\u03bbn \u2265 0 stored in the diagonal matrix \u03a3, define I(m) = ( \u2211m i=1 \u03bbi)/( \u2211Nstate i=1 \u03bbi). 3: Choose k, the dimension of the POD basis, such that k = minm{I(m) : I(m) \u2265 \u03b3} where 0 \u2264 \u03b3 \u2264 1 is the percentage of total information captured by the reduced space X k = range(U\u00b50). Usually \u03b3 = 0.99.\nNext, a Galerkin projection of the full model state and equations (2) onto the space X k spanned by the POD basis elements is used\nto obtain the reduced order model:\ndx\u0303(\u00b50, t)\ndt = UT\u00b50 F\n( U\u00b50 x\u0303(\u00b5 0, t), t, \u00b50 ) , x\u0303(\u00b50, 0) = UT\u00b50 x(0). (2)\nThe efficiency of the POD-Galerkin technique is limited to linear or bilinear terms, since the projected nonlinear terms at every discrete time step still depend on the number of variables of the full model. In case of polynomial nonlinearities the tensorial POD technique [88] can be employed to efficiently remove the dependence on the full dimension by manipulating the order of computions. A considerable reduction in complexity is achieved by the Discrete Empirical Interpolation Method (DEIM) [19, 86], a discrete variation of Empirical Interpolation Method [8], for any type of nonlinear terms.\nWhile being accurate for the given parameter configuration, the reduced model (2) loses accuracy when moving away from the initial setting. Several strategies have been proposed to derive a basis that spans the entire parameter space. These includes the reduced basis methods combined with the use of error estimates [74, 69, 68], global POD [89, 80], Krylov-based sampling methods [22, 94], and greedy techniques [36, 63]. The fundamental assumption used by these approaches is that a smooth low-dimensional global manifold characterizes the model solutions over the entire parameter domain. However, in order to ensure high accuracy of the reduced solution across the parameter space, the dimension of the reduced basis has to be increased in practice, leading to high computational costs. To alleviate this drawback we propose an alternative approach based on local parametric reduced order models."}, {"heading": "3 Problem Description and Solution Methodology", "text": "This work addresses the following problems in the construction of reduced order models: designing the parametric map, selecting the best two already existing bases for a new parameter configuration from accuracy point of view, and selecting the optimal dimension of the reduced basis. We formulate them in detail below."}, {"heading": "3.1 Designing the parametric map", "text": "Problem 1 (Efficient approximate ROMs). For an arbitrary parameter configuration \u00b50 \u2208 P\u0303 construct a reduced order model (2) that provides an accurate and efficient approximation of the high-fidelity solution (1):\n\u2016x(\u00b50, ti)\u2212 U x\u0303(\u00b50, ti)\u20162 < \u03b5\u0304, i = 1, .., Nt, (3)\nfor some prescribed admissible error level \u03b5\u0304 > 0. The snapshots used to generate the basis U can be obtained with any parametric configuration that belongs to P\u0303 .\nA simple solution is to solve the high-fidelity model for the specific configuration \u00b50, and then build the corresponding reduced order model. However, this approach is computationally expensive.\nOur methodology proposes to select a small countable subset I = {\u00b5j , j = 1, ..,M} \u2282 P\u0303, M > 0 and for each \u00b5j , a reduced order basis U\u00b5j along with the reduced operators are constructed for j = 1, ..,M . We denote by U the set of bases U\u00b5j , j = 1, ..,M . Then for each \u00b50 \u2208 P\u0303 we can find a basis U\u00b5j \u2208 U and the suitable reduced order model such that its solution satisfies (3) for U = U\u00b5j .\nThis strategy relies on the assumption that a reduced order basis and operators built for a specific parameter configuration \u00b5j \u2208 P\u0303 can be used to design a reduced order model capable to accurately approximate the solution of the high-fidelity model (1) for all \u00b50 \u2208 B(\u00b5j , rj)\u2229 P\u0303 , where B(\u00b5j , rj) is the closed ball of radius rj \u2265 0 centered at \u00b5j . Specifically, for \u00b50 \u2208 B(\u00b5j , rj)\u2229 P\u0303 , the reduced order model is constructed by employing the basis U\u00b5j and the reduced operators designed at \u00b5j , i.e.\ndx\u0303(\u00b50, \u00b5j , t)\ndt = UT\u00b5j F\n( U\u00b5j x\u0303(\u00b5 0, \u00b5j , t), t, \u00b5 0 ) , x\u0303(\u00b50, 0) = UT\u00b5j x(0). (4)\nThen, by selecting a small radius rj , one can be able to obtain\n\u2016x(\u00b50, ti)\u2212 U\u00b5j x\u0303(\u00b50, \u00b5j , ti)\u20162 < \u03b5\u0304, i = 1, .., Nt, (5)\nfor all \u00b50 \u2208 B(\u00b5j , r) \u2229 P\u0303 . The parametric map construction process ends as soon as the entire parameter domain P\u0303 is covered with a finite union of overlapping balls B(\u00b5j , rj), j = 1, ..,M , corresponding to different reduced order bases and local models\nP\u0303 \u2282 M\u22c3 j=1 B(\u00b5j , rj), (6)\nsuch that for each j = 1, 2, ..,M and \u2200\u00b50 \u2208 B(\u00b5j , r)\u2229 P\u0303 , the solution of the reduced order model (4) depending on the basis U\u00b5j fulfils the accuracy condition (5)."}, {"heading": "3.2 Selecting the best two already existing bases for a new parameter configuration", "text": "This approach is inspired from the construction of local reduced order models where the time domain is split in multiple regions [70, 66]. In this way the reduced basis dimension is kept small allowing for fast on-line simulations. The cardinality of I depends inversely proportional with the prescribed level of accuracy \u03b5\u0304. As the desired error threshold \u03b5\u0304 decreases, the map changes since usually the radii rj are expected to become smaller, and more balls are required to cover the parametric domain, i.e. M is increased.\nThe construction of the parametric map (6) using the local reduced order models requires the following ingredients:\n1. The ability to probe the vicinity of \u00b5j \u2208 P\u0303 and to efficiently estimate the level of error\n\u03b5 = max i=1,..,Nt\n\u2016x(\u00b50, ti)\u2212 U\u00b5j x\u0303(\u00b50, \u00b5j , ti)\u20162, (7)\n2. The ability to find rj > 0 such that \u03b5 \u2264 \u03b5\u0304 for all \u00b50 \u2208 B(\u00b5j , r)\u2229 P\u0303 . This can be theoretically achieved by assuming that the error \u03b5 is monotonically increasing with larger distances d(\u00b50, \u00b5j). However, this is not necessarily true and in practice this is obtained by sampling.\n3. The ability to identify the location of a new \u00b5` (for the construction of a new local reduced order model) given the locations of the previous local parameters \u00b5j , j = 1, .., `\u2212 1, so that\nB(\u00b5`, r`) 6\u2282 ( `\u22121\u22c3 i=1 B(\u00b5i, ri) ) , B(\u00b5`, r`) \u22c2( `\u22121\u22c3 i=1 B(\u00b5i, ri) ) 6= \u2205. (8)\nThe implementation of the first ingredient does not employ a-posteriori error estimation formulas [63]. Inspired from the \u2019multifidelity correction\u2019 and ROMES methodologies we construct probabilistic models to approximate the level of error \u03b5 in (7). Gaussian process and artificial neural networks are used to build the probabilistic functions to model the mapping (\u00b50, \u00b5j , k) \u2192 log(\u03b5). Since the dimension of basis determines the level of error we include it among the input features. To design probabilistic models with reduced variances we look to approximate the logarithm of the error as suggested in [25].\nThe above machine learning techniques allow to sample the vicinity of \u00b5j and estimate the error for each sample parameter value. Based on these error estimates we construct the ball B(\u00b5j , r), or perhaps a larger set called a \u00b5j\u2212feasible region, where the local reduced order model is accurate within the prescribed threshold \u03b5\u0304.\nNext, a greedy algorithm is applied to identify the location of a new parametric configuration \u00b5` (for the construction of a new basis) depending on the locations of the previous \u00b5i, i = 1, .., `\u2212 1. Constraint (8) is imposed so the entire parametric domain P\u0303 satisfies (6) after the map construction is finished.\nFor the parametric area situated at the intersection of different feasible regions we can assign a new reduced order model based on the information required to construct the initial feasible regions. This is achieved by interpolation or concatenation of the underlying reduced bases or interpolation of the available high-fidelity solutions, as described in detail in Section 5."}, {"heading": "3.2 Selecting the best two already existing bases for a new parameter configuration", "text": "Since the error of the reduced order solution at a new parameter location \u00b50 is not necessarily smaller with the decrease of the distance d(\u00b50, \u00b5j), the following more general problem is posed.\nProblem 2 (Selection of best bases). For a new parameter configuration \u00b50 find the best available bases (among the existing ones) that provide the most accurate reduced order model solution.\nThe capability of the already proposed probability models can be used to estimate the error\n\u03b5 = \u2016x(\u00b50, ti)\u2212 U\u00b5j x\u0303(\u00b50, \u00b5j , ti)\u20162 , i = 1, .., Nt, (9)\nfor all available U\u00b5j , j = 1, 2, .. in the database, and the bases that lead to the smallest estimated errors are selected. This approach is discussed in Section 6.3."}, {"heading": "3.3 Selecting the dimension of the reduced basis", "text": "Problem 3 (Optimal basis dimension). Find the optimal dimension of the basis U\u00b5 for the parametric configuration \u00b5 such that the error is smaller than the prescribed threshold\n\u2016x(\u00b5, ti)\u2212 U\u00b5 x\u0303(\u00b5, \u00b5, ti)\u20162 \u2264 \u03b5\u0304, i = 1, .., Nt. (10)\nBy optimal we understand the smallest dimension that enables the reduced order model solution to satisfy the error constraint (10). The basis dimension represents one of the most important characteristics of a reduced order model. The reduced manifold size directly affects both the on-line computational complexity of the reduced order model and its accuracy [51, 39, 31]. By increasing the size of the basis the projection error decreases and the accuracy of the reduced order model is enhanced. Consequently, the spectrum of the\nsnapshots matrix offers guidance regarding the choice of the reduced basis size when some prescribed reduced order model error is desired. However the accuracy depends also on integration errors in the case of unsteady models as stated in [41].\nWe seek to estimate the optimal size of the reduced order model by accounting for both the projection and integration errors. For this we use data fitting models to approximate the mapping {\u00b5, log \u03b5\u0304} \u2192 k, as explained in Section 6.5.\nFor all the problems addressed in this study a general probabilistic framework is introduced in Section 4, along with the description of the supervised machine learning techniques used to construct the discussed probabilistic models. For each problem we discuss the dataset, the features, as well as the accuracy and stability of the predictions of the associated data fitting models."}, {"heading": "4 Supervised Machine Learning Techniques", "text": "Consider a random vector z. Neural networks and Gaussian processes are used to build a probabilistic model \u03c6 : z \u2192 y\u0302, where \u03c6 is a transformation function that learns through the input feature vector z \u2208 \u2126 (the sample space) to estimate the deterministic output y \u2208 R [59]. The estimator y\u0302 is expected to have a low variance. The features of z should be descriptive of the underlying problem at hand [11]. The accuracy and stability of estimations are assessed using the K-fold cross validation technique. The samples are split into K subsets (\u201cfolds\u201d), where typically 3 \u2264 K \u2264 10. The machine is trained on K \u2212 1 sets and tested on the K-th set in a round-robin fashion [59]. Each fold induces a specific error quantified as the average of the absolute values of the differences between the predicted and the K-th set values.\nEfold = \u2211N i=1 |y\u0302i \u2212 yi|\nN , VARfold =\n\u2211N i=1 (y\u0302i \u2212 Efold) 2\nN , (11a)\nwhere N is the number of test samples in the fold. The error is then averaged over all folds:\nE = \u2211K\nfold=1 Efold K\n, VAR = \u2211K fold=1 (Efold \u2212 E) 2\nK . (11b)\nThe variance of the prediction results (11a) accounts for the sensitivity of the model to the particular choice of data set. It quantifies the stability of the model in response to the new training samples. A smaller variance indicates more stable predictions, however, this sometimes translates into a larger bias of the model. Models with small variance and high bias make strong assumptions about the data and tend to underfit the truth, while models with high variance and low bias tend to overfit the truth [62] . The trade-off between bias and variance in learning algorithms is usually controlled via techniques such as regularization or bagging and boosting [11].\nIn what follows we briefly review the Gaussian process and Neural network techniques."}, {"heading": "4.1 Gaussian process kernel method", "text": "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution [71]. A Gaussian process is fully described by its mean and covariance functions\n\u03c6(z) \u223c gp ( m(z), k(zi, zj) ) , (12)\nwhere [71] m(z) = E [\u03c6(z)] , k(zi, zj) = E [(\u03c6(zi)\u2212m(zi)) (\u03c6(zj)\u2212m(zj))] .\nIn this work we employ the commonly used squared-exponential-covariance Gaussian kernel with [71]:\nk(zi, zj) = \u03c3 2 \u03c6 exp\n( \u2212 (zi \u2212 zj) 2\n2 `2\n) + \u03c32n \u03b4i,j , (13)\nwhere zi and zj are the pairs of data points in training or test samples, and \u03b4 is the Kronecker delta symbol. The model (13) has three hyper-parameters. The length-scale ` governs the correlation among data points. The signal variance \u03c32\u03c6 and the noise variance \u03c3 2 n govern the precision of variance and noise, respectively. Consider a set of training data points z1, z2, \u00b7 \u00b7 \u00b7 zn, and the corresponding noisy observations y1, y2, \u00b7 \u00b7 \u00b7 yn,\nyi = \u03c6(zi) + i, i \u223c N ( 0, \u03c32n ) , i = 1, . . . , n. (14)\nConsider also the set of test points z\u22171, z \u2217 2, \u00b7 \u00b7 \u00b7 , z\u2217m and the predictions y\u03021, y\u03022, \u00b7 \u00b7 \u00b7 y\u0302m\ny\u0302i = \u03c6 (z \u2217 i ) , i = 1, . . . ,m. (15)\nFor a Gaussian prior the joint distribution of training outputs y and test outputs y\u0302 is:[ y y\u0302 ] \u223c N ([ m(z) m(z\u2217) ] , [ k(z, z) + \u03c32nI k(z, z \u2217) k(z\u2217, z) k(z\u2217, z\u2217) ]) (16)"}, {"heading": "4.2 Artificial Neural networks", "text": "The predictive distribution represents the posterior after observing the data [11] and is given by: p (y\u0302|z, y, z\u2217) \u223c N ( k(z\u2217, z) ( k(z, z) + \u03c32nI )\u22121 y , k(z\u2217, z\u2217)\u2212 k(z\u2217, z) ( k(z, z) + \u03c32nI )\u22121 k(z, z\u2217) ) . (17)\nThe prediction of Gaussian process will depend on the choice of the mean and covariance functions, and on their hyperparameters. The hyperparametrs can be inferred from the data by minimizing the marginal negative log-likelihood function \u03b8 = arg min L(\u03b8), where\nL(\u03b8) = \u2212 log p(y|z, \u03b8) = 1 2 log |k (z, z) |+ 1 2 (y \u2212m(z))T k (z, z)\u22121 (y \u2212m(z)) + n 2 log (2\u03c0) ."}, {"heading": "4.2 Artificial Neural networks", "text": "The study of artificial neural networks (ANNs) begin in the 1910s in order to intimate human brain\u2019s biological structure. Pioneering work was carried out by Rosenblatt, who proposed a three-layered network structure, the perceptron [37] . ANNs detect the pattern of data by discovering the input\u2013output relationships. Applications include the approximation of functions, regression analysis, time series prediction, pattern recognition, and speech synthesis and recognition [46, 7].\nThe architecture of ANNs is schematically represented in Figure 1. ANNs consist of neurons and connections between the neurons (weights). Neurons are organized in layers, where at least three layers of neurons (an input layer, a hidden layer, and an output layer) are required for construction of a neural network. The input layer distributes input signals z1, z2, \u00b7 \u00b7 \u00b7 , zn to the hidden layer. For a neural network with L hidden layers and m neurons in the hidden layer, let y\u0302j be the vector of outputs from layer `, b` the biases at layer `, and w`kj the weight connecting the neuron j to the kth input. Then the feed-forward operation is:\nx`+1j = \u2211 k=1 w `+1 kj y\u0302 ` k + b `+1 j , y\u0302 0 j = zj , j = 1, \u00b7 \u00b7 \u00b7m\ny\u0302`+1j = \u03c6 ( x`+1j ) , ` = 0, 1, \u00b7 \u00b7 \u00b7 , L\u2212 1\nThe differentiable function \u03c6 is the transfer function and can be log-sigmoid, hyperbolic tangent sigmoid, or linear transfer function.\nThe training process of ANN adjusts the weights wi,j in order to reproduce the desired outputs when fed the given inputs. The training process via the back propagation algorithm [75] uses a gradient descent method to modify weights and thresholds such that the error between the desired output and the output signal of the network is minimized [32]. In supervised learning the network is provided with samples from which it discovers the relations of inputs and outputs. The output of the network is compared with the desired output,and the error is back-propagated through the network and the weights will be adjusted. This process is repeated during several epochs, until the network output is close to the desired output [38]. In unsupervised learning a model of the data distribution is formed in order to extract significant data features.The network extracts data features without being shown a set of inputs and outputs [46]."}, {"heading": "5 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": "The POD method produces an orthogonal basis that approximately spans the state solution space of the model for a specific parameter configuration. Moving away from the initial parametric configuration requires the construction of new bases since the initial reduced order model is not accurate anymore. However, if states depend continuously on parameters, the POD basis constructed for one parameter configuration can approximately span the solution space at different parametric settings in a local vicinity.\nSeveral methods to combine the available information to generate more accurate reduced order models for new parameter configurations have been proposed in the literature. One is the interpolation of the available reduced order bases U\u00b5j computed for the parametric"}, {"heading": "5.1 Basis interpolation", "text": "configurations \u00b5j , j = 1, ..,M . The dependence of the bases on the parameters has been modeled with various linear and nonlinear spatially-dependent interpolants.\nHere we compare the performances of different strategies that involve Lagrange interpolation of bases in the matrix space and in the tangent space of the Grassmann manifold. In addition we propose to concatenate the available reduced bases followed by an orthogonalization process, and to interpolate the solutions of the high fidelity model as a mean to derive the reduced order basis for a new parameter configuration."}, {"heading": "5.1 Basis interpolation", "text": "Lagrange interpolation of bases Assuming the reduced manifold U : P\u0303 \u2192 RNstate\u00d7k poses a continuous and linear dependency with respect to the parametric space, and if M discrete bases U\u00b5j = U(\u00b5j) have been already constructed for various parametric configurations \u00b5j , j = 1, 2, ..,M , then a basis corresponding to the new configuration \u00b50 can be obtained using Lagrange\u2019s interpolation formula\nU\u00b50 = M\u2211 j=1 U\u00b5jLj(\u00b5 0), Lj(\u00b5) = \u220f i 6=j \u00b5\u2212 \u00b5i \u00b5j \u2212 \u00b5i . (18)\nDrawbacks of this approach include the orthogonalization requirement for the resulting interpolated basis vectors, and the lack of linear variation in the angles between pairs of subspace planes [54] spanned by the reduced bases U\u00b5j . Differential geometry results can be employed to alleviate these deficiencies.\nGrassmann manifold In the study proposed by Amsallem and Farhat [4] basis (matrix) interpolation in the tangent space of the Grassmann manifold at a careful selected point S representing a subspace spanned by one of the available reduced bases was performed. It has been shown that Grassmann manifold can be endowed with a differentiable structure [1, 27], i.e., at each point S of the manifold a tangent space exists. The mapping from the manifold to the tangent space at S is called the logarithmic mapping, while the backward projection is referred to as exponential mapping [10]. According to [4] the construction of a new subspace S\u00b50 associated with a new parameter \u00b50 can be obtained by interpolating the known subspaces {Si}Mi=1 spanned by the already computed reduced bases U\u00b5i , i = 1, ..,M . The steps required by this methodology [4] are described in the Algorithm2.\nAlgorithm 2 Interpolation in a tangent space to a Grassmann manifold algorithm [4] 1: Select a point Si0 of the manifold to represent the origin point for the interpolation spanned by the basis U\u00b5i0 . 2: The tangent space TSi0 and the subspaces {Si} M i=1 are considered. Each point Si sufficiently close to Si0 is mapped to a matrix \u0393i\nrepresenting a point of TSi0 using the logarithm map LogSi0 [10]\n(I \u2212 U\u00b5i0U T \u00b5i0 )U\u00b5i(U\u00b5i0U\u00b5i) \u22121 = Ri\u039biQ T i (SVD factorization),\n\u0393i = Ri tan \u22121(\u039bi)Q T i .\n3: Each entry of the matrix \u03930 associated with the target parameter \u00b50 is computed by interpolating the corresponding entries of the matrices \u0393i \u2208 RNstate\u00d7k associated with the parameter points \u00b5i, i = 1, ..,M \u2212 1. A univariate or multivariate Lagrange interpolation may be chosen similar with the one introduced in (18). 4: The matrix \u03930 representing a point in the tangent space TSi0 is mapped to a subspace S 0 on the Grassmann manifold spanned by a\nmatrix U\u00b50 using the exponential map [10]\n\u03930 = R0\u039b0Q0 T\n(SVD factorization), U\u00b50 = U\u00b5i0Q 0 cos(\u039b0) +R0 sin(\u039b0).\nAmsallem and Cortial [3] proved that the subspace angle interpolation [54, 53] is identical to the interpolation in a tangent space to the Grassmann manifold of two reduced-order bases, thus the latter methodology can be viewed as a generalization of the former approach."}, {"heading": "5.2 Basis concatenation", "text": "Basis concatenation idea was introduced in [93] and emerged from the notion of a global basis [89, 80]. In the global strategy, the existent high-fidelity snapshots corresponding to various parameter configurations are collected in a single snapshot matrix and then a matrix factorization is performed to extract the most energetic singular vectors. This global basis is then used to build reduced order models for parameter values not available in the initial snapshots set."}, {"heading": "5.3 Interpolation of high-fidelity model solutions", "text": "Assuming x\u00b51 , x\u00b52 \u2208 RNstate\u00d7Nt are the snapshots corresponding to two high-fidelity model trajectories, the following error estimate holds [91, Proposition 2.16]:\nx\u0304 = [x\u00b51x\u00b52 ] = U\u0304\u039bV\u0304 T , (SVD factorization) (19a)\n\u2016x\u0304\u2212 U\u0304(:, 1 : k) x\u0303\u2016F = Nt\u2211\ni=k+1\n\u03bbi = O(\u03bbk+1), (19b)\nwhere \u03bbi is the ith singular value of x\u0304 , U\u0304(:, 1 : k) are the first k singular vectors of U\u0304 and x\u0303 \u2208 Rk\u00d72Nt . By \u2016 \u00b7 \u2016F we refer to the Frobenius norm. The snapshot matrix typically stores correlated data and therefore contains linearly dependent columns. For rank deficient snapshot matrices, and in the case where the reduced order bases U\u00b51 and U\u00b52 corresponding to the trajectories \u00b51 and \u00b52 are available, we can construct a basis U\u0302 by simply concatenating columns of U\u00b51 and U\u00b52 such that the accuracy level in (19) is preserved.\nProposition 5.1. Consider the following SVD expansions of rank deficient snapshots matrices x\u00b51 , x\u00b52 \u2208 RNstate\u00d7Nt\nx\u00b5j = U\u00b5j \u039bj V T \u00b5j , j = 1, 2. (20)\nThere are positive integers k1, k2, and x\u0302 \u2208 R(k1+k2)\u00d72Nt , such that x\u0304 defined in (19) satisfies\n\u2016x\u0304\u2212 U\u0302 x\u0302\u2016F = O(\u03bbk+1), (21)\nwhere \u03bbk+1 is the (k + 1)-st singular value of snapshots matrix x\u0304, and U\u0302 = [U\u00b51(:, 1 : k1) U\u00b52(:, 1 : k2)] \u2208 RNstate\u00d7(k1+k2).\nProof. Since x\u00b51 , x\u00b52 \u2208 RNstate\u00d7Nt are rank deficient matrices, there exist at least two positive integers k1 and k2, such that the singular values associated with x\u00b51 and x\u00b52 satisfy\n\u03bb1k1+1, \u03bb 2 k2+1 \u2264 \u03bbk+1 2 , \u2200k = 0, .., Nt\u2212 1. (22)\nNext, from [91, Proposition 2.16] and (20) we have the following estimates:\n\u2016x\u00b5j \u2212 U\u00b5j (:, 1 : kj) x\u0303j\u2016F = Nt\u2211\ni=kj+1\n\u03bbji = O(\u03bb j kj+1 ), (23)\nwhere \u03bbji is the i th singular value of x\u00b5j and x\u0303j \u2208 Rkj\u00d7Nt , for j = 1, 2.\nBy denoting\nx\u0302 = [ x\u03031 01 02 x\u03032 ] ,\nwhere the null matrix 0j belongs to Rkj\u00d7Nt , j = 1, 2, we have\n\u2016x\u0304\u2212 U\u0302 x\u0302\u2016F = \u2016[x\u00b51 x\u00b52 ]\u2212 [U\u00b51(:, 1 : k1)x\u03031 U\u00b52(:, 1 : k2)x\u03032]\u2016F \u2264 (24) \u2016x\u0304\u00b51 \u2212 U\u00b51(:, 1 : k1)x\u03031\u2016F + \u2016x\u0304\u00b52 \u2212 U\u00b52(:, 1 : k2)x\u03032\u2016F \u2264 O(\u03bb1k1+1) +O(\u03bb 2 k2+1) = O(\u03bbk+1). (25)\nIt is crucial for the proof that U\u00b51 and U\u00b52 are rank deficient since they have at least one null singular value. In practice usually k1 + k2 is larger than k thus more bases functions are required to form U\u0302 to achieve a similar level of precision as in (19) where U\u0304 is built using a global singular value decomposition. This matrix factorization of x\u0304 is more costly than both singular value decompositions of matrices x\u00b5j , j = 1, 2, (23) combined for large space dimension Nstate. However the off-line stage of the concatenation method also includes the application of a Gramm-Schmidt-type algorithm to orthogonalize the overall set of vectors in U\u0302 .\nFor full rank matrices the precision is controlled by the spectra of snapshots matrices U\u00b51 and U\u00b52 but there is no guarantee that the concatenated basis U\u0302 can provide similar accuracy precision as U\u0304 in (19) for all k = 1, 2, .., Nt.\nWhile the Lagrange interpolation of bases mixes the different energetic singular vectors in an order dictated by the singular values magnitude, this strategy concatenates the dominant singular vectors for each case and preserves their structure."}, {"heading": "5.3 Interpolation of high-fidelity model solutions", "text": "The method discussed herein assumes that the model solution dependents continuously on the parameters. Thus it is natural to consider constructing the basis for a new parameter configuration by interpolating the existent high-fidelity model solutions associated with\nvarious parameter settings, and then performing a SVD factorization of the interpolated results. For example, the Lagrange solution interpolant is given by\nx\u00b50 = M\u2211 j=1 x\u00b5jLj(\u00b5 0), (26)\nwhere x\u00b5j \u2208 RNstate\u00d7Nt is the model solution corresponding to parameter \u00b5j and the interpolation polynomials are defined in (18). A new basis is constructed from the interpolated model solution (26) for the new parametric configuration \u00b50. By integrating the corresponding reduced order model the output projected solution will present variations in comparison with the high-fidelity interpolation solution x\u00b50 thus the nonlinear dynamics of the model influence the final solution too.\nFrom computational point of view the complexity of the off-line stage of the solution interpolation method (26) is smaller than in the case of the bases concatenation and interpolation approaches. Only one singular value decomposition is required in contrast with the multiple factorizations needed in the latter two strategies where the involved matrices have the same size Nstate \u00d7Nt. Having only Nt snapshots the size of the outcome basis should be smaller than in the case of basis concatenation approach.\nIf for each model solution x\u00b5j a singular value decomposition is available such that\nx\u00b5j \u2248 U\u00b5j x\u0303j, j = 1, 2, ..,M, (27)\nthen from (26) we get\nx\u00b50 \u2248 M\u2211 j=1 Lj(\u00b5 0)U\u00b5j x\u0303j . (28)\nNow the basis U\u00b50 associated with the new configuration is obtained following a matrix factorization, i.e..\nM\u2211 j=1 Lj(\u00b5 0)U\u00b5j x\u0303j = U\u00b50S\u00b50V T \u00b50 . (29)\nIn the basis interpolation method (18), the basis U\u00b50 (denoted in (30) by U\u0304\u00b50 to differentiate it from the basis described in equation (29)) corresponding to the new parametric setting \u00b50 is derived from the following\nM\u2211 j=1 Lj(\u00b5 0)U\u00b5j = U\u0304\u00b50 S\u0304\u00b50 V\u0304 T \u00b50 . (30)\nWhile there is a close relationship between the philosophy behind the solution interpolation method and the bases interpolation strategy, the algebraic formulations of the bases U\u00b50 (29) and U\u0304\u00b50 (30) are significantly different. Consequently an assumption of linear variation of the basis over the parametric interval does not imply that the high-fidelity solution varies linear over the parametric domain. The inverse proposition does not hold neither thus the choice of method depends on the model under study and its input."}, {"heading": "6 Numerical Experiments", "text": "We illustrate the application of the proposed machine learning methodologies to the construction of error models for the reduced order models solutions for a one-dimensional Burgers model and their subsequent utilizations. The model proposed herein is characterized by two scalar parameters, but the envisioned parametric map is designed to cover variation in the viscosity coefficient space only. For each of the problems introduced in Section 3 (constructing the parametric map, selecting the best two already existing bases for a new parameter configuration from accuracy point of view, and selecting the dimension of the reduced basis) we present in detail the proposed solution approaches and the corresponding numerical results.\nTo assess the performance of probabilistic models we employ various cross validation tests. The dimensions of the training and testing data sets are chosen empirically based on the number of samples. For artificial neural network models the number of hidden layers and neurons in each hidden layer vary for each type of problems under study. The squared-exponential-covariance kernel (13) is used for Gaussian process models."}, {"heading": "6.1 One-dimensional Burgers\u2019 equation", "text": "Burgers\u2019 equation is an important partial differential equation from fluid mechanics [14]. The evolution of the velocity u of a fluid evolves according to\n\u2202u \u2202t + \u03bdu \u2202u \u2202x = \u00b5\n\u22022u \u2202x2 , x \u2208 [0, L], t \u2208 (0, tf], (31)\nwith tf = 1 and L = 1. Here \u00b5 is the viscosity coefficient.The parameter \u03bd has no physical significance and it is used to control the non-linear effect of the advection term. The model has homogeneous Dirichlet boundary conditions u(0, t) = u(L, t) = 0, t \u2208 (0, tf]. A smooth initial condition is used, described by a seventh degree polynomial and shown in Figure 2."}, {"heading": "6.2 Designing the parametric map", "text": "The discretization uses a spatial mesh of Ns = 201 equidistant points on [0, L], with \u2206x = L/(Ns \u2212 1). A uniform temporal mesh withNt = 301 points covers the interval [0, tf], with \u2206t = tf/(Nt\u22121). The discrete velocity vector is u(tj) \u2248 [u(xi, tj)]i=1,2,..,Nstate \u2208 RNstate , N = j = 1, 2, ..Nt, where Nstate = Ns \u2212 2 (the known boundaries are removed). The semi-discrete version of the model (31) is:\nu\u2032 = \u2212\u03bdu Axu + \u00b5Axxu, (32)\nwhere u\u2032 is the time derivative of u, andAx, Axx \u2208 RNstate\u00d7Nstate are the central difference first-order and second-order space derivative operators, respectively, which take into account the boundary conditions too. The model is implemented in Matlab and the backward Euler method is employed for time discretization. The nonlinear algebraic systems are solved using Newton-Raphson method and the allowed number of Newton iterations per each time step is set to 50. The solution is considered accurate enough when the euclidian norm of the residual is less then 10\u221210.\nThe viscosity parameter space is set to the interval [0.01, 1]. Smaller values of \u00b5 correspond to sharper gradients in the solution, and lead to a dynamics that is more difficult to approximate by a reduced order model."}, {"heading": "6.2 Designing the parametric map", "text": "We seek to build a parameter map for the 1D-Burgers model for the viscosity domain consisting in the interval [0.01, 1]. The nonphysical parameter \u03bd is set to 1. As discussed in Section 3, we take the following steps. First we construct probabilistic models to approximate the error of reduced order solution. Next, we identify \u201c\u00b5-feasible\u201d intervals [d`, dr] in the parameter space such that local reduced order model depending only on the high-fidelity trajectory at \u00b5 is accurate within the prescribed threshold for any \u00b50 \u2208 [d`, dr]. Finally, a greedy algorithm generates the parametric map by covering the parameter space with a union of \u00b5i feasible intervals.\n[0.01, 1] \u2282 M\u22c3 i=1 [ di`, d i r ] , (33)\nwhere each \u00b5i-feasible interval is characterized by an error threshold \u03b5\u0304i (which can vary from one interval to another). This relaxation is suggested since for intervals associated with small parameters \u00b5, it is difficult to achieve small reduced order models errors similar to those obtained for larger parametric configurations. One way to mantain the error thresholds constant is to start with a larger proposal.\nIn existing reduced basis methods a global reduced order model depending on multiple high-fidelity trajectories is constructed. In contrast, our approach decomposes the parameter space into smaller regions where the local reduced order model solutions are accurate within some tolerance levels. Since the local bases required for the construction of the local reduced order models depend on only a single full simulation, the size of the reduced manifolds is small, leading to lower on-line computational complexity."}, {"heading": "6.2.1 Error estimation of ROM solutions", "text": "We first construct probabilistic models \u03c6 : z\u2192 \u03b5\u0302, (34)\nwhere the input features z include a new viscosity parameter value \u00b50, a parameter value \u00b5 associated with the full model run that generated the basis U\u00b5, and the dimension of the reduced manifold. The target \u03b5\u0302 is the estimated error of the reduced order model solution at \u00b50 using the basis U\u00b5 and the corresponding reduced operators computed using the Frobenius norm\n\u03b5 = \u2016[x(\u00b50, ti)]i=1,..,Nt \u2212 U\u00b5[x\u0303(\u00b50, \u00b5, ti)]i=1,..,Nt\u2016F . (35)"}, {"heading": "6.2 Designing the parametric map", "text": "The training data set includes equally distributed values of \u00b5 and \u00b50 over the entire interval, \u00b5 \u2208 {0.1, 0.2, . . . , 0.9, 1} and \u00b50 \u2208 {0.01, 0.02, . . . , 0.99, 1}, respectively, reduced basis dimensions spanning the interval [4, 5, . . . , 14, 15] and the reduced order model error \u03b5. The entire training data set contains nearly 12, 000 samples, and for each sample a high-fidelity model solution is calculated. Figure 3(b) shows isocontours of the reduced order model error \u03b5 for viscosity parameter values \u00b50 and various POD basis dimensions. The design of the reduced order models relies on the high-fidelity trajectory for \u00b5 = 0.8. Since target values \u03b5 vary over a wide range (from 300 to 10\u22126) we consider the logarithms of the errors log(\u03b5) to decrease the variance of the predicted results, i.e.\n\u03c6 : z\u2192 l\u0302og(\u03b5). (36)\nFigure 3(a) shows isocontours for the logarithms of the errors log(\u03b5).\nWe construct two probabilistic models for estimating the ROM model errors, the first one uses a Gaussian process with a squaredexponential covariance kernel (13) and the second one uses a neural network with six hidden layers and hyperbolic tangent sigmoid activation function in each layer. Tables 1 and 2 show the averages and variances of errors in prediction provided by GP and ANN for different sample sizes. The results are obtained using a conventional validation with 80% of the whole data set involved in training process and the remaining 20% employed for testing. The misfit is computed using the same formulas presented in (11a) to evaluate the prediction errors of one-fold set in the K-fold cross validation approach. Table 2 shows the prediction errors of (34) computed via equation (11a) with y = \u03b5 and y\u0302 = \u03b5\u0302, i.e. no data scaling; the predictions have a large variance and a low accuracy. Scaling the data and targeting log(\u03b5) results using (36), reduce the variance of the predictions, and increase the accuracy, as shown in Table 1. The same formula (11a) with y = log(\u03b5) and y\u0302 = l\u0302og(\u03b5) was applied. In both cases ANN outperforms the GP. Moreover, as the number of data points grows, the accuracy increases and the variance decreases faster for ANN.\n6.2 Designing the parametric map\nFigures 4 and 5 show the corresponding histogram of the predicted models errors (36) and (34) using 100 and 1000 training samples for both ANN and GP models. As the number of training samples increase, the uncertainty in the prediction decreases. The histograms can also asses the validity of GP assumptions (16), (12), (14). The difference between the true and estimated values should behave as samples from the distribution N (0, \u03c32n) [25]. In our case they are hardly normally distributed and this indicates that the data set for the problems we are working with, are not from Gaussian distributions.\nScaling the data and targeting log \u03b5 errors clearly improve the performance of our probabilistic models. Consequently for the rest of the manuscript we will only use model (36). To asses the quality of the probabilistic models a five-fold cross-validation process is also used. The results computed using formula (11b) are shown in Table 3. Neural network outperforms the Gaussian process and estimates the errors more accurately. It also has less variance than the Gaussian process which indicates it has more stable predictions."}, {"heading": "6.2 Designing the parametric map", "text": "Figure 6 illustrates the average of errors in predictions over five different ANN and GP configurations. In each configuration, the machine is trained on random 80% split of data set and tested on the fixed selected test data shown in figure 6. Getting the average of predictions on different trained models decreases the bias in predictions."}, {"heading": "6.2 Designing the parametric map", "text": ""}, {"heading": "6.2.2 Construction of a \u00b5\u2212feasible interval", "text": "We saw in the previous subsection that probabilistic models can accurately estimate the error \u03b5 (35) associated with reduced order models. Thus we can employ them to establish a range of viscosity parameters around \u00b5 such that the reduced order solutions depending on U\u00b5 satisfy some desired accuracy level. More precisely, starting from parameter \u00b5, a fixed POD basis dimension and a tolerance error log(\u03b5\u0304), we are searching for an interval [dl, dr] such that the estimated prediction l\u0302og(\u03b5) of the true error log(\u03b5) (35) meets the requirement\nl\u0302og(\u03b5) < log(\u03b5\u0304),\u2200\u00b50 \u2208 [dl, dr]. (37)\nOur proposed strategy makes use of a simply incremental approach by sampling the vicinity of \u00b5 to account for the estimated errors l\u0302og(\u03b5) forecasted by the probabilistic models defined before. A grid of new parameters \u00b50 is build around \u00b5 and the machines predict the errors outward of \u00b5. Once the machines outputs are larger than the prescribed error log(\u03b5\u0304), the previous \u00b50 satisfying the constrain (37) is set as dl, for \u00b50 < \u00b5 or dr for \u00b50 > \u00b5.\nFigure 7 illustrates the range of parameters estimated by the neural network and Gaussian process against the true feasible interval and the results show good agreement. For this experiment we set \u00b5 = 0.7, dimension of POD=9 and error threshold \u03b5\u0304 = 10\u22122. Values of \u00b50 = \u00b5 \u00b1 0.001 \u00b7 i, i = 1, 2, .. are passed to the probabilistic models. The average range of parameters obtained over five different configurations with neural network is [0.650, 0.780] while in the case of Gaussian process we obtained [0.655, 0.780]. In each configuration, we train the model with 80% random split of the data set and test it over the fixed test set of figure 7. For this design, the true range of parameters is [0.650, 0.785] underlying the predicting potential of machine learning models."}, {"heading": "6.2 Designing the parametric map", "text": ""}, {"heading": "6.2.3 The parametric map as a reunion of \u00b5\u2212feasible intervals", "text": "A reunion of different \u00b5k-feasible intervals can be designed to cover a general entire 1D-parametric domain [A,B]. We refer to this reunion as a parametric map and once such construction is available will allow for reduced order simulations with a-priori error quantification for any value of viscosity parameter \u00b50 \u2208 [A,B].\nA greedy strategy is described in Algorithm 3 and its output is a collection of feasible intervals \u222ank=1[dkl , dkr ] \u2283 [A,B]. Each interval [dkl , d k r ] is associated with some accuracy threshold \u03b5\u0304k. For small viscous parametric values we found out that designing \u00b5k\u2212feasible intervals associated with higher precision levels (i.e. very small thresholds \u03b5\u0304k) is impossible since the dynamics of parametric 1DBurgers model solutions changes dramatically with smaller viscosity parameters. In consequence we decided to let \u03b5\u0304k vary along the parametric domain to accommodate the solution physical behaviour. Thus a small threshold \u03b5\u03040 will be initially set and as we will advance charting the parameter interval [A,B] from right to left, the threshold \u03b5\u0304k will be increased.\nThe algorithm starts by selecting the first centered parameter \u00b50 responsible for basis generation. It can be set to \u00b50 = B but may take any value in the proximity of B, \u00b50 \u2264 B. This choice depends on the variability of parametric solutions in this domain region and by electing \u00b50 to differ from the right edge of the domain, the number n of feasible intervals will be decreased.\nThe next step is to set the threshold \u03b5\u03040 along with the maximum permitted size of the initial feasible interval to be constructed. This is set to 2 \u00b7 r0, thus r0 can be referred as the interval radius. Along with the radius, the parameter \u2206s will decide the maximum number of probabilistic model calls employed for the construction of the \u00b50-feasible interval. While the radius is allowed to vary during the algorithm iterations, \u2206s is kept constant. Finally the dimension of POD basis has to be selected together with three parameters \u03b21, \u03b22 and \u03b23 responsible for changes in the threshold, radius and selecting a new parameter location \u00b5k encountered during the procedure.\nThe instructions between lines 5 and 20 generate the \u00b5k-feasible interval, for the case when the current centered parameter \u00b5k represents an interior point of [dkl , d k r ]. For situation when \u00b5k = d k l or \u00b5k = d k l , the threshold has to be increased (by setting \u03b5\u0304k+1 = \u03b21\u03b5\u0304k at line 22), since the reduced order model solutions can not satisfy the desired precision according to the estimated probabilistic errors. At this stage the radius is decreased too since we reached a parameter region where model dynamics changes rapidly and a larger feasible interval is not possible. Once the new centered parameter \u00b5k+1 is proposed the algorithm checks if the following constrain is satisfied\n[dk+1l , d k+1 r ] \u22c2( k\u22c3 i=1 [dil, d i r] ) 6= \u2205. (38)\nThis is achieved by checking the estimated reduced order model solution error at dkl using the basis defined by the high-fidelity trajectory at \u00b5k+1 (see instruction 25). If the predicted error is smaller than the current threshold, assuming a monotonically increasing error with larger distances d(\u00b50, \u00b5k+1), the reduced order model solutions should satisfy the accuracy threshold for all \u00b50 \u2208 [\u00b5k+1, dkl ]. In consequence the equation (38) will be satisfied for the current \u00b5k+1, if we set rk+1 = \u00b5k+1 \u2212 dkl (see instruction 28). In the case the error estimate is larger than the present threshold, the centered parameter \u00b5k+1 is updated to the middle point between old \u00b5k+1 and dkl . For the situation where the monotonic property of the error does not hold in practice, a simply safety net is used at instruction 12. The entire algorithm stops when \u00b5k \u2264 A."}, {"heading": "6.2 Designing the parametric map", "text": "For our experiments we set A = 0.01, B = 1, \u03b5\u03040 = 1.e\u2212 2, \u2206s = 5.e\u2212 3, r0 = 0.5, dim = 9, \u03b21 = 1.2, \u03b22 = 0.9 and \u03b23 = 1.4. We initiate the algorithm by setting \u00b50 = 0.87, and the first feasible interval [0.7700, 1.0500] is obtained. Next the algorithm selects \u00b51 = 0.73 with the associated range of [0.6700, 0.8250] using the same initial threshold level. As we cover the parametric domain from right to left, i.e. selecting smaller and smaller parameters \u00b5k, the algorithm enlarges the current threshold \u03b5\u0304k, otherwise the reduced order models would not satisfy the initial precision. We continue this process until we get the threshold 6.25 with \u00b532 = 0.021 and the corresponding feasible interval [0.00940, 0.039]. The obtained parametric map is depicted in Figure 8 where the associated threshold varies with the parameter change.\nAlgorithm 3 Generation of parametric map for reduced order models usage 1: Select \u00b50 as the right edge of the parameter interval, i.e. \u00b50 = B. 2: Set error threshold \u03b5\u03020, step size \u2206s for selection of new parameter locations \u00b50, the maximum search radius r0, dimension of POD\nbasis dim and \u03b21, \u03b22 and \u03b23. 3: Set k = 0. 4: DO 5: FOR i=1 to int( rk\u2206s ) 6: Set \u00b5o+ = \u00b5k + i\u2206s 7: IF \u03c6(\u00b5o+, \u00b5k, dim) > log(\u03b5\u0304k) THEN 8: Set dkr = \u00b5k + (i\u2212 1)\u2206s. EXIT. 9: END IF\n10: END FOR 11: IF k > 0 THEN 12: IF dkr < d k\u22121 l THEN \u00b5k = \u00b5k+d k\u22121 l\n2 . GOTO 5. 13: END IF 14: END IF 15: FOR j = 1 to int( rk\u2206s ) 16: Set \u00b5o\u2212 = \u00b5k \u2212 j\u2206s 17: IF \u03c6(\u00b5o\u2212, \u00b5k, dim) > log(\u03b5\u0304k) THEN 18: Set dkl = \u00b5k \u2212 (j \u2212 1)\u2206s. EXIT. 19: END IF 20: END FOR 21: IF (i=1).OR.(j=1) THEN 22: Set \u03b5\u0304k = \u03b21 \u00b7 \u03b5\u0304k; rk = \u03b22 \u00b7 rk; GOTO 5. 23: ELSE \u00b5k+1 = \u00b5k \u2212 \u03b23(j \u2212 1)\u2206s; \u03b5\u0304k+1 = \u03b5\u0304k. 24: END IF 25: WHILE \u03c6(dkl , \u00b5k+1, dim) > log(\u03b5\u0304k+1) DO 26: \u00b5k+1 = \u00b5k+1+d k l\n2 . 27: END WHILE 28: Set rk+1 = \u00b5k+1 \u2212 dkl . 29: k = k + 1. 30: WHILE \u00b5k \u2265 A THEN STOP."}, {"heading": "6.3 Select the best already existing ROMs for a new parameter value \u00b50", "text": ""}, {"heading": "6.3 Select the best already existing ROMs for a new parameter value \u00b50", "text": "An important and practical concern associated with the reduced order models is their lack of robustness with respect to parameter change. Here, we propose to solve another practical problem, i.e. giving a collection of reduced bases computed at various locations in the parameter space, find the one that proposes the most accurate reduced order solution for a new viscosity parameter \u00b50. We will rely on similar probabilistic models built in subsection 6.2.1. The input features for the GP and ANN are the new parameter \u00b50, a parameter \u00b5 whose corresponding trajectory is used as snapshots for generating basis U\u00b5 and the dimension of the reduced manifold. The target random variable y\u0302 = \u0302log \u03b5 is the estimated log of error of the reduced order model solution at \u00b50 using the basis U\u00b5. For our experiments, approximately 12, 000 samples were generated and include different values of POD basis dimensions from 4 to 15, new viscosity parameter \u00b50 \u2208 [0.1, 1] and parameters \u00b5k \u2208 [ 10\u22122, 1 ] equally distributed with interval 0.1 and 0.01, respectively and the corresponding log of the Frobenius norm of true ROM errors log(\u03b5) (35). The data set constructed for this problem is similar to the one employed for designing the errors models. For that problem we fixed the parameter \u00b5 associated with the high-fidelity trajectory used for basis generation and computed the ROM errors corresponding to different \u00b50. Here we fixed \u00b50 and then computed the ROM errors using different bases U\u00b5.\nThe data set is randomly partitioned into 5 equal size sub-samples and a 5\u2212 fold cross-validation process is used to asses the quality of the probabilistic models. A neural network with 6 hidden layers and hyperbolic tangent sigmoid activation function in each layer is used while for the Gaussian process we have employed the squared-exponential-covariance kernel (13). Table 4 shows numerical results obtained with Neural networks and Gaussian process, over five folds using (11b). The Gaussian process outperforms the neural network and has less variance which indicates the GP is more accurate and stable for this specific problem than neural network.\nThe results in Figure 9 illustrate the errors in prediction of the reduced order models errors for two viscosity parameters \u00b50 = 0.35 and 0.65 and various bases represented along the y axis. The mean of the estimates in the case of the ANN are closer than the true errors in comparison with the output of the Gaussian Process for this particular example.\nMoreover we can notice that the estimation curves are crossing initially close to \u00b5 = 0.45. It suggests that one can choose the high-fidelity trajectory \u00b5 = 0.45 to construct a reduced order basis such that to obtain similar accuracy levels for both reduced order solutions computed at new viscosity parameters \u00b50 = 0.35 and 0.65. This reveals the non-monotonic property of the reduced order model error with respect to the distance d(\u00b50, \u00b5)."}, {"heading": "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": ""}, {"heading": "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": "Experiments in subsection 6.3 revealed the potential of probabilistic models to select a hierarchy of reduced manifolds that produces higher accurate solutions. Figure 9 depicts the accuracy of reduced order models for \u00b50 = 0.35 and 0.65 using POD basis computed at various locations in the parameter interval. Assuming only 10 existing POD subspaces constructed for parameters equally distributed along the interval [0.1, 1], \u00b5 = 0.1, 0.2, 0.3, .., figure 9 shows that the most accurate reduced solutions for \u00b50 = 0.35 are produced using the bases computed at \u00b51 = 0.3 and \u00b52 = 0.4. Consequently, the numerical experiments described here focus on the construction of a POD basis for \u00b50 = 0.35 by combining the data available for \u00b51 = 0.3 and \u00b52 = 0.4.\nThe performances of the discussed methods (bases concatenation, Lagrange interpolation of bases in the matrix space and in the tangent space of the Grassmann manifold, Lagrange interpolation of high-fidelity solutions) are shown in the case of three main experiments: variation in the final time tf , in the non-linear advection coefficient \u03bd and POD basis size. The first two experiments scale the time and space and modify the linear and nonlinear characteristics of the model. For example, in the case of a tiny small final time and advection coefficient, the diffusion linear part represents the main dynamical engine of the model thus it behaves linearly. The results are compared against reduced order models constructed using U\u00b51 and U\u00b52 , respectively.\nThe experiments make use of a space mesh of 201 points while 301 time steps are used. Figure 10 illustrates the Frobenius norm error between the high fidelity and reduced order model solutions for the final time tf = 0.01. Panel (a) presents the accuracy results as a function of the advection coefficient \u03bd. Interpolating the high-fidelity solutions leads to the most accurate reduced order model. For large advection coefficients all of the methods suffer accuracy losses. Among the potential explanations we include the constant size of the POD basis and its linear dependence on the viscosity parameter assumed by all of the methods in various forms. Keeping the POD basis size constant represents a source of errors as seen in Figure 3 where the viscosity parameter is varied.\nBy interpolating the geometry characteristics of the reduced subspaces via subspace angle interpolation or Grassmann manifold approach (only one is shown in the figures since for interpolating two bases the methods are the same) we expect to obtain more accurate reduced order models. While Lagrangian interpolation of the bases is performed in both matrix space and tangent space of the Grassmann manifold (shown in cyan and green in Figure 10, the later approach performs better in this scenario confirming our expectations. The concatenation of bases using Gram-Schmidt was successful only for larger advection coefficients (red curve in Figure 10)(a) for a POD size set to 14.\nIncreasing the dimension of the basis enhances the so called Gram-Schmidt reduced order model solution accuracy for \u03bd = 1 (see Figure 10(b)). For this case Lagrange interpolation in the matrix space shows better performances in comparison with the output of the Grassmann manifold approach.\nNext we increase the nonlinearity characteristics of the model by setting the final time to tf = 1 and Figure 11 illustrates the Frobenius norm errors as a function of the advection coefficient \u03bd and POD dimension size. The errors produced by the reduced order model derived via Grassmann manifold method are similar with the ones obtained by the surrogate model relying on a POD basis computed via the Lagrange interpolation of the high-fidelity model solutions.\nThe Lagrange interpolation of bases in the matrix space is not successful as seen in both panels of figure 11. Increasing the POD size to 20, the Gram-Schmidt approach enhance the accuracy of the solution (see Figure 11(b))."}, {"heading": "6.5 Optimal size of reduced order model", "text": ""}, {"heading": "6.5 Optimal size of reduced order model", "text": "Here we propose two alternative approaches to select the reduced basis size that accounts for specified accuracy levels in the reduced order model solutions. These techniques employ construction of probabilistic models \u03c6 : z \u2192 y\u0302 via neural network and Gaussian process as stated in section 4. The input features z for this problem consist of the viscosity parameter \u00b5 \u2208 [0.01, 1] and the log of the Frobenius norm of the error between the high-fidelity and reduced order models log(\u03b5) (35). The searched output y\u0302 is the dimension of the reduced manifold d. For the training phase, the data set is generated using several runs of the 1D-Burgers model with various viscosity parameters \u00b5, different basis sizes d and the log of the Frobenius norms of the discrepancies between the full and the projected reduced order solutions log(\u03b5). The machines will learn the sizes of reduced order basis d associated with the parameter \u00b5 and the corresponding log(\u03b5). Later it will be able to estimate the proper size of reduced basis by providing it the specific viscosity parameter \u00b5 and the desired error log(\u03b5). The computational cost is low once the probabilistic models are constructed. The output indicates the dimension of the reduced manifold for which the ROM solution satisfies the corresponding error threshold. Thus we do not need to compute the entire spectrum of the snapshots matrix in advance which for large spatial discretization meshes translates into important computational costs reduction.\nFor our experiments, approximately 9000 samples were generated for different values of the viscosity parameter \u00b5 equally distributed within the interval [10\u22122, 1], various reduced basis dimensions from 4 to 15 and the corresponding log(\u03b5). Figure 12 illustrates the contours of the log of reduced order model error, over the viscosity parameter domain and various POD sizes.\nA neural network with 5 hidden layers and hyperbolic tangent sigmoid activation function in each layer is used while for the Gaussian process we have used the squared-exponential-covariance kernel (13). Table 5 show the average and variance of error in GP and ANN estimations using different sample sizes. The ANN outperforms the GP and as the number of data points grows, the accuracy increases"}, {"heading": "6.5 Optimal size of reduced order model", "text": "and the variance decreases. The results are obtained using a conventional validation with 80% of the sample size dedicated for training data and the other 20% for the test data. The employed formula is described in equation (11a).\nFigures 13 and 14 shows the corresponding errors in estimation on 100 and 1000 training samples for both ANN and GP model. These histograms as stated before, can assess the validity of GP assumptions. The data set distribution shape is closer to a Gaussian profile than in the case of the data set distribution discussed in section 6.2.1 used for generation of reduced order model error probabilistic models.\nTo assess the accuracy of the probabilistic models, the data set is randomly partitioned into 5 equal size sub-samples, and 5\u2212 fold cross-validation test is implemented. The 5 results from the folds are averaged and they are presented in table 6. The neural network model correctly estimated the size of the reduced manifold in 87% cases. Gaussian process correctly estimates the POD size 53% of the times. The variance results shows that the GP model has more stable predictions indicating a higher bias in the outcomes.\nIn figure 15, we compare the output of our probabilistic approaches against the eigenvalues estimation on a set of randomly selected"}, {"heading": "6.5 Optimal size of reduced order model", "text": "test data. The eigenvalue estimation is the standard method for selecting the optimal reduced manifold dimension when a prescribed level of accuracy of the reduced solution is desired. Here the desired accuracy \u03b5 was set to 10\u22123. The mismatches between the predicted and true dimensions are depicted in the figure 15. The predicted values are the averages over five different models where each model of ANN and GP are trained on random 80% split of dataset and tested on the fixed selected 20% test data. We notice that the snapshots matrix spectrum underestimates the true size of the manifold as expected since the errors due to integration in the reduced space are not accounted. The neural network predictions were extremely accurate for most of the samples while the Gaussian process usually overestimated the reduced manifold dimensions."}, {"heading": "7 Conclusions", "text": "This work demonstrates the value of machine learning approaches to guide the construction of parametric space partitioning for the usage of efficient and accurate local reduced order models. While the current methodologies are defined in the sense of Voronoi tessellation [26] and rely on K-means algorithms [5, 6, 47], our approach delimitates sub-regions of the parametric space by making use of Gaussian Processing and Artificial Neural Networks models for the errors of reduced order models and parametric domain sampling. The employed machine learning models differ from the one proposed in [25] having more additional features such as reduced subspace dimension and are specially projected for accurate predictions of local reduced order models errors. For each sub-region, an associated reduced order basis and operators are constructed depending on a single representative high-fidelity trajectory and, the corresponding local reduced order models solutions have known precision levels. The novel methodology is applied for a 1D-Burgers model, and a parametric map covering the viscosity domain with parametric sub-intervals and associated errors thresholds is designed.\nOur numerical experiments revealed the non-monotonic property of the reduced order model error with respect to the distance between the current parametric configuration and the one used to generate the reduced subspace. Thus we proposed machine learning models for selecting a hierarchy of reduced bases producing the most accurate solutions for a new parameter configuration. Based on this hierarchy, three already existing methods involving bases interpolation and concatenation and high-fidelity model solutions interpolation are applied to enhance the quality of the associated reduced order model solutions. It has been shown that the assumption of linear variation of the basis over the parametric space leads to a different reduced basis formulation than if the linear variation hypothesis of the high-fidelity solution over the parametric domain is followed. Several experiments were performed by scaling the time and space and modifying the nonlinear characteristics of the model. In most cases, interpolating the already existing high-fidelity trajectories generated the most accurate reduced order models for a new viscous parameter revealing that the solution behavior over the parametric region under study can be linearly approximated. Lagrange interpolation of bases in the tangent space of the Grassmann manifold and concatenation of bases for larger reduced subspaces showed also good performances.\nFinally we addressed the problem of selecting the dimension of the reduced order model when its solution must satisfy a desired level of accuracy. Our approach based on learning better estimates the ROM basis dimension in comparison with the results obtained by truncating the spectrum of the snapshots matrix.\nA future goal is to decrease the computational complexity of the parametric map design procedure. Currently the training data required by the probabilistic models relies on many high-fidelity simulations. By employing fast a-posteriori error estimation results [63], this dependency will be much decreased. In addition we plan to incorporate residual norm and rigorous error bounds among the data fitting models features to enhance their prediction capabilities as remarked in [25]."}, {"heading": "Acknowledgements", "text": "This work was supported in part and by the award NSF CCF 1218454 and by the Computational Science Laboratory at Virginia Tech."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Reduced order models are computationally inexpensive approximations that capture the important dynamical characteristics of large, high-fidelity computer models of physical systems. This paper applies machine learning techniques to improve the design of parametric reduced order models. Specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, to chose the best two already existing bases for a new parameter configuration from accuracy point of view and to pre-select the optimal dimension of the reduced basis such as to meet the desired accuracy. By combining available information using bases concatenation and interpolation as well as high-fidelity solutions interpolation we are able to build accurate reduced order models associated with new parameter settings. Promising numerical results with a viscous Burgers model illustrate the potential of machine learning approaches to help design better reduced order models. key words reduced order models, high-fidelity models, data fitting, machine learning, feasible region of parameters, local reduced order models.", "creator": "LaTeX with hyperref package"}}}