{"id": "1511.03771", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Improving performance of recurrent neural network with relu nonlinearity", "abstract": "in recent years significant progress has undoubtedly been made in successfully training recurrent neural networks ( rnns ) on sequential sequence learning problems involving long range temporal scaling dependencies. the progress has been made also on three fronts : ( a ) algorithmic improvements involving sophisticated optimization techniques, ( b ) network design involving complex hidden layer nodes and specialized recurrent layer connections and ( c ) weight initialization methods. in this paper, we focus on recently proposed weight initialization with identity matrix for changing the recurrent weights in a rnn. this value initialization treatment is specifically tentatively proposed for hidden nodes with rectified elementary linear unit ( relu ) non linearity. we offer a simple dynamical logic systems perspective on weight initialization treatment process, which allows us to propose a modified multiple weight initialization strategy. but we show that this initialization technique leads to successfully training other rnns composed also of relus. we demonstrate that changing our proposal too produces comparable or better linear solution for three toy problems involving evolving long range temporal structure : consider the addition problem, the multiplication problem and the mnist classification problem using integrated sequence of pixels. in addition, we present results for a benchmark action recognition problem.", "histories": [["v1", "Thu, 12 Nov 2015 04:35:41 GMT  (414kb,D)", "http://arxiv.org/abs/1511.03771v1", "10 pages 6 figures; under consideration for publication with ICLR 2016"], ["v2", "Mon, 11 Jan 2016 01:14:54 GMT  (471kb,D)", "http://arxiv.org/abs/1511.03771v2", "10 pages 6 figures; under consideration for publication with ICLR 2016"], ["v3", "Thu, 23 Jun 2016 12:52:26 GMT  (472kb,D)", "http://arxiv.org/abs/1511.03771v3", "10 pages 6 figures; under consideration for publication with ICLR 2016"]], "COMMENTS": "10 pages 6 figures; under consideration for publication with ICLR 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sachin s talathi", "aniket vartak"], "accepted": false, "id": "1511.03771"}, "pdf": {"name": "1511.03771.pdf", "metadata": {"source": "CRF", "title": "IMPROVING PERFORMANCE OF RECURRENT NEURAL NETWORK WITH RELU NONLINEARITY", "authors": ["Sachin S. Talathi", "Aniket Vartak"], "emails": ["stalathi@qti.qualcomm.com", "avartak@qti.qualcomm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recurrent neural networks (RNNs) are neural networks with cyclical connections between the hidden nodes. These cyclical connections offer RNNs the ability to encode memory and as such these networks, if successfully trained, are suitable for sequence learning applications. Traditionally, training RNNs using stochastic gradient descent methods such as back-propagation through time (BPTT) (Rumelhart et al., 1986) have been riddled with difficulty. Early attempts suffered from the so-called vanishing gradient or exploding gradient problems resulting in difficulties to learn longrange temporal dependencies (Hochreiter et al., 2001).\nSeveral methods have been proposed to overcome the difficulty in training RNNs. These methods broadly fall into three categories: (i) Algorithmic improvements involving sophisticated optimization techniques. The Hessian-Free (HF) optimization method (Martens, 2010; Martens & Sutskever, 2011) for training RNNs falls under this category. HF method works by taking large steps in the weight space in directions of low gradient and curvature. As noted by the authors, it is the ability of HF to identify and pursue these directions, which allow HF to fully optimize deep neural networks, such as the RNNs unfolded in time. (ii) Network design, the most successful technique to date under this category is the Long Short Term Memory (LSTM) RNN (Hochreiter & Schmidhuber, 1997). The LSTM is a standard RNN in which the monotonic nonlinearity such as the tanh or sigmoid is replaced with a memory unit that can efficiently store continuous values and transmit information over a long temporal range. Each memory unit in the LSTM has fixed linear dynamics with a feedback loop of weight one. As a result the error signal neither decays nor explodes as it back propagates through the memory unit. LSTM RNNs and variants thereof (Cho et al., 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al., 2014; Bahdanau et al., 2014), image captioning (Kiros et al., 2014), predicting output of simple computer programs (Zaremba & Sutskever, 2014) and action recognition from video clips (Ng Yue-Hui et al.,\nar X\niv :1\n51 1.\n03 77\n1v 1\n[ cs\n.N E\n] 1\n2 N\nov 2\n01 5\n2015). More recently, a modification to the standard RNN architecture, the clockwork RNN (CWRNN) has been proposed (Koutn\u0131\u0301k et al., 2014). The long range dependency problem of vanishing and exploding gradients in the standard RNN is solved in CW-RNN by dividing the hidden layer of the CW-RNN into separate modules, each running at different clock speeds. This allows CW-RNNs to efficiently learn different time scales inherent in complex-signals. (iii) Weight initialization, more recently Quoc et al, (Quoc et al., 2015) proposed a simpler solution to the long-range dependence problem with RNNs composed of rectified linear units (RELU)s. The authors proposed to use the Identity matrix to initialize the recurrent weight matrix.They call the resulting RNN, Identity-RNN (IRNN). The basic idea underlying IRNN is that in the absence of any input, a RNN composed of RELUs and initialized with identity weight matrix just stays the same indefinitely. Experimental results with IRNN on several benchmark problems with long-range temporal structure are comparable to the those produced by LSTM RNNs (Quoc et al., 2015). In this paper, we investigate the significance of identity weight initialization in IRNNs from a dynamical systems perspective. We demonstrate that in absence of any input and with identity weight initialization the hidden nodes exhibit neutrally stable fixed point dynamics. While this observation produces the effect that the hidden node dynamics stays the same indefinitely in absence of any input, the node dynamics is also extremely sensitive to input perturbations. We hypothesize that this sensitivity of hidden node dynamics to input may influence the success of RNN for sequence learning tasks. Motivated by this hypothesis we propose a new weight initialization matrix, the normalized-positive definite weight matrix for RNNs comprised of RELU hidden units. The resulting RNN is refered to as the np-RNN. We show that on several of the tasks investigated by (Quoc et al., 2015), np-RNN performs better than IRNN or the corresponding scaled version of the IRNN, the iRNN, where the recurrent weight matrix is initialized to 0.01I. The paper is organized as follows: In Section 2, we describe the simple RNN (sRNN) model and explain how the identity weight matrix initialization for hidden layer weights provide initial conditions to alleviate the long range problem in training sRNN. We then offer a dynamical systems perspective on the identity matrix weight initialization and present our proposal for weight initialization of hidden layer weight matrix to construct the np-RNN. In Section 3, we present an overview of the experiments to evaluate the performance of np-RNN, followed by results from our experiments on benchmark datasets in Section 4. The conclusion is presented in Section 5.\n2 DYNAMICAL SYSTEMS PERSPECTIVE ON RNNS\nIn Figure 1 we show the schematic of a sRNN. The sRNN consists of an input layer, a hidden layer with recurrent connections and an output layer. Given an input sequence X = {x0, x1, x2 \u00b7 \u00b7 \u00b7xT }, the sRNN will take the input xt \u2208 RN and generate the prediction yt \u2208 RC for the output sequence Z = {z0, z1, z2 \u00b7 \u00b7 \u00b7 zT }, where zt \u2208 RC . In between the input and the output there is a hidden layer with M units, which store information on the previous values (t \u2032 < t) of the input sequence. More precisely, the sRNN takes X as input and generates an estimate Y of the output Z by iterating the\nequations\nst = Whxxt +Whhht\u22121 + bh (1) ht = f(st) (2) ot = Wyhht + by (3) yt = g(ot) (4)\nwhere Whx, Whh and Wyh are the weight matrices, bh and by are the biases, st \u2208 RM and ot \u2208 RC are inputs to the hidden layer and the output layer respectively and f and g are functions. For the purpose of this paper, f is a ReLU and depending on the benchmark under investigations; g is a linear function or a softmax function. In addition; all hidden layer nodes are assumed to be initialized to a fixed bias bi such that at t=0 h0 = bi. Unless otherwise stated, for all experiments we set bi = 0. The objective function, O(\u03b8) for sRNN with a single training pair (x,y) is defined as O(\u03b8) = \u2211 t Lt(z, y(\u03b8)), where \u03b8 represents the set of parameters (weights and biases) in the\nsRNN. For regression problems, Lt = \u2225\u2225(zt \u2212 yt)2\u2225\u2225 and for multi-class classification problems,\nLt = \u2212 \u2211 j ztj log(ytj).\nThe IRNN model proposed by (Quoc et al., 2015) is a special case of sRNN with the following modifications: (i) the nonlinearity f is RELU and (ii) the hidden layer weights Whh are initialized with identity matrix and the bias terms are set to zero. In order to better understand how these modifications may enable RNN to overcome some of the long range problems, let us look at the gradient equation in the BPTT algorithm (Pascanu et al., 2013):\n\u2202C \u2202\u03b8 = \u2211 t\u2264T \u2202Lt \u2202\u03b8\n(5)\n= \u2211 t\u2264T \u2202Lt \u2202hT \u2202hT \u2202ht \u2202+ht \u2202\u03b8\n(6)\nwhere, \u2202hT \u2202ht = \u2202hT \u2202hT\u22121 \u2202hT\u22121 \u2202hT\u22122 \u00b7 \u00b7 \u00b7 \u2202ht+1 \u2202ht\n(7)\nEach Jacobian \u2202ht+1\u2202ht is a product of two matrices: (a) the recurrent weight matrix Whh and (b) diagonal matrix composed of the derivative of non-linearity, f , associated with the hidden nodes. In absence of any input, i.e., xt = 0, and with the choice of initial conditions for IRNN,the 2- Norm of each Jacobian in Equation 7 is identically one and the error gradients do not grow or decay exponential over time.\nIn order to understand what the design of IRNNs means from a dynamical systems perspective, let us consider a simple example of sRNN with two hidden nodes with ReLU nonlinearity. We again assume that the sRNN receives zero input and all the biases are set to zero. We further assume that the recurrent weight matrix, Whh, is positive definite. The dynamics of hidden nodes is then given by the following equation:\nht = Whhht\u22121 if ht\u22121 > 0 = 0 ohterwise (8)\nIn Figure 2a-d, we show the schematic diagram of the phase-space of the hidden node dynamical system in Equation 8, dependent on the eigenvalues, \u03bb, of the recurrent weight matrix (Strogatz, 2014). The IRNN model corresponds to the case shown in Figure 2a. In absence of any perturbations, every choice of initial conditions for the hidden nodes represents a fixed point that is neutrally stable. As a result, the evolution of the hidden node dynamics is strongly dependent on the input perturbations.\nThe dynamics in 2b, corresponds to the case when all eigenvalues are less than 1. In this case, the hidden node dynamical system has a single stable fixed point at the origin. In absence of any input the the hidden nodes will evolve to zero. Increasingly stronger input perturbations are required to prevent the hidden node dynamics from evolving to origin as the hidden nodes evolve closer to the stable fixed point.\nThe case in 2c corresponds to a situation where one eigenvalue is unity and the other eigenvalue is less than 1. In this case, the principal axis with eigenvalue of unity is the stable manifold. The trajectory of hidden nodes all evolve towards this manifold, eliminating the conditions for exploding gradients. The hidden node dynamics do not evolve to origin except for cases where the initial conditions are on one of the principal axes with eigenvalue less than unity, thereby decreasing the likelihood for the system to exhibit the problem of vanishing gradients. We hypothesize that recurrent weight matrix initialized to such a weight matrix offers the best opportunity for training sRNN without the long range problem of vanishing or exploding gradients. We conduct a series of experiments in support of the hypothesis.\nFinally, the case in Figure 2d corresponds to the case when at least one eigenvalue is greater than 1. In this case, the hidden node dynamical system does not have any stable attractor and in absence of any input, the trajectory of hidden nodes will exponentially diverge to\u221e quickly resulting in the exploding gradient problem.\nBased on the above observation, we propose the following equation for choosing the initial recurrent weight matrix, Whh and the corresponding RNN is referred to as the np-RNN. Assuming there are N hidden nodes, we propose the following,\nA = 1\nN\n\u2329 RT , R \u232a e = max(\u03bb(A))\nWhh = A\ne (9)\nwhere,\u3008\u3009 refers to dot product, R is a standard normal matrix with values drawn i.i.d. from a Gaussian distribution with mean zero and unit variance, and \u03bb(A) is the set of all eigenvalues of A. Equation 9 guarantees that Whh is a positive definite matrix with highest eigenvalue of unity and all the remainder eigenvalues less than 1. We refer to sRNN designed with this initial condition as the np-RNN."}, {"heading": "3 EXPERIMENTS", "text": "In the following experiments we compare np-RNN against IRNN, iRNN and RNNs that use ReLU with random Gaussian initialization (gRNN). These are summarized in Table 1. For all our investigations, the RNNs were designed to comprise of one hidden layer with 100 hidden nodes. The values for the non-recurrent weight matrix Why are drawn i.i.d. from a Gaussian distribution with zero mean and variance 1/N and scaled by a factor of \u03b1 given by (Sussillo & Abbott, 2015)\n\u03b1 = \u221a 2 exp\n( 1.2\nmax(N, 6)\u2212 2.4\n) (10)\nThe values for the non-recurrent weight matrix Wyh are also drawn i.i.d. from a Gaussian distribution with zero mean with variance of 2/(fanin+fanout) (Glorot & Bengio, 2019). Unless otherwise stated, all RNNs are trained using stochastic gradient descent (SGD) with BPTT. Except for the benchmark action recognition problem, we did not specifically tune any of the hyperparameters associated with SGD training via a grid search method. We consider the following benchmark datasets in our investigations:\n1 The Addition problem The addition problem is a toy benchmark used to evaluate the power of RNNs in learning long-term dependencies (Hochreiter & Schmidhuber, 1997). The input to the RNN is a two-dimensional sequence X = {x0(t), x1(t)}|Tt=0. At each time step the input consists of a random signal x0 \u2208 [0, 1] and a mask signal x1(t). The mask signal has all zeros except at two time steps when it has a value of 1. The RNN is to be trained to predict the sum of x0 at these two time steps.\n2 The multiplication problem The multiplication problem is another toy benchmark very similar to the addition problem. The difference is that instead of predicting the sum of x0 at two time steps when the mask has a value of 1, the goal is to predict the product of x0 when the masking signal is unity.\n3 MNIST classification with sequential presentation of pixels This is yet another challenging toy problem where the objective is to classify the MNIST digits when the 784 pixels are presented sequentially to the recurrent net. We follow the procedure described in (Quoc et al., 2015) and the RNN reads one pixel at a time in a scanline order, starting at the top left corner of the MNIST image and ending at the bottom right corner of the image. The RNN is asked to predict the label for the input image after reading all the 784 pixel values resulting in a large long range dependency problem.\n4 Action recognition benchmark We consider one real world example of action recognition task based on the standard UCF101 benchmark (Soomro et al., 2012). The task is to classify a given video clip of length varying from 2 s to>10 s into one of the 101 action classes. The benchmark contains a total of 13320 video clips divided into 25 groups. Each group contain video clips from all 101 action classes. Video clips of a given action in a given group share some commonalities. Usually the performance of a given classifier on this data set is evaluated using three splits of the training/testing data. For our experiments we report results for the split one, with training data made up of video clips from groups 8-24 and the test data made up of videos derived from groups 1-7."}, {"heading": "4 RESULTS", "text": ""}, {"heading": "4.1 ADDITION PROBLEM", "text": "As suggested in (Quoc et al., 2015), a basic baseline solution would be to always predict 1 as the output regardless of the inputs. This will produce a Mean Squared Error (MSE) of approximately 0.166. To solve this problem the recurrent net has to remember the two relevant numbers and their sum while ignoring the irrelevant numbers. The task gets difficult with increasing length T of the sequence.\nIn order to evaluate the RNN networks (iRNN,IRNN,np-RNN,gRNN) we generate 100,000 training examples and 10,000 test examples. For all the reported results, we trained the network using SGDBPTT optimization. The learning rate was fixed at 0.001 and the gradient clip parameter was set to 10. The batchsize was set to 1 and all the networks were trained for 10 epochs. Following from (Quoc et al., 2015), we began by evaluating the performance of RNNs on the addition benchmark for sequences of length T = {150, 200, 300, 400}.\nThe results are summarized in Figure 3. We plot the total number of correctly classified sequences in the test data, defined as the sequences for which the absolute prediction error is less than 0.04 (Martens & Sutskever, 2011). For sequences up to T = 300, our results for gRNN and IRNN match those obtained by (Quoc et al., 2015). We also note that the np-RNN performs at par or better than the other RNNs for T <= 300. However, for sequence of length T = 400, contrary to findings in the (Quoc et al., 2015), we were able to train the IRNN as well as the np-RNN. We therefore also evaluated the performance of the RNNs for sequence of length T = 500. Here we find that np-RNN can still be successfully trained whereas the training is challenging for the IRNN and all other RNNs."}, {"heading": "4.2 MULTIPLICATION PROBLEM", "text": "In this case a baseline solution would be to always predict the two numbers to be 0.5 such that the product is 0.25. This will produce a Mean Squared Error (MSE) of around 0.2025. Again the goal is to train RNN to produce MSE much lower than 0.2025. We follow (Martens & Sutskever, 2011) and evaluate the performance of RNNs for sequence of length T = {50, 100, 200}. Since gRNN could not be trained on the addition benchmark for any sequence of length T \u2265 150 , we did not pursue training of gRNN for the multiplication problem. We again generated 100,000 training examples and 10,000 test examples and trained all the networks using SGD-BPTT optimization. The training\ncontinued for 100 epochs with batch size of 16. The learning rate initially was set at 0.0002 and then cooled twice by a factor of 10 in equal intervals. The results are summarized in Figure 4. Similar to the addition benchmark, we plot the total number of correctly classified sequences in the test data, defined as the sequences for which the absolute prediction error is less than 0.04. We see that neither iRNN, nor IRNN can correctly classify greater than 50 % of test sequences of lengths T investigated. However, np-RNN produces classification accuracy > 90 % for all sequences investigated."}, {"heading": "4.3 MNIST CLASSIFICATION WITH SEQUENTIAL PRESENTATION OF PIXELS", "text": "This is another challenging toy problem where the goal is to classify MNIST digits (Lecun et al.) when the 784 pixels are presented sequentially to the RNN. The network is asked to predict the digit after all the 784 pixels are presented to the RNN serially one pixel at a time. We began by first attempting to reproduce the findings from (Quoc et al., 2015). However using the SGD-BPTT optimization, we were unable to train IRNN on MNIST using the set of parameters reported in (Quoc et al., 2015). Using RMSprop (Tieleman & Hinton), however, we were successful in training IRNN with learning parameter of 10\u22126. The np-RNN was trained using SGD-BPTT optimization with cooling. We trained both IRNN and np-RNN network for 100 epochs with batch size of 16, corresponding to about 375000 iterations. The np-RNN was trained for 33 epochs with initial learning rate of 10\u22124, followed by training for another 33 epochs with learning rate cooled by factor of 10 and then again trained unto 100 epochs by cooling learning rate by another factor of 10. In Figure 5, we plot the evolution of the cost function on the test batch as a function of the number of epochs. The resulting accuracy is reported in Table 2. We note that while the IRNN produces accuracy of about 83 %, the np-RNN produces an accuracy of about 92 %, which is inline with the results reported by (Quoc et al., 2015). The accuracy grows to 96.8% on training the np-RNN for 500 epochs,which corresponds to 937500 epochs, while for IRNN trained with the given parameters, the accuracy grows to about 93%. Based on our findings on the performance of IRNN and np-RNN on the action recognition benchmark (see Section 4.4) and our inability to reproduce the results reported in (Quoc et al., 2015), we conclude that IRNN is extremely sensitive to the choice of the training protocol and the hyper parameters of the network. On the other hand, the observation that np-RNN performs as well as published results for this toy problem is encouraging."}, {"heading": "4.4 ACTION RECOGNITION BENCHMARK", "text": "We also benchmarked IRNN, np-RNN and the LSTMs on the UCF-101 action recognition dataset (Soomro et al., 2012). We followed the approach proposed in (Ng Yue-Hui et al., 2015) and trained the RNNs on the sequence of features generated by passing the sequence of video frames through a\npre-trained Convolutional Neural Network (CNN). Motivated by the recent findings of (Jain et al., 2015), we use a CNN trained on 15000 Imagenet object categories as the feature generator. The CNN architecture was modeled after the Googlenet-CNN (Szegedy et al., 2014) and the features were extracted from the last fully-connected layer of the Googlenet-CNN.\nThe UCF101 benchmark contains 13,320 video clips, divided into 25 groups. The groups are divided into 3 splits of training and testing for evaluation. As the training data is very small, we did not fine tune the CNN, but rather only focused on training the RNNs using raw features generated by a pretrained feature learner. Furthermore to avoid overfitting, we introduced dropout between the hidden and the output layer of the RNN.\nOur implementation of LSTM was standard including the forget gate. For all the 3 RNNs, we did a grid search over learning rates l = {10\u22125, 5\u00d7 10\u22125, 10\u22124, 5\u00d7 10\u22124, 10\u22123, 10\u22122} and the dropout parameter d = {0.5, 0.7, 0.9}. The training protocol was: RMSprop for optimization, batchsize of 256, training for a total of 70 epochs, with training for the first 25 epochs with initial learning rate, followed by training for additional 30 epochs with learning rate cooled by factor of 10 and final phase training for additional 15 epochs by cooling the learning rate by another factor of 10. For all our investigations, we used the split one of the training/testing data comprised of video clips from groups 8 through 25 in the training set and video clips from group 1 through 7 in the test set. In order to search over the space of hyperparameters {l, d}, we further split the split one training data into training and validation data set. Data from groups 8 through 24 was used to train all the RNNs and data from group 25 was used for validation. In Table 3 we report the score on the split one test data using RNN with best validation score over the grid search. The scores for np-RNN are closer to LSTM than IRNN and are comparable to the published state-of-the-art for split one of training/test\ndata for the UCF101 benchmark (Ng Yue-Hui et al., 2015; Jain et al., 2015). Note that while the LSTM produces the best scores for this benchmark, LSTM is five times computational complex relative to the sRNN (Klaus et al., 2015).\nIn Figure 6, we plot the validation scores for the 3 RNNs as function of the grid search parameters. It is clear from the Figure that the performance of IRNN is much strongly dependent on the right choice of hyperparameters, which is not the case for np-RNN or the LSTM."}, {"heading": "5 CONCLUSION", "text": "We offer a dynamical systems perspective on the Identity weight initialization for the recurrent weight matrix for RNNs with hidden nodes made up of ReLUs. We hypothesize that the sensitivity of hidden nodes to input perturbations resulting from the Identity weight matrix initialization, may make IRNN sensitive to the choice of hyperparameters for successful training. We offer an alternative weight initialization strategy, the normalized positive-definite weight matrix, that attempts to reduce the sensitivity of the hidden nodes to input perturbations by collapsing the dynamics to a one-dimensional manifold. We compare the performance of IRNN to np-RNN on several toy examples and also one real world action recognition benchmark and demonstrate that np-RNN either performs better or is comparable to IRNN on these benchmarks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We acknowledge valuable feedback from our colleagues at Qualcomm Research: Daniel Fontijne and Anthony Sarah."}], "references": [{"title": "Neural machine translation by jointly learning to alogn and translate", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2019\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2019}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proceedings of 31st Internation Conference on Machine Learning,", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "A field guide to dynamical recurrent network, chapter Gradient flow in recurrent nets", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "The schematic diagram of the work flow for training rnn on ucf-101 benchmark is shown in figure 6", "author": ["M. Jain", "J.C. van Gemert", "C. Snoek"], "venue": "IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["G. Klaus", "R.K. Srivastava", "J. Koutnik", "B.R. Steunebrink", "J. Schmidheuber"], "venue": null, "citeRegEx": "Klaus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klaus et al\\.", "year": 2015}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of 27th International Conference on Machine Learning,", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Martens and Sutskever,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever", "year": 2011}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J. Ng Yue-Hui", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": null, "citeRegEx": "Yue.Hui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yue.Hui et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A simple way to initialize recurrent network of rectified linear units", "author": ["V.L. Quoc", "Navdeep", "G.E", "Hinton"], "venue": null, "citeRegEx": "Quoc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Quoc et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating", "author": ["D. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "error. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering", "author": ["S. Strogatz"], "venue": "Westview Press, second edition,", "citeRegEx": "Strogatz,? \\Q2014\\E", "shortCiteRegEx": "Strogatz", "year": 2014}, {"title": "Random walk intialization for training very deep networks", "author": ["D. Sussillo", "A. Abbott"], "venue": null, "citeRegEx": "Sussillo and Abbott,? \\Q2015\\E", "shortCiteRegEx": "Sussillo and Abbott", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Traditionally, training RNNs using stochastic gradient descent methods such as back-propagation through time (BPTT) (Rumelhart et al., 1986) have been riddled with difficulty.", "startOffset": 116, "endOffset": 140}, {"referenceID": 6, "context": "Early attempts suffered from the so-called vanishing gradient or exploding gradient problems resulting in difficulties to learn longrange temporal dependencies (Hochreiter et al., 2001).", "startOffset": 160, "endOffset": 185}, {"referenceID": 10, "context": "The Hessian-Free (HF) optimization method (Martens, 2010; Martens & Sutskever, 2011) for training RNNs falls under this category.", "startOffset": 42, "endOffset": 84}, {"referenceID": 1, "context": "LSTM RNNs and variants thereof (Cho et al., 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 3, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al.", "startOffset": 111, "endOffset": 125}, {"referenceID": 1, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al., 2014; Bahdanau et al., 2014), image captioning (Kiros et al.", "startOffset": 191, "endOffset": 232}, {"referenceID": 0, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al., 2014; Bahdanau et al., 2014), image captioning (Kiros et al.", "startOffset": 191, "endOffset": 232}, {"referenceID": 8, "context": ", 2014), image captioning (Kiros et al., 2014), predicting output of simple computer programs (Zaremba & Sutskever, 2014) and action recognition from video clips (Ng Yue-Hui et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 14, "context": "(iii) Weight initialization, more recently Quoc et al, (Quoc et al., 2015) proposed a simpler solution to the long-range dependence problem with RNNs composed of rectified linear units (RELU)s.", "startOffset": 55, "endOffset": 74}, {"referenceID": 14, "context": "Experimental results with IRNN on several benchmark problems with long-range temporal structure are comparable to the those produced by LSTM RNNs (Quoc et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 14, "context": "We show that on several of the tasks investigated by (Quoc et al., 2015), np-RNN performs better than IRNN or the corresponding scaled version of the IRNN, the iRNN, where the recurrent weight matrix is initialized to 0.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": "The IRNN model proposed by (Quoc et al., 2015) is a special case of sRNN with the following modifications: (i) the nonlinearity f is RELU and (ii) the hidden layer weights Whh are initialized with identity matrix and the bias terms are set to zero.", "startOffset": 27, "endOffset": 46}, {"referenceID": 13, "context": "In order to better understand how these modifications may enable RNN to overcome some of the long range problems, let us look at the gradient equation in the BPTT algorithm (Pascanu et al., 2013): \u2202C \u2202\u03b8 = \u2211", "startOffset": 173, "endOffset": 195}, {"referenceID": 17, "context": "In Figure 2a-d, we show the schematic diagram of the phase-space of the hidden node dynamical system in Equation 8, dependent on the eigenvalues, \u03bb, of the recurrent weight matrix (Strogatz, 2014).", "startOffset": 180, "endOffset": 196}, {"referenceID": 14, "context": "We follow the procedure described in (Quoc et al., 2015) and the RNN reads one pixel at a time in a scanline order, starting at the top left corner of the MNIST image and ending at the bottom right corner of the image.", "startOffset": 37, "endOffset": 56}, {"referenceID": 16, "context": "4 Action recognition benchmark We consider one real world example of action recognition task based on the standard UCF101 benchmark (Soomro et al., 2012).", "startOffset": 132, "endOffset": 153}, {"referenceID": 14, "context": "As suggested in (Quoc et al., 2015), a basic baseline solution would be to always predict 1 as the output regardless of the inputs.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "Following from (Quoc et al., 2015), we began by evaluating the performance of RNNs on the addition benchmark for sequences of length T = {150, 200, 300, 400}.", "startOffset": 15, "endOffset": 34}, {"referenceID": 14, "context": "For sequences up to T = 300, our results for gRNN and IRNN match those obtained by (Quoc et al., 2015).", "startOffset": 83, "endOffset": 102}, {"referenceID": 14, "context": "However, for sequence of length T = 400, contrary to findings in the (Quoc et al., 2015), we were able to train the IRNN as well as the np-RNN.", "startOffset": 69, "endOffset": 88}, {"referenceID": 14, "context": "We began by first attempting to reproduce the findings from (Quoc et al., 2015).", "startOffset": 60, "endOffset": 79}, {"referenceID": 14, "context": "However using the SGD-BPTT optimization, we were unable to train IRNN on MNIST using the set of parameters reported in (Quoc et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 14, "context": "We note that while the IRNN produces accuracy of about 83 %, the np-RNN produces an accuracy of about 92 %, which is inline with the results reported by (Quoc et al., 2015).", "startOffset": 153, "endOffset": 172}, {"referenceID": 14, "context": "4) and our inability to reproduce the results reported in (Quoc et al., 2015), we conclude that IRNN is extremely sensitive to the choice of the training protocol and the hyper parameters of the network.", "startOffset": 58, "endOffset": 77}, {"referenceID": 16, "context": "We also benchmarked IRNN, np-RNN and the LSTMs on the UCF-101 action recognition dataset (Soomro et al., 2012).", "startOffset": 89, "endOffset": 110}, {"referenceID": 7, "context": "Motivated by the recent findings of (Jain et al., 2015), we use a CNN trained on 15000 Imagenet object categories as the feature generator.", "startOffset": 36, "endOffset": 55}, {"referenceID": 7, "context": "data for the UCF101 benchmark (Ng Yue-Hui et al., 2015; Jain et al., 2015).", "startOffset": 30, "endOffset": 74}, {"referenceID": 9, "context": "Note that while the LSTM produces the best scores for this benchmark, LSTM is five times computational complex relative to the sRNN (Klaus et al., 2015).", "startOffset": 132, "endOffset": 152}], "year": 2017, "abstractText": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.", "creator": "LaTeX with hyperref package"}}}