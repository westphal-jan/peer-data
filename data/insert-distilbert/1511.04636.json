{"id": "1511.04636", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Deep Reinforcement Learning with a Natural Language Action Space", "abstract": "in this paper, we propose the deep reinforcement relevance network ( drrn ), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text - based games. for a consistent particular class spectrum of games, a user must choose among a variable number of actions described by manipulating text, with the goal of further maximizing long - term reward. in these games, the best action is distributed typically that which fits the best to the current situation ( modeled as a state in the drrn ), similarly also described by text. because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. therefore, it is very difficult to consciously pre - define the action set as in the deep q - network ( dqn ). to address this challenge, the drrn extracts high - level embedding vectors from the various texts that describe states and executing actions, respectively, and computes the inner products distinguish between the state and action embedding vectors to approximate the q - function. we evaluate the drrn on two popular text games, showing superior performance over the dqn.", "histories": [["v1", "Sat, 14 Nov 2015 23:30:39 GMT  (1489kb,D)", "http://arxiv.org/abs/1511.04636v1", null], ["v2", "Thu, 19 Nov 2015 20:24:12 GMT  (1515kb,D)", "http://arxiv.org/abs/1511.04636v2", null], ["v3", "Sun, 10 Jan 2016 01:51:20 GMT  (2990kb,D)", "http://arxiv.org/abs/1511.04636v3", null], ["v4", "Sat, 16 Jan 2016 23:43:40 GMT  (2990kb,D)", "http://arxiv.org/abs/1511.04636v4", null], ["v5", "Wed, 8 Jun 2016 05:58:34 GMT  (3008kb,D)", "http://arxiv.org/abs/1511.04636v5", "accepted by ACL 2016"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["ji he", "jianshu chen", "xiaodong he", "jianfeng gao", "lihong li", "li deng", "mari ostendorf"], "accepted": true, "id": "1511.04636"}, "pdf": {"name": "1511.04636.pdf", "metadata": {"source": "CRF", "title": "DEEP REINFORCEMENT LEARNING WITH AN UNBOUNDED ACTION SPACE", "authors": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf"], "emails": ["ostendor}@uw.edu", "deng}@microsoft.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "We consider a sequential text understanding and decision making problem with an unbounded action space in the context of learning to play text games. At each time step, the learning agent will be given a text string that describes a certain state of the game (i.e., environment, defined as the \u201cstate text\u201d) and several text strings that describe all the possible actions we could take (defined as the \u201caction texts\u201d). After selecting one of the actions (e.g., by clicking the corresponding hyperlink of that text), the environment will move to another state and further reveal some texts about the next state. The reward signal could either be given during each transition or in the end. The objective is to understand, at each step, the state text and all the action texts to pick up the best action so that we navigate through the best sequence of texts with the highest long-term reward (e.g., the game reaches a good ending of the story). To this end, the learning agent need to find the most relevant action text for the current state text. Here we define a new notion of relevance based on their joint impact on the reward: an action text string is said to be \u201cmore relevant\u201d (to a state text string) than the other action texts if taking that action would lead to higher long-term reward. We formulate the problem as a reinforcement learning problem and propose a novel deep reinforcement relevance network (DRRN) for the text understanding and navigation task. Deep neural networks (DNN) are used to map text strings into embedding vectors in a common finite-dimensional space, where \u201crelevance\u201d could be measured numerically by their inner product. In particular, the outcome of the inner product defines the value of the Q-function for the current state-action pair, which characterizes the longterm reward for pairing these two text strings. The embedding and the Q-function will be learned in an end-to-end manner by maximizing the long-term reward.\nMoreover, this work is also a novel contribution to deep reinforcement learning. Recently, deep reinforcement learning using deep Q-network (DQN) has been shown to outperform human level\nar X\niv :1\n51 1.\n04 63\n6v 1\n[ cs\n.A I]\n1 4\nN ov\nperformance in several benchmarks of Atari games (Mnih et al., 2015). In DQN, a deep convolutional neural network is used to extract high-level features from images, and is then further mapped into Q-function values of different actions via a linear transform. Therefore, in DQN, the action sets are pre-defined and fixed during the entire learning and decision making process. However, in text understanding, this is usually not the case since each action in itself might be a string of text. Because of exponential complexity of natural language with respect to sentence length, there could be unboundedly many unique actions in this scenario. Therefore, in order to understand texts and make the best decision, we need to extract high-level features (embedding vectors) not only from the text that describes the state, but also from the text that describes each action. This is an important structural difference from the prior art of DQN, which only extracts high-level features from the state side. Enabled by this novel structure in DRRN, the number of actions can be different at different steps of the decision making process. In this paper, we will evaluate our proposed method on two real-world text games, which show its superior performance over the Deep Q-networks (DQN).\nThe rest of the paper is organized as follow. In Section 2, we review related works. Then, we develop the deep reinforcement relevance network in Section 3, which is evaluated on two real-world text games in Section 4. Finally, in Section 5, we conclude the entire paper."}, {"heading": "2 RELATED WORK", "text": "Reinforcement learning is a learning approach to making decisions that maximize long-term rewards (Sutton & Barto, 1998). Recently, significant progress has been made by combining deep learning (LeCun et al., 2015) with reinforcement learning. The \u201cDeep Q Network\u201d (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and was shown to achieve human level performance by applying convolutional neural networks to the raw image pixels. A deep neural network is used as a function approximation in a variant of Q learning (Watkins & Dayan, 1992), and a couple of techniques are introduced to ensure the algorithm converges stably. Another stream of work focuses on continuous control with deep reinforcement learning (Lillicrap et al., 2015), where an actor-critic algorithm operates over continuous action space. Continuous action space differs from unbounded action space in that the action space is known, while the DRRN framework treats text games as a black-box, thus the potential action space defined through natural language is unbounded.\nThere has also been increasing interest in applying reinforcement learning, especially DQN, to other problems. Li et al. (2015) developed a joint training approach for recurrent reinforcement learning and demonstrate its effectiveness on a customer relationship management task. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler & Young, 2002; Singh et al., 1999). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al., 2009). Narasimhan et al. (2015) applied an LSTM (long short term memory) -DQN framework to the task of learning control policies for parser-based text games, which achieves higher average reward than the random and BOW (bag-of-words) -DQN baselines. Due to the potentially infinite input space, modeling parser-based text games requires restrictions on player input (Narasimhan et al., 2015), such as fixed command structures (one action and one argument object), and limited action-side vocabulary size. Because of exponential complexity of natural language with respect to sentence length, this approach has the drawback of complex action space, and it is infeasible for Choice-based or Hypertext text games (see a brief review of different text games in Section 4.1). To the best of our knowledge, deep reinforcement learning for text games with variable natural language actions is relatively limited."}, {"heading": "3 DEEP REINFORCEMENT RELEVANCE NETWORK", "text": ""}, {"heading": "3.1 SEQUENTIAL DECISION MAKING IN TEXT UNDERSTANDING", "text": "We consider the sequential decision making problem for text understanding in Figure 1, which we model as a Markov decision process (MDP). At each time step t, the agent will receive a string of text that describes the state st = s (i.e., \u201cstate-text\u201d) and several strings of text that describe\nall the potential actions at (i.e., \u201caction-text\u201d). The agent is required to understand the texts from both the state side and the action side, measuring their relevance to the current context st for the purpose of maximizing the long-term reward, and then picking up the best action a\u2217t . Then, the environment will transition to another state st+1 = s\u2032 according to the probability p(s\u2032|s, a), and the agent will receive a reward rt for that particular transition. The policy of the agent is defined to be the probability \u03c0(at|st) of taking action at at state st. We further define the Q-function Q(s, a) that characterizes the expected return starting from s, taking the action a, and thereafter following policy \u03c0(a|s) to be:\nQ\u03c0(s, a) = E { +\u221e\u2211 k=0 \u03b3krt+k \u2223\u2223\u2223\u2223st = s, at = a }\n(1)\nwhere \u03b3 denotes a discount factor. The optimal Q\u2217(s, a) is shown to be the solution to the Bellman Equation (Richard, 1957; Sutton & Barto, 1998):\nQ\u2217(s, a) = E [ rt + \u03b3max\na Q\u2217(s\u2032, a) \u2223\u2223st = s, at = a] (2) where the expectation is evaluated with respect to the randomness of st+1 = s\u2032 and rt given st = s and at = a. The optimal policy and Q-function could be found by using the Q-learning algorithm (Watkins & Dayan, 1992):\nQ(st, at)\u2190 Q(st, at) + \u03b7t \u00b7 ( rt + \u03b3 \u00b7max\na Q(st+1, a)\u2212Q(st, at)\n) (3)\nwhere \u03b7t is the learning rate of the algorithm.\nA common situation in text navigation is when there are a variable number of actions at state st, denoted as a1t , a 2 t , . . . , a |At| t , where |At| is the total number of action options at time t. A straightforward DQN implementation in variable action situation is shown in Figure 2. In this paper, we use a softmax selection strategy as the exploration policy, which chooses the action at at state st according to the following probability:\n\u03c0(at = a i t|st) = exp(\u03b1 \u00b7Q(st, ait))\u2211|At| j=1 exp(\u03b1 \u00b7Q(st, a j t )) , i = 1, . . . , |At| (4)\nwhere \u03b1 denotes the scaling factor in softmax operation1. We choose this to be a constant throughout learning time. Since all methods in this paper initialize with a small random weights, initial Q-value differences will be small, thus making the Q-learning more explorative at beginning. As Q-values approximate better to true values, a reasonable \u03b1 will make action selection put most probability on optimal action (exploitation), but still maintain a small explorative probability.\n1\u03b1 controls the balance between exploitation and exploration. For instance, if \u03b1 = 0, the actions are uniformly sampled, whereas \u03b1 =\u221e, there is no exploration."}, {"heading": "3.2 DRRN ARCHITECTURE: FORWARD ACTIVATION", "text": "Note that the vanilla Q-learning recursion (3) requires us to maintain a table of size |S|\u00d7 |A|, where |S| and |A| denote the size of the state space and the action space, respectively. Therefore, it only applies to the situation where the state space is small. For text understanding tasks, the vocabulary size is usually large and the text usually contains multiple sentences of variable lengths, which leads to an unbounded state space. To address such an issue, we need to use function approximation for Q-learning. In DQN, a deep neural network (DNN, or deep convolutional neural network for Atari games) is used for such a function approximation. The DNN has been shown to have high capacity and scalability in function approximation, which leads to state-of-the-art performance in many machine learning tasks (Hinton et al., 2012; Krizhevsky et al., 2012). Specifically, the DNN takes the raw data of the state (e.g., the text or the image pixels) as its input and generates |A| outputs, each of which represents the value of Q(s, a) for a particular action a; that is, the DQN architecture treats Q(s, a) as |A| different functions, and uses its output value at each output unit to approximate one of these |A| functions. In DQN, high-level vector representations of the raw states are extracted and stored as hidden activations. Then, they are mapped into the Q-function\nvalues for each action via a linear transform. This architecture has been successfully applied to solve Artari Game problems, where the action space is pre-defined and fixed to be a small set over time. However, for text understanding tasks, each action is usually represented by a certain text, which could be different over time, and the number of actions could also be different at different time steps. Therefore, besides using a DNN to understand the texts from the state, we also need another set of DNNs to understand the texts from the action side, (one for each action), in order to choose the most relevant action.\nMotivated by the aforementioned challenges, we develop deep reinforcement relevance network (DRRN), which is a novel deep architecture for handling an unbounded action space in sequential text understanding. As shown in Figure 3, DRRN consists of a pair of DNNs, one for state text embedding and the other for action text embedding. Given any state/action text pair (st, ait), the DRRN estimates the Q-functionQ(st, ait) in two steps. First, map both st and a i t to their embedding vectors using the corresponding DNNs, respectively. Second, approximate Q(st, ait) as the inner product of the embedding vectors. Then, given a particular state st, we can select the optimal action at among the set of actions via at = arg maxait Q(st, a i t).\nWhile we allow the DNN at the action side to have different model parameters from the DNN on the state side, the DNNs for the action-texts are set to have tied model parameters. The reason for having tied DNN model parameters on the action side is that the texts that describe different actions usually share common statistics and it is thus natural to use a common model to extract the corresponding high level representations from them. This special tied structure enables each training sample to be shared with the DNNs on all the actions, making them more efficiently trained by the existing data samples. This is in contrast to the DQN structure (see Figure 2), where different texts for different actions are essentially assigned independent model parameters, which does not make full use of the particular structure inherent in this text understanding problem. On the other hand, the text that describes the state could have different statistics from the texts on the action side. Therefore, we allow the model parameters of the DNN on the state side to be untied from the ones on the action side. More formally, let hl,s and hl,a denote the l-th hidden layer for state and action side neural networks, respectively. For state side, Wl,s and bl,s denote the linear transformation weight matrix and bias vector between the (l \u2212 1)-th and l-th hidden layers, respectively. For actions side, Wl,a and bl,a denote the equivalent parameters for the action side. The DRRN has L hidden layers on each side.\nh1,s = f(W1,sst + b1,s) (5)\nhi1,a = f(W1,aa i t + b1,a), i = 1, 2, 3, ..., |At| (6)\nhl,s = f(Wl\u22121,shl\u22121,s + bl\u22121,s), l = 2, 3, ..., L (7)\nhil,a = f(Wl\u22121,ah i l\u22121,a + bl\u22121,a), i = 1, 2, 3, ..., |At|, l = 2, 3, ..., L (8)\nwhere f(\u00b7) is the nonlinear activation function at the hidden layers, which, for example, could be chosen as tanh (x) = (1 \u2212 exp (\u22122x))/(1 + exp (\u22122x)), and At denotes the set of all actions at time t. To measure the relevance between the current state st and each action ait, we compute the inner product between the hidden representations of the state-text and each of the action-texts. The inner product is used to approximate the Q-function values, Q(s, a), in the following parametric form:\nQ(st, a i t; \u0398) = h T L,sh i L,a (9)\nwhere \u0398 denotes all the model parameters, and T denotes transpose. Note that, the relevance here does not necessarily imply similarity in semantic meaning, but will be determined by the rewards via the learning process (see Sec. 3.3)."}, {"heading": "3.3 LEARNING THE DRRN: BACK PROPAGATION", "text": "To learn the DRRN, we use the \u201cexperience-replay\u201d strategy (Lin, 1993), which uses a fixed exploration policy to interact with the environment to obtain a data trajectory. Then, we randomly sample a transition tuple (sk, ak, rk, sk+1), denote temporal difference error dk = rk + \u03b3 \u00b7 maxaQ(sk+1, a; \u0398k\u22121) \u2212 Q(sk, ak; \u0398k\u22121), and update the model according to the following recursions:\nWs,k = Ws,k\u22121 + \u03b7kdk \u00b7 \u2202Q(sk, ak)\n\u2202Ws , bs,k = bs,k\u22121 + \u03b7kdk \u00b7\n\u2202Q(sk, ak)\n\u2202bs (10)\nAlgorithm 1 Learning algorithm for DRRN 1: Initialize replay memory D to capacity N . 2: Initialize DRRN with small random weights. 3: Initialize game simulator and load dictionary. 4: for episode = 1, . . . ,M do 5: Restart game simulator. 6: Read raw state text and a list of action text from the simulator, and convert them to represen-\ntation s1 and a11, a 2 1, . . . , a |A1| 1 .\n7: for t = 1, . . . , T do 8: Compute Q(st, ait; \u0398) for the list of actions using DRRN forward activation (Section 3.2).\n9: Select an action at based on probability distribution \u03c0(at = ait|st) (Equation 4) 10: Execute action at in simulator 11: Observe reward rt. Read the next state text and the next list of action texts, and convert\nthem to representation st+1 and a1t+1, a 2 t+1, . . . , a |At+1| t+1 .\n12: Store transition (st, at, rt, st+1) in D. 13: Sample random mini batch of transitions (sk, ak, rk, sk+1) from D.\n14: Set yk = { rk if sk+1 is terminal rk + \u03b3maxa\u2032 Q(sk+1, a\n\u2032; \u0398)) otherwise 15: Perform a gradient descent step on (yk \u2212 Q(sk, ak; \u0398))2 with respect to the network pa-\nrameters \u0398 (Section 3.3). Back-propagation is performed only for ak even though there are |Ak| actions at time k.\n16: end for 17: end for\nWa,k = Wa,k\u22121 + \u03b7kdk \u00b7 \u2202Q(sk, ak)\n\u2202Wa , ba,k = ba,k\u22121 + \u03b7kdk \u00b7\n\u2202Q(sk, ak)\n\u2202ba (11)\nwhere the expressions for \u2202Q\u2202Ws , \u2202Q \u2202Wa , \u2202Q\u2202bs , \u2202Q \u2202ba are given in Appendix B. Note that we are using the notation k to denote the index of the transition tuple sampled from the trajectory. This essentially\nscrambles the trajectory from experience replay into a \u201cbag-of-transitions\u201d, which has been shown to avoid oscillations or divergence and achieve faster convergence in the parameters in Q-learning (Mnih et al., 2015). As illustrated in Figure 3, since the models on the action side are tied, models associated with other actions will also be updated even though the back propagation is only over one action.\nThe full algorithm is summarized in Algorithm 1. Note from Figure 3 that we do not pre-define the meaning of relevance between two text strings in the network. Instead, we apply back propagation to learn how to pair the text strings from the reward signals in an end-to-end manner. The representation vectors for the state-text and the action-text will be automatically learned to approach each other in the text embedding space from the reward signals. In Figure 4, we used Principal Component Analysis (PCA) to project the 100-dimension last hidden layer (before inner product) representation to a 2-D plane. The vector embeddings start with small values, due to small random initialization in model parameters, and after 600 episodes of experience replay training, the text embedding vectors are close to converged embedding vectors (after 4000 episodes). Eventually, the embedding vector of optimal action (Action 1) converges to a positive inner product with state embedding vector, while Action 2 converges to a negative inner product."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "4.1 OVERVIEW OF TEXT GAMES", "text": "Text games, although simple compared to video games, still enjoy high popularity in world-wide online communities.2 There are annual competitions3 held online since 1995 and has received hundreds of submissions so far. Text games communicate to players in the form of text display, and players understand the revealed texts and respond by typing or clicking text (Adams, 2014). That is, the games have a high requirement for the text understanding ability of the agent.\nThere are three different types of text games: parser-based (Figure 5(a)), choice-based (Figure 5(b)), and hypertext-based (Figure 5(c)). Parser-based games were popular among early personal computer users, and are the least user-friendly text games. Their prominent feature involves a natural-language parser inside simulator that accepts typed-in commands from the player, usually in the form of verb phrases, such as \u201ceat apple\u201d, \u201cget key\u201d, or \u201cgo east\u201d. Choice-based and hypertext-based games, on the other hand, do not require a text parser. These two types of text games embed actions after, or within story text. The player chooses one of these actions, and the story continues based on the action taken at this particular state. With the development of web browsing and richer HTML display, choice-based and hypertext-based text games have become increasingly popular in online community, increasing in percentage from 8% in 2010 to 62% in 2014.\nText games are complex due to two major reasons. First, they often involve language understanding of the story and pragmatic clues under each action. Players usually have to combine both the story and choices to infer the appropriate actions (e.g. Given \u201cIn front there is a lion\u201d, and action \u201cgo ahead\u201d, the player is more likely to die). The second reason is long term dependency. A player\u2019s early action might influence the later-on story development, as in the example of finding a key to unlock an object that is encountered later. Because a player\u2019s behavior (policy) changes how\n2http://www.intfiction.org/forum/, http://ifdb.tads.org/ 3http://www.ifcomp.org/ 4http://www.ifcomp.org/about/if\nthe environment interacts with him, reinforcement learning is appropriate for modeling long-term dependency in text games.\nCompared to parser-based text games, choice-based and hypertext-based text games are especially difficult to solve automatically. This is due to the potential for unbounded representation of natural language sentences. Thus they serve as good proxy tasks for the DRRN. Previously for parser-based text games, Narasimhan et al. (2015) have defined a fixed set of 222 actions, which is the total number of possible phrases the parser accepts. Parser-based text games are reduced to standard deep Q learning, with function approximation using a deep neural network. However, for choice-based and hypertext-based text games, the size of action space is exponential with the length of action sentences. Furthermore, another salient feature of choice-based and hypertext-based text games is that they usually have a variable number of input actions, which can be difficult to integrate into a fixed-output neural network architecture like a DQN. Nevertheless, we will show that the new DRRN model is especially suitable for solving both the problems of an unbounded action space and variable input actions, by properly combining the text understanding power of DNN with reinforcement learning in a new deep learning architecture.\nIn this study, we evaluate the DRRN with two choice-based games: a deterministic text game task called \u201cSaving Johns\u201d and a larger-scale stochastic text game called \u201cMachine of Death\u201d from a public archive5. The basic text statistics of these tasks are shown in Table 1. The maximum possible number of actions is 4 in the game \u201cSaving John\u201d, and 9 in the game \u201cMachine of Death\u201d."}, {"heading": "4.2 EXPERIMENT SETUP", "text": "The DRRN is evaluated on the two games described above. We manually annotate final rewards for all distinct endings in both games (as shown in Appendix C). The magnitude of reward scores are given to describe sentiment polarity of good/bad endings. On the other hand, each non-terminating step we assign with a small negative reward. For the text game \u201cMachine of Death\u201d, we restrict an episode to be no longer than 500 steps. In \u201cSaving John\u201d all actions are choice-based, for which the mapping from text strings to at are clear. In \u201cMachine of Death\u201d, when actions are hypertext, the actions are substrings of the state. In this case st is associated with the full state description, and at are given by the substrings without any surrounding context. In our examples, we use different vocabulary for state side and actions side. For text input, we use raw bag-of-words as features. The deterministic nature of state transitions in \u201cSaving John\u201d might lead to faster convergence than stochastic games, but as we will see in next paragraphs, the exploration-exploitation in learning will always add stochasticity to training.\nWe apply DRRN with both 1 hidden layer and 2 hidden layers structures. For simplicity, we set the hidden dimension to be the same for each hidden layer, and for both state side and action side. In this way we could directly compute inner product at the last hidden layer to approximate Q-function values. We use DRRNs with 100-dimension in hidden layer(s) and build learning curves during experience replay training. In testing, we apply softmax selection. This is consistent with training, and the resulting average rewards will not converge to one particular ending. We record average final rewards as performance of the model. Learning rate \u03b7t is set to be 0.001 throughout time.\nThe DRRN is compared to three baselines: a linear model and two DQNs (with one hidden layer and two hidden layers, respectively). All baselines use the same Q-learning framework with dif-\n5http://www.ifarchive.org\nferent function approximators to predict Q(st, at) given the current state and actions. For the three baselines, the input is the text strings of state and action descriptions together at a bag of words, with the number of output equal to the maximum number of actions. When there are fewer actions than maximum, then the highest scoring available action is used.\nWe use softmax selection, which is widely-applied in practice, to trade-off exploration vs. exploitation. Specifically, for each experience replay, we first generate 200 episodes of data (about 3K tuples in \u201cSaving John\u201d and 16K tuples in \u201cMachine of Death\u201d) using the softmax selection rule in (4), where we set \u03b1 = 0.2 for the first game and \u03b1 = 1.0 for the second game. We then shuffle the generated data tuples (st, at, rt, st+1) and apply Algorithm 1 to update the model. The model is trained with multiple epochs for all configurations, and is evaluated after each experience replay. The discount factor \u03b3 is set to 0.9. For DRRN and all baselines, network weights are initialized with small random values. To prevent algorithms from \u201cremembering\u201d state text patterns and make good choices, each time the algorithm/player reads text from simulator, we randomly shuffle the list of actions. This will enforce the algorithms to learn text relevance between state and actions."}, {"heading": "4.3 PERFORMANCE", "text": "In Figure 6, we show the learning curves of different models, where the dimension of hidden layer in DQN and DRRN are all set to 100. The error bars are obtained by running five independent experiments. As we see from Figure 6, both proposed methods and baseline methods started at about the same performance (-6.9\u223c -7.1 average rewards for Game 1, and -7.9\u223c -8.2 average rewards for Game 2; most of the time fall into bad endings), which is the random guess policy. After around 3000 episodes of experience replay training, all methods converge. The DRRN converges much faster than the other three baselines and achieves better long-term reward. We hypothesize this is because the DRRN architecture is better at capturing relevance between state text and actions text. While for the linear and DQN methods, concatenating input features does not make effective utilization of information from both sides. In fact, the DRRN contains far fewer free parameters compared to the DQN (in \u201cMachine of Death\u201d, DRRN with 2 hidden layers has 288k free parameters, compared to\n614k free parameters in DQN with 2 hidden layers). Furthermore, the architecture of the DRRN is capable of handling variable input actions.\nThe final performance (at convergence) for both baselines and proposed methods are shown in Table 2 and Table 3. We test for different model sizes with 10, 20, 50, and 100 dimensions in the hidden layers. We see that the DRRN performs consistently better than all baselines (including DQN), and often with a lower variance. The converged performance continues to grow by increasing the number of hidden layers. However, deep models also converge more slowly than 1 hidden layer DRRN or DQN models, as shown in Figure 6.\nIn Appendix D, we show examples with pairs of state/actions text, and their corresponding Q-values from a trained DRRN. Actions that are more likely to result in good endings are learned with high Q-values. For example, given \u201cAs you move forward, the people surrounding you suddenly look up with terror in their faces, and flee the street.\u201d, a better action is to \u201cLook up\u201d rather than \u201cIgnore the alarm of others and continue moving forward.\u201d This also shows that our method can gain a reasonable level of language understanding."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "In this paper we develop a deep reinforcement relevance network in situation of an unbounded action space. The proposed architecture is capable of handling variable number of actions defined through natural language in text games setting. In addition, we show that the DRRN is able to converge faster and to a better solution than the baseline deep Q-networks, by using fewer parameters. Future work includes (i) using a more general operation (e.g. tensor product) to correlate state space and action space, (ii) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (iii) applying the proposed methods on other tasks with unbounded action space."}, {"heading": "A PERCENTAGE OF CHOICE-BASED AND HYPERTEXT-BASED TEXT GAMES", "text": "As shown in Table 4."}, {"heading": "B BACK PROPAGATION FORMULA FOR LEARNING DRRN", "text": "Let hl,s and hl,a denote the l-th hidden layer for state and action side neural networks, respectively. For state side, Wl,s and bl,s denote the linear transformation weight matrix and bias vector between the (l\u2212 1)-th and l-th hidden layers. For actions side, Wl,a and bl,a denote the linear transformation weight matrix and bias vector between the (l\u22121)-th and l-th hidden layers. The DRRN has L hidden layers on each side.\nForward:\nh1,s = f(W1,sst + b1,s) (12)\nhi1,a = f(W1,aa i t + b1,a), i = 1, 2, 3, ..., |At| (13)\nhl,s = f(Wl\u22121,shl\u22121,s + bl\u22121,s), l = 2, 3, ..., L (14)\nhil,a = f(Wl\u22121,ah i l\u22121,a + bl\u22121,a), i = 1, 2, 3, ..., |At|, l = 2, 3, ..., L (15)\nQ(st, a i t) = h T L,sh i L,a (16)\nwhere f(\u00b7) is the nonlinear activation function at the hidden layers, which is chosen as tanh (x) = (1\u2212 exp (\u22122x))/(1 + exp (\u22122x)), and At denotes the set of all actions at time t. Backward:\nNote we only back propagate for actions that are actually taken. More formally, let at be action the DRRN takes at time t, and denote \u2206 = [Q(st, at) \u2212 (rt + \u03b3maxaQ(st+1, a))]2/2. Denote \u03b4l,s = \u03b4bl,s = \u2202Q/\u2202bs, \u03b4l,a = \u03b4bl,a = \u2202Q/\u2202ba, and we have (by following chain rules):\n\u03b4Q = \u2202\u2206\n\u2202Q = Q(st, at)\u2212 (rt + \u03b3max a Q(st+1, a))\n{ \u03b4L,s = \u03b4Q \u00b7 hL,a (1\u2212 hL,s) (1 + hL,s) \u03b4l\u22121,s = W\nT l,s\u03b4l,s (1\u2212 hl\u22121,s) (1 + hl\u22121,s), l = 2, 3, ..., L{\n\u03b4L,a = \u03b4Q \u00b7 hL,s (1\u2212 hL,a) (1 + hL,a) \u03b4l\u22121,a = W\nT l,a\u03b4l,a (1\u2212 hl\u22121,a) (1 + hl\u22121,a), l = 2, 3, ..., L{\n\u03b4W1,s = \u2202Q/\u2202W1,s = \u03b41,s \u00b7 sTt \u03b4Wl,s = \u2202Q/\u2202Wl,s = \u03b4l,s \u00b7 hTl\u22121,s, l = 2, 3, ..., L{ \u03b4W1,a = \u2202Q/\u2202W1,a = \u03b41,a \u00b7 aTt \u03b4Wl,a = \u2202Q/\u2202Wl,a = \u03b4l,a \u00b7 hTl\u22121,a, l = 2, 3, ..., L\nwhere denotes element-wise Hadamard product."}, {"heading": "C DEFINE FINAL REWARDS IN THE TWO TEXT GAMES", "text": "As shown in Table 5 and Table 6."}, {"heading": "D EXAMPLES OF STATE-ACTION PAIRS IN THE TWO TEXT GAMES", "text": "As shown in Table 7 and Table 8."}], "references": [{"title": "Fundamentals of game design", "author": ["Adams", "Ernest"], "venue": "Pearson Education,", "citeRegEx": "Adams and Ernest.,? \\Q2014\\E", "shortCiteRegEx": "Adams and Ernest.", "year": 2014}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Chen", "Harr", "Zettlemoyer", "Luke", "Barzilay", "Regina"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["SRK Branavan", "Silver", "David", "Barzilay", "Regina"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent Reinforcement Learning: A Hybrid Approach", "author": ["X. Li", "L. Li", "J. Gao", "X. He", "J. Chen", "L. Deng", "J. He"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Dynamic programming", "author": ["Richard", "Bellman"], "venue": null, "citeRegEx": "Richard and Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Richard and Bellman.", "year": 1957}, {"title": "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning", "author": ["Scheffler", "Konrad", "Young", "Steve"], "venue": "In Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "Scheffler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Scheffler et al\\.", "year": 2002}, {"title": "Reinforcement learning for spoken dialogue systems", "author": ["Singh", "Satinder P", "Kearns", "Michael J", "Litman", "Diane J", "Walker", "Marilyn A"], "venue": "In Nips, pp", "citeRegEx": "Singh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1999}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "performance in several benchmarks of Atari games (Mnih et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 8, "context": "The \u201cDeep Q Network\u201d (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and was shown to achieve human level performance by applying convolutional neural networks to the raw image pixels.", "startOffset": 68, "endOffset": 106}, {"referenceID": 9, "context": "The \u201cDeep Q Network\u201d (DQN) was developed and applied to Atari games (Mnih et al., 2013; Mnih et al., 2015) and was shown to achieve human level performance by applying convolutional neural networks to the raw image pixels.", "startOffset": 68, "endOffset": 106}, {"referenceID": 6, "context": "Another stream of work focuses on continuous control with deep reinforcement learning (Lillicrap et al., 2015), where an actor-critic algorithm operates over continuous action space.", "startOffset": 86, "endOffset": 110}, {"referenceID": 13, "context": "In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler & Young, 2002; Singh et al., 1999).", "startOffset": 178, "endOffset": 223}, {"referenceID": 2, "context": "There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 1, "context": ", 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al., 2009).", "startOffset": 86, "endOffset": 109}, {"referenceID": 10, "context": "Due to the potentially infinite input space, modeling parser-based text games requires restrictions on player input (Narasimhan et al., 2015), such as fixed command structures (one action and one argument object), and limited action-side vocabulary size.", "startOffset": 116, "endOffset": 141}, {"referenceID": 3, "context": "Li et al. (2015) developed a joint training approach for recurrent reinforcement learning and demonstrate its effectiveness on a customer relationship management task.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping natural language instructions to sequences of executable actions (Branavan et al., 2009). Narasimhan et al. (2015) applied an LSTM (long short term memory) -DQN framework to the task of learning control policies for parser-based text games, which achieves higher average reward than the random and BOW (bag-of-words) -DQN baselines.", "startOffset": 98, "endOffset": 249}, {"referenceID": 3, "context": "The DNN has been shown to have high capacity and scalability in function approximation, which leads to state-of-the-art performance in many machine learning tasks (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 163, "endOffset": 209}, {"referenceID": 4, "context": "The DNN has been shown to have high capacity and scalability in function approximation, which leads to state-of-the-art performance in many machine learning tasks (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 163, "endOffset": 209}, {"referenceID": 9, "context": "scrambles the trajectory from experience replay into a \u201cbag-of-transitions\u201d, which has been shown to avoid oscillations or divergence and achieve faster convergence in the parameters in Q-learning (Mnih et al., 2015).", "startOffset": 197, "endOffset": 216}, {"referenceID": 10, "context": "Previously for parser-based text games, Narasimhan et al. (2015) have defined a fixed set of 222 actions, which is the total number of possible phrases the parser accepts.", "startOffset": 40, "endOffset": 65}], "year": 2017, "abstractText": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN). To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over the DQN.", "creator": "LaTeX with hyperref package"}}}