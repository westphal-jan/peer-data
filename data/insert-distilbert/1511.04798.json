{"id": "1511.04798", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization", "abstract": "typed emotional media content is a key element in user - generated videos. however, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user - generated content and the sparsity of video frames that supposedly express emotion. in this paper, for the first time, we study posed the problem of transferring knowledge from heterogeneous external physical sources, including image and textual data, to facilitate three related tasks in video emotion understanding : emotion recognition, emotion attribution and emotion - oriented summarization. specifically, our framework ( 1 ) learns a video encoding input from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and ( 2 ) transfers knowledge from an auxiliary physiological textual corpus for zero - shot \\ pl { operator recognition } of emotion classes unseen during training. the proposed technique for knowledge capture transfer facilitates novel applications of emotion control attribution and emotion - oriented summarization. a comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "histories": [["v1", "Mon, 16 Nov 2015 01:40:15 GMT  (5200kb,D)", "http://arxiv.org/abs/1511.04798v1", "13 pages, 11 figures"]], "COMMENTS": "13 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["baohan xu", "yanwei fu", "yu-gang jiang", "boyang li", "leonid sigal"], "accepted": false, "id": "1511.04798"}, "pdf": {"name": "1511.04798.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization", "authors": ["Baohan Xu", "Yanwei Fu", "Yu-Gang Jiang", "Boyang Li", "Leonid Sigal"], "emails": ["bhxu14@fudan.edu.cn.", "ygj@fudan.edu.cn.", "y.fu@qmul.ac.uk,", "bert.li}@disneyresearch.com."], "sections": [{"heading": null, "text": "Index Terms\u2014Video, Emotion Recognition, Transfer Learning, Zero-Shot Learning, Summarization.\nF"}, {"heading": "1 INTRODUCTION", "text": "RAPID development of mobile devices has led to anexplosive growth of user-generated images and videos, which creates a demand for computational understanding of visual media content. In addition to recognition of objective content, such as objects and scenes, an important dimension of video understanding is emotional or affective content. Such content can strongly resonate with viewers and plays a crucial role in the video-watching experience. Some successes have been achieved with the use of deeplearning architectures trained for text at both sentenceand document-level [39] or image sentiment analysis [8]. However, the ability to understand emotions from video, to a large extent, remains an unsolved problem.\nThe understanding of emotion information in videos has many real-world applications. Video recommendation services, such as those employed by YouTube and Netflix, can benefit from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction. Better understanding of video emotions may enable the placement of advertisements that are consistent with the main video\u2019s mood and help avoid social inappropriateness such as placing a funny advertisement alongside a funeral video. Video summarization can also benefit from understanding emotions, since a good summary should probably include strong emotional content from the original video.\nThis paper tackles three inter-related problems in video emotion understanding. We start with emotion recogni-\n\u2022 Baohan Xu and Yu-Gang Jiang are with the School of Computer Science, Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China. Email:{bhxu14,ygj}@fudan.edu.cn. \u2022 Yanwei Fu, Boyang Li and Leonid Sigal are with Disney Research Pittsburgh, PA, USA. Email: y.fu@qmul.ac.uk, {lsigal, albert.li}@disneyresearch.com.\ntion in both supervised and zero-shot conditions. Zero-shot video emotion recognition aims to recognize emotion classes that are not seen during training. This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.g. Ekman\u2019s six emotions [16]). Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense. Therefore, when operating in the real world, recognition systems trained with a small set of emotion labels will inevitably encounter emotional and affective expressions not in its training set. In zero-shot learning, we rely on heterogeneous sources of background knowledge to alleviate reliance on training data and improve generalization to labels previously unseen. Since our goal is to recognize both emotional and affective impact of video from a computer vision perspective, we use \u201cemotion\u201d as a shorthand to refer to both emotion and affect. the boundary between which can be blurry (see, for example, the argument around surprise [57].)\nWe then define a novel task called video emotion attribution, which aims to identify the contribution of each frame in a video to its overall emotion. Not every frame contributes emotional information. This realization further enables the third task \u2013 emotion-oriented video summarization. In the emotion-oriented video summarization, we summarize a video and at the same time preserve emotional content as much as possible.\nOur knowledge transfer framework performs two types of knowledge transfer. In supervised emotion recognition, we learn an effective video encoding from a large-scale auxiliary emotional image dataset. This auxiliary Image Transfer Encoding (ITE) process can generate a video representation more conducive to emotion recognition than al-\nar X\niv :1\n51 1.\n04 79\n8v 1\n[ cs\n.C V\n] 1\n6 N\nov 2\n01 5\n2\nFig. 1: Humans make use of contextual information in emotion recognition. The face of U.S. Senator Jim Webb, when inspected in isolation on the left, appears agitated and angry. However, when put back in the context on the right, he appears happy and excited. Image credit: Doug Mills/The New York Times. Reproduced from [4].\nternative methods. In zero-shot emotion recognition, where the emotion to be recognized is not in the training set, we learn embeddings for emotional words from a largescale text corpus, so that an unknown emotion word can be semantically related to labeled or known emotions and subsequently recognized. The knowledge transfer technique further facilitates two novel applications: emotion attribution and emotion-oriented summarization. Video sub-shots more related to the overall emotion are identified and emphasized in the automatically generated summary.\nContributions: We introduce a framework for transferring knowledge from heterogeneous sources (image and text) for the understanding of emotions in videos. For the first time, we demonstrate zero-shot emotion recognition by utilizing knowledge learned from text sources to the video domain. We also propose the first definitions and solutions for the problems of emotion video attribution and emotion-oriented summarization. We show that our emotion-oriented summaries are better than alternative methods that do not consider emotion. Finally, we introduce and will make available to the community two new emotion-centric video datasets: the Plutchik and Ekman emotion datasets."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 Psychological Theories of Emotion", "text": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17]. The most well known model is probably Ekman\u2019s six pan-cultural basic emotions, including happiness, sadness, disgust, anger, fear, and surprise [16], [17]. However, the exact categories can vary from one model to another. Plutchik [59] added anticipation and trust to the list. Ortony, Clore and Collins\u2019s [56] model of emotion defined up to 22 emotions, including categories like hope, shame, and gratitude.\nNevertheless, more recent empirical findings and theories from the psychological construction approach [3], [44] suggest emotional experiences are much more varied than previously assumed. It is argued that the categories are modal or stereotypical emotions, and large fuzzy areas exist\nin the emotion landscape. Instead of associating a fixed set of facial expressions with each emotion, emotion recognition is influenced by visual and language context [4], [7]. The facial expression of smile, for example, can indicate happiness, embarrassment, or being subordinate in different contexts [3]. See Fig. 1 for a real-world example.\nOther theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes. Together, the complex dynamics and interactions lead to a rich set of emotional and affective experiences, and correspondingly rich natural language descriptions, such as ecstasy, nostalgia, or suspense. In order to cope with diverse emotional descriptions that may be practically difficult (or at least very costly to label), in this paper we investigate emotion recognition in a zero-shot setting (in addition to, a traditional, supervised setting). Our emotion recognizer is tested against emotional classes that do not appear in the training set. The zero-shot recognition task is designed to test the system\u2019s ability to make use of knowledge learned from heterogeneous sources in order to adapt to unseen tags."}, {"heading": "2.2 Automatic Emotion Analysis", "text": "In this section, we briefly review three relevant areas of research: recognition of facial expressions from images and videos, recognition of emotional impact of images on viewers, and recognition of emotional impact from videos.\nRecognition of facial expressions from images and videos. In the light of findings that humans rely on contextual information to recognize emotion [4], [7], in this paper, we aim to recognize the overall emotional impact of a video from all of its information. This contrasts with the identification of facial expressions and the associated emotion in static images and video, which has been a subject extensively studied. Two recent reviews of the topic can be found in [58] and [65]. Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held. Notably, Liu et al. [47] construct a mid-level representation called expressionlet from spatio-temporal manifold. Cruz et al. [10] proposed a dynamic downsampling of facial expressions in video\n3 at a rate proportional to the rate of temporal changes of visual information. When apex labels are provided, videos are downsampled around the apex of an expression.\nA few recent works focused on predicting emotions, affects, and emotion-related cognitive states outside the basic emotion categories. Kaliouby and Robinson [18] recognized emotion-related mental states, such as agreeing or feeling unsure. Bosch et al. [6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].\nRecognizing the emotional impact of still images on viewers. Machajdik and Hanbury [51] classified images into 8 affective categories: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. In addition to color, texture, and statistics about faces and skin area present in the image, they also make use of composition features such as the rule of the third and depth of field. Lu et al. [48] studied shape features along the dimensions of rounded-angular and simple-complex, and their effects in arousing viewers\u2019 emotions. You et al. [77] designed a deep convolutional neural network (CNN) for visual sentiment analysis. After training on the entire training set, images on which the CNN performs poorly are stochastically removed. The remaining images were used to fine-tune the network. A few work [8], [74] also employed off-the-shelf CNN features.\nRecognizing emotional impact from videos. For a more comprehensive review, we refer reader to the latest survey [71]. A large number of early work studied emotion in movies (e.g. [30], [37], [68]). Wang and Cheong [68] used an SVM with diverse audio-visual features to classify 2040 scenes in 36 Hollywood movies into 7 emotions. Jou et al. [36] worked on animated GIF files. Irie et al. [30] use Latent Dirichlet Allocation to extract audio-visual topics as midlevel features, which are combined with a Hidden-Markovlike dynamic model.\nSentiBank [5] contains a set of 1, 553 adjective-noun pairs, such as \u201cbeautiful flowers\u201d and \u201csad eyes\u201d, and images exemplifying each pair. One linear SVM detector was trained for each pair. The best-performing 1, 200 detectors provide a mid-level representation for emotion recognition. Chen et al. [8] replaced the SVM detectors with deep convolutional neural networks. Jiang et al. [33] explored a large set of features and confirmed the effectiveness of mid-level representations like SentiBank. In this work, we transfer knowledge learned from the same set of Flickr images for the purpose of video emotion analysis.\nThe implicit approach for recognizing the emotional impact of a video is to recognize emotions exhibited by viewers of that video. This clever trick delegates the complex task of video understanding to human viewers, thereby simplifying the problem. McDuff et al. [53] analyzed facial expressions exhibited by viewers of video advertisements recorded with webcams. Histogram of Oriented Gradient (HOG) features were extracted based on 22 key points on the faces. Purchase intent is predicted based on the entire emotion trajectory over time. Kapoor et al. [38] used video, skin conductance, and pressure sensors on the chair and the mouse to predict frustration when a user interacted with an intelligent tutoring system. However, the success of this\napproach depends on the availability of a large number of human participants.\nNevertheless, all previous work are limited as they aim to predict emotion or sentiment classes present in the training set. In this work, we utilize knowledge from other domains like images and text in order to identify emotion classes unseen in the training set. In addition, we also investigate related practical applications like emotionoriented video attribution and summarization."}, {"heading": "2.3 Multi-Instance Learning", "text": "The knowledge transfer approach adopted in this work is related to multi-instance learning (MIL), which has been extensively studied in the machine learning community and utilized in other domains. We therefore briefly review related techniques in the following. MIL refers to recognition problems where each label is associated with a bag of instances, such as a bag of video frames. It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78]. The problem investigated in this work is intrinsically a multi-instance learning case as each video consists of many frame instances with possibly different emotions.\nThere are basically two branches of MIL algorithms. In the first branch, many works attempted to enable singleinstance supervised learning algorithms to be directly applicable to multi-instance feature bags. This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others. These algorithms achieved satisfactory accuracies in several applications, but most of them can only handle small or moderate-sized data. In other words, they are computationally expensive and cannot be applied to deal with large-scale video data.\nThe second branch of works are more recent, where researchers tried to solve the MIL problems by adapting multi-instance bags to specific points in the original instance space. Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9]. Inspired by these works, we encode the video frame bags into single-instance representations of the bag-level information. Our approach is different from this category of MIL algorithms in the following: (1) our emotion recognition task is a multi-class multi-instance problem, while most of the previous MIL algorithms aimed at binary classification; (2) We perform the encoding process by using auxiliary data like images, and demonstrate that transferring such knowledge is important for video emotion analysis."}, {"heading": "2.4 Video Summarization", "text": "Video summarization has been studied for more than two decades. A complete review is beyond the scope of this paper and we refer readers to [66].\nThere are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72]. Video summarization has been explored for various types of content, including professional videos (e.g., movies or news reports) [55], [70], [72], surveillance videos [19],\n4 VideosAuxiliary Images Feature Extraction with Convolutional Neural Networks Image Deep Features\u2026 Feature Clustering Cluster Centers Frame Representation by Attribution Score: Video Representation by Image Transfer Encoding (ITE): Binary Classifier 1 Binary Classifier 2 Binary Classifier 3 \u2026 \u2018Anger\u2019 \u2018Joy\u2019 \u2018Fear\u2019 Emotion Recognition Frame 1 Frame n \u2026 Video ITE The Most Representative Clip Emotion Attribution Video Clip 1 \u2026 Video ITE Selected Clips for Summary Zero-Shot Emotion Recognition Auxiliary Texts Language Modeling Emotion Summarization Feature Space Mapping Semantic Space \u2026 Semantic Word Vectors Zero-Shot Recognition \u2026\u2026 \u2026 \u2026 \u2026 \u2026 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\".\".\".\".\".\".\" .\".\".\".\".\".\".\".\".\".\".\".\".\" + + + + + %% % % % + + + + + %% % % % + + + + + %% % % % Video Frame Deep Features \u2026 \u2026 \u2026 \u2026 Frame 1 Frame n \u2026Frame 1 Frame n Video Clip p Improved Space with T1S Anger & Surprise& Joy & Anger& Surprise & Joy & Anger& Surprise& Joy & \u2026 \u2026\nFig. 2: An overview of our framework. Information from the auxiliary images (bottom left) is used to extract an emotioncentric dictionary from CNN-encoded image elements, which is subsequently used to encode video (bottom middle) and recognize emotion (top left). The same encoding is used for emotion attribution and summarization (top middle). Finally, information from a large text corpus is utilized for zero-shot recognition of emotions, as illustrated on the right.\n[20], and, to a lesser extent, user-generated videos [72]. To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.g. object trajectories [45], tag localization [70] and semantic recognition [72]. Facial expressions has been considered by Dhall and Roland [13] who extracted video summaries by considering smile/happy face expression. However, none of these approaches have considered video summarization based on more general video emotion content. Such video emotion is an important cue for finding the most \u201cinteresting\u201d or \u201cimportant\u201d video highlights. For example, a good summary of a birthday party, or a graduation ceremony, should capture the most emotional moments in the event. Not considering the valuable emotion dimension in the video summarization task risks losing these precious moments in the summary."}, {"heading": "3 APPROACH", "text": "The overview of the proposed framework is illustrated in Figure 2. We start by introducing the problem formulation and common notation, and then discuss the auxiliary image transfer encoding for supervised recognition, auxiliary textbased transfer encoding for zero-shot recognition, and video emotion attribution and summarization."}, {"heading": "3.1 Problem Setup", "text": "Suppose we have a training video dataset\nTr = {(Vi, Xi, si, zi)}i=1,\u00b7\u00b7\u00b7 ,nTr ,\nwhere the ith video Vi = {fi,1, \u00b7 \u00b7 \u00b7 , fi,ni} has ni frames, with the features Xi = {xi,1, \u00b7 \u00b7 \u00b7 ,xi,ni}, where the subscripts i, j denote the jth frame of video Vi. xi,j is extracted by the state-of-the-art deep Convolutional Neural Network (CNN) architecture which was recently shown to greatly outperform more traditional hand-crafted low-level features, such as HOG and SIFT, on several benchmark datasets including MNIST and ImageNet [40] in machine learning and computer vision communities. Specifically, we retrain AlexNet [40] with 2, 600 ImageNet classes and use the seventh layer (\u201cfc7\u201d) representation as xi,j , computed on the input frame fi,j .\nWe use si to denote the encoded video-level feature of video Vi obtained using auxiliary image transfer encoding, which is introduced in Section 3.2; zi \u2208 ZTr is the class label of video Vi from the set of training labels ZTr and nTr is the total number of training videos. The testing data set is similarly defined as\nTe = {(Vi, Xi, si, z\u0303i)}i=1,\u00b7\u00b7\u00b7 ,nTe ,\nwhere nTe is the total number of testing videos. Under the zero-shot learning setting, the labels in training and testing sets have different domains: zi \u2208 ZTr and z\u0303i \u2208 ZTr \u222aZTe, ZTr \u2229ZTe = \u2205, denoting the testing set contains classes previously unseen.\nTo enable knowledge transfer, we introduce a largescale auxiliary image set and a text sentiment dataset. We denote the auxiliary image sentiment dataset as A = {(ai,\u03c6i)}i=1,\u00b7\u00b7\u00b7 ,|A|, where \u03c6i is the deep CNN feature of an image ai which is extracted with the same trained model as xi,j above.\n5 The textual data are represented as a sequence of words W = (w0, . . . , w|W |), wj \u2208 V where the vocabulary V is the set of unique words. We learn a K-dimensional embedding \u03c8w for each w \u2208 V ."}, {"heading": "3.2 Auxiliary Image Transfer Encoding (ITE) for Supervised Emotion Recognition", "text": "In multi-instance learning, the natural way of encoding multi-instance bags into single-instance representations is to cluster the instances of all the bags to several groups. Examples of this include CCE [78] and Mi-FV [73]. Such clustering enables the re-representations of each bag as the new Bag-of-Words (BoW) features. However, this comes with three challenges. First, the feature set is learned for the tasks of image classification, rather than for video emotion recognition. Second, the large and complex space of video visual elements (such as objects, events, and people) and their possible long-term temporal progression and interaction makes the domain intrinsically more complex than the previous image sentiment and MIL works. Third, the emotion is often expressed on certain limited (sparse) keyframes or video clips, and many video frames may not be directly related to the expressed emotion.\nTo address these challenges, we utilize emotion information from a large-scale emotional image dataset to encode the video content using a BoW representation. This can be intuitively explained from the perspective of entropy. A dictionary built from the auxiliary emotion-related images can efficiently encode a video frame with emotion information as a sparse vector which concentrates on a few dimensions. In comparison, a frame without emotion information will likely be encoded less efficiently, producing a denser vector with small values in many dimensions. As a result, a non-emotional frame will have higher entropy than the emotional frame, and hence less impact on the resulted BoW representation.\nMore formally, we first find D clusters from the auxiliary images and encode the video frames based on the cluster centers. Specifically, we perform a spherical k-means clustering on the auxiliary image dataset, which is to solve:\nmin |A|\u2211 i=1 (1\u2212 \u03b3i,d cos (\u03c6i,\u03c6d)). (1)\nThe goal is to find D spherical cluster centers \u03c61 . . . ,\u03c6D . The cosine similarity is defined as\ncos(\u03c6i,\u03c6d) = \u03c6>i \u03c6d \u2016\u03c6i\u2016\u2016\u03c6d\u2016 . (2)\nThe variable \u03b3i,d assigns an image ai to the closet cluster center d, which is defined as\n\u03b3i,d = { 1 if d = argmaxj cos (\u03c6i,\u03c6j) , 0 otherwise.\n(3)\nTo encode one video bag into its corresponding singleinstance representations, a BoW scheme is used to translate the feature set Xi into a D-dimensional vector si = (si,1, . . . , si,d, . . . si,D). Specifically, to encode the feature vector of a video Vi, we fix the cluster centers {\u03c61, ...,\u03c6D} found by the k-means and identify the K nearest cluster\nFig. 3: A frame from a graduation video that contains both smiling and weeping.\ncenters for each frame fi,j . We thus can get the assignments of each frame to each cluster, \u03bdi,j,d:\n\u03bdi,j,d = { 1 if \u03c6d \u2208 KNN (xi,j) , 0 otherwise,\n(4)\nwhere KNN (xi,j) denotes the spherical K nearest neighbours1 to xi,j from the cluster centers {\u03c61, ..., \u03c6D}. We then accumulate the effects of each frame on the dth dimension to compute the feature vector si:\nsi,d = ni\u2211 j=1 \u03bdi,j,d \u00b7 cos (xi,j ,\u03c6d) . (5)\nOur encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34]. First, the traditional BoW encodes local descriptors, such as SIFT and STIP, which requires a dictionary orders of magnitude greater than our frame set. Thus directly using standard BoW [64] to our problem will make the generated video-level features too sparse to be discriminative. Second, the soft-weighting encoding assigns one visual feature point to multiple clusters using typically exponential or Gaussian kernel, to downweight the contribution to the clusters that are not closest to the feature. However, in our problem one video frame can express multiple emotions simultaneously. For example, Figure 3 shows an example of both grief and happiness. Thus in Eq (5), one feature instance xi,j can equally contribute to different encoding bins. In other words, we make use of a uniform kernel instead.\nThe encoding scheme from the frame-level deep features to the video-level emotional representation helps the standard video emotion recognition tasks. Given a test video Vk \u2208 Te, its class label can be estimated as\nz\u0302k = argmax z\nL (sk | sTr, zTr) , (6)\nwhere L (\u00b7) is the predictor trained from the video-level feature set STr of the training video set Tr. In this paper, we use the support vector machines (SVM) classifier with chi-square kernel as the predictor L (\u00b7). The procedure of the ITE based supervised emotion recognition is summarized in Algorithm 1."}, {"heading": "3.3 Zero-Shot Emotion Recognition", "text": "Canonical emotion theories such as Ekman [17] often provide detailed textual definitions for a fixed number of\n1. Generally, we require that K 1, since videos can express much more \u2018versatile\u2019 emotions that those of images (e.g. Figure 3).\n6 Algorithm 1 Pseudo-code describing of ITE based supervised emotion recognition.\nRequire: \u2022 A : an auxiliary image dataset. \u2022 Tr : training video set; \u2022 Te : testing video set.\n1: Generate {xd} and {\u03b3ai,d} by the spherical k-means on A \u2190 Eq (1); 2: Generate a video-level feature set STr and STe by encoding the videos in Tr and Te\u2190 Eq (5); 3: Train chi-square SVM predictor L (\u00b7) using STr ; 4: Recognize emotions in Te with the predictor\u2190 Eq (6).\nprototypical emotions. However, recent research [3], [44] questioned the validity of basic emotional categories and highlighted differences within each category. This raises an interesting question: if we face a more diverse list of emotions than those in the training set, can we identify these emotions purely based on their textual description? This is the zero-shot recognition problem. To address it, we need to employ an auxiliary set of textual information to help encode the emotion classes that have never been visually seen before. Here, we use wTr and wTe to indicate the emotion label words of auxiliary and testing dataset.\nThe encoding algorithm finds a distributed representation for each word in the vocabulary, by training models from large-scale textural copra containing sentiment data. Specifically, we find a low-dimensional vector representation \u03c8w \u2208 RK by using each word wt in a corpus to predict nearby words by maximizing the following log probability:\nmax J = max 1\nT T\u2211 t=1 \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt), (7)\np(wo|wi) = 1\nZ exp\n( \u03c8>wo\u03c8wi ) ,\nwhere c is the context window for prediction; wo and wi are \u201cinput\u201d and \u201coutput\u201d words respectively, and \u03c8 is the continuous word representation to learn. Optimizing J is an effective way to learn continuous word representations, but the total computational cost has been intractable until recent deep learning developments such as word2vec [54], and GloVe [26].\nThis acquired vector representation serves as the intermediary between an emotion word w and its corresponding video emotion class. Mapping video-level features into this semantic space requires a regressor from the video feature space to the word embedding space:\ng : sTr \u2192 \u03c8wTr , (8)\nIn this work, we train a support vector regressor with a linear kernel for each dimension of the word vector \u03c8wTr . Similar support vector classifiers have also been used in the attribute learning works [22], [42].\nNote that the regression models in Eq (8) potentially have a generalization problem which is largely caused by the different visual distribution of disjointed training and testing classes in zero-shot learning settings. For example, videos of joy usually have positive frames, whilst a sad video would have negative ones. To ameliorate such generalization problems, we take inspiration from [21] and\napply Transductive 1-Step Self-Training (T1S) to adjust the word vector of new emotion classes. Specifically, for a class z? \u2208 ZTe that is previously unseen, and the corresponding word vector \u03c8z? , we compute a smoothed version \u03c8\u0304z? :\n\u03c8\u0304z? = 1\nK K\u2211 g(si)\u2208KNN(\u03c8z? ),Vi\u2208Te g (si) , (9)\nwhere KNN (\u03c8z?) denotes the set of spherical K nearest neighbors to \u03c8z? of the predicted testing word vector in the semantic space. Eq (9) aims at transductively ameliorating such visual differences by averaging the \u03c8\u0304z? with its nearest neighbor testing instances. Here, to prevent the semantic drift of self-training, we only do self-training for one step. By using Eq (9), we can get the updated word vector testing set \u03c8\u0304wTe .\nThus, given a test video Vj in the testing set, its class label z\u0303j can be estimated as\n\u02c6\u0303zj = argmax z\u2208ZTe\ncos ( g (sj) , \u03c8\u0304z ) . (10)\nCompared with the zero-shot learning algorithm in [21], we skip the intermediate level of latent attributes and directly apply the 1-step self-training in the semantic word vector space. In addition, we use cosine similarity as the metric rather than the Euclidean distance. The process is summarized in Algorithm 2.\nAlgorithm 2 Pseudo-code describing the T1S algorithm.\nRequire: \u2022 C : an auxiliary text dataset . \u2022 Tr : training video set; \u2022 Te : testing video set. \u2022 ZTr \u2229 ZTe = \u2205.\n1: Train the word2vec language model with the large-scale auxiliary text dataset\u2190 Eq (7) 2: Project emotion words of training and testing sets to continuous vectors vwTr and vwTe ; 3: Train regressors\u2190 Eq (8); 4: Perform zero-shot emotion recognition \u2190 Eq (9) and Eq\n(10)."}, {"heading": "3.4 Attribution and Summarization", "text": "We define the video attribution problem as one of attributing the emotion of a video to its frames/clips. The video emotion attribution problem is inspired by, and yet different from, text-based sentiment attribution [39]. The difference is that sentiment attribution only considers positive or negative attitudes while we consider more diverse emotions for emotion attribution.\nEmotion attribution can help us find video highlights [29], which are the interesting or important events happened in the video. Generally, the concepts of \u201cinteresting\u201d and \u201dimportant\u201d may be variable for different target video domains and applications, such as the scoring of a goal in soccer videos, applause and cheering in talk-show videos, and exciting speech in presentation videos. Nevertheless, most of these \u201cinteresting\u201d or \u201cimportant\u201d video events may contribute to/convey very strong video emotions which are thus highlighting the core parts of the whole video.\n7 Formally, for one video sequence Vi = {fi,1, . . . , fi,ni}, we want to find the video frames that highly contribute to the overall expressed video emotion. Using Eq (2), we can encode a frame fi,j as a similarity score vector:\nhi,j = [\u00b7 \u00b7 \u00b7 , \u03bdi,j,d \u00b7 cos(xi,j ,\u03c6d), \u00b7 \u00b7 \u00b7 ] , (11)\nwhere \u03bdi,j,d is defined in Eq (4). The vector hi,j uses the auxiliary image dataset A to evaluate the emotions in the jth frame.\nEquipped with the frame-level emotion score vector, the video emotion attribution can be formulated as measuring the similarity between the video-level emotion vector and the frame-level vectors. Specifically, the attribution score of the jth video frame is computed as the cosine similarity between si, the feature vector of the entire video, and hi,j . To find the frame j? that contributes the most to the overall emotion of Vi, we simply take the argmax over all frames:\nj? = argmax j\u2208[1,...,ni] cos (si,hi,j) . (12)\nSimilarly, emotion attribution may be performed on a list of pre-partitioned video clips. Given a list of clips {E1, . . . EP }, we can compute the similarity score vector for each clip as\nhp = \u00b7 \u00b7 \u00b7 , \u2211 j\u2208Ep \u03bdi,j,,d \u00b7 cos(xi,j ,\u03c6d), \u00b7 \u00b7 \u00b7  , (13) and then perform a similar argmax operation.\nEmotion attribution can facilitate interesting applications, such as video summarization and retrieval. To further enable the emotion-based video summarization, we can employ the emotion attribution technique described above and select a set of the most representative frames (or, the corresponding short clips containing the frames). Suppose V Ki is the set of key frames of the ith video, defined as\nV Ki = argmax j\u2208V Ki\ncos (si,hi,j)+\u03bb \u2211\nj\u2208V Ki \u2211 k\u2208[1,...,ni] cos (xi,j ,xi,k) ,\n(14) where \u03bb is a weight parameter, and the second term cos (xi,j ,xi,k) rewards key frames that are the most similar to other frames in the same video, which means that the selected set of key frames are representatives of the entire video.\nComparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips). Thus our method can produce a condensed, succinct and emotionrich summary which can facilitate the browsing, retrieval and storage of the original video content. Particularly, our summary results are more emotional interpretable due to the emotion attribution."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 Datasets and Settings", "text": "We adopt three video emotion datasets for evaluation. Among them, two (the Plutchik and the Ekman datasets)\nare introduced by us and will be made available to the community once this paper is accepted.\nThe YouTube emotion dataset [33]. This dataset contains 1,101 videos annotated with 8 basic emotions from the Plutchik\u2019s Wheel of Emotions. To better evaluate the zeroshot emotion recognition tasks, we re-annotate the videos into 24 emotions according to Plutchik\u2019s definitions by adding 3 variations to each of the basic emotions. For example, we split the basic joy class into ecstasy, joy, and serenity along the arousal dimension. We use the shorthand YouTube-8 and YouTube-24 for the original and reannotated datasets respectively. YouTube-24 is used as a more difficult task for evaluating the zero-shot recognition.\nThe Plutchik dataset. This dataset is derived from the recently proposed VideoStory dataset [28]. We use the keywords of the Plutchik\u2019s Wheel of Emotions [59] to query the textual descriptions of the VideoStory dataset. Since most of the textual descriptions come from the video captions, the emotions of the returned videos are accurately described by their corresponding emotional keywords. We manually pruned noisy videos from the returned set, which leads to a set of 626 videos belonging to 14 emotion classes.\nThe Ekman dataset. As discussed in the related work, the studies of Ekman [17] found a high agreement of emotions across cultures can be labelled as 6 basic emotion types. These 6 emotions are a subset of the 8 classes defined by Plutchik. Based on this, we collect the Ekman emotion dataset which has 1, 637 videos in the 6 emotion classes, with a minimum number of 221 videos per class. These videos are collected from social video-sharing websites, such as YouTube and Flickr. The dataset is crowdsource labelled by 10 different annotators, each video was labeled by at least 3 of those annotators. Final annotation for each video is produced by a majority vote among the labels assigned by the annotators.\nAuxiliary emotional image and text datasets. From the Flickr image dataset [5], we select as the auxiliary image data a subset of 110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see\nTable 2 in [5]). These images are clustered into 2, 000 clusters whose centers are used in Eq (2). As shown in [54], the large-scale text data can greatly benefit the trained language model. We train the Skip-gram model (Eq 7) on a large-scale text corpus, which includes around 7 billion words from the UMBC WebBase (3 billion words), the latest Wikipedia articles (3 billion words) and some other documents (1 billion words). The trained model has around 4 million unique words and phrases. Most of the documents are about scientific articles and professional reports which have very strict definitions, descriptions and usage of the emotion and sentiment related words.\nExperimental settings. Each video is uniformly sampled at 5 frame increments for feature extraction to reduce the computational cost. The dimension of the real-valued semantic vectors \u03c8 (Eq (7)) is set to 500. Our AlexNet CNN model is trained by ourselves using 2, 600 ImageNet classes with the Caffe toolkit [32], and we use the 4, 096-dimensional activations of the 7th fully-connected layer after the Rectified\n8 Linear Units as features. The number of nearest neighbors in Eq (4) is empirically set to 10% of the image clusters, which balances the computational cost with a good representation in Eq (5)."}, {"heading": "4.2 Video Emotion Recognition", "text": ""}, {"heading": "4.2.1 Supervised Recognition", "text": "We first report results on the supervised emotion recognition task, and compare our ITE encoding method with the following alternative baselines.\nMaxP [46]. The instance-level classifiers are trained using the labels inherited from their corresponding bags. These classifiers can be used to predict instance labels of testing videos. The final bag labels are produced by majority vote of instance labels. This method is a variant of the Key Instance Detection (KID) [46] in multi-class multi-instance setting.\nAvgP [76]. We average the frame-level image features of one video as video-level feature descriptions for classification. For the ith video, its average pooling feature is computed as 1 ni \u2211ni j=1 xi,j . The average pooling is the standard approach of aggregating frame-level features into video-level descriptions as mentioned in [76].\nMi-FV [73]. MIL bags of training videos are mapped into a new bag-level Fisher Vector representation. Mi-FV is able handle large-scale MIL data efficiently.\nCCE [78]. The instances of all training bags are clustered into b groups, and each bag is re-represented by b binary features, where the values of the ith feature is 1 if the concerned bag has instances falling into the ith group and 0 otherwise. This is essentially a simplified version of our ITE method encoded by training instances only.\nAs for the SVM predictor in Eq (6), the linear kernel is used for Mi-FV and MaxP due to the large number of samples/dimensions, and the Chi-square kernel2 is used for others. A binary two-class SVM model is trained for each emotion class separately.\nThe experimental results are shown in Figure 4, which clearly demonstrate that our ITE method significantly outperforms the four alternatives with large margins on all three datasets. This validates the effectiveness of our method in generating a better video-level feature representation based on the auxiliary images. In particular, the improvement of ITE over CCE and Mi-FV verifies that the knowledge transferred from the auxiliary emotional image dataset is probably more critical than that existing in the training video frames. This supports our argument that most of the frames of these videos have no direct relation to the emotions expressed by the videos, and underscores the importance of knowledge transfer.\nOne should notice that CCE has the worst performance. CCE re-encodes the multi-instances into binary representations by ensemble clustering. Such representations may have better performance than the hand-crafted features used in [78], but they cannot beat the recently proposed deep\n2. The RBF kernel is also evaluated but shows slightly lower performance than that of the Chi-square kernel.\nfeatures, which have been shown to be able to extract higher level information [40]. In other words, the re-encoding process of CCE loses discriminative information gained from the deep features, and is therefore unsuited for the task.\nIn addition, Mi-FV and MaxP have similar performance: MaxP is slightly better on Plutchik, Ekman and Mi-FV is slightly better on YouTube. However, the results of MiFV and MaxP are much worse than those of AvgP. These differences can be explained by the different choices of kernels. We validate that the AvgP with linear SVM classifier has similar performance (with a variance of 2%) as MaxP and Mi-FV. Nevertheless, due to high dimensions of Fisher Vectors and large amount of training instances in MaxP, nonlinear kernels will introduce prohibitive computational cost. Thus, in subsequent experiments, we use AvgP as the major alternative baseline to ITE since other alternatives do not demonstrate competitive advantages."}, {"heading": "4.2.2 Zero-Shot Recognition", "text": "Experiments on zero-shot emotion recognition are conducted on two datasets: the Plutchik and the YouTube. We use anger, joy, surprise, and terror as testing classes in the Plutchik dataset (300 testing instances in total). We validate our methods on both versions of YouTube dataset: YouTube8 and YouTube-24. For YouTube-8, we use fear and sadness as the testing classes. For YouTube-24, we randomly split the 24 classes into 18 training and 6 testing classes with 5-round repeated experiments. In this zero-shot setting, no testing classes are seen during training.\nFirst and foremost, we need to highlight that without the heterogeneous knowledge transferred from text, there is no way to enable zero-shot emotion learning. There is no previous work that had ever reported the successful experiments on zero-shot emotion learning, mainly due to the difficulties in clearly defining the semantic word/attribute vectors of emotion classes. In this paper, a language model trained from large-scale text data is employed to transfer the knowledge of emotion words to the video domain via the semantic word vectors. Such a transferring step enables the zero-shot emotion recognition.\nWe compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42]. For DAP, at test time each dimension of the word vectors of each test sample is predicted, from which the test class labels are inferred. DAP can be taken as directly using Eq (10) without the word vector smoothing by Eq (9). Four variants are compared: (a) using different video-level feature representation (AvgP or ITE); (b) using different zero-shot learning algorithm (T1S or DAP). Figure 5 shows the results. Our ITE+T1S approach is the best performing method, outperforming the second best by 3.6, 4.8, and 1.2 absolute percentage points respectively. The results confirm the effectiveness of the knowledge transfer scheme, which improves upon the random baseline by 8.1, 6.3, and 15.9 absolute percentage points. The second best model for Plutchik and YouTube-24 is AvgP+T1S, whose performance is close to the best method. This suggests the T1S technique contributes the biggest performance gain when the training classes bear some similarity to the unseen testing classes. However, when the training classes are very different from the testing classes, the ITE encoding scheme plays an important role. Overall, the experiments show the\n9 20 30 40 50 60\nPlutchik Ekman YouTube\nAc cu\nra cy\n36 .9\n% 4 3.\n9%\n35 .3\n%\n27 .3\n%\n45 .1\n%\n39 %\n48 .3\n%\n36 .4\n%\n31 .5\n%\n51 .5\n%\n34 .5\n% 40 .1\n%\n38 .4\n%\n30 .2\n%\n44 .7\n%\nMaxP AvgP Mi-FV CCE ITE\nFig. 4: Supervised Learning on emotion dataset analysis. We use 2000 bins for our method. The two baselines are used both linear kernel/chi-square kernel.\n20\n30\n40\n50\n60\nAc cu\nra cy\nPlutchik YouTube-8 YouTube-24\n25 %\n25 .6\n% 29 .5\n%\n25 .7\n% 3 3.\n1%\n50 %\n48 .6\n%\n50 .3\n%\n51 .5\n% 56 .3\n%\n16 .7\n% 22 .6\n%\n31 .4\n%\n23 .3\n%\n32 .6\n%\nChance AvgP+DAP AvgP+T1S ITE+DAP ITE+T1S\nFig. 5: Zero-shot Learning results of Youtube dataset. Illustrated is the Mean Accuracy.\ncombination of ITE+T1S is effective under different zeroshot learning conditions. Given the inherent difficulties of the zero-shot learning task, we consider the results to be very promising."}, {"heading": "4.2.3 Key Implementation Choices", "text": "In this subsection, we discuss the settings of two important experimental options in our framework.\nNumber of clusters for auxiliary emotional images. We vary the number of clusters (i.e., D) of the auxiliary images in Eq (2). These experiments are conducted on the Ekman dataset. The results are plotted in Figure 6, which further validate the effectiveness of our ITE method: when varying the number of clusters from 100\u2212 5000, our ITE results are gradually improved. This is due to the fact that the increased number of clusters help capture additional discriminative information.\nFine-tuning of the AlexNet CNN model. We also validate the experiments by using the auxiliary images to fine-tune the weights in the AlexNet CNN model. After the finetuning step, the classification results on all the three datasets have only small changes (\u00b10.5%) rather than significant\nimprovements. We postulate that the videos in our datasets have more diversified contents and relatively lower quality than the Flickr images. In other words, there are some differences between the distributions of the images and the videos, which is the reason that fine-tuning with images is not very helpful.\n10"}, {"heading": "4.3 Video Emotion Attribution", "text": "As discussed earlier, another advantage of our encoding scheme is that we can identify the video clips3 that have high impact on the overall video emotion.\nAs the first work on video emotion attribution, we define the evaluation protocol of user study to evaluate the performance of different algorithms for this task: Ten subjects, unaware of project goals, were invited for the user study. Given all emotion keywords of the corresponding dataset and clip computed from the video, participants are asked to guess the name of the emotion expressed in the clip. These clips are selected by different methods discussed below. Since the ground-truth video emotion labels are known, we are able to compute the fraction of participants who assigned the correct emotion label for each clip.\nWe randomly select 20 videos from each of the three datasets. For each video, we compute the 2-second video clip that contains the highest attribution towards video emotion, using Eq (12).\nWe compare several different methods: Chance. It is the prior probability of guessing the emotion without viewing any portion of the video Random sampling. We first randomly sample 2 other clips (2 seconds each) from the same video that do not overlap with the first clip. Face_present. \u201cface_present\u201d features [79] is used to rank all the videos frames. Larger and more faces are detected in those frames that are highly ranked. 2\u2212 clips are thus generated by using the top rank frames.\n3. Note that in term of our pilot study, the emotions are sparsely expressed in the video. On averagely around 10% of totally video frames are related to emotion on our three dataset.\nThe results are shown in Figure 7. Our method achieve best accuracy, and outperforms \"face_present\" baseline by 16\u201326 absolute percentage points. It is true that facial expression can convey strong emotions. Nevertheless, a large portion of videos in our dataset are user-generated videos which are more general than face videos used in traditional facial emotion recognition tasks. Those videos do not even have the human faces present. This result indicates that our method can consistently identify video clips that convey emotions recognizably similar to the emotion conveyed by the original video. That is, the identified clips contribute to the overall video emotion.\nA qualitative result of emotion attribution is shown in Figure 8, where the video is uniformly sampled every 10 frames. The bar chart shows scores of different frames, where the key frames are shown above the bars. The figure demonstrates that clips with stronger emotional contents are given higher scores of attribution, validating the effectiveness of our method."}, {"heading": "4.4 Emotion-Oriented Video Summarization", "text": "Finally, we evaluate our framework on emotion-oriented video summarization. We compare with four baselines: (1) Uniform sampling. It uniformly samples several clips from video. (2) K-means sampling. It simply clusters the clips and selects a clip closest to each cluster centroid. (3) Story-driven summarization [49]. This approach was developed to summarize very long egocentric videos. We slightly modify the implementation and make the length of the summary controllable for our task. (4) Real-time summarization [72]. Wang et al. [72] aim at efficient summarization of videos based on semantic content recognition results. For all the methods, the length of summary is fixed to 6 seconds if the original video is longer than 1 minute. For short videos, the length is fixed at 10% of original video duration.\nFollowing [66], we conduct a user study to evaluate different summarization methods. Ten subjects unfamiliar with the project participated in the study. We show the summary results of all the methods (without the audio information) to each participant. Participants are asked to rate each result on a five-point scale for each of the following evaluation metrics: (1) Accuracy: the summary accurately describes the \u201cdominating\" high-level semantics of the original video; (2) Coverage: the summary covers as much visual content using\n11\nas few frames as possible. (3) Quality: the overall subjective quality of the summary; (4) Emotion: the summary conveys the same main emotion as the original video.\nThe results are shown in Figure 9. The average score is shown in the \u201cOverall\u201d column. Our method (\u201cEmotionVS\") performs better than the other methods on the accuracy and the emotion metrics. On the emotion metric, we beat the best baseline by a margin of 0.87. Although we are doing slightly worse on the coverage metric (-0.13 compared to the best baseline), the drop in quality is minimal (-0.04 compared to the best baseline). The results suggest that the selection of emotional key frames and clips does not only capture the emotion of the original video, but also improves the overall accuracy of the summary, since an accurate summary should capture emotional content. Our emotionoriented summarization method significantly increases the amount of emotional contents captured by the summary without material loss on other quality measures.\nA qualitative evaluation is shown in Figure 10. At the\ntop, the figure shows a video of an art therapist (the woman in green). Different from other methods, our summarization not only captured the therapy procedure, but also focused on the sadness of the therapist, which is the central emotion conveyed in this video. At the bottom, we illustrate a user-generated video where a father surprises his daughter during a baseball game by dressing as the catcher and revealing himself. All baseline methods are more focused on the baseball game itself, which, however, is the least related to the emotion of this video. In contrast, our method clearly captures the revealing of the father, the surprised daughter, and the subsequent, emotional hug."}, {"heading": "5 CONCLUSIONS", "text": "This paper provides the first study of knowledge transfer from heterogeneous sources for the task of video emotion understanding, which includes supervised and zeroshot emotion recognition, emotion attribution and emotionoriented summarization. For effective knowledge transfer,\n12\nwe learn encoding schemes from a large-scale emotional image data set and a large, 7-billion-word text corpus. This transfer facilitates the creation of a representation conducive to the tasks of video emotion understanding. In zero-shot emotion recognition, an unknown emotional word is related to known emotion classes through the use of a distributed representation in order to identify emotions unseen during training. Our experiments on three challenging datasets clearly demonstrate the benefit of utilizing external knowledge. Our framework also enables novel applications such as emotion attribution and emotion-oriented video summarization. A user study shows that our summaries accurately capture emotional content consistent with the overall emotion of the original video. Future work will address the joint application of emotion-oriented summarization and storydriven summarization, which should allow us to create complete and emotionally compelling stories."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Chong-Wah Ngo for his constructive advise."}], "references": [{"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["J. Amores"], "venue": "Artif. Intell.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Are emotions natural kinds", "author": ["L.F. Barrett"], "venue": "Perspectives on Psychological Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Language as context for the perception of emotion", "author": ["L.F. Barrett", "K.A. Lindquist", "M. Gendron"], "venue": "Trends in cognitive sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Largescale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T.M. Breuel", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Automatic detection of learningcentered affective states in the wild", "author": ["N. Bosch", "S. D\u2019Mello", "R. Baker", "J. Ocumpaugh", "V. Shute", "M. Ventura", "L. Wang", "W. Zhao"], "venue": "In the 2015 International Conference on Intelligent User Interfaces,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Do facial expressions signal specific emotions? Judging emotion from the face in context", "author": ["J.M. Carroll", "J.A. Russell"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "Darrell", "S.-F. Chang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Miles: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "IEEE TPAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Vision and attention theory based sampling for continuous facial emotion recognition", "author": ["A.C. Cruz", "B. Bhanu", "N. S"], "venue": "IEEE TAC,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Video summarization by curve simplification", "author": ["D. DeMenthon", "V. Kobla", "D. Doermann"], "venue": "In ACM MM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Emotion recognition in the wild challenge", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "M. Wagner", "T. Gedeon"], "venue": "In ACM ICMI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Group expression intensity estimation in videos via gaussian processes", "author": ["A. Dhall", "G. Roland"], "venue": "In ICPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artif. Intell.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Emotion, cognition, and behavior", "author": ["R.J. Dolan"], "venue": "Science, 298:1191\u2013", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Universals and cultural differences in facial expressions of emotion", "author": ["P. Ekman"], "venue": "In Nebraska Symposium on Motivation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1972}, {"title": "An argument for basic emotions", "author": ["P. Ekman"], "venue": "Cognition & emotion,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Real-time inference of complex mental states from facial expressions and head gestures", "author": ["R. el Kaliouby", "P. Robinson"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Multi-view metric learning for multi-view video summarization", "author": ["Y. Fu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Multi-view video summarization", "author": ["Y. Fu", "Y. Guo", "Y. Zhu", "F. Liu", "C. Song", "Z.-H. Zhou"], "venue": "IEEE TMM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Learning multimodal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "IEEE TPAMI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Interestingness prediction by robust learning to rank", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong", "Y. Yao"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Multiinstance kernels", "author": ["T. G\u00e4rtner", "P.A. Flach", "A. Kowalczyk", "A.J. Smola"], "venue": "Proc. 19th International Conf. on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Emotion regulation: Affective", "author": ["J.J. Gross"], "venue": "cognitive, and social consequences. Psychophysiology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Videostory: A new multimedia embedding for few-example recognition and translation of events", "author": ["A. Habibian", "T. Mensink", "C.G.M. Snoek"], "venue": "In ACM MM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "An integrated scheme for automated video abstraction based on unsupervised cluster-validity analysis", "author": ["A. Hanjalic", "H. Zhang"], "venue": "IEEE TCSVT,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Affective audio-visual words and latent topic driving model for realizing movie affective scene classification", "author": ["G. Irie", "T. Satou", "A. Kojima", "T. Yamasaki", "K. Aizawa"], "venue": "IEEE TMM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A probabilistic framework for modeling and real-time monitoring human fatigue. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["Q. Ji", "P. Lan", "C. Looney"], "venue": "IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Predicting emotions in usergenerated videos", "author": ["Y.-G. Jiang", "B. Xu", "X. Xue"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Representations of keypoint-based semantic concept detection: A comprehensive study", "author": ["Y.-G. Jiang", "J. Yang", "C.-W. Ngo", "A.G. Hauptmann"], "venue": "IEEE TMM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Understanding and predicting interestingness of videos", "author": ["Y.-G. Jiang", "YanranWang", "R. Feng", "X. Xue", "Y. Zheng", "H. Yang"], "venue": "In AAAI,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Predicting viewer perceived emotions in animated gifs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Affective content detection using HMMs", "author": ["H.-B. Kang"], "venue": "In ACM MM,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Automatic prediction of frustration", "author": ["A. Kapoor", "W. Burleson", "R.W. Picard"], "venue": "Int. J. Hum.-Comput. Stud.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Deep multiinstance transfer learning", "author": ["D. Kotzias", "M. Denil", "P. Blunsom", "N. de Freitas"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE TPAMI,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "A dynamic and dual-process theory of humor", "author": ["B. Li"], "venue": "In The 3rd Annual Conference on Advances in Cognitive Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "The brain basis of emotion: a meta-analytic review", "author": ["K.A. Lindquist", "T.D. Wager", "H. Kober", "E. Bliss-Moreau", "L.F. Barrett"], "venue": "Trends in cognitive sciences,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A hierarchical visual model for video object summarization", "author": ["D. Liu", "G. Hua", "T. Chen"], "venue": "IEEE TPAMI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Key instance detection in multiinstance learning", "author": ["G. Liu", "J. Wu", "Z. Zhou"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["M. Liu", "S. Shan", "R. Wang", "X. Chen"], "venue": "In IEEE CVPR,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "On shape and the computability of emotions", "author": ["X. Lu", "P. Suryanarayan", "R.B. Adams", "J. Li", "M.G. Newman", "J.Z. Wang"], "venue": "In ACM MM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Story-driven summarization for egocentric video", "author": ["Z. Lu", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "A user attention model for video summarization", "author": ["Y.-F. Ma", "L. Lu", "H.-J. Zhang", "M. Li"], "venue": "In ACM MM,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2002}, {"title": "Affective image classication using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "In ACM MM,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "EMA: A process model of appraisal dynamics", "author": ["S. Marsella", "J. Gratch"], "venue": "Journal of Cognitive Systems Research,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads", "author": ["D. McDuff", "R.E. Kaliouby", "J.F. Cohn", "R. Picard"], "venue": "IEEE TAC,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Video summarization and scene detection by graph modeling", "author": ["C.-W. Ngo", "Y.-F. Ma", "H.-J. Zhang"], "venue": "IEEE TCSVT,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "The Cognitive Structure of Emotions", "author": ["A. Ortony", "G. Clore", "A. Collins"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1988}, {"title": "What\u2019s basic about basic emotions", "author": ["A. Ortony", "T.J. Turner"], "venue": "Psychological Review,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1990}, {"title": "Machine analysis of facial behaviour: Naturalistic and dynamic behaviour", "author": ["M. Pantic"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}, {"title": "Emotion: Theory, research, and experience", "author": ["R. Plutchik"], "venue": "In Theories of Emotion,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1980}, {"title": "Avec 2011\u2014The first international audio/visual emotion challenge", "author": ["B. Schuller", "M.F. Valstar", "F. Eyben", "G. McKeown", "R. Cowie", "M. Pantic"], "venue": "ICACII,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Recognising interest in conversational speechcomparing bag of frames and supra-segmental features", "author": ["B.R.G. Schuller"], "venue": "In IN- TERSPEECH,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2009}, {"title": "Smile or smirk? automatic detection of spontaneous asymmetric smiles to understand viewer experience", "author": ["T. Senechal", "J. Turcot", "R.E. Kaliouby"], "venue": "In IEEE FG,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Weakly supervised pain localization using multiple instance learning", "author": ["K B.M. Sikka", "A. Dhall"], "venue": "In IEEE FG,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "Video google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In ICCV,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2003}, {"title": "Facial expression recognition", "author": ["Y. Tian", "T. Kanade", "J. Cohn"], "venue": "Handbook of Face Recognition,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Video abstraction: A systematic review and classification", "author": ["B.T. Truong", "S. Venkatesh"], "venue": "ACM TOMM,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2007}, {"title": "The first facial expression recognition and analysis challenge", "author": ["M. Valstar", "B. Jiang", "M. Mehu", "M. Pantic", "S. Klaus"], "venue": "In IEEE FG,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "Affective understanding in film", "author": ["H.-L. Wang", "L.-F. Cheong"], "venue": "IEEE TCSVT,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "In ICML,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2000}, {"title": "Event driven web video summarization by tag localization and key-shot identification", "author": ["M. Wang", "R. Hong", "G. Li", "Z.-J. Zha", "S. Yan", "T.-S. Chua"], "venue": "IEEE TMM,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2012}, {"title": "Video affective content analysis: a survey of state of the art methods", "author": ["S. Wang", "Q. Ji"], "venue": "IEEE TAC,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2015}, {"title": "Realtime summarization of user-generated videos based on semantic recognition", "author": ["X. Wang", "Y. Jiang", "Z. Chai", "Z. Gu", "X. Du", "D. Wang"], "venue": "In ACM MM,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2014}, {"title": "Scalable multi-instance learning", "author": ["X.-S. Wei", "J. Wu", "Z.-H. Zhou"], "venue": "In ICDM,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Visual sentiment prediction with deep convolutional neural networks", "author": ["C. Xu", "S. Cetintas", "K.-C. Lee", "L.-J. Li"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2014}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "A discriminative CNN video representation for event detection", "author": ["Z. Xu", "Y. Yang", "A.G. Hauptmann"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2014}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "In AAAI,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Solving multi-instance problems with classifier ensemble based on constructive clustering", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Knowledge and Information Systems,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2007}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X.R.D. Zhu"], "venue": "In CVPR,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "Some successes have been achieved with the use of deeplearning architectures trained for text at both sentenceand document-level [39] or image sentiment analysis [8].", "startOffset": 129, "endOffset": 133}, {"referenceID": 7, "context": "Some successes have been achieved with the use of deeplearning architectures trained for text at both sentenceand document-level [39] or image sentiment analysis [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 21, "context": "Video recommendation services, such as those employed by YouTube and Netflix, can benefit from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction.", "startOffset": 188, "endOffset": 192}, {"referenceID": 32, "context": "Video recommendation services, such as those employed by YouTube and Netflix, can benefit from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction.", "startOffset": 200, "endOffset": 204}, {"referenceID": 2, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 41, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Ekman\u2019s six emotions [16]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 142, "endOffset": 146}, {"referenceID": 40, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 148, "endOffset": 152}, {"referenceID": 49, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 154, "endOffset": 158}, {"referenceID": 54, "context": "the boundary between which can be blurry (see, for example, the argument around surprise [57].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Reproduced from [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 16, "context": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17].", "startOffset": 224, "endOffset": 228}, {"referenceID": 15, "context": "The most well known model is probably Ekman\u2019s six pan-cultural basic emotions, including happiness, sadness, disgust, anger, fear, and surprise [16], [17].", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "The most well known model is probably Ekman\u2019s six pan-cultural basic emotions, including happiness, sadness, disgust, anger, fear, and surprise [16], [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 56, "context": "Plutchik [59] added anticipation and trust to the list.", "startOffset": 9, "endOffset": 13}, {"referenceID": 53, "context": "Ortony, Clore and Collins\u2019s [56] model of emotion defined up to 22 emotions, including categories like hope, shame, and gratitude.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Nevertheless, more recent empirical findings and theories from the psychological construction approach [3], [44] suggest emotional experiences are much more varied than previously assumed.", "startOffset": 103, "endOffset": 106}, {"referenceID": 41, "context": "Nevertheless, more recent empirical findings and theories from the psychological construction approach [3], [44] suggest emotional experiences are much more varied than previously assumed.", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "Instead of associating a fixed set of facial expressions with each emotion, emotion recognition is influenced by visual and language context [4], [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Instead of associating a fixed set of facial expressions with each emotion, emotion recognition is influenced by visual and language context [4], [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "The facial expression of smile, for example, can indicate happiness, embarrassment, or being subordinate in different contexts [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 24, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 40, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 49, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "In the light of findings that humans rely on contextual information to recognize emotion [4], [7], in this paper, we aim to recognize the overall emotional impact of a video from all of its information.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "In the light of findings that humans rely on contextual information to recognize emotion [4], [7], in this paper, we aim to recognize the overall emotional impact of a video from all of its information.", "startOffset": 94, "endOffset": 97}, {"referenceID": 55, "context": "Two recent reviews of the topic can be found in [58] and [65].", "startOffset": 48, "endOffset": 52}, {"referenceID": 62, "context": "Two recent reviews of the topic can be found in [58] and [65].", "startOffset": 57, "endOffset": 61}, {"referenceID": 64, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 87, "endOffset": 91}, {"referenceID": 57, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 184, "endOffset": 188}, {"referenceID": 44, "context": "[47] construct a mid-level representation called expressionlet from spatio-temporal manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed a dynamic downsampling of facial expressions in video", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Kaliouby and Robinson [18] recognized emotion-related mental states, such as agreeing or feeling unsure.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 0, "endOffset": 3}, {"referenceID": 59, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 147, "endOffset": 151}, {"referenceID": 48, "context": "Machajdik and Hanbury [51] classified images into 8 affective categories: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness.", "startOffset": 22, "endOffset": 26}, {"referenceID": 45, "context": "[48] studied shape features along the dimensions of rounded-angular and simple-complex, and their effects in arousing viewers\u2019 emotions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "[77] designed a deep convolutional neural network (CNN) for visual sentiment analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A few work [8], [74] also employed off-the-shelf CNN features.", "startOffset": 11, "endOffset": 14}, {"referenceID": 71, "context": "A few work [8], [74] also employed off-the-shelf CNN features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 68, "context": "For a more comprehensive review, we refer reader to the latest survey [71].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "[30], [37], [68]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[30], [37], [68]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 65, "context": "[30], [37], [68]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 65, "context": "Wang and Cheong [68] used an SVM with diverse audio-visual features to classify 2040 scenes in 36 Hollywood movies into 7 emotions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "[36] worked on animated GIF files.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] use Latent Dirichlet Allocation to extract audio-visual topics as midlevel features, which are combined with a Hidden-Markovlike dynamic model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "SentiBank [5] contains a set of 1, 553 adjective-noun pairs, such as \u201cbeautiful flowers\u201d and \u201csad eyes\u201d, and images exemplifying each pair.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8] replaced the SVM detectors with deep convolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[33] explored a large set of features and confirmed the effectiveness of mid-level representations like SentiBank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] analyzed facial expressions exhibited by viewers of video advertisements recorded with webcams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] used video, skin conductance, and pressure sensors on the chair and the mouse to predict frustration when a user interacted with an intelligent tutoring system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 68, "endOffset": 72}, {"referenceID": 58, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 94, "endOffset": 98}, {"referenceID": 75, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 52, "endOffset": 55}, {"referenceID": 60, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 76, "endOffset": 79}, {"referenceID": 72, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 92, "endOffset": 96}, {"referenceID": 66, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 127, "endOffset": 131}, {"referenceID": 75, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 72, "endOffset": 76}, {"referenceID": 70, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 63, "context": "A complete review is beyond the scope of this paper and we refer readers to [66].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 67, "endOffset": 71}, {"referenceID": 42, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 94, "endOffset": 98}, {"referenceID": 52, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 100, "endOffset": 104}, {"referenceID": 67, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 106, "endOffset": 110}, {"referenceID": 69, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 112, "endOffset": 116}, {"referenceID": 52, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 26, "endOffset": 30}, {"referenceID": 67, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 32, "endOffset": 36}, {"referenceID": 69, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "[20], and, to a lesser extent, user-generated videos [72].", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[20], and, to a lesser extent, user-generated videos [72].", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.", "startOffset": 120, "endOffset": 124}, {"referenceID": 52, "context": "To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 20, "endOffset": 24}, {"referenceID": 67, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 43, "endOffset": 47}, {"referenceID": 69, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Facial expressions has been considered by Dhall and Roland [13] who extracted video summaries by considering smile/happy face expression.", "startOffset": 59, "endOffset": 63}, {"referenceID": 37, "context": "xi,j is extracted by the state-of-the-art deep Convolutional Neural Network (CNN) architecture which was recently shown to greatly outperform more traditional hand-crafted low-level features, such as HOG and SIFT, on several benchmark datasets including MNIST and ImageNet [40] in machine learning and computer vision communities.", "startOffset": 273, "endOffset": 277}, {"referenceID": 37, "context": "Specifically, we retrain AlexNet [40] with 2, 600 ImageNet classes and use the seventh layer (\u201cfc7\u201d) representation as xi,j , computed on the input frame fi,j .", "startOffset": 33, "endOffset": 37}, {"referenceID": 75, "context": "Examples of this include CCE [78] and Mi-FV [73].", "startOffset": 29, "endOffset": 33}, {"referenceID": 70, "context": "Examples of this include CCE [78] and Mi-FV [73].", "startOffset": 44, "endOffset": 48}, {"referenceID": 61, "context": "Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34].", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 61, "context": "Thus directly using standard BoW [64] to our problem will make the generated video-level features too sparse to be discriminative.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Canonical emotion theories such as Ekman [17] often provide detailed textual definitions for a fixed number of", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "However, recent research [3], [44] questioned the validity of basic emotional categories and highlighted differences within each category.", "startOffset": 25, "endOffset": 28}, {"referenceID": 41, "context": "However, recent research [3], [44] questioned the validity of basic emotional categories and highlighted differences within each category.", "startOffset": 30, "endOffset": 34}, {"referenceID": 51, "context": "Optimizing J is an effective way to learn continuous word representations, but the total computational cost has been intractable until recent deep learning developments such as word2vec [54], and GloVe [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 23, "context": "Optimizing J is an effective way to learn continuous word representations, but the total computational cost has been intractable until recent deep learning developments such as word2vec [54], and GloVe [26].", "startOffset": 202, "endOffset": 206}, {"referenceID": 39, "context": "Similar support vector classifiers have also been used in the attribute learning works [22], [42].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "To ameliorate such generalization problems, we take inspiration from [21] and apply Transductive 1-Step Self-Training (T1S) to adjust the word vector of new emotion classes.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "Compared with the zero-shot learning algorithm in [21], we skip the intermediate level of latent attributes and directly apply the 1-step self-training in the semantic word vector space.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "The video emotion attribution problem is inspired by, and yet different from, text-based sentiment attribution [39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "Emotion attribution can help us find video highlights [29], which are the interesting or important events happened in the video.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 41, "endOffset": 45}, {"referenceID": 63, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 47, "endOffset": 51}, {"referenceID": 69, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "The YouTube emotion dataset [33].", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "This dataset is derived from the recently proposed VideoStory dataset [28].", "startOffset": 70, "endOffset": 74}, {"referenceID": 56, "context": "We use the keywords of the Plutchik\u2019s Wheel of Emotions [59] to query the textual descriptions of the VideoStory dataset.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "As discussed in the related work, the studies of Ekman [17] found a high agreement of emotions across cultures can be labelled as 6 basic emotion types.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "From the Flickr image dataset [5], we select as the auxiliary image data a subset of 110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see Table 2 in [5]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "From the Flickr image dataset [5], we select as the auxiliary image data a subset of 110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see Table 2 in [5]).", "startOffset": 193, "endOffset": 196}, {"referenceID": 51, "context": "As shown in [54], the large-scale text data can greatly benefit the trained language model.", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "Our AlexNet CNN model is trained by ourselves using 2, 600 ImageNet classes with the Caffe toolkit [32], and we use the 4, 096-dimensional activations of the 7th fully-connected layer after the Rectified", "startOffset": 99, "endOffset": 103}, {"referenceID": 43, "context": "MaxP [46].", "startOffset": 5, "endOffset": 9}, {"referenceID": 43, "context": "This method is a variant of the Key Instance Detection (KID) [46] in multi-class multi-instance setting.", "startOffset": 61, "endOffset": 65}, {"referenceID": 73, "context": "AvgP [76].", "startOffset": 5, "endOffset": 9}, {"referenceID": 73, "context": "The average pooling is the standard approach of aggregating frame-level features into video-level descriptions as mentioned in [76].", "startOffset": 127, "endOffset": 131}, {"referenceID": 70, "context": "Mi-FV [73].", "startOffset": 6, "endOffset": 10}, {"referenceID": 75, "context": "CCE [78].", "startOffset": 4, "endOffset": 8}, {"referenceID": 75, "context": "Such representations may have better performance than the hand-crafted features used in [78], but they cannot beat the recently proposed deep", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": "features, which have been shown to be able to extract higher level information [40].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "We compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "We compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 76, "context": "\u201cface_present\u201d features [79] is used to rank all the videos frames.", "startOffset": 24, "endOffset": 28}, {"referenceID": 46, "context": "(3) Story-driven summarization [49].", "startOffset": 31, "endOffset": 35}, {"referenceID": 69, "context": "(4) Real-time summarization [72].", "startOffset": 28, "endOffset": 32}, {"referenceID": 69, "context": "[72] aim at efficient summarization of videos based on semantic content recognition results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Following [66], we conduct a user study to evaluate different summarization methods.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "creator": "LaTeX with hyperref package"}}}