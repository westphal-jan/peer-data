{"id": "1511.05212", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Binary embeddings with structured hashed projections", "abstract": "we essentially consider the hashing mechanism for constructing binary data embeddings, that involves pseudo - random projections followed by nonlinear ( sign function ) mappings. the pseudo - random projection is described by a matrix, where not all entries are independent random variables but instead a fixed'' budget of randomness'' is distributed externally across the matrix. such matrices can be efficiently stored in sub - quadratic algebra or even linear space, provide reduction in randomness usage ( i. e. number of required random values ), and very often lead to computational speed ups. we prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the conserved angular distance between input high - dimensional vectors. to the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices accurately in the nonlinear setting. thus, they significantly generalize slightly previous extensions of the johnson - lindenstrauss lemma theorem and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. consequently, we show that many structured matrices can be used as an efficient information compression mechanism. our findings ourselves also build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. we are interested in how the action of random projection once followed by non - linear transformation may influence learning. we empirically verify our theoretical findings and show the dependence of learning via structured hashed sparse projections on the network performance.", "histories": [["v1", "Mon, 16 Nov 2015 23:01:12 GMT  (479kb,D)", "https://arxiv.org/abs/1511.05212v1", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v2", "Tue, 2 Feb 2016 03:56:09 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v2", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v3", "Tue, 24 May 2016 01:51:33 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v3", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v4", "Tue, 31 May 2016 12:05:06 GMT  (555kb,D)", "http://arxiv.org/abs/1511.05212v4", "arXiv admin note: text overlap witharXiv:1505.03190"], ["v5", "Fri, 1 Jul 2016 16:39:05 GMT  (557kb,D)", "http://arxiv.org/abs/1511.05212v5", "arXiv admin note: text overlap witharXiv:1505.03190"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1505.03190", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "krzysztof choromanski", "mariusz bojarski", "tony jebara", "sanjiv kumar", "yann lecun"], "accepted": true, "id": "1511.05212"}, "pdf": {"name": "1511.05212.pdf", "metadata": {"source": "META", "title": "Binary embeddings with structured hashed projections", "authors": ["Anna Choromanska", "Krzysztof Choromanski", "Mariusz Bojarski", "Tony Jebara", "Sanjiv Kumar", "Yann LeCun"], "emails": ["ACHOROMA@CIMS.NYU.EDU", "KCHORO@GOOGLE.COM", "MBOJARSKI@NVIDIA.COM", "JEBARA@CS.COLUMBIA.EDU", "SANJIVK@GOOGLE.COM", "YANN@CS.NYU.EDU"], "sections": [{"heading": null, "text": "1Equal contribution."}, {"heading": "1. Introduction", "text": "The paradigm of binary embedding for data compression is of the central focus of this paper. The paradigm has been studied in some earlier works (see: (Weiss et al., 2008), (Gong et al., 2012), (Gong et al., 2013a), (Wang et al., 2012), (Gong et al., 2013b), (Plan & Vershynin, 2014), (Yu et al., 2014), (Yi et al., 2015)), and in particular it was observed that by using linear projections and then applying sign function as a nonlinear map one does not loose completely the information about the angular distance between vectors, but instead the information might be approximately reconstructed from the Hamming distance between hashes. In this paper we are interested in using pseudo-random projections via structured matrices in the linear projection phase. The pseudo-random projection is described by a matrix, where not all the entries are independent random variables but instead a fixed \u201cbudget of randomness\u201d is distributed across the matrix. Thus they can be efficiently stored in a sub-quadratic or even linear space and provide reduction in the randomness usage. Moreover, using them often leads to computational speed ups since they provide fast matrix-vector multiplications via Fast Fourier Transform. We prove an extension of the Johnson-Lindenstrauss lemma (Sivakumar, 2002) for general pseudo-random structured projections followed by nonlinear mappings. We show that the angular distance between high-dimensional data vectors is approximately preserved in the hashed space. This result is also new compared to previous extensions (Hinrichs & Vybral, 2011; Vybral, 2011) of the Johnson-Lindenstrauss lemma, that consider special cases of our structured projections (namely: circulant matrices) and do not consider at all the action of the non-linear mapping. We give theoretical explanation of the approach that was so far only heuristically confirmed for some special structured matrices (see: (Yi et al., 2015), (Yu et al., 2014)).\nOur theoretical findings imply that many types of matrices, such as circulant or Toeplitz Gaussian matrices, can be used as a preprocessing step in neural networks. Structured matrices were used before in different contexts also in deep learning, e.g. (Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)). Our theoretical results however extend to more general class of matrices.\nar X\niv :1\n51 1.\n05 21\n2v 5\n[ cs\n.L G\n] 1\nJ ul\n2 01\n6\nOur work has primarily theoretical focus, but we also ask an empirical question: how the action of the random projection followed by non-linear transformation may influence learning? We focus on the deep learning setting, where the architecture contains completely random or pseudo-random structured layers that are not trained. Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006). The high-level intuition justifying the success of these approaches is that not only does the performance of the deep learning system depend on learning, but also on the intrinsic properties of the architecture. These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015). In this paper we consider a simple model of the fully-connected feed-forward neural network where the input layer is hashed by a structured pseudo-random projection followed by a point-wise nonlinear mapping. Thus the input is effectively hashed and learning is conducted in the fully connected subsequent layers that act in the hashed space. We empirically verify how the distortion introduced in the first layer by hashing (where we reduce the dimensionality of the data) affects the performance of the network (in the supervised learning setting). Finally, we show how our structured nonlinear embeddings can be used in the knn setting (Altman, 1992).\nThis article is organized as follows: Section 2 discusses related work, Section 3 explains the hashing mechanism, Section 4 provides theoretical results, Section 5 shows experimental results, and Section 6 concludes. Supplement contains additional proofs and experimental results."}, {"heading": "2. Related work", "text": "The idea of using random projections to facilitate learning with high-dimensional data stems from the early work on random projections (Dasgupta, 1999) showing in particular that learning of high-dimensional mixtures of Gaussians can be simplified when first projecting the data into a randomly chosen subspace of low dimension (this is a consequence of the curse of dimensionality and the fact that high-dimensional data often has low intrinsic dimensionality). This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al., 2006; Choromanska et al., 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al., 2006), and more recently - deep learning (see (Saxe et al., 2011) for convenient review of such approaches). Using linear projections with completely random Gaus-\nsian weights, instead of learned ones, was recently studied from both theoretical and practical point of view in (Giryes et al., 2015), but that work did not consider structured matrices which is a central point of our interest since structured matrices can be stored much more efficiently. Beyond applying methods that use random Gaussian matrix projections (Dasgupta, 1999; 2000; Giryes et al., 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al., 2009). In some sense these methods use structured matrices as well, yet they do not have the same projection efficiency of circulant matrices and projections explored in this article. Our hybrid approach, where a fixed \u201cbudget of randomness\u201d is distributed across the entire matrix in the structured way enables us to take advantage of both: the ability of completely random projection to preserve information and the compactness that comes from the highly-organized internal structure of the linear mapping.\nThis work studies the paradigm of constructing a binary embedding for data compression, where hashes are obtained by applying linear projections to the data followed by the non-linear (sign function) mappings. The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here). Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009). In the context of our work, one of the recent articles (Yi et al., 2015) is especially important since the authors introduce the pipeline of constructing hashes with the use of structured matrices in the linear step, instead of completely random ones. They prove several theoretical results regarding the quality of the produced hash, and extend some previous theoretical results (Jacques et al., 2011; Plan & Vershynin, 2014). Their pipeline is more complicated than ours, i.e. they first apply Hadamard transformation and then a sequence of partial Toeplitz Gaussian matrices. Some general results (unbiasedness of the angular distance estimator) were also known for short hashing pipelines involving circulant matrices ((Yu et al., 2014)). These works do not provide guarantees regarding concentration of the estimator around its mean, which is crucial for all practical applications. Our results for general structured matrices, which include circulant Gaussian matrices and a larger class of Toeplitz Gaussian matrices as special subclasses, provide such concentration guarantees, and thus establish a solid mathematical foundation for using various types of structured matrices in binary embeddings. In contrast to (Yi et al., 2015), we present our theoretical results for simpler hashing models (our hashing mechanism is explained in Section 3 and consists of two very simple steps that we call preprocessing step and the actual hashing step, where\nthe latter consists of pseudo-random projection followed by the nonlinear mapping). In (Yu et al., 2015) theoretical guarantees regarding bounding the variance of the angle estimator in the circulant setting were presented. Strong concentration results regarding several structured matrices were given in (Choromanski & Sindhwani, 2016; Choromanski et al., 2016), following our work.\nIn the context of deep learning, using random network parametrization, where certain layers have random and untrained weights, often accelerates training. Introducing randomness to networks was explored for various architectures, in example feedforward networks (Huang et al., 2006), convolutional networks (Jarrett et al., 2009; Saxe et al., 2011), and recurrent networks (Jaeger & Haas, 2004; White et al., 2004; Boedecker et al., 2009). We also refer the reader to (Ganguli & Sompolinsky, 2012), where the authors describe how neural systems cope with the challenge of processing data in high dimensions and discuss random projections. Hashing in neural networks that we consider in this paper is a new direction of research. Very recently (see: (Chen et al., 2015)) it was empirically showed that hashing in neural nets may achieve drastic reduction in model sizes with no significant loss of the quality, by heavily exploiting the phenomenon of redundancies in neural nets. HashedNets introduced in (Chen et al., 2015) do not give any theoretical guarantees regarding the quality of the proposed hashing. Our work aims to touch both grounds. We experimentally show the plausibility of the approach, but also explain theoretically why the hashing we consider compresses important information about the data that suffices for accurate classification. Dimensionality reduction techniques were used also to approximately preserve certain metrics defined on graph objects ((Shaw & Jebara, 2009)). Structured hashing was applied also in (Szlam et al., 2012), but in a very different context than ours."}, {"heading": "3. Hashing mechanism", "text": "In this section we explain our hashing mechanism for dimensionality reduction that we next analyze."}, {"heading": "3.1. Structured matrices", "text": "We first introduce the aforementioned family of structured matrices, that we call: \u03a8-regular matrices P . This is the key ingredient of the method.\nDefinition 3.1. M is a circulant Gaussian matrix if its first row is a sequence of independent Gaussian random variables taken from the distribution N (0, 1) and next rows are obtained from the previous ones by either only one-left shifts or only one-right shifts.\nDefinition 3.2. M is a Toeplitz Gaussian matrix if each of its descending diagonals from left to right is of the form (g, ..., g) for some g \u223c N (0, 1) and different descending diagonals are independent.\nRemark 3.1. The circulant Gaussian matrix with right\nshifts is a special type of the Toeplitz Gaussian matrix.\nAssume that k is the size of the hash and n is the dimensionality of the data. Definition 3.3. Let t be the size of the pool of independent random Gaussian variables {g1, ..., gt}, where each gi \u223c N (0, 1). Assume that k \u2264 n \u2264 t \u2264 kn. We take \u03a8 to be a natural number, i.e. \u03a8 \u2208 N. P is \u03a8-regular random matrix if it has the following form \u2211 l\u2208S1,1 gl ... \u2211 l\u2208S1,j gl ... \u2211 l\u2208S1,n gl ... ... ... ... ...\u2211 l\u2208Si,1 gl ... \u2211 l\u2208Si,j gl ... \u2211 l\u2208Si,n gl\n... ... ... ... ...\u2211 l\u2208Sk,1 gl ... \u2211 l\u2208Sk,j gl ... \u2211 l\u2208Sk,n gl  where Si,j \u2286 {1, ..., t} for i \u2208 {1, ..., k}, j \u2208 {1, ..., n}, |Si,1| = ... = |Si,n| for i = 1, ..., k, Si,j1 \u2229 Si,j2 = \u2205 for i \u2208 {1, ..., k}, j1, j2 \u2208 {1, ..., n}, j1 6= j2, and furthermore the following holds: for any two different rows R1,R2 of P the number of random variables gl, where l \u2208 {1, ..., t}, such that gl is in the intersection of some column with both R1 andR2 is at most \u03a8. Remark 3.2. Circulant Gaussian matrices and Toeplitz Gaussian matrices are special cases of the 0-regular matrices. Toeplitz Gaussian matrix is 0-regular, where subsets Si,j are singletons.\nIn the experimental section of this paper we consider six different kinds of structured matrices, which are examples of general structured matrices covered by our theoretical analysis. Those are: \u2022 Circulant (see: Definition 3.1), \u2022 BinCirc - a matrix, where the first row is partitioned\ninto consecutive equal-length blocks of elements and each row is obtained by the right shift of the blocks from the first row,\n\u2022 HalfShift - a matrix, where next row is obtained from the previous one by swapping its halves and then performing right shift by one,\n\u2022 VerHorShift - a matrix that is obtained by the following two phase-procedure: first each row is obtained from the previous one by the right shift of a fixed length and then in the obtained matrix each column is shifted up by a fixed number of elements,\n\u2022 BinPerm - a matrix, where the first row is partitioned into consecutive equal-length blocks of elements and each row is obtained as a random permutation of the blocks from the first row,\n\u2022 Toeplitz (see: Definition 3.2). Remark 3.3. Computing hashes for structured matrices: Toeplitz, BinCirc, HalfShift, and VerHorShift can be done faster than in time O(nk) (e.g. for Toeplitz one can use FFT to reduce computations to O(n log k)). Thus our structured approach leads to speed-ups, storage compression (since many structured matrices covered by our theoretical model can be stored in linear space) and reduction\nin randomness usage. The goal of this paper is not to analyze in details fast matrix-vector product algorithms since that requires a separate paper. We however point out that well-known fast matrix-vector product algorithms are some of the key advantages of our structured approach."}, {"heading": "3.2. Hashing methods", "text": "Let \u03c6 be a function satisfying limx\u2192\u221e \u03c6(x) = 1 and limx\u2192\u2212\u221e \u03c6(x) = \u22121. We will consider two hashing methods, both of which consist of what we refer to as a preprocessing step followed by the actual hashing step, where the latter consists of pseudo-random projection followed by nonlinear (sign function) mapping. The first mechanism, that we call extended \u03a8-regular hashing, applies first random diagonal matrix R to the data point x, then the L2normalized Hadamard matrix H, next another random diagonal matrix D, then the \u03a8-regular projection matrix P\u03a8 and finally function \u03c6 (the latter one applied point-wise). The overall scheme is presented below:\nx R\u2212\u2192 xR H\u2212\u2192 xH D\u2212\u2192 xD\ufe38 \ufe37\ufe37 \ufe38\npreprocessing\nP\u03a8\u2212\u2212\u2192 xP\u03a8 \u03c6\u2212\u2192 h(x)\ufe38 \ufe37\ufe37 \ufe38\nhashing\n\u2208 Rk. (1)\nThe diagonal entries of matrices R and D are chosen independently from the binary set {\u22121, 1}, each value being chosen with probability 12 . We also propose a shorter pipeline, that we call short \u03a8-regular hashing, where we avoid applying first random matrix R and the Hadamard matrixH, i.e. the overall pipeline is of the form:\nx D\u2212\u2192 xD\ufe38 \ufe37\ufe37 \ufe38 preprocessing P\u03a8\u2212\u2212\u2192 xP\u03a8 \u03c6\u2212\u2192 h(x)\ufe38 \ufe37\ufe37 \ufe38\nhashing\n\u2208 Rk. (2)\nThe goal is to compute good approximation of the angular distance between given vectors p, r, given their compact hashed versions: h(p), h(r). To achieve this goal we consider the L1-distances in the k-dimensional space of hashes. Let \u03b8p,r denote the angle between vectors p and r. We define the normalized approximate angle between p and r as:\n\u03b8\u0303np,r = 1\n2k \u2016h(p)\u2212 h(r)\u20161 (3)\nIn the next section we show that the normalized approximate angle between vectors p and r leads to a very precise estimation of the actual angle for \u03c6(x) = sign(x) if the chosen parameter \u03a8 is not too large. Furthermore, we show an intriguing connection between theoretical guarantees regarding the quality of the produced hash and the chromatic number of some specific undirected graph encoding the structure of P . For many of the structured matrices under consideration this graph is induced by an algebraic group operation defining the structure of P (for instance, for the circular matrix the group is a single shift and the underlying graph is a collection of pairwise disjoint cycles, thus its chromatic number is at most 3)."}, {"heading": "4. Theoretical results", "text": ""}, {"heading": "4.1. Unbiasedness of the estimator", "text": "We are ready to provide theoretical guarantees regarding the quality of the produced hash. Our guarantees will be given for a sign function, i.e for \u03c6 defined as: \u03c6(x) = 1 for x \u2265 0, \u03c6(x) = \u22121 for x < 0. Using this nonlinearity will be important to preserve approximate information about the angle between vectors, while filtering out the information about their lengths. We first show that \u03b8\u0303np,r is an unbiased estimator of \u03b8p,r\u03c0 , i.e. E(\u03b8\u0303 n p,r) = \u03b8p,r \u03c0 .\nLemma 4.1. LetM be a \u03a8-regular hashing model (either extended or a short one) and \u2016p\u20162 = \u2016r\u20162 = 1. Then \u03b8\u0303np,r is an unbiased estimator of \u03b8p,r\u03c0 , i.e. E(\u03b8\u0303 n p,r) = \u03b8p,r \u03c0 .\nLet us give a short sketch of the proof first. Note that the value of the particular entry in the constructed hash depends only on the sign of the dot product between the corresponding Gaussian vector representing the row of the \u03a8-regular matrix and the given vector. Fix two vectors: p and r with angular distance \u03b8. Note that considered dot products (and thus also their signs) are preserved when instead of taking the Gaussian vector representing the row one takes its projection onto a linear space spanned by p and r. The Hamming distance between hashes of p and r is built up by these entries for which one dot product is negative and the other one is positive. One can note that this happens if the projected vector is inside a 2-dimensional cone covering angle 2\u03b8. The last observation that completes the proof is that the projection of the Gaussian vector is isotropic (since it is also Gaussian), thus the probability that the two dot products will have different signs is \u03b8\u03c0 (also see: (Charikar, 2002)).\nProof. Note first that the ith row, call it gi, of the matrix P is a n-dimensional Gaussian vector with mean 0 and where each element has variance \u03c32i for \u03c3 2 i = |Si,1| = ... = |Si,n| (i = 1, ..., k). Thus, after applying matrix D the new vector giD is still Gaussian and of the same distribution. Let us consider first the short \u03a8-regular hashing model. Fix some vectors p, r (without loss of generality we may assume that they are not collinear). LetHp,r, shortly called by usH , be the 2-dimensional hyperplane spanned by {p, r}. Denote by giD,H the projection of g i D into H and by g i D,H,\u22a5 the line in H perpendicular to giD,H . Let \u03c6 be a sign function. Note that the contribution to the L1-sum \u2016h(p) \u2212 h(r)\u20161 comes from those gi for which giD,H,\u22a5 divides an angle between p and r (see: Figure 1), i.e. from those gi for which giD,H is inside the union Up,r of two 2-dimensional cones bounded by two lines inH perpendicular to p and r respectively. If the angle is not divided (see: Figure 2) then the two corresponding entries in the hash have the same value and thus do not contribute to the overall distance between hashes.\nObserve that, from what we have just said, we can conclude\nthat \u03b8\u0303np,r = X1+...+Xk k , where:\nXi = { 1 if giD,H \u2208 Up,r, 0 otherwise. (4)\nNow it suffices to note that vector giD,H is a 2-dimensional Gaussian vector and thus its direction is uniformly distributed over all directions. Thus each Xi is nonzero with probability exactly \u03b8p,r\u03c0 and the theorem follows. For the extended \u03a8-regular hashing model the analysis is very similar. The only difference is that data is preprocessed by applying HR linear mapping first. Both H and R are orthogonal matrices though, thus their product is also an orthogonal matrix. Since orthogonal matrices do not change angular distance, the former analysis can be applied again and yields the proof.\nWe next focus on the concentration of the random variable \u03b8\u0303np,r around its mean \u03b8p,r \u03c0 . We prove strong exponential concentration results for the extended \u03a8-regular hashing method. Interestingly, the application of the Hadamard mechanism is not necessary and it is possible to get concentration results, yet weaker than in the former case, also for short \u03a8-regular hashing.\n4.2. The P-chromatic number\nThe highly well organized structure of the projection matrix P gives rise to the underlying undirected graph that encodes dependencies between different entries of P . More formally, let us fix two rows ofP of indices 1 \u2264 k1 < k2 \u2264 k respectively. We define a graph GP(k1, k2) as follows: \u2022 V (GP(k1, k2)) = {{j1, j2} : \u2203l \u2208 {1, ..., t}s.t.gl \u2208 Sk1,j1 \u2229 Sk2,j2 , j1 6= j2},\n\u2022 there exists an edge between vertices {j1, j2} and {j3, j4} iff {j1, j2} \u2229 {j3, j4} 6= \u2205.\nThe chromatic number \u03c7(G) of the graph G is the minimal number of colors that can be used to color the vertices of the graph in such a way that no two adjacent vertices have the same color. Definition 4.1. Let P be a \u03a8-regular matrix. We define the P-chromatic number \u03c7(P) as:\nThe graph associated with each structured matrix that we have just described enables us to encode dependencies between entries of the structured matrix in the compact form and gives us quantitative ways to efficiently measure these dependencies by analyzing several core parameters of this graph such as its chromatic number. More dependencies that usually lead to more structured form mean more edges in the associated graph and often lead to higher chromatic number. On the other hand, fewer dependencies produce graphs with much lower chromatic number (see Figure 3, where the graph associated with the circulant matrix is a collection of vertex disjoint cycles and has chromatic number 3 if it contains an odd length cycle and 2 otherwise).\n4.3. Concentration inequalities for structured hashing with sign function\nWe present now our main theoretical results. The proofs are deferred to the Supplementary material. We focus on the concentration results regarding produced hashes that are crucial for practical applications of the proposed scheme.\nWe first start with the short description of the methods used and then rigorously formulate all the results. If all the rows\nof the projection matrix are independent then standard concentration inequalities can be used. This is however not the case in our setting since the matrix is structured. We still want to say that any two rows are \u201cclose\u201d to independent Gaussian vectors and that will give us bounds regarding the variance of the distance between the hashes (in general, we observe that any system of k rows is \u201cclose\u201d to the system of k independent Gaussian vectors and get bounds involving kth moments). We proceed as follows:\n\u2022 We take two rows and project them onto the linear space spanned by given vectors: p and r.\n\u2022 We consider the four coordinates obtained in this way (two for each vector). They are obviously Gaussian, but what is crucial, they are \u201calmost independent\u201d.\n\u2022 The latter observation is implied by the fact that these are the coordinates of the projection of a fixed Gaussian vector onto \u201calmost orthogonal\u2019 directions\u2019.\n\u2022 We use the property of the Gaussian vector that its projections onto orthogonal directions are independent.\n\u2022 To prove that directions considered in our setting are close to orthogonal with high probability, we compute their dot product. This is the place where the structure of the matrix, the chromatic number of the underlying graph and the fact that in our hashing scheme we use random diagonal matrices come into action. We decompose each dot product into roughly speaking \u03c7 components (\u03c7 is the chromatic number), such that each component is a sum of independent random variables with mean 0. Now we can use standard concentration inequalities to get tight concentration results.\n\u2022 The Hadamard matrix used in the extended model preprocesses input vectors to distribute their mass uniformly over all the coordinates, while not changing L2 distances (it is a unitary matrix). Balanced vectors lead to much stronger concentration results.\nNow we are ready to rigorously state our results. By poly(x) we denote a function xr for some r > 0. The following theorems guarantee strong concentration of \u03b8\u0303np,r around its mean and therefore justify theoretically the effectiveness of the structured hashing method.\nLet us consider first the extended \u03a8-regular hashing model.\nTheorem 4.1. Consider extended \u03a8-regular hashing modelM with t independent Gaussian random variables: g1, ..., gt, each of distribution N (0, 1). Let N be the size of the dataset D. Denote by k the size of the hash and by n the dimensionality of the data. Let f(n) be an arbitrary positive function. Let \u03b8p,r be the angular distance between vectors p, r \u2208 D. Then for a = on(1), > 0, t \u2265 n and n large enough:\nP ( \u2200p,r\u2208D \u2223\u2223\u2223\u2223\u03b8\u0303np,r \u2212 \u03b8p,r\u03c0 \u2223\u2223\u2223\u2223 \u2264 ) \u2265[\n1\u2212 4 ( N\n2\n) e\u2212 f2(n) 2 \u2212 4\u03c7(P) ( k\n2\n) e \u2212 2a2t f4(t) ] (1\u2212 \u039b),\nwhere \u039b = 1\u03c0 \u2211k j=b k2 c 1\u221a j (kej ) j\u00b5j(1 \u2212 \u00b5)k\u2212j + 2e\u2212 2k 2 and \u00b5 = 8( \u221a a\u03c7(P)+\u03a8 f 2(n)\u221a n )\n\u03b8p,r .\nNote how the upper bound on the probability of failure P depends on the P-chromatic number. The theorem above guarantees strong concentration of \u03b8\u0303np,r around its mean and therefore justifies theoretically the effectiveness of the structured hashing method. It becomes more clear below.\nAs a corollary, we obtain the following result:\nCorollary 4.1. Consider extended \u03a8-regular hashing modelM. Assume that the projection matrix P is Toeplitz Gaussian. Let N,n, k be as above and denote by \u03b8p,r be the angular distance between vectors p, r \u2208 D. Then the following is true for n large enough:\nP ( \u2200p,r\u2208D \u2223\u2223\u2223\u2223\u03b8\u0303np,r \u2212 \u03b8p,r\u03c0 \u2223\u2223\u2223\u2223 \u2264 k\u2212 13) \u2265[\n1\u2212O ( N2\nepoly(n) + k2e\u2212n\n3 10 )]( 1\u2212 3e\u2212 k 1 3 2 ) .\nCorollary 4.1 follows from Theorem 4.1 by taking: a = n\u2212 1 3 , = k\u2212 1 3 , f(n) = np for small enough constant p > 0, noticing that every Toeplitz Gaussian matrix is 0- regular and the corresponding P-chromatic number \u03c7(P) is at most 3.\nTerm O ( N2\nepoly(n)\n) is related to the balancedness property.\nTo clarify, the goal of multiplying by HR in the preprocessing step is to make each input vector balanced, or in other words to spread out the mass of the vector across all the dimensions in approximately uniform way. This property is required to obtain theoretical results (also note it was unnecessary in the unstructured setting) and does not depend on the number of projected dimensions.\nLet us consider now the short \u03a8-regular hashing model. The theorem presented below is an application of the Chebyshev\u2019s inequality preceded by the careful analysis of the variance of \u03b8\u0303np,r.\nTheorem 4.2. Consider short \u03a8-regular hashing model M, where P is a Toeplitz Gaussian matrix. Denote by k\nthe size of the hash. Let \u03b8p,r be the angular distance between vectors p, r \u2208 D, where D is the dataset. Then the following is true\n\u2200p,r\u2208DV ar(\u03b8\u0303np,r) \u2264 1\nk \u03b8p,r(\u03c0 \u2212 \u03b8p,r) \u03c02 + ( log(k) k2 ) 1 3 , (5)\nand thus for any c > 0 and p, r \u2208 D:\nP \u2223\u2223\u2223\u2223\u03b8\u0303np,r \u2212 \u03b8p,r\u03c0 \u2223\u2223\u2223\u2223 \u2265 c (\u221a log(k) k ) 1 3  = O( 1 c2 ) .\nFigure 4 shows the dependence of the upper bound on the variance of the normalized approximate angle \u03b8\u0303np,r on resp. the true angular distance \u03b8p,r and the size of the hash k when resp. k and \u03b8p,r are fixed.\nRate k\u2212 1 3 that appears in the theoretical results we obtained and the non-linear with k variance decay of Figure 4 (right) is a consequence of the structured setting, where the quality of the nonlinear embedding is affected by the existence of dependencies between entries of the structured matrix."}, {"heading": "5. Numerical experiments", "text": "In this section we demonstrate that all considered structured matrices achieve reasonable performance in comparison to fully random matrices. Specifically we show: i) the dependence of the performance on the size of the hash and the reduction factor nk for different structured matrices and ii) the performance of different structured matrices when used with neural networks and 1-NN classifier. Experiments confirm our novel theoretical results.\nWe performed experiments on MNIST dataset downloaded from http://yann.lecun.com/exdb/mnist/. The data was preprocessed2 according to the short hashing scheme (the extended hashing scheme gave results of no\n2Preprocessing is discussed in Section 3.\nsignificant statistical difference) before being given to the input of the network. We first considered a simple model of the fully-connected feed-forward neural network with two hidden layers, where the first hidden layer had k units that use sign non-linearity (we explored k = {16, 32, 64, 128, 256, 512, 1024}), and the second hidden layer had 100 units that use ReLU non-linearity. The size of the second hidden layer was chosen as follows. We first investigated the dependence of the test error on this size in case when n = k and the inputs instead of being randomly projected are multiplied by identity (it is equivalent to eliminating first hidden layer). We then chose as a size the threshold below which test performance was rapidly deteriorating.\nThe first hidden layer contains random untrained weights, and we only train the parameters of the second layer and the output layer. The network we consider is shown in Figure 5. Each experiment was initialized from a random set of parameters sampled uniformly within the unit cube, and was repeated 1000 times. All networks were trained for 30 epochs using SGD (Bottou, 1998). The experiments with constant learning rate are reported (we also explored learning rate decay, but obtained similar results), where the learning rate was chosen from the set {0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1} to minimize the test error. The weights of the first hidden layer correspond to the entries in the \u201cpreprocessed\u201d structured matrix. We explored seven kinds of random matrices (first six are structured): Circulant, Toeplitz, Half-\nShift, VerHorShift, BinPerm, BinCirc, and Random (entries are independent and drawn from Gaussian distribution N (0, 1)). All codes were implemented in Torch7.\nFigure 6a shows how the mean test error is affected by the size of the hash, and Figure 6b shows how the mean test error changes with the size of the reduction, where the size of the reduction is defined as the ratio n/k. In Table 1 we report both the mean and the standard deviation (std) of the test error across our experiments. Training results are reported in the Supplementary material. Baseline refers to the network with one hidden layer containing 100 hidden\nunits, where all parameters are trained.\nExperimental results show how the performance is affected by using structured hashed projections to reduce data dimensionality. Figure 6b and Table 1 show close to linear dependence between the error and the size of the reduction. Simultaneously, this approach leads to computational savings and the reduction of memory storage. i.e. the reduction of the number of input weights for the hidden layer (in example for Circulant matrix this reduction is of the order O(n/k)4). Memory complexity, i.e. memory required to store the matrix, and the number of required random values for different structured matrices and Random matrix are summarized in Table 2.\nExperiments show that using fully random matrix gives the best performance as predicted in theory. BinPerm matrix exhibits comparable performance to the Random matrix, which might be explained by the fact that applying permutation itself adds an additional source of randomness. The next best performer is HalfShift, whose generation is less random than the one of BinPerm or Random. Thus its performance, as expected, is worse than for these two other matrices. However, as opposed to BinPerm and Random matrices, HalfShift matrix can be stored in linear space. The results also show that in general all structured matrices perform relatively well for medium-size reductions. Finally, all structured matrices except for BinPerm lead to the biggest memory savings and require the smallest \u201cbudget of randomness\u201d. Moreover, they often lead to computational efficiency, e.g. Toeplitz matrix-vector multiplications can be efficiently implemented via Fast Fourier Transform (Yu et al., 2014). But, as mentioned before, faster than naive matrix-vector product computations can be performed also for BinPerm, HalfShift, and VerHorShift.\nFinally, we also report how the performance of 1-NN algorithm is affected by using structured hashed projections for the dimensionality reduction. We obtained similar plots as\n3Original plot is in the Supplement. 4The memory required for storing Circulant matrix is negligi-\nble compared to the number of weights.\nfor the case of neural networks. They are captured in Figure 7. The table showing the mean and the standard deviation of the test error for experiments with 1-NN is enclosed in the Supplementary material."}, {"heading": "6. Conclusions", "text": "This paper shows that structured hashed projections well preserve the angular distance between input data instances. Our theoretical results consider mapping the data to lowerdimensional space using various structured matrices, where the structured linear projections are followed by the sign nonlinearity. This non-linear operation was not considered for such a wide range of structured matrices in previous related theoretical works. The theoretical setting naturally applies to the multilayer network framework, where the basic components of the architecture perform matrix-vector multiplication followed by the nonlinear mapping. We empirically verify our theoretical findings and show how using structured hashed projections for dimensionality reduction affects the performance of neural network and nearest neighbor classifier."}, {"heading": "7. Proof of Theorem 4.1", "text": "We start with the following technical lemma: Lemma 7.1. Let {Z1, ..., Zk} be the set of k independent random variables defined on \u2126 such that each Zi has the same distribution and Zi \u2208 {0, 1}. Let {F1, ...,Fk} be the set of events, where each Fi is in the \u03c3-field defined by Zi (in particular Fi does not depend on the \u03c3field \u03c3(Z1, ..., Zi\u22121, Zi+1, ...Zk)). Assume that there exists \u00b5 < 12 such that: P(Fi) \u2264 \u00b5 for i = 1, ..., k. Let {U1, ..., Uk} be the set of k random variables such that Ui \u2208 {0, 1} and Ui|Fi = Zi|Fi for i = 1, ..., k, where X|F stands for the random variable X truncated to the event F . Assume furthermore that E(Ui) = E(Zi) for i = 1, ..., k. Denote U = U1+...+Ukk . Then the following is true.\nP(|U\u2212EU |> )\u2264 1 \u03c0 k\u2211 r= k2 1\u221a r ( ke r )r\u00b5r(1\u2212\u00b5)k\u2212r+2e\u2212 2k 2 .\n(6)\nProof. Let us consider the event Fbad = F1\u222a ...\u222aFk. Note that Fbad may be represented by the union of the so-called r-blocks, i.e.\nFbad = \u22c3\nQ\u2286{1,...,k} ( \u22c2 q\u2208Q Fq \u22c2 q\u2208{1,...,k}\\Q Fcq ), (7)\nwhere Fc stands for the complement of event F . Let us fix now some Q \u2286 {1, ..., k}. Denote\nFQ = \u22c2 q\u2208Q Fq \u22c2 q\u2208{1,...,k}\\Q Fcq . (8)\nnote that P(FQ) \u2264 \u00b5r(1\u2212 \u00b5)k\u2212r. It follows directly from the Bernoulli scheme.\nDenote Z = Z1+...+Zkk . From what we have just said and from the definition of {F1, ...,Fk} we conclude that for any given c the following holds:\nP(|U \u2212 Z| > c) \u2264 k\u2211\nr=ck\n( k\nr\n) \u00b5r(1\u2212 \u00b5)k\u2212r. (9)\nNote also that from the assumptions of the lemma we trivially get: E(U) = E(Z).\nLet us consider now the expression P(|U \u2212 E(U)|) > . We get: P(|U \u2212 E(U)| > ) = P(|U \u2212 E(Z)| > ) = P(|U\u2212Z+Z\u2212E(Z)| > ) \u2264 P(|U\u2212Z|+ |Z\u2212E(Z)| > ) \u2264 P(|U \u2212 Z| > 2 ) + P(|Z \u2212 E(Z)| > 2 ).\nFrom 9 we get:\nP(|U \u2212 Z| > 2\n) \u2264 k\u2211\nr= 2\n( k\nr\n) \u00b5r(1\u2212 \u00b5)k\u2212r. (10)\nLet us consider now the expression:\n\u03be = k\u2211 r= k2 ( k r ) \u00b5r(1\u2212 \u00b5)k\u2212r. (11)\nWe have:\n\u03be \u2264 k\u2211\nr= k2\n(k \u2212 r + 1)...(k) r! \u00b5r(1\u2212 \u00b5)k\u2212r\n\u2264 k\u2211\nr= k2\nkr r! \u00b5r(1\u2212 \u00b5)k\u2212r (12)\nFrom the Stirling\u2019s formula we get: r! = 2\u03c0r r+ 1 2\ner (1 + or(1)). Thus we obtain:\n\u03be \u2264 (1 + or(1)) k\u2211\nr= k2\nkrer\n2\u03c0rr+ 1 2\n\u00b5r(1\u2212 \u00b5)k\u2212r\n\u2264 1 \u03c0 k\u2211 r= k2 1\u221a r ( ke r )r\u00b5r(1\u2212 \u00b5)k\u2212r (13)\nfor r large enough.\nNow we will use the following version of standard Azuma\u2019s inequality:\nLemma 7.2. Let W1, ...,Wk be k independent random variables such that E(W1) = ...E(Wk) = 0. Assume that \u2212\u03b1i \u2264 Wi+1 \u2212Wi \u2264 \u03b2i for i = 2, ..., k \u2212 1. Then the following is true:\nP(| k\u2211 i=1 Wi| > a) \u2264 2e \u2212 2a2\u2211k i=1 (\u03b1i+\u03b2i) 2\nNow, using Lemma 7.2 for Wi = Xi \u2212 E(Xi) and \u03b1i = E(Xi), \u03b2i = 1\u2212 E(Xi) we obtain:\nP(|X \u2212 EX| > a 2 ) \u2264 2e\u2212 a 2k 2 . (14)\nCombining 13 and 14, we obtain the statement of the lemma.\nOur next lemma explains the role the Hadamard matrix plays in the entire extended \u03a8-regular hashing mechanism. Lemma 7.3. Let n denote data dimensionality and let f(n) be an arbitrary positive function. Let D be the set of all L2-normalized data points, where no two data points are identical. Assume that |D| = N . Consider the ( N 2 ) hyperplanes Hp,r spanned by pairs of different vectors {p, r} from D. Then after applying linear transformation HR each hyperplane Hp,r is transformed into another hyperplane HHRp,r . Furthermore, the probability PHRthat for every HHRp,r there exist two orthonormal vectors x = (x1, ..., xn), y = (y1, ..., yn) in HHRp,r such that: |xi|, |yi| \u2264 f(n)\u221an satisfies:\nPHR \u2265 1\u2212 4 ( N\n2\n) e\u2212 f2(n) 2 .\nProof. We have already noted in the proof of Lemma 4.1 that HR is an orthogonal matrix. Thus, as an isometry, it clearly transforms each 2-dimensional hyperplane into another 2-dimensional hyperplane. For every pair {p, r}, let us consider an arbitrary fixed orthonormal pair {u, v} spanning Hp,r. Denote u = (u1, ..., un). Let us denote by uHR vector obtained from u after applying transformation HR. Note that the jth coordinate of uHR is of the form:\nuHRj = u1T1 + ...+ unTn, (15)\nwhere T1, ..., Tn are independent random variables satisfying:\nTi =\n{ 1\u221a n\nw.p 12 , \u2212 1\u221a\nn otherwise. (16)\nThe latter comes straightforwardly from the form of the L2-normalized Hadamard matrix (i.e a Hadamard matrix, where each row and column is L2-normalized).\nBut then, from Lemma 7.2, and the fact that \u2016u\u20162 = 1, we get for any a > 0:\nP(|u1T1 + ...+ unTn| \u2265 a) \u2264 2e \u2212 2a2\u2211n i=1 (2ui) 2 \u2264 2e\u2212 a 2\n2 . (17)\nSimilar analysis is correct for vHR. Note that vHR is orthogonal to uHR since v and u are orthogonal. Furthermore, both vHR and uHR are L2-normalized. Thus {uHR, vHR} is an orthonormal pair. To complete the proof, it suffices to take a = f(n) and apply the union bound over all vectors uHR, vHR for all( N 2 ) hyperplanes.\nFrom the lemma above we see that applying Hadamard matrix enables us to assume with high probability that for every hyperplane Hp,r there exists an orthonormal basis consisting of vectors with elements of absolute values at most f(n)\u221a n\n. We call this event Ef . Note that whether Ef holds or not is determined only byH,R and the initial dataset D. Let us proceed with the proof of Theorem 4.1. Let us assume that event Ef holds. Without loss of generality we may assume that we have the short \u03a8-regular hashing mechanism with an extra property that every Hp,r has an orthonormal basis consisting of vectors with elements of absolute value at most f(n)\u221a\nn . Fix two vectors p, r from the\ndataset D. Denote by {x, y} the orthonormal basis of Hp,r with the above property. Let us fix the ith row of P and denote it as (pi,1, ..., pi,n). After being multiplied by the diagonal matrix D we obtain another vector:\nw = (Pi,1d1, ...,Pi,ndn), (18)\nwhere:\nDi,j =  d1 0 \u00b7 \u00b7 \u00b7 0 0 d2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 dn  . (19) We have already noted that in the proof of Lemma 4.1 that it is the projection of w into Hp,r that determines whether the value of the associated random variableXi is 0 or 1. To be more specific, we showed that Xi = 1 iff the projection is in the region Up,r. Let us write down the coordinates of the projection of w into Hp,r in the {x, y}-coordinate system. The coordinates are the dot-products of w with x and y respectively thus in the {x, y}-coordinate system we can write w as:\nw{x,y} = (Pi,1d1x1, ...,Pi,ndnxn,Pi,1d1y1, ...,Pi,ndnyn). (20)\nNote that both coordinates are Gaussian random variables and they are independent since they were constructed by projecting a Gaussian vector into two orthogonal vectors. Now note that from our assumption about the structure ofP we can conclude that both coordinates may be represented as sums of weighted Gaussian random variables gi for i = 1, ..., t, i.e.:\nw{x,y} = (g1si,1 + ...+ gtsi,t, g1vi,1 + ...+ gtvi,t), (21)\nwhere each si,j , vi,j is of the form dzxz or dzyz for some z that depends only on i, j. Note also that\ns2i,1 + ...+ s 2 i,t = v 2 i,1 + ...+ v 2 i,t. (22)\nThe latter inequality comes from the fact that, by 20, both coordinates of w{x,y} have the same distribution.\nLet us denote si = (si,1, ..., si,t), vi = (vi,1, ..., vi,t) for i = 1, ..., k. We need the following lemma stating that with high probability vectors s1, ..., sk, v1, ..., vk are close to be pairwise orthogonal. Lemma 7.4. Let us assume that Ef holds. Let f(n) be an arbitrary positive function. Then for every a > 0 with\nprobability at least Psucc \u2265 1\u2212 4 ( k 2 ) e \u2212 2a2n f4(n) , taken under coin tosses used to construct D, the following is true for every 1 \u2264 i1 6= i2 \u2264 k:\n| n\u2211 u=1 si1,uvi1,u| \u2264 a\u03c7(P) + \u03a8 f2(n) n , | n\u2211 u=1 si1,usi2,u| \u2264 a\u03c7(P) + \u03a8 f2(n) n ,\n| n\u2211 u=1 vi1,uvi2,u| \u2264 a\u03c7(P) + \u03a8 f2(n) n ,\n| n\u2211 u=1 si1,uvi2,u| \u2264 a\u03c7(P) + \u03a8 f2(n) n .\nProof. Note that the we get the first inequality for free from the fact that x is orthogonal to y (in other words,\u2211n u=1 si1,uvi1,u can be represented as C \u2211n u=1 xiyi and the latter expression is clearly 0). Let us consider now one of the three remaining expressions. Note that they can be rewritten as:\nE = n\u2211 i=1 d\u03c1(i)d\u03bb(i)x\u03b6(i)x\u03b3(i) (23)\nor\nE = n\u2211 i=1 d\u03c1(i)d\u03bb(i)y\u03b6(i)y\u03b3(i) (24)\nor\nE = n\u2211 i=1 d\u03c1(i)d\u03bb(i)x\u03b6(i)y\u03b3(i) (25)\nfor some \u03c1, \u03bb, \u03b6, \u03b3. Note also that from the \u03a8-regularity condition we immediately obtain that \u03c1(i) = \u03bb(i) for at most \u03a8 elements of each sum. Get rid of these elements from each sum and consider the remaining ones. From the definition of the P-chromatic number, those remaining ones can be partitioned into at most \u03c7(P) parts, each consisting of elements that are independent random variables (since in the corresponding graph there are no edges between them). Thus, for the sum corresponding to each part one can apply Lemma 7.2. Thus one can conclude that the sum differs from its expectation (which clearly is 0 since E(didj) = 0 for i 6= j) by a with probability at most:\nPa \u2264 2e \u2212 2a2\u2211n i=1 x\u03b6(i)x\u03b3(i) , (26)\nor\nPa \u2264 2e \u2212 2a2\u2211n i=1 y\u03b6(i)y\u03b3(i) , (27)\nor\nPa \u2264 2e \u2212 2a2\u2211n i=1 x\u03b6(i)y\u03b3(i) . (28)\nNow it is time to use the fact that event Ef holds. Then we know that: |xi|, |yi| \u2264 f(n)\u221an for i = 1, ..., n. Substituting this upper bound for |xi|, |yi| in the derived expressions on the probabilities coming from Lemma 7.2, and then taking the union bound, we complete the proof.\nWe can finish the proof of Theorem 4.1. From Lemma 7.4 we see that s1, ..., sk, v1, ..., vk are close to pairwise orthogonal with high probability. Let us fix some positive function f(n) > 0 and some a > 0. Denote\n\u2206 = a\u03c7(P) + \u03a8f 2(n)\nn . (29)\nNote that, by Lemma 7.4 we see that applying GramSchmidt process we can obtain a system of pairwise orthogonal vectors s\u03031, ..., s\u0303k, v\u03031, ..., v\u0303k such that\n\u2016v\u0303i \u2212 vi\u20162 \u2264 \u03c3(k)\u2206. (30)\nand \u2016s\u0303i \u2212 si\u20162 \u2264 \u03c3(k)\u2206, (31)\nwhere \u03c3(k) > 0 is some function of k (it does not depend on n and t). Note that for n, t large enough we have: \u03c3(k)\u2206 \u2264 \u221a a\u03c7(P) + \u03a8 f\n2(n)\u221a n .\nLet us consider again wx,y . Replacing si by s\u0303i and vi by v\u0303i in the formula on wx,y , we obtain another Gaussian vector: w\u0303x,y for each row i of the matrix P . Note however that vectors w\u0303x,y have one crucial advantage over vectors wx,y , namely they are independent. That comes from the fact that s\u03031, ..., s\u0303k,v\u03031, ..., v\u0303k are pairwise orthogonal. Note also that from 36 and 37 we obtain that the angular distance between wx,y and w\u0303x,y is at most \u03c3(k)\u2206.\nLet Zi for i = 1, ...k be an indicator random variable that is zero if w\u0303x,y is inside the region Up,r and zero otherwise. Let Ui for i = 1, ...k be an indicator random variable that is zero if wx,y is inside the region Up,r and zero otherwise. Note that \u03b8\u0303np,r = U1+...+Uk k . Furthermore, random variables Z1, ..., Zk, U1, ..., Uk satisfy the assumptions of Lemma 7.1 with \u00b5 \u2264 8\u03c4\u03b8p,r , where \u03c4 = \u03c3(k)\u2206. Indeed, random variables Zi are independent since vectors w\u0303x,y are independent. From what we have said so far we know that each of them takes value one with probability exactly \u03b8p,r\u03c0 . Furthermore Zi 6= Ui only ifwx,y is inside Up,r and w\u0303x,y is outside Up,r or vice versa. The latter event implies (thus it is included in the event) that wx,y is near the border of the region Up,r, namely within an angular distance \u03b8p,r from one of the four semi-lines defining Up,r. Thus in particular, an event Zi 6= Ui is contained in the event of probability at most 2 \u00b7 4 \u00b7 \u03b8p,r that depends only on one wx,y .\nBut then we can apply Lemma 7.1. All we need is to assume that the premises of Lemma 7.4 are satisfied. But this\nis the case with probability specified in Lemma 7.3 and this probability is taken under random coin tosses used to product H and R, thus independently from the random coin tosses used to produce D. Putting it all together we obtain the statement of Theorem 4.1."}, {"heading": "8. Proof of Theorem 4.2", "text": "We will borrow some notation from the proof of Theorem 4.1. Note however that in this setting no preprocessing with the use of matricesH andR is applied. Lemma 8.1. Define U1, ..., Uk as in the proof of Theorem 4.1. Assume that the following is true:\n| n\u2211 u=1 si1,uvi1,u| \u2264 \u2206, | n\u2211 u=1 si1,usi2,u| \u2264 \u2206,\n| n\u2211 u=1 vi1,uvi2,u| \u2264 \u2206,\n| n\u2211 u=1 si1,uvi2,u| \u2264 \u2206.\nfor some 0 < \u2206 < 1. The the following is true for every fixed 1 \u2264 i < j \u2264 k:\n|P(UiUj = 1)\u2212 P(Ui = 1)P(Uj = 1)| = O(\u2206).\nThe lemma follows from the exactly the same analysis that was done in the last section of the proof of Theorem 4.1 thus we leave it to the reader as an exercise.\nNote that we have:\nV ar(\u03b8\u0303np,r) =V ar( U1 + ...+ Uk\nk )\n= 1\nk2 ( k\u2211 i=1 V ar(Ui) + \u2211 i 6=j Cov(Ui, Uj)). (32)\nSince Ui is an indicator random variable that takes value one with probability \u03b8p,r\u03c0 , we get:\nV ar(Ui) = E(U 2 i )\u2212 E(Ui)2 = \u03b8p,r \u03c0 (1\u2212 \u03b8p,r \u03c0 ). (33)\nThus we have:\nV ar(\u03b8\u0303np,r) = 1\nk \u03b8p,r(\u03c0 \u2212 \u03b8p,r) \u03c02 + 1 k2 \u2211 i 6=j Cov(Ui, Uj).\n(34)\nNote however that Cov(Ui, Uj) is exactly: P(UiUj = 1)\u2212 P(Ui = 1)P(Uj = 1).\nTherefore, using Lemma 8.1, we obtain:\nV ar(\u03b8\u0303np,r) = 1\nk \u03b8p,r(\u03c0 \u2212 \u03b8p,r) \u03c02 +O(\u2206). (35)\nIt suffices to estimate parameter \u2206. We proceed as in the previous proof. We only need to be a little bit more cautious since the condition: |xi|, |yi| \u2264 f(n)\u221an cannot be assumed right now. We select two rows: i1, i2 of P . Note that again we see that applying Gram-Schmidt process, we can obtain a system of pairwise orthogonal vectors s\u0303i1 , s\u0303ii , v\u0303ii , v\u0303i2 such that\n\u2016v\u0303i1 \u2212 vi2\u20162 = O(\u2206), (36)\nand \u2016s\u0303i1 \u2212 si2\u20162 = O(\u2206). (37)\nThe fact that right now the above upper bounds are not multiplied by k, as it was the case in the previous proof, plays key role in obtaining nontrivial concentration results even when no Hadamard mechanism is applied.\nWe consider the related sums: E1 = \u2211n i=1 d\u03c1(i)d\u03bb(i)x\u03b6(i)x\u03b3(i), E2 =\u2211n\ni=1 d\u03c1(i)d\u03bb(i)y\u03b6(i)y\u03b3(i), E3 = \u2211n i=1 d\u03c1(i)d\u03bb(i)x\u03b6(i)y\u03b3(i) as before. We can again partition each sum into at most \u03c7(P) sub-chunks, where this time \u03c7(P) \u2264 3 (since P is Toeplitz Gaussian). The problem is that applying Lemma 7.2, we get bounds that depend on the expressions of the form\n\u03b1x,i = n\u2211 j=1 x2jx 2 j+i, (38)\nand\n\u03b1y,i = n\u2211 j=1 y2j y 2 j+i, (39)\nwhere indices are added modulo n and this time we cannot assume that all |xi|, |yi| are small. Fortunately we have:\nn\u2211 i=1 \u03b1x,i = 1, (40)\nand n\u2211 i=1 \u03b1y,i = 1 (41)\nLet us fix some positive function f(k). We can conclude that the number of variables \u03b1x,i such that \u03b1x,i \u2265 f(k)(k2) is at most ( k 2)\nf(k) . Note that each such \u03b1x,i and each such \u03b1y,i corresponds to a pair {i1,2 } of rows of the matrix P and consequently to the unique element Cov(Ui1 , Ui2) of the entire covariance sum (scaled by 1k2 ). Since trivially\nwe have |Cov(Ui1 , Ui2)| = O(1), we conclude that the contribution of these elements to the entire covariance sum is of order 1f(k) . Let us now consider these \u03b1x,i and \u03b1y,i that are at most f(k)\n(k2) . These sums are small (if we take\nf(k) = o(k2)) and thus it makes sense to apply Lemma 7.2 to them. That gives us upper bound a = \u2126(\u2206) with probability:\nP\u2217 \u2265 1\u2212 e\u2212\u2126(a 2 k2 f(k) ). (42)\nTaking f(k) = ( k 2 log(k) ) 1 3 and a = O(\u2206) = 1f(k) , we get: P\u2217 \u2265 1\u2212O( 1k ) and furthermore:\nV ar(\u03b8\u0303np,r) = 1\nk \u03b8p,r(\u03c0 \u2212 \u03b8p,r) \u03c02 + ( log(k) k2 ) 1 3 . (43)\nThus, from the Chebyshev\u2019s inequality, we get the following for every c > 0 and fixed points p, r:\nP(|\u03b8\u0303np,r \u2212 \u03b8p,r \u03c0 | \u2265 c(\n\u221a log(k)\nk )\n1 3 ) = O(\n1 c2 ). (44)\nThat completes the proof of Theorem 4.2."}, {"heading": "9. Additional figures", "text": "Figure 8a and Figure 8b show how the mean train error is affected by the size of the hash, and Figure 8c shows how the mean train error changes with the size of the reduction for the neural network experiment. In Table 3 we report both the mean and the standard deviation of the train error across our neural network experiments. Baseline refers to the network with one hidden layer containing 100 hidden units, where all parameters are trained.\nFigure 9a shows the original version of Figure 6a (before zoom). Figure 9b shows the original version of Figure 7a (before zoom). Finally, Table 4 shows the mean and the standard deviation of the test error versus the size of the hash (k)/size of the reduction (n/k) for 1-NN."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Achlioptas,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas", "year": 2003}, {"title": "An Introduction to Kernel and NearestNeighbor Nonparametric Regression", "author": ["N.S. Altman"], "venue": "The American Statistician,", "citeRegEx": "Altman,? \\Q1992\\E", "shortCiteRegEx": "Altman", "year": 1992}, {"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "In KDD,", "citeRegEx": "Bingham and Mannila,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila", "year": 2001}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["A. Blum"], "venue": "In SLSFS,", "citeRegEx": "Blum,? \\Q2006\\E", "shortCiteRegEx": "Blum", "year": 2006}, {"title": "Initialization and self-organized optimization of recurrent neural network connectivity", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "HFSP journal,", "citeRegEx": "Boedecker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boedecker et al\\.", "year": 2009}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou,? \\Q1998\\E", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Charikar", "Moses"], "venue": "In Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21,", "citeRegEx": "Charikar and Moses.,? \\Q2002\\E", "shortCiteRegEx": "Charikar and Moses.", "year": 2002}, {"title": "Compressing neural networks with the hashing", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Cheng", "Yu", "Felix X", "Feris", "Rog\u00e9rio Schmidt", "Kumar", "Sanjiv", "Choudhary", "Alok N", "Chang", "Shih-Fu"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Differentially-private learning of low dimensional manifolds", "author": ["A. Choromanska", "K. Choromanski", "G. Jagannathan", "C. Monteleoni"], "venue": "In ALT,", "citeRegEx": "Choromanska et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2013}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "Arous", "G. Ben", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Recycling randomness with structure for sublinear time kernel", "author": ["Choromanski", "Krzysztof", "Sindhwani", "Vikas"], "venue": "expansions. ICML2016,", "citeRegEx": "Choromanski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choromanski et al\\.", "year": 2016}, {"title": "Triplespin - a generic compact paradigm for fast machine learning computations", "author": ["Choromanski", "Krzysztof", "Fagan", "Francois", "Gouy-Pailler", "C\u00e9dric", "Morvan", "Anne", "Sarl\u00f3s", "Tam\u00e1s", "Atif", "Jamal"], "venue": "CoRR, abs/1605.09046,", "citeRegEx": "Choromanski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choromanski et al\\.", "year": 2016}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta", "year": 1999}, {"title": "Experiments with random projection", "author": ["S. Dasgupta"], "venue": "In UAI,", "citeRegEx": "Dasgupta,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta", "year": 2000}, {"title": "Random projection trees and low dimensional manifolds", "author": ["S. Dasgupta", "Y. Freund"], "venue": "In STOC,", "citeRegEx": "Dasgupta and Freund,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Freund", "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N.D. Freitas"], "venue": "In NIPS", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In NIPS", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Random projection for high dimensional data clustering: A cluster ensemble approach", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "In ICML,", "citeRegEx": "Fern and Brodley,? \\Q2003\\E", "shortCiteRegEx": "Fern and Brodley", "year": 2003}, {"title": "Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis", "author": ["S. Ganguli", "H. Sompolinsky"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Ganguli and Sompolinsky,? \\Q2012\\E", "shortCiteRegEx": "Ganguli and Sompolinsky", "year": 2012}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "CoRR, abs/1504.08291,", "citeRegEx": "Giryes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Giryes et al\\.", "year": 2015}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Verma", "Vishal", "Lazebnik", "Svetlana"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Rowley", "Henry A", "Lazebnik", "Svetlana"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Toeplitz compressed sensing matrices with applications to sparse channel estimation", "author": ["J. Haupt", "W.U. Bajwa", "G. Raz", "R. Nowak"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Haupt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haupt et al\\.", "year": 2010}, {"title": "Johnson-lindenstrauss lemma for circulant matrices", "author": ["A. Hinrichs", "J. Vybral"], "venue": "Random Struct. Algorithms,", "citeRegEx": "Hinrichs and Vybral,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs and Vybral", "year": 2011}, {"title": "Extreme learning machine: Theory and applications", "author": ["Huang", "G.-B", "Zhu", "Q.-Y", "Siew", "C.-K"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors", "author": ["L. Jacques", "J.N. Laska", "P. Boufounos", "R.G. Baraniuk"], "venue": "CoRR, abs/1104.3160,", "citeRegEx": "Jacques et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jacques et al\\.", "year": 2011}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, pp", "citeRegEx": "Jaeger and Haas,? \\Q2004\\E", "shortCiteRegEx": "Jaeger and Haas", "year": 2004}, {"title": "Efficient and Robust Compressed Sensing Using Optimized Expander Graphs", "author": ["S. Jafarpour", "W. Xu", "B. Hassibi", "R. Calderbank"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Jafarpour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2009}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Suprema of chaos processes and the restricted isometry property", "author": ["F. Krahmer", "S. Mendelson", "H. Rauhut"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Krahmer et al\\.,? \\Q1877\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 1877}, {"title": "Very sparse random projections", "author": ["P. Li", "T.J. Hastie", "K.W. Church"], "venue": "In KDD,", "citeRegEx": "Li et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Li et al\\.", "year": 2006}, {"title": "Random projectionbased multiplicative data perturbation for privacy preserving distributed data mining", "author": ["K. Liu", "H. Kargupta", "J. Ryan"], "venue": "IEEE Trans. on Knowl. and Data Eng.,", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In ICLR,", "citeRegEx": "Mathieu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2014}, {"title": "An Evaluation of the Invariance Properties of a Biologically-Inspired System for Unconstrained Face Recognition", "author": ["N. Pinto", "D.D. Cox"], "venue": "In BIONETICS,", "citeRegEx": "Pinto and Cox,? \\Q2010\\E", "shortCiteRegEx": "Pinto and Cox", "year": 2010}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS Computational Biology,", "citeRegEx": "Pinto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2009}, {"title": "Dimension reduction by random hyperplane tessellations", "author": ["Y. Plan", "R. Vershynin"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Plan and Vershynin,? \\Q2014\\E", "shortCiteRegEx": "Plan and Vershynin", "year": 2014}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "In NIPS", "citeRegEx": "Raginsky and Lazebnik,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik", "year": 2009}, {"title": "Restricted isometries for partial random circulant matrices", "author": ["H. Rauhut", "J.K. Romberg", "J.A. Tropp"], "venue": "CoRR, abs/1010.1847,", "citeRegEx": "Rauhut et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2010}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A. Ng"], "venue": "In ICML,", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Structure preserving embedding", "author": ["Shaw", "Blake", "Jebara", "Tony"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Shaw et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2009}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T. Sainath", "S. Kumar"], "venue": "In NIPS,", "citeRegEx": "Sindhwani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2015}, {"title": "Algorithmic derandomization via complexity theory", "author": ["D. Sivakumar"], "venue": "In STOC,", "citeRegEx": "Sivakumar,? \\Q2002\\E", "shortCiteRegEx": "Sivakumar", "year": 2002}, {"title": "Fast approximations to structured sparse coding and applications to object classification", "author": ["Szlam", "Arthur", "Gregor", "Karol", "LeCun", "Yann"], "venue": "In Computer Vision - ECCV 2012 - 12th European Conference on Computer", "citeRegEx": "Szlam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Szlam et al\\.", "year": 2012}, {"title": "A variant of the johnsonlindenstrauss lemma for circulant matrices", "author": ["J. Vybral"], "venue": "Journal of Functional Analysis,", "citeRegEx": "Vybral,? \\Q2011\\E", "shortCiteRegEx": "Vybral", "year": 2011}, {"title": "Semisupervised hashing for large-scale search", "author": ["Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Short-term memory in orthogonal neural networks", "author": ["O.L. White", "D.D. Lee", "H. Sompolinsky"], "venue": "Physical review letters,", "citeRegEx": "White et al\\.,? \\Q2004\\E", "shortCiteRegEx": "White et al\\.", "year": 2004}, {"title": "The restricted isometry property for block diagonal matrices", "author": ["H.L. Yap", "A. Eftekhari", "M.B. Wakin", "C.J. Rozell"], "venue": "In CISS,", "citeRegEx": "Yap et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yap et al\\.", "year": 2011}, {"title": "Binary embedding: Fundamental limits and fast algorithm", "author": ["X. Yi", "C. Caramanis", "E. Price"], "venue": "CoRR, abs/1502.05746,", "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["F.X. Yu", "S. Kumar", "Y. Gong", "Chang", "S.-F"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "On binary embedding using circulant matrices", "author": ["Yu", "Felix X", "Bhaskara", "Aditya", "Kumar", "Sanjiv", "Gong", "Yunchao", "Chang", "Shih-Fu"], "venue": "CoRR, abs/1511.06480,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": ", 2008), (Gong et al., 2012), (Gong et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 46, "context": ", 2013a), (Wang et al., 2012), (Gong et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 50, "context": ", 2013b), (Plan & Vershynin, 2014), (Yu et al., 2014), (Yi et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 49, "context": ", 2014), (Yi et al., 2015)), and in particular it was observed that by using linear projections and then applying sign function as a nonlinear map one does not loose completely the information about the angular distance between vectors, but instead the information might be approximately reconstructed from the Hamming distance between hashes.", "startOffset": 9, "endOffset": 26}, {"referenceID": 43, "context": "We prove an extension of the Johnson-Lindenstrauss lemma (Sivakumar, 2002) for general pseudo-random structured projections followed by nonlinear mappings.", "startOffset": 57, "endOffset": 74}, {"referenceID": 45, "context": "This result is also new compared to previous extensions (Hinrichs & Vybral, 2011; Vybral, 2011) of the Johnson-Lindenstrauss lemma, that consider special cases of our structured projections (namely: circulant matrices) and do not consider at all the action of the non-linear mapping.", "startOffset": 56, "endOffset": 95}, {"referenceID": 49, "context": "We give theoretical explanation of the approach that was so far only heuristically confirmed for some special structured matrices (see: (Yi et al., 2015), (Yu et al.", "startOffset": 136, "endOffset": 153}, {"referenceID": 50, "context": ", 2015), (Yu et al., 2014)).", "startOffset": 9, "endOffset": 26}, {"referenceID": 40, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 42, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 8, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 34, "context": "(Saxe et al., 2011; Sindhwani et al., 2015; Cheng et al., 2015; Mathieu et al., 2014)).", "startOffset": 0, "endOffset": 85}, {"referenceID": 40, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 8, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 42, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 30, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 36, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 26, "context": "Little is known from the theoretical point of view about these fast deep architectures, which achieve significant speed ups of computation and space usage reduction with simultaneous little or no loss in performance (Saxe et al., 2011; Cheng et al., 2015; Sindhwani et al., 2015; Jarrett et al., 2009; Pinto et al., 2009; Pinto & Cox, 2010; Huang et al., 2006).", "startOffset": 216, "endOffset": 360}, {"referenceID": 16, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 17, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 10, "context": "These findings coincide with the notion of high redundancy in network parametrization (Denil et al., 2013; Denton et al., 2014; Choromanska et al., 2015).", "startOffset": 86, "endOffset": 153}, {"referenceID": 1, "context": "Finally, we show how our structured nonlinear embeddings can be used in the knn setting (Altman, 1992).", "startOffset": 88, "endOffset": 102}, {"referenceID": 13, "context": "The idea of using random projections to facilitate learning with high-dimensional data stems from the early work on random projections (Dasgupta, 1999) showing in particular that learning of high-dimensional mixtures of Gaussians can be simplified when first projecting the data into a randomly chosen subspace of low dimension (this is a consequence of the curse of dimensionality and the fact that high-dimensional data often has low intrinsic dimensionality).", "startOffset": 135, "endOffset": 151}, {"referenceID": 14, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al.", "startOffset": 84, "endOffset": 125}, {"referenceID": 3, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al.", "startOffset": 277, "endOffset": 289}, {"referenceID": 33, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al., 2006; Choromanska et al., 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 362, "endOffset": 406}, {"referenceID": 9, "context": "This idea was subsequently successfully applied to both synthetic and real datasets (Dasgupta, 2000; Bingham & Mannila, 2001), and then adopted to a number of learning approaches such as random projection trees (Dasgupta & Freund, 2008), kernel and featureselection techniques (Blum, 2006), clustering (Fern & Brodley, 2003), privacy-preserving machine learning (Liu et al., 2006; Choromanska et al., 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 362, "endOffset": 406}, {"referenceID": 0, "context": ", 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 32, "context": ", 2013), learning with large databases (Achlioptas, 2003), sparse learning settings (Li et al., 2006), and more recently - deep learning (see (Saxe et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 40, "context": ", 2006), and more recently - deep learning (see (Saxe et al., 2011) for convenient review of such approaches).", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "Using linear projections with completely random Gaussian weights, instead of learned ones, was recently studied from both theoretical and practical point of view in (Giryes et al., 2015), but that work did not consider structured matrices which is a central point of our interest since structured matrices can be stored much more efficiently.", "startOffset": 165, "endOffset": 186}, {"referenceID": 13, "context": "Beyond applying methods that use random Gaussian matrix projections (Dasgupta, 1999; 2000; Giryes et al., 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 20, "context": "Beyond applying methods that use random Gaussian matrix projections (Dasgupta, 1999; 2000; Giryes et al., 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 0, "context": ", 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 29, "context": ", 2015) and random binary matrix projections (Achlioptas, 2003), it is also possible to construct deterministic projections that preserve angles and distances (Jafarpour et al., 2009).", "startOffset": 159, "endOffset": 183}, {"referenceID": 24, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 39, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 48, "context": "The point-wise nonlinearity was not considered in many previous works on structured matrices (Haupt et al., 2010; Rauhut et al., 2010; Krahmer et al., 2014; Yap et al., 2011) (moreover note that these works also consider the set of structured matrices which is a strict subset of the class of matrices considered here).", "startOffset": 93, "endOffset": 174}, {"referenceID": 46, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 50, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 49, "context": "Designing binary embeddings for high dimensional data with low distortion is addressed in many recent works (Weiss et al., 2008; Wang et al., 2012; Gong et al., 2013b;a; 2012; Yu et al., 2014; Yi et al., 2015; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009).", "startOffset": 108, "endOffset": 266}, {"referenceID": 49, "context": "In the context of our work, one of the recent articles (Yi et al., 2015) is especially important since the authors introduce the pipeline of constructing hashes with the use of structured matrices in the linear step, instead of completely random ones.", "startOffset": 55, "endOffset": 72}, {"referenceID": 27, "context": "They prove several theoretical results regarding the quality of the produced hash, and extend some previous theoretical results (Jacques et al., 2011; Plan & Vershynin, 2014).", "startOffset": 128, "endOffset": 174}, {"referenceID": 50, "context": "Some general results (unbiasedness of the angular distance estimator) were also known for short hashing pipelines involving circulant matrices ((Yu et al., 2014)).", "startOffset": 144, "endOffset": 161}, {"referenceID": 49, "context": "In contrast to (Yi et al., 2015), we present our theoretical results for simpler hashing models (our hashing mechanism is explained in Section 3 and consists of two very simple steps that we call preprocessing step and the actual hashing step, where", "startOffset": 15, "endOffset": 32}, {"referenceID": 51, "context": "In (Yu et al., 2015) theoretical guarantees regarding bounding the variance of the angle estimator in the circulant setting were presented.", "startOffset": 3, "endOffset": 20}, {"referenceID": 11, "context": "Strong concentration results regarding several structured matrices were given in (Choromanski & Sindhwani, 2016; Choromanski et al., 2016), following our work.", "startOffset": 81, "endOffset": 138}, {"referenceID": 26, "context": "Introducing randomness to networks was explored for various architectures, in example feedforward networks (Huang et al., 2006), convolutional networks (Jarrett et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 30, "context": ", 2006), convolutional networks (Jarrett et al., 2009; Saxe et al., 2011), and recurrent networks (Jaeger & Haas, 2004; White et al.", "startOffset": 32, "endOffset": 73}, {"referenceID": 40, "context": ", 2006), convolutional networks (Jarrett et al., 2009; Saxe et al., 2011), and recurrent networks (Jaeger & Haas, 2004; White et al.", "startOffset": 32, "endOffset": 73}, {"referenceID": 47, "context": ", 2011), and recurrent networks (Jaeger & Haas, 2004; White et al., 2004; Boedecker et al., 2009).", "startOffset": 32, "endOffset": 97}, {"referenceID": 4, "context": ", 2011), and recurrent networks (Jaeger & Haas, 2004; White et al., 2004; Boedecker et al., 2009).", "startOffset": 32, "endOffset": 97}, {"referenceID": 7, "context": "Very recently (see: (Chen et al., 2015)) it was empirically showed that hashing in neural nets may achieve drastic reduction in model sizes with no significant loss of the quality, by heavily exploiting the phenomenon of redundancies in neural nets.", "startOffset": 20, "endOffset": 39}, {"referenceID": 7, "context": "HashedNets introduced in (Chen et al., 2015) do not give any theoretical guarantees regarding the quality of the proposed hashing.", "startOffset": 25, "endOffset": 44}, {"referenceID": 44, "context": "Structured hashing was applied also in (Szlam et al., 2012), but in a very different context than ours.", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "All networks were trained for 30 epochs using SGD (Bottou, 1998).", "startOffset": 50, "endOffset": 64}, {"referenceID": 50, "context": "Toeplitz matrix-vector multiplications can be efficiently implemented via Fast Fourier Transform (Yu et al., 2014).", "startOffset": 97, "endOffset": 114}], "year": 2016, "abstractText": "We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudorandom projection is described by a matrix, where not all entries are independent random variables but instead a fixed \u201cbudget of randomness\u201d is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input highdimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. In particular, they generalize previous extensions of the JohnsonLindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier. Equal contribution.", "creator": "LaTeX with hyperref package"}}}