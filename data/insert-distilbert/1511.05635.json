{"id": "1511.05635", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Competitive Multi-scale Convolution", "abstract": "in this paper, we significantly introduce a new deep feedback convolutional neural network ( convnet ) module designed that promotes competition among a set of distinct multi - sensor scale convolutional filters. this new module is inspired by the inception module, where we replace the original collaborative pooling stage ( consisting highly of a concatenation part of the multi - scale selected filter outputs ) resolved by leaving a competitive pooling simulation represented by a maxout activation unit. this extension has the following two objectives : one 1 ) the selection of the maximum response among states the multi - scale filters prevents filter trigger co - adaptation and allows both the formation of multiple sub - networks within the same model, which has been shown to facilitate the training of complex learning problems ; and 2 ) completing the maxout completion unit reduces the dimensionality of the outputs from the multi - scale filters. we show that the use of our proposed response module in typical deep convnets produces classification results that are either better than or comparable to the state of jean the beaux art on the following benchmark matching datasets : mnist, cifar - 10, cifar - 100 and svhn.", "histories": [["v1", "Wed, 18 Nov 2015 01:19:00 GMT  (1701kb,D)", "http://arxiv.org/abs/1511.05635v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["zhibin liao", "gustavo carneiro"], "accepted": false, "id": "1511.05635"}, "pdf": {"name": "1511.05635.pdf", "metadata": {"source": "CRF", "title": "Competitive Multi-scale Convolution", "authors": ["Zhibin Liao", "Gustavo Carneiro"], "emails": ["zhibin.liao@adelaide.edu.au", "gustavo.carneiro@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "The use of competitive activation units in deep convolutional neural networks (ConvNets) is generally understood as a way of building one network by the combination of multiple sub-networks, where each one is capable of solving a simpler task when compared to the complexity of the original problem involving the whole dataset [22]. Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22]. For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units. As shown by Srivastava et al. [22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]\n\u2217This research was supported by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016)\nand generally produces superior classification results [22]. In this paper, we introduce a new module for deep ConvNets composed of several multi-scale convolutional filters that are joined by a maxout activation unit, which promotes competition among these filters. Our idea has been inspired by the recently proposed inception module [24], which currently produces state-of-the-art results on the ILSVRC 2014\n1\nar X\niv :1\n51 1.\n05 63\n5v 1\n[ cs\n.C V\n] 1\n8 N\nov 2\n01 5\nclassification and detection challenges [17]. The gist of our proposal is depicted in Fig. 1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27]. Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) [5] that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in [11]). Finally, the multi-scale filter outputs, weighted by BNU, are joined with a maxout unit [4] that reduces the dimensionality of the joint filter outputs and promotes competition among the multi-scale filters, which prevents filter co-adaptation and allows the formation of multiple sub-networks. We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8]."}, {"heading": "2. Literature Review", "text": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig. 2). In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns [22], dividing the input data points (and more generally the training space) into regions [14], where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set. In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23]. In practice, ReLU allows the division of the input space into two regions, but maxout and LWTA can divide the space in as many regions as the number of inputs, so for this reason, the latter two functions can estimate exponentially complex functions more effectively because of the larger number of sub-networks that are jointly trained. An important aspect about deep ConvNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate [5], but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions [11]. Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.\nAnother aspect of the current research on deep ConvNets is the idea of making the network deeper, which has been shown to improve classification results [3]. However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space [19, 24]. For the Szegedy et al.\u2019s model [24], this is achieved with the use of 1 \u00d7 1 convolutional filters [12] that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter. In Simonyan et al.\u2019s approach [19], the idea is to use a large number of layers with convolutional filters of very small size (e.g., 3\u00d7 3). In this work, we restrict the complexity of the deep ConvNet with the use of maxout activation units, which selects only one of the input nodes, as shown in Fig, 2.\nFinally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27]. Essentially, multiscale filtering follows a neuroscience model [18] that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes [24]. We explore this idea in our proposal, as depicted in Fig. 1, but we also argue (and show some evidence) that the multi-scale nature of the filters can prevent their co-adaptation during training."}, {"heading": "3. Methodology", "text": "Assume that an image is represented by x : \u2126 \u2192 R, where \u2126 denotes the image lattice, and that an image patch of size (2k \u2212 1)\u00d7 (2k \u2212 1) (for k \u2208 {1, 2, ...,K}) centred at position i \u2208 \u2126 is represented by xi\u00b1(k\u22121). The models being proposed in this paper follow the structure of the the NIN model [12], and is in general defined as follows:\nf(x, \u03b8f ) = fout \u25e6 fL \u25e6 ... \u25e6 f2 \u25e6 f1(x), (1)\nwhere \u25e6 denotes the composition operator, \u03b8f represents all the ConvNet parameters (i.e., weights and biases), fout(.) denotes an averaging pooling unit followed by a softmax activation function [12], and the network has blocks represented by l \u2208 {1, ..., L}, with each block containing a\ncomposition of Nl modules with fl(x) = f (Nl) l \u25e6 ... \u25e6 f (2) l \u25e6 f (1) l (x). Each module f (n) l (.) at a particular position i \u2208 \u2126 of the input data for block l is defined by\nf (n) l (xi) = \u03c3 ( \u03b31W > 1 xi + \u03b21, \u03b33W > 3 xi\u00b11 + \u03b23, ...,\n\u03b32k\u22121W > 2k\u22121xi\u00b1(k\u22121) + \u03b22k\u22121, \u03b3pW > 1 p3\u00d73(xi\u00b11) + \u03b2p ) .\n(2)\nwhere \u03c3(.) represents the maxout activation function [4], the convolutional filters of the module are represented by the weight matrices W2k\u22121 for k \u2208 {1, ...,Kl} (i.e., filters of size 2k \u2212 1 \u00d7 2k \u2212 1 \u00d7 #filters, with #filters denoting the number of 2-D filters present in W), which means that each module n in block l has Kl different filter sizes and #filters different filters, \u03b3 and \u03b2 represent the batch normalization scaling and shifting parameters [5], and p3\u00d73(xi\u00b11) represents a max pooling operator on the 3 \u00d7 3 subset of the input data for layer l centred at i \u2208 \u2126, i.e. xi\u00b11.\nUsing the ConvNet module defined in (2), our proposed models differ mainly in the presence or absence of the node with the max-pooling operator within the module (i.e., the node represented by \u03b3pW>1 p3\u00d73(xi\u00b11) + \u03b2p). When the module does not contain such node, it is called Competitive Multi-scale Convolution (see Fig. 3-(a)), but when the module has the max-pooling node, then we call it Competitive Inception (see Fig. 3-(b)) because of its similarity to the original inception module [24]. The original inception module is also implemented for comparison purposes (see Fig. 3-(c)), and we call this model the Inception Style, which is similar to (1) and (2) but with the following differences: 1) the function \u03c3(.) in (2) denotes the concatenation of the input parameters; 2) a 1\u00d7 1 convolution is applied to the input x before a second round of convolutions with filter sizes larger than or equal to 3\u00d73; and 3) a ReLU activation function [1] is present after each convolutional layer.\nAn overview of all models with the structural parameters is displayed in Fig. 3. Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11]. In particular, we replace the original 5 \u00d7 5 convolutional layers of MIM by multi-scale filters of sizes 1\u00d71, 3\u00d73, 5\u00d75, and 7\u00d77. For the inception style model, we ensure that the number of output units in each module is the same as for the competitive inception and competitive multi-scale convolution, and we also use a 3 \u00d7 3 max-pooling path in each module, as used in the original inception module [24]. Another important point is that in general, when designing the inception style network, we follow the suggestion by Szegedy et al. [24] and include a relatively larger number of 3\u00d73 and 5\u00d75 filters in each module, compared to filters of other sizes (e.g., 1 \u00d7 1 and 7 \u00d7 7). An important distinction between the original GoogLeNet [24] and the inception style network in Fig. 3-(c) is the fact that we replace the fully connected layer in the last layer by a single 3 \u00d7 3 convolution node in the last module, followed by an average pooling and a softmax unit, similarly to the NIN model [12]. We propose this mod-\nification to limit the number of training parameters (with the removal of the fully connected layer) and to avoid the concatenation of the nodes from different paths (i.e., maxpooling, 1 \u00d7 1 convolution filter, and etc.) into a number of channels that is equal to the number of classes (i.e., each channel is averaged into a single node, which is used by a single softmax unit), where the concatenation would imply that some of the paths would be directly linked to a subset of the classes."}, {"heading": "3.1. Competitive Multi-scale Convolution Prevent", "text": "Filter Co-adaptation\nThe main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22]. More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22]. It is worth noting that these regions can only be formed if the underlying convolutional filters do not coadapt, otherwise all input training samples will fall into only one region of the competitive unit, which degenerates into a simple linear transform, preventing the formation of the sub-networks.\nA straightforward solution to avoid such co-adaptation can be achieved by limiting the number of training samples in a mini-batch during stochastic gradient descent. These small batches allow the generation of \u201cnoisy\u201d gradient directions during training that can activate different maxout gates, so that the different linear pieces of the activation unit can be fitted, allowing the formation of an exponentially large number of regions. However, the drawback of this approach lies in the determination of the \u201cright\u201d number of samples per mini-batch. A mini-batch size that is too small leads to poor convergence, and if it is too large, then it may not allow the formation of many sub-networks. Recently, Liao and Carneiro [11] propose a solution to this problem based on the use of BNU [5] that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks. However, there is still a potential problem with that approach [11], which is that the underlying convolutional filters are trained using feature spaces of the same size (i.e., the underlying filters are of fixed size), which can induce the filters to co-adapt and converge to similar regions of the feature space, also preventing the formation of the subnetworks.\nThe competitive multi-scale convolution module proposed in this paper represents a way to fix the issue introduced above [11]. Specifically, the different sizes of the convolutional filters within a competitive unit force the feature spaces of the filters to be different from each other, reducing the chances that these filters will converge to similar regions of the feature space. For instance, say you have two filters of sizes 3 \u00d7 3 and 5 \u00d7 5 being joined by a competitive unit, so this means that the former filter will have a 9-dimensional space, while the latter filter will have 16 additional dimensions for a total of 25 dimensions, where these new dimensions will allow the training process for the 5 \u00d7 5 filter to have a significantly larger feature space (i.e., for these two filters to converge to similar values, the additional 16 dimensions will have to be pushed towards zero and the remaining 9 dimensions to converge to the same values as the 3 \u00d7 3 filter). In other words, the different filter sizes within a competitive unit imposes a soft constraint that the filters must converge to different values, avoiding the co-adaptation issue. In some sense, this idea is similar to DropConnect [26], which, during training, drops to zero the weights of randomly picked network connections with the goal of training regularization. Nevertheless, the underlying filters will have the same size, which promotes coadaptation even with random connections being dropped to zero. Compared with DropConnect that stochastically drops filter connections during training, our approach deterministically drops the border connections of a 7\u00d7 7 filter (e.g., a 5 \u00d7 5 filter is a 7 \u00d7 7 filter with the 24 border connections dropped to zero, and a 3 \u00d7 3 filter is a 7 \u00d7 7 filter with the 40 border connections forced to zero - see Fig. 5). We show in the experiments that our approach is more effective than DropConnect at the task of preventing filter co-adaptation within competitive units."}, {"heading": "4. Experiments", "text": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16]. We first describe the experimental setup, then using CIFAR-10 and MNIST, we show a quantitative analysis (in terms of classification error, number of model parameters and train/test time) of the two proposed models, the Inception Style model presented in Sec. 3, and two additional versions of the proposed models that justify the use of multi-scale filters, explained in Sec. 3.1. Finally, we compare the performance of the proposed Competitive Multi-scale Convolution and Competitive Inception with respect to the current state of the art in the four benchmark datasets mentioned above.\nThe CIFAR-10 [7] dataset contains 60000 images of 10 commonly seen object categories (e.g., animals, vehicles, etc.), where 50000 images are used for training and the rest 10000 for testing, and all 10 categories have equal volume of training and test images. The images of CIFAR-10\nconsist of 32 \u00d7 32-pixel RGB images, where the objects are well-centered in the middle of the image. The CIFAR100 [7] dataset extends CIFAR-10 by increasing the number of categories to 100, whereas the total number of images remains the same, so the CIFAR-100 dataset is considered as a harder classification problem than CIFAR-10 since it contains 10 times less images per class and 10 times more categories. The well-known MNIST [8] dataset contains 28 \u00d7 28 grayscale images comprising 10 handwritten digits (from 0 to 9), where the dataset is divided into 60000 images for training and 10000 for testing, but note that the number of images per digit is not uniformly distributed. Finally, the Street View House Number (SVHN) [16] is also a digit classification benchmark dataset that contains 600000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images is centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and a extra set with 530000 images that are less difficult and can be used for helping with the training process. We do not use data augmentation in any of the experiments, and we only compare our results with other methods that do not use data augmentation.\nIn all these benchmark datasets we minimize the softmax loss function present in the last layer of each model for the respective classification in each dataset, and we report the results as the proportion of misclassified test images, which is the standard way of comparing algorithms in these benchmark datasets. The reported results are generated with the models trained using an initial learning rate of 0.1 and following a multi-step decay to a final learning rate of 0.001 (in 80 epochs for CIFAR-10 and CIFAR-100, 50 epochs for MNIST, and 40 epochs for SVHN). The stopping criterion is determined by the convergence observed in the error on the validation set. The mini-batch size for CIFAR10, CIFAR-100, and MNIST datasets is 100, and 128 for SVHN dataset. The momentum and weight decay are set to standard values 0.9 and 0.0005, respectively. For each result reported, we compute the mean and standard deviation of the test error from five separately trained models, where for each model, we use the same training set and parameters (e.g., the learning rate sequence, momentum, etc.), and we change only the random initialization of the filter weights and randomly shuffle the training samples.\nWe use the GPU-accelerated ConvNet library MatConvNet [25] to perform the experiments specified in this paper. Our experimental environment is a desktop PC equipped with i7-4770 CPU, 24G memory and a 12G GTX TITAN X graphic card. Using this machine, we report the mean training and testing times of our models."}, {"heading": "4.1. Model Design Choices", "text": "In this section, we show the results from several experiments that show the design choices for our models, where we provide comparisons in terms of their test errors, the number of parameters involved in the training process and\nthe training and testing times. Tables 1 and 2 show the results on CIFAR-10 and MNIST for the models Competitive Multi-scale Convolution, Competitive Inception, and Inception Style models, in addition to other models explained below. Note that all models in Tables 1 and 2 are constrained to have the same numbers of input channels and output channels in each module, and all networks contain three blocks [12], each with three modules (so there is a total of nine modules in each network), as shown in Fig. 3.\nWe argue that the multi-scale nature of the filters within the competitive module is important to avoid the coadaptation issue explained in Sec. 3.1. We assess this importance by comparing both the number of parameters and the test error results between the proposed models and the model Competitive Single-scale Convolution, which has basically the same architecture as the Competitive Multiscale Convolution model represented in Fig. 3-(a), but with the following changes: the first two blocks contain four sets of 7\u00d77 filters in the first module, and in the second and third modules, two sets of 3 \u00d7 3 filters; and the third block has three filters of size 5\u00d75 in the first module, followed by two modules with two 3\u00d73 filters. Notice that this configuration implies that we replace the multi-scale filters by the filter of the largest size of the module in each node, which is a configuration similar to the recently proposed MIM model [11]. The configuration for the Competitive Single-scale Convolution has around two times more parameters than the Competitive Multi-scale Convolution model and takes longer to train, as displayed in Tables 1 and 2. The idea behind the use of the largest size filters within each module is based on the results obtained from the training of the batch normalisation units of the Competitive Multi-scale Convolution modules, which indicates that the highest weights (represented by \u03b3 in (2)) are placed in the largest size filters within each module, as shown in Fig. 4. The classification results of the Competitive Single-scale Convolution, shown in Tables 1 and 2, demonstrate that it is consistently inferior to the Competitive Multi-scale Convolution model.\nAnother important point that we test in this section is the relevance of dropping connections in a deterministic or stochastic manner when training the competitive convolution modules. Recall that the one of the questions posed in Sec. 3.1 is if the deterministic masking provided by our proposed Competitive Multi-scale Convolution module is more effective at avoiding filter co-adaptation than the stochastic masking provided by DropConnect [26]. We run a quantitative analysis of the Competitive DropConnect Singlescale Convolution, where we take the Competitive Singlescale Convolution proposed before and randomly drop connections using a rate, which is computed such that it has on average the same number of parameters to learn in each round of training as the Competitive Multi-scale Convolution, but notice that the Competitive DropConnect Singlescale Convolution has in fact the same number of parameters as the Competitive Single-scale Convolution. Using Fig. 5, we see that the DropConnect rate is 0.57 for the\nmodule 1 of blocks 1 and 2 specified in Fig. 3. The results in Tables 1 and 2 show that it has around two times more parameters, takes longer to train and performs significantly worse than the Competitive Multi-scale Convolution model.\nFinally, the reported training and testing times in Tables 1 and 2 show a clear relation between the number of model parameters and those times."}, {"heading": "4.2. Comparison with the State of the Art", "text": "We now show the performances of the proposed Competitive Multi-scale and Competitive Inception Convolution models on CIFAR-10, CIFAR-100, MNIST and SVHN, and compare them with the current state of the art in the field,\nwhich can be listed as follows. Stochastic Pooling [28] proposes a regularization based on a replacement of the deterministic pooling (e.g., max or average pooling) by a stochastic procedure, which randomly selects the activation within each pooling region according to a multinomial distribution, estimated from the activation of the pooling unit. Maxout Networks [4] introduces a piece-wise linear activation unit that is used together with dropout training [20] and is introduced in Fig. 2-(c). The Network in Network (NIN) [12] model consists of the introduction of multilayer perceptrons as activation functions to be placed between convolution layers, and the replacement of a final fully connected layer by average pooling, where the number of output channels represent the final number of classes in the classification problem. Deeply-supervised nets [9] introduce explicit training objectives to all hidden layers, in addition to the back-propagated errors from the last softmax layer. The use of a recurrent structure that replaces the purely feed-forward structure in ConvNets is explored by the model RCNN [10]. An extension of the NIN model based on the use of maxout activation function instead of the multilayer perceptron is introduced in the MIM model [11], which also shows that the use of batch normalization units are crucial for allowing an effective training of several single-scale filters that are joined by maxout units. Finally, the Tree based Priors [21] model proposes a training method for classes with few samples, using a generative prior that is learned from the data and shared between related classes during the model learning.\nThe comparison on CIFAR-10 [7] dataset is shown in Tab. 3, where results are sorted based on the performance of each method, and the results of our proposed methods are highlighted. The results on CIFAR-100[7] dataset are displayed in Tab.4. Table 5 shows the results on MNIST [8], where it is worth reporting that the best result (over the five trained models) produced by our Competitive Multi-scale Convolution model is a test error of 0.29%, which is bet-\nter than the single result from Liang and Hu [10]. Finally, the comparison on SVHN[16] dataset is shown in Table 6, where two out of the five models show test error results of 1.69%."}, {"heading": "5. Discussion and Conclusions", "text": "In terms of the model design choices in Sec. 4.1, we can see that the proposed Competitive Multi-scale Convolution produces more accurate classification results than the proposed Competitive Inception. Given that the main differ-\nence between these two models is the presence of the maxpooling path within each module, we can conclude that this path does not help with the classification accuracy of the model. The better performance of both models with respect to the Inception Style model can be attributed to the maxout unit that induces competition among the underlying filters, which helps more the classification results when compared with the collaborative nature of the Inception module. Considering model complexity, it is important to notice that the relation between the number of parameters and training and testing times is not linear, where even though the Inception Style model has 10\u00d7 fewer parameters, it trains and tests 2 to 1.5\u00d7 faster than the proposed Competitive Multi-scale Convolution and Competitive Inception models.\nWhen answering the questions posed in Sec. 3.1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region [14, 22]. We argue that the main consequence of that is a potential lower classification accuracy, depending on the complexity of the original classification problem. Using this assumption, we note from Tables 1 and 2 that the use of multi-scale filters within a competitive module is in fact important to avoid the co-adaptation of the filters, as shown by the more accurate classification results of the Multi-scale, compared to the Single-scale model. Furthermore, the use of deterministic, as opposed to stochastic, mapping also appears to be more effective in avoiding filter co-adaptation given the more accurate classification results of the former mapping. Nevertheless, the reason behind the worse performance of the stochastic mapping may be due to the fact that DropConnect has been designed for the fully connected layers only [26], while our test bed for the comparison is set in the convolutional filters. To be more specific, we think that a fully connected layer usually encapsulates hundreds to thousands of weights for inputs of similar scale of dimensions, thus a random dropping on a subset of weight elements can hardly change the distribution of the outputs pattern. However, the convolution filters are of small dimensions, and each of our maxout unit controls 4 to 5 filters at most, so such masking scheme over small weights matrix could result in \u201ccatastrophic forgetting\u201d [13] which explains why the Competitive DropConnect Single-scale Convolution performs even worse than Competitive Singlescale Convolution on CIFAR-10.\nWe also run an experiment that assesses whether filters of larger size within a competitive module can improve the classification accuracy at the expense of having a larger number of parameters to train. We test the inclusion of two more filters of sizes 9 \u00d7 9 and 11 \u00d7 11 in module 1 of blocks 1 and 2, and two more filter sizes 7\u00d7 7 and 9\u00d7 9 in module 1 of block 3 (see Fig. 3). The classification re-\nsult obtained is 7.36\u00b1 0.16% on CIFAR-10, and number of model parameters is 13.11 M. This experiment shows that increasing the number of filters of larger sizes do not necessarily help improve the classification results. An important modification that can be suggested for our proposed Competitive Multi-scale Convolution model is the replacement of the maxout by ReLU activation, where only the largest size filter of each module is kept and all other filters are removed. One can argue that such model is perhaps less complex (in terms of the number of parameters) and probably as accurate as the proposed model. However, the results we obtained with such model on CIFAR-10 show that this model has 3.28 M parameters (i.e., just slightly less complex than the proposed models, as shown in Tab. 1) and has a classification test error of 8.16\u00b1 0.15%, which is significantly larger than for our proposed models. On MNIST, this model has 0.81 M parameters and produces a classification error of 0.37\u00b1 0.05%, which also shows no advantage over the proposed models.\nThe comparisons with the state of the art in Tables 3- 6 of Sec. 4.2 show that the proposed Competitive Multiscale Convolution model produces the best results in the field for three out of the four considered datasets. However, note that this comparison is not strictly fair to us because we run a five-model validation experiment (using different model initializations and different sets of mini batches for the stochastic gradient descent), which provides a more robust performance assessment of our method. In contrast, most of the methods in the field only show one single result of their performance. If we consider only the best result out of the five results in the experiment, then our Competitive Multi-scale Convolution model has the best results in all four datasets (with, for example, 0.29% on MNIST and 1.69% on SVHN). An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM [11] and RCNN [10] models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.\nIn this paper, we show the effectiveness of using competitive units on modules that contain multi-scale filters. We argue that the main reason of the superior classification results of our proposal, compared with the current state of the art in several benchmark datasets, lies in the following points: 1) the deterministic masking implicitly used by the multi-scale filters avoids the issue of filter co-adaptation; 2) the competitive unit that joins the underlying filters and the batch normalization units promote the formation of a large number of sub-networks that are specialized in the classification problem restricted to a small area of the input space and that are regularized by the fact that they are trained together within the same model; and 3) the maxout unit allows the reduction of the number of parameters in the model. It is important to note that such modules can be applied in several types of deep learning networks, and we plan to apply it to other types of models, such as the recurrent neural network [10]."}], "references": [{"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 315\u2013323", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2014, pages 392\u2013407. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "The 30th International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural computation, 3(1):79\u201387", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep, 1(4):7", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Proceedings of AISTATS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3367\u20133375", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "On the importance of normalisation layers in deep learning with piecewise linear activation units", "author": ["Z. Liao", "G. Carneiro"], "venue": "CoRR, abs/1508.00330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "The psychology of learning and motivation, 24(109-165):92", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2924\u20132932", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807\u2013814", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada, Spain", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, pages 1\u201342", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):411\u2013426", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2094\u20132102", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding locally competitive networks", "author": ["R.K. Srivastava", "J. Masci", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2310\u20132318", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), pages 1058\u20131066", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "The use of competitive activation units in deep convolutional neural networks (ConvNets) is generally understood as a way of building one network by the combination of multiple sub-networks, where each one is capable of solving a simpler task when compared to the complexity of the original problem involving the whole dataset [22].", "startOffset": 327, "endOffset": 331}, {"referenceID": 5, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 81, "endOffset": 84}, {"referenceID": 22, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 172, "endOffset": 180}, {"referenceID": 21, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 172, "endOffset": 180}, {"referenceID": 0, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 188, "endOffset": 191}, {"referenceID": 22, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 3, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 22, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 23, "context": "(c) Original inception module [24]", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "and generally produces superior classification results [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Our idea has been inspired by the recently proposed inception module [24], which currently produces state-of-the-art results on the ILSVRC 2014", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "classification and detection challenges [17].", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 23, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 25, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 4, "context": "Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) [5] that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in [11]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) [5] that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in [11]).", "startOffset": 330, "endOffset": 334}, {"referenceID": 3, "context": "Finally, the multi-scale filter outputs, weighted by BNU, are joined with a maxout unit [4] that reduces the dimensionality of the joint filter outputs and promotes competition among the multi-scale filters, which prevents filter co-adaptation and allows the formation of multiple sub-networks.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 15, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 262, "endOffset": 265}, {"referenceID": 13, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 176, "endOffset": 184}, {"referenceID": 21, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 176, "endOffset": 184}, {"referenceID": 0, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 211, "endOffset": 214}, {"referenceID": 22, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns [22], dividing the input data points (and more generally the training space) into regions [14], where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns [22], dividing the input data points (and more generally the training space) into regions [14], where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set.", "startOffset": 218, "endOffset": 222}, {"referenceID": 0, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 3, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 22, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 4, "context": "An important aspect about deep ConvNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate [5], but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions [11].", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": "An important aspect about deep ConvNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate [5], but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions [11].", "startOffset": 446, "endOffset": 450}, {"referenceID": 10, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 65, "endOffset": 73}, {"referenceID": 21, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "ReLU [1] (a) is active when the input is bigger than 0, LWTA [23]", "startOffset": 5, "endOffset": 8}, {"referenceID": 22, "context": "ReLU [1] (a) is active when the input is bigger than 0, LWTA [23]", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "(b) activates only the node that has the maximum value (setting to zero the other ones), and maxout [4] (c) has only one output containing the maximum value from the input.", "startOffset": 100, "endOffset": 103}, {"referenceID": 21, "context": "1 of [22].", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "Another aspect of the current research on deep ConvNets is the idea of making the network deeper, which has been shown to improve classification results [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 18, "context": "However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space [19, 24].", "startOffset": 175, "endOffset": 183}, {"referenceID": 23, "context": "However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space [19, 24].", "startOffset": 175, "endOffset": 183}, {"referenceID": 23, "context": "\u2019s model [24], this is achieved with the use of 1 \u00d7 1 convolutional filters [12] that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s model [24], this is achieved with the use of 1 \u00d7 1 convolutional filters [12] that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "\u2019s approach [19], the idea is to use a large number of layers with convolutional filters of very small size (e.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 23, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 25, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 17, "context": "Essentially, multiscale filtering follows a neuroscience model [18] that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes [24].", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "Essentially, multiscale filtering follows a neuroscience model [18] that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes [24].", "startOffset": 244, "endOffset": 248}, {"referenceID": 11, "context": "The models being proposed in this paper follow the structure of the the NIN model [12], and is in general defined as follows:", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": ") denotes an averaging pooling unit followed by a softmax activation function [12], and the network has blocks represented by l \u2208 {1, .", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": ") represents the maxout activation function [4], the convolutional filters of the module are represented by the weight matrices W2k\u22121 for k \u2208 {1, .", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": ", filters of size 2k \u2212 1 \u00d7 2k \u2212 1 \u00d7 #filters, with #filters denoting the number of 2-D filters present in W), which means that each module n in block l has Kl different filter sizes and #filters different filters, \u03b3 and \u03b2 represent the batch normalization scaling and shifting parameters [5], and p3\u00d73(xi\u00b11) represents a max pooling operator on the 3 \u00d7 3 subset of the input data for layer l centred at i \u2208 \u03a9, i.", "startOffset": 288, "endOffset": 291}, {"referenceID": 23, "context": "3-(b)) because of its similarity to the original inception module [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": ") in (2) denotes the concatenation of the input parameters; 2) a 1\u00d7 1 convolution is applied to the input x before a second round of convolutions with filter sizes larger than or equal to 3\u00d73; and 3) a ReLU activation function [1] is present after each convolutional layer.", "startOffset": 227, "endOffset": 230}, {"referenceID": 11, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "For the inception style model, we ensure that the number of output units in each module is the same as for the competitive inception and competitive multi-scale convolution, and we also use a 3 \u00d7 3 max-pooling path in each module, as used in the original inception module [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 23, "context": "[24] and include a relatively larger number of 3\u00d73 and 5\u00d75 filters in each module, compared to filters of other sizes (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "An important distinction between the original GoogLeNet [24] and the inception style network in Fig.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "3-(c) is the fact that we replace the fully connected layer in the last layer by a single 3 \u00d7 3 convolution node in the last module, followed by an average pooling and a softmax unit, similarly to the NIN model [12].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 3, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 22, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 21, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 281, "endOffset": 285}, {"referenceID": 21, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 452, "endOffset": 456}, {"referenceID": 21, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 563, "endOffset": 567}, {"referenceID": 10, "context": "Recently, Liao and Carneiro [11] propose a solution to this problem based on the use of BNU [5] that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "Recently, Liao and Carneiro [11] propose a solution to this problem based on the use of BNU [5] that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "However, there is still a potential problem with that approach [11], which is that the underlying convolutional filters are trained using feature spaces of the same size (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "the inception style model uses ReLU [15] after all convolutional layers, the number of filters per convolutional node is represented by the number in brackets, and these models assume a 10-class classification problem.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "The competitive multi-scale convolution module proposed in this paper represents a way to fix the issue introduced above [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "In some sense, this idea is similar to DropConnect [26], which, during training, drops to zero the weights of randomly picked network connections with the goal of training regularization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 196, "endOffset": 199}, {"referenceID": 6, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 220, "endOffset": 223}, {"referenceID": 15, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "The CIFAR-10 [7] dataset contains 60000 images of 10 commonly seen object categories (e.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The CIFAR100 [7] dataset extends CIFAR-10 by increasing the number of categories to 100, whereas the total number of images remains the same, so the CIFAR-100 dataset is considered as a harder classification problem than CIFAR-10 since it contains 10 times less images per class and 10 times more categories.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "The well-known MNIST [8] dataset contains 28 \u00d7 28 grayscale images comprising 10 handwritten digits (from 0 to 9), where the dataset is divided into 60000 images for training and 10000 for testing, but note that the number of images per digit is not uniformly distributed.", "startOffset": 21, "endOffset": 24}, {"referenceID": 15, "context": "Finally, the Street View House Number (SVHN) [16] is also a digit classification benchmark dataset that contains 600000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Note that all models in Tables 1 and 2 are constrained to have the same numbers of input channels and output channels in each module, and all networks contain three blocks [12], each with three modules (so there is a total of nine modules in each network), as shown in Fig.", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "Notice that this configuration implies that we replace the multi-scale filters by the filter of the largest size of the module in each node, which is a configuration similar to the recently proposed MIM model [11].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "1 is if the deterministic masking provided by our proposed Competitive Multi-scale Convolution module is more effective at avoiding filter co-adaptation than the stochastic masking provided by DropConnect [26].", "startOffset": 205, "endOffset": 209}, {"referenceID": 26, "context": "Stochastic Pooling [28] proposes a regularization based on a replacement of the deterministic pooling (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Maxout Networks [4] introduces a piece-wise linear activation unit that is used together with dropout training [20] and is introduced in Fig.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "Maxout Networks [4] introduces a piece-wise linear activation unit that is used together with dropout training [20] and is introduced in Fig.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "The Network in Network (NIN) [12] model consists of the introduction of multilayer perceptrons as activation functions to be placed between convolution layers, and the replacement of a final fully connected layer by average pooling, where the number of output channels represent the final number of classes in the classification problem.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Deeply-supervised nets [9] introduce explicit training objectives to all hidden layers, in addition to the back-propagated errors from the last softmax layer.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "The use of a recurrent structure that replaces the purely feed-forward structure in ConvNets is explored by the model RCNN [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "An extension of the NIN model based on the use of maxout activation function instead of the multilayer perceptron is introduced in the MIM model [11], which also shows that the use of batch normalization units are crucial for allowing an effective training of several single-scale filters that are joined by maxout units.", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Finally, the Tree based Priors [21] model proposes a training method for classes with few samples, using a generative prior that is learned from the data and shared between related classes during the model learning.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "The comparison on CIFAR-10 [7] dataset is shown in Tab.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "The results on CIFAR-100[7] dataset are displayed in Tab.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Table 5 shows the results on MNIST [8], where it is worth reporting that the best result (over the five trained models) produced by our Competitive Multi-scale Convolution model is a test error of 0.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "31% MIM [11] 8.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "20% RCNN-160 [10] 8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "Deeply-supervised nets [9] 9.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "Network in Network [12] 10.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Maxout Networks [4] 11.", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "Stochastic Pooling [28] 15.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "CIFAR-10 [7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 10, "context": "25% MIM [11] 29.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "20% RCNN-160 [10] 31.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "Deeply-supervised nets [9] 34.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "Network in Network [12] 35.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Tree based Priors [21] 36.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "Maxout Networks [4] 38.", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "Stochastic Pooling [28] 42.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "proposed models (highlighted) and the state-of-the-art methods on CIFAR-100 [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "RCNN-96 [10] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "04% MIM [11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "03% Deeply-supervised nets [9] 0.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "02% Network in Network [12] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Maxout+Dropout [4] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "Stochastic Pooling [28] 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "MNIST [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "07% RCNN-192 [10] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "05% Deeply-supervised nets [9] 1.", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "Drop-connect [26] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "MIM [11] 1.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "08% Network in Network [12] 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Maxout+Dropout [4] 2.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "Stochastic Pooling [28] 2.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "proposed models (highlighted) and the state-of-the-art methods on SVHN [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "ter than the single result from Liang and Hu [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "Finally, the comparison on SVHN[16] dataset is shown in Table 6, where two out of the five models show test error results of 1.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region [14, 22].", "startOffset": 430, "endOffset": 438}, {"referenceID": 21, "context": "1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region [14, 22].", "startOffset": 430, "endOffset": 438}, {"referenceID": 24, "context": "Nevertheless, the reason behind the worse performance of the stochastic mapping may be due to the fact that DropConnect has been designed for the fully connected layers only [26], while our test bed for the comparison is set in the convolutional filters.", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "However, the convolution filters are of small dimensions, and each of our maxout unit controls 4 to 5 filters at most, so such masking scheme over small weights matrix could result in \u201ccatastrophic forgetting\u201d [13] which explains why the Competitive DropConnect Single-scale Convolution performs even worse than Competitive Singlescale Convolution on CIFAR-10.", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM [11] and RCNN [10] models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM [11] and RCNN [10] models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "It is important to note that such modules can be applied in several types of deep learning networks, and we plan to apply it to other types of models, such as the recurrent neural network [10].", "startOffset": 188, "endOffset": 192}], "year": 2015, "abstractText": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.", "creator": "LaTeX with hyperref package"}}}