{"id": "1511.05743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Sparse learning of maximum likelihood model for optimization of complex loss function", "abstract": "traditional machine learning methods usually minimize a simple intrinsic loss function to learn a predictive model, and then use a complex performance comparison measure to measure the prediction \" performance. ) however, minimizing that a simple loss function cannot guarantee that an optimal performance. in this paper, historically we study the problem of optimizing the simpler complex performance measure directly to obtain a predictive model. we proposed to construct a truncated maximum likelihood model for this problem, and to learn the model parameter, we have minimize a com - plex loss function \u03bb corresponding locally to the desired complex performance measure. to optimize the loss function, we approximate the upper bound of the complex loss. we could also propose impose the sparsity to the underlying model parameter to obtain a sparse model. such an objective is constructed by combining the upper bound algorithm of obtaining the loss function and the sparsity of the model approximation parameter, and basically we develop an iterative algorithm to minimize smoothing it by using specifically the fast iterative shrinkage - thresholding algorithm framework. the research experiments on optimization on combining three different complex performance measures, including f - score, receiver operating characteristic curve, and recall precision curve algorithm break even point, combined over three real - world applications, aircraft event site recognition of civil aviation safety, in - trusion detection in wireless mesh networks, and image classification, show the conceptual advantages of the proposed method over state - of - the - art methods.", "histories": [["v1", "Wed, 18 Nov 2015 11:40:02 GMT  (50kb)", "http://arxiv.org/abs/1511.05743v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ning zhang", "prathamesh chandrasekar"], "accepted": false, "id": "1511.05743"}, "pdf": {"name": "1511.05743.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Prathamesh Chandrasekar", "Ning Zhang"], "emails": ["zhangning115@yahoo.com", "prathameshchandrasekar@yahoo.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n05 74\n3v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\nKeywords Machine learning \u00b7 Complex multivariate performance \u00b7 Sparse learning \u00b7 Maximum likelihood \u00b7 Civil aviation safety\nN. Zhang Guangzhou Civil Aviation College, Guangzhou 510403, China E-mail: zhangning115@yahoo.com\nP. Chandrasekar Uttar Pradesh Technical University, Lucknow, Uttar Pradesh 226021, India E-mail: prathameshchandrasekar@yahoo.com"}, {"heading": "1 Introduction", "text": "Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18]. In this paper, we focus on the machine learning problem of binary pattern classification. In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30]. To learn the predictive model, i.e., the classification model, we usually compare the true class label of data point against the predicted label using a loss function. For example, hinge loss, logistic loss, and squared \u21132 norm loss. By minimizing the loss functions over the training set with regard to the parameter of the classification model, we can obtain a optimal classification model. To evaluate the performance of the model, we apply it to a set of test data points to predict their class labels, and then compare the predicted class labels to their true class labels. This comparison can be conducted by using some multivariate performance measures. For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22]. A problem of such machine learning procedure is that in the training process, we optimize a simple loss function, such as hinge loss, but in the test process, we use a different and complex performance measure to evaluate the prediction results. It is obvious that the optimization of the loss function cannot leads to a optimization of the performance measure. For example, in the formulation of support vector machine (SVM), the hinge loss is minimized, but usually in the test procedure, the AUROC is used as a performance measure. However, in many real-world application, the optimization of a specific performance measure is desired. To solve this problem, direct optimization of some complex loss functions corresponding to some desired performance measures are studied. These methods try to a complex loss function in the objective function, and the loss functions is corresponding to the performance measure directly. By minimizing the loss function directly to obtain the predictive model, the desired performance measure can be optimized by the predictive model directly. In this paper, we study this problem, and propose a novel method based on sparse learning and maximum likelihood optimization.\n1.1 Related works\nSome existing works proposed to optimize a complex multivariate loss function are briefly introduced as follows.\n\u2013 Joachims proposed to learn a support vector machine to optimize a complex loss function [8]. In the proposed model, the complexity of the predictive model is reduced by minimizing squared \u21132 norm of the model parameter. To minimize the complex loss, its upper bound is approximated and minimized.\n\u2013 Mao and Tsang improved the Joachims\u2019s work by integrating feature selection to support vector machine for complex loss optimization [17]. A weight is assigned to each feature before the predictive model is learned. Moreover, the feature weights and the predictive model parameter is learned jointly in an iterative algorithm. \u2013 Li et al. proposed a classifier adaptation method to extend Joachims\u2019s work [12]. The predictive model is a combination of a base classifier and an adaptation function, and the learning of the optimal model is transferred to the learning of the parameter of the adaptation function. \u2013 Zhang et al. [40] proposed a novel smoothing strategy by using Nesterov\u2019s accelerated gradient method to improve the convergence rate of the method proposed by Joachims [8]. This method, according to the results reported in [40], can achieve converges significantly faster than Joachims\u2019s method [8], but it does not scarify generalization ability.\nAlmost all the existing methods are limited to the support vector machine for multivariate complex loss function. This method uses a linear function to construct the predictive model, and seek both the minimum complexity and loss.\n1.2 Contribution\nIn this paper, we propose a novel predictive model to optimize a complex loss function. This model is based on the likelihood of a positive or negative class given a input feature vector of a data point. The likelihood function is constructed based on a Sigmoid function of a linear function. Given a group of data points, we organize them as a data tuple, and the predicted class label tuple is the one that maximize the logistic likelihood of the data tuple. The learning target is to learn a predictive model parameter, so that with the corresponding predicted class label tuple, the complex loss function can be minimized. Moreover, we also hope the model parameter can be as sparse as possible, so that only the useful can be kept in the model. To this end, we construct an objective function, which is composed of two terms. The first term is the \u21131 norm of the parameter to impose the sparsity of the parameter, and the second term is the complex loss function to seek the optimal desired performance measure. The problem is transferred to a minimization problem of the objective function with regard to the parameter. To solve this problem, we first approximate the upper bound of the complex as a logistic function of the parameter, and then optimize it by using the fast iterative shrinkagethresholding algorithm (FISTA). The novelty of this paper is summarized as follows,\n1. For the first time, we propose to use the maximum likelihood model to construct a predictive model for the optimization of complex losses. 2. We construct a novel optimization problem for the learning of the model parameter by considering the sparsity of the model, and the minimization of the complex loss jointly.\n3. We develop a novel iterative algorithm to optimize the proposed minimization problem, and a novel method to approximate the upper bound of the complex loss. The approximation of the upper bound of the complex loss is obtained as a logistic function, and the problem is optimized by a FISTA algorithm.\n1.3 Paper organization\nThis paper is organized as follows: In section 2, we introduce the proposed method, in section 3, we evaluate the proposed method on two real-world applications, and in section 4, the paper is concluded and some future works are given.\n2 Proposed method\n2.1 Problem formulation\nSuppose we have a data set of n data points, and we denote them as {(xi, yi)}| n i=1, where xi \u2208 R d is the d-dimensional feature vector of the i-th data point, and yi \u2208 {+1,\u22121} is it corresponding class label. We consider the data points as a data tuple, x = (x1, \u00b7 \u00b7 \u00b7 ,xn), and their corresponding class labels as a label tuple, y = (y1, \u00b7 \u00b7 \u00b7 , yn). Under the framework of complex performance measure optimization, we try to learn a multivariate mapping function to map the data tuple x to a class label tuple y\u2217 = (y\u22171 , \u00b7 \u00b7 \u00b7 , y \u2217 n) \u2208 Y, where y \u2217 i \u2208 {+1,\u22121} is the predicted label of the i-th data point, and Y = {+1,\u22121}n. To measure the performance of the multivariate mapping function, h(x), we use a predefined complex loss function \u2206(y, y\u2217) to compare the true class label tuple y against the predicted class label tuple y\u2217.\nTo construct the multivariate mapping function h(x), we proposed to apply a linear discriminate function to match the i-th data point xi against the i-th class label y\u2032i in a candidate tuple y \u2032 = (y\u20321, \u00b7 \u00b7 \u00b7 , y \u2032 n),\nfw(xi, y \u2032 i) = y \u2032 iw \u22a4xi, (1)\nwhere w = [w1, \u00b7 \u00b7 \u00b7 , wd] \u2208 R d is the parameter vector of the function. And then we apply a Sigmoid function to the response of this function to impose it to a range of [0, 1],\ng(xi, y \u2032 i) =\n1\n1 + exp (\u2212f(xi, y\u2032i))\n= 1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n.\n(2)\nMoreover,\ng(xi,+1) = 1\n1 + exp (\u2212w\u22a4xi)\n=\n( 1 + exp ( \u2212w\u22a4xi )) \u2212 exp ( \u2212w\u22a4xi ) )\n1 + exp (\u2212w\u22a4xi)\n= 1\u2212 exp\n( \u2212w\u22a4xi ) )\n1 + exp (\u2212w\u22a4xi)\n= 1\u2212 1\n1 + exp (w\u22a4xi)\n= 1\u2212 g(xi,\u22121),\n(3)\nthus we can treat g(xi, yi) as the conditional probability of y = y \u2032 i given x = xi,\nPr(y = y\u2032i|x = xi) = g(xi, y \u2032 i). (4)\nWe also assume that the data points in the tuple x are conditional independent from each other, and thus the conditional probability of y = y\u2032 given the x is\nPr(y = y\u2032|x) =\nn \u220f\ni=1\nPr(y = y\u2032i|x = xi)\n=\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n.\n(5)\nTo constructed the complex mapping function, we map the data tuple to the class tuple y\u2217 which can give the maximum log-likelihood,\ny\u2217 \u2190 h(x) = argmax y\u2032\u2208Y log (Pr(y = y\u2032|x))\n= argmax y\u2032\u2208Y log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n.\n(6)\nIn this way, we seek the maximum likelihood estimator of the class label tuple as the mapping result for a data tuple.\nTo learn the parameter of the linear discriminative function, w, so that the complex loss function \u2206(y, y\u2217) can be minimized, we consider the following problems,\n\u2013 Encouraging sparsity of w: We assume that in a feature vector a data point, only a few features are useful, while most of the remaining features are useless. Thus we need to conduct a feature selection procedure to remove the useless features and keep the useful features, so that we can obtain a parse feature vector. In our method, instead of seeking sparsity of the feature vectors, we seek the sparsity of the parameter vector w. With a sparse w, we can also control the sparsity of the feature effective to the prediction results. To encourage the sparsity of w, we use the \u21131 norm of w to present its sparsity, and minimize the \u21131,\nmin w\n\n\n\n1 2 \u2016w\u20161 = 1 2\nd \u2211\nj=1\n|wj |\n= 1\n2\nd \u2211\nj=1\nw2j\n|wj | =\n1 2 w\u22a4diag\n(\n1\n|w1| , \u00b7 \u00b7 \u00b7 ,\n1\n|wd|\n)\nw\n= 1\n2 w\u22a4\u039bw\n}\n,\n(7)\nwhere diag (\n1 |w1| , \u00b7 \u00b7 \u00b7 , 1|wd|\n)\n\u2208 Rd\u00d7d is a diagonal matrix with its diagonal\nelements as 1|w1| , \u00b7 \u00b7 \u00b7 , 1 |wd| , and\n\u039b = diag\n(\n1\n|w1| , \u00b7 \u00b7 \u00b7 ,\n1\n|wd|\n)\n(8)\nWhen the \u21131 norm of w is minimized, most elements of w will shrink to zeros, and leads a sparse w. \u2013 Minimizing complex performance lose \u2206(y, y\u2217): Given the predicted label tuple y\u2217, we can measure the prediction performance by comparing it against the true label tuple y by using a complex performance measure. To obtain an optimal mapping function, we minimize a corresponding complex loss of a complex performance measure, \u2206(y, y\u2217),\nmin w\n\u2206(y, y\u2217) (9)\nDue to its complexity, we minimize its upper boundary instead of itself. We have the following Theorem to define the upper boundary of \u2206(y, y\u2217).\nTheorem 1: \u2206(y, y\u2217) satisfies\n\u2206(y, y\u2217) \u2264max y\u2032\u2208Y\n{\nlog\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2032)\n}\n=\n{\nlog\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032\u2032i w \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2032\u2032)\n}\n,\n(10) where y\u2032 = (y\u20321, \u00b7 \u00b7 \u00b7 , y \u2032 n), and y \u2032\u2032 = (y\u20321, \u00b7 \u00b7 \u00b7 , y \u2032 n),\ny\u2032\u2032 =argmax y\u2032\u2208Y\n{\nlog\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2032)\n}\n(11) The proof of this Theorem is found in Appendix section.\nAfter we have the upper bound of the loss function, we minimize it instead of \u2206(y, y\u2217) to obtain the mapping function parameter, w,\nmin w\n{\nlog\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032\u2032i w \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2032\u2032)\n=\nn \u2211\ni=1\nlog\n(\n1 + exp(\u2212yiw \u22a4xi) 1 + exp(\u2212y\u2032\u2032i w \u22a4xi)\n)\n+\u2206(y, y\u2032\u2032)\n}\n.\n(12) Please note that y\u2032\u2032 is also a function of w.\nThe overall optimization problem is obtained by combining the problems in (7) and (12),\nmin w\n{\nf(w) = 1\n2 w\u22a4\u039bw+ C\n[\nn \u2211\ni=1\nlog\n(\n1 + exp(\u2212yiw \u22a4xi) 1 + exp(\u2212y\u2032\u2032i w \u22a4xi)\n)\n+\u2206(y, y\u2032\u2032)\n]}\n(13) where C is a tradeoff parameter. Please not that in this objective, both \u039b and y\u2032\u2032 are functions of w. In the first term of the objective, we impose the sparsity of the w, and in the second term, we minimize the upper bound of \u2206(y, y\u2217).\n2.2 Optimization\nTo solve the problem of (13), we try to employ the FISTA algorithm with constant step-size to minimize the objective f(w). This algorithm is an iterative algorithm, and in each iteration, we first update a search point according to a previous solution of the parameter vector, and then update the next parameter vector based on the search point. The basic procedures are summarized as the two following steps:\n1. Search point step: In this step, we assume the previous solution of w is wpre, and seek a search point v \u2208 R d based on w is wpre and a stepsize L.\n2. Weighting factor step: In this step, we assume we have a weighting factor of previous iteration, \u03c4pre, and we update it to a new weighting factor \u03c4cur. 3. Solution update step: In this step, we update the new solution of the variable according to the search point. The updated solution is a weighted version of the previous search points, weighted by the weighting factors.\nIn the follows, we will discuss how to implement these three steps."}, {"heading": "2.2.1 Search point step", "text": "In this step, when we want to minimize an objective function f(w) with regard to a variable vector w with a step-size L and a previous solution wpre, we seek a search point u\u2217 as follows,\nu\u2217 = argmin u\n{\nL\n2\n\u2225 \u2225 \u2225 \u2225 u\u2212 ( wpre \u2212 1\nL \u2207f(wpre)\n) \u2225\n\u2225 \u2225 \u2225\n2\n2\n}\n, (14)\nwhere \u2207f(w) is the gradient function of f(w). Due to the complexity of function f(w), the close form of gradient function \u2207f(w) is difficult to obtain. Thus instead of seeking gradient function directly, we seek the sub-gradient of this function. This end, we use the EM algorithm strategy. In each iteration, we first fix w as wpre, and calculate \u039b according to (8), and y \u2032\u2032 i | n i=1 according to (11). Then we fix \u039b and y\u2032\u2032i | n i=1 and seek the sub-gradient \u2207f(w),\n\u2207f(w) = \u039bw+ C\nn \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4xi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4xi)\n\u2212 yixi exp(\u2212yiw\n\u22a4xi)\n1 + exp(\u2212yiw\u22a4xi)\n)\n. (15)\nAfter we have the sub-gradient function \u2207f(w), we substitute it to (14), and we have\nu\u2217 = argmin u\n{\nL\n2\n\u2225 \u2225 \u2225 \u2225 u\u2212 ( wpre \u2212 1\nL \u2207f(wpre)\n)\u2225\n\u2225 \u2225 \u2225\n2\n2\n}\n= argmin u\n{\nL\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 u\u2212 [ wpre \u2212 1 L ( \u039bwpre + C n \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1 + exp(\u2212yiw\u22a4prexi)\n))]\u2225\n\u2225 \u2225 \u2225 \u2225\n2\n2\n\n\n\n= argmin u\n{\nL\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 u\u2212 [ ( I \u2212 1 L \u039b ) wpre \u2212 C L n \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1 + exp(\u2212yiw\u22a4prexi)\n)]\u2225\n\u2225 \u2225 \u2225 \u2225\n2\n2\n= g(u)\n\n\n\n.\n(16)\nTo solve this problem, we set the gradient function of the objective function g(u) to zero,\n\u2207g(u) = L\n{\nu\u2212\n[\n(\nI \u2212 1\nL \u039b\n)\nwpre \u2212 C\nL\nn \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1 + exp(\u2212yiw\u22a4prexi)\n)]}\n= 0\n\u21d2 u\u2217 =\n(\nI \u2212 1\nL \u039b\n)\nwpre \u2212 C\nL\nn \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1 + exp(\u2212yiw\u22a4prexi)\n)\n.\n(17)\nIn this way, we obtain the search point u\u2217."}, {"heading": "2.2.2 Weighting factor step", "text": "We assume that weighting factor of previous iteration is \u03c4pre, we can obtain the weighting factor of current iteration, \u03c4cur, as follows,\n\u03c4cur = 1+\n\u221a\n1 + 4\u03c4pre2\n2 . (18)"}, {"heading": "2.2.3 Solution update step", "text": "After we have the search point of this current iteration, u\u2217, the search point of previous iteration, u\u2217pre, and the weighting factor of this iteration and previous iteration, \u03c4cur and \u03c4pre, we can have the following update procedure for the solution of this iteration,\nwcur = u \u2217 +\n(\n\u03c4pre \u2212 1\n\u03c4cur\n)\n( u\u2217 \u2212 u\u2217pre )\n=\n(\n\u03c4cur + \u03c4pre \u2212 1\n\u03c4cur\n)\nu\u2217 \u2212\n(\n\u03c4pre \u2212 1\n\u03c4cur\n)\nu\u2217pre.\n(19)\nIn this equation, we can see that the updated solution of wcur is a weighted version of the current search point, u\u2217, and the previous search point, u\u2217pre.\n2.3 Iterative algorithm\nWith the optimization in the previous section, we summarize the iterative algorithm to optimize the problem in (13). The iterative algorithm is given in Algorithm 1.\nAlgorithm 1: FISTA with constant stepsize to optimize (13) 1. Input: L, a constant step-size; 2. Step 0: Take w1 = u0, \u03c41 = 1. 3. Step k (k \u2265 1):\n(a) Update \u039bk according to (8) by fixing w = wk. (b) Update y\u2032\u2032i k| n i=1 according to (11) by fixing w = wk. (c) Update uk according to (17) by fixing wpre = wk, \u039b = \u039bk, and y \u2032\u2032 i =\ny\u2032\u2032i k, i = 1, \u00b7 \u00b7 \u00b7 , n. (d) Updating \u03c4k according to (18) by fixing \u03c4k\u22121 = \u03c4pre. (e) Updating wk according to (19) by fixing u \u2217 = uk, u \u2217 pre = uk\u22121, \u03c4cur =\n\u03c4k, and \u03c4pre = \u03c4k\u22121.\n4. Output: wk\nIn this algorithm, we can see that in each iteration, we first update \u039b and y\u2032\u2032i | n i=1, and then use them to update the search point. With the search point and a updated weighting factor, we update the mapping function parameter vector, w. This algorithm is called learning of sparse maximum likelihood model (SMLM).\n2.4 Scaling up to big data based on Hadoop\nIn this section, we discuss how to fit the proposed algorithm to big data set. We assume that the number of the training data points, n, is extremely large. One single machine is not able to store the entire data set, the data set is split to m subsets, and stored in m different clusters. The clusters are managed by a big data platform, Hadoop [4,39,10,27]. Hadoop is a software of distributed data management and processing. Given a large data set, it split it to subsets, and store them in different clusters. To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3]. This framework requires a Map program and a Reduce program from the users. The Hadoop software deliver the Map program to each cluster and uses it to process the subset to produce some median results, and then use the Reduce program to combine the median results to produce the final outputs. Using the Map-Reduce the framework, by defining our own Map and Reduce functions, we can implement the critical steps in Algorithm 1. For example, in the sub-step (c) of step k, we need to calculate uk from (17). In this step, the most time consuming step is to calculate the summation of a function over all the data points,\noutput =\nn \u2211\ni=1\n(\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1 + exp(\u2212y\u2032\u2032i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1 + exp(\u2212yiw\u22a4prexi)\n)\n= n \u2211\ni=1\nfunction(xi, yi, y \u2032\u2032 i ),\n(20)\nwhere function(xi, yi, y \u2032\u2032 i ) =\ny\u2032\u2032i xi exp(\u2212y \u2032\u2032 i w \u22a4 prexi)\n1+exp(\u2212y\u2032\u2032 i w \u22a4 prexi)\n\u2212 yixi exp(\u2212yiw\n\u22a4 prexi)\n1+exp(\u2212yiw\u22a4prexi) is the\nfunction applied to each data point. Since the entire data set is split to m subsets, Xm| m j=1, we can design a Map function to calculate the summation over each subset, and then design a Reduce function to combine the to obtain the final output. The Map and Reduce functions are as follows.\nMap function applied to the j-th subset 1. Input: Data points of the j-th subset, {(xi, yi, y \u2032\u2032 i )}|i:xi\u2208Xj . 2. Input: Previous parameter, wpre. 3. Initialize: Outputj = 0. 4. For i : xi \u2208 Xj\n(a) Outputj = Outputj + function(xi, yi, y \u2032\u2032 i );\n5. Endfor 6. Output: Outputj\nReduce function to calculate the final output 1. Input: Median outputs of m Map functions, Outputj| m j=1. 2. Initialize: Output = 0. 3. For j = 1, \u00b7 \u00b7 \u00b7 ,m\n(a) Output = Output+Outputj; 4. Endfor 5. Output: Output"}, {"heading": "3 Experiment", "text": "In this section, we evaluate the proposed SMLM for the optimization of complex loss function. Three different applications are considered, which are aircraft event recognition, intrusion detection in wireless mesh networks, and image classification.\n3.1 Aircraft event recognition\nRecognizing aircraft event of aircraft landing is an important problem in the area of civil aviation safety research. This procedure provides important information for fault diagnosis and structure maintenance of aircraft [36]. Given a landing condition, we want to predict if it is normal and abnormal. To this end, we extract some features, and use them to predict the aircraft event of normal or abnormal. In this experiment, we evaluate the proposed algorithm in this application, and use it as a model for the prediction of aircraft event recognition."}, {"heading": "3.1.1 Data set", "text": "In this experiment, we collect a data set of 160 data points. Each data point is a landing condition, and we describe the landing condition by five features,\nincluding vertical acceleration, vertical speed, lateral acceleration, roll angle, and pitch rate. The data points are classified to two classes, normal class and abnormal. The normal class is treated as positive class, while the abnormal class is treated as negative class. The number of positive data points is 108, and the number of negative data points is 52."}, {"heading": "3.1.2 Experiment setup", "text": "In this experiment, we use the 10-fold cross validation. The data set is split into 10 folds randomly, and each fold contains 16 data points. Each fold is used as a test set in turn, and the remaining 9 folds are combined and used as training set. The proposed model is training over the training set, and then used to predict the class labels of the testing data points in the test set. The prediction results are evaluated by a performance measurement. This performance measurement is used to compare the true class labels of the test data points against the predicted class labels. In the training procedure, a complex loss function corresponding to the performance measurement is minimized.\nIn our experiments, we consider three performance measurements, which are F-score, area under receiver operating characteristic curve (AUROC), and precision-recall curve break-even point (RPBEP). To define these performance measures, we first need to define the following items,\n\u2013 true positive (TP), the number of correctly predicted positive data points, \u2013 true negative (TN), the number of correctly predicted negative data points, \u2013 false positive (FP), the number of negative data points wrongly predicted\nto positive data points, and \u2013 false negative (FN), the number of positive data points wrongly predicted\nto negative data points.\nWith these measures, we can define F-score as follows,\nF = 2\u00d7 TP\n2\u00d7 TP + FP + FN . (21)\nMoreover, we can also define true positive rate (TPR) and the false positive rate (FPR) as follows,\nTPR = TP\nTP + FN , FPR =\nFP\nFP + TN . (22)\nWith different thresholds we can have different pair TPR and FPR. By plotting TPR against FPR values, we can have a curve of receiver operating characteristic (ROC). The area under this curve is obtained as AUROC. The recall and precision are defined as follows,\nrecall = TP\nTP + FN , precision =\nTP\nTP + FP . (23)\nWith different thresholds, we can also have different pair of recall and precision values. We can obtain a recall-precision (RP) curve, by plotting different\nprecision values against recall values. RPBEP is the value of the point of the RP curve where recall and precision are equal to each other."}, {"heading": "3.1.3 Experiment result", "text": "We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17]. The boxplots of the optimized F-scores of 10-fold cross validation of different algorithms on the aircraft event recognition problem are given in Fig. 1, these of optimized AUROC are given in Fig. 2, and these of the optimized PRBEP are given Fig. 3. From these figures, we can see that the proposed method, SMLM, outperforms the compared algorithms on three different optimized performances. For example, in Fig. 3, we can see that the boxplot of PRBEP of SMLM is significantly higher than other methods, the median value is almost 0.6, while that of other methods are much lower than 0.6. In Fig. 2, we can also have similar observation, the overall AUROC values optimized by SMLM is much higher than these of other methods. A reason for this outperforming is that our method seeks the maximum likelihood and sparsity of the model simultaneously.\n3.2 Intrusion detection in wireless mesh networks\nWireless mesh network (WMN) is a new generation technology of wireless networks, and it has been used in many different applications. However, due to its openness in wireless communication, it is vulnerable to intrusions, thus it is extremely important to detect intrusion in WMN. Given an attack record,\nthe problem of intrusion detection is to classify it to one of the following classes, denial service attacks, detect attacks, obtain root privileges and remote attack unauthorized access attacks. In this paper, we use the proposed method, SMLM, for the problem intrusion detection,"}, {"heading": "3.2.1 Data set", "text": "In this experiment, we use the KDD CPU1999 data set. In this data set, contains 4,0000 attack records, and for each class, there are 1,0000 records. For each record, we first preprocess the record, and then convert the features into digital signature as the new features."}, {"heading": "3.2.2 Experiment setup", "text": "In this experiment, we also use the 10-fold cross validation, and we also use the F-score, AUROC, and RPBEP performance measures."}, {"heading": "3.2.3 Experiment result", "text": "The boxplots of the optimized F-scores of 10-fold cross validation are given in Fig. 4, the boxplots of AUROC are given in Fig. 5, and the boxplots of PRBEP are given in Fig. 6. Similar to the results on aircraft event recognition problem, the outperforming of the proposed algorithm, SMLM, over other methods are also significant. This is a strong evidence of the advantages of sparse learning and maximum likelihood.\n3.3 ImageNet image classification\nIn the third experiment, we use a large image set to test the performance of the proposed algorithm with big data."}, {"heading": "3.3.1 Data set", "text": "In this experiment, we use a large data set, ImageNet [11]. This data set contains over 15 million images, and the images belong to 22,000 classes. These images are from web pages, and are labeled by people manually. The entire data set are split into three subsets, which are one training set, one validation set, and on testing set. The training set contains 1.2 million images, the validation set contains 50,000 images, and the testing set contains 150,000 images. To represent each image, we use the bag-of-features method. Local SIFT features are extracted from each image, and quantized to a histogram. The features can be downloaded directly from http://image-net.org/download-features."}, {"heading": "3.3.2 Experiment setup", "text": "In this experiment, we do not use the 10-fold cross validation, but use the given training/ validation/ testing set splitting. We first perform the proposed algorithm to the training set to learn the classifier, then use the validation set to justify the optimal tradeoff parameters, finally test the classifier over the testing set. The performances of F-score, AUROC, and RPBEP are considered in this experiment. To handle the multi-classification problem, we have a binary classification problem for each class, and in this problem, the considered class is a positive class, while the combination of all other classes is a negative class."}, {"heading": "3.3.3 Experiment results", "text": "The Boxplots of the optimized F-score, AUROC, and RPBEP of different classes are given in Fig. 7, Fig. 8, and Fig. 9. From these figures, we clearly see that the proposed algorithm outperforms the competing methods. This is another strong evidence of the effectiveness of the SMLM algorithm. Moreover, it also shows that the proposed algorithm also works well over the big data.\n3.4 Running time\nThe running time of the proposed algorithm on the three used data sets are given in Fig. 10. It can be observed from this figure that the first two experiments do not consume much time, while the third large scale data set based experiment costs a lot of time. This is natural, because in each iteration of the\nalgorithm, we have a function for each data point, and a summation over the responses of this function."}, {"heading": "4 Conclusion", "text": "In this paper, we investigate the problem of optimization of complex corresponding to a complex multivariate performance measure. We propose a novel predicative model to solve this problem. This model is based on the maximum likelihood of a class label tuple given a input data tuple. To solve the model parameter, we propose an optimization problem based on the approximation of the upper bound of the loss function and the sparsity of the model. Moreover, an iterative algorithm is developed to solve it. Experiments on two real-world applications show its advantages over state-of-the-art. In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].\nAppendix\nProof of Theorem 1: According to (6), we have\ny\u2217 = argmax y\u2032\u2208Y log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n,\n\u21d2 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2217iw \u22a4xi)\n)\n\u2265 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n, \u2200y\u2032 \u2208 Y,\n\u21d2 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2217iw \u22a4xi)\n)\n\u2265 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n,\n\u21d2 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2217iw \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n\u2265 0,\n\u21d2 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2217iw \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2217) \u2265 \u2206(y, y\u2217),\n\u21d2max y\u2032\u2208Y\n{\nlog\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212y\u2032iw \u22a4xi)\n)\n\u2212 log\n(\nn \u220f\ni=1\n1\n1 + exp (\u2212yiw\u22a4xi)\n)\n+\u2206(y, y\u2032)\n}\n\u2265 \u2206(y, y\u2217),\n(24) thus we have (10)."}], "references": [{"title": "A simple and practical control of the authenticity of organic sugarcane samples based on the use of machine-learning algorithms and trace elements determination by inductively coupled plasma mass spectrometry", "author": ["R. Barbosa", "B. Batista", "C. Bario", "R. Varrique", "V. Coelho", "A. Campiglia", "F. Barbosa"], "venue": "Food Chemistry 184, 154\u2013159", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "On shape properties of the receiver operating characteristic curve", "author": ["B. Bhattacharya", "G. Hughes"], "venue": "Statistics and Probability Letters 103, 73\u201379", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Using statistics for computing joins with map reduce", "author": ["T. Csar", "R. Pichler", "E. Sallinger", "V. Savenkov"], "venue": "CEUR Workshop Proceedings, vol. 1378, pp. 69\u201374", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance and energy efficiency of big data applications in cloud environments: A hadoop case study", "author": ["E. Feller", "L. Ramakrishnan", "C. Morin"], "venue": "Journal of Parallel and Distributed Computing 79-80, 80\u201389", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-ring local binary patterns for rotation invariant texture classification", "author": ["Y. He", "N. Sang"], "venue": "Neural Computing and Applications 22(3-4), 793\u2013802", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable multidimensional anonymization algorithm over big data using map reduce on public cloud", "author": ["A. Irudayasamy", "L. Arockiam"], "venue": "Journal of Theoretical and Applied Information Technology 74(2), 221\u2013231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation of hemoglobin performance in the assessment of iron stores  20  Ning Zhang, Prathamesh Chandrasekar in feto-maternal pairs in a high-risk population: Receiver operating characteristic curve analysis", "author": ["J. Jaime-Prez", "G. Garca-Arellano", "N. Mndez-Ramrez", "T. Gonzlez-Llano", "D. GmezAlmaguer"], "venue": "Revista Brasileira de Hematologia e Hemoterapia 37(3), 178\u2013183", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning, pp. 377\u2013384", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "Proceedings of the 22nd international conference on Machine learning, pp. 377\u2013384. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Computational fluid dynamics simulation based on hadoop ecosystem and heterogeneous computing", "author": ["M. Kim", "Y. Lee", "H.H. Park", "S. Hahn", "C.G. Lee"], "venue": "Computers and Fluids 115, 1\u201310", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, vol. 2, pp. 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z.H. Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(6), 1370\u20131382", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Determination of internal qualities of newhall navel oranges based on nir spectroscopy using machine learning", "author": ["C. Liu", "S. Yang", "L. Deng"], "venue": "Journal of Food Engineering 161, 16\u201323", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale simulations and mechanics of biological materials pp. 301\u2013317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Handling big data efficiently by using map reduce technique", "author": ["S. Maitrey", "C. Jha", "C. Jha"], "venue": "Proceedings - 2015 IEEE International Conference on Computational Intelligence and Communication Technology, CICT 2015, pp. 703\u2013708", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A feature selection method for multivariate performance measures", "author": ["Q. Mao", "I.W.H. Tsang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(9), 2051\u2013 2063", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Intelligent facial emotion recognition using a layered encoding cascade optimization model", "author": ["S. Neoh", "L. Zhang", "K. Mistry", "M. Hossain", "C. Lim", "N. Aslam", "P. Kinghorn"], "venue": "Applied Soft Computing Journal 34, 72\u201393", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The precision-recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases", "author": ["B. Ozenne", "F. Subtil", "D. Maucort-Boulch"], "venue": "Journal of Clinical Epidemiology 68(8), 855\u2013859", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics", "author": ["B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu"], "venue": "Nanoscale Research Letters 10(1), 235", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sorted consecutive local binary pattern for texture classification", "author": ["J. Ryu", "S. Hong", "H. Yang"], "venue": "IEEE Transactions on Image Processing 24(7), 2254\u20132265", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets", "author": ["T. Saito", "M. Rehmsmeier"], "venue": "PLoS ONE 10(3)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Walking pattern classification using a granular linguistic analysis", "author": ["D. Sanchez-Valdes", "A. Alvarez-Alvarez", "G. Trivino"], "venue": "Applied Soft Computing Journal 33, 100\u2013113", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Mixture models in diagnostic meta-analyses - clustering summary receiver operating characteristic curves accounted for heterogeneity and correlation", "author": ["P. Schlattmann", "M. Verba", "M. Dewey", "M. Walther"], "venue": "Journal of Clinical Epidemiology 68(1), 61\u201372", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "How to read a receiver operating characteristic curve", "author": ["P. Sedgwick"], "venue": "BMJ (Online) 350", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Jomr: Multi-join optimizer technique to enhance map-reduce job", "author": ["M. Shanoda", "S. Senbel", "M. Khafagy"], "venue": "2014 9th International Conference on Informatics and Systems, INFOS 2014, pp. PDC80\u2013PDC87", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Mammoth: Gearing hadoop towards memory-intensive mapreduce applications", "author": ["X. Shi", "M. Chen", "L. He", "X. Xie", "L. Lu", "H. Jin", "Y. Chen", "S. Wu"], "venue": "IEEE Transactions on Parallel and Distributed Systems 26(8), 2300\u20132315", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "\u03c5-nonparallel support vector machine for pattern classification", "author": ["Y. Tian", "Q. Zhang", "D. Liu"], "venue": "Neural Computing and Applications 25(5), 1007\u20131020", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Precision-recalloptimization in learning vector quantization classifiers for improved medical classification systems", "author": ["T. Villmann", "M. Kaden", "M. Lange", "P. Sturmer", "W. Hermann"], "venue": "pp. 71\u201377", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "2014 IEEE 26th International Conference on Tools with Artificial Intelligence (ICTAI 2014), pp. 853\u2013858", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J. Wang", "H. Bensmail"], "venue": "SMC2015", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Image tag completion by local learning", "author": ["J. Wang", "Y. Zhou", "H. Wang", "X. Yang", "F. Yang", "A. Peterson"], "venue": "Advances in Neural Networks\u2013ISNN 2015, pp. 232\u2013239. Springer", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Representing data by sparse combination of contextual data points for classification", "author": ["J. Wang", "Y. Zhou", "M. Yin", "S. Chen", "B. Edwards"], "venue": "Advances in Neural Networks\u2013 ISNN 2015, pp. 373\u2013381. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics 53(3), 403\u2013412", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental support vector machine learning method for aircraft event recognition", "author": ["X. Wang", "P. Shu"], "venue": "Proceedings - 2nd International Conference on Enterprise Systems, ES 2014, pp. 201\u2013204", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Enabling precision/recall preferences for semisupervised svm training", "author": ["Z. Wen", "R. Zhang", "K. Ramamohanarao"], "venue": "pp. 421\u2013430", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with positive and unlabeled examples using biased twin support vector machine", "author": ["Z. Xu", "Z. Qi", "J. Zhang"], "venue": "Neural Computing and Applications 25(6), 1303\u20131311", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Gom-hadoop: A distributed framework for efficient analytics on ordered datasets", "author": ["J. Yin", "Y. Liao", "M. Baldi", "L. Gao", "A. Nucci"], "venue": "Journal of Parallel and Distributed Computing 83, 58\u201369", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S. Vishwanathan"], "venue": "The Journal of Machine Learning Research 13(1), 3623\u20133680", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Human face recognition based on ensemble of polyharmonic extreme learning machine", "author": ["J. Zhao", "Z. Zhou", "F. Cao"], "venue": "Neural Computing and Applications 24(6), 1317\u20131326", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C 118(26), 14,586\u201314,594", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 40, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 37, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 12, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 33, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 0, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 17, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 4, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 27, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 20, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 32, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 30, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 29, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 6, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 23, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 24, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 1, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 36, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 18, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 28, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 21, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 7, "context": "\u2013 Joachims proposed to learn a support vector machine to optimize a complex loss function [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "Title Suppressed Due to Excessive Length 3 \u2013 Mao and Tsang improved the Joachims\u2019s work by integrating feature selection to support vector machine for complex loss optimization [17].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "proposed a classifier adaptation method to extend Joachims\u2019s work [12].", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "[40] proposed a novel smoothing strategy by using Nesterov\u2019s accelerated gradient method to improve the convergence rate of the method proposed by Joachims [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[40] proposed a novel smoothing strategy by using Nesterov\u2019s accelerated gradient method to improve the convergence rate of the method proposed by Joachims [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 39, "context": "This method, according to the results reported in [40], can achieve converges significantly faster than Joachims\u2019s method [8], but it does not scarify generalization ability.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "This method, according to the results reported in [40], can achieve converges significantly faster than Joachims\u2019s method [8], but it does not scarify generalization ability.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "And then we apply a Sigmoid function to the response of this function to impose it to a range of [0, 1], g(xi, y \u2032 i) = 1 1 + exp (\u2212f(xi, y\u2032 i)) = 1 1 + exp (\u2212y\u2032 iw xi) .", "startOffset": 97, "endOffset": 103}, {"referenceID": 3, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 38, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 9, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 26, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 5, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 25, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 15, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 2, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 35, "context": "This procedure provides important information for fault diagnosis and structure maintenance of aircraft [36].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 215, "endOffset": 218}, {"referenceID": 11, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 291, "endOffset": 295}, {"referenceID": 16, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 372, "endOffset": 376}, {"referenceID": 10, "context": "1 Data set In this experiment, we use a large data set, ImageNet [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 41, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 14, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 19, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 33, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 31, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 30, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 32, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}], "year": 2015, "abstractText": "Traditional machine learning methods usually minimize a simple loss function to learn a predictive model, and then use a complex performance measure to measure the prediction performance. However, minimizing a simple loss function cannot guarantee that an optimal performance. In this paper, we study the problem of optimizing the complex performance measure directly to obtain a predictive model. We proposed to construct a maximum likelihood model for this problem, and to learn the model parameter, we minimize a complex loss function corresponding to the desired complex performance measure. To optimize the loss function, we approximate the upper bound of the complex loss. We also propose impose the sparsity to the model parameter to obtain a sparse model. An objective is constructed by combining the upper bound of the loss function and the sparsity of the model parameter, and we develop an iterative algorithm to minimize it by using the fast iterative shrinkagethresholding algorithm framework. The experiments on optimization on three different complex performance measures, including F-score, receiver operating characteristic curve, and recall precision curve break even point, over three real-world applications, aircraft event recognition of civil aviation safety, intrusion detection in wireless mesh networks, and image classification, show the advantages of the proposed method over state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}