{"id": "1511.05942", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks", "abstract": "large amount of electronic health decision record ( ehr ) data have been collected over millions ml of patients over traditionally multiple years. the rich longitudinal ehr data documented the collective experiences of physicians including diagnosis, medication prescription and concurrent procedures. we argue it is possible now to leverage improve the ehr data to model how physicians behave, strategy and we call our clinical model doctor ai. towards this direction of modeling clinical bahavior of physicians, we develop a successful application of recurrent neural networks ( rnn ) to jointly forecast the future patients disease diagnosis and subsequent medication prescription along with their timing. unlike a traditional classification model assessed where a single target is of interest, our model can assess entire history of patients and immediately make continuous and multilabel prediction based on patients'historical data. we evaluate the performance of the proposed method accurately on a large projected real - money world ehr data over 250k patients over 8 years. we observe doctor ai achieves cases up to averaged 79 % recall @ 30, significantly implying higher priority than analyzing several baselines.", "histories": [["v1", "Wed, 18 Nov 2015 20:47:44 GMT  (471kb,D)", "http://arxiv.org/abs/1511.05942v1", null], ["v2", "Wed, 25 Nov 2015 12:40:27 GMT  (412kb,D)", "http://arxiv.org/abs/1511.05942v2", "ICLR 2016 submission. Updating the acknowledgment etc"], ["v3", "Mon, 14 Dec 2015 14:21:11 GMT  (319kb,D)", "http://arxiv.org/abs/1511.05942v3", "Updating the acknowledgment"], ["v4", "Wed, 6 Jan 2016 19:18:22 GMT  (331kb,D)", "http://arxiv.org/abs/1511.05942v4", "Updating the ICLR submission based on reviews"], ["v5", "Thu, 7 Jan 2016 18:23:06 GMT  (624kb,D)", "http://arxiv.org/abs/1511.05942v5", "Updating the ICLR submission based on reviews"], ["v6", "Wed, 17 Feb 2016 22:47:47 GMT  (320kb,D)", "http://arxiv.org/abs/1511.05942v6", "Updating"], ["v7", "Thu, 10 Mar 2016 13:46:11 GMT  (324kb,D)", "http://arxiv.org/abs/1511.05942v7", "Updating"], ["v8", "Sat, 19 Mar 2016 17:02:10 GMT  (323kb,D)", "http://arxiv.org/abs/1511.05942v8", "Updating"], ["v9", "Thu, 4 Aug 2016 02:52:55 GMT  (356kb,D)", "http://arxiv.org/abs/1511.05942v9", "Presented at 2016 Machine Learning and Healthcare Conference (MLHC 2016), Los Angeles, CA"], ["v10", "Tue, 30 Aug 2016 06:05:18 GMT  (355kb,D)", "http://arxiv.org/abs/1511.05942v10", "Presented at 2016 Machine Learning and Healthcare Conference (MLHC 2016), Los Angeles, CA"], ["v11", "Wed, 28 Sep 2016 19:11:19 GMT  (433kb,D)", "http://arxiv.org/abs/1511.05942v11", "Presented at 2016 Machine Learning and Healthcare Conference (MLHC 2016), Los Angeles, CA"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["edward choi", "mohammad taha bahadori", "andy schuetz", "walter f stewart", "jimeng sun"], "accepted": false, "id": "1511.05942"}, "pdf": {"name": "1511.05942.pdf", "metadata": {"source": "CRF", "title": "DOCTOR AI: PREDICTING CLINICAL EVENTS VIA RECURRENT NEURAL NETWORKS", "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Jimeng Sun"], "emails": ["recall@30,"], "sections": [{"heading": "1 INTRODUCTION", "text": "The introduction of Electronic Health Records (EHR) in health care has made available a massive amount of high-quality data. This has created an opportunity to perform sophisticated clinical analysis that was too complex to efficiently perform before. Forecasting future clinical events for patients is an especially challenging, yet important task that attempts to predict occurrence of various diseases over time for individual patients and what type of actions will be taken by the physicians. Successful forecasting will be, in a sense, equivalent to modeling the behavior of human doctors, which will not only facilitate patient-specific care and timely intervention, but also reduce the health care costs.\nAlthough parts of this problem such as disease progression modeling has been studied by many researchers over several decades, e.g. (Heckerman, 1990; Chapman et al., 2001; Lange et al., 2015), most works do not achieve the required accuracy and scalability, lack generality, or need excessive expert domain knowledge. With our work, the Doctor AI will be able to diagnose multiple problems and prescribe appropriate medications for anyone who can provide his/her medical history. Furthermore, the Doctor AI is able to predict when the patient will make the next visit, a service doctors are not known to often provide. The Doctor AI has huge potential to help both service providers (e.g. insurance companies, hospitals) and service consumers (e.g. individual patients). Additionally, the fact that it does not need any domain knowledge such as medical ontologies makes it easier to learn from the EHR of various sources.\nThe problem in its full generality can be described as a multilabel marked point process modeling task. The task is different from common sequential learning tasks such as those in natural language processing as it requires prediction of multiple categories over the continuous time axis. The key challenge in this task is to find a flexible model that is capable of jointly performing multiple types of prediction for patients.\nar X\niv :1\n51 1.\n05 94\n2v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5\nIn\nte\nns\nity\nTime\nKidney failure\nThe two main class of techniques, continuous-time Markov chain based models (Nodelman et al., 2002; Lange et al., 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al., 2015) are expensive to generalize to nonlinear and multilabel settings. Furthermore, they often make strong assumptions about the data generation process which make them not only inaccurate, but also not scalable to large scale datasets.\nThe key idea of this paper is to solve these challenges via learning an effective representation of the patient status over time. Once we concisely summarize the status of patients in a numerical vector, we would be able to jointly predict different future quantities about the patients such as their future diagnoses and medications and the time until their next visit. To learn such patient representations we propose to use recurrent neural networks, considering the fact that patients have different length of medical records and that recurrent neural networks have been shown to be particularly successful for representation learning in sequential data, e.g. (Graves, 2013; Graves & Jaitly, 2014; Sutskever et al., 2014; Kiros et al., 2014; Zaremba & Sutskever, 2014). In particular, we make the following main contributions in this paper:\n\u2022 We demonstrate a successful application of recurrent neural networks in representing the status of patients, predicting the future medical events and the timing of the events on the patient record. The trained RNN is able to achieve above 64% recall in its top 10 predicted diagnosis codes, proving that it can serve as an effective differential diagnosis machine.\n\u2022 We propose an efficient initialization scheme for RNNs using Skip-gram embedding (Mikolov et al., 2013) and show that it improves the performance of the RNN in our problem."}, {"heading": "2 METHODOLOGY", "text": "This section describes the main proposed neural network model. After mathematically describing the problem as a multilabel point process modeling, we outline our approach for addressing the challenges. The proceeding sections are devoted to details of the model and the technical details of the learning procedure.\nProblem setting. For each patient, we are given a sample of size n from a univariate multilabel marked point process in the form of (ti,xi) for i = 1, . . . , n. Each pair represents an event, such as a hospital visit, during which multiple medical codes such as ICD-9 diagnosis codes, procedure codes, or medication codes are assigned to a patient. The multi-hot label vector xi \u2208 {0, 1}p represents the medical codes assigned at time ti, where p denotes the number of unique medical codes. At each time stamp, we may extract higherlevel codes for prediction purposes and denote it by yi, see the details in Section 4.1. The number of events for each patient may differ.\nDescription of neural network architecture. Our goal is to learn an effective vector representation for the status of patients at each time stamp ti. Using the representation for the status of patients, we would be\nduration until next visit. [\ud835\udc51\", \ud835\udc99\"] [\ud835\udc51%&\",\ud835\udc99%&\"] [\ud835\udc51%,\ud835\udc99\ud835\udc8a]\n\ud835\udc89\" \"\n\ud835\udc89\" )\n\ud835\udc89\" ,\n\ud835\udc89%&\" \"\n\ud835\udc89%&\" )\n\ud835\udc89%&\" ,\n\ud835\udc89% ,\n\ud835\udc89% )\n\ud835\udc89% \"\nable to jointly predict different future quantities about this patient such as future diagnoses and medications yi+1 and the time duration until next event di+1 = ti+1\u2212ti. Finally, we would like to perform all these steps jointly in a single supervised learning scheme. As we discussed in the introduction, we use recurrent neural networks to learn such patient representations. We treat the state vector of RNNs as the latent representation for the patient status and use it for predicting multiple forms of outputs.\nThe proposed architecture is shown if Figure 2. The input at each timestamp ti is the concatenation of the multi-hot coding xi of the multilabel categories and the duration di since the last event. In our datasets, this input have as large as 40, 000 dimensions. Thus, the next layer maps into a lower dimensional space. Then, we pass the lower dimensional vector through units of an RNN. The RNN units can be simple RNN units (Le et al., 2015) or more complex recurrent units such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Graves et al., 2009) or Gated Recurrent Units (GRU) (Chung et al., 2014). We also can stack multiple units of RNN on top of each other to increase the representative power of the network. Finally, we use a softmax layer to predict the future codes and a rectified linear unit to predict the time duration until next event.\nDetails of the RNN. Specifically, we implemented our RNN with GRU. Although LSTM has drawn much attention from many researchers, GRU has recently shown to have similar performance as LSTM, while employing a simpler architecture (Chung et al., 2014). In order to precisely describe the network used in this work, we reiterate the mathematical formulation of GRU as follows:\nzi = \u03c3(Wzxi +Uzhi\u22121 + bz)\nri = \u03c3(Wrxi +Urhi\u22121 + br)\nh\u0303i = tanh(Whxi + ri \u25e6Uhhi\u22121 + bh) hi = zi \u25e6 hi\u22121 + (1\u2212 zi) \u25e6 h\u0303i\nwhere zi and ri respectively represent the update gate and the reset gate, h\u0303i the intermediate memory unit, hi the hidden layer, all at time stamp ti. For predicting the diagnosis codes and the medication codes at each time stamp ti, a Softmax layer is stacked on top of the GRU, using the hidden layer hi as the input: y\u0302i+1 = softmax(Wcode\n>hi + bcode). For predicting the time duration until the next visit, a rectified linear unit (ReLU) is placed on top of the GRU, again using the hidden layer hi as the input: d\u0302i+1 = max(wtime\n>hi + btime, 0). The objective of training our model is to learn the weights W{z,r,h,code}, U{z,r,h}, b{z,r,h,code}, wtime and btime. The values of all W \u2019s and U \u2019s were initialized to orthonormal matrices using singular value decomposition of matrices generated from the normal distribution (Saxe et al., 2013). The initial value of wtime was chosen from the uniform distribution between \u22120.1 and 0.1. All b\u2019s and btime were initialized to zeros. The joint loss function consists of the cross entropy for the code prediction and the squared loss for the time duration prediction, as described below for a single patient:\nL(W ,U , b,wtime, btime) = n\u22121\u2211 i=1 {( yi+1 log(y\u0302i+1) + (1\u2212 yi+1) log(1\u2212 y\u0302i+1) ) + 1 2 \u2016di+1 \u2212 d\u0302i+1\u201622 } As mentioned above, the multi-hot vectors xi of almost 40,000 dimensions are first projected to a lower dimensional space, then put into the GRU. We employed two different approaches for this: (1) We put an extra layer of a certain size between the multi-hot input xi and the GRU, and call it the embedding layer. We denote the weight matrix between the multi-hot input vector and the embedding layer as Wemb. Then we learn the weight Wemb as we train the entire model. (2) We initialize the weight Wemb with a matrix generated by Skip-gram algorithm (Mikolov et al., 2013), then refine the weight Wemb as we train the entire model. This can be seen as using the pre-trained Skip-gram vectors as the input to the RNN and fine-tuning them with the joint prediction task. The brief description of learning the Skip-gram vectors from the EHR is provided in Appendix A. The first and second approach can be formulated as follows:\nh (1) i = [tanh(xi >Wemb + bemb), di] (1) h (1) i = [xi >Wemb, di] (2)\nwhere [\u00b7, \u00b7] is the concatenation operation used for appending the time duration to the multi-hot vector h(1)i to make it an input vector to the GRU."}, {"heading": "3 RELATED WORK", "text": "In this section, we briefly overview the common approaches to modeling multilabel event data with special focus on the models that have been applied to medical data.\nDiscretization vs Continuous-time modeling. There are two main approaches to modeling point process data: with or without discretization (binning) of time. When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015). However, this approach is inefficient as it produces long time series whose elements are mostly zero. Furthermore, discretization of time introduces noise in the time stamps of visits. Finally, these models are often not able to model the duration until next event. Thus, it is advantageous not to discretize the data both in terms of modeling and computation.\nContinuous-time models. Among the continuous-time models, there are two main techniques: continuoustime Markov chain based models (Nodelman et al., 2002; Foucher et al., 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al., 2013; Linderman & Adams, 2014; Choi et al., 2015). The latter has been shown to have computational advantages over the former. Moreover, modeling multilabel marked point processes with continuoustime Markov chains expands their state-space and make them even more expensive.\nHowever, Hawkes processes only depend linearly on the past observation times; while there are limited classes of non-linear Hawkes process (Zhu, 2013), the temporal dynamics can be more complex. Moreover, there is no scalable multi-label extension for Hawkes processes. Finally, Hawkes processes are known to have a flat loss function near optimal value of the parameters which renders the gradient-based learning algorithms inefficient (Veen & Schoenberg, 2008). In this paper we address these challenges by designing a recurrent neural network which has been shown to be successful in learning complex sequential patterns."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we describe the details of our experiments, the datasets that we have used and the baselines. Throughout this section, we demonstrate the success of the proposed approach in forecasting the future events of the patients."}, {"heading": "4.1 DATASET DESCRIPTION", "text": "We use a health records dataset provided by Sutter Health; its basic statistics are summarized in Table 1.\nPopulation and source of data The source population for this study was primary care patients from Sutter Palo Alto Medical Foundation (PAMF) Clinics, a multispecialty group practice with large primary care practices that has used EHR for more than 8 years. The dataset was extracted from a case-control study for heart failure nested within Sutter-PAMF. The dataset consists of encounter orders, medication orders, problem list records and procedure orders.\nData processing For input, we used diagnosis codes, medication codes, and procedure codes. Diagnosis codes, which are presented in the ICD-9 format, could be found in the encounter orders, medication orders, problem list records and procedure orders. Medication and procedure codes could be found in medication orders and procedure orders respectively. We extracted all diagnosis, medication and procedure codes from the dataset for each patient, and laid them out in a temporal order. If a patient received multiple codes in a single visit, those codes were assigned the same timestamp. By excluding patients that made less than two visits, we were left with 263,706 patients who made on average 54.61 visits per person.\nMedical code grouping The number of ICD-9 diagnosis codes are approximately 11,000. The number is approximately 18,000 for medication codes. Many codes in this set are very granular; for example, pulmonary tuberculosis (ICD-9 code 011) is divided into 70 subcategories (ICD-9 code 011.01, 011.02, ..., 011.95, 011.96). In a practical perspective, however, simply knowing that a patient is likely to have pulmonary tuberculosis is enough to increase the doctor\u2019s awareness of the severity of the clinical situation. Therefore, in order to predict future diagnosis codes and medication codes,we group the codes into a higherorder codes to decrease the granularity among the codes that we predict. For the diagnosis codes, we use the 3-digit ICD-9 code system, where the the number of unique codes are 1,183 in our dataset. For the medication codes, we use the Sutter in-house medication grouper, which groups the medication codes into 595 unique codes. Therefore, the future code prediction problem reduces from approximately a 29,000 class classification to a 1,778 class classification. The yi in Figure 2 is the 1,778-dimensional vector representing the grouped diagnosis codes and medication codes.\nTraining specifics. For training all models including the baselines, we used 85% of the patients as the training set and 15% as the test set. We did not use a validation set as we fixed the hyperparameters to reasonable values as described in the following paragraphs. We used dropout between the GRU layer and the prediction layer (i.e. code prediction and time duration prediction). Dropout was also used between GRU layers if we were using a multi-layer GRU. We also apply norm-2 regularization on both Wcode and wtime. The both regularization parameters were set to 0.001.\nThe size of the hidden layer hi of the GRU was set to 2000. After sets of preliminary experiments where we tried the size from 100 to 2000, we noticed that the code prediction performance saturated around 1600\u223c1800. Therefore we chose 2000 to guarantee the best performance. The model was implemented with Theano (Bastien et al., 2012) and trained on a machine equipped with two Nvidia Tesla K80 GPUs."}, {"heading": "4.2 EVALUATION METRICS", "text": "We use the following metrics for evaluating the performance of the algorithms in predicting the codes and the time duration until next visit.\nTop-k recall. In order to evaluate the performance of algorithms for forecasting next events, we use the top-k recall measure defined as follows:\ntop-k recall = # of true positives in the top k predictions\n# of true positives\nThis metric is consistent with the differential diagnosis framework where the machine suggests k possible codes and we measure the fraction of true codes that are correctly retrieved. We choose k = 10, 20, 30 because as shown in Table 1 on average every visit includes more than three codes. Thus, selecting small k may result in inaccurate evaluation.\nCoefficient of determination or R2 is a metric for evaluation of predictive performance of regression and forecasting algorithms. It compares the accuracy of prediction with respect to simple prediction by mean of the target variable.\nR2 = 1\u2212 \u2211 i (yi \u2212 y\u0302i) 2\u2211\ni (yi \u2212 yi) 2\nGiven the fact that the time duration varies significantly over time and the interest in accurately predicting the durations decreases as the patients visit after a long period of time, we measure the R2 performance of the algorithms in predicting log(di) to lower the impact of anomalous long durations in the performance metric. In the same spirit, we train all models to predict the logarithm of the time duration between visits."}, {"heading": "4.3 BASELINES", "text": "We compare our model against several baselines as described below. Some of the existing techniques based on CTMC and latent space models were not scalable enough to be trained in the entire dataset in a reasonable amount of time; thus comparison could not be fair.\nIntuitive baselines. We compare our algorithms against simple baselines that are based on experts\u2019 intuition about the dynamics of events in clinical settings. The first baseline is to use a patient\u2019s medical codes in his last visit as the prediction for his current visit. This baseline is competitive when the status of a patient with a chronic condition stabilizes over time. We can make this baseline stronger, by using the top-k most frequent labels observed in visits prior to the current visits. In the experiments we observe that the second baseline is quite competitive.\nLogistic and Neural Network time series models. A common way to perform prediction task is to use (xi\u22121, di\u22121) to predict the codes in the next event xi. We can use logistic regression or multilayer preceptron (MLP). To make this baseline stronger, we can use the data from L time lags before and concatenate the (xi\u2212`, di\u2212`) for ` = 1, . . . , L to create the features for prediction of xi. Similarly, we can have a model that predicts the time until next event using rectified linear units as the output activation. While increasing the number of lags allows the model to capture longer history, it results in two disadvantages compared to RNNs: it increases the number of parameters of the model and also prevents the model to be used for prediction of the first L visits of the patients. Because of this limitation, we do not choose L to be bigger than 5 because the model loses its practicality for patients with short visit history. Due to lack of space, we describe the details of MLP design in Appendix B."}, {"heading": "4.4 RESULTS", "text": "Table 2 compares the results of different algorithms with RNN based Doctor AI. We report the results in three settings: when we are interested in (1) only predicting disease codes (Dx), (2) only medication codes (Rx), and (3) jointly predicting Dx, Rx, and time to next visit. The results confirm that the proposed approach is able to outperform the baseline algorithms by a large margin. Note that the recall values for the joint task are lower than those for single Dx or Rx prediction because the hypothesis space is larger for the joint prediction\ntask. Comparing RNN-based and most frequent past pattern algorithm with the lagged multilayer perceptron algorithm, we postulate that the status of the patients in this dataset depends on more than 5 lags. This can be because this dataset is collected for study of heart failure which shows long-term dynamics.\nThe results also suggest that increasing the number of lags in the logistic and MLP approaches may not significantly improve the performance. Given the size of the dataset, this can be due to overparameterization of the model that may push the models to the boundary of high-dimensionality. While we use L1 regularization in both cases, the result indicates that it cannot fully prevent noise accumulation due to noisy input dimensions.\nTable 2 confirms that learning patient representation with RNN is easier with the input vectors that are already efficient representations of the medical codes. The RNN trained with the Skip-gram vectors (denoted by RNN-IR) consistently outperforms the RNN that learns the weight matrix Wemb directly from the data, with only one exception, the medication prediction Recall@30, although the differences are insignificant. The results also confirm that having multiple layers when using RNN improves its ability to learn more efficient representations. The results also indicate that a single layer RNN might have enough representative power to capture the dynamics of medications, and adding more layers may not improve the performance.\nThe results also indicate that our approach significantly improves the accuracy of predicting the time duration until the next visit compared to the baselines. However, the absolute value of R2 metric shows that accurate prediction of time intervals remains as a challenge. We believe achieving significantly better time prediction without extra features should be difficult because the timing of a clinical visit can be affected by many personal factors such as financial status, location of residence, means of transportation, and life style, to name a few. Thus, without such sensitive personal information, which is rarely included in the EHR, accurate prediction of time intervals should be unlikely.\nIn order to study the applicability of our model in a real-world setting where patients have varying length of medical records, we conducted an additional experiment to study the relationship between the length of the patient medical history and the prediction performance. To this end, we selected 5,800 patients from the test set who had more than 100 visits. We used the best trained model to predict the diagnosis codes at visits at different times and found the mean and standard error of recall across the selected patients. Figure 3 shows the result of the experiment. We believe that the increase in performance can be due to two reasons: (1) RNN is able to learn a better estimate of the patient status as it sees longer patient records and (2) the patient\u2019s status stabilizes over time and the prediction task becomes easier.\nFigure 3: Prediction performance of Doctor AI as it sees a longer history of the patients. The increase in performance can be due to two reasons: (1) RNN is able to learn a better estimate of the patient status as it sees longer patient records and (2) the patient\u2019s status stabilizes over time and the prediction task becomes easier.\n5 10 20 30 40 50 60 70 80 90 100 Number of Visits\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nR ec\nal l\nTop-10 SE Top-10 Mean Top-20 SE Top-20 Mean Top-30 SE Top-30 Mean\nAnother interesting experiment was conducted to evaluate how the performance of Doctor AI changes as the duration since the last visit of the patient increases. To find this relationship, we create pairs of (di, ri) where di = ti\u2212 ti\u22121 and ri denote the duration before and the recall for estimation of ith visit in a patient. We find the non-linear correlation between di and ri using Spearman\u2019s \u03c1 coefficient. The corresponding correlation coefficients are \u22120.1066, \u22120.1119, and \u22120.1081 for top 10, 20, and 30 recalls. The negative sign confirms our hypothesis that the longer it has been since a patient last visited a hospital, the more difficult it becomes to predict the outcome of his/her next visit.\nIn order to take a closer look at the performance of Doctor AI, in Table 3 (in Appendix C) we list the predicted, true, and historical diagnosis codes for five visits of different patients. The blue items represent the correct predictions. The results are promising and show that, given the history of the patient, the Doctor AI can predict the true diagnostic codes. The results highly mimic the way a human doctor will interpret the disease predictions from the history. For all five of the cases shown in Table 3, the set of predicted diseases contain most, if not all of the true diseases. For example, in the first case, the top 3 predicted diseases match the true diseases. A human doctor would likely predict similar diseases to the ones predicted with Doctor AI, since old myocardial infarction and chronic ischemic heart disease can be associated with infections and diabetes (Stevens et al., 1978). In the fourth case, visual disturbances can be associated with migraines and essential hypertension (Keith et al., 1939). Further, essential hypertension may be linked to cognitive function (Kuusisto et al., 1993), which plays a role in anxiety disorders and dissociative and somatoform disorders. Regarding codes that are guessed incorrectly with the fourth case, they can still be plausible given the history. For example, cataracts, and disorders of refraction and accomodation could have been guessed based on a history of visual disturbances, as well as strabismus and disorders of binocular eyemovements. Allergic rhinitis could have been guessed, because there was a history of allergic rhinitis. In summary, Doctor AI is able to very accurately predict the true diagnoses in the sample patients. The results are promising and should motivate future studies involving the application of Doctor AI on different datasets exhibiting other populations of patients."}, {"heading": "5 CONCLUSION", "text": "In this work, we proposed Doctor AI, which is able to analyze patient records and predict the future outcomes for patients. We demonstrated that using a system based on recurrent neural networks, we can achieve 64.30% recall@10. This is a significant improvement compared to the baselines and indicates that the computational algorithms are on the path to be an assistant in improving the quality of health care. The qualitative analysis by a medical expert confirms that Doctor AI not only mimics the predictive power of human doctors, it also provides diagnostic results that are clinically meaningful in that all the predicted codes\nwere within the boundary of medical possibility, indicating its potential to be used as a differential diagnosis machine.\nThe success of Doctor AI opens up new avenues for further improvement of computational health algorithms. Application of Doctor AI in diverse clinical settings should follow to discover the extent of its power. Furthermore, since many hospitals provide clinical data in various forms such as lab measures and clinical note, successfully incorporating such information into Doctor AI would take it to new level of intelligent medical assistance."}, {"heading": "A LEARNING THE SKIP-GRAM VECTORS FROM THE EHR", "text": "Learning efficient representations of medical codes (e.g. diagnosis codes, medication codes, and procedure codes) may lead to improved performance of many clinical applications. We specifically used Skip-gram (Mikolov et al., 2013) to learn real-valued multidimensional vectors to capture the latent representation of medical codes from the EHR.\nWe processed the Sutter PAMF dataset so that diagnosis codes, medication codes, procedure codes are laid out in a temporal order. If there are multiple codes at a single visit, they were laid out in a random order. Then using the context window size of 5 to the left and 5 to the right, and applying Skip-gram, we were able to project diagnosis codes, medication codes and procedure codes into the same lower dimensional space, where similar or related codes are embedded close to one another. For example, hypertension, obesity, hyperlipidemia all share similar values compared to pneumonia or bronchitis. The trained Skip-gram vectors are then plugged into RNN so that a multi-hot vector can be converted to vector representations of medical codes."}, {"heading": "B DETAILS OF THE BASELINES", "text": "We use a multilayer perceptron with a hidden layer of width 2,000. We use dropout with p = 0.3 and apply L1 regularization to all of the weight matrices. The activation functions in the first and output layers are selected to be sigmoid and softmax functions, respectively. For prediction of time intervals, we used rectified linear units. The regularization parameter has been increased with the number of lags to keep the performance of the model in high-dimensional regimes.\nC DETAILED INSPECTION OF RESULTS\nTa bl\ne 3:\nC om\npa ri\nso n\nof th\ne di\nag no\nse s\nby D\noc to\nrA Ia\nnd th\ne tr\nue fu\ntu re\ndi ag\nno se\ns.\nPr ed\nic te\nd Tr\nue H\nis to ry IC D 9 D es cr ip tio n IC D 9 D es cr ip tio n IC D 9 D es cr ip tio n 41 2 V 58 41 4 27 2 25 0 58 5 42 8 28 5 V 04 V 76 O ld m yo ca rd ia li nf ar ct io n E nc ou nt er fo r ot he r an d un sp ec ifi ed pr oc ed ur es O th er fo rm so fc hr on ic is ch em ic he ar td is ea se D is or de rs of lip oi d m et ab ol is m D ia be te s m el lit us C hr on ic ki dn ey di se as e (C K D ) H ea rt fa ilu re O th er an d un sp ec ifi ed an em ia s N ee d fo rp ro ph yl ac tic va cc in at io n an d in oc ul at io n ag ai ns tc er ta in di se as es Sp ec ia ls cr ee ni ng fo rm al ig na nt ne op la sm s 41 4 41 2 V 58 O th er fo rm so fc hr on ic is ch em ic he ar td is ea se O ld m yo ca rd ia li nf ar ct io n E nc ou nt er fo r ot he r an d un sp ec ifi ed pr oc ed ur es 46 5 25 0 36 6 V 58 36 2 A cu te up pe rr es pi ra to ry in fe ct io ns of m\nul tip\nle or\nun sp\nec ifi\ned si\nte s\nD ia\nbe te\ns m\nel lit us C at ar ac t E nc ou nt er fo ro th\ner an\nd un\nsp ec\nifi ed\npr oc\ned ur es O th er re tin al di so rd er s\nV 07 47 7 78 0 40 1 78 6 49 3 30 0 46 1 53 0 71 9\nN ee\nd fo\nr is\nol at\nio n\nan d\not he\nr pr\nop hy\nla ct\nic m\nea su\nre s\nA lle\nrg ic\nrh in\niti s\nG en\ner al\nsy m\npt om s E ss en tia lh yp er\nte ns\nio n\nSy m\npt om\nsi nv\nol vi\nng re\nsp ir\nat or\ny sy\nst em\nan d\not he\nr ch\nes ts\nym pt\nom s\nA st\nhm a\nA nx\nie ty\n,d is\nso ci\nat iv\ne an\nd so\nm at\nof or\nm di\nso rd\ner s\nA cu\nte si\nnu si tis D is ea se s of es\nop ha\ngu s\nO th\ner an\nd un\nsp ec\nifi ed\ndi so\nrd er\ns of\njo in\nt\nV 07 40 1 78 6 78 2\nN ee\nd fo\nr is\nol at\nio n\nan d\not he\nr pr\nop hy\nla ct\nic m\nea su\nre s\nE ss\nen tia\nlh yp\ner te\nns io n Sy m pt om si nv ol vi ng re sp ir at\nor y\nsy st\nem an\nd ot\nhe r\nch es\nts ym\npt om s Sy m pt om s in vo lv in g sk in an d ot he ri nt eg um en ta ry tis su\ne\n78 2 47 7 V 07 56 4 40 1\nSy m\npt om\ns in\nvo lv\nin g\nsk in\nan d\not he\nri nt\neg um\nen ta\nry tis\nsu e\nA lle\nrg ic\nrh in\niti s\nN ee\nd fo\nri so\nla tio\nn an\nd ot\nhe rp\nro ph\nyl ac\ntic m\nea su\nre s\nFu nc\ntio na\nld ig\nes tiv\ne di\nso rd\ner s,\nno te\nls ew\nhe re\ncl as\nsi fie d E ss en tia lh yp er te ns io n\n45 3 V 58 71 9 V 12 V 43 72 9 71 5 73 3 72 6 45 1\nO th\ner ve\nno us\nem bo\nlis m\nan d\nth ro\nm bo\nsi s\nE nc\nou nt\ner fo\nr ot\nhe r\nan d\nun sp\nec ifi\ned pr\noc ed\nur es\nO th\ner an\nd un\nsp ec\nifi ed\ndi so\nrd er\nso fj\noi nt\nPe rs\non al\nhi st\nor y\nof ce\nrt ai\nn ot\nhe r\ndi se\nas es\nO rg\nan or\ntis su\ne re\npl ac\ned by\not he\nrm ea ns O th er di so rd er s of so ft tis su es O st eo ar th ro si sa nd al lie d di so rd er s O th er di so rd er s of bo ne an d ca rt ila ge Pe ri ph er al en th es op at hi es an d al lie d sy nd\nro m es Ph le bi tis an d th ro m bo ph le bi tis\n71 5 V 12 71 9 V 58\nO st\neo ar\nth ro\nsi sa\nnd al\nlie d\ndi so\nrd er s Pe rs on al hi st or y of ce rt ai n ot he r di\nse as es O th er an d un sp ec ifi ed di so rd er so fj oi nt E nc ou nt er fo r ot he r an d un sp ec ifi ed pr oc\ned ur\nes\n45 3 95 6 V 43\nO th\ner ve\nno us\nem bo\nlis m\nan d\nth ro\nm bo\nsi s\nIn ju\nry to\npe ri\nph er\nal ne\nrv e(\ns) of\npe lv\nic gi\nrd le\nan d\nlo w\ner lim b O rg an or tis su e re pl ac ed by ot he rm ea ns\n47 7 78 0 30 0 40 1 34 6 36 6 V 43 36 7 36 8 27 2\nA lle\nrg ic\nrh in\niti s\nG en\ner al\nsy m\npt om s A nx ie ty ,d is so ci at\niv e\nan d\nso m\nat of\nor m\ndi so\nrd er s E ss en tia lh yp er te ns io n M ig ra in e C at ar ac t O rg an or tis su e re pl ac ed by ot he rm ea ns D is or de rs of re fr ac tio n an d ac co m m od at io n V is ua ld is tu rb an ce s D is or de rs of lip oi d m et ab ol is m\n40 1 78 0 34 6 30 0\nE ss\nen tia\nlh yp\ner te\nns io n G en er al sy m pt om s M ig ra in e A nx ie ty ,d is so ci at iv e\nan d\nso m\nat of\nor m\ndi so\nrd er\ns\n78 2 47 7 69 2 36 8 37 8\nSy m\npt om\ns in\nvo lv\nin g\nsk in\nan d\not he\nri nt\neg um\nen ta\nry tis\nsu e\nA lle\nrg ic\nrh in\niti s\nC on\nta ct\nde rm\nat iti\ns an\nd ot\nhe re\ncz em a V is ua ld is tu rb an ce s St ra bi sm us an d ot he rd is or de rs of\nbi no\ncu la\nre ye\nm ov\nem en\nts\n42 8 42 7 27 2 40 1 78 6 18 5 25 0 41 4 78 8 42 4\nH ea\nrt fa\nilu re\nC ar\ndi ac\ndy sr\nhy th\nm ia s D is or de rs of lip oi d m et\nab ol\nis m\nE ss\nen tia\nlh yp\ner te\nns io n Sy m pt om s in vo lv in g re sp ir\nat or\ny sy\nst em\nan d\not he\nrc he\nst sy\nm pt\nom s\nM al\nig na\nnt ne\nop la\nsm of\npr os\nta te\nD ia\nbe te\nsm el\nlit us\nO th\ner fo\nrm s\nof ch\nro ni\nc is\nch em\nic he\nar td\nis ea se Sy m pt om s in vo lv in g ur in ar y sy st em O th er di se as es of en do ca rd iu m\n25 0 40 2 42 8 27 2 42 7\nD ia\nbe te\nsm el\nlit us\nH yp\ner te\nns iv\ne he\nar td\nis ea se H ea rt fa ilu re D is or de rs of lip oi d m et\nab ol\nis m\nC ar\ndi ac\ndy sr\nhy th\nm ia\ns\n46 6 42 8 78 6 78 5 25 0\nA cu\nte br\non ch\niti s\nan d\nbr on\nch io\nlit is\nH ea\nrt fa\nilu re\nSy m\npt om\ns in\nvo lv\nin g\nre sp\nir at\nor y\nsy st\nem an\nd ot\nhe rc\nhe st\nsy m\npt om s Sy m pt om s in vo lv in g ca rd io va sc ul ar sy st em D ia be te s m el lit us"}], "references": [{"title": "Fast structure learning in generalized stochastic processes with latent factors", "author": ["Bahadori", "Mohammad Taha", "Liu", "Yan", "Xing", "Eric P"], "venue": "In KDD, pp", "citeRegEx": "Bahadori et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bahadori et al\\.", "year": 2013}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A simple algorithm for identifying negated findings and diseases in discharge summaries", "author": ["Chapman", "Wendy W", "Bridewell", "Will", "Hanbury", "Paul", "Cooper", "Gregory F", "Buchanan", "Bruce G"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Chapman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2001}, {"title": "Constructing disease network and temporal progression model via context-sensitive hawkes process", "author": ["Choi", "Edward", "Du", "Nan", "Chen", "Robert", "Song", "Le", "Sun", "Jimeng"], "venue": "In ICDM,", "citeRegEx": "Choi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A semi-markov model for multistate and interval-censored data with multiple terminal events. application in renal transplantation", "author": ["Foucher", "Yohann", "Giral", "Magali", "Soulillou", "Jean-Paul", "Daures", "Jean-Pierre"], "venue": "Statistics in medicine,", "citeRegEx": "Foucher et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Foucher et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML, pp", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Fern\u00e1ndez", "Santiago", "Bertolami", "Roman", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["Heckerman", "David"], "venue": "In UAI,", "citeRegEx": "Heckerman and David.,? \\Q1990\\E", "shortCiteRegEx": "Heckerman and David.", "year": 1990}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bayesian nonparametric hidden semi-markov models", "author": ["Johnson", "Matthew J", "Willsky", "Alan S"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Some different types of essential hypertension: their course and prognosis", "author": ["Keith", "Norman M", "Wagener", "Henry P", "Barker", "Nelson W"], "venue": "The American Journal of the Medical Sciences,", "citeRegEx": "Keith et al\\.,? \\Q1939\\E", "shortCiteRegEx": "Keith et al\\.", "year": 1939}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Essential hypertension and cognitive function", "author": ["Kuusisto", "Johanna", "Koivisto", "Keijo", "L Mykk\u00e4nen", "Helkala", "Eeva-Liisa", "Vanhanen", "Matti", "T H\u00e4nninen", "K Py\u00f6r\u00e4l\u00e4", "Riekkinen", "Paavo", "Laakso", "Markku"], "venue": "the role of hyperinsulinemia. Hypertension,", "citeRegEx": "Kuusisto et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kuusisto et al\\.", "year": 1993}, {"title": "Latent Continuous Time Markov Chains for Partially-Observed Multistate Disease Processes", "author": ["Lange", "Jane"], "venue": "PhD thesis,", "citeRegEx": "Lange and Jane.,? \\Q2014\\E", "shortCiteRegEx": "Lange and Jane.", "year": 2014}, {"title": "A joint model for multistate disease processes and random informative observation times, with applications to electronic medical records", "author": ["Lange", "Jane M", "Hubbard", "Rebecca A", "Inoue", "Lurdes YT", "Minin", "Vladimir N"], "venue": "data. Biometrics,", "citeRegEx": "Lange et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Discovering latent network structure in point process data", "author": ["Linderman", "Scott", "Adams", "Ryan"], "venue": "In ICML, pp", "citeRegEx": "Linderman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Linderman et al\\.", "year": 2014}, {"title": "Multivariate hawkes processes", "author": ["Liniger", "Thomas Josef"], "venue": "PhD thesis, Diss., Eidgeno\u0308ssische Technische Hochschule ETH Zu\u0308rich, Nr", "citeRegEx": "Liniger and Josef.,? \\Q2009\\E", "shortCiteRegEx": "Liniger and Josef.", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Continuous time bayesian networks", "author": ["Nodelman", "Uri", "Shelton", "Christian R", "Koller", "Daphne"], "venue": "In UAI,", "citeRegEx": "Nodelman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2002}, {"title": "The survival filter: Joint survival analysis with a latent time series", "author": ["Ranganath", "Rajesh", "Perotte", "Adler", "Elhadad", "No\u00e9mie", "Blei", "David M"], "venue": "In UAI,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Diabetic cataract formation: potential role of glycosylation of lens crystallins", "author": ["Stevens", "Victor J", "Rouzer", "Carol A", "Monnier", "Vincent M", "Cerami", "Anthony"], "venue": null, "citeRegEx": "Stevens et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 1978}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In NIPS, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects", "author": ["Truccolo", "Wilson", "Eden", "Uri T", "Fellows", "Matthew R", "Donoghue", "John P", "Brown", "Emery N"], "venue": "Journal of neurophysiology,", "citeRegEx": "Truccolo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Truccolo et al\\.", "year": 2005}, {"title": "Estimation of space\u2013time branching process models in seismology using an em\u2013type", "author": ["Veen", "Alejandro", "Schoenberg", "Frederic P"], "venue": "algorithm. JASA,", "citeRegEx": "Veen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Veen et al\\.", "year": 2008}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes", "author": ["Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le"], "venue": "In AISTATS, pp", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Nonlinear Hawkes Processes", "author": ["Zhu", "Lingjiong"], "venue": "PhD thesis,", "citeRegEx": "Zhu and Lingjiong.,? \\Q2013\\E", "shortCiteRegEx": "Zhu and Lingjiong.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "(Heckerman, 1990; Chapman et al., 2001; Lange et al., 2015), most works do not achieve the required accuracy and scalability, lack generality, or need excessive expert domain knowledge.", "startOffset": 0, "endOffset": 59}, {"referenceID": 16, "context": "(Heckerman, 1990; Chapman et al., 2001; Lange et al., 2015), most works do not achieve the required accuracy and scalability, lack generality, or need excessive expert domain knowledge.", "startOffset": 0, "endOffset": 59}, {"referenceID": 21, "context": "The two main class of techniques, continuous-time Markov chain based models (Nodelman et al., 2002; Lange et al., 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al.", "startOffset": 76, "endOffset": 144}, {"referenceID": 16, "context": "The two main class of techniques, continuous-time Markov chain based models (Nodelman et al., 2002; Lange et al., 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al.", "startOffset": 76, "endOffset": 144}, {"referenceID": 3, "context": ", 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al., 2015) are expensive to generalize to nonlinear and multilabel settings.", "startOffset": 113, "endOffset": 158}, {"referenceID": 25, "context": "(Graves, 2013; Graves & Jaitly, 2014; Sutskever et al., 2014; Kiros et al., 2014; Zaremba & Sutskever, 2014).", "startOffset": 0, "endOffset": 108}, {"referenceID": 13, "context": "(Graves, 2013; Graves & Jaitly, 2014; Sutskever et al., 2014; Kiros et al., 2014; Zaremba & Sutskever, 2014).", "startOffset": 0, "endOffset": 108}, {"referenceID": 20, "context": "\u2022 We propose an efficient initialization scheme for RNNs using Skip-gram embedding (Mikolov et al., 2013) and show that it improves the performance of the RNN in our problem.", "startOffset": 83, "endOffset": 105}, {"referenceID": 17, "context": "The RNN units can be simple RNN units (Le et al., 2015) or more complex recurrent units such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Graves et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 8, "context": ", 2015) or more complex recurrent units such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Graves et al., 2009) or Gated Recurrent Units (GRU) (Chung et al.", "startOffset": 78, "endOffset": 131}, {"referenceID": 4, "context": ", 2009) or Gated Recurrent Units (GRU) (Chung et al., 2014).", "startOffset": 39, "endOffset": 59}, {"referenceID": 4, "context": "Although LSTM has drawn much attention from many researchers, GRU has recently shown to have similar performance as LSTM, while employing a simpler architecture (Chung et al., 2014).", "startOffset": 161, "endOffset": 181}, {"referenceID": 23, "context": "The values of all W \u2019s and U \u2019s were initialized to orthonormal matrices using singular value decomposition of matrices generated from the normal distribution (Saxe et al., 2013).", "startOffset": 159, "endOffset": 178}, {"referenceID": 20, "context": "(2) We initialize the weight Wemb with a matrix generated by Skip-gram algorithm (Mikolov et al., 2013), then refine the weight Wemb as we train the entire model.", "startOffset": 81, "endOffset": 103}, {"referenceID": 26, "context": "When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).", "startOffset": 198, "endOffset": 268}, {"referenceID": 0, "context": "When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).", "startOffset": 198, "endOffset": 268}, {"referenceID": 22, "context": "When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).", "startOffset": 198, "endOffset": 268}, {"referenceID": 21, "context": "Among the continuous-time models, there are two main techniques: continuoustime Markov chain based models (Nodelman et al., 2002; Foucher et al., 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al.", "startOffset": 106, "endOffset": 189}, {"referenceID": 5, "context": "Among the continuous-time models, there are two main techniques: continuoustime Markov chain based models (Nodelman et al., 2002; Foucher et al., 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al.", "startOffset": 106, "endOffset": 189}, {"referenceID": 29, "context": ", 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al., 2013; Linderman & Adams, 2014; Choi et al., 2015).", "startOffset": 122, "endOffset": 200}, {"referenceID": 3, "context": ", 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al., 2013; Linderman & Adams, 2014; Choi et al., 2015).", "startOffset": 122, "endOffset": 200}, {"referenceID": 1, "context": "The model was implemented with Theano (Bastien et al., 2012) and trained on a machine equipped with two Nvidia Tesla K80 GPUs.", "startOffset": 38, "endOffset": 60}, {"referenceID": 24, "context": "A human doctor would likely predict similar diseases to the ones predicted with Doctor AI, since old myocardial infarction and chronic ischemic heart disease can be associated with infections and diabetes (Stevens et al., 1978).", "startOffset": 205, "endOffset": 227}, {"referenceID": 12, "context": "In the fourth case, visual disturbances can be associated with migraines and essential hypertension (Keith et al., 1939).", "startOffset": 100, "endOffset": 120}, {"referenceID": 14, "context": "Further, essential hypertension may be linked to cognitive function (Kuusisto et al., 1993), which plays a role in anxiety disorders and dissociative and somatoform disorders.", "startOffset": 68, "endOffset": 91}, {"referenceID": 20, "context": "We specifically used Skip-gram (Mikolov et al., 2013) to learn real-valued multidimensional vectors to capture the latent representation of medical codes from the EHR.", "startOffset": 31, "endOffset": 53}], "year": 2017, "abstractText": "Large amount of Electronic Health Record (EHR) data have been collected over millions of patients over multiple years. The rich longitudinal EHR data documented the collective experiences of physicians including diagnosis, medication prescription and procedures. We argue it is possible now to leverage the EHR data to model how physicians behave, and we call our model Doctor AI. Towards this direction of modeling clinical bahavior of physicians, we develop a successful application of Recurrent Neural Networks (RNN) to jointly forecast the future disease diagnosis and medication prescription along with their timing. Unlike a traditional classification model where a single target is of interest, our model can assess entire history of patients and make continuous and multilabel prediction based on patients\u2019 historical data. We evaluate the performance of the proposed method on a large real-world EHR data over 250K patients over 8 years. We observe Doctor AI achieves up to 79% recall@30, significantly higher than several baselines.", "creator": "LaTeX with hyperref package"}}}