{"id": "1511.06104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction", "abstract": "the commercially recent early promising achievements of deep learning rely predominantly on the large amount of labeled data. considering the abundance of data on the web, most of them don't have labels at all. therefore, it is constantly important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. in this work, we revisit graph - based semi - supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. we consider an em - path like algorithm for semi - supervised learning on deep neural database networks : in forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. we demonstrate the strength of our online approach compared to the usually conventional ones algorithms whose graph is constructed on fuzzy static loops but not robust enough feature representations beforehand.", "histories": [["v1", "Thu, 19 Nov 2015 09:44:57 GMT  (737kb,D)", "http://arxiv.org/abs/1511.06104v1", null], ["v2", "Tue, 19 Jan 2016 00:56:08 GMT  (0kb,I)", "http://arxiv.org/abs/1511.06104v2", "As the original submission of iclr is withdrawn, the arxiv submission should be withdrawn as well"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["sheng-yi bai", "sebastian agethen", "ting-hsuan chao", "winston hsu"], "accepted": false, "id": "1511.06104"}, "pdf": {"name": "1511.06104.pdf", "metadata": {"source": "CRF", "title": "ONLINE GRAPH CONSTRUCTION", "authors": ["Sheng-Yi Bai", "Sebastian Agethen", "Ting-Hsuan Chao"], "emails": ["r02922161@ntu.edu.tw", "d01944015@ntu.edu.tw", "r02922047@ntu.edu.tw", "whsu@ntu.edu.tw"], "sections": [{"heading": null, "text": "The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand."}, {"heading": "1 INTRODUCTION", "text": "Recently, deep learning has shown big success on image (Szegedy et al., 2015), video (Karpathy et al., 2014), text (Severyn & Moschitti, 2015), speech (Sak et al., 2014), etc. Specifically, convolution neural networks (CNNs) which combine the learning of feature representation and classifier have become the most common practice for image and video recognition problems in recent work. For example, Krizhevsky et al. (2012); Zeiler & Fergus (2014); Szegedy et al. (2015) showed promising results on ImageNet, which is a manually annotated large-scale dataset. However, it is infeasible to collect a large-scale dataset with high-quality images for every specific application. At the same time, with the explosive growth of modern web services and social media, there are tons of images with weak or missing labels. Considering that the trend of deep networks is to go deeper, larger and more complex, it is important to utilize those imperfect labeled data for scaling things up.\nIn this work, we focus on leveraging those unlabeled data for supervised tasks in semi-supervised fashion. We revisit the graph-based semi-supervised approach, and propose a technique to construct the graph online simultaneous with CNN training. The cluster assumption of semi-supervised learning (Chapelle et al., 2006) states that the decision boundary should not cross high-density regions, and points in the same cluster are more likely have the same label. Semi-supervised learning methods aim to propagate down existing label information to regions lacking annotations relying on the structural density among data points. To model the structural density, graph is thus constructed to measure the proximity (similarity) between data points in graph-based semi-supervised learning. However, it does not scale well due to the computational complexity. We propose an online graph construction technique to construct the graph on a local scope of data (i.e., mini-batch) in CNN. With the graphs constructed during learning process, they not only scale better to large-scale datasets but also base on better feature representations which evolve during CNN training. In addition, as our approach is simple to apply, it can serve as a framework suitable for semi-supervised learning for CNNs with other effective graphs.\nar X\niv :1\n51 1.\n06 10\n4v 1\n[ cs\n.N E\n] 1\n9 N\nov 2\nIn the experiments, we discuss the factors of our approach and evaluate on MNIST and CIFAR-10 datasets, which have been widely used for testing semi-supervised learning. We demonstrate the strength of constructing graphs over both data and labels in contrast to those based on data solely. Based on kNN graph and spectral label refinement (Song et al., 2015), we show the advantage of our online approach compared to the others trained with offline constructed graphs on static feature representations. We conclude by inspecting the impact of the number of clusters in spectral label refinement with CNNs on classification performance."}, {"heading": "2 RELATED WORK", "text": "As literature on semi-supervised learning is abundant, we focus on the key work which are more specific to deep learning or inspiring to this work in this section.\nRecently, several papers addressed the issues of training on noisy labeled image data via weaklysupervised approaches. Mnih & Hinton (2012) trained deep neural networks for learning to label aerial images and proposed two robust loss functions to deal with the incomplete labeling and error registration issues. Sukhbaatar et al. (2015) developed a bottom-up approach to solve the possible issues of training on noisy labeled data by adding a linear layer in CNN to model the noise distribution. Reed et al. (2015) proposed a bootstrapping loss function with special consideration to prediction consistency, such that similar points lead to the same prediction result.\nOn the other hand, some work proposed automatic systems for learning visual knowledges based the abundance of web data. Chen et al. (2013) developed the NEIL (Never Ending Image Learner) system for automatic image understanding from web data by using a semi-supervised learning algorithm to discover the common semantic relationship and the labels of a given category. Likewise, Divvala et al. (2014) developed an automatic system to learn a wide variety of variations (e.g., actions, attributes, etc.) for any given concept.\nIn contrast to recent deep approaches, early work in semi-supervised learning relied on shallow techniques. Fergus et al. (2009) proposed an approach whose computational time is linear to the number of images to obtain approximations for semi-supervised learning based on normalized graph Laplacian. Guillaumin et al. (2010) aimed at multimodal semi-supervised learning and proposed using both image contents and text information to train a multiple kernel learning classifier and then applying the classifier to unlabeled data.\nAs the rising of deep neural networks, more recent work proposed approaches combined with the strength of them. Weston et al. (2008) proposed using semi-supervised embeddings based on the contrastive loss function (Hadsell et al., 2006) for semi-supervised learning on deep neural networks. However, the approach relies on the graph which is offline constructed. Lee (2013) followed the idea of entropy regularization (Grandvalet & Bengio, 2004) and proposed using \u201cPseudo-Labels\u201d as the true labels of unlabeled data to train neural networks. Without using graphs, they achieved the same goal as Weston et al. (2008). Dosovitskiy & Springenberg (2014) focused on unsupervised feature learning and proposed pre-training a CNN by treating each unlabeled image as its own class. They then applied random transformations onto images for data augmentation to learn invariant feature representations, and used the learned models for supervised tasks."}, {"heading": "3 GRAPH-BASED SEMI-SUPERVISED LEARNING", "text": "Our goal is to leverage unlabeled data for supervised classification tasks faced with both labeled data pairs (XL,YL) = {(x1, y1) , (x2, y2) , . . . , (xl, yl)} and unlabeled data XU = {xl+1, xl+2, . . . , xn}, where xi \u2208 RD and yi \u2208 {1, . . . , C}. To take advantage of unlabeled data, in graph-based semi-supervised learning, a graph G = (V,E) is built where the vertices V are all available data points x1, . . . , xn, and the edges E are represented by an n\u00d7 n adjacency matrix W . Entry Wij represents the edge weight between node i and node j, and therefore the similarity or dissimilarity in between. The detail discussion will be in the following paragraphs. In this section, we first talk about the benefits of online graph construction, then introduce the graphs we use in this work, and finally expose the overall loss function with the combination of Pseudo-Label (Lee, 2013) technique."}, {"heading": "3.1 ONLINE VS. OFFLINE GRAPH CONSTRUCTION", "text": "There exist many heuristics for constructing the adjacency matrix, for example, (1) sharing a higher value of weight between adjacent frames than other frames in video (Weston et al., 2008), and (2) setting image pairs of different views from the same object/identity Wij = 1, and otherwise Wij = 0 (Bell & Bala, 2015; Khalil-Hani & Sung, 2014; Lin et al., 2015). However, these kinds of graph construction are done before the learning process starts, that is, serving as an input for preprocessing or training, and therefore we regard them as offline graph construction in contrast to the technique we propose in the following.\nOur proposal is to construct the graph online in a local scope, i.e., on a few samples of X, which is practically equivalent to the concept of mini-batch in the training of modern neural networks. Namely, in each forward pass, an adjacency matrix W \u20321 is constructed on a mini-batch of both labeled and unlabeled data, and W \u2032 is later used as a kind of label for loss calculation. Graph construction is generally expensive in terms of time: Taking kNN graph as an example, to compute an exact kNN graph in brute-force fashion takes O(n2) time for n data points, though more efficient methods exist (Chen et al., 2009; Zhang et al., 2013). Nevertheless, the computational complexity makes kNN graph infeasible to scale to large-scale (say 1M) datasets. By chunking data into small subsets with equal size (i.e., mini-batch), graph construction is thus more efficient and also affordable for online computation simultaneous with CNN training. Since a very limited portion of W is actually used in each batch for CNN training, we believe that W \u2032, constructed on a local scope of data points, still holds the cluster assumption and is as functional asW in this case (see Section 4.2).\nIn addition, graph construction is often based on some kind of embedding (feature representation) of data points rather than the data itself when the data is complicated (e.g., image, text, speech, etc.). However, in terms of image classification problems, there is no generic and robust feature descriptor available yet. Though CNN is becoming stronger, domain shift problem still exists (Donahue et al., 2014). We argue that if the feature representations are not robust enough, the graph based on them might mislead the learning process and hurt our goal of improving supervised tasks with unlabeled data, and justify our argument in Section 4.2. Regarding our online graph construction for neural network, it works like an EM-algorithm: In forward pass, W \u2032 is constructed based on the output of the network in current state, and W \u2032 is then used for loss calculation to help update the network by back propagation in the backward pass. This property enables us to have a better and evolving adjacency matrix, which is crucial to graph-based semi-supervised learning, compared to the one constructed offline based on static but not robust enough representations."}, {"heading": "3.2 APPROACHES TO CONSTRUCT ADJACENCY MATRICES", "text": "Weston et al. (2008) explored several semi-supervised embedding algorithms and finally used contrastive loss function (Hadsell et al., 2006) based on the argument that it is more suitable for gradient descent optimization compared to the others. Also, the contrastive loss function is still widely used in recent deep neural network works such as invariant feature learning (Bell & Bala, 2015; KhalilHani & Sung, 2014; Lin et al., 2015; Wang et al., 2015). Motivated by Weston et al. (2008), we hope to regularize the supervised task by adding a semi-supervised loss term as an auxiliary task to the network. In other words, we focus more on regularizing the classifier which is learned in a supervised fashion with few labeled examples. Combining the online graph construction we propose, it is natural to use the network output as the feature representations for graph construction. Therefore we apply the contrastive loss function on the adjacency matrix W for a pair of data represented as follows:\nLcont(fi, fj ,Wij) = 1\n2 { \u2016fi \u2212 fj\u201622 if Wij = 1 max (0,m\u2212 \u2016fi \u2212 fj\u20162) 2 if Wij = 0 , (1)\nwhere fi is the network output of xi. The effect of contrastive loss is to draw similar examples closer to one another, and push away dissimilar ones to a distance of at least m.\nClearly, W for the contrastive loss function is binary, which is either similar or dissimilar between nodes i, j. Here comes a simple and common practice for constructing a graph for such W , kNN graph, where Wij is 1 if fj is one of fi\u2019s k-nearest neighbors, and 0 otherwise2. We will show\n1To distinguish from the offline one W , but we may use both interchangeably in the paragraphs below. 2Here we just talk about one kind of construction methods of kNN, but many variations exist.\nthat even such simple graph like kNN graph still benefits from our online construction approach on performance. Aside from graphs solely based on data, we seek a kind of graph which is not only based on the data but also the labels. Song et al. (2015) provided a label refinement algorithm to adjust the labels for datasets with noisy or missing labels, and showed great success on several text benchmarks. We find that the algorithm can be helpful to construct graphs with higher quality as the label information of labeled samples can be utilized now.\nThe effectiveness of this algorithm on image data will be investigated in our experiments by terms of clustering evaluation. Here, we revisit the algorithm first and then show how we make use of it for online graph construction and the contrastive loss. For brevity, we omit some of the deduction, and refer the readers for details to Song et al. (2015). Several notations need to be defined first: S is an adjacency matrix (to distinguish from W above) based on data with Sij = exp (\u2212 \u2016fi \u2212 fj\u20162/2\u03c3i\u03c3j), where \u03c3i is the distance from fi to its bk/2c-th nearest neighbor. Let D be a diagonal matrix whose diagonal elements are given by Dii = \u2211 j Sij and the normalized graph Laplacian is defined as L\u0304 = I \u2212D\u22121/2SD\u22121/2, where I is an identity matrix. Similarly, a matrix B of a graph based on the similarity and dissimilarity of labels is defined as\nBij =  a if yi = yj \u2212b if yi 6= yj 0 otherwise , (2)\nwhere a and b are the coefficients that control the balance of similarity and dissimilarity. With the diagonal matrix DB,ii = \u2211 j |Bij |, B\u0304 = D \u22121/2 B BD \u22121/2 B . Then Song et al. (2015) proposed using the data-label smoothness ratio criterion as follows:\nf\u2217 = argmax s.t. fT f=1\nfT B\u0304f fT L\u0304f , (3)\nwhere f are the relaxed labels, to obtain better labels. This leads to a generalized eigenvalue decomposition problem,\nB\u0304f\u2217k = \u03bb \u2217 kL\u0304f \u2217 k, (4)\nwhere \u03bb\u2217k is the k-th largest eigenvalue with the corresponding eigenvector f \u2217 k, and f \u2217 k constitutes the k-th column vector of the matrix F \u2217. Then F \u2217 is used to obtain an optimal indicator/partition matrix H \u2208 RN\u00d7K whose elements are Hij = 1 if node i belongs to j-th class, with \u2211 j Hij = 1, N is the number of data points, and K is the number of clusters, by discretization. After H is obtained, we simply construct W similar to B, i.e.,\nWij = { 1 if y\u0302i = y\u0302j 0 otherwise , (5)\nwhere y\u0302i = c if Hic = 1, and y\u0302i \u2208 {1, . . . ,K}. According to our tests, the spectral label refinement algorithm in practice takes more than cubic time in terms of N , when N is larger than 1K, which means it is totally infeasible for large scale datasets3. In contrast, with our online graph construction, spectral label refinement is manageable and can be used in CNN training, showing its strength even on image data."}, {"heading": "3.3 SEMI-SUPERVISED LOSS FUNCTION", "text": "Now that we have all the ingredients we need, the loss function of network becomes\nL = l\u2211 i=1 Lc (fi, yi) + \u03bb n\u2211 i,j=1 Lcont (fi, fj ,Wij) (6)\nin general, where Lc is classification loss (softmax loss is used often), fi is the network output of xi, and \u03bb is the balancing term. But we hope for the number of similar and dissimilar pairs to be balanced, and describe our approach as follows: In each batch, for every example i, we randomly\n3Probably, there is room for further optimization, but the bottleneck lies in the eigenvalue decomposition.\nsample two examples j, k with the constraints that Wij = 1 and Wik = 0. Then we rewrite the loss function in perspective of the mini-batch:\nL = l\u2032\u2211 i=1 Lc (fi, yi) + \u03bb n\u2032\u2211 i=1 (Lcont (fi, fj ,Wij) + Lcont (fi, fk,Wik)), (7)\nwhere l\u2032, n\u2032 are respectively the number of labeled samples and all samples in the mini-batch.\nApart from our graph-based approach, Lee (2013) proposed Pseudo-Labels of unlabeled data combined with conventional supervised loss term, by labeling the unlabeled data with the class which has the maximum predicted probability as their true labels. The maximum a posteriori probability (MAP) estimate encourages this approach to have the decision boundary in low density regions which has the same goal of our approach under the cluster assumption. The major difference between the Pseudo-Label approach and ours lies on individual or group. Namely, Pseudo-Label encourages every single example to have an extreme 1-of-C prediction (network output), while our approach relies on the relationship between a pair of examples, and consequently the two approaches are complementary to each other. Therefore, we can extend Eq. (7) to have Pseudo-Label integrated in as follows:\nL\u2032 = l\u2032\u2211 i=1 Lc (fi, yi) +\u03b1 n\u2032\u2211 i=l\u2032+1 Lc (fi, y\u0303i) +\u03bb n\u2032\u2211 i=1 (Lcont (fi, fj ,Wij) + Lcont (fi, fk,Wik)), (8)\nwhere \u03b1 is the balancing term for Pseudo-Label, and y\u0303i = argmaxc\u2208{1,...,C} fi,c is the PseudoLabel for unlabeled example i."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "In our experiments, we pre-train two CNNs with labeled data only in a supervised fashion. For MNIST, there are 60,000 training images. We use 600 labeled samples for supervised training and regard the remaining data as unlabeled for later usage. Similarly, 50,000 training images are available in CIFAR-10, and 10,000 of them are used for supervised training. Both of the two datasets have 10,000 testing images for our evaluation. Our implementations are based on Caffe (Jia et al., 2014), and the reference network architectures for MNIST and CIFAR-10 are LeNet (LeCun et al., 1998) and Caffe\u2019s CIFAR10-quick respectively. In the following experiments, \u03b1 and \u03bb is set to 1 and 0.1 respectively, and the mini-batch size is fixed to 100. The a and b in spectral label refinement is empirically set to 1 and 0.001 respectively as in Song et al. (2015). In our figures, \u201cbaseline\u201d refers to the pre-trained model using labeled data only. Except baselines, all the other models are trained on both labeled and unlabeled data in semi-supervised fashion. \u201cKNN\u201d refers to training with W based on kNN graph, \u201cSLR\u201d refers to training with W based on spectral label refinement, and the methods with \u201c+PL\u201d means Eq. (8) is used as the network objective.\nWe first demonstrate the utilization of label information of spectral label refinement technique compared to traditional spectral clustering which is solely based on data (features). Then we illustrate the effectiveness of our methods, and inspect the effect of choice of the number of clusters in spectral label refinement at the end of the section."}, {"heading": "4.1 SPECTRAL CLUSTERING VS. SPECTRAL LABEL REFINEMENT", "text": "For comparison on clustering results, Normalized Mutual Information (NMI) is chosen to be the metric. NMI score is 1 when the result is perfect, and 0 when fully random. We vary the missing label rate to inspect the strength of spectral label refinement which takes labels into account. To fit our CNN training protocols, we extract deep features on the test set of both MNIST and CIFAR-10 based on our pre-trained networks. We then randomly select 100 samples, label a fixed fraction of those samples as unlabeled, and repeat 50 times to average the NMI scores. The number of clusters is set to be the exact number of classes, i.e., 10 for both datasets. Figure 1 shows that spectral label refinement provides better clustering results even on extreme cases (e.g., few samples with high missing label rate as 0.8). The major difference between the clustering experiments on MNIST and CIFAR-10 is the feature quality which could be inferred from the classification accuracies of networks. We find that when the features are not robust enough (compare the CIFAR-10 case with\nthe MNIST case), label information helps a lot and plays an important role for achieving better clustering results. We believe that this strength of spectral label refinement also produces better matrix W in our online graph construction during CNN training."}, {"heading": "4.2 EVALUATION OF OUR ONLINE GRAPH CONSTRUCTION METHODS", "text": "We mix labeled and unlabeled data to conduct the following experiments in semi-supervised fashion for all methods as mentioned in Section 3, with initial weights from our pre-trained models. In this subsection, we first show the advantage of the online graph construction during CNN training over the pre-construction before CNN training. We perform offline graph construction of kNN graph and spectral label refinement based on the features of the whole training set, mixing labeled and unlabeled data, which are extracted from our pre-trained models to obtain the matrixW beforehand. The offline kNN graphs are constructed on the whole training set at once. However, due to the time complexity of spectral label refinement technique, it is infeasible to construct the graph on a large amount of data points, e.g., 50,000. We therefore decide to use a smaller scope to form local clusters on 2000 data points each time, and iterate the process until the whole training set is done. As a result, in terms of spectral label refinement, the difference between offline and online approach lies on the local scope size (2000 vs. 100) and whether the deep representations are static or not.\nIn Figure 2, it is clear that, with unlabeled data introduced, all the semi-supervised approaches have better performance than the baseline. We can observe that without the continuous learning of deep representations, precomputing matrix W in an offline fashion leads to worse performance than our online approaches. Although more samples used in graph construction could have smoother and better clustering results, underlying feature representation is also critical. When graph-based methods are solely used, they are comparable to or slightly better than Pseudo-Label (Lee, 2013). In addition, combined with Pseudo-Label (PL), our methods can make steps towards better performance. As the spectral label refinement technique (Song et al., 2015) is to refine labels based on data similarity and labels with certain confidence, features that are not robust enough could hurt the performance. This explains why SLR-based methods perform relatively better on MNIST than on CIFAR-10 when compared with PL and KNN-based methods.\nFigure 4 shows that the semi-supervised methods make class boundary more clear than that of the baseline model. Pseudo-Label benefits from MAP but could make things worse as well. For example, bird dots in Figure 4b lie in the boundary of deer, dog and airplane, and do not form an obvious cluster. By contrast, even though ambiguities still exist, our KNN+PL and SLR+PL methods form better clusters which lead to better performance in terms of accuracy. To summarize, Figure 2 suggests that methods based on online graph construction are better than offline ones, and can be improved by combination with the Pseudo-Label technique.\n4.3 EFFECT OF CHOICE FOR CLUSTER NUMBER IN SPECTRAL LABEL REFINEMENT\nWe fix all the hyperparameters in spectral label refinement except the number of clusters to train several networks for comparison. The choice of the number of clusters is varied from 2 to twice the number of classes, which is 20 in this case. Intuitively, we may choose the exact number of classes, i.e., 10 for both MNIST and CIFAR-10, as the number of classes used in spectral label refinement. Figure 3 shows the effect of the number of clusters upon accuracy of our SLR+PL approach on MNIST and CIFAR-10. Surprisingly, that choice does not have obvious advantage compared to other nearby choices on MNIST, and is not the best choice on CIFAR-10 (cf. the number of clusters is 8). Moreover, when the number of clusters is 2 in MNIST, the performance is sometimes close to that of 10, even though the remaining hyperparameters fit the choice of 10 best. We believe that the choice of the number of clusters and the corresponding margin m is quite dataset-dependent. For MNIST, images are handwritten digits composed by few and simple strokes, so that the choice of the number of clusters is relatively non-sensitive. For CIFAR-10, some classes are visually similar (see Figure 4), for example, cat and dog, and hence it is difficult to get 10 distinct clusters by spectral label refinement when most images are unlabeled and features are not robust enough. Under these circumstances, the choice of the number of clusters does not need to be exactly the number of classes in a dataset. With good observation of dataset characteristic and corresponding margin m, our methods can approach the goal of improving generalization on supervised tasks."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this work, we developed a simple but effective technique to construct the graph online simultaneous with CNN training. Then we revisited the kNN graph and spectral label refinement, and showed how they work and perform by our online graph construction technique. We also demonstrated the effectiveness of our approach on MNIST and CIFAR-10 for semi-supervised deep learning with CNNs. Our approach can be easily applied to other state-of-the-art CNN architectures for semisupervised learning, and can serve as a framework for other graphs which are effective or specific to some applications. In future work, as some graph approach like spectral label refinement has ability to leverage labels, we may investigate more on the utilization of labels. Moreover, as W is limited to be binary by the contrastive loss function, it may be promising to consider a more effective loss function which is more suitable for modern graphs."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand.", "creator": "LaTeX with hyperref package"}}}