{"id": "1511.06267", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Asymmetrically Weighted CCA And Hierarchical Kernel Sentence Embedding For Image & Text Retrieval", "abstract": "joint modeling of language and vision has been drawing increasing visual interest. a geometric multimodal data regression representation allowing for bidirectional retrieval of images by sentences and vice versa is a key aspect of this modeling. speaking in this paper second we show that canonical network correlation analysis ( cca ) can be adapted to bidirectional retrieval by a simple task linear dependent asymmetric weighting, regression which solves optimally the retrieval problem in a least squares sense. while regularizing cca is known to improve numerical stability as well as generalization performance, less widespread attention has been brought to the efficient computation of the regularization path of cca, which is key to model selection. in presenting this paper half we develop seemingly efficient algorithms to compute the full regularization path matching of complex cca within the classical tikhonov and the binary truncated svd ( t - svd cca ) regularization frameworks. given t - svd cca is new to the best domain of our knowledge, and its general regularization path can be computed essentially more efficiently than assuming its tikhonov counterpart.", "histories": [["v1", "Thu, 19 Nov 2015 17:29:00 GMT  (590kb)", "http://arxiv.org/abs/1511.06267v1", "submitted to ICLR 2016"], ["v2", "Thu, 3 Dec 2015 03:53:40 GMT  (797kb)", "http://arxiv.org/abs/1511.06267v2", "submitted to ICLR 2016; updated the paper to have data splits and scorings comparable with the literature"], ["v3", "Thu, 7 Jan 2016 16:20:50 GMT  (855kb)", "http://arxiv.org/abs/1511.06267v3", "submitted to ICLR 2016; updated the paper to have data splits and scorings comparable with the literature"], ["v4", "Tue, 9 Feb 2016 15:53:18 GMT  (882kb)", "http://arxiv.org/abs/1511.06267v4", null], ["v5", "Mon, 5 Dec 2016 21:06:43 GMT  (3016kb,D)", "http://arxiv.org/abs/1511.06267v5", "Under Review CVPR 2017"]], "COMMENTS": "submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["youssef mroueh", "etienne marcheret", "vaibhava goel"], "accepted": false, "id": "1511.06267"}, "pdf": {"name": "1511.06267.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mroueh@us.ibm.com", "etiennem@us.ibm.com", "vgoel@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 26\n7v 1\n[ cs\n.L G\n] 1\n9 N\nov 2"}, {"heading": "1 INTRODUCTION: MULTIMODAL RETRIEVAL", "text": "Modeling jointly language and vision has attracted a lot of attention recently. Generative models such as deep recurrent networks for the language modeling, in conjunction with deep convolutional neural networks on the image side have shown remarkable success in the image captioning task Karpathy & Li (2015); Mao et al. (2014); Vinyals et al. (2015); Socher et al. (2011). Image and Text retrieval has been the focus of many recent works. Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al. (2014); Kulkarni et al. (2011); Farhadi et al. (2010). Given an image and sentence embeddings the goal is to design a multimodal representation that captures the correlation between the two modalities, and allows for bidirectional search in a joint embedding space. In this paper we address the bidirectional retrieval problem and show that an asymmetric task dependent embedding of the images and sentences yields better performance on both retrieval tasks. Key to our representation is the careful analysis and usage of Canonical Correlation Analysis (CCA) in bidirectional retrieval, and the development of an efficient model selection for the regularized CCA embedding. We report our retrieval results on the COCO benchmark Lin et al. (2014)."}, {"heading": "1.1 THIS PAPER: CONTRIBUTIONS", "text": "The main contributions of this paper are :\n1. Asymmetric and task dependent Correlation Weighting for CCA: We cast the bidirectional retrieval problem as a least squares problem and motivate the retrieval in the CCA weighted space. We theoretically and experimentally show that asymmetrically weighting the canonical weights with the canonical correlations in a task dependent way, solves each task optimally in the least squares sense (See Table 1). This improves the performance of each task and achieves state of the art on the COCO benchmark with off the shelf unsupervised features.\n2. Efficient Tikhonov/Truncated SVD Regularization Path for CCA : While CCA is an old and widely studied problem that has been used as a multimodal data representation in many machine learning applications, we found that a computationally efficient cross-validation for regularized CCA has been less studied. Model selection is not only important to the numerical stability of CCA it also improves the generalization ability of this simple yet powerful multimodal representation. To this end we present two regularization algorithms for the batch CCA problem based on the Bjork Golub Algorithm Golub et al. (1973) (Algorithm 1), that make the computation of the full regularization path for CCA efficient. We present our cross validation algorithms within two regularization frameworks: Tikhonov Regularization (Algorithm 2) and truncated SVD regularization (Algorithm 3) . While Tikhonov regularization is more popular for regularized CCAVinod (1976), we also derive the regularization path for CCA using truncated SVD covariances. Truncated SVD is a popular regularizer for least squares regression problems Hansen (1986), but to our knowledge has not been utilized before in the CCA Context. We show that the truncated SVD CCA regularization path can be computed more efficiently and competes favorably with Tikhonov regularization.\nNotation. Given a multimodal training set S = {(xi, yi)|xi \u2208 X \u2282 Rmx , yi \u2208 Y \u2282 Rmy , i = 1 . . . n}, (n > max(mx,my)), let X \u2208 Rn\u00d7mx , and Y \u2208 Rn\u00d7my be the two data matrices corresponding to each modality. Define \u00b5X , \u00b5Y to be the means of X and Y respectively. Let CXX = (X \u2212 \u00b5X) \u22a4 (X \u2212 \u00b5X) \u2208 R mx\u00d7mx , and CY Y = (Y \u2212 \u00b5Y ) \u22a4 (Y \u2212 \u00b5Y ) \u2208 R\nmy\u00d7my be the covariances matrices of X and Y respectively. Let CXY = (X \u2212 \u00b5X) \u22a4 (Y \u2212 \u00b5Y ) \u2208 R mx\u00d7my be the correlation matrix. Define Ik to be the identity matrix in k dimensions. SVD stands for the thin singular value decomposition. A validation set Sv is given for model selection. Our goal is to index a test set S\u2217 = {(x\u2217i , y \u2217 i )|x \u2217 i \u2208 S \u2217 x \u2282 X , y \u2217 i \u2208 S \u2217 y \u2282 Y, i = 1 . . . n\n\u2217} for bidirectional search. X and Y are assumed to be centered."}, {"heading": "1.2 BIDIRECTIONAL RETRIEVAL AS A LEAST SQUARES PROBLEM", "text": "We start by defining more formally the bidirectional retrieval tasks. Given pairs of high dimensional points (xi, yi) \u2208 X \u00d7 Y where xi corresponds to the feature representation of an image given by a deep convolutional neural network, and yi a sentence embedding of an associated caption. Our goal is to index this multimodal data in a way that enables bidirectional retrieval. Enabling the image annotation task which consists of retrieving for an image query x \u2208 X the associated caption in the set Y . Whereas the image search task consists of retrieving for a caption query y \u2208 Y the associated image x \u2208 X .\nIn this Section we cast the retrieval tasks as a simple least squares problem. In order to reduce the variance of our estimators we put ourselves in the whitened space of the data in X , and Y , i.e we consider the bidirectional mapping between the whitened data spaces. Assuming for the moment\nthat both covariances are non singular, we can write the whitened data as XC \u2212\n1 2\nXX , and Y C \u2212\n1 2\nY Y . We will show later how Tikhonov/Truncated SVD regularization alleviates this issue of non singular covariances. Hence for the image search task we can minimize the following problem:\nmin T\u2208Rmx\u00d7my\n\u2225 \u2225 \u2225 XC \u2212 1 2\nXXT \u2212 Y C \u2212\n1 2 Y Y\n\u2225 \u2225 \u2225\n2 F , (1)\nThis defines a linear transform which maps the image set to the query caption. We follow this general principle of mapping the space of the search to the space of the query rather than the converse as found in Socher et al. (2013). The solution of Problem (1) is given simply by T = C \u2212 1 2 ,\u22a4\nXX CXY C \u2212 1 2\nY Y , hence for a query test sentence y\u2217 we can find its corresponding image by finding :\nargmin x\u2217\u2208S\u2217x\n\u2225 \u2225 \u2225 C \u2212 1 2 ,\u22a4\nY Y y \u2217 \u2212 T\u22a4C\n\u2212 1\n2 ,\u22a4\nXX x \u2217\n\u2225 \u2225 \u2225\n2\n, (2)\nWe define the image to caption mapping as follows:\nfi\u2192c(x \u2217) = T\u22a4C\n\u2212 1\n2 ,\u22a4\nXX x \u2217. (3)\nThe image search problem, then reduces to transforming the image set through the mapping fi\u2192c, and then finding the nearest neighbor of the query caption represented by the whitened vector\nC \u2212\n1 2 ,\u22a4\nY Y y \u2217 in the transformed image set.\nSwapping the roles of X and Y we obtain equivalently that the image annotation problem can be solved also by a linear least squares. We define the caption to image mapping:\nfc\u2192i(y \u2217) = TC\n\u2212 1 2 ,\u22a4 Y Y y \u2217, (4)\ntherefore the image annotation problem reduces to transforming the caption set via the mapping fc\u2192i, and then finding the nearest neighbor of the query image represented by the whitened vector C \u2212 1 2 ,\u22a4\nXX x \u2217 in the transformed caption set:\nargmin y\u2217\u2208S\u2217y\n\u2225 \u2225 \u2225 C \u2212 1 2 ,\u22a4\nXX x \u2217 \u2212 TC\n\u2212 1\n2 ,\u22a4\nY Y y \u2217\n\u2225 \u2225 \u2225\n2\n. (5)"}, {"heading": "1.3 SIMPLIFYING THE EXPRESSIONS WITH SINGULAR VALUE DECOMPOSITION", "text": "In this Section we simplify the expressions for image search and image annotation given in Equations (3), and (4) respectively, using the the singular value decompositions of X and Y . Let X = Ux\u03a3xV \u22a4 x be the SVD of X and Y = Uy\u03a3yV \u22a4 y be the SVD of Y . It is easy to show that the whitened data XC \u2212 1 2\nXX = XVx\u03a3 \u22121 x = Ux, and Y C\n\u2212 1 2 Y Y = Y Vy\u03a3 \u22121 y = Uy. For x \u2208 X and\ny \u2208 Y , let ux and uy be the whitened data points:\nux = \u03a3 \u22121 x V \u22a4 x x, and uy = \u03a3 \u22121 y V \u22a4 y y. (6)\nNote that : T = C \u2212 1 2 ,\u22a4 XX CXY C \u2212 1 2 Y Y = \u03a3 \u22121 x V \u22a4 x X \u22a4Y Vy\u03a3 \u22121 y = U \u22a4 x Uy. T corresponds to the correlation in the whitened spaces of X and Y . Hence the expression in (3) for the image to caption mapping is simply:\nfi\u2192c(x \u2217) = T\u22a4u\u2217x =\nn \u2211\ni=1\n\u3008ux\u2217 , uxi\u3009 uyi. (7)\nThe image search problem corresponds to finding for a query sentence y\u2217 the image x\u2217 solving:\nargmin x\u2217\u2208S\u2217x\n\u2016uy\u2217 \u2212 fi\u2192c(x \u2217)\u20162 . (8)\nSimilarly the expression in (4) for the caption to image mapping is simply:\nfc\u2192i(y \u2217) = Tu\u2217y =\nn \u2211\ni=1\n\u3008uy\u2217 , uyi\u3009uxi. (9)\nThe image annotation problem corresponds to finding for a query image x\u2217 the caption y\u2217 solving:\nargmin y\u2217\u2208S\u2217y\n\u2016ux\u2217 \u2212 fc\u2192i(y \u2217)\u2016 2 . (10)\nWe see that the operator T plays a central role in the retrieval problem. For an image x, fi\u2192c(x) = T\u22a4ux = \u2211n\ni=1 \u3008ux, uxi\u3009uyi , is our best guess in the least squares sense for a sentence in the whitened space of Y . For a caption y, fc\u2192i(y) = Tuy = \u2211n\ni=1 \u3008uy, uyi\u3009 uxi, is our best guess in the least squares sense for an image in the whitened space of X ."}, {"heading": "2 RETRIEVAL: FROM LEAST SQUARES TO CCA GAINING IN EFFICIENCY", "text": ""}, {"heading": "2.1 CANONICAL CORRELATION ANALYSIS", "text": "We review in this section Canonical Correlation Analysis due to Hotelling (1936). For data matrices X \u2208 Rn\u00d7mx and Y \u2208 Rn\u00d7my , let k = min(mx,my) the canonical correlation \u03c31, . . . \u03c3k , and their corresponding pairs of correlations weights {(ui, vi)}i=1...k, columns of U \u2208 Rmx\u00d7k, and V \u2208 Rmy\u00d7k are the solution of the following maximization problem:\nmax U\u22a4CXXU=Ik ,V \u22a4CY Y V =Ik\nTrace(U\u22a4CXY V ),\nwhere \u03c3i = u\u22a4i CXY vi, i = 1 . . . k. Intuitively CCA finds the directions that are maximally correlated and that are orthonormal in the metric defined by each covariance matrix, respectively. The following Lemma due to Bjork and Golub shows that the canonical correlation weights can be computed using the singular value decomposition of the data matrices X and Y , and the correlation matrix in the whitened space T defined in Section 1.3:\nLemma 1 (Golub et al. (1973)). Let X = Ux\u03a3xV \u22a4x , and Y = Uy\u03a3yV \u22a4 y be the singular value decomposition of X and Y (Ux \u2208 Rn\u00d7mx ,\u03a3x \u2208 Rmx\u00d7mx , Vx \u2208 Rmx\u00d7mx). Let k = min(mx,my), and T = U\u22a4x Uy. Let\nT = Px\u03a3P \u22a4 y (11)\nbe its SVD, Px \u2208 Rmx\u00d7k,\u03a3 \u2208 Rk\u00d7k, Py \u2208 Rmy\u00d7k. The canonical correlations of X and Y are the diagonal elements of \u03a3, with canonical weights of X given by U = Vx\u03a3\u22121x Px, and the canonical weights of Y given by V = Vy\u03a3\u22121y Py . (Proof in Appendix A).\nAlgorithm 1 summarizes the Bjork Golub procedure to compute CCA. While the original algorithm of Bjork and Golub uses the QR factorization of X and Y , we follow Avron et al. (2014) in exposing the algorithm fully with SVD. Note that both U and V correspond to a whitening step followed by a projection to a common space of dimension k. The total computational complexity of this algorithm assuming my < mx is O(nm2x+nm 2 y+mxm 2 y). While The Bjork Golub SVD algorithm is intuitive and efficient, it is less popular in machine learning than the generalized eigenvalue implementation of CCA.\nAlgorithm 1 Bjork Golub\n1: procedure BJORK GOLUB ALGORITHM (X \u2208 Rn\u00d7mx , Y \u2208 Rn\u00d7my ) 2: [Ux,\u03a3x, Vx] \u2190 SV D(X). 3: [Uy,\u03a3y, Vy] \u2190 SVD(Y ). 4: T = U\u22a4x Uy \u2208 R mx\u00d7my 5: [Px,\u03a3, Py] = SVD(T ) 6: U = Vx\u03a3 \u22121 x Px 7: V = Vy\u03a3 \u22121 y Py 8: return U, V 9: end procedure"}, {"heading": "2.2 BIDIRECTIONAL RETRIEVAL USING CCA DECOMPOSITION", "text": "As we have seen in Sections 1.3 and 2.1, the correlation operator T plays a central role in the least squares formulation as well in the CCA formulation. Turning back to the image to caption mapping given in equation (7), using the expressions of the whitened data points in Equation (6) and the SVD of T given in Equation (11), we obtain:\nfi\u2192c(x \u2217) = T\u22a4u\u2217x = Py\u03a3P \u22a4 x \u03a3 \u22121 x V \u22a4 x x \u2217 = Py\u03a3U \u22a4x\u2217,\nwhere U is the canonical weight for X and \u03a3 the canonical correlation matrix. Note that fi\u2192c is still my dimensional vector, we can reduce its dimension by projecting it to the k-dimensional column space of Py . It follows that: P\u22a4y fi\u2192c(x\n\u2217) = \u03a3U\u22a4x\u2217 \u2208 Rk. On the other hand: P\u22a4y uy\u2217 = P\u22a4y \u03a3 \u22121 y V \u22a4 y y \u2217 = V \u22a4y\u2217 \u2208 Rk, where V is the canonical correlation weight for Y .\nFor the image search problem, we can therefore perform the nearest neighbor search given in equation (8), in the k-dimensional reduced space defined by\n(P\u22a4y fi\u2192c(x \u2217), P\u22a4y uy\u2217) = (\u03a3U \u22a4x\u2217, V \u22a4y\u2217).\nIt follows that the image search problem can be written using the canonical weights U, V and the canonical correlation \u03a3. Note that the correlation matrix is weighting the canonical weight in an asymmetric way. Cosine similarities are usually used for retrieval with CCA, hence we use cosine similarity between the two embeddings. The image search problem reduces to finding for a query caption y\u2217, the image x\u2217 solving:\nargmax x\u2217\u2208S\u2217x\n\u2329 \u03a3U\u22a4x\u2217, V \u22a4y\u2217 \u232a\n\u2016\u03a3U\u22a4x\u2217\u2016 \u2016V \u22a4y\u2217\u2016 . (12)\nSimilarly the caption to image mapping can be expressed for a caption y\u2217 as: fc\u2192i(y\u2217) = Px\u03a3V \u22a4y\u2217, and we can perform the search in the k-dimensional column space of Px:\n(P\u22a4x ux\u2217 , P \u22a4 x fc\u2192i(y \u2217)) = (U\u22a4x\u2217,\u03a3V \u22a4y\u2217).\nUsing a cosine similarity the image annotation problem reduces to finding for a query image x\u2217, the caption y\u2217 solving:\nargmax y\u2217\u2208S\u2217y\n\u2329 U\u22a4x\u2217,\u03a3V \u22a4y\u2217 \u232a\n\u2016U\u22a4x\u2217\u2016 \u2016\u03a3V \u22a4y\u2217\u2016 . (13)\nNote that using the canonical weights and correlations in the image search and annotation given in Equations (12) and (13), the search is done in a space of dimension k = min(mx,my), rather than a space of dimensions mx for image search as in Equation (8) and my for image annotation as in Equation (10). This results in a much more efficient search than the least squares formulation. Note that \u03a3 appears in an asymmetric way in the embedding of the points depending on the task. Hence we call our method asymmetrically weighted CCA. Asymmetrically weighted CCA (Table 1) performs approximately1 the optimal nearest neighbor search in the least square sense for each task."}, {"heading": "3 CCA REGULARIZATION PATH: TIKHONOV VERSUS TRUNCATED SVD REGULARIZATION", "text": "While CCA is a powerful statistical tool less attention has been brought to the efficient computation of the full regularization path of CCA in a learning context where we need to do model selection on a validation set. For now we have assumed that the covariances CXX and CY Y are non-singular, and we presented an SVD version of the Bjork Golub Algorithm in this context. Regularizing CCA does not only allow for numerical stability in the non singular case, efficient model selection on a validation set allows for better generalization properties and avoids overfitting. Tikhonov regularization is the most common regularization used in CCA and consists in replacing the covariances by CXX + \u03b3xImx and CY Y + \u03b3yImy , where \u03b3x, \u03b3y > 0 are the regularization parameters subject to cross-validation. We specialize the Bjork Golub algorithm in Section 3.1 for handling efficiently a grid of parameters \u03b3x, \u03b3y . One main contribution of this paper is in introducing the truncated SVD regularization to the covariances in the CCA problem. We emphasize that our truncation is applied to the SVD of the data matrices X and Y , not the SVD of the whitened correlation matrix T in the Bjork Golub Algorithm. Truncating the SVD of T and choosing an embedding dimension k < min(mx,my) is sometimes referred as the truncated SVD CCA, hence our clarification. We replace the covariance CXX by its best kx-rank approximation given by the SVD of X , and CY Y by its best ky-rank approximation given by the SVD of Y , where kx, ky \u2208 N, kx \u2264 mx, and ky \u2264 my . In this framework kx ky are the regularization parameters and are subject to cross-validation. We show in Section 3.2, how to specialize the Bjork Golub algorithm to handle efficiently a grid of parameters kx ky accounting for the regularization path of CCA. Proofs are given in Appendix A."}, {"heading": "3.1 TIKHONOV REGULARIZATION: REGULARIZING THE COVARIANCES", "text": "The Regularized CCA problem can be written in this form:\nmax U\u22a4(CXX+\u03b3xImx )U=I,V \u22a4(CY Y +\u03b3yImy )V =I Tr(U\u22a4CXY V ) (14)\nAlgorithm 2 shows how to compute the regularization path of CCA in an efficient way. The computational complexity of the cross-validation is dominated by the computation of SVD of T within\n1Using thin SVD Py is not a complete orthonormal basis of Rmy .\nthe loop that is C = min(O(mxm2y), O(mym 2 x)). If we have Nx and Ny values of \u03b3x and \u03b3y , the total complexity is NxNyC. Note that this can be fully parallelizable since each SVD computation is independent from the rest. \u03c3x,1 and \u03c3x,mx are minimum and maximum singular value of X .\nAlgorithm 2 Tikhonov Regularized CCA\n1: procedure TIKHONOV REGULARIZED CCA(X \u2208 Rn\u00d7mx , Y \u2208 Rn\u00d7my ) 2: [Ux,\u03a3x, Vx] \u2190 SV D(X). 3: [Uy,\u03a3y, Vy] \u2190 SVD(Y ). 4: T0 = \u03a3x(U \u22a4 x Uy)\u03a3y \u2208 R mx\u00d7my 5: for \u03b3x \u2208 {\u03c32x,1, . . . \u03c3 2 x,mx\n} do \u22b2 The set of singular values squared or a subsampled grid 6: for \u03b3y \u2208 {\u03c32y,1, . . . \u03c3 2 y,my } do 7: T \u2190 ( \u03a32x + \u03b3xI )\u2212 1 2 T0 ( \u03a32y + \u03b3yI )\u2212 1 2 8: [Px,\u03a3 \u03b3x,\u03b3y , Py] = SV D(T ) 9: U = Vx ( \u03a32x + \u03b3xI )\u2212 1 2 Px\n10: V = Vy ( \u03a32y + \u03b3yI )\u2212 1\n2 Py 11: Compute performance using U, V,\u03a3\u03b3x,\u03b3y on a validation Set. 12: (Bidirectional Retrieval is done using a sorted list of the scores in Eqs (12), and (13).) 13: end for 14: end for 15: return U, V,\u03a3\u03b3x,\u03b3y , \u03b3x, \u03b3y with best validation performance for each task. 16: end procedure\nLemma 2. Algorithm 2 computes the regularization path of Tikhonov regularized CCA.\nThe values chosen for \u03b3x, \u03b3y (Steps 5 and 6) are very important and are related to a technique called spectral filtering Hansen (1986). A full discussion of the latter and its relation to truncated SVD is beyond the scope of this paper and will be subject to study in a forthcoming work."}, {"heading": "3.2 TRUNCATED-SVD REGULARIZATION: TRUNCATING THE COVARIANCES", "text": "Let kx \u2264 mx and let Xkx be the best kx-rank approximation of X given by the truncated SVD:\nXkx = Ukx\u03a3kxV \u22a4 kx , Ukx \u2208 R n\u00d7kx ,\u03a3kx \u2208 R kx\u00d7kx , Vkx \u2208 R mx\u00d7kx ,\nSimilarly for Y , we define the best ky-rank approximation (ky \u2264 my):\nYky = Uky\u03a3kyV \u22a4 ky , Uky \u2208 R n\u00d7ky ,\u03a3ky \u2208 R ky\u00d7ky , Vky \u2208 R my\u00d7ky\nWe define the truncated SVD CCA as follows:\nmax U\u22a4X\u22a4\nkx XkxU=I,V \u22a4Y \u22a4 ky YkyV=I Tr(U\u22a4X\u22a4Y V ) (15)\nAlgorithm 3, shows how to compute the regularization path of the truncated SVD CCA formulation in an efficient way. The computational complexity of the cross-validation is dominated by the computation of SVD of T within the loop. While in Algorithm 2 for Tikhonov regularization T is always a matrix of dimension mx \u00d7 my . One advantage of the truncation in Algorithm 3 is that T becomes a kx \u00d7 ky dimension matrix, hence the SVD computation is more efficient and costs min(O(kxk2y), O(kyk 2 x)). The total cost of the cross validation in Algorithm 3 is effectively more efficient than the one in Algorithm 2 (See Appendix B for timing experiments). Note that this also can be fully parallelizable since each SVD computation is independent from the rest. Note that one advantage of this truncated SVD for CCA is that the dimension of the embedding space k = min(kx, ky) is also automatically selected by the algorithm, while it is fixed in the Tikhonov case to k = min(mx,my) (or is subject to another cross validation).\nAlgorithm 3 Truncated SVD CCA 1: procedure CROSS VALIDATION TRUNCATED SVD -CCA (X \u2208 Rn\u00d7mx , Y \u2208 Rn\u00d7my ) 2: [Ux,\u03a3x, Vx] \u2190 SV D(X). 3: [Uy,\u03a3y, Vy] \u2190 SVD(Y ). 4: T = U\u22a4x Uy \u2208 R mx\u00d7my\n5: W x = Vx\u03a3 \u22121 x \u2208 R mx\u00d7mx 6: W y = Vy\u03a3 \u22121 y \u2208 R my\u00d7my 7: for kx \u2208 [mx] do \u22b2 [mx] = {1 . . .mx} or a subsampled grid. 8: for ky \u2208 [my] do 9: [Pkx ,\u03a3\nkx,ky , Pky ] = SV D(T1:kx,1:ky ) \u22b2 T1:kx,1:ky : extracts the first kx rows ,and the first ky columns of T . 10: U = W x:,1:kxPkx 11: V = W y:,1:kyPky 12: Compute performance using U, V,\u03a3kx,ky on a validation Set. 13: (Bidirectional Retrieval is done using a sorted list of the scores in Eqs (12), and (13).) 14: end for 15: end for 16: return U, V,\u03a3kx,ky , kx, ky with best validation performance for each task. 17: end procedure\nLemma 3. Algorithm 3 computes the regularization path of the truncated SVD regularized CCA."}, {"heading": "4 RELATION TO PREVIOUS WORK", "text": "Joint modeling of language and vision is a rapidly growing field, we focus here on some recent works that use bidirectional maps in the retrieval tasks and their relation to this paper. Klein et al. (2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x\u2217, y\u2217), a joint embedding of the form (\u03a3\u03b1U\u22a4x\u2217,\u03a3\u03b1V \u22a4y\u2217), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case. The symmetric weighting in Gong et al. (2014) was a heuristic found to improve performance and is not theoretically motivated as in the asymmetric weighting of this paper. Indeed our experimental results show that asymmetric weighting gives higher performance and makes the image search and the image annotation on par in term of performance. A discussion of the optimality of the asymmetric weighting and an empirical study is given in Appendix C. Skip thought vectors introduced in Kiros et al. (2015) for representing sentences were used for bidirectional search in conjunction with VGG features Simonyan & Zisserman (2015) on the image side, with linear embeddings learned with a discriminative triplets loss, instead of a CCA loss. A similar discriminative loss was used for building a joint image and text representation in Lebret et al. (2015). One advantage of CCA over the discriminative losses is that the batch solution can be found efficiently and does not need stochastic optimization, as well as the efficient model selection introduced in this paper providing a means to select the dimension of the joint embedding leading to better generalization properties."}, {"heading": "5 NUMERICAL RESULTS", "text": "We performed image annotation and search tasks on the COCO benchmark Lin et al. (2014) using our task dependent asymmetrically weighted CCA, as described in Table 1. Retrieval was performed using cosine similarities given in Equations (12) and (13). The training set contains 82783 images, along with 5 captions each. Similarly to Klein et al. (2015), we used the splits from Karpathy & Li (2015), and performed cross-validation on 5 validation splits of 5K images, and tested our models on 5 testing splits of 5K images each, as well as 25 splits of 1K images each. We report for both tasks the recall rate at one result, five results, or ten first results (r@1,5,10), as well as the median rank of the first ground truth retrieval. Cross validation was performed using the Asymetrically Weighted Truncated SVD CCA (AWT-CCA) Algorithm 3 to select the model corresponding to the minimum median rank (regularization paths are given in Appendix D).\nFor feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image. The mean intensity of each crop is subtracted in each color channel, and then encoded by VGG19 (the final FC -4096 layer). The average of the resulting 10 feature vectors corresponding to each crop is used as the image representation. On the text side, we use two sentence embeddings: the first one proposed by Klein et al. (2015) consisting in averaging the word2vec Mikolov et al. (2013) representation of words in each sentence. Word2vec representations are available on code.google.com/p/word2vec/, we followed the recommendation of Klein et al. (2015) in preprocessing the word2vec vocabulary matrix of COCO with PCA, to ensure averaging uncorrelated channels in the sentence embedding. The second embedding we used are skip thought vectors introduced in Kiros et al. (2015), which encodes sentences to vectors using an LSTM. Image and text features were centered before learning CCA.\nWe see in Table 2 that using off the shelf unsupervised features (VGG, word2vec, skip thoughts vectors), AWT-CCA achieves state of the art results at both image search and annotation, and competes with supervised features extraction such as GMM+HGLMM of Klein et al. (2015)2 . More importantly the asymmetric weighting and the efficient model selection makes the performance of both tasks on par where we see an imbalance in the other methods ."}, {"heading": "6 CONCLUSION", "text": "In this paper we showed how an asymmetric weighting and a computationally efficient cross validation of CCA improves the performance of bidirectional retrieval tasks. While the exposition of this paper was on image and caption retrieval, our methods are generic and can be applied to any multimodal setting. We solved in this paper CCA in its batch formulation, for handling larger scale datasets we can use the randomized SVD of Halko et al. (2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al. (2015), SGDCCA (Linear and Deep CCA) was proposed recently and can be applied to our problem. It would be interesting to study an efficient model selection for the SGD-CCA such as early stopping and truncated SVD for a finer model selection. A simple extension to kernel CCA regularization of this present paper would be to leverage random Fourier features for approximating Kernel CCA Lopez-Paz et al. (2014). Truncated SVD-CCA would be particularly interesting in this setting since kx, ky would be balancing the regularization and the estimation error of the kernel by means of random features.\n2Note that the 1K results on Mean Vec (ICA +CCA) of Klein et al. (2015) and the skip thought of Kiros et al. (2015) are not comparable to ours since they are reported on only 1K test set, we report the mean of 25 splits 1K test sets."}, {"heading": "A APPENDIX: PROOFS", "text": "Proof of Lemma 1. We give the proof here as it will be useful in the the development of the full regularization path of CCA with truncated SVD regularization of the covariances CXX and CY Y . Let Px = \u03a3xV \u22a4x U \u2208 R mx\u00d7k equivalently U = Vx\u03a3\u22121x Px and Py = \u03a3yV \u22a4 y V \u2208 R my\u00d7k equivalently V = Vy\u03a3\u22121y Py . Hence we obtain by this change of variable:\nU\u22a4X\u22a4Y V = P\u22a4x \u03a3 \u22121 x V \u22a4 x Vx\u03a3xU \u22a4 x Uy\u03a3yV \u22a4 y Vy\u03a3 \u22121 y Py = P \u22a4 x\n( U\u22a4x Uy ) Py.\nSimilarly: U\u22a4X\u22a4XU = P\u22a4x Px V \u22a4Y \u22a4Y V = P\u22a4y Py . Hence replacing U, V with Px, Py we have: max\nP\u22a4x Px=I,P \u22a4 y Py=I\nTr(P\u22a4x ( U\u22a4x Uy ) Py),\nThis is solved by an SVD of T = U\u22a4x Uy. [Px,\u03a3, Py] = SV D(T ), (Px \u2208 R mx\u00d7k,\u03a3 \u2208 Rk\u00d7k, Py \u2208 R my\u00d7k). where k = min(mx,my), and finally we have U = Vx\u03a3\u22121x Px, V = Vy\u03a3 \u22121 y Py .\nProof of Lemma 2. The proof of this Lemma is elementary we give it here for completion.\n[Ux,\u03a3x, Vx] = SVD(X) Ux \u2208 R n\u00d7mx ,\u03a3x \u2208 R mx\u00d7mx , Vx \u2208 R mx\u00d7mx X = Ux\u03a3xV \u22a4 x .\n[Uy,\u03a3y, Vy] = SVD(Y ) Uy \u2208 R n\u00d7my ,\u03a3x \u2208 R my\u00d7my , Vx \u2208 R my\u00d7my Y = Uy\u03a3yV \u22a4 y .\nX\u22a4X + \u03b3xI = Vx ( \u03a32x + \u03b3xI ) V \u22a4x . Y \u22a4Y + \u03b3yI = Vy ( \u03a32y + \u03b3yI ) V \u22a4y .\nLet Px = \u221a \u03a32x + \u03b3xIV \u22a4 x U \u2208 R mx\u00d7k equivalently U = Vx ( \u03a32x + \u03b3xI )\u2212\n1\n2 Px. Let Py = \u221a\n\u03a32y + \u03b3yIV \u22a4 y V \u2208 R my\u00d7k equivalently V = Vy ( \u03a32y + \u03b3yI )\u2212 1 2 Py . Hence we obtain by this\nchange of variable:\nU\u22a4X\u22a4Y V = P\u22a4x ( \u03a32x + \u03b3xI )\u2212\n1 2 V \u22a4x Vx\u03a3xU \u22a4 x Uy\u03a3yV \u22a4 y Vy ( \u03a32y + \u03b3yI )\u2212 1 2 Py\n= P\u22a4x ( \u03a32x + \u03b3xI )\u2212 1 2 \u03a3x ( U\u22a4x Uy ) \u03a3y ( \u03a32y + \u03b3yI )\u2212 1 2 Py.\nLet T = (\n\u03a32x + \u03b3xI )\u2212\n1 2 \u03a3x ( U\u22a4x Uy ) \u03a3y ( \u03a32y + \u03b3yI )\u2212 1 2 .\nHence we obtain that: [Px,\u03a3, Py] = SVD(T ).\nProof of Lemma 3. Let\nPkx = \u03a3kxV \u22a4 kx U \u2208 Rkx\u00d7k, equivalently U = Vkx\u03a3 \u22121 kx Pkx \u2208 R mx\u00d7k, k = min(kx, ky)\nPky = \u03a3kyV \u22a4 ky V \u2208 Rky\u00d7k, equivalently V = Vky\u03a3 \u22121 ky Pky \u2208 R my\u00d7k, k = min(kx, ky)\nHence we obtain by this change of variable:\nU\u22a4X\u22a4Y V = P\u22a4kx\u03a3 \u22121 kx V \u22a4kxVx\u03a3xU \u22a4 x Uy\u03a3yV \u22a4 y VKy\u03a3 \u22121 ky Pky ,\nNow we turn to:\n\u03a3\u22121kx (V \u22a4 kx Vx)\u03a3xU \u22a4 x = \u03a3 \u22121 kx [Ikx\u00d7kx0kx\u00d7(mx\u2212kx)]\u03a3xU \u22a4 x\n= \u03a3\u22121kx [\u03a3kx0kx\u00d7(mx\u2212kx)]U \u22a4 x\n= [Ikx\u00d7kx0kx\u00d7(mx\u2212kx)]U \u22a4 x\n= U\u22a4kx\nHence we keep the first kx columns of Ux, that is Ukx . The same argument hold for Uky . It follows that using truncated SVD, we have:\nU\u22a4X\u22a4Y V = P\u22a4kxU \u22a4 kx UkyPky .\nHence truncated SVD-CCA can be solved finding:\n[Pkx ,\u03a3 kx,ky , Pky ] = SV D(U \u22a4 kx Uky ).\nTurning now to U\u22a4kxUky this can be computed efficiently by precomputing T = U \u22a4 x Uy \u2208 R mx\u00d7my and then extracting the submatrix consisting of kx rows and ky columns. we return therefore U = Vkx\u03a3 \u22121 kx Pkx , V = Vky\u03a3 \u22121 ky Pky ."}, {"heading": "B EFFICIENCY OF T-SVD CCA VERSUS TIKHONOV CCA", "text": ""}, {"heading": "C OPTIMALITY OF THE TASK DEPENDENT ASYMMETRIC WEIGHTING", "text": "D AWT-SVD CCA REGULARIZATION PATHS"}], "references": [{"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["Avron", "Haim", "Boutsidis", "Christos", "Toledo", "Sivan", "Zouzias", "Anastasios"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Fang", "Hao", "Gupta", "Saurabh", "Iandola", "Forrest N", "Srivastava", "Rupesh K", "Deng", "Li", "Dollr", "Piotr", "Gao", "Jianfeng", "He", "Xiaodong", "Mitchell", "Margaret", "Platt", "John C", "Zitnick", "C. Lawrence", "Zweig", "Geoffrey"], "venue": "In CVPR,", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi", "Ali", "Hejrati", "Seyyed Mohammad Mohsen", "Sadeghi", "Mohammad Amin", "Young", "Peter", "Rashtchian", "Cyrus", "Hockenmaier", "Julia", "Forsyth", "David A"], "venue": null, "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Numerical methods for computing angles between linear subspaces", "author": ["Golub", "Gene H", "Bjlirck", "He"], "venue": "Math. Comp,", "citeRegEx": "Golub et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1973}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Gong", "Yunchao", "Wang", "Liwei", "Hodosh", "Micah", "Hockenmaier", "Julia", "Lazebnik", "Svetlana"], "venue": "In ECCV,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Rev.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "The truncated svd as a method for regularization", "author": ["Hansen", "Per C"], "venue": "Technical report,", "citeRegEx": "Hansen and C.,? \\Q1986\\E", "shortCiteRegEx": "Hansen and C.", "year": 1986}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": null, "citeRegEx": "Hotelling,? \\Q1936\\E", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Li", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Li", "Fei-Fei"], "venue": "In NIPS,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Associating neural word embeddings with deep image representations using fisher vectors", "author": ["Klein", "Benjamin", "Lev", "Guy", "Sadeh", "Gil", "Wolf", "Lior"], "venue": "In CVPR,", "citeRegEx": "Klein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2015}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Kulkarni", "Girish", "Premraj", "Visruth", "Dhar", "Sagnik", "Li", "Siming", "Choi", "Yejin", "Berg", "Alexander C", "Tamara L"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Phrase-based image captioning", "author": ["Lebret", "Rmi", "Pinheiro", "Pedro O", "Collobert", "Ronan"], "venue": "In ICML,", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Dollr", "Piotr", "Zitnick", "C. Lawrence"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Ma", "Zhuang", "Lu", "Yichao", "Foster", "Dean P"], "venue": "In ICML,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan L"], "venue": "CoRR, abs/1410.1090,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff Chiung-Yu", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In ICML,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher", "Richard", "Karpathy", "Andrej", "Le", "Quoc V", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "Vinod,? \\Q1976\\E", "shortCiteRegEx": "Vinod", "year": 1976}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Wang", "Weiran", "Arora", "Raman", "Srebro", "Nati", "Livescu", "Karen"], "venue": "In Allerton Conference,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "We see that this symmetric weighting boosts the performance for a particular \u03b1 = 1 as reported", "author": ["Klein"], "venue": "D AWT-SVD CCA REGULARIZATION PATHS", "citeRegEx": "Klein,? \\Q2015\\E", "shortCiteRegEx": "Klein", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Generative models such as deep recurrent networks for the language modeling, in conjunction with deep convolutional neural networks on the image side have shown remarkable success in the image captioning task Karpathy & Li (2015); Mao et al. (2014); Vinyals et al.", "startOffset": 231, "endOffset": 249}, {"referenceID": 6, "context": "Generative models such as deep recurrent networks for the language modeling, in conjunction with deep convolutional neural networks on the image side have shown remarkable success in the image captioning task Karpathy & Li (2015); Mao et al. (2014); Vinyals et al. (2015); Socher et al.", "startOffset": 231, "endOffset": 272}, {"referenceID": 6, "context": "Generative models such as deep recurrent networks for the language modeling, in conjunction with deep convolutional neural networks on the image side have shown remarkable success in the image captioning task Karpathy & Li (2015); Mao et al. (2014); Vinyals et al. (2015); Socher et al. (2011). Image and Text retrieval has been the focus of many recent works.", "startOffset": 231, "endOffset": 294}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al.", "startOffset": 0, "endOffset": 62}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al.", "startOffset": 0, "endOffset": 113}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al.", "startOffset": 0, "endOffset": 133}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al. (2014); Kulkarni et al.", "startOffset": 0, "endOffset": 155}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al. (2014); Kulkarni et al. (2011); Farhadi et al.", "startOffset": 0, "endOffset": 179}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al. (2014); Kulkarni et al. (2011); Farhadi et al. (2010). Given an image and sentence embeddings the goal is to design a multimodal representation that captures the correlation between the two modalities, and allows for bidirectional search in a joint embedding space.", "startOffset": 0, "endOffset": 202}, {"referenceID": 1, "context": "Fang et al. (2015); Klein et al. (2015); Lebret et al. (2015); Kiros et al. (2015; 2014); Karpathy et al. (2014); Gong et al. (2014); Socher et al. (2014); Kulkarni et al. (2011); Farhadi et al. (2010). Given an image and sentence embeddings the goal is to design a multimodal representation that captures the correlation between the two modalities, and allows for bidirectional search in a joint embedding space. In this paper we address the bidirectional retrieval problem and show that an asymmetric task dependent embedding of the images and sentences yields better performance on both retrieval tasks. Key to our representation is the careful analysis and usage of Canonical Correlation Analysis (CCA) in bidirectional retrieval, and the development of an efficient model selection for the regularized CCA embedding. We report our retrieval results on the COCO benchmark Lin et al. (2014).", "startOffset": 0, "endOffset": 894}, {"referenceID": 3, "context": "To this end we present two regularization algorithms for the batch CCA problem based on the Bjork Golub Algorithm Golub et al. (1973) (Algorithm 1), that make the computation of the full regularization path for CCA efficient.", "startOffset": 114, "endOffset": 134}, {"referenceID": 3, "context": "To this end we present two regularization algorithms for the batch CCA problem based on the Bjork Golub Algorithm Golub et al. (1973) (Algorithm 1), that make the computation of the full regularization path for CCA efficient. We present our cross validation algorithms within two regularization frameworks: Tikhonov Regularization (Algorithm 2) and truncated SVD regularization (Algorithm 3) . While Tikhonov regularization is more popular for regularized CCAVinod (1976), we also derive the regularization path for CCA using truncated SVD covariances.", "startOffset": 114, "endOffset": 472}, {"referenceID": 3, "context": "To this end we present two regularization algorithms for the batch CCA problem based on the Bjork Golub Algorithm Golub et al. (1973) (Algorithm 1), that make the computation of the full regularization path for CCA efficient. We present our cross validation algorithms within two regularization frameworks: Tikhonov Regularization (Algorithm 2) and truncated SVD regularization (Algorithm 3) . While Tikhonov regularization is more popular for regularized CCAVinod (1976), we also derive the regularization path for CCA using truncated SVD covariances. Truncated SVD is a popular regularizer for least squares regression problems Hansen (1986), but to our knowledge has not been utilized before in the CCA Context.", "startOffset": 114, "endOffset": 644}, {"referenceID": 19, "context": "We follow this general principle of mapping the space of the search to the space of the query rather than the converse as found in Socher et al. (2013). The solution of Problem (1) is given simply by T = C \u2212 1 2 ,\u22a4 XX CXY C \u2212 1 2 Y Y , hence for a query test sentence y we can find its corresponding image by finding :", "startOffset": 131, "endOffset": 152}, {"referenceID": 7, "context": "We review in this section Canonical Correlation Analysis due to Hotelling (1936). For data matrices X \u2208 Rx and Y \u2208 Ry , let k = min(mx,my) the canonical correlation \u03c31, .", "startOffset": 64, "endOffset": 81}, {"referenceID": 2, "context": "Lemma 1 (Golub et al. (1973)).", "startOffset": 9, "endOffset": 29}, {"referenceID": 0, "context": "While the original algorithm of Bjork and Golub uses the QR factorization of X and Y , we follow Avron et al. (2014) in exposing the algorithm fully with SVD.", "startOffset": 97, "endOffset": 117}, {"referenceID": 9, "context": "Klein et al. (2015), and Gong et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity.", "startOffset": 12, "endOffset": 31}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al.", "startOffset": 12, "endOffset": 302}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case.", "startOffset": 12, "endOffset": 339}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case. The symmetric weighting in Gong et al. (2014) was a heuristic found to improve performance and is not theoretically motivated as in the asymmetric weighting of this paper.", "startOffset": 12, "endOffset": 391}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case. The symmetric weighting in Gong et al. (2014) was a heuristic found to improve performance and is not theoretically motivated as in the asymmetric weighting of this paper. Indeed our experimental results show that asymmetric weighting gives higher performance and makes the image search and the image annotation on par in term of performance. A discussion of the optimality of the asymmetric weighting and an empirical study is given in Appendix C. Skip thought vectors introduced in Kiros et al. (2015) for representing sentences were used for bidirectional search in conjunction with VGG features Simonyan & Zisserman (2015) on the image side, with linear embeddings learned with a discriminative triplets loss, instead of a CCA loss.", "startOffset": 12, "endOffset": 849}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case. The symmetric weighting in Gong et al. (2014) was a heuristic found to improve performance and is not theoretically motivated as in the asymmetric weighting of this paper. Indeed our experimental results show that asymmetric weighting gives higher performance and makes the image search and the image annotation on par in term of performance. A discussion of the optimality of the asymmetric weighting and an empirical study is given in Appendix C. Skip thought vectors introduced in Kiros et al. (2015) for representing sentences were used for bidirectional search in conjunction with VGG features Simonyan & Zisserman (2015) on the image side, with linear embeddings learned with a discriminative triplets loss, instead of a CCA loss.", "startOffset": 12, "endOffset": 972}, {"referenceID": 4, "context": "(2015), and Gong et al. (2014) used CCA to build a joint representation using the cosine similarity. In both works a symmetric weighting of the CCA canonical weights was used, i.e for an image caption pair (x, y), a joint embedding of the form (\u03a3Ux,\u03a3V y), was used, where \u03b1 = 0, in Klein et al. (2015) case and \u03b1 > 0 in Gong et al. (2014) case. The symmetric weighting in Gong et al. (2014) was a heuristic found to improve performance and is not theoretically motivated as in the asymmetric weighting of this paper. Indeed our experimental results show that asymmetric weighting gives higher performance and makes the image search and the image annotation on par in term of performance. A discussion of the optimality of the asymmetric weighting and an empirical study is given in Appendix C. Skip thought vectors introduced in Kiros et al. (2015) for representing sentences were used for bidirectional search in conjunction with VGG features Simonyan & Zisserman (2015) on the image side, with linear embeddings learned with a discriminative triplets loss, instead of a CCA loss. A similar discriminative loss was used for building a joint image and text representation in Lebret et al. (2015). One advantage of CCA over the discriminative losses is that the batch solution can be found efficiently and does not need stochastic optimization, as well as the efficient model selection introduced in this paper providing a means to select the dimension of the joint embedding leading to better generalization properties.", "startOffset": 12, "endOffset": 1196}, {"referenceID": 13, "context": "We performed image annotation and search tasks on the COCO benchmark Lin et al. (2014) using our task dependent asymmetrically weighted CCA, as described in Table 1.", "startOffset": 69, "endOffset": 87}, {"referenceID": 11, "context": "Similarly to Klein et al. (2015), we used the splits from Karpathy & Li (2015), and performed cross-validation on 5 validation splits of 5K images, and tested our models on 5 testing splits of 5K images each, as well as 25 splits of 1K images each.", "startOffset": 13, "endOffset": 33}, {"referenceID": 11, "context": "Similarly to Klein et al. (2015), we used the splits from Karpathy & Li (2015), and performed cross-validation on 5 validation splits of 5K images, and tested our models on 5 testing splits of 5K images each, as well as 25 splits of 1K images each.", "startOffset": 13, "endOffset": 79}, {"referenceID": 10, "context": "For feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image.", "startOffset": 33, "endOffset": 53}, {"referenceID": 10, "context": "For feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image.", "startOffset": 33, "endOffset": 135}, {"referenceID": 10, "context": "For feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image. The mean intensity of each crop is subtracted in each color channel, and then encoded by VGG19 (the final FC -4096 layer). The average of the resulting 10 feature vectors corresponding to each crop is used as the image representation. On the text side, we use two sentence embeddings: the first one proposed by Klein et al. (2015) consisting in averaging the word2vec Mikolov et al.", "startOffset": 33, "endOffset": 597}, {"referenceID": 10, "context": "For feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image. The mean intensity of each crop is subtracted in each color channel, and then encoded by VGG19 (the final FC -4096 layer). The average of the resulting 10 feature vectors corresponding to each crop is used as the image representation. On the text side, we use two sentence embeddings: the first one proposed by Klein et al. (2015) consisting in averaging the word2vec Mikolov et al. (2013) representation of words in each sentence.", "startOffset": 33, "endOffset": 656}, {"referenceID": 10, "context": "For feature extraction we follow Klein et al. (2015), and use on the image side the VGG CNN representation Simonyan & Zisserman (2015), where each image is cropped in 10 ways into 224 by 224 pixel images: the four corners, the center, and their x-axis mirror image. The mean intensity of each crop is subtracted in each color channel, and then encoded by VGG19 (the final FC -4096 layer). The average of the resulting 10 feature vectors corresponding to each crop is used as the image representation. On the text side, we use two sentence embeddings: the first one proposed by Klein et al. (2015) consisting in averaging the word2vec Mikolov et al. (2013) representation of words in each sentence. Word2vec representations are available on code.google.com/p/word2vec/, we followed the recommendation of Klein et al. (2015) in preprocessing the word2vec vocabulary matrix of COCO with PCA, to ensure averaging uncorrelated channels in the sentence embedding.", "startOffset": 33, "endOffset": 823}, {"referenceID": 10, "context": "The second embedding we used are skip thought vectors introduced in Kiros et al. (2015), which encodes sentences to vectors using an LSTM.", "startOffset": 68, "endOffset": 88}, {"referenceID": 10, "context": "5 Mean Vec (vocabICA +CCA) Klein et al. (2015) 24.", "startOffset": 27, "endOffset": 47}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.", "startOffset": 31, "endOffset": 51}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.9 60 74.6 NA 33.8 67.7 82.1 NA GMM+HGLMM+CCA Klein et al. (2015) 25.", "startOffset": 31, "endOffset": 119}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.9 60 74.6 NA 33.8 67.7 82.1 NA GMM+HGLMM+CCA Klein et al. (2015) 25.6 60.4 76.8 4.0 38.9 68.4 80.1 2.0 5K test images: BRNN Karpathy & Li (2015) 8.", "startOffset": 31, "endOffset": 199}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.9 60 74.6 NA 33.8 67.7 82.1 NA GMM+HGLMM+CCA Klein et al. (2015) 25.6 60.4 76.8 4.0 38.9 68.4 80.1 2.0 5K test images: BRNN Karpathy & Li (2015) 8.9 24.9 36.3 19.5 11.8 32.5 45.4 12.2 Mean Vec (vocabICA + CCA ) Klein et al. (2015) 10.", "startOffset": 31, "endOffset": 285}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.9 60 74.6 NA 33.8 67.7 82.1 NA GMM+HGLMM+CCA Klein et al. (2015) 25.6 60.4 76.8 4.0 38.9 68.4 80.1 2.0 5K test images: BRNN Karpathy & Li (2015) 8.9 24.9 36.3 19.5 11.8 32.5 45.4 12.2 Mean Vec (vocabICA + CCA ) Klein et al. (2015) 10.3 27.2 38.4 18.0 12.8 32.1 44.6 14.0 Mean Vec (vocabPCA+ AWT-CCA) 11.63 30.35 42.31 15 11.34 30.30 42.31 15 Skip thoughts (AWT-CCA) 11.61 31.70 44.42 14 12.37 32.08 44.37 14 Skip thoughts +Triplets loss Kiros et al. (2015) NA NA NA NA NA NA NA NA GMM+HGLMM + CCA Klein et al.", "startOffset": 31, "endOffset": 511}, {"referenceID": 10, "context": "0 Skip thoughts +Triplets loss Kiros et al. (2015) 25.9 60 74.6 NA 33.8 67.7 82.1 NA GMM+HGLMM+CCA Klein et al. (2015) 25.6 60.4 76.8 4.0 38.9 68.4 80.1 2.0 5K test images: BRNN Karpathy & Li (2015) 8.9 24.9 36.3 19.5 11.8 32.5 45.4 12.2 Mean Vec (vocabICA + CCA ) Klein et al. (2015) 10.3 27.2 38.4 18.0 12.8 32.1 44.6 14.0 Mean Vec (vocabPCA+ AWT-CCA) 11.63 30.35 42.31 15 11.34 30.30 42.31 15 Skip thoughts (AWT-CCA) 11.61 31.70 44.42 14 12.37 32.08 44.37 14 Skip thoughts +Triplets loss Kiros et al. (2015) NA NA NA NA NA NA NA NA GMM+HGLMM + CCA Klein et al. (2015) 11.", "startOffset": 31, "endOffset": 571}, {"referenceID": 11, "context": "We see in Table 2 that using off the shelf unsupervised features (VGG, word2vec, skip thoughts vectors), AWT-CCA achieves state of the art results at both image search and annotation, and competes with supervised features extraction such as GMM+HGLMM of Klein et al. (2015)2 .", "startOffset": 254, "endOffset": 274}, {"referenceID": 4, "context": "We solved in this paper CCA in its batch formulation, for handling larger scale datasets we can use the randomized SVD of Halko et al. (2011) or the subsampled CCA of Avron et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al.", "startOffset": 32, "endOffset": 102}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al. (2015), SGDCCA (Linear and Deep CCA) was proposed recently and can be applied to our problem.", "startOffset": 32, "endOffset": 122}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al. (2015), SGDCCA (Linear and Deep CCA) was proposed recently and can be applied to our problem. It would be interesting to study an efficient model selection for the SGD-CCA such as early stopping and truncated SVD for a finer model selection. A simple extension to kernel CCA regularization of this present paper would be to leverage random Fourier features for approximating Kernel CCA Lopez-Paz et al. (2014). Truncated SVD-CCA would be particularly interesting in this setting since kx, ky would be balancing the regularization and the estimation error of the kernel by means of random features.", "startOffset": 32, "endOffset": 525}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al. (2015), SGDCCA (Linear and Deep CCA) was proposed recently and can be applied to our problem. It would be interesting to study an efficient model selection for the SGD-CCA such as early stopping and truncated SVD for a finer model selection. A simple extension to kernel CCA regularization of this present paper would be to leverage random Fourier features for approximating Kernel CCA Lopez-Paz et al. (2014). Truncated SVD-CCA would be particularly interesting in this setting since kx, ky would be balancing the regularization and the estimation error of the kernel by means of random features. Note that the 1K results on Mean Vec (ICA +CCA) of Klein et al. (2015) and the skip thought of Kiros et al.", "startOffset": 32, "endOffset": 784}, {"referenceID": 0, "context": "(2011) or the subsampled CCA of Avron et al. (2014). Stochastic Gradient Descent CCA Ma et al. (2015); Wang et al. (2015), SGDCCA (Linear and Deep CCA) was proposed recently and can be applied to our problem. It would be interesting to study an efficient model selection for the SGD-CCA such as early stopping and truncated SVD for a finer model selection. A simple extension to kernel CCA regularization of this present paper would be to leverage random Fourier features for approximating Kernel CCA Lopez-Paz et al. (2014). Truncated SVD-CCA would be particularly interesting in this setting since kx, ky would be balancing the regularization and the estimation error of the kernel by means of random features. Note that the 1K results on Mean Vec (ICA +CCA) of Klein et al. (2015) and the skip thought of Kiros et al. (2015) are not comparable to ours since they are reported on only 1K test set, we report the mean of 25 splits 1K test sets.", "startOffset": 32, "endOffset": 828}], "year": 2017, "abstractText": "Joint modeling of language and vision has been drawing increasing interest. A multimodal data representation allowing for bidirectional retrieval of images by sentences and vice versa is a key aspect of this modeling. In this paper we show that canonical correlation analysis (CCA) can be adapted to bidirectional retrieval by a simple task dependent asymmetric weighting, which solves optimally the retrieval problem in a least squares sense. While regularizing CCA is known to improve numerical stability as well as generalization performance, less attention has been brought to the efficient computation of the regularization path of CCA, which is key to model selection. In this paper we develop efficient algorithms to compute the full regularization path of CCA within the classical Tikhonov and the truncated SVD (T-SVD CCA) regularization frameworks. T-SVD CCA is new to the best of our knowledge, and its regularization path can be computed more efficiently than its Tikhonov counterpart.", "creator": "LaTeX with hyperref package"}}}