{"id": "1511.06303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Alternative structures for character-level RNNs", "abstract": "recurrent neural networks are convenient and efficient models for language modeling. however, when applied on the level of characters instead of simple words, typically they suffer from several problems. in order to successfully model long - term dependencies, the hidden representation needs to be large. this in turn implies vastly higher computational costs, which can become prohibitive in practice. we propose two alternative structural modifications to fit the classical rnn model. the first one consists on conditioning the wrong character level representation on the previous word representation. the other deeper one uses the simpler character knowledge history to condition the output probability. additionally we evaluate the performance of the two newly proposed modifications on challenging, multi - lingual real world data.", "histories": [["v1", "Thu, 19 Nov 2015 18:46:21 GMT  (52kb)", "http://arxiv.org/abs/1511.06303v1", null], ["v2", "Tue, 24 Nov 2015 17:35:35 GMT  (52kb)", "http://arxiv.org/abs/1511.06303v2", "First revision. Updated Table 3, extended Sec. 5.3 and added a paragraph to the conclusion,"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["piotr bojanowski", "armand joulin", "tomas mikolov"], "accepted": false, "id": "1511.06303"}, "pdf": {"name": "1511.06303.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Piotr Bojanowski"], "emails": ["piotr.bojanowski@inria.fr", "tmikolov@fb.com", "ajoulin@fb.com", "leonb@fb.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 30\n3v 1\n[ cs\n.L G\n] 1"}, {"heading": "1 INTRODUCTION", "text": "Modeling sequential data is a fundamental problem in machine learning with applications in language modeling (Goodman, 2001), speech recognition(Young et al., 1997) and machine translation Koehn et al. (2007). In particular, recurrent neural network based language models (RNNs) are now widely used and have demonstrated state-of-the-art performance on many tasks, such as automatic speech recognition and statistical machine translation (Mikolov, 2012).\nWhile RNNs have been shown to outperform n-gram language models and feeedforward neural network language models in numerous experiments, they are mostly based on the word level information and thus are oblivious to subword information. For example, RNNs consider words such as \u201cbuild\u201d, \u201cbuilding\u201d and \u201cbuildings\u201d as orthogonal input vectors that share no similarity. This can potentially result in poor representation of words that are rarely seen during training. Even worse, the words that appear only in the test data, will not be represented at all. This situation can happen for languages with extremely large vocabularies, such as agglutinative languages where words can be created by concatenating morphemes (Finnish and Turkish being well-studied examples). Further, in many real-world applications, typos and spelling mistakes artificially increase the size of the vocabulary by adding several versions of the same word. This requires ad-hoc spell checking approaches that are disjoint from the main language modeling task.\n\u2217Most of the work was done while interning at Facebook AI Research.\nIn this paper, we investigate the use of character based recurrent neural networks (Char-RNNs) to capture subword information. While this type of models has been widely studied in the past (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al., 2012). This drop of performance is unlikely due to the difficulty for character level model to capture long term memory, since even Longer Short Term Memory (LSTM) models work better on words than characters (Graves, 2013). Another explanation could be that Char-RNNs have a very small number of parameters in their input and output matrices, compared to word level RNNs (W-RNNs). Increasing the capacity of char-RNNs can only be done by scaling up the size of the hidden layer, causing them to be computationally expensive. Ad-hoc solutions based on larger sub-word units seem to be able to both deal with new words and offer reasonable accuracy and training speed (Mikolov et al., 2012). However, these ad-hoc solutions have several issues: one has to specify how to create the sub-word units, which can differ from language to language; and a word can have multiple segmentation into the sub-word units. In this paper, we investigate extensions of a standard Char-RNN which do not require any ad-hoc constructions and can work on small vocabularies. These extensions aim at increasing the capacity of the model while limiting the access to it. An example of such sparse computation is the look-up table used over the input matrix of an W-RNN. Such sparse computation allows to use richer representations with little additional computational cost.\nFirst, we describe the standard RNN in the context of character prediction problem in Sec. 2, then we propose two different structural modifications of this model in order to speed-up computations in cases when the cardinality of the input and output symbols is low. The first modification, described in Sec. 3, combines two networks, one working with characters at the input, and the other with words. They are jointly trained in order to support each other. The second approach, described in Sec. 4, attempts to increase capacity of the RNN model by conditioning the softmax output on the recent history. This effectively allows to increase the total number of parameters in the model, without increasing the number of parameters that have to be accessed at every time step. While these extensions are presented in the contet of character level models, they can be apply to any problems where the input and output have a small number of possible symbols."}, {"heading": "1.1 RELATED WORK", "text": "Agglutinative languages such as Finnish or Turkish have very large vocabularies, making word based models impractical (Kurimo et al., 2006). Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsima\u0308ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages.\nA mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs.\nRecurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks. Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al., 2012).\nOthers have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013). Luong et al. (2013) build word embeddings by applying a recursive neural network over morpheme embeddings, while Bilmes & Kirchhoff (2003) build their embedding by concatenating features built on previously seen words."}, {"heading": "2 SIMPLE RECURRENT NETWORK", "text": "In this section, we describe the simple RNN model popularized by Elman (1990), in the context of language modeling. We formulate language modeling as a discrete sequence prediction problem. That is, we want to predict the next token in a sequence given its past. We suppose a fixed size dictionary of k words formed from d different characters. We denote by ct the one-hot encoding of t-th character in the sequence, and wp the one-hot encoding of the p-th word. Our basic unit is the character.\nAn RNN consists of an input layer, a hidden layer and an output layer. its hidden layer has a recurrent connection which allows the propagation through time of information. More precisely, the state of the m hidden units, ht is updated as a function of its previous state, ht\u22121 and the current character one-hot representation ct:\nht = \u03c3(Act +Rht\u22121),\nwhere \u03c3(x) 7\u2192 1/(1+exp(\u2212x)) is the pointwise sigmoid function, A is the m\u00d7d embedding matrix and R the m\u00d7m recurrent matrix. This hidden representation is supposed to act as a memory, and should be able to convey long-term dependencies. With sufficiently high-dimensional hidden representation, it should be a priori possible to store the whole history. However, using a big hidden layer implies high computational costs which are prohibitive in practice. Using its hidden representation, the RNN compute a probability distribution yt over the next character:\nyt = f(Uht),\nwhere U is a d\u00d7m matrix and f is the pointwise softmax function, i.e., [f(x)]i = exp(xi)/ \u2211 j exp(xj).\nOptimization. In order to learn the parameters \u03b8 = (A, R, U) of the model, we minimize the negative log-likelihood (NLL):\nNLL(\u03b8) = \u2212 T\u2211\nt=1\nc\u22a4t+1 log yt, (1)\nwith a stochastic gradient descent method and backpropagation through time (Rumelhart et al., 1985; Werbos, 1988). We clip the gradient in order to avoid gradient explosion. The details of the implementation are given in the experiment section.\nCharacter level RNNs have been shown to perform poorly compared to word level ones Mikolov et al. (2012). In particular, they require massive hidden layer in order to obtain results which are on par with word level models, this makes them very expensive to compute. In the following sections, we describe two different structural modification of the char-RNNs in order to add capacity while reducing the overall computational cost."}, {"heading": "3 CONDITIONING ON WORDS", "text": "In this section, we consider an extension of character level RNN by conditioning it to a word level information. This allow a more direct flow of information from the previous words to the character level prediction. We propose to condition the character level on a context vector zt as follow:\nht = \u03c3(Act +Rht\u22121 +Qzt), (2)\nwhere Q is a matrix. This context vector is built by gather information at word level using a word level RNN. The word level RNN is similar to the one described in the previous section. Its input for the p-th word is its one-hot representation wp. The context vector is then simply the state of the hidden layer gp. If the t-th character belongs to the p-th word, then zt = gp\u22121. Figure 1 provides an illustration of this hybrid word and character level RNN.\nIn order to train this model, we combine a loss on characters with a loss on words. However, computing a full softmax at the word level is expensive, in particular when facing large vocabularies. Many solutions have been proposed to reduce the cost of this step, such as hierachical softmax, or sampling classes. In this work, we simply restrict the word vocabulary for the output of the word level RNN. We keep the most frequent words (between 3000 and 5000) and associate the other ones to the <UNK> token. The loss that we use to train our model can be written as:\nNLL(\u03b8) = \u2212\u03bb T\u2211\nt=1\nc\u22a4t+1 log yt \u2212 (1\u2212 \u03bb) P\u2211\np=1\nw\u22a4p+1 log vp, (3)\nwhere vp is the prediction made by the word level RNN and \u03bb > 0 is the interpolation parameter. Note that the restricted vocabulary is only used for the output of the word level RNN, the rest of the model works on the large vocabulary. In the next section, we describe another structural modification that we propose to speed up language modeling on the character level."}, {"heading": "4 CONDITIONING PREDICTION ON RECENT HISTORY", "text": "In a character based RNN, the classifier has a very small number of parameters. We propose to condition this classifier on the recent contextual information to increase its capacity while keeping the computational cost constant. This context information is related to simple short-term statitics, such as coocurrence in a language. Explicitly modeling this information in the classifier, removes some of the burden from the hidden layer, allowing to use smaller recurrent matrices while maintaining the performance.\nThere are many relevant contextual information on which we can condition the classifier. In particular, simple short term dependencies are easily captured by n-grams, which are memory expensive but very efficient. Using such cheap information to condition the classifier of the RNN gives a simple way to increase the capacity of the model while encouraging the rest of the RNN to focus on more complex stastitical patterns. The condition of the classifier on n-grams can be written as a bilinear model. More precisely, denoting by nt the one-hot representation of the t-th n-gram, the prediction fo our model is:\nyt = f(n \u22a4 t Uht),\nwhere U is a tensor going from the product space of hidden representation and n-grams to character. Obviously, the exponential number of n-grams makes this model impossible to learn. Worse it is impossible to\ngeneralize at test time to unseen n-grams. To avoid these problems, we restrict our set of n-grams N to those that are smaller than a length N and appears frequently enough in the training set. If multiple n-grams can be used at the t-th character of the text, we fix nt to be the longest appearing in N . This insures that each n-gram nt is associated with enough example to learn a statistically meaningful tensor U . The great thing with this solution is the fact that apart from characters that don\u2019t appear in the training set, we always have a non-trivial output model at test time. In the worst case, this procedure will select the model corresponding to the last unigram. The n-gram cut-off frequence allows to control the overfitting of the model."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "We evaluate the proposed models on the Penn Treebank corpus and on a subset of the Europarl dataset. For the sake of simplicity, we compare our models to a plain RNN but all the modifications that we propose can be applied to more complex units (LSTM etc.). We train our model using stochastic gradient descent and select hyper parameters on a validation set. We set a constant learning rate \u03b3 and when the validation entropy starts to increase, we divide it by a factor \u03b1 after every epoch (values arround \u03b3 = 0.1 and \u03b1 = 1.5 work well in practice). Our implementation is a single threaded CPU code which could easily be parallelized. Code for training both models is publicly available1.\nWe evaluate our method using entropy in bits per character. It is defined as the empirical estimate of the cross-entropy between the target distribution and the model output in base 2. This corresponds to the negative log likelihood that we use to train our model up to a multiplicative factor: BPC(\u03b8) = 1\nT log(2) NLL(\u03b8)."}, {"heading": "5.1 EXPERIMENTS ON PENN TREEBANK", "text": "We first carry out experiments on the Penn Treebank corpus (Marcus et al., 1993). This a dataset with a training set composed of 930k normalized words, yielding a total of 5017k characters. All characters are in the ASCII format which leads to a limited size of character vocabulary C. The text was normalized and the word dictionary limited to 10000 most frequent words of the training set. The other words were replaced by a <UNK> token in the training, validation and testing sets.\nWe evaluate both models described in this paper on the Penn Treebank dataset. For the mixed model from Sec. 3 (Mixed), we fix size of the word-level hidden representations to 200. For the conditional model presented in Sec. 4 (Cond.), we choose the optimal N on the validation set. We compare these models with our own implementation of a character-level RNN as it allows us to fairly compare run times (a significant part of the code is shared). All models are trained for various sizes of the hidden layer m. We report entropy in bits per character on the validation set and the training time per epoch in Table 1.\n1https://github.com/facebook/Conditional-character-based-RNN\nThe character-level performance we obtain for the \u201cvanilla\u201d RNN is coherent with numbers published in the past on this dataset (Mikolov et al., 2012). We observe three important things: (a), for any size of character hidden layer, both proposed models perform better than the plain one. This of course comes at the expense of some additional computational cost and the benefits seem to decrease when the size of ht grows. (b) using this model, we manage to obtain a comparable performance to the heavy 1000-dimensional character RNN with a hidden representation of only 300. This corresponds to an important reduction in the number of recurent parameters and to a five times speedup per epoch at training time. (c) when the hidden representation is small, the best working model is the conditional one. However, when m gets larger, the mixed one seems to work best, and provides competitive entropy for a reasonable runtime.\nFor the conditional model, we observed in our experiments, that there seems to be a clear trade off in the choice of N , reached on Penn Treebank at roughly N = 1000. Indeed, in the limit case, when N is very large, we only have one output model, and the network is exactly equivalent to a RNN. On the other hand, when N is small, we keep a separate model for any sequence and therefore overfits to the training set."}, {"heading": "5.2 BINARY PENN TREEBANK", "text": "We carry out some experiments on the binary representation of Penn Treebank. As mentionned in the introduction, we would like to develop models that would be independent of the representation in use. Working with binary representation would allow to have models of sequential data that would be agnostic to the nature of the sequence. This could straightforwadly be applied to language modelling but also speech recognition directly from wave files etc.\nWe run the conditional model and a baseline bit-level RNN, both with a hidden representation of 100. For the conditional model we select the optimal N by choosing it on the validation set (N = 2000). We evaluate both models by computing the entropy per bit and per character. The results for this experiment are presented in Table 2. This setting corresponds to the extreme case where the dictionary is as small as it could be. The input and output model only have 2\u00d7m which can be a serious limitation for RNNs. As we see in Table 2, the conditional model works much better as it compensates this small output model by storing several ones instead."}, {"heading": "5.3 THE MULTILINGUAL EUROPARL DATASET", "text": "We perform another set of experiments on the Europarl dataset (Koehn, 2005). It is a corpus for machine translation with sentences from 20 different languages aligned with their English correspondence. For almost every language, there is more than 500k sentences, composed of more that 10M words. Because of its size, we restrict our experiments to a subset of sentences for each language. We randomly permute lines of the transcriptions, select 60k sentences for training, 10k for validation and 10k for testing. The permutation we use will be made publicly available upon publication.\nIn this experiment, as in the one described in Sec. 5.1, we compare our models to a character-level RNN. We train our mixed model with a word hidden of 200 and a character hidden of 300. For the conditional one, we fix the hidden representation and select the optimal N on the validation set. As a baseline, we train a character-level RNN for two sizes of hidden layers: 200 and 500. These results are summarized in Table 3. We group together \u201clight\u201d and \u201cheavy\u201d models togetger.\nCONCLUSION\nIn this work we investigated modifications of RNNs for general discrete sequence prediction when the number of symbols is very small, such as in char-RNNLM. We found that with certain tricks, one can train the model much faster, and overall we observed that the fully connected RNN architecture has its weaknesses, especially related to the excessive computational complexity. We believe more research is needed to develop general mechanisms that would allow us to train RNNs with richer internal structure. We expect such research can greatly simplify many pipelines, for example in the NLP applications where we could avoid having separate systems that perform spell checking, text normalization, and modeling of the language disjointly."}], "references": [{"title": "Turkish broadcast news transcription and retrieval. Audio, Speech, and Language Processing", "author": ["Arisoy", "Ebru", "Can", "Do\u011fan", "Parlak", "Siddika", "Sak", "Ha\u015fim", "Sara\u00e7lar", "Murat"], "venue": "IEEE Transactions on,", "citeRegEx": "Arisoy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2009}, {"title": "Factored language models and generalized parallel backoff", "author": ["Bilmes", "Jeff A", "Kirchhoff", "Katrin"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003\u2013short papers-Volume", "citeRegEx": "Bilmes et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bilmes et al\\.", "year": 2003}, {"title": "Morph-based speech recognition and modeling of out-of-vocabulary words across languages", "author": ["Creutz", "Mathias", "Hirsim\u00e4ki", "Teemu", "Kurimo", "Mikko", "Puurula", "Antti", "Pylkk\u00f6nen", "Janne", "Siivola", "Vesa", "Varjokallio", "Matti", "Arisoy", "Ebru", "Sara\u00e7lar", "Murat", "Stolcke", "Andreas"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "A bit of progress in language modeling", "author": ["Goodman", "Joshua T"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman and T.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and T.", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Unlimited vocabulary speech recognition with morph language models applied to finnish", "author": ["Hirsim\u00e4ki", "Teemu", "Creutz", "Mathias", "Siivola", "Vesa", "Kurimo", "Mikko", "Virpioja", "Sami", "Pylkk\u00f6nen", "Janne"], "venue": "Computer Speech & Language,", "citeRegEx": "Hirsim\u00e4ki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hirsim\u00e4ki et al\\.", "year": 2006}, {"title": "Mandarin word-character hybrid-input neural network language model", "author": ["Kang", "Moonyoung", "Ng", "Tim", "Nguyen", "Long"], "venue": "In INTERSPEECH, pp", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn", "Philipp", "Hoang", "Hieu", "Birch", "Alexandra", "Callison-Burch", "Chris", "Federico", "Marcello", "Bertoldi", "Nicola", "Cowan", "Brooke", "Shen", "Wade", "Moran", "Christine", "Zens", "Richard"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Unlimited vocabulary speech recognition for agglutinative languages", "author": ["Kurimo", "Mikko", "Puurula", "Antti", "Arisoy", "Ebru", "Siivola", "Vesa", "Hirsim\u00e4ki", "Teemu", "Pylkk\u00f6nen", "Janne", "Alum\u00e4e", "Tanel", "Saraclar", "Murat"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Kurimo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kurimo et al\\.", "year": 2006}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong", "Minh-Thang", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comput. Linguist.,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis, PhD thesis, Brno University of Technology.,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Mikolov", "Tomas", "Kombrink", "Stefan", "Deoras", "Anoop", "Burget", "Lukar", "Cernocky", "Jan"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Subword language modeling with neural networks. preprint (http://www", "author": ["Mikolov", "Tom\u00e1\u0161", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan", "J. Cernocky"], "venue": "fit. vutbr. cz/imikolov/rnnlm/char. pdf),", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Morphology-based language modeling for arabic speech recognition", "author": ["Vergyri", "Dimitra", "Kirchhoff", "Katrin", "Duh", "Kevin", "Stolcke", "Andreas"], "venue": "In INTERSPEECH,", "citeRegEx": "Vergyri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Vergyri et al\\.", "year": 2004}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural Networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}], "referenceMentions": [{"referenceID": 10, "context": ", 1997) and machine translation Koehn et al. (2007). In particular, recurrent neural network based language models (RNNs) are now widely used and have demonstrated state-of-the-art performance on many tasks, such as automatic speech recognition and statistical machine translation (Mikolov, 2012).", "startOffset": 32, "endOffset": 52}, {"referenceID": 16, "context": "While this type of models has been widely studied in the past (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al.", "startOffset": 62, "endOffset": 122}, {"referenceID": 19, "context": "While this type of models has been widely studied in the past (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al.", "startOffset": 62, "endOffset": 122}, {"referenceID": 17, "context": ", 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al., 2012).", "startOffset": 114, "endOffset": 136}, {"referenceID": 17, "context": "Ad-hoc solutions based on larger sub-word units seem to be able to both deal with new words and offer reasonable accuracy and training speed (Mikolov et al., 2012).", "startOffset": 141, "endOffset": 163}, {"referenceID": 11, "context": "Agglutinative languages such as Finnish or Turkish have very large vocabularies, making word based models impractical (Kurimo et al., 2006).", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 6, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 0, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 2, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 16, "context": "Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al.", "startOffset": 54, "endOffset": 114}, {"referenceID": 19, "context": "Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al.", "startOffset": 54, "endOffset": 114}, {"referenceID": 17, "context": ", 2011; Graves, 2013) or syllables (Mikolov et al., 2012).", "startOffset": 35, "endOffset": 57}, {"referenceID": 12, "context": "Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013).", "startOffset": 59, "endOffset": 137}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages.", "startOffset": 8, "endOffset": 87}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese.", "startOffset": 8, "endOffset": 309}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs.", "startOffset": 8, "endOffset": 369}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks.", "startOffset": 8, "endOffset": 611}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks. Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al., 2012). Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013). Luong et al. (2013) build word embeddings by applying a recursive neural network over morpheme embeddings, while Bilmes & Kirchhoff (2003) build their embedding by concatenating features built on previously seen words.", "startOffset": 8, "endOffset": 1081}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks. Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al., 2012). Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013). Luong et al. (2013) build word embeddings by applying a recursive neural network over morpheme embeddings, while Bilmes & Kirchhoff (2003) build their embedding by concatenating features built on previously seen words.", "startOffset": 8, "endOffset": 1200}, {"referenceID": 18, "context": "with a stochastic gradient descent method and backpropagation through time (Rumelhart et al., 1985; Werbos, 1988).", "startOffset": 75, "endOffset": 113}, {"referenceID": 15, "context": "Character level RNNs have been shown to perform poorly compared to word level ones Mikolov et al. (2012). In particular, they require massive hidden layer in order to obtain results which are on par with word level models, this makes them very expensive to compute.", "startOffset": 83, "endOffset": 105}, {"referenceID": 13, "context": "We first carry out experiments on the Penn Treebank corpus (Marcus et al., 1993).", "startOffset": 59, "endOffset": 80}, {"referenceID": 17, "context": "The character-level performance we obtain for the \u201cvanilla\u201d RNN is coherent with numbers published in the past on this dataset (Mikolov et al., 2012).", "startOffset": 127, "endOffset": 149}], "year": 2017, "abstractText": "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.", "creator": "LaTeX with hyperref package"}}}