{"id": "1511.06312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Good, Better, Best: Choosing Word Embedding Context", "abstract": "we propose two methods of learning vector representations of key words and phrases that each combine sentence context with relevant structural features extracted from dependency trees. using several variations of neural network classifier, we therefore show that these combined methods fundamentally lead to improved performance when used as input features for supervised term - size matching.", "histories": [["v1", "Thu, 19 Nov 2015 19:13:58 GMT  (12kb)", "http://arxiv.org/abs/1511.06312v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["james cross", "bing xiang", "bowen zhou"], "accepted": false, "id": "1511.06312"}, "pdf": {"name": "1511.06312.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bing Xiang", "Bowen Zhou"], "emails": ["crossj@oregonstate.edu", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 31\n2v 1\n[ cs\n.C L\n] 1\n9 N"}, {"heading": "1 Introduction", "text": "Continuous-space embeddings of words and phrases are very appealing for many applications in NLP. When used as a feature to represent words, they are much more compact than \u201cone-hot\u201d vectors. In addition, their geometry can encode a wealth of information about relationships between words, and thus their meanings, which can be exploited by downstream applications.\nThe critical question is how to best achieve that geometry for a given task. There are two types of training data which are often used to learn word vectors. Approaches based on the \u201cdistributional hypothesis\u201d assume that the semantics of a word can be determined through the contexts in which it appears. They model vectors through some form of dimension compression on word co-occurrence statistics taken from an example text. Other methods learn vectors from pre-existing information about specific relationships between words, such as those found in a structured knowledge base.\nWe propose two vector-learning methods which combine sentence-context input with structural features extracted automatically from dependency trees\nfor the same underlying corpus. We compare them with vectors trained from only one of these types of input, and show that they lead to improved performance on simple classification tasks."}, {"heading": "2 Related Work", "text": "Many different approaches to learning vector representations of words have been taken. Methods trained from large text examples by relying directly or indirectly on word co-occurrence counts date back to latent semantic analysis (Deerwester, 1990). Recent examples include the window-based approach of Mikolov et al. (2013c) and the GloVe method (Pennington et al., 2014). There has also been work incorporating prior knowledge with bagof-words context (Yu and Dredze, 2014).\nIt is also possible to perform unsupervised learning on structured information about the underlying entities or concepts being represented, such as the relationships encoded in an online knowledge base. Approaches for utilizing this type of training data include the use of neural networks and/or kernel density estimation (Bordes et al., 2011) and encoding relations as translations on hyperplanes (Wang et al., 2014).\nThe most direct precursor to our work is the wellknown Word2Vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). It consists of a simple single-layer neural model where the goal is to maximize the ability of the word representations to predict neighbors of the word in a large raw-text corpus. It was observed by Levy and Goldberg (2014) that this algorithm could be generalized to use any type of discrete features with which words\ncould be paired as the \u201ccontexts\u201d to be predicted."}, {"heading": "3 Combined Objective", "text": "In this work, we combine sentence-window context with structural features described in Section 4. The joint objective consists of using both types of training data in parallel so that the training objective is to maximize the combined log probability:\n1\nT\nT\u2211\nt=1\n\u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog p(wt+j |wt)\n+ \u03b1\n|P |\n\u2211\n(w,f)\u2208P\nlog p(f |w)\n(1)\nwhere T is the number of words in the training corpus, c is the context window size, P is the set of all pairs of words and structural features (w, f), and \u03b1 is a weighting parameter as between the two types of training data.\nThe conditional probabilities are modeled with vectors as in Word2Vec, and two sets of output vectors, internal to the algorithm, are learned to represent words and structural features as contexts. This objective is implemented with a joint algorithm which performs updates using the sentence context and the word/feature pairs in parallel. Both types of updates are based on the negative sampling training method (Mikolov et al., 2013c).\nWe also implemented a sequential algorithm where vectors trained with just sentence-window context are used as initial representations which are subsequently refined by updates using the structural features. The relative success of vectors trained in this way suggests that proceeding from a more general representation toward one which is more directly tailored for a given task is a promising avenue.\nIn the sections that follow, we describe a number of simple classification experiments designed to compare sets of word embeddings trained using these various methods. For the term-matching tasks we considered, the structural features alone led to better-performing vectors than those trained with just linear word context, while the combinedobjective methods provided further improvement."}, {"heading": "4 Structural Features", "text": "As a representative means of training word vectors on functional relationships between words in\nsentences, rather than just word proximity, we began with first-order dependency arcs of the type described in Levy and Goldberg (2013). For every dependency arc, each word is annotated with a feature which consists of the arc label, whether the word is the head or tail of the arc, and the adjoined word.1 We also extracted two types of higher-order features from the dependency trees: noun-relationship patterns, and subject-object relationships.\nNoun-relationship features capture certain taxonomic relationships between nouns, as reflected in the structure of a sentence, such as when one word names an instance of a category defined by another word, or when they are different names for the same thing. This methodology is based on the syntactic patterns used to recognize is-a relationships in (Fan et al., 2011). Examples of these dependency patterns and the relationships they identify can be seen in Table 1.\nIn addition, second-order features directly linking subjects and objects of transitive verbs (and including the verb) are also extracted. Thus for the sentence \u201cThe woman ate the paella\u201d the word \u201cwoman\u201d will be annotated with a feature indicating that it is the subject of the verb \u201cate\u201d with the direct object \u201cpaella\u201d, and \u201cpaella\u201d will receive a feature marking it as the object of \u201cate\u201d with the subject\n1Also included from that work and in the \u201cDependency\u201d test set are features coming from the \u201cflattening\u201d of prepositional phrases to directly link the modified head and the object of the preposition (with the preposition itself included in the feature name).\n\u201cwoman\u201d. The rationale for using both of these types of features is that nouns contain much of the semantic nexus of a sentence, and they have direct dependency relationships with essentially all other types of words. By refining word vectors based on specific relationships between nouns, the quality of the vector representations as a whole can be improved.\nThe features used to train vectors for our experiments were extracted from dependency trees produced by the Stanford Neural Network Dependency Parser (Chen and Manning, 2014) using the provided pre-trained model with Stanford dependency labels. The underlying corpus for all methods was the full text of English Wikipedia as of June 15, 2014.2"}, {"heading": "5 Qualitative Comparison", "text": "To evaluate the relative qualities captured by the various embedding methods, we looked at the closest words to a number of different target words, just as in Levy and Goldberg (2014). Unsurprisingly, the nearest words were very similar across the different vector sets, as all were trained with the same base corpus.\nIn particular, jointly-trained vectors lead to similar words which are subjectively very much like those produced by the vectors trained with structural features only. There is reason to believe, however, that the joint-objective vectors capture some additional semantic nuance stemming from the inter-\n2As maintained by the KOPI project (Pataki et al., 2012).\nsection of (and interplay between) topical and functional similarity.\nFor the word \u201cpart-of-speech\u201d the most similar words under all vector sets skew toward hyphenated or compound modifiers (though the set induced by bag-of-words training includes nominals). While all of the vector sets seem to reflect the fact that this modifier is frequently used with the word \u201ctag(s),\u201d the jointly-trained vectors also capture its affinity with linguistics in such similar words as \u201cverbnoun\u201d and \u201cmultiword.\u201d Likewise with other words such as \u201cdynamic\u201d and \u201cnormalize\u201d the jointlytrained vectors show properties which are similar to those using structural features alone, but also seem to encode meaning in a somewhat more targeted way (\u201cever-changing\u201d; \u201chomogenize\u201d)."}, {"heading": "6 Experiments", "text": "In order to compare their predictive properties quantitatively, we tested word embeddings produced with each method as inputs to binary term-matching classifiers using two different data sets, described below. We used shallow neural network classifiers with three alternative architectures, with a uniform hidden-layer size of 200. The results are presented in Table 3.\nFor the standard multilayer perceptron (\u201cMLP\u201d) algorithm, the vector representations of each word in a pair were concatenated to make the input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013). We also used a version where the two vectors were each used as input to the same hidden layer, i.e., the weights were shared, and the hidden-layer output for each, together with their element-wise absolutevalue difference, were concatenated as input to the logistic regression classifier layer (\u201cShared MLP\u201d).\nThe third classifier similarly used shared weights to process each vector input (with a hyperbolic tangent activation function), but classification was made based on the cosine similarity of the hiddenlayer output, with a threshold of 0.5. In this way, the network learns how to project the vectors into a space where their angular separation is a good measure of their similarity for the task at hand. We observed that this network converged very fast during training, but had performance nearly on par with\nmore complicated models."}, {"heading": "6.1 WordNet Synonyms", "text": "The first source of term-matching pairs we considered were synonyms from the WordNet lexical database (Miller, 1995). WordNet groups words into synsets which are groups of words (and multi-word phrases) with the same meaning. We created match data by taking all distinct pairs of words within synsets of any part of speech, and generated negative examples by random shuffling.3\nThis turns out to be a relatively difficult task with unsupervised vectors alone. A possible reason is that the vectors do not distinguish between word senses, essentially folding together all usage cases into a single representation. The WordNet data is characterized by relatively common words where the synonymy may rely on a secondary or figurative definition. Examples of this in our data are the pairs (fastball, bullet) and (arrest, collar). In particular, since the raw-text data tends to group words by domain of discourse rather than function, it makes sense that a figurative usage as for \u201cbullet\u201d and \u201ccollar\u201d in those pairs might cause problems.\nIn this case, the structural features were much more useful than bag-of-words context. The pattern-\n3The WordNet data, as filtered for coverage by these vector sets, consists of 21,089 training pairs, 2,636 development pairs, and 2,637 test pairs, split evenly between matches and non-matches.\nbased features also led to a clear improvement over using just the subset of first-order dependency relations, which we hypothesize is largely due to the subject-object relations. The joint vectors showed little gain over using structural features only. This seems linked to the large gap in utility between the two types of training data, and points to the importance of application domain in determining embedding methods. The sequentially trained vectors performed better across the board, however, which suggests the usefulness of \u201cpriming\u201d in this type of vector learning algorithm."}, {"heading": "6.2 Paraphrase Database Lexical Pairs", "text": "As another source of term-matching data, we used pairs from the small (highest-confidence) lexical paraphrase dataset of the Paraphrase Database (Ganitkevitch et al., 2013). They consist of instances where two distinct words were used to mean the same thing in bi-texts. In contrast to the WordNet pairs, this dataset skews toward less common words. It includes many instances of things like alternate spellings and alternate names (or transliterations) for proper nouns.4\nHere, the performance difference between the raw-text and structural-feature vectors was much smaller. The combined-objective methods were better than either. This suggests that when the two types of training data have comparable degrees of utility, but work through different avenues, a combined training method can create embeddings which capture the advantages of both approaches."}, {"heading": "7 Conclusion and Future Work", "text": "It is clear that the best method of training continuous word embeddings depends on the domain of application. When comparably useful information is encoded in both sentence context and structured representations, combining both as training data can lead to superior vector representations.\nFor future work, we plan to try using richer feature sets, such as knowledge-base relations, as training inputs, and explore the mathematics behind the sequential training mechanism.\n4The PPDB data consists of 79,696 training pairs, 9,962 validation pairs, and 9,962 test pairs, split 20%/80% between matches and non-matches."}], "references": [{"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Mining Knowledge from Large Corpora for Type Coercion in Question Answering. Web Scale Knowledge Extraction (WEKEX", "author": ["Fan et al.2011] James Fan", "Aditya Kalyanpur", "J. William Murdock", "Branimir K. Boguraev"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2011}, {"title": "Automatic knowledge extraction from documents", "author": ["Fan et al.2012] James Fan", "Aditya Kalyanpur", "D.C. Gondek", "David A. Ferrucci"], "venue": "IBM Journal of Research and Development", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "PPDB: The Paraphrase Database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Dependency-Based Word Embeddings. In Association for Computational Linguistics (ACL)", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models. In Workshop on Deep Learning for Audio, Speech, and Language Processing", "author": ["Maas et al.2013] Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Continuous Space Word Representations", "author": ["lya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: a lexical database for English. Communications of the ACM", "author": ["George A. Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improving Lexical Embeddings with Semantic Knowledge. In Association for Computational Linguistics (ACL)", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "(2013c) and the GloVe method (Pennington et al., 2014).", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "Recent examples include the window-based approach of Mikolov et al. (2013c) and the GloVe method (Pennington et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 1, "context": "Approaches for utilizing this type of training data include the use of neural networks and/or kernel density estimation (Bordes et al., 2011) and encoding relations as translations on hyperplanes (Wang et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 15, "context": ", 2011) and encoding relations as translations on hyperplanes (Wang et al., 2014).", "startOffset": 62, "endOffset": 81}, {"referenceID": 10, "context": "The most direct precursor to our work is the wellknown Word2Vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). It consists of a simple single-layer neural model where the goal is to maximize the ability of the word representations to predict neighbors of the word in a large raw-text corpus. It was observed by Levy and Goldberg (2014) that this algorithm could be generalized to use any type of discrete features with which words", "startOffset": 75, "endOffset": 370}, {"referenceID": 4, "context": "This methodology is based on the syntactic patterns used to recognize is-a relationships in (Fan et al., 2011).", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "For the standard multilayer perceptron (\u201cMLP\u201d) algorithm, the vector representations of each word in a pair were concatenated to make the input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013).", "startOffset": 212, "endOffset": 252}, {"referenceID": 9, "context": "For the standard multilayer perceptron (\u201cMLP\u201d) algorithm, the vector representations of each word in a pair were concatenated to make the input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013).", "startOffset": 212, "endOffset": 252}, {"referenceID": 13, "context": "The first source of term-matching pairs we considered were synonyms from the WordNet lexical database (Miller, 1995).", "startOffset": 102, "endOffset": 116}, {"referenceID": 6, "context": "As another source of term-matching data, we used pairs from the small (highest-confidence) lexical paraphrase dataset of the Paraphrase Database (Ganitkevitch et al., 2013).", "startOffset": 145, "endOffset": 172}], "year": 2015, "abstractText": "We propose two methods of learning vector representations of words and phrases that each combine sentence context with structural features extracted from dependency trees. Using several variations of neural network classifier, we show that these combined methods lead to improved performance when used as input features for supervised term-matching.", "creator": "LaTeX with hyperref package"}}}