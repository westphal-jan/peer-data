{"id": "1511.06425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks", "abstract": "in this paper, we propose and clearly study a novel visual space object tracking approach based on convolutional interaction networks and recurrent networks. collectively the proposed approach philosophy is distinct simultaneously from the existing approaches to visual data object tracking, such as filtering - based ones and tracking - by - detection ones, in the complementary sense that the tracking system is explicitly trained off - line to track anonymous objects in a noisy environment. the proposed visual tracking model is end - to - end trainable, minimizing any mutual adversarial effect from mismatches in object handler representation and between merging the true underlying dynamics and learning dynamics. we empirically show that the proposed tracking approach works well in various dynamic scenarios especially by generating artificial color video sequences with varying conditions ; the number of objects, amount of noise and the match between randomly the training shapes and test shapes.", "histories": [["v1", "Thu, 19 Nov 2015 22:24:15 GMT  (589kb,D)", "http://arxiv.org/abs/1511.06425v1", null], ["v2", "Wed, 25 Nov 2015 19:44:15 GMT  (660kb,D)", "http://arxiv.org/abs/1511.06425v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["quan gan", "qipeng guo", "zheng zhang", "kyunghyun cho"], "accepted": false, "id": "1511.06425"}, "pdf": {"name": "1511.06425.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECURRENT NEURAL NETWORKS", "Quan Gan", "Qipeng Guo", "Zheng Zhang", "Kyunghyun Cho"], "emails": ["zz@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Visual object tracking is a problem of building a computational model that is able to predict the location of a designated object from a video clip consisting of many consecutive video frames. Using deep learning techniques, we attack this problem with a model-free statistical approach.\nOur goal in this paper is to build a model that can track an anonymous object in an image sequence. This task finds immediate applications in important scenarios such as self-driving cars. As safety is first-order consideration, identifying what class the object belongs to is much less critical than identifying their whereabouts to avoid a collision. It is also an important step towards dramatically improving the generalizability of a tracking system, since real-world objects far exceeds labelled categories.\nOur model integrates convolutional network with recurrent network, and deploys attention at multiple representation layers. The recurrent network outputs a bounding box prediction of the target object. It fuses past predictions along with their corresponding visual features produced by the convolutional network. Finally, the prediction can optionally emphasizes attention areas in the input before feeding it into convolutional network. The entire model is end-to-end trained off-line. We use synthesized data set that simulates changing trajectory and acceleration of the target, various degree of foreground occlusions, and distraction of background clutter and other targets. Experimental results show that our model delivers robust performance.\nThe rest of the paper is organized as follows. We start by reviewing two important categories of conventional visual object tracking in Sec. 2, which are filtering-based tracking and tracking-bydetection. In Sec. 3, we describe a novel recurrent tracking model we propose in this paper, followed by discussing related works in Sec. 3.4. The settings for experiments are extensively described in Sec. 4\u20135, which is followed by the results and analysis in Sec. 6. We finalize this article with potential future research directions in Sec. 7.\n? Equal contribution Fudan University, Shanghai qgan11@fudan.edu.cn,guoqipeng831@gmail.com \u2022 Courant Institute of Mathematical Sciences, New York University {zz,kyunghyun.cho}@nyu.edu \u2217 NYU Shanghai \u25e6 Center for Data Science, New York University\nar X\niv :1\n51 1.\n06 42\n5v 1\n[ cs\n.C V\n] 1\n9 N\nov 2\n01 5"}, {"heading": "2 BACKGROUND: VISUAL OBJECT TRACKING AND LIMITATIONS", "text": "A system for visual object tracking often comprises two main components; object detection and object tracking. Object detection refers to the process by which a designated object in each video frame is detected, while object tracking refers to the process of continually predicting the location of the detected object.\nThe goal of object detection is to extract a feature vector \u03c6(x) of an object which often encodes both the object\u2019s shape and location, given each video frame x. The specifics of the feature vector heavily depend on the choice of object representation. For instance, if an object is represented as a point, the feature vector of the detected object is a two-dimensional coordinate vector corresponding to the center of gravity of the object in each frame.\nObject detection is followed by object tracking. There are many approaches proposed over a number of decades (see, e.g., Yilmaz et al. (2006),) and we are interested in this proposal a statistical approach. A statistical approach assumes that the feature vector \u03c6(x) from the object detection stage is a noisy observation of the true, underlying state (location) of the object which is not observed. The goal is to infer the underlying state for each video frame, given a sequence of observations, i.e., features vectors.\nFiltering-based Visual Object Tracking In filtering-based object tracking, it is natural to establish a probabilistic graphical model, or often referred to as a state-space model, by defining\n\u2022 Observation model: p(\u03c6(xt)|zt) \u2022 Transition model: p(zt |zt\u22121),\nwhere \u03c6(xt) and zt are the observation and the latent state at time t.\nOne of the most well known filtering-based tracking model is so called Kalman filter Broida & Chellappa (1986). Kalman filter assume that both the observation and transition models are Gaussian distributions such that\n\u03c6(xt)|zt \u223cN (Wxzt ,Cx), zt |zt\u22121 \u223cN (Wzzt\u22121,Cz), where N (\u00b5,C) is a Gaussian distribution with its mean \u00b5 and covariance matrix C.\nOnce the model is defined, the goal is to infer the following posterior distribution:\np(zt |\u03c6(x1),\u03c6(x2), . . .\u03c6(xt)). (1) In other words, we try to find the distribution over the potential object location in the t-th video frame given all the detected objects (represented by the feature vectors) up to the (t\u2212 1)-th frame. See Fig. 1 (a) for the graphical illustration.\nIn this scheme, object detection and tracking are largely considered separate from each other. This means that the object tracking stage is designed to work while being blind to the object representation or feature vectors from the object detection stage. However, this is not to say that there is no effect of the choice of object and/or feature representation, as mismatch between the model definition in object tracking and the distribution based on the selected feature representation will lead to suboptimal tracking result.\nTracking-by-Detection An approach more relevant to our proposal is tracking-by-detection (see, e.g., Li et al. (2013).) This approach is more holistic than the previous approach, because a single model is trained online to track an object in interest. In other words, tracking-by-detection builds a discriminative model that directly approximates the posterior distribution over the underlying location/state of the object in Eq. (1). See Fig. 1 (b) for the graphical illustration.\nOften, tracking-by-detection is done not as regression but as classification of regions of a video frame. The classifier is initialized to work well on the first few frames where a separate object detector or human expert has classified each region of the frames as either foreground or background. This classifier is used to detect/track the object in the next frame of which ground-truth labeling is not available. Once each region in the next frame is classified, the classifier is further fine-tuned with these new examples. This continues until the video clip reaches its end or the object disappears (all regions are classified negative.)\nThis approach of tracking-by-detection has a number of limitations, of which the most severe ones are drifting (accumulation of error over multiple frames) and inability to handle occlusion easily. We notice that both of these issues arise from the fact that this approach effectively assumes Markov property over the posterior distribution in Eq. (1), meaning\np(zt |\u03c6(x1),\u03c6(x2), . . .\u03c6(xt)))\u2248 p(zt |\u03c6(xt\u22121)).\nBecause the tracking model considers only two consecutive frames (or more precisely concentrates heavily on a latest pair of consecutive frames only), it is not possible for the model to adjust for accumulated error over many subsequent frames. If the model has access to the whole history of frames and its own prediction of the object\u2019s locations, it will be possible for the model to take the errors made throughout the video clip and address the issue of drifting.\nThe tracking model\u2019s lack of access to the history of the previous frames makes it nearly impossible for the model to handle occlusion. On the other hand, if the tracking model has the history of its previous observations and predictions of the object\u2019s location, it can more easily infer that the object has not disappeared totally from the view but moved behind some other background objects. The tracking model may even be able to infer the object\u2019s location even while it is being hidden from the view by understanding its motion based on the history the object\u2019s locations.\nLimitations We have noticed three limitations in these conventional approaches. First, object representation is designed independently from the rest of the tracking pipeline, and any suboptimal choice of object representation may negatively impact the subsequent stages. Second, the filteringbased approach is not robust to a mismatch between the underlying model description and the reality. Third, the lack of access to the history of all the previous video frames makes tracking-by-detection sensitive to complex motions and structured noise such as occlusion. Lastly, tracking-by-detection requires a classifier, or a regressor, to be tuned at each frame, making it less suitable to be applied in real-time applications."}, {"heading": "3 VISUAL OBJECT TRACKING WITH DEEP NEURAL NETWORKS", "text": "Here we describe a novel approach to visual anonymous object tracking by using techniques from deep learning (LeCun et al., 2015). Our aim in introducing a novel approach is to build a system that avoids the four limitations of the conventional visual tracking systems we discussed in the earlier section.\nThe proposed model is a deep recurrent network that takes as input raw video frames of pixel intensities and returns the coordinates of a bounding box of an object being tracked for each frame. Mathematically, this is equivalent to saying that the proposed model factorizes the full tracking probability into\np(z1,z2, . . . ,zT |x1,x2, . . . ,xT ) = T\n\u220f t=1 p(zt |z<t ,x\u2264t),\nwhere zt and xt are the location of an object and an input frame, respectively, at time t. z<t is a history of all previous locations before time t, and x\u2264t is a history of all input frames up to time t.\n3.1 MODEL DESCRIPTION\nAt each time step t, an input frame xt is first processed by a convolutional network, which has recently become a de facto standard in handling visual input (see, e.g., LeCun et al., 1998; Krizhevsky et al., 2012). This results in a feature vector \u03c6(xt) of the input frame xt :\n\u03c6(xt) = conv\u03b8 c(m(xt , z\u0303t\u22121)), (2)\nwhere conv\u03b8 c(\u00b7) is a convolutional network with its parameters \u03b8 c, and m(\u00b7, \u00b7) is a preprocessing routine for the raw frame. We will discuss this preprocessing routine at the end of this section. z\u0303t\u22121 is the predicted location of an object from the previous frame xt\u22121, which we describe in more detail below.\nThis feature vector of the input frame is fed into a recurrent neural network. The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt):\nht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3)\nwhere rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r. This formulation lets the recurrent neural network to summarize the history of predicted locations z<t and input frames x\u2264t up to time step t.\nWith the newly updated memory state ht , the recurrent neural network computes the predictive distribution over the object\u2019s location (see Eq. (1). This is done again by a deep neural network out\u03b8 o Pascanu et al. (2014):\np(zt |z<t ,x\u2264t) = out\u03b8 o(ht), (4) where \u03b8 o is a set of parameters defining the output neural network. We take the mean of this predictive distribution as a predicted location z\u0303t at time t:\nz\u0303t = E [z|z<t ,x\u2264t ] . (5)\nThis whole process (Eqs. (2)\u2013(5)) is iteratively applied as a stream of new frames arrives. This is graphically illustrated in Fig. 2.\nPreprocessing Input Frame xt We considered a number of possible strategies for building a preprocessing routine m(\u00b7, \u00b7) from Eq. (2). The following two approaches are implemented and compared. The most obvious and straightforward choice is to simply have an identity function, which will be equivalent to simply passing a raw frame into the convolutional network conv\u03b8 c .\nOn the other hand, we can design a preprocessing function m such that it will facilitate tracking. One possible choice is to weight each pixel of the raw frame xt such that a region surrounding the predicted location of an object in the previous frame is given higher weights. This will help the subsequent layers, i.e., conv\u03b8 c , rec\u03b8 r and out\u03b8 o , to focus more on that region. For instance, it is possible to adapt the weight mechanism from the selective attention model recently proposed by Gregor et al. (2015), which relies on both the current frame xt and the previous prediction z\u0303t\u22121. Later we explain a specific variant of the weight mechanism used in the experiments more in detail."}, {"heading": "3.2 TRAINING", "text": "Unlike the existing approaches to visual object tracking, described in Sec. 2, we take an off-line training strategy. This means that the trained model is used as is in test time.\nAn obvious advantage to this approach is that there is no overhead in finetuning the model on-the-fly during test time. On the other hand, there is an obvious disadvantage that the model must be able\nto track an object whose shape or texture information was not available during training time. We propose here a training strategy that aims at overcoming this disadvantage.\nWe were motivated from recent observations from many research groups that a deep convolutional network, pretrained on a large image dataset of generic, natural images, is useful for subsequent vision tasks which may not necessarily involve the same types of objects (see, e.g., Sermanet et al., 2013; Bar et al., 2015). As the model we propose here consists of a convolutional network and recurrent network, we expect a similar benefit by training the whole model with generic shapes, which may not appear during test time.\nAs usual when training a deep neural network, we use stochastic gradient descent (SGD). At each SGD update, we generate a minibatch of examples by the following steps:\n1. Select a random background image from a large set of image. 2. Randomly choose a shape of an object from a predefined set of generic shapes. 3. Create a sequence of frames by randomly moving the selected object with cluttered back-\nground and foreground. 4. (Optional) Add various types of noise, including motion and scale change of both object\nand clutters.\nAfter these steps, each training example is a pair of a video clip, which contains a randomly chosen background and a moving shape, and a sequence of ground-truth locations, i.e., ((x1,z\u22171), . . . ,(xT ,z \u2217 T )).\nWe use this minibatch of N generated examples to compute the gradient of the minibatch loglikelihood L , where\nL (\u03b8 c,\u03b8 r,\u03b8 o) = 1 N\nN\n\u2211 n=1\nT\n\u2211 t=k+1 log p(znt = z \u2217 t |z\u2217<t ,x\u2264t).\nAs this is an anonymous object tracking system, the model is given the ground-truth locations of the object for the first k frames.\nAnother training criterion is possible, if our prediction z\u0303t as each step t is a differentiable function. In this case, we let the model freely track an object given a training video sequence and maximize the log-probability of the ground-truth location only at the last frame:\nL (\u03b8 c,\u03b8 r,\u03b8 o) = 1 N\nN\n\u2211 n=1 log p(znT = z \u2217 T |z\u0303<T ,x\u2264T ). (6)\nAccording to our preliminary experiments, we use this second strategy throughout this paper.\nOf course, in this case, there is no guarantee that any intermediate prediction made by the model correspond to the correct object location. To avoid this issue, we add the following auxiliary cost to the new cost above:\nL\u0303 (\u03b8 c,\u03b8 r,\u03b8 o) = 1 N\nN\n\u2211 n=1\nT\n\u2211 t=k+1 log p(znt = z \u2217 t |z\u0303<T ,x\u2264T ). (7)\nMinimizing this auxiliary cost encourages the model to following the object in the intermediate frames.\nIn our case, the model predicts two points zt = [x0,y0,x1,y0] in the input frame which corresponds to the top-left (x0,y0) and bottom-right (x1,y1) corners of a bounding box. We use a Gaussian distribution with an fixed, identity covariance matrix, whose mean is computed from ht (see Eq. (3).) In order to reduce variance, we do not sample from this distribution, but simply take the mean as the prediction:\nz\u0303t = E [zt |z\u0303<T ,x\u2264T ] .\nThis effectively reduces the auxiliary cost in Eq. (7) as well as the main cost in Eq. (6) to a meansquared error.\nWe can use, for instance, ImageNet Data from http://www.image-net.org/, or create one with random clutters as in MNIST cluttered from https://github.com/deepmind/mnist-cluttered.\nIn the case of using the selective attention model to preprocess the input frame, there are two additional output elements which are the standard deviation \u03c3 and the stride \u03b4 . As we consider both of these as real values, this simply makes the predictive distribution to be a six-dimensional vector, i.e., dim(zt) = 4."}, {"heading": "3.3 CHARACTERISTICS", "text": "There are three main characteristics that set the proposed approach apart from the previous works on visual object tracking.\nFirst, the proposed model is trained end-to-end, including object representation extraction, object detection and object tracking. The model works directly on the raw pixel intensities of each frame. This is unlike conventional object tracking systems, in which appearance modeling is considered largely separate from the actual tracking system (see, e.g., Li et al., 2013). This largely prevents potential performance degradation from having suboptimal, hand-engineered object representation and detector.\nSecond, the proposed model works with anonymous objects by design. As we train a model with a large set of generic-shaped objects offline, the model learns to detect a generic object that was pointed out initially rather than to detect a certain, predefined set of objects. As the proposed model is a recurrent neural network which can maintain the history of the object\u2019s trajectory, it implicitly learns to find a region in an input frame which has a similar activation pattern from the previous frames. In fact, human babies are known to be able to track objects even before having an ability to classify it into one of the object categories Ruff & Rothbart (2001).\nLastly, training is done fully off-line. We note first that this is both an advantage and disadvantage of the proposed approach. This off-line training strategy is advantageous in test time, as there is no need to re-tune any part of the system. As the model parameters are fixed during test time, it will be more straightforward to implement the trained model on a hardware, achieving a desirable level of power consumption and speed (Farabet et al., 2011). On the other hand, this implies that the proposed system lacks the adaptability to novel objects that are novel w.r.t. the shapes used during training, which is exactly a property fully exploited by any existing tracking-by-detection visual tracking system."}, {"heading": "3.4 RELATED WORK", "text": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network. Here let us describe the similarities and differences between their recurrent attentive tracking model (RATM) with the proposed tracking approach.\nA major common feature between these two approaches is that both of these use a recurrent neural network as a main component. A major difference between the RATM and the proposed approach is in training. Both the RATM and the model proposed in this paper use the intermediate locations of an object as an auxiliary target (see Eq. (7).) Kahou et al. (2015) report that this use of auxiliary cost stabilized the tracking quality, which is further confirmed by our experiments presented later in this paper.\nA major difference is that Kahou et al. (2015) used a classification error, averaged over all the frames, as a final cost, while we propose to use the final localization error. Furthermore, they use the selective attention mechanism from (Gregor et al., 2015), allowing the RATM only a small subregion of the whole canvas at each frame. This is contrary to the recurrent visual tracker proposed in this paper which has access to the full frame every time.\nEarlier, Denil et al. (2012) proposed a visual object tracking system based on deep neural networks. Their model can be considered as an intermediate step away from the conventional tracking approaches toward the one proposed here and by Kahou et al. (2015). They used a restricted Boltzmann machine (Smolensky, 1986) as an object detection model together in a filtering-based visual tracking (state-space model with particle filtering for inference.)\nAlthough they are not specifically for visual object tracking, two recent works on image/video description generation tasks have shown that a recurrent network, together with a convolutional network, is able to attend (detect and localize) to objects in both an image and a video clip (Xu et al.,\n2015; Yao et al., 2015). Similarly, Mnih et al. (2014) and Ba et al. (2014) showed that a recurrent network tracks an object if it were trained to classify an object, or multiple objects, in an image."}, {"heading": "4 DATA GENERATION", "text": "We evaluate the proposed approach of visual anonymous object tracking on artificially generated datasets. We vary the configurations of generating these datasets to make sure that the following empirical evaluations support our claim and conjectures about the proposed model.\nAll the datasets are based on the cluttered MNIST. Each video sequence used for training consists of 20 frames, and each frame is 100\u00d7 100 large. The cluttered MNIST was chosen as a basis of generating further datasets, as one of the most important criterion we aim to check with the proposed approach is the robustness to noise. In order to make sure that these clutters acts as both background noise and as objects hindering the sight of the models, we put some clutters in a background layer and the others in a foreground layer (overshadowing the target object.) Furthermore, the clutters move rather than stay in the initial positions to make it more realistic.\nThe target has a random initial velocity (vx0 ,vy0) and position (x0,y0). At each time frame, the position is changed by (\u2206xt ,\u2206yt) = (kvx,t\u22121,kvy,t\u22121) where k is a hyper-parameter (0.1 in our experiments) correlated to the frame rate. We change the velocity by (\u2206vx,t ,\u2206vy,t) \u223cN (0,v\u2032I), where v\u2032 = 0.1 is a hyper-parameter controlling the scale of velocity changes. This change in the velocity introduces acceleration, making it more difficult for a tracking system.\nTo ensure that our dataset is as realistic as possible, we include other transformations. For example, at each time step, the target changes its scale by a random factor f = pexp( f\u0303 ), where f\u0303 \u223c U (\u22120.5,0.5) and p = 0.1 controls the magnitude of scale change. Finally, the intensities of each clutter and the moving MNIST digit are uniform-randomly scaled between 64 and 255 (before normalization.)\nMultiple Digits We evaluate our model on two different cases. In the first case, there is only a single digit moving around in each video sequence. Although there are clutters in the background/foreground, this setting is sufficiently easy and can be considered as a sanity check. We call this dataset MNIST-Single-Same.\nThe second dataset, MNIST-Multi-Same, contains frames of which each contains more than one digits. More specifically, we generate each video sequence such that there are two digits simultaneously moving around. In order for the tracking system to work well, the system needs to be able to detect the characteristics of the object in interest from the first few frames (when the ground-truth locations are given) and maintain it throughout the sequence.\nNovel Test Digit Classes As our goal is to build an anonymous object tracking system. We evaluate a trained model with sequences containing objects that do not appear during training time. More specifically, we test the models on two sets of sequences containing one or two MNIST-2 digits, where one MNIST-2 digit is created by randomly overlapping two randomly selected normal MNIST digits on top of each other (Wang et al., 2014). We call these datasets MNIST-Single-Diff and MNIST-Multi-Diff, respectively.\nGeneralization to Longer Video Sequence In all the cases, we evaluate a trained model on test sequences that are longer than the training sequences. This is a necessary check for any model based on recurrent neural networks, as some recent findings suggest that on certain tasks recurrent neural networks fail to generalize to longer test sequences (Joulin & Mikolov, 2015; Grefenstette et al., 2015). We vary the lengths of test sequences among {20,40,80,160}, while all the models are trained with 20-frame-long training sequences."}, {"heading": "5 MODELS AND TRAINING", "text": "We test five models on each of the four cases, MNIST-{Single,Multi}-{Same,Diff}.\nhttps://github.com/deepmind/mnist-cluttered\nRecurrent Visual Object Tracker (RecTracker-X) The first model, RecTracker-ID, is the proposed recurrent visual object tracker (see Sec. 3.1.) As described earlier, it consists of a convolutional network and a recurrent network. In this case, we use an identity function to preprocess each input frame xt :\nm(xt , z\u0303t\u22121) = xt , (8)\nwhere z\u0303t\u22121 is ignored. This is equivalent to simply letting the tracker have a full, unadjusted view of the whole frame.\nWe also test the same model with a different preprocessing routine. We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking. A model employing this novel weighting scheme is is referred to by RecTracker-Att-N, where N denotes the size of grid G. We emphasize that we only weight the pixels of each frame and do not extract a patch, as was done by Gregor et al. (2015) and Kahou et al. (2015), because this approach of ignoring an out-of-patch region may lose too much information needed for tracking.\nIn this model, the recurrent network outputs the coordinates of (1) top-left corner (x0,y0), (2) bottom-right corner (x1,y1), (3) log-scale s, (4) log-ratio r between the stride and the image size, and (5) log-amplitude a. Given these output, we weight each pixel by a mixture of N\u00d7N Gaussians. Each Gaussian (i, j) is centered at(\nx0 + x1 2 +\n( i\u2212 N\n2 \u22120.5\n) exp(r)K,\ny0 + y1 2 +\n( j\u2212 N\n2 \u22120.5\n) exp(r)K ) ,\nand has the standard deviation of exp(s) K2 . K corresponds to the width or height of a frame. These Gaussians are used to form a mask G(zt\u22121) which is used as\nm(xt ,zt\u22121) = xt \u00b7G(zt\u22121) (9)\nWe refer to this preprocessing routine as an attentive weight scheme.\nIn this paper, we only evaluate the case of N = 1, RecTracker-Att-1.\nConvolutional Network Only Tracker (ConvTracker) The third model is a simpler variant of the proposed model where the recurrent network is omitted from the proposed recurrent visual tracking model. Instead, this model considers four frames (three preceding frames + current frame) and predict the location of an object at the current frame. We only test the identity preprocessing routine in Eq. (8). We call this model ConvTracker. Other than the omission of the recurrent network, all the other details are identical to those of RecTracker-ID, RecTracker-Att-1 and RecTracker-Att-3.\nKernelized Correlation Filters based Tracker (KerCorrTracker) Lastly, we use the kernelized correlation filters-based visual tracker proposed by Henriques et al. (2015). This is one of the stateof-the-art visual object tracking systems and follows a tracking-by-detection strategy (see Sec. 2.) This third model is chosen to highlight the differences between the existing tracking approaches and the proposed one."}, {"heading": "5.1 NETWORK ARCHITECTURES", "text": "We describe the architectures of each network\u2013convolutional and recurrent networks\u2013 in Appendix A."}, {"heading": "5.2 TRAINING", "text": "We describe the architectures of each network\u2013convolutional and recurrent networks\u2013 in Appendix B.\nhttp://home.isr.uc.pt/\u02dchenriques/circulant/"}, {"heading": "6 RESULT AND ANALYSIS", "text": ""}, {"heading": "6.1 EVALUATION METRIC", "text": "We use the intersection-over-union (IOU) as an evaluation metric. The IOU is defined as IOU(z\u2217t , z\u0303t) = \u2223\u2223M\u2217\u2229 M\u0303\u2223\u2223\u2223\u2223M\u2217\u222a M\u0303\u2223\u2223 ,\n(0.05 in our experiments) where M\u2217 and M\u0303 are binary masks whose pixels inside a bounding box (either ground-truth \u2217 or predicted )\u0303 are 1 and otherwise 0. A higher IOU implies better tracking quality, and it is bounded between 0 and 1. For each video sequence, we compute the average IOU across all the frames."}, {"heading": "6.2 QUANTITATIVE ANALYSIS", "text": "Importance of Recurrent Network First, we compare the ConvTracker and RecTracker-ID. They both use the identity preprocessing routine, and the only difference is that the latter has a recurrent network while the former does not. The results, in terms of the average IOU, are presented in Table 1.\nIn this table, the importance of having a recurrent layer is clearly demonstrated. Across all the cases\u2013 different data configurations and test sequence lengths\u2013 the RecTracker-ID significantly outperforms the ConvTracker. Also, we notice that the tracking quality degrades as the length of test sequences increases (up to 8 folds.)\nEffect of Attentive Weight Scheme Next, we evaluate the effect of the attentive weight scheme (see Eq. (9) and surrounding text) against the simple identity preprocessing scheme (see Eq. (8) and surrounding text.) From Table 2, we notice an interesting pattern. When the training and test shapes are of the same classes (similar shapes), it is better to use the identity preprocessing scheme (RecTracker-ID). On the other hand, RecTracker-Att-1 outperforms the simpler one, when the shapes are different between training and test.\nComparison against Tracking-by-Detection Table 3 compares the tracking quality between the proposed recurrent tracking model RecTracker-Att-1 and the kernelized correlation filters based\none KerCorrTracker by Henriques et al. (2015). We observe that RecTracker-Att-1 outperforms the KerCorrTracker when there\u2019s only a single object in a test sequence. However, when there are two objects in a sequence and the model was asked to track only one of them, the RecTracker-Att1 and KerCorrTracker perform comparably to each other. When the sequence is short, the latter results in better tracking, while with longer sequences, the former does better.\nWe however remind readers that this result should be taken with a grain of salt. A more accurate comparison will be to compare these two in more realistic settings involving natural scenes, real objects and dynamics. We leave this as future research.\nMulti-Object Tracking As a preliminary investigation into multi-object visual tracking, we ran the proposed recurrent trackers to track two moving digits simultaneously. The models were trained with the training sequences of having two digits (MNIST-Multi-Same or MNIST-Multi-Diff) to track both digits.\nIn Table 4, we present the results using ConvTracker, RecTracker-ID and RecTracker-Att-1. The under-performing ConvTracker clearly demonstrates that it is important for a tracking model to be capable of capturing temporal dynamics. Among the proposed recurrent tracking models, the RecTracker-ID performs better when the same types of objects are used both during training and test, but in the other case the RecTracker-Att-1 does better. This is similar to the observation we made when these models were tracking a single object.\nIt is however unclear how the proposed tracking model can be extended to track arbitrarily many objects, and thus, the multi-object tracking experiments are preliminary. In the future, the investigation is needed on the extension of the proposed recurrent tracking model to an arbitrary number of objects."}, {"heading": "6.3 VISUALIZATION OF TRACKING", "text": "We have prepared video clips of tracking results by all the tested models at http://barclayii. github.io/tracking-with-rnn. See Fig. 3 for one such example.\nOur visual inspection reveals that\n1. ConvTracker model is failing in most test cases. Showing that the model is unable to track anonymous objects. A possible reason is that the memory it possesses, either the memory for remembering positions, or for remembering features, is too small. Such incapability calls for a recurrent structure which has more memory capacity than normal feed-forward networks.\n2. For RecTracker-ID models, there are cases where the model loses focus at the very first frame. Further frames showed that the model still follows the wrong object. This demonstrates the necessity for attention.\n3. The RecTracker-Att-1 model mainly stays on the object we want to track, but when another object is brighter, the model is distracted away. The reason may be that when overlaying the image with a weighted mask, the darker pixels are further reduced, while the brighter ones are not reduced that much, emitting a stronger signal to the RNN, stating that the region with the brighter object is more significant."}, {"heading": "7 CONCLUSION", "text": "In this paper, we proposed an end-to-end trainable visual object tracking model based on convolutional and recurrent neural networks. Unlike conventional tracking approaches, the full pipeline of visual tracking\u2013object representation, object extraction and location prediction\u2013 is jointly tuned to maximize the tracking quality. The proposed tracking model combines many recent advances in deep learning and was found to well perform on challenging, artificially generated sequences.\nWe consider this work as a first step toward building a full end-to-end trainable visual object tracking system. There are a number of issues to be investigated and resolved in the future. First, the proposed models must be evaluated on natural scenes with real objects and their dynamics. Second, there needs to be research on algorithms to adapt a pre-trained model online. Third, we need to find a network architecture that can track an arbitrary number of objects without a predefined upper limit."}, {"heading": "A NETWORK ARCHITECTURES", "text": "Convolutional Network We use a single convolutional layer with 32 10\u00d710 filters. These filters are applied with stride 5 to the input frame. As it is important to maintain as much spatial information as possible for tracking to work well, we do not use any pooling. This convolutional layer is immediately followed by an element-wise tanh.\nIn the case of ConvTracker, a fully-connected layer with 200 tanh units follows the convolutional layer. This fully-connected layer also receives as the input the predicted locations of the four preceding frames.\nRecurrent Network We use 200 gated recurrent units (GRU, Cho et al., 2014) to build a recurrent network. At each time step, the activation of the convolutional layer (see above) and the predicted object location z\u0303t\u22121 in the previous frame are fed into the recurrent network."}, {"heading": "B TRAINING", "text": "We train each model up to 50 epochs, or until the training cost stops improving, using a training set of 3,200,000 randomly-generated examples. We use RMSProp, which was implemented according to (Graves, 2013), with minibatches of size 32 examples."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Deep learning with non-medical training used for chest pathology identification", "author": ["Bar", "Yaniv", "Diamant", "Idit", "Wolf", "Lior", "Greenspan", "Hayit"], "venue": "In SPIE Medical Imaging, pp. 94140V\u201394140V. International Society for Optics and Photonics,", "citeRegEx": "Bar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bar et al\\.", "year": 2015}, {"title": "Estimation of object motion parameters from noisy images", "author": ["Broida", "Ted J", "Chellappa", "Rama"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Broida et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Broida et al\\.", "year": 1986}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Large-scale fpga-based convolutional networks", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "High-speed tracking with kernelized correlation filters", "author": ["J.F. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Henriques et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henriques et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Ratm: Recurrent attentive tracking model", "author": ["Kahou", "Samira Ebrahimi", "Michalski", "Vincent", "Memisevic", "Roland"], "venue": "arXiv preprint arXiv:1510.08660,", "citeRegEx": "Kahou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kahou et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A survey of appearance models in visual object tracking", "author": ["Li", "Xi", "Hu", "Weiming", "Shen", "Chunhua", "Zhang", "Zhongfei", "Dick", "Anthony", "Hengel", "Anton Van Den"], "venue": "ACM transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representation (ICLR),", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Attention in early development: Themes and variations", "author": ["Ruff", "Holly Alliger", "Rothbart", "Mary Klevjord"], "venue": null, "citeRegEx": "Ruff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ruff et al\\.", "year": 2001}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Attentional neural network: Feature selection using cognitive feedback", "author": ["Wang", "Qian", "Zhang", "Jiaxing", "Song", "Sen", "Zheng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Object tracking: A survey", "author": ["Yilmaz", "Alper", "Javed", "Omar", "Shah", "Mubarak"], "venue": "Acm computing surveys (CSUR),", "citeRegEx": "Yilmaz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": ", Yilmaz et al. (2006),) and we are interested in this proposal a statistical approach.", "startOffset": 2, "endOffset": 23}, {"referenceID": 15, "context": ", Li et al. (2013).) This approach is more holistic than the previous approach, because a single model is trained online to track an object in interest.", "startOffset": 2, "endOffset": 19}, {"referenceID": 13, "context": "At each time step t, an input frame xt is first processed by a convolutional network, which has recently become a de facto standard in handling visual input (see, e.g., LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 157, "endOffset": 213}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r.", "startOffset": 289, "endOffset": 307}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r.", "startOffset": 289, "endOffset": 369}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r. This formulation lets the recurrent neural network to summarize the history of predicted locations z<t and input frames x\u2264t up to time step t. With the newly updated memory state ht , the recurrent neural network computes the predictive distribution over the object\u2019s location (see Eq. (1). This is done again by a deep neural network out\u03b8 o Pascanu et al. (2014): p(zt |z<t ,x\u2264t) = out\u03b8 o(ht), (4) where \u03b8 o is a set of parameters defining the output neural network.", "startOffset": 289, "endOffset": 802}, {"referenceID": 8, "context": "For instance, it is possible to adapt the weight mechanism from the selective attention model recently proposed by Gregor et al. (2015), which relies on both the current frame xt and the previous prediction z\u0303t\u22121.", "startOffset": 115, "endOffset": 136}, {"referenceID": 1, "context": "We were motivated from recent observations from many research groups that a deep convolutional network, pretrained on a large image dataset of generic, natural images, is useful for subsequent vision tasks which may not necessarily involve the same types of objects (see, e.g., Sermanet et al., 2013; Bar et al., 2015).", "startOffset": 266, "endOffset": 318}, {"referenceID": 5, "context": "As the model parameters are fixed during test time, it will be more straightforward to implement the trained model on a hardware, achieving a desirable level of power consumption and speed (Farabet et al., 2011).", "startOffset": 189, "endOffset": 211}, {"referenceID": 14, "context": ", Li et al., 2013). This largely prevents potential performance degradation from having suboptimal, hand-engineered object representation and detector. Second, the proposed model works with anonymous objects by design. As we train a model with a large set of generic-shaped objects offline, the model learns to detect a generic object that was pointed out initially rather than to detect a certain, predefined set of objects. As the proposed model is a recurrent neural network which can maintain the history of the object\u2019s trajectory, it implicitly learns to find a region in an input frame which has a similar activation pattern from the previous frames. In fact, human babies are known to be able to track objects even before having an ability to classify it into one of the object categories Ruff & Rothbart (2001). Lastly, training is done fully off-line.", "startOffset": 2, "endOffset": 820}, {"referenceID": 8, "context": "Furthermore, they use the selective attention mechanism from (Gregor et al., 2015), allowing the RATM only a small subregion of the whole canvas at each frame.", "startOffset": 61, "endOffset": 82}, {"referenceID": 20, "context": "They used a restricted Boltzmann machine (Smolensky, 1986) as an object detection model together in a filtering-based visual tracking (state-space model with particle filtering for inference.", "startOffset": 41, "endOffset": 58}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network. Here let us describe the similarities and differences between their recurrent attentive tracking model (RATM) with the proposed tracking approach. A major common feature between these two approaches is that both of these use a recurrent neural network as a main component. A major difference between the RATM and the proposed approach is in training. Both the RATM and the model proposed in this paper use the intermediate locations of an object as an auxiliary target (see Eq. (7).) Kahou et al. (2015) report that this use of auxiliary cost stabilized the tracking quality, which is further confirmed by our experiments presented later in this paper.", "startOffset": 32, "endOffset": 648}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network. Here let us describe the similarities and differences between their recurrent attentive tracking model (RATM) with the proposed tracking approach. A major common feature between these two approaches is that both of these use a recurrent neural network as a main component. A major difference between the RATM and the proposed approach is in training. Both the RATM and the model proposed in this paper use the intermediate locations of an object as an auxiliary target (see Eq. (7).) Kahou et al. (2015) report that this use of auxiliary cost stabilized the tracking quality, which is further confirmed by our experiments presented later in this paper. A major difference is that Kahou et al. (2015) used a classification error, averaged over all the frames, as a final cost, while we propose to use the final localization error.", "startOffset": 32, "endOffset": 844}, {"referenceID": 4, "context": "Earlier, Denil et al. (2012) proposed a visual object tracking system based on deep neural networks.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": "Earlier, Denil et al. (2012) proposed a visual object tracking system based on deep neural networks. Their model can be considered as an intermediate step away from the conventional tracking approaches toward the one proposed here and by Kahou et al. (2015). They used a restricted Boltzmann machine (Smolensky, 1986) as an object detection model together in a filtering-based visual tracking (state-space model with particle filtering for inference.", "startOffset": 9, "endOffset": 258}, {"referenceID": 15, "context": "Similarly, Mnih et al. (2014) and Ba et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "(2014) and Ba et al. (2014) showed that a recurrent network tracks an object if it were trained to classify an object, or multiple objects, in an image.", "startOffset": 11, "endOffset": 28}, {"referenceID": 21, "context": "More specifically, we test the models on two sets of sequences containing one or two MNIST-2 digits, where one MNIST-2 digit is created by randomly overlapping two randomly selected normal MNIST digits on top of each other (Wang et al., 2014).", "startOffset": 223, "endOffset": 242}, {"referenceID": 7, "context": "This is a necessary check for any model based on recurrent neural networks, as some recent findings suggest that on certain tasks recurrent neural networks fail to generalize to longer test sequences (Joulin & Mikolov, 2015; Grefenstette et al., 2015).", "startOffset": 200, "endOffset": 251}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking.", "startOffset": 85, "endOffset": 106}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking. A model employing this novel weighting scheme is is referred to by RecTracker-Att-N, where N denotes the size of grid G. We emphasize that we only weight the pixels of each frame and do not extract a patch, as was done by Gregor et al. (2015) and Kahou et al.", "startOffset": 85, "endOffset": 394}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking. A model employing this novel weighting scheme is is referred to by RecTracker-Att-N, where N denotes the size of grid G. We emphasize that we only weight the pixels of each frame and do not extract a patch, as was done by Gregor et al. (2015) and Kahou et al. (2015), because this approach of ignoring an out-of-patch region may lose too much information needed for tracking.", "startOffset": 85, "endOffset": 418}, {"referenceID": 9, "context": "Kernelized Correlation Filters based Tracker (KerCorrTracker) Lastly, we use the kernelized correlation filters-based visual tracker proposed by Henriques et al. (2015). This is one of the stateof-the-art visual object tracking systems and follows a tracking-by-detection strategy (see Sec.", "startOffset": 145, "endOffset": 169}, {"referenceID": 9, "context": "one KerCorrTracker by Henriques et al. (2015). We observe that RecTracker-Att-1 outperforms the KerCorrTracker when there\u2019s only a single object in a test sequence.", "startOffset": 22, "endOffset": 46}], "year": 2017, "abstractText": "In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.", "creator": "LaTeX with hyperref package"}}}