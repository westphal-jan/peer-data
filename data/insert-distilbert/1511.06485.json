{"id": "1511.06485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "On the energy landscape of deep networks", "abstract": "instead we study here a theoretical model that connects deep learning to finding either the ground state of the hamiltonian of a spherical spin glass. existing results motivated from statistical physics show that deep networks have a highly non - convex energy landscape with exponentially many symmetric local minima and energy perception barriers beyond which frequency gradient descent algorithms cannot make progress. we leverage a technique known as topology trivialization where, upon induced perturbation by an external magnetic field, the energy landscape of the spin glass hamiltonian changes simultaneously dramatically from exponentially many local minima to \" rotational total trivialization \", denoted i. e., a constant number of local minima. there there also exists a transitional regime with thus polynomially many local minima which interpolates between these latter extremes. we show that a number versions of regularization schemes in deep learning can benefit from this phenomenon. as importantly a consequence, precisely our analysis provides order heuristics to choose regularization parameters \u2014 and motivates annealing schemes aiming for manipulating these perturbations.", "histories": [["v1", "Fri, 20 Nov 2015 04:31:05 GMT  (2005kb,D)", "http://arxiv.org/abs/1511.06485v1", "Submission to ICLR'16"], ["v2", "Mon, 4 Jan 2016 08:43:53 GMT  (8812kb,D)", "http://arxiv.org/abs/1511.06485v2", "Submission to ICLR'16, updated with experiments on CNNs"], ["v3", "Thu, 7 Jan 2016 19:06:59 GMT  (5215kb,D)", "http://arxiv.org/abs/1511.06485v3", "Submission to ICLR'16, updated with experiments on CNNs"], ["v4", "Mon, 8 Feb 2016 02:20:02 GMT  (9607kb,D)", "http://arxiv.org/abs/1511.06485v4", null], ["v5", "Fri, 21 Apr 2017 22:56:46 GMT  (4351kb,D)", "http://arxiv.org/abs/1511.06485v5", null]], "COMMENTS": "Submission to ICLR'16", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pratik chaudhari", "stefano soatto"], "accepted": false, "id": "1511.06485"}, "pdf": {"name": "1511.06485.pdf", "metadata": {"source": "CRF", "title": "TRIVIALIZING THE ENERGY LANDSCAPE OF DEEP NETWORKS", "authors": ["Pratik Chaudhari", "Stefano Soatto"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning has enjoyed enormous success in recent years in applications like image recognition (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al., 2015). The fact that similar architectures and concepts are sweepingly applicable to such vastly different problems is both surprising and profound. Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc. This paper is in the spirit of results that analyze algorithms for training deep networks using knowledge of the landscape of the cost function, e.g., Dauphin et al. (2014); Haeffele and Vidal (2015); Janzamin et al. (2015); Choromanska et al. (2014).\nWe derive motivation from the theory of spin glasses which are some of the oldest models for neural networks (Hopfield, 1982). In recent years, the connections of spin glasses to combinatorial optimization, random k-SAT, LDPC codes etc. have become popular (Mezard and Montanari, 2009) and an understanding of the energy landscape and thermodynamics of such systems has led to a number of efficient algorithms (Krzakala et al., 2011; Braunstein et al., 2005). Spin glasses are particularly attractive models for large-scale problems such as deep networks because although their energy landscape is a high-dimensional, non-convex Gaussian surface, using results from probability theory and statistical physics, we can explicitly compute various quantities of interest such as the number of stationary points (Auffinger et al., 2013; Fyodorov, 2013), low temperature topology and the structure of the Gibbs distribution (Talagrand, 2003; Mezard et al., 1987).\nIn this work, we consider a model for deep networks with random, sparse weights and connect its loss function to the Hamiltonian of a spherical spin glass in order to analyze and modify its energy\n\u2217Computer Science, UCLA. pratikac@ucla.edu \u2020Computer Science, UCLA. soatto@cs.ucla.edu\nar X\niv :1\n51 1.\n06 48\n5v 1\n[ cs\n.L G\n] 2\n0 N\nov 2\n01 5\nlandscape. Existing results drawn from statistical physics have shown that there are exponentially many local minima and saddle points of the Hamiltonian and gradient descent algorithms cannot make progress due to such a landscape. An effective way of changing this topology is to add an external perturbation. We show that there exists a critical threshold of the perturbation below which the exponentially many stationary points persist and above which the landscape becomes \u201ctrivial\u201d, i.e., there is only one local minimum. This phase transition is however not sharp, i.e., there exists a small band around the perturbation threshold that interpolates between the two extremes resulting in polynomially many stationary points. This polynomial regime gives a good trade-off between a complex landscape with exponentially many local minima and a completely degenerate loss function due to large perturbations.\nWe interpret topology trivialization as a regularization of the original loss function of a deep network, for instance, additive noise is immediately seen as a random perturbation of the Hamiltonian. The analysis in this paper gives the right magnitude of noise to achieve polynomially many stationary points. We extend the idea to other types of regularization, e.g, weight decay, dropout, multiplicative noise and compute the appropriate magnitude of the regularization parameters in the presence of perturbations. The analysis also motivates a very natural annealing scheme for the perturbations. The magnitude of the perturbation is gradually changed to transition from the trivialized landscape in the beginning to recover the original landscape as the training progresses."}, {"heading": "1.1 RELATED WORK", "text": "Our work is closely related to Choromanska et al. (2014) where the authors use spin glasses to analyze the energy landscape of deep networks. They show in their experiments that although there are exponentially many saddle points in the energy landscape, the Hessian at the optima recovered by stochastic gradient descent (SGD) has very few negative eigenvalues. Choromanska et al. (2014) and Sagun et al. (2014) also show that SGD fails to progress beyond a very specific energy barrier, one that can be seen in spin glass computations as the onset of high-order saddle points and local minima. We build upon these results and demonstrate a way to use topology trivialization to modify this energy landscape so as to make it more amenable to gradient descent.\nThe random, sparse model discussed in Sec. 2.1 is motivated from recent results like Arora et al. (2014) and Neyshabur and Panigrahy (2013). Empirically, a number of works such as Denil et al. (2013); Chen et al. (2015) have reported that as much as 95% connections in a typical deep network have almost zero weights. Such considerations have also resulted in state-of-the-art architectures, for instance, the \u201cinception network\u201d (Szegedy et al., 2014) maintains a high-dimensional and sparse feature map at each layer and compresses it when convolutions of these feature maps become expensive. While a random, sparse network is unlikely to be a good model for convolutions, it allows us to make necessary approximations to get a good analytical handle on its properties.\nAs we train deep networks on increasingly large and complex problems, hyper-parameter search is fast becoming essential (Bergstra and Bengio, 2012; Snoek et al., 2012). Parameters such as kernel sizes, mini-batch size, data augmentation etc. are primarily driven by hardware while others such as regularization parameters, learning rates, gradient descent hyper-parameters (Sutskever et al., 2013; Bengio, 2012) are based on either experience or a grid-based search. Our analysis suggests the magnitude of regularization parameters in terms of the average number of neurons on each layer and the total number of layers."}, {"heading": "2 PRELIMINARIES", "text": ""}, {"heading": "2.1 A MODEL FOR DEEP NETWORKS", "text": "Let us consider a deep networks with p hidden layers and n neurons on each layer. Denote by X \u2208 {0,1}n the observed layer at the bottom which is generated by h \u2208 {0,1}n at the top. More\nprecisely, X can be written as: X = g ( J(1) > g ( J(2) > . . . g ( J(p) > h ) . . . )) ; (1)\nwhere J(k) \u2208 [\u22121,1]n\u00d7n for k \u2264 p are the connectivity matrices and g are threshold non-linearities defined by g(x)= 1{x\u22650}. 1A denotes the indicator function of the set A. The output of the intermediate\nlayers is thus given by h(k\u22121) = g ( J(k) > g ( J(k+1) > . . . g ( J(p) > h ) . . . ))\n\u2208 {0,1}n; for k \u2264 p. Note that we have X = h(0) and h = h(p). To make this model analytically tractable, we make the following assumptions: (i) h has at most \u03c1 non-zero entries, and (ii) for d = n1/p, every entry J(k)i j is a\nzero-mean random variable with P ( J(k)i j > 0 ) = P ( J(k)i j < 0 ) = d2n and zero otherwise. This results on an average d non-zero entries in every row and column. These assumptions make our model a random, sparse neural network and help us analyze it using techniques from probability. The first assumption implies that each data sample Xi is generated from a hidden feature vector hi which has at most a constant number of non-zeros, i.e., if we interpret every entry of hi as an indicator of some class, at most \u03c1 classes are present in every data sample Xi. The Chernoff bound then implies that with high probability, each Xi has a constant \u03c1 ( d 2 )p fraction non-zero entries. The above model enjoys a number of useful properties by virtue of it being random and sparsely connected (Arora et al., 2014). For instance, for low sparsity p > 5, the network acts as a denoising auto-encoder using the same weights (\u201cweight tying\u201d), i.e., h = g ( J(p)g ( J(p\u22121) . . .g\u2032 ( J(1)X\u0303\u2212 d31n ) \u2212 d31n ) . . . ) where\n1n denotes a vector of 1s of length n and X\u0303 = X +\u03b4 for some small noise \u03b4 . Or equivalently, the feature vector h can be obtained from X by running the same network in reverse with a different thresholding function g\u2032(x) = 1{x\u2265 d3}. Arora et al. (2014) also devise a community detection style algorithm to provably learn such a network. Our objective in this paper will be to use this model as an analytical tool to enable computations that connect deep networks to spin glasses. With a slightly different set of assumptions, there are other ways of achieving this, e.g., the model in Choromanska et al. (2014) assumes that paths from each Xi to the loss layer are independent, while we show that the contribution of correlations of Xi is small due to our assumption of sparsity.\nClassification model: In a typical classification scenario, one uses a support vector machine (SVM) or a soft-max layer on the feature vector h to obtain an output Y \u2208 {0,1}. We model this as:\nY = g\u2032 ( J(p+1)g\u2032 ( J(p) . . .g\u2032 ( J(1)X ) . . . )) ; (2)\nwhere g\u2032(x) = 1{x\u2265 d3} is a thresholding function and J (p+1) \u2208 [\u22121,1]n is a d-sparse row vector. The data sample X \u2208 {0,1}n is denoted as the \u201cinput\u201d while the output is Y \u2208 {0,1}. We will henceforth consider (2) as the canonical model of a deep neural network."}, {"heading": "2.2 DEEP NETWORKS AS SPIN GLASSES", "text": "Spin glasses are models in statistical physics used to analyze properties of magnetic alloys and show a number of fascinating properties due to presence of both ferromagnetic and anti-ferromagnetic interactions. There is a wealth of literature from both physicists (Mezard et al., 1987; Mezard and Montanari, 2009) and mathematicians (Talagrand, 2003) that analyzes the energy landscape of spin glasses as well as their applications to optimization problems. A p-spin glass for an integer p\u2265 1 with n \u201cspins\u201d is given by an energy function (usually called the Hamiltonian)\n\u2212Hn,p(\u03c3) = 1\nn(p\u22121)/2\nn\n\u2211 i1,i2,...,ip=1 Ji1,...,ip\u03c3i1 . . .\u03c3ip ; (3)\nwhere \u03c3 = (\u03c31, . . . ,\u03c3n) is a configuration of the spins and Ji1,...,ip are iid zero-mean standard Gaussian random variables. Note that Hn,p is zero-mean and Gaussian as well. The constant n\u2212(p\u22121)/2 is present to ensure that the Hamiltonian is extensive, i.e., it scales as O(n). For the purpose of analysis, we will assume a spherically constrained spin glass, i.e., n\u22121 \u2211i \u03c32i = 1, or equivalently \u03c3 \u2208 Sn\u22121( \u221a n)\u2282 Rn.\nWe will also use a mean-field model wherein we assume that the disorder Ji1,...,ip is independent standard Gaussian random variables. We can now show that the loss function for the model for deepnets introduced in Sec. 2.1 can be written as the Hamiltonian of a p-spin glass. Please see Appendix C for the proofs. Lemma 1. The output Y of the deep network in (2) can be approximated by\nY\u0302 (\u03c3) law= n\n\u2211 i1,i2,...,ip=1 Ji1,...,ip \u03c3i1 . . .\u03c3ip .\nwhere Ji1,...,ip is a zero-mean Gaussian random variable with variance n \u2212(p\u22121) and \u03c3 \u2208 [\u22121,1]n.\nFor the zero-one loss defined as L(Y\u0302 ,Y t) = \u2223\u2223\u2223Y\u0302 \u2212Y t \u2223\u2223\u2223 where Y t is the true label, we can define a\ncorresponding spin glass Hamiltonian and study its distribution as the following lemma shows. Lemma 2. If the true label Y t \u223c Ber(q) for some q < 1 is independent of data, the Hamiltonian defined as Hn,p = 11\u22122q \u2212 11\u22122q L(Y\u0302 ,Y t) has the distribution\n\u2212Hn,p(\u03c3) law= 1\nn(p\u22121)/2\nn\n\u2211 i1,i2,...,ip=1 Ji1,...,ip \u03c3i1 . . .\u03c3ip . (4)\nThe Hamiltonian as defined above is a zero-mean Gaussian random variable for a fixed \u03c3 ."}, {"heading": "3 PERTURBATIONS OF THE HAMILTONIAN", "text": "In this section, we present results wherein a perturbation term is added to a general isotropic Hamiltonian and depending upon the magnitude of the perturbation, one can radically change the number of local minima of the Hamiltonian. If the magnitude of the perturbations is large, the energy landscape is dominated by the perturbation term, while if the magnitude is small, the Hamiltonian retains its original landscape. This section uses some basic terminology of the Gaussian Orthogonal Ensemble (GOE) which is reviewed in Appendix B.\nLet us define the total number of stationary points of the normalized Hamiltonian in the set (\u2212\u221e,u] is given by crt(u) = \u2211\u03c3 :\u2207H(\u03c3)=0 1 { n\u22121 H(\u03c3)\u2264 u }\n. The index of a stationary point is the number of negative eigenvalues of the Hessian at that point \u03c3 and we denote stationary points of index k in this interval by crtk(u). In this notation, local minima are therefore given by crt0(u). Note that crtk and crt are random variables but as Auffinger et al. (2013) shows, we can explicitly compute their limiting values. For any k \u2265 0 and p\u2265 2 we have\nlim n\u2192\u221e 1 n logE crtk(u) = \u0398k(u), (5)\nlim n\u2192\u221e 1 n logE crt(u) = \u0398(u); (6)\nwhere \u0398k(u) and \u0398(u) are large deviation rate functions that can be computed analytically and are shown in Fig. 1 for p = 3 (also see Appendix A). The functions \u0398k and \u0398 are also known as the complexity of a spin glass \u2014 if \u0398k(u) < 0 in some interval u, (5) roughly shows that there are a vanishing number of critical points (with the logarithmic scaling). The following theorem is a similar computation as (5) and (6), we use it to introduce topology trivialization. With minor abuse of notation denote crt(\u221e) = crt(H).\nTheorem 3 (Fyodorov (2013)). Consider a Hamiltonian H(\u03c3) where \u03c3 \u2208 Sn\u22121(\u221an) and the covariance E H(\u03c3)H(\u03c3 \u2032) = F(\u03c3>\u03c3 \u2032) is invariant under the transformations \u03c3 7\u2192 O\u03c3 , \u03c3 \u2032 7\u2192 O\u03c3 \u2032 for any n\u00d7n orthogonal matrix O. The expected number of stationary points of H is given by\nE crt(H) = 4n (\n1+B 1\u2212B\n)n/2\u221a 1\u2212B G(B);\nwhere\nB = F \u2032\u2032(n)\u2212F \u2032(n)/n F \u2032\u2032(n)+F \u2032(n)/n\n, G(B) = \u222b \u221e\n0 e\u2212nBt 2/2 \u03c1n(t) dt,\nand \u03c1n(t) is the density of the eigenvalues of GOE.\nNow consider a \u201cperturbed\u201d version of the p-spin glass Hamiltonian in (4) given by\n\u2212 H\u0303(\u03c3) = n\n\u2211 i1,i2,...,ip=1\nJi1,...,ip \u03c3i1 . . .\u03c3ip + n\n\u2211 i=1 hi \u03c3i; (7)\nwhere (as before) Ji1,...,ip is a zero-mean Gaussian random coupling with variance J2\nnp\u22121 but we have added an external magnetic field given by \u2211i hi\u03c3i. This is a random external field, i.e., hi are iid zero-mean Gaussian random variables with variance E h2i = \u03bd2. An application of Thm. 3 now shows that the parameter \u03bd results in three distinct regimes for the expected number of critical points. First note that H\u0303(\u03c3) satisfies the covariance structure of Thm. 3 with\nE H\u0303(\u03c3) H\u0303(\u03c3 \u2032) = n f ( \u03c3>\u03c3 \u2032\nn\n) , f (u) = J2 up +\u03bd2u;\nand B can be calculated to be\nB = J2 p(p\u22122)\u2212\u03bd2 J2 p2 +\u03bd2 \u2208 ( \u22121, p\u22122 p ] . (8)\nTheorem 4 (Fyodorov (2013)). For p > 2, there exists a critical value \u03bdc = J \u221a\np(p\u22122), such that if B > 0 there exist exponentially many critical points, if B =\u2212O(n\u22121) there exist polynomially many critical points while if B =\u2212\u2126(n\u22121) the number of critical points of H\u0303 is exactly two. More precisely,\nlim n\u2192\u221e E crt(H\u0303) =  2 if \u03bd > \u03bdc, 4n1/2 \u221a 1+B \u03c0B exp ( n 2 log 1+B 1\u2212B ) if \u03bd < \u03bdc, 2n\u221a\n\u03c0 \u03c4 \u22123/2 if B =\u2212 \u03c4n , \u03c4 1.\n(9)\nThe first phase in the above theorem demonstrates total trivialization of the energy landscape, indeed as any smooth function on the sphere Sn\u22121( \u221a n) has at least one minimum and at least one maximum, there is in fact exactly one minimum if the perturbations to the Hamiltonian are larger than the critical value J \u221a p(p\u22122). If the perturbations are smaller than this, we are back to the regime of exponentially many critical points in (6). There is also an interesting \u201cedge-scaling\u201d regime where if B =\u2212O(n\u22121) the spin glass has polynomially many critical points. This is the regime where we would typically like to choose our regularization parameters to be, to not only avoid exponentially many critical points, but also avoid total trivialization. Finally, let us remark that the number of local minima is at most the number of critical points. Note that these cases are computed in Fyodorov (2013) using asymptotics of the eigenvalue distribution and for large n and are hence only approximate. Also, Thm. 4 only describes the behavior of the expected number of critical points, however as the authors in Subag (2015) show, for any interval of the real line u we have limn\u2192\u221e crt(u) E crt(u) = 1 in L 2 and thus the number of critical points converges to the expected number of critical points in probability.\nCorollary 5. For p 2, as n\u2192 \u221e, we need \u03bd = O (Jp(1+2\u03c4/n)) to be in the polynomial regime. In particular, for the deep network model considered in Sec. 2.1 where p = logd n, we require \u03bd = O(J logd n (1+2\u03c4/n)) to be in the polynomial regime.\nThe above corollary follows from (8) and Thm. 4 and we use it to decide the order of magnitude for different types of regularization in deep learning literature in the next section. It can be further shown that the parameter \u03c4 lets us smoothly interpolate between the two regimes, i.e., for large n, when \u03c4 \u22121, the number of critical points is of constant order, while when \u03c4 1, we have exponentially many critical points (Fyodorov, 2013). Let us now consider the scaling of local minima. The following theorem shows that quite surprisingly one can simply replace the GOE eigenvalue density in Thm. 3 by the density of the largest eigenvalue to get the expected number of local minima. This translates to the following equivalent version of Thm. 4.\nTheorem 6 (Fyodorov (2013)). Given the Hamiltonian as described in Thm. 3, for large n, the expected number of local minima for \u03bd < \u03bdc is given by\nE crt0(H) =C(n,B) exp { n (\n1 2 log 1+B 1\u2212B \u2212B\n)} . (10)\nIn the transitional regime around B = 0, with the scaling B =\u2212\u03ba2 n\u22121/3 and C > 0, we have\nE crt0(H) =  1 as \u03ba \u2192 \u221e, 2 at \u03ba = 0, C exp ( \u03ba2 24 + 4 \u221a 2 |\u03ba|3/2 3 ) as \u03ba \u2192\u2212\u221e.\n(11)\nThe parameter \u03ba interpolates between the two extremes, exponentially many local minima as given by (10) for B > 0 and constant number of local minima as B 0. Note that since \u03ba = 2Bn1/3 we can obtain our desired \u201cpolynomially many\u201d local minima by setting the appropriate value of \u03ba .\nCorollary 7. If B=O ( n\u22121/3 logn ) , the expected number of local minima scales as O ( exp(logn)2 ) .\nIn particular, for the spin glass Hamiltonian in (7) and B given by (8), for p 2 we require \u03bd = O ( Jp(1+n\u22121/3) ) .\nNote that for the spin glass model, the number of critical points and local minima is of the same order in all regimes. Moreover, just as we replaced the eigenvalue density by the density of the largest eigenvalue \u03bbmax, we can replace it instead by the probability density of the kth largest eigenvalue of GOE to get critical points with index k\u22121. A short analysis then shows that there are exponentially many critical points of all indices k; note that we also saw this in (5)."}, {"heading": "4 REGULARIZATION IN DEEP NETWORKS", "text": "We now interpret different types of regularization in deep networks as perturbations of the original loss function (4) and compute the scaling of regularization parameters in terms of the number of neurons n and the number of layers p."}, {"heading": "4.1 ADDITIVE NOISE", "text": "The weight update equation in the presence of additive noise (Jim et al., 1996) can be written as\n\u03c3 \u2190 \u03c3 \u2212\u03b7 \u2207Hn,p(\u03c3)\u2212\u2206 where \u2206 \u2208 Rn is a zero-mean random vector with variance \u03bd2In\u00d7n, \u2207Hn,p(\u03c3) is the back-propagated gradient and \u03b7 is the learning rate. Additive noise is typically used to escape local minima and thus it is important to pick the right magnitude of var \u2206 to avoid large jumps in the weights \u03c3 at every iteration that are uncorrelated with the gradient. The perturbed Hamiltonian which gives the weight update equation above is\u2212H\u0303add(\u03c3) =\u2212Hn,p(\u03c3)+\u2211ni=1 \u2206i \u03c3i. This is the same as our original perturbed Hamiltonian (7) and for a constant \u03c4 , we have from Cor. 5 that\n\u03bd = J p (\n1+ 2\u03c4 n\n)1/2 ."}, {"heading": "4.2 WEIGHT DECAY", "text": "Weight decay (Moody et al., 1995) implies an `2 regularization on the weights or equivalently, a Gaussian prior on them. We write the weight updates as\n\u03c3 \u2190 \u03c3 \u2212\u03b7 \u2207Hn,p(\u03c3)\u2212 p \u03b1\u221a\nn \u03c3\nfor a constant \u03b1; the factor of pn\u22121/2 is merely for convenience. The decay term can be obtained using an additive of the form p \u03b12\u221an \u2211 n i=1 \u03c32i . This is akin to introducing a spherical constraint on the\nspins weighted by the Lagrange multiplier p \u03b1/(2 \u221a\nn) and hence will not lead any phase transitions in the number of critical points. However, if we add a perturbation term in addition to weight decay, we can expect similar behavior as Sec. 3. Consider the zero-mean perturbation\n\u2212 H\u0303decay(\u03c3) =\u2212Hn,p(\u03c3)+ p\u03b1\n2 \u221a n\nn\n\u2211 i=1 \u03c32i +\u2211 i hi \u03c3i. (12)\nwith var hi = \u03bd2 which creates an additive term of h in the update rule along with the weight decay term. H\u0303decay is now amenable to the analysis of Thm. 4 with the covariance given by\nE H\u0303decay(\u03c3) H\u0303decay(\u03c3 \u2032) = J2\nnp\u22121 \u2211i1,i2,...,ip \u03c3i1\u03c3 \u2032 i1 . . .\u03c3ip\u03c3 \u2032 ip +\np2\u03b12 4 \u2211i \u2211j ( \u03c3i\u03c3 \u2032j )2 +\u03bd2 \u2211 i \u03c3i\u03c3 \u2032i ;\n\u2248 n ( J2up + p2\u03b12\n4 u2 +\u03bd2u\n) , where u = \u03c3T \u03c3 \u2032\nn ; , n f ( \u03c3T \u03c3 \u2032\nn\n) .\nThe approximation in the second step is obtained by assuming that E\u03c3i\u03c3 \u2032j \u2248 0 for all i 6= j which roughly says, averaged over the disorder J, two deepnets given by \u03c3 ,\u03c3 \u2032 have uncorrelated ith and jth neurons on each layer. We now use the above expression to calculate the value of B in (8) to get B = J\n2 p(p\u22122)\u2212\u03bd2 J2 p2+p2\u03b12+\u03bd2 . Note that the value of \u03bdc which corresponds to B = 0 is unchanged from Thm. 4,\nbut to stay in the polynomial regime, if we set B =\u2212\u03c4n\u22121 we now have, for large n,\n\u03bd = Jp ( 1+ \u03c4 \u03b12\nn + 2\u03c4 n\n)1/2 (13)\nContrast this with Cor. 5 or additive noise in Sec. 4.1, the strength of the perturbation needs to be larger to account for the curvature of the `2 regularization term."}, {"heading": "4.3 MULTIPLICATIVE NOISE", "text": "If instead of a uniform constant, we use a random matrix \u2206 \u2208Rn\u00d7n (usually diagonal) with iid entires E \u2206ii = 0 and E \u22062ii = \u03b12 to multiply the weights, we get multiplicative noise (Nalisnick et al., 2015) with the update rule\n\u03c3 \u2190 \u03c3 \u2212\u03b7 \u2207Hn,p(\u03c3)\u2212 p\u221a n \u2206 \u03c3 .\nSimilar to weight decay, we add a perturbation term to the corresponding Hamiltonian to get\n\u2212 H\u0303mult(\u03c3) =\u2212Hn,p(\u03c3)+ p\n2 \u221a n\nn\n\u2211 i=1 \u2206ii \u03c32i +\u2211 i hi\u03c3i. (14)\nThe analysis follows that of Sec. 4.2 and we get the same expression as (13) although since entries of \u2206 are iid, we need not make the approximation E\u03c3i\u03c3 \u2032j \u2248 0 for all i 6= j."}, {"heading": "4.4 DROP-OUT AND DROP-CONNECT", "text": "Drop-Out (Hinton et al., 2012b) is a form of regularization typically used for fully connected layers where each output is forwarded with a fixed probability q and set to zero otherwise. Numerous experiments have shown that dropout leads to a better generalization error for deep networks by forcing each layer to explain the layer above using as few features as possible. Drop-Connect (Wan et al., 2013) is very similar to Drop-Out except that one sets the individual weights to zero with probability 1\u2212q. This induces sparsity in the weight matrix at each layer. We model drop-connect by introducing a Bernoulli random variable \u03b4i1 . . .\u03b4ip that multiplies the disorder Ji1,...,ip to give\n\u2212 H\u0303drop-connect(\u03c3) = \u2211 i1,i2,...,ip \u03b4i1 . . .\u03b4ip Ji1,...,ip\u03c3i1 . . .\u03c3ip +\u2211 i hi\u03c3i (15)\nwhere as before Ji1,...,ip is zero-mean Gaussian with variance J 2n\u2212(p\u22121). Such a Hamiltonian is known as a \u201cdiluted\u201d spin glass. Since \u03b4i1 . . .\u03b4ip = 0 is like setting the entire path (i1, . . . , ip) to zero, we set \u03b4i1 . . .\u03b4ip \u223c Ber(qp). Similar to the proof of Lem. 1, we now replace \u03b4i1 . . .\u03b4ipJi1,...,ip by a Gaussian random variable to put it into the framework of Sec. 3. If we denote c , qp, this amounts to scaling the constant J by \u221a c(1\u2212 c) which gives var Ji1,...,ip = n\u2212(p\u22121) c(1\u2212 c)J2. For the perturbation term to result in polynomially many critical points, we now require\n\u03bd = J p \u221a c(1\u2212 c) (\n1+ 2\u03c4 n\n)1/2 . (16)\nQuite intuitively, since c(1\u2212 c)< 1/2, drop-connect requires that the magnitude of the perturbation term be reduced by an appropriate factor. As a minor aside, following the idea that drop-connect sets entire \u201cpaths\u201d from the data layer to loss layer to zero we should pick the dropout probability such that qp is a constant, i.e., q = c1/p.\nTo model dropout, we multiply each spin in (7) by \u03b4i \u223c Ber(q) to get\n\u2212 H\u0303drop-out(\u03c3) = \u2211 i1,i2,...,ip\nJi1,...,ip p\n\u220f k=1 \u03b4ik \u03c3ik +\u2211 i hi\u03c3i (17)\nFor our model, the connectivity matrices Ji1,...,ip are such that each neuron ik is connected to ik+1 in the next layer uniformly randomly. This enables us to interpret the factor \u220fk \u03b4ik as a path i1\u2192 . . .\u2192 ip similar to (15) and gives the same expression for the perturbation as (16)."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we discuss simulations of a 3-spin glass to demonstrate topology trivialization. In our notation, this is a fully-connected neural network with three hidden layers and threshold non-linearities. Consider the perturbed Hamiltonian (3) given by\n\u2212 H\u0303n,3 = 1 n\nn\n\u2211 i, j,k=1 Ji, j,k \u03c3i\u03c3 j\u03c3k +\u2211 i hi \u03c3i; (18)\nwith hi \u2208 N(0,\u03bd2) and \u03c3 \u2208 Sn\u22121( \u221a n). The gradient can be computed to be\n\u2212\u2207H\u0303n,3 = 1 n \u2211j,k\n[ Ji jk\u03c3 j\u03c3k + J jik\u03c3 j\u03c3k + J jki\u03c3 j\u03c3k ] +h.\nFor a given disorder J we find the ground state of the above Hamiltonian for values of h in the three regimes considered in this paper (cf. Cor. 5), \u03bd = 1/n for exponentially many stationary points, \u03bd = (1+ \u03c4/n)1/2 with \u03c4 \u2248 n/2 to be in the polynomial regime and \u03bd = p to be in the totally trivialized regime with a single minima. Note that \u2016h\u20162 = \u03bd \u221a n. For an initial\nconfiguration \u03c31 \u2208 Sn\u22121(\u221an), we perform gradient descent with a fixed step size \u03b7 = 0.1 and stop when the gradient \u2016\u2207H\u0303\u20162 \u2264 10\u22124. Note that at each step, we need to project the gradient on the tangent plane of Sn\u22121( \u221a n) and re-project \u03c3 k+1 \u2190 \u03c3 k \u2212 \u03b7\u2207H\u0303(\u03c3 k) back on the sphere.\nFig. 2 shows the distribution of the Hamiltonian in the exponential regime for n = 100. Note that even without the spin glass term in (18) (which scales as O(n)), the sum \u2211i hi\u03c3i is maximized by \u03c3 \u2208 Sn\u22121(\u221an) that aligns with the magnetic field, i.e., \u03c3 = \u221a n h/\u2016h\u2016. This can cause the normalized Hamiltonian in Fig. 2 to shift to the left by at most 1/n = 0.01. It is however interesting that for a given disorder J with even a small perturbation, the probability of gradient descent finding minima very close to the theoretical minimum of the Hamiltonian (cf. Fig. 1) is high, which is seen by the positive skew of the density.\nIn order to visualize the energy landscape, we perform a t-SNE (Van der Maaten and Hinton, 2008) of the local minima obtained by starting gradient descent from N = 20,000 uniformly sampled points of Sn\u22121( \u221a n) for n = 100; denote this set by \u03a3\u2217. t-SNE minimizes the crossentropy between the original high-dimensional data and its projection, and thus separates clusters in the data. Also note that normalizing an isotropic Gaussian vector makes it uniform on the unit sphere and hence we can sample Sn\u22121( \u221a n) easily. Figs. 4a, 4b show the results of this simulation for the exponential and polynomial regime while Fig. 3 shows the local min-\nima in the completely trivialized landscape. The effect in the latter case is quite drastic, all executions of gradient descent converge at the same local minimum. This is seen by the fact that the radius of the data computed as stddev \u03a3\u2217 is 0.02 for Fig. 3 while it is 1.16 in Fig. 4a; note that \u2016\u03c3\u20162 = 10."}, {"heading": "6 CONCLUSIONS", "text": "We employed results from spin glass theory to introduce a novel topology trivialization phenomenon for interpreting regularization techniques in deep learning. While the original energy landscape is complex with exponentially many local minima and saddle points, we showed that it undergoes dramatic changes in structure upon adding small perturbations. We can compute the threshold for the magnitude of perturbations below which the landscape is qualitatively unchanged and above which it becomes trivial. There also exists a critical band for the perturbation which results in polynomially many stationary points of the Hamiltonian and thus interpolates between these regimes. Our analysis provides order estimates of the magnitude of a number of different regularization methods in the deep learning literature. We also motivate an annealing scheme for these perturbations wherein the loss function can be gradually modified from the polynomial regime to the original exponential regime during the course of training. As future work, we aim to evaluate such annealing strategies for training of convolutional neural networks."}, {"heading": "A ENERGY LANDSCAPE OF DEEP NETWORKS", "text": "Theorem 8 (Thm 2.5 in Auffinger et al. (2013)). For all p\u2265 2 and k \u2265 0 fixed\nlim n\u2192\u221e 1 n logE crtk(u) = \u0398k(u);\nwhere \u0398k(u) is a large deviation rate function given by\n\u0398k(u) =\n{ 1 2 log(p\u22121)\u2212 p\u22122 4(p\u22121) u\n2\u2212 (k+1)I1(u), if u\u2264\u2212E\u221e, 1 2 log(p\u22121)\u2212 p\u22122 p , else;\nwhere E\u221e = 2 \u221a p\u22121 p and I1 : (\u2212\u221e,\u2212E\u221e]\u2192 R is given by\nI1(u) = 2\nE2\u221e \u222b \u2212E\u221e u (z2\u2212E2\u221e)1/2 dz.\nTheorem 9 (Thm. 2.8 in Auffinger et al. (2013)). For all p\u2265 2,\nlim n\u2192\u221e 1 n logE crt(u) = \u0398(u);\n\u0398(u) =  1 2 log(p\u22121)\u2212 p\u22122 4(p\u22121) u 2\u2212 I1(u), if u\u2264\u2212E\u221e, 1 2 log(p\u22121)\u2212 p\u22122 4(p\u22121) u\n2, if \u2212E\u221e \u2264 u\u2264 0, 1 2 log(p\u22121), else.\nWe do not make use of the following fact, but as Thms. 2.14 and 2.15 in Auffinger et al. (2013) prove, the energy landscape shows a very intricate structure. For \u03c3 \u2208 Sn\u22121(\u221an), if we define the lower bound of the Hamiltonian as limn\u2192\u221e n\u22121 H(\u03c3) =\u2212E0, as one progresses from the global minimum at energy level \u2212nE0, saddle points of increasing indices start appearing, in particular, between energy levels (\u2212nE0,\u2212nEk), only critical points of indices less than k exist (in the limit as n\u2192 \u221e). Most importantly, note that there are exponentially many local minima in any non-zero measure interval between (\u2212nE0,\u2212nE\u221e). This fact is crucial to construct algorithms that rely on gradient descent, for instance, stochastic gradient descent can get stuck in any one of these exponentially many local minima and indeed, as experiments in Choromanska et al. (2014); Sagun et al. (2014) show, the minima obtained by SGD are always low order critical points, i.e., they are either local minima which could be very close to the energy barrier or they are saddle points very deep in the energy landscape. Empirically, SGD cannot penetrate the energy barrier \u2212nE\u221e below which the layered structure presents itself. The main motivation of this paper is to add perturbations to the Hamiltonian to reduce the number of local minima and saddle points."}, {"heading": "B KAC-RICE FORMULA AND GOE", "text": "Given a smooth, random function H(\u03c3) of n variables \u03c3 = (\u03c31, . . . ,\u03c3n), if Ns(D) is the total number of stationary points in a subset D \u2282 Rn, let \u03c1s(\u03c3) be their corresponding density. We thus have Ns(D) = \u222b D \u03c1s(\u03c3)d\u03c3 . The Kac-Rice formula (Adler and Taylor, 2009) then gives\nE\u03c1s(\u03c3) = E {\u2223\u2223det(\u2202 2k1,k2V)\u2223\u2223 n\u220f k=1 \u03b4 (\u2202kH) } . (19)\nSimilarly, if one is interested only in the number of local minima, we have\nE\u03c1s(\u03c3) = E { det ( \u2202 2k1,k2H ) \u03b8 ( \u2202 2k1,k2H ) n \u220f k=1 \u03b4 (\u2202kH) } .\nwhere \u03b8 ( \u2202 2k1,k2H )\nis the Heavyside step function. We consider isotropic Hamiltonians with a covariance given by\nE H(\u03c3)H(\u03c3 \u2032) = F(\u03c3>\u03c3 \u2032)\nfor some functions F . Note that the covariance is invariant to changes \u03c3 7\u2192 O\u03c3 for any unitary, orthogonal matrix O. We also impose a spherical constraint \u2016\u03c3\u20162 = \u221a n to match with the model in Sec. 2.1.\nUsing the Kac-Rice formula, we can completely characterize the expected number of critical points and local minima of isotropic Hamiltonians. The Hamiltonians we consider are high-dimensional zero-mean Gaussian surfaces and hence such an analysis is tied to the properties of random matrices with Gaussian entries which we describe briefly. (see e.g., Mehta (2004) for an elaborate exposition). A real symmetric matrix G \u2208 Rn\u00d7n is said to belong to the Gaussian Orthogonal Ensemble (GOE) if every entry Gi j for i\u2264 j is an iid zero-mean Gaussian random variable with E G2i j = 1+\u03b4i j n . The density\nof GOE matrix is proportional to exp ( \u2212 n4 tr G2 ) and is hence invariant to orthogonal conjugation. The joint eigenvalue distribution of such matrices is a well-studied object, it is usually characterized as the density \u03c1n(t) = n\u22121 E \u2211ni=1 \u03b4 (t\u2212\u03bbi) for eigenvalues \u03bbi. We see in Thm. 3 that the expression for the expected number of critical points is given as a function of the eigenvalue density. On the other hand, the expected number of local minima is a function of the density of the largest eigenvalue of GOE matrices (cf. Thm. 6)."}, {"heading": "C PROOFS", "text": "Proof of Lem. 1. We can write the expression (2) in terms of the number of \u201cactive paths\u201d to get rid of the nonlinearities g\u2032(\u00b7), i.e., all paths with non-zero weight from an input Xi to the output Y . We then have\nY = n\n\u2211 i=1 \u2211 \u03b3\u2208\u0393i Xi J\u03b3 ; (20)\nwhere \u0393i is the set of all active paths connecting the ith input neuron to the output Y . Let J\u03b3 be the weight along each path \u03b3 . Now define \u03b3i,i1,...,ip to be an indicator random variable that is 1 iff the path i\u2192 i1\u2192 . . .\u2192 ip\u2192 Y , i.e., the one that connects the ith1 neuron in the first hidden layer, ith2 neuron in the second hidden layer and so on, is active. With some abuse of notation, we will use the variable \u03b3 to denote both the path and the tuple (i, i1, i2, . . . , ip). We now work towards removing the dependency of the input X in (20). First split the path into two parts, the first edge on layer 1 and the rest of the path as: \u03b3i,i1,i2,...,ip = \u03b3i,i1 \u03b3i1,...,ip (conditional upon \u03b3i,i1 = 1, the sub-paths are independent). This gives\nY = n\n\u2211 i,i1,i2,...,ip=1 \u03b3i,i1 \u03b3i1 . . .\u03b3ip Xi Ji,i1 Ji1,...,ip .\nThis expression has correlations between different paths that are introduced by the sum over inputs Xi; we approximate these correlations to obtain an expression that resembles the Hamiltonian in (3). Define the net coupling due to the sum over the inputs as\nZi := Xi \u03b3i,i1 Ji,i1 , Z := n\n\u2211 i=1 Zi.\nSince Xi \u2208 {0,1}, we have var(Xi)\u2264 1/4 from Popoviciu\u2019s inequality (for a bounded random variable m\u2264 X \u2264M, the variance is bounded as var X \u2264 14 (M\u2212m)2), and Ji,i+1 is positive or negative with equal probability d/2n while \u03b3i,i1 = 1 with probability d/n for a fixed i1. Note that EZi = 0 while EZ2i \u2264 d2/(4n2). We now use Bernstein\u2019s inequality to see that:\nP (\u2223\u2223\u2223\u2223\u2223 n\u2211i=1 Zi \u2223\u2223\u2223\u2223\u2223> t ) \u2264 exp ( \u2212 t 2/2 var(Z)+ t/3 ) \u21d2 |Z| \u2264 n\u2212 1 2+ 1 p+\u03b5 \u2264 n\u22121/5;\nwith high probability for any small \u03b5 > 0 (we picked \u03b5 = 1/10 above). We have thus shown that the net coupling due to inputs in our model is bounded. Now note that \u03b3i1 . . .\u03b3ip is a Bernoulli random variable with\nP ( \u03b3i1,...,ip = 1 ) = d\u03c1 n ( \u03c1 d 2 1 n ) . . . ( \u03c1 ( d 2 )p\u22121 1 n ) = O(n\u2212(p\u22121)/2).\nSince the entries of weight matrices J(k) are iid, the weights along each path \u03b3 \u2208 \u0393i, i.e., Ji1,...,ip are independent and identically distributed, in particular, each Jik,ik+1 only depends upon ik. We therefore write them as a product of \u201cspins\u201d \u03c3i1 . . .\u03c3ip \u2208 [\u22121,1]p to get Y law = \u2211ni1,i2,...,ip=1 Z \u03b3i1,i2,...,ip \u03c3i1 . . .\u03c3ip . We now approximate the Bernoulli random variable \u03b3i1 . . .\u03b3ip with a Gaussian of variance n 1\u2212p and define the scaled output Y\u0302 = Y n1/5 to get\nY\u0302 law= 1\nn(p\u22121)/2\nn\n\u2211 i1,i2,...,ip=1 Ji1,...,ip \u03c3i1 . . .\u03c3ip . (21)\nwhere Ji1,...,ip is standard Gaussian.\nProof of Lem. 2. The zero-one loss L for Y t \u223c Ber(q) can be written as L = EY t \u2223\u2223\u2223Y\u0302 (X t)\u2212Y t \u2223\u2223\u2223\n= q(1\u2212 Y\u0302 )+(1\u2212q) Y\u0302 = q+(1\u22122q) Y\u0302 .\nNow define Hn,p = (q\u2212L)/(1\u22122q) to see that \u2212Hn,p has the same distribution as Y\u0302 ."}], "references": [{"title": "Random fields and geometry", "author": ["R.J. Adler", "J.E. Taylor"], "venue": "Springer Science & Business Media.", "citeRegEx": "Adler and Taylor,? 2009", "shortCiteRegEx": "Adler and Taylor", "year": 2009}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "Proceedings of the 31st International Conference on Machine Learning, pages 584\u2013592.", "citeRegEx": "Arora et al\\.,? 2014", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Random matrices and complexity of spin glasses", "author": ["A. Auffinger", "G.B. Arous", "J. \u010cern\u1ef3"], "venue": "Communications on Pure and Applied Mathematics, 66(2):165\u2013201.", "citeRegEx": "Auffinger et al\\.,? 2013", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "Neural Networks: Tricks of the Trade, pages 437\u2013478. Springer.", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research, 13(1):281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Survey propagation: An algorithm for satisfiability", "author": ["A. Braunstein", "M. M\u00e9zard", "R. Zecchina"], "venue": "Random Structures & Algorithms, 27(2):201\u2013226.", "citeRegEx": "Braunstein et al\\.,? 2005", "shortCiteRegEx": "Braunstein et al\\.", "year": 2005}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv:1405.3531.", "citeRegEx": "Chatfield et al\\.,? 2014", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "arXiv:1504.04788.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "arXiv:1412.0233.", "citeRegEx": "Choromanska et al\\.,? 2014", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Deep gaussian processes", "author": ["A.C. Damianou", "N.D. Lawrence"], "venue": "arXiv:1211.0358.", "citeRegEx": "Damianou and Lawrence,? 2012", "shortCiteRegEx": "Damianou and Lawrence", "year": 2012}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 2933\u20132941.", "citeRegEx": "Dauphin et al\\.,? 2014", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Le", "Q. V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "High-dimensional random fields and random matrix theory", "author": ["Y.V. Fyodorov"], "venue": "arXiv:1307.2379.", "citeRegEx": "Fyodorov,? 2013", "shortCiteRegEx": "Fyodorov", "year": 2013}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": "arXiv:1506.07540.", "citeRegEx": "Haeffele and Vidal,? 2015", "shortCiteRegEx": "Haeffele and Vidal", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012b", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences, 79(8):2554\u20132558.", "citeRegEx": "Hopfield,? 1982", "shortCiteRegEx": "Hopfield", "year": 1982}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "arXiv:1506.08473.", "citeRegEx": "Janzamin et al\\.,? 2015", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "An analysis of noise in recurrent neural networks: convergence and generalization", "author": ["Jim", "K.-C.", "C.L. Giles", "B.G. Horne"], "venue": "IEEE Transactions on Neural Networks, 7(6):1424\u20131438.", "citeRegEx": "Jim et al\\.,? 1996", "shortCiteRegEx": "Jim et al\\.", "year": 1996}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Statistical physics-based reconstruction in compressed sensing", "author": ["F. Krzakala", "M. M\u00e9zard", "F. Sausset", "Y. Sun", "L. Zdeborov\u00e1"], "venue": "arXiv:1109.4424.", "citeRegEx": "Krzakala et al\\.,? 2011", "shortCiteRegEx": "Krzakala et al\\.", "year": 2011}, {"title": "Random matrices, volume 142", "author": ["M.L. Mehta"], "venue": "Academic press.", "citeRegEx": "Mehta,? 2004", "shortCiteRegEx": "Mehta", "year": 2004}, {"title": "Information, physics, and computation", "author": ["M. Mezard", "A. Montanari"], "venue": "Oxford University Press.", "citeRegEx": "Mezard and Montanari,? 2009", "shortCiteRegEx": "Mezard and Montanari", "year": 2009}, {"title": "Spin glass theory and beyond", "author": ["M. Mezard", "G. Parisi", "M.A. Virasoo"], "venue": null, "citeRegEx": "Mezard et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Mezard et al\\.", "year": 1987}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "A simple weight decay can improve generalization", "author": ["J. Moody", "S. Hanson", "A. Krogh", "J.A. Hertz"], "venue": "Advances in neural information processing systems, 4:950\u2013957.", "citeRegEx": "Moody et al\\.,? 1995", "shortCiteRegEx": "Moody et al\\.", "year": 1995}, {"title": "A scale mixture perspective of multiplicative noise in neural networks", "author": ["E. Nalisnick", "A. Anandkumar", "P. Smyth"], "venue": "arXiv:1506.03208.", "citeRegEx": "Nalisnick et al\\.,? 2015", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2015}, {"title": "Sparse matrix factorization", "author": ["B. Neyshabur", "R. Panigrahy"], "venue": "arXiv:1311.3315.", "citeRegEx": "Neyshabur and Panigrahy,? 2013", "shortCiteRegEx": "Neyshabur and Panigrahy", "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "arXiv:1412.1897.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "A Probabilistic Theory of Deep Learning", "author": ["A.B. Patel", "T. Nguyen", "R.G. Baraniuk"], "venue": "arXiv:1504.00641.", "citeRegEx": "Patel et al\\.,? 2015", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Explorations on high dimensional landscapes", "author": ["L. Sagun", "V.U. Guney", "Y. LeCun"], "venue": "arXiv:1412.6615.", "citeRegEx": "Sagun et al\\.,? 2014", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 448\u2013455.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Seide et al\\.,? 2014", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in neural information processing systems, pages 2951\u20132959.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "The complexity of spherical p-spin models \u2014 A second moment approach", "author": ["E. Subag"], "venue": "arXiv:1504.02251.", "citeRegEx": "Subag,? 2015", "shortCiteRegEx": "Subag", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proceedings of the 30th international conference on machine learning, pages 1139\u20131147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv:1409.4842.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Spin glasses: A challenge for mathematicians: Cavity and mean field models, volume 46", "author": ["M. Talagrand"], "venue": "Springer Science & Business Media.", "citeRegEx": "Talagrand,? 2003", "shortCiteRegEx": "Talagrand", "year": 2003}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton,? 2008", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "arXiv:1412.7580.", "citeRegEx": "Vasilache et al\\.,? 2014", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}, {"title": "Regularization of neural networks using Drop-Connect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1058\u20131066.", "citeRegEx": "Wan et al\\.,? 2013", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "else. We do not make use of the following fact, but as Thms", "author": ["Auffinger"], "venue": null, "citeRegEx": "Auffinger,? \\Q2013\\E", "shortCiteRegEx": "Auffinger", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "Deep learning has enjoyed enormous success in recent years in applications like image recognition (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 98, "endOffset": 123}, {"referenceID": 4, "context": ", 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 26, "context": ", 2003) and reinforcement learning (Mnih et al., 2015).", "startOffset": 35, "endOffset": 54}, {"referenceID": 31, "context": "Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al.", "startOffset": 121, "endOffset": 202}, {"referenceID": 33, "context": "Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al.", "startOffset": 121, "endOffset": 202}, {"referenceID": 10, "context": "Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al.", "startOffset": 121, "endOffset": 202}, {"referenceID": 7, "context": ", 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al.", "startOffset": 100, "endOffset": 145}, {"referenceID": 30, "context": ", 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al.", "startOffset": 100, "endOffset": 145}, {"referenceID": 34, "context": ", 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc.", "startOffset": 59, "endOffset": 122}, {"referenceID": 12, "context": ", 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc.", "startOffset": 59, "endOffset": 122}, {"referenceID": 41, "context": ", 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc.", "startOffset": 59, "endOffset": 122}, {"referenceID": 18, "context": "We derive motivation from the theory of spin glasses which are some of the oldest models for neural networks (Hopfield, 1982).", "startOffset": 109, "endOffset": 125}, {"referenceID": 24, "context": "have become popular (Mezard and Montanari, 2009) and an understanding of the energy landscape and thermodynamics of such systems has led to a number of efficient algorithms (Krzakala et al.", "startOffset": 20, "endOffset": 48}, {"referenceID": 22, "context": "have become popular (Mezard and Montanari, 2009) and an understanding of the energy landscape and thermodynamics of such systems has led to a number of efficient algorithms (Krzakala et al., 2011; Braunstein et al., 2005).", "startOffset": 173, "endOffset": 221}, {"referenceID": 6, "context": "have become popular (Mezard and Montanari, 2009) and an understanding of the energy landscape and thermodynamics of such systems has led to a number of efficient algorithms (Krzakala et al., 2011; Braunstein et al., 2005).", "startOffset": 173, "endOffset": 221}, {"referenceID": 2, "context": "Spin glasses are particularly attractive models for large-scale problems such as deep networks because although their energy landscape is a high-dimensional, non-convex Gaussian surface, using results from probability theory and statistical physics, we can explicitly compute various quantities of interest such as the number of stationary points (Auffinger et al., 2013; Fyodorov, 2013), low temperature topology and the structure of the Gibbs distribution (Talagrand, 2003; Mezard et al.", "startOffset": 347, "endOffset": 387}, {"referenceID": 14, "context": "Spin glasses are particularly attractive models for large-scale problems such as deep networks because although their energy landscape is a high-dimensional, non-convex Gaussian surface, using results from probability theory and statistical physics, we can explicitly compute various quantities of interest such as the number of stationary points (Auffinger et al., 2013; Fyodorov, 2013), low temperature topology and the structure of the Gibbs distribution (Talagrand, 2003; Mezard et al.", "startOffset": 347, "endOffset": 387}, {"referenceID": 39, "context": ", 2013; Fyodorov, 2013), low temperature topology and the structure of the Gibbs distribution (Talagrand, 2003; Mezard et al., 1987).", "startOffset": 94, "endOffset": 132}, {"referenceID": 25, "context": ", 2013; Fyodorov, 2013), low temperature topology and the structure of the Gibbs distribution (Talagrand, 2003; Mezard et al., 1987).", "startOffset": 94, "endOffset": 132}, {"referenceID": 2, "context": ", 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al., 2015). The fact that similar architectures and concepts are sweepingly applicable to such vastly different problems is both surprising and profound. Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc. This paper is in the spirit of results that analyze algorithms for training deep networks using knowledge of the landscape of the cost function, e.g., Dauphin et al. (2014); Haeffele and Vidal (2015); Janzamin et al.", "startOffset": 39, "endOffset": 823}, {"referenceID": 2, "context": ", 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al., 2015). The fact that similar architectures and concepts are sweepingly applicable to such vastly different problems is both surprising and profound. Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc. This paper is in the spirit of results that analyze algorithms for training deep networks using knowledge of the landscape of the cost function, e.g., Dauphin et al. (2014); Haeffele and Vidal (2015); Janzamin et al.", "startOffset": 39, "endOffset": 850}, {"referenceID": 2, "context": ", 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al., 2015). The fact that similar architectures and concepts are sweepingly applicable to such vastly different problems is both surprising and profound. Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc. This paper is in the spirit of results that analyze algorithms for training deep networks using knowledge of the landscape of the cost function, e.g., Dauphin et al. (2014); Haeffele and Vidal (2015); Janzamin et al. (2015); Choromanska et al.", "startOffset": 39, "endOffset": 874}, {"referenceID": 2, "context": ", 2012a), natural language processing (Bengio et al., 2003) and reinforcement learning (Mnih et al., 2015). The fact that similar architectures and concepts are sweepingly applicable to such vastly different problems is both surprising and profound. Consequently, a number of recent papers have focused on different aspects of this problem such as hierarchical structure (Patel et al., 2015; Salakhutdinov and Hinton, 2009; Damianou and Lawrence, 2012), robustness of representations (Chatfield et al., 2014; Nguyen et al., 2014), large scale optimization on distributed platforms (Seide et al., 2014; Dean et al., 2012; Vasilache et al., 2014) etc. This paper is in the spirit of results that analyze algorithms for training deep networks using knowledge of the landscape of the cost function, e.g., Dauphin et al. (2014); Haeffele and Vidal (2015); Janzamin et al. (2015); Choromanska et al. (2014). We derive motivation from the theory of spin glasses which are some of the oldest models for neural networks (Hopfield, 1982).", "startOffset": 39, "endOffset": 901}, {"referenceID": 38, "context": "Such considerations have also resulted in state-of-the-art architectures, for instance, the \u201cinception network\u201d (Szegedy et al., 2014) maintains a high-dimensional and sparse feature map at each layer and compresses it when convolutions of these feature maps become expensive.", "startOffset": 112, "endOffset": 134}, {"referenceID": 5, "context": "As we train deep networks on increasingly large and complex problems, hyper-parameter search is fast becoming essential (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 120, "endOffset": 167}, {"referenceID": 35, "context": "As we train deep networks on increasingly large and complex problems, hyper-parameter search is fast becoming essential (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 120, "endOffset": 167}, {"referenceID": 37, "context": "are primarily driven by hardware while others such as regularization parameters, learning rates, gradient descent hyper-parameters (Sutskever et al., 2013; Bengio, 2012) are based on either experience or a grid-based search.", "startOffset": 131, "endOffset": 169}, {"referenceID": 3, "context": "are primarily driven by hardware while others such as regularization parameters, learning rates, gradient descent hyper-parameters (Sutskever et al., 2013; Bengio, 2012) are based on either experience or a grid-based search.", "startOffset": 131, "endOffset": 169}, {"referenceID": 5, "context": "Our work is closely related to Choromanska et al. (2014) where the authors use spin glasses to analyze the energy landscape of deep networks.", "startOffset": 31, "endOffset": 57}, {"referenceID": 5, "context": "Our work is closely related to Choromanska et al. (2014) where the authors use spin glasses to analyze the energy landscape of deep networks. They show in their experiments that although there are exponentially many saddle points in the energy landscape, the Hessian at the optima recovered by stochastic gradient descent (SGD) has very few negative eigenvalues. Choromanska et al. (2014) and Sagun et al.", "startOffset": 31, "endOffset": 389}, {"referenceID": 5, "context": "Our work is closely related to Choromanska et al. (2014) where the authors use spin glasses to analyze the energy landscape of deep networks. They show in their experiments that although there are exponentially many saddle points in the energy landscape, the Hessian at the optima recovered by stochastic gradient descent (SGD) has very few negative eigenvalues. Choromanska et al. (2014) and Sagun et al. (2014) also show that SGD fails to progress beyond a very specific energy barrier, one that can be seen in spin glass computations as the onset of high-order saddle points and local minima.", "startOffset": 31, "endOffset": 413}, {"referenceID": 1, "context": "1 is motivated from recent results like Arora et al. (2014) and Neyshabur and Panigrahy (2013).", "startOffset": 40, "endOffset": 60}, {"referenceID": 1, "context": "1 is motivated from recent results like Arora et al. (2014) and Neyshabur and Panigrahy (2013). Empirically, a number of works such as Denil et al.", "startOffset": 40, "endOffset": 95}, {"referenceID": 1, "context": "1 is motivated from recent results like Arora et al. (2014) and Neyshabur and Panigrahy (2013). Empirically, a number of works such as Denil et al. (2013); Chen et al.", "startOffset": 40, "endOffset": 155}, {"referenceID": 1, "context": "1 is motivated from recent results like Arora et al. (2014) and Neyshabur and Panigrahy (2013). Empirically, a number of works such as Denil et al. (2013); Chen et al. (2015) have reported that as much as 95% connections in a typical deep network have almost zero weights.", "startOffset": 40, "endOffset": 175}, {"referenceID": 1, "context": "The above model enjoys a number of useful properties by virtue of it being random and sparsely connected (Arora et al., 2014).", "startOffset": 105, "endOffset": 125}, {"referenceID": 1, "context": "The above model enjoys a number of useful properties by virtue of it being random and sparsely connected (Arora et al., 2014). For instance, for low sparsity p > 5, the network acts as a denoising auto-encoder using the same weights (\u201cweight tying\u201d), i.e., h = g ( J(p)g ( J(p\u22121) . . .g\u2032 ( J(1)X\u0303\u2212 31n ) \u2212 31n ) . . . ) where 1n denotes a vector of 1s of length n and X\u0303 = X +\u03b4 for some small noise \u03b4 . Or equivalently, the feature vector h can be obtained from X by running the same network in reverse with a different thresholding function g\u2032(x) = 1{x\u2265 d3} Arora et al. (2014) also devise a community detection style algorithm to provably learn such a network.", "startOffset": 106, "endOffset": 579}, {"referenceID": 1, "context": "The above model enjoys a number of useful properties by virtue of it being random and sparsely connected (Arora et al., 2014). For instance, for low sparsity p > 5, the network acts as a denoising auto-encoder using the same weights (\u201cweight tying\u201d), i.e., h = g ( J(p)g ( J(p\u22121) . . .g\u2032 ( J(1)X\u0303\u2212 31n ) \u2212 31n ) . . . ) where 1n denotes a vector of 1s of length n and X\u0303 = X +\u03b4 for some small noise \u03b4 . Or equivalently, the feature vector h can be obtained from X by running the same network in reverse with a different thresholding function g\u2032(x) = 1{x\u2265 d3} Arora et al. (2014) also devise a community detection style algorithm to provably learn such a network. Our objective in this paper will be to use this model as an analytical tool to enable computations that connect deep networks to spin glasses. With a slightly different set of assumptions, there are other ways of achieving this, e.g., the model in Choromanska et al. (2014) assumes that paths from each Xi to the loss layer are independent, while we show that the contribution of correlations of Xi is small due to our assumption of sparsity.", "startOffset": 106, "endOffset": 937}, {"referenceID": 25, "context": "There is a wealth of literature from both physicists (Mezard et al., 1987; Mezard and Montanari, 2009) and mathematicians (Talagrand, 2003) that analyzes the energy landscape of spin glasses as well as their applications to optimization problems.", "startOffset": 53, "endOffset": 102}, {"referenceID": 24, "context": "There is a wealth of literature from both physicists (Mezard et al., 1987; Mezard and Montanari, 2009) and mathematicians (Talagrand, 2003) that analyzes the energy landscape of spin glasses as well as their applications to optimization problems.", "startOffset": 53, "endOffset": 102}, {"referenceID": 39, "context": ", 1987; Mezard and Montanari, 2009) and mathematicians (Talagrand, 2003) that analyzes the energy landscape of spin glasses as well as their applications to optimization problems.", "startOffset": 55, "endOffset": 72}, {"referenceID": 2, "context": "Note that crtk and crt are random variables but as Auffinger et al. (2013) shows, we can explicitly compute their limiting values.", "startOffset": 51, "endOffset": 75}, {"referenceID": 14, "context": "Theorem 3 (Fyodorov (2013)).", "startOffset": 11, "endOffset": 27}, {"referenceID": 14, "context": "Theorem 4 (Fyodorov (2013)).", "startOffset": 11, "endOffset": 27}, {"referenceID": 14, "context": "Note that these cases are computed in Fyodorov (2013) using asymptotics of the eigenvalue distribution and for large n and are hence only approximate.", "startOffset": 38, "endOffset": 54}, {"referenceID": 14, "context": "Note that these cases are computed in Fyodorov (2013) using asymptotics of the eigenvalue distribution and for large n and are hence only approximate. Also, Thm. 4 only describes the behavior of the expected number of critical points, however as the authors in Subag (2015) show, for any interval of the real line u we have limn\u2192\u221e crt(u) E crt(u) = 1 in L 2 and thus the number of critical points converges to the expected number of critical points in probability.", "startOffset": 38, "endOffset": 274}, {"referenceID": 14, "context": ", for large n, when \u03c4 \u22121, the number of critical points is of constant order, while when \u03c4 1, we have exponentially many critical points (Fyodorov, 2013).", "startOffset": 137, "endOffset": 153}, {"referenceID": 14, "context": ", for large n, when \u03c4 \u22121, the number of critical points is of constant order, while when \u03c4 1, we have exponentially many critical points (Fyodorov, 2013). Let us now consider the scaling of local minima. The following theorem shows that quite surprisingly one can simply replace the GOE eigenvalue density in Thm. 3 by the density of the largest eigenvalue to get the expected number of local minima. This translates to the following equivalent version of Thm. 4. Theorem 6 (Fyodorov (2013)).", "startOffset": 138, "endOffset": 491}, {"referenceID": 20, "context": "The weight update equation in the presence of additive noise (Jim et al., 1996) can be written as \u03c3 \u2190 \u03c3 \u2212\u03b7 \u2207Hn,p(\u03c3)\u2212\u2206 where \u2206 \u2208 Rn is a zero-mean random vector with variance \u03bdIn\u00d7n, \u2207Hn,p(\u03c3) is the back-propagated gradient and \u03b7 is the learning rate.", "startOffset": 61, "endOffset": 79}, {"referenceID": 27, "context": "Weight decay (Moody et al., 1995) implies an `2 regularization on the weights or equivalently, a Gaussian prior on them.", "startOffset": 13, "endOffset": 33}, {"referenceID": 28, "context": "3 MULTIPLICATIVE NOISE If instead of a uniform constant, we use a random matrix \u2206 \u2208Rn\u00d7n (usually diagonal) with iid entires E \u2206ii = 0 and E \u2206ii = \u03b12 to multiply the weights, we get multiplicative noise (Nalisnick et al., 2015) with the update rule \u03c3 \u2190 \u03c3 \u2212\u03b7 \u2207Hn,p(\u03c3)\u2212 p \u221a n \u2206 \u03c3 .", "startOffset": 202, "endOffset": 226}, {"referenceID": 17, "context": "Drop-Out (Hinton et al., 2012b) is a form of regularization typically used for fully connected layers where each output is forwarded with a fixed probability q and set to zero otherwise.", "startOffset": 9, "endOffset": 31}, {"referenceID": 42, "context": "Drop-Connect (Wan et al., 2013) is very similar to Drop-Out except that one sets the individual weights to zero with probability 1\u2212q.", "startOffset": 13, "endOffset": 31}], "year": 2015, "abstractText": "We study a theoretical model that connects deep learning to finding the ground state of the Hamiltonian of a spherical spin glass. Existing results motivated from statistical physics show that deep networks have a highly non-convex energy landscape with exponentially many local minima and energy barriers beyond which gradient descent algorithms cannot make progress. We leverage a technique known as topology trivialization where, upon perturbation by an external magnetic field, the energy landscape of the spin glass Hamiltonian changes dramatically from exponentially many local minima to \u201ctotal trivialization\u201d, i.e., a constant number of local minima. There also exists a transitional regime with polynomially many local minima which interpolates between these extremes. We show that a number of regularization schemes in deep learning can benefit from this phenomenon. As a consequence, our analysis provides order heuristics to choose regularization parameters and motivates annealing schemes for these perturbations.", "creator": "LaTeX with hyperref package"}}}