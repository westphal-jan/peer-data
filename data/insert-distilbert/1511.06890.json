{"id": "1511.06890", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "abstract": "this paper presents a novel nonmyopic adaptive gaussian process planning ( gpp ) framework endowed with a general class of lipschitz continuous reward functions that can unify some active learning / sensing and bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks / problems. in how particular, it utilizes importantly a principled bayesian weighted sequential decision strategy problem framework for jointly and naturally optimizing the exploration - exploitation trade - off. in general, the resulting induced gpp policy variable cannot be derived exactly due to an uncountable set of candidate observations. a key contribution of defending our work here thus lies in exploiting in the lipschitz continuity of the reward bucket functions to solve for a nonmyopic adaptive epsilon - optimal gpp ( epsilon - gpp ) policy. to plan in real time, we further propose an asymptotically optimal, branch - and - bound anytime variant of epsilon - gpp with performance guarantee. we empirically demonstrate the exact effectiveness of our epsilon - gpp policy and its anytime variant constructed in bayesian optimization and an energy harvesting task.", "histories": [["v1", "Sat, 21 Nov 2015 14:57:48 GMT  (574kb,D)", "http://arxiv.org/abs/1511.06890v1", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 17 pages"]], "COMMENTS": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 17 pages", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG cs.RO", "authors": ["chun kai ling", "kian hsiang low", "patrick jaillet"], "accepted": true, "id": "1511.06890"}, "pdf": {"name": "1511.06890.pdf", "metadata": {"source": "META", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "authors": ["Chun Kai Ling", "Kian Hsiang Low", "Patrick Jaillet"], "emails": ["lowkh}@comp.nus.edu.sg,", "jaillet@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The fundamental challenge of integrated planning and learning is to design an autonomous agent that can plan its actions to maximize its expected total rewards while interacting with an unknown task environment. Recent research efforts tackling this challenge have progressed from the use of simple Markov models assuming discrete-valued, independent observations (e.g., in Bayesian reinforcement learning (BRL) (Poupart et al. 2006)) to that of a rich class of Bayesian nonparametric Gaussian process (GP) models characterizing continuous-valued, correlated observations in order to represent the latent structure of more complex, possibly noisy task environments with higher fidelity. Such a challenge is posed by the following important problems in machine learning, among others: Active learning/sensing (AL). In the context of environmental sensing (e.g., adaptive sampling in oceanography (Leonard et al. 2007), traffic sensing (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.e., task environment) modeled by a GP subject to some sampling budget constraints (e.g., number of sensors,\nenergy consumption). The rewards of an AL agent are defined based on some formal measure of predictive uncertainty such as the entropy or mutual information criterion. To resolve the issue of sub-optimality (i.e., local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al. 2014; Low, Dolan, and Khosla 2009; 2008; 2011), some of which have further investigated the performance advantage of adaptivity by proposing nonmyopic adaptive observation selection policies that depend on past observations. Bayesian optimization (BO). Its objective is to select and gather the most informative (possibly noisy) observations for finding the global maximum of an unknown, highly complex (e.g., non-convex, no closed-form expression nor derivative) objective function (i.e., task environment) modeled by a GP given a sampling budget (e.g., number of costly function evaluations). The rewards of a BO agent are defined using an improvement-based (Brochu, Cora, and de Freitas 2010) (e.g., probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012; Herna\u0301ndez-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al. 2010). A limitation of most BO algorithms is that they are myopic. To overcome this limitation, approximation algorithms for nonmyopic adaptive BO (Marchant, Ramos, and Sanner 2014; Osborne, Garnett, and Roberts 2009) have been proposed, but their performances are not theoretically guaranteed. General tasks/problems. In practice, other types of rewards (e.g., logarithmic, unit step functions) need to be specified for an agent to plan and operate effectively in a given realworld task environment (e.g., natural phenomenon like wind or temperature) modeled by a GP, as detailed in Section 2.\nAs shall be elucidated later, similarities in the structure of the above problems motivate us to consider whether it is possible to tackle the overall challenge by devising a nonmyopic adaptive GP planning framework with a general class of reward functions unifying some AL and BO criteria and affording practitioners some flexibility to specify their desired choices for defining new tasks/problems. Such an integrated planning and learning framework has to address\nar X\niv :1\n51 1.\n06 89\n0v 1\n[ st\nat .M\nL ]\n2 1\nN ov\n2 01\nthe exploration-exploitation trade-off common to the above problems: The agent faces a dilemma between gathering observations to maximize its expected total rewards given its current, possibly imprecise belief of the task environment (exploitation) vs. that to improve its belief to learn more about the environment (exploration).\nThis paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some AL and BO criteria (e.g., UCB) discussed earlier and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems (Section 2). In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off, consequently allowing planning and learning to be integrated seamlessly and performed simultaneously instead of separately (Deisenroth, Fox, and Rasmussen 2015). In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive -optimal GPP ( -GPP) policy given an arbitrarily userspecified loss bound (Section 3). To plan in real time, we further propose an asymptotically optimal, branch-andbound anytime variant of -GPP with performance guarantee. Finally, we empirically evaluate the performances of our -GPP policy and its anytime variant in BO and an energy harvesting task on simulated and real-world environmental fields (Section 4). To ease exposition, the rest of this paper will be described by assuming the task environment to be an environmental field and the agent to be a mobile robot, which coincide with the setup of our experiments."}, {"heading": "2 Gaussian Process Planning (GPP)", "text": "Notations and Preliminaries. Let S be the domain of an environmental field corresponding to a set of sampling locations. At time step t > 0, a robot can deterministically move from its previous location st\u22121 to visit location st \u2208 A(st\u22121) and observes it by taking a corresponding realized (random) field measurement zt (Zt) where A(st\u22121) \u2286 S denotes a finite set of sampling locations reachable from its previous location st\u22121 in a single time step. The state of the robot at its initial starting location s0 is represented by prior observations/data d0 , \u3008s0, z0\u3009 available before planning where s0 and z0 denote, respectively, vectors comprising locations visited/observed and corresponding field measurements taken by the robot prior to planning and s0 is the last component of s0. Similarly, at time step t > 0, the state of the robot at its current location st is represented by observations/data dt , \u3008st, zt\u3009 where st , s0 \u2295 (s1, \u00b7 \u00b7 \u00b7 st) and zt , z0 \u2295 (z1, \u00b7 \u00b7 \u00b7 zt) denote, respectively, vectors comprising locations visited/observed and corresponding field measurements taken by the robot up until time step t and \u2018\u2295\u2019 denotes vector concatenation. At time step t > 0, the robot also receives a reward R(zt, st) to be defined later. Modeling Environmental Fields with Gaussian Processes (GPs). The GP can be used to model a spatially varying environmental field as follows: The field is assumed to be a re-\nalization of a GP. Each location s \u2208 S is associated with a latent field measurement Ys. Let YS , {Ys}s\u2208S denote a GP, that is, every finite subset of YS has a multivariate Gaussian distribution (Rasmussen and Williams 2006). Then, the GP is fully specified by its prior mean \u00b5s , E[Ys] and covariance kss\u2032 , cov[Ys, Ys\u2032 ] for all s, s\u2032 \u2208 S, the latter of which characterizes the spatial correlation structure of the environment field and can be defined using a covariance function. A common choice is the squared exponential covariance function kss\u2032 , \u03c32y exp{\u22120.5(s\u2212s\u2032)>M\u22122(s\u2212s\u2032)}where \u03c32y is the signal variance controlling the intensity of measurements and M is a diagonal matrix with length-scale components l1 and l2 governing the degree of spatial correlation or \u201csimilarity\u201d between measurements in the respective horizontal and vertical directions of the 2D fields in our experiments.\nThe field measurements taken by the robot are assumed to be corrupted by Gaussian white noise, i.e., Zt , Yst + \u03b5 where \u03b5 \u223c N (0, \u03c32n) and \u03c32n is the noise variance. Supposing the robot has gathered observations dt = \u3008st, zt\u3009 from time steps 0 to t, the GP model can perform probabilistic regression by using dt to predict the noisy measurement at any unobserved location st+1 \u2208 A(st) as well as provide its predictive uncertainty using a Gaussian predictive distribution p(zt+1|dt, st+1) = N (\u00b5st+1|dt , \u03c32st+1|st) with the following posterior mean and variance, respectively:\n\u00b5st+1|dt , \u00b5st+1 + \u03a3st+1st\u0393 \u22121 stst(zt \u2212 \u00b5st) > \u03c32st+1|st , kst+1st+1 + \u03c3 2 n \u2212 \u03a3st+1st\u0393\u22121stst\u03a3stst+1\nwhere \u00b5st is a row vector with mean components \u00b5s for every location s of st, \u03a3st+1st is a row vector with covariance components kst+1s for every location s of st, \u03a3stst+1 is the transpose of \u03a3st+1st , and \u0393stst , \u03a3stst + \u03c3 2 nI such that \u03a3stst is a covariance matrix with components kss\u2032 for every pair of locations s, s\u2032 of st. An important property of the GP model is that, unlike \u00b5st+1|dt , \u03c3 2 st+1|st is independent of zt. Problem Formulation. To frame nonmyopic adaptive Gaussian process planning (GPP) as a Bayesian sequential decision problem, let an adaptive policy \u03c0 be defined to sequentially decide the next location \u03c0(dt) \u2208 A(st) to be observed at each time step t using observations dt over a finite planning horizon of H time steps/stages (i.e., sampling budget of H locations). The value V \u03c00 (d0) under an adaptive policy \u03c0 is defined to be the expected total rewards achieved by its selected observations when starting with some prior observations d0 and following \u03c0 thereafter and can be computed using the following H-stage Bellman equations:\nV \u03c0t (dt) , Q \u03c0 t (dt, \u03c0(dt))\nQ\u03c0t (dt, st+1) , E[R(Zt+1, st+1) + V \u03c0t+1(\u3008st+1, zt \u2295 Zt+1\u3009)|dt, st+1]\nfor stages t = 0, . . . ,H \u2212 1 where V \u03c0H(dH) , 0. To solve the GPP problem, the notion of Bayes-optimality1 is exploited for selecting observations to achieve the largest\n1Bayes-optimality has been studied in discrete BRL (Poupart et al. 2006) whose assumptions (Section 1) do not hold in GPP. Continuous BRLs (Dallaire et al. 2009; Ross, Chaib-draa, and Pineau 2008) assume a known parametric form of observation function,\npossible expected total rewards with respect to all possible induced sequences of future Gaussian posterior beliefs p(zt+1|dt, st+1) for t = 0, . . . ,H \u2212 1 to be discussed next. Formally, this involves choosing an adaptive policy \u03c0 to maximize V \u03c00 (d0), which we call the GPP policy \u03c0\n\u2217. That is, V \u22170 (d0) , V \u03c0\u2217 0 (d0) = max\u03c0 V \u03c0 0 (d0). By plugging \u03c0 \u2217 into V \u03c0t (dt) and Q \u03c0 t (dt, st+1) above,\nV \u2217t (dt) , maxst+1\u2208A(st)Q \u2217 t (dt, st+1)\nQ\u2217t (dt, st+1) , E[R(Zt+1, st+1)|dt, st+1] + E[V \u2217t+1(\u3008st+1, zt \u2295 Zt+1\u3009)|dt, st+1]\n(1)\nfor stages t = 0, . . . ,H \u2212 1 where V \u2217H(dH) , 0. To see how the GPP policy \u03c0\u2217 jointly and naturally optimizes the exploration-exploitation trade-off, its selected location \u03c0\u2217(dt) = arg maxst+1\u2208A(st)Q \u2217 t (dt, st+1) at each time step t affects both the immediate expected reward E[R(Zt+1, st \u2295 \u03c0\u2217(dt))|dt, \u03c0\u2217(dt)] given current belief p(zt+1|dt, \u03c0\u2217(dt)) (i.e., exploitation) as well as the Gaussian posterior belief p(zt+2|\u3008st \u2295 \u03c0\u2217(dt), zt \u2295 zt+1\u3009, \u03c0\u2217(\u3008st \u2295 \u03c0\u2217(dt), zt \u2295 zt+1\u3009)) at next time step t + 1 (i.e., exploration), the latter of which influences expected future rewards E[V \u2217t+1(\u3008st \u2295 \u03c0\u2217(dt), zt \u2295 Zt+1\u3009)|dt, \u03c0\u2217(dt)].\nIn general, the GPP policy \u03c0\u2217 cannot be derived exactly because the expectation terms in (1) usually cannot be evaluated in closed form due to an uncountable set of candidate measurements (Section 1) except for degenerate cases like R(zt+1, st+1) being independent of zt+1 and H \u2264 2. To overcome this difficulty, we will show in Section 3 later how the Lipschitz continuity of the reward functions can be exploited for theoretically guaranteeing the performance of our proposed nonmyopic adaptive -optimal GPP policy, that is, the expected total rewards achieved by its selected observations closely approximates that of \u03c0\u2217 within an arbitrarily user-specified loss bound > 0. Lipschitz Continuous Reward Functions. R(zt, st) , R1(zt)+R2(zt)+R3(st) whereR1,R2, andR3 are user-defined reward functions that satisfy the conditions below: \u2022 R1(zt) is Lipschitz continuous in zt with Lipschitz con-\nstant `1. So, h\u03c3(u) , (R1 \u2217 N (0, \u03c32))(u) is Lipschitz continuous in u with `1 where \u2018\u2217\u2019 denotes convolution; \u2022 R2(zt): Define g\u03c3(u) , (R2 \u2217 N (0, \u03c32))(u) such that (a) g\u03c3(u) is well-defined for all u \u2208 R, (b) g\u03c3(u) can be evaluated in closed form or computed up to an arbitrary precision in reasonable time for all u \u2208 R, and (c) g\u03c3(u) is Lipschitz continuous2 in u with Lipschitz constant `2(\u03c3); \u2022 R3(st) only depends on locations st visited/observed by the robot up until time step t and is independent of realized measurement zt. It can be used to represent some sampling or motion costs or explicitly consider exploration by defining it as a function of \u03c32st+1|st . Using the above definition of R(zt, st), the immediate expected reward in (1) evaluates to E[R(Zt+1, st+1)|dt, st+1]\nthe reward function to be independent of measurements and past states, and/or, when exploiting GP, maximum likelihood observations during planning with no provable performance guarantee.\n2Unlike R1, R2 does not need to be Lipschitz continuous (or continuous); it must only be Lipschitz continuous after convolution with any Gaussian kernel. An example of R2 is unit step function.\n= (h\u03c3st+1|st +g\u03c3st+1|st ) ( \u00b5st+1|dt ) +R3(st+1) which is Lipschitz continuous in the realized measurements zt:\nLemma 1 Let \u03b1(st+1) , \u2016\u03a3st+1st\u0393\u22121stst\u2016 and d \u2032 t , \u3008st, z\u2032t\u3009. Then,|E[R(Zt+1,st+1)|dt,st+1]\u2212E[R(Zt+1,st+1)|d\u2032t,st+1]| \u2264 \u03b1(st+1) ( `1 + `2(\u03c3st+1|st) ) \u2016zt \u2212 z\u2032t\u2016 .\nIts proof is in Appendix A. Lemma 1 will be used to prove the Lipschitz continuity of V \u2217t in (1) later. Before doing this, let us consider how the Lipschitz continuous reward functions defined above can unify some AL and BO criteria discussed in Section 1 and be used for defining new tasks/problems. Active learning/sensing (AL). Setting R(zt+1, st+1) = R3(st+1) = 0.5 log(2\u03c0e\u03c3\n2 st+1|st) yields the well-known\nnonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c32st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u00b5st+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u00b5st+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements\u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = \u03b2\u03c3st+1|st , E[R(Zt+1, st+1)|dt, st+1] = \u00b5st+1|dt + \u03b2\u03c3st+1|st . In particular, when \u03b2 = 0, it can be derived that our GPP policy \u03c0\u2217 maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense. So, unlike greedy UCB, our nonmyopic GPP framework does not have to explicitly consider an additional weighted exploration term (i.e., \u03b2\u03c3st+1|st ) in its reward function because it can jointly and naturally optimize the exploration-exploitation trade-off, as explained earlier. Nevertheless, if a stronger exploration behavior is desired (e.g., in online planning), then \u03b2 has to be fine-tuned. Different from nonmyopic BO algorithm of Marchant, Ramos, and Sanner (2014) using UCB-based rewards, our proposed nonmyopic -optimal GPP policy (Section 3) does not need to impose an extreme assumption of maximum likelihood observations during planning and, more importantly, provides a performance guarantee, including for the extreme\nassumption made by nonmyopic UCB. Our GPP framework differs from nonmyopic BO algorithm of Osborne, Garnett, and Roberts (2009) in that every selected observation contributes to the total field measurements taken by the robot instead of considering just the expected improvement for the last observation. So, it usually does not have to expend all the given sampling budget to find the global maximum. General tasks/problems. In practice, the necessary reward function can be more complex than the ones specified above that are formed from an identity function of the field measurement. For example, consider the problem of placing wind turbines in optimal locations to maximize the total power production. Though the average wind speed in a region can be modeled by a GP, the power output is not a linear function of the steady-state wind speed. In fact, power production requires a certain minimum speed known as the cutin speed. After this threshold is met, power output increases and eventually plateaus. Assuming the cut-in speed is 1, this effect can be modeled with a logarithmic reward function3: R(zt+1, st+1) = R1(zt+1) gives a value of log(zt+1) if zt+1 > 1, and 0 otherwise where `1 = 1. To the best of our knowledge, h\u03c3st+1|st (u) has no closed-form expression. In Appendix B, we present other interesting reward functions like unit step function2 and Gaussian distribution that can be represented by R(zt+1, st+1) and used in real-world tasks.\nTheorem 1 below reveals that V \u2217t (dt) (1) with Lipschitz continuous reward functions is Lipschitz continuous in zt with Lipschitz constant Lt(st) defined below: Definition 1 Let LH(sH) , 0. For t = 0, . . . ,H \u2212 1, define Lt(st) , maxst+1\u2208A(st) \u03b1(st+1) ( `1 + `2(\u03c3st+1|st) ) +\nLt+1(st+1) \u221a 1 + \u03b1(st+1)2 . Theorem 1 (Lipschitz Continuity of V \u2217t ) For t = 0, . . . ,H , |V \u2217t (dt)\u2212 V \u2217t (d\u2032t)| \u2264 Lt(st)\u2016zt \u2212 z\u2032t\u2016 . Its proof uses Lemma 1 and is in Appendix C. The result below is a direct consequence of Theorem 1 and will be used to theoretically guarantee the performance of our proposed nonmyopic adaptive -optimal GPP policy in Section 3: Corollary 1 For t = 0, . . . ,H , |V \u2217t (\u3008st, zt\u22121 \u2295 zt\u3009) \u2212 V \u2217t (\u3008st, zt\u22121 \u2295 z\u2032t\u3009)| \u2264 Lt(st)|zt \u2212 z\u2032t|."}, {"heading": "3 -Optimal GPP ( -GPP)", "text": "The key idea of constructing our proposed nonmyopic adaptive -GPP policy is to approximate the expectation terms in (1) at every stage using a form of deterministic sampling, as illustrated in the figure below. Specifically, the measurement space of p(zt+1|dt, st+1) is first partitioned into n \u2265 2 intervals \u03b60, . . . , \u03b6n\u22121 such that intervals \u03b61, . . . , \u03b6n\u22122 are equally spaced within the bounded gray region [\u00b5st+1|dt \u2212 \u03c4\u03c3st+1|st , \u00b5st+1|dt + \u03c4\u03c3st+1|st ] specified by a user-defined width parameter \u03c4 \u2265 0 while intervals \u03b60 and \u03b6n\u22121 span the two infinitely long red tails. Note that \u03c4 > 0 requires n > 2 for the partition to be valid. The n sample measurements z0 . . . zn\u22121 are then selected by setting z0 as upper limit of red interval \u03b60, zn\u22121 as lower limit of red interval \u03b6n\u22121, and z1, . . . , zn\u22122 as centers of the respective gray intervals\n3In reality, the speed-power relationship is not exactly logarithmic, but this approximation suffices for the purpose of modeling.\np(zt+1|dt, st+1)\nz0 , \u00b5st+1|dt\u2212\u03c4\u03c3st+1|st ; zn\u22121 , \u00b5st+1|dt+\u03c4\u03c3st+1|st ;\nzi,z0+i\u22120.5n\u22122 (z n\u22121\u2212z0)\nfor i = 1, . . . , n\u2212 2.\nwi,\u03a6( 2i\u03c4n\u22122\u2212\u03c4)\u2212\u03a6( 2(i\u22121)\u03c4 n\u22122 \u2212\u03c4) for i = 1, . . . , n\u2212 2; w0 = wn\u22121 , \u03a6(\u2212\u03c4).\nz0\nw0\nz1\nw1\n. . .\n. . .\n. . .\n. . .\nzi-1\nwi-1\nzi\nwi\nzi+1\nwi+1\nzn-1\nwn-1\nzn-2\nwn-2\n. . .\n. . .\n. . .\n. . .\nzt+1\n\u03b61 \u03b6i-1 \u03b6i \u03b6i+1 \u03b6n-2\u03b60 \u03b6n-1\n\u03b61, . . . , \u03b6n\u22122. Next, the weights w0 . . . wn\u22121 for the corresponding sample measurements z0, . . . , zn\u22121 are defined as the areas under their respective intervals \u03b60, . . . , \u03b6n\u22121 of the Gaussian predictive distribution p(zt+1|dt, st+1). So,\u2211n\u22121 i=0 w\ni = 1. An example of such a partition is given in Appendix D. The selected sample measurements and their corresponding weights can be exploited for approximating V \u2217t with Lipschitz continuous reward functions (1) using the following H-stage Bellman equations:\nV t (dt) , maxst+1\u2208A(st)Q t(dt, st+1) Q t(dt, st+1) , g\u03c3st+1|st ( \u00b5st+1|dt ) +R3(st+1) +\nn\u22121\u2211 i=0 wi ( R1(z i) + V t+1(\u3008st+1, zt \u2295 zi\u3009) )(2)\nfor stages t = 0, . . . ,H\u2212 1 where V H(dH) , 0. The resulting induced -GPP policy \u03c0 jointly and naturally optimizes the exploration-exploitation trade-off in a similar manner as that of the GPP policy \u03c0\u2217, as explained in Section 2. It is interesting to note that setting \u03c4 = 0 yields z0 = . . . = zn\u22121 = \u00b5st+1|dt , which is equivalent to selecting a single sample measurement of \u00b5st+1|dt with corresponding weight of 1. This is identical to the special case of maximum likelihood observations during planning which is the extreme assumption used by nonmyopic UCB (Marchant, Ramos, and Sanner 2014) for sampling to gain time efficiency. Performance Guarantee. The difficulty in theoretically guaranteeing the performance of our -GPP policy \u03c0 (i.e., relative to that of GPP policy \u03c0\u2217) lies in analyzing how the values of the width parameter \u03c4 and deterministic sampling size n can be chosen to satisfy the user-specified loss bound , as discussed below. The first step is to prove that V t in (2) approximates V \u2217t in (1) closely for some chosen \u03c4 and n values, which relies on the Lipschitz continuity of V \u2217t in Corollary 1. Define \u039b(n, \u03c4) to be equal to the value of \u221a 2/\u03c0 if n \u2265 2\u2227\u03c4 = 0, and value of \u03ba(\u03c4)+\u03b7(n, \u03c4) if n > 2\u2227\u03c4 > 0 where \u03ba(\u03c4) , \u221a 2/\u03c0 exp(\u22120.5\u03c42) \u2212 2\u03c4\u03a6(\u2212\u03c4), \u03b7(n, \u03c4) , 2\u03c4(0.5\u2212\u03a6(\u2212\u03c4))/(n\u2212 2), and \u03a6 is a standard normal CDF. Theorem 2 Suppose that \u03bb > 0 is given. For all dt and t = 0, . . . ,H , if\n\u03bb \u2265 \u039b(n, \u03c4)\u03c3st+1|st(`1 + Lt+1(st+1)) (3) for all st+1 \u2208 A(st), then |V t (dt)\u2212 V \u2217t (dt)| \u2264 \u03bb(H \u2212 t) . Its proof uses Corollary 1 and is given in Appendix E. Remark 1. From Theorem 2, a tighter bound on the error |V t (dt) \u2212 V \u2217t (dt)| can be achieved by decreasing the sam-\npling budget of H locations4 and increasing the deterministic sampling size n; increasing n reduces \u03b7(n, \u03c4) and hence \u039b(n, \u03c4), which allows \u03bb to be reduced as well. The width parameter \u03c4 has a mixed effect on this error bound: Note that \u03ba(\u03c4) (\u03b7(n, \u03c4)) is proportional to some upper bound on the error incurred by the extreme sample measurements z0 and zn\u22121 (z1, . . . , zn\u22122), as shown in Appendix E. Increasing \u03c4 reduces \u03ba(\u03c4) but unfortunately raises \u03b7(n, \u03c4). So, in order to reduce \u039b(n, \u03c4) further by increasing \u03c4 , it has to be complemented by raising n fast enough to keep \u03b7(n, \u03c4) from increasing. This allows \u03bb to be reduced further as well. Remark 2. A feasible choice of \u03c4 and n satisfying (3) can be expressed analytically in terms of the given \u03bb and hence computed prior to planning, as shown in Appendix F. Remark 3. \u03c3st+1|st and Lt+1(st+1) for all st+1 and t = 0, . . . ,H \u2212 1 can be computed prior to planning as they depend on s0 and all reachable locations from s0 but not on their measurements.\nUsing Theorem 2, the next step is to bound the performance loss of our -GPP policy \u03c0 relative to that of GPP policy \u03c0\u2217, that is, policy \u03c0 is -optimal:\nTheorem 3 Given the user-specified loss bound > 0, V \u22170 (d0) \u2212 V \u03c0\n0 (d0) \u2264 by substituting \u03bb = /(H(H + 1)) into the choice of \u03c4 and n stated in Remark 2 above.\nIts proof is in Appendix G. It can be observed from Theorem 3 that a tighter bound on the error V \u22170 (d0)\u2212 V \u03c0\n0 (d0) can be achieved by decreasing the sampling budget of H locations4 and increasing the deterministic sampling size n. The effect of width parameter \u03c4 on this error bound is the same as that on the error bound of |V t (dt) \u2212 V \u2217t (dt)|, as explained in Remark 1 above. Anytime -GPP. Unlike GPP policy \u03c0\u2217, our -GPP policy \u03c0 can be derived exactly since its incurred time is independent of the size of the uncountable set of candidate measurements. However, expanding the entire search tree of -GPP (2) incurs time containing a O(nH) term and is not always necessary to achieve -optimality in practice. To mitigate this computational difficulty5, we propose an anytime variant of -GPP that can produce a good policy fast and improve its approximation quality over time, as briefly discussed here and detailed with the pseudocode in Appendix H.\nThe key intuition is to expand the sub-trees rooted at \u201cpromising\u201d nodes with the highest weighted uncertainty of their corresponding values V \u2217t (dt) so as to improve their estimates. To represent such uncertainty at each encountered node, upper & lower heuristic bounds (respectively, V \u2217 t (dt) and V \u2217t (dt)) are maintained, like in (Smith and Simmons 2006). A partial construction of the entire tree is maintained and expanded incrementally in each iteration of anytime - GPP that incurs linear time in n and comprises 3 steps: Node selection. Traverse down the partially constructed tree by repeatedly selecting nodes with largest difference between their upper and lower bounds (i.e., uncertainty) discounted by weightwi \u2217 of its preceding sample measurement\n4This changes -GPP by reducing its planning horizon though. 5The value of n is a bigger computational issue than that of H\nwhen is small and in online planning.\nzi \u2217\nuntil an unexpanded node, denoted by dt, is reached. Expand tree. Construct a \u201cminimal\u201d sub-tree rooted at node dt by sampling all possible next locations and only their median sample measurements z i\u0304 recursively up to full heightH. Backpropagation. Backpropagate bounds from the leaves of the newly constructed sub-tree to node dt, during which the refined bounds of expanded nodes are used to inform the bounds of unexpanded siblings by exploiting the Lipschitz continuity of V \u2217t (Corollary 1), as explained in Appendix H. Backpropagate bounds to the root of the partially constructed tree in a similar manner.\nBy using the lower heuristic bound to produce our anytime -GPP policy, its performance loss relative to that of GPP policy \u03c0\u2217 can be bounded, as proven in Appendix H."}, {"heading": "4 Experiments and Discussion", "text": "This section empirically evaluates the online planning performance and time efficiency of our -GPP policy \u03c0 and its anytime variant under limited sampling budget in an energy harvesting task on a simulated wind speed field and in BO on simulated plankton density (chl-a) field and real-world logpotassium (lg-K) concentration (mg l\u22121) field (Appendix I) of Broom\u2019s Barn farm (Webster and Oliver 2007). Each simulated (real-world lg-K) field is spatially distributed over a 0.95 km by 0.95 km (520 m by 440 m) region discretized into a 20 \u00d7 20 (14 \u00d7 12) grid of sampling locations. These fields are assumed to be realizations of GPs. The wind speed (chl-a) field is simulated using hyperparameters \u00b5s = 0,6 l1 = l2 = 0.2236 (0.2) km, \u03c32n = 10\n\u22125, and \u03c32y = 1. The hyperparameters \u00b5s = 3.26, l1 = 42.8 m, l2 = 103.6 m, \u03c32n = 0.0222, and \u03c3 2 y = 0.057 of lg-K field are learned using maximum likelihood estimation (Rasmussen and Williams 2006). The robot\u2019s initial starting location is near to the center of each simulated field and randomly selected for lg-K field. It can move to any of its 4 adjacent grid locations at each time step and is tasked to maximize its total rewards over 20 time steps (i.e., sampling budget of 20 locations).\nIn BO, the performances of our -GPP policy \u03c0 and its anytime variant are compared with that of state-of-the-art nonmyopic UCB (Marchant, Ramos, and Sanner 2014) and greedy PI, EI, UCB (Brochu, Cora, and de Freitas 2010; Srinivas et al. 2010). Three performance metrics are used: (a) Total rewards achieved over the evolved time steps (i.e., higher total rewards imply less total regret in BO (Section 2)), (b) maximum reward achieved during experiment, and (c) search tree size in terms of no. of nodes (i.e., larger tree size implies higher incurred time). All experiments are run on a Linux machine with Intel Core i5 at 1.7 GHz. Energy Harvesting Task on Simulated Wind Speed Field. A robotic rover equipped with a wind turbine is tasked to harvest energy/power from the wind while exploring a polar region (Chen et al. 2014). It is driven by the logarithmic reward function described under \u2018General tasks/problems\u2019 in Section 2. Fig. 1 shows results of performances of our -GPP policy and its anytime variant averaged over 30 independent realizations of the wind speed field. It can be observed\n6Its actual prior mean is not zero; we have applied zero-mean GP to Ys \u2212 \u00b5s for simplicity.\nvs. no. of time steps for energy harvesting task. The plot of \u2217 = 5 uses our anytime variant with a maximum tree size of 5\u00d7 104 nodes while the plot of = 250 effectively assumes maximum likelihood observations during planning like that of nonmyopic UCB (Marchant, Ramos, and Sanner 2014).\nthat the gradients of the achieved total rewards (i.e., power production) increase over time, which indicate a higher obtained reward with an increasing number of time steps as the robot can exploit the environment more effectively with the aid of exploration from previous time steps. The gradients eventually stop increasing when the robot enters a perceived high-reward region. Further exploration is deemed unnecessary as it is unlikely to find another preferable location within H \u2032 time steps; so, the robot remains near-stationary for the remaining time steps. It can also be observed that the incurred time is much higher in the first few time steps. This is expected because the posterior variance \u03c3st+1|st decreases with increasing time step t, thus requiring a decreasing deterministic sampling size n to satisfy (3).\nInitially, all -GPP policies achieve similar total rewards as the robots begin from the same starting location. After some time, -GPP policies with lower user-specified loss bound and longer online planning horizonH \u2032 achieve considerably higher total rewards at the cost of more incurred time. In particular, it can be observed that a robot assuming maximum likelihood observations during planning (i.e., = 250) like that of nonmyopic UCB or using a greedy policy (i.e., H \u2032 = 1) performs poorly very quickly. In the former case (Fig. 1a), the gradient of its total rewards stops increasing quite early (i.e., from time step 9 onwards), which indicates that its perceived local maximum is reached prematurely. Interestingly, it can be observed from Fig. 1d that the -GPP policy with H \u2032 = 2 and = 0.06 incurs more time than that with H \u2032 = 3 and = 0.8 despite the latter achieving higher total rewards. This suggests trading off tighter loss bound for longer online planning horizon H \u2032, especially when is too small that in turn requires a very large n and consequently incurs significantly more time5."}, {"heading": "BO on Real-World Log-Potassium Concentration Field.", "text": "An agricultural robot is tasked to find the peak lg-K measurement (i.e., possibly in an over-fertilized area) while exploring the Broom\u2019s Barn farm (Webster and Oliver 2007). It is driven by the UCB-based reward function described under \u2018BO\u2019 in Section 2. Fig. 2 shows results of performances of our -GPP policy and its anytime variant, nonmyopic UCB (i.e., = 25), and greedy PI, EI, UCB (i.e., H \u2032 = 1) aver-\naged over 25 randomly selected robot\u2019s initial starting location. It can be observed from Figs. 2a and 2b that the gradients of the achieved total normalized7 rewards generally increase over time. In particular, from Fig. 2a, nonmyopic UCB assuming maximum likelihood observations during planning obtains much less total rewards than the other - GPP policies and the anytime variant after 20 time steps and finds a maximum lg-K measurement of 3.62 that is at least 0.4\u03c3y worse after 20 time steps. The performance of the anytime variant is comparable to that of our best-performing - GPP policy with = 3. From Fig. 2b, the greedy policy (i.e., H \u2032 = 1) with \u03b2 = 0 performs much more poorly than its nonmyopic -GPP counterparts and finds a maximum lg-K measurement of 3.56 that is lower than that of greedy PI and EI due to its lack of exploration. By increasing H \u2032 to 2-4, our -GPP policies with \u03b2 = 0 outperform greedy PI and EI as they can naturally and jointly optimize the explorationexploitation trade-off. Interestingly, Fig. 2c shows that our -GPP policy with \u03b2 = 2 achieves the highest total rewards after 20 time steps, which indicates the need of a slightly stronger exploration behavior than that with \u03b2 = 0. This may be explained by a small length-scale (i.e., spatial correlation) of the lg-K field, thus requiring some exploration to find the peak measurement. By increasing H \u2032 beyond 4 or with larger spatial correlation (Appendix J), we expect a diminishing role of the \u03b2\u03c3st+1|st term. It can also be observed that aggressive exploration (i.e., \u03b2 \u2265 10) hurts the performance. Results of the tree size (i.e., incurred time) of our -GPP policy and its anytime variant are in Appendix I.\nDue to lack of space, we present additional experimental results for BO on simulated plankton density field in Appendix J that yield similar observations to the above."}, {"heading": "5 Conclusion", "text": "This paper describes a novel nonmyopic adaptive -GPP framework endowed with a general class of Lipschitz continuous reward functions that can unify some AL and BO criteria and be used for defining new tasks/problems. In particular, it can jointly and naturally optimize the explorationexploitation trade-off. We theoretically guarantee the performances of our -GPP policy and its anytime variant and em-\n7To ease interpretation of the results, each reward is normalized by subtracting the prior mean from it.\npirically demonstrate their effectiveness in BO and an energy harvesting task. For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al. 2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al. 2014). Acknowledgments. This work was supported by SingaporeMIT Alliance for Research and Technology Subaward Agreement No. 52 R-252-000-550-592."}, {"heading": "27: \u3008V \u2217t+1(\u3008st+1, zt \u2295 zi", "text": "\u2217\u3009), V \u2217t+1(\u3008st+1, zt \u2295 zi \u2217\u3009)\u3009 \u2190 CONSTRUCTTREE(t+ 1, \u3008st+1, zt \u2295 zi\n\u2217\u3009, \u03bb) 28: REFINEBOUNDS(t, dt, st+1, i\u2217, \u03bb) 29: r \u2190 g\u03c3st+1|st (\u00b5st+1|dt) +R3(st+1) 30: Q \u2217 t (dt, st+1)\u2190 r + (\u2211n\u22121 i=0 w i(R1(z i) + V \u2217 t+1(\u3008st+1, zt \u2295 zi\u3009)) ) + \u03bb\n31: Q\u2217 t (dt, st+1)\u2190 r + (\u2211n\u22121 i=0 w i(R1(z i) + V \u2217t+1(\u3008st+1, zt \u2295 zi\u3009)) ) \u2212 \u03bb 32: V \u2217 t (dt)\u2190 maxs\u2032t+1\u2208A(st)Q \u2217 t (dt, s \u2032 t+1) 33: V \u2217t (dt)\u2190 maxs\u2032t+1\u2208A(st)Q \u2217 t (dt, s \u2032 t+1) 34: return \u3008V \u2217t (dt), V \u2217 t (dt)\u3009 35: else 36: return EXPANDTREE(t, dt, \u03bb) 37: function ANYTIME- -GPP(d0, ) 38: for all reachable st+1 from s0 do 39: precompute \u03c3st+1|st and Lt+1(st+1) for all st+1 and t = 0, . . . ,H \u2212 1 40: \u03bb\u2190 /(H(H + 1)) 41: while resources permit do 42: \u3008V \u22170(d0), V \u2217 0(d0)\u3009 \u2190 CONSTRUCTTREE(0, d0, \u03bb) 43: \u03b1\u2190 V \u22170(d0)\u2212 V \u2217 0(d0) 44: return \u03c0\u03b1 (d0)\u2190 argmaxs1\u2208A(s0)Q \u2217 0 (d0, s1)\n\u2022 The result below exploits the Lipschitz continuity of V \u2217t (Corollary 1) for producing informed upper and lower heuristic bounds in lines 20 and 21 of Algorithm 1, respectively: Theorem 5 For t = 0, . . . ,H and i, k = 0, . . . , n\u2212 1,\nV \u2217t (\u3008st, zt\u22121 \u2295 zk\u3009)\u2212 Lt(st)|zi \u2212 zk| \u2264 V \u2217t (\u3008st, zt\u22121 \u2295 zi\u3009) \u2264 V \u2217 t (\u3008st, zt\u22121 \u2295 zk\u3009) + Lt(st)|zi \u2212 zk|.\nProof.\nV \u2217t (\u3008st, zt\u22121 \u2295 zi\u3009) \u2264 V \u2217t (\u3008st, zt\u22121 \u2295 zk\u3009) + Lt(st)|zi \u2212 zk| \u2264 V \u2217t (\u3008st, zt\u22121 \u2295 zk\u3009) + Lt(st)|zi \u2212 zk| .\nThe first inequality is due to Corollary 1 while the second inequality follows from Theorem 4. Similarly,\nV \u2217t (\u3008st, zt\u22121 \u2295 zi\u3009) \u2265 V \u2217t (\u3008st, zt\u22121 \u2295 zk\u3009)\u2212 Lt(st)|zi \u2212 zk| \u2265 V \u2217t (\u3008st, zt\u22121 \u2295 zk\u3009)\u2212 Lt(st)|zi \u2212 zk| .\nThe first inequality is due to Corollary 1 while the second inequality follows from Theorem 4. \u2022 Finally, the performance loss of our anytime -GPP policy relative to that of GPP policy \u03c0\u2217 can be bounded:\nTheorem 6 Suppose that the user-specified loss bound > 0 is given, our anytime -GPP is terminated at \u03b1 , V \u22170(d0) \u2212 V \u22170(d0), and our anytime -GPP policy is defined as \u03c0 \u03b1 (d0) , argmaxs1\u2208A(s0)Q \u2217 0 (d0, s1). Then, V \u22170 (d0) \u2212 V \u03c0 \u03b1\n0 (d0) \u2264 \u03b1H by setting and substituting \u03bb = /(H(H + 1)) into the choice of \u03c4 and n stated in Remark 2 in Section 3. Proof. It follows that V \u22170 (d0) \u2212 V \u2217 0(d0) \u2264 \u03b1. In general, given that the length of planning horizon is reduced to H \u2212 t for t = 0, . . . ,H \u2212 1, this inequality is equivalent to\nV \u2217t (dt)\u2212 V \u2217 t (dt) \u2264 \u03b1 (16)\nby increasing/shifting the indices of V \u22170 (d0) and V \u2217 0 above from 0 to t so that these value functions start at stage t instead. We will give a proof by induction on t that\nV \u2217t (dt)\u2212 V \u03c0 \u03b1 t (dt) \u2264 \u03b1(H \u2212 t) . (17)\nThe base case of t = H is trivially true. Supposing (17) holds for t+1 (i.e., induction hypothesis), we will prove that it holds for 0 \u2264 t < H . For all st+1 \u2208 A(st),\n|Q\u2217t (dt, st+1)\u2212Q\u03c0 \u03b1 t (dt, st+1)| \u2264 \u221e\u222b \u2212\u221e p(zt+1|dt, st+1) \u2223\u2223\u2223V \u2217t+1(dt+1)\u2212 V \u03c0\u03b1 t+1 (dt+1)\u2223\u2223\u2223 dzt+1 \u2264 \u03b1(H \u2212 t\u2212 1) .\n(18)\nThe first inequality follows from definitions of Q\u2217t (dt, st+1) (1) and Q \u03c0\u03b1\nt (dt, st+1). The last inequality follows from the induction hypothesis.\nV \u2217t (dt)\u2212 V \u03c0 \u03b1\nt (dt) = V \u2217t (dt)\u2212 V \u2217 t (dt) + V \u2217 t (dt)\u2212 V \u03c0 \u03b1\nt (dt) \u2264 \u03b1+Q\u2217\nt (dt, \u03c0\n\u03b1 (dt))\u2212Q\u03c0 \u03b1 t (dt, \u03c0 \u03b1 (dt))\n\u2264 \u03b1+Q\u2217 t (dt, \u03c0 \u03b1 (dt))\u2212Q\u2217t (dt, \u03c0\u03b1 (dt))\ufe38 \ufe37\ufe37 \ufe38 \u22640 +Q\u2217t (dt, \u03c0 \u03b1 (dt))\u2212Q\u03c0 \u03b1 t (dt, \u03c0 \u03b1 (dt))\ufe38 \ufe37\ufe37 \ufe38\n\u2264\u03b1(H\u2212t\u22121) \u2264 \u03b1(H \u2212 t) .\nThe first inequality is due to (16). The second inequality follows from (15) and (18). By substituting t = 0, Theorem 6 results.\nI Bayesian Optimization (BO) on Real-World Log-Potassium Concentration Field\nJ Bayesian Optimization (BO) on Simulated Plankton Density Field A robotic boat equipped with a fluorometer is tasked to find the peak chl-a measurement (i.e., possibly in an algal bloom) while exploring a lake (Dolan et al. 2009). It is driven by the UCB-based reward function described under \u2018Bayesian optimization\u2019 in Section 2.\nFig. 6 shows results of performances of our -GPP policy and its anytime variant, nonmyopic UCB (i.e., = 250), and greedy PI, EI, UCB (i.e., H \u2032 = 1) averaged over 30 independent realizations of the chl-a field. Similar to that of the energy harvesting task, the gradients of their achieved total rewards increase over time. In particular, it can be observed from Fig. 6a that nonmyopic UCB assuming maximum likelihood observations during planning obtains much less total rewards than the other -GPP policies and the anytime variant and finds a maximum chl-a measurement of 1.25 that is at least 0.26\u03c3y worse after 20 time steps. The anytime variant performs reasonably well, albeit slightly worse than our -GPP policy with = 25.0. From\nFig. 6b, the greedy policy (i.e., H \u2032 = 1) performs much more poorly than its nonmyopic counterparts and finds a maximum chl-a measurement of 1.28 that is at least 0.22\u03c3y worse after 20 time steps. Such a greedy policy with \u03b2 = 0 is also worse than greedy PI and EI due to its lack of exploration. However, by increasing H \u2032 to 3 or 4, our -GPP policies with \u03b2 = 0 outperform greedy PI and EI as they can naturally and jointly optimize the exploration-exploitation trade-off. To see this, Figs. 6c to 6f reveal that the weighted exploration term \u03b2\u03c3st+1|st in the UCB-based reward function becomes redundant when H\n\u2032 \u2265 3; in fact, such nonmyopic -GPP policies with \u03b2 = 0 achieve the highest total rewards among all -GPP policies withH \u2032 = 1, 2, 3, 4 and \u03b2 = 0, 1, 10, 20 and their performances degrade with stronger exploration behavior (i.e., \u03b2 > 0). Results of the tree size (i.e., incurred time) of our -GPP policy and its anytime variant are similar to that of the energy harvesting task and shown in Fig. 7."}], "references": [{"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Design and power management of a wind-solar-powered polar rover", "author": ["J. Chen", "J. Liang", "T. Wang", "T. Zhang", "Y. Wu"], "venue": "Journal of Ocean and Wind Energy 1(2):65\u201373.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobility-ondemand systems", "author": ["J. Chen", "K.H. Low", "P. Jaillet", "Y. Yao"], "venue": "IEEE Trans. Autom. Sci. Eng. 12:901\u2013921.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Bayesian reinforcement learning in continuous POMDPs with Gaussian processes", "author": ["P. Dallaire", "C. Besse", "S. Ross", "B. Chaib-draa"], "venue": "Proc. IEEE/RSJ IROS, 2604\u2013 2609.", "citeRegEx": "Dallaire et al\\.,? 2009", "shortCiteRegEx": "Dallaire et al\\.", "year": 2009}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(2):408\u2013423.", "citeRegEx": "Deisenroth et al\\.,? 2015", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2015}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "JMLR 13:1809\u2013 1837.", "citeRegEx": "Hennig and Schuler,? 2012", "shortCiteRegEx": "Hennig and Schuler", "year": 2012}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "Proc. NIPS.", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? 2014", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2014}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML, 739\u2013747.", "citeRegEx": "Hoang et al\\.,? 2014", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "author": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "Proc. ICML, 569\u2013578.", "citeRegEx": "Hoang et al\\.,? 2015", "shortCiteRegEx": "Hoang et al\\.", "year": 2015}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "JMLR 9:235\u2013284.", "citeRegEx": "Krause et al\\.,? 2008", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Collective motion, sensor networks, and ocean sampling", "author": ["N.E. Leonard", "D.A. Palley", "F. Lekien", "R. Sepulchre", "D.M. Fratantoni", "R.E. Davis"], "venue": "Proc. IEEE 95:48\u201374.", "citeRegEx": "Leonard et al\\.,? 2007", "shortCiteRegEx": "Leonard et al\\.", "year": 2007}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "author": ["K.H. Low", "J. Yu", "J. Chen", "P. Jaillet"], "venue": "Proc. AAAI.", "citeRegEx": "Low et al\\.,? 2015", "shortCiteRegEx": "Low et al\\.", "year": 2015}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "Sequential Bayesian optimisation for spatial-temporal monitoring", "author": ["R. Marchant", "F. Ramos", "S. Sanner"], "venue": "Proc. UAI.", "citeRegEx": "Marchant et al\\.,? 2014", "shortCiteRegEx": "Marchant et al\\.", "year": 2014}, {"title": "Gaussian processes for global optimization", "author": ["M.A. Osborne", "R. Garnett", "S.J. Roberts"], "venue": "Proc. 3rd International Conference on Learning and Intelligent Optimization.", "citeRegEx": "Osborne et al\\.,? 2009", "shortCiteRegEx": "Osborne et al\\.", "year": 2009}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proc. ICML, 697\u2013704.", "citeRegEx": "Poupart et al\\.,? 2006", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2006", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Bayesian reinforcement learning in continuous POMDPs with application to robot navigation", "author": ["S. Ross", "B. Chaib-draa", "J. Pineau"], "venue": "Proc. IEEE ICRA, 2845\u20132851.", "citeRegEx": "Ross et al\\.,? 2008", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Maximum entropy sampling", "author": ["M.C. Shewry", "H.P. Wynn"], "venue": "J. Applied Statistics 14(2):165\u2013170.", "citeRegEx": "Shewry and Wynn,? 1987", "shortCiteRegEx": "Shewry and Wynn", "year": 1987}, {"title": "Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic", "author": ["T. Smith", "R. Simmons"], "venue": "Proc. AAAI, 1227\u20131232.", "citeRegEx": "Smith and Simmons,? 2006", "shortCiteRegEx": "Smith and Simmons", "year": 2006}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "Proc. ICML, 1015\u20131022.", "citeRegEx": "Srinivas et al\\.,? 2010", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Geostatistics for Environmental Scientists", "author": ["R. Webster", "M. Oliver"], "venue": "NY: John Wiley & Sons, Inc., 2nd edition.", "citeRegEx": "Webster and Oliver,? 2007", "shortCiteRegEx": "Webster and Oliver", "year": 2007}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. Ozgul"], "venue": "Proc. AAAI, 2585\u20132592.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Near-optimal active learning of multi-output Gaussian processes", "author": ["Y. Zhang", "T.N. Hoang", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. AAAI.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 24, "context": ", in Bayesian reinforcement learning (BRL) (Poupart et al. 2006)) to that of a rich class of Bayesian nonparametric Gaussian process (GP) models characterizing continuous-valued, correlated observations in order to represent the latent structure of more complex, possibly noisy task environments with higher fidelity.", "startOffset": 43, "endOffset": 64}, {"referenceID": 15, "context": ", adaptive sampling in oceanography (Leonard et al. 2007), traffic sensing (Chen et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "2007), traffic sensing (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.", "startOffset": 23, "endOffset": 84}, {"referenceID": 5, "context": "2007), traffic sensing (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.", "startOffset": 23, "endOffset": 84}, {"referenceID": 16, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 23, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 32, "context": ", local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008; Low et al. 2012; Ouyang et al. 2014; Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 12, "context": "2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013; Hoang et al. 2014; Low, Dolan, and Khosla 2009; 2008; 2011), some of which have further investigated the performance advantage of adaptivity by proposing nonmyopic adaptive observation selection policies that depend on past observations.", "startOffset": 113, "endOffset": 199}, {"referenceID": 10, "context": ", probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012; Hern\u00e1ndez-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al.", "startOffset": 107, "endOffset": 180}, {"referenceID": 29, "context": ", probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012; Hern\u00e1ndez-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al. 2010).", "startOffset": 235, "endOffset": 257}, {"referenceID": 25, "context": "Let YS , {Ys}s\u2208S denote a GP, that is, every finite subset of YS has a multivariate Gaussian distribution (Rasmussen and Williams 2006).", "startOffset": 106, "endOffset": 135}, {"referenceID": 24, "context": "Bayes-optimality has been studied in discrete BRL (Poupart et al. 2006) whose assumptions (Section 1) do not hold in GPP.", "startOffset": 50, "endOffset": 71}, {"referenceID": 7, "context": "Continuous BRLs (Dallaire et al. 2009; Ross, Chaib-draa, and Pineau 2008) assume a known parametric form of observation function,", "startOffset": 16, "endOffset": 73}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field.", "startOffset": 105, "endOffset": 127}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret.", "startOffset": 106, "endOffset": 1069}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = \u03b2\u03c3st+1|st , E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + \u03b2\u03c3st+1|st . In particular, when \u03b2 = 0, it can be derived that our GPP policy \u03c0\u2217 maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense.", "startOffset": 106, "endOffset": 1730}, {"referenceID": 27, "context": "5 log(2\u03c0e\u03c3 2 st+1|st) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(zt+1, st+1) is independent of zt+1, the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = 0.5 log(2\u03c0e\u03c3 st+1|st), E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + 0.5 log(2\u03c0e\u03c3 2 st+1|st). Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion \u03bcst+1|dt + \u03b2\u03c3st+1|st (\u03b2 \u2265 0) to approximately optimize the global BO objective of total field measurements \u2211H t=1 zt taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R1(zt+1) = 0, R2 and g\u03c3st+1|st as identity functions with `2(\u03c3st+1|st) = 1, and R3(st+1) = \u03b2\u03c3st+1|st , E[R(Zt+1, st+1)|dt, st+1] = \u03bcst+1|dt + \u03b2\u03c3st+1|st . In particular, when \u03b2 = 0, it can be derived that our GPP policy \u03c0\u2217 maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense. So, unlike greedy UCB, our nonmyopic GPP framework does not have to explicitly consider an additional weighted exploration term (i.e., \u03b2\u03c3st+1|st ) in its reward function because it can jointly and naturally optimize the exploration-exploitation trade-off, as explained earlier. Nevertheless, if a stronger exploration behavior is desired (e.g., in online planning), then \u03b2 has to be fine-tuned. Different from nonmyopic BO algorithm of Marchant, Ramos, and Sanner (2014) using UCB-based rewards, our proposed nonmyopic -optimal GPP policy (Section 3) does not need to impose an extreme assumption of maximum likelihood observations during planning and, more importantly, provides a performance guarantee, including for the extreme", "startOffset": 106, "endOffset": 2224}, {"referenceID": 28, "context": "To represent such uncertainty at each encountered node, upper & lower heuristic bounds (respectively, V \u2217 t (dt) and V t (dt)) are maintained, like in (Smith and Simmons 2006).", "startOffset": 151, "endOffset": 175}, {"referenceID": 30, "context": "This section empirically evaluates the online planning performance and time efficiency of our -GPP policy \u03c0 and its anytime variant under limited sampling budget in an energy harvesting task on a simulated wind speed field and in BO on simulated plankton density (chl-a) field and real-world logpotassium (lg-K) concentration (mg l\u22121) field (Appendix I) of Broom\u2019s Barn farm (Webster and Oliver 2007).", "startOffset": 375, "endOffset": 400}, {"referenceID": 25, "context": "057 of lg-K field are learned using maximum likelihood estimation (Rasmussen and Williams 2006).", "startOffset": 66, "endOffset": 95}, {"referenceID": 29, "context": "In BO, the performances of our -GPP policy \u03c0 and its anytime variant are compared with that of state-of-the-art nonmyopic UCB (Marchant, Ramos, and Sanner 2014) and greedy PI, EI, UCB (Brochu, Cora, and de Freitas 2010; Srinivas et al. 2010).", "startOffset": 184, "endOffset": 241}, {"referenceID": 4, "context": "A robotic rover equipped with a wind turbine is tasked to harvest energy/power from the wind while exploring a polar region (Chen et al. 2014).", "startOffset": 124, "endOffset": 142}, {"referenceID": 30, "context": ", possibly in an over-fertilized area) while exploring the Broom\u2019s Barn farm (Webster and Oliver 2007).", "startOffset": 77, "endOffset": 102}, {"referenceID": 3, "context": "For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 17, "context": "For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 31, "context": "2015), online learning (Xu et al. 2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al.", "startOffset": 23, "endOffset": 39}, {"referenceID": 12, "context": "2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al. 2014).", "startOffset": 123, "endOffset": 142}, {"referenceID": 9, "context": ", possibly in an algal bloom) while exploring a lake (Dolan et al. 2009).", "startOffset": 53, "endOffset": 72}], "year": 2015, "abstractText": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive -optimal GPP ( -GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of -GPP with performance guarantee. We empirically demonstrate the effectiveness of our -GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.", "creator": "TeX"}}}