{"id": "1511.07053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation", "abstract": "we propose a broadly structured prediction architecture for images centered around deep recurrent neural networks. the proposed network, called reseg, is based on the recently ) introduced renet model for object classification. technically we modify and extend only it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel - wise tasks running for images. frequently the reseg layer is composed of four recurrent neural networks that sweep the image horizontally overhead and vertically in both directions, along with a final marginal layer packet that visually expands the prediction value back to the original image size. reseg combines multiple reseg layers with successively several possible input layers as well as a final layer node which expands the prediction back to the original image size, making it suitable locally for a variety of structured prediction tasks. we evaluate reseg on the specific task segment of object segmentation with three widely - used image segmentation weighted datasets, ( namely weizmann horse, fashionista and oxford flower. the results suggest that reseg can significantly challenge past the state of the art in object segmentation, and may have contributed further applications specialising in structured prediction at large.", "histories": [["v1", "Sun, 22 Nov 2015 19:25:27 GMT  (3227kb,D)", "http://arxiv.org/abs/1511.07053v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Mon, 11 Jan 2016 14:41:56 GMT  (3272kb,D)", "http://arxiv.org/abs/1511.07053v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Tue, 24 May 2016 15:55:41 GMT  (2488kb,D)", "http://arxiv.org/abs/1511.07053v3", "In CVPR Deep Vision Workshop, 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["francesco visin", "marco ciccone", "adriana romero", "kyle kastner", "kyunghyun cho", "yoshua bengio", "matteo matteucci", "aaron courville"], "accepted": false, "id": "1511.07053"}, "pdf": {"name": "1511.07053.pdf", "metadata": {"source": "CRF", "title": "RESEG: A RECURRENT NEURAL NETWORK FOR OBJECT SEGMENTATION", "authors": ["Francesco Visin", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio", "Matteo Matteucci", "Kyunghyun Cho"], "emails": ["matteo.matteucci}@polimi.it"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, convolutional neural networks (CNN, Fukushima, 1980; LeCun et al., 1989) have become the de facto standard in many computer vision tasks. Object classification from an image is almost always done with very deep convolutional neural networks (see, e.g., Lin et al., 2014; Simonyan & Zisserman, 2015; Szegedy et al., 2014) by directly training them in a supervised manner. Furthermore, the convolutional neural networks have been found to extract good, generic image representations, when they were trained on a large set of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2014). These representations from the convolutional neural network have been used in a wide variety of computer vision tasks, ranging from image caption generation (see, e.g., Vinyals et al., 2014; Xu et al., 2015), video description generation (see, e.g., Yao et al., 2015), object localization/detection (see, e.g., Sermanet et al., 2014) to object segmentation (Chen et al., 2015).\nOn the other hand, recurrent neural networks (RNN) have become the method of choice for modeling sequential data, especially in the field of natural language processing. RNNs have become one of the most widely used methods for natural language tasks such as language modeling (see, e.g., Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). Also, they have been widely employed by speech communities (see, e.g., Chorowski et al., 2014; Graves & Jaitly, 2014) as well. More recently, recurrent neural networks have begun to be employed in a few computation vision tasks (see, e.g., Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015; Graves & Schmidhuber, 2009).\n? Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, 20133, Italy {francesco.visin, matteo.matteucci}@polimi.it \u25e6Montreal Institute for Learning Algorithms (MILA), University of Montreal, Montreal, QC, H3T 1J4, Canada {kyle.kastner, aaron.courville, yoshua.bengio}@umontreal.ca Courant Institute and Center for Data Science, New York University, New York, NY 10012, United States kyunghyun.cho@nyu.edu \u2022 CIFAR Senior Fellow\nar X\niv :1\n51 1.\n07 05\n3v 1\n[ cs\n.C V\n] 2\n2 N\nov 2\n01 5\nAmong computer vision tasks, object segmentation and object reconstruction have not witnessed a comparatively strong adoption of pure CNNs- or RNNs-based models as the rest of the computer vision field. Graph based methods (see, e.g., Shi & Malik, 2000; Felzenszwalb & Huttenlocher, 2004; Boykov & Funka-Lea, 2006) that represent the pixels of the image as nodes of the graph, greedy approaches (such as, e.g., Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).\nThe architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image.\nA similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2d). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing.\nIn this work we extend the results of Visin et al. (2015) modifying and extending the ReNet model to the more ambitious task of object segmentation. We test the performances of the model in the object segmentation domain on one of the historically most used datasets in this field, the Weizmann Horse dataset (Borenstein, 2004), the Oxford Flowers 17 dataset (Nilsback & Zisserman, 2006) and the more recent Fashionista dataset (Yamaguchi, 2012).\nOur experiments show that the proposed adaptation of the ReSeg for pixel-level object segmentation can challenge the state of the art. The proposed architecture can be easily merged with that proposed in Visin et al. (2015) into a joint network to perform both object classification and object segmentation at the same time, sharing most of the computation. This could be interesting in application domains where object classification and segmentation have to be performed simultaneously, such as, autonomous driving and object retrieval."}, {"heading": "2 MODEL DESCRIPTION", "text": "As depicted in Figure 2, the ReSeg model is composed of multiple Recurrent Neural Networks coupled together to capture the spatial relationship of the input data and an expansion layer followed by a softmax operation. We will define more in details each component in the following Section."}, {"heading": "2.1 RESEG", "text": "We take as an input an image (or the feature map of the previous layer) X of elements {xi,j,c} \u2208 Rw\u00d7h\u00d7c, where w, h and c are respectively the width, height and number of channels (or features). We then sweep over each of its columns Xi,\u2022 a first time with two RNNs, fFV and f B V , that move\ntop-down and bottom-up respectively. Note that the processing of each column is independent and can be done in parallel.\nAt every time step each RNN reads the next non-overlapping patch pi,j \u2208 Rwp\u00d7hp\u00d7c and, based on its previous hidden state, emits a projection vi,j and updates its hidden state zi,j :\nvFi,j = f F V(z F i,j\u22121, pi,j), for j = 1, \u00b7 \u00b7 \u00b7 , J (1)\nvRi,j = f B V (z R i,j+1, pi,j), for j = J, \u00b7 \u00b7 \u00b7 , 1 (2)\nWe stress that the choice to read non-overlapping patches is a modeling choice and not a limitation of the architecture.\nOnce the first two vertical RNNs have processed the whole inputX , we concatenate their projections vFi,j and v R i,j to obtain a composite feature map V = {vi,j}j=1,...,Ji=1,...,I . Note that, with d the number of recurrent units of each RNN, every element vi,j \u2208 R2d can be seen as the activation of a feature detector at the location (i, j) with respect to all the patches in the i-th column of the original input. We denote what we described so far as the first ReSeg sublayer.\nAfter obtaining the concatenated feature map V , we sweep over each of its rows with a pair of new RNNs, fFH and f B H . With a similar but specular procedure as the one described before, we proceed\nreading one element vi,j at each step, to obtain a concatenated feature map H = {hi,j}, once again with hi,j \u2208 R2d. Each element hi,j of this second ReSeg sublayer represents the features of one of the input image patches pi,j with contextual information from the whole image.\nThese two sublayers together form what we call a ReSeg layer. It is trivial to note that it is possible to concatenate many ReSeg layers one after the other and train it with any optimization algorithm that performs gradient descent, as the composite model is a smooth, continuous function.\nNote also that fF* and f F * represent a generic Recurrent Neural Network that can be implemented as a vanilla tanh RNN layer, as a Gated Recurrent Unit (GRU) layer (Cho et al., 2014) or as a Long Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997)."}, {"heading": "2.2 GATED RECURRENT UNITS", "text": "Recurrent units with added memory and gating, such as gated recurrent units (GRU, Cho et al., 2014) and long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components to many successful applications using recurrent neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015). The previous work on ReNet has shown that the ReNet model can perform well with little concern for the specific recurrent unit used, but for this particular set of experiments we have chosen to use the GRU unit.\nThe hidden state of the GRU at time t is computed by\nht = (1\u2212 ut) ht\u22121 + ut h\u0303t, where\nh\u0303t = tanh (Wxt + U(rt ht\u22121) + b)\nand [ut; rt] = \u03c3 (Wgxt + Ught\u22121 + bg) .\nFor an in-depth comparison of the similarities and trade-offs between GRU and LSTM, there are several resources including (Chung et al., 2015)."}, {"heading": "2.3 UPSAMPLING LAYER", "text": "Since each ReSeg layer processes non-overlapping patches, the size of the last composite feature map will be smaller than the size of the original inputX . We therefore need to add one or more layers to expand it back to the size of the image to be able to compute the corresponding segmentation mask. We explored several different architectures to accomplish this, that we will discuss in further detail in this section."}, {"heading": "2.3.1 LINEAR FULLY-CONNECTED UPSAMPLING", "text": "The easiest way to enlarge the last composite ReSeg feature map before feeding it into a Softmax classifier is to first use a linear fully-connected layer to obtain an extended feature map with c \u00b7 e features:\nE = H \u00b7W + b (3) where W \u2208 R2d\u00d7c\u00b7u and b \u2208 Rc\u00b7u, with c the number of classes and u the upsampling factor, computed as u = uw \u00d7 uh, where uw and uh are the extending factors along both axes, computed over each axis a as:\nua = \u220f\nl\u2208 layers\na(l)p (4)\nThe feature map E can then be re-arranged so that each of its elements ei,j \u2208 R1\u00d7c\u00b7u is mapped to a patch on the output feature map oi,j \u2208 Ruw\u00d7uh\u00d7c. A softmax can then be applied on the output feature map O to get the per-pixel class prediction probabilities."}, {"heading": "2.3.2 CONVOLUTIONAL FULLY-CONNECTED UPSAMPLING", "text": "One problem of the linear fully-connected upsampling is that each patch on the pre-softmax feature map O depends only on one element of the last composite ReSeg feature map. To have a wider context over the hidden state of the RNN it is possible to interpose a same convolution before the linear fully-connected upsampling layer so that the upsampling layer performs a transformation hi:i+cw,j:j+ch \u2192 ok:k+uw,l:l+uh where cw and ch are the width and the height of the kernel of the convolution respectively."}, {"heading": "2.3.3 FULL CONVOLUTION", "text": "The typical option to enlarge a feature map is to use a full convolution. We will not get into the details of this method as it is widely adopted and very well documented. We want to stress, though, that the main downside of using a full convolution is that it takes many full convolutional layers and/or very big kernels to get a high upsampling ratio. For most of the models we explored in this work, as we will discuss more in detail in Section 3, this alternative proved to be too expensive in terms of both memory usage and computational time to be considered viable."}, {"heading": "2.3.4 GRADIENT-BASED UPSAMPLING", "text": "A last alternative we explored to enlarge the composite feature map of the ReSeg is to exploit the gradient of a convolution by using it in the forward pass instead of using it in the conventional way for the backward pass. More in details, it is possible to think of the last ReSeg feature map H as of the result of a convolution of a kernel K with the desired pre-sigmoid feature map O:\nH = K \u25e6O, with \u25e6 = convolution operator (5)\nTo invert this function and compute the contribution of each element of the composite feature map Hi,j to each element of the pre-sigmoid feature map Ok,l it is then possible to use the gradient of this fictitious convolution."}, {"heading": "2.4 INTERMEDIATE PREDICTIONS", "text": "In some of the experiments we introduced an intermediate classification pathway after some of the ReSeg sublayers to provide a stronger learning signal to the network (see Fig. 3). This is justified by previous work such as Bengio (2014) and (Simonyan & Zisserman, 2015). It is important to highlight that the prediction coming from this pathway is only used to compute the gradient and is not taken into account to determine the actual output of the network."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 DATASETS", "text": "We evaluated the proposed ReSeg on several widely-used benchmark datasets; in this section we describe each dataset in detail."}, {"heading": "3.1.1 WEIZMANN HORSE", "text": "The Weizmann Horse dataset, introduced in Borenstein (2004), is an image segmentation dataset consisting of 329 variable size images in both RGB and gray scale format, matched with an equal number of groundtruth segmentation images, of the same size as the corresponding image. The groundtruth segmentations contain a foreground/background mask of the focused horse, encoded as a real-value between 0 and 255. To convert this into a boolean mask, we threshold in the center of the range setting all smaller values to 0, and all greater values to 1. This dataset is one of the primary small-scale benchmarks found in existing image segmentation literature."}, {"heading": "3.1.2 FASHIONISTA", "text": "The Fashionista dataset from Yamaguchi (2012) contains 685 RGB images of fashion models wearing a variety of different clothing. Each image and its corresponding mask are 400 pixels in width by 600 pixels in height, with encoded values for 53 clothing items. In this work we focus on foreground/background segmentation, and build the appropriate masks from the more complex maps provided by the dataset by creating a new map which preserves the background class as 0, and sets pixels which belong to all other classes to 1. This appears to be the same procedure undertaken in Yang et al. (2015) to create a foreground/background task for this dataset."}, {"heading": "3.1.3 OXFORD FLOWERS 17", "text": "The Oxford Flowers 17 class dataset from Nilsback & Zisserman (2006) contains 1363 variable size RGB images, with 848 image segmentations maps associated with a subset of the RGB images. There are 8 unique segmentation classes defined over all maps, including flower, sky, and grass. To build a foreground/background mask, we take the original segmentation maps, and set any pixel not belonging to class 38 (flower class) to 0, and setting the flower class pixels to 1. This binary segmentation task for Oxford Flowers 17 is further described in Wu & Kashino (2014). A larger 102 class Oxford Flowers dataset is available from the same authors."}, {"heading": "3.2 PREPROCESSING", "text": ""}, {"heading": "3.2.1 DATA AUGMENTATION", "text": "Adding prior knowledge by augmenting the given data with transformed versions is well known to help generalization (see, e.g., Krizhevsky et al., 2012). In light of this, we decided to employ several methods of data augmentation: flipping, shifting, color flipping, and resizing.\nFor each sample there was a 50% chance to flip the image horizontally. This mirrors the intuition that images which are inverted horizontally generally seem like the same scene visually.\nThe shifting procedure was as follows: we either moved 2 pixels to the left with 25% chance, 2 pixels to the right with 25% chance, or performed no shifting. After this step, we then shifted up with 25% chance, shifted down with 25% chance, or left the image as is was before this step. This augmentation should make the model more robust to slight shifts of the object in the image.\nWhen working with gray-scale images, another augmentation which can be useful is to randomly invert the color of the image changing darker colors into lighter colors and vice-versa, with some change percentage. This is helpful in cases where there are more dark objects than light or vice versa, and it proved to be especially helpful when working with greyscale versions of the Weizmann Horse dataset to improve the segmentation performance of light horses that are less represented in the dataset.\nResizing of images can also be of benefit, as eliminating unnecessary and easily explained variance can help the model focus on harder to model characteristics, which generally leads to better performance on the task at hand, especially in the case of segmentation where object scale between images has little impact on the class category. A common choice for resizing is to resize every image to the mean width and height, calculated over the entire dataset of variable size images.\nIt should be noted that all transformations which involve changes in dimensionality or position must also be applied in some inverse form to the predicted segmentation mask to compare it with the ground truth, and great care must be taken (especially during resizing/shifting) not to introduce unexpected errors. It is also paramount to highlight that it is the prediction that should be resized to the ground truth size and not the opposite, not to misrepresent the segmentation accuracy.\nAll of these possible augmentation procedures were treated as hyperparameters for training, and selected based on the best validation performance per dataset."}, {"heading": "3.3 MODEL ARCHITECTURES", "text": "As we pointed out in the previous sections we explored several different alternatives for the topology of the network. The parameters that describe the core of the proposed ReSeg model are the number of features (dRE) of each sublayer and the size of the patches they read (wp \u00d7 hp), the numbers of ReSeg layer nl employed, the kind of upsampling strategy adopted (see 2.3) and its parameters (e.g. the number of convolutions nc, their kernel size wck \u00d7 hck and stride wcs \u00d7 hcs and their activation ac), if any intermediate prediction pathway has been added to the topology and in which point of the network. We tried many combinations of hyperparameters, but in the reported experiments the number of features dRE was between 50 and 300, the patch size wp \u00d7 hp was always 2 \u00d7 2, the number of layers 2, and the upsampling strategy we chose was either linear fully-connected or gradient-based.\nAnother important hyperparameter is the kind of preprocessing techniques applied on the input data and the training parameters such as the initialization strategy of each component, the batch size b, the amount of weight decay wd, the amount of weight noise wn, the gradient clipping threshold gc, the learning rate lr and the optimization algorithm used to adapt the learning rate. In Sections 3.2 and 3.4 we discuss in detail each parameter and the default values used in the experiments."}, {"heading": "3.4 TRAINING", "text": "The first ingredient to training neural networks is initialization. In this paper, we utilized the fan-in plus fan-out initialization described in Glorot & Bengio (2010) for all feedforward and convolutional initializations. The recurrent weight matrices were initialized to be orthonormal, following the procedure defined in Saxe et al. (2014).\nAn adaptive learning rate algorithm known as Adam (Kingma & Ba, 2014) was a key ingredient to stable learning, though others such as (Zeiler, 2012) were useful during model development. In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013).\nRegularization proved to be another important part of our process, especially on the smaller Weizmann Horse dataset. Weight noise, as described in Graves (2011), with a scale 0.075 was applied to all weight matrices before each forward pass during some experiments. Dropout (Srivastava et al., 2014) on each forward connection with drop probability of 0.2 on the input, and/or with drop probability of 0.5 on the hidden projections was applied during some experiments. Nearly all experiments used L2 regularization (Krogh & Hertz, 1992), also known as weight decay, set to 0.0005 to avoid instability at the end of training. The effect of Batch Normalization in Recurrent Neural Networks has been recently in the focus of attention in Laurent et al. (2015) and since it does not seem to provide a reliable improvement in performances we decided not to adopt it.\nMany experiments were performed using a minibatch size of 1. When computational constraints allowed, we also used larger batch sizes of 5, 10, 20, 35, and 50. Larger batch sizes greatly smoothed learning (as expected), and removed some issues related to spurious occurrences in the datasets such as misaligned or poorly segmented groundtruth masks.\nmPA mJI Model \u2013 88.33% (Yang et al., 2015) 94.65% 79.30% ReSeg \u2013 64.23% (Rother et al., 2004)? 77.74% 0.0% All background baseline 22.26% 22.26% All foreground baseline\n(b) Fashionista\n? as reported by (Yang et al., 2015)"}, {"heading": "4 RESULTS AND ANALYSIS", "text": "In Table 1, we present the results on three datasets, along with previously reported results. There are number of difficulties in directly comparing our performance with the reported scores, some of\nwhich are intrinsic to using neural based models for segmentation, while others are specific to the evaluation datasets."}, {"heading": "4.1 DIFFICULTIES IN NEURAL MODEL EVALUATION", "text": "The general procedure for training neural network based models is to take the dataset and split it into three parts: training, validation, and test. To learn model parameters, an update rule based on iterations over training is used, and the best model is generally chosen by evaluating a secondary stopping criterion on a held out validation set. Once this is complete, the best model is evaluated over the never before seen test set. It is not always clear\nThis general procedure is very different than a leave-one-out or k-fold crossvalidation scheme, as is common in much of the segmentation literature, and \u201dlosing\u201d data to a validation set is especially painful in limited data setups such as the three reported experiments. Nonetheless we report our scores in line with approaches which use other training schemes, as we think the performance metrics are viable regardless of the exact methodology. As hardware capabilities continue to improve, other training schemes such as k-fold crossvalidation should become more viable."}, {"heading": "4.2 DATASET SPECIFIC ISSUES", "text": "In the Weizmann Horse and Oxford Flowers datasets there appear to be a small number of partially corrupted images or masks in the download. This corruption was present over several machines on different networks, so it appears to be a problem in the dataset itself, rather than the transfer to our machines. For larger datasets, this kind of corruption will not make a large difference in the overall performance accuracy, but for small evaluation sets such as Weizmann Horse (128 images) and Oxford Flowers (197 images), a corruption of one or two images could result in large performance shifts. It is not clear whether this corruption is recent, but we contacted several of the cited authors to find out whether the issues we found were also present during their analysis and, if so, to know how the reported results were collected. It is also important to note that this issue has far less impact when adopting a k-fold or leave one out approaches, which are usually not a viable solution for large neural models.\nIn our case the 2 corrupted Oxford Flowers masks were not interpretable, so we were forced to remove those masks and the corresponding images in our procedures. For Weizmann Horse instead, one error is a blank RGB input image (image 318), with a mask corresponding to a valid horse. Interestingly the gray scale version of the same image is present. Furthermore, a nearby RGB image (317) appears to have been replaced with its gray scale version. Not having any way to tell if the models we compete with suffered from the same problems, we opted for a conservative approach and we left these images in the dataset. We will update the paper and the results if we will receive more information about this from the authors we contacted."}, {"heading": "5 CONCLUSION", "text": "The ReSeg model proposed in this work performs competitively on several small to medium sized benchmarks for object segmentation, which is typically not a strong suit of neural models with a large number of parameters. The success of a ReSeg based architecture on this task has shown that recurrent neural networks can be a viable alternative to energy based, parts based, or application specific models, even when these models are combined with the power of a deep pretrained CNN. We hope these results will encourage continued work in applying neural models to segmentation tasks, and plan to tackle larger and more modern datasets for segmentation using similar techniques."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We are especially thankful to Pascal Lamblin, Arnaud Bergeron and Fre\u0301de\u0301ric Bastien for their technical assistance and dedication, in particular in the late hours, to Ce\u0301sar Laurent for the insightful discussions on Batch Normalization and for the moral support and to Vincent Dumoulin for his brilliant suggestions on how to tackle the upsampling problem.\nWe also acknowledge the support of the following organizations for research funding and computing support: NSERC, IBM Watson Group, IBM Research, NVIDIA, Samsung, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations (ICLR 2015),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Frederic", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "Submited to the Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "How auto-encoders could provide credit assignment in deep networks via target", "author": ["Bengio", "Yoshua"], "venue": "propagation. CoRR,", "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio", "Yoshua", "Boulanger-Lewandowski", "Nicolas", "Pascanu", "Razvan"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Kernelized structural svm learning for supervised object segmentation", "author": ["Bertelli", "Luca", "Yu", "Tianli", "Vu", "Diem", "Gokturk", "Burak"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bertelli et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertelli et al\\.", "year": 2011}, {"title": "On the statistical analysis of dirty pictures", "author": ["Besag", "Julian"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Besag and Julian.,? \\Q1986\\E", "shortCiteRegEx": "Besag and Julian.", "year": 1986}, {"title": "Visual Reconstruction", "author": ["Blake", "Andrew", "Zisserman"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Blake et al\\.", "year": 1987}, {"title": "Combining top-down and bottom-up segmentation", "author": ["Borenstein", "Eran"], "venue": "Proceedings IEEE workshop on Perceptual Organization in Computer Vision, CVPR, pp", "citeRegEx": "Borenstein and Eran.,? \\Q2004\\E", "shortCiteRegEx": "Borenstein and Eran.", "year": 2004}, {"title": "Graph cuts and efficient nd image segmentation", "author": ["Boykov", "Yuri", "Funka-Lea", "Gareth"], "venue": "International journal of computer vision,", "citeRegEx": "Boykov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2006}, {"title": "Fast and robust fuzzy c-means clustering algorithms incorporating local information for image segmentation", "author": ["Cai", "Weiling", "Chen", "Songcan", "Zhang", "Daoqiang"], "venue": "Pattern Recognition,", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["Chen", "Liang-Chieh", "Barron", "Jonathan T", "Papandreou", "George", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "arXiv preprint arXiv:1511.03328,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "First results", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Efficient graph-based image segmentation", "author": ["Felzenszwalb", "Pedro F", "Huttenlocher", "Daniel P"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2004}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Geman", "Stuart", "Donald"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Geman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1984}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML\u20192014,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "[cs.LG],", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A simple weight decay can improve generalization", "author": ["Krogh", "Anders", "Hertz", "John A"], "venue": "In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "Krogh et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Krogh et al\\.", "year": 1992}, {"title": "Figure-ground segmentation by transferring window masks", "author": ["Kuettel", "Daniel", "Ferrari", "Vittorio"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kuettel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuettel et al\\.", "year": 2012}, {"title": "URL http://arxiv.org/ abs/1510.01378", "author": ["Laurent", "C\u00e9sar", "Pereyra", "Gabriel", "Brakel", "Philemon", "Zhang", "Ying", "Bengio", "Yoshua"], "venue": "Batch normalized recurrent neural networks. CoRR,", "citeRegEx": "Laurent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning to combine bottom-up and top-down segmentation", "author": ["Levin", "Anat", "Weiss", "Yair"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Network in network", "author": ["Lin", "Min", "Chen", "Qiang", "Yan", "Shuicheng"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Crf learning with cnn features for image segmentation", "author": ["Liu", "Fayao", "Lin", "Guosheng", "Shen", "Chunhua"], "venue": "Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "A visual vocabulary for flower classification", "author": ["Nilsback", "M-E", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Nilsback et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nilsback et al\\.", "year": 2006}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["Rother", "Carsten", "Kolmogorov", "Vladimir", "Blake", "Andrew"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Rother et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rother et al\\.", "year": 2004}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Normalized cuts and image segmentation", "author": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS\u20192014,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Show and tell: a neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv 1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Renet: A recurrent neural network based alternative to convolutional networks", "author": ["Visin", "Francesco", "Kastner", "Kyle", "Cho", "Kyunghyun", "Matteucci", "Matteo", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1505.00393,", "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Tri-map self-validation based on least gibbs energy for foreground segmentation", "author": ["Wu", "Xiaomeng", "Kashino", "Kunio"], "venue": "In Proceedings of the British Machine Vision Conference. BMVA Press,", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy Lei", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Parsing clothing in fashion photographs", "author": ["Yamaguchi", "Kota"], "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yamaguchi and Kota.,? \\Q2012\\E", "shortCiteRegEx": "Yamaguchi and Kota.", "year": 2012}, {"title": "Patchcut: Data-driven object segmentation via local shape transfer", "author": ["Yang", "Jimei", "Price", "Brian", "Cohen", "Scott", "Lin", "Zhe", "Ming-Hsuan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Video description generation incorporating spatio-temporal features and a soft-attention mechanism", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "In recent years, convolutional neural networks (CNN, Fukushima, 1980; LeCun et al., 1989) have become the de facto standard in many computer vision tasks.", "startOffset": 47, "endOffset": 89}, {"referenceID": 42, "context": "Object classification from an image is almost always done with very deep convolutional neural networks (see, e.g., Lin et al., 2014; Simonyan & Zisserman, 2015; Szegedy et al., 2014) by directly training them in a supervised manner.", "startOffset": 103, "endOffset": 182}, {"referenceID": 25, "context": "Furthermore, the convolutional neural networks have been found to extract good, generic image representations, when they were trained on a large set of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2014).", "startOffset": 159, "endOffset": 234}, {"referenceID": 42, "context": "Furthermore, the convolutional neural networks have been found to extract good, generic image representations, when they were trained on a large set of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2014).", "startOffset": 159, "endOffset": 234}, {"referenceID": 46, "context": "These representations from the convolutional neural network have been used in a wide variety of computer vision tasks, ranging from image caption generation (see, e.g., Vinyals et al., 2014; Xu et al., 2015), video description generation (see, e.", "startOffset": 157, "endOffset": 207}, {"referenceID": 11, "context": ", 2014) to object segmentation (Chen et al., 2015).", "startOffset": 31, "endOffset": 50}, {"referenceID": 41, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 12, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 0, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 23, "context": "More recently, recurrent neural networks have begun to be employed in a few computation vision tasks (see, e.g., Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015; Graves & Schmidhuber, 2009).", "startOffset": 101, "endOffset": 206}, {"referenceID": 11, "context": "More recently, recurrent neural networks have begun to be employed in a few computation vision tasks (see, e.g., Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015; Graves & Schmidhuber, 2009).", "startOffset": 101, "endOffset": 206}, {"referenceID": 10, "context": ", Shi & Malik, 2000; Felzenszwalb & Huttenlocher, 2004; Boykov & Funka-Lea, 2006) that represent the pixels of the image as nodes of the graph, greedy approaches (such as, e.g., Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.", "startOffset": 162, "endOffset": 208}, {"referenceID": 44, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 23, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 11, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification.", "startOffset": 15, "endOffset": 373}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN.", "startOffset": 15, "endOffset": 1231}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d).", "startOffset": 15, "endOffset": 1381}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al.", "startOffset": 15, "endOffset": 1897}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach.", "startOffset": 15, "endOffset": 1925}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach.", "startOffset": 15, "endOffset": 2034}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing.", "startOffset": 15, "endOffset": 2313}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing. In this work we extend the results of Visin et al. (2015) modifying and extending the ReNet model to the more ambitious task of object segmentation.", "startOffset": 15, "endOffset": 2539}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing. In this work we extend the results of Visin et al. (2015) modifying and extending the ReNet model to the more ambitious task of object segmentation. We test the performances of the model in the object segmentation domain on one of the historically most used datasets in this field, the Weizmann Horse dataset (Borenstein, 2004), the Oxford Flowers 17 dataset (Nilsback & Zisserman, 2006) and the more recent Fashionista dataset (Yamaguchi, 2012). Our experiments show that the proposed adaptation of the ReSeg for pixel-level object segmentation can challenge the state of the art. The proposed architecture can be easily merged with that proposed in Visin et al. (2015) into a joint network to perform both object classification and object segmentation at the same time, sharing most of the computation.", "startOffset": 15, "endOffset": 3152}, {"referenceID": 12, "context": "Note also that fF * and f F * represent a generic Recurrent Neural Network that can be implemented as a vanilla tanh RNN layer, as a Gated Recurrent Unit (GRU) layer (Cho et al., 2014) or as a Long Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997).", "startOffset": 166, "endOffset": 184}, {"referenceID": 41, "context": ", 2014) and long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components to many successful applications using recurrent neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015).", "startOffset": 170, "endOffset": 239}, {"referenceID": 46, "context": ", 2014) and long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components to many successful applications using recurrent neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015).", "startOffset": 170, "endOffset": 239}, {"referenceID": 14, "context": "For an in-depth comparison of the similarities and trade-offs between GRU and LSTM, there are several resources including (Chung et al., 2015).", "startOffset": 122, "endOffset": 142}, {"referenceID": 48, "context": "This appears to be the same procedure undertaken in Yang et al. (2015) to create a foreground/background task for this dataset.", "startOffset": 52, "endOffset": 71}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013).", "startOffset": 93, "endOffset": 114}, {"referenceID": 40, "context": "Dropout (Srivastava et al., 2014) on each forward connection with drop probability of 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 34, "context": "The recurrent weight matrices were initialized to be orthonormal, following the procedure defined in Saxe et al. (2014). An adaptive learning rate algorithm known as Adam (Kingma & Ba, 2014) was a key ingredient to stable learning, though others such as (Zeiler, 2012) were useful during model development.", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013). Regularization proved to be another important part of our process, especially on the smaller Weizmann Horse dataset. Weight noise, as described in Graves (2011), with a scale 0.", "startOffset": 94, "endOffset": 277}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013). Regularization proved to be another important part of our process, especially on the smaller Weizmann Horse dataset. Weight noise, as described in Graves (2011), with a scale 0.075 was applied to all weight matrices before each forward pass during some experiments. Dropout (Srivastava et al., 2014) on each forward connection with drop probability of 0.2 on the input, and/or with drop probability of 0.5 on the hidden projections was applied during some experiments. Nearly all experiments used L2 regularization (Krogh & Hertz, 1992), also known as weight decay, set to 0.0005 to avoid instability at the end of training. The effect of Batch Normalization in Recurrent Neural Networks has been recently in the focus of attention in Laurent et al. (2015) and since it does not seem to provide a reliable improvement in performances we decided not to adopt it.", "startOffset": 94, "endOffset": 873}, {"referenceID": 48, "context": "03% (Yang et al., 2015) 95.", "startOffset": 4, "endOffset": 23}, {"referenceID": 32, "context": "0% (Liu et al., 2015) 95.", "startOffset": 3, "endOffset": 21}, {"referenceID": 5, "context": "08% (Bertelli et al., 2011) 74.", "startOffset": 4, "endOffset": 27}, {"referenceID": 48, "context": "33% (Yang et al., 2015) 94.", "startOffset": 4, "endOffset": 23}, {"referenceID": 35, "context": "23% (Rother et al., 2004) 77.", "startOffset": 4, "endOffset": 25}, {"referenceID": 35, "context": "3% (Rother et al., 2004) 86.", "startOffset": 3, "endOffset": 24}, {"referenceID": 48, "context": "? as reported by (Yang et al., 2015)", "startOffset": 17, "endOffset": 36}, {"referenceID": 4, "context": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 52, "endOffset": 97}, {"referenceID": 1, "context": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 52, "endOffset": 97}], "year": 2017, "abstractText": "We propose a structured prediction architecture for images centered around deep recurrent neural networks. The proposed network, called ReSeg, is based on the recently introduced ReNet model for object classification. We modify and extend it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images. The ReSeg layer is composed of four recurrent neural networks that sweep the image horizontally and vertically in both directions, along with a final layer that expands the prediction back to the original image size. ReSeg combines multiple ReSeg layers with several possible input layers as well as a final layer which expands the prediction back to the original image size, making it suitable for a variety of structured prediction tasks. We evaluate ReSeg on the specific task of object segmentation with three widely-used image segmentation datasets, namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest that ReSeg can challenge the state of the art in object segmentation, and may have further applications in structured prediction at large.", "creator": "LaTeX with hyperref package"}}}