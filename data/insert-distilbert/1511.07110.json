{"id": "1511.07110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization", "abstract": "recently diversity - inducing regularization methods for latent variable models ( lvms ), which encourage the components in lvms to be diverse, have been studied to address several issues involved in latent variable modeling : ( 1 ) how far to capture long - tail query patterns underlying data ; ( 2 ) how to reduce model complexity without sacrificing expressivity ; ( 3 ) how to improve the interpretability of learned patterns. while the nonlinear effectiveness role of diversity - inducing regularizers such as the mutual angular regularizer has been demonstrated empirically, facilitating a rigorous theoretical analysis of them is still missing. in establishing this paper, we aim to bridge this gap and analyze how the mutual angular regularizer ( mar ) affects the generalization performance of supervised lvms. we use neural network ( nn ) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in nn would reduce estimation error and increase approximation empirical error. in addition to theoretical analysis, we also present empirical study which demonstrates that the mar can greatly improve the parameter performance of nn and the empirical observations are in perfect accordance with the theoretical basis analysis.", "histories": [["v1", "Mon, 23 Nov 2015 04:51:49 GMT  (368kb,D)", "http://arxiv.org/abs/1511.07110v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pengtao xie", "yuntian deng", "eric xing"], "accepted": false, "id": "1511.07110"}, "pdf": {"name": "1511.07110.pdf", "metadata": {"source": "CRF", "title": "On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization", "authors": ["Pengtao Xie", "Yuntian Deng", "Eric Xing"], "emails": [], "sections": [{"heading": null, "text": "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis."}, {"heading": "1 Introduction", "text": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6]. Among the various ML models and algorithms designed for pattern discovery,\nlatent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].\nAlthough LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1]. (2) To cope with the rapidly growing complexity of patterns present in big data, ML practitioners typically increase the size and capacity of LVMs, which incurs great challenges for model training, inference, storage and maintenance [25]. How to reduce model complexity without compromising expressivity is a challenging issue. (3) There exist substantial redundancy and overlapping amongst patterns discovered by existing LVMs from massive data, making them hard to interpret [26].\nTo address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other. First, concerning the long-tail phenomenon in extracting latent patterns (e.g., clusters, topics) from data: if the model components are biased to be far apart from each other, then one would expect that such components will tend to be less overlapping and less aggregated over dominant patterns (as one often experiences in standard clustering algorithms [27]), and therefore more likely to capture the long-tail patterns. Second, reducing\nar X\niv :1\n51 1.\n07 11\n0v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\n01 5\nmodel complexity without sacrificing expressivity: if the model components are preferred to be different from each other, then the patterns captured by different components are likely to have less redundancy and hence complementary to each other. Consequently, it is possible to use a small set of components to sufficiently capture a large proportion of patterns. Third, improving the interpretability of the learned components: if model components are encouraged to be distinct from each other and non-overlapping, then it would be cognitively easy for human to associate each component to an object or concept in the physical world. Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].\nWhile the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear. In this paper, we aim to bridge this gap and make the first attempt to formally understand why and how introducing diversity into LVMs can lead to better modeling effects. We focus on the mutual angular regularizer proposed in [1] and analyze how it affects the generalization performance of supervised latent variable models. Specifically, we choose neural network (NN) as a model instance to carry out the analysis while noting that the analysis could be extended to other LVMs such as Restricted Boltzmann Machine and Distance Metric Learning. The major insights distilled from the analysis are: as the diversity (which will be made precise later) of hidden units in NN increases, the estimation error of NN decreases while the approximation error increases; thereby the overall generalization error (which is the sum of estimation error and generalization error) reaches the minimum if an optimal diversity level is chosen. In addition to the theoretical study, we also conduct experiments to empirically show that with the mutual angular regularization, the performance of neural networks can be greatly improved. And the empirical results are consistent with the theoretical analysis.\nThe major contributions of this paper include:\n\u2022 We propose a diversified neural network with mutual angular regularization (MAR-NN).\n\u2022 We analyze the generalization performance of MAR-NN, and show that the mutual angular regularizer can help reduce generalization error.\n\u2022 Empirically, we show that mutual angular regularizer can greatly improve the performance of\nNNs and the experimental results are in accordance with the theoretical analysis.\nThe rest of the paper is organized as follows. Section 2 introduces mutual angle regularized neural networks (MAR-NNs). The estimation and approximation errors of MAR-NN are analyzed in Section 3. Section 4 presents empirical studies of MAR-NN. Section 5 reviews related works and Section 6 concludes the paper."}, {"heading": "2 Diversify Neural Networks with Mutual Angular Regularizer", "text": "In this section, we review diversity-regularized latent variable models and propose diversified neural networks with mutual angular regularization."}, {"heading": "2.1 Diversity-Promoting Regularization of", "text": "Latent Variable Models\nUncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6]. Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task. The knowledge and structures hidden behind data are usually composed of multiple patterns. For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education. Accordingly, latent variable models are parametrized by multiple components where each component aims to capture one pattern in the knowledge and is represented with a parameter vector. For instance, the components in Latent Dirichlet Allocation [10] are called topics and each topic is parametrized by a multinomial vector.\nTo address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:\nmaxA L(A) + \u03bb\u2126(A) (1)\nwhere each column of A \u2208 Rd\u00d7k is the parameter vector of a component, L(A) is the objective function of the original LVM, \u2126(A) is a regularizer encouraging the components in A to be diverse and \u03bb is a tradeoff parameter. Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].\nHere we present a detailed review of the mutual angular regularizer [1] as our theoretical analysis is based on it. This regularizer is defined with the rationale that if each pair of components are mutually different, then\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nthe set of components are diverse in general. They utilize the non-obtuse angle \u03b8ij = arccos( |ai\u00b7aj | \u2016a\u2016i\u2016a\u2016j ) to measure the dissimilarity between component ai and aj as angle is insensitive to geometry transformations of vectors such as scaling, translation, rotation, etc. Given a set of components, angles {\u03b8ij} between each pair of components are computed and the MAR is defined as the mean of these angles minus their variance\n\u2126(A) = 1K(K\u22121) \u2211K i=1 \u2211K j=1,j 6=i \u03b8ij \u2212 \u03b3\n1 K(K\u22121)\u2211K\ni=1 \u2211K j=1,j 6=i(\u03b8ij \u2212 1 K(K\u22121) \u2211K p=1 \u2211K q=1,q 6=p \u03b8pq) 2\n(2) where \u03b3 > 0 is a tradeoff parameter between mean and variance. The mean term summarizes how these vectors are different from each on the whole. A larger mean indicates these vectors share a larger angle in general, hence are more diverse. The variance term is utilized to encourage the vectors to evenly spread out to different directions. A smaller variance indicates that these vectors are uniformly different from each other."}, {"heading": "2.2 Neural Network with Mutual Angular Regularization", "text": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc. Neural networks are nonlinear models with large capacity and rich expressiveness. If trained properly, they can capture the complex patterns underlying data and achieve notable performance in many machine learning tasks. NNs are composed of multiple layers of computing units and units in adjacent layers are connected with weighted edges. NNs are a typical type of LVMs where each hidden unit is a component aiming to capture the latent features underlying data and is characterized by a vector of weights connecting to units in the lower layer.\nWe instantiate the general framework of diversityregularized LVM to neural network and utilize the mutual angular regularizer to encourage the hidden units (precisely their weight vectors) to be different from each other, which could lead to several benefits: (1) better capturing of long-tail latent features; (2) reducing the size of NN without compromising modeling power. Let L({Ai}l\u22121i=0) be the loss function of a neural network with l layers where Ai are the weights between layer i and layer i+ 1, and each column of Ai corresponds to a unit. A diversified NN with mutual angular regularization (MAR-NN) can be defined as\nmin{Ai}l\u22121i=0 L({Ai}l\u22121i=0)\u2212 \u03bb \u2211l\u22122 i=0 \u2126(Ai) (3)\nwhere \u2126(Ai) is the mutual angular regularizer and \u03bb > 0 is a tradeoff parameter. Note that the regularizer is\nnot applied to Al\u22121 since in the last layer are output units which are not latent components."}, {"heading": "3 Generalization Error Analysis", "text": "In this section, we analyze how the mutual angular regularizer affects the generalization error of neural networks. Let L(f) = E(x,y)\u223cp\u2217 [`(x, y, f)] denote the generalization error of hypothesis f , where p\u2217 is the distribution of input-output pair (x, y) and `(\u00b7) is the loss function. Let f\u2217 \u2208 argminf\u2208FL(f) be the expected risk minimizer. Let L\u0302(f) = 1n \u2211n i=1 `(x (i), y(i), f) be the training error and f\u0302 \u2208 argminf\u2208F L\u0302(f) be the empirical risk minimizer. We are interested in the generalization error L(f\u0302) of the empirical risk min-\nimizer f\u0302 , which can be decomposed into two parts L(f\u0302) = L(f\u0302) \u2212 L(f\u2217) + L(f\u2217), where L(f\u0302) \u2212 L(f\u2217) is the estimation error (or excess risk) and L(f\u2217) is the approximation error. The estimation error represents how well the algorithm is able to learn and usually depends on the complexity of the hypothesis and the number of training samples. A lower hypothesis complexity and a larger amount of training data incur lower estimation error bound. The approximation error indicates how expressive the hypothesis set is to effectively approximate the target function.\nOur analysis below shows that the mutual angular regularizer can reduce the generalization error of neural networks. We assume with high probability \u03c4 , the angle between each pair of hidden units is lower bounded by \u03b8. \u03b8 is a formal characterization of diversity. The larger \u03b8 is, the more diverse these hidden units are. The analysis in the following sections suggests that \u03b8 incurs a tradeoff between estimation error and approximation error: the larger \u03b8 is, the smaller the estimation error bound is and the larger the approximation error bound is. Since the generalization error is the sum of estimation error and approximation error, \u03b8 has an optimal value to yield the minimal generalization error. In addition, we can show that under the same probability \u03c4 , increasing the mutual angular regularizer can increase \u03b8. Given a set of hidden units A learned by the MAR-NN, we assume their pairwise angles {\u03b8ij} are i.i.d samples drawn from a distribution p(X) where the expectation and variance of random variable X is \u00b5 and \u03c3 respectively. Lemma 1 states that \u03b8 is an increasing function of \u00b5 and decreasing function of \u03c3. By the definition of MAR, it encourages larger mean and smaller variance. Thereby, the larger the MAR is, the larger \u03b8 is. Hence properly controlling the MAR can generate a desired \u03b8 that produces the lowest generalization error.\nLemma 1. With probability at least \u03c4 , we have X \u2265 \u03b8 = \u00b5\u2212 \u221a \u03c3\n1\u2212\u03c4\nProof. According to Chebyshev inequality [31],\n\u03c3 t2 \u2265 p(|X \u2212 \u00b5| > t) \u2265 p(X < \u00b5\u2212 t) (4)\nLet \u03b8 = \u00b5\u2212 t, then p(X < \u03b8) \u2264 \u03c3(\u00b5\u2212\u03b8)2 . Hence p(X \u2265 \u03b8) \u2265 1 \u2212 \u03c3(\u00b5\u2212\u03b8)2 . Let \u03c4 = 1 \u2212 \u03c3 (\u00b5\u2212\u03b8)2 , then \u03b8 = \u00b5 \u2212\u221a\n\u03c3 1\u2212\u03c4 ."}, {"heading": "3.1 Setup", "text": "For the ease of presentation, we first consider a simple neural network whose setup is described below. Later on we extend the analysis to more complicated neural networks.\n\u2022 Network structure: one input layer, one hidden layer and one output layer\n\u2022 Activation function: Lipschitz continuous function h(t) with constant L. Example: rectified linear h(t) = max(0, t), L = 1; tanh h(t) = tanh(t), L = 1; sigmoid h(t) = sigmoid(t), L = 0.25.\n\u2022 Task: univariate regression\n\u2022 Let x \u2208 Rd be the input vector with \u2016x\u20162 \u2264 C1\n\u2022 Let y be the response value with |y| \u2264 C2\n\u2022 Let wj \u2208 Rd be the weights connecting to the jth hidden unit, j = 1, \u00b7 \u00b7 \u00b7 ,m, with \u2016wj\u20162 \u2264 C3. Further, we assume with high probability \u03c4 , the\nangle \u03c1(wi,wj) = arccos( |wi\u00b7wj|\n\u2016wi\u20162\u2016wj\u20162 ) between wi and wj is lower bounded by a constant \u03b8 for all i 6= j.\n\u2022 Let \u03b1j be the weight connecting the hidden unit j to the output with \u2016\u03b1\u20162 \u2264 C4 \u2022 Hypothesis set: F = {f |f(x) = m\u2211 j=1 \u03b1jh(wj Tx)}\n\u2022 Loss function set: A = {`|`(x, y) = (f(x)\u2212 y)2}"}, {"heading": "3.2 Estimation Error", "text": "We first analyze the estimation error bound of MARNN and are interested in how the upper bound is related with the diversity (measured by \u03b8) of the hidden units. The major result is presented in Theorem 1.\nTheorem 1. With probability at least (1\u2212 \u03b4)\u03c4\nL(f\u0302)\u2212 L(f\u2217) \u2264 8( \u221a J + C2)(2LC1C3C4 + C4|h(0)|) \u221a m\u221a n\n+( \u221a J + C2)2\n\u221a 2 log(2/\u03b4)\nn\n(5)\nwhere J = mC24h2(0)+L2C21C23C24 ((m\u22121) cos \u03b8+1)+ 2 \u221a mC1C3C 2 4L|h(0)| \u221a (m\u2212 1) cos \u03b8 + 1.\nNote that the right hand side is a decreasing function w.r.t \u03b8. A larger \u03b8 (denoting the hidden units are more diverse) would induce a lower estimation error bound."}, {"heading": "3.2.1 Proof", "text": "A well established result in learning theory is that the estimation error can be upper bounded by the Rademacher complexity. We start from the Rademacher complexity, seek a further upper bound of it and show how the diversity of the hidden units affects this upper bound. The Rademacher complexity Rn(A) of the loss function set A is defined as\nRn(A) = E[sup`\u2208A 1n \u2211n i=1 \u03c3i`(f(x\n(i)), y(i))] (6)\nwhere \u03c3i is uniform over {\u22121, 1} and {(x(i), y(i))}ni=1 are i.i.d samples drawn from p\u2217. The Rademacher complexity can be utilized to upper bound the estimation error, as shown in Lemma 2.\nLemma 2. [32, 33, 34] With probability at least 1\u2212 \u03b4 L(f\u0302)\u2212 L(f\u2217) \u2264 4Rn(A) +B \u221a 2 log(2/\u03b4)\nn (7)\nfor B \u2265 supx,y,f |`(f(x), y)|\nOur analysis starts from this lemma and we seek further upper bound of Rn(A). The analysis needs an upper bound of the Rademacher complexity of the hypothesis set F , which is given in Lemma 3. Lemma 3. Let Rn(F) denote the Rademacher complexity of the hypothesis set F = {f |f(x) = m\u2211 j=1 \u03b1jh(wj Tx)}, then\nRn(F) \u2264 2LC1C3C4 \u221a m\u221a\nn + C4|h(0)|\n\u221a m\u221a\nn (8)\nProof. Rn(F) = E[supf\u2208F 1n \u2211n i=1 \u03c3i \u2211m j=1 \u03b1jh(wj Txi)]\n= E[supf\u2208F 1n \u2211m j=1 \u03b1j \u2211n i=1 \u03c3ih(wj Txi)]\n(9) Let \u03b1 = [\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1m]T and h = [ \u2211n i=1 \u03c3ih(w1 Txi), \u00b7 \u00b7 \u00b7 , \u2211n i=1 \u03c3ih(wm Txi)] T , the inner product \u03b1 \u00b7 h \u2264 \u2016\u03b1\u20161\u2016x\u2016\u221e as \u2016 \u00b7 \u20161 and \u2016 \u00b7 \u2016\u221e are dual norms. Therefore\n\u03b1 \u00b7 h \u2264 \u2016\u03b1\u20161\u2016h\u2016\u221e = ( \u2211m j=1 |\u03b1j |)(maxj=1,\u00b7\u00b7\u00b7 ,m | \u2211n i=1 \u03c3ih(wj\nTxi)|) \u2264 \u221a m\u2016\u03b1\u20162 \u00b7maxj=1,\u00b7\u00b7\u00b7 ,m | \u2211n i=1 \u03c3ih(wj\nTxi)| \u2264 \u221a mC4 \u00b7maxj=1,\u00b7\u00b7\u00b7 ,m | \u2211n i=1 \u03c3ih(wj\nTxi)| (10)\nSo Rn(F) \u2264 \u221a mC4E[supf\u2208F 1n | \u2211n i=1 \u03c3ih(wj\nTxi)|]. Denote R||(F) = E[supf\u2208F | 2n\u03c3if(xi)|], which is another form of Rademacher complexity used in some\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nliterature such as [33]. Let F \u2032 = {f \u2032|f \u2032(x) = h(wTx)} where w,x satisfy the conditions specified in Section 3.1, then Rn(F) \u2264 \u221a mC4 2 R||(F \u2032). Let G = {g|g(x) = wTx} where w,x satisfy the conditions specified in Section 3.1, thenR||(F \u2032) = R||(h\u25e6g). Let h\u2032(\u00b7) = h(\u00b7) \u2212 h(0), then h\u2032(0) = 0 and h\u2032 is also L-Lipschitz. Then\nR||(F \u2032) = R||(h \u25e6 g) = R||(h\u2032 \u25e6 g + h(0)) \u2264 R||(h\u2032 \u25e6 g) + 2|h(0)|\u221an (Theorem 12 in [33]) \u2264 2LR||(g) + 2|h(0)|\u221an (Theorem 12 in [33]) (11)\nNow we bound R||(g):\nR||(g) = E[supg\u2208G | 2n \u2211n i=1 \u03c3iw\nTxi|] \u2264 2nE[supg\u2208G \u2016w\u20162 \u00b7 \u2016 \u2211n i=1 \u03c3ixi\u2016]\n\u2264 2C3n E[\u2016 \u2211n i=1 \u03c3ixi\u20162]\n= 2C3n Ex[E\u03c3[\u2016 \u2211n i=1 \u03c3ixi\u20162]]\n\u2264 2C3n Ex[ \u221a E\u03c3[\u2016 \u2211n i=1 \u03c3ixi\u201622]] (concavity of \u221a \u00b7)\n= 2C3n Ex[ \u221a E\u03c3[ \u2211n i=1 \u03c3 2 i xi\n2]] (\u2200i 6= j \u03c3i \u22a5\u22a5 \u03c3j) = 2C3n Ex[ \u221a\u2211n i=1 xi\n2] \u2264 2C1C3\u221a\nn\n(12) Putting Eq.(11) and Eq.(12) together, we have R||(F \u2032) \u2264 4LC1C3\u221an + 2|h(0)|\u221a n . Plugging into Rn(F) \u2264 \u221a mC4 2 R||(F \u2032) completes the proof.\nIn addition, we need the following bound of |f(x)|.\nLemma 4. With probability at least \u03c4\nsup x,f |f(x)| \u2264\n\u221a J (13)\nwhere J = mC24h2(0)+L2C21C23C24 ((m\u22121) cos \u03b8+1)+ 2 \u221a mC1C3C 2 4L|h(0)| \u221a (m\u2212 1) cos \u03b8 + 1.\nProof. Let \u03b1 = [\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1m]T , W = [w1, \u00b7 \u00b7 \u00b7 ,wm], h = [h(w1 Tx), \u00b7 \u00b7 \u00b7 , h(wmTx)]T , then we have\nf2(x) = ( \u2211m j=1 \u03b1jh(wj Tx))2 = (\u03b1 \u00b7 h)2 \u2264 (\u2016\u03b1\u20162\u2016h\u20162)2 \u2264 C24\u2016h\u201622\n(14)\nNow we want to derive an upper bound for \u2016h\u20162. As h(t) is L-Lipschitz, |h(wjTx)| \u2264 L|wjTx| + |h(0)|.\nTherefore\n\u2016h\u201622 = \u2211m j=1 h 2(wj Tx)\n\u2264 \u2211m j=1(L|wjTx|+ |h(0)|)2\n= \u2211m j=1 h 2(0) + L2(wj Tx)2 + 2L|h(0)||wjTx| = mh2(0) + L2\u2016WTx\u201622 + 2L|h(0)||WTx|1 \u2264 mh2(0) + L2\u2016WTx\u201622 + 2 \u221a mL|h(0)|\u2016WTx\u20162 \u2264 mh2(0) + L2\u2016WT \u20162op\u2016x\u201622 +2 \u221a mL|h(0)|\u2016WT \u2016op\u2016x\u20162 = mh2(0) + L2\u2016W\u20162op\u2016x\u201622 +2 \u221a mL|h(0)|\u2016W\u2016op\u2016x\u20162 \u2264 mh2(0) + L2C21\u2016W\u20162op + 2 \u221a mC1L|h(0)|\u2016W\u2016op\n(15) where \u2016 \u00b7\u2016op denotes the operator norm. We can make use of the lower bound of \u03c1(wj,wk) for j 6= k to get a bound for \u2016W\u2016op:\n\u2016W\u20162op = sup\u2016u\u20162=1 \u2016Wu\u2016 2 2 = sup\u2016u\u20162=1(u TWTWu)\n= sup\u2016u\u20162=2 \u2211m j=1 \u2211m k=1 ujukwj \u00b7wk\n\u2264 sup\u2016u\u20162=2 \u2211m j=1 \u2211m k=1\n|uj ||uk||wj||wk| cos(\u03c1(wj,wk)) \u2264 C23 sup\u2016u\u20162=2 \u2211m j=1 \u2211m k=1,k 6=j\n|uj ||uk| cos \u03b8 + \u2211m j=1 |uj |2 (with probability at least \u03c4)\n(16)\nDefine u\u2032 = [|u1|, \u00b7 \u00b7 \u00b7 , |ump |]T , Q \u2208 Rm p\u00d7mp : Qjk = cos \u03b8 for j 6= k and Qjj = 1, then \u2016u\u2032\u20162 = \u2016u\u2016 and\n\u2016W\u20162op \u2264 C23 sup\u2016u\u20162=2 u \u2032TQu\u2032 \u2264 C23 sup\u2016u\u20162=2 \u03bb1(Q)\u2016u \u2032\u201622 \u2264 C23\u03bb1(Q)\n(17)\nwhere \u03bb1(Q) is the largest eigenvalue of Q. By simple linear algebra we can get \u03bb1(Q) = (m\u2212 1) cos \u03b8+ 1, so\n\u2016W\u20162op \u2264 ((m\u2212 1) cos \u03b8 + 1)C23 (18)\nSubstitute to Eq.(15), we have\n\u2016h\u201622 \u2264 mh2(0) + L2C21C23 ((m\u2212 1) cos \u03b8 + 1)+\n2 \u221a mC1C3L|h(0)| \u221a (m\u2212 1) cos \u03b8 + 1 (19)\nSubstitute to Eq.(14):\nf2(x) \u2264 mC24h2(0) + L2C21C23C24 ((m\u2212 1) cos \u03b8 + 1)+\n2 \u221a mC1C3C 2 4L|h(0)| \u221a (m\u2212 1) cos \u03b8 + 1\n(20) In order to simplify our notations, define\nJ = mC24h2(0) + L2C21C23C24 ((m\u2212 1) cos \u03b8 + 1)+\n2 \u221a mC1C3C 2 4L|h(0)| \u221a (m\u2212 1) cos \u03b8 + 1 (21)\nThen supx,f |f(x)| \u2264 \u221a supx,f f 2(x) = \u221a J . Proof completes.\nGiven these lemmas, we proceed to prove Theorem 1. The Rademacher complexity Rn(A) of A is\nRn(A) = E[supf\u2208F 1n \u2211n i=1 \u03c3i`(f(x), y)]\n(22)\n`(\u00b7, y) is Lipschitz continuous with respect to the first argument, and the constant L is supx,y,f |f(x)\u2212 y| \u2264 2 supx,y,f (|f(x)| + |y|) = 2( \u221a J + C2). Applying the composition property of Rademacher complexity, we have\nRn(A) \u2264 2( \u221a J + C2)Rn(F) (23)\nUsing Lemma 3, we have\nRn(A) \u2264 2( \u221a J +C2)( 2LC1C3C4 \u221a m\u221a\nn + C4|h(0)|\n\u221a m\u221a\nn )\n(24) Note that supx,y,f |`(f(x), y)| \u2264 ( \u221a J+C2)2, and plugging Eq.(24) into Lemma 2 completes the proof."}, {"heading": "3.2.2 Extensions", "text": "In the above analysis, we consider a simple neural network described in Section 3.1. In this section, we present how to extend the analysis to more complicated cases, such as neural networks with multiple hidden layers, other loss functions and multiple outputs.\nMultiple Hidden Layers The analysis can be extended to multiple hidden layers by recursively applying the composition property of Rademacher complexity to the hypothesis set.\nWe define the hypothesis set FP for neural network with P hidden layers in a recursive manner:\nF0 = {f0|f0(x) = w0 \u00b7 x} F1 = F = {f1|f1(x) = \u2211m0 j=1 wj 1h(f0j (x)), f0j \u2208 F0} Fp = {fp|fp(x) = \u2211mp\u22121 j=1 wj ph(fp\u22121j (x)),\nfp\u22121j \u2208 Fp\u22121}(l = 2, \u00b7 \u00b7 \u00b7 , P ) (25)\nwhere we assume there are mp units in hidden layer p and wpj is the connecting weight from the j-th unit in hidden layer p\u2212 1 to p. (we index hidden layers from 0, w0 is the connecting weight from input to hidden layer 0). When P = 1 the above definition recovers the one-hidden-layer case in Section 3.1 if we treat w1 as \u03b1. We make similar assumptions as Section 3.1: h(\u00b7) is L-Lipschitz, \u2016x\u20162 \u2264 C1, \u2016wp\u20162 \u2264 Cp3 . We also assume that the pairwise angles of the connecting weights \u03c1(wpj ,w p k) for j 6= k are lower bounded by \u03b8p with probability at least \u03c4p. Under these assumptions, we have the following result:\nTheorem 2. For a neural network with P hidden layers, with probability at least (1\u2212 \u03b4) \u220fP\u22121 p=0 \u03c4 p\nL(f\u0302)\u2212 L(f\u2217) \u2264 8( \u221a J p + C2)( (2L) PC1C 0 3\u221a\nn\n\u220fP\u22121 p=0 \u221a mpCp3\n+ |h(0)|\u221a n \u2211P\u22121 p=0 (2L) P\u22121\u2212p\u220fP\u22121 j=p \u221a mjCj3) +( \u221a J p + C2)2 \u221a 2 log(2/\u03b4)\nn\n(26) where\nJ 0 = C21 ((m0 \u2212 1) cos \u03b80 + 1) J p = (Cp3 )2((mp \u2212 1) cos \u03b8p + 1)L2J p\u22121 +2(Cp3 ) 2L|h(0)| \u221a mp\u22121((mp \u2212 1) cos \u03b8p + 1) \u221a J p\u22121+ (Cp3 ) 2((mp \u2212 1) cos \u03b8p + 1)mp\u22121h2(0)(p = 1, \u00b7 \u00b7 \u00b7 , P )\n(27)\nWhen P = 1, Eq.(26) reduces to the estimation error bound of neural network with one hidden layer. Note that the right hand side is a decreasing function w.r.t \u03b8p, hence making the hidden units in each hidden layer to be diverse can reduce the estimation error bound of neural networks with multiple hidden layers.\nIn order to prove Theorem 2, we first bound the Rademacher complexity of the hypothesis set FP : Lemma 5. Let Rn(FP ) denote the Rademacher complexity of the hypothesis set FP , then\nRn(FP ) \u2264 (2L)PC1C 0 3\u221a\nn\nP\u22121\u220f p=0 \u221a mpCp3\n+ |h(0)|\u221a\nn P\u22121\u2211 p=0 (2L)P\u22121\u2212p P\u22121\u220f j=p \u221a mjCj3 (28)\nProof. Notice that Rn(FP )) \u2264 12R||(F P ):\nRn(FP ) = E[supf\u2208FP 1n \u2211n i=1 \u03c3if(xi)]\n\u2264 E[supf\u2208FP | 1n \u2211n i=1 \u03c3if(xi)|] = 12R||(F P )\n(29)\nSo we can bound Rn(FP )) by bounding 12R||(F P ). We bound 12R||(F p) recursively: \u2200p = 1, \u00b7 \u00b7 \u00b7 , P , we have\nR||(Fp) = E[supf\u2208Fp | 2n \u2211n i=1 \u03c3if(xi)|] = E[supfj\u2208Fp\u22121 | 2 n \u2211n i=1 \u03c3i \u2211ml\u22121 j=1 wj\nlh(fj(xi))|] \u2264 \u221a mp\u22121Cp3E[supfj\u2208Fp\u22121 | 2 n \u2211n i=1 \u03c3ih(fj(xi))|]\n\u2264 \u221a mp\u22121Cp3 (2LR||(Fp\u22121) + 2|h(0)|\u221a n )\n(30) where the last two steps are similar to the proof of Lemma 3. Applying the inequality in Eq.(30) recursively, and noting from the proof of Lemma 3 that\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nR||(F0) \u2264 2C1C\n0 3\u221a\nn we have\nR||(FP ) \u2264 2(2L)PC1C 0 3\u221a\nn\n\u220fP\u22121 p=0 \u221a mpCp3\n+ 2|h(0)|\u221a n \u2211P\u22121 p=0 (2L) P\u22121\u2212p\u220fP\u22121 j=p \u221a mjCj3\n(31) Plugging into Eq.(29) completes the proof.\nIn addition, we need the following bound. Lemma 6. With probability at least \u220fP\u22121 p=0 \u03c4 p, supx,fP\u2208Fp |fP (x)| \u2264 \u221a J P , where\nJ 0 = C21 ((m0 \u2212 1) cos \u03b80 + 1) J p = (Cp3 )2((mp \u2212 1) cos \u03b8p + 1)L2J p\u22121 +2(Cp3 ) 2L|h(0)| \u221a mp\u22121((mp \u2212 1) cos \u03b8p + 1) \u221a J p\u22121 +(Cp3 ) 2((mp \u2212 1) cos \u03b8p + 1)mp\u22121h2(0)(1, \u00b7 \u00b7 \u00b7 , P )\n(32)\nProof. For a given neural network, we denote the outputs of the p-th hidden layer before applying the activation function as vp:\nv0 = [w01 T x, \u00b7 \u00b7 \u00b7 ,w0m0x]T vp = [ \u2211mp\u22121 j=1 w p j,1h(v\np\u22121 j ), \u00b7 \u00b7 \u00b7 ,\u2211mp\u22121\nj=1 w p j,mph(v p\u22121 j )] T (p = 1, \u00b7 \u00b7 \u00b7 , P ) (33)\nwhere wpj,i is the connecting weight from the j-th unit of the hidden layer p\u2212 1 to the i-th unit of the hidden layer p.\nTo facilitate the derivation of bounds, we also denote\nwpi = [w p 1,i, \u00b7 \u00b7 \u00b7 ,w p mp\u22121,i] T (34)\nand\nhp = [h(vp\u221211 ), \u00b7 \u00b7 \u00b7 , h(v p\u22121 mp\u22121)] T (35)\nwhere vp\u22121i is the i-th element of v p\u22121.\nUsing the above notations, we can write vp as\nvp = [wp1 \u00b7 hp, \u00b7 \u00b7 \u00b7 ,w p mp \u00b7 hp]T (36)\nHence we can bound the L2 norm of v p recursively:\n\u2016vp\u201622 = \u2211mp i=1(w p i \u00b7 hp)2 (37)\nDenote W = [wp1 , \u00b7 \u00b7 \u00b7 ,w p mp ], then\n\u2016vp\u201622 = \u2016WThp\u201622 \u2264 \u2016WT \u20162op\u2016hp\u201622 = \u2016W\u20162op\u2016hp\u201622\n(38)\nwhere \u2016 \u00b7 \u2016op denotes the operator norm.\nWe can make use of the lower bound of \u03c1(wpj ,w p k) for j 6= k to get a bound for \u2016W\u2016op:\n\u2016W\u20162op = sup\u2016u\u20162=1 \u2016Wu\u2016 2 2 = sup\u2016u\u20162=1(u TWTWu)\n= sup\u2016u\u20162=2 \u2211mp j=1 \u2211mp k=1 ujukw p j \u00b7w p k\n\u2264 sup\u2016u\u20162=2 \u2211mp j=1 \u2211mp k=1\n|uj ||uk||wpj ||w p k | cos(\u03c1(w p j ,w p k)) \u2264 (Cp3 )2 sup\u2016u\u20162=2 \u2211mp j=1 \u2211mp k=1,k 6=j\n|uj ||uk| cos \u03b8p + \u2211mp j=1 |uj |2\n(with probability at least \u220fP\u22121 p=0 \u03c4 p)\n(39)\nDefine u\u2032 = [|u1|, \u00b7 \u00b7 \u00b7 , |ump |]T , Q \u2208 Rm p\u00d7mp : Qjk = cos \u03b8p for j 6= k and Qjj = 1, then \u2016u\u2032\u20162 = \u2016u\u2016 and\n\u2016W\u20162op \u2264 (Cp3 )2 sup\u2016u\u20162=2 u \u2032TQu\u2032 \u2264 (Cp3 )2 sup\u2016u\u20162=2 \u03bb1(Q)\u2016u \u2032\u201622 \u2264 (Cp3 )2\u03bb1(Q)\n(40)\nwhere \u03bb1(Q) is the largest eigenvalue of Q. By simple linear algebra we can get \u03bb1(Q) = (m\np \u2212 1) cos \u03b8p + 1, so\n\u2016W\u20162op \u2264 ((mp \u2212 1) cos \u03b8p + 1)(C p 3 ) 2 (41)\nSubstituting Eq.(41) back to Eq.(38), we have\n\u2016vp\u201622 \u2264 (C p 3 ) 2((mp \u2212 1) cos \u03b8p + 1)\u2016hp\u201622 (42)\nThen we make use of the Lipschitz-continuous property of h(t) to further bound \u2016hp\u201622:\n\u2016hp\u201622 = \u2211mp\u22121 j=1 h 2(vp\u22121j )\n\u2264 \u2211mp\u22121 j=1 (|h(0)|+ L|v p\u22121 j |)2\n= \u2211mp\u22121 j=1 h 2(0) + L2(vp\u22121j ) 2 + 2L|h(0)||vp\u22121j | = mp\u22121h2(0) + L2\u2016vp\u22121\u201622 + 2L|h(0)|\u2016vp\u22121\u20161 \u2264 mp\u22121h2(0) + L2\u2016vp\u22121j \u201622 + 2L|h(0)| \u221a mp\u22121\u2016vp\u22121\u20162\n(43) Substituting Eq.(43) to Eq.(42), we have\n\u2016vp\u201622 \u2264 (C p 3 ) 2((mp \u2212 1) cos \u03b8p + 1)L2\u2016vp\u22121j \u2016 2 2\n+ 2(Cp3 ) 2L|h(0)| \u221a mp\u22121((mp \u2212 1) cos \u03b8p + 1)\u2016vp\u22121\u20162\n+ (Cp3 ) 2((mp \u2212 1) cos \u03b8p + 1)mp\u22121h2(0) (44)\nAnd noticing that \u2016v0\u201622 \u2264 ((m0\u22121) cos \u03b80 +1)\u2016x\u201622 \u2264 C21 ((m\n0\u22121) cos \u03b80 +1), we can bound \u2016vp\u2016 recursively now. Denote\nJ 0 = C21 ((m0 \u2212 1) cos \u03b80 + 1) J p = (Cp3 )2((mp \u2212 1) cos \u03b8p + 1)L2J p\u22121 +2(Cp3 ) 2L|h(0)| \u221a mp\u22121((mp \u2212 1) cos \u03b8p + 1) \u221a J p\u22121+ (Cp3 ) 2((mp \u2212 1) cos \u03b8p + 1)mp\u22121h2(0)(p = 1, \u00b7 \u00b7 \u00b7 , P )\n(45)\nthen \u2016vp\u201622 \u2264 J p and J p decreases when \u03b8i(i = 0, \u00b7 \u00b7 \u00b7 , p) increases.\nNow we are ready to bound supx,fP\u2208FP |fP (x)|:\nsupx,fP\u2208FP |fP (x)| = supx,fP\u2208FP |vP | \u2264 \u221a J P\n(46)\nGiven these lemmas, we proceed to prove Theorem 2. The Rademacher complexity Rn(A) of A is\nRn(A) = E[supf\u2208F 1n \u2211n i=1 \u03c3i`(f(xi), y)] (47)\n`(\u00b7, y) is Lipschitz continuous with respect to the first argument, and the constant L is supx,y,f |f(x)\u2212 y| \u2264 2 supx,y,f (|f(x)| + |y|) = 2( \u221a J + C2). Applying the composition property of Rademacher complexity, we have\nRn(A) \u2264 2( \u221a J + C2)Rn(F) (48)\nUsing Lemma 5, we have\nRn(A) \u2264 2( \u221a J + C2)( (2L)PC1C 0 3\u221a\nn\nP\u22121\u220f p=0 \u221a mpCp3\n+ |h(0)|\u221a\nn P\u22121\u2211 p=0 (2L)P\u22121\u2212p P\u22121\u220f j=p \u221a mjCj3) (49)\nNote that supx,y,f |`(f(x), y)| \u2264 ( \u221a J+C2)2, and plugging Eq.(49) into Lemma 2 completes the proof.\nOther Loss Functions Other than regression, a more popular application of neural network is classification. For binary classification, the most widely used loss functions are logistic loss and hinge loss. Estimation error bounds similar to that in Theorem 1 can also be derived for these two loss functions.\nLemma 7. Let the loss function `(f(x), y) = log(1 + exp(\u2212yf(x))) be the logistic loss where y \u2208 {\u22121, 1}, then with probability at least (1\u2212 \u03b4)\u03c4\nL(f\u0302)\u2212 L(f\u2217) \u2264 4\n1+exp(\u2212 \u221a J ) (2LC1C3C4 + C4|h(0)|) \u221a m\u221a n + log(1 + exp( \u221a J)) \u221a 2 log(2/\u03b4)\nn\n(50)\nProof.\n|\u2202l(f(x), y) \u2202f | = exp(\u2212yf(x)) 1 + exp(\u2212yf(x)) = 1\n1 + exp(yf(x)) (51)\nAs | 11+exp(yf(x)) | \u2264 1 1+exp(\u2212 supf,x |f(x)|) = 1 1+exp(\u2212 \u221a J ) , we have proved that the Lipschitz constant L of `(\u00b7, y) can be bounded by 1\n1+exp(\u2212 \u221a J ) .\nAnd the loss function `(f(x), y) can be bounded by\n|`(f(x), y)| \u2264 log(1 + exp( \u221a J)) (52)\nSimilar to the proof of Theorem 1, we can finish the proof by applying the composition property of Rademacher complexity, Lemma 3 and Lemma 2.\nLemma 8. Let `(f(x), y) = max(0, 1 \u2212 yf(x)) be the hinge loss where y \u2208 {\u22121, 1}, then with probability at least (1\u2212 \u03b4)\u03c4\nL(f\u0302)\u2212 L(f\u2217) \u2264 4(2LC1C3C4 + C4|h(0)|) \u221a m\u221a n\n+(1 + \u221a J)\n\u221a 2 log(2/\u03b4)\nn\n(53)\nProof. Given y, `(\u00b7, y) is Lipschitz with constant 1. And the loss function can be bounded by\n|`(f(x), y)| \u2264 1 + \u221a J (54)\nThe proof can be completed using similar proof of Lemma 7.\nMultiple Outputs The analysis can be also extended to neural networks with multiple outputs, provided the loss function factorizes over the dimensions of the output vector. Let y \u2208 RK denote the target output vector, x be the input feature vector and `(f(x),y) be the loss function. If `(f(x),y) factorizes\nover k, i.e., `(f(x),y) = \u2211K k=1 `\n\u2032(f(x)k, yk), then we can perform the analysis for each `\u2032(f(x)k, yk) as that in Section 3.2.1 separately and sums the estimation error bounds up to get the error bound for `(f(x),y). Here we present two examples. For multivariate regression, the loss function `(f(x),y) is a squared loss: `(f(x),y) = \u2016f(x) \u2212 y\u201622, where f(\u00b7) is the prediction function. This squared loss can be factorized as \u2016f(x)\u2212 y\u201622 = \u2211K k=1(f(x)k \u2212 yk)2. We can obtain an estimation error bound for each (f(x)k \u2212 yk)2 according to Theorem 1, then sum these bounds together to get the bound for \u2016f(x)\u2212 y\u201622.\nFor multiclass classification, the commonly used loss function is cross-entropy loss: `(f(x),y) =\n\u2212 \u2211K k=1 yk log ak, where ak = exp(f(x)k)\u2211K j=1 exp(f(x)j) . We can also derive error bounds similar to that in Theorem 1 by using the composition property of Rademacher complexity. First we need to find the Lipschitz constant:\nLemma 9. Let `(x,y, f) be the cross-entropy loss,\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nthen for any f , f \u2032\n|`(f(x), y)\u2212 `(f \u2032(x), y)| \u2264\nK \u2212 1 K \u2212 1 + exp(\u22122 \u221a J ) K\u2211 k=1 |f(x)k \u2212 f \u2032(x)k| (55)\nProof. Note that y is a 1-of-K coding vector where exactly one element is 1 and all others are 0. Without loss of generality, we assume yk\u2032 = 1 and yk = 0 for k 6= k\u2032. Then\n`(f(x),y) = \u2212 log exp(f(x)k \u2032)\u2211K\nj=1 exp(f(x)j) (56)\nHence for k 6= k\u2032 we have\n|\u2202l(f(x),y)\u2202f(x)k | = 11+ \u2211 j 6=k\u2032 exp(f(x)j) \u2264 1 1+(K\u22121) exp(\u22122 \u221a J )\n(57)\nand for k\u2032 we have\n|\u2202l(f(x),y)\u2202f(x)k\u2032 | = \u2211 j 6=k\u2032 exp(f(x)j)\n1+ \u2211 j 6=k\u2032 exp(f(x)j)\n\u2264 K\u22121 K\u22121+exp(\u22122 \u221a J )\n(58)\nAs K\u22121 K\u22121+exp(\u22122 \u221a J ) \u2265 1 1+(K\u22121) exp(\u22122 \u221a J ) , we have proved that for any k, |\u2202l(f(x),y)\u2202f(x)k | \u2264 K\u22121 K\u22121+exp(\u22122 \u221a J ) . Therefore\n\u2016\u2207f(x)`(f(x), y)\u2016\u221e \u2264 K \u2212 1\nK \u2212 1 + exp(\u22122 \u221a J ) (59)\nUsing mean value theorem, for any f , f \u2032, \u2203\u03be such that\n|`(f(x), y)\u2212 `(f \u2032(x), y)| = \u2207\u03be`(\u03be, y) \u00b7 (f(x)\u2212 f \u2032(x)) \u2264 \u2016\u2207f(x)`(f(x), y)\u2016\u221e\u2016f(x)\u2212 f \u2032(x)\u20161 \u2264 K\u22121\nK\u22121+exp(\u22122 \u221a J ) \u2211K k=1 |f(x)k \u2212 f \u2032(x)k| (60)\nWith Lemma 9, we can get the Rademacher complexity of cross entropy loss by performing the Rademacher complexity analysis for each f(x)k as that in Section 3.2.1 separately, and multiplying the sum of them by K\u22121 K\u22121+exp(\u22122 \u221a J ) to get the Rademacher complexity of `(f(x), y). And as the loss function can be bounded by\n|`(f(x), y)| \u2264 log(1 + (K \u2212 1) exp(2 \u221a J )) (61)\nwe can use similar proof techniques as in Theorem 1 to get the estimation error bound."}, {"heading": "3.3 Approximation Error", "text": "Now we proceed to investigate how the diversity of weight vectors affects the approximation error bound. For the ease of analysis, following [35], we assume the target function g belongs to a function class with smoothness expressed in the first moment of its Fourier representation: we define function class \u0393C as the set of functions g satisfying\u222b\n\u2016x\u20162\u2264C1 |w||g\u0303(w)|dw \u2264 C (62)\nwhere g\u0303(w) is the Fourier representation of g(x) and we assume \u2016x\u20162 \u2264 C1 throughout this paper. We use function f in F = {f |f(x) = \u2211m j=1 \u03b1jh(w T j x)} which is the NN function class defined in Section 3.1, to approximate g \u2208 \u0393C . Recall the following conditions of F :\n\u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016wj\u20162 \u2264 C3 (63) \u2016\u03b1\u20162 \u2264 C4 (64) \u2200j 6= k, \u03c1(wj , wk) \u2265 \u03b8(with probability at least \u03c4)\n(65)\nwhere the activation function h(t) is the sigmoid function and we assume \u2016x\u20162 \u2264 C1. The following theorem states the approximation error.\nTheorem 3. Given C > 0, for every function g \u2208 \u0393C with g(0) = 0, for any measure P , if\nC1C3 \u2265 1 (66) C4 \u2265 2 \u221a mC (67) m \u2264 2(b \u03c0 2 \u2212 \u03b8 \u03b8 c+ 1) (68)\nthen with probability at least \u03c4 , there is a function f \u2208 F such that\n\u2016g\u2212f\u2016L \u2264 2C( 1\u221a n + 1 + 2 lnC1C3 C1C3 )+4mCC1C3 sin(\n\u03b8\u2032 2 )\n(69) where \u2016f\u2016L = \u221a\u222b x f2(x)dP (x), \u03b8\u2032 = min(3m\u03b8, \u03c0).\nNote that the approximation error bound in Eq.(69) is an increasing function of \u03b8. Hence increasing the diversity of hidden units would hurt the approximation capability of neural networks."}, {"heading": "3.4 Proof", "text": "Before proving Theorem 3, we need the following lemma:\nLemma 10. For any three nonzero vectors u1, u2, u3, let \u03b812 = arccos(\nu1\u00b7u2 \u2016u1\u20162\u2016u2\u20162 ), \u03b823 = arccos( u2\u00b7u3 \u2016u2\u20162\u2016u3\u20162 ),\n\u03b813 = arccos( u1\u00b7u3 \u2016u1\u20162\u2016u3\u20162 ). We have \u03b813 \u2264 \u03b812 + \u03b823.\nProof. Without loss of generality, assume \u2016u1\u20162 = \u2016u2\u20162 = \u2016u3\u20162 = 1. Decompose u1 as u1 = u1// +u1\u22a5 where u1// = c12u2 for some c12 \u2208 R and u1\u22a5 \u22a5 u2. As u1 \u00b7 u2 = cos \u03b812, we have c12 = cos \u03b812 and \u2016u1\u22a5\u20162 = sin \u03b812.\nSimilarly, decompose u3 as u3 = u3// + u3\u22a5 where u3// = c32u2 for some c32 \u2208 R and u3\u22a5 \u22a5 u2. We have c23 = cos \u03b823 and \u2016u3\u22a5\u20162 = sin \u03b823.\nSo we have\ncos \u03b813 = u1 \u00b7 u3 = (u1// + u1\u22a5) \u00b7 (u3// + u3\u22a5) = u1// \u00b7 u3// + u1\u22a5 \u00b7 u3\u22a5 = cos \u03b812 cos \u03b823 + u1\u22a5 \u00b7 u3\u22a5 \u2265 cos \u03b812 cos \u03b823 \u2212 sin \u03b812 \u223c \u03b823 = cos(\u03b812 + \u03b823)\n(70)\nIf \u03b812 + \u03b823 \u2264 \u03c0, arccos(cos(\u03b812 + \u03b823)) = \u03b812 + \u03b823. As arccos(\u00b7) is monotonously decreasing, we have \u03b813 \u2264 \u03b812 + \u03b823. Otherwise, \u03b813 \u2264 \u03c0 \u2264 \u03b812 + \u03b823.\nIn order to approximate the function class \u0393C , we first remove the constraints \u03c1(wj , wk) \u2265 \u03b8 and obtain an approximation error:\nLemma 11. Let F \u2032 = {f |f(x) = \u2211m j=1 \u03b1jh(w T j x)} be the function class satisfying the following constraints:\n\u2022 |\u03b1j | \u2264 2C\n\u2022 \u2016wj\u20162 \u2264 C3\nThen for every g \u2208 \u0393C with g(0) = 0, \u2203f \u2032 \u2208 F \u2032 such that\n\u2016g(x)\u2212 f \u2032(x)\u2016L \u2264 2C( 1\u221a n + 1 + 2 lnC1C3 C1C3 ) (71)\nProof. Please refer to Theorem 3 in [35] for the proof. Note that the \u03c4 used in their paper is C1C3 here. Furthermore, we omit the bias term b as we can always add a dummy feature 1 to the input x to avoid using the bias term.\nWe also need the following lemma: Lemma 12. For any 0 < \u03b8 < \u03c02 , m \u2264 2(b \u03c0 2\u2212\u03b8 \u03b8 c+ 1), (w\u2032j) m j=1, \u2203(wj)mj=1 such that\n\u2200j 6= k \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u03c1(wj , wk) \u2265 \u03b8 (72) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016wj\u20162 = \u2016w\u2032j\u20162 (73) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, arccos( wj \u00b7 w\u2032j\n\u2016wj\u20162\u2016w\u2032j\u20162 ) \u2264 \u03b8\u2032 (74)\nwhere \u03b8\u2032 = min(3m\u03b8, \u03c0).\nProof. To simplify our notations, let \u03c6(a, b) = arccos( a\u00b7b\u2016a\u20162\u2016b\u20162 ). We begin our proof by considering a 2-dimensional case: Let\nk = b \u03c0 2 \u2212 \u03b8 \u03b8 c (75)\nLet index set I = {\u2212(k+ 1),\u2212k, \u00b7 \u00b7 \u00b7 ,\u22121, 1, 2, \u00b7 \u00b7 \u00b7 , k+ 1}. We define a set of vectors (ei)i\u2208I : ei = (sin \u03b8i, cos \u03b8i), where \u03b8i \u2208 (\u2212\u03c02 , \u03c0 2 ) is defined as follows:\n\u03b8i = sgn(i)( \u03b8\n2 + (|i| \u2212 1)\u03b8) (76)\nFrom the definition we can verify the following conclusions:\n\u2200i 6= j \u2208 I, \u03c1(ei, ej) \u2265 \u03b8 (77)\n\u2212 \u03c0 2 + \u03b8 2 \u2264 \u03b8\u2212(k+1) < \u2212 \u03c0 2 + 3 2 \u03b8 (78) \u03c0\n2 \u2212 3 2 \u03b8 < \u03b8k+1 \u2264 \u03c0 2 \u2212 \u03b8 2 (79)\n(80)\nAnd we can further verify that \u2200i \u2208 I, there exists different i1, \u00b7 \u00b7 \u00b7 , i2k+1 \u2208 I\\i such that \u03c6(ei, eij ) \u2264 j\u03b8.\nFor any e = (sin\u03b2, cos\u03b2) with \u03b2 \u2208 [\u2212\u03c02 , \u03c0 2 ], we can find i \u2208 I such that \u03c6(ei, e) \u2264 32\u03b8:\n\u2022 if \u03b2 \u2265 \u03b8k+1, take i = k + 1, we have \u03c6(ei, e) \u2264 \u03c0 2 \u2212 \u03b8k+1 < 3 2\u03b8.\n\u2022 if \u03b2 \u2264 \u03b8\u2212(k+1), take i = \u2212(k + 1), we also have \u03c6(ei, e) \u2264 32\u03b8\n\u2022 otherwise, take i = sgn(\u03b2)d\u03b2\u2212 \u03b8 2\n\u03b8 e, we also have \u03c6(ei, e) \u2264 \u03b8 < 32\u03b8.\nRecall that for any i, there exists different i1, \u00b7 \u00b7 \u00b7 , i2k+1 \u2208 I\\i such that \u03c6(ei, eij ) \u2264 j\u03b8, and use Lemma 10, we can draw the conclusion that for any e = (sin\u03b2, cos\u03b2) with \u03b2 \u2208 [\u2212\u03c02 , \u03c0 2 ], there exists different i1, \u00b7 \u00b7 \u00b7 , i2k+2 such that \u03c6(ei, eij ) \u2264 32\u03b8+ (j\u2212 1)\u03b8 = (j + 12 )\u03b8. For any (w\u2032j) m j=1, assume w \u2032 j = \u2016w\u2032j\u20162(sin\u03b2j , cos\u03b2j), and we assume \u03b2j \u2208 [\u2212\u03c02 , \u03c0 2 ]. Using the above conclusion, for w\u20321, we can find some r1 such that \u03c6(w \u2032 1, er1) \u2264 3 2\u03b8. For w \u2032 2, we can find different i1, i2 such that \u03c6(w\u20322, ei1) \u2264 32\u03b8 < ( 3 2 + 1)\u03b8 and \u03c6(w \u2032 2, ei2) \u2264 ( 32 + 1)\u03b8. So we can find r2 6= r1 such that \u03c6(w\u20322, er2) \u2264 ( 32 +1)\u03b8. Following this scheme, we can find rj /\u2208 {r1, \u00b7 \u00b7 \u00b7 , rj\u22121} and \u03c6(w\u2032j , erj ) \u2264 (j + 12 )\u03b8 < 3m\u03b8 for j = 1, \u00b7 \u00b7 \u00b7 ,m, as we have assumed that m \u2264 2(k + 1). Let wj = \u2016w\u2032j\u20162erj , then we have constructed (wj)mj=1 such that\n\u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u03c6(w\u2032j , wj) \u2264 3m\u03b8 (81) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016w\u2032j\u20162 = \u2016wj\u20162 (82) \u2200j 6= k, \u03c1(wj , wk) \u2265 \u03b8 (83)\nOn the Generalization Error Bounds of Diversity Regularized Neural Networks\nNote that we have assumed that \u2200j = 1, \u00b7 \u00b7 \u00b7 ,m, \u03b2j \u2208 [\u2212\u03c02 , \u03c0 2 ]. In order to show that the conclusion holds for general w\u2032j , we need to consider the case where \u03b2j \u2208 [\u2212 32\u03c0,\u2212 \u03c0 2 ]. For that case, we can let \u03b2 \u2032 j = \u03b2j +\u03c0, then \u03b2\u2032j \u2208 [\u2212\u03c02 , \u03c0 2 ]. Let w \u2032\u2032 j = \u2016w\u2032j\u20162(sin\u03b2\u2032j , cos\u03b2\u2032j), we can find the erj such that \u03c6(w \u2032\u2032 j , erj ) \u2264 m\u03b8 following the same procedure. Let wj = \u2212\u2016w\u2032j\u20162erj , then \u03c6(w\u2032j , wj) = \u03c6(w \u2032\u2032 j , erj ) \u2264 2m\u03b8 and as \u03c1(\u2212erj , ek) = \u03c1(erj , ek), the \u03c1(wj , wk) \u2265 \u03b8 condition is still satisfied. Also note that \u03c6(a, b) \u2264 \u03c0, the proof for 2-dimensional case is completed.\nNow we consider a general d-dimensional case. Similar to the 2-dimensional one, we construct a set of vectors with unit l2 norm such that the pairwise angles \u03c1(wj , wk) \u2265 \u03b8 for j 6= k. We do the construction in two phases:\nIn the first phase, we construct a sequence of unit vector sets indexed by I = {\u2212(k + 1), \u00b7 \u00b7 \u00b7 ,\u22121, 1, \u00b7 \u00b7 \u00b7 , k + 1}:\n\u2200i \u2208 I, Ei = {e \u2208 Rd|\u2016e\u20162 = 1, e \u00b7 (1, 0, \u00b7 \u00b7 \u00b7 , 0) = cos \u03b8i} (84) where \u03b8i = sgn(i)( \u03b8 2 + (|i| \u2212 1)\u03b8) is defined the same as we did in Eq.(76). It can be shown that \u2200i 6= j, \u2200ei \u2208 Ei, ej \u2208 Ej ,\n\u03c1(ei, ej) \u2265 \u03b8 (85)\nThe proof is as follows. First, we write ei as ei = (cos \u03b8i, 0, \u00b7 \u00b7 \u00b7 , 0)+ ri, where \u2016ri\u20162 = | sin \u03b8i|. Similarly, ej = (cos \u03b8j , 0, \u00b7 \u00b7 \u00b7 , 0) + rj , where \u2016rj\u20162 = | sin \u03b8j |. Hence we have\nei \u00b7 ej = cos \u03b8i cos \u03b8j + ri \u00b7 rj (86)\nHence\ncos(\u03c1(ei, ej)) = |ei \u00b7 ej | \u2264 cos \u03b8i cos \u03b8j + | sin \u03b8i sin \u03b8j | = max(cos(\u03b8i + \u03b8j), cos(\u03b8i \u2212 \u03b8j))\n(87)\nWe have shown in the 2-dimensional case that cos(\u03b8i+ \u03b8j) \u2265 cos \u03b8 and cos(\u03b8i \u2212 \u03b8j) \u2265 cos \u03b8, hence \u03c1(ei, ej) \u2265 \u03b8. In other words, we have proved that for any two vectors from Ei and Ej , their pairwise angle is lower bounded by \u03b8. Now we proceed to construct a set of vectors for each Ei such that the pairwise angles can also be lower bounded by \u03b8. The construction is as follows.\nFirst, we claim that for any Ei, if W \u2282 E satisfies\n\u2200wj 6= wk \u2208 W, \u03c6(wj , wk) \u2265 \u03b8 (88)\nthen |W | is finite. In order to prove that, we first define B(x, r) = {y \u2208 Rn : \u2016y \u2212 x\u20162 < r}. Then\nEi \u2282 \u222ae\u2208EiB(e, 1\u2212cos \u03b82 1+cos \u03b82\n). From the definition of Ei, it is a compact set, so the open cover has a finite subcover. Therefore we have \u2203V \u2282 Ei with |V | being finite and\nEi \u2282 \u222av\u2208VB(v, 1\u2212 cos \u03b82 1 + cos \u03b82 ) (89)\nFurthermore, we can verify that \u2200v \u2208 V,\u2200e1, e2 \u2208 B(v,\n1\u2212cos \u03b82 1+cos \u03b82\n), \u03c6(e1, e2) \u2264 \u03b8. So if W \u2282 Ei satisfies \u2200wj 6= wk \u2208 W, \u03c6(wj , wk) \u2265 \u03b8, then for each v, |B(v, 1\u2212cos \u03b8 2\n1+cos \u03b82 ) \u2229W| = 1. As W \u2282 Ei, we have\n|W | = |W \u2229 Ei| = |W \u2229 (\u222av\u2208VB(v,\n1\u2212cos \u03b82 1+cos \u03b82 ))|\n= | \u222av\u2208V W \u2229B(v, 1\u2212cos \u03b82 1+cos \u03b82 )| \u2264 \u2211 v\u2208V |W \u2229B(v,\n1\u2212cos \u03b82 1+cos \u03b82\n)| \u2264 \u2211 v\u2208V 1 = |V |\n(90)\nTherefore, we have proved that |W | is finite. Using that conclusion, we can construct a sequence of vectors wj \u2208 Ei(j = 1, \u00b7 \u00b7 \u00b7 , l) in the following way:\n1. Let w1 \u2208 Ei be any vector in Ei.\n2. For j = 2, \u00b7 \u00b7 \u00b7 , let wj \u2208 Ei be any vector satisfying\n\u2200k = 1, \u00b7 \u00b7 \u00b7 , j \u2212 1, \u03c6(wj , wk) \u2265 \u03b8 (91) \u2203k \u2208 {, \u00b7 \u00b7 \u00b7 , j \u2212 1}, \u03c6(wj , wk) = \u03b8 (92)\nuntil we cannot find such vectors any more.\n3. As we have proved that |W | is finite, the above process will end in finite steps. Assume that the last vector we found is indexed by l.\nWe can verify that such constructed vectors satisfy\n\u2200j 6= k \u2208 {1, \u00b7 \u00b7 \u00b7 , l}, \u03c1(wj , wk) \u2265 \u03b8 (93)\nNote that due to the construction, \u03c6(wj , wk) \u2265 \u03b8, as \u03c1(wj , wk) = min(\u03c6(wj , wk), \u03c0 \u2212 \u03c6(wj , wk)), we only need to show that \u03c0 \u2212 \u03c6(wj , wk) \u2265 \u03b8. To show that, we use the definition of Ei to write wj as wj = (cos \u03b8i, 0, \u00b7 \u00b7 \u00b7 , 0)+rj , where \u2016rj\u20162 = | sin \u03b8i|. Similarly, wk = (cos \u03b8i, 0, \u00b7 \u00b7 \u00b7 , 0) + rk, where \u2016rk\u20162 = | sin \u03b8i|. Therefore cos(\u03c6(wj , wk)) = wj \u00b7wk \u2265 cos2 \u03b8i\u2212sin2 \u03b8i = cos(2\u03b8i) \u2265 cos(\u03c0\u2212\u03b8), where the last inequality follows from the construction of \u03b8i. So \u03c0\u2212 \u03c6(wj , wk) \u2265 \u03b8, the proof for \u03c1(wj , wk) \u2265 \u03b8 is completed.\nNow we will show that \u2200e \u2208 Ei, we can find j \u2208 {1, \u00b7 \u00b7 \u00b7 , l} such that \u03c6(e, wj) \u2264 \u03b8. We prove it by contradiction: assume that there exists e such that\nminj\u2208{1,\u00b7\u00b7\u00b7 ,l} \u03c6(e, wj) > \u03b8, then as Ej is a connected set, there is a path q : t \u2208 [0, 1] \u2192 Ej connecting e to w1, and when t = 0, the path starts at q(0) = e; when t = 1, the path ends at q(1) = w1. We define functions rj(t) = \u03c6(q(t), wj) for t \u2208 [0, 1] and j = 1, \u00b7 \u00b7 \u00b7 , l. It is straightforward to see that rj(t) is continuous, hence minj(rj(t)) is also continuous. As minj(rj(0)) > \u03b8 and minj(rj(0)) = 0 < \u03b8, there exists t\n\u2217 \u2208 (0, 1) such that minj(rj(0)) = \u03b8. Then q(t\n\u2217) satisfies Condition 91, which contradicts the construction in W as the construction only ends when we cannot find such vectors. Hence we have proved that\n\u2200e \u2208 Ei,\u2203j \u2208 {1, \u00b7 \u00b7 \u00b7 , l}, \u03c6(e, wj) \u2264 \u03b8 (94)\nNow we can proceed to prove the main lemma. For each i \u2208 I, we use Condition 91 to construct a sequence of vectors wij . Then such constructed vectors wij have pairwise angles greater than or equal to \u03b8. Then for any e \u2208 Rd with \u2016e\u20162 = 1, we write e in sphere coordinates as e = (cos r1, sin r1 cos r2, \u00b7 \u00b7 \u00b7 , \u220fd j=1 sin rj). Use the same method as we did for the 2-dimensional case, we can find \u03b8i such that |\u03b8i \u2212 r| \u2264 32\u03b8. Then e \u2032 =\n(cos \u03b8i, sin \u03b8i cos r2, \u00b7 \u00b7 \u00b7 , sin \u03b8i \u220fd j=2 sin rj) \u2208 Ei. It is easy to verify that \u03c6(e, e\u2032) = |\u03b8i \u2212 r| \u2264 32\u03b8. As e\u2032 \u2208 Ei, there exists wij as we constructed such that \u03c6(e\u2032, wij) \u2264 \u03b8. So \u03c6(e, wij) \u2264 \u03c6(e, e\u2032) + \u03c6(e\u2032, wij) \u2264 5 2\u03b8 < 3\u03b8. So we have proved that for any e \u2208 R\nd with \u2016e\u20162 = 1, we can find wij such that \u03c6(e, wij) < 3\u03b8.\nFor any wij , assume i + 1 \u2208 I, we first project wij onto w\u2217 \u2208 Ei+1 with \u03c6(wij , w\u2217) \u2264 32\u03b8, then we find wi+1,j\u2032 \u2208 Ei+1 such that \u03c6(wi+1,j\u2032 , w\u2217) \u2264 \u03b8. So we have found wi+1,j\u2032 such that \u03c6(wij , wi+1,j\u2032) \u2264 52\u03b8 < 3\u03b8. We can use similar scheme to prove that \u2200wij , there exists different wi1,j1 \u00b7 \u00b7 \u00b7 , wi2k+1,j2k+1 such that (ir, jr) 6= (i, j) and \u03c6(wij , wir,jr ) \u2264 3r\u03b8. Following the same proof as the 2-dimensional case, we can prove that if m \u2264 2k + 1, then we can find a set of vectors (wj) m j=1 such that\n\u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u03c6(w\u2032j , wj) \u2264 min(3m\u03b8, \u03c0) (95) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016w\u2032j\u20162 = \u2016wj\u20162 (96) \u2200j 6= k, \u03c1(wj , wk) \u2265 \u03b8 (97)\nThe proof completes.\nLemma 13. For any f \u2032 \u2208 F \u2032, \u2203f \u2208 F \u2032\u2032 such that\n\u2016f \u2032 \u2212 f\u2016L \u2264 4mCC1C3 sin( \u03b8\u2032\n2 ) (98)\nwhere \u03b8\u2032 = min(3m\u03b8, \u03c0).\nProof. According to the definition of F \u2032, \u2200f \u2032 \u2208 F \u2032, there exists (\u03b1\u2032j) m j=1, (w \u2032 j) m j=1 such that\nf \u2032 = m\u2211 j=1 \u03b1\u2032jh(w \u2032T j x) (99) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, |\u03b1\u2032j | \u2264 2C (100) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016w\u2032j\u20162 \u2264 C4 (101)\nAccording to Lemma 12, there exists (wj) m j=1 such that\n\u2200j 6= k \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u03c1(wj , wk) \u2265 \u03b8 (102) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2016wj\u20162 = \u2016w\u2032j\u20162 (103) \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, arccos( wj \u00b7 w\u2032j\n\u2016wj\u20162\u2016w\u2032j\u20162 ) \u2264 \u03b8\u2032 (104)\nwhere \u03b8\u2032 = min(m\u03b8, \u03c02 ). Let f = \u2211m j=1 \u03b1jh(w \u2032T j x),\nthen \u2016\u03b1\u20162 \u2264 \u221a \u2016\u03b1\u20161\u2016\u03b1\u2016\u221e \u2264 2 \u221a mC \u2264 C4. Hence f \u2208 F . Then all we need to do is to bound \u2016f \u2212 f \u2032\u2016L:\n\u2016f \u2212 f \u2032\u20162L = \u222b \u2016x\u20162\u2264C1(f(x)\u2212 f \u2032(x))2dP (x)\n= \u222b \u2016x\u20162\u2264C1( \u2211 j \u03b1jh(w T j x)\u2212 \u2211 j \u03b1jh(w \u2032T j x)) 2dP (x)\n= \u222b \u2016x\u20162\u2264C1( \u2211 j \u03b1j(h(w T j x)\u2212 h(w\u2032Tj x)))2dP (x)\n\u2264 \u222b \u2016x\u20162\u2264C1( \u2211 j |\u03b1j ||wTj x\u2212 w\u2032Tj x|)2dP (x)\n\u2264 C21 \u222b \u2016x\u20162\u2264C1( \u2211 j |\u03b1j |\u2016wj \u2212 w\u2032j\u20162)2dP (x)\n(105)\nAs arccos( wj \u00b7w\u2032j\n\u2016wj\u20162\u2016w\u2032j\u20162 ) \u2264 \u03b8\u2032, we have wj \u00b7 w\u2032j \u2265\n\u2016wj\u201622 cos \u03b8\u2032. Hence\n\u2016wj \u2212 w\u2032j\u201622 = 2\u2016wj\u201622 \u2212 2wj \u00b7 w\u2032j \u2264 2\u2016wj\u201622 \u2212 2\u2016wj\u201622 cos \u03b8\u2032 \u2264 4C23 sin2( \u03b8 \u2032 2 )\n(106)\nSubstituting back to Eq.(105), we have\n\u2016f \u2212 f \u2032\u20162L \u2264 C21 \u222b \u2016x\u20162\u2264C1( \u2211 j |\u03b1j |2C3 sin( \u03b8\u2032 2 )) 2dP (x) \u2264 16m2C2C21C23 sin2( \u03b8 \u2032\n2 ) (107)\nWith this lemma, we can proceed to prove Theorem 3. For every g \u2208 \u0393C with g(0) = 0, according to Lemma 11, \u2203f \u2032 \u2208 F \u2032 such that\n\u2016g \u2212 f \u2032\u2016L \u2264 2C( 1\u221a n + 1 + 2 lnC1C4 C1C4 ) (108)\nAccording to Lemma 13, we can find f \u2208 F such that\n\u2016f \u2212 f \u2032\u2016L \u2264 4mCC1C3 sin( \u03b8\u2032\n2 ) (109)\nThe proof is completed by noting\n\u2016g \u2212 f\u2016L \u2264 \u2016g \u2212 f \u2032\u2016L + \u2016f \u2032 \u2212 f\u2016L (110)"}, {"heading": "4 Experiments", "text": "In this section, we present the experimental results on MAR-NN. Specifically, we are interested in how the performance of neural networks varies as the tradeoff parameter \u03bb in MAR-NN increases. A larger \u03bb would induce a stronger regularization, which generates a larger angle lower bound \u03b8. We apply MAR-NN for phoneme classification [36] on the TIMIT1 speech dataset. The inputs are MFCC features extracted with context windows and the outputs are class labels generated by the HMM-GMM model through forced alignment [36]. The feature dimension is 360 and the number of classes is 2001. There are 1.1 million data instances in total. We use 70% data for training and 30% for testing. The activation function is sigmoid and loss function is cross-entropy. The networks are trained with stochastic gradient descent and the minibatch size is 100.\nFigure 1 shows the testing accuracy versus the tradeoff parameter \u03bb achieved by four neural networks with one hidden layer. The number of hidden units varies in {50, 100, 200, 300}. As can be seen from these figures, under various network architectures, the best accuracy is achieved under a properly chosen \u03bb. For example, for the neural network with 100 hidden units, the best accuracy is achieved when \u03bb = 0.01. These empirical observations are aligned with our theoretical analysis that the best generalization performance is achieved under a proper diversity level. Adding this regularizer greatly improves the performance of neural networks, compared with unregularized NNs. For example, in a NN with 200 hidden units, the mutual angular regularizer improves the accuracy from \u223c0.415 (without regularization) to 0.45."}, {"heading": "5 Related Works", "text": ""}, {"heading": "5.1 Diversity-Promoting Regularization", "text": "Diversity-promoting regularization approaches, which encourage the parameter vectors in machine learning\n1https://catalog.ldc.upenn.edu/LDC93S1\nmodels to be different from each other, have been widely studied and found many applications. Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity. Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity. In a multi-class classification problem, [44] proposed to use the determinant of the covariance matrix to encourage classifiers to be different from each other. Our work focuses on the theoretical analysis of diversity regularized latent variable models, using neural network as an instance to study how the mutual angular regularizer affects the generalization error."}, {"heading": "5.2 Regularization of Neural Networks", "text": "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data. Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48]. In this paper, we study a new type of regularization approach of NN: diversity-promoting regularization, which bears new properties and functionalities complementary to the existing regularizers."}, {"heading": "5.3 Generalization Performance of Neural Networks", "text": "The generalization performance of neural networks, in particular the approximation error and estimation error, has been widely studied in the past several decades. For the approximation error, [49] demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function. [50] showed that neural networks with a single hidden layer, sufficiently many hidden units and arbitrary bounded and nonconstant activation function are universal approximators. [51] proved that multi-\nlayer feedforward networks with a non-polynomial activation function can approximate any function. Various error rates have also been derived based on different assumptions of the target function. [52] showed that if the target function is in the hypothesis set formed by neural networks with one hidden layer of m units, then the approximation error rate is O(1/ \u221a m). [35] showed that neural networks with one layer of m hidden units and sigmoid activation function can achieve approximation error of order O(1/ \u221a m), where the target function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. [53] proved that if the target function is of the form f(x) = \u222b Q c(w, b)h(wTx + b)d\u00b5, where c(\u00b7, \u00b7) \u2208 L\u221e(Q,\u00b5), then neural networks with one layer of m hidden units can approximate it with an error rate of n\u22121/2\u22121/(2d) \u221a log n, where d is the dimension of input x. As for the estimation error, please refer to [32] for an extensive review, which introduces various estimation error bounds based on VC-dimension, flat-shattering dimension, pseudo dimension and so on."}, {"heading": "6 Conclusions", "text": "In this paper, we provide theoretical analysis regarding why the diversity-promoting regularizers can lead to better latent variable modeling. Using neural network as an instance, we analyze how the generalization performance of supervised latent variable models is affected by the mutual angular regularizer. Our analysis shows that increasing the diversity of hidden units leads to the decrease of estimation error bound and increase of approximation error bound. Overall, if the diversity level is set appropriately, a low generalization error can be achieved. The empirical experiments demonstrate that with mutual angular regularization, the performance of neural networks can be greatly improved and the empirical observations are consistent with the theoretical implications."}], "references": [{"title": "Diversifying restricted boltzmann machine for document modeling", "author": ["Pengtao Xie", "Yuntian Deng", "Eric P. Xing"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Data mining: concepts and techniques: concepts and techniques", "author": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Introduction to statistical pattern recognition", "author": ["Keinosuke Fukunaga"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Machine learning: Trends, perspectives, and prospects", "author": ["MI Jordan", "TM Mitchell"], "venue": "Science, 349(6245):255\u2013260,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Latent variable models. In Learning in graphical models, pages 371\u2013403", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Latent variable models and factor analysis", "author": ["Martin Knott", "David J Bartholomew"], "venue": "Number 7. Edward Arnold,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Build, compute, critique, repeat: Data analysis with latent variable models", "author": ["David M Blei"], "venue": "Annual Review of Statistics and Its Application,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1985}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A Olshausen", "David J Field"], "venue": "Vision research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P Xing", "Michael I Jordan", "Stuart Russell", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Bayesian haplotype inference via the dirichlet process", "author": ["Eric P Xing", "Michael I Jordan", "Roded Sharan"], "venue": "Journal of Computational Biology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Keller: estimating time-varying interactions between genes", "author": ["Le Song", "Mladen Kolar", "Eric P Xing"], "venue": "Bioinformatics, 25(12):i128\u2013i136,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Tied boltzmann machines for cold start recommendations", "author": ["Asela Gunawardana", "Christopher Meek"], "venue": "In Proceedings of the 2008 ACM conference on Recommender systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "IEEE Computer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Peacock: Learning longtail topic features for industrial applications", "author": ["Yi Wang", "Xuemin Zhao", "Zhenlong Sun", "Hao Yan", "Lifeng Wang", "Zhihui Jin", "Liubin Wang", "Yang Gao", "Ching Law", "Jia Zeng"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning compact and effective distance metrics with diversity regularization", "author": ["Pengtao Xie"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Priors for diversity in generative latent variable models", "author": ["James Y. Zou", "Ryan P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "All of statistics: a concise course in statistical inference", "author": ["Larry Wasserman"], "venue": "Springer Science & Business Media,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Neural network learning: Theoretical foundations", "author": ["Martin Anthony", "Peter L Bartlett"], "venue": "cambridge university press,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Lecture notes of statistical learning theory", "author": ["Percy Liang"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R Barron"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1993}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Abdel-rahman Mohamed", "Tara N Sainath", "George Dahl", "Bhuvana Ramabhadran", "Geoffrey E Hinton", "Michael Picheny"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": "Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["Ludmila I Kuncheva", "Christopher J Whitaker"], "venue": "Machine learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["Gavin Brown", "Jeremy Wyatt", "Rachel Harris", "Xin Yao"], "venue": "Information Fusion,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Ensemble diversity measures and their application to thinning", "author": ["Robert E Banfield", "Lawrence O Hall", "Kevin W Bowyer", "W Philip Kegelmeyer"], "venue": "Information Fusion,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E Ke Tang", "Ponnuthurai N Suganthan", "Xin Yao"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Focused ensemble selection: A diversitybased method for greedy ensemble selection", "author": ["Ioannis Partalas", "Grigorios Tsoumakas", "Ioannis P Vlahavas"], "venue": "In European Conference on Artificial Intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Diversity regularized machine", "author": ["Yang Yu", "Yu-Feng Li", "Zhi-Hua Zhou"], "venue": "In IJCAI Proceedings- International Joint Conference on Artificial Intelligence. Citeseer,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Ratio semi-definite classifiers", "author": ["Jonathan Malkin", "Jeff Bilmes"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Generalization performance of regularized neural network models", "author": ["Jan Larsen", "Lars Kai Hansen"], "venue": "In Neural Networks for Signal Processing", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1994}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1412.8690,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1991}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1993}, {"title": "A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training", "author": ["Lee K Jones"], "venue": "The annals of Statistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1992}, {"title": "Uniform approximation by neural networks", "author": ["Y Makovoz"], "venue": "Journal of Approximation Theory,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 2, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 3, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 4, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 184, "endOffset": 190}, {"referenceID": 5, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 6, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 7, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 8, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 9, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 10, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 11, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 12, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 13, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 14, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 15, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 16, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 13, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 333, "endOffset": 341}, {"referenceID": 8, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 333, "endOffset": 341}, {"referenceID": 14, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 359, "endOffset": 367}, {"referenceID": 15, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 359, "endOffset": 367}, {"referenceID": 5, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 388, "endOffset": 395}, {"referenceID": 17, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 388, "endOffset": 395}, {"referenceID": 18, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 419, "endOffset": 427}, {"referenceID": 19, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 419, "endOffset": 427}, {"referenceID": 20, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 452, "endOffset": 460}, {"referenceID": 21, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 452, "endOffset": 460}, {"referenceID": 22, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 357, "endOffset": 364}, {"referenceID": 0, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 357, "endOffset": 364}, {"referenceID": 22, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 474, "endOffset": 481}, {"referenceID": 0, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 474, "endOffset": 481}, {"referenceID": 23, "context": "(2) To cope with the rapidly growing complexity of patterns present in big data, ML practitioners typically increase the size and capacity of LVMs, which incurs great challenges for model training, inference, storage and maintenance [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 24, "context": "(3) There exist substantial redundancy and overlapping amongst patterns discovered by existing LVMs from massive data, making them hard to interpret [26].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 0, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 23, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 25, "context": ", clusters, topics) from data: if the model components are biased to be far apart from each other, then one would expect that such components will tend to be less overlapping and less aggregated over dominant patterns (as one often experiences in standard clustering algorithms [27]), and therefore more likely to capture the long-tail patterns.", "startOffset": 278, "endOffset": 282}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 220, "endOffset": 224}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 254, "endOffset": 258}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 289, "endOffset": 292}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 319, "endOffset": 322}, {"referenceID": 25, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 0, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 23, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 0, "context": "We focus on the mutual angular regularizer proposed in [1] and analyze how it affects the generalization performance of supervised latent variable models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 2, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 3, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 4, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 12, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 5, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 13, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 14, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 6, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 7, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 15, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 16, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 8, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 9, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 10, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 11, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 26, "context": "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.", "startOffset": 74, "endOffset": 82}, {"referenceID": 8, "context": "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.", "startOffset": 74, "endOffset": 82}, {"referenceID": 8, "context": "For instance, the components in Latent Dirichlet Allocation [10] are called topics and each topic is parametrized by a multinomial vector.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 0, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 23, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 25, "context": "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "Here we present a detailed review of the mutual angular regularizer [1] as our theoretical analysis is based on it.", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "According to Chebyshev inequality [31],", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 32, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "literature such as [33].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "R||(F \u2032) = R||(h \u25e6 g) = R||(h \u25e6 g + h(0)) \u2264 R||(h \u25e6 g) + 2|h(0)| n (Theorem 12 in [33]) \u2264 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "R||(F \u2032) = R||(h \u25e6 g) = R||(h \u25e6 g + h(0)) \u2264 R||(h \u25e6 g) + 2|h(0)| n (Theorem 12 in [33]) \u2264 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "For the ease of analysis, following [35], we assume the target function g belongs to a function class with smoothness expressed in the first moment of its Fourier representation: we define function class \u0393C as the set of functions g satisfying \u222b", "startOffset": 36, "endOffset": 40}, {"referenceID": 33, "context": "Please refer to Theorem 3 in [35] for the proof.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "minj\u2208{1,\u00b7\u00b7\u00b7 ,l} \u03c6(e, wj) > \u03b8, then as Ej is a connected set, there is a path q : t \u2208 [0, 1] \u2192 Ej connecting e to w1, and when t = 0, the path starts at q(0) = e; when t = 1, the path ends at q(1) = w1.", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "We define functions rj(t) = \u03c6(q(t), wj) for t \u2208 [0, 1] and j = 1, \u00b7 \u00b7 \u00b7 , l.", "startOffset": 48, "endOffset": 54}, {"referenceID": 34, "context": "We apply MAR-NN for phoneme classification [36] on the TIMIT speech dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "The inputs are MFCC features extracted with context windows and the outputs are class labels generated by the HMM-GMM model through forced alignment [36].", "startOffset": 149, "endOffset": 153}, {"referenceID": 35, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 36, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 37, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 38, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 39, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 40, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 41, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 25, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 0, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 23, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 42, "context": "In a multi-class classification problem, [44] proposed to use the determinant of the covariance matrix to encourage classifiers to be different from each other.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.", "startOffset": 136, "endOffset": 144}, {"referenceID": 44, "context": "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.", "startOffset": 136, "endOffset": 144}, {"referenceID": 45, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 51, "endOffset": 55}, {"referenceID": 43, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 73, "endOffset": 80}, {"referenceID": 1, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 73, "endOffset": 80}, {"referenceID": 1, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 97, "endOffset": 100}, {"referenceID": 44, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 131, "endOffset": 135}, {"referenceID": 47, "context": "For the approximation error, [49] demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 48, "context": "[50] showed that neural networks with a single hidden layer, sufficiently many hidden units and arbitrary bounded and nonconstant activation function are universal approximators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] proved that multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] showed that if the target function is in the hypothesis set formed by neural networks with one hidden layer of m units, then the approximation error rate is O(1/ \u221a m).", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] showed that neural networks with one layer of m hidden units and sigmoid activation function can achieve approximation error of order O(1/ \u221a m), where the target function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[53] proved that if the target function is of the form f(x) = \u222b", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "As for the estimation error, please refer to [32] for an extensive review, which introduces various estimation error bounds based on VC-dimension, flat-shattering dimension, pseudo dimension and so on.", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis.", "creator": "LaTeX with hyperref package"}}}