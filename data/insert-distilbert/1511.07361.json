{"id": "1511.07361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "abstract": "this 2009 paper proposes algorithms for learning two - level boolean rules in conjunctive normal form ( cnf, i. e. and - of - ors ) or disjunctive normal form ( dnf, i. e. or - out of - ands ) as a type of human - interpretable classification model, jointly aiming for recognizing a favorable trade - off between the classification accuracy and the simplicity of the rule. two formulations are proposed. the objective first is an optimal integer program whose objective function is a combination of the total number of errors and the total number of features used in the sampling rule. whereas we generalize a previously proposed linear programming ( lp ) relaxation from one - level to two - level rules. notably the second term formulation tentatively replaces the 0 - state 1 classification error with the hamming distance transformation from the greatest current two - level rule to the closest rule that correctly classifies a sample. based on this second approximation formulation, integral block coordinate descent and alternating minimization constraint algorithms are developed. experiments show strongly that the two - level rules can yield noticeably better performance than one - level logic rules due to their dramatically larger modeling capacity, and whilst the two algorithms based slightly on the hamming distance formulation are generally superior to their the other two - threshold level rule learning methods in our fourier comparison. a proposed approach to binarize any sufficiently fractional \u03b4 values in the optimal solutions of lp relaxations is also shown to be least effective.", "histories": [["v1", "Mon, 23 Nov 2015 18:52:21 GMT  (64kb)", "http://arxiv.org/abs/1511.07361v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["guolong su", "dennis wei", "kush r varshney", "dmitry m malioutov"], "accepted": false, "id": "1511.07361"}, "pdf": {"name": "1511.07361.pdf", "metadata": {"source": "CRF", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "authors": ["Guolong Su", "Dennis Wei", "Kush R. Varshney", "Dmitry M. Malioutov"], "emails": ["guolong@mit.edu)", "dmalioutov@us.ibm.com)"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n07 36\n1v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\n01 5\nThis paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from onelevel to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.\nKeywords\nInterpretable Classifier, Linear Programming Relaxation"}, {"heading": "1 Introduction", "text": "Boolean rules are an important classification model for machine learning and data mining. A typical Boolean rule connects a subset of binary input features with the logical operators conjunction (\u201cAND\u201d), disjunction (\u201cOR\u201d), and negation (\u201cNOT\u201d) to form the prediction. As an example, a Boolean rule in [8] for the prediction of 10 year coronary heart disease (CHD) risk for a 45\n\u2217Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, USA. (email: guolong@mit.edu)\n\u2020Mobile, Solutions, and Mathematical Sciences Department, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA. (email: dwei, krvarshn, dmalioutov@us.ibm.com)\nyear old male can be expressed as follows:\nIF 1. NOT smoke; OR 2. total cholesterol < 160; AND systolic blood pressure < 140; THEN (10 year CHD risk < 5%)=TRUE.\nThis is a two-level rule in DNF (OR-of-ANDs), where in the lower level, conjunctions of binary features build clauses and in the upper level, the disjunction of the clauses forms the predictor.\nAn advantage of Boolean rules is high human interpretability [3, 4]. The features included in the learned rule provide key reasons behind the prediction results; in the example above, not smoking may be the reason for the prediction of low 10 year CHD risk. These reasons can be easily understood by the users.\nHuman interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker/agent who makes the final decision. Such a decision maker often needs an understanding of the reasons for the prediction before accepting the result; thus, high prediction accuracy without providing the reasons is not sufficient for the model to be trusted. As an example, medical diagnosis models [8] may predict a high risk of certain diseases for a patient; a doctor then needs to know the underlying factors to compare with his/her domain knowledge, take the correct action, and communicate with the patient. Another application requiring interpretability is fraud detection [6], where convincing reasons are needed to justify further auditing.\nThis paper considers learning two-level Boolean rules from datasets, with the joint criteria of both classification accuracy and human interpretability measured by the total number of features used (i.e. sparsity) [3]. Two optimization-based formulations are introduced. The objective function in the first formulation is a weighted combination of the total number of classification errors and the sparsity, based on which we extend a previously proposed LP relaxation approach from one-level to two-level rules. The second formulation replaces the 0-1 classification error cost with the Hamming distance from the current rule to the closest rule that correctly classifies a sample; we propose\nblock coordinate descent and alternating minimization approaches for optimizing the objective in the second formulation. To tackle the issue of fractional optimal solutions to LP relaxations, we introduce a new binarization method to convert LP solutions into binary values. Experiments show that compared with one-level rules, the two-level rules can have noticeably lower error rate as well as more flexible accuracy-simplicity tradeoffs. The two algorithms based on the Hamming distance formulation generally have superior performance among the approaches for two-level rule learning that we compare, and the new binarization method is shown to be effective.\nThe remainder of this paper is organized as follows. Section 2 reviews related work and fields. After the problem formulations in Section 3, optimization approaches are introduced in Section 4 and evaluated in Section 5. Section 6 concludes this work."}, {"heading": "2 Review of Existing Work", "text": "The two-level Boolean rules in this work are examples of sparse decision rule lists, one of the major classes of interpretable models [4]. Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4]. Section 2.1 and 2.2 focus on existing work in learning one-level and two-level Boolean rules, respectively. The one-level rule learning method in [10] forms a building block in the current work.\n2.1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10]. We have a training dataset with n labeled samples; the ith sample has a binary label yi \u2208 {0, 1} and in total d binary features ai,j \u2208 {0, 1} (1 \u2264 j \u2264 d). The goal is to learn a classifier y\u0302(\u00b7) that can generalize well to unseen feature vectors sampled from the same distribution as the training dataset.\nThe class of classifiers considered in [10] consists of one-level Boolean rules, which take only a conjunction (or disjunction) of selected features. De Morgan\u2019s laws show an equivalence between corresponding conjunctive and disjunctive rules\ny = \u2227\nj\u2208C\nxj \u21d4 y = \u2228\nj\u2208C\nxj ,\nwhere y and xj mean the negation of binary variables y and xj , respectively. Due to this equivalence, algorithms in [10] focus on the disjunctive rule\ny\u0302i =\nd \u2228\nj=1\nai,jwj , for 1 \u2264 i \u2264 n,\nwhere binary decision variable wj \u2208 {0, 1} indicates whether the jth feature is selected in the rule, and y\u0302i \u2208 {0, 1} is the prediction of the i\nth sample. Replacing binary operators with linear-algebraic expressions, a mixed integer program is formulated for the one-level rule learning problem [10]:\nmin wj\nn \u2211\ni=1\n\u03bei + \u03b8 \u00b7\nd \u2211\nj=1\nwj(2.1)\ns.t. \u03bei = max\n\n\n\n0,\n\n1\u2212\nd \u2211\nj=1\nai,jwj\n\n\n\n\n\n, for yi = 1,\n(2.2)\n\u03bei = d \u2211\nj=1\nai,jwj , for yi = 0,(2.3)\nwj \u2208 {0, 1}, for 1 \u2264 j \u2264 d.(2.4)\nThe objective function is a combination of accuracy and sparsity with the balance controlled by the parameter \u03b8. The accuracy related costs \u03bei for false negatives and false positives are formulated in (2.2) and (2.3), respectively.\nRelaxation of (2.4) into 0 \u2264 wj \u2264 1 yields a linear program that is efficiently solvable [10]. Sufficient conditions for the relaxation to be exact are discussed in [10].\n2.2 Two-level Rule Learning Two-level Boolean rules have significantly larger modeling capacity than one-level rules. In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.\nTwo algorithms are proposed in [10] for rule set learning, based on the one-level learning algorithm. The first algorithm uses the set covering approach [11] and obtains a two-level rule. Suppose we want to learn a rule in DNF (OR-of-ANDs). After training the first clause with the entire training set, we remove the samples with output 1 from the first clause; the predictions on these samples have been determined regardless of the other clauses. Then we train the second clause with the remaining samples, and repeat this removetrain procedure for the rest of clauses. Since this set covering approach is a one-pass greedy-style algorithm, there should be space for improvement. The second algorithm for rule sets in [10] applies boosting, in which the predictor is a weighted combination of rules rather than a two-level rule and thus hinders interpretability.\nAnother algorithm for DNF learning is the Hamming Clustering (HC) approach [13], which uses greedy methods to iteratively cluster samples in the same category and with features close to each other in Hamming\ndistance. HC may be seen as bottom-up, whereas our algorithms are top-down and treat the training dataset more globally. Experiments in [13] seem to imply HC produces a high number of clauses, which hinders interpretability.\nThere are a number of other methods and fields related to two-level rule learning. First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists. However, the assignment of prior and likelihood in the Bayesian framework may not always be clear, and certain approximate inference algorithms may have high computational cost. Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.e. not a two-level rule. Third, learnability of Boolean formulae is considered in [7] from the perspective of probably approximately correct (PAC) learning. Different from our problem, the setup of [7] and related work typically assumes positive or negative samples can be generated on demand and without noise. Fourth, two-level logic optimization in circuit design [12] considers simplifying two-level rules that exactly match a given truth table. However, in rule learning, it is neither needed nor desirable to exactly match a noisy dataset."}, {"heading": "3 Problem Formulation", "text": "The goal is to learn a two-level Boolean rule in the Conjunctive Normal Form (AND-of-ORs) from a training dataset1, with the same setup of binary supervised classification as in Section 2.1. In the lower level of the rule, we form each clause by the disjunction of a selected subset of input features; in the upper level, the final predictor is formed by the conjunction of all clauses. Suppose the total number of clauses is fixed and denoted by R. If we let the binary decision variables wj,r represent whether to include the jth feature in the rth clause, then the output of the rth clause for the ith sample is\n(3.5) v\u0302i,r = d \u2228\nj=1\n(ai,jwj,r) , for 1 \u2264 i \u2264 n, 1 \u2264 r \u2264 R.\nThen, the predictor y\u0302i satisfies\n(3.6) y\u0302i =\nR \u2227\nr=1\nv\u0302i,r, for 1 \u2264 i \u2264 n.\nAlthough this setup has a fixedR, an option to \u201cdisable\u201d a clause can be introduced to reduce the total number\n1We assume the negation of each feature is included as another input feature; if not, we can pad the input features with negations.\nof actual clauses if the assigned R is too large. For a CNF rule, a clause can be regarded as disabled if its output is always 1. Thus, we can pad the input feature matrix with a trivial \u201calways true\u201d feature ai,0 = 1 for all samples, and also include the corresponding decision variables w0,r for all clauses. The sparsity cost for w0,r can be lower than other variables or even zero. If w0,r = 1, then the r\nth clause has output 1 and is thus disabled in the CNF rule. This option might reduce accuracy with the tradeoff of improved sparsity.\nIn certain cases, DNF rules could be more natural than CNF. A CNF learning algorithm can apply to DNF learning by De Morgan\u2019s laws:\ny =\nR \u2228\nr=1\n\n\n\u2227\nj\u2208Cr\nxj\n\n \u21d4 y =\nR \u2227\nr=1\n\n\n\u2228\nj\u2208Cr\nxj\n\n\nwhere Cr is the index set of features selected in the rth clause. To learn a DNF rule with a CNF learning algorithm, we can first negate both features and labels of all samples, then learn a CNF rule with the negated features and labels, and finally use the decision variables wj,r with the original features to construct a DNF rule. Thus, Sections 3 and 4 focus on CNF only.\nTwo formulations are introduced with different accuracy costs in Section 3.1 and 3.2, respectively.\n3.1 Formulation with 0-1 Error Cost A natural choice of the accuracy-related cost is the total number of misclassifications (i.e. 0-1 error for each sample). With the sparsity cost as the sum of the number of features used in each clause, a formulation is as below\nmin wj,r\nn \u2211\ni=1\n|y\u0302i \u2212 yi|+ \u03b8 \u00b7\nR \u2211\nr=1\nd \u2211\nj=1\nwj,r(3.7)\ns.t. y\u0302i =\nR \u2227\nr=1\n\n\nd \u2228\nj=1\n(ai,jwj,r)\n\n , for 1 \u2264 i \u2264 n,(3.8)\nwj,r \u2208 {0, 1}, for 1 \u2264 j \u2264 d, 1 \u2264 r \u2264 R.\nThere are a few challenges in this formulation. First, the same as one-level rule learning, the two-level rule learning problem is combinatorial. Second, all the clauses are symmetric in (3.7) and (3.8), however, we generally would like the clauses to be distinct since duplication of clauses is inefficient.\n3.2 Formulation with Minimal Hamming Distance This formulation has the two motivations below. First, it is potentially desirable to have a finer-grained accuracy cost than the 0-1 error in (3.7). As an example, consider two CNF rules, both with two clauses, predicting the same sample with ground truth label yi = 1.\nSuppose both clauses in the first rule predict 0, while only one clause in the second rule predicts 0 and the other predicts 1. Although both rules misclassify this sample after taking \u201cAND\u201d of their two clauses, the second rule is closer to correct than the first one. If we use an iterative algorithm to refine the learned rule, it might be beneficial for the accuracy cost term to favor the second rule in this example, which could push the solution towards being correct. The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].\nIn the new formulation, the accuracy cost for a single sample is the minimal Hamming distance from a given CNF rule to an ideal CNF rule, where the latter means a rule that correctly classifies this sample. The Hamming distance between two CNF rules is the total number of wj,r that are different in the two rules. An intuitive explanation of this minimal Hamming distance is the smallest number of modifications (i.e. negations) of the current rule wj,r that are needed to correct a misclassification on a sample, i.e. how far is the rule from being correct.\nFor mathematical formulation, we introduce ideal clause outputs vi,r with 1 \u2264 i \u2264 n and 1 \u2264 r \u2264 R to represent a CNF rule that correctly classifies the ith sample. The values of vi,r are always consistent with the ground truth labels, i.e. yi = \u2227R r=1 vi,r for all 1 \u2264 i \u2264 n. We let vi,r have a ternary alphabet {0, 1,DC}, where vi,r = DC means that we \u201cdon\u2019t care\u201d about the value of vi,r. With this setup, if yi = 1, then vi,r = 1 for all 1 \u2264 r \u2264 R; if yi = 0, then vi,r0 = 0 for at least one value of r0, and we can have vi,r = DC for all r 6= r0. In implementation, vi,r = DC implies the removal of the ith sample in the training or updating for the rth clause, which generally leads to a different training subset for each clause.\nDenote \u03b7i as the minimal Hamming distance from the current CNF rule wj,r to an ideal CNF rule for the ith sample. We derive \u03b7i for positive and negative samples, respectively. Since yi = 1 implies vi,r = 1 for all r, for each clause with output 0 in the current rule, at least one positive feature needs to be included to match vi,r = 1. Thus, the minimal Hamming distance for a positive sample is the number of clauses with output 0:\n\u03b7i =\nR \u2211\nr=1\nmax\n\n\n\n0,\n\n1\u2212\nd \u2211\nj=1\nai,jwj,r\n\n\n\n\n\n, for yi = 1.\nFor yi = 0, we first consider for fixed r the minimal Hamming distance between the rth clauses of the current rule and an ideal rule where vi,r = 0. We need to negate wj,r in the current rule for j with\nwj,r = ai,j = 1 to match vi,r = 0, and thus the minimal Hamming distance of this clause is \u2211d\nj=1 ai,jwj,r. Then, since vi,r = 0 needs to hold for at least one value of r while all other vi,r can be DC, the minimal Hamming distance of the CNF rule is given by the minimum over r, i.e. setting vi,r0 = 0 with\n(3.9) r0 = argmin 1\u2264r\u2264R\n\n\nd \u2211\nj=1\nai,jwj,r\n\n .\nCombining all analysis above, the new formulation with the minimal Hamming distance cost is as below\nmin wj,r\nn \u2211\ni=1\n\u03b7i + \u03b8 \u00b7\nR \u2211\nr=1\nd \u2211\nj=1\nwj,r(3.10)\ns.t. \u03b7i =\nR \u2211\nr=1\nmax\n\n\n\n0,\n\n1\u2212\nd \u2211\nj=1\nai,jwj,r\n\n\n\n\n\n, for yi = 1,\n\u03b7i = min 1\u2264r\u2264R\n\n\nd \u2211\nj=1\nai,jwj,r\n\n , for yi = 0,(3.11)\nwj,r \u2208 {0, 1}, for 1 \u2264 j \u2264 d, 1 \u2264 r \u2264 R.\nThe binary variables wj,r can be further relaxed to 0 \u2264 wj,r \u2264 1. The minimum over r in (3.11) implies the continuous relaxation of (3.10) is generally non-convex with R > 1, making the exact solution challenging.\nLetting R = 1 in formulation (3.10), we can see it becomes identical to formulation (2.1) in one-level rule learning [10]. Thus, the accuracy cost in (2.1) could be interpreted as the minimal Hamming distance.\nTo simplify description of algorithms later, we show a formulation (3.12) below, which is equivalent to (3.10) but involves both vi,r and wj,r. Taking the minimization over vi,r in (3.12) with fixed wj,r eliminates the variables vi,r, and (3.12) becomes identical to (3.10).\nmin wj,r , vi,r\nn \u2211\ni=1\nR \u2211\nr=1\n[\n1vi,r=1 \u00b7max\n{\n0,\n(\n1\u2212\nd \u2211\nj=1\nai,jwj,r\n)}\n(3.12)\n+ 1vi,r=0 \u00b7\nd \u2211\nj=1\nai,jwj,r\n]\n+ \u03b8 \u00b7\nR \u2211\nr=1\nd \u2211\nj=1\nwj,r\ns.t.\nR \u2227\nr=1\nvi,r = yi, for 1 \u2264 i \u2264 n,(3.13)\nvi,r \u2208 {0, 1,DC}, for 1 \u2264 i \u2264 n, 1 \u2264 r \u2264 R,\nwj,r \u2208 {0, 1}, for 1 \u2264 j \u2264 d, 1 \u2264 r \u2264 R."}, {"heading": "4 Optimization Approaches", "text": "This section discusses various optimization approaches to the two-level rule learning problem. Based on the for-\nmulation in Section 3.1, we generalize the LP approach from one-level rule learning to two-level rules by proper relaxation in Section 4.1. Based on the formulation in Section 3.2, we propose the block coordinate descent algorithm in Section 4.2 and the alternating minimization algorithm in Section 4.3 for the objective (3.12). Since all algorithms utilize LP relaxations, Section 4.4 considers the binarization problem if the result of the LP is not binary.\n4.1 Two-level Linear Programming Relaxation This approach considers the 0-1 error formulation (3.7) and directly generalizes the idea of replacing binary operations \u201cAND\u201d and \u201cOR\u201d with linear-algebraic operations, as used in one-level rule learning [10]. Since \u201cAND\u201d and \u201cOR\u201d are defined only on binary points, there are various interpolations of these functions on fractional points, and thus both convex and concave interpolations exist for both operators. The \u201cOR\u201d function has the following interpolations [5]\nd \u2228\nj=1\nxj = max 1\u2264j\u2264d {xj} = min\n\n\n\n1,\nd \u2211\nj=1\nxj\n\n\n\n,\nwhere the first is convex and the second is concave, both of which are the respective tightest interpolations.\nThe logical \u201cAND\u201d operator also has the tightest convex and concave interpolations as [5]\nd \u2227\nj=1\nxj = max\n\n\n\n0,\n\n\nd \u2211\nj=1\nxj\n\n\u2212 (d\u2212 1)\n\n\n = min 1\u2264j\u2264d {xj}.\nSince the predictor y\u0302i of the two-level rule in (3.8) is a composition of \u201cAND\u201d and \u201cOR\u201d operators, it is possible to properly interpolate it using both a convex function and a concave function by composing the individual interpolations of the two operators. From (3.5) and (3.6), a convex interpolation of y\u0302i is\ny\u0302i = max\n{\n0,\n(\nR \u2211\nr=1\nmax 1\u2264j\u2264d {ai,jwj,r}\n)\n\u2212 (R \u2212 1)\n}\n,\nand a concave interpolation can be obtained similarly. Denote the 0-1 error cost for the ith sample as \u03c8i. Since the errors \u03c8i in (3.7) should be minimized, if yi = 1, then \u03c8i = 1 \u2212 y\u0302i and thus we need the concave interpolation for y\u0302i; if yi = 0, then \u03c8i = y\u0302i and thus the convex interpolation is needed. Finally, the formulation in (3.7) can be exactly converted into a mixed integer\nprogram as follows:\nmin wj,r,\u03c8i,\u03b2i,r\nn \u2211\ni=1\n\u03c8i + \u03b8 \u00b7\nR \u2211\nr=1\nd \u2211\nj=1\nwj,r(4.14)\ns.t. \u03c8i \u2265 0, \u2200i,\n\u03c8i \u2265 1\u2212\nd \u2211\nj=1\nai,jwj,r, for yi = 1, \u2200r,\n\u03c8i \u2265\n(\nR \u2211\nr=1\n\u03b2i,r\n)\n\u2212 (R\u2212 1), for yi = 0,\n\u03b2i,r \u2265 ai,jwj,r, for yi = 0, \u2200j, \u2200r,\nwj,r \u2208 {0, 1}, \u2200j, \u2200r.\nIf we relax the decision variables wj,r to the interval [0, 1], then we have a linear program.\nUnfortunately, numerical results seem to suggest that this LP relaxation is likely to have fractional values in the optimal solution wj,r, and the optimal \u03c8i may possibly be all close to 0, which may be undesirable since \u03c8i aims to represent the 0-1 error cost term. A possible reason is that the gap between the convex and concave interpolations may loosen the LP and enable fractional results with lower cost than binary solutions.\n4.2 Block Coordinate Descent Algorithm This algorithm considers the decision variables in a single clause (wj,r with a fixed r) as a block of coordinates, and performs block coordinate descent to minimize the Hamming distance objective function in (3.12). Each iteration updates a single clause with all the other (R \u2212 1) clauses fixed, using the one-level rule learning algorithm [10]. We denote r0 as the clause to be updated.\nThe optimization of (3.12) even with (R\u22121) clauses fixed still involves a joint minimization over wj,r0 and the ideal clause outputs vi,r for yi = 0 (vi,r = 1 for yi = 1 and thus fixed), so the exact solution could still be challenging. To simplify, we fix the values of vi,r for yi = 0 and r 6= r0 to the actual clause outputs v\u0302i,r in (3.5) with the current wj,r (r 6= r0). Now we assign vi,r0 for yi = 0: if there exists vi,r = v\u0302i,r = 0 with r 6= r0, then this sample is guaranteed to be correctly classified and we can assign vi,r0 = DC to minimize the objective in (3.12); in contrast, if v\u0302i,r = 1 holds for all r 6= r0, then the constraint (3.13) requires vi,r0 = 0.\nThis derivation leads to the updating process as follows. To update the rth\n0 clause, we remove all samples\nthat have label yi = 0 and are already predicted as 0 by at least one of the other (R \u2212 1) clauses, and then update the rth\n0 clause with the remaining samples using\nthe one-level rule learning algorithm [10].\nThere are different choices of which clause to update in an iteration. For example, we can update clauses cyclically or randomly, or we can try the update for each clause and then greedily choose the one with the minimum cost. The greedy update is used in our experiments.\nThe initialization of wj,r in this algorithm also has different choices. For example, one option is the set covering method [10], as is used in our experiments. Random or all-zero initialization can also be used.\n4.3 Alternating Minimization Algorithm This algorithm uses the Hamming distance formulation (3.12) and alternately minimizes with respect to the decision variables wj,r and the ideal clause outputs vi,r . Each iteration has two steps: update vi,r with the current wj,r, and update wj,r with the new vi,r. The latter step is simpler and will be first discussed.\nWith fixed values of vi,r, the minimization over wj,r is relatively straight-forward: the objective in (3.12) becomes separated into R terms, each of which depends only on a single clause wj,r with a fixed r. Thus, all clauses are decoupled in the minimization over wj,r , and the problem becomes parallel learning of R onelevel clauses. Explicitly, the update of the rth clause will first remove all the samples with vi,r = DC, and then utilize the one-level rule learning algorithm [10].\nThe update over vi,r with fixed wj,r follows the discussion in Section 3.2: for positive samples yi = 1, vi,r = 1, and for the negative samples yi = 0, vi,r0 = 0 for r0 defined in (3.9) and vi,r = DC for r 6= r0. For negative samples with a \u201ctie\u201d, i.e. non-unique r0 in (3.9), tie breaking is achieved by a \u201cclustering\u201d approach similar to the spirit of [13]. First, for each clause 1 \u2264 r0 \u2264 R, we compute its cluster center in the feature space by taking the average of ai,j (for each j) over samples i for which r0 is minimal in (3.9) (including ties). Then, each sample with a tie is assigned to the clause with the closest cluster center in \u21131-norm among all minimal r0 in (3.9).\nSimilar to the block coordinate descent algorithm, various options exist for initializing wj,r in this algorithm. The set covering approach [10] is used in our experiments.\n4.4 Redundancy Aware Binarization This section discusses a solution to a potential issue with the LP relaxation that is widely used in the algorithms proposed in this paper. Although there are conditions under which the optimal solution to the LP relaxation for one-level rule learning is guaranteed to be binary [10], we are not aware of similar guarantees in two-level rule learning; in addition, these conditions are unlikely to\nalways hold with a real-world and noisy dataset. Thus, the optimal solution to LP may have fractional values, in which case we need to convert them into binary. If LP already yields a binary optimal solution, then the binarization methods here will not change it.\nA straight-forward binarization method is to compare each wj,r from LP with a specified threshold, as done in [10]. However, empirical results seem to suggest that the resulting binarized rule may have redundancy, making the rule unnecessarily complex and possibly influencing the accuracy.\nThe following improved binarization method considers three types of redundancy sets of binary features in a single disjunctive clause. Among the features in each redundancy sets, no more than one feature will appear in any single clause of the optimal CNF rule2.\nThe first type of redundancy set corresponds to nested features. If binary features ai,j1 and ai,j2 satisfy ai,j1 \u2264 ai,j2 for all samples, then these two features cannot both appear in a single clause in the optimal CNF rule; otherwise, since ai,j1 \u2228\nai,j2 = ai,j2 , removing ai,j1 from the clause keeps the same output and improves the sparsity, leading to a better rule. If there is a nested set ai,j1 \u2264 ai,j2 \u2264 . . . \u2264 ai,jP (\u22001 \u2264 i \u2264 n), then at most one feature from this set can be selected in a single clause in the optimal CNF rule.\nThe second type consists of complementary binary features, when we have the option to \u201cdisable\u201d a clause as explained in Section 3. Since complementary features ai,j1 and ai,j2 satisfy ai,j1 \u2228\nai,j2 = 1 (\u2200i), the optimal CNF rule cannot have both of them in a single clause, otherwise disabling this clause by w0,r = 1 and wj,r = 0 (j > 0) keeps the output and improves sparsity.\nThe third type also applies only when we have the option to disable a clause. This type can happen with two nested sets that are pairwise complementary, especially if some binary features are obtained by thresholding continuous valued features. For example, suppose we have six binary features from thresholding the same continuous feature ci with thresholds \u03c41 < \u03c42 < \u03c43:\nai,1 = (ci \u2264 \u03c41) , ai,2 = (ci \u2264 \u03c42) , ai,3 = (ci \u2264 \u03c43) ,\nai,4 = (ci > \u03c41) , ai,5 = (ci > \u03c42) , ai,6 = (ci > \u03c43) .\nThe \u201czigzag\u201d path (ai,4, ai,5, ai,2, ai,3) forms a redundancy set, since at most one out of the four features can be selected in a fixed clause of the optimal CNF rule, otherwise either the first or the second redundancies above will happen and thus the rule is not optimal. There are typically multiple \u201czigzag\u201d paths, e.g.\n2This statement holds for both formulations (3.7) and (3.12); for simplicity, we will focus on the 0-1 error cost formulation for illustration.\n(ai,4, ai,1, ai,2, ai,3) and (ai,4, ai,5, ai,6, ai,3). The new binarization approach takes the above types of redundancies into account. For illustration, suppose all binary features are obtained by thresholding continuous valued features. In a clause and for a fixed continuous valued feature, we sweep over all nonredundant combinations of the binary features induced by this continuous feature and obtain the one with minimal cost. Since the total number of non-redundant combinations for nested and zigzag features is linear and quadratic with the number of thresholds, respectively, the sweeping is relatively efficient with a single continuous feature. However, joint minimization across all continuous features seems combinatorial and challenging. Thus, we first sort continuous features in the decreasing order by the sum of corresponding decision variables in the optimal solution to the LP relaxation, and then sequentially binarize the decision variables induced by each continuous feature."}, {"heading": "5 Numerical Evaluation", "text": "5.1 Setup This section evaluates the algorithms with UCI repository datasets [9], including connectionist bench sonar (Sonar), BUPA liver disorders (Liver), Pima Indian diabetes (Pima), and Parkinsons (Parkin). The continuous valued features in these datasets are converted to binary using quantile thresholds.\nThe goal is to learn a DNF rule (OR-of-ANDs) from each dataset. We use stratified 10-fold cross validation, and then average the test and training error rates over these 10 folds. All LPs are solved by CPLEX version 12 [1]. The sparsity parameter \u03b8 = A\u00d710B where we sweep A = 1, 2, 5 and B = \u22124,\u22123,\u22122,\u22121, 0, 1, for a total of 18 values. We sweep the total number of clauses in the DNF rule between R = 1 and R = 5; the option to \u201cdisable\u201d a clause (which can reduce R) is not used in the evaluation, except in Section 5.5 where we compare the results with/without such an option.\nAlgorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.0: C5.0 with rule set option in IBM SPSS, CART: classification and regression trees algorithm in Matlab\u2019s classregtree function). TLP, BCD, and AM all use the redundancy-aware binarization. The maximum number of iterations in BCD and AM is set as 100.\nWe show results on both the minimal average test error rate obtained from the 18 different values of \u03b8 and the Pareto front for the tradeoff between accuracy and sparsity.\n5.2 Minimal Average Test Error Rate The minimal average test error rates achieved among the 18 values of \u03b8 for all algorithms are listed in Table 1, where R denotes the total number of clauses. The results for DL, C5.0, and CART are cited from [10]. We refer the reader to [10] for results from other classifiers that are generally not interpretable; the accuracy of our algorithms is generally quite competitive with them.\nFor each algorithm and each dataset, the number marked with bold font is the lowest error rate among 1 \u2264 R \u2264 5. There are a few observations from these results. First, most bold-font numbers appear in rows with R > 1. Since R = 1 corresponds to the one-level rules while R > 1 corresponds to two-level rules, the two-level rules can reduce error rate on these datasets, especially significant for block coordinate descent and alternating minimization algorithms. Second, the block coordinate descent and alternating minimization algorithms generally have superior performance to the other methods for two-level rule learning in our comparison; however, the two-level LP relaxation does not seem to have as good performance. Thus, we focus on block coordinate descent and alternating minimization algorithms in the remainder of this section. Third, for the Sonar dataset with the same R, the set covering approach with new binarization has noticeably lower error rates than with simple binarization, which shows the effectiveness of the redundancy-aware binarization.\nFourth, we can see that for a fixed dataset and a fixed algorithm, the error rate does not decrease monotonically with R, indicating overfitting with too many clauses.\nAs a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13]. HC has 25.0% test error rate with an average of 85 features used in the rule as reported in [13], while block coordinate descent and alternating minimization algorithms have lower minimal error rates of 24.9% (average of 6.3 features used) and 22.7% (average of 6 features used) when R = 2, respectively. Thus, our algorithms on Pima dataset produce rules with higher accuracy and significantly fewer features used 3."}, {"heading": "5.3 Pareto Fronts with Different Numbers of", "text": "Clauses The Pareto fronts with different numbers of clauses are shown in Fig. 1, where we vary R from 1 to 5. Fig. 1 (a) and (b) show the average test and training error rates of the alternating minimization algorithm on the Pima dataset, while Fig. 1 (c) and (d) show the error rates of the block coordinate descent algorithm on the Liver dataset. Each point in the figure corresponds to the pair of average error rate and the average number of features in the learned DNF rule that is obtained at one of the 18 values of \u03b8, and the Pareto fronts are denoted by lines for ease of visualization.\nThe following observations are implied by Fig. 1. First, a comparison of the Pareto fronts of R = 1 and R > 1 suggests that two-level rules may have more flexible tradeoff between accuracy and simplicity. Second, as shown in Fig. 1 (b) and (d), with the increase of R, the learned rule typically uses more features and has lower training error rates. However, the exact tendency of the Pareto front of the test error rate may depend on the complexity of datasets: in Fig. 1 (a), the Pareto front becomes worse with the increase of R when R > 2, implying overfitting on this relatively simple dataset; in contrast, for the relatively complex Liver dataset in Fig. 1 (c), the minimum test error rate has a decrease at R = 5 with more features used, which seems to suggest that R = 5 does not overfit yet.\n5.4 Pareto Fronts of Different Algorithms The Pareto fronts of the average test error rates for different algorithms on the Sonar and Liver datasets with R = 5 are shown in Fig. 2 (a) and (b), respectively. Comparing the block coordinate descent and alternating minimization algorithms, we can see that when the total number of features used is very small, the block\n3There are two differences in setup: HC uses 12-fold cross validation [13], while we use 10-fold; the parameters to convert continuous features into binary may potentially be different.\ncoordinate descent algorithm typically has lower error rate; however, when the total number of features used increases, the alternating minimization algorithm may start to outperform. Comparing the set covering approach with the simple and new binarization, the new binarization generally obtains sparser rules with improved or similar accuracy.\n5.5 Pareto Fronts with/without the Option to Disable a Clause Fig. 3 shows the comparison of Pareto fronts of the average test error rates with and without the option to \u201cdisable\u201d a clause by an \u201calways true\u201d feature. This option generally improves sparsity, while the error rate may remain similar or increase."}, {"heading": "6 Conclusion", "text": "This paper has provided two optimization-based formulations for two-level Boolean rule learning, the first based on 0-1 classification error and the second on Hamming distance. Three algorithms have been developed, namely the two-level LP relaxation, block coordinate descent, and alternating minimization. A redundancyaware binarization method has been introduced.\nNumerical results show that two-level Boolean\nrules have noticeably lower error rate and more flexible accuracy-simplicity tradeoffs on certain complex datasets than one-level rules. However, too many clauses may cause overfitting, and the optimal number of clauses may depend on the complexity of the dataset.\nThe block coordinate descent and alternating minimization algorithms can work with noisy datasets and generally outperform the other methods for two-level rule learning in our comparison. For the tradeoff between accuracy and simplicity, block coordinate descent algorithm may dominate alternating minimization when we require the total number of features used to be very small; in contrast, alternating minimization algorithm may outperform with more features used.\nThe new redundancy-aware binarization has been shown more effective than simple thresholding binarization in certain situations."}, {"heading": "Acknowledgment", "text": "The authors thank for V. S. Iyengar, A. Mojsilovic\u0301, K. N. Ramamurthy, and E. van den Berg for conversations\nand support. The authors are thankful for the assistance in experiments by using datasets from [9]."}], "references": [{"title": "An implementation of logical analysis of data", "author": ["E. Boros", "P.L. Hammer", "T. Ibaraki", "A. Kogan", "E. Mayoraz", "I. Muchnik"], "venue": "IEEE Trans. Knowl. Data Eng., 12 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimization of Boolean complexity in human concept learning", "author": ["J. Feldman"], "venue": "Nature, 407 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Comprehensible classification models \u2013 a position paper", "author": ["A.A. Freitas"], "venue": "ACM SIGKDD Explor., 15 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Proof theory of many-valued logic\u2013linear optimization\u2013logic design: Connections and interactions", "author": ["R. H\u00e4hnle"], "venue": "Soft Comput., 1 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Computer-aided auditing of prescription drug claims", "author": ["V.S. Iyengar", "K.B. Hermiz", "R. Natarajan"], "venue": "Health Care Manag. Sci., 17 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "On the learnability of Boolean formulae", "author": ["M. Kearns", "M. Li", "L. Pitt", "L. Valiant"], "venue": "Proc. Annu. ACM Symp. on Theory of Comput.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1987}, {"title": "Building interpretable classifiers with rules using Bayesian analysis", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "Department of Stat. Tech. Report tr609, Univ. of Washington, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "http://archive.ics.uci.edu/ml, Univ. of Calif., Irvine, School of Information and Computer Sciences", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact rule learning via Boolean compressed sensing", "author": ["D.M. Malioutov", "K.R. Varshney"], "venue": "Proc. Int. Conf. Mach. Learn.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The set covering machine", "author": ["M. Marchand", "J. Shawe-Taylor"], "venue": "J. Mach. Learn. Res., 3 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "ESPRESSO- SIGNATURE: A new exact minimizer for logic functions", "author": ["P.C. McGeer", "J.V. Sanghavi", "R.K. Brayton", "A.L. Sangiovanni-Vincentelli"], "venue": "IEEE Trans. VLSI Syst., 1 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Binary rule generation via Hamming Clustering", "author": ["M. Muselli", "D. Liberati"], "venue": "IEEE Trans. Knowl. Data Eng., 14 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "Int. J. Man- Mach. Studies, 27 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Mach. Learn., 2 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Bayesian Or\u2019s of And\u2019s for interpretable classification with application to context aware recommender systems", "author": ["T. Wang", "C. Rudin", "F. Doshi-Velez", "Y. Liu", "E. Klampfl", "P. MacNeille"], "venue": "tech. report, MIT", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "As an example, a Boolean rule in [8] for the prediction of 10 year coronary heart disease (CHD) risk for a 45", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "An advantage of Boolean rules is high human interpretability [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "An advantage of Boolean rules is high human interpretability [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker/agent who makes the final decision.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker/agent who makes the final decision.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "As an example, medical diagnosis models [8] may predict a high risk of certain diseases for a patient; a doctor then needs to know the underlying factors to compare with his/her domain knowledge, take the correct action, and communicate with the patient.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "Another application requiring interpretability is fraud detection [6], where convincing reasons are needed to justify further auditing.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "sparsity) [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "2 Review of Existing Work The two-level Boolean rules in this work are examples of sparse decision rule lists, one of the major classes of interpretable models [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4].", "startOffset": 215, "endOffset": 218}, {"referenceID": 8, "context": "The one-level rule learning method in [10] forms a building block in the current work.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "The class of classifiers considered in [10] consists of one-level Boolean rules, which take only a conjunction (or disjunction) of selected features.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Due to this equivalence, algorithms in [10] focus on the disjunctive rule", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Replacing binary operators with linear-algebraic expressions, a mixed integer program is formulated for the one-level rule learning problem [10]:", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "4) into 0 \u2264 wj \u2264 1 yields a linear program that is efficiently solvable [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Sufficient conditions for the relaxation to be exact are discussed in [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.", "startOffset": 134, "endOffset": 142}, {"referenceID": 8, "context": "Two algorithms are proposed in [10] for rule set learning, based on the one-level learning algorithm.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "The first algorithm uses the set covering approach [11] and obtains a two-level rule.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "The second algorithm for rule sets in [10] applies boosting, in which the predictor is a weighted combination of rules rather than a two-level rule and thus hinders interpretability.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Another algorithm for DNF learning is the Hamming Clustering (HC) approach [13], which uses greedy methods to iteratively cluster samples in the same category and with features close to each other in Hamming", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Experiments in [13] seem to imply HC produces a high number of clauses, which hinders interpretability.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists.", "startOffset": 30, "endOffset": 37}, {"referenceID": 14, "context": "First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists.", "startOffset": 30, "endOffset": 37}, {"referenceID": 0, "context": "Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "Third, learnability of Boolean formulae is considered in [7] from the perspective of probably approximately correct (PAC) learning.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Different from our problem, the setup of [7] and related work typically assumes positive or negative samples can be generated on demand and without noise.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "Fourth, two-level logic optimization in circuit design [12] considers simplifying two-level rules that exactly match a given truth table.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 9, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 13, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 8, "context": "1) in one-level rule learning [10].", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "7) and directly generalizes the idea of replacing binary operations \u201cAND\u201d and \u201cOR\u201d with linear-algebraic operations, as used in one-level rule learning [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "The \u201cOR\u201d function has the following interpolations [5]", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "The logical \u201cAND\u201d operator also has the tightest convex and concave interpolations as [5]", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "Each iteration updates a single clause with all the other (R \u2212 1) clauses fixed, using the one-level rule learning algorithm [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "To update the r 0 clause, we remove all samples that have label yi = 0 and are already predicted as 0 by at least one of the other (R \u2212 1) clauses, and then update the r 0 clause with the remaining samples using the one-level rule learning algorithm [10].", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "For example, one option is the set covering method [10], as is used in our experiments.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Explicitly, the update of the r clause will first remove all the samples with vi,r = DC, and then utilize the one-level rule learning algorithm [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "9), tie breaking is achieved by a \u201cclustering\u201d approach similar to the spirit of [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "The set covering approach [10] is used in our experiments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Although there are conditions under which the optimal solution to the LP relaxation for one-level rule learning is guaranteed to be binary [10], we are not aware of similar guarantees in two-level rule learning; in addition, these conditions are unlikely to always hold with a real-world and noisy dataset.", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "A straight-forward binarization method is to compare each wj,r from LP with a specified threshold, as done in [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "1 Setup This section evaluates the algorithms with UCI repository datasets [9], including connectionist bench sonar (Sonar), BUPA liver disorders (Liver), Pima Indian diabetes (Pima), and Parkinsons (Parkin).", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "Algorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.", "startOffset": 161, "endOffset": 169}, {"referenceID": 9, "context": "Algorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.", "startOffset": 161, "endOffset": 169}, {"referenceID": 13, "context": "2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "0, and CART are cited from [10].", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "We refer the reader to [10] for results from other classifiers that are generally not interpretable; the accuracy of our algorithms is generally quite competitive with them.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "0% test error rate with an average of 85 features used in the rule as reported in [13], while block coordinate descent and alternating minimization algorithms have lower minimal error rates of 24.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "There are two differences in setup: HC uses 12-fold cross validation [13], while we use 10-fold; the parameters to convert continuous features into binary may potentially be different.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "The authors are thankful for the assistance in experiments by using datasets from [9].", "startOffset": 82, "endOffset": 85}], "year": 2015, "abstractText": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from onelevel to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.", "creator": "LaTeX with hyperref package"}}}