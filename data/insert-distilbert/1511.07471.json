{"id": "1511.07471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize", "abstract": "we consider the emphatic temporal - discrete difference ( td ) algorithm, etd ( $ \\ lambda $ ), for learning the value functions of stationary policies in a discounted, finite state and action markov decision process. the etd ( $ \\ lambda $ ) algorithm was recently proposed by sutton, mahmood, and white to solve a long - standing divergence problem of the standard td algorithm when it is applied to off - policy training, where data from an exploratory policy are used to evaluate other policies of interest. the almost sure convergence of etd ( $ \\ lambda $ ) has been proved in our recent work dealing under general off - policy training conditions, but for a narrow range of diminishing stepsize. in this paper we present convergence sensitivity results for constrained versions of etd ( $ \\ lambda $ ) with constant stepsize and with diminishing stepsize from a broad range. our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of etd ( $ \\ lambda $ ) with powerful convergence theorems from the weak convergence function methods in stochastic approximation theory. for the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit state as the stepsize parameter approaches zero, we also analyze models their behavior appropriately for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. these results are obtained by exploiting the weak feller property of the markov chains associated with the algorithms, and by using quantitative ergodic theorems for weak feller markov chains, in conjunction with modeling the convergence results we get from the weak convergence methods. besides etd ( $ \\ lambda $ ), our analysis also applies to the off - policy td ( $ \\ int lambda $ ) algorithm, when the divergence issue is avoided by setting $ \\ lambda $ sufficiently large.", "histories": [["v1", "Mon, 23 Nov 2015 21:29:43 GMT  (60kb)", "https://arxiv.org/abs/1511.07471v1", "53 pages"], ["v2", "Tue, 10 May 2016 18:38:32 GMT  (61kb)", "http://arxiv.org/abs/1511.07471v2", "minor changes from the first version (updated some references and corrected some typos); 53 pages"], ["v3", "Fri, 20 Jan 2017 18:35:27 GMT  (61kb)", "http://arxiv.org/abs/1511.07471v3", "Minor edits; 53 pages. Longer and more proof details than the journal version"]], "COMMENTS": "53 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huizhen yu"], "accepted": false, "id": "1511.07471"}, "pdf": {"name": "1511.07471.pdf", "metadata": {"source": "CRF", "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize\u2217", "authors": ["Huizhen Yu"], "emails": ["(janey.hzyu@gmail.com)"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n07 47\n1v 3\n[ cs\n.L G\n] 2\n0 Ja\nKeywords: Markov decision processes; approximate policy evaluation; reinforcement learning; temporal difference methods; importance sampling; stochastic approximation; convergence\n\u2217This research was supported by a grant from Alberta Innovates\u2014Technology Futures. \u2020RLAI Lab, Department of Computing Science, University of Alberta, Canada (janey.hzyu@gmail.com)\n1"}, {"heading": "2 Weak Convergence Properties of Constrained ETD Learning", "text": "Contents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Preliminaries 5", "text": "2.1 Off-policy Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 The ETD(\u03bb) Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Associated Bellman Equations and Approximation and Convergence Properties of\nETD(\u03bb) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 Constrained ETD(\u03bb), Averaged Processes and Mean ODE . . . . . . . . . . . . . . . 9"}, {"heading": "3 Convergence Results for Constrained ETD(\u03bb) 12", "text": "3.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Two Variants of Constrained ETD(\u03bb) with Biases . . . . . . . . . . . . . . . . . . . . 14 3.3 More about the Constant-stepsize Case . . . . . . . . . . . . . . . . . . . . . . . . . 16"}, {"heading": "4 Proofs for Section 3 19", "text": "4.1 Proofs for Theorems 3.1 and 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.1.1 Conditions to Verify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.1.2 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.2 Proofs for Theorems 3.3 and 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.2.1 Proofs for the First Variant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4.2.2 Proofs for the Second Variant . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.3 Further Analysis of the Constant-stepsize Case . . . . . . . . . . . . . . . . . . . . . 33 4.3.1 Weak Feller Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.3.2 Proofs of Theorems 3.5 and 3.6 . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.3.3 Proofs of Theorems 3.7 and 3.8 . . . . . . . . . . . . . . . . . . . . . . . . . . 40"}, {"heading": "5 Discussion 45", "text": "5.1 The Case without Assumption 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.2 Off-policy TD(\u03bb) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.3 Open Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nReferences 49\nAppendix 51\nAppendix A: Key Properties of Trace Iterates 52\n3"}, {"heading": "1 Introduction", "text": "We consider discounted finite state and action Markov decision processes (MDPs) and the problem of learning an approximate value function for a given policy from off-policy data, that is, from data due to a different policy. The first policy is called the target policy and the second the behavior policy. The case of on-policy learning, where the target and behavior policies are the same, has been well-studied and widely applied (see e.g., [41, 48] and the books [4, 44]). Off-policy learning provides additional flexibilities and is useful in many contexts. For example, one may want to avoid executing the target policy before estimating the potential risk for safety concerns, or one may want to learn value functions for many target policies in parallel from one exploratory behavior. These require off-policy learning. In addition, insofar as value functions (with respect to different reward/cost assignments) reflect statistical properties of future outcomes, off-policy learning can be used by an autonomous agent to build an experience-based internal model of the world in artificial intelligence applications [43]. Algorithms for off-policy learning are thus not only useful as modelfree computational methods for solving MDPs, but can also potentially be a step toward the goal of making autonomous agents capable of learning over a long life-time, facing a sequence of diverse tasks.\nIn this paper we focus on a new off-policy learning algorithm proposed recently by Sutton, Mahmood, and White [46]: the emphatic temporal-difference (TD) learning algorithm, or ETD(\u03bb). The algorithm is similar to the standard TD(\u03bb) algorithm with linear function approximation [41], but uses a novel scheme to resolve a long-standing divergence problem in TD(\u03bb) when applied to off-policy data. Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]). The difficulty is intrinsic to sampling states according to an arbitrary distribution. Since then alternative algorithms without convergence issues have been sought for off-policy learning. In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.1 In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques. (See the surveys [13, 10] for other algorithm examples.)\nCompared to the algorithms just mentioned, ETD(\u03bb) is closer to the standard TD(\u03bb) algorithm and addresses the issue in TD(\u03bb) more directly. It introduces a novel weighting scheme to re-weight the states when forming the eligibility traces in TD(\u03bb), so that the weights reflect the occupation frequencies of the target policy rather than the behavior policy. An important result of this weighting scheme is that under natural conditions on the function approximation architecture, the average dynamics of ETD(\u03bb) can be described by an affine function involving a negative definite matrix [46, 52],2 which provides a desired stability property, similar to the case of convergent on-policy TD algorithms.\nThe almost sure convergence of ETD(\u03bb), under general off-policy training conditions, has been shown in our recent work [52] for diminishing stepsize. That result, however, requires the stepsize to diminish at the rate of O(1/t), with t being the time index of the iterate sequence. This range of stepsize is too narrow for applications. In practice, algorithms tend to make progress too slowly if the stepsize becomes too small, and the environment may be non-stationary, so it is often preferred to use a much larger stepsize or constant stepsize.\n1An efficient algorithm for solving the estimated equations is the one given in [50] based on the line search method. It can also be applied to finding approximate solutions under additional penalty terms suggested by [33].\n2The papers [46, 28] work with the negation of the matrix that we associate with ETD(\u03bb) in this paper. The negative definiteness property we discuss here corresponds to the positive definiteness property discussed in [46, 28]."}, {"heading": "4 Weak Convergence Properties of Constrained ETD Learning", "text": "The purpose of this paper is to provide an analysis of ETD(\u03bb) for a broad range of stepsizes. Specifically, we consider constant stepsize and stepsize that can diminish at a rate much slower than O(1/t). We will maintain general off-policy training conditions, without placing restrictions on the behavior policy. However, we will consider constrained versions of ETD(\u03bb), which constrain the iterates to be in a bounded set, and a mode of convergence that is weaker than almost sure convergence. Constraining the ETD(\u03bb) iterates is not only needed in analysis, but also a means to control the variances of the iterates, which is important in practice since off-policy learning algorithms generally have high variances. Almost sure convergence is no longer guaranteed for algorithms using large stepsizes; hence we analyze their behavior with respect to a weaker convergence mode.\nWe study a simple, basic version of constrained ETD(\u03bb) and several variations of it, some of which are biased but can mitigate the variance issue better. To give an overview of our results, we shall refer to the first algorithm as the unbiased algorithm, and its biased variations as the biased variants. Two groups of results will be given to characterize the asymptotic behavior of the trajectory of iterates produced by these algorithms. The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19]. The results show (roughly speaking) that:\n(i) In the case of diminishing stepsize, under mild conditions, the trajectory of iterates produced by the unbiased algorithm eventually spends nearly all its time in an arbitrarily small neighborhood of the desired solution, with an arbitrarily high probability (Theorem 3.1); and the trajectory produced by the biased algorithms has a similar behavior, when the algorithmic parameters are set to make the biases sufficiently small (Theorem 3.3). These results entail the convergence in mean to the desired solution for the unbiased algorithm (Cor. 3.1), and the convergence in probability to some vicinity of the desired solution for the biased variants.\n(ii) In the case of constant stepsize, imagine that we run the algorithms for all stepsizes; then conclusions similar to those in (i) hold in the limit as the stepsize parameter approaches zero (Theorems 3.2, 3.4). In particular, a smaller stepsize parameter results in an increasingly longer segment of the trajectory to spend, with an increasing probability, nearly all its time in some neighborhood of the desired solution. The size of the neighborhood can be made arbitrarily small as the stepsize parameter approaches zero and, in the case of the biased variants, also as their biases are reduced.\nThe next group of results are for the constant-stepsize case and complement the results in (ii) by focusing on the asymptotic behavior of the algorithms for a fixed stepsize. Among others, they show (roughly speaking) that:\n(iii) For any given stepsize parameter, asymptotically, the expected maximal deviation of multiple consecutive averaged iterates from the desired solution can be bounded in terms of the masses that the invariant probability measures of certain associated Markov chains assign to a small neighborhood of the desired solution. Those probability masses approach one when the stepsize parameter approaches zero and, in the case of the biased variants, also when their biases are sufficiently small (Theorems 3.5, 3.6).\n(iv) For a perturbed version of the unbiased algorithm and its biased variants, the maximal deviation of averaged iterates from the desired solution, under a given stepsize parameter, can be bounded almost surely in terms of those probability masses mentioned in (iii), for each initial condition (Theorems 3.7, 3.8).\nTo derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19]. This theory builds on the ordinary differential equation (ODE) based proof method, treats the trajectory of iterates as a whole, and studies its asymptotic behavior through the continuous-time processes corresponding to left-shifted and interpolated iterates. The probability distributions of these continuous-time interpolated processes are analyzed (as probability measures on a function space) by the weak convergence methods, leading to a characterization of their limiting distributions, from which asymptotic properties of the\n5 trajectory of iterates can be obtained. Most of our efforts in the first part of our analysis are to prove that the constrained ETD(\u03bb) algorithms satisfy the conditions required by the general convergence theorems just mentioned. We prove this by using key properties of ETD(\u03bb) iterates, most importantly, the ergodicity and uniform integrability properties of the trace iterates, and the convergence of certain averaged processes which, intuitively speaking, describe the averaged dynamics of ETD(\u03bb). Some of these properties were established earlier in our work [52] when analyzing the almost sure convergence of ETD(\u03bb). Building upon that work, we prove the remaining properties needed in the analysis.\nTo derive the second group of results, we exploit the fact that in the case of constant stepsize, the iterates together with other random variables involved in the algorithms form weak Feller Markov chains, and such Markov chains have nice ergodicity properties. We use ergodic theorems for weak Feller Markov chains [29, 30], together with the properties of ETD(\u03bb) iterates and the convergence results we get from the weak convergence methods, in this second part of our analysis.\nBesides ETD(\u03bb), the analysis we give in the paper also applies to off-policy TD(\u03bb), when the divergence issue mentioned earlier is avoided by setting \u03bb sufficiently close to 1. The reason is that in that case the off-policy TD(\u03bb) iterates have the same properties as the ones used in our analysis of ETD(\u03bb) and therefore, the same conclusions hold for constrained versions of off-policy TD(\u03bb), regarding their asymptotic convergence properties for constant or slowly diminishing stepsize (these results are new, to our knowledge). Similarly, our analysis also applies directly to the ETD(\u03bb, \u03b2) algorithm, a variation of ETD(\u03bb) recently proposed by Hallak et al. [15].\nRegarding practical performance of the algorithms, the biased ETD variant algorithms are much more robust than the unbiased algorithm despite the latter\u2019s superior asymptotic convergence properties. (This is not a surprise, for the biased algorithms are in fact defined by using a well-known robustifying approach from stochastic approximation theory [19].) Their behavior is demonstrated by experiments in [28, 53]. In particular, [53] is our companion note for this paper and includes several simulation results to illustrate some of the theorems we give here regarding the behavior of multiple consecutive iterates of the biased algorithms.\nThe paper is organized as follows. In Section 2 we provide the background for the ETD(\u03bb) algorithm. In Section 3 we present our convergence results on constrained ETD(\u03bb) and several variants of it, and we give the proofs in Section 4. We conclude the paper in Section 5 with a brief discussion on direct applications of our convergence results to the off-policy TD(\u03bb) algorithm and the ETD(\u03bb, \u03b2) algorithm, as well as to ETD(\u03bb) under relaxed conditions, followed by a discussion on several open issues. In Appendix A we include the key properties of the ETD(\u03bb) trace iterates that are used in the analysis."}, {"heading": "2 Preliminaries", "text": "In this section we describe the policy evaluation problem in the off-policy case, the ETD(\u03bb) algorithm and its constrained version, and we also review the results from our prior work [52] that are needed in this paper."}, {"heading": "2.1 Off-policy Policy Evaluation", "text": "Let S = {1, . . . , N} be a finite set of states, and let A be a finite set of actions. Without loss of generality we assume that for all states, every action in A can be applied. If a \u2208 A is applied at state s \u2208 S, the system moves to state s\u2032 with probability p(s\u2032 | s, a) and yields a random reward with mean r(s, a, s\u2032) and bounded variance, according to a probability distribution q(\u00b7 | s, a, s\u2032). These are the parameters of the MDP model we consider; they are unknown to the learning algorithms to be introduced.\nA stationary policy is a time-invariant decision rule that specifies the probability of taking an action at each state. When actions are taken according to such a policy, the states and actions"}, {"heading": "6 Weak Convergence Properties of Constrained ETD Learning", "text": "(St, At) at times t \u2265 0 form a (time-homogeneous) Markov chain on the space S \u00d7 A, with the marginal state process {St} being also a Markov chain.\nLet \u03c0 and \u03c0o be two given stationary policies, with \u03c0(a | s) and \u03c0o(a | s) denoting the probability of taking action a at state s under \u03c0 and \u03c0o, respectively. While the system evolves under the policy \u03c0o, generating a stream of state transitions and rewards, we wish to use these observations to evaluate the performance of the policy \u03c0, with respect to a discounted reward criterion, the definition of which will be given shortly. Here \u03c0 is the target policy and \u03c0o the behavior policy. It is allowed that \u03c0o 6= \u03c0 (the off-policy case), provided that at each state, all actions taken by \u03c0 can also be taken by \u03c0o (cf. Assumption 2.1(ii) below).\nLet \u03b3(s) \u2208 [0, 1], s \u2208 S, be state-dependent discount factors, with \u03b3(s) < 1 for at least one state. We measure the performance of \u03c0 in terms of the expected discounted total rewards attained under \u03c0 as follows: for each state s \u2208 S,\nv\u03c0(s) := E \u03c0\n[\nR0 + \u221e \u2211\nt=1\n\u03b3(S1) \u03b3(S2) \u00b7 \u00b7 \u00b7 \u03b3(St) \u00b7Rt \u2223 \u2223 \u2223 S0 = s\n]\n, (2.1)\nwhere Rt is the random reward received at time t, and E \u03c0 denotes expectation with respect to the probability distribution of the states, actions and rewards, (St, At, Rt), t \u2265 0, generated under the policy \u03c0. The function v\u03c0 on S is called the value function of \u03c0. The special case of \u03b3 being a constant less than 1 corresponds to the \u03b3-discounted reward criterion: v\u03c0(s) = E \u03c0 [ \u2211\u221e t=0 \u03b3 tRt | S0 = s]. In the general case, by letting \u03b3 depend on the state, the formulation is able to also cover certain undiscounted total reward MDPs with termination;3 however, for v\u03c0 to be well-defined (i.e., to have the right-hand side of (2.1) well-defined for each state), a condition on the target policy is needed, which is stated below and will be assumed throughout the paper.\nLet P\u03c0 denote the transition matrix of the Markov chain on S induced by \u03c0. Let \u0393 denote the N \u00d7N diagonal matrix with diagonal entries \u03b3(s), s \u2208 S.\nAssumption 2.1 (conditions on the target and behavior policies).\n(i) The target policy \u03c0 is such that (I \u2212 P\u03c0\u0393)\u22121 exists. (ii) The behavior policy \u03c0o induces an irreducible Markov chain on S, and moreover, for all (s, a) \u2208\nS \u00d7A, \u03c0o(a | s) > 0 if \u03c0(a | s) > 0.\nUnder Assumption 2.1(i), the value function v\u03c0 in (2.1) is well-defined, and furthermore, v\u03c0 satisfies uniquely the Bellman equation4\nv\u03c0 = r\u03c0 + P\u03c0\u0393 v\u03c0, i.e., v\u03c0 = (I \u2212 P\u03c0\u0393)\u22121r\u03c0,\nwhere r\u03c0 is the expected one-stage reward function under \u03c0 (i.e., r\u03c0(s) = E \u03c0[R0 | S0 = s] for s \u2208 S)."}, {"heading": "2.2 The ETD(\u03bb) Algorithm", "text": "Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s)\n\u22a4\u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 Rn and n-dimensional feature representations \u03c6(s) for the states. (Here \u03c6(s) is a column vector and \u22a4 stands for transpose.) In matrix notation, denote by \u03a6 the N \u00d7 n matrix with \u03c6(s)\u22a4, s \u2208 S, as its rows.\n3We may view v\u03c0(s) as the expected (undiscounted) total rewards attained under \u03c0 starting from the state s and up to a random termination time \u03c4 \u2265 1 that depends on the states in a Markovian way. In particular, if at time t \u2265 1, the state is s and termination has not occurred yet, the probability of \u03c4 = t (terminating at time t) is 1\u2212 \u03b3(s). Then v\u03c0(s) can be equivalently written as v\u03c0(s) = E\u03c0 [ \u2211\u03c4\u22121 t=0 Rt | S0 = s ]\n. 4One can verify this Bellman equation directly. It also follows from the standard MDP theory, as by definition v\u03c0 here can be related to a value function in a discounted MDP where the discount factors depend on state transitions, similar to discounted semi-Markov decision processes (see e.g., [37]).\n7 Then the columns of \u03a6 span the subspace of approximate value functions, and the approximation problem is to find in that subspace a function v = \u03a6\u03b8 \u2248 v\u03c0.\nWe focus on a general form of the ETD(\u03bb) algorithm, which uses state-dependent \u03bb values specified by a function \u03bb : S \u2192 [0, 1]. Inputs to the algorithm are the states, actions and rewards, {(St, At, Rt)}, generated under the behavior policy \u03c0o, where Rt is the random reward received upon the transition from state St to St+1 with action At. The algorithm can access the following functions, in addition to the features \u03c6(s):\n(i) the state-dependent discount factor \u03b3(s) that defines v\u03c0 , as described earlier;\n(ii) \u03bb : S \u2192 [0, 1], which determines the single or multi-step Bellman equation for the algorithm (cf. the subsequent Eqs. (2.6)-(2.7) and Footnote 6);\n(iii) \u03c1 : S \u00d7A \u2192 R+ given by \u03c1(s, a) = \u03c0(a | s)/\u03c0o(a | s) (with 0/0 = 0), which gives the likelihood ratios for action probabilities that can be used to compensate for sampling states and actions according to the behavior policy \u03c0o instead of the target policy \u03c0; (iv) i : S \u2192 R+, which gives the algorithm additional flexibility to weigh states according to the degree of \u201cinterest\u201d indicated by i(s). The algorithm also uses a sequence \u03b1t > 0, t \u2265 0, as stepsize parameters. We shall consider only deterministic {\u03b1t}.\nTo simplify notation, let\n\u03c1t = \u03c1(St, At), \u03b3t = \u03b3(St), \u03bbt = \u03bb(St).\nETD(\u03bb) calculates recursively \u03b8t \u2208 Rn, t \u2265 0, according to\n\u03b8t+1 = \u03b8t + \u03b1t et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t ) , (2.2)\nwhere et \u2208 Rn, called the \u201celigibility trace,\u201d is calculated together with two nonnegative scalar iterates (Ft,Mt) according to 5\nFt = \u03b3t \u03c1t\u22121 Ft\u22121 + i(St), (2.3) Mt = \u03bbt i(St) + (1\u2212 \u03bbt)Ft, (2.4) et = \u03bbt \u03b3t \u03c1t\u22121 et\u22121 +Mt \u03c6(St). (2.5)\nFor t = 0, (e0, F0, \u03b80) are given as an initial condition of the algorithm. We recognize that the iteration (2.2) has the same form as TD(\u03bb), but the trace et is calculated differently, involving an \u201cemphasis\u201d weight Mt on the state St, which itself evolves along with the iterate Ft, called the \u201cfollow-on\u201d trace. If Mt is always set to 1 regardless of Ft and i(\u00b7), then the iteration (2.2) reduces to the off-policy TD(\u03bb) algorithm in the case where \u03b3 and \u03bb are constants."}, {"heading": "2.3 Associated Bellman Equations and Approximation and Convergence Properties of ETD(\u03bb)", "text": "Let \u039b denote the diagonal matrix with diagonal entries \u03bb(s), s \u2208 S. Associated with ETD(\u03bb) is a generalized multistep Bellman equation of which v\u03c0 is the unique solution [42]: 6\nv = r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v. (2.6)\n5The definition (2.5) we use here differs slightly from the original definition of et in [46], but the two are equivalent and (2.5) appears to be more convenient for our analysis.\n6For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46]. We remark that similar to the standard one-step Bellman equation, which is a recursive relation that expresses v\u03c0 in terms of the expected one-stage reward and the expected total future rewards given by v\u03c0 itself, one can use the strong Markov property to derive other recursive relations satisfied by v\u03c0, in which the expected one-stage reward is replaced by the expected rewards attained by \u03c0 up to some random stopping time. This gives rise to a general class of Bellman equations, of which (2.6) is one example. Earlier works on using such equations in TD learning include [42] and [4, Chap. 5.3]. The recent work [49] considers an even broader class of Bellman equations using the concept"}, {"heading": "8 Weak Convergence Properties of Constrained ETD Learning", "text": "Here P\u03bb\u03c0,\u03b3 is an N \u00d7 N substochastic matrix, r\u03bb\u03c0,\u03b3 \u2208 RN is a vector of expected discounted total rewards attained by \u03c0 up to some random time depending on the function \u03bb, and they can be expressed in terms of P\u03c0 and r\u03c0 as\nP\u03bb\u03c0,\u03b3 = I \u2212 (I \u2212 P\u03c0\u0393\u039b)\u22121 (I \u2212 P\u03c0\u0393), r\u03bb\u03c0,\u03b3 = (I \u2212 P\u03c0\u0393\u039b)\u22121 r\u03c0. (2.7)\nETD(\u03bb) aims to solve a projected version of the Bellman equation (2.6) [46], which takes the following forms in the space of approximate value functions and in the space of the \u03b8-parameters, respectively:\nv = \u03a0 ( r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v ) , v \u2208 column-space(\u03a6), \u21d0\u21d2 C\u03b8 + b = 0, \u03b8 \u2208 Rn. (2.8)\nHere \u03a0 is a projection onto the approximation subspace with respect to a weighted Euclidean norm or seminorm, under a condition on the approximation architecture that will be explained shortly. The weights that define this norm also define the diagonal entries M\u0304ss, s \u2208 S, of a diagonal matrix M\u0304 , which are given by\ndiag(M\u0304) = d\u22a4\u03c0o,i(I \u2212 P\u03bb\u03c0,\u03b3)\u22121, with d\u03c0o,i \u2208 RN , d\u03c0o,i(s) = d\u03c0o(s) \u00b7 i(s), s \u2208 S, (2.9)\nwhere d\u03c0o(s) > 0 denotes the steady state probability of state s for the behavior policy \u03c0 o, under Assumption 2.1(ii). For the corresponding linear equation in the \u03b8-space in (2.8),\nC = \u2212\u03a6\u22a4M\u0304 (I \u2212 P\u03bb\u03c0,\u03b3)\u03a6, b = \u03a6\u22a4M\u0304 r\u03bb\u03c0,\u03b3 . (2.10)\nFrom the expression (2.9) of the diagonal matrix M\u0304 , the most important difference between the earlier TD algorithms and ETD(\u03bb) can be seen. For on-policy TD(\u03bb), in stead of (2.9), the diagonal matrix M\u0304 is determined by the steady state probabilities of the states under the target policy \u03c0 under an ergodicity assumption [48], and for off-policy TD(\u03bb), it is determined by the steady state probabilities d\u03c0o(s) under the behavior policy \u03c0\no. Here, due to the emphatic weighting scheme (2.3)-(2.5), the diagonals of M\u0304 given by (2.9) reflect the occupation frequencies (with respect to P\u03bb\u03c0,\u03b3) of the target policy rather than the behavior policy.\nLet | \u00b7 | denote the (unweighted) Euclidean norm. The matrix C is said to be negative definite if there exists c > 0 such that \u03b8\u22a4C\u03b8 \u2264 \u2212c|\u03b8|2 for all \u03b8 \u2208 Rn; and negative semidefinite if in the preceding inequality c = 0. A salient property of ETD(\u03bb) is that the matrix C is always negative semidefinite [46], and under natural and mild conditions, C is negative definite. This is proved in [52] and summarized below.\nCall those states s with M\u0304ss > 0 emphasized states (define this set of states to be empty if M\u0304 given by (2.9) is ill-defined, a case we will not encounter).\nAssumption 2.2 (condition on the approximation architecture). The set of feature vectors of emphasized states, {\u03c6(s) | s \u2208 S, M\u0304ss > 0}, contains n linearly independent vectors.\nTheorem 2.1 ([52, Prop. C.2]). Under Assumption 2.1, the matrix C is negative definite if and only if Assumption 2.2 holds.\nAssumption 2.2, which implies the linear independence of the columns of \u03a6, is satisfied in particular if the set of feature vectors, {\u03c6(s) | s \u2208 S, i(s) > 0}, contains n linearly independent vectors,\nof estimating equations from statistics, and the recent work [55] focuses on a special class of generalized Bellman equations and discusses their potential advantages from an approximation viewpoint. But an in-depth study of the application of such equations is still lacking currently. Because generalized Bellman equations offer flexible ways to address the bias vs. variance problem in learning the value functions of a policy, they are especially important and deserve further study, in our opinion.\n9 since states with positive interest i(s) are among the emphasized states.7 So this assumption can be easily satisfied in reinforcement learning without model knowledge.8\nIn view of Theorem 2.1, under Assumptions 2.1-2.2, the equation C\u03b8+b = 0 has a unique solution \u03b8\u2217; equivalently, \u03a6\u03b8\u2217 is the unique solution to the projected Bellman equation (2.7):\n\u03a6\u03b8\u2217 = \u03a0 ( r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 \u03a6\u03b8 \u2217 ) ,\nwhere \u03a0 is a well-defined projection operator that projects a vector in RN onto the approximation subspace with respect to the seminorm on RN given by\n\u221a\n\u2211\ns\u2208S M\u0304ss \u00b7 v(s)2, \u2200 v \u2208 RN\n(which is a norm if M\u0304ss > 0 for all s \u2208 S). The relation between the approximate value function v = \u03a6\u03b8\u2217 and the desired value function v\u03c0, in particular, the approximation error, can be characterized by using the oblique projection viewpoint [40] for projected Bellman equations.9\nThe almost sure convergence of ETD(\u03bb) to \u03b8\u2217 is proved in [52, Theorem 2.2] under Assumptions 2.1 and 2.2, for diminishing stepsize satisfying \u03b1t = O(1/t) and\n\u03b1t\u2212\u03b1t+1 \u03b1t\n= O(1/t). Despite this convergence guarantee, the stepsize range is too narrow for applications, as we discussed in the introduction. In this paper we will focus on constrained ETD(\u03bb) algorithms that restrict the \u03b8-iterates in a bounded set, but can operate with much larger stepsizes and also suffer less from the issue of high variance in off-policy learning. We will analyze their behavior under Assumptions 2.1 and 2.2, although our analysis extends to the case without Assumption 2.2 (see the discussion in Section 5.1)."}, {"heading": "2.4 Constrained ETD(\u03bb), Averaged Processes and Mean ODE", "text": "We consider first a constrained version of ETD(\u03bb) that simply scales the \u03b8-iterates, if necessary, to keep them bounded:\n\u03b8t+1 = \u03a0B\n(\n\u03b8t + \u03b1t et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t )\n)\n, (2.11)\nwhere \u03a0B is the Euclidean projection onto a closed ball B \u2282 Rn at the origin with radius rB : B = {\u03b8 \u2208 Rn | |\u03b8| \u2264 rB}. Under Assumptions 2.1 and 2.2, when the radius rB is sufficiently large (greater than the threshold given in Lemma 2.1 below), from any given initial (e0, F0, \u03b80), the algorithm (2.11) converges almost surely to \u03b8\u2217, for diminishing stepsize \u03b1t = O(1/t) [52, Theorem 4.1].\nOur interest in this paper is to apply (2.11) with a much larger range of stepsize, in particular, constant stepsize or stepsize that diminishes much more slowly than O(1/t). In Sections 3 and 4, we\n7This follows from the definition (2.9) of the diagonals M\u0304ss. Since (I \u2212P\u03bb\u03c0,\u03b3) \u22121 = I + \u2211\u221e k=1(P \u03bb \u03c0,\u03b3) k \u2265 I, we have\ndiag(M\u0304 ) = d\u22a4\u03c0o,i(I \u2212 P \u03bb \u03c0,\u03b3) \u22121 \u2265 d\u22a4\u03c0o,i. Hence i(s) > 0 implies M\u0304ss \u2265 d\u03c0o (s) \u00b7 i(s) > 0. 8There is another way to verify Assumption 2.2 without calculating M\u0304 . Suppose ETD(\u03bb) starts from a state S0 with i(S0) > 0. Then it can be shown that if St = s and Mt > 0, we must have M\u0304ss > 0. This means that as soon as we find among states St with emphasis weights Mt > 0 n states that have linearly independent feature vectors, we can be sure that Assumption 2.2 is satisfied.\n9Briefly speaking, [40] showed that the solutions of projected Bellman equations are oblique projections of v\u03c0 on the approximation subspace. An oblique projection is defined by two nonorthogonal subspaces of equal dimensions and is the projection onto the first subspace orthogonally to the second [39]. In the special case of ETD(\u03bb), the first of these two subspaces is the approximation subspace {v \u2208 RN | v = \u03a6\u03b8 for some \u03b8 \u2208 Rn}, and the second is the image of the approximation subspace under the linear transformation (I \u2212 P\u03bb\u03c0,\u03b3)\n\u22a4M\u0304 . Essentially it is the angle between the two subspaces that determines the approximation bias \u03a6\u03b8\u2217 \u2212 \u03a0v\u03c0 in the worst case, for a worst-case choice of r\u03bb\u03c0,\u03b3 . (For details, see also [55, Sec. 2.2].) Recently, for the case of constant \u03bb, i and \u03b3, [15] derived bounds on the approximation bias that are based on contraction arguments and are comparable to the bound for on-policy TD(\u03bb) [48]. These bounds lie above the bounds given by the oblique projection view (cf. [54] and [55, Sec. 2.2]); however, they are expressed in terms of \u03bb and \u03b3, so they give us explicit numbers instead of analytical expressions to bound the approximation bias."}, {"heading": "10 Weak Convergence Properties of Constrained ETD Learning", "text": "will analyze the algorithm (2.11) and its two variants for such stepsizes. To prepare for the analysis, in the rest of this section, we review several results from [52] that will be needed.\nFirst, we discuss about the \u201cmean ODE\u201d that we wish to associate with (2.11). It is the projected ODE x\u0307 = h\u0304(x) + z, z \u2208 \u2212NB(x), (2.12) where the function h\u0304 is the left-hand side of the equation Cx+ b = 0 we want to solve:\nh\u0304(x) = Cx+ b; (2.13)\nNB(x) is the normal cone of B at x, i.e.,\nNB(x) = {0} for x in the interior of B, NB(x) = {ax | a \u2265 0} for x on the boundary of B;\nand z is the boundary reflection term that cancels out the component of h\u0304(x) in NB(x) (i.e., z = \u2212y where y is the projection of h\u0304(x) on NB(x)), and it is the \u201cminimal force\u201d needed to keep the solution x(\u00b7) of (2.12) in B [19, Chap. 4.3].\nThe negative definiteness of the matrix C ensures that when the radius of B is sufficiently large, the boundary reflection term is zero for all x \u2208 B and the projected ODE (2.12) has no stationary points other than \u03b8\u2217 (see [52, Sec. 4.1] for a simple proof):\nLemma 2.1. Let c > 0 be such that x\u22a4Cx \u2264 \u2212c|x|2 for all x \u2208 Rn. Suppose B has a radius rB > |b|/c. Then \u03b8\u2217 lies in the interior of B; a solution x(\u03c4), \u03c4 \u2208 [0,\u221e), to the projected ODE (2.12) for an initial condition x(0) \u2208 B coincides with the unique solution to x\u0307 = h\u0304(x), with the boundary reflection term being z(\u00b7) \u2261 0; and the only solution x(\u03c4), \u03c4 \u2208 (\u2212\u221e,+\u221e), of (2.12) in B is x(\u00b7) \u2261 \u03b8\u2217.\nInformally speaking, suppose we have proved that (2.12) is the mean ODE for the algorithm (2.11) under stepsizes of our interest. Then applying powerful convergence theorems from the stochastic approximation theory [19], we can assert that the iterates \u03b8t will eventually \u201cfollow closely\u201d a solution of the mean ODE. This together with the solution property of the mean ODE given in Lemma 2.1 will then give us a characterization of the asymptotic behavior of the algorithm (2.11) for a constraint set B with sufficiently large radius.\nSeveral properties of the ETD(\u03bb) iterates will be important in proving that (2.12) is indeed the mean ODE for (2.11) and reflects its average dynamics. We now discuss two such properties (other key properties will be given in Appendix A). They concern the ergodicity of the Markov chain {(St, At, et, Ft)} on the joint space of states, actions and traces, and the convergence of certain averaged sequences associated with the algorithm (2.11). They will also be useful in analyzing variants of (2.11).\nLet Zt = (St, At, et, Ft), t \u2265 0. It is shown in [52] that under Assumption 2.1, {Zt} is a weak Feller Markov chain10 on the infinite state space S \u00d7 A \u00d7 Rn+1 and is ergodic. Specifically, on a metric space, a sequence of probability measures {\u00b5t} is said to converge weakly to a probability measure \u00b5 if for any bounded continuous function f , \u222b fd\u00b5t \u2192 \u222b\nfd\u00b5 as t \u2192 \u221e [12, Chap. 9.3]. We are interested in the weak convergence of the occupation probability measures of the process {Zt}, where for each initial condition Z0 = z, the occupation probability measures \u00b5z,t, t \u2265 0, are defined by \u00b5z,t(D) = 1 t+1 \u2211t k=0 1(Zk \u2208 D) for any Borel subset D of S \u00d7 A\u00d7 Rn+1, with 1(\u00b7) denoting the indicator function.\nTheorem 2.2 (ergodicity of {Zt}; [52, Theorem 3.2]). Under Assumption 2.1, the Markov chain {Zt} has a unique invariant probability measure \u03b6, and for each initial condition Z0 = z, the sequence {\u00b5z,t} of occupation probability measures converges weakly to \u03b6, almost surely.\n10See Section 4.3.1 or the book [30, Chap. 6] for the definition and properties of weak Feller Markov chains.\n11\nLet E\u03b6 denote expectation with respect to the stationary process {Zt} with \u03b6 as its initial distribution. By the definition of weak convergence, the weak convergence of {\u00b5z,t} given in Theorem 2.2 implies that for each given initial condition of Z0, the averages 1 t \u2211t\u22121 k=0 f(Zk) converge almost surely to E\u03b6{f(Z0)} for any bounded continuous function f .11 To study the average dynamics of the algorithm (2.11), however, we need to also consider unbounded functions. In particular, the function related to both (2.11) and the unconstrained ETD(\u03bb) is h : Rn \u00d7 \u039e \u2192 Rn,\nh(\u03b8, \u03be) = e \u00b7 \u03c1(s, a) ( r(s, a, s\u2032) + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 ) , (2.14)\nwhere\n\u03be = (e, F, s, a, s\u2032) \u2208 \u039e := Rn+1 \u00d7 S \u00d7A\u00d7 S.\nWriting \u03bet for the traces and transition at time t: \u03bet = (et, Ft, St, At, St+1), we can express the recursion (2.11) equivalently as\n\u03b8t+1 = \u03a0B ( \u03b8t + \u03b1t h(\u03b8t, \u03bet) + \u03b1t et \u00b7 \u03c9\u0303t+1 ) , (2.15)\nwhere \u03c9\u0303t+1 = \u03c1t (Rt \u2212 r(St, At, St+1)) is the noise part of the observed reward. The convergence to h\u0304(\u03b8) of the averaged sequence 1t \u2211t\u22121 k=0 h(\u03b8, \u03bek), with \u03b8 held fixed and t going to infinity, will be needed to prove that (2.12) is the mean ODE of (2.11). Since h\u0304(\u03b8) = C\u03b8+ b, this convergence for each fixed \u03b8 can be identified with the convergence of the matrix and vector iterates calculated by ELSTD(\u03bb) (the least-squares version of ETD(\u03bb)) to approximate the left-hand side of the equation C\u03b8 + b = 0. It was proved in [52] as a special case of the convergence of averaged sequences for a larger set of functions including h(\u03b8, \u00b7). Since this general result will be needed in analyzing variants of (2.11), we give its formulation here.\nThroughout the rest of the paper, we let \u2016 \u00b7 \u2016 denote the infinity norm of a Euclidean space, and we use this notation for both vectors and matrices (viewed as vectors). For Rm-valued random variables Xt, we say {Xt} converges to a random variable X in mean if E[\u2016Xt\u2212X\u2016] \u2192 0 as t \u2192 \u221e.\nConsider a vector-valued function g : \u039e \u2192 Rm such that with \u03be = (e, F, s, a, s\u2032), g(\u03be) is Lipschitz continuous in (e, F ) uniformly in (s, a, s\u2032). That is, there exists a finite constant Lg such that for any (e, F ), (e\u0302, F\u0302 ) \u2208 Rn+1, \u2225\n\u2225g(e, F, s, a, s\u2032)\u2212 g(e\u0302, F\u0302 , s, a, s\u2032) \u2225 \u2225 \u2264 Lg \u2225 \u2225(e, F )\u2212 (e\u0302, F\u0302 ) \u2225 \u2225, \u2200 (s, a, s\u2032) \u2208 S \u00d7 A\u00d7 S. (2.16)\nFor each \u03b8 \u2208 Rn, the function h(\u03b8, \u00b7) in (2.14) is a special case of g. The convergence of the averaged sequence 1t \u2211t\u22121 k=0 g(\u03bek) is given in the theorem below; the part on convergence in mean will be used frequently later in this paper (and was actually also needed in [52] to prove the ergodicity of {Zt} given earlier). The convergence of 1t \u2211t\u22121 k=0 h(\u03b8, \u03bek) then follows as a special case.\nTheorem 2.3 (convergence of averaged sequences; [52, Theorems 3.1-3.3]). Let g be a vector-valued function satisfying the Lipschitz condition (2.16). Then under Assumption 2.1, E\u03b6 [ \u2016g(\u03be0)\u2016 ]\n< \u221e and for any given initial (e0, F0) \u2208 Rn+1, as t \u2192 \u221e, 1t \u2211t\u22121 k=0 g(\u03bek) converges to g\u0304 = E\u03b6 [ g(\u03be0) ]\nin mean and almost surely.\nCorollary 2.1 ([52, Theorem 2.1]). Under Assumption 2.1, for the functions h\u0304, h given in (2.13), (2.14) respectively, the following hold: For each \u03b8 \u2208 Rn, E\u03b6 [ \u2016h(\u03b8, \u03be0)\u2016 ] < \u221e and h\u0304(\u03b8) = E\u03b6 [ h(\u03b8, \u03be0) ] ; and for any given initial (e0, F0) \u2208 Rn+1, as t \u2192 \u221e, 1t \u2211t\u22121\nk=0 h(\u03b8, \u03bek) converges to h\u0304(\u03b8) in mean and almost surely.\n11With the usual discrete topology for the finite space S \u00d7A and the usual topology for the Euclidean space Rn+1, the space S \u00d7A\u00d7 Rn+1 equipped with the product topology is metrizable. A continuous function f(s, a, e, F ) on this space is a function that is continuous in (e, F ) for each (s, a) \u2208 S \u00d7A."}, {"heading": "12 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "3 Convergence Results for Constrained ETD(\u03bb)", "text": "In this section we present the convergence properties of the constrained ETD(\u03bb) algorithm (2.11) and several variants of it, for constant stepsize and for stepsize that diminishes slowly. We will explain briefly how the results are obtained, leaving the detailed analyses to Section 4. The first set of results about the algorithm (2.11) will be given first in Section 3.1, followed by similar results in Section 3.2 for two variant algorithms that have biases but can mitigate the variance issue in off-policy learning better. These results are obtained through applying two general convergence theorems from [19], which concern weak convergence of stochastic approximation algorithms for diminishing and constant stepsize. Finally, the constant-stepsize case will be analyzed further in Section 3.3, in order to refine some results of Sections 3.1-3.2 so that the asymptotic behavior of the algorithms for a fixed stepsize can be characterized explicitly. In that subsection, besides the three algorithms just mentioned, we will also discuss another variant algorithm with perturbation.\nRegarding notation, recall that 1(\u00b7) is the indicator function, |\u00b7| stands for the usual (unweighted) Euclidean norm and \u2016 \u00b7 \u2016 the infinity norm for Rm. We denote by N\u03b4(D) the \u03b4-neighborhood of a set D \u2282 Rm: N\u03b4(D) = {x \u2208 Rm | infy\u2208D |x \u2212 y| \u2264 \u03b4}, and we write N\u03b4(\u03b8\u2217) for the \u03b4-neighborhood of \u03b8\u2217. For the iteration index t, the notation t \u2208 [k1, k2] or t \u2208 [k1, k2) will be used to mean that the range of t is the set of integers in the interval [k1, k2] or [k1, k2). More definitions and notation will be introduced later where they are needed."}, {"heading": "3.1 Main Results", "text": "We consider first the algorithm (2.11) for diminishing stepsize. Let the stepsize change slowly in the following sense.\nAssumption 3.1 (condition on diminishing stepsize). The (deterministic) nonnegative sequence {\u03b1t} satisfies that \u2211 t\u22650 \u03b1t = \u221e, \u03b1t \u2192 0 as t \u2192 \u221e, and for some sequence of integers mt \u2192 \u221e,\nlim t\u2192\u221e sup 0\u2264j\u2264mt\n\u2223 \u2223 \u2223 \u2223 \u03b1t+j \u03b1t \u2212 1 \u2223 \u2223 \u2223 \u2223 = 0. (3.1)\nThe condition (3.1) is the condition A.8.2.8 in [19, Chap. 8] and allows stepsizes much larger than O(1/t). We can have \u03b1t = O(t\n\u2212\u03b2), \u03b2 \u2208 (0, 1], and even larger stepsizes are possible. For example, partition the time interval [0,\u221e) into increasingly longer intervals Ik, k \u2265 0, and set \u03b1t to be constant within each interval Ik. Then the condition (3.1) can be fulfilled by letting the constants for each Ik decrease as O(k\n\u2212\u03b2), \u03b2 \u2208 (0, 1]. We now state the convergence result. For any T > 0, let m(k, T ) = min{t \u2265 k | \u2211t+1j=k \u03b1j > T }. If we draw a continuous timeline and put each iteration of the algorithm at a specific moment, with the stepsize \u03b1j being the length of time between iterations j and j + 1, then m(k, T ) is the latest iteration before time T has elapsed since the k-th iteration. If \u03b1t = O(t\n\u2212\u03b2), \u03b2 \u2208 (0, 1], for example, then for fixed T , there are O(k\u03b2) iterates between the k-th and m(k, T )-th iteration.\nTheorem 3.1 (convergence properties of constrained ETD with diminishing stepsize). Suppose Assumptions 2.1, 2.2 hold and the radius of B exceeds the threshold given in Lemma 2.1. Let {\u03b8t} be generated by the algorithm (2.11) with stepsize {\u03b1t} satisfying Assumption 3.1, from any given initial condition (e0, F0). Then there exists a sequence Tk \u2192 \u221e such that for any \u03b4 > 0,\nlim sup k\u2192\u221e\nP ( \u03b8t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k, m(k, Tk) ] ) = 0.\nThis theorem implies \u03b8t \u2192 \u03b8\u2217 in probability. Since {\u03b8t} is bounded, by [12, Theorem 10.3.6], \u03b8t must also converge to \u03b8\u2217 in mean:\n13\nCorollary 3.1 (convergence in mean). In the setting of Theorem 3.1, E [ \u2016\u03b8t \u2212 \u03b8\u2217\u2016 ] \u2192 0 as t \u2192 \u221e.\nAnother important note is that the conclusion of Theorem 3.1 is much stronger than that \u03b8t \u2192 \u03b8\u2217 in probability. Here as k \u2192 \u221e, we consider an increasingly longer segment [k,m(k, Tk)] of iterates, and are able to conclude that the probability of that entire segment being inside an arbitrarily small neighborhood of \u03b8\u2217 approaches 1. (This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.)\nIn the case of constant stepsize, we consider all the trajectories that can be produced by the algorithm (2.11) using some constant stepsize, and we ask what the properties of these trajectories are in the limit as the stepsize parameter approaches 0. Here there is a common timeline used in relating trajectories generated with different stepsizes (and it comes from the ODE-based analysis): we imagine again a continuous timeline, along which we put the iterations at moments that are evenly separated in time by \u03b1, if the stepsize parameter is \u03b1. The scalars T, T\u03b1 in the theorem below represent amounts of time with respect to this continuous timeline.\nTheorem 3.2 (convergence properties of constrained ETD with constant stepsize). Suppose Assumptions 2.1, 2.2 hold and the radius of B exceeds the threshold given in Lemma 2.1. For each \u03b1 > 0, let {\u03b8\u03b1t } be generated by the algorithm (2.11) with constant stepsize \u03b1, from any given initial condition (e0, F0). Let {k\u03b1 | \u03b1 > 0} be any sequence of nonnegative integers that are nondecreasing as \u03b1 \u2192 0. Then the following hold: (i) For any \u03b4 > 0,\nlim T\u2192\u221e lim \u03b1\u21920\n1\nT/\u03b1\nk\u03b1+\u230aT/\u03b1\u230b \u2211\nt=k\u03b1\n1 ( \u03b8\u03b1t \u2208 N\u03b4(\u03b8\u2217) ) = 1 in probability.\n(ii) Let \u03b1k\u03b1 \u2192 \u221e as \u03b1 \u2192 0. Then there exists a sequence {T\u03b1 | \u03b1 > 0} with T\u03b1 \u2192 \u221e as \u03b1 \u2192 0, such that for any \u03b4 > 0,\nlim sup \u03b1\u21920\nP (\n\u03b8\u03b1t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k\u03b1, k\u03b1 + T\u03b1/\u03b1 ]\n)\n= 0.\nPart (ii) above is similar to Theorem 3.1. Here as \u03b1 \u2192 0, an increasingly longer segment [k\u03b1, k\u03b1+T\u03b1/\u03b1] of the tail of the trajectory {\u03b8\u03b1t } is considered, and it is concluded that the probability of that entire segment being inside an arbitrarily small neighborhood of \u03b8\u2217 approaches 1. Part (i) above, roughly speaking, says that as \u03b1 diminishes, within the segment [k\u03b1, k\u03b1 + T/\u03b1], the fraction of iterates \u03b8\u03b1t that lie in a small \u03b4-neighborhood of \u03b8 \u2217 approaches 1 for sufficiently large T .\nWe give the proofs of Theorems 3.1-3.2 in Section 4.1. As mentioned earlier, most of our efforts will be to use the properties of ETD iterates to show that the conditions of two general convergence theorems from stochastic approximation theory [19, Theorems 8.2.2, 8.2.3] are satisfied by the algorithm (2.11). After that we can specialize the conclusions of those theorems to obtain Theorems 3.1-3.2. Specifically, after furnishing their conditions, applying [19, Theorems 8.2.2, 8.2.3] will give us directly the desired conclusions in Theorems 3.1-3.2 with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217), where N\u03b4(LB) is the \u03b4-neighborhood of the limit set LB for the projected ODE (2.12). This limit set is defined as follows:\nLB := \u2229\u03c4\u0304>0 \u222ax(0)\u2208B{x(\u03c4), \u03c4 \u2265 \u03c4\u0304}\nwhere x(\u03c4) is a solution of the projected ODE (2.12) with initial condition x(0), the union is over all the solutions with initial x(0) \u2208 B, and D for a set D denotes taking the closure of D. It can be shown that LB = {\u03b8\u2217} under our assumptions, so Theorems 3.1-3.2 will then follow as special cases of [19, Theorems 8.2.2, 8.2.3]."}, {"heading": "14 Weak Convergence Properties of Constrained ETD Learning", "text": "Remark 3.1 (on weak convergence methods). The theorems from [19] which we will apply are based on the weak convergence methods. While it is beyond the scope of this paper to explain these powerful methods, let us mention here a few basic facts about them to elucidate the origin of the convergence theorems we gave above. In the framework of [19], one studies a trajectory of iterates produced by an algorithm by working with continuous-time processes that are piecewise constant or linear interpolations of the iterates. (Often one also left-shifts a trajectory of iterates to bring the \u201casymptotic part\u201d of the trajectory closer to the origin of the continuous time axis.) In the case of our problem, for example, for diminishing stepsize, these continuous-time processes are xk(\u03c4), \u03c4 \u2208 [0,\u221e), indexed by k \u2265 0, where for each k, xk is a piecewise constant interpolation of \u03b8k+t, t \u2265 0, given by xk(\u03c4) = \u03b8k for \u03c4 \u2208 [0, \u03b1k) and xk(\u03c4) = \u03b8k+t for \u03c4 \u2208 [ \u2211t\u22121 m=0 \u03b1k+m, \u2211t m=0 \u03b1k+m), t \u2265 1. Similarly, for constant stepsize, the continuous-time processes involved are x\u03b1(\u03c4), \u03c4 \u2208 [0,\u221e), indexed by \u03b1 > 0, and for each \u03b1, x\u03b1 is a piecewise constant interpolation of \u03b8\u03b1k\u03b1+t, t \u2265 0, given by x\u03b1(\u03c4) = \u03b8k\u03b1+t for \u03c4 \u2208 [t\u03b1, (t+1)\u03b1). The behavior of the sequence {xk} or {x\u03b1} as k \u2192 \u221e or \u03b1 \u2192 0, tells us the asymptotic properties of the algorithm as the number of iterations grows to infinity or as the stepsize parameter approaches 0. With the weak convergence methods, one considers the probability distributions of the continuous-time processes in such sequences, and analyze the convergence of these probability distributions and their limiting distributions along any subsequences. Here each continuous-time process takes values in a space of vector-valued functions on [0,\u221e) or (\u2212\u221e,\u221e) that are rightcontinuous and have left-hand limits, and this function space equipped with an appropriate metric, known as the Skorohod metric, is a complete separable metric space [19, p. 238-240]. On this space, one analyzes the weak convergence of the probability distributions of the continuous-time processes. Under certain conditions on the algorithm, the general conclusions from [19, Theorems 8.2.2, 8.2.3] are that any subsequence of these probability distributions contains a further subsequence which is convergent, and that all the limiting probability distributions must assign the full measure 1 to the set of solutions of the mean ODE associated with the algorithm. This general weak convergence property then yields various conclusions about the asymptotic behavior of the algorithm and its relation with the mean ODE solutions. When further combined with the solution properties of the mean ODE, it leads to specific results such as the theorems we give in this section."}, {"heading": "3.2 Two Variants of Constrained ETD(\u03bb) with Biases", "text": "We now consider two simple variants of (2.11). They constrain the ETD iterates even more, at a price of introducing biases in this process, so that unlike (2.11), they can no longer get to \u03b8\u2217 arbitrarily closely. Instead they aim at a small neighborhood of \u03b8\u2217, the size of which depends on how they modify the ETD iterates. On the other hand, because the trace iterates {(et, Ft)} can have unbounded variances and are also naturally unbounded in common off-policy situations (see discussions in [51, Prop. 3.1 and Footnote 3, p. 3320-3322] and [52, Remark A.1, p. 23]), these variant algorithms have the advantage that they make the \u03b8-iterates more robust against the drastic changes that can occur to the trace iterates. Indeed our definition of the variant algorithms below follows a well-known approach to \u201crobustifying\u201d algorithms in stochastic approximation theory (see discussions in [19, p. 23 and p. 141]).\nThe two variant algorithms are defined as follows. For each K > 0, let \u03c8K : R n \u2192 Rn be a\nbounded Lipschitz continuous function such that\n\u2016\u03c8K(x)\u2016 \u2264 \u2016x\u2016 \u2200x \u2208 Rn, and \u03c8K(x) = x if \u2016x\u2016 \u2264 K. (3.2)\n(For instance, let \u03c8K(x) = r\u0304x/|x| if |x| \u2265 r\u0304 and \u03c8K(x) = x otherwise, for r\u0304 = \u221a nK; or let \u03c8K(x) be the result of truncating each component of x to be within [\u2212K,K].) For the first variant of the algorithm (2.11), we replace et in (2.11) by \u03c8K(et):\n\u03b8t+1 = \u03a0B\n(\n\u03b8t + \u03b1t \u03c8K(et) \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t )\n)\n. (3.3)\n15\nFor the second variant, we apply \u03c8K to bound the entire increment in (2.11) before it is multiplied by the stepsize \u03b1t and added to \u03b8t:\n\u03b8t+1 = \u03a0B (\u03b8t + \u03b1t \u03c8K(Yt)) , where Yt = et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t ) . (3.4)\nAs will be proved later, these two algorithms are associated with mean ODEs of the form,\nx\u0307 = h\u0304K(x) + z, z \u2208 \u2212NB(x), (3.5)\nwhere h\u0304K : R n \u2192 Rn is determined by each algorithm and deviates from the function h\u0304(x) = Cx+ b due to the alterations introduced by \u03c8K . This ODE is similar to the projected ODE (2.12), except that since h\u0304K is an approximation of h\u0304, \u03b8\n\u2217 is no longer a stable or stationary point for the mean ODE (3.5). The two variant algorithms thus have a bias in their \u03b8-iterates, and the bias can be made smaller by choosing a larger K. This is reflected in the two convergence theorems given below. They are similar to the previous two theorems for the algorithm (2.11), except that now given a desired small neighborhood of \u03b8\u2217, a sufficiently large K needs to be used in order for the \u03b8-iterates to reach that neighborhood of \u03b8\u2217 and exhibit properties similar to those shown in the previous case.\nTheorem 3.3 (convergence properties of constrained ETD variants with diminishing stepsize). In the setting of Theorem 3.1, let {\u03b8t} be generated instead by the algorithm (3.3) or (3.4), with a bounded Lipschitz continuous function \u03c8K satisfying (3.2), and with stepsize {\u03b1t} satisfying Assumption 3.1. Then for each \u03b4 > 0, there exists K\u03b4 > 0 such that if K \u2265 K\u03b4, then it holds for some sequence Tk \u2192 \u221e that\nlim sup k\u2192\u221e\nP ( \u03b8t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k, m(k, Tk) ] ) = 0.\nTheorem 3.4 (convergence properties of constrained ETD variants with constant stepsize). In the setting of Theorem 3.2, let {\u03b8\u03b1t } be generated instead by the algorithm (3.3) or (3.4), with a bounded Lipschitz continuous function \u03c8K satisfying (3.2) and with constant stepsize \u03b1 > 0. Let {k\u03b1 | \u03b1 > 0} be any sequence of nonnegative integers that are nondecreasing as \u03b1 \u2192 0. Then for each \u03b4 > 0, there exists K\u03b4 > 0 such that the following hold if K \u2265 K\u03b4: (i)\nlim T\u2192\u221e lim \u03b1\u21920\n1\nT/\u03b1\nk\u03b1+\u230aT/\u03b1\u230b \u2211\nt=k\u03b1\n1 ( \u03b8\u03b1t \u2208 N\u03b4(\u03b8\u2217) ) = 1 in probability.\n(ii) Let \u03b1k\u03b1 \u2192 \u221e as \u03b1 \u2192 0. Then there exists a sequence {T\u03b1 | \u03b1 > 0} with T\u03b1 \u2192 \u221e as \u03b1 \u2192 0, such that\nlim sup \u03b1\u21920\nP (\n\u03b8\u03b1t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k\u03b1, k\u03b1 + T\u03b1/\u03b1 ]\n)\n= 0.\nWe give the proofs of the above two theorems in Section 4.2. The arguments are largely the same as those that we will use first in Section 4.1 to prove Theorems 3.1-3.2 for the algorithm (2.11). Indeed, for all the three algorithms, the main proof step is the same, which is to apply the general conclusions of [19, Theorems 8.2.2, 8.2.3] to establish the connection between the iterates of an algorithm and the solutions of an associated mean ODE, and this step does not concern what the solutions of the ODE are actually. (For the two variant algorithms, verifying that the conditions of [19, Theorems 8.2.2, 8.2.3] are met is, in fact, easier than for the algorithm (2.11), because various functions involved in the analysis become bounded due to the use of the bounded function \u03c8K .) For the two variant algorithms, the result of this step is that the same conclusions given in Theorems 3.1- 3.2 hold with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217), where LB is the limit set of the projected mean ODE (3.5) associated with each variant algorithm. To attain Theorems 3.3-3.4, we then combine this with the fact that by choosing K sufficiently large, one can make the limit set LB \u2282 N\u03b4(\u03b8\u2217) for an arbitrarily small \u03b4."}, {"heading": "16 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "3.3 More about the Constant-stepsize Case", "text": "For the constant-stepsize case, the results given in Theorems 3.2 and 3.4 bear similarities to their counterparts for the diminishing stepsize case given in Theorems 3.1 and 3.3. However, they characterize the behavior of the iterates in the limit as the stepsize parameter approaches 0, and deal with only a finite segment of the iterates for each stepsize (although in their part (ii) both the segment\u2019s length T\u03b1/\u03b1 \u2192 \u221e and its starting position k\u03b1 \u2192 \u221e as \u03b1 \u2192 0). So unlike in the diminishing stepsize case, these results do not tell us explicitly about the behavior of \u03b8\u03b1t for a fixed stepsize \u03b1 as we take t to infinity.\nThe purpose of the present subsection is to analyze further the case of a fixed stepsize just mentioned. We observe that for a fixed stepsize \u03b1, the iterates \u03b8\u03b1t together with Zt = (St, At, et, Ft) form a weak Feller Markov chain {(Zt, \u03b8\u03b1t )} (see Lemma 4.3). Thus we can apply several ergodic theorems for weak Feller Markov chains (Meyn [29], Meyn and Tweedie [30]) to analyze the constant-stepsize case and combine the implications from these theorems with the results we obtained previously using stochastic approximation theory.\nWe now present our results using this approach. Let M\u03b1 denote the set of invariant probability measures of the Markov chain {(Zt, \u03b8\u03b1t )}. This set depends on the particular algorithm used to generate the \u03b8-iterates, but we shall use the notation M\u03b1 for all the algorithms we discuss here, for notational simplicity. We know that {Zt} has a unique invariant probability measure (Theorem 2.2), but it need not be so for the Markov chain {(Zt, \u03b8\u03b1t )} when {\u03b8\u03b1t } is generated by the algorithm (2.11) or its two variants. The set M\u03b1 can therefore have multiple elements (it is nonempty; see Prop. 4.8). We denote by M\u0304\u03b1 the set that consists of the marginal of \u00b5 on B (the space of the \u03b8\u2019s), for all the invariant probability measures \u00b5 \u2208 M\u03b1.\nAs in the previous analysis, we are interested in the behavior of multiple consecutive \u03b8-iterates. In order to characterize that, we consider for each m \u2265 1, the Markov chain\n{(\n(Zt, \u03b8 \u03b1 t ), (Zt+1, \u03b8 \u03b1 t+1), . . . , (Zt+m\u22121, \u03b8 \u03b1 t+m\u22121)\n)}\nt\u22650\n(i.e., each state now consists of m consecutive states of the chain {(Zt, \u03b8\u03b1t )}). We shall refer to this chain as the m-step version of {(Zt, \u03b8\u03b1t )}. Similar to M\u03b1, denote by Mm\u03b1 the set of invariant probability measures of the m-step version of {(Zt, \u03b8\u03b1t )}, and correspondingly define M\u0304m\u03b1 to be the set of marginals of \u00b5 on Bm for all \u00b5 \u2208 Mm\u03b1 . The set Mm\u03b1 is, of course, determined by M\u03b1, since each invariant probability measure in Mm\u03b1 is just the m-dimensional distribution of a stationary Markov chain {(Zt, \u03b8\u03b1t )}.\nOur first result, given in Theorem 3.5 below, says that for the algorithm (2.11), as the stepsize \u03b1 approaches zero, the invariant probability measures in Mm\u03b1 will concentrate their masses on an arbitrarily small neighborhood of (\u03b8\u2217, . . . , \u03b8\u2217) (m copies of \u03b8\u2217). Moreover, for a fixed stepsize, as the number of iterations grows to infinity, the expected maximal deviation of the m consecutive averaged iterates from \u03b8\u2217 can be bounded in terms of the masses those invariant probability measures assign to the vicinities of (\u03b8\u2217, . . . , \u03b8\u2217). Here by averaged iterates, we mean\n\u03b8\u0304\u03b1t = 1\nt\nt\u22121 \u2211\nk=0\n\u03b8\u03b1k , \u2200 t \u2265 1, (3.6)\nand we shall refer to {\u03b8\u0304\u03b1t } as the averaged sequence corresponding to {\u03b8\u03b1t }. This iterative averaging is also known as \u201cPolyak-averaging\u201d when it is applied to accelerate the convergence of the \u03b8-iterates (see [34], [19, Chap. 10], and the references therein). This is not the role of the averaging operation here, however. The purpose here is to bring to bear the ergodic theorems for weak Feller Markov chains, in particular, the weak convergence of certain averaged probability measures or occupation probability measures to the invariant probability measures of the m-step version of {(Zt, \u03b8\u03b1t )}. (For the details see Section 4.3, where the proofs of the results of this subsection will be given.) It can also be seen that for a sequence {\u03b2t} with \u03b2t \u2208 [0, 1), \u03b2t \u2192 0 as t \u2192 \u221e, if we drop a fraction \u03b2t of\n17\nthe terms in (3.6) when averaging the \u03b8\u2019s at each time t, the resulting differences in the averaged iterates \u03b8\u0304\u03b1t are asymptotically negligible. Therefore, although our results below will be stated for (3.6), they apply to a variety of averaging schemes.\nRecall that N\u03b4(\u03b8 \u2217) denotes the closed \u03b4-neighborhood of \u03b8\u2217. In what follows, N \u2032\u03b4(\u03b8 \u2217) denotes the open \u03b4-neighborhood of \u03b8\u2217, i.e., the open ball around \u03b8\u2217 with radius \u03b4. We write [N\u03b4(\u03b8\n\u2217)]m or [N \u2032\u03b4(\u03b8 \u2217)]m for the Cartesian product of m copies of N\u03b4(\u03b8 \u2217) or N \u2032\u03b4(\u03b8\n\u2217). Recall also that rB is the radius of the constraint set B.\nTheorem 3.5. In the setting of Theorem 3.2, let {\u03b8\u03b1t } be generated by the algorithm (2.11) with constant stepsize \u03b1 > 0, and let {\u03b8\u0304\u03b1t } be the corresponding averaged sequence. Then the following hold for any \u03b4 > 0 and m \u2265 1: (i) lim inf\u03b1\u21920 inf\u00b5\u2208M\u0304m\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m ) = 1, and more strongly, with m\u03b1 = \u2308m\u03b1 \u2309,\nlim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\u03b1\n\u00b5 ( [N\u03b4(\u03b8 \u2217)]m\u03b1 ) = 1.\n(ii) For each stepsize \u03b1 and any initial condition of (e0, F0, \u03b8 \u03b1 0 ),\nlim sup k\u2192\u221e E\n[\nsup k\u2264t<k+m\n\u2223 \u2223\u03b8\u0304\u03b1t \u2212 \u03b8\u2217 \u2223 \u2223\n]\n\u2264 \u03b4 \u03ba\u03b1,m + 2rB (1 \u2212 \u03ba\u03b1,m),\nwhere \u03ba\u03b1,m = inf\u00b5\u2208M\u0304m\u03b1 \u00b5([N \u2032 \u03b4(\u03b8 \u2217)]m).\nNote that in part (ii) above, \u03ba\u03b1,m \u2192 1 as \u03b1 \u2192 0 by part (i). Note also that for m = 1, the conclusions from the preceding theorem take the simplest form:\nlim inf \u03b1\u21920 inf \u00b5\u2208M\u0304\u03b1\n\u00b5 ( N\u03b4(\u03b8 \u2217) ) = 1,\nlim sup t\u2192\u221e\nE [ \u2223 \u2223\u03b8\u0304\u03b1t \u2212 \u03b8\u2217 \u2223 \u2223 ] \u2264 \u03b4 \u03ba\u03b1 + 2rB (1\u2212 \u03ba\u03b1), for \u03ba\u03b1 = inf \u00b5\u2208M\u0304\u03b1 \u00b5 ( N \u2032\u03b4(\u03b8 \u2217) ) .\nThe conclusions for m > 1 are, however, much stronger. They also suggest that in practice, instead of simply choosing the last iterate of the algorithm as its final output at the end of its run, one can base that choice on the behavior of multiple consecutive \u03b8\u0304\u03b1t during the run.\nFor the two variant algorithms (3.3) and (3.4), we have a similar result given in Theorem 3.6 below. Here the neighborhood of (\u03b8\u2217, . . . , \u03b8\u2217) around which the masses of the invariant probability measures are concentrated, depends not only on the stepsize \u03b1 but also on the biases of these algorithms. The proofs of Theorems 3.5-3.6 are given in Section 4.3.2.\nTheorem 3.6. In the setting of Theorem 3.2, let {\u03b8\u03b1t } be generated instead by the algorithm (3.3) or (3.4), with constant stepsize \u03b1 > 0 and with a bounded Lipschitz continuous function \u03c8K satisfying (3.2). Let {\u03b8\u0304\u03b1t } be the corresponding averaged sequence. Then the following hold: (i) For any given \u03b4 > 0, there exists K\u03b4 > 0 such that for all K \u2265 K\u03b4,\nlim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\n\u00b5 ( [N\u03b4(\u03b8 \u2217)]m ) = 1, \u2200m \u2265 1,\nand more strongly, with m\u03b1 = \u2308m\u03b1 \u2309, lim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m\u03b1 ) = 1, \u2200m \u2265 1.\n(ii) Regardless of the choice of K, given any \u03b4 > 0,m \u2265 1 and stepsize \u03b1, for each initial condition of (e0, F0, \u03b8 \u03b1 0 ),\nlim sup k\u2192\u221e E\n[\nsup k\u2264t<k+m\n\u2223 \u2223\u03b8\u0304\u03b1t \u2212 \u03b8\u2217 \u2223 \u2223\n]\n\u2264 \u03b4 \u03ba\u03b1,m + 2rB (1 \u2212 \u03ba\u03b1,m),\nwhere \u03ba\u03b1,m = inf\u00b5\u2208M\u0304m\u03b1 \u00b5([N \u2032 \u03b4(\u03b8 \u2217)]m)."}, {"heading": "18 Weak Convergence Properties of Constrained ETD Learning", "text": "Finally, we consider a simple modification of the preceding algorithms, for which the conclusions of Theorems 3.5(ii) and 3.6(ii) can be strengthened. This is our motivation for introducing the modification, but we shall postpone the discussion till Remark 3.2 at the end of this subsection.\nFor any of the algorithms (2.11), (3.3) or (3.4), if the original recursion under a constant stepsize \u03b1 can be written as\n\u03b8\u03b1t+1 = \u03a0B ( \u03b8\u03b1t + \u03b1Y \u03b1 t ) ,\nwe now modify this recursion formula by adding a perturbation term \u03b1\u2206\u03b1\u03b8,t as follows. Let\n\u03b8\u03b1t+1 = \u03a0B ( \u03b8\u03b1t + \u03b1Y \u03b1 t + \u03b1\u2206 \u03b1 \u03b8,t ) , (3.7)\nwhere for each \u03b1 > 0, \u2206\u03b1\u03b8,t, t \u2265 0, are Rn-valued random variables such that12 (i) they are independent of each other and also independent of the process {Zt}; (ii) they are identically distributed with zero mean and finite variance, where the variance can be\nbounded uniformly for all \u03b1; and\n(iii) they have a positive continuous density function with respect to the Lebesgue measure.\nBelow we refer to (3.7) as the perturbed version of the algorithm (2.11), (3.3) or (3.4).\nTheorem 3.7. In the setting of Theorem 3.2, let {\u03b8\u03b1t } be generated instead by the perturbed version (3.7) of the algorithm (2.11) for a constant stepsize \u03b1 > 0, and let {\u03b8\u0304\u03b1t } be the corresponding averaged sequence. Then the conclusions of Theorems 3.2 and 3.5 hold. Furthermore, let the stepsize \u03b1 be given. Then the Markov chain {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure \u00b5\u03b1, and for any \u03b4 > 0, m \u2265 1, and initial condition of (e0, F0, \u03b8\u03b10 ), almost surely,\nlim inf t\u2192\u221e\n1\nt\nt\u22121 \u2211\nk=0\n1\n(\nsup k\u2264j<k+m\n\u2223 \u2223\u03b8\u03b1j \u2212 \u03b8\u2217 \u2223\n\u2223 < \u03b4 )\n\u2265 \u00b5\u0304(m)\u03b1 ( [N \u2032\u03b4(\u03b8 \u2217)]m )\nand lim sup t\u2192\u221e \u2223 \u2223\u03b8\u0304\u03b1t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 \u03b4 \u03ba\u03b1 + 2rB (1 \u2212 \u03ba\u03b1), with \u03ba\u03b1 = \u00b5\u0304\u03b1 ( N \u2032\u03b4(\u03b8 \u2217) ) ,\nwhere \u00b5\u0304 (m) \u03b1 is the unique element in M\u0304m\u03b1 , and \u00b5\u0304\u03b1 is the marginal of \u00b5\u03b1 on B.\nTheorem 3.8. In the setting of Theorem 3.2, let {\u03b8\u03b1t } be generated instead by the perturbed version (3.7) of the algorithm (3.3) or (3.4), with a constant stepsize \u03b1 > 0 and with a bounded Lipschitz continuous function \u03c8K satisfying (3.2). Let {\u03b8\u0304\u03b1t } be the corresponding averaged sequence. Then the conclusions of Theorems 3.4 and 3.6 hold. Furthermore, for any given stepsize \u03b1, the conclusions of the second part of Theorem 3.7 also hold.\nNote that in the second part of Theorem 3.7, both \u00b5\u0304 (m) \u03b1 ( [N \u2032\u03b4(\u03b8 \u2217)]m ) and \u03ba\u03b1 approach 1 as \u03b1 \u2192 0, since by the first part of the theorem, the conclusions of Theorem 3.5 hold. For the second part of Theorem 3.8, the same is true provided that K is sufficiently large (so that N\u03b4(LB) \u2282 N\u03b4(\u03b8\u2217) where LB is the limit set of the ODE associated with the algorithm), and this can be seen from the conclusions of Theorem 3.6(i), which holds for the perturbed version (3.7) of the two variant algorithms, as the first part of Theorem 3.8 says. The proofs of Theorems 3.7-3.8 are given in Section 4.3.3.\nRemark 3.2 (on the role of perturbation). At first sight it may seem counter-productive to add noise to the \u03b8-iterates in the algorithm (3.7). Our motivation for such random perturbations of the \u03b8-iterates is that this can ensure that the Markov chain {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure (see Prop. 4.11). The uniqueness allows us to invoke a result of Meyn [29] on the convergence\n12We adopt these conditions for simplicity. They are not the weakest possible for our purpose, and our proof techniques can be applied to other types of perturbations as well. See the discussions in Remark 3.2, Remark 4.1, and before Prop. 4.10 in Section 4.3.3.\n19\nof the occupation probability measures of a weak Feller Markov chain, so that we can bound the deviation of the averaged iterates from \u03b8\u2217 not only in an expected sense as before, but also for almost all sample paths under each initial condition, as in the second part of Theorems 3.7-3.8. For the unperturbed algorithms, we can only prove such pathwise bounds on lim supt\u2192\u221e |\u03b8\u0304\u03b1t \u2212 \u03b8\u2217| for a subset of the initial conditions of (Z0, \u03b8 \u03b1 0 ). A more detailed discussion of this is given in Remark 4.1, at the end of Section 4.3.3, after the proofs of the preceding theorems. Regarding other effects of the perturbation, intuitively, larger noise terms may help the Markov chain \u201cmix\u201d faster, but they can also result in less probability mass \u00b5\u0304\u03b1 ( N \u2032\u03b4(\u03b8 \u2217) )\naround \u03b8\u2217 than in the case without perturbation. What is a suitable amount of noise to add to achieve a desired balance? We do not yet have an answer. It seems reasonable to us to let the magnitude of the variance of the perturbation terms \u2206\u03b1\u03b8,t be approximately \u03b1\n2\u01eb for some \u01eb \u2208 (0, 1], so that a typical perturbation \u03b1\u2206\u03b1\u03b8,t is at a smaller scale relative to the \u201csignal part\u201d \u03b1Y \u03b1 t in an iteration. Further investigation is needed."}, {"heading": "4 Proofs for Section 3", "text": "We now prove the theorems given in the preceding section."}, {"heading": "4.1 Proofs for Theorems 3.1 and 3.2", "text": "In this subsection we prove Theorems 3.1 and 3.2 on convergence properties of the constrained ETD(\u03bb) algorithm (2.11). We will apply two theorems from [19], Theorems 8.2.2 and 8.2.3, which concern weak convergence of stochastic approximation algorithms for constant and diminishing stepsize, respectively. This requires us to show that the conditions of those theorems are satisfied by our algorithm. The major conditions concern the uniform integrability, tightness, and convergence in mean of certain sequences of random variables involved in the algorithm. Our proofs will rely on many properties of the ETD iterates that we have established in [52] when analyzing the almost sure convergence of the algorithm."}, {"heading": "4.1.1 Conditions to Verify", "text": "We need some definitions and notation, before describing the conditions required. For some index set K, let {Xk}k\u2208K be a set of random variables taking values in a metric space X (in our context X will be Rm or \u039e). The set {Xk}k\u2208K is said to be tight or bounded in probability, if there exists for each \u03b4 > 0 a compact set D\u03b4 \u2282 X such that\ninf k\u2208K\nP(Xk \u2208 D\u03b4) \u2265 1\u2212 \u03b4.\nFor Rm-valued Xk, the set {Xk}k\u2208K is said to be uniformly integrable (u.i.) if\nlim a\u2192\u221e sup k\u2208K\nE [ \u2016Xk\u2016 1 ( \u2016Xk\u2016 \u2265 a )] = 0.\nTo analyze the constrained ETD(\u03bb) algorithm (2.11), which is given by\n\u03b8t+1 = \u03a0B(\u03b8t + \u03b1tYt), where Yt := et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t ) ,\nlet Et denote expectation conditioned on Ft, the sigma-algebra generated by \u03b8m, \u03bem,m \u2264 t, where we recall \u03bem = (em, Fm, Sm, Am, Sm+1) and its space R\nn+1 \u00d7S \u00d7A\u00d7S is denoted by \u039e. By writing Yt = Et[Yt] + (Yt \u2212 Et[Yt]), we have the equivalent form of (2.11) given in (2.15):\n\u03b8t+1 = \u03a0B(\u03b8t + \u03b1t h(\u03b8t, \u03bet) + \u03b1t et \u00b7 \u03c9\u0303t+1) ."}, {"heading": "20 Weak Convergence Properties of Constrained ETD Learning", "text": "In other words, h(\u03b8t, \u03bet) = Et[Yt] and et \u00b7\u03c9\u0303t+1 = Yt\u2212Et[Yt], a noise term that satisfies Et[et \u00b7\u03c9\u0303t+1] = 0. This algorithm belongs to the class of stochastic approximation algorithms with \u201cexogenous noises\u201d studied in the book [19]\u2014the term \u201cexogenous noises\u201d reflects the fact that the evolution of {\u03bet} is not driven by the \u03b8-iterates. Theorems 3.1 and 3.2 will follow as special cases from Theorems 8.2.3 and 8.2.2 of [19, Chap. 8], respectively, if we can show that the algorithm (2.11) satisfies the following conditions. Conditions for the case of diminishing stepsize:\n(i) The sequence {Yt} = {h(\u03b8t, \u03bet) + et \u00b7 \u03c9\u0303t+1} is u.i. (This corresponds to the condition A.8.2.1 of [19].)\n(ii) The function h(\u03b8, \u03be) is continuous in \u03b8 uniformly in \u03be \u2208 D, for each compact set D \u2282 \u039e. (This corresponds to the condition A.8.2.3 of [19].)\n(iii) The sequence {\u03bet} is tight. (This corresponds to the condition A.8.2.4 of [19].) (iv) The sequence {h(\u03b8t, \u03bet)} is u.i., and so is {h(\u03b8, \u03bet)} for each fixed \u03b8 \u2208 B. (This corresponds to\nthe condition A.8.2.5 of [19].)\n(v) There is a continuous function h\u0304(\u00b7) such that for each \u03b8 \u2208 B and each compact set D \u2282 \u039e,\nlim k\u2192\u221e,t\u2192\u221e\n1\nk\nt+k\u22121 \u2211\nm=t\nEt\n[ h(\u03b8, \u03bem)\u2212 h\u0304(\u03b8) ] 1 ( \u03bet \u2208 D ) = 0 in mean,\nwhere k and t are taken to \u221e in any way possible. In other words, if we denote the average on the left-hand side by Xk,t, then the requirement \u201climk\u2192\u221e,t\u2192\u221e Xk,t = 0 in mean\u201d means that along any subsequences kj \u2192 \u221e, tj \u2192 \u221e, we must have limj\u2192\u221e E[\u2016Xkj,tj\u2016] = 0. (This condition corresponds to the condition A.8.2.7 of [19].)\nFor the case of constant stepsize, we consider the iterates that could be generated by the algorithm for all stepsizes. To distinguish between the iterates associated with different stepsizes, in the conditions below, the superscript \u03b1 is attached to the variables involved in the algorithm with stepsize \u03b1, and similarly, the conditional expectation Et is denoted by E \u03b1 t instead.\nConditions for the case of constant stepsize: In addition to the condition (ii) above (which corresponds to the condition A.8.1.6 of [19] for the case of constant stepsize), the following conditions are required.\n(i\u2032) The set {Y \u03b1t | t \u2265 0, \u03b1 > 0} := {h(\u03b8\u03b1t , \u03be\u03b1t ) + e\u03b1t \u00b7 \u03c9\u0303\u03b1t+1 | t \u2265 0, \u03b1 > 0} is u.i. (This corresponds to the condition A.8.1.1 of [19].) (iii\u2032) The set {\u03be\u03b1t | t \u2265 0, \u03b1 > 0} is tight. (This corresponds to the condition A.8.1.7 of [19].) (iv\u2032) The set {h(\u03b8\u03b1t , \u03be\u03b1t ) | t \u2265 0, \u03b1 > 0} is u.i., in addition to the uniform integrability of {h(\u03b8, \u03be\u03b1t ) |\nt \u2265 0, \u03b1 > 0} for each \u03b8 \u2208 B. (This corresponds to the condition A.8.1.8 of [19].) (v\u2032) There is a continuous function h\u0304(\u00b7) such that for each \u03b8 \u2208 B and each compact set D \u2282 \u039e,\nlim k\u2192\u221e,t\u2192\u221e,\u03b1\u21920\n1\nk\nt+k\u22121 \u2211\nm=t\nE \u03b1 t\n[ h(\u03b8, \u03be\u03b1m)\u2212 h\u0304(\u03b8) ] 1 ( \u03be\u03b1t \u2208 D ) = 0 in mean,\nwhere \u03b1 is taken to 0 and k, t are taken to \u221e in any way possible. (This condition corresponds to the condition A.8.1.9 of [19], and it is in fact stronger than the latter condition but is satisfied by our algorithms as we will show.)\nThe preceding conditions allow \u03be\u03b1t and \u03b8 \u03b1 t to be generated under different initial conditions for different \u03b1. While we will need this generality later in Section 4.3, here we will focus on a common initial condition for all stepsizes, for simplicity. Then, the preceding conditions for the constantstepsize case are essentially the same as those for the diminishing stepsize case, because except for the \u03b8-iterates, all the other variables (such as \u03bet and \u03c9\u0303t) involved in the algorithm have identical probability distributions for all stepsizes \u03b1 and are not affected by the \u03b8-iterates. For this reason, in\n21\nthe proofs below, except for the \u03b8-iterates, we simply omit the superscript \u03b1 for other variables in the case of constant stepsize, and to verify the two sets of conditions above, we shall treat the case of diminishing stepsize and the case of constant stepsize simultaneously.\nAs mentioned in Section 2.4, these conditions are to ensure that the projected ODE (2.12), x\u0307 = h\u0304(x) + z, z \u2208 \u2212NB(x), is the mean ODE for the algorithm (2.11) and reflects its average dynamics. Among the proofs for these conditions given next, the proof for the convergence in mean condition (v) and (v\u2032) will be the most involved."}, {"heading": "4.1.2 Proofs", "text": "The condition (ii) is clearly satisfied. In what follows, we prove that the rest of the conditions are satisfied as well. We start with the tightness conditions (iii) and (iii\u2032), as they are immediately implied by a property of the trace iterates we already know. We then tackle the uniform integrability conditions (i), (i\u2032), (iv) and (iv\u2032), before we address the convergence in mean required in (v) and (v\u2032). The proofs build upon several key properties of the ETD iterates we have established in [52] and recounted in Section 2.4 and Appendix A.\nFirst, we show that the tightness conditions (iii) and (iii\u2032) are satisfied. This is implied by the following property of traces: for any given initial condition (e0, F0), supt\u22650 E [ \u2225 \u2225(et, Ft) \u2225 \u2225 ]\n< \u221e (see Prop. A.1, Appendix A).\nProposition 4.1. Under Assumption 2.1, for each given initial (e0, F0) \u2208 Rn+1, {(et, Ft)} is tight and hence {\u03bet} is tight.\nProof. By Prop. A.1, c := supt\u22650 E [\u2225 \u2225(et, Ft) \u2225 \u2225 ] < \u221e. Then, by the Markov inequality, for a > 0, supt\u22650 P (\u2225 \u2225(et, Ft) \u2225 \u2225 \u2265 a )\n\u2264 c/a \u2192 0 as a \u2192 \u221e. This implies that {(et, Ft)} is tight. Since the space S \u00d7A\u00d7 S is finite and \u03bet = (et, Ft, St, At, St+1), {\u03bet} is also tight.\nWe now handle the uniform integrability conditions (i), (i\u2032), (iv) and (iv\u2032). The uniform integrability of the trace sequence {et}, as we will prove, is important here.\nProposition 4.2. Under Assumption 2.1, for each given initial (e0, F0) \u2208 Rn+1, the following sets of random variables are u.i.:\n(i) {et}; (ii) {h(\u03b8, \u03bet)} for each fixed \u03b8 \u2208 B; (iii) {h(\u03b8t, \u03bet)} in the case of diminishing stepsize; and {h(\u03b8\u03b1t , \u03bet) | t \u2265 0, \u03b1 > 0} in the case of\nconstant stepsize;\n(iv) {h(\u03b8t, \u03bet)+ et \u03c9\u0303t+1} in the case of diminishing stepsize; and {h(\u03b8\u03b1t , \u03bet)+ et \u03c9\u0303t+1 | t \u2265 0, \u03b1 > 0} in the case of constant stepsize.\nThe proof of this proposition will use facts about u.i. sequences of random variables given in the lemma below.\nLemma 4.1. Let Xk, Yk, k \u2208 K (some index set) be real-valued random variables with Xk and Yk defined on a common probability space for each k.\n(i) If {Xk}k\u2208K, {Yk}k\u2208K are u.i., then {Xk + Yk}k\u2208K is u.i. (ii) If {Xk}k\u2208K is u.i. and for all k, |Yk| \u2264 |Xk| a.s., then {Yk}k\u2208K is u.i. (iii) If {Xk}k\u2208K, {Yk}k\u2208K are u.i. and for some c \u2265 0, E[|Yk| | Xk] \u2264 c a.s. for all k, then\n{XkYk}k\u2208K is u.i.\nProof. Part (i) follows immediately from the definition of uniform integrability and the inequality that for any a > 0,\n|Xk + Yk| 1 ( |Xk + Yk| \u2265 a ) \u2264 2|Xk| 1 ( |Xk| \u2265 a2 ) + 2|Yk| 1 ( |Yk| \u2265 a2 ) ."}, {"heading": "22 Weak Convergence Properties of Constrained ETD Learning", "text": "Under the assumption of (ii), for any a > 0, we have |Yk|1 ( |Yk| \u2265 a ) \u2264 |Xk|1 ( |Xk| \u2265 a )\na.s., and (ii) is then evident.\nUnder the assumption of (iii), for any a > a\u0304 > 0, using the inequality\n|XkYk| 1 ( |XkYk| \u2265 a ) \u2264 |XkYk| 1 ( |Xk| \u2264 a\u0304, |Yk| \u2265 aa\u0304 ) + |XkYk| 1 ( |Xk| \u2265 a\u0304 ) ,\nwe have\nE [ |XkYk| 1 ( |XkYk| \u2265 a )] \u2264 E [ |XkYk| 1 ( |Xk| \u2264 a\u0304, |Yk| \u2265 aa\u0304 )] + E [ |XkYk| 1 ( |Xk| \u2265 a\u0304 )]\n\u2264 a\u0304 \u00b7 E [ |Yk| 1 ( |Yk| \u2265 aa\u0304 )] + E [ |Xk| 1 ( |Xk| \u2265 a\u0304 ) \u00b7 E [ |Yk| | Xk ]] \u2264 a\u0304 \u00b7 E [ |Yk| 1 ( |Yk| \u2265 aa\u0304 )] + c \u00b7 E [ |Xk| 1 ( |Xk| \u2265 a\u0304 )] , (4.1)\nwhere we used the assumption E[|Yk| | Xk] \u2264 c a.s. in the last inequality. Since {Xk}k\u2208K, {Yk}k\u2208K are assumed to be u.i., we have\nlim a\u2192\u221e sup k\u2208K\nE [ |Yk| 1 ( |Yk| \u2265 aa\u0304 )]\n= 0 for any given a\u0304 > 0, lim a\u0304\u2192\u221e sup k\u2208K\nE [ |Xk| 1 ( |Xk| \u2265 a\u0304 )] = 0.\n(4.2) From (4.1), by taking first a \u2192 \u221e and then a\u0304 \u2192 \u221e, and by using (4.2), we then obtain\nlim a\u2192\u221e sup k\u2208K\nE [ |XkYk| 1 ( |XkYk| \u2265 a )] = 0,\nand this proves (iii).\nWe now proceed to prove Prop. 4.2. The proof will involve auxiliary variables, which we call truncated traces. They are defined similarly to the trace iterates (et, Ft), but instead of depending on all the past states and actions, they only depend on a certain number of the most recent states and actions. Specifically, for each integer K \u2265 1, we define truncated traces (e\u0303t,K , F\u0303t,K) as follows:\n(e\u0303t,K , F\u0303t,K) = (et, Ft) for t \u2264 K, and for t \u2265 K + 1, with the shorthand \u03b2t := \u03c1t\u22121\u03b3t\u03bbt,\nF\u0303t,K = t \u2211\nk=t\u2212K\ni(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) , (4.3)\nM\u0303t,K = \u03bbt i(St) + (1\u2212 \u03bbt)F\u0303t,K , (4.4)\ne\u0303t,K =\nt \u2211\nk=t\u2212K\nM\u0303k,K \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) . (4.5)\nNote that when t \u2265 2K + 1, the traces (e\u0303t,K , F\u0303t,K) no longer depend on the initial (e0, F0); being functions of the states and actions between time t \u2212 2K and t only, they lie in a bounded set determined by K, since the state and action spaces are finite. For t = 0, . . . , 2K, (e\u0303t,K , F\u0303t,K) also lie in a bounded set, which is determined by K and the initial (e0, F0). We will use these bounded truncated traces to approximate the original traces {(et, Ft)} in the analysis.\nAn important approximation property, given in Prop. A.3 (Appendix A), is that for each K and any initial (e0, F0) from a given bounded set E,\nsup t\u22650 E\n[\n\u2225 \u2225(et, Ft)\u2212 (e\u0303t,K , F\u0303t,K) \u2225 \u2225\n]\n\u2264 LK ,\nwhere LK is a finite constant that depends on K and E and decreases monotonically to 0 as K increases: LK \u2193 0 as K \u2192 \u221e. We will use this property in the following analysis.\n23\nProof of Prop. 4.2. First, we prove {et} is u.i. We then use this to show the uniform integrability of the other sets required in parts (ii)-(iv).\n(i) To prove {et} is u.i., we shall exploit its relation with the truncated traces, e\u0303t,K , t \u2265 0 for integers K \u2265 1. Note that since the state and action spaces are finite, the truncated traces {e\u0303t,K} lie in a bounded set (this set depends on K and the initial (e0, F0)), so there exists a constant aK such that \u2016e\u0303t,K\u2016 \u2264 aK for all t. This fact will greatly simplify the analysis. Let us first fix K and consider a \u2265 ak. Denote a\u0304 = a\u2212 aK \u2265 0. Then\n\u2016et\u2016 1 ( \u2016et\u2016 \u2265 a ) \u2264 \u2016et\u2016 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 )\n\u2264 \u2016et \u2212 e\u0303t,K\u2016 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 ) + \u2016e\u0303t,K\u2016 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 ) \u2264 \u2016et \u2212 e\u0303t,K\u2016 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 ) + aK 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 ) . (4.6)\nFor the second term on the right-hand side, we can bound its expectation by\nE [ aK 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 )] = aKP(\u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304) \u2264 aK \u00b7 LK/a\u0304, \u2200 t, (4.7) where in the last inequality LK is a constant that depends on K (and the initial (e0, F0)) and has the property that LK \u2193 0 as K \u2192 \u221e, and this inequality is derived by combing the Markov inequality P(\u2016et\u2212 e\u0303t,K\u2016 \u2265 a\u0304) \u2264 E[\u2016et\u2212 e\u0303t,K\u2016]/a\u0304 with Prop. A.3, which bounds supt\u22650 E[\u2016et\u2212 e\u0303t,K\u2016] by LK . Similarly, for the first term on the right-hand side of (4.6), using Prop. A.3, we can bound its expectation by LK :\nE [ \u2016et \u2212 e\u0303t,K\u2016 1 ( \u2016et \u2212 e\u0303t,K\u2016 \u2265 a\u0304 )] \u2264 E[\u2016et \u2212 e\u0303t,K\u2016] \u2264 LK , \u2200 t. (4.8) From (4.6)-(4.8) it follows that\nsup t\u22650\nE [ \u2016et\u2016 1 ( \u2016et\u2016 \u2265 a )] \u2264 LK + aK \u00b7 LK/(a\u2212 aK),\nso for fixed K, by taking a \u2192 \u221e, we obtain lim a\u2192\u221e sup t\u22650 E [ \u2016et\u2016 1 ( \u2016et\u2016 \u2265 a )] \u2264 LK .\nSince LK \u2193 0 as K \u2192 \u221e (Prop. A.3), this implies lima\u2192\u221e supt\u22650 E [ \u2016et\u2016 1 ( \u2016et\u2016 \u2265 a )]\n= 0, which proves the uniform integrability of {et}. (ii) We now prove for each \u03b8, {h(\u03b8, \u03bet)} is u.i. Since the state and action spaces are finite and \u03b8 is given, using the expression of h(\u03b8, \u03bet), we can bound it as \u2016h(\u03b8, \u03bet)\u2016 \u2264 L\u2016et\u2016 for some constant L. As just proved, {et} is u.i. (equivalently {\u2016et\u2016} is u.i.) and thus {L\u2016et\u2016} is u.i., so by Lemma 4.1(ii), {h(\u03b8, \u03bet)} is u.i. (since this is by definition equivalent to {\u2016h(\u03b8, \u03bet)\u2016} being u.i., which is true by Lemma 4.1(ii)).\n(iii) The uniform integrability of {h(\u03b8t, \u03bet)} in the case of diminishing stepsize or {h(\u03b8\u03b1t , \u03bet) | t \u2265 0, \u03b1 > 0} in the case of constant stepsize follows from the same argument given for (ii) above, because \u03b8t or \u03b8 \u03b1 t for all t \u2265 0 and \u03b1 > 0 lie in the bounded set B by the definition of the constrained ETD(\u03bb) algorithm.\n(iv) Consider first the case of diminishing stepsize. We prove that {h(\u03b8t, \u03bet) + et \u03c9\u0303t+1} is u.i. (recall \u03c9\u0303t+1 = \u03c1t (Rt \u2212 r(St, At, St+1)) is the noise part of the observed reward). Since we already showed that {h(\u03b8t, \u03bet)} is u.i., by Lemma 4.1(i), it is sufficient to prove that {et \u03c9\u0303t+1} is u.i. Now {et} is u.i. by part (i). Since the random rewards Rt in our model have bounded variances, the noise variables \u03c9\u0303t+1, t \u2265 0, also have bounded variances. This implies that {\u03c9\u0303t+1} is u.i. [6, p. 32] and that E[|\u03c9\u0303t+1| | et] < c for some constant c (independent of t). It then follows from Lemma 4.1(iii) that {et \u03c9\u0303t+1} is u.i., and hence {h(\u03b8t, \u03bet) + et \u03c9\u0303t+1} is u.i.\nSimilarly, in the case of constant stepsize, it follows from Lemma 4.1(i) that the set {h(\u03b8\u03b1t , \u03bet) + et \u03c9\u0303t+1 | t \u2265 0, \u03b1 > 0} is u.i., because {h(\u03b8\u03b1t , \u03bet) | t \u2265 0, \u03b1 > 0} is u.i. by part (iii) proved earlier and {et \u03c9\u0303t+1} is u.i. as we just proved."}, {"heading": "24 Weak Convergence Properties of Constrained ETD Learning", "text": "Finally, we handle the conditions (v) and (v\u2032) stated in Section 4.1.1. The two conditions are the same condition in the case here, because they concern each fixed \u03b8, whereas {\u03bet} is not affected by the stepsize and the \u03b8-iterates. So we can focus just on the condition (v) in presenting the proof, for notational simplicity. For the algorithm (2.11), the continuous function h\u0304 required in the condition is the function h\u0304(\u03b8) = C\u03b8 + b associated with the desired mean ODE (2.12). We now prove the required convergence in mean by using the properties of trace iterates and the convergence results given in Theorem 2.3 and Cor. 2.1.\nProposition 4.3. Let Assumption 2.1 hold. For each \u03b8 \u2208 B and each compact set D \u2282 \u039e,\nlim k\u2192\u221e,t\u2192\u221e\n1\nk\nt+k\u22121 \u2211\nm=t\nEt\n[ h(\u03b8, \u03bem)\u2212 h\u0304(\u03b8) ] 1 ( \u03bet \u2208 D ) = 0 in mean.\nProof. Denote Xk,t = 1 k \u2211t+k\u22121 m=t ( h(\u03b8, \u03bem) \u2212 h\u0304(\u03b8) ) 1 ( \u03bet \u2208 D ) . Since E [\u2225 \u2225Et{Xk,t} \u2225 \u2225 ] \u2264 E[\u2016Xk,t\u2016], to prove limk,t E [\u2225 \u2225Et{Xk,t} \u2225 \u2225 ]\n= 0 (here and in what follows we simply write \u201ck, t\u201d under a limit symbol for \u201ck \u2192 \u221e, t \u2192 \u221e\u201d), it is sufficient to prove limk,t E[\u2016Xk,t\u2016] = 0, that is, to prove\nlim k,t\n1\nk\nt+k\u22121 \u2211\nm=t\n( h(\u03b8, \u03bem)\u2212 h\u0304(\u03b8) ) 1 ( \u03bet \u2208 D ) = 0 in mean. (4.9)\nFurthermore, since\nlim sup k,t\nE [ \u2016Xk,t\u2016 1 ( \u03bet \u2208 D )] \u2264 \u2211\n(s,a,s\u2032)\u2208S\u00d7A\u00d7S\nlim sup k,t\nE [ \u2016Xk,t\u2016 1 ( \u03bet \u2208 D, (St, At, St+1) = (s, a, s\u2032) )] ,\nit is sufficient in the proof to consider only those compact sets D of the form D = E \u00d7 {(s, a, s\u2032)}, for each compact set E \u2282 Rn+1 and each (s, a, s\u2032) \u2208 S \u00d7A\u00d7S. Henceforth, let us fix a compact set E together with a triplet (s, a, s\u2032) as the set D under consideration, and for this set D, we proceed to prove (4.9).\nTo show (4.9), what we need to show is that for two arbitrary subsequences of integers kj \u2192 \u221e, tj \u2192 \u221e,\nlim j\u2192\u221e\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bem)\u2212 h\u0304(\u03b8) ) 1 ( \u03betj \u2208 D ) = 0 in mean. (4.10)\nTo this end, we first define auxiliary trace variables to decompose each difference term h(\u03b8, \u03bem)\u2212h\u0304(\u03b8) into two difference terms as follows:\n(a) Fix a point (e\u0304, F\u0304 ) \u2208 E. (b) For each j \u2265 1, define a sequence of trace pairs, (ejm, F jm), m \u2265 tj , by using the same recursion\n(2.3)-(2.5) that defines the traces {(et, Ft)}, based on the same trajectory {(St, At)}, but starting at time m = tj with the initial (e j tj , F j tj ) = (e\u0304, F\u0304 ).\nDenote \u03bejm = (e j m, F j m, Sm, Am, Sm+1) for m \u2265 tj ; it differs from \u03bem only in the two trace components. Next, for each m, we write h(\u03b8, \u03bem) \u2212 h\u0304(\u03b8) = (h(\u03b8, \u03bejm) \u2212 h\u0304(\u03b8)) + (h(\u03b8, \u03bem) \u2212 h(\u03b8, \u03bejm)) and correspondingly, we write\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bem)\u2212 h\u0304(\u03b8) ) = 1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bejm)\u2212 h\u0304(\u03b8) )\n+ 1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bem)\u2212 h(\u03b8, \u03bejm) ) .\nWe see that for (4.10) to hold, it is sufficient that\nlim j\u2192\u221e\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bejm)\u2212 h\u0304(\u03b8) ) 1 ( \u03betj \u2208 D ) = 0 in mean, (4.11)\n25\nand\nlim j\u2192\u221e\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bem)\u2212 h(\u03b8, \u03bejm) ) 1 ( \u03betj \u2208 D ) = 0 in mean. (4.12)\nLet us now prove these two statements.\nProof of (4.11): Since the set D = E \u00d7 {(s, a, s\u2032)} and 1 ( \u03betj \u2208 D ) \u2264 1 ( (Stj , Atj , Stj+1) = (s, a, s \u2032) )\n, we can remove \u03betj from consideration and show instead\nlim j\u2192\u221e\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bejm)\u2212 h\u0304(\u03b8) ) 1 ( (Stj , Atj , Stj+1) = (s, a, s \u2032) ) = 0 in mean, (4.13)\nwhich will imply (4.11). By definition \u03bejm,m \u2265 tj , are generated from the initial trace pairs (e\u0304, F\u0304 ) and initial transition (Stj , Atj , Stj+1) at time m = tj . So if (Stj , Atj , Stj+1) = (s, a, s\n\u2032), then conditioned on this transition at tj , the sequence {\u03bejm,m \u2265 tj} has the same probability distribution as a sequence \u03be\u0302m,m \u2265 0, where \u03be\u0302m = (e\u0302m, F\u0302m, S\u0302m, A\u0302m, S\u0302m+1) is generated from the initial condition \u03be\u03020 = (e\u0304, F\u0304 , s, a, s\n\u2032) by the same recursion (2.3)-(2.5) and a trajectory {(S\u0302m, A\u0302m)} of states and actions under the behavior policy. This shows that\nE\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 kj tj+kj\u22121 \u2211\nm=tj\n( h(\u03b8, \u03bejm)\u2212 h\u0304(\u03b8) ) 1 ( (Stj , Atj , Stj+1) = (s, a, s \u2032) )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2264 E   \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 kj kj\u22121 \u2211\nm=0\n( h(\u03b8, \u03be\u0302m)\u2212 h\u0304(\u03b8) )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225   ,\nfrom which we see that the convergence in mean stated by (4.13) holds if we have\nlim k\u2192\u221e\n1\nk\nk\u22121 \u2211\nm=0\n( h(\u03b8, \u03be\u0302m)\u2212 h\u0304(\u03b8) ) = 0 in mean. (4.14)\nNow since for each \u03b8, the function h(\u03b8, \u00b7) is Lipschitz continuous in e uniformly in the other arguments, (4.14) holds by Theorem 2.3 and its implication Cor. 2.1. Consequently, (4.13) holds, and this implies (4.11).\nProof of (4.12): Using the expression of h and the finiteness of the state and action spaces, we can bound the difference h(\u03b8, \u03bem)\u2212 h(\u03b8, \u03bejm) by\n\u2225 \u2225h(\u03b8, \u03bem)\u2212 h(\u03b8, \u03bejm) \u2225 \u2225 \u2264 c \u00b7 \u2225 \u2225em \u2212 ejm \u2225 \u2225\nfor some constant c (independent of m, j). Let us show\nlim j\u2192\u221e\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 ejm \u2225 \u2225 1 ( \u03betj \u2208 D ) = 0 in mean, (4.15)\nwhich will imply (4.12). To prove (4.15), similarly to the preceding proof, we first decompose each difference term em\u2212ejm in (4.15) into several difference terms, by using truncated traces {(e\u0303m,K , F\u0303m,K)} and {(e\u0303jm,K , F\u0303 jm,K) | m \u2265 tj}, j \u2265 1,K \u2265 1, which we now introduce. Specifically, for each K \u2265 1, {(e\u0303m,K , F\u0303m,K)} are defined by (4.3)-(4.5). For each j \u2265 1 and K \u2265 1, the truncated traces {(e\u0303jm,K , F\u0303 jm,K) | m \u2265 tj} are also defined by (4.3)-(4.5), except that the initial time is set to be tj (instead of 0) and for m \u2264 tj +K, (e\u0303jm,K , F\u0303 jm,K) is set to be (ejm, F jm) (instead of (em, Fm)). Let us fix K for now. We bound the difference em \u2212 ejm by the sum of three difference terms as\n\u2225 \u2225em \u2212 ejm \u2225 \u2225 \u2264 \u2225 \u2225em \u2212 e\u0303m,K \u2225 \u2225+ \u2225 \u2225ejm \u2212 e\u0303jm,K \u2225 \u2225+ \u2225 \u2225e\u0303m,K \u2212 e\u0303jm,K \u2225 \u2225, (4.16)"}, {"heading": "26 Weak Convergence Properties of Constrained ETD Learning", "text": "and correspondingly, we consider the following three sequences of variables, as j tends to \u221e:\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 e\u0303m,K \u2225 \u2225 1 ( \u03betj \u2208 D )\n, 1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225ejm \u2212 e\u0303jm,K \u2225 \u2225, (4.17)\nand\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225e\u0303m,K \u2212 e\u0303jm,K \u2225 \u2225 1 ( \u03betj \u2208 D ) . (4.18)\nIn what follows, we will bound their expected values as j \u2192 \u221e and then take K \u2192 \u221e; this will lead to (4.15).\nThe analyses for the two sequences in (4.17) are similar. Recall D = E \u00d7 {(s, a, s\u2032)}, so \u03betj \u2208 D implies (etj , Ftj ) \u2208 E. Since the set E is bounded, if (etj , Ftj ) \u2208 E, then we can use Prop. A.3 to bound the expectation of \u2016em \u2212 e\u0303m,K\u2016 for m \u2265 tj conditioned on Ftj , and this gives us the bound\nsup m\u2265tj Etj\n[ \u2225 \u2225em \u2212 e\u0303m,K \u2225 \u2225 ] 1 ( \u03betj \u2208 D ) \u2264 LK\nwhere LK is a constant that depends on K and the set E, and has the property that LK \u2193 0 as K \u2192 \u221e. From this bound, we obtain\nE\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 e\u0303m,K \u2225 \u2225 1 ( \u03betj \u2208 D )\n\n \u2264 LK , \u2200 j \u2265 1. (4.19)\nSimilarly, for the second sequence in (4.17), by Prop. A.3 we have\nE\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225ejm \u2212 e\u0303jm,K \u2225 \u2225\n\n \u2264 LK , \u2200 j \u2265 1, (4.20)\nwhere LK is some constant that can be chosen to be the same constant in (4.19) (because the point (e\u0304, F\u0304 ), which is the initial trace pair for (ejm, F j m) at time m = tj , lies in E).\nConsider now the sequence in (4.18). As discussed after the definition (4.3)-(4.5) of truncated traces, because of truncation, these traces lie in a bounded set determined by K and the set in which the initial trace pair lies. Therefore, there exists a finite constant cK which depends on K and E, such that for all m \u2265 tj ,\n\u2016e\u0303jm,K\u2016 \u2264 cK , and \u2016e\u0303m,K\u2016 \u2264 cK if (etj , Ftj ) \u2208 E.\nAlso by their definition, once m is sufficiently large, the truncated traces do not depend on the initial trace pairs; in particular,\ne\u0303jm,K = e\u0303m,K , \u2200m \u2265 tj + 2K + 1.\nFrom these two arguments it follows that\nE\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225e\u0303m,K \u2212 e\u0303jm,K \u2225 \u2225 1 ( \u03betj \u2208 D )\n  \u2264 (2K + 1) \u00b7 2cK kj \u2192 0 as j \u2192 \u221e. (4.21)\n27\nFinally, combining (4.19)-(4.21) with (4.16), we obtain\nlim sup j\u2192\u221e E\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 ejm \u2225 \u2225 1 ( \u03betj \u2208 D )\n\n \u2264 lim sup j\u2192\u221e E\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 e\u0303m,K \u2225 \u2225 1 ( \u03betj \u2208 D )\n\n\n+ lim sup j\u2192\u221e E\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225ejm \u2212 e\u0303jm,K \u2225 \u2225\n\n\n+ lim j\u2192\u221e E\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225e\u0303m,K \u2212 e\u0303jm,K \u2225 \u2225 1 ( \u03betj \u2208 D )\n\n\n\u2264 2LK . Since LK \u2193 0 as K \u2192 \u221e (Prop. A.3), by taking K \u2192 \u221e, we obtain\nlim j\u2192\u221e E\n\n\n1\nkj\ntj+kj\u22121 \u2211\nm=tj\n\u2225 \u2225em \u2212 ejm \u2225 \u2225 1 ( \u03betj \u2208 D )\n\n = 0.\nThis proves (4.15), which implies (4.12).\nWith Props. 4.1-4.3, we have furnished all the conditions required in order to apply [19, Theorems 8.2.2, 8.2.3] to the constrained ETD algorithm (2.11), so we can now specialize the conclusions of these two theorems to our problem. In particular, they tell us that the projected ODE (2.12) is the mean ODE for (2.11), and furthermore, by [19, Theorem 8.2.3] (respectively, [19, Theorem 8.2.2]), the conclusions of Theorem 3.1 (respectively, Theorem 3.2) hold with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217), where N\u03b4(LB) is the \u03b4-neighborhood of the limit set LB for the projected ODE (2.12). Recall that this limit set is given by LB = \u2229\u03c4\u0304>0 \u222ax(0)\u2208B{x(\u03c4), \u03c4 \u2265 \u03c4\u0304} where x(\u03c4) is a solution of the projected ODE (2.12) with initial condition x(0), the union is over all the solutions with initial x(0) \u2208 B, and D for a set D denotes taking the closure of D.\nNow when the matrix C is negative definite (as implied by Assumptions 2.1, 2.2) and when the radius of B exceeds the threshold given in Lemma 2.1, by the latter lemma, the solutions x(\u03c4), \u03c4 \u2208 [0,\u221e), of the ODE (2.12) coincide with the solutions of x\u0307 = h\u0304(x) = Cx + b for all initial x(0) \u2208 B. Then from the negative definiteness of C (Theorem 2.1), it follows that as \u03c4 \u2192 \u221e, x(\u03c4) \u2192 \u03b8\u2217 uniformly in the initial condition, and consequently, LB = {\u03b8\u2217}.13 ThusN\u03b4(LB) = N\u03b4(\u03b8\u2217) and we obtain Theorems 3.1 and 3.2."}, {"heading": "4.2 Proofs for Theorems 3.3 and 3.4", "text": "In this subsection we prove Theorems 3.3-3.4 for the two variants of the constrained ETD(\u03bb) algorithm given in (3.3) and (3.4). Like in the previous subsection, we will apply [19, Theorems 8.2.2, 8.2.3] and show, separately for each variant algorithm, that the required conditions are met. Using the properties of the mean ODEs of the variant algorithms, we will then specialize the conclusions of those theorems to obtain the desired results.\n13The details for this statement are as follows. Since h\u0304 is bounded on B and the boundary reflection term z(\u00b7) \u2261 0 under our assumptions (Lemma 2.1), a solution x(\u00b7) of (2.12) is Lipschitz continuous on [0,\u221e). We calculate V\u0307 (\u03c4) for the Lyapunov function V (\u03c4) = |x(\u03c4) \u2212 \u03b8\u2217|2. By the negative definiteness of the matrix C, for some c > 0, x\u22a4Cx \u2264 \u2212c|x|2 for all x \u2208 Rn. Then, since h\u0304(x) = Cx + b = C(x \u2212 \u03b8\u2217), we have V\u0307 (\u03c4) = 2 \u2329 x(\u03c4) \u2212 \u03b8\u2217 , h\u0304(x(\u03c4)) \u232a \u2264 \u22122c \u2223 \u2223x(\u03c4) \u2212 \u03b8\u2217 \u2223 \u2223 2 , and hence for any \u03b4 > 0, there exists \u01eb > 0 such that V\u0307 (\u03c4) \u2264 \u2212\u01eb if V (\u03c4) = |x(\u03c4)\u2212 \u03b8\u2217|2 \u2265 \u03b42. This together with the continuity of the solution x(\u00b7) implies that for any x(0) \u2208 B, within time \u03c4\u0304 = r2B/\u01eb, the trajectory x(\u03c4) must reach N\u03b4(\u03b8\n\u2217) and stay in that set thereafter. By the definition of the limit set and the arbitrariness of \u03b4, this implies LB = {\u03b8 \u2217}."}, {"heading": "28 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "4.2.1 Proofs for the First Variant", "text": "Consider the first variant algorithm (3.3):\n\u03b8t+1 = \u03a0B\n(\n\u03b8t + \u03b1t \u03c8K(et) \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t )\n)\n.\nWe define a function hK : R n \u00d7 \u039e \u2192 Rn by\nhK(\u03b8, \u03be) = \u03c8K(e) \u00b7 \u03c1(s, a) ( r(s, a, s\u2032) + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 ) , for \u03be = (e, F, s, a, s\u2032), (4.22)\nand write (3.3) equivalently as\n\u03b8t+1 = \u03a0B\n( \u03b8t + \u03b1t hK(\u03b8t, \u03bet) + \u03b1t \u03c8K(et) \u00b7 \u03c9\u0303t+1 )\nwith \u03c9\u0303t+1 = \u03c1t(Rt \u2212 r(St, At, St+1)) as before. Note that Et [\u03c8K(e) \u03c9\u0303t+1] = 0, and the algorithm is similar to the algorithm (2.11) (equivalently, (2.15)), except that we have hK and \u03c8K(et) in place of h and et, respectively.\nWe note two properties of the function hK . They follow from direct calculations and will be useful in our analysis shortly:\n(a) Using the Lipschitz continuity of the function \u03c8K (cf. (3.2)), we have that for each \u03b8 \u2208 Rn, there exists a finite c > 0 such that with \u03be = (e, F, s, a, s\u2032) and \u03be\u2032 = (e\u2032, F \u2032, s, a, s\u2032),\n\u2016hK(\u03b8, \u03be) \u2212 hK(\u03b8, \u03be\u2032)\u2016 \u2264 c \u2016e\u2212 e\u2032\u2016, \u2200 (s, a, s\u2032) \u2208 S \u00d7A\u00d7 S. (4.23)\nThus hK(\u03b8, \u00b7) is Lipschitz continuous in (e, F ) uniformly in (s, a, s\u2032). (b) Since the set B is bounded, we can bound the difference hK(\u03b8, \u03be) \u2212 h(\u03b8, \u03be) for all \u03b8 in B as\nfollows. For some finite constant c > 0,\n\u2016hK(\u03b8, \u03be)\u2212 h(\u03b8, \u03be)\u2016 \u2264 c \u2016\u03c8K(e)\u2212 e\u2016 \u2264 2c \u2016e\u2016 \u00b7 1(\u2016e\u2016 \u2265 K), \u2200 \u03b8 \u2208 B, (4.24)\nwhere the last inequality follows from the property (3.2) of \u03c8K :\n\u2016\u03c8K(x)\u2016 \u2264 \u2016x\u2016 \u2200x \u2208 Rn, and \u03c8K(x) = x if \u2016x\u2016 \u2264 K.\nWe now apply [19, Theorems 8.2.2, 8.2.3] to obtain the desired conclusions in Theorems 3.3-3.4 for the algorithm (3.3). This requires us to show that the conditions (i)-(v) and (i\u2032)-(v\u2032) given in Section 4.1.1 are still satisfied when we replace et by \u03c8K(et) and h by hK . The uniform integrability conditions (i), (i\u2032), (iv) and (iv\u2032) require the following sets to be u.i.: {hK(\u03b8t, \u03bet) + \u03c8K(et) \u00b7 \u03c9\u0303t+1} and {hK(\u03b8\u03b1t , \u03bet) + \u03c8K(et) \u00b7 \u03c9\u0303t+1 | t \u2265 0, \u03b1 > 0}, {hK(\u03b8t, \u03bet)} and {hK(\u03b8\u03b1t , \u03bet) | t \u2265 0, \u03b1 > 0}, and {hK(\u03b8, \u03bet)} for each \u03b8. These conditions are evidently satisfied, in view of the boundedness of the functions \u03c8K and hK(\u03b8, \u00b7) for each \u03b8, the boundedness of the \u03b8-iterates due to constraints, and the finite variances of {\u03c9\u0303t}. The condition (ii) on the continuity of hK(\u00b7, \u03be) uniformly in \u03be \u2208 D, for each compact set D \u2282 \u039e, is also clearly satisfied, whereas the condition (iii) (equivalently (iii\u2032)) on the tightness of {\u03bet} was already verified earlier in Prop. 4.1.\nWhat remains is the condition (v) (which is equivalent to (v\u2032), for the same reason as discussed immediately before Prop. 4.3). It requires the existence of a continuous function h\u0304K : R\nn \u2192 Rn such that for each \u03b8 \u2208 B and each compact set D \u2282 \u039e,\nlim k\u2192\u221e,t\u2192\u221e\n1\nk\nt+k\u22121 \u2211\nm=t\nEt\n[ hK(\u03b8, \u03bem)\u2212 h\u0304K(\u03b8) ] 1 ( \u03bet \u2208 D ) = 0 in mean. (4.25)\nIf this condition is satisfied as well, then the mean ODE for the algorithm (3.3) is given by\nx\u0307 = h\u0304K(x) + z, z \u2208 \u2212NB(x). (4.26)\n29\nTo furnish the condition (v), we first identify the function h\u0304K(\u03b8) to be E\u03b6 [hK(\u03b8, \u03be0)], the expectation of hK(\u03b8, \u03be0) under the stationary distribution of the process {Zt} with the invariant probability measure \u03b6 as its initial distribution. We relate the functions h\u0304K ,K > 0, to h\u0304 in the proposition below, and we will use it to characterize the bias of the algorithm (3.3) later.\nProposition 4.4. Let Assumption 2.1 hold. Consider the setting of the algorithm (3.3), and for each \u03b8 \u2208 Rn, let h\u0304K(\u03b8) = E\u03b6 [hK(\u03b8, \u03be0)]. Then the function h\u0304K is Lipschitz continuous on Rn, and\nsup \u03b8\u2208B\n\u2016h\u0304K(\u03b8)\u2212 h\u0304(\u03b8)\u2016 \u2192 0 as K \u2192 \u221e. (4.27)\nProof. For each \u03b8, the function hK(\u03b8, \u00b7) is by definition bounded. Under Assumption 2.1, the Markov chain {(St, At, et, Ft)} has a unique invariant probability measure \u03b6 (Theorem 2.2). Therefore, h\u0304K(\u03b8) is well-defined and finite. Let c1 = supe\u2208Rn \u2016\u03c8K(e)\u2016 < \u221e (since \u03c8K is bounded). For any \u03b8, \u03b8\u2032, using the definition of hK , a direct calculation shows that for some c2 > 0, \u2016hK(\u03b8, \u03be)\u2212 hK(\u03b8\u2032, \u03be)\u2016 \u2264 c1c2\u2016\u03b8 \u2212 \u03b8\u2032\u2016 for all \u03be \u2208 \u039e, from which it follows that\n\u2016h\u0304K(\u03b8)\u2212 h\u0304K(\u03b8\u2032)\u2016 \u2264 E\u03b6 [\u2016hK(\u03b8, \u03be0)\u2212 hK(\u03b8\u2032, \u03be0)\u2016] \u2264 c1c2\u2016\u03b8 \u2212 \u03b8\u2032\u2016.\nThis shows that h\u0304K is Lipschitz continuous. We now prove (4.27). Since h\u0304K(\u03b8) = E\u03b6 [hK(\u03b8, \u03be0)] by definition and h\u0304(\u03b8) = E\u03b6 [h(\u03b8, \u03be0)] by Cor. 2.1, it is sufficient to prove the following statement, which entails (4.27):\nsup \u03b8\u2208B E\u03b6\n[ \u2225 \u2225hK(\u03b8, \u03be0)\u2212 h(\u03b8, \u03be0) \u2225 \u2225 ] \u2192 0 as K \u2192 \u221e. (4.28)\nBy (4.24), for some constant c > 0,\n\u2016hK(\u03b8, \u03be0)\u2212 h(\u03b8, \u03be0)\u2016 \u2264 2c \u2016e0\u2016 \u00b7 1(\u2016e0\u2016 \u2265 K), \u2200 \u03b8 \u2208 B,\nand therefore, sup \u03b8\u2208B E\u03b6 [ \u2225 \u2225hK(\u03b8, \u03be0)\u2212 h(\u03b8, \u03be0) \u2225 \u2225 ] \u2264 2cE\u03b6 [ \u2016e0\u2016 \u00b7 1(\u2016e0\u2016 \u2265 K) ] . By Theorem 2.3, E\u03b6 [\u2016e0\u2016] < \u221e and hence E\u03b6 [\u2016e0\u2016 \u00b7 1(\u2016e0\u2016 \u2265 K)] \u2192 0 as K \u2192 \u221e. Together with the preceding inequality, this implies (4.28), which in turn implies (4.27).\nWe now show that the convergence in mean required in (4.25) is satisfied.\nProposition 4.5. Under Assumption 2.1, the conclusion of Prop. 4.3 holds in the setting of the algorithm (3.3), with the functions hK and h\u0304K in place of h and h\u0304, respectively.\nProof. The same arguments given in the proof of Prop. 4.3 apply here, with the functions hK , h\u0304K in place of h, h\u0304, respectively. Only two details are worth noting here. The proof relies on the Lipschitz continuity property of hK given in (4.23). As mentioned earlier, this property implies that for each \u03b8, with \u03be = (e, F, s, a, s\u2032), hK(\u03b8, \u03be) is Lipschitz continuous in (e, F ) uniformly in (s, a, s\n\u2032), so we can apply Theorem 2.3 to conclude that (4.14) and hence (4.11) hold in this case (for hK , h\u0304K instead of h, h\u0304). The property (4.23) also allows us to obtain (4.12) in this case, by exactly the same proof given earlier.\nThus we have furnished all the conditions required by [19, Theorems 8.2.2, 8.2.3]. As in the case of (2.11), by these two theorems, the assertions of Theorems 3.1 and 3.2 hold for the variant algorithm (3.3) with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217), where LB is the limit set of the projected mean ODE associated with (3.3): x\u0307 = h\u0304K(x) + z, z \u2208 \u2212NB(x). To finish the proof for Theorems 3.3-3.4, it is now sufficient to show that for any given \u03b4 > 0, we can choose a number K\u03b4 large enough so that LB \u2282 N\u03b4(\u03b8\u2217) for all K \u2265 K\u03b4. We prove this below, using Prop. 4.4. Note that the set LB reflects the bias of the constrained algorithm (3.3), so what we are showing now is that this bias decreases as K increases."}, {"heading": "30 Weak Convergence Properties of Constrained ETD Learning", "text": "Lemma 4.2. Let Assumptions 2.1, 2.2 hold, and let the radius of the set B exceed the threshold given in Lemma 2.1. Then for all K sufficiently large, given any initial condition x(0) \u2208 B, a solution to the projected ODE (4.26) coincides with the unique solution to x\u0307 = h\u0304K(x), with the boundary reflection term being z(\u00b7) \u2261 0. Given \u03b4 > 0, there exists K\u03b4 such that for K \u2265 K\u03b4, the limit set LB of (4.26) satisfies LB \u2282 N\u03b4(\u03b8\u2217).\nProof. Under Assumptions 2.1, 2.2, the matrix C is negative definite (Theorem 2.1), and when the radius of the set B exceeds the threshold given in Lemma 2.1, there exists a constant \u01eb > 0 such that for all boundary points x of B, \u3008x, h\u0304(x)\u3009 < \u2212\u01eb. At such points x, the normal cone NB(x) = {ax | a \u2265 0}, and\n\u3008x, h\u0304K(x)\u3009 = \u3008x, h\u0304(x)\u3009 + \u3008x, h\u0304K(x) \u2212 h\u0304(x)\u3009 < \u2212\u01eb+ \u3008x, h\u0304K(x)\u2212 h\u0304(x)\u3009.\nBy (4.27) in Prop. 4.4, \u3008x, h\u0304K(x)\u2212h\u0304(x)\u3009 \u2192 0 uniformly on B asK \u2192 \u221e. Thus when K is sufficiently large, at all boundary points x of B, \u3008x, h\u0304K(x)\u3009 < 0; i.e., h\u0304K(x) points inside B and the boundary reflection term z = 0. It then follows that for such K, given an initial condition x(0) \u2208 B, a solution to (4.26) coincides with the unique solution to x\u0307 = h\u0304K(x), where the uniqueness is ensured by the Lipschitz continuity of h\u0304K proved in Prop. 4.4 (cf. [7, Chap. 11.2]).\nTo prove the second statement concerning the limit set of the projected ODE, let K be large enough so that the conclusion of the first part holds. Let x(\u03c4), \u03c4 \u2208 [0,\u221e), be the solution of (4.26) for a given initial x(0) \u2208 B. Since h\u0304K is bounded on B, x(\u00b7) is Lipschitz continuous on [0,\u221e). Let V (\u03c4) = |x(\u03c4) \u2212 \u03b8\u2217|2, and we calculate V\u0307 (\u03c4). Since for all x, h\u0304(x) = Cx + b = C(x \u2212 \u03b8\u2217) and x\u22a4Cx \u2264 \u2212c|x|2 for some c > 0 by the negative definiteness of C, a direct calculation shows that\nV\u0307 (\u03c4) = 2 \u2329 x(\u03c4) \u2212 \u03b8\u2217 , h\u0304K(x(\u03c4)) \u232a = 2 \u2329 x(\u03c4) \u2212 \u03b8\u2217 , h\u0304(x(\u03c4)) \u232a + 2 \u2329 x(\u03c4) \u2212 \u03b8\u2217, h\u0304K(x(\u03c4)) \u2212 h\u0304(x(\u03c4)) \u232a\n\u2264 \u22122c \u2223 \u2223x(\u03c4) \u2212 \u03b8\u2217 \u2223 \u2223 2 + 2 \u2223 \u2223x(\u03c4) \u2212 \u03b8\u2217 \u2223 \u2223 \u00b7 \u2223 \u2223h\u0304K(x(\u03c4)) \u2212 h\u0304(x(\u03c4)) \u2223 \u2223.\nBy (4.27) in Prop. 4.4, supx\u2208B |hK(x) \u2212 h\u0304(x)| \u2192 0 as K \u2192 \u221e. It then follows that for any \u03b4 > 0, there exist \u01eb > 0 and K\u03b4 > 0 such that for all K \u2265 K\u03b4, V\u0307 (\u03c4) \u2264 \u2212\u01eb if V (\u03c4) = |x(\u03c4)\u2212 \u03b8\u2217|2 \u2265 \u03b42. This together with the continuity of the solution x(\u00b7) shows that for any x(0) \u2208 B, within time \u03c4\u0304 = r2B/\u01eb (where rB is the radius of B), the trajectory x(\u03c4) must reach N\u03b4(\u03b8\n\u2217) and stay in that set thereafter. Consequently, for all K \u2265 K\u03b4, the limit set LB = \u2229\u03c4\u0304\u22650 \u222ax(0)\u2208B{x(\u03c4), \u03c4 \u2265 \u03c4\u0304} \u2282 N\u03b4(\u03b8\u2217).\nThis completes the proofs of Theorems 3.3 and 3.4 for the first variant."}, {"heading": "4.2.2 Proofs for the Second Variant", "text": "We now analyze the second variant algorithm (3.4),\n\u03b8t+1 = \u03a0B (\u03b8t + \u03b1t \u03c8K(Yt)) , where Yt = et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t ) .\nSimilarly to the previous case, with \u03be = (e, F, s, a, s\u2032), we define a bounded function hK : R n\u00d7\u039e \u2192 Rn by\nhK(\u03b8, \u03be) =\n\u222b\n\u03c8K\n(\ne \u00b7 \u03c1(s, a) ( r + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 )\n)\nq(dr | s, a, s\u2032),\nwhere we recall that q(dr | s, a, s\u2032) is the conditional probability distribution of the reward given the state transition (s, s\u2032) under the action a. We can write the algorithm (3.4) equivalently in terms of hK as\n\u03b8t+1 = \u03a0B\n(\n\u03b8t + \u03b1t hK(\u03b8t, \u03bet) + \u03b1t \u2206t\n)\n,\nwhere \u2206t = \u03c8K(Yt)\u2212 hK(\u03b8t, \u03bet), and it can be seen that hK(\u03b8t, \u03bet) = Et[\u03c8K(Yt)] and Et[\u2206t] = 0. Two properties of the function hK will be useful shortly in our analysis:\n31\n(a) The Lipschitz continuity property (4.23) holds for the function hK here. In particular, let c1 > 0 be the Lipschitz modulus of the function \u03c8K with respect to \u2016 \u00b7 \u2016. A direct calculation using the Lipschitz property of \u03c8K shows that with \u03be = (e, F, s, a, s \u2032) and \u03be\u2032 = (e\u2032, F \u2032, s, a, s\u2032),\n\u2016hK(\u03b8, \u03be)\u2212 hK(\u03b8, \u03be\u2032)\u2016 \u2264 \u222b c1 \u2225 \u2225 \u2225 (e \u2212 e\u2032) \u00b7 \u03c1(s, a) ( r + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 ) \u2225 \u2225 \u2225 q(dr|s, a, s\u2032),\nso for each \u03b8, there exists a finite constant c > 0 such that\n\u2016hK(\u03b8, \u03be) \u2212 hK(\u03b8, \u03be\u2032)\u2016 \u2264 c \u2016e\u2212 e\u2032\u2016, \u2200 (s, a, s\u2032) \u2208 S \u00d7A\u00d7 S. (4.29)\n(b) The second property given below also follows from a direct calculation using the Lipschitz continuity of \u03c8K : there exists a finite constant c > 0 such that for any \u03b8, \u03b8 \u2032,\n\u2016hK(\u03b8, \u03be)\u2212 hK(\u03b8\u2032, \u03be)\u2016 \u2264 c\u2016e\u2016 \u00b7 \u2016\u03b8 \u2212 \u03b8\u2032\u2016, \u2200 \u03be \u2208 \u039e. (4.30)\nWe now proceed to prove Theorems 3.3-3.4 for the algorithm (3.4). As before, we will apply [19, Theorems 8.2.2, 8.2.3], and this requires us to show that the conditions (i)-(v) and (i\u2032)-(v\u2032) given in Section 4.1.1, with the function hK above in place of h, are satisfied. The conditions (i)-(iv) and (i\u2032)-(iv\u2032) are clearly met. In particular, the uniform integrability conditions (i), (i\u2032), (iv) and (iv\u2032) are trivially fulfilled because by the definitions of hK and the algorithm (3.4), {\u03c8K(Yt)}, {hK(\u03b8t, \u03bet)}, and {hK(\u03b8, \u03bet)} for each \u03b8, regardless of stepsizes, all lie in a bounded set determined by K. As for the continuity condition (ii), in view of the boundedness and Lipschitz continuity of \u03c8K , it is also clear that hK(\u03b8, \u03be) is bounded and continuous in \u03b8 uniformly in \u03be \u2208 D, for each compact set D \u2282 \u039e (cf. (4.30)).\nThe condition (v) (equivalently (v\u2032)) requires the existence of a continuous function h\u0304K : R n \u2192 Rn\nsuch that for each \u03b8 \u2208 B and each compact set D \u2282 \u039e,\nlim k\u2192\u221e,t\u2192\u221e\n1\nk\nt+k\u22121 \u2211\nm=t\nEt\n[ hK(\u03b8, \u03bem)\u2212 h\u0304K(\u03b8) ] 1 ( \u03bet \u2208 D ) = 0 in mean. (4.31)\nSimilarly to the analysis for the first variant algorithm, we identify this function h\u0304K(\u03b8) to be the expectation of hK(\u03b8, \u03be0) with respect to the stationary distribution of the process {Zt}, and if condition (v) is satisfied, then the mean ODE of the algorithm (3.4) will be given by\nx\u0307 = h\u0304K(x) + z, z \u2208 \u2212NB(x).\nProposition 4.6. Let Assumption 2.1 hold. Consider the setting of the algorithm (3.4), and for each \u03b8 \u2208 Rn, let h\u0304K(\u03b8) = E\u03b6 [hK(\u03b8, \u03be0)]. Then the conclusion of Prop. 4.4 holds.\nProof. The function hK is by definition bounded, and under Assumption 2.1, the Markov chain {(St, At, et, Ft)} has a unique invariant probability measure \u03b6 (Theorem 2.2). The function h\u0304K(\u03b8) is therefore well-defined and bounded. Using the property (4.30) of the function hK , we have that there exists a finite constant c > 0 such that for any \u03b8, \u03b8\u2032,\n\u2016h\u0304K(\u03b8)\u2212 h\u0304K(\u03b8\u2032)\u2016 \u2264 E\u03b6 [\u2225 \u2225hK(\u03b8, \u03be0)\u2212 hK(\u03b8\u2032, \u03be0) \u2225 \u2225 ] \u2264 cE\u03b6 [\u2016e0\u2016] \u00b7 \u2016\u03b8 \u2212 \u03b8\u2032\u2016.\nSince E\u03b6{\u2016e0\u2016} < \u221e by Theorem 2.3, this shows that h\u0304K is Lipschitz continuous. To prove (4.27), let us prove\nsup \u03b8\u2208B E\u03b6\u2032\n[\n\u2225 \u2225h\u0302K(\u03b8, \u03be0, R0)\u2212 h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225\n]\n\u2192 0 as K \u2192 \u221e, (4.32)"}, {"heading": "32 Weak Convergence Properties of Constrained ETD Learning", "text": "where h\u0302K , h\u0302 : R n \u00d7 \u039e\u00d7 R \u2192 Rn are defined by\nh\u0302K(\u03b8, \u03be, r) := \u03c8K\n(\ne \u00b7 \u03c1(s, a) ( r + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 )\n)\n,\nh\u0302(\u03b8, \u03be, r) := e \u00b7 \u03c1(s, a) ( r + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 ) ,\nand \u03b6\u2032 denotes the unique invariant probability measure of the Markov chain {(\u03bet, Rt)}, the existence and uniqueness of such a measure being implied by Theorem 2.2, and the expectation E\u03b6\u2032 is over (\u03be0, R0) with respect to \u03b6 \u2032. By taking expectation over R0 conditioned on \u03be0, we have\nE\u03b6\u2032 [ h\u0302K(\u03b8, \u03be0, R0) ] = E\u03b6 [ hK(\u03b8, \u03be0) ] = h\u0304K(\u03b8), E\u03b6\u2032 [ h\u0302(\u03b8, \u03be0, R0) ] = E\u03b6 [ h(\u03b8, \u03be0) ] = h\u0304(\u03b8),\nso (4.32) implies (4.27).\nWe now prove (4.32). Note that h\u0302K(\u03b8, \u03be0, R0) = \u03c8K(h\u0302(\u03b8, \u03be0, R0)). So using the property (3.2) of \u03c8K , we have for any \u03b8,\n\u2225 \u2225h\u0302K(\u03b8, \u03be0, R0)\u2212 h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225 \u2264 2 \u2225 \u2225h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225 \u00b7 1 (\u2225 \u2225h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225 \u2265 K ) ,\nand using the definition of h\u0302 and the boundedness of B, we also have that for some constants c1, c2 > 0,\n\u2225 \u2225h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225 \u2264 c1\u2016e0R0\u2016+ c2\u2016e0\u2016 =: g(e0, R0), \u2200 \u03b8 \u2208 B. Combining the preceding two relations, we have for any \u03b8 \u2208 B,\n\u2225 \u2225h\u0302K(\u03b8, \u03be0, R0)\u2212 h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225 \u2264 2 g(e0, R0) \u00b7 1 ( g(e0, R0) \u2265 K )\nand hence\nsup \u03b8\u2208B E\u03b6\u2032\n[\n\u2225 \u2225h\u0302K(\u03b8, \u03be0, R0)\u2212 h\u0302(\u03b8, \u03be0, R0) \u2225 \u2225\n]\n\u2264 2E\u03b6\u2032 [ g(e0, R0) \u00b7 1 ( g(e0, R0) \u2265 K )] . (4.33)\nSince E\u03b6\u2032 [g(e0, R0)] = E\u03b6\u2032 [c1\u2016e0R0\u2016+ c2\u2016e0\u2016] < \u221e (we obtain the finiteness of the expectation here by first taking expectation over R0 conditioned on \u03be0 and then applying Theorem 2.3), the expectation on the right-hand side of (4.33) converges to 0 as K \u2192 \u221e. We thus obtain (4.32), which implies (4.27).\nThe rest of the analysis is similar to that for the first variant algorithm. First, using the Lipschitz continuity property (4.29) of the function hK given earlier, we obtain that the convergence-in-mean condition (4.31) holds, by the same proof arguments for Prop. 4.3:\nProposition 4.7. Under Assumption 2.1 and in the setting of the algorithm (3.4), (4.31) holds for each \u03b8 \u2208 B and each compact set D \u2282 \u039e.\nNow we have furnished all the conditions required by [19, Theorems 8.2.2, 8.2.3]. By these two theorems, we can assert that the conclusions of Theorems 3.1-3.2 hold for the variant algorithm (3.4) with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217), where LB is the limit set of the projected mean ODE associated with (3.4): x\u0307 = h\u0304K(x) + z, z \u2208 \u2212NB(x). So to finish the proof for Theorems 3.3-3.4, it is sufficient to show that for any given \u03b4 > 0, we can choose a number K\u03b4 large enough so that LB \u2282 N\u03b4(\u03b8\u2217) for all K \u2265 K\u03b4. In other words, the conclusions of Lemma 4.2 hold for the case of the algorithm (3.4). This is true by the same proof for Lemma 4.2 with Prop. 4.6 in place of Prop. 4.4. This completes the proofs of Theorems 3.3-3.4 for the second variant.\n33"}, {"heading": "4.3 Further Analysis of the Constant-stepsize Case", "text": "We now consider again the case of constant stepsize, and prove Theorems 3.5-3.8 given in Section 3.3. The proofs will be based on combining the results we obtained earlier by using the stochastic approximation theory, with the ergodic theorems of weak Feller Markov chains. As before the proofs will also rely on the key properties of the ETD iterates."}, {"heading": "4.3.1 Weak Feller Markov Chains", "text": "We shall focus on Markov chains on complete separable metric spaces. For such a Markov chain {Xt} with state space X, let P (\u00b7, \u00b7) denote its transition kernel, that is, P : X\u00d7 B(X) \u2192 [0, 1],\nP (x,D) = Px(X1 \u2208 D), \u2200x \u2208 X, D \u2208 B(X),\nwhere B(X) denotes the Borel sigma-algebra on X, and Px denotes the probability distribution of {Xt} conditioned on X0 = x. Multiple-step transition kernels will also be needed. For t \u2265 1, the t-step transition kernel P t(\u00b7, \u00b7) : X\u00d7 B(X) \u2192 [0, 1] is given by\nP t(x,D) = Px(Xt \u2208 D), \u2200x \u2208 X, D \u2208 B(X),\nand for t = 0, P 0 is defined as P 0(x, \u00b7) = \u03b4x, the Dirac measure that assigns probability 1 to the point x, for each x \u2208 X. Define averaged probability measures P\u0304k(x, \u00b7) for k \u2265 1 and x \u2208 X, as\nP\u0304k(x, \u00b7) = 1\nk\nk\u22121 \u2211\nt=0\nP t(x, \u00b7).\nThe Markov chain {Xt} has the weak Feller property if for every bounded continuous function f on X,\nPf(x) :=\n\u222b\nf(y)P (x, dy) = E [ f(X1) | X0 = x ]\nis a continuous function of x [30, Prop. 6.1.1]. Weak Feller Markov chains have nice properties. In our analysis, we will use in particular several properties relating to the invariant probability measures of these chains and convergence of certain probability measures to the invariant probability measures.\nRecall that if \u00b5 and \u00b5t, t \u2265 0, are probability measures on X, {\u00b5t} is said to converge weakly to \u00b5 if \u222b fd\u00b5t \u2192 \u222b\nfd\u00b5 for every bounded continuous function f on X. For {\u00b5t} that is not necessarily convergent, we shall call the limiting probability measure of any of its convergent subsequence, in the sense of weak convergence, a weak limit of {\u00b5t}. For an (arbitrary) index set K, a set of probability measures {\u00b5k}k\u2208K on X is said to be tight if for every \u03b4 > 0, there exists a compact set D\u03b4 \u2282 X such that \u00b5k(D\u03b4) \u2265 1 \u2212 \u03b4 for all k \u2208 K. An important fact is that on a complete separable metric space, any tight sequence of probability measures has a further subsequence that converges weakly to some probability measure [12, Theorem 11.5.4].\nFor weak Feller Markov chains, their averaged probability measures {P\u0304k(x, \u00b7)}k\u22651 are known to have the following property; see e.g., the proof of Lemma 4.1 in [29]. It will be needed in our proofs of Theorems 3.5-3.6.\nLemma 4.3. Let {Xt} be a weak Feller Markov chain with transition kernel P (\u00b7, \u00b7) on a metric space X. For each x \u2208 X, any weak limit of {P\u0304k(x, \u00b7)} is an invariant probability measure of {Xt}.\nRecall that the occupation probability measures of {Xt}, denoted {\u00b5x,t} for each initial condition x \u2208 X, are defined as follows:\n\u00b5x,t(D) := 1\nt\nt\u22121 \u2211\nk=0\n1(Xk \u2208 D), \u2200D \u2208 B(X),"}, {"heading": "34 Weak Convergence Properties of Constrained ETD Learning", "text": "where the chain {Xt} starts from X0 = x, and each \u00b5x,t is a random variable taking values in the space of probability measures on X. Let \u201cPx-a.s.\u201d stand for \u201calmost surely with respect to Px.\u201d The next lemma concerns the convergence of occupation probability measures of a weak Feller Markov chain. It is a result of Meyn [29] and will be needed in our proofs of Theorems 3.7-3.8.\nLemma 4.4 ([29, Prop. 4.2]). Let {Xt} be a weak Feller Markov chain with transition kernel P (\u00b7, \u00b7) on a complete separable metric space X. Suppose that\n(i) {Xt} has a unique invariant probability measure \u00b5; (ii) for each compact set E \u2282 X, the set {P\u0304k(x, \u00b7) | x \u2208 E, k \u2265 1} is tight; and (iii) for all initial conditions x \u2208 X, there exists a sequence of compact sets Ek \u2191 X (that is\nEk \u2282 Ek+1 for all k and \u222akEk = X) such that\nlim k\u2192\u221e lim inf t\u2192\u221e \u00b5x,t(Ek) = 1, Px-a.s.\nThen, for each initial condition x \u2208 X, the sequence {\u00b5x,t} of occupation probability measures converges weakly to \u00b5, Px-almost surely.\nThe condition (iii) above is equivalent to that the sequence {\u00b5x,t} of occupation probability measures is almost surely tight for each initial condition."}, {"heading": "4.3.2 Proofs of Theorems 3.5 and 3.6", "text": "In this subsection we prove Theorem 3.5 for the algorithm (2.11) and Theorem 3.6 for its two variants (3.3) and (3.4). We also show that the conclusions of Theorems 3.5-3.6 hold for the perturbed version (3.7) of these algorithms as well. The proof arguments are largely the same for all the algorithms we consider here. So except where noted otherwise, it will be taken for granted through out this subsection that {\u03b8\u03b1t } is generated by either of the six algorithms just mentioned, for a constant stepsize \u03b1 > 0.\nWe start with some preliminary analysis given in the next two lemmas. Recall Zt = (St, At, et, Ft) and {Zt} is a weak Feller Markov chain on Z := S \u00d7A\u00d7Rn+1 [52, Sec. 3.1], and its evolution is not affected by the \u03b8-iterates. We consider the Markov chain {(Zt, \u03b8\u03b1t )} on the state space Z \u00d7B (note that this is a complete separable metric space). This chain also has the weak Feller property:\nLemma 4.5. Let Assumption 2.1(ii) hold. The process {(Zt, \u03b8\u03b1t )} is a weak Feller Markov chain.\nProof. We omit the superscript \u03b1 in the proof. We need to show that for any bounded continuous function f(z, \u03b8), the function E[f(Z1, \u03b81) | Z0 = z, \u03b80 = \u03b8] is continuous in (z, \u03b8). Express z in terms of its components as z = (s, a, e, F ). Given that the space S\u00d7A is discrete, a function continuous in (z, \u03b8) is a function that is continuous in (e, F, \u03b8) for each (s, a) \u2208 S \u00d7A. So in view of the finiteness of S \u00d7A, what we need to show is that for any two state-action pairs (s, a), (s\u2032, a\u2032) \u2208 S \u00d7A,\ng\u0304(e, F, \u03b8) := E [ f(Z1, \u03b81) \u2223 \u2223 (e0, F0, \u03b80) = (e, F, \u03b8), (S0, A0, S1, A1) = (s, a, s \u2032, a\u2032) ]\n(4.34)\nis a continuous function of (e, F, \u03b8). Consider first the case where \u03b8t is generated by either of the algorithms (2.11), (3.3) and (3.4). By the definitions of these algorithms, given (S0, A0, S1, A1) as in (4.34), e1 and F1 are continuous functions of (e0, F0), and \u03b81 is a continuous function of (e0, F0, \u03b80, R0). Thus f(Z1, \u03b81) is a continuous function of (e0, F0, \u03b80, R0); to simplify notation, denote this function by g(x, r) with x = (e, F, \u03b8) and r corresponding to the reward variable R0. Then, to show the continuity of the function in (4.34) is to show the continuity of the function\ng\u0304(x) =\n\u222b\ng(x, r) q(dr |s, a, s\u2032).\n35\nTo verify that g\u0304 is continuous, consider an arbitrary point x\u0304 = (e\u0304, F\u0304 , \u03b8\u0304) \u2208 Rn+1 \u00d7 B. Given any \u01eb > 0, pick r\u0304 > 0 large enough so that q([\u2212r\u0304, r\u0304] |s, a, s\u2032) \u2265 1\u2212 \u01eb/(4\u2016g\u2016\u221e) (where \u2016g\u2016\u221e \u2264 \u2016f\u2016\u221e < \u221e, and \u2016g\u2016\u221e, \u2016f\u2016\u221e are the infinity norm of the functions g, f , respectively). Since g is continuous, it is uniformly continuous on any compact set. Therefore, there exists \u03b4 > 0 small enough so that for any r \u2208 [\u2212r\u0304, r\u0304] and x \u2208 Rn+1 \u00d7 B with |x \u2212 x\u0304| \u2264 \u03b4, |g(x, r) \u2212 g(x\u0304, r)| \u2264 \u01eb/2. Consequently, for any x \u2208 Rn+1 \u00d7B with |x\u2212 x\u0304| \u2264 \u03b4, we have\n\u2223 \u2223g\u0304(x) \u2212 g\u0304(x\u0304) \u2223 \u2223 \u2264 \u222b \u2223 \u2223g(x, r) \u2212 g(x\u0304, r) \u2223 \u2223 q(dr |s, a, s\u2032)\n=\n\u222b\n{|r|>r\u0304}\n\u2223 \u2223g(x, r) \u2212 g(x\u0304, r) \u2223 \u2223 q(dr |s, a, s\u2032) + \u222b\n[\u2212r\u0304,r\u0304]\n\u2223 \u2223g(x, r) \u2212 g(x\u0304, r) \u2223 \u2223 q(dr |s, a, s\u2032)\n\u2264 2\u2016g\u2016\u221e \u00b7 \u01eb/(4\u2016g\u2016\u221e) + \u01eb/2 = \u01eb.\nThis shows that g\u0304 is a continuous function, and proves that {(Zt, \u03b8\u03b1t )} is a weak Feller chain. The proof for the perturbed version (3.7) of the algorithms (2.11), (3.3) and (3.4) follows the same arguments, except that g is now a continuous function of (x, r,\u2206) where \u2206 is the perturbation variable, and g\u0304 is defined by the integration over both r and \u2206. The proof details are almost identical to those given above and therefore omitted. We only note a minor difference in the last step of the proof: in addition to choosing a sufficiently large r\u0304, we also choose a sufficiently large compact set E\u2206 on the space of \u2206, and we work with the resulting compact set Ex\u0304 \u00d7 [\u2212r\u0304, r\u0304] \u00d7 E\u2206 for a closed neighborhood Ex\u0304 of the point x\u0304, and use the uniform continuity of the function g on compact sets, as in the above proof.\nIn order to study the behavior of multiple consecutive \u03b8-iterates, we consider for m \u2265 1, the m-step version of {(Zt, \u03b8\u03b1t )}, that is, the Markov chain {Xt} on (Z \u00d7 B)m where each state Xt consists of m consecutive states of the original chain {(Zt, \u03b8\u03b1t )}:\nXt = ( (Zt, \u03b8 \u03b1 t ), . . . , (Zt+m\u22121, \u03b8 \u03b1 t+m\u22121) ) .\nSimilarly to the proof of Lemma 4.5, it is straightforward to show that the m-step version of a weak Feller Markov chain is a weak Feller chain as well. Thus the m-step version of {(Zt, \u03b8\u03b1t )} is also a weak Feller Markov chain, and we can apply the ergodic theorems for weak Feller Markov chains to analyze it. In particular, in this subsection we will use Lemma 4.3 to prove Theorems 3.5-3.6; in the next subsection we will also use Lemma 4.4.\nIn analyzing the m-step version of {(Zt, \u03b8\u03b1t )}, sometimes it will be more convenient for us to take as its initial condition the condition of just (Z0, \u03b8 \u03b1 0 )\u2014instead of (Z0, \u03b8 \u03b1 0 , . . . , Z \u03b1 m\u22121, \u03b8 \u03b1 m\u22121)\u2014and to work with the following objects that are essentially equivalent to the averaged probability measures {P\u0304k(x, \u00b7)} and the occupation probability measures {\u00b5x,t} defined earlier for a general Markov chain {Xt}. Specifically, with {Xt} denoting the m-step version of {(Zt, \u03b8\u03b1t )}, for each (z, \u03b8) \u2208 Z \u00d7B, we define probability measures P\u0304\n(m,k) (z,\u03b8) , k \u2265 1, on the space X = (Z \u00d7B)m, by\nP\u0304 (m,k) (z,\u03b8) (D) :=\n1\nk\nk\u22121 \u2211\nt=0\nP(z,\u03b8) ( Xt \u2208 D ) , \u2200D \u2208 B(X). (4.35)\nSimilarly, we define occupation probability measures {\u00b5(m)(z,\u03b8),t} for each (z, \u03b8) \u2208 Z \u00d7B by\n\u00b5 (m) (z,\u03b8),t(D) :=\n1\nt\nt\u22121 \u2211\nk=0\n1 ( Xk \u2208 D ) , \u2200D \u2208 B(X), (4.36)\nwhere the initial (Z0, \u03b8 \u03b1 0 ) = (z, \u03b8). Compared with the definitions of {P\u0304k(x, \u00b7)} and {\u00b5x,t} for {Xt}, apparently, all the previous conclusions given in Section 4.3.1 for {P\u0304k(x, \u00b7)} and {\u00b5x,t} hold for\n36 Weak Convergence Properties of Constrained ETD Learning\n{ P\u0304 (m,k) (z,\u03b8) } and { \u00b5 (m) (z,\u03b8),t } as well; therefore we can use the objects { P\u0304 (m,k) (z,\u03b8) } and {P\u0304k(x, \u00b7)}, and {\n\u00b5 (m) (z,\u03b8),t } and {\u00b5x,t}, interchangeably in our analysis.\nLemma 4.6. Let Assumption 2.1 hold. For m \u2265 1, let {Xt} be the m-step version of {(Zt, \u03b8\u03b1t )} on X = (Z \u00d7 B)m, with transition kernel P (\u00b7, \u00b7). Then {Xt} satisfies the conditions (ii)-(iii) of Lemma 4.4.\nProof. To show that the condition (ii) of Lemma 4.4 is satisfied, fix a compact set E \u2282 X and let us first show that the set {P t(x, \u00b7) | x \u2208 E, t \u2265 0} is tight. Since the set B is compact and the state and action spaces are finite, of concern here is just the tightness of the marginals of these probability measures on the space of the trace components (et, Ft, . . . , et+m\u22121, Ft+m\u22121) of the state Xt. By Prop. A.1, for all initial conditions of (e0, F0) in a given bounded subset of R\nn+1, supt\u22650 E[\u2016(et, Ft)\u2016] \u2264 L for a constant L (that depends on the subset). So for the set E, applying the Markov inequality together with the union bound, we have that there exists a constant L > 0 such that for all x \u2208 E and a > 0, Px ( supk\u2264t<k+m \u2016(et, Ft)\u2016 \u2265 a )\n\u2264 mL/a for all k \u2265 0. Now for any given \u03b4 > 0, let a be large enough so that mL/a < \u03b4 and let Da be the closed ball in R n+1 centered at the origin with radius a. Then for the compact set D = (S \u00d7 A \u00d7Da \u00d7 B)m, we have P k(x,D) = Px ( supk\u2264t<k+m \u2016(et, Ft)\u2016 \u2264 a )\n\u2265 1\u2212 \u03b4 for all x \u2208 E and all k \u2265 0. This shows that the set {P t(x, \u00b7) | x \u2208 E, t \u2265 0} is tight. Consequently, the averages of the probability measures in this set must also form a tight set; in particular, the set {P\u0304k(x, \u00b7) | x \u2208 E, k \u2265 1} must be tight. Hence {Xt} satisfies the condition (ii) of Lemma 4.4.\nConsider now the condition (iii) of Lemma 4.4. For positive integers k, let Ek in that condition be the compact set (S \u00d7A\u00d7Dk \u00d7B)m, where Dk is the closed ball of radius k in Rn+1 centered at the origin. We wish to show that for each initial condition x \u2208 X,\nlim k\u2192\u221e lim inf t\u2192\u221e \u00b5x,t(Ek) = 1, Px-a.s.\nSince the \u03b8-iterates do not affect the evolution of Zt, they can be neglected in the proof. It is sufficient to consider instead the m-step version of {Zt} and show that for the compact sets E\u0302k = (S \u00d7A\u00d7Dk)m, it holds for any initial condition z \u2208 Z of Z0 that\nlim k\u2192\u221e lim inf t\u2192\u221e\n\u00b5\u0302 (m) z,t ( E\u0302k ) = 1, Pz-a.s., (4.37)\nwhere {\u00b5\u0302(m)z,t } are the occupation probability measures of the m-step version of {Zt}, defined analogously to (4.36) with (Zt, . . . , Zt+m\u22121) in place of Xt.\nTo prove (4.37), consider {Zt} first and its occupation probability measures {\u00b5\u0302z,t} for each initial condition Z0 = z \u2208 Z. By Theorem 2.2, Pz-almost surely, {\u00b5\u0302z,t} converges weakly to \u03b6 (the unique invariant probability measure of {Zt}). So by [12, Theorem 11.1.1], for the open set D\u0303k = S\u00d7A\u00d7Dok, where Dok denotes the interior of Dk (i.e., D o k is the open ball with radius k), almost surely,\nlim inf t\u2192\u221e\n\u00b5\u0302z,t ( D\u0303k ) \u2265 \u03b6(D\u0303k), and hence lim k\u2192\u221e lim inf t\u2192\u221e \u00b5\u0302z,t ( D\u0303k ) = 1. (4.38)\nNow for the m-step version of {Zt}, with [D\u0303k]m denoting the Cartesian product of m copies of D\u0303k, we have\n\u00b5\u0302 (m) z,t ( [D\u0303k] m )\n:= 1\nt\nt\u22121 \u2211\nj=0\n1 ( Zj+j\u2032 \u2208 D\u0303k, 0 \u2264 j\u2032 < m ) \u2265 1\u2212 m\u22121 \u2211\nj\u2032=0\n1\nt\nt\u22121 \u2211\nj=0\n1 ( Zj+j\u2032 6\u2208 D\u0303k ) . (4.39)\nFor each j\u2032 < m, by the definition of \u00b5\u0302z,t, lim supt\u2192\u221e 1 t \u2211t\u22121 j=0 1 ( Zj+j\u2032 6\u2208 D\u0303k ) = lim supt\u2192\u221e \u00b5\u0302z,t ( D\u0303ck ) , where D\u0303ck denotes the complement of D\u0303k in S\u00d7A\u00d7Rn+1. By (4.38), limk\u2192\u221e lim supt\u2192\u221e \u00b5\u0302z,t ( D\u0303ck ) =\n37\n0 almost surely. Hence for each j\u2032 < m, limk\u2192\u221e lim supt\u2192\u221e 1 t \u2211t\u22121 j=0 1 ( Zj+j\u2032 6\u2208 D\u0303k )\n= 0 almost surely. We then obtain from (4.39), by taking the limits as t \u2192 \u221e and k \u2192 \u221e, that\nlim inf k\u2192\u221e lim inf t\u2192\u221e\n\u00b5\u0302 (m) z,t ( [D\u0303k] m )\n\u2265 1\u2212 m\u22121 \u2211\nj\u2032=0\nlim sup k\u2192\u221e lim sup t\u2192\u221e\n1\nt\nt\u22121 \u2211\nj=0\n1 ( Zj+j\u2032 6\u2208 D\u0303k ) = 1\nalmost surely. The desired equality (4.37) then follows, since [D\u0303k] m \u2282 E\u0302k.\nRecall that Mm\u03b1 is the set of invariant probability measures of the m-step version of {(Zt, \u03b8\u03b1t )}. By Lemma 4.6 the latter Markov chain satisfies the condition (ii) of Lemma 4.4, and this implies that the set {\nP\u0304 (m,k) (z,\u03b8) } k\u22651 is tight for each initial condition (Z0, \u03b8 \u03b1 0 ) = (z, \u03b8). Recall that any subsequence\nof a tight sequence has a further convergent subsequence [12, Theorem 11.5.4]. For { P\u0304 (m,k) (z,\u03b8) } k\u22651 , all the weak limits (i.e., the limits of its convergent subsequences) must be invariant probability measures in Mm\u03b1 , by the property of weak Feller Markov chains given in Lemma 4.3:\nProposition 4.8. Under Assumption 2.1, consider the m-step version of {(Zt, \u03b8\u03b1t )} for m \u2265 1. For each (z, \u03b8) \u2208 Z \u00d7 B, the sequence {\nP\u0304 (m,k) (z,\u03b8) } k\u22651 of probability measures is tight, and any weak\nlimit of this sequence is an invariant probability measure of the m-step version of {(Zt, \u03b8\u03b1t )}. (Thus Mm\u03b1 6= \u2205.)\nWe are now ready to prove Theorems 3.5-3.6. The idea is to use the conclusions on the \u03b8-iterates that we can obtain by applying [19, Theorem 8.2.2], to infer the concentration of the mass around a small neighborhood of (\u03b8\u2217, . . . , \u03b8\u2217) (m copies of \u03b8\u2217) for the marginals of all the invariant probability measures in the set Mm\u03b1 , when \u03b1 is sufficiently small. This can then be combined with Prop. 4.8 to prove the desired conclusions on the \u03b8-iterates for a given stepsize.\nRecall that M\u03b1 is the set of invariant probability measures of {(Zt, \u03b8\u03b1t )}. Recall also that M\u0304m\u03b1 denotes the set of marginals of the invariant probability measures in Mm\u03b1 , on the space of the \u03b8\u2019s.\nProposition 4.9. In the setting of Theorem 3.2, for each \u03b1 > 0, let {\u03b8\u03b1t } be generated instead by the algorithm (2.11) or its perturbed version (3.7), with constant stepsize \u03b1 and under the condition that the initial (Z0, \u03b8 \u03b1 0 ) is distributed according to some invariant probability measure in M\u03b1. Then the conclusions of Theorem 3.2 continue to hold.\nProof. The proof arguments are the same as those for Theorem 3.2 given in Section 4.1. We only need to show that the conditions (ii) and (i\u2032)-(v\u2032) given in Section 4.1.1 for applying [19, Theorem 8.2.2] are still satisfied under our present assumptions.\nFor the algorithm (2.11), the only difference from the previous assumptions in Theorem 3.2 is that here for each stepsize \u03b1, the initial (Z0, \u03b8 \u03b1 0 ) has a distribution \u00b5\u03b1 \u2208 M\u03b1. The condition (ii) does not depend on such initial conditions, so it continues to hold. For the other conditions, note that since {Zt} has a unique invariant probability measure \u03b6 (Theorem 2.2), regardless of the choice of \u00b5\u03b1, for all \u03b1, {Zt} is stationary and has the same distribution. Then the tightness condition (iii\u2032) trivially holds because as {\u03bet} is also stationary and unaffected by the stepsize, each \u03be\u03b1t in (iii\u2032) has the same distribution. Similarly, since {et} is stationary and unaffected by the stepsize, and each et has the same distribution with the mean of \u2016et\u2016 given by E\u03b6 [\u2016et\u2016] < \u221e (Theorem 2.3), we obtain that {et} is u.i. From this the uniform integrability required in the conditions (i\u2032) and (iv\u2032) follows as a consequence, as shown in the proof of Prop. 4.2(ii)-(iv). Lastly, the convergence in mean condition (v\u2032) continues to hold (by the same proof given for Prop. 4.3). This is because {\u03bet} has the same distribution regardless of the stepsize, and because the condition (v\u2032) is for each compact set D and concerns tails of a trajectory starting at instants t with \u03bet \u2208 D, which renders any initial condition on Z0 ineffective. Thus all the required conditions are met, and we obtain the same conclusions on the \u03b8-iterates as given in Theorem 3.2."}, {"heading": "38 Weak Convergence Properties of Constrained ETD Learning", "text": "For the perturbed version (3.7) of the algorithm (2.11), the only difference to (2.11) under the present assumptions is the perturbation variables \u2206\u03b1\u03b8,t involved in each iteration. But by definition these variables have conditional zero mean: E\u03b1t [\u2206 \u03b1 \u03b8,t] = 0, so the only condition in which they appear is the uniform integrability condition (i\u2032): {Y \u03b1t | t \u2265 0, \u03b1 > 0} is u.i., where Y \u03b1t is now given by Y \u03b1t = h(\u03b8 \u03b1 t , \u03bet) + et \u00b7 \u03c9\u0303t+1 + \u2206\u03b1\u03b8,t. By definition \u2206\u03b1\u03b8,t for all \u03b1 and t have bounded variance, and hence {\u2206\u03b1\u03b8,t} is u.i. [6, p. 32]. The set {h(\u03b8\u03b1t , \u03bet) + et \u00b7 \u03c9\u0303t+1 | t \u2265 0, \u03b1 > 0} is u.i., which follows from the u.i. of {et}, as we just verified in the case of the algorithm (2.11). Therefore, by Lemma 4.1(i), {Y \u03b1t | t \u2265 0, \u03b1 > 0} is u.i. and the condition (i\u2032) is satisfied. Since the perturbed version (3.7) meets all the required conditions, and shares with (2.11) the same mean ODE, the same conclusions given in Theorem 3.2 hold for this algorithm as well.\nWe now prove Theorem 3.5 for the algorithm (2.11). We prove its part (i) and part (ii) separately, as the arguments are different. Our proofs below also apply to the perturbed version (3.7) of the algorithm (2.11), and together with the preceding proposition, they establish the first part of Theorem 3.7 (which says that the conclusions of both Theorem 3.2 and Theorem 3.5 hold for the perturbed algorithm).\nProof of Theorem 3.5(i). Proof by contradiction. Consider the statement of Theorem 3.5(i):\n\u2200 \u03b4 > 0, lim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m\u03b1 ) = 1, where m\u03b1 = \u2308m\u03b1 \u2309.\nSuppose it is not true. Then there exist \u03b4, \u01eb > 0, m \u2265 1, a sequence \u03b1k \u2192 0, and a sequence \u00b5\u03b1k \u2208 M\u0304mk\u03b1k , where mk = m\u03b1k , such that\n\u00b5\u03b1k([N\u03b4(\u03b8 \u2217)]mk) \u2264 1\u2212 \u01eb, \u2200 k \u2265 0. (4.40)\nEach \u00b5\u03b1k corresponds to an invariant probability measure of {(Zt, \u03b8\u03b1kt )} in M\u03b1k , which we denote by \u00b5\u0302\u03b1k . For each k \u2265 0, generate the iterates {\u03b8\u03b1kt } using \u00b5\u0302\u03b1k as the initial distribution of (Z0, \u03b8\u03b1k0 ). For other values of \u03b1, generate the iterates {\u03b8\u03b1t } using some \u00b5\u0302\u03b1 \u2208 M\u03b1 as the initial distribution of (Z0, \u03b8 \u03b1 0 ). By Prop. 4.9, the conclusions of Theorem 3.2 hold:\nlim sup \u03b1\u21920\nP (\n\u03b8\u03b1t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k\u03b1, k\u03b1 + T\u03b1/\u03b1 ]\n)\n= 0,\nwhere T\u03b1 \u2192 \u221e as \u03b1 \u2192 0, and this implies for the given m,\nlim sup \u03b1\u21920\nP (\n\u03b8\u03b1t 6\u2208 N\u03b4(\u03b8\u2217), some t \u2208 [ k\u03b1, k\u03b1 + \u2308m\u03b1 \u2309 )\n)\n= 0. (4.41)\nBut for each \u03b1 > 0, the process {(Zt, \u03b8\u03b1t )} with the initial distribution \u00b5\u0302\u03b1 is stationary, so the probability in the left-hand side of (4.41) is just 1 \u2212 \u00b5\u03b1([N\u03b4(\u03b8\u2217)]m\u03b1), for the marginal probability measure \u00b5\u03b1 \u2208 M\u0304m\u03b1\u03b1 that corresponds to the invariant probability measure \u00b5\u0302\u03b1. Therefore, by (4.41), lim inf\u03b1\u21920 \u00b5\u03b1([N\u03b4(\u03b8 \u2217)]m\u03b1) = 1. On the other hand, by (4.40), lim inf\u03b1\u21920 \u00b5\u03b1([N\u03b4(\u03b8 \u2217)]m\u03b1) \u2264 lim infk\u2192\u221e \u00b5\u03b1k([N\u03b4(\u03b8 \u2217)]mk) < 1, a contradiction. Thus the statement of Theorem 3.5(i) recounted at the beginning of this proof must hold. This also proves the other statement of Theorem 3.5(i), lim inf\u03b1\u21920 inf\u00b5\u2208M\u0304m\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m )\n= 1, because for \u03b1 < 1, by the correspondences between those invariant probability measures in Mm\u03b1 and those in Mm\u03b1\u03b1 , inf\u00b5\u2208M\u0304m\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m ) \u2265 inf\u00b5\u2208M\u0304m\u03b1\u03b1 \u00b5 ( [N\u03b4(\u03b8 \u2217)]m\u03b1 ) . This completes the proof.\nProof of Theorem 3.5(ii). We suppress the superscript \u03b1 of \u03b8\u03b1t in the proof. The statement is trivially true if \u03b4 \u2265 2rB, so consider the case \u03b4 < 2rB. Let (z, \u03b8) \u2208 Z \u00d7 B be the initial condition of (Z0, \u03b80). By convexity of the Euclidean norm, \u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 1t \u2211t\u22121\nj=0 |\u03b8j \u2212 \u03b8\u2217|, and therefore, for all k \u2265 1,\nsup k\u2264t<k+m\n\u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 1 k\nk\u22121 \u2211\nj=0\nsup j\u2264t<j+m\n|\u03b8t \u2212 \u03b8\u2217|, (4.42)\n39\nand\nE\n[\nsup k\u2264t<k+m\n\u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223\n]\n\u2264 1 k\nk\u22121 \u2211\nj=0\nE\n[\nsup j\u2264t<j+m\n|\u03b8t \u2212 \u03b8\u2217| ] . (4.43)\nWith N \u2032\u03b4(\u03b8 \u2217) denoting the open \u03b4-neighborhood of \u03b8\u2217, we have\n1\nk\nk\u22121 \u2211\nj=0\nE\n[\nsup j\u2264t<j+m\n|\u03b8t \u2212 \u03b8\u2217| ] \u2264 1 k\nk\u22121 \u2211\nj=0\nE\n[\n(\nsup j\u2264t<j+m\n|\u03b8t \u2212 \u03b8\u2217| ) \u00b7 1 ( \u03b8t \u2208 N \u2032\u03b4(\u03b8\u2217), j \u2264 t < j +m )\n]\n+ 1\nk\nk\u22121 \u2211\nj=0\nE\n[\n(\nsup j\u2264t<j+m\n|\u03b8t \u2212 \u03b8\u2217| ) \u00b7 1 ( \u03b8t 6\u2208 N \u2032\u03b4(\u03b8\u2217), some t \u2208 [j, j +m) )\n]\n\u2264 \u03b4 \u00b7 P\u0304 (m,k)(z,\u03b8) (D\u03b4) + 2rB \u00b7 ( 1\u2212 P\u0304 (m,k)(z,\u03b8) (D\u03b4) ) , (4.44)\nwhere D\u03b4 = {( z1, \u03b81, . . . , zm, \u03b8m ) \u2208 (Z \u00d7B)m \u2223 \u2223 sup1\u2264j\u2264m \u2223 \u2223\u03b8j \u2212 \u03b8\u2217 \u2223 \u2223 < \u03b4 } , and the second inequality follows from the definition (4.35) of the averaged probability measure P\u0304 (m,k) (z,\u03b8) .\nBy Prop. 4.8, { P\u0304 (m,k) (z,\u03b8) } k\u22651 is tight and all its weak limits are in Mm\u03b1 , the set of invariant probability measure of the m-step version of {(Zt, \u03b8t)}. There is also the fact that on a metric space, if a sequence of probability measures \u00b5k converges to some probability measure \u00b5 weakly, then lim infk\u2192\u221e \u00b5k(D) \u2265 \u00b5(D) for any open set D [12, Theorem 11.1.1]. From these two arguments we have that for the set D\u03b4, which is open with respect to the topology on (Z \u00d7B)m,\nlim inf k\u2192\u221e P\u0304 (m,k) (z,\u03b8) (D\u03b4) \u2265 inf\u00b5\u2208Mm\u03b1 \u00b5(D\u03b4) = inf \u00b5\u2208M\u0304m\u03b1 \u00b5 ( [N \u2032\u03b4(\u03b8 \u2217)]m ) =: \u03ba\u03b1,m. (4.45)\nCombining the three inequalities (4.43)-(4.45), and using also the relation \u03b4 < 2rB, we obtain\nlim sup k\u2192\u221e E\n[\nsup k\u2264t<k+m\n\u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223\n]\n\u2264 \u03b4 \u03ba\u03b1,m + 2rB ( 1\u2212 \u03ba\u03b1,m ) .\nWe prove Theorem 3.6 in exactly the same way as we proved Theorem 3.5, so we omit the details and only outline the proof here. First, for the variant algorithms (3.3) and (3.4) as well as their perturbed version (3.7), we consider fixed K and \u03c8K . Similar to Prop. 4.9, we show that if for each stepsize \u03b1, the initial (Z0, \u03b8 \u03b1 0 ) is distributed according to some invariant probability measure in M\u03b1, then the algorithms continue to satisfy the conditions given in Section 4.1.1, so we can apply [19, Theorem 8.2.2] to assert that the conclusions of Theorem 3.2 continue to hold with N\u03b4(\u03b8\n\u2217) replaced by the limit set N\u03b4(LB) of the mean ODE associated with each algorithm. (Recall Theorem 3.4 is also obtained in this way.) Subsequently, with N\u03b4(LB) in place of N\u03b4(\u03b8\n\u2217) again, and with K and \u03c8K still held fixed, we use the same proof for Theorem 3.5(i) to obtain that for any \u03b4 > 0 and m \u2265 1,\nlim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\u03b1\n\u00b5 ( [N\u03b4(LB)] m\u03b1 ) = 1, where m\u03b1 = \u2308m\u03b1 \u2309.\nFinally, we combine this with the fact that given any \u03b4 > 0, the limit set N\u03b4(LB) \u2282 N\u03b4(\u03b8\u2217) for all K sufficiently large (see Lemma 4.2, which holds for (3.3) and (3.4), as well as their perturbed version (3.7) since the latter has the same mean ODE as the original algorithm). Theorem 3.6(i) then follows: given \u03b4 > 0, for all K sufficiently large,\nlim inf \u03b1\u21920 inf \u00b5\u2208M\u0304m\u03b1\u03b1\n\u00b5 ( [N\u03b4(\u03b8 \u2217)]m\u03b1 ) = 1.\nThe proof for Theorem 3.6(ii) is exactly the same as that for Theorem 3.5(ii) given earlier. In particular, this proof relies solely on the weak Feller property of the Markov chain {(Zt, \u03b8\u03b1t )} and"}, {"heading": "40 Weak Convergence Properties of Constrained ETD Learning", "text": "the convergence property of the averaged probability measures of the m-step version of {(Zt, \u03b8\u03b1t )}, all of which were proved for the algorithms (3.3) and (3.4) and their perturbed version (3.7) in this subsection.\nThe preceding arguments also show that the first part of Theorem 3.8 holds; that is, the conclusions of Theorem 3.4 and Theorem 3.6 hold for the perturbed version (3.7) of the algorithm (3.3) or (3.4) as well."}, {"heading": "4.3.3 Proofs of Theorems 3.7 and 3.8", "text": "In this subsection we establish completely Theorems 3.7 and 3.8 regarding the perturbed version (3.7) of the algorithms (2.11), (3.3) and (3.4). We have already proved the first part of both of these theorems in the previous subsection. Below we tackle their second part, which, as we recall, is stronger than the corresponding part of Theorems 3.5 and 3.6 in that for a fixed stepsize \u03b1, the deviation of the averaged iterates {\u03b8\u0304\u03b1t } from \u03b8\u2217 in the limit as t \u2192 \u221e is now characterized not in an expected sense but for almost all sample paths.\nTo simplify the presentation, except where noted otherwise, it will be taken for granted throughout this subsection that {\u03b8\u03b1t } is generated by the perturbed version (3.7) of any of the three algorithms (2.11), (3.3) and (3.4). Recall that when updating \u03b8\u03b1t to \u03b8 \u03b1 t+1, the perturbed algorithm (3.7) adds the perturbation term \u03b1\u2206\u03b8,t to the iterate before the projection \u03a0B, where \u2206\u03b8,t, t \u2265 0, are assumed to be i.i.d. Rn-valued random variables that have zero mean and bounded variances and have a positive continuous density function with respect to the Lebesgue measure. (Here and in what follows, we omit the superscript \u03b1 of the noise terms \u2206\u03b8,t since we deal with a fixed stepsize \u03b1 in this part of the analysis.) As mentioned in Section 3.3, these conditions are not as weak as possible. Indeed, the purpose of the perturbation is to make the invariant probability measure of {(Zt, \u03b8\u03b1t )} unique so that we can invoke the ergodic theorem for weak Feller Markov chains given in Lemma 4.4. Therefore, any conditions that can guarantee the uniqueness of the invariant probability measure can be used. In the present paper, for simplicity, we focus on the conditions we assumed earlier on \u2206\u03b8,t, and prove the uniqueness just mentioned under these conditions, although our proof arguments can be useful for weaker conditions as well.\nProposition 4.10. Under Assumption 2.1, {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure.\nThe next two lemmas are the intermediate steps to prove Prop. 4.10. We need the notion of a stochastic kernel, of which the transition kernel of a Markov chain is one example. For two topological spaces X and Y, a function Q : B(X) \u00d7Y \u2192 [0, 1] is a (Borel measurable) stochastic kernel on X given Y, if for each y \u2208 Y, Q(\u00b7 | y) is a probability measure on B(X) and for each D \u2208 B(X), Q(D | y) is a Borel measurable function on Y. For the algorithms we consider, the iteration that generates (Zt+1, \u03b8 \u03b1 t+1) from (Zt, \u03b8 \u03b1 t ) can be equivalently described in terms of stochastic kernels. In particular, the transition from Zt to Zt+1 is described by the transition kernel of the Markov chain {Zt}, and the probability distribution of \u03b8\u03b1t+1 given \u03b8\u03b1t and \u03bet = (et, Ft, St, At, St+1) is described by another stochastic kernel, which will be our focus in the analysis below.\nLemma 4.7. Let Assumption 2.1(ii) hold. Let Q(d\u03b8\u2032 | \u03be, \u03b8) be the stochastic kernel (on B given \u039e\u00d7B) that describes the probability distribution of \u03b8\u03b1t+1 given \u03bet = \u03be, \u03b8\u03b1t = \u03b8. Then for each bounded set E \u2282 \u039e, there exist \u03b2 \u2208 (0, 1] and a probability measure Q1 on B such that\nQ(d\u03b8\u2032 | \u03be, \u03b8) \u2265 \u03b2 Q1(d\u03b8\u2032), \u2200 \u03be \u2208 E, \u03b8 \u2208 B. (4.46)\nProof. We consider only the case where {\u03b8\u03b1t } is generated by the perturbed version of the algorithm (2.11); the proof for the perturbed version of the two other algorithms (3.3) and (3.4) follows exactly the same arguments. In the proof below we use the notation that for a scalar c and a set D \u2282 Rn, the set cD = {cx | x \u2208 D}.\n41\nBy the definitions of the algorithms (2.11) and (3.7), for \u03be = (e, F, s, a, s\u2032) \u2208 \u039e and \u03b8 \u2208 B, we can express Q(\u00b7 | \u03be, \u03b8) as\nQ(D | \u03be, \u03b8) = \u222b \u222b 1 ( \u03a0B ( \u03b8 + \u03b1f(\u03be, \u03b8, r) + \u03b1\u2206 ) \u2208 D ) p(d\u2206) q(dr | s, a, s\u2032), \u2200D \u2208 B(B), (4.47)\nwhere f(\u03be, \u03b8, r) = e \u00b7 \u03c1(s, a) ( r + \u03b3(s\u2032)\u03c6(s\u2032)\u22a4\u03b8 \u2212 \u03c6(s)\u22a4\u03b8 ) , and p(\u00b7) is the common distribution of the perturbation variables \u2206\u03b8,t. Let r\u0304 > 0 be large enough so that for some c > 0, q([\u2212r\u0304, r\u0304] | s\u0304, a\u0304, s\u0304\u2032) \u2265 c for all (s\u0304, a\u0304, s\u0304\u2032) \u2208 S \u00d7 A\u00d7 S. Let E be an arbitrary bounded subset of \u039e. For all \u03be \u2208 E, \u03b8 \u2208 B and r \u2208 [\u2212r\u0304, r\u0304], since E and B are bounded, g(\u03be, \u03b8, r) := (\u03b8+\u03b1f(\u03be, \u03b8, r))/\u03b1 lies in a compact subset of Rn, which we denote by D\u0304. Let \u01eb \u2208 (0, rB/\u03b1] and let D\u0304\u01eb be the \u01eb-neighborhood of D\u0304. By our assumption on the perturbation variables involved in the algorithm (3.7), p(\u00b7) has a positive continuous density function with respect to the Lebesgue measure \u2113(\u00b7). Therefore, there exists some c\u2032 > 0 such that for any Borel subset D of the compact set \u2212D\u0304\u01eb := {\u2212x | x \u2208 D\u0304\u01eb}, p(D) \u2265 c\u2032\u2113(D).\nNow consider an arbitrary \u03be \u2208 E, \u03b8 \u2208 B, and r \u2208 [\u2212r\u0304, r\u0304]. We have y := g(\u03be, \u03b8, r) \u2208 D\u0304. Let B\u01eb(\u2212y) be the \u01eb-neighborhood of \u2212y, and let B\u01eb denote the closed ball in Rn centered at the origin with radius \u01eb. If \u2206 \u2208 B\u01eb(\u2212y), then \u03b8 + \u03b1f(\u03be, \u03b8, r) + \u03b1\u2206 = \u03b1y + \u03b1\u2206 \u2208 \u03b1B\u01eb \u2282 B (since \u03b1\u01eb \u2264 rB). Therefore, for any D \u2208 B(B),\n\u222b\n1\n(\n\u03a0B ( \u03b8 + \u03b1f(\u03be, \u03b8, r) + \u03b1\u2206 ) \u2208 D ) p(d\u2206) \u2265 \u222b\nB\u01eb(\u2212y)\n1 ( \u03b1y + \u03b1\u2206 \u2208 D ) p(d\u2206)\n\u2265 c\u2032 \u222b\nB\u01eb(\u2212y)\n1 ( \u03b1y + \u03b1\u2206 \u2208 D ) \u2113(d\u2206)\n= c\u2032\u2113 ( 1 \u03b1D \u2229B\u01eb ) , (4.48)\nwhere in the second inequality we used the fact that B\u01eb(\u2212y) \u2282 \u2212D\u0304\u01eb and restricted to B(\u2212D\u0304\u01eb), p(d\u2206) \u2265 c\u2032\u2113(d\u2206), as discussed earlier.\nTo finish the proof, define the probability measure Q1 on B by Q1(D) = \u2113( 1 \u03b1D \u2229 B\u01eb)/\u2113(B\u01eb) for\nall D \u2208 B(B). Then for all \u03be \u2208 E and \u03b8 \u2208 B, using (4.47) and (4.48) and our choice of r\u0304, we have\nQ(D | \u03be, \u03b8) \u2265 \u222b\n[\u2212r\u0304,r\u0304]\nc\u2032\u2113(B\u01eb) \u00b7Q1(D) q(dr | s, a, s\u2032) \u2265 c \u00b7 c\u2032\u2113(B\u01eb) \u00b7Q1(D), D \u2208 B(B),\nand the desired inequality (4.46) then follows by letting \u03b2 = cc\u2032\u2113(B\u01eb) > 0 (we must have \u03b2 \u2264 1 since we can choose D = B in the inequality above).\nWe will use the preceding result in the proof of the next lemma.\nLemma 4.8. Let Assumption 2.1 hold. Let {\u00b5x,t} be the sequence of occupation probability measures of {(Zt, \u03b8\u03b1t )} for each initial condition x \u2208 Z \u00d7 B. Suppose that for some x = (z, \u03b8) \u2208 Z \u00d7 B and \u00b5 \u2208 M\u03b1, {\u00b5x,t} converges weakly to \u00b5, Px-almost surely. Then for each \u03b8\u2032 \u2208 B and x\u2032 = (z, \u03b8\u2032), {\u00b5x\u2032,t} also converges weakly to \u00b5, Px\u2032-almost surely. Proof. We use a coupling argument to prove the statement. In the proof, we suppress the superscript \u03b1 of \u03b8\u03b1t . Let {Xt} denote the process {(Zt, \u03b8t)} with initial condition x = (z, \u03b8), and let {X \u2032t} denote the process {(Zt, \u03b8t)} with initial condition x\u2032 = (z, \u03b8\u2032), for an arbitrary \u03b8\u2032 \u2208 B. In what follows, we first define a sequence {(Zt, \u03b8\u0303t, \u03b8\u0303\u2032t)} with (Z0, \u03b8\u03030, \u03b8\u0303\u20320) = (z, \u03b8, \u03b8\u2032), in such a way that the two marginal processes {(Zt, \u03b8\u0303t)} and {(Zt, \u03b8\u0303\u2032t)} have the same probability distributions as {Xt} and {X \u2032t}, respectively. We then relate the occupation probability measures {\u00b5x,t}, {\u00b5x\u2032,t} to those of the marginal processes, {\u00b5\u0303x,t}, {\u00b5\u0303x\u2032,t}, which are defined as\n\u00b5\u0303x,t(D) = 1\nt\nt\u22121 \u2211\nk=0\n1 ( (Zk, \u03b8\u0303k) \u2208 D ) , \u00b5\u0303x\u2032,t(D) = 1\nt\nt\u22121 \u2211\nk=0\n1 ( (Zk, \u03b8\u0303 \u2032 k) \u2208 D ) , \u2200D \u2208 B(Z \u00d7B)."}, {"heading": "42 Weak Convergence Properties of Constrained ETD Learning", "text": "We now define {(Zt, \u03b8\u0303t, \u03b8\u0303\u2032t)}. First, let {Zt} be generated as before with Z0 = z. Denote \u03bet = (et, Ft, St, At, St+1) as before, and let Q be the stochastic kernel that describes the evolution of \u03b8t+1 given (\u03bet, \u03b8t). By Lemma 4.6, the occupation probability measures of {Zt} is almost surely tight for each initial condition. This implies the existence of a compact set E\u0304 \u2282 Rn+1 such that for the compact set E = E\u0304\u00d7S \u00d7A\u00d7S \u2282 \u039e, the sequence {\u03bet} visits E infinitely often with probability one. For this set E, by Lemma 4.7, there exist some \u03b2 \u2208 (0, 1] and probability measure Q1 on B such that Q(\u00b7 | \u03be\u0304, \u03b8\u0304) \u2265 \u03b2Q1(\u00b7) for all \u03be\u0304 \u2208 E and \u03b8\u0304 \u2208 B. Therefore, on E \u00d7B, we can write Q(\u00b7 | \u03be\u0304, \u03b8\u0304) as the convex combination of Q1 and another stochastic kernel Q0 as follows:\nQ(\u00b7 | \u03be\u0304, \u03b8\u0304) = \u03b2 Q1(\u00b7) + (1\u2212 \u03b2)Q0(\u00b7 | \u03be\u0304, \u03b8\u0304), \u2200 \u03be\u0304 \u2208 E, \u03b8\u0304 \u2208 B, (4.49)\nwhere Q0(\u00b7 | \u03be\u0304, \u03b8\u0304) = [ Q(\u00b7 | \u03be\u0304, \u03b8\u0304)\u2212 \u03b2 Q1(\u00b7) ] /(1\u2212 \u03b2) and Q0 is a stochastic kernel on B given E \u00d7 B. Next, independently of {Zt}, generate a sequence {Yt}t\u22651 of i.i.d., {0, 1}-valued random variables such that Yt = 1 with probability \u03b2 and Yt = 0 with probability 1\u2212 \u03b2. Set Y0 = 0. Let\ntY = min{t \u2265 1 | Yt = 1, \u03bet\u22121 \u2208 E}.\nThen tY < \u221e with probability one. (Since {\u03bet} visits E infinitely often and the process {Yt} is independent of {\u03bet}, this follows easily from applying the Borel-Cantelli lemma to {(\u03betk , Ytk+1)}k\u22651, where tk is when the k-th visit to E by {\u03bet} occurs.)\nNow for each t \u2265 0, let us define the pair (\u03b8\u0303t+1, \u03b8\u0303\u2032t+1) according to the following rule, based on the values of (\u03be0, \u03b8\u03030, \u03b8\u0303 \u2032 0), . . . , (\u03bet, \u03b8\u0303t, \u03b8\u0303 \u2032 t) and (Y0, . . . , Yt, Yt+1):\n(i) In the case t < tY and \u03bet 6\u2208 E, generate \u03b8\u0303t+1 and \u03b8\u0303\u2032t+1 according to Q(\u00b7 | \u03bet, \u03b8\u0303t) and Q(\u00b7 | \u03bet, \u03b8\u0303\u2032t) respectively.\n(ii) In the case t < tY and \u03bet \u2208 E, if Yt+1 = 0, generate \u03b8\u0303t+1 and \u03b8\u0303\u2032t+1 according to Q0(\u00b7 | \u03bet, \u03b8\u0303t) and Q0(\u00b7 | \u03bet, \u03b8\u0303\u2032t) respectively; if Yt+1 = 1, generate \u03b8\u0303t+1 according to Q1(\u00b7) and let \u03b8\u0303\u2032t+1 = \u03b8\u0303t+1.\n(iii) In the case t \u2265 tY , generate \u03b8\u0303t+1 according to Q(\u00b7 | \u03bet, \u03b8\u0303t) and let \u03b8\u0303\u2032t+1 = \u03b8\u0303t+1. In view of (4.49), it can be verified directly by induction on t that the marginal process {(Zt, \u03b8\u0303t)} (resp. {(Zt, \u03b8\u0303\u2032t)}) in the preceding construction has the same probability distribution as {Xt} (resp. {X \u2032t}). This implies that {\u00b5x,t} (resp. {\u00b5x\u2032,t}) converges weakly to \u00b5 with probability one if and only if {\u00b5\u0303x,t} (resp. {\u00b5\u0303x\u2032,t}) converges weakly to \u00b5 with probability one. On the other hand, by construction \u03b8\u0303t = \u03b8\u0303 \u2032 t for t \u2265 tY , where tY < \u221e with probability one, so except on a null set, {\u00b5\u0303x,t} and {\u00b5\u0303x\u2032,t} have the same weak limits. Combining these two arguments with the assumption that {\u00b5x,t} converges weakly to \u00b5 with probability one, it follows that the three sequences {\u00b5\u0303x,t}, {\u00b5\u0303x\u2032,t}, and {\u00b5x\u2032,t} must all converge weakly to \u00b5 with probability one.\nProof of Prop. 4.10. We suppress the superscript \u03b1 of \u03b8\u03b1t in the proof. Let {Xt} = {(Zt, \u03b8t)}. By Prop. 4.8, the set M\u03b1 of invariant probability measures of {Xt} is nonempty. Recall also that since the evolution of {Zt} is not affected by the \u03b8-iterates, the marginal of any \u00b5 \u2208 M\u03b1 on the space Z must equal \u03b6, the unique invariant probability measure of {Zt} (Theorem 2.2).\nSuppose {Xt} has multiple invariant probability measures; i.e., there exist \u00b5, \u00b5\u2032 \u2208 M\u03b1 with \u00b5 6= \u00b5\u2032. Then by [12, Theorem 11.3.2] there exists a bounded continuous function f on Z \u00d7B such that\n\u222b f d\u00b5 6= \u222b f d\u00b5\u2032. (4.50)\nOn the other hand, since \u00b5 is an invariant probability measure of {Xt}, applying a strong law of large numbers for stationary processes [11, Chap. X, Theorem 2.1] (see also [30, Lemma 17.1.1 and Theorem 17.1.2]) to the stationary Markov chain {Xt} with initial distribution \u00b5, we have that there exist a set D1 \u2282 Z \u00d7B with \u00b5(D1) = 1 and a measurable function gf on Z \u00d7B such that (i) for each x \u2208 D1, with the initial condition X0 = x, limt\u2192\u221e 1t \u2211t\u22121 k=0 f(Xk) = gf(x), Px-a.s.;\n(ii) E\u00b5[gf (X0)] = E\u00b5[f(X0)] (i.e., \u222b gfd\u00b5 = \u222b fd\u00b5).\n43\nThe same is true for the invariant probability measure \u00b5\u2032: there exist a set D2 \u2282 Z \u00d7 B with \u00b5\u2032(D2) = 1 and a measurable function g \u2032 f(x) such that\n(i) for each x \u2208 D2, with the initial condition X0 = x, limt\u2192\u221e 1t \u2211t\u22121 k=0 f(Xk) = g \u2032 f(x), Px-a.s.;\n(ii) E\u00b5\u2032 [g \u2032 f (X0)] = E\u00b5\u2032 [f(X0)] (i.e.,\n\u222b\ng\u2032fd\u00b5 \u2032 =\n\u222b\nfd\u00b5\u2032).\nAlso, since {Xt} is a weak Feller Markov chain (Lemma 4.5), by [29, Prop. 4.1], for a set of initial conditions x with \u00b5-measure 1, the occupation probability measures {\u00b5x,t} of {Xt} converge weakly, Px-almost surely, to some (nonrandom) \u00b5\u0303x \u2208 M\u03b1 that depends on the initial x. The same is true for \u00b5\u2032. So by excluding from D1 a \u00b5-null set and from D2 a \u00b5\n\u2032-null set if necessary, we can assume that the sets D1, D2 above also satisfy that for each x \u2208 D1\u222aD2, the occupation probability measures {\u00b5x,t} converge weakly to an invariant probability measure \u00b5\u0303x almost surely. Then since 1 t \u2211t\u22121 k=0 f(Xk) is the same as \u222b\nfd\u00b5x,t for X0 = x, we have, by the weak convergence of {\u00b5x,t} just discussed, that\ngf (x) =\n\u222b fd\u00b5\u0303x for each x \u2208 D1, g\u2032f (x) = \u222b fd\u00b5\u0303x for each x \u2208 D2. (4.51)\nCertainly we must have gf (x) = g \u2032 f(x) onD1\u2229D2. We now relate the values of these two functions at points that share the same z-component. In particular, let proj(D1) denote the projection of D1 on Z: proj(D1) = {z \u2208 Z | \u2203 \u03b8 with (z, \u03b8) \u2208 D1}, and let D1,z be the vertical section of D1 at z: D1,z = {\u03b8 | (z, \u03b8) \u2208 D1}. Define proj(D2) and D2,z similarly. If x = (z, \u03b8) \u2208 D1 \u222a D2 and x\u2032 = (z, \u03b8\u2032) \u2208 D1 \u222aD2, then in view of Lemma 4.8 and the weak convergence of {\u00b5x,t} and {\u00b5x\u2032,t}, we must have \u00b5\u0303x = \u00b5\u0303x\u2032 . Consequently, by (4.51), for each z \u2208 proj(D1), gf (z, \u00b7) is constant on D1,z; for each z \u2208 proj(D2), g\u2032f (z, \u00b7) is constant on D2,z; and for each z \u2208 proj(D1) \u2229 proj(D2), the constants that gf (z, \u00b7), g\u2032f(z, \u00b7) take on D1,z, D2,z, respectively, are the same.\nWe now show \u222b fd\u00b5 = \u222b fd\u00b5\u2032 to contradict (4.50) and finish the proof. Since \u00b5(D1) = \u00b5 \u2032(D2) = 1 and by Theorem 2.2 \u00b5, \u00b5\u2032 have the same marginal distribution on Z, which is \u03b6, there exists a Borel set E \u2282 proj(D1) \u2229 proj(D2) with \u03b6(E) = 1. Consider the sets (E \u00d7 B) \u2229 D1 and (E \u00d7 B) \u2229 D2, which have \u00b5-measure 1 and \u00b5\u2032-measure 1, respectively. By [12, Prop. 10.2.8], we can decompose \u00b5, \u00b5\u2032 into the marginal \u03b6 on Z and the conditional distributions \u00b5(d\u03b8 | z), \u00b5\u2032(d\u03b8 | z) for z \u2208 Z. Then\n1 = \u00b5 ( (E \u00d7B)\u2229D1 ) =\n\u222b\nE\n\u222b\nD1,z\n\u00b5(d\u03b8 | z) \u03b6(dz), 1 = \u00b5\u2032 ( (E \u00d7B)\u2229D2 ) =\n\u222b\nE\n\u222b\nD2,z\n\u00b5\u2032(d\u03b8 | z) \u03b6(dz),\nwhere the equality for the iterated integral in each relation follows from [12, Theorem 10.2.1(ii)]. These relations imply that for some set E0 \u2282 E with \u03b6(E0) = 0,\n\u222b\nD1,z\n\u00b5(d\u03b8 | z) = \u222b\nD2,z\n\u00b5\u2032(d\u03b8 | z) = 1, \u2200 z \u2208 E \\ E0. (4.52)\nWe now calculate \u222b gfd\u00b5 and \u222b g\u2032fd\u00b5 \u2032. We have\n\u222b\ngf d\u00b5 =\n\u222b\n(E\u00d7B)\u2229D1\ngf d\u00b5 =\n\u222b\nE\n\u222b\nD1,z gf(z, \u03b8)\u00b5(d\u03b8 | z) \u03b6(dz), (4.53) \u222b\ng\u2032f d\u00b5 \u2032 =\n\u222b\n(E\u00d7B)\u2229D2\ng\u2032f d\u00b5 \u2032 =\n\u222b\nE\n\u222b\nD2,z\ng\u2032f (z, \u03b8)\u00b5 \u2032(d\u03b8 | z) \u03b6(dz), (4.54)\nwhere the equality for the iterated integral in each relation also follows from [12, Theorem 10.2.1(ii)]. As discussed earlier, for each z \u2208 E \u2282 proj(D1) \u2229 proj(D2), the two constant functions, gf (z, \u00b7) on D1,z and g \u2032 f (z, \u00b7) on D2,z, have the same value. Using this together with (4.52), we conclude that\n\u222b\nD1,z\ngf(z, \u03b8)\u00b5(d\u03b8 | z) = \u222b\nD2,z\ng\u2032f (z, \u03b8)\u00b5 \u2032(d\u03b8 | z), \u2200 z \u2208 E \\ E0. (4.55)"}, {"heading": "44 Weak Convergence Properties of Constrained ETD Learning", "text": "Since \u03b6(E0) = 0, we obtain from (4.53)-(4.55) that\n\u222b\ngf d\u00b5 =\n\u222b\ng\u2032f d\u00b5 \u2032.\nBut \u222b gfd\u00b5 = \u222b fd\u00b5 and \u222b g\u2032fd\u00b5 \u2032 = \u222b fd\u00b5\u2032 (as we obtained at the beginning of the proof), so \u222b\nfd\u00b5 = \u222b fd\u00b5\u2032, a contradiction to (4.50). This proves that {Xt} must have a unique invariant probability measure.\nProposition 4.10 implies that for every m \u2265 1, the m-step version of {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure. This together with Lemma 4.6 furnishes the conditions (A1)-(A3) of [29, Prop. 4.2] for weak Feller Markov chains (these conditions are the conditions (i)-(iii) of our Lemma 4.4). We can therefore apply the conclusions of [29, Prop. 4.2] (see Lemma 4.4 in our Section 4.3.1) to the m-step version of {(Zt, \u03b8\u03b1t )} here, and the result is the following proposition:\nProposition 4.11. Under Assumption 2.1, for each m \u2265 1, the m-step version of {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure \u00b5(m), and the occupation probability measures \u00b5\n(m) (z,\u03b8),t, t \u2265 1, as\ndefined by (4.36), converge weakly to \u00b5(m) almost surely, for each initial condition (z, \u03b8) \u2208 Z \u00d7B of (Z0, \u03b8 \u03b1 0 ).\nWith Prop. 4.11 we can proceed to prove the second part of Theorems 3.7 and 3.8. Given that we have already established their first part in the previous subsection, the arguments for their second part are the same for both theorems and are given below. The proof is similar to that for Theorem 3.5(ii), except that here, instead of working with the averaged probability measures {\nP\u0304 (m,k) (z,\u03b8) } , Prop. 4.11 allows us to work with the occupation probability measures.\nProof of the second part of both Theorem 3.7 and Theorem 3.8. We suppress the superscript \u03b1 of \u03b8\u03b1t in the proof. By Prop. 4.11, {(Zt, \u03b8t)} has a unique invariant probability measure \u00b5\u03b1, and its m-step version has a corresponding unique invariant probability measure \u00b5 (m) \u03b1 . We prove first the statement that for each initial condition (z, \u03b8) \u2208 Z \u00d7B, almost surely,\nlim inf t\u2192\u221e\n1\nt\nt\u22121 \u2211\nk=0\n1\n(\nsup k\u2264j<k+m\n\u2223 \u2223\u03b8j \u2212 \u03b8\u2217 \u2223 \u2223 < \u03b4 ) \u2265 \u00b5\u0304(m)\u03b1 ( [N \u2032\u03b4(\u03b8 \u2217)]m ) , (4.56)\nwhere \u00b5\u0304 (m) \u03b1 is the unique element in M\u0304m\u03b1 , and N \u2032\u03b4(\u03b8\u2217) is the open \u03b4-neighborhood of \u03b8\u2217. For each t, by the definition (4.36) of the occupation probability measure \u00b5 (m) (z,\u03b8),t, the average in the left-hand side above is the same as \u00b5 (m) (z,\u03b8),t(D\u03b4), whereD\u03b4 = {( z1, \u03b81, . . . , zm, \u03b8m ) \u2208 (Z\u00d7B)m \u2223 \u2223 sup1\u2264j\u2264m \u2223\n\u2223\u03b8j\u2212 \u03b8\u2217 \u2223 \u2223 < \u03b4 }\n. By Prop. 4.11, P(z,\u03b8)-almost surely, {\u00b5(m)(z,\u03b8),t} converges weakly to \u00b5 (m) \u03b1 , and therefore,\nexcept on a null set of sample paths, we have by [12, Theorem 11.1.1] that for the open set D\u03b4,\nlim inf t\u2192\u221e\n\u00b5 (m) (z,\u03b8),t(D\u03b4) \u2265 \u00b5(m)\u03b1 (D\u03b4) = \u00b5\u0304(m)\u03b1 ( [N \u2032\u03b4(\u03b8 \u2217)]m ) .\nThis proves (4.56). We now prove the statement that for each initial condition (z, \u03b8) \u2208 Z \u00d7B, almost surely,\nlim sup t\u2192\u221e\n\u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 \u03b4 \u03ba\u03b1 + 2rB (1\u2212 \u03ba\u03b1), where \u03ba\u03b1 = \u00b5\u0304\u03b1 ( N \u2032\u03b4(\u03b8 \u2217) ) , (4.57)\nand \u00b5\u0304\u03b1 is the marginal of \u00b5\u03b1 on B. The statement is trivially true if \u03b4 \u2265 2rB, so consider the case \u03b4 < 2rB. Fix an initial condition (z, \u03b8) \u2208 Z \u00d7B for (Z0, \u03b80), and let {\u00b5(z,\u03b8),t} be the corresponding\n45\noccupation probability measures of {(Zt, \u03b8t)}. For the averaged sequence {\u03b8\u0304t}, by convexity of the norm | \u00b7 |,\n\u2223 \u2223\u03b8\u0304t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 1 t\nt\u22121 \u2211\nk=0\n|\u03b8k \u2212 \u03b8\u2217|. (4.58)\nWe have\n1\nt\nt\u22121 \u2211\nk=0\n|\u03b8k \u2212 \u03b8\u2217| \u2264 1\nt\nt\u22121 \u2211\nk=0\n|\u03b8k \u2212 \u03b8\u2217| \u00b7 1 ( \u03b8k \u2208 N \u2032\u03b4(\u03b8\u2217) )\n+ 1\nt\nt\u22121 \u2211\nk=0\n|\u03b8t \u2212 \u03b8\u2217| \u00b7 1 ( \u03b8k 6\u2208 N \u2032\u03b4(\u03b8\u2217) )\n\u2264 \u03b4 \u00b7 \u00b5(z,\u03b8),t(D\u03b4) + 2rB \u00b7 ( 1\u2212 \u00b5(z,\u03b8),t(D\u03b4) ) , (4.59)\nwhere D\u03b4 = { (z1, \u03b81) \u2208 Z \u00d7 B \u2223 \u2223 |\u03b81 \u2212 \u03b8\u2217| < \u03b4 } . By Prop. 4.11, P(z,\u03b8)-almost surely, {\u00b5(z,\u03b8),t} converges weakly to \u00b5\u03b1. Therefore, except on a null set of sample paths, we have by [12, Theorem 11.1.1] that for the open set D\u03b4,\nlim inf t\u2192\u221e\n\u00b5(z,\u03b8),t(D\u03b4) \u2265 \u00b5\u03b1(D\u03b4) = \u00b5\u0304\u03b1 ( N \u2032\u03b4(\u03b8 \u2217) ) . (4.60)\nCombining the three inequalities (4.58)-(4.60), and using also the relation \u03b4 < 2rB , we obtain that (4.57) holds almost surely for each initial condition (z, \u03b8) \u2208 Z \u00d7B. This completes the proof. Remark 4.1 (on the role of perturbation again). As mentioned before Prop. 4.10, our purpose of perturbing the constrained ETD algorithms is to guarantee that the Markov chain {(Zt, \u03b8\u03b1t )} has a unique invariant probability measure. Without the perturbation, this cannot be ensured, so we cannot apply the ergodic theorem given in Lemma 4.4 to exploit the convergence of occupation probability measures, as we did in the preceding proof, even though {(Zt, \u03b8\u03b1t )} satisfies the remaining two conditions required by that ergodic theorem (cf. Lemma 4.6, Section 4.3.2).\nIn connection with this discussion, let us clarify a point. We know that the occupation probability measures of {Zt} converge weakly to its unique invariant probability measure \u03b6 almost surely for each initial condition of Z0 (Theorem 2.2). But this fact alone cannot rule out the possibility that {(Zt, \u03b8\u03b1t )} has multiple invariant probability measures and that its occupation probability measures do not converge for some initial condition (z, \u03b8).\nFinally, another property of weak Feller Markov chains and its implication for our problem are worth noting here. By [29, Prop. 4.1], for a weak Feller Markov chain {Xt}, provided that an invariant probability measure \u00b5 exists, we have that for a set of initial conditions x with \u00b5-measure 1, the occupation probability measures {\u00b5x,t} converge weakly, Px-almost surely, to an invariant probability measure \u00b5x that depends on the initial condition. Thus, for the unperturbed algorithms (2.11), (3.3) and (3.4), despite the possibility of {(Zt, \u03b8\u03b1t )} having multiple invariant probability measures, the preceding proof can be applied to those initial conditions from which the occupation probability measures converge almost surely. In particular, this argument leads to the following conclusion. In the case of the algorithm (2.11), (3.3) or (3.4), under the same conditions as in Theorem 3.5 or 3.6, it holds for any invariant probability measure \u00b5 of {(Zt, \u03b8\u03b1t )} that for each initial condition (z, \u03b8) from some set of initial conditions with \u00b5-measure 1,\nlim sup t\u2192\u221e\n\u2223 \u2223\u03b8\u0304\u03b1t \u2212 \u03b8\u2217 \u2223 \u2223 \u2264 \u03b4 \u03ba\u03b1 + 2rB (1 \u2212 \u03ba\u03b1) P(z,\u03b8)-a.s.,\nwhere \u03ba\u03b1 = inf\u00b5\u2208M\u0304\u03b1 \u00b5(N \u2032 \u03b4(\u03b8 \u2217)). The limitation of this result, however, is that the set of initial conditions involved is unknown and can be small."}, {"heading": "5 Discussion", "text": "In this section we discuss direct applications of our convergence results to ETD(\u03bb) under relaxed conditions and to two other algorithms, the off-policy TD(\u03bb) algorithm and the ETD(\u03bb, \u03b2) algorithm proposed by Hallak et al. [15]. We then discuss several open issues to conclude the paper."}, {"heading": "46 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "5.1 The Case without Assumption 2.2", "text": "Let Assumption 2.1 hold. Recall from Section 2.3 that ETD(\u03bb) aims to solve the equation C\u03b8+b = 0, where b = \u03a6\u22a4M\u0304 r\u03bb\u03c0,\u03b3 , C = \u2212\u03a6\u22a4G\u03a6 with G = M\u0304(I \u2212 P\u03bb\u03c0,\u03b3). In this paper we have focused on the case where Assumption 2.2 holds and C is negative definite (Theorem 2.1). If Assumption 2.2 does not hold, then either there are less than n emphasized states (i.e., states s with M\u0304ss > 0), or the feature vectors of emphasized states are not rich enough to contain n linearly independent vectors. In either case the function approximation capacity is not fully utilized. It is hence desirable to fulfill Assumption 2.2 by adding more states with positive interest weights i(s) or by enriching the feature representation.\nNevertheless, suppose Assumption 2.2 does not hold (in which case C is negative semidefinite [46]). This essentially has no effects on the convergence properties of the constrained or unconstrained ETD(\u03bb) algorithms, because of the emphatic weighting scheme (2.3)-(2.5), as we explain now.\nLet there be at least one state s with interest weight i(s) > 0 (the case is vacuous otherwise). Partition the state space into the set of emphasized states and the set of non-emphasized states:\nJ1 = {s \u2208 S | M\u0304ss > 0}, J0 = {s \u2208 S | M\u0304ss = 0}.\nCorresponding to the partition, by rearranging the indices of states if necessary, we can write\n\u03a6 =\n[\n\u03a61 \u03a60\n]\n, r\u03bb\u03c0,\u03b3 =\n[\nr1 r0\n]\n, M\u0304 =\n[\nM\u0302 0|J1|\u00d7|J0| 0|J0|\u00d7|J1| 0|J0|\u00d7|J0|\n]\n,\nwhere 0m\u00d7m\u2032 denotes an m \u00d7 m\u2032 zero matrix, M\u0302 is a diagonal matrix with M\u0304ss, s \u2208 J1, as its diagonals. Let Q\u0302 be the sub-matrix of P\u03bb\u03c0,\u03b3 that consists of the entries whose row/column indices are in J1. For the equation C\u03b8+b = 0, clearly b = \u03a6\u22a41 M\u0302r1. Consider now the matrix C. It is shown in the proof of Prop. C.2 in [52] that G has a block-diagonal structure with respect to the partition {J1,J0},\nG =\n[\nG\u0302 0|J1|\u00d7|J0| 0|J0|\u00d7|J1| 0|J0|\u00d7|J0|\n]\n,\nwhere the block corresponding to J0 is a zero matrix as shown above, and the block G\u0302 corresponding to J1 is a positive definite matrix given by\nG\u0302 = M\u0302(I \u2212 Q\u0302), (5.1)\nand M\u0302 can be expressed explicitly as\ndiag(M\u0302) = d1\u03c0o,i \u22a4 (I \u2212 Q\u0302)\u22121, d1\u03c0o,i \u2208 R|J1|, d1\u03c0o,i(s) = d\u03c0o(s) \u00b7 i(s), s \u2208 J1. (5.2)\nThus the matrix C has a special structure:\nTheorem 5.1 (structure of the matrix C; [52, Appendix C.2, p. 41-44]). Let Assumption 2.1 hold, and let i(s) > 0 for at least one state s \u2208 S. Then"}, {"heading": "C = \u2212\u03a6\u22a41 G\u0302\u03a61, where G\u0302 = M\u0302(I \u2212 Q\u0302) is positive definite.", "text": "Let range(A) denote the range space of a matrix A. By the positive definiteness of the matrix G\u0302 given in the preceding theorem, the negative semidefinite matrix C possesses the following properties (we omit the straightforward proof):\nProposition 5.1. Let Assumption 2.1 hold, and let i(s) > 0 for at least one state s \u2208 S. Then the matrix C satisfies that\n47\n(i) range(C) = range(C\u22a4) = span{\u03c6(s) | s \u2208 J1}; and (ii) there exists c > 0 such that for all x \u2208 span{\u03c6(s) | s \u2208 J1}, x\u22a4Cx \u2264 \u2212c |x|2.\nTwo observations then follow immediately:\n(i) Since b = \u03a6\u22a41 M\u0302r1 \u2208 span{\u03c6(s)|s \u2208 J1}, Prop. 5.1(i) shows that the equation C\u03b8 + b = 0 admits a solution, and a unique one in span{\u03c6(s) | s \u2208 J1}, which we denote by \u03b8\u2217.14\n(ii) Prop. 5.1(ii) shows that C acts like a negative definite matrix on the space of feature vectors, span{\u03c6(s)|s \u2208 J1}, that the ETD(\u03bb) algorithms naturally operate on.15 We remark that for an arbitrary negative semidefinite matrix C, neither of these conclusions holds. They hold here as direct consequences of the positive definiteness of the matrix G\u0302 that underlies C, and this positive definiteness property is due to the emphatic weighting scheme (2.3)-(2.5) employed by ETD(\u03bb).\nNow let us discuss the behavior of the constrained ETD(\u03bb) algorithms starting from some state S0 of interest (i.e., i(S0) > 0), in the absence of Assumption 2.2. Recall that earlier we did not need Assumption 2.2 when applying the two general convergence theorems from [19], and we used the negative definiteness of C implied by this assumption only near the end of our proofs to get the solution properties of the mean ODE associated with each algorithm. In the absence of Assumption 2.2, for the unperturbed algorithms (2.11), (3.3) and (3.4), we can simply restrict attention to the subspace span{\u03c6(s)|s \u2208 J1} and use the property in Prop. 5.1(ii) in lieu of negative definiteness. After all, the \u03b8-iterates of these algorithms always lie in the span of the feature vectors if the initial \u03b80, e0 \u2208 span{\u03c6(s)|s \u2208 J1} and in the case of the two biased algorithms (3.3) and (3.4), if the function \u03c8K(x) does not change the direction of x. On the subspace span{\u03c6(s)|s \u2208 J1}, in view of Prop. 5.1(ii), the function |\u03b8 \u2212 \u03b8\u2217|2 serves again as a Lyapunov function for analyzing the ODE solutions in exactly the same way as before. Thus, in the absence of Assumption 2.2, for the algorithms (2.11), (3.3) and (3.4) that set \u03b80, e0 and \u03c8K as just described, and for rB > |b|/c where c is as in Prop. 5.1(ii), the conclusions of Theorems 3.1-3.6 in Section 3 continue to hold with N\u03b4(\u03b8\n\u2217) or N \u2032\u03b4(\u03b8 \u2217) replaced by N\u03b4(\u03b8 \u2217) \u2229 span{\u03c6(s)|s \u2208 J1} or N \u2032\u03b4(\u03b8\u2217) \u2229 span{\u03c6(s)|s \u2208 J1}.\nThe same is true for the almost sure convergence of the unconstrained ETD(\u03bb) algorithm (2.2) under diminishing stepsize: with i(S0) > 0 and \u03b80, e0 \u2208 span{\u03c6(s)|s \u2208 J1}, the conclusion of [52, Theorem 2.2] continues to hold in the absence of Assumption 2.2; that is, for \u03b1t = O(1/t) with \u03b1t\u2212\u03b1t+1\n\u03b1t = O(1/t), \u03b8t a.s.\u2192 \u03b8\u2217. It can be seen now that without Assumption 2.2, complications can only arise through initializing the algorithms outside the desired subspace. We discuss such situations briefly, although they do not seem natural and can be easily avoided. Suppose for some reason we give the initial \u03b80, e0 a component perpendicular to span{\u03c6(s)|s \u2208 J1}. Let i(S0) > 0. The behavior of the unconstrained ETD(\u03bb) algorithm (2.2) is easy to describe. For each t, write \u03b8t = \u03b8 (1) t + \u03b8 (0) t and et = e (1) t + e (0) t , where \u03b8 (1) t , e (1) t \u2208 span{\u03c6(s)|s \u2208 J1} and \u03b8(0)t , e(0)t \u22a5 span{\u03c6(s)|s \u2208 J1}. The latter components do not affect the evolution of {\u03b8(1)t } and {e(1)t }, and they also do not affect the approximate value functions \u03a61\u03b8t = \u03a61\u03b8 (1) t for the emphasized states. For the component process {\u03b8(1)t }, by the\n14From the structures of G, P\u03bb\u03c0,\u03b3 , Q\u0302 and M\u0302 shown in [52, Appendix C.2, p. 41-44], which give rise to (5.1)-(5.2), we also have the following facts. The approximate value function v = \u03a61\u03b8\u2217 for the emphasized states J1 is the unique solution of the projected Bellman equation v = \u03a0(r1 + Q\u0302v), where \u03a0 is the projection onto the column space of \u03a61 with respect to the weighted Euclidean norm on R|J1| defined by the weights M\u0304ss, s \u2208 J1 (the diagonals of M\u0302). The equation v = r1 + Q\u0302v is indeed a generalized Bellman equation for the emphasized states only, and has v\u03c0(s), s \u2208 J1, as its unique solution. Then for the emphasized states, the relation between the approximate value function \u03a61\u03b8\u2217 and v\u03c0 on J1, in particular the approximation error, can again be characterized using the oblique projection viewpoint [40], similar to the case with Assumption 2.2 discussed in Section 2.3.\n15Start ETD(\u03bb) from a state S0 with i(S0) > 0. It can be verified that the emphatic weighting scheme dictates that if St \u2208 J0, then the emphasis weight Mt for that state must be zero. Consequently, et is a linear combination of the features of the emphasized states and the initial e0. So when e0 \u2208 span{\u03c6(s)|s \u2208 J1}, et \u2208 span{\u03c6(s)|s \u2208 J1} always, and if in addition \u03b80 \u2208 span{\u03c6(s)|s \u2208 J1}, then \u03b8t \u2208 span{\u03c6(s)|s \u2208 J1} always. This is very similar to the case of TD(\u03bb) with possibly linearly dependent features discussed in [48]."}, {"heading": "48 Weak Convergence Properties of Constrained ETD Learning", "text": "discussion earlier, for stepsize \u03b1t = O(1/t) with \u03b1t\u2212\u03b1t+1\n\u03b1t = O(1/t) , \u03b8\n(1) t a.s.\u2192 \u03b8\u2217. If e(0)0 = 0, then clearly the components e\n(0) t = 0 and \u03b8 (0) t = \u03b8 (0) 0 through out the iterations. If e (0) 0 6= 0, then by\nrelating to the case where this component is zero and applying Prop. A.2, we have e (0) t a.s.\u2192 0. In the on-policy case with \u03b3(s)\u03bb(s) < 1 for all s, the magnitude of e\n(0) t in fact shrinks exponentially fast\nand consequently it can be shown that {\u03b8(0)t } converges to a point depending on the sample path. In the general off-policy case, depending on how fast e\n(0) t shrinks, \u03b8 (0) t may not converge, we think,\nalthough this does not affect the approximate value function for the emphasized states, as noted earlier.\nFor the constrained ETD(\u03bb) algorithms, if we decompose their iterates into two components as above, the evolution of {\u03b8(1)t } and {e(1)t } can be affected by the components perpendicular to span{\u03c6(s)|s \u2208 J1} through the scaling performed by \u03a0B or \u03c8K (assuming again that \u03c8K(x) maintains the direction of x). Nevertheless, the asymptotic behavior of the algorithms is still characterized by the limit set of their respective mean ODEs. For the algorithm (2.11), the mean ODE is x\u0307 = h\u0304(x) + z, z \u2208 NB(x), where h\u0304(x) = Cx + b = C(x \u2212 \u03b8\u2217). Let rB > |b|/c where c is as in Prop. 5.1(ii). Let (x(\u03c4), z(\u03c4)) be a solution of this ODE with x(0) \u2208 B. Decompose x as x(\u03c4) = x(1)(\u03c4) + x(0)(\u03c4), with x(1) lying inside the subspace span{\u03c6(s)|s \u2208 J1} and x(0) perpendicular to that subspace. Then since h\u0304(x) = C(x(1) \u2212 \u03b8\u2217) \u2208 span{\u03c6(s)|s \u2208 J1} by Prop. 5.1(i), based on the Euclidean geometry and Prop. 5.1(ii), we observe that for V1(\u03c4) = |x(1)(\u03c4) \u2212 \u03b8\u2217|2 and V0(\u03c4) = |x(0)(\u03c4)|2, we have V\u03071(\u03c4) < 0 whenever x(1)(\u03c4) 6= \u03b8\u2217, and V\u03070(\u03c4) \u2264 0 always and V\u03070(\u03c4) < 0 whenever z(\u03c4) 6= 0. Following this observation it can be worked out that the limit set LB \u2282 { \u03b8\u2217 + y | y \u22a5 span{\u03c6(s)|s \u2208 J1} }\n\u2229 B; i.e., LB is a subset of the solutions of C\u03b8 + b = 0 in B. Then the conclusions in Section 3 about the algorithm (2.11) and its perturbed version (3.7) hold with the cylindrical solution neighborhood N\u03b4(LB) or N \u2032 \u03b4(LB) in place of N\u03b4(\u03b8 \u2217) or N \u2032\u03b4(\u03b8 \u2217). Similar conclusions hold for the biased algorithms (3.3) and (3.4) and their perturbed version (3.7), in view of the uniform approximation property given in (4.27) for the functions h\u0304K involved in their mean ODEs. We omit the details in part because it does not seem natural to initialize \u03b80, e0 with a component perpendicular to span{\u03c6(s)|s \u2208 J1} in the first place.\nAs a final note, in the absence of Assumption 2.2, any solution \u03b8\u0304 of C\u03b8+b = 0 gives the same approximate value function for emphasized states, but the approximate values \u03a60\u03b8\u0304 for non-emphasized states in J0 are different for different solutions \u03b8\u0304. Thus one needs to be cautious in using the approximate values \u03a60\u03b8\u0304. They correspond to different extrapolations from the approximate values \u03a61\u03b8 \u2217 for the emphasized states, whereas \u03a61\u03b8 \u2217 is not defined to take into account approximation errors for those states in J0, although its approximation error for emphasized states can be well characterized (cf. Footnote 14)."}, {"heading": "5.2 Off-policy TD(\u03bb)", "text": "Applying TD(\u03bb) to off-policy learning by using importance sampling techniques was first proposed in [36, 35], and the focus there was on episodic data. The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large. Specifically, we consider constant \u03b3 \u2208 [0, 1) and constant \u03bb \u2208 [0, 1], and an infinitely long trajectory generated by the behavior policy as before. The algorithm is the same as TD(\u03bb) except for incorporating the importance sampling weight \u03c1t: 16\n\u03b8t+1 = \u03b8t + \u03b1t et \u00b7 \u03c1t ( Rt + \u03b3\u03c6(St+1) \u22a4\u03b8t \u2212 \u03c6(St)\u22a4\u03b8t ) ,\n16It is not necessary to multiply the term \u03c6(St)\u22a4\u03b8t by \u03c1t, and that version of the algorithm was the one given in [5, 51]. The experimental results in [10] suggest to us that each version can have less variance than the other in some occasions, however. As far as convergence analysis is concerned, the two versions are essentially the same and the analyses given in [51, 52] and this paper indeed apply simultaneously to both versions of the algorithm.\n49\nwhere\net = \u03bb\u03b3\u03c1t\u22121 et\u22121 + \u03c6(St).\nThe constrained versions of the algorithm are defined similarly to those for ETD(\u03bb).\nUnder Assumption 2.1(ii), the associated projected Bellman equation is the same as that for onpolicy TD(\u03bb) [48] except that the projection norm is the weighted Euclidean norm with weights given by the steady state probabilities d\u03c0o(s), s \u2208 S. Assuming \u03a6 has full column rank, the corresponding equation in the \u03b8-space, C\u03b8+ b = 0, has the desired property that the matrix C is negative definite, if \u03bb is sufficiently large (in particular if \u03bb = 1) [5]. For that case, the conclusions given in this paper for constrained ETD(\u03bb) all hold for the corresponding versions of off-policy TD(\u03bb). (Similarly, for the case of C being negative semidefinite due to \u03a6 having rank less than n, the discussion given in the previous subsection for ETD(\u03bb) also applies.) The reason is that besides the property of C, the other properties of the iterates that we used in our analysis, which are given in Section 2 and Appendix A, all hold for off-policy TD(\u03bb). (In fact, some of these properties were first derived for off-policy LSTD(\u03bb) and TD(\u03bb) in [51] and extended later in [52] to ETD(\u03bb).)\nFor the same reason, the convergence analyses we gave in [52] and this paper for ETD also apply to a variation of the ETD algorithm, ETD(\u03bb, \u03b2), proposed recently by Hallak et al. [15], when the parameter \u03b2 is set in an appropriate range."}, {"heading": "5.3 Open Issues", "text": "A major difficulty in applying off-policy TD learning, especially with \u03bb > 0, is the high variances of the iterates. For ETD(\u03bb), off-policy TD(\u03bb) and their least-squares versions, because of the growing variances of products of the importance sampling weights \u03c1t\u03c1t+1 \u00b7 \u00b7 \u00b7 along a trajectory, and because of the amplifying effects these weights can have on the traces, the variances of the traces iterates can grow unboundedly with time, severely affecting the behavior of the algorithms in practice. (The problem of growing variances when applying importance sampling to simulate Markov systems was also known earlier and discussed in prior works; see e.g., [14, 38].) The two biased constrained algorithms discussed in this paper were motivated by the need to mitigate the variance problem, and their robust behavior has been observed in our experiments [28, 53]. However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26]. To overcome the variance problem in off-policy learning, further research is required.\nRegarding convergence analysis of ETD(\u03bb), the results we gave in [52] and this paper concern only the convergence properties and not the rates of convergence. For on-policy TD(\u03bb) and LSTD(\u03bb), convergence rate analyses are available [16, Chap. 6]. Such analyses in the off-policy case will give us better understanding of the asymptotic behavior of the off-policy algorithms. Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying."}, {"heading": "Acknowledgement", "text": "I thank Professors Richard Sutton and Csaba Szepesva\u0301ri for helpful discussions, and I thank two anonymous reviewers for their helpful feedback. This research was supported by a grant from Alberta Innovates\u2014Technology Futures."}, {"heading": "50 Weak Convergence Properties of Constrained ETD Learning", "text": "[2] Antos, A., Szepesv\u0301ari, C., and Munos, R. (2008). Learning near-optimal policies with Bellman residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71:89\u2013129. [3] Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In The 12th International Conference on Machine Learning (ICML). [4] Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific, Belmont, MA. [5] Bertsekas, D. P. and Yu, H. (2009). Projected equation methods for approximate solution of large linear systems. Journal of Computational and Applied Mathematics, 227(1):27\u201350. [6] Billingsley, P. (1968). Convergence of Probability Measures. John Wiley & Sons, New York. [7] Borkar, V. S. (2008). Stochastic Approximation: A Dynamic Viewpoint. Cambridge University Press,\nCambridge. [8] Boyan, J. A. (1999). Least-squares temporal difference learning. In The 16th International Conference\non Machine Learning (ICML). [9] Bradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learning.\nMachine Learning, 22(2):33\u201357. [10] Dann, C., Neumann, G., and Peters, J. (2014). Policy evaluation with temporal differences: A survey\nand comparison. Journal of Machine Learning Research, 15:809\u2013883. [11] Doob, J. L. (1953). Stochastic Processes. John Wiley & Sons, New York. [12] Dudley, R. M. (2002). Real Analysis and Probability. Cambridge University Press, Cambridge. [13] Geist, M. and Scherrer, B. (2014). Off-policy learning with eligibility traces: A survey. Journal of\nMachine Learning Research, 15:289\u2013333. [14] Glynn, P. W. and Iglehart, D. L. (1989). Importance sampling for stochastic simulations. Management\nScience, 35:1367\u20131392. [15] Hallak, A., Tamar, A., Munos, R., and Mannor, S. (2016). Generalized emphatic temporal difference\nlearning: Bias-variance analysis. In The 30th AAAI Conference on Artificial Intelligence. [16] Konda, V. R. (2002). Actor-Critic Algorithms. PhD thesis, MIT. [17] Kushner, H. J. and Clark, D. S. (1978). Stochastic Approximation Methods for Constrained and Un-\nconstrained Systems. Springer-Verlag, New York. [18] Kushner, H. J. and Shwartz, A. (1984). Weak convergence and asymptotic properties of adaptive filters\nwith constant gains. IEEE Transactions on Information Theory, 30:177\u2013182. [19] Kushner, H. J. and Yin, G. G. (2003). Stochastic Approximation and Recursive Algorithms and Appli-\ncations. Springer-Verlag, New York, 2nd edition. [20] Lazaric, A., Ghavamzadeh, M., and Munos, R. (2012). Finite-sample analysis of least-squares policy\niteration. Journal of Machine Learning Research, 13:3041\u20133074. [21] Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M. (2015). Finite-sample analysis of\nproximal gradient TD algorithms. In The 31st Conference on Uncertainty in Artificial Intelligence (UAI). [22] Liu, B., Mahadevan, S., and Liu, J. (2009). Regularized off-policy TD-learning. In Advances in Neural\nInformation Processing Systems (NIPS) 22. [23] Maei, H. R. (2011). Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of\nAlberta. [24] Mahadevan, S. and Liu, B. (2012). Sparse Q-learning with mirror descent. In The 28th Conference on\nUncertainty in Artificial Intelligence (UAI). [25] Mahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., and Liu, J. (2014).\nProximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. arXiv:1405.6757. [26] Mahmood, A. R. and Sutton, R. S. (2015). Off-policy learning based on weighted importance sampling with linear computational complexity. In The 31st Conference on Uncertainty in Artificial Intelligence (UAI). [27] Mahmood, A. R., van Hasselt, H., and Sutton, R. S. (2014). Weighted importance sampling for offpolicy learning with linear function approximation. In Advances in Neural Information Processing Systems (NIPS) 27. [28] Mahmood, A. R., Yu, H., White, M., and Sutton, R. S. (2015). Emphatic temporal-difference learning. In European Workshops on Reinforcement Learning (EWRL). [29] Meyn, S. (1989). Ergodic theorems for discrete time stochastic systems using a stochastic Lyapunov function. SIAM Journal on Control and Optimization, 27:1409\u20131439.\n51\n[30] Meyn, S. and Tweedie, R. L. (2009). Markov Chains and Stochastic Stability. Cambridge University Press, Cambridge, 2nd edition. [31] Munos, R. and Szepesv\u0301ari, C. (2008). Finite time bounds for fitted value iteration. Journal of Machine Learning Research, 9:815\u2013857. [32] Neveu, J. (1975). Discrete-Parameter Martingales. North-Holland, Amsterdam. [33] Pires, B. A. and Szepesv\u0301ari, C. (2012). Statistical linear estimation with penalized estimators: An\napplication to reinforcement learning. In The 29th International Conference on Machine Learning (ICML). [34] Polyak, B. T. and Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30:838\u2013855. [35] Precup, D., Sutton, R. S., and Dasgupta, S. (2001). Off-policy temporal-difference learning with function\napproximation. In The 18th International Conference on Machine Learning (ICML). [36] Precup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for off-policy policy evaluation. In\nThe 17th International Conference on Machine Learning (ICML). [37] Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John\nWiley & Sons, New York. [38] Randhawa, R. S. and Juneja, S. (2004). Combining importance sampling and temporal difference\ncontrol variates to simulate Markov chains. ACM Transactions on Modeling and Computer Simulation, 14(1):1\u201330. [39] Saad, Y. (2003). Iterative Methods for Sparse Linear Systems. SIAM, Philadelphia, 2nd edition. [40] Scherrer, B. (2010). Should one compute the temporal difference fix point or minimize the Bellman\nresidual? The unified oblique projection view. In The 27th International Conference on Machine Learning (ICML). [41] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3:9\u201344. [42] Sutton, R. S. (1995). TD models: Modeling the world at a mixture of time scales. In The 12th International Conference on Machine Learning (ICML). [43] Sutton, R. S. (2009). The grand challenge of predictive empirical abstract knowledge. In IJCAI Workshop on Grand Challenges for Reasoning from Experiences. [44] Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning. MIT Press, Cambridge, MA. [45] Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesva\u0301ri, C., and Wiewiora, E.\n(2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. In The 26th International Conference on Machine Learning (ICML). [46] Sutton, R. S., Mahmood, A. R., and White, M. (2016). An emphatic approach to the problem of off-policy temporal-difference learning. Journal of Machine Learning Research, 17(73):1\u201329. [47] Sutton, R. S., Szepesva\u0301ri, C., and Maei, H. (2008). A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation. In Advances in Neural Information Processing Systems (NIPS) 21. [48] Tsitsiklis, J. N. and Van Roy, B. (1997). An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674\u2013690. [49] Ueno, T., Maeda, S., Kawanabe, M., and Ishii, S. (2011). Generalized TD learning. Journal of Machine Learning Research, 12:1977\u20132020. [50] Yao, H. S. and Liu, Z. Q. (2008). Preconditioned temporal difference learning. In The 25th International Conference on Machine Learning (ICML). [51] Yu, H. (2012). Least squares temporal difference methods: An analysis under general conditions. SIAM Journal on Control and Optimization, 50:3310\u20133343. [52] Yu, H. (2015). On convergence of emphatic temporal-difference learning. http://arxiv.org/abs/1506.02582; a shorter version appeared in The 28th Annual Conference on Learning Theory (COLT), 2015. [53] Yu, H. (2016). Some simulation results for emphatic temporal-difference learning algorithms. http://arxiv.org/abs/1605.02099. [54] Yu, H. and Bertsekas, D. P. (2010). Error bounds for approximations from projected linear equations. Mathematics of Operations Research, 35(2):306\u2013329. [55] Yu, H. and Bertsekas, D. P. (2012). Weighted Bellman equations and their applications in approximate dynamic programming. LIDS Technical Report 2876, MIT."}, {"heading": "52 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "Appendix A: Key Properties of Trace Iterates", "text": "In this appendix we list four key properties of trace iterates {(et, Ft)} generated by the ETD(\u03bb) algorithm. Three of them were derived in [52, Appendix A], and used in the convergence analysis of ETD(\u03bb) in both [52] and the present paper.\nAs discussed in Section 3.2, {(et, Ft)} can have unbounded variances and is naturally unbounded in common off-policy situations. However, as the proposition below shows, {(et, Ft)} is bounded in a stochastic sense.\nProposition A.1. Under Assumption 2.1, given a bounded set E \u2282 Rn+1, there exists a constant L < \u221e such that if the initial (e0, F0) \u2208 E, then supt\u22650 E [ \u2225 \u2225(et, Ft) \u2225 \u2225 ] < L.\nThe preceding proposition is the same as [52, Prop. A.1] except that the conclusion is for all the initial (e0, F0) from the set E, instead of a fixed initial (e0, F0). By making explicit the dependence of the constant L on the initial (e0, F0), the same proof of [52, Prop. A.1] (which is a relatively straightforward calculation) applies to the preceding proposition.\nWe note that Prop. A.1 does not imply the uniform integrability of {(et, Ft)}\u2014this stronger property does hold for the trace iterates, as we proved in Prop. 4.2(i). (The latter and its proof focus on {et} only, but the same argument applies to {(et, Ft)}.)\nThe next proposition concerns the change in the trace iterates due to the change in its initial condition. It is the same as [52, Prop. A.2]; its proof is more involved than the proofs of the two other properties of the trace iterates and uses, among others, a theorem for nonnegative random processes [32]. We did not use this proposition directly in the analysis of the present paper, but it is important in establishing that the Markov chain {Zt} has a unique invariant probability measure (Theorem 2.2), which the results of the present paper rely on. In addition, it is helpful for understanding the behavior of the trace iterates.\nLet (e\u0302t, F\u0302t), t \u2265 1, be defined by the same recursion (2.3)-(2.5) that defines (et, Ft), using the same state and action random variables {(St, At)}, but with a different initial condition (e\u03020, F\u03020). We write a zero vector in any Euclidean space as 0.\nProposition A.2. Under Assumption 2.1, for any two given initial conditions (e0, F0) and (e\u03020, F\u03020),\nFt \u2212 F\u0302t a.s.\u2192 0, et \u2212 e\u0302t a.s.\u2192 0.\nThe third proposition below concerns approximating the trace iterates (et, Ft) by truncated traces that depend on a fixed number of the most recent states and actions only. First, let us express the traces (et, Ft), by using their definitions (cf. Eqs. (2.3)-(2.5)), as\nFt = F0 \u00b7 ( \u03c10\u03b31 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) +\nt \u2211\nk=1\ni(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) , (A.1)\net = e0 \u00b7 ( \u03b21 \u00b7 \u00b7 \u00b7\u03b2t ) + t \u2211\nk=1\nMk \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) , (A.2)\nwhere \u03b2k = \u03c1k\u22121\u03b3k\u03bbk and\nMk = \u03bbk i(Sk) + (1\u2212 \u03bbk)Fk.\nFor each integer K \u2265 1, the truncated traces (e\u0303t,K , F\u0303t,K) are defined by limiting the summations in (A.1)-(A.2) to be over K + 1 terms only as follows:\n(e\u0303t,K , F\u0303t,K) = (et, Ft) for t \u2264 K,\n53\nand for t \u2265 K + 1,\nF\u0303t,K = t \u2211\nk=t\u2212K\ni(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) , (A.3)\nM\u0303t,K = \u03bbt i(St) + (1\u2212 \u03bbt)F\u0303t,K , (A.4)\ne\u0303t,K =\nt \u2211\nk=t\u2212K\nM\u0303k,K \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) . (A.5)\nWe have the following approximation property for truncated traces, in which the notation \u201cLK \u2193 0\u201d means that LK decreases monotonically to 0 as K \u2192 \u221e.\nProposition A.3. Let Assumption 2.1 hold. Given a bounded set E \u2282 Rn+1, there exist constants LK ,K \u2265 1, with LK \u2193 0 as K \u2192 \u221e, such that if the initial (e0, F0) \u2208 E, then\nsup t\u22650 E\n[\n\u2225 \u2225(et, Ft)\u2212 (e\u0303t,K , F\u0303t,K) \u2225 \u2225\n]\n\u2264 LK .\nThe preceding proposition is the same as [52, Prop. A.3(i)], except that the initial (e0, F0) can be from a bounded set E instead of being fixed. The proof given in [52] applies here as well, similar to the case of Prop. A.1. This proposition about truncated traces was used in [52] to obtain the convergence in mean given in Theorem 2.3 and allowed us to work with simple finite-space Markov chains, instead of working with the infinite-space Markov chain {Zt} directly, in that proof. In the present paper, it has expedited our proofs of Props. 4.2, 4.3 regarding the uniform integrability and convergence in mean conditions for constrained ETD(\u03bb).\nFinally, the uniform integrability of {(et, Ft)} (proved in Prop. 4.2(i) in this paper, as already mentioned) is important both for convergence analysis and for understanding the behavior of the trace iterates."}], "references": [{"title": "Adaptive importance sampling technique for Markov chains using stochastic approximation. Operations Research, 54:489\u2013504", "author": ["T.P. Ahamed", "V.S. Borkar", "S. Juneja"], "venue": "Weak Convergence Properties of Constrained ETD Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning near-optimal policies with Bellman residual minimization based fitted policy iteration and a single sample path", "author": ["A. Antos", "C. Szepesv\u0301ari", "R. Munos"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In The 12th International Conference on Machine Learning (ICML)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Projected equation methods for approximate solution of large linear systems", "author": ["D.P. Bertsekas", "H. Yu"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Convergence of Probability Measures", "author": ["P. Billingsley"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "Stochastic Approximation: A Dynamic Viewpoint", "author": ["V.S. Borkar"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "In The 16th International Conference on Machine Learning (ICML)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Stochastic Processes", "author": ["J.L. Doob"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1953}, {"title": "Real Analysis and Probability", "author": ["R.M. Dudley"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Importance sampling for stochastic simulations", "author": ["P.W. Glynn", "D.L. Iglehart"], "venue": "Management Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "In The 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Actor-Critic Algorithms", "author": ["V.R. Konda"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H.J. Kushner", "D.S. Clark"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1978}, {"title": "Weak convergence and asymptotic properties of adaptive filters with constant gains", "author": ["H.J. Kushner", "A. Shwartz"], "venue": "IEEE Transactions on Information", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1984}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H.J. Kushner", "G.G. Yin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Finite-sample analysis of least-squares policy iteration", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Finite-sample analysis of proximal gradient TD algorithms", "author": ["B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik"], "venue": "In The 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Regularized off-policy TD-learning", "author": ["B. Liu", "S. Mahadevan", "J. Liu"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta", "author": ["H.R. Maei"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Sparse Q-learning with mirror descent", "author": ["S. Mahadevan", "B. Liu"], "venue": "In The 28th Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. arXiv:1405.6757", "author": ["S. Mahadevan", "B. Liu", "P. Thomas", "W. Dabney", "S. Giguere", "N. Jacek", "I. Gemp", "J. Liu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "In The 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Weighted importance sampling for offpolicy learning with linear function approximation", "author": ["A.R. Mahmood", "H. van Hasselt", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "In European Workshops on Reinforcement Learning (EWRL)", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Ergodic theorems for discrete time stochastic systems using a stochastic Lyapunov function", "author": ["S. Meyn"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Markov Chains and Stochastic Stability", "author": ["S. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Finite time bounds for fitted value iteration", "author": ["R. Munos", "C. Szepesv\u0301ari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Discrete-Parameter Martingales", "author": ["J. Neveu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1975}, {"title": "Statistical linear estimation with penalized estimators: An application to reinforcement learning", "author": ["B.A. Pires", "C. Szepesv\u0301ari"], "venue": "In The 29th International Conference on Machine Learning (ICML)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1992}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "In The 18th International Conference on Machine Learning (ICML)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In The 17th International Conference on Machine Learning (ICML)", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Combining importance sampling and temporal difference control variates to simulate Markov chains", "author": ["R.S. Randhawa", "S. Juneja"], "venue": "ACM Transactions on Modeling and Computer Simulation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Iterative Methods for Sparse Linear Systems", "author": ["Y. Saad"], "venue": "SIAM, Philadelphia,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "In The 27th International Conference on Machine Learning (ICML)", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In The 12th International Conference on Machine Learning (ICML)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1995}, {"title": "The grand challenge of predictive empirical abstract knowledge", "author": ["R.S. Sutton"], "venue": "In IJCAI Workshop on Grand Challenges for Reasoning from Experiences", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In The 26th International Conference on Machine Learning (ICML)", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "H. Maei"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}, {"title": "Generalized TD learning", "author": ["T. Ueno", "S. Maeda", "M. Kawanabe", "S. Ishii"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Preconditioned temporal difference learning", "author": ["H.S. Yao", "Z.Q. Liu"], "venue": "In The 25th International Conference on Machine Learning (ICML)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning. http://arxiv.org/abs/1506.02582; a shorter version appeared in The 28th", "author": ["H. Yu"], "venue": "Annual Conference on Learning Theory (COLT),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Some simulation results for emphatic temporal-difference learning algorithms. http://arxiv.org/abs/1605.02099", "author": ["H. Yu"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Error bounds for approximations from projected linear equations", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Mathematics of Operations Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}], "referenceMentions": [{"referenceID": 44, "context": "The ETD(\u03bb) algorithm was recently proposed by Sutton, Mahmood, and White [46] to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest.", "startOffset": 73, "endOffset": 77}, {"referenceID": 39, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 46, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 42, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 41, "context": "In addition, insofar as value functions (with respect to different reward/cost assignments) reflect statistical properties of future outcomes, off-policy learning can be used by an autonomous agent to build an experience-based internal model of the world in artificial intelligence applications [43].", "startOffset": 295, "endOffset": 299}, {"referenceID": 44, "context": "In this paper we focus on a new off-policy learning algorithm proposed recently by Sutton, Mahmood, and White [46]: the emphatic temporal-difference (TD) learning algorithm, or ETD(\u03bb).", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "The algorithm is similar to the standard TD(\u03bb) algorithm with linear function approximation [41], but uses a novel scheme to resolve a long-standing divergence problem in TD(\u03bb) when applied to off-policy data.", "startOffset": 92, "endOffset": 96}, {"referenceID": 46, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 169, "endOffset": 176}, {"referenceID": 46, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 169, "endOffset": 176}, {"referenceID": 42, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 222, "endOffset": 229}, {"referenceID": 44, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 251, "endOffset": 259}, {"referenceID": 26, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 251, "endOffset": 259}, {"referenceID": 3, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 51, "endOffset": 58}, {"referenceID": 49, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 51, "endOffset": 58}, {"referenceID": 7, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 137, "endOffset": 143}, {"referenceID": 6, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 137, "endOffset": 143}, {"referenceID": 45, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 43, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 21, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 20, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 19, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 102, "endOffset": 110}, {"referenceID": 23, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "(See the surveys [13, 10] for other algorithm examples.", "startOffset": 17, "endOffset": 25}, {"referenceID": 8, "context": "(See the surveys [13, 10] for other algorithm examples.", "startOffset": 17, "endOffset": 25}, {"referenceID": 44, "context": "An important result of this weighting scheme is that under natural conditions on the function approximation architecture, the average dynamics of ETD(\u03bb) can be described by an affine function involving a negative definite matrix [46, 52], which provides a desired stability property, similar to the case of convergent on-policy TD algorithms.", "startOffset": 229, "endOffset": 237}, {"referenceID": 50, "context": "An important result of this weighting scheme is that under natural conditions on the function approximation architecture, the average dynamics of ETD(\u03bb) can be described by an affine function involving a negative definite matrix [46, 52], which provides a desired stability property, similar to the case of convergent on-policy TD algorithms.", "startOffset": 229, "endOffset": 237}, {"referenceID": 50, "context": "The almost sure convergence of ETD(\u03bb), under general off-policy training conditions, has been shown in our recent work [52] for diminishing stepsize.", "startOffset": 119, "endOffset": 123}, {"referenceID": 48, "context": "An efficient algorithm for solving the estimated equations is the one given in [50] based on the line search method.", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "It can also be applied to finding approximate solutions under additional penalty terms suggested by [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 44, "context": "The papers [46, 28] work with the negation of the matrix that we associate with ETD(\u03bb) in this paper.", "startOffset": 11, "endOffset": 19}, {"referenceID": 26, "context": "The papers [46, 28] work with the negation of the matrix that we associate with ETD(\u03bb) in this paper.", "startOffset": 11, "endOffset": 19}, {"referenceID": 44, "context": "The negative definiteness property we discuss here corresponds to the positive definiteness property discussed in [46, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 26, "context": "The negative definiteness property we discuss here corresponds to the positive definiteness property discussed in [46, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 15, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 16, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 17, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 15, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 16, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 17, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 50, "context": "Some of these properties were established earlier in our work [52] when analyzing the almost sure convergence of ETD(\u03bb).", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "We use ergodic theorems for weak Feller Markov chains [29, 30], together with the properties of ETD(\u03bb) iterates and the convergence results we get from the weak convergence methods, in this second part of our analysis.", "startOffset": 54, "endOffset": 62}, {"referenceID": 28, "context": "We use ergodic theorems for weak Feller Markov chains [29, 30], together with the properties of ETD(\u03bb) iterates and the convergence results we get from the weak convergence methods, in this second part of our analysis.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "(This is not a surprise, for the biased algorithms are in fact defined by using a well-known robustifying approach from stochastic approximation theory [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 26, "context": ") Their behavior is demonstrated by experiments in [28, 53].", "startOffset": 51, "endOffset": 59}, {"referenceID": 51, "context": ") Their behavior is demonstrated by experiments in [28, 53].", "startOffset": 51, "endOffset": 59}, {"referenceID": 51, "context": "In particular, [53] is our companion note for this paper and includes several simulation results to illustrate some of the theorems we give here regarding the behavior of multiple consecutive iterates of the biased algorithms.", "startOffset": 15, "endOffset": 19}, {"referenceID": 50, "context": "2 Preliminaries In this section we describe the policy evaluation problem in the off-policy case, the ETD(\u03bb) algorithm and its constrained version, and we also review the results from our prior work [52] that are needed in this paper.", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "Let \u03b3(s) \u2208 [0, 1], s \u2208 S, be state-dependent discount factors, with \u03b3(s) < 1 for at least one state.", "startOffset": 11, "endOffset": 17}, {"referenceID": 39, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 57, "endOffset": 65}, {"referenceID": 46, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 57, "endOffset": 65}, {"referenceID": 44, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": ", [37]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "We focus on a general form of the ETD(\u03bb) algorithm, which uses state-dependent \u03bb values specified by a function \u03bb : S \u2192 [0, 1].", "startOffset": 120, "endOffset": 126}, {"referenceID": 0, "context": "The algorithm can access the following functions, in addition to the features \u03c6(s): (i) the state-dependent discount factor \u03b3(s) that defines v\u03c0 , as described earlier; (ii) \u03bb : S \u2192 [0, 1], which determines the single or multi-step Bellman equation for the algorithm (cf.", "startOffset": 182, "endOffset": 188}, {"referenceID": 40, "context": "Associated with ETD(\u03bb) is a generalized multistep Bellman equation of which v\u03c0 is the unique solution [42]: 6 v = r \u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v.", "startOffset": 102, "endOffset": 106}, {"referenceID": 44, "context": "5) we use here differs slightly from the original definition of et in [46], but the two are equivalent and (2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 81, "endOffset": 89}, {"referenceID": 42, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 81, "endOffset": 89}, {"referenceID": 44, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "Earlier works on using such equations in TD learning include [42] and [4, Chap.", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "The recent work [49] considers an even broader class of Bellman equations using the concept", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": "6) [46], which takes the following forms in the space of approximate value functions and in the space of the \u03b8-parameters, respectively: v = \u03a0 ( r \u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v ) , v \u2208 column-space(\u03a6), \u21d0\u21d2 C\u03b8 + b = 0, \u03b8 \u2208 R.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "9), the diagonal matrix M\u0304 is determined by the steady state probabilities of the states under the target policy \u03c0 under an ergodicity assumption [48], and for off-policy TD(\u03bb), it is determined by the steady state probabilities d\u03c0o(s) under the behavior policy \u03c0 .", "startOffset": 146, "endOffset": 150}, {"referenceID": 44, "context": "A salient property of ETD(\u03bb) is that the matrix C is always negative semidefinite [46], and under natural and mild conditions, C is negative definite.", "startOffset": 82, "endOffset": 86}, {"referenceID": 50, "context": "This is proved in [52] and summarized below.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "The relation between the approximate value function v = \u03a6\u03b8 and the desired value function v\u03c0, in particular, the approximation error, can be characterized by using the oblique projection viewpoint [40] for projected Bellman equations.", "startOffset": 197, "endOffset": 201}, {"referenceID": 38, "context": "Briefly speaking, [40] showed that the solutions of projected Bellman equations are oblique projections of v\u03c0 on the approximation subspace.", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "An oblique projection is defined by two nonorthogonal subspaces of equal dimensions and is the projection onto the first subspace orthogonally to the second [39].", "startOffset": 157, "endOffset": 161}, {"referenceID": 13, "context": ") Recently, for the case of constant \u03bb, i and \u03b3, [15] derived bounds on the approximation bias that are based on contraction arguments and are comparable to the bound for on-policy TD(\u03bb) [48].", "startOffset": 49, "endOffset": 53}, {"referenceID": 46, "context": ") Recently, for the case of constant \u03bb, i and \u03b3, [15] derived bounds on the approximation bias that are based on contraction arguments and are comparable to the bound for on-policy TD(\u03bb) [48].", "startOffset": 187, "endOffset": 191}, {"referenceID": 52, "context": "[54] and [55, Sec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "To prepare for the analysis, in the rest of this section, we review several results from [52] that will be needed.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Then applying powerful convergence theorems from the stochastic approximation theory [19], we can assert that the iterates \u03b8t will eventually \u201cfollow closely\u201d a solution of the mean ODE.", "startOffset": 85, "endOffset": 89}, {"referenceID": 50, "context": "It is shown in [52] that under Assumption 2.", "startOffset": 15, "endOffset": 19}, {"referenceID": 50, "context": "It was proved in [52] as a special case of the convergence of averaged sequences for a larger set of functions including h(\u03b8, \u00b7).", "startOffset": 17, "endOffset": 21}, {"referenceID": 50, "context": "The convergence of the averaged sequence 1t \u2211t\u22121 k=0 g(\u03bek) is given in the theorem below; the part on convergence in mean will be used frequently later in this paper (and was actually also needed in [52] to prove the ergodicity of {Zt} given earlier).", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "These results are obtained through applying two general convergence theorems from [19], which concern weak convergence of stochastic approximation algorithms for diminishing and constant stepsize.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 16, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "The theorems from [19] which we will apply are based on the weak convergence methods.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "In the framework of [19], one studies a trajectory of iterates produced by an algorithm by working with continuous-time processes that are piecewise constant or linear interpolations of the iterates.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Thus we can apply several ergodic theorems for weak Feller Markov chains (Meyn [29], Meyn and Tweedie [30]) to analyze the constant-stepsize case and combine the implications from these theorems with the results we obtained previously using stochastic approximation theory.", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Thus we can apply several ergodic theorems for weak Feller Markov chains (Meyn [29], Meyn and Tweedie [30]) to analyze the constant-stepsize case and combine the implications from these theorems with the results we obtained previously using stochastic approximation theory.", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "This iterative averaging is also known as \u201cPolyak-averaging\u201d when it is applied to accelerate the convergence of the \u03b8-iterates (see [34], [19, Chap.", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "The uniqueness allows us to invoke a result of Meyn [29] on the convergence We adopt these conditions for simplicity.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "We will apply two theorems from [19], Theorems 8.", "startOffset": 32, "endOffset": 36}, {"referenceID": 50, "context": "Our proofs will rely on many properties of the ETD iterates that we have established in [52] when analyzing the almost sure convergence of the algorithm.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "This algorithm belongs to the class of stochastic approximation algorithms with \u201cexogenous noises\u201d studied in the book [19]\u2014the term \u201cexogenous noises\u201d reflects the fact that the evolution of {\u03bet} is not driven by the \u03b8-iterates.", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "1 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "3 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "4 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "5 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "7 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "6 of [19] for the case of constant stepsize), the following conditions are required.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "1 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "7 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "8 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "9 of [19], and it is in fact stronger than the latter condition but is satisfied by our algorithms as we will show.", "startOffset": 5, "endOffset": 9}, {"referenceID": 50, "context": "The proofs build upon several key properties of the ETD iterates we have established in [52] and recounted in Section 2.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "For such a Markov chain {Xt} with state space X, let P (\u00b7, \u00b7) denote its transition kernel, that is, P : X\u00d7 B(X) \u2192 [0, 1], P (x,D) = Px(X1 \u2208 D), \u2200x \u2208 X, D \u2208 B(X), where B(X) denotes the Borel sigma-algebra on X, and Px denotes the probability distribution of {Xt} conditioned on X0 = x.", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "For t \u2265 1, the t-step transition kernel P t(\u00b7, \u00b7) : X\u00d7 B(X) \u2192 [0, 1] is given by P (x,D) = Px(Xt \u2208 D), \u2200x \u2208 X, D \u2208 B(X), and for t = 0, P 0 is defined as P (x, \u00b7) = \u03b4x, the Dirac measure that assigns probability 1 to the point x, for each x \u2208 X.", "startOffset": 62, "endOffset": 68}, {"referenceID": 27, "context": "1 in [29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "It is a result of Meyn [29] and will be needed in our proofs of Theorems 3.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "For two topological spaces X and Y, a function Q : B(X) \u00d7Y \u2192 [0, 1] is a (Borel measurable) stochastic kernel on X given Y, if for each y \u2208 Y, Q(\u00b7 | y) is a probability measure on B(X) and for each D \u2208 B(X), Q(D | y) is a Borel measurable function on Y.", "startOffset": 61, "endOffset": 67}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "2 does not hold (in which case C is negative semidefinite [46]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 50, "context": "2 in [52] that G has a block-diagonal structure with respect to the partition {J1,J0}, G = [", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "2 when applying the two general convergence theorems from [19], and we used the negative definiteness of C implied by this assumption only near the end of our proofs to get the solution properties of the mean ODE associated with each algorithm.", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "Then for the emphasized states, the relation between the approximate value function \u03a61\u03b8 and v\u03c0 on J1, in particular the approximation error, can again be characterized using the oblique projection viewpoint [40], similar to the case with Assumption 2.", "startOffset": 207, "endOffset": 211}, {"referenceID": 46, "context": "This is very similar to the case of TD(\u03bb) with possibly linearly dependent features discussed in [48].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "2 Off-policy TD(\u03bb) Applying TD(\u03bb) to off-policy learning by using importance sampling techniques was first proposed in [36, 35], and the focus there was on episodic data.", "startOffset": 119, "endOffset": 127}, {"referenceID": 33, "context": "2 Off-policy TD(\u03bb) Applying TD(\u03bb) to off-policy learning by using importance sampling techniques was first proposed in [36, 35], and the focus there was on episodic data.", "startOffset": 119, "endOffset": 127}, {"referenceID": 3, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 49, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 8, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 0, "context": "Specifically, we consider constant \u03b3 \u2208 [0, 1) and constant \u03bb \u2208 [0, 1], and an infinitely long trajectory generated by the behavior policy as before.", "startOffset": 63, "endOffset": 69}, {"referenceID": 3, "context": ", It is not necessary to multiply the term \u03c6(St)\u03b8t by \u03c1t, and that version of the algorithm was the one given in [5, 51].", "startOffset": 113, "endOffset": 120}, {"referenceID": 49, "context": ", It is not necessary to multiply the term \u03c6(St)\u03b8t by \u03c1t, and that version of the algorithm was the one given in [5, 51].", "startOffset": 113, "endOffset": 120}, {"referenceID": 8, "context": "The experimental results in [10] suggest to us that each version can have less variance than the other in some occasions, however.", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "As far as convergence analysis is concerned, the two versions are essentially the same and the analyses given in [51, 52] and this paper indeed apply simultaneously to both versions of the algorithm.", "startOffset": 113, "endOffset": 121}, {"referenceID": 50, "context": "As far as convergence analysis is concerned, the two versions are essentially the same and the analyses given in [51, 52] and this paper indeed apply simultaneously to both versions of the algorithm.", "startOffset": 113, "endOffset": 121}, {"referenceID": 46, "context": "1(ii), the associated projected Bellman equation is the same as that for onpolicy TD(\u03bb) [48] except that the projection norm is the weighted Euclidean norm with weights given by the steady state probabilities d\u03c0o(s), s \u2208 S.", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "Assuming \u03a6 has full column rank, the corresponding equation in the \u03b8-space, C\u03b8+ b = 0, has the desired property that the matrix C is negative definite, if \u03bb is sufficiently large (in particular if \u03bb = 1) [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 49, "context": "(In fact, some of these properties were first derived for off-policy LSTD(\u03bb) and TD(\u03bb) in [51] and extended later in [52] to ETD(\u03bb).", "startOffset": 90, "endOffset": 94}, {"referenceID": 50, "context": "(In fact, some of these properties were first derived for off-policy LSTD(\u03bb) and TD(\u03bb) in [51] and extended later in [52] to ETD(\u03bb).", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": ") For the same reason, the convergence analyses we gave in [52] and this paper for ETD also apply to a variation of the ETD algorithm, ETD(\u03bb, \u03b2), proposed recently by Hallak et al.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[15], when the parameter \u03b2 is set in an appropriate range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": ", [14, 38].", "startOffset": 2, "endOffset": 10}, {"referenceID": 36, "context": ", [14, 38].", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ") The two biased constrained algorithms discussed in this paper were motivated by the need to mitigate the variance problem, and their robust behavior has been observed in our experiments [28, 53].", "startOffset": 188, "endOffset": 196}, {"referenceID": 51, "context": ") The two biased constrained algorithms discussed in this paper were motivated by the need to mitigate the variance problem, and their robust behavior has been observed in our experiments [28, 53].", "startOffset": 188, "endOffset": 196}, {"referenceID": 36, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 122, "endOffset": 129}, {"referenceID": 34, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 33, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 25, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 50, "context": "Regarding convergence analysis of ETD(\u03bb), the results we gave in [52] and this paper concern only the convergence properties and not the rates of convergence.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 1, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 18, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 19, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}], "year": 2017, "abstractText": "We consider the emphatic temporal-difference (TD) algorithm, ETD(\u03bb), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD(\u03bb) algorithm was recently proposed by Sutton, Mahmood, and White [46] to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD(\u03bb) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD(\u03bb) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(\u03bb), our analysis also applies to the off-policy TD(\u03bb) algorithm, when the divergence issue is avoided by setting \u03bb sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD(\u03bb) with constant or slowly diminishing stepsize.", "creator": "LaTeX with hyperref package"}}}