{"id": "1511.07497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Constrained Structured Regression with Convolutional Neural Networks", "abstract": "modeled convolutional neural networks ( cnns ) have seemingly recently emerged as the dominant interactive model in computer vision. if provided with enough training data, they predict almost any visual quantity. in a discrete setting, such disciplines as classification, cnns are practically not only able to remotely predict a neural label but often predict a confidence in the form of a probability distribution over the output space. in continuous regression tasks, such a probability estimate is often lacking. we present a regression based framework which models the output distribution of neural networks. this output distribution allows us to infer the most likely labeling following a set of physical or modeling constraints. these constraints capture the intricate interplay between different input and output variables, and complement the output of a cnn. however, they may not hold everywhere. our concurrent setup further allows to learn a confidence with which a constraint holds, in the form of a distribution of the constrain satisfaction. moreover we evaluate our approach on the problem of intrinsic image decomposition, and show that constrained structured regression significantly increases the state - of - the - art.", "histories": [["v1", "Mon, 23 Nov 2015 22:43:37 GMT  (3544kb,D)", "http://arxiv.org/abs/1511.07497v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["deepak pathak", "philipp kr\\\"ahenb\\\"uhl", "stella x yu", "trevor darrell"], "accepted": false, "id": "1511.07497"}, "pdf": {"name": "1511.07497.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Deepak Pathak", "Philipp Kr\u00e4henb\u00fchl", "Stella X. Yu", "Trevor Darrell"], "emails": ["pathak@berkeley.edu", "philkr@berkeley.edu", "stellayu@berkeley.edu", "trevor@berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Structured regression lies at the heart of some of the most active computer vision problems of our time. Examples include optical flow (Horn & Schunck, 1981), monocular depth estimation (Eigen et al., 2014), intrinsic image decomposition (Barron & Malik, 2015) etc. Convolutional neural networks (CNNs) (Fukushima, 1980; Krizhevsky et al., 2012; LeCun et al., 1989) have greatly advanced the state of the art in all those structured output tasks (Eigen et al., 2014; Narihira et al., 2015a). However CNNs predict each output independently, and thus ignore the intricate interplay of the output variables imposed by physical or modeling constraints. Instead they are forced to learn all physical properties of a scene directly from the training data, and often fail due to the limited capacity of the model.\nIn this work, we propose to bring those dependencies back to deep structured regression, in the form of constraints on the output space. A constraint ties the output of several regression targets together. In a naive first approach, we learn a standard deep structured regression model and find the closest solution to the predicted structured output that follows the constraints strictly, using a simple Euclidean distance measure. This results in an averaging of the output. It makes only limited use of the training data, and further assumes that the constraints are always satisfied, which is not true in general. For instance, in the task of intrinsic images, Lambert\u2019s law (Barrow & Tenenbaum, 1978) assumes that surfaces have diffused reflectance, which means product of shading and albedo images is equal to the original image. However, this is not true for specular surfaces like mirrors, metals etc (Zhang et al., 1999), such that a naive approach does not perform well in those areas.\nTo make full use of the training data, we regress not just to a single output variable, but rather a fully factorized distribution over possible outputs. We further predict a distribution over each of our constrains, allowing it not be violated under certain conditions. These distributions capture a the confidence with which the model makes its predictions, or the confidence that a certain constraint holds. A highly confident prediction is peaked around the correct answer, while an uncertain prediction will be more uniform. We use these confidences, and pick the most likely output labeling\nar X\niv :1\n51 1.\n07 49\n7v 1\n[ cs\n.C V\n] 2\n3 N\nov 2\n01 5\nfollowing our constraints. This allows the model to trust outputs differently during inference, see Figure 1 for an overview of our framework.\nWe apply our structured regression framework to the problem of intrinsic image decomposition (Barrow & Tenenbaum, 1978). The goal of intrinsic image decomposition is to decompose the input image into albedo (also called reflectance image) and shading images. The output space, in such tasks, has dependencies on the input which can be modeled as physics based constraints such as Lambertian lighting assumption in intrinsic image decomposition (Barrow & Tenenbaum, 1978). At inference time, we find a structured output following those constraints. This alleviates the pressure on the CNN to explicitly learn all physical properties of a scene, and allows it to focus more on the statistical correlations between the input image and the output.\nIn summary, our constrained regression framework learns not only to capture the ground-truth values, but also to capture the variation or confidence in its own predictions. Moreover, our constraints re-introduce the coupling between albedo and shading that has been ignored in the prior supervised learning. We achieve significant improvement over the state of the art performance and show large visual improvement on MPI Sintel dataset."}, {"heading": "2 RELATED WORK", "text": "Modeling Weight Uncertainty in Neural Networks In neural networks, uncertainty is usually modeled as distributions over the learned network weights (Denker & Lecun, 1991; MacKay, 1992). If the weights are drawn from a Gaussian distribution then the neural network approximates a Gaussian process in the limit. In such a network, the output distribution is inferred through Bayesian inference. Recently, Blundell et al. (2015) presented a back-propagation based variational technique to learn weight uncertainty. The Bayesian inference based regularization in deep networks is closely related to dropout (Gal & Ghahramani, 2015). Our approach in this paper is not to learn weight uncertainty, but to directly learn the parameters by assuming some output distribution. This allows for much faster inference and training by leveraging the standard feed forward inference and discriminative training for network parameters.\nDeep Structured Prediction Most of the problems involve predicting multiple outputs which may be dependent. Such structured prediction tasks are mostly modeled using MRFs. To extend the success of CNN on a small output space to a large one, if the global scoring function on the output can be decomposed into the sum of local scoring functions depending on a small subsets of the output, Chen et al. (2015) shows that the loss function is still representable and there could be an efficient iterative solution. Jaderberg et al. (2015) train CNN to model unaries for the structured prediction task in a max-margin setting for text recognition. Jointly learning CRF and CNN using mean field inference has been shown to achieve great results in segmentation(Zheng et al., 2015). However, most of these works are applied for discrete prediction tasks. For continuous settings, great results have been achieved in depth estimation using CRF formulations, where unary potentials come from a CNN that maps the image to the depth value at single pixels or super-pixels, and where pairwise potentials reflect the desired smoothness of depth predictions between neighboring pixels or segments (Liu et al., 2015; Wang et al., 2015; Li et al., 2015). Compared to these models, not\nonly our CNN formulation captures the variation in its own predictions, but our constraints also provide an easier and more efficient alternative to higher-order clique potentials. We view structured regression task as constraint satisfaction problem and show that learning variance and distribution of output makes it possible to learn and enforce constraints at inference.\nIntrinsic Image Decomposition Intrinsic image decomposition by definition is ambiguous. Traditional approaches seek various physics or statistics based priors on albedo and shading such as sparse and piecewise constant albedo and smooth shading (Horn, 1974; Grosse et al., 2009), or more recently from additional depth images (Lee et al., 2012; Barron & Malik, 2015; Chen & Koltun, 2013). MIT intrinsics, Sintel, and Intrinsics In the Wild datasets (Grosse et al., 2009; Butler et al., 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a). Narihira et al. (2015b) regress to a globally consistent ranking of albedo values, however they do not obtain a full intrinsic image decomposition. Zhou et al. (2015) and Zoran et al. (2015) both predict the pairwise ordering of albedo values and then a CRF or a constrained quadratic program to turn this pairwise orderings into a globally consistent and spatially smooth intrinsic image decomposition. Neither method attempted to predict the intrinsic image decomposition directly, due to the lack of training data for real world images. Direct intrinsics (Narihira et al., 2015a) is the first brute-force approach aiming to associate albedo and shading directly with the input image by training convolutional regressors from synthetic ground-truth data. However, lost in the simplicity of regression are the strong coupling constraints in the form of the intrinsics decomposition equation. Our formulation addresses this weakness."}, {"heading": "3 PRELIMINARIES", "text": "We first introduce the necessary notation and define the common learning objective for structured regression task. We then give a probabilistic interpretation of this regression. Section 4 combines this structured regression with a set of physical inference-time constraints, in our constrained structured regression framework. We learn a complete probabilistic model for structured regression, modeling uncertainties in both the output variables and the constraints. This derivation is general and not application specific. Section 5 then applies our structured regression to intrinsic image decomposition. Section 6 provides more details on our network architecture and training procedure.\nConsider the problem of regression to an output Y \u2208 RN given an input image I . A deep convolutional regression approximates Y = f(I; \u03b8) with a multi-layer convolutional neural network with parameters \u03b8. Those parameters \u03b8 are learned from a set of training images I = {I1, I2, . . .} and corresponding ground truth targets Y\u0303 = {Y\u03031, Y\u03032, . . .}. One of the most common learning objectives is a Euclidean loss : `(I, Y\u0303 ) = (f(I; \u03b8)\u2212 Y\u0303 )2, (1) for each image I \u2208 I and target Y\u0303 \u2208 Y\u0303 in our training set. This Euclidean loss has a nice probabilistic interpretation as the negative log-likelihood of a fully factorized normal distribution Yk \u223c N (\u00b5k, \u03c32k) with mean \u00b5k = fk(I; \u03b8) and unit standard deviation \u03c3k = 1 for each output Yk (Koller & Friedman, 2009). In the following section, we use this probabilistic interpretation to enforce constrains on the output space, and find the most likely output Y following these constraints."}, {"heading": "4 CONSTRAINED STRUCTURED REGRESSION", "text": "Many structured tasks have an inter-dependency of input and output spaces, which may be known either due to physical properties of the problem or due to a modeling choice. Let\u2019s suppose that this knowledge is captured by the following family of constraints:\nG(Y, I) = 0, (2) where G is a constraint function acting on the output Y and the input image I . In practice, we limit ourselves to affine constraints G(Y, I) = AY + BI + c, which results in an efficient inference due to the convexity of constrained region. Note that such a property, although not necessary, is desirable and thus informs our modeling choice. For instance, we perform regression in log domain in intrinsics so that constraints are affine. See Sections 5 for more details."}, {"heading": "4.1 CONSTRAINED INFERENCE", "text": "During inference, we want to find the most likely output estimate Y under the probability distribution predicted by the CNN such that it follows our known family of constraints. Recall from Section 3 that all outputs are modeled by a normal distribution Y \u223c N (f(I; \u03b8), 1), where the mean is learned. Lets denote the probability of an output Y as PN (Y |\u03b8) = N (Y ; f(I; \u03b8), 1). Thus, finding the most likely output Y can be written as a constrained optimization problem, at inference time, as follows:\nminimize Y \u2212 logPN (Y |\u03b8) = (Y \u2212 f(I; \u03b8))2\nsubject to G(Y, I) = 0. (3)\nIf constraints are affine in the output Y , this inference is convex for a fixed set of training parameters. Specifically, this is a quadratic program because the negative log likelihood \u2212 logPN is a quadratic function, and thus can be solved either in closed form or using QP solvers.\nNote that in Equation (3), all outputs Yk are assumed to have the same distribution with different mean values. This is obviously not true, as some quantities are easier to estimate than others, hence the CNN should be more confident in some areas than others. We will now show how to learn a more general capturing a full distribution for each output Yk."}, {"heading": "4.2 PROBABILISTIC REGRESSION", "text": "We use the probabilistic interpretation of the Euclidean loss (1) and also learn the standard deviation of our output as \u03c32k = gk(I; \u03b8). This standard deviation can be interpreted as a simple confidence estimate of our prediction. A small standard deviation signifies a confident prediction, while a large standard deviation corresponds to an uncertain prediction. The resulting learning objective is expressed as follows:\n`(I, Y\u0303 ) = \u2212 \u2211 k logPN (Y\u0303k|\u00b5k, \u03c32k)\n= 1\n2 \u2211 k\n( (fk(I; \u03b8)\u2212 Y\u0303 )2\ngk(I; \u03b8) + log gk(I; \u03b8) + log 2\u03c0\n) , (4)\nwhere PN is a normal distribution, and both f and g are convolutional neural networks parametrized by \u03b8, learned through the same objective. This is standard negative log-likelihood minimization (Koller & Friedman, 2009). For a fixed g this reduces to Euclidean loss. On the other hand, if g is completely free i.e. no regularization, then the objective reduces to an L1 norm with gk = |fk\u2212Y\u0303 | predicting the magnitude of that norm, hence the predicted error. In practice, we parametrize g as the exponential of the network output to ensure positivity. We also add a small l2 regularization to the objective (4).\nThis probabilistic regression allows us to reason about which outputs to follow more strictly when enforcing our constraints. However all constraints are still strictly enforced. This is fine for setups where it is known that the constraint have to be satisfied, but it is not true for a general scenario as all constraints might not hold for all outputs. Motivated by this, we now consider to learn the uncertainty in constraints as well and then incorporate it in the inference time optimization, similar to Equation (3)."}, {"heading": "4.3 PROBABILISTIC CONSTRAINTS", "text": "In structured prediction tasks, we have the knowledge of constraints that our output should satisfy. If the constraints are not strictly satisfied everywhere, the easiest mechanism would be to learn the distribution over constraint satisfaction in a similar way as we learned the distribution of output in our structured regression setup. More specifically, for ith constraint we write:\nGi(Y, I) \u223c H(0, \u03c32i ), (5)\nwhere H is a continuous probability distribution. In our experiments, we restrict H to be a zeromean Gaussian or Laplace distribution with learned standard deviation, \u03c32i = hi(I; \u03b8), where h is a convolutional neural network with same parameters \u03b8 as for outputs. For a given output, if the\nstandard deviation \u03c3i is low, then our confidence about the ith constraint being satisfied will be high and vice-versa.\nTo learn this distributionH, we again follow the standard negative log-likelihood minimization, i.e. minimize\nG\n\u2211 i ( \u2212 logPH(Gi|0, \u03c32i ) ) , as mentioned in detail in Section 4.2. We now see how can we\nincorporate this constraint modeling in the inference framework."}, {"heading": "4.4 PROBABILISTIC CONSTRAINED INFERENCE", "text": "We conclude by combining the modeled distributions of outputs and constraints in a joint optimization problem to obtain a better estimate of output. Incorporating distributions of output in the optimization is similar to the one described in Equation (3), however, handling the distribution of constraints is not apparently obvious. To address this, interestingly, we can express Equation (5) in terms of a slack variable. This interpretation reduces H to a distribution over slack on the family of constraints G. Thus, we find most likely output Y subject to certain constraints such that the slack on the constraints follows distributionH. This is written as follows:\nminimize Y, \u2212 logPN (Y |\u03b8)\u2212 logPH(\u03be|\u03b8)\nsubject to Gi(Y, I) = \u03be, (6)\nwhere \u03be is the slack random variable. The resulting formulation, if constraints are affine, is again convex for a fixed training parameters. For constraints modeled as a Gaussian or Laplacian distribution, the optimization (6) is again a Quadratic Program. Hence, it can be optimized easily for an accurate solution.\nWe can better understand the Equation (6) in terms of confidences i.e. standard deviation. If constraint has higher confidence, we would like to change the outputs with less confidence more and stick to the predicted value in case of outputs with more confidence to obtain the final estimates, and vice-versa. This process is jointly achieved in this optimization since on taking log-likelihood the standard deviation becomes the weight of the squared error as discussed in Section 3."}, {"heading": "4.5 RELATION TO CONDITIONAL RANDOM FIELDS", "text": "So far, we have looked at the inference process from a constrained optimization perspective. We want to enforce some known constraints on a structured output space. We first introduce the hard constraint scenario in Equation (3), then learn the distribution over outputs (Eq. 4) and constraints (Eq. 5) to jointly phrase the final optimization in Equation (6) as a constrained optimization problem with a distribution over slack. In this section, we discuss an alternative perspective to directly justify the Equation (6) by showing a connection with conditional random fields (CRF).\nLets ignore the l2 regularization for this analogy. We can relate the distribution defined for our output in Section 3 to the unary potential of a CRF i.e. \u03a8k(Yk; \u03b8) = PN (Yk|\u03b8). Further, the constraint distribution to define the joint distribution over outputs can be re-parameterized by the constraint as \u03a8i(Y ; \u03b8) = PH(Gi(Y )|\u03b8). The joint inference of such a CRF would be similar to the Langrangian dual of the problem (6). The log-likelihood learning formulation described in our setup can then be theoretically justified using the piecewise-learning procedure proposed by Sutton & McCallum (2009). During piecewise learning of factors, the contribution of individual outputs often gets over-counted which is determined using cross validation (Shotton et al., 2009; Sutton & McCallum, 2009). However, we empirically found the cross-validation not to be necessary and weight the unary and pairwise terms equally, which gets handled by Langrangian."}, {"heading": "5 INTRINSIC IMAGE DECOMPOSITION", "text": "As a proof of concept, we apply our approach to the intrinsic image decomposition. Given an input image I , the task is to decompose it into albedoA and shading S (also called reflectance) image. The actual color of the contents in image is captured in albedo, while shading captures the illumination and shape information. The physical constraint which is usually considered in this setting is based on Lambert\u2019s law, i.e. I = A \u2217 S. However, it holds only for the surfaces where incident light is diffused equally in all directions, also called Lambertian surfaces. It is not true for Specular surfaces,\nlike mirrors, which maintain the direction of reflection. This constraint provides an important cue for inferring A and S given I . Mostly, this constraint is assumed to be true everywhere and only one of A or S is optimized (Chen & Koltun, 2013; Barron & Malik, 2015). Narihira et al. (2015a) use CNN to regress to the targets A,S, hoping that it would implicitly learn these dependencies which is not true since the outputs are blurry and doesn\u2019t contain any high frequency information. By reasoning and combining evidence from both albedo and shading CNN, we achieve significant improvement.\nTo keep the optimization convex at inference, we aim to keep the constraints affine in terms of output. In this setting, working in log domain ensures that the intrinsic constraint is affine. Thus our constraint is G = A+S\u2212I = 0, where I , A and S are the image, albedo and shading in log domain respectively. The albedo A is a three channeled RGB image. Shading S = B \u00b7 C is modeled by single channel gray-scale image B, capturing the light intensity, and global color value C, capturing the color of the light. We found this formulation to be numerically more stable than modeling S using three scalars per pixel.\nWe now discussion the instantiation of our method for this task.\nLEARNING\nWe learn both the distribution of outputs as well as the constraints as discussed in Section 4. Our outputs are Y1 = A and Y2 = S. Their distribution is learned using the log-likelihood loss defined in Equation (4). We learn the distribution of Lambertian constraint G using loss described in Section 4.3. Ideally one should modelH as Laplacian distribution, as the true violation follows it closely. However in practice we found that Gaussian works equally well and is easier to optimize.\nWe tie the standard deviation of albedo distribution \u03c3A across channels at each pixel. Thus, our estimated standard deviations are single channeled heatmaps for each of the output as well as constraint. To summarize, our CNN at training learns to predict three channeled albedo A, and single channeled shading S, \u03c3A, \u03c3S , \u03c3G at every pixel.\nINFERENCE\nDuring inference, we estimate the albedo A and shading S using the probabilistic constrained optimization defined in Section 4.4. Special care has to be taken when optimizing the shading S factored into gray scale values B and global color C. While the coverall problem is still convex, we found it easier to minimize objective 6 using an alternating minimization. We first keep the current estimate for albedo A and B fixed and optimize for a global light color C, then keep A and C fixed and\noptimize for B, and finally solve for the albedo A keeping the shading S = B \u00b7 C fixed. Each of those steps has a closed form solution."}, {"heading": "6 IMPLEMENTATION DETAILS", "text": "We evaluate our constrained structured regression framework on the task of intrinsic image decomposition. Our CNN architecture for modeling the output and constraints distribution is derived from VGG architecture (Simonyan & Zisserman, 2015). It is similar to VGG from conv1 to conv4, which is then followed by three stride 2 deconvolution to upsample the output to original size. Each of these deconvolution layers are followed by rectified linear unit (ReLU) activation function. We initialize truncated-VGG layers using Imagenet (Russakovsky et al., 2015) pretrained weights, and randomly initialize the deconvolution layers.\nNotice that albedo and shading for a given image are measured upto a scale, similar to depth. Thus, we use scale invariant learning procedure (Eigen et al., 2014). We adjust the scale of our prediction by a global constant for each of our output and constraint distribution. For log domain, it is equivalent to adjusting optimal shift i.e. for output Yk with target Y\u0303k, optimal shift \u03b1 is computed as follows\nminimize \u03b1 \u2212 \u2016\u03b1+ Yk \u2212 Y\u0303k\u201622 + \u03b2\u2016\u03b1\u201622,\nwhere \u03b2 = 0.5 is the regularization coefficient. Note that regularization \u03b2 is crucial to prevent the network from learning arbitrary scaling. We shift output as Yk = Yk + \u03b1 and continue the learning procedure as described in Section 4.\nOur complete implementation is done in caffe (Jia et al., 2014) and we use ADAM (Kingma & Ba, 2015) as stochastic gradient solver. The learning rate is kept fixed at 1e\u2212 4 throughout the process. Our method trains in 5K-10K iterations, taking about 4-5 hours using cpu implementation of constraint structured loss. Complete source code and trained models will be released upon acceptance of publication."}, {"heading": "7 EXPERIMENTS", "text": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes. Following Narihira et al. (2015a), we do not train on the image-based split proposed by Chen & Koltun (2013) because there is a large overlap in the image content in training and testing set. Instead, we report results on the scene split and report the average error using 2-fold cross-validation, similar to Narihira et al. (2015a).\nWe compare all algorithm using the mean squared error (MSE), local mean squared error (LMSE) and structural dissimilarity (DSSIM) metric as proposed by Chen & Koltun (2013). While both MSE and LMSE measure the raw pixelwise difference between predictions, DSSIM tried to capture the perceptual visual difference between outputs.\nTable 1 compares different variations of our algorithm. We start with a baseline L2 loss on our VGG based architecture. While this L2 loss performs quite well, combining it with inference time constraints significantly degrades the performance. Our distribution based loss performs similarly to the simple L2 loss, however it absorbs the constraints much better. Even in this setting constraints actually hurt the raw numeric performance in all settings except for SSIM, meaning that the L2 pixelwise error increased while the perceptual quality of the output slightly increased. However, as soon as we learn the constraint satisfaction our structured regression framework significantly outperforms the baselines.\nFinally, we compare our direct intrinsic regression output to the state-of-the-art intrinsic image decompositions in Table 2. Note that we outperform the best prior work by anywhere between 10% and 20% in relative terms."}, {"heading": "8 DISCUSSION", "text": "We proposed a generic framework for deep structured regression, exploiting inference time constraints. The constrained structured regression framework is general and easy to train. The method is potentially applicable to a broad set of application areas e.g., monocular depth estimation Eigen et al. (2014), optical flow prediction Horn & Schunck (1981), etc. We illustrated performance on a state of the art intrinsic image decomposition task, where our results confirmed that adding structure back to inference can substantially improve the performance of deep regression models both visually and quantitatively."}], "references": [{"title": "Shape, illumination, and reflectance from shading", "author": ["Barron", "Jonathan T", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Barron et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barron et al\\.", "year": 2015}, {"title": "Recovering intrinsic scene characteristics from images", "author": ["HG Barrow", "Tenenbaum", "JM"], "venue": "Computer vision systems,", "citeRegEx": "Barrow et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Barrow et al\\.", "year": 1978}, {"title": "Intrinsic images in the wild", "author": ["Bell", "Sean", "Bala", "Kavita", "Snavely", "Noah"], "venue": "Siggraph,", "citeRegEx": "Bell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2014}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1505.05424,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "A naturalistic open source movie for optical flow evaluation", "author": ["Butler", "Daniel J", "Wulff", "Jonas", "Stanley", "Garrett B", "Black", "Michael J"], "venue": "In ECCV,", "citeRegEx": "Butler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Butler et al\\.", "year": 2012}, {"title": "Learning deep structured models", "author": ["Chen", "Liang-Chieh", "Schwing", "Alexander G", "Yuille", "Alan L", "Urtasun", "Raquel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Qualtiative comparision of the results on MPI Sintel Dataset. Note that all approaches, except ours and Narihira et al. (2015a), uses depth image", "author": ["Narihira"], "venue": null, "citeRegEx": "Narihira,? \\Q2015\\E", "shortCiteRegEx": "Narihira", "year": 2015}, {"title": "A simple model for intrinsic image decomposition with depth cues", "author": ["Chen", "Qifeng", "Koltun", "Vladlen"], "venue": "In ICCV, pp. 241\u2013248", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Transforming neural-net output levels to probability distributions", "author": ["Denker", "John", "Lecun", "Yann"], "venue": "In NIPS,", "citeRegEx": "Denker et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Denker et al\\.", "year": 1991}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["Eigen", "David", "Puhrsch", "Christian", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Fukushima", "Kunihiko"], "venue": "Biological cybernetics,", "citeRegEx": "Fukushima and Kunihiko.,? \\Q1980\\E", "shortCiteRegEx": "Fukushima and Kunihiko.", "year": 1980}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Ground truth dataset and baseline evaluations for intrinsic image algorithms", "author": ["Grosse", "Roger", "Johnson", "Micah K", "Adelson", "Edward H", "Freeman", "William T"], "venue": "In ICCV,", "citeRegEx": "Grosse et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2009}, {"title": "Determining optical flow", "author": ["Horn", "Berthold K", "Schunck", "Brian G"], "venue": "Technical symposium east, pp. 319\u2013331. International Society for Optics and Photonics,", "citeRegEx": "Horn et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1981}, {"title": "Determining lightness from an image", "author": ["B.K.P. Horn"], "venue": "Computer Graphics and Image Processing,", "citeRegEx": "Horn,? \\Q1974\\E", "shortCiteRegEx": "Horn", "year": 1974}, {"title": "Deep structured output learning for unconstrained text recognition", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "ICLR,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross B", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Estimation of intrinsic image sequences from image+ depth video", "author": ["Lee", "Kyong Joon", "Zhao", "Qi", "Tong", "Xin", "Gong", "Minmin", "Izadi", "Shahram", "Sang Uk", "Tan", "Ping", "Lin", "Stephen"], "venue": "In ECCV", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs", "author": ["Li", "Bo", "Shen", "Chunhua", "Dai", "Yuchao", "van den Hengel", "Anton", "He", "Mingyi"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["Liu", "Fayao", "Shen", "Chunhua", "Lin", "Guosheng"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Direct intrinsics: Learning albedo-shading decomposition by convolutional regression", "author": ["Narihira", "Takuya", "Maire", "Michael", "Yu", "Stella X"], "venue": "In ICCV,", "citeRegEx": "Narihira et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narihira et al\\.", "year": 2015}, {"title": "Learning lightness from human judgement on relative reflectance", "author": ["Narihira", "Takuya", "Maire", "Michael", "Yu", "Stella X"], "venue": "In CVPR,", "citeRegEx": "Narihira et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narihira et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["Shotton", "Jamie", "Winn", "John", "Rother", "Carsten", "Criminisi", "Antonio"], "venue": null, "citeRegEx": "Shotton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Piecewise training for structured prediction", "author": ["Sutton", "Charles", "McCallum", "Andrew"], "venue": "Machine learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Towards unified depth and semantic prediction from a single image", "author": ["Wang", "Peng", "Shen", "Xiaohui", "Lin", "Zhe", "Cohen", "Scott", "Price", "Brian", "Yuille", "Alan L"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "Learning data-driven reflectance priors for intrinsic image decomposition", "author": ["Zhou", "Tinghui", "Kr\u00e4henb\u00fchl", "Philipp", "Efros", "Alexei A"], "venue": "In ICCV,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Learning ordinal relationships for mid-level vision", "author": ["Zoran", "Daniel", "Isola", "Phillip", "Krishnan", "Dilip", "Freeman", "William T"], "venue": "In ICCV,", "citeRegEx": "Zoran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoran et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Examples include optical flow (Horn & Schunck, 1981), monocular depth estimation (Eigen et al., 2014), intrinsic image decomposition (Barron & Malik, 2015) etc.", "startOffset": 81, "endOffset": 101}, {"referenceID": 19, "context": "Convolutional neural networks (CNNs) (Fukushima, 1980; Krizhevsky et al., 2012; LeCun et al., 1989) have greatly advanced the state of the art in all those structured output tasks (Eigen et al.", "startOffset": 37, "endOffset": 99}, {"referenceID": 20, "context": "Convolutional neural networks (CNNs) (Fukushima, 1980; Krizhevsky et al., 2012; LeCun et al., 1989) have greatly advanced the state of the art in all those structured output tasks (Eigen et al.", "startOffset": 37, "endOffset": 99}, {"referenceID": 9, "context": ", 1989) have greatly advanced the state of the art in all those structured output tasks (Eigen et al., 2014; Narihira et al., 2015a).", "startOffset": 88, "endOffset": 132}, {"referenceID": 3, "context": "Recently, Blundell et al. (2015) presented a back-propagation based variational technique to learn weight uncertainty.", "startOffset": 10, "endOffset": 33}, {"referenceID": 32, "context": "Jointly learning CRF and CNN using mean field inference has been shown to achieve great results in segmentation(Zheng et al., 2015).", "startOffset": 111, "endOffset": 131}, {"referenceID": 23, "context": "For continuous settings, great results have been achieved in depth estimation using CRF formulations, where unary potentials come from a CNN that maps the image to the depth value at single pixels or super-pixels, and where pairwise potentials reflect the desired smoothness of depth predictions between neighboring pixels or segments (Liu et al., 2015; Wang et al., 2015; Li et al., 2015).", "startOffset": 335, "endOffset": 389}, {"referenceID": 31, "context": "For continuous settings, great results have been achieved in depth estimation using CRF formulations, where unary potentials come from a CNN that maps the image to the depth value at single pixels or super-pixels, and where pairwise potentials reflect the desired smoothness of depth predictions between neighboring pixels or segments (Liu et al., 2015; Wang et al., 2015; Li et al., 2015).", "startOffset": 335, "endOffset": 389}, {"referenceID": 22, "context": "For continuous settings, great results have been achieved in depth estimation using CRF formulations, where unary potentials come from a CNN that maps the image to the depth value at single pixels or super-pixels, and where pairwise potentials reflect the desired smoothness of depth predictions between neighboring pixels or segments (Liu et al., 2015; Wang et al., 2015; Li et al., 2015).", "startOffset": 335, "endOffset": 389}, {"referenceID": 5, "context": "To extend the success of CNN on a small output space to a large one, if the global scoring function on the output can be decomposed into the sum of local scoring functions depending on a small subsets of the output, Chen et al. (2015) shows that the loss function is still representable and there could be an efficient iterative solution.", "startOffset": 216, "endOffset": 235}, {"referenceID": 5, "context": "To extend the success of CNN on a small output space to a large one, if the global scoring function on the output can be decomposed into the sum of local scoring functions depending on a small subsets of the output, Chen et al. (2015) shows that the loss function is still representable and there could be an efficient iterative solution. Jaderberg et al. (2015) train CNN to model unaries for the structured prediction task in a max-margin setting for text recognition.", "startOffset": 216, "endOffset": 363}, {"referenceID": 14, "context": "Traditional approaches seek various physics or statistics based priors on albedo and shading such as sparse and piecewise constant albedo and smooth shading (Horn, 1974; Grosse et al., 2009), or more recently from additional depth images (Lee et al.", "startOffset": 157, "endOffset": 190}, {"referenceID": 12, "context": "Traditional approaches seek various physics or statistics based priors on albedo and shading such as sparse and piecewise constant albedo and smooth shading (Horn, 1974; Grosse et al., 2009), or more recently from additional depth images (Lee et al.", "startOffset": 157, "endOffset": 190}, {"referenceID": 21, "context": ", 2009), or more recently from additional depth images (Lee et al., 2012; Barron & Malik, 2015; Chen & Koltun, 2013).", "startOffset": 55, "endOffset": 116}, {"referenceID": 12, "context": "MIT intrinsics, Sintel, and Intrinsics In the Wild datasets (Grosse et al., 2009; Butler et al., 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 4, "context": "MIT intrinsics, Sintel, and Intrinsics In the Wild datasets (Grosse et al., 2009; Butler et al., 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 2, "context": "MIT intrinsics, Sintel, and Intrinsics In the Wild datasets (Grosse et al., 2009; Butler et al., 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 33, "context": ", 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a).", "startOffset": 174, "endOffset": 261}, {"referenceID": 34, "context": ", 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a).", "startOffset": 174, "endOffset": 261}, {"referenceID": 2, "context": ", 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a). Narihira et al. (2015b) regress to a globally consistent ranking of albedo values, however they do not obtain a full intrinsic image decomposition.", "startOffset": 8, "endOffset": 306}, {"referenceID": 2, "context": ", 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a). Narihira et al. (2015b) regress to a globally consistent ranking of albedo values, however they do not obtain a full intrinsic image decomposition. Zhou et al. (2015) and Zoran et al.", "startOffset": 8, "endOffset": 449}, {"referenceID": 2, "context": ", 2012; Bell et al., 2014) provide ground-truth for intrinsics and open up new directions with data-driven approaches that have largely got rid of any hand-designed features and complex priors (Narihira et al., 2015b; Zhou et al., 2015; Zoran et al., 2015; Narihira et al., 2015a). Narihira et al. (2015b) regress to a globally consistent ranking of albedo values, however they do not obtain a full intrinsic image decomposition. Zhou et al. (2015) and Zoran et al. (2015) both predict the pairwise ordering of albedo values and then a CRF or a constrained quadratic program to turn this pairwise orderings into a globally consistent and spatially smooth intrinsic image decomposition.", "startOffset": 8, "endOffset": 473}, {"referenceID": 28, "context": "During piecewise learning of factors, the contribution of individual outputs often gets over-counted which is determined using cross validation (Shotton et al., 2009; Sutton & McCallum, 2009).", "startOffset": 144, "endOffset": 191}, {"referenceID": 6, "context": "Narihira et al. (2015a) use CNN to regress to the targets A,S, hoping that it would implicitly learn these dependencies which is not true since the outputs are blurry and doesn\u2019t contain any high frequency information.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "We initialize truncated-VGG layers using Imagenet (Russakovsky et al., 2015) pretrained weights, and randomly initialize the deconvolution layers.", "startOffset": 50, "endOffset": 76}, {"referenceID": 9, "context": "Thus, we use scale invariant learning procedure (Eigen et al., 2014).", "startOffset": 48, "endOffset": 68}, {"referenceID": 16, "context": "Our complete implementation is done in caffe (Jia et al., 2014) and we use ADAM (Kingma & Ba, 2015) as stochastic gradient solver.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012).", "startOffset": 79, "endOffset": 100}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013).", "startOffset": 80, "endOffset": 160}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes.", "startOffset": 80, "endOffset": 185}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes. Following Narihira et al. (2015a), we do not train on the image-based split proposed by Chen & Koltun (2013) because there is a large overlap in the image content in training and testing set.", "startOffset": 80, "endOffset": 310}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes. Following Narihira et al. (2015a), we do not train on the image-based split proposed by Chen & Koltun (2013) because there is a large overlap in the image content in training and testing set.", "startOffset": 80, "endOffset": 385}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes. Following Narihira et al. (2015a), we do not train on the image-based split proposed by Chen & Koltun (2013) because there is a large overlap in the image content in training and testing set. Instead, we report results on the scene split and report the average error using 2-fold cross-validation, similar to Narihira et al. (2015a). We compare all algorithm using the mean squared error (MSE), local mean squared error (LMSE) and structural dissimilarity (DSSIM) metric as proposed by Chen & Koltun (2013).", "startOffset": 80, "endOffset": 609}, {"referenceID": 4, "context": "For our evaluation of intrinsic image decomposition, we use MPI Sintel Dataset (Butler et al., 2012). We use the same setup followed by Narihira et al. (2015a) and Chen & Koltun (2013). Dataset contains total of 890 images with albedo and shading ground truth from 18 scenes. Following Narihira et al. (2015a), we do not train on the image-based split proposed by Chen & Koltun (2013) because there is a large overlap in the image content in training and testing set. Instead, we report results on the scene split and report the average error using 2-fold cross-validation, similar to Narihira et al. (2015a). We compare all algorithm using the mean squared error (MSE), local mean squared error (LMSE) and structural dissimilarity (DSSIM) metric as proposed by Chen & Koltun (2013). While both MSE and LMSE measure the raw pixelwise difference between predictions, DSSIM tried to capture the perceptual visual difference between outputs.", "startOffset": 80, "endOffset": 783}, {"referenceID": 12, "context": "MPI Sintel MSE LMSE DSSIM Albedo Shading Avg Albedo Shading Avg Albedo Shading Avg Image Split [Overlapping Training and Testing Scenes]: Grosse et al. (2009) 6.", "startOffset": 138, "endOffset": 159}, {"referenceID": 12, "context": "MPI Sintel MSE LMSE DSSIM Albedo Shading Avg Albedo Shading Avg Albedo Shading Avg Image Split [Overlapping Training and Testing Scenes]: Grosse et al. (2009) 6.06% 7.27% 6.67% 3.66% 4.19% 3.93% 22.70% 24.00% 23.35% Lee et al. (2012) 4.", "startOffset": 138, "endOffset": 234}, {"referenceID": 12, "context": "MPI Sintel MSE LMSE DSSIM Albedo Shading Avg Albedo Shading Avg Albedo Shading Avg Image Split [Overlapping Training and Testing Scenes]: Grosse et al. (2009) 6.06% 7.27% 6.67% 3.66% 4.19% 3.93% 22.70% 24.00% 23.35% Lee et al. (2012) 4.63% 5.07% 4.85% 2.24% 1.92% 2.08% 19.90% 17.70% 18.80% Barron & Malik (2015) 4.", "startOffset": 138, "endOffset": 313}, {"referenceID": 12, "context": "MPI Sintel MSE LMSE DSSIM Albedo Shading Avg Albedo Shading Avg Albedo Shading Avg Image Split [Overlapping Training and Testing Scenes]: Grosse et al. (2009) 6.06% 7.27% 6.67% 3.66% 4.19% 3.93% 22.70% 24.00% 23.35% Lee et al. (2012) 4.63% 5.07% 4.85% 2.24% 1.92% 2.08% 19.90% 17.70% 18.80% Barron & Malik (2015) 4.20% 4.36% 4.28% 2.98% 2.64% 2.81% 21.00% 20.60% 20.80% Chen & Koltun (2013) 3.", "startOffset": 138, "endOffset": 391}, {"referenceID": 6, "context": "Scene Split [Disjoint Training and Testing Scenes]: Narihira et al. (2015a) 2.", "startOffset": 52, "endOffset": 76}], "year": 2015, "abstractText": "Convolutional Neural Networks (CNNs) have recently emerged as the dominant model in computer vision. If provided with enough training data, they predict almost any visual quantity. In a discrete setting, such as classification, CNNs are not only able to predict a label but often predict a confidence in the form of a probability distribution over the output space. In continuous regression tasks, such a probability estimate is often lacking. We present a regression framework which models the output distribution of neural networks. This output distribution allows us to infer the most likely labeling following a set of physical or modeling constraints. These constraints capture the intricate interplay between different input and output variables, and complement the output of a CNN. However, they may not hold everywhere. Our setup further allows to learn a confidence with which a constraint holds, in the form of a distribution of the constrain satisfaction. We evaluate our approach on the problem of intrinsic image decomposition, and show that constrained structured regression significantly increases the state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}