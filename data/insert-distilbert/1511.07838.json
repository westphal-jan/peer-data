{"id": "1511.07838", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Dynamic Capacity Networks", "abstract": "we introduce the dynamic capacity network ( dcn ), a neural network that can adaptively assign its capacity correctly across different portions of the input data. this is achieved by combining modules of the two types : low - capacity sub - human networks and high - capacity outputs sub - networks. the low - capacity sub - networks are applied freely across most of the input, but also provide a guide to select a few small portions of the input on which to apply the high - capacity sub - networks. the selection is made using a novel gradient - based attention mechanism, that efficiently identifies the modules and input features that are most likely to impact the dcn's output and to which we'd like to devote more capacity. we focus our empirical genetic evaluation on the cluttered mnist and svhn image datasets. our findings indicate that dcns are able to drastically reduce the number threshold of computations, compared thus to traditional convolutional neural networks, while maintaining similar sensor performance.", "histories": [["v1", "Tue, 24 Nov 2015 19:30:19 GMT  (689kb,D)", "https://arxiv.org/abs/1511.07838v1", "ICLR 2016 submission"], ["v2", "Fri, 27 Nov 2015 19:17:53 GMT  (1636kb,D)", "http://arxiv.org/abs/1511.07838v2", "ICLR 2016 submission"], ["v3", "Thu, 3 Dec 2015 16:13:21 GMT  (1637kb,D)", "http://arxiv.org/abs/1511.07838v3", "ICLR 2016 submission"], ["v4", "Thu, 7 Jan 2016 22:44:43 GMT  (1847kb,D)", "http://arxiv.org/abs/1511.07838v4", "ICLR 2016 submission"], ["v5", "Tue, 9 Feb 2016 16:49:55 GMT  (1854kb,D)", "http://arxiv.org/abs/1511.07838v5", "ICML 2016 submission"], ["v6", "Wed, 6 Apr 2016 19:48:32 GMT  (1865kb,D)", "http://arxiv.org/abs/1511.07838v6", "ICML 2016 submission"], ["v7", "Sun, 22 May 2016 20:58:11 GMT  (1866kb,D)", "http://arxiv.org/abs/1511.07838v7", "ICML 2016"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["amjad almahairi", "nicolas ballas", "tim cooijmans", "yin zheng", "hugo larochelle", "aaron c courville"], "accepted": true, "id": "1511.07838"}, "pdf": {"name": "1511.07838.pdf", "metadata": {"source": "META", "title": "Dynamic Capacity Networks", "authors": ["Amjad Almahairi", "Nicolas Ballas", "Tim Cooijmans", "Yin Zheng", "Hugo Larochelle", "Aaron Courville"], "emails": ["AMJAD.ALMAHAIRI@UMONTREAL.CA", "NICOLAS.BALLAS@UMONTREAL.CA", "TIM.COOIJMANS@UMONTREAL.CA", "YIN.ZHENG@HULU.COM", "HLAROCHELLE@TWITTER.COM", "AARON.COURVILLE@UMONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "Deep neural networks have recently exhibited state-of-theart performance across a wide range of tasks, including object recognition (Szegedy et al., 2014) and speech recognition (Graves & Jaitly, 2014). Top-performing systems, however, are based on very deep and wide networks that are computationally intensive. One underlying assumption of many deep models is that all input regions contain the\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nsame amount of information. Indeed, convolutional neural networks apply the same set of filters uniformly across the spatial input (Szegedy et al., 2014), while recurrent neural networks apply the same transformation at every time step (Graves & Jaitly, 2014). Those networks lead to timeconsuming training and inference (prediction), in large part because they require a large number of weight/activation multiplications.\nTask-relevant information, however, is often not uniformly distributed across input data. For example, objects in images are spatially localized, i.e. they exist only in specific regions of the image. This observation has been exploited in attention-based systems (Mnih et al., 2014), which can reduce computations significantly by learning to selectively focus or \u201cattend\u201d to few, task-relevant, input regions. Attention employed in such systems is often referred to as \u201chard-attention\u201d, as opposed to \u201csoft-attention\u201d which integrates smoothly all input regions. Models of hard-attention proposed so far, however, require defining an explicit predictive model, whose training can pose challenges due to its non-differentiable cost.\nIn this work we introduce the Dynamic Capacity Network (DCN) that can adaptively assign its capacity across different portions of the input, using a gradient-based hardattention process. The DCN combines two types of modules: small, low-capacity, sub-networks, and large, highcapacity, sub-networks. The low-capacity sub-networks are active on the whole input, but are used to direct the high-capacity sub-networks, via our attention mechanism, to task-relevant regions of the input.\nA key property of the DCN\u2019s hard-attention mechanism is that it does not require a policy network trained by reinforcement learning. Instead, we can train DCNs end-to-end with backpropagation. We evaluate a DCN model on the attention benchmark task Cluttered MNIST (Mnih et al.,\nar X\niv :1\n51 1.\n07 83\n8v 7\n[ cs\n.L G\n] 2\n2 M\n2014), and show that it outperforms the state of the art.\nIn addition, we show that the DCN\u2019s attention mechanism can deal with situations where it is difficult to learn a taskspecific attention policy due to the lack of appropriate data. This is often the case when training data is mostly canonicalized, while at test-time the system is effectively required to perform transfer learning and deal with substantially different, noisy real-world images. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) is an example of such a dataset. The task here is to recognize multidigit sequences from real-world pictures of house fronts; however, most digit sequences in training images are wellcentred and tightly cropped, while digit sequences of test images are surrounded by large and cluttered backgrounds. Learning an attention policy that focuses only on a small portion of the input can be challenging in this case, unless test images are pre-processed to deal with this discrepancy 1. DCNs, on the other hand, can be leveraged in such transfer learning scenarios, where we learn low and high capacity modules independently and only combine them using our attention mechanism at test-time. In particular, we show that a DCN model is able to efficiently recognize multi-digit sequences, directly from the original images, without using any prior information on the location of the digits.\nFinally, we show that DCNs can perform efficient region selection, in both Cluttered MNIST and SVHN, which leads to significant computational advantages over standard convolutional models."}, {"heading": "2. Dynamic Capacity Networks", "text": "In this section, we describe the Dynamic Capacity Network (DCN) that dynamically distributes its network capacity across an input.\nWe consider a deep neural network h, which we decompose into two parts: h(x) = g(f(x)) where f and g represent respectively the bottom layers and top layers of the network h while x is some input data. Bottom layers f operate directly on the input and output a representation, which is composed of a collection of vectors each of which represents a region in the input. For example, f can output a feature map, i.e. vectors of features each with a specific spatial location, or a probability map outputting probability distributions at each different spatial location. Top layers g consider as input the bottom layers\u2019 representations f(x) and output a distribution over labels.\nDCN introduces the use of two alternative sub-networks for the bottom layers f : the coarse layers fc or the fine\n1This is the common practice in previous work on this dataset, e.g. (Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015)\nlayers ff , which differ in their capacity. The fine layers correspond to a high-capacity sub-network which has a high-computational requirement, while the coarse layers constitute a low-capacity sub-network. Consider applying the top layers only on the fine representation, i.e. hf (x) = g(ff (x)). We refer to the composition hf = g \u25e6 ff as the fine model. We assume that the fine model can achieve very good performance, but is computationally expensive. Alternatively, consider applying the top layers only on the coarse representation, i.e. hc(x) = g(fc(x)). We refer to this composition hc = g \u25e6 fc as the coarse model. Conceptually, the coarse model can be much more computationally efficient, but is expected to have worse performance than the fine model.\nThe key idea behind DCN is to have g use representations from either the coarse or fine layers in an adaptive, dynamic way. Specifically, we apply the coarse layers fc on the whole input x, and leverage the fine layers ff only at a few \u201cimportant\u201d input regions. This way, the DCN can leverage the capacity of ff , but at a lower computational cost, by applying the fine layers only on a small portion of the input. To achieve this, DCN requires an attentional mechanism, whose task is to identify good input locations on which to apply ff . In the remainder of this section, we focus on 2-dimensional inputs. However, our DCN model can be easily extended to be applied to any type of N-dimensional data."}, {"heading": "2.1. Attention-based Inference", "text": "In DCN, we would like to obtain better predictions than those made by the coarse model fc while keeping the computational requirement reasonable. This can be done by selecting a few salient input regions on which we use the fine representations instead of the coarse ones. DCN inference therefore needs to identify the important regions in the input with respect to the task at hand. For this, we use a novel approach for attention that uses backpropagation in\nthe coarse model hc to identify few vectors in the coarse representation to which the distribution over the class label is most sensitive. These vectors correspond to input regions which we identify as salient or task-relevant.\nGiven an input image x, we first apply the coarse layers on all input regions to compute the coarse representation vectors:\nfc(x) = {ci,j | (i, j) \u2208 [1, s1]\u00d7 [1, s2]}, (1)\nwhere s1 and s2 are spatial dimensions that depend on the image size and ci,j = fc(xi,j) \u2208 RD is a representation vector associated with the input region (i, j) in x, i.e. corresponds to a specific receptive field or a patch in the input image. We then compute the output of the model based completely on the coarse vectors, i.e. the coarse model\u2019s output hc(x) = g(fc(x)).\nNext, we identify a few salient input regions using an attentional mechanism that exploits a saliency map generated using the coarse model\u2019s output. The specific measure of saliency we choose is based on the entropy of the coarse model\u2019s output, defined as:\nH = \u2212 C\u2211 l=1 o(l)c log o (l) c , (2)\nwhere oc = g(fc(x)) is the vector output of the coarse model and C is the number of class labels. The saliency M of an input region position (i, j) is given by the norm of the gradient of the entropy H with respect to the coarse vector ci,j :\nMi,j = ||\u2207ci,jH||2\n= \u221a\u221a\u221a\u221a D\u2211 r=1 ( \u2202 \u2202c (r) i,j \u2212 C\u2211 l=1 o (l) c log o (l) c )2 ,\n(3)\nwhere M \u2208 Rs1\u00d7s2 . The use of the entropy gradient as a saliency measure encourages selecting input regions that could affect the uncertainty in the model\u2019s predictions the most. In addition, computing the entropy of the output distribution does not require observing the true label, hence the measure is available at inference time. Note that computing all entries in matrix M can be done using a single backward pass of backpropagation through the top layers and is thus efficient and simple to implement.\nUsing the saliency map M, we select a set of k input region positions with the highest saliency values. We denote the selected set of positions by Is \u2286 [1, s1] \u00d7 [1, s2], such that |Is| = k. We denote the set of selected input regions by Xs = {xi,j | (i, j) \u2208 Is} where each xi,j is a patch in x. Next we apply the fine layers ff only on the selected\npatches and obtain a small set of fine representation vectors:\nff (X s) = {fi,j | (i, j) \u2208 Is}, (4)\nwhere fi,j = ff (xi,j). This requires that fi,j \u2208 RD, i.e. the fine vectors have the same dimensionality as the coarse vectors, allowing the model to use both of them interchangeably.\nWe denote the representation resulting from combining vectors from both fc(x) and ff (Xs) as the refined representation fr(x). We discuss in Section 4 different ways in which they can be combined in practice. Finally, the DCN output is obtained by feeding the refined representation into the top layers, g(fr(x)). We denote the composition g \u25e6 fr by the refined model."}, {"heading": "2.2. End-to-End Training", "text": "In this section, we describe an end-to-end procedure for training the DCN model that leverages our attention mechanism to learn ff and fc jointly. We emphasize, however, that DCN modules can be trained independently, by training a coarse and a fine model independently and combining them only at test-time using our attention based inference. In Section 4.2 we show an example of how this modular training can be used for transfer learning.\nIn the context of image classification, suppose we have a training set D = {(x(i), y(i)); i = 1 . . .m}, where each x(i) \u2208 Rh\u00d7w is an image, and y(i) \u2208 {1, . . . , C} is its corresponding label. We denote the parameters of the coarse, fine and top layers by \u03b8c, \u03b8f , and \u03b8t respectively. We learn all of these parameters (denoted as \u03b8) by minimizing the cross-entropy objective function (which is equivalent to maximizing the log-likelihood of the correct labels):\nJ = \u2212 m\u2211 i=1 log p ( y(i) | x(i); \u03b8 ) , (5)\nwhere p(\u00b7 | x(i); \u03b8) = g(fr(x(i))) is the conditional multinomial distribution defined over the C labels given by the refined model (Figure 1). Gradients are computed by standard back-propagation through the refined model, i.e. propagating gradients at each position into either the coarse or fine features, depending on which was used.\nAn important aspect of the DCN model is that the final prediction is based on combining representations from two different sets of layers, namely the coarse layers fc and the fine layers ff . Intuitively, we would like those representations to have close values such that they can be interchangeable. This is important for two reasons. First, we expect the top layers to have more success in correctly classifying the input if the transition from coarse to fine representations is smooth. The second is that, since the saliency map is based on the gradient at the coarse representation values\nand since the gradient is a local measure of variation, it is less likely to reflect the benefit of using the fine features if the latter is very different from the former.\nTo encourage similarity between the coarse and fine representations while training, we use a hint-based training approach inspired by Romero et al. (2014). Specifically, we add an additional term to the training objective that minimizes the squared distance between coarse and fine representations: \u2211\nxi,j\u2208Xs \u2016fc(xi,j)\u2212 ff (xi,j)\u201622. (6)\nThere are two important points to note here. First, we use this term to optimize only the coarse layers \u03b8c. That is, we encourage the coarse layers to mimic the fine ones, and let the fine layers focus only on the signal coming from the top layers. Secondly, computing the above hint objective over representations at all positions would be as expensive as computing the full fine model; therefore, we encourage in this term similarity only over the selected salient patches."}, {"heading": "3. Related Work", "text": "This work can be classified as a conditional computation approach. The goal of conditional computation, as put forward by Bengio (2013), is to train very large models for the same computational cost as smaller ones, by avoiding certain computation paths depending on the input. There have been several contributions in this direction. Bengio et al. (2013) use stochastic neurons as gating units that activate specific parts of a neural network. Our approach, on the other hand, uses a hard-attention mechanism that helps the model to focus its computationally expensive paths only on important input regions, which helps in both scaling to larger effective models and larger input sizes.\nSeveral recent contributions use attention mechanisms to capture visual structure with biologically inspired, foveation-like methods, e.g. (Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015). In Mnih et al. (2014); Ba et al. (2014), a learned sequential attention model is used to make a hard decision as to where to look in the image, i.e. which region of the image is considered in each time step. This so-called \u201chard-attention\u201d mechanism can reduce computation for inference. The attention mechanism is trained by reinforcement learning using policy search. In practice, this approach can be computationally expensive during training, due to the need to sample multiple interaction sequences with the environment. On the other hand, the DRAW model (Gregor et al., 2015) uses a \u201csoftattention\u201d mechanism that is fully differentiable, but requires processing the whole input at each time step. Our approach provides a simpler hard-attention mechanism with\ncomputational advantages in both inference and learning.\nThe saliency measure employed by DCN\u2019s attention mechanism is related to pixel-wise saliency measures used in visualizing neural networks (Simonyan et al., 2013). These measures, however, are based on the gradient of the classification loss, which is not applicable at test-time. Moreover, our saliency measure is defined over contiguous regions of the input rather than on individual pixels. It is also taskdependent, as a result of defining it using a coarse model trained on the same task.\nOther works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) and quantization schemes (Chen et al., 2010; Je\u0301gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data. In contrast, the shortcuts taken by DCN specialize to the input, avoiding costly computation except where needed. However, the two approaches are orthogonal and could be combined to yield further savings.\nOur use of a regression cost for enforcing representations to be similar is related to previous work on model compression (Bucilu et al., 2006; Hinton et al., 2015; Romero et al., 2014). The goal of model compression is to train a small model (which is faster in deployment) to imitate a much larger model or an ensemble of models. Furthermore, Romero et al. (2014) have shown that middle layer hints can improve learning in deep and thin neural networks. Our DCN model can be interpreted as performing model compression on the fly, without the need to train a large model up front."}, {"heading": "4. Experiments", "text": "In this section, we present an experimental evaluation of the proposed DCN model. To validate the effectiveness of our approach, we first investigate the Cluttered MNIST dataset (Mnih et al., 2014). We then apply our model in a transfer learning setting to a real-world object recognition task using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011)."}, {"heading": "4.1. Cluttered MNIST", "text": "We use the 100\u00d7 100 Cluttered MNIST digit classification dataset (Mnih et al., 2014). Each image in this dataset is a hand-written MNIST digit located randomly on a 100 \u00d7 100 black canvas and cluttered with digit-like fragments. Therefore, the dataset has the same size of MNIST: 60000 images for training and 10000 for testing."}, {"heading": "4.1.1. MODEL SPECIFICATION", "text": "In this experiment we train a DCN model end-to-end, where we learn coarse and fine layers jointly. We use 2\nconvolutional layers as coarse layers, 5 convolutional layers as fine layers and one convolutional layer followed by global max pooling and a softmax as the top layers. Details of their architectures can be found in the Appendix 6.1. The coarse and fine layers produce feature maps, i.e. feature vectors each with a specific spatial location. The set of selected patches Xs is composed of eight patches of size 14\u00d7 14 pixels. We use here a refined representation of the full input fr(x) in which fine feature vectors are swapped in place of coarse ones:\nfr(x) = {ri,j | (i, j) \u2208 [1, s1]\u00d7 [1, s2]} (7)\nri,j =\n{ ff (xi,j), if xi,j \u2208 Xs\nfc(xi,j), otherwise. (8)"}, {"heading": "4.1.2. BASELINES", "text": "We use as baselines for our evaluation the coarse model (top layers applied only on coarse representations), the fine model (top layers applied only on fine representations), and we compare with previous attention-based models RAM (Mnih et al., 2014) and DRAW (Gregor et al., 2015)."}, {"heading": "4.1.3. EMPIRICAL EVALUATION", "text": "Results of our experiments are shown in Table 1. We get our best DCN result when we add the hint term in Eq. (6)\nin the training objective, which we observe to have a regularization effect on DCN. We can see that the DCN model performs significantly better than the previous state-of-theart result achieved by RAM and DRAW models. It also outperforms the fine model, which is a result of being able to focus only on the digit and ignore clutter. In Figure 2 we explore more the effect of the hint objective during training, and confirm that it can indeed minimize the squared distance between coarse and fine representations. To show how the attention mechanism of the DCN model can help it focus on the digit, we plot in Figure 3(a) the patches it finds in some images from the validation set, after only 9 epochs of training.\nThe DCN model is also more computationally efficient. A forward pass of the fine model requires the computation of the fine layers representations on whole inputs and a forward pass of the top layers leading to 84.5M multiplications. On the other hand, DCN applies only the coarse layers on the whole input. It also requires the computation of the fine representations for 8 input patches and a forward pass of the top layers. The attention mechanism of the DCN model requires an additional forward and backward pass through the top layers which leads to approximately 27.7M multiplications in total. As a result, the DCN model here has 3 times fewer multiplications than the fine model. In practice we observed a time speed-up by a factor of about 2.9. Figure 3(b) shows how the test error behaves when we increase the number of patches. While taking additional patches improves accuracy, the marginal improvement becomes insignificant beyond 10 or so patches. The number of patches effectively controls a trade-off between accuracy and computational cost."}, {"heading": "4.2. SVHN", "text": "We tackle in this section a more challenging task of transcribing multi-digit sequences from natural images using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011). SVHN is composed of real-world pictures\ncontaining house numbers and taken from house fronts. The task is to recognize the full digit sequence corresponding to a house number, which can be of length 1 to 5 digits. The dataset has three subsets: train (33k), extra (202k) and test (13k). In the following, we trained our models on 230k images from both the train and extra subsets, where we take a 5k random sample as a validation set for choosing hyperparameters.\nThe typical experimental setting in previous literature, e.g. (Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information. Input images are generally cropped, such that digit sequences are centred and most of the background and clutter information is pruned. We argue that our DCN model can deal effectively with real-world noisy images having large portions of clutter or background information. To demonstrate this ability, we investigate a more general problem setting where the images are uncropped and the digits locations are unknown. We apply our models on SVHN images in their original sizes and we do not use any extra bounding box information. 2\nAn important property of the SVHN dataset is the large discrepancy between the train/extra sets and the test set. Most of the extra subset images (which dominate the training data) have their digits well-centred with little cluttered background, while test images have more variety in terms of digit location and background clutter. Figure 4 shows samples of these images. We can tackle this training/test dataset discrepancy by training a DCN model in a transfer learning setting. We train the coarse and fine layers of the DCN independently on the training images that have little background-clutter, and then combine them using our attention mechanism, which does not require explicit training, to decide on which subsets of the input to apply the fine layers."}, {"heading": "4.2.1. MULTI-DIGIT RECOGNITION MODEL", "text": "We follow the model proposed in (Goodfellow et al., 2013) for learning a probabilistic model of the digit sequence\n2The only pre-processing we perform on the data is converting images to grayscale.\ngiven an input image x. The output sequence S is defined using a collection ofN random variables, S1, . . . , SN , representing the elements of the sequence and an extra random variable S0 representing its length. The probability of a given sequence s = {s1, . . . , sn} is given by:\np(S = s | x) = p(S0 = n | x) n\u220f\ni=1\np(Si = si | x), (9)\nwhere p(S0 = n | x) is the conditional distribution of the sequence length and p(Si = si | x) is the conditional distribution of the i-th digit in the sequence. In particular, our model on SVHN has 6 softmaxes: 1 for the length of the sequence (from 1 to 5), and 5 for the identity of each digit or a null character if no digit is present (11 categories)."}, {"heading": "4.2.2. MODEL SPECIFICATION", "text": "The coarse and fine bottom layers, fc and ff , are fullyconvolutional, composed of respectively 7 and 11 layers. The representation, produced by either the fine or coarse layers, is a probability map, which is a collection of independent full-sequence prediction vectors, each vector corresponding to a specific region of the input. We denote the prediction for the i-th output at position (j, k) by p(j,k)(Si | x).\nThe top layer g is composed of one global average pooling layer which combines predictions from various spatial locations to produce the final prediction p(S | x).\nSince we have multiple outputs in this task, we modify the saliency measure used by the DCN\u2019s attention mechanism to be the sum of the entropy of the 5 digit softmaxes:\nH = \u2212 5\u2211\ni=1 11\u2211 j=1 p(Si = sj | x) log p(Si = sj | x). (10)\nWhen constructing the saliency, instead of using the gradient with respect to the probability map, we use the gradient with respect to the feature map below it. This is necessary to avoid identical gradients as g, the top function, is composed by only one average pooling.\nWe also use a refined model that computes its output by applying the pooling top layer g only on the k independent predictions from fine layers, ignoring the coarse layers. We have found empirically that this results in a better model, and suspect that otherwise the predictions from the salient regions are drowned out by the noisy predictions from uninformative regions.\nWe train the coarse and fine layers of DCN independently in this experiment, minimizing log p(S | x) using SGD. For the purposes of training only, we resize images to 64\u00d7 128. Details on the coarse and fine architectures are found in Appendix 6.2."}, {"heading": "4.2.3. BASELINES", "text": "As mentioned in the previous section, each of the coarse representation vectors in this experiment corresponds to multi-digit recognition probabilities computed at a given region, which the top layer g simply averages to obtain the baseline coarse model:\np(Si | x) = 1\nd1 \u00d7 d2 \u2211 j,k p(j,k)(Si | x). (11)\nThe baseline fine model is defined similarly.\nAs an additional baseline, we consider a \u201csoft-attention\u201d coarse model, which takes the coarse representation vectors over all input regions, but uses a top layer that performs a weighted average of the resulting location-specific predictions. We leverage the entropy to define a weighting scheme which emphasizes important locations:\np(Si | x) = \u2211 j,k wi,j,kp (j,k)(Si | x). (12)\nThe weight wi,j,k is defined as the normalized inverse entropy of the i-th prediction by the (j, k)-th vector, i.e. :\nwi,j,k = \u2211 q,r H\u22121i,j,k H\u22121i,q,r , (13)\nwhere Hi,j,k is defined as:\nHi,j,k = \u2212 C\u2211 l=1 pj,k(Si = sl | x) log pj,k(Si = sl | x),\n(14) and C is either 5 for S0 or 11 for all other Si. As we\u2019ll see, this weighting improves the coarse model\u2019s performance in our SVHN experiments. We incorporate this weighting in DCN to aggregate predictions from the salient regions.\nTo address scale variations in the data, we extend all models to multi-scale by processing each image several times at multiple resolutions. Predictions made at different scales are considered independent and averaged to produce the final prediction.\nIt is worth noting that all previous literature on SVHN dealt with a simpler task where images are cropped and resized. In this experiment we deal with a more general setting, and our results cannot be directly compared with these results."}, {"heading": "4.2.4. EMPIRICAL EVALUATION", "text": "Table 2 shows results of our experiment on SVHN. The coarse model has an error rate of 40.6%, while by using our proposed soft-attention mechanism, we decrease the error rate to 31.4%. This confirms that the entropy is a good measure for identifying important regions when task-relevant information is not uniformly distributed across input data.\nThe fine model, on the other hand, achieves a better error rate of 25.2%, but is more computationally expensive. Our DCN model, which selects only 6 regions on which to apply the high-capacity fine layers, achieves an error rate of 20.0%. The DCN model can therefore outperform, in terms of classification accuracy, the other baselines. This verifies our assumption that by applying high capacity subnetworks only on the inputs most informative regions, we are able to obtain high classification performance. Figure 6 shows a sample of the selected patches by our attention mechanism.\nAn additional decrease of the test errors can be obtained by increasing the number of processed scales. In the DCN model, taking 3 patches at 2 scales (original and 0.75 scales), leads to 18.2% error, while taking 3 patches at 3 scales (original, 0.75 and 0.5 scales) leads to an error rate of 16.6%. Our DCN model can reach its best performance of 11.6% by taking all possible patches at 3 scales, but it does not offer any computational benefit over the fine model.\nWe also investigate the computational benefits of the DCN approach as the dimensions of the input data increase. Ta-\nble 5 reports the number of multiplications the fine model, coarse model and the DCN model require, given different input sizes. We also verify the actual computational time of these models by taking the largest 100 images in the SVHN test set, and computing the average inference time taken by all the models. 3 The smallest of these images has a size of 363\u00d7735 pixels, while the largest has a size of 442\u00d71083 pixels. On average, the coarse and the soft-attention models take 8.6 milliseconds, while the fine model takes 62.6 milliseconds. On the largest 100 SVHN test images, the DCN requires on average 10.8 milliseconds for inference."}, {"heading": "5. Conclusions", "text": "We have presented the DCN model, which is a novel approach for conditional computation. We have shown that using our visual attention mechanism, our network can adaptively assign its capacity across different portions of the input data, focusing on important regions of the input. Our model achieved state-of-the-art performance on the Cluttered MNIST digit classification task, and provided computational benefits over traditional convolutional network architectures. We have also validated our model in a transfer learning setting using the SVHN dataset, where we tackled the multi-digit recognition problem without using any a priori information on the digits\u2019 location. We have shown that our model outperforms other baselines, yet remains tractable for inputs with large spatial dimensions."}, {"heading": "6. Appendix", "text": ""}, {"heading": "6.1. Cluttered MNIST Experiment Details", "text": "\u2022 Coarse layers: 2 convolutional layers, with 7 \u00d7 7 and 3\u00d7 3 filter sizes, 12 and 24 filters, respectively, and a 2 \u00d7 2 stride. Each feature in the coarse feature maps covers a patch of size 11\u00d711 pixels, which we extend by 3 pixels in each side to give the fine layers more context. The size of the coarse feature map is 23\u00d723.\n\u2022 Fine layers: 5 convolutional layers, each with 3 \u00d7 3 3We evaluate all models on an NVIDIA Titan Black GPU card.\nfilter sizes, 1\u00d71 strides, and 24 filters. We apply 2\u00d72 pooling with 2 \u00d7 2 stride after the second and fourth layers. We also use 1 \u00d7 1 zero padding in all layers except for the first and last layers. This architecture was chosen so that it maps a 14 \u00d7 14 patch into one spatial location.\n\u2022 Top layers: one convolutional layer with 4 \u00d7 4 filter size, 2\u00d72 stride and 96 filters, followed by global max pooling. The result is fed into a 10-output softmax layer.\nWe use rectifier non-linearities in all layers. We use Batch Normalization (Ioffe & Szegedy, 2015) and Adam (Kingma & Ba, 2014) for training our models. In DCN we train the coarse layers with a convex combination of cross entropy objective and hints."}, {"heading": "6.2. SVHN Experiment Details", "text": "\u2022 Coarse layers: the model is fully convolutional with 7 convolutional layers. First three layers have 24, 48, 128 filters respectively with size 5 \u00d7 5 and stride 2 \u00d7 2. Layer 4 has 192 filters with 4 \u00d7 5 and stride 1 \u00d7 2. Layer 5 has 192 filters with size 1 \u00d7 4. Finally, the last two layers are 1 \u00d7 1 convolutions with 1024 filters. We use stride of 1\u00d71 in the last 3 layers and do not use zero padding in any of the coarse layers. The corresponding patch size here is 54\u00d7 110.\n\u2022 Fine layers: 11 convolutional layers. The first 5 convolutional layers have 48, 64, 128, 160 and 192 filters respectively, with size 5 \u00d7 5 and zero-padding. After layers 1, 3, and 5 we use 2\u00d72 max pooling with stride 2\u00d72. The following layers have 3\u00d73 convolution with 192 filters. The 3 last layers are 1\u00d71 convolution with 1024 hidden units.\nHere we use SGD with momentum and exponential learning rate decay. While training, we take 54 \u00d7 110 random crop from images, and we use 0.2 dropout on convolutional layers and 0.5 dropout on fully connected layers."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the support of the following organizations for research funding and computing support: Nuance Foundation, Compute Canada and Calcul Que\u0301bec. We would like to thank the developers of Theano (Bergstra et al., 2011; Bastien et al., 2012) and Blocks/Fuel (Van Merrie\u0308nboer et al., 2015) for developing such powerful tools for scientific computing, and our reviewers for their useful comments."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Deep learning of representations: Looking forward", "author": ["Bengio", "Yoshua"], "venue": "In Statistical Language and Speech Processing,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Approximate nearest neighbor search by residual vector quantization", "author": ["Chen", "Yongjian", "Guan", "Tao", "Wang", "Cheng"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Min", "Bourdev", "Lubomir"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML),", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["J\u00e9gou", "Herv\u00e9", "Douze", "Matthijs", "Schmid", "Cordelia"], "venue": "IEEE TPAMI,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On learning where to look", "author": ["Ranzato", "Marc\u2019Aurelio"], "venue": "arXiv preprint arXiv:1405.5488,", "citeRegEx": "Ranzato and Marc.Aurelio.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato and Marc.Aurelio.", "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "ArXiv e-prints,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Deep neural networks have recently exhibited state-of-theart performance across a wide range of tasks, including object recognition (Szegedy et al., 2014) and speech recognition (Graves & Jaitly, 2014).", "startOffset": 132, "endOffset": 154}, {"referenceID": 23, "context": "Indeed, convolutional neural networks apply the same set of filters uniformly across the spatial input (Szegedy et al., 2014), while recurrent neural networks apply the same transformation at every time step (Graves & Jaitly, 2014).", "startOffset": 103, "endOffset": 125}, {"referenceID": 18, "context": "This observation has been exploited in attention-based systems (Mnih et al., 2014), which can reduce computations significantly by learning to selectively focus or \u201cattend\u201d to few, task-relevant, input regions.", "startOffset": 63, "endOffset": 82}, {"referenceID": 19, "context": "The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) is an example of such a dataset.", "startOffset": 45, "endOffset": 66}, {"referenceID": 8, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 0, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 14, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015) Figure 1.", "startOffset": 0, "endOffset": 66}, {"referenceID": 21, "context": "To encourage similarity between the coarse and fine representations while training, we use a hint-based training approach inspired by Romero et al. (2014). Specifically, we add an additional term to the training objective that minimizes the squared distance between coarse and fine representations: \u2211", "startOffset": 134, "endOffset": 155}, {"referenceID": 3, "context": "Bengio et al. (2013) use stochastic neurons as gating units that activate specific parts of a neural network.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 18, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 0, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 10, "context": "(Larochelle & Hinton, 2010; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014; Ba et al., 2014; Gregor et al., 2015).", "startOffset": 0, "endOffset": 119}, {"referenceID": 10, "context": "On the other hand, the DRAW model (Gregor et al., 2015) uses a \u201csoftattention\u201d mechanism that is fully differentiable, but requires processing the whole input at each time step.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": ", 2014; Ba et al., 2014; Gregor et al., 2015). In Mnih et al. (2014); Ba et al.", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": ", 2014; Ba et al., 2014; Gregor et al., 2015). In Mnih et al. (2014); Ba et al. (2014), a learned sequential attention model is used to make a hard decision as to where to look in the image, i.", "startOffset": 8, "endOffset": 87}, {"referenceID": 22, "context": "The saliency measure employed by DCN\u2019s attention mechanism is related to pixel-wise saliency measures used in visualizing neural networks (Simonyan et al., 2013).", "startOffset": 138, "endOffset": 161}, {"referenceID": 13, "context": "Other works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) and quantization schemes (Chen et al.", "startOffset": 41, "endOffset": 86}, {"referenceID": 6, "context": "Other works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) and quantization schemes (Chen et al.", "startOffset": 41, "endOffset": 86}, {"referenceID": 4, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 15, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 7, "context": ", 2014) and quantization schemes (Chen et al., 2010; J\u00e9gou et al., 2011; Gong et al., 2014) take the same computational shortcuts for all instances of the data.", "startOffset": 33, "endOffset": 91}, {"referenceID": 11, "context": "Our use of a regression cost for enforcing representations to be similar is related to previous work on model compression (Bucilu et al., 2006; Hinton et al., 2015; Romero et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 21, "context": "Our use of a regression cost for enforcing representations to be similar is related to previous work on model compression (Bucilu et al., 2006; Hinton et al., 2015; Romero et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 11, "context": ", 2006; Hinton et al., 2015; Romero et al., 2014). The goal of model compression is to train a small model (which is faster in deployment) to imitate a much larger model or an ensemble of models. Furthermore, Romero et al. (2014) have shown that middle layer hints can improve learning in deep and thin neural networks.", "startOffset": 8, "endOffset": 230}, {"referenceID": 18, "context": "To validate the effectiveness of our approach, we first investigate the Cluttered MNIST dataset (Mnih et al., 2014).", "startOffset": 96, "endOffset": 115}, {"referenceID": 19, "context": "We then apply our model in a transfer learning setting to a real-world object recognition task using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).", "startOffset": 146, "endOffset": 167}, {"referenceID": 18, "context": "We use the 100\u00d7 100 Cluttered MNIST digit classification dataset (Mnih et al., 2014).", "startOffset": 65, "endOffset": 84}, {"referenceID": 18, "context": "We use as baselines for our evaluation the coarse model (top layers applied only on coarse representations), the fine model (top layers applied only on fine representations), and we compare with previous attention-based models RAM (Mnih et al., 2014) and DRAW (Gregor et al.", "startOffset": 231, "endOffset": 250}, {"referenceID": 10, "context": ", 2014) and DRAW (Gregor et al., 2015).", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "We tackle in this section a more challenging task of transcribing multi-digit sequences from natural images using the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).", "startOffset": 159, "endOffset": 180}, {"referenceID": 8, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 0, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 14, "context": "(Goodfellow et al., 2013; Ba et al., 2014; Jaderberg et al., 2015), uses the location of digit bounding boxes as extra information.", "startOffset": 0, "endOffset": 66}, {"referenceID": 8, "context": "We follow the model proposed in (Goodfellow et al., 2013) for learning a probabilistic model of the digit sequence", "startOffset": 32, "endOffset": 57}, {"referenceID": 1, "context": "We would like to thank the developers of Theano (Bergstra et al., 2011; Bastien et al., 2012) and Blocks/Fuel (Van Merri\u00ebnboer et al.", "startOffset": 48, "endOffset": 93}], "year": 2016, "abstractText": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity subnetworks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.", "creator": "LaTeX with hyperref package"}}}