{"id": "1511.07953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Exploring Correlation between Labels to improve Multi-Label Classification", "abstract": "this paper attempts multi - label classification performance by instead extending the idea of independent binary classification models evaluated for each output label, and exploring how the inherent correlation between output track labels can be used to improve predictions. logistic regression, naive numerical bayes, chaotic random forest, and experimental svm loop models were constructed, with svm giving the best results : an improvement effectiveness of 12. 9 \\ % over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.", "histories": [["v1", "Wed, 25 Nov 2015 05:21:53 GMT  (580kb,D)", "http://arxiv.org/abs/1511.07953v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["amit garg", "jonathan noyola", "romil verma", "ashutosh saxena", "aditya jami"], "accepted": false, "id": "1511.07953"}, "pdf": {"name": "1511.07953.pdf", "metadata": {"source": "CRF", "title": "Exploring Correlation between Labels to improve Multi-Label Classification", "authors": ["Amit Garg", "Jonathan Noyola", "Romil Verma", "Ashutosh Saxena", "Aditya Jami"], "emails": [], "sections": [{"heading": null, "text": "Exploring Correlation between Labels to improve Multi-Label Classification\nAmit Garg, Jonathan Noyola, Romil Verma, Ashutosh Saxena, Aditya Jami\nStanford University"}, {"heading": "I. Abstract", "text": "This paper attempts multi-label classification by extending the idea of independent binary classification models for each output label, and exploring how the inherent correlation between output labels can be used to improve predictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models were constructed, with SVM giving the best results: an improvement of 12.9% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.\nII. Introduction\nMulti-label classification is the set of classification problems where the output vector has a variable length. The average number of labels per review varies across datasets, and in general is a function of the semantics of the text rather than the syntax. The learning algorithm needs to estimate the number of labels and make the correct predictions.\nPreviously multi-label classification problems were solved using problem transformation techniques (converting the problem into binary classification problems per output label), or by adapting the algorithm to directly perform multi-label classification [1]. This paper extracts correlation information between labels and factors the joint probabilities into the model."}, {"heading": "III. Task Definition", "text": "Given large datasets of product reviews from Amazon and Twitter and manually labelled multi-label classifications for each (ground truth), the algorithm aims to predict all classifications for a review [2]. We aim to make the algorithm portable across various datasets, i.e., training a model on amazon reviews and using it to classify tweets from Twitter."}, {"heading": "IV. Dataset", "text": "The Amazon dataset contains reviews for 400,000 products across 40,000 different categories. Among these, we focused on books and book subcategories. The Twitter dataset has 100,000 tweets. Both the datasets have the same schema, and each review and tweet has a unique ID, pre-pruned content, and a tree of all of the product labels for the review up to the Book category at the root."}, {"heading": "V. Theory", "text": "Our approach augments the independent binary classification model. In addition to the individual probabilities, this also looks at the probability of labels\noccurring together. Hence the Inference algorithm models the formula \u2013\nP ( y(m)|x(m) ) = \u220f i P ( y (m) i |x (m) )\u220f j,k P ( y (m) j , y (m) k )\u03b1 where yi = {0, 1}, j, k \u2208 {i|yi = 1}, 0 \u2264 \u03b1 \u2264 1\nP (y (m) i |x(m)) represents the probability of the independent binary model for label i classifying as 0\nor 1 given input x. \u220f i P ( y (m) i |x(m) ) represents the assignment of 0 or 1 to all yi that maximizes the joint probability.\nThe pairwise correlation probability between each pair of labels is stored in the correlation matrix. This normalized rows of the matrix represent probability of a particular label\u2019s coupling with every other label except for itself. This probability is computed as a prior for the entire dataset and is not dependent on the input feature vector itself. It is represented by P (y (m) j , y\n(m) k ).\u220f j,k P ( y (m) j , y (m) k )\u03b1 represents the joint probability of all pairwise combinations of predicted labels, each discounted by \u03b1. The probabilities obtained from the correlation matrix are given less weight than the probabilities obtained from independent models, hence the discount. This is because co-occurrence of labels is not completely characterized by correlation, it also requires the higher order moments which significantly increase the computational overhead."}, {"heading": "VI. Methodology", "text": "The entire algorithm is split into 3 steps after parsing \u2013 preprocessing, training independent classifiers, and incorporating co-occurrence probability to make the final label set. Hyper-parameters need to be tuned for each dataset.\nLIBLINEAR works well for larger datasets while LIBSVM works well for custom and smaller datasets. LIBLINEAR only supports a linear Kernel but is very fast relative to LIBSVM and hence used in the learning algorithms [4]."}, {"heading": "Parsing", "text": "The Amazon and Twitter datasets are parsed to extract Book reviews and their corresponding labels (ground truth). The labels are structured in a hierarchy - for Books, there are 31 top level labels and for each\nar X\niv :1\n51 1.\n07 95\n3v 1\n[ cs\n.L G\n] 2\n5 N\nov 2\n01 5\n2 label there are several sub-labels. For instance a top level label is \u201dLiterature and Fiction\u201d and few sub labels associated with it are \u201dFolklore\u201d, \u201dMysteries\u201d and \u201dClassics.\u201d The reviews are labelled with these sub-level labels which we map to one of the 31 top-level labels, and use for multi-label classification."}, {"heading": "Preprocessing", "text": "Training and testing of the algorithm is done on at most 3,000 reviews per dataset due to computational constraints during learning. Each Book review is mapped to a bag-of-words based input feature. On parsing a review, its content is pruned and stemmed and then a tf-idf based vector is generated which is used as the input feature x. The labels corresponding to this review are associated as its outputs y.\nThe correlation matrix is built by parsing 100,000 Book reviews and creating pairwise counts of different labels \u2013 a 31 \u00d7 31 row normalized matrix with Laplace Smoothing. After normalization the matrix is not symmetric, so the geometric mean of cells (i, j)and (j, i) is treated as the actual correlation probability for two labels. Fig 1. plots the row normalized covariance matrix for Amazon dataset. The existence of large peaks is the motivation behind the algorithm."}, {"heading": "Algorithm", "text": "\u2022 The inputs to the learning algorithm are tf-idf based input features, their manually assigned labels, and the prior generated correlation matrix\n\u2022 31 independent binary models are trained using L2regularized SVM. This serves as the baseline.\n\u2022 For a review, the J labels with highest probabilities from the independent models are selected.\n\u2022 Probabilities of pairwise-combinations of these labels are computed as P (a)P (b)P (a, b) \u03b1\n\u2022 K (a, b) pairs with maximum probability product are chosen and distinct labels constitute the predicted set.\nThe algorithm does not compute the theoretical model exactly but is an approximation. It is similar to beam search."}, {"heading": "Hyperparameters", "text": ""}, {"heading": "VII. Error Metric", "text": "The overall prediction error can be seen as a combination of two different kinds of errors - false positives and false negatives. False positives are the labels that are in the predicted label set but not in groundtruth, and false negatives are the labels present in groundtruth, but not in predicted label set.\nWe do not weigh the two identically, but rather give slightly higher importance to false negatives. The reason behind this is that over-predictions can be further controlled using added heuristics, but the labels missed can never be regained without looking at the entire output label set again.\nError = 1\u2212 ( Total labels correctly predicted\nNumber of labels in ground truth ) \u00d7 ( Total labels correctly predicted\nTotal labels predicted )\u03b3 \u03b3 is a hyper-parameter which cannot be tuned since that would put it to \u221e, and J and K as 31, i.e. output all the labels, irrespective of the input, since this gives zero error."}, {"heading": "VIII. Results", "text": "Hold out cross validation with 70%/30% split"}, {"heading": "1. Train on Amazon and Test on Amazon Dataset", "text": "Fig 2. gives the performance of our algorithm with the \u201d31 independent models\u201d as the dataset size is varied. As is evident, the algorithm extracts about 10% improvement over the baseline, just by looking at the correlation matrix, after tuning the hyper-parameters. This thus confirms our assumption regarding the utility of the correlation matrix for multi-label classification\n3 with variable number of outputs."}, {"heading": "2. Train and Test on Twitter Dataset", "text": "Fig 3. is the performance of the algorithm when the Twitter dataset is used both for training and testing, after appropriately tweaking the hyper-parameters. The gain is only about 0.1%, which can be attributed to the scarcity of content per Twitter review (the dataset size for Twitter was about 12% of Amazon\u2019s for same number of reviews). Upon inspecting the output labels predicted by our algorithm, it was apparent that, the over predictions were the major contributors to the error, unlike Amazon where Misses were the prime error contributors."}, {"heading": "3. Train on Amazon and Test on Twitter Dataset", "text": "For this case, we train on 100% data from Amazon and test on 100% data from Twitter. Fig 4. gives the performance of our algorithm when the training was\ndone using Amazon\u2019s dataset and testing was done on Twitter\u2019s dataset. The hyper-parameters used are the same as those for Fig 2. Thus, the improvement observed is much smaller, the reason being the same, i.e., scarcity of content per tweet."}, {"heading": "10% K-fold Cross Validation", "text": "K-fold (K=10) cross-validation revealed some important information regarding the algorithms\u2019 performance. While our algorithm had observed a 10 % improvement over the collection of independent binary models for the Hold out cross validation, the same did not hold true for the results obtained from the K-fold cross validation.\nTraining and testing on Amazon no longer gave any improvement, but about a 2% degradation in performance. The results for Twitter dataset also substantially changed, but since both the algorithms got similar shifts, the actual improvements were still marginal as before.\nWe attribute the correlation algorthim\u2019s poor performance to two causes. First, Amazon dataset\u2019s number\n4 of labels per review has a mean of 3.06 and a standard deviation of 1.3. Because the standard deviation is reasonably high, it can be expected that the average number of labels per review for a given partition may be quite different from the global average. This causes our algorithm to produce many false positives when testing on some partitions where the number of labels per review is much lower than average.\nAlso, because our fixed correlation matrix is calculated from 100,000 samples tending towards the true correlation values, some test data partitions are likely to deviate from this average (highly biased test datasets), causing our algorithm to predict based on the true correlation, while the baseline algorithm ignores correlation and trains only on the training data. This hypothesis is supported by smoothing out the bias by increasing K, in which case the two algorithms\u2019 errors converge."}, {"heading": "Comparing Learning Algorithms", "text": "We compare the baseline results of three different models: SVM, Naive Bayes, and Random Forest. Naive Bayes did not scale well, but converged quickly. Additionally, Naive Bayes in Scikit-Learn [5] returned binary probabilities which do not lend themselves well to multi-label classification where each review has a variable number of labels. SVM however, had a noticeable trend and scaled best with increasing dataset.\nThen each model was augmented with the correlation matrix. SVM gave best results and was then optimized by adjusting hyper-parameters."}, {"heading": "Other Approaches Considered", "text": "Another approach was to train 31C2 = 465 (per label pair) separate correlation models. This conditioned the correlation probability on the train data instead of being fixed for a dataset. This increased error slightly and the computational time greatly. This method did not perform as well because, in the new correlation models the error to match an input vector to a label got\nFIG. 7: Baseline error for SVM, Naive Bayes, and Random Forest\nFIG. 8: Correlation algorithm error for SVM, Naive Bayes, and Random Forest\ncompounded in with the correlation error. As a fix, the algorithm weighed both the correlation probability by \u03b1 and independent probabilities by 1\u2212\u03b1. This marginally reduced the error hence did not continue further."}, {"heading": "IX. Analysis", "text": ""}, {"heading": "Precision, Recall and F1 Score", "text": "Train and test on 3000 Twitter reviews, \u03b2 = 1/3 Train and test on 3000 Amazon, Twitter reviews The correlation algorithm weighs false positives and false negatives differently, hence the F\u03b2 score gives a\n5 Predicted Correlated Baseline\nActual TP 1124 FN 658 TP 1124 FN 658 FP 433 TN 25716 FP 382 TN 25767\nTABLE VI: Confusion Matrix for Train Amazon, Test Twitter\nPrecision Recall F1 F\u03b2 Baseline 0.0869 0.3712 0.1409 0.2797 Correlation 0.1334 0.3299 0.1900 0.2876\nTABLE VII: Scores for Training Amazon, Test Twitter\nmore accurate understanding of the algorithm\u2019s performance. F\u03b2 for the correlation algorithm\u2019s test on Amazon is 30.19% better than the baseline\u2019s test, 2.82% better than the baseline\u2019s test on cross-domain learning, and 0.3% worse than baseline\u2019s test on Twitter. This validates the trend in the plots.\nThe false positive counts when training and testing purely on one dataset are usually higher than the false positive counts from the baseline, as in our approach we tend to discount the error of making false positives given we meet true positives.\nTable V shows the confusion matrix for cross domain learning where the correlation algorithm\u2019s true negative count is much higher than that of the baseline. By using the correlation between output labels, the algorithm discards labels that independently had a high probability but were not well correlated with other high probability labels."}, {"heading": "Bias and Variance", "text": "As evident, the observed training error for each dataset was much less than the observed test error, indicating high variance and low bias. To further investigate, Principal Component Analysis was used. Since the feature vectors were based on word occurrences in text, they were significantly larger than the dataset size \u2013 the observed feature size for 2500 dataset size of Amazon reviews was 12000. Due to computational limitations, straightforward PCA was infeasible.\nSparse notation was used to represent feature vector,\nfrom Python\u2019s numpy library. Then Sparse SVD (singular value decomposition) found the smallest subspace that the Feature matrix mapped to, keeping all singular values greater than 1. This significantly reduced the dataset size. The 2500 dataset of Amazon reviews, now reduced to a feature vector of size 2300. While the number of features was still comparable to the number of data points, it was substantially lower than the size of the raw feature vector (1/6).\nThis experimentation exposed two facts. The errors were still the same, but the computation time rose significantly (about five times the previous duration). It was seen that LibLinear, the python library, was tuned to work with large datasets with document based feature vectors (as compared to LibSvm), and not for any general features.\nSince the baseline itself suffers from the issue of high variance and low bias, any augmentations over it would be unable to resolve this by themselves, and would require modifying how the TF-IDF vectors are generated. We had already stemmed the words to check this issue, but evidently this would require a more careful processing of the reviews, before they are converted to features. Moreover, since SVMs enforce larger margin as a metric to evaluate each point, they end with a much lower VC dimension than the size of the feature vector. This was another reason in favor of SVM over Logistic Regression, Naive Bayes, and Random Forest.\nThe baseline resulted in about 10% training error, while our algorithm resulted in about 15% training error. Thus there is an increased bias, although the higher variance is a more pressing issue because the test error is significantly higher than training error for both algorithms."}, {"heading": "X. Conclusion", "text": "Using the correlation matrix to help classify reviews appeared to be a good approach, justified by peaks in correlation. The results show, however, that when performing multi-label classification with a sufficiently sized feature set, augmentation of an independent model with second order correlation probability shows only marginal improvements.\nThe model was modified from the baseline by adding a single feature: second order correlation. This is the greedy method of selecting features, in contrast to exploring correlation of all combinations of labels. In such a case where we did not explore higher order correlation we could have missed, for example, that labels A, B, and C never occur together \u2013 a piece of information that could have proved vital to the model\u2019s representation of the true data. Thus, if computation time and space are not an issue, higher order correlation should be considered.\n6 Additionally, SVM, as was already known, has shown to be best for learning on large feature sets because it will attempt to reduce the set and hence generalize better."}, {"heading": "XI. Challenges & Future Work", "text": "The correlation matrix was built specific for every dataset, so to apply the learning algorithms on cross datasets a generalized correlation framework is needed.\nCurrently the hyper-parameter K is optimized to choose number of predicted labels using supervised techniques. The number of labels for each review could be predicted by using unsupervised techniques such as k-means clustering on label probabilities.\nThe datasets have different inherent structures which\ndo not generalize well when using a common learning algorithm. For the Twitter dataset, each feature vector is about 12% the size of a corresponding feature vector in the Amazon dataset. The features should be supplemented with options such as semantic relations in text to make classifications and context of user history.\nThe reviews have sub-level labels associated with them which are rolled up to first-level labels to make the classifications. The algorithm should extend to make hierarchical classifications."}, {"heading": "XII. Acknowledgments", "text": "Sincere thanks to Professor Ashutosh Saxena and to Aditya Jami for advising the research team and providing the labelled datasets of product reviews from Amazon and Twitter.\n[1] Tsoumakas, Grigorios; Katakis, Ioannis (2007). \u201dMultilabel classification: an overview\u201d . International Journal of Data Warehousing and Mining 3 (3): 1\u201313 doi:104018/jdwm.2007070101 [2] cs.stanford.edu/people/adityaj/cs229_fall2014_ dataset.html [3] miscsnapnets, author = Jure Leskovec and Andrej Krevl, title = SNAP Datasets: Stanford Large Network Dataset\nCollection, howpublished = http://snap.stanford.edu/ data, month = jun, year = 2014 [4] D. Albanese, R. Visintainer, S. Merler, S. Riccadonna, G. Jurman, C. Furlanello. mlpy: Machine Learning Python, 2012. arXiv:1202.6548 [5] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."}], "references": [{"title": "Multilabel classification: an overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "Journal of Data Warehousing and Mining", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Krevl, title = SNAP Datasets: Stanford Large Network Dataset  Collection, howpublished = http://snap.stanford.edu/ data, month = jun, year", "author": ["miscsnapnets", "author = Jure Leskovec", "Andrej"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "C", "author": ["D. Albanese", "R. Visintainer", "S. Merler", "S. Riccadonna", "G. Jurman"], "venue": "Furlanello. mlpy: Machine Learning Python", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Previously multi-label classification problems were solved using problem transformation techniques (converting the problem into binary classification problems per output label), or by adapting the algorithm to directly perform multi-label classification [1].", "startOffset": 254, "endOffset": 257}, {"referenceID": 2, "context": "LIBLINEAR only supports a linear Kernel but is very fast relative to LIBSVM and hence used in the learning algorithms [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "[1] Tsoumakas, Grigorios; Katakis, Ioannis (2007).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "html [3] miscsnapnets, author = Jure Leskovec and Andrej Krevl, title = SNAP Datasets: Stanford Large Network Dataset Collection, howpublished = http://snap.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "edu/ data, month = jun, year = 2014 [4] D.", "startOffset": 36, "endOffset": 39}], "year": 2015, "abstractText": "I. Abstract This paper attempts multi-label classification by extending the idea of independent binary classification models for each output label, and exploring how the inherent correlation between output labels can be used to improve predictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models were constructed, with SVM giving the best results: an improvement of 12.9% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.", "creator": "LaTeX with hyperref package"}}}