{"id": "1511.08032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning to detect video events from zero or very few video examples", "abstract": "in this work we deal critically with the problem of utilizing high - level event detection in video. specifically, we study the challenging problems of i ) learning to detect video events from solely a textual description of the event, without using any positive supporting video examples, and ii ) additionally exploiting very few positive training samples together with storing a small specified number of ` ` already related'' videos. for learning only from an event's textual description, additionally we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. consequently for additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the support vector machine that allows us to exploit ` ` related'' event videos by automatically introducing different weights for subsets of the videos in the overall training set. experimental ensemble evaluations performed on the large - scale trecvid med _ 2014 video dataset provide insight on the effectiveness of the proposed methods.", "histories": [["v1", "Wed, 25 Nov 2015 12:17:50 GMT  (1057kb,D)", "http://arxiv.org/abs/1511.08032v1", "Image and Vision Computing Journal, Elsevier, 2015, accepted for publication"]], "COMMENTS": "Image and Vision Computing Journal, Elsevier, 2015, accepted for publication", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["christos tzelepis", "damianos galanopoulos", "vasileios mezaris", "ioannis patras"], "accepted": false, "id": "1511.08032"}, "pdf": {"name": "1511.08032.pdf", "metadata": {"source": "CRF", "title": "Learning to detect video events from zero or very few video examples", "authors": ["Christos Tzelepis", "Damianos Galanopoulos", "Vasileios Mezaris", "Ioannis Patras"], "emails": ["tzelepis@iti.gr).", "dgalanop@iti.gr).", "bmezaris@iti.gr).", "i.patras@qmul.ac.uk)."], "sections": [{"heading": null, "text": "Index Terms\u2014Video event detection, textual event description, zero positive examples, few positive examples, related videos\nI. INTRODUCTION\nH IGH-level (or complex) video event detection is theproblem of finding, within a set of videos, which of them depict a given event. Typically, an event is defined as an interaction among humans or between humans and physical objects [1]. Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2], [3]. For instance, in MED 2014 [2], the defined complex events include Attempting a bike trick, Cleaning an appliance, or Beekeeping, to name a few.\nThe detection of such events in video has recently drawn significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5]. In [6], Brown studies how high-level events play a substantial role in the mechanism of structuring memories and recalling past experiences. This leads to the expectation that event-based organization of video content can significantly contribute to\nC. Tzelepis is with the Information Technologies Institute/Centre for Research and Technology Hellas (CERTH), Thermi 57001, Greece, and also with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London E1 4NS, U.K. (email: tzelepis@iti.gr).\nD. Galanopoulos is with the Information Technologies Institute/Centre for Research and Technology Hellas (CERTH), Thermi 57001, Greece (email: dgalanop@iti.gr).\nV. Mezaris is with the Information Technologies Institute/Centre for Research and Technology Hellas (CERTH), Thermi 57001, Greece (email: bmezaris@iti.gr).\nI. Patras is with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London E1 4NS, U.K. (e-mail: i.patras@qmul.ac.uk).\nbridging the existing semantic gap between human and machine understanding of multimedia content.\nThere are several challenges associated with building an effective detector of video events. One of them is finding a video representation that reduces the gap between the traditional low-level audio-visual features that can be extracted from the video and the semantic-level actors and elementary actions that are by the definition the constituent parts of an event. In this direction, several works have shown the importance of using simpler visual concepts as a stepping stone for detecting complex events (e.g. [7], [8]). Another major challenge is to learn an association between the chosen video representation and the event or events of interest; for this, supervised machine learning methods are typically employed, together with suitably annotated training video corpora. While developing efficient and effective machine learning algorithms is a challenge in its own right, finding a sufficient number of videos that depict the event so as to use them as positive training samples for training any machine learning method is also not an easy feat. In fact, video event detection is even more challenging when the available positive training samples are limited, or even non-existent; that is, when one needs to train an event detector using only textual information that a human can provide about the event of interest.\nIn this work we study the problems of i) learning an event detector solely from a textual description of the event, without using any positive video examples, and ii) learning from very few positive training samples together with a small number of \u201crelated\u201d videos. The paper is organized as follows. In Section II, related work in video event detection using zero or a few positive examples is reviewed. In Section III, we present and examine different design choices for a framework that learns video event detectors based solely on textual information for training, while in Section IV the combination of the above methods with learning from a few positive examples, as well as from related training examples, is examined. Results of the application of the proposed techniques to the TRECVID MED 2014 dataset are provided in Section V. Finally, conclusions are drawn and discussed in Section VI."}, {"heading": "II. RELATED WORK", "text": "There are two broad categories concerning the learning conditions under which a video event detector is trained. That is, training may be done either by using a number of positive and negative training video examples, or using no video examples at all. Within the first category, one can distinguish between i) methods assuming that positive video examples are\nar X\niv :1\n51 1.\n08 03\n2v 1\n[ cs\n.L G\n] 2\n5 N\nov 2\n01 5\n2 in abundance, and ii) methods explicitly embracing the fact that the positive samples that are available for training are typically limited, in practice. Regardless of the assumptions on the number of positive training samples, it is assumed that negative samples can be found without much effort, and thus the number of negative video samples is not a restrictive parameter in the process of learning. The above training conditions are typically simulated by the 100Ex and 10Ex MED subtasks of TRECVID [3], where 100 and 10 positive video samples are available for training video event detectors, respectively.\nTraining based solely on a textual description of each event class is reported in a few works, mostly in the context of the TRECVID MED 0Ex and Semantic Query (SQ) subtasks [3], where no positive video examples are provided. Instead, event detectors are trained using textual resources, which typically include the event\u2019s title, a short free-form text explanation of what may be depicted in a video that belongs to this event class, as well as brief references to visual and audio cues that are typically expected to be present in such a video (Fig. 1)."}, {"heading": "A. Learning from zero positive examples", "text": "Learning from zero positive examples has recently drawn significant attention in various learning problems, due to its challenging nature and the extensive applicability it has. For instance, due to the rapidly increasing number of images on the Web, extensive research efforts have been devoted in multi-label, zero-example (or few-example) classification in images [9]. Similarly, a method for zero-example classification of fMRI data was proposed in [10]. In [11], a method for predicting unseen image classes from a textual description, using knowledge transfer from textual to visual features, was proposed.\nIn the video domain, learning from zero positive examples is investigated primarily in the context of video event detection. In [12] this problem is addressed by transforming both the event\u2019s textual description and the visual content of un-classified videos in a high dimensional concept-based representation, using a large pool of concept detectors; then relevant videos are retrieved by computing the similarities between these representations. Similarly, in [13], each event class title is used as an input query to a text retrieval system, and the most relevant documents are retrieved. The vectorized words of these documents are then projected into the most semantically similar concepts from different modalities, such as ASR (Automatic Speech Recognition), OCR (Optical Character Recognition), and high-level features coming from applying audio-visual and DCNN (Deep Convolutional Neural Networks) concept detectors. Using these concepts, relevant videos are retrieved, and late fusion is used for combining the different ranked lists of videos so as to generate the final event detection results. In [14], multiple low-level representations using both the visual and the audio content of the videos are extracted, along with higher-level semantic features coming from ASR transcripts, OCR, and off-the-shelf video concept detectors. This way, both audio-visual and textual features are expressed in a common high-dimensional concept space,\nwhere the computation of similarity is possible. In [15], logical operators are used to discover different types of composite concepts, which leads to better event detection performance.\nMoreover, in [16], a relevance feedback approach is used in order to improve event detection results in the zero-example problem using features computed from several modalities. The main idea is to use the textual information that describes the event class in order to create queries for each modality. Then, the system results in ranked video lists, one per each modality. The top videos from these lists are used as a \u201cpseudo label\u201d video set on which a joint model is trained, and a new ranked list is produced and used for creating a new \u201cpseudo label\u201d set; this process is iterated a few times.\nIn [17], E-Lamp was proposed, which is a zero-example event detection system made of four subsystems. The first one is an off-line indexing component, while the rest of them compose the on-line event search module. In the off-line module, each video is represented with 4043 visual concepts along with ASR and OCR high-level features. Then, in the on-line search module, the user-specified event description is translated into a set of relevant concepts, called system query. This system query is used to retrieve the videos that are most relevant to the event. Finally, a pseudo-relevance feedback approach is exploited in order to improve the results.\nOur approach goes beyond the classic semantic similarity comparison between a given event title (or other user-specified event cues) and each concept title from a concept pool. We try to enrich each concept by automatically searching in Google or Wikipedia in order to find more information for it; this enables finding semantic similarities between events and concepts more effectively. Exploiting Google Search or Wikipedia is, to the best of our knowledge, novel."}, {"heading": "B. Learning from a few positive examples", "text": "A limited number of studies have considered the problem of learning video event detectors from very few (e.g. 10) positive training examples [18], [19], [20], [21]. In [18], visual static (e.g. SIFT [22], Transformed Color Histogram [23]), and motion (e.g. MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27]. ASR and OCR techniques are also used for exploiting audio and textual information in video streams, as well as audio features and visual features based in DCNNs trained in ImageNet [28]. DCNN-based features typically comprise one or more of the network\u2019s hidden layers [29], providing high discriminative power [5]. Based on these features, video event detectors are trained using multiple SVM classifiers and fusion techniques (early and late) for combining different modalities. In SESAME [19], the authors also use DCNN classification scores as video features, which are subsequently fed to a kernel SVM for obtaining event detectors. ASR and low-level audio features are also extracted, and a logistic regression-based fusion technique is used for combining scores from different modalities in order to obtain the final event detectors. In [13], [30], [20], a similar training framework for learning from a few positive samples is used; low-level visual and/or audio features are combined with\n3\nconcept detectors\u2019 output scores for obtaining event detectors. Concept detectors are trained either using low-level features and VLAD vectors [31], or DCNN output layers.\nFurthermore, it is not unusual to have in the training set videos that do not exactly fulfill the requirements to be characterized as true positive examples, but nevertheless are closely related to an event class and can be seen as \u201crelated\u201d examples of it. This is simulated in the TRECVID MED task [3] by the \u201cnear-miss\u201d video examples provided for each target event class. Differently from our method, none of the above works takes full advantage of these related videos for learning from few positive samples; instead, the \u201crelated\u201d samples are either excluded from the training procedure, or they are treated as true positive or true negative instances [32].\nIn our study, we will consider \u201crelated\u201d training samples as videos that can contribute to event detector training but do not merit being treated as either true positive or true negative instances. To take advantage of them, we employ a Relevance Degree SVM (RDSVM), which can treat these samples as either weighted negatives or weighted positives in conjunction with an automatic weighting selection scheme."}, {"heading": "III. DETECTING VIDEO EVENTS USING AN EVENT\u2019S TEXTUAL DESCRIPTION", "text": ""}, {"heading": "A. General architecture", "text": "In this section we propose a framework for video event detection without using any visual knowledge about the events. We assume that the only knowledge available, with respect to each event class, is a textual description of it, which consists of a title, a free-form text, and a list of possible visual and audio cues, as in [33], [16]. Fig. 1 shows an example of such a textual description for the event class Attempting a bike trick. For linking this textual information with the visual content of the videos that we want to examine, similarly to [14], [18], we a) use a pool of Nc concepts along with their titles and in some cases a limited number of subtitles (e.g. concept bicycle-built-for-two has the subtitles tandem bicycle and tandem), and b) a pre-trained detector (based on DCNN output scores) for each concept. Fig. 2 illustrates the structure of the proposed framework. Shaded blocks indicate processing stages for which different design choices are examined in this work. Each of these stages is discussed below in detail, while Table I sums up the different considered design choices.\nFigure 2a shows a first process that receives a textual description of an event class and a list of concepts, and generates an event detector. Given the textual description of the event class, our framework first identifies N words or phrases that most closely relate to the event class; we call this wordset the Event Language Model (ELM). In parallel, for each of the Nc concepts of our concept pool, it similarly identifies M words or phrases; we call this set the Concept Language Model (CLM) of the corresponding concept.\nSubsequently, for each word in ELM and each word in each one of CLMs we calculate the Explicit Semantic Analysis (ESA) distance [34] between them. For each CLM, the resulting N \u00d7M distance matrix expresses the relation between the given event class and the corresponding concepts. In order to compute a single score expressing this relation, we apply to this matrix different operators, such as various matrix norms or distance measures. Consequently, a score is computed for each pair of ELM and CLM. The Nc considered concepts are ordered according to these scores (in descending order) and the K top concepts along with their scores constitute our event detector. In order to perform event detection in a video collection, we compare this event detector with the output scores of concept detectors applied on each video, using different similarity measures (Fig. 2b). Thus, the final output is a ranked list of the most relevant videos. Alternatively, multiple event detectors can be generated using more than one different algorithm variations in each of the shaded blocks of Fig. 2a and 2b, and these can be used as pseudo-positive samples for training an SVM, which can then be applied to the videos so as to generate a ranked list of those depicting the target event (Fig. 2c)."}, {"heading": "B. Language Models", "text": "We examine the construction of three different types of ELMs, depending on the textual information that they use (Fig. 1). The first type of ELM, which will be denoted as \u201cTitle\u201d hereafter, is based on the automatic extraction of word terms solely from the title of an event class. The second type of ELM, denoted as \u201cVisual\u201d, is constructed by using only the visual cues provided along with the textual description of each event class. Both title and visual cues consist only of words and single phrases, such as attempting a bike trick, bike, riding bike on one wheel, etc. These words can automatically be extracted from the textual description without any humanexpert intervention. Finally, the third type of ELM is obtained by the automatic extraction of words based on the visual and audio cues, as well as on the short free form text of an event class, and it is denoted as \u201cAudioVisual\u201d.\nAccordingly, a CLM is constructed for each one of the Nc concepts. We examine the construction of six different types of CLMs, depending on the textual information used for each concept, as well as the weighting technique (e.g. Tf-Idf) adopted for transforming this textual information in a Bag of Words (BoW) [35] representation. As a first approach, the title of a concept, along with any available subtitle, are used as a query to the Google search engine, and the text of the top20 search results per query are retrieved. By applying text\n4 (a) Creation of event detector without positive video samples.\n(b) Event detection in a video collection, using the event detector of the above sub-figure.\n(c) Pseudo-positive sample creation and training.\nFig. 2: The proposed framework for detecting video events using zero positive examples.\ncleaning techniques (removing html tags, stop words, etc.) and Bag of Words statistics, we select the most frequently occurring words. These, together with the concept\u2019s title and subtitles, constitute the M words of the CLM. As a second approach, the concept title and any subtitle are similarly used as a query in Wikipedia, and the 20 most relevant articles per query are retrieved. By following the same procedure as above, we end up with the top-M words of the concept\u2019s CLM. As a third approach, we use only the title and the subtitles in the CLM. In the three above approaches, the Bag-of-Words (BoW) can be constructed with or without using Tf-Idf weighting [36], this resulting in six different types of CLMs.\nThe above types of ELMs and CLMs are introduced as different design choices, as shown in Fig. 2a. Table I summarizes the specific types of each one of them."}, {"heading": "C. Building an event detector", "text": "The constructed language models represent the given event class and each of the available concepts as ranked lists of words. Thus, we can calculate the similarities between them by computing the semantic similarity between each word in the ELM and each word belonging to the CLMs. To this end, we use the ESA semantic relatedness measure [37], which calculates the similarity distance between two terms by computing the cosine similarity between their weighted\nvectors of Wikipedia articles. In this way, an N \u00d7M matrix with scores for each pair of event-concept is computed. Let S denote the aforementioned similarity matrix; that is, its (i, j)-th entry, si,j , denotes the similarity distance between the word wi in ELM and the word wj in the respective CLM. Then, we can arrive at a single score expressing the relation between the above ELM and CLM by evaluating one of the matrix operators shown in Table I (row C3). It is worth noting that \u03bbmax(S>S) denotes the maximum eigenvalue of the covariance matrix S>S.\nFollowing this process, for each event class we end up with a list of concept scores. The event detector is then defined as the set of the top-K scores and the corresponding concept labels. An example of such an event detector is given in Fig. 3 for the event class Attempting a bike trick, where we observe that the three most dominant concepts are the following: crash helmet, bicycle-built-for-two, and mountain bike."}, {"heading": "D. Applying the event detector to a video dataset", "text": "The constructed event detector is used as shown in Fig. 2b. The DCNN-based detectors for the Nc concepts in our concept pool are applied to the videos of the dataset, thus having these videos represented as vectors of concept detector output scores (hereafter called model vectors). The output scores that correspond to the K concepts comprising the event detector are selected, and are compared with the corresponding values of the event detector. For this comparison, different choices of a distance function are possible, as shown in Table I (row C4); that is, we examine the use of following distance functions: Euclidean, Histogram Intersection, Chisquare, Kullback-Leibler, and cosine distance. For a chosen distance function, repeating the above process for all videos in a dataset we get a ranked list, in descending order, of the videos that most closely relate to the sought event."}, {"heading": "E. Using multiple event detectors as pseudo-positive training samples", "text": "Figure 2c shows an alternative approach to using event detectors that are built as presented in section III-C. An event detector, i.e. a ranked list of concept scores (Fig. 2a, Fig. 3) can be considered as a pseudo-positive sample for the respective event class. Since multiple event detectors can be obtained for a given event class by varying the design choices C1 to C3 of Fig. 2a (as shown in Table I)), a set of pseudo-positive samples can be obtained for each event class. Negative samples for the given event class can also be obtained, in two ways. First, samples that are pseudo-positive for other event classes can be considered as pseudo-negative for a particular event class. Secondly, real videos can be selected from the Web, in analogy to how images are often selected as negative samples for training concept detectors from Web data, e.g. [38], [39]. Using these pseudo-positives and (pseudo-)negatives, a new machine learning-based event detector can be obtained by training an SVM model (for instance using the RBF kernel). This can then be evaluated on any video dataset, following the application of trained concept detectors to the videos, (as in section III-D), to give us a new ranked list of event-related videos."}, {"heading": "IV. LEARNING A VIDEO EVENT DETECTOR FROM A FEW POSITIVE AND RELATED VIDEO EXAMPLES", "text": ""}, {"heading": "A. Exploiting related videos for learning from a few examples", "text": "As discussed in section II, in the problem of video event detection, it is not unusual to be provided, or be able to find, some related video samples, i.e. videos that are closely related with a given complex event, but do not meet the exact requirements for being a true positive event instance. Exploiting related samples can be particularly interesting when only a few true positive samples are available, since in the opposite case, when an abundance of positive samples are available, one can effectively learn from them.\nIn this section, Relevance Degree Support Vector Machine (RDSVM), proposed in [21] for handling \u201crelated\u201d training samples, is employed such that related samples are taken into consideration as weighted negative or weighted positive examples, where weighting is carried out completely automatically. RDSVM extends the standard SVM algorithm such that each training sample is assigned with a confidence value in (0, 1]\nindicating the degree of relevance of each training sample with the class it is related.\nLet X = {(xi, yi) : xi \u2208 Rd, yi \u2208 {0,\u00b11}, i = 1, . . . , l} be an annotated dataset of l observations, where xi \u2208 Rd is the feature vector representation of the i-th observation in the ddimensional space with label yi \u2208 {0,\u00b11} denoting that the ith observation is a positive (yi = +1), a negative (yi = \u22121), or a related (yi = 0) instance of the class. To allow the use of the RDSVM, the above is reformulated as X = {(xi, yi, ui) : xi \u2208 Rd, yi \u2208 {\u00b11}, ui \u2208 {0, 1}, i = 1, . . . , l}, where ui is the socalled relevance label denoting that the i-th observation is a true (ui = 0) or a related (ui = 1) instance of the class yi.\nFor the exploitation of the related observations, as proposed in [21], [40], each training sample xi is associated with a adjustable confidence value vi, and a monotonically increasing function g(vi) (called slack normalization function) is used to weight each slack variable \u03bei denoting the loss introduced by a misclassified sample. In this way, support vectors (SVs) that are associated with a higher confidence value have greater\n6\ncontribution to the computation of the decision function. In [21], this function is modified so that only related class observations are associated with a confidence value ci \u2208 (0, 1] (called hereafter relevance degree) indicating the degree of relevance of the i-th observation with the class it is related. That is, g is defined as follows:\ng(ui) = { 1 if ui = 0, ci if ui = 1.\n(1)\nThus, the contribution of the related samples in the computation of the decision function can be regulated using appropriate relevance degrees ci.\nIn this study, we follow the approach proposed in [21] for handling the related samples as a subclass of the positive or negative class, for which a global relevance degree, c = ci \u2200i, is assigned to all related samples. Parameter c is optimized using a cross-validation procedure. The proposed technique for the automatic selection of related samples as weighted positives, or weighted negatives, includes the following steps. First, an RDSVM is trained using the related videos as weighted positive examples (i.e., yi = +1 and ui = 1). Then, another RDSVM is trained using the related videos weighted negative examples (i.e., yi = \u22121 and ui = 1). The above classifiers are denoted in Fig. 4 as RDSVMp and RDSVMn, respectively.\nIn both cases the basic parameters of the RDSVMs (e.g. C, \u03b3 for an RDSVM with RBF kernel) are optimized by conducting cross-validation (grid search) with c set to 1, i.e. treating the related samples as pure positive and pure negative samples, respectively. Subsequently, the relevance degree parameter c is optimized using cross-validation (line search) in the range (0, 1]. After both RDSVMp and RDSVMn are trained in this way, one of the two is chosen (i.e., a parameter set {C, \u03b3, c}, where c \u2208 [\u22121, 1] is chosen) by looking at the average performance measure (e.g. average precision (AP)) values attained during cross-validation by the RDSVMp and RDSVMn (the one with the highest AP is selected). The chosen RDSVM is a learned event detector, that can be applied to a set of videos (where again the video representation xi can be the same as in section III: a model vector produced by application of DCNN-based concept detectors)."}, {"heading": "B. Treating pseudo-positive training samples as related ones", "text": "In section III-E we discussed how we could use the output of processing an event\u2019s textual description as a set of pseudopositive event samples, for training a standard SVM. When we also have a few true positive videos available for training,\none possibility would be to merge the two positive/pseudopositive sets and use their union for SVM training. Since, however, the pseudo-positive samples were artificially created and thus may be corrupt by errors/noise, a better option would be to treat them as \u201crelated\u201d samples, rather than true positives. This can by straightforwardly achieved by use of the RDSVM methodology presented in the previous section. Evaluation of this approach will be presented in section V-D."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Datasets and experimental setup", "text": "The proposed techniques are tested on the large-scale video dataset of the TRECVID Multimedia Event Detection (MED) 2014 task (hereafter referred to as MED14). The groundtruth annotated portion of it consists of three different video subsets: the \u201cpre-specified\u201d (PS) video subset (2000 videos, 80 hours, 20 event classes), the \u201cad-hoc\u201d (AH) video subset (1000 videos, 40 hours, 10 event classes), and the \u201cbackground\u201d (BG) video subset (5000 videos, 200 hours). Each video in the above dataset belongs to either one of 30 target event classes (Table II), or (in the case of the BG subset) to the \u201crest of the world\u201d class . The above video dataset (PS+AH+BG) is partitioned such that a training and an evaluation set are created, as follows: \u2022 Training Set\n\u2013 50 positive samples per event class \u2013 25 related (near-miss) samples per event class \u2013 2496 background samples (negative for all event\nclasses) \u2022 Evaluation Set\n\u2013 \u223c 50 positive samples per event class \u2013 \u223c 25 related (near-miss) samples per event class \u2013 2496 background samples (negative for all event\nclasses) For video representation, approximately 2 keyframes per second were extracted. Each keyframe was represented using the last hidden layer of a pre-trained Deep CNN. More specifically, a 16-layer pre-trained deep ConvNet network provided in [29] was used. This network had been trained on the ImageNet data [28] and provides scores for 1000 concepts. Thus, each keyframe has a 1000-element vector representation. Then, a video-level model vector for each video is computed by taking the average of the corresponding keyframe-level representations.\nB. Video event detection using the event\u2019s textual description\nThe framework proposed in Section III allows for different design choices in its various stages (Table I, and shaded processing stages in Fig. 2). There are 5 stages that can be parameterized: the ELM and CLM information sources selection (C1, C2a), the CLM textual vector weighting strategy (C2b), the matrix operator used (C3), and the distance function selected (C4). Based on the possible choices listed in Table I, 450 different combinations are possible and were tested.\nTable III presents the 10 best-performing combinations in terms of mean average precision (MAP) across all 30 events.\n7 Target Events E021 - Attempting a bike trick E036 - Felling a tree E022 - Cleaning an appliance E037 - Parking a vehicle E023 - Dog show E038 - Playing fetch E024 - Giving directions to a location E039 - Tailgating E025 - Marriage proposal E040 - Tuning a musical instrument E026 - Renovating a home E041 - Baby Shower E027 - Rock climbing E042 - Building a Fire E028 - Town hall meeting E043 - Busking E029 - Winning a race without a vehicle E044 - Decorating for a Celebration E030 - Working on a metal crafts project E045 - Extinguishing a Fire E031 - Beekeeping E046 - Making a Purchase E032 - Wedding shower E047 - Modeling E033 - Non-motorized vehicle repair E048 - Doing a Magic Trick E034 - Fixing musical instrument E049 - Putting on Additional Apparel E035 - Horse riding competition E050 - Teaching Dance Choreography\nTABLE II: Target events in the employed TRECVID MED dataset.\nNote that, when the \u201cTitle\u201d is used for both ELM and CLM construction (processing stages C1 and C2a), meaning that both ELM and CLMs are represented by a single word or phrase (N = M = 1), then all matrix operations result in the same score, making no difference in the final result. This is the reason why a dash sometimes appears in the C3 column of Table III. Moreover, when \u201cTitle\u201d is selected for the construction of the CLMs (C2a processing stage), there is no need for Bag-of-Words encoding, thus no use of weighting technique (Tf-Idf), since no enrichment by searching in Google or in Wikipedia was carried out. This is why there is no choice in the weighting stage (C2b) when \u201cTitle\u201d is selected. As can be seen, the enrichment of the CLM through Google search, without Tf-Idf weighting and the usage of as much as possible information for constructing the EML resulted in the best result overall.\nIn order to explore the effectiveness of the different design choice combinations, we experimented with changing one design choice at a time, keeping the choices for all other processing stages unaffected (as in the best-performing combination of Table III). Table IV shows the performance of different ELMs in stage C1, clearly suggesting that the exploitation of as much as possible information for the particular parameter leads to better detection results. In Table V, it is shown that enrichment of the CLM in stage C2a by searching in Google is the most effective way to do so. As Table VI shows, using Tf-Idf weighting does not seem to provide any improvement to the detection performance. Table VII suggests that the use of the Hausdorff distance, as a similarity matrix operator, outperforms the rest of the respective choices. Finally, Table VIII shows the performance of different similarity measures (stage C4) in the event detection step (Fig. 2b), where the cosine and the Histogram Intersection measures outperform the rest of them.\nIn Table IX, we present the performance of the training step of our framework, as illustrated in Fig. 2c (denoted as T10), compared to the best combination of the event detection step, shown in Fig. 2b (denoted as T0). As discussed above, for T10 we consider the event detectors generated as shown in Fig. 2a as pseudo-positive instances of the respective event class. Also, as discussed previously, negative samples can be obtained using two different approaches. First, the pseudo-\npositive samples from the rest of the event classes are used as pseudo-negative samples, and, secondly, real negative videos for all event classes, belonging to the \u201cbackground\u201d (BG) training subset, are used. Finally, a linear late fusion approach, using the arithmetic mean operator, is used in order to improve the individual detection results, denoted as T0\u2295T10. Hereafter, we will denote with \u2295 the late fusion scheme where the arithmetic mean operator is used for combining the event detection output scores. We can observe that the combination of the event detection stage, T0, (Fig. 2b) with SVM training with pseudo-negative samples, T10, (Fig. 2c) achieves the best performance, resulting in MAP equal to 12.38%.\nWe compare the proposed method with the E-Lamp framework, a state-of-the-art system that participated in the TRECVID 2014 MED task [18] and is described in detail in [17]. As mentioned in section II-A, the E-Lamp system consists of four major subsystems, namely Video Semantic Indexing (VSIN), Semantic Query Generator (SQG), Multimodal Search (MS) and Pseudo-Relevance Feedback (PRF). The VSIN subsystem represents the input videos as a set of low- and high-level features from several modalities. The highlevel features, i.e. the result of semantic concept detection, are used as input to the SQG subsystem, in which the textual description of an event class is translated into a set of relevant concepts termed system query. The system query is then used in the MS subsystem as input to several well-known text retrieval models in order to find the most relevant videos. These results can be then refined by the PRF subsystem.\nAs SQG leads to the creation of an event detector using semantic concepts, a correspondence exists (and comparison is possible) with our approach to build an event detector as described in sections III-B and III-C (Fig. 2a). Similarly, the MS subsystem corresponds to (and can be compared with) our event detection module presented in section III-D (Fig. 2b). We compared with four SQG approaches that are presented in the E-Lamp system. These are: i) Exact word matching, ii) WordNet mapping using Wu & Palmer measure (Wu) iii) WordNet mapping using the structural depth distance in WordNet hierarchy (Path), and iv) Word Embedding Mapping (WEP). Concerning the MS stage, we compared with the following retrieval methods: the Vector Space Model [33], the Okapi BM25, and two unigram language models with JelinekMercer smoothing (LM-JM) and Dirichlet smoothing (LMDL) respectively [41].\nTable X shows the performance, in terms of mean average precision (MAP), of the above combinations in comparison to the best-performing event detector and distances proposed in the present work (Table VIII). From this Table it is clear that the proposed method for building an event detector outperforms the rest of the compared methods, irrespective of the similarity measure that they are combined with. Out of\nthe event detection creation methods of [17], the exact word seems to perform considerably better than the others (but much worse than the proposed method). This is because the concept labels from our concept pool that are most related to an event are often well-represented in the event\u2019s textual description, e.g. for the event Beekeeping, the word bee is observed 31 times, and this word is directly associated with the concepts bee and bee eater. The WordNet and WEP mappings on the other hand, are not always successful in finding the semantic similarity between two words. Regarding the compared similarity measures, the VSM and LM-DL generally perform better than BM25 and LM-JM, but the proposed cosine and Histogram Intersection distances are consistently among the top-performing measures. It should be noted that the number of visual concepts used in [17] is significantly greater than the 1000 concepts used throughout our experiments, and other modalities (e.g. audio) are also exploited in [17]; this explains the often higher MAP values that are reported in the latter work."}, {"heading": "C. Learning from a few positive and related video examples", "text": "In this section, we validate the performance of the proposed framework for handling related samples for the problem of learning video event detectors from a few positive samples.\nTo this end, we compare the following approaches in order to investigate whether and in what way it is beneficial to use related samples in training: a) using no related samples, b) related samples are used as pure negative ones, as in [32], c) related samples are used as pure positive ones, again as in [32], and d) related samples are used as weighted negative or positive ones under the RD-SVM framework of Section IV-A, which involves an automatic procedure for selecting both the labels of the related samples and their weights.\nAs discussed in section V-A, the MED14 dataset provides 50 positive samples per each of the 30 event classes. However, we want to simulate the case where only a few positive samples are available for training, thus we choose to use only 10 training samples per event class. To this end, for each experiment we randomly draw a subset of 10 positive samples for each event class, and we repeat this 10 times. That is, for each compared approach in this section, the obtained performance of the corresponding classifier (RDSVM or standard SVM) is averaged over 10 iterations. For the approaches that use related samples, 10 such samples were chosen from the pool of 25 related samples that are available for each event class; these, as suggested in [21], were selected as the 10 that are the nearest to the median of all 25 related samples in the employed feature space.\nTable XI shows the results of these comparisons, in terms of mean average precision (MAP), across the 30 event classes of the MED14 dataset. We observe that when related samples are used as pure negative or pure positive ones as in [32], the overall detection performance is lower than the baseline approach which does not use these samples at all (P10). In contrast to this, the proposed approach that treats related samples as positive/negative ones with automatic weighting achieved better performance than the baseline, reaching a MAP of 18.95%.\nIn Fig. 5 the results of the above comparisons are given separately for each of the 30 event classes of the dataset. As can be seen, despite the fact that treating related samples as either pure negatives or pure positives jointly for all the event classes leads to worse average detection performance (MAP), compared to the case where they are not used at all in the training process, there are event classes where it is beneficial to use them in such a way. Specifically, for 15 out of 30 events it is better to use related samples as pure positives rather that to exclude them from the training process, and similarly for one event it is beneficial to use them as pure negatives. Automatically selecting to use related samples as weighted negatives or weighted positives using RD-SVM [21], on the other hand, is better than excluding them from the annotation\nprocess for 19 out of the 30 event classes. Moreover, the automatic weight selection leads to better results than both approaches that use related samples without any weight for 18 out of the 30 events. The above confirm our hypothesis that treating related samples as weighted negatives/positives using RD-SVM [21] can lead to better event detection performance."}, {"heading": "D. Combining textual event detectors with learning from a few positive and related samples", "text": "Our first attempt to combine textual event detectors and learning from a few examples is based on exploiting the pseudo-positive samples, which were computed according to section III-E, as related samples in RD-SVM. We chose a subset of 10 pseudo-positive samples for each event class, using a same selection strategy as in the case of related samples in section V-C; that is, the 10 nearest to the median pseudopositive sample for each event class were selected. Using RDSVM for handling pseudo-positive samples as weighted negative or positive ones (with automatic weighting selection) resulted in a MAP equal to 18.26%, across 30 events (denoted as R10p in Table XII). It is worth noting that, similarly to the previous results, this experimental result stands for the average of 10 iterations (using 10 different, randomly selected sets of 10 positive examples each). We observe that, using pseudopositive samples this way outperforms learning from solely 10 positive samples by a small margin.\nIn a second attempt to further combine text-based and learning-based event detectors, we examine the late fusion of detectors. As shown in the last column of Table IX , T0 \u2295 T10 denotes the best approach for learning video event detectors using the textual description of each event class alone, resulting in a MAP equal to 12.38%. The results of combining the above approach, as well as R10p (which already jointly uses textual information and real training samples) with the P10 and R10 learned detectors (Table XI), are shown in Table XII. As we can see, the best-performing combination is that of R10 and R10p, which achieves MAP equal to 20.11%. This is higher that the best performance that is achieved in our experiments using visual examples alone (R10), and much\n10\nhigher than that achieved using only textual information about the sought event, highlighting the importance of combining visual examples and a textual description of the event for learning. Other combinations, most notably that of T0 \u2295 T10 with P10, R10 or R10p, do not seem to offer any improvement over R10p alone. This can be attributed to the fact that the T0 \u2295 T10 detector is much weaker than P10, R10, and R10p, thus introducing mostly noise to the results of the latter at the late fusion stage.\nIn Fig. 6, we present the event detection results per event class, using Average Precision (AP), for i) the bestperforming proposed approach that learns from the events\u2019 textual descriptions (last column of Table IX); ii) the bestperforming proposed approach for learning from a few positive and related examples (last column of Table XI); and, iii) the best combination of Table XII (last row) for exploiting both video examples and a textual description of the event. We observe that the latter combination outperforms the former two approaches for 22 of the 30 event classes. Notable exceptions, where using just the textual description of the\nevent performs the best, are events E031 (Beekeeping), E037 (Parking a vehicle), and E045 (Extinguishing a Fire). This can be attributed to the fact that we have in our concept pool a wealth of concepts related to these events (e.g. for E037: beach wagon, car mirror, electric locomotive, minibus, parking meter, recreational vehicle, sports car, streetcar, etc.; for E045: fire engine, fire screen, fireboat, gas pump, stove, water tower, etc.) and at the same time the textual description which is performed for these events allows us to identify those related concepts (e.g. for E031, the top-4 concepts that T0\u2295T10 identifies are: honeycomb, apiary, bee, bee eater). In contrast to this, for several of other events only one or two concept (or even no concepts at all) closely relate to them, e.g. for event E047 (Modeling) the closest-related concept that is included in our concept pool is kimono and, similarly, concept grocery store for event E046 Making a Purchase."}, {"heading": "VI. CONCLUSION", "text": "In this paper we proposed a framework for learning video event detectors from solely a textual description of an event class, or from a very few positive and related training samples. We identified a general learning framework and studied the impact of various design choices for different stages of this framework. For exploiting related video samples we employed an SVM extension (RDSVM) such that related samples are automatically treated as weighted negative or positive samples. The experimental evaluation of the proposed approaches, as well as the combination of them on the challenging, large-scale TRECVID MED 2014 video dataset verified the applicability of the proposed methods in the cases where positive samples are not available, or scarce, and provided useful insight on how to train video event detectors under such conditions.\n11"}, {"heading": "VII. ACKNOWLEDGMENT", "text": "This work was supported by the European Commission under contracts FP7-600826 ForgetIT and FP7-287911 LinkedTV."}], "references": [{"title": "High-level event recognition in unconstrained videos", "author": ["Y.-G. Jiang", "S. Bhattacharya", "S.-F. Chang", "M. Shah"], "venue": "International Journal of Multimedia Information Retrieval, pp. 1\u201329, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "An overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["P. Over", "G. Awad", "M. Michel", "J. Fiscus", "G. Sanders", "W. Kraaij", "A.F. Smeaton", "G. Quenot"], "venue": "Proc. of TRECVID 2013. NIST, USA, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "An overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["\u2014\u2014"], "venue": "Proc. of TRECVID 2014. NIST, USA, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Event detection in baseball video using superimposed caption recognition", "author": ["D. Zhang", "S.-F. Chang"], "venue": "Proc. of the 10th ACM Int. Conf. on Multimedia. ACM, 2002, pp. 315\u2013318.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A large-scale benchmark dataset for event recognition in surveillance video", "author": ["S. Oh", "A. Hoogs", "A. Perera", "N. Cuntoor", "C.-C. Chen", "J.T. Lee", "S. Mukherjee", "J. Aggarwal", "H. Lee", "L. Davis"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2011, pp. 3153\u20133160.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "On the prevalence of event clusters in autobiographical memory", "author": ["N.R. Brown"], "venue": "Social Cognition, vol. 23, pp. 35\u201369, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Video event detection using a subclass recoding errorcorrecting output codes framework", "author": ["N. Gkalelis", "V. Mezaris", "M. Dimopoulos", "I. Kompatsiaris", "T. Stathaki"], "venue": "Multimedia and Expo (ICME), IEEE Int. Conf. on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Video event detection using generalized subclass discriminant analysis and linear support vector machines", "author": ["N. Gkalelis", "V. Mezaris"], "venue": "Proc. of Int. Conf. on multimedia retrieval. ACM, 2014, p. 25.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "COSTA: Co-occurrence statistics for zero-shot classification", "author": ["T. Mensink", "E. Gavves", "C.G. Snoek"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2014, pp. 2441\u20132448.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, Eds. Curran Associates, Inc., 2009, pp. 1410\u20131418.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Write a classifier: Zero-shot learning using purely textual descriptions", "author": ["M. Elhoseiny", "B. Saleh", "A. Elgammal"], "venue": "Computer Vision (ICCV), IEEE Int. Conf. on. IEEE, 2013, pp. 2584\u20132591.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Videostory: A new multimedia embedding for few-example recognition and translation of events", "author": ["A. Habibian", "T. Mensink", "C.G. Snoek"], "venue": "Proc. of the ACM Int. Conf. on Multimedia. ACM, 2014, pp. 17\u201326.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "BBN VISER TRECVID 2014 multimedia event detection and multimedia event recounting systems", "author": ["Y. Guangnan", "L. Dong", "C. Shih-Fu", "S. Ruslan", "M. Vlad", "D. Larry", "G. Abhinav", "H. Ismail", "G. Sadiye", "M. Ashutosh"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Zeroshot event detection using multi-modal fusion of weakly supervised concepts", "author": ["S. Wu", "S. Bondugula", "F. Luisier", "X. Zhuang", "P. Natarajan"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2014, pp. 2665\u20132672.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Composite concept discovery for zero-shot video event detection", "author": ["A. Habibian", "T. Mensink", "C.G. Snoek"], "venue": "Proc. of Int. Conf. on Multimedia Retrieval. ACM, 2014, p. 17.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-example event search using multimodal pseudo relevance feedback", "author": ["L. Jiang", "T. Mitamura", "S.-I. Yu", "A.G. Hauptmann"], "venue": "Proc. of Int. Conf. on Multimedia Retrieval. ACM, 2014, p. 297.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Bridging the ultimate semantic gap: A semantic search engine for internet videos", "author": ["L. Jiang", "S.-I. Yu", "D. Meng", "T. Mitamura", "A.G. Hauptmann"], "venue": "Int. Conf. on Multimedia Retrieval, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Informedia at TRECVID 2014 MED and MER", "author": ["S.-I. Yu", "L. Jiang", "Z. Mao", "X. Chang", "X. Du", "C. Gan", "Z. Lan", "Z. Xu", "X. Li", "Y. Cai"], "venue": "NIST TRECVID Video Retrieval Evaluation Workshop, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The 2014 SESAME multimedia event detection and recounting system", "author": ["R. Bolles", "B. Burns", "J. Herson"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "ITI-CERTH participation to TRECVID 2014", "author": ["N. Gkalelis", "F. Markatopoulou", "A. Moumtzidou", "D. Galanopoulos", "K. Avgerinakis", "N. Pittaras", "S. Vrochidis", "V. Mezaris", "I. Kompatsiaris", "I. Patras"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving event detection using related videos and relevance degree support vector machines", "author": ["C. Tzelepis", "N. Gkalelis", "V. Mezaris", "I. Kompatsiaris"], "venue": "Proceedings of the 21st ACM Int. Conf. on Multimedia. ACM, 2013, pp. 673\u2013676.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluating color descriptors for object and scene recognition", "author": ["K.E. Van De Sande", "T. Gevers", "C.G. Snoek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 32, no. 9, pp. 1582\u2013 1596, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "MoSIFT: Recognizing human actions in surveillance videos", "author": ["M.-y. Chen", "A. Hauptmann"], "venue": "Technical Report.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 0}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International Journal of Computer Vision, vol. 105, no. 3, pp. 222\u2013245, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "Computer Vision\u2013ECCV 2010. Springer, 2010, pp. 143\u2013156.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "SRI-Sarnoff AURORA system at TRECVID 2014 multimedia event detection and recounting", "author": ["H. Cheng", "J. Liu", "I. Chakraborty", "G. Chen", "Q. Liu", "M. Elhoseiny", "G. Gan", "A. Divakaran", "H. Sawhney", "J. Allan", "J. Foley", "M. Shah", "A. Dehghan", "M. Witbrock", "J. Curtis"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The INRIA-LIM-VocR and AXES submissions to TRECVID 2014 multimedia event detection", "author": ["M. Douze", "D. Oneata", "M. Paulin", "C. Leray", "N. Chesneau", "D. Potapov", "J. Verbeek", "K. Alahari", "Z. Harchaoui", "L. Lamel", "J.-L. Gauvain", "C.A. Schmidt", "C. Schmid"], "venue": "2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal knowledge-based analysis in multimedia event detection", "author": ["E. Younessian", "T. Mitamura", "A. Hauptmann"], "venue": "Proc. of the 2nd ACM Int. Conf. on Multimedia Retrieval. ACM, 2012, pp. 51:1\u201351:8.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis.", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "in IJCAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Introduction to modern information", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1986}, {"title": "Mining of massive datasets", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "EasyESA: A low-effort infrastructure for explicit semantic analysis", "author": ["D. Carvalho", "C. Call\u0131", "A. Freitas", "E. Curry"], "venue": "Proc. of the 13th Int. Semantic Web Conf. (ISWC), 2014, pp. 177\u2013180.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Negative pseudo-relevance feedback in content-based video retrieval", "author": ["R. Yan", "A.G. Hauptmann", "R. Jin"], "venue": "Proc. of the 11th ACM Int. Conf. on Multimedia. ACM, 2003, pp. 343\u2013346.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Bootstrapping visual categorization with relevant negatives", "author": ["X. Li", "C.G. Snoek", "M. Worring", "D. Koelma", "A.W. Smeulders"], "venue": "IEEE Transactions on Multimedia, vol. 15, no. 4, pp. 933\u2013945, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating prior knowledge with weighted margin support vector machines", "author": ["X. Wu", "R. Srihari"], "venue": "Proc. 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining. ACM, 2004, pp. 326\u2013333.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 22, no. 2, pp. 179\u2013214, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Typically, an event is defined as an interaction among humans or between humans and physical objects [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2], [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2], [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "For instance, in MED 2014 [2], the defined complex events include Attempting a bike trick, Cleaning an appliance, or Beekeeping, to name a few.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "In [6], Brown studies how high-level events play a substantial role in the mechanism of structuring memories and recalling past experiences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "[7], [8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [8]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "The above training conditions are typically simulated by the 100Ex and 10Ex MED subtasks of TRECVID [3], where 100 and 10 positive video samples are available for training video event detectors, respectively.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Training based solely on a textual description of each event class is reported in a few works, mostly in the context of the TRECVID MED 0Ex and Semantic Query (SQ) subtasks [3], where no positive video examples are provided.", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "For instance, due to the rapidly increasing number of images on the Web, extensive research efforts have been devoted in multi-label, zero-example (or few-example) classification in images [9].", "startOffset": 189, "endOffset": 192}, {"referenceID": 9, "context": "Similarly, a method for zero-example classification of fMRI data was proposed in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "In [11], a method for predicting unseen image classes from a textual description, using knowledge transfer from textual to visual features, was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [12] this problem is addressed by transforming both the event\u2019s textual description and the visual content of un-classified videos in a high dimensional concept-based representation, using a large pool of concept detectors; then relevant videos are retrieved by computing the similarities between these representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Similarly, in [13], each event", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In [14], multiple low-level representations", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [15], logical operators are used to discover different types of composite concepts, which leads to better event detection performance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Moreover, in [16], a relevance feedback approach is used in order to improve event detection results in the zero-example problem using features computed from several modalities.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "In [17], E-Lamp was proposed, which is a zero-example event detection system made of four subsystems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "In [18], visual static (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "SIFT [22], Transformed Color Histogram [23]), and motion (e.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "SIFT [22], Transformed Color Histogram [23]), and motion (e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "ASR and OCR techniques are also used for exploiting audio and textual information in video streams, as well as audio features and visual features based in DCNNs trained in ImageNet [28].", "startOffset": 181, "endOffset": 185}, {"referenceID": 28, "context": "layers [29], providing high discriminative power [5].", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "layers [29], providing high discriminative power [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "In SESAME [19], the authors also use DCNN classification scores as video features,", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "This is simulated in the TRECVID MED task [3] by the \u201cnear-miss\u201d video examples provided for each target event class.", "startOffset": 42, "endOffset": 45}, {"referenceID": 30, "context": "Differently from our method, none of the above works takes full advantage of these related videos for learning from few positive samples; instead, the \u201crelated\u201d samples are either excluded from the training procedure, or they are treated as true positive or true negative instances [32].", "startOffset": 282, "endOffset": 286}, {"referenceID": 31, "context": "We assume that the only knowledge available, with respect to each event class, is a textual description of it, which consists of a title, a free-form text, and a list of possible visual and audio cues, as in [33], [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 15, "context": "We assume that the only knowledge available, with respect to each event class, is a textual description of it, which consists of a title, a free-form text, and a list of possible visual and audio cues, as in [33], [16].", "startOffset": 214, "endOffset": 218}, {"referenceID": 13, "context": "content of the videos that we want to examine, similarly to [14], [18], we a) use a pool of Nc concepts along with their titles and in some cases a limited number of subtitles (e.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "content of the videos that we want to examine, similarly to [14], [18], we a) use a pool of Nc concepts along with their titles and in some cases a limited number of subtitles (e.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "Subsequently, for each word in ELM and each word in each one of CLMs we calculate the Explicit Semantic Analysis (ESA) distance [34] between them.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "Tf-Idf) adopted for transforming this textual information in a Bag of Words (BoW) [35] representation.", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "[36], this resulting in six different types of CLMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "To this end, we use the ESA semantic relatedness measure [37], which calculates the similarity distance between two terms by computing the cosine similarity between their weighted vectors of Wikipedia articles.", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "[38], [39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38], [39].", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "In this section, Relevance Degree Support Vector Machine (RDSVM), proposed in [21] for handling \u201crelated\u201d training samples, is employed such that related samples are taken into", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "For the exploitation of the related observations, as proposed in [21], [40], each training sample xi is associated with a adjustable confidence value vi, and a monotonically increasing function g(vi) (called slack normalization function) is used to weight each slack variable \u03bei denoting the loss introduced by a misclassified sample.", "startOffset": 65, "endOffset": 69}, {"referenceID": 38, "context": "For the exploitation of the related observations, as proposed in [21], [40], each training sample xi is associated with a adjustable confidence value vi, and a monotonically increasing function g(vi) (called slack normalization function) is used to weight each slack variable \u03bei denoting the loss introduced by a misclassified sample.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "In [21], this function is modified so that only related class observations are associated with a confidence value ci \u2208 (0, 1] (called hereafter relevance degree) indicating the degree of relevance of the i-th observation with the class it is related.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In this study, we follow the approach proposed in [21] for handling the related samples as a subclass of the positive or negative class, for which a global relevance degree, c = ci \u2200i, is assigned to all related samples.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "More specifically, a 16-layer pre-trained deep ConvNet network provided in [29] was used.", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "the ImageNet data [28] and provides scores for 1000 concepts.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "We compare the proposed method with the E-Lamp framework, a state-of-the-art system that participated in the TRECVID 2014 MED task [18] and is described in detail in [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "We compare the proposed method with the E-Lamp framework, a state-of-the-art system that participated in the TRECVID 2014 MED task [18] and is described in detail in [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 31, "context": "Concerning the MS stage, we compared with the following retrieval methods: the Vector Space Model [33], the", "startOffset": 98, "endOffset": 102}, {"referenceID": 39, "context": "Okapi BM25, and two unigram language models with JelinekMercer smoothing (LM-JM) and Dirichlet smoothing (LMDL) respectively [41].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "the event detection creation methods of [17], the exact word seems to perform considerably better than the others (but much worse than the proposed method).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "It should be noted that the number of visual concepts used in [17] is significantly greater than the 1000 concepts used throughout our experiments, and other modalities (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "audio) are also exploited in [17]; this explains", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Event detector creation Similarity measures [17] Similarity measures (proposed) VSM BM25 LM-JM LM-DL Cosine Histogram Intersection WordNet - Wu 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "To this end, we compare the following approaches in order to investigate whether and in what way it is beneficial to use related samples in training: a) using no related samples, b) related samples are used as pure negative ones, as in [32], c) related samples are used as pure positive ones, again as in [32], and d) related samples are used as weighted negative or positive ones under the RD-SVM framework of Section IV-A, which involves an automatic procedure for selecting both the labels of the related samples and their weights.", "startOffset": 236, "endOffset": 240}, {"referenceID": 30, "context": "To this end, we compare the following approaches in order to investigate whether and in what way it is beneficial to use related samples in training: a) using no related samples, b) related samples are used as pure negative ones, as in [32], c) related samples are used as pure positive ones, again as in [32], and d) related samples are used as weighted negative or positive ones under the RD-SVM framework of Section IV-A, which involves an automatic procedure for selecting both the labels of the related samples and their weights.", "startOffset": 305, "endOffset": 309}, {"referenceID": 20, "context": "For the approaches that use related samples, 10 such samples were chosen from the pool of 25 related samples that are available for each event class; these, as suggested in [21], were selected as the 10 that are the nearest to the median of all 25 related samples in the employed feature space.", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "We observe that when related samples are used as pure negative or pure positive ones as in [32], the overall detection performance is lower than the baseline approach which does not use these samples at all (P10).", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "Automatically selecting to use related samples as weighted negatives or weighted positives using RD-SVM [21], on the other hand, is better than excluding them from the annotation process for 19 out of the 30 event classes.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The above confirm our hypothesis that treating related samples as weighted negatives/positives using RD-SVM [21] can lead to better event detection performance.", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "Experimental Scenario P10 (baseline) Related as negatives (as in [32]) Related as positives (as in [32]) R10 (proposed)", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Experimental Scenario P10 (baseline) Related as negatives (as in [32]) Related as positives (as in [32]) R10 (proposed)", "startOffset": 99, "endOffset": 103}], "year": 2015, "abstractText": "In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of \u201crelated\u201d videos. For learning only from an event\u2019s textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit \u201crelated\u201d event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.", "creator": "LaTeX with hyperref package"}}}