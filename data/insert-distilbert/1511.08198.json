{"id": "1511.08198", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Towards Universal Paraphrastic Sentence Embeddings", "abstract": "now in referencing this paper, we show how to create paraphrastic sentence embeddings using the paraphrase database ( ganitkevitch et al., 2013 ), an extensive semantic library resource with millions of phrase pairs. we consider several hierarchical compositional architectures and evaluate all them on 24 textual similarity datasets encompassing geographic domains such as news, tweets, web forums, news headlines, machine translation sentence output, glosses, translations and image and video captions. we present the interesting result that simple verbal compositional architectures efficiently based on successively updated vector domain averaging vastly outperform long short - variable term memory ( lstm ) yielding recurrent neural networks and that conversely these simpler architectures allow us to jointly learn models with superior generalization. our models are efficient, very easy to use, and competitive with task - tuned systems. we make them available to guide the research community with the hope that they can serve as the new baseline foundation for further work starting on universal paraphrastic sentence embeddings.", "histories": [["v1", "Wed, 25 Nov 2015 20:52:15 GMT  (28kb)", "http://arxiv.org/abs/1511.08198v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Tue, 12 Jan 2016 20:59:39 GMT  (32kb)", "http://arxiv.org/abs/1511.08198v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Fri, 4 Mar 2016 20:54:30 GMT  (40kb)", "http://arxiv.org/abs/1511.08198v3", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["john wieting", "mohit bansal", "kevin gimpel", "karen livescu"], "accepted": true, "id": "1511.08198"}, "pdf": {"name": "1511.08198.pdf", "metadata": {"source": "CRF", "title": "TOWARDS UNIVERSAL PARAPHRASTIC SENTENCE EMBEDDINGS", "authors": ["John Wieting", "Mohit Bansal Kevin Gimpel", "Karen Livescu"], "emails": ["jwieting@ttic.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n08 19\n8v 1\n[ cs\n.C L\n] 2\n5 N\nov 2\n01 5"}, {"heading": "1 INTRODUCTION", "text": "Word embeddings have become ubiquitous in natural language processing (NLP). Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015). There has also been recent work on creating representations for word sequences such as phrases or sentences. Many functional architectures have been proposed to model compositionality in language, ranging from those based on simple operations like addition (Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al., 2015). However, there is little work on learning representations that can be used across domains with the same ease and effectiveness as word embeddings. In this paper, we explore transferable compositional models that can encode arbitrary word sequences into a vector with the property that sequences with similar meaning have high cosine similarity, and that can, importantly, also transfer easily across domains. We consider several compositional architectures based on neural networks and train them on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013).\nWe focus our empirical comparison on two of these models: vector averaging and LSTMs. When learning the vector averaging model, there are no additional compositional parameters in the architecture \u2014 the only parameters being learned are the word vectors themselves, which are learned so as to produce effective embeddings for a word sequence when averaging is performed over the sequence. We also consider an optional transformation layer atop the output of vector averaging. Lastly, we consider LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).\nTo evaluate our models, we consider 2 tasks drawn from the same data distribution as the training data, as well as 22 SemEval textual similarity datasets from a variety of domains (such as news,\n1Trained models and code for training and evaluation are available at http://ttic.uchicago.edu/\u02dcwieting.\ntweets, web forums, and image and video captions). Interestingly, we find that the LSTM performs well on the in-domain task, but performs much worse on the out-of-domain tasks. We discover surprisingly strong performance for the models based on updated vector averaging, which perform well on both the in-domain and out-of-domain tasks, beating the best LSTM model by 12.6 points on average.\nOur new embeddings2 place in the top 25% of all submitted systems in every SemEval STS task from 2012 through 2015, having the best performance on 3 of the datasets.3 This is surprising because the submitted systems were designed for those particular tasks, with access to training and tuning data specifically developed for the various tasks. We simply train on noisy paraphrase pairs from the Paraphrase Database, using a small, manually-annotated portion of it for model selection. Moreover, we find that learning word embeddings in the context of vector averaging performs much better than simply averaging pretrained, state-of-the-art word embeddings. Our average Pearson\u2019s r over all 22 SemEval datasets is 16.7 points higher than averaging GloVe vectors4 and 12.4 points higher than averaging PARAGRAM-SL999 vectors.5\nWhile the above experiments focus on transfer, we also consider the fully supervised setting. We choose the 2014 SemEval SICK task, because it has been widely studied and has clearly defined training, development, and test sets. We again find strong performance for the vector averaging models, even rivaling the current state of the art. We find further gains by initializing and regularizing towards our PPDB-trained models. We also perform qualitative analysis of the word embeddings learned by our vector averaging model, showing important differences from the high-quality word embeddings used to initialize training. We make available our best trained model, and since it consists merely of a new set of word embeddings, it is extremely efficient and easy to use for downstream applications. Our hope is that they can provide a new simple and strong baseline for any-domain semantic textual similarity tasks."}, {"heading": "2 RELATED WORK", "text": "Researchers have developed many ways to embed word sequences for NLP. They mostly focus on the question of compositionality: given vectors for words, how should we create a vector for a word sequence? Mitchell & Lapata (2008; 2010) considered bigram compositionality, comparing many functions for composing two word vectors into a single vector to represent their bigram. Follow-up work by Blacoe & Lapata (2012) found again that simple operations such as vector addition performed strongly. Many other compositional architectures have been proposed, often using neural networks. These include neural bag-of-words (NBOW) models (Kalchbrenner et al., 2014), deep NBOW models (Iyyer et al., 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; I\u0307rsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on hierarchical structure but not parses (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). In this paper, we focus our comparison between methods based on updated vector addition and those based on LSTMs.6\nMost of the work mentioned above learns compositional models in the context of supervised learning. That is, a training set is provided with annotations and the composition function is learned for the purposes of optimizing an objective function based on those annotations. The models are then evaluated on a test set drawn from the same distribution as the training set.\nIn this paper, in contrast, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. There have been research efforts also targeting this goal.\n2Denoted as PARAGRAM-PHRASE-XXL and discussed in Section B.2. 3As measured by the average Pearson\u2019s r over all datasets in each task. 4We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common Crawl corpus, available at http://nlp.stanford.edu/projects/glove/. 5These are 300-dimensional vectors from Wieting et al. (2015) and are available at http://ttic.uchicago.edu/\u02dcwieting 6In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.\nOne approach is to train an autoencoder in an attempt to learn the latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al., 2015b). Other recently proposed methods, including paragraph vectors (Le & Mikolov, 2014) and skip-thought vectors (Kiros et al., 2015), learn sequence representations that are predictive of words inside the sequence or in neighboring sequences. These methods produce generic representations that can be used to provide features for text classification or sentence similarity tasks. While skip-thought vectors capture similarity in terms of discourse context, in this paper we are interested in capturing paraphrastic similarity, i.e., whether two sentences have the same meaning."}, {"heading": "3 MODELS AND TRAINING", "text": "Our goal is to embed sequences into a low-dimensional space such that cosine similarity in the space corresponds to the strength of the paraphrase relationship between the sequences. We experimented with three models with varying degrees of complexity. The simplest model embeds a word sequence x = \u3008x1, x2, ..., xn\u3009 by averaging the vectors of its tokens. The only parameters learned by this model are the word embedding matrix Ww :\ngPARAGRAM-PHRASE (x) = 1\nn\nn \u2211\ni\nW xiw\nwhere W xiw is the word embedding for word xi. We call the learned embeddings PARAGRAMPHRASE embeddings.\nIn our second model, we learn a projection in addition to the word embeddings:\ngproj(x) = Wp\n(\n1\nn\nn \u2211\ni\nW xiw\n)\n+ b\nwhere Wp is the projection matrix and b is a bias vector.\nOur third and final model is the most expressive. We use long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), a recurrent neural network (RNN) architecture designed to model sequences with long-distance dependencies. LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015). We use the version from Gers et al. (2003) which has the following equations:\nit = \u03c3 (WxiW xt w +Whiht\u22121 +Wcict\u22121 + bi) ft = \u03c3 (WxfW xt w +Whfht\u22121 +Wcfct\u22121 + bf ) ct = ftct\u22121 + it tanh (WxcW xt w +Whcht\u22121 + bc) ot = \u03c3 (WxoW xt w +Whoht\u22121 +Wcoct + bo)\nht = ot tanh(ct)\ngLSTM(x) = h\u22121\nwhere \u03c3 is the logistic sigmoid function and h\u22121 refers to the hidden vector of the last token. We found that the choice of whether or not to include the output gate had a significant impact on performance, so we used two versions of the LSTM model, one with the output gate and one without. For all models, we learn the word embeddings themselves, denoting the trainable word embedding parameters by Ww. We denote all other trainable parameters by Wc (\u201ccompositional parameters\u201d), though the PARAGRAM-PHRASE model has no compositional parameters. In practice, we initialize Ww using some embeddings pretrained from large corpora."}, {"heading": "3.1 TRAINING", "text": "We mostly follow the approach of Wieting et al. (2015). The training data consists of (possibly noisy) pairs taken directly from the original Paraphrase Database (PPDB) and we optimize a marginbased loss.\nOur training data consists of a set X of phrase pairs \u3008x1, x2\u3009, where x1 and x2 are assumed to be paraphrases. The objective function follows:\nmin Wc,Ww\n1\n|X |\n(\n\u2211\n\u3008x1,x2\u3009\u2208X\nmax(0, \u03b4 \u2212 cos(g(x1), g(x2)) + cos(g(x1), g(t1)))\n+max(0, \u03b4 \u2212 cos(g(x1), g(x2)) + cos(g(x2), g(t2)))\n)\n+\u03bbc \u2016Wc\u2016 2 + \u03bbw \u2016Wwinitial \u2212Ww\u2016 2 (1)\nwhere g is the embedding function in use (e.g., gLSTM), \u03b4 is the margin, \u03bbc and \u03bbw are regularization parameters, Wwinitial is the initial word embedding matrix, and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization. The intuition is that we want the two phrases to be more similar to each other (cos(g(x1), g(x2))) than either is to their respective negative examples t1 and t2, by a margin of at least \u03b4."}, {"heading": "3.1.1 SELECTING NEGATIVE EXAMPLES", "text": "To select t1 and t2 in Eq. 1, we tune the choice between two approaches. The first, MAX, simply chooses the most similar phrase in some set of phrases (other than those in the given phrase pair). For simplicity and to reduce the number of tunable parameters, we use the mini-batch, but this could be a separate set. E.g., for choosing t1 for a given \u3008x1, x2\u3009:\nt1 = argmax t:\u3008t,\u00b7\u3009\u2208Xb\\{\u3008x1,x2\u3009} cos(g(x1), g(t))\nwhere Xb \u2286 X is the current mini-batch. That is, we want to choose a negative example ti that is similar to xi according to the current model parameters. The downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi.\nThe second strategy selects negative examples using MAX with probability 0.5 and selects them randomly from the mini-batch otherwise. We call this sampling strategy MIX. We tune over the strategy in our experiments."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 DATA", "text": "We experiment on 24 textual similarity datasets, covering many domains, including all datasets from every SemEval semantic textual similarity (STS) task (2012-2015). We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).\nThe first STS task was held in 2012 and they have been held every year since. Given two sentences, the objective of the task is to predict how similar they are on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 different datasets and the tasks cover a wide variety of domains which we have categorized below. Most submissions for these tasks use supervised models that are trained and tuned on either provided training data or data on similar datasets from older tasks. Details on the number of teams and submissions for each task and the performance of the submitted systems for each dataset are included in Table 1 and Table 2 respectively. For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Diab, 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks.\nBelow are the textual domains contained in the STS tasks: News: Newswire text was used in the 2012 task (MSRpar) and the 2013 and 2014 tasks (deft news). Image and Video Descriptions: Image descriptions generated by Amazon Turkers were used in the 2013 and 2014 tasks (images). Video descriptions are used in the 2012 task (MSRvid). Glosses: Glosses from Wordnet, OntoNotes, and FrameNet were used in the 2012, 2013, and 2014 tasks (OnWN and FNWN).\nMT evaluation: The output of machine translation systems was used in the 2012 task (SMT-eur and SMT-news) and 2013 task (SMT). Headlines: Headlines of news articles were used in the 2013, 2014, and 2015 tasks. Web Forum: Forum posts were used in the 2014 task (deft forum). Twitter: Consists of pairs of a tweet and a sentence related to the same news. This dataset (tweet news) was used in the 2014 task. Belief: Text from the Deft Committed Belief Annotation (LDC2014E55) (belief) was used in the 2015 task. Questions and Answers: Paired answers to the same question from StackExchange (answersforums) and the BEETLE corpus (Dzikovska et al., 2010) (answers-students) were used in 2015."}, {"heading": "4.2 EXPERIMENTAL SETTINGS", "text": "We used the XL section of PPDB as training data.7 However, due to the long training time of the LSTM model, we used 500k examples sampled from PPDB XXL for tuning purposes, training for 5 epochs. Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models.\nThe reason we chose the task from Pavlick et al. for tuning is that we wanted our entire procedure to only make use of PPDB and use no other resources. So we considered annotated datasets that are exclusively composed of phrases in PPDB. We chose this dataset over Annotated-PPDB from Wieting et al. (2015) due to the larger amount of labeled data in the former (26,456 examples compared to 1,000 examples). However, in practice the datasets are very similar and selecting either to tune on produces similar results. Evidence for this can be seen in Table 3 where the oracle results on the task in Pavlick et al. (2015) and the results tuned on the test set of Annotated-PPDB are very similar.\nTo learn model parameters for all experiments in this section, we minimize Eq. 1 using AdaGrad Duchi et al. (2011) with mini-batches. We fix the initial learning rate to 0.05 for all experiments.\nOur models have the following tunable parameters:8 \u03bbc, the L2 regularizer on the compositional parameters Wc (only applicable to gproj and gLSTM), the pool of phrases used to obtain negative examples (coupled with mini-batch size b, to reduce the number of tunable parameters) , \u03bbw, the regularizer on the word embeddings, and \u03b4, the margin. We also tune on whether to use MIX or MAX sampling, and whether to include an output gate for the LSTM. We fix the output dimensionalities of the LSTM and projection models to the dimensionality of our word embeddings (300)."}, {"heading": "4.3 RESULTS", "text": "The results on all STS tasks as well as the SICK and Twitter tasks are shown in Table 2. We also include results on the PPDB tasks in Table 3. We report the performance of our four models:\n7PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The phrases are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases.\n8For \u03bbc we searched over {10\u22124, 10\u22125, 10\u22126}, for b we searched over {25, 50, 100}, for \u03bbw we searched over {10\u22126, 10\u22127, 10\u22128} as well as the setting in which we do not update Ww, and for \u03b4 we searched over {0.4, 0.6, 0.8}.\nPARAGRAM-PHRASE (PP), projection (proj.), LSTM with output gate (o.g.), and LSTM without output gate (no o.g.). We compare to several baselines: skip-thought vectors9 (Kiros et al., 2015), denoted \u201cST\u201d, averaged GloVe10 vectors (Pennington et al., 2014), and averaged PARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d. Note that the GloVe vectors were used to initialize the PARAGRAM-SL999 vectors which were, in turn, used to initialize our PARAGRAM-PHRASE embeddings. We compare to the skip-thought vectors because trained models are publicly available and they show impressive performance when used as features on several tasks including textual similarity.\nThe results in Table 2 show strong performance of our PARAGRAM-PHRASE embeddings as they outperform the other models on all but 5 of the 22 datasets in the table. The projection model has the next best performance, while the LSTM models lag behind. These results stand in marked contrast to those in Table 3, which shows nearly-identical performance of the four models on the in-domain PPDB tasks. For the LSTM models, it is interesting to note how removing the output gate results in stronger performance on the textual similarity tasks. This model outperforms the LSTM with an output gate on 17 of the 22 datasets, despite our tuning procedure favoring the latter LSTM with the output gate (Table 3). It also performs reasonably well compared to our strong PARAGRAM-SL999 addition baseline, beating it on half of the datasets."}, {"heading": "4.4 USING REPRESENTATIONS FOR DOWNSTREAM TASKS", "text": "We explore two natural questions regarding our representations learned from PPDB: (1) can these embeddings improve the performance of other models through initialization and regularization? (2) can they effectively be used as features for downstream textual similarity tasks? To address these questions, we evaluate on the SemEval 2014 SICK task. We choose this particular dataset as it is well-studied and has clearly-defined train/development/test splits. To learn the models in this\n9Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokenizer. We found that doing so significantly improves the performance of the skip-thought vectors.\n10We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common Crawl corpus, available at http://nlp.stanford.edu/projects/glove/.\nsection, we minimize the objective function11 from Tai et al. (2015). Given a score for a sentence pair in the range [1,K], where K is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute:\nh\u00d7 = hL \u2299 hR, h+ = |hL \u2212 hR|,\nhs = \u03c3 ( W (\u00d7)h\u00d7 +W (+)h+ + b (h) ) ,\np\u0302\u03b8 = softmax ( W (p)hs + b (p) ) ,\ny\u0302 = rT p\u0302\u03b8,\nwhere rT = [1 2 . . . K]. They then define a sparse target distribution p that satisfies y = rT p:\npi =\n\n\n\ny \u2212 \u230ay\u230b, i = \u230ay\u230b+ 1\n\u230ay\u230b \u2212 y + 1, i = \u230ay\u230b\n0 otherwise\nfor 1 \u2264 i \u2264 K . Then they use the following loss, the regularized KL-divergence between p and p\u0302\u03b8:\nJ(\u03b8) = 1\nm\nm \u2211\nk=1\nKL ( p(k) \u2225 \u2225\n\u2225 p\u0302 (k) \u03b8\n)\n,\nwhere m is the number of training pairs and where we always use L2 regularization on all compositional parameters12 but omit these terms for clarity.\nWe tuned hyper parameters on the development set13 and trained our models for 20 epochs. The results are shown in the first row of Table 4. We find strong performance for both the PARAGRAMPHRASE and projection models. They outperform both LSTM models, though we find that the LSTM without the output gate works better, echoing our results from above."}, {"heading": "4.4.1 REGULARIZATION AND INITIALIZATION TO IMPROVE TEXTUAL SIMILARITY MODELS", "text": "We initialize each respective model to the parameters learned from PPDB (calling them universal parameters) and augment Eq. 4.4 with three separate regularization terms with the following weights:\n11This objective function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error.\n12Word embeddings are regularized toward their initial state. 13All models tuned batch-size over {25, 50, 100}, output dimension over {50, 150, 300}, \u03bbc over {10\u22123, 10\u22124, 10\u22125, 10\u22126}, \u03bbs = \u03bbc, and \u03bbw over {10\u22123, 10\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128} as well as the option of not updating the embeddings for the projection and LSTM models. For the projection and LSTM models, the layer size was fixed at 300 (the same dimension as the input). Additionally, the LSTM was tuned on whether or not to include the output gate. Peephole connections were used in the LSTM model.\n\u03bbs which regularizes the classification parameters (the two layers used in the classification step after obtaining representations), \u03bbw for regularizing the word parameters toward the learned Ww from PPDB, and \u03bbc for regularizing the compositional parameters (the projection and LSTM parameters) back to their initial values.14 In all cases, we regularize to the universal parameters using L2 regularization. The results are shown in the second row of Table 4. They show that regularizing to our universal parameters significantly improves results for all models. In fact the performance of our projection model with regularization (0.8668) is close to state-of-the-art results using a tree-LSTM (0.8676) (Tai et al., 2015) or a convolutional neural network (0.8686) (He et al., 2015)."}, {"heading": "4.4.2 REPRESENTATIONS AS FEATURES", "text": "We use a similar set-up as in Kiros et al. (2015) where we encode the sentences with our models and then just learn the classification parameters without updating the embeddings or compositional model parameters.15 Our results (final row of Table 4) are not as strong on this task as the skipthought vectors, whose best model (that does not have additional features) has a Pearson\u2019s r of 0.8584 on the task. However, our representations are significantly smaller at 300 dimensions versus the 4800 dimensional skip-thought vectors, and we use a much simpler architecture to arrive at them."}, {"heading": "5 DISCUSSION", "text": "It is interesting that the LSTM, with or without output gates, is outperformed by much simpler models on the tasks studied in this paper. We now consider possible explanations for this trend.\nThe first hypothesis we test is based on length. Since PPDB contains short text snippets of a few words, the LSTM may not know how to handle the longer sentences that occur in our evaluation tasks. If this is true, the LSTM would perform much better on short text snippets and its performance would degrade as their length increases. To test this hypothesis, we took all 12,108 pairs from the 20 SemEval STS tasks and binned them by length.16 We then computed Pearson\u2019s r for each bin. The results are shown in Table 5 and show that while the LSTM models do perform better on the shortest text pairs, they are still outperformed, at all lengths, by the PARAGRAM-PHRASE and projection models.\nWe next consider whether the LSTM has worse generalization due to overfitting on the training data. To test this, we took each of our tuned models and analyzed how they performed on the training data (PPDB XL) by computing the average difference between the cosine similarity of the gold phrase pairs and the negative examples.17 We found that all models had very similar scores: 0.7535, 0.7572, 0.7565, and 0.7463 for PARAGRAM-PHRASE, projection, LSTM (o.g.), and LSTM (no o.g.). This, along with the similar performance of the models on the PPDB tasks in Table 3, suggests that overfitting is not the cause of the worse performance of the LSTM model.\nLastly, we consider if the LSTM\u2019s weak performance was a result of insufficient tuning. We first note that we actually ran twice as many hyperparameter tuning experiments for the LSTM models\n14We tuned \u03bbs over {10\u22123, 10\u22124, 10\u22125, 10\u22126}, \u03bbc over {10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126}, and \u03bbw over {10\u22123, 10\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128}. All other hyperparameters were tuned as previously.\n15We tuned \u03bbs over {10\u22123, 10\u22124, 10\u22125, 10\u22126}. 16For each pair, we computed the number of tokens in each of the two pieces of text, took the max, and then binned based on this value. 17More precisely, for each gold pair \u3008g1, g2\u3009, and ni, the respective negative example of each gi, we computed 2 \u00b7 cos(g1, g2)\u2212 cos(n1, g1)\u2212 cos(n2, g2) and averaged this value over all pairs.\nthan either the PARAGRAM-PHRASE or projection models, since we tuned the decision to use an output gate. We also mention Tai et al. (2015) which had a similar LSTM result on the SICK dataset (Pearson\u2019s r of 85.28 to our 85.63) to show that our LSTM implementation/tuning procedure is able to match or exceed performance of another published LSTM result. Finally, the similar performance across models on our PPDB development task (Table 3) suggests that no model had a large advantage during tuning; all found hyperparameters that comfortably beat the PARAGRAM-SL999 addition baseline."}, {"heading": "6 QUALITATIVE ANALYSIS", "text": "When we take a closer look at our PARAGRAM-PHRASE embeddings, we find that information-rich words, such as poverty, kidding, humanitarian, 18, and july have the largest L2 norms, while less semantically-meaningful words such as of, it, to, hereby and the have the smallest. Interestingly, this explains much of the success of the model. In order to quantify exactly how much, we calculated a weight for each token in our working vocabulary18 simply by summing up the absolute value of all components of its PARAGRAM-PHRASE vector. We then computed the average Pearson\u2019s r over all 22 datasets in Table 2. The PARAGRAM-SL999 vectors had an average correlation of 0.5494, the PARAGRAM-PHRASE vectors had 0.6683, and PARAGRAM-SL999 vectors, where each is multiplied by its computed weight, had an average Pearson\u2019s r of 0.6264. Therefore, it can be surmised that at least 64.76% of the improvement over the initial PARAGRAM-SL999 vectors is due to weighting tokens by their importance.19\nWe also investigated into how trivial this weighting would be to reproduce and its novelty. To do so, we calculated the frequency of all tokens in PPDB XL.20 We then normalized these by the total number of tokens in PPDB XL. Lastly, we took the reciprocal of these scores to be the token weights. Thus more frequent words have more weight than less frequent words. With this baseline weighting method, the average Pearson\u2019s r is 0.4552 - indicating that the weights we obtain for these words is more sophisticated than just frequency counts. These weights are, in fact, novel and potentially useful for other applications where computing the importance of a word is an integral part such as information retrieval and search.\nWe also show some nearest neighbors of our PARAGRAM-PHRASE vectors in comparison to the PARAGRAM-SL999 vectors that initialized them. To obtain the nearest neighbors, we used the 10,000 most common tokens in PPDB XL as our vocabulary to ensure that the PARAGRAM-PHRASE vectors were not too under-trained. The neighbors are shown in Table 1. In the first four rows, we see the PARAGRAM-PHRASE embeddings have neighbors that have a strong paraphrasing relationship. The embeddings tend to avoid having neighbors that are antonyms or co-hyponyms such as unlike and alike or 2 and 3 which are an issue for the PARAGRAM-SL999 embeddings. In contrast to these first four rows, the last row shows an interesting problem with our approach, and a limitation to our bag-of-words style modeling as agree is the nearest neighbor to disagree. The reason for this exception is that there are numerous pairs in PPDB XL such as i disagree and i do not agree that encourage disagree and agree to have high cosine similarity. A model that takes into account context would solve this issue. The difficulty would be finding one that generalizes well, as we found\n18This corresponds to the 42,091 tokens that appear in the test sets of all tasks in our evaluation as well as all tokens in PPDB XL plus an unknown word token\n19Interestingly, we trained a model where we only a learned a single weight for each word in our vocabulary and kept the word embeddings fixed. We then trained for 10 epochs on all the phrase pairs in PPDB XL. The resulting average Pearson\u2019s r, after tuning on the PPDB task in Pavlick et al. (2015), was 0.6206, lower than by using the absolute value of the PARAGRAM-PHRASE vectors as the weight.\n20those that did not appear in PPDB XL were assigned a frequency of 1\nthat our PARAGRAM-PHRASE embeddings generalize better than learning a weight matrix or using a recurrent neural network. We leave this for future work."}, {"heading": "7 CONCLUSION", "text": "We have introduced an approach to create universal paraphrase embeddings that outperform various strong baselines on many text similarity tasks and many domains. We are not aware of any other paraphrase models that performs nearly as well out-of-the-box as our PARAGRAM-PHRASE embeddings. Moreover, our representations are very easy to use. They do not require the use of any neural network architecture, instead the embeddings can be downloaded and then summed for a given phrase/sentence inside of an application to create the phrase/sentence embedding. Moreover, we also find that our representations can improve general text similarity models as we show significant improvement on word-averaging, projection, and LSTM models by initializing and regularizing toward our representations. Future work will focus on improving our embeddings by effectively handling undertrained words as well as by discovering new models that generalize even better to the large suite of text similarity tasks we have used in our experiments."}, {"heading": "APPENDIX A UNDER-TRAINED EMBEDDINGS", "text": "One limitation of our new PARAGRAM-PHRASE vectors is that many of our embeddings are undertrained. The number of unique tokens occurring in our training data, PPDB XL, is 37,366. However, the number of tokens appearing more than 100 times is just 7,113. Thus, one clear source of improvement for our model would be to address under-trained embeddings for tokens appearing in our test data.\nIn order to gauge what effect these embeddings have on our model, we calculate the fraction of words in each of our 22 SemEval datasets that do not occur at least 100 times in PPDB XL along with our performance deviation from the 75th percentile of each dataset. The results are shown in Table 6. It is interesting that even though that the quality of submissions has improved though the years (this can be seen by looking at the performance statistics for similar datasets in Table 2), that there is a definite trend. Datasets where the OOV fraction is under 0.16, usually perform better than the 75th percentile, otherwise they usually perform worse than the 75th percentile."}, {"heading": "APPENDIX B ADDING MORE PPDB", "text": "B.1 PERFORMANCE VERSUS AMOUNT OF TRAINING DATA\nModels in related work such as Kiros et al. (2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks. Moreover, from these papers, the dependency between the amount of training data or training time versus performance of the model is unclear.\nIn order to illustrate this dependency for our PARAGRAM-PHRASE model, we trained on different amounts of data and plotted the performance. The results are shown in Figure 2. We start with PPDB XL which has 3,033,753 unique phrase pairs. and then divide by powers of two until there are less than 10 phrase pairs.21 For all data points (each division by two), we trained a model with that amount of phrase pairs for 10 epochs. We use the average Pearson correlation for all 22 datasets in Table 2 in our plot.\nWe experimented with two different sets of training data. The first is keeping the order of the phrase pairs in PPDB, which ensures the smaller datasets contain higher quality phrase pairs. In the second, we permute PPDB XL before constructing our smaller datasets. In both ways, each larger dataset contains the previous one plus as many new phase pairs.\nWe fixed the parameters for all models to those found in Section 4.2 and tuned on the PPDB task from Pavlick et al. (2015) to select the best parameters during training.\nThere are three interesting things to note about the plot. The first is that performance continually increases as more training data is added. This is encouraging as our embeddings can continually\n21The smallest dataset contained 5 pairs.\nimprove with more data. Secondly, it is interesting that there is such a large improvement (4 points) over the baseline, adding PARAGRAM-SL999 embeddings, by just training on even 92 phrase pairs\nfrom PPDB. Lastly, it is interesting to compare randomly permuting the training data versus using the order from PPDB which reflects the confidence that these phrase pairs have a paraphrasing relationship. From the plot, performance of the randomly permuted data surpasses is generally better than that of the ordered pairs, until the performance gap vanishes when half of PPDB XL is used. We suspect this behavior is due to the safe phrase pairs that occur in the beginning of the PPDB. These usually have only slight differences and therefore are not as useful to the model. Since the safe pairs are mostly at the beginning of PPDB, the random data has less of them in comparison to the ordered data and in turn, has more of the noisier, but potentially richer, pairs that occur in the latter parts of PPDB XL.\nLastly, it is worth noting that our models are fast to train. Timing results are shown in Table 7 where we timed each model on 100k random phrase pairs from PPDB XXL. The hyper-parameters used in these experiments were the ones selected from the tuning process described in Section 4.2, with the exception of batch-size which was fixed to 100 and the negative sampling method which was just random pairing. We timed these experiments using a 3.07 GHz Intel i7 processor for the CPU experiment and an Nvidia GeForce GTX TITAN X for the GPU experiments.\nB.2 PARAGRAM-PHRASE XXL\nSince we found that overall, the PARAGRAM-PHRASE embeddings have the best performance, we trained this model on more data from PPDB. We also used more data for hyper-parameter tuning by randomly selecting 2,000,000 phrase pairs from PPDB XXL, training again for 5 epochs. Then we trained our final model, for 10 epochs, on the entire phrase section of PPDB XXL, consisting of 9,123,575 unique phrase pairs. We show the results of this improved model, PARAGRAM-PHRASEXXL in Table 8 along with the median, 75th percentile, and maximum score from our suite of textual similarity tasks. PARAGRAM-PHRASE-XXL has the best performance on 3 of the datasets (SMT-news, deft forum, and belief), and is within 3 points of the best performance in 8 of the 22 SemEval textual similarity datasets."}], "references": [{"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Agirre", "Eneko", "Diab", "Mona", "Cer", "Daniel", "Gonzalez-Agirre", "Aitor"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Agirre", "Eneko", "Banea", "Carmen", "Cardie", "Claire", "Cer", "Daniel", "Diab", "Mona", "Gonzalez-Agirre", "Aitor", "Guo", "Weiwei", "Mihalcea", "Rada", "Rigau", "German", "Wiebe", "Janyce"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability", "author": ["Agirre", "Eneko", "Banea", "Carmen"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Arabic diacritization with recurrent neural networks", "author": ["Belinkov", "Yonatan", "Glass", "James"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Belinkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2015}, {"title": "Natural language processing with Python. ", "author": ["Bird", "Steven", "Klein", "Ewan", "Loper", "Edward"], "venue": "O\u2019Reilly Media,", "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "William", "Lapata", "Mirella"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen", "Xinchi", "Qiu", "Xipeng", "Zhu", "Chenxi", "Liu", "Pengfei", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Sentence modeling with gated recursive neural network", "author": ["Chen", "Xinchi", "Qiu", "Xipeng", "Zhu", "Chenxi", "Wu", "Shiyu", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Beetle ii: a system for tutoring and computational linguistics experimentation", "author": ["Dzikovska", "Myroslava O", "Moore", "Johanna D", "Steinhauser", "Natalie", "Campbell", "Gwendolyn", "Farrow", "Elaine", "Callaway", "Charles B"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Dzikovska et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dzikovska et al\\.", "year": 2010}, {"title": "Sentence compression by deletion with lstms", "author": ["Filippova", "Katja", "Alfonseca", "Enrique", "Colmenares", "Carlos A", "Kaiser", "Lukasz", "Vinyals", "Oriol"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He", "Hua", "Gimpel", "Kevin", "Lin", "Jimmy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu", "Baotian", "Lu", "Zhengdong", "Li", "Hang", "Chen", "Qingcai"], "venue": "In Advances in Neural Information Processing Systems, pp. 2042\u20132050,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["\u0130rsoy", "Ozan", "Cardie", "Claire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Li", "Jiwei", "Luong", "Minh-Thang", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1506.01057,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["Li", "Jiwei", "Luong", "Thang", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel", "Fermandez", "Ramon", "Amir", "Silvio", "Marujo", "Luis", "Tiago"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Liu", "Pengfei", "Qiu", "Xipeng", "Chen", "Xinchi", "Wu", "Shiyu", "Huang", "Xuanjing"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny", "Bethard", "Steven J", "McClosky", "David"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marelli", "Marco", "Bentivogli", "Luisa", "Baroni", "Bernardi", "Raffaella", "Menini", "Stefano", "Zamparelli", "Roberto"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Pavlick", "Ellie", "Rastogi", "Pushpendre", "Ganitkevich", "Juri", "Durme", "Benjamin Van", "Callison-Burch", "Chris"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennin", "Jeffrey", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew", "Potts", "Christopher"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["Turian", "Joseph", "Et", "Dpartement Dinformatique", "(diro", "Recherche Oprationnelle", "Montral", "Universit De", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Di", "Nyberg", "Eric"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen", "Tsung-Hsien", "Gasic", "Milica", "Mrk\u0161i\u0107", "Nikola", "Su", "Pei-Hao", "Vandyke", "David", "Young", "Steve"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen", "Roth", "Dan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)", "author": ["Xu", "Wei", "Callison-Burch", "Chris", "Dolan", "William B"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu", "Yan", "Mou", "Lili", "Li", "Ge", "Chen", "Yunchuan", "Peng", "Hao", "Jin", "Zhi"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Wenpeng", "Sch\u00fctze", "Hinrich"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Yu", "Mo", "Dredze", "Mark"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Zhao", "Han", "Lu", "Zhengdong", "Poupart", "Pascal"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 31, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 35, "context": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al.", "startOffset": 88, "endOffset": 159}, {"referenceID": 41, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 36, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 3, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 40, "context": ", 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015).", "startOffset": 72, "endOffset": 164}, {"referenceID": 36, "context": ", 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 22, "context": ", 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 40, "context": ", 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al., 2015).", "startOffset": 75, "endOffset": 93}, {"referenceID": 15, "context": "Lastly, we consider LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 16, "context": "Lastly, we consider LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 39, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 42, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 18, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 27, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 44, "context": ", 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).", "startOffset": 24, "endOffset": 147}, {"referenceID": 22, "context": "These include neural bag-of-words (NBOW) models (Kalchbrenner et al., 2014), deep NBOW models (Iyyer et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 36, "context": ", 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; \u0130rsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on hierarchical structure but not parses (Zhao et al.", "startOffset": 108, "endOffset": 185}, {"referenceID": 45, "context": ", 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; \u0130rsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on hierarchical structure but not parses (Zhao et al.", "startOffset": 108, "endOffset": 185}, {"referenceID": 51, "context": ", 2015), recursive networks based on hierarchical structure but not parses (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al.", "startOffset": 75, "endOffset": 114}, {"referenceID": 22, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 20, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 17, "context": ", 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 40, "endOffset": 133}, {"referenceID": 40, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 27, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 28, "context": ", 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 45, "context": "edu/ \u0303wieting In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.", "startOffset": 108, "endOffset": 130}, {"referenceID": 7, "context": ", 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch\u00fctze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). In this paper, we focus our comparison between methods based on updated vector addition and those based on LSTMs.6 Most of the work mentioned above learns compositional models in the context of supervised learning. That is, a training set is provided with annotations and the composition function is learned for the purposes of optimizing an objective function based on those annotations. The models are then evaluated on a test set drawn from the same distribution as the training set. In this paper, in contrast, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. There have been research efforts also targeting this goal. Denoted as PARAGRAM-PHRASE-XXL and discussed in Section B.2. As measured by the average Pearson\u2019s r over all datasets in each task. We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common Crawl corpus, available at http://nlp.stanford.edu/projects/glove/. These are 300-dimensional vectors from Wieting et al. (2015) and are available at http://ttic.", "startOffset": 8, "endOffset": 1317}, {"referenceID": 36, "context": "One approach is to train an autoencoder in an attempt to learn the latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 12, "context": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015).", "startOffset": 109, "endOffset": 216}, {"referenceID": 7, "context": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015). We use the version from Gers et al. (2003) which has the following equations:", "startOffset": 110, "endOffset": 261}, {"referenceID": 45, "context": "We mostly follow the approach of Wieting et al. (2015). The training data consists of (possibly noisy) pairs taken directly from the original Paraphrase Database (PPDB) and we optimize a marginbased loss.", "startOffset": 33, "endOffset": 55}, {"referenceID": 30, "context": ", 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data (Wieting et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 45, "context": ", 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).", "startOffset": 49, "endOffset": 93}, {"referenceID": 34, "context": ", 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).", "startOffset": 49, "endOffset": 93}, {"referenceID": 0, "context": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Diab, 2013), 2014 (Agirre et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 1, "context": ", 2012), 2013 (Diab, 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": ", 2014), and 2015 (Agirre et al., 2015) tasks.", "startOffset": 18, "endOffset": 39}, {"referenceID": 11, "context": "Questions and Answers: Paired answers to the same question from StackExchange (answersforums) and the BEETLE corpus (Dzikovska et al., 2010) (answers-students) were used in 2015.", "startOffset": 116, "endOffset": 140}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs.", "startOffset": 90, "endOffset": 112}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. The reason we chose the task from Pavlick et al. for tuning is that we wanted our entire procedure to only make use of PPDB and use no other resources. So we considered annotated datasets that are exclusively composed of phrases in PPDB. We chose this dataset over Annotated-PPDB from Wieting et al. (2015) due to the larger amount of labeled data in the former (26,456 examples compared to 1,000 examples).", "startOffset": 90, "endOffset": 580}, {"referenceID": 33, "context": "Then after finding the hyper-parameters which maximize Spearman\u2019s \u03c1 on the PPDB task from Pavlick et al. (2015), we trained models on the entire XL section of PPDB for 10 epochs. We use PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. The reason we chose the task from Pavlick et al. for tuning is that we wanted our entire procedure to only make use of PPDB and use no other resources. So we considered annotated datasets that are exclusively composed of phrases in PPDB. We chose this dataset over Annotated-PPDB from Wieting et al. (2015) due to the larger amount of labeled data in the former (26,456 examples compared to 1,000 examples). However, in practice the datasets are very similar and selecting either to tune on produces similar results. Evidence for this can be seen in Table 3 where the oracle results on the task in Pavlick et al. (2015) and the results tuned on the test set of Annotated-PPDB are very similar.", "startOffset": 90, "endOffset": 893}, {"referenceID": 10, "context": "1 using AdaGrad Duchi et al. (2011) with mini-batches.", "startOffset": 16, "endOffset": 36}, {"referenceID": 35, "context": ", 2015), denoted \u201cST\u201d, averaged GloVe10 vectors (Pennington et al., 2014), and averaged PARAGRAM-SL999 vectors (Wieting et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 45, "context": ", 2014), and averaged PARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d.", "startOffset": 45, "endOffset": 67}, {"referenceID": 29, "context": "Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 5, "context": ", 2014) rather than the included NLTK (Bird et al., 2009) tokenizer.", "startOffset": 38, "endOffset": 57}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al. (2015) (test) Wieting et al.", "startOffset": 6, "endOffset": 59}, {"referenceID": 34, "context": "Model Pavlick et al. (2015) (oracle) Pavlick et al. (2015) (test) Wieting et al. (2015) PARAGRAM-PHRASE 0.", "startOffset": 6, "endOffset": 88}, {"referenceID": 34, "context": "For the task in Pavlick et al. (2015), we include the oracle result, since this dataset was used for model selection for all tasks, as well as test results where models were tuned on Annotated-PPDB.", "startOffset": 16, "endOffset": 38}, {"referenceID": 40, "context": "section, we minimize the objective function11 from Tai et al. (2015). Given a score for a sentence pair in the range [1,K], where K is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute: h\u00d7 = hL \u2299 hR, h+ = |hL \u2212 hR|, hs = \u03c3 (", "startOffset": 51, "endOffset": 69}, {"referenceID": 40, "context": "8676) (Tai et al., 2015) or a convolutional neural network (0.", "startOffset": 6, "endOffset": 24}, {"referenceID": 17, "context": "8686) (He et al., 2015).", "startOffset": 6, "endOffset": 23}, {"referenceID": 40, "context": "We also mention Tai et al. (2015) which had a similar LSTM result on the SICK dataset (Pearson\u2019s r of 85.", "startOffset": 16, "endOffset": 34}, {"referenceID": 34, "context": "The resulting average Pearson\u2019s r, after tuning on the PPDB task in Pavlick et al. (2015), was 0.", "startOffset": 68, "endOffset": 90}, {"referenceID": 25, "context": "(2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks.", "startOffset": 11, "endOffset": 29}, {"referenceID": 25, "context": "(2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks. Moreover, from these papers, the dependency between the amount of training data or training time versus performance of the model is unclear. In order to illustrate this dependency for our PARAGRAM-PHRASE model, we trained on different amounts of data and plotted the performance. The results are shown in Figure 2. We start with PPDB XL which has 3,033,753 unique phrase pairs. and then divide by powers of two until there are less than 10 phrase pairs.21 For all data points (each division by two), we trained a model with that amount of phrase pairs for 10 epochs. We use the average Pearson correlation for all 22 datasets in Table 2 in our plot. We experimented with two different sets of training data. The first is keeping the order of the phrase pairs in PPDB, which ensures the smaller datasets contain higher quality phrase pairs. In the second, we permute PPDB XL before constructing our smaller datasets. In both ways, each larger dataset contains the previous one plus as many new phase pairs. We fixed the parameters for all models to those found in Section 4.2 and tuned on the PPDB task from Pavlick et al. (2015) to select the best parameters during training.", "startOffset": 11, "endOffset": 1233}], "year": 2015, "abstractText": "In this paper, we show how to create paraphrastic sentence embeddings using the Paraphrase Database (Ganitkevitch et al., 2013), an extensive semantic resource with millions of phrase pairs. We consider several compositional architectures and evaluate them on 24 textual similarity datasets encompassing domains such as news, tweets, web forums, news headlines, machine translation output, glosses, and image and video captions. We present the interesting result that simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks and that these simpler architectures allow us to learn models with superior generalization. Our models are efficient, very easy to use, and competitive with task-tuned systems. We make them available to the research community1 with the hope that they can serve as the new baseline for further work on universal paraphrastic sentence embeddings.", "creator": "LaTeX with hyperref package"}}}