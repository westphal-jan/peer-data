{"id": "1511.08308", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "abstract": "named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve dramatically high performance. in this paper, we present a novel neural network architecture that simply automatically detects word - handling and character - frame level features using a similar hybrid bidirectional lstm and cnn architecture, eliminating the need for most feature engineering. we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing exact match approaches. extensive evaluation shows that, normally given only tokenized text, publicly available word vectors, and an automatically constructed lexicon from open sources, our system is able to surpass the reported state - knowledge of - the - art on the ontonotes 5. 0 dataset by issuing 2. 35 f1 points and achieves competitive results on the conll. 2003 dataset, rivaling systems that employ heavy feature engineering, proprietary lexicons, and sometimes rich entity linking information.", "histories": [["v1", "Thu, 26 Nov 2015 07:40:33 GMT  (72kb,D)", "http://arxiv.org/abs/1511.08308v1", null], ["v2", "Fri, 25 Mar 2016 09:23:52 GMT  (93kb,D)", "http://arxiv.org/abs/1511.08308v2", null], ["v3", "Tue, 29 Mar 2016 06:25:57 GMT  (93kb,D)", "http://arxiv.org/abs/1511.08308v3", null], ["v4", "Thu, 16 Jun 2016 06:15:49 GMT  (94kb,D)", "http://arxiv.org/abs/1511.08308v4", "To appear in Transactions of the Association for Computational Linguistics"], ["v5", "Tue, 19 Jul 2016 05:02:51 GMT  (94kb,D)", "http://arxiv.org/abs/1511.08308v5", "To appear in Transactions of the Association for Computational Linguistics"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["jason p c chiu", "eric nichols"], "accepted": true, "id": "1511.08308"}, "pdf": {"name": "1511.08308.pdf", "metadata": {"source": "CRF", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "authors": ["Jason P.C. Chiu", "Eric Nichols"], "emails": ["jsonchiu@gmail.com", "e.nichols@jp.honda-ri.com"], "sections": [{"heading": "1 Introduction", "text": "Named entity recognition is an important task in NLP. High performance approaches have been dominated by applying statistical models such as CRF, SVM, or perceptron models to hand-crafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). However, Collobert et al. (2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text \u2013 an approach made possible by recent advancements in unsupervised learning of word embeddings on mas-\nsive amount of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al., 1986).\nUnfortunately there are many limitations to the model proposed by Collobert et al. (2011b). First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word \u2013 an approach that discards useful long-distance relations between words. Second, by depending solely on word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare words where word embeddings are poorly trained. We seek to address these issues by proposing a more powerful neural network model.\nA well-studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network (RNN) (Goller and Kuchler, 1996). Recently, RNNs have shown great success in diverse NLP tasks such as speech recognition (Graves et al., 2013), machine translation (Cho et al., 2014), and language modeling (Mikolov et al., 2011). The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned (Gers et al., 2000). For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model (Graves et al., 2013). While LSTMs have been studied in the past for the NER task by Hammerton (2003), the lack of computational power (which led to the use of very small models) and quality word embeddings ar X iv :1 51 1.\n08 30\n8v 1\n[ cs\n.C L\n] 2\n6 N\nov 2\n01 5\nlimited their effectiveness. Similarly, convolutional neural networks (CNN) have become very popular in image processing for feature extraction. Recently, dos Santos (2015) and Labeau (2015) successfully employed CNNs to extract character-level features for use in NER and POS-tagging respectively. Collobert et al. (2011b) also applied CNNs to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures (Blunsom et al., 2014). However, the effectiveness of character-level CNNs has not been evaluated for English NER.\nOur main contribution lies in combining these neural network models for the NER task. We present a hybrid model of bidirectional LSTMs and CNNs that learns both character- and word-level features, presenting the first evaluation of such an architecture on well-established English language evaluation datasets. Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the exact match only approach of Collobert et al. (2011b). Extensive evaluation shows that our proposed method performs competitively on the CoNLL-2003 NER shared task and outperforms all known state-of the-art systems on the OntoNotes 5.0 dataset where significantly more training data is available, albeit with a more complex tagset."}, {"heading": "2 Model", "text": "Our neural network is inspired by the work of Collobert et al. (2011b), where feature vectors are computed by lookup tables and concatenated together, and then fed into a multi-layer network. Instead of a feed-forward network, we use the more powerful recurrent neural network; in particular, we choose the bidirectional long-short term memory (LSTM) network, which has been used successfully in other sequence labelling tasks such as speech recognition (Graves et al., 2013). In addition, to induce character-level features, we draw on the idea of using a convolutional neural network, which has been successfully applied to Spanish and Portuguese NER (dos Santos et al., 2015) and German POStagging (Labeau et al., 2015)."}, {"heading": "2.1 Word-level Features", "text": ""}, {"heading": "2.1.1 Word Embeddings", "text": "Our best model uses the publicly available 50- dimensional word embeddings released by Collobert et al. (2011b)1, which were trained on Wikipedia and Reuters RCV-1 corpus.\nWe also experimented with two other sets of published embeddings, namely Stanford\u2019s GloVe embeddings2 trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014) and Google\u2019s word2vec embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013).\nIn addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation4 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 dataset as well.5\nFollowing Collobert et al. (2011b), all words are lower-cased before passing through the lookup table to convert to their corresponding embeddings. The pre-trained embeddings are allowed to be modified during training6."}, {"heading": "2.1.2 Capitalization Feature", "text": "As capitalization information is erased during lookup of the word embedding, we evaluate Collobert\u2019s method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo (Collobert et al., 2011b). This method is compared with the character type features we introduce in Section 2.2.2.\n1Part of SENNA: http://ml.nec-labs.com/ senna/\n2Available at http://nlp.stanford.edu/ projects/glove/\n3Available at https://code.google.com/p/ word2vec/\n4We used our in-house reimplementation to train word vectors because it uses distributed processing to train much quicker than the publicly-released implementation of word2vec and its performance on the word analogy task was higher than reported by Mikolov et al. (2013).\n5While Collobert et al. (2011b) used Wikipedia text from 2007, we used Wikipedia text from 2011.\n6Preliminary experiments showed that modifiable vectors performed better than so-called \u201cfrozen vectors.\u201d"}, {"heading": "2.1.3 Lexicons", "text": "Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).\nFor each of the four categories (Person, Organization, Location, Miscellaneous) defined by the CoNLL 2003 NER shared task, we compiled a list of known named entities from DBpedia (Auer et al., 2007), by extracting all descendants of DBpedia types corresponding to the CoNLL categories7. We did not construct separate lexicons for the OntoNotes tagset because correspondences between DBpedia categories and its tags could not be found in many instances. In addition, for each entry we first removed parentheses and all text contained within, then stripped trailing punctuation8, and finally tokenized it with the Penn Treebank tokenization script for the purpose of partial matching. Table 1 shows the size of each category in our lexicon.\nFigure 1 shows an example of how the lexicon features are applied9. For each lexicon category, we match every n-gram (up to the length of the longest lexicon entry) against entries in the lexicon. A match is successful when the n-gram matches the prefix or suffix of an entry and is at least half the length of the entry. Because of the high potential for spurious matches, for all categories except Person, we discard partial matches less than 2 tokens in length. When there are multiple overlap-\n7The Miscellaneous category was populated by entities of the DBpedia categories Artifact and Work.\n8The punctuation stripped was period, comma, semi-colon, colon, forward slash, backward slash, and question mark.\n9As can been seen in this example, the lexicons \u2013 in particular Miscellaneous \u2013 still contain a lot of noise. Better lexicon construction remains an area of future work.\nping matches within the same category, we prefer exact matches over partial matches, and then longer matches over shorter matches, and finally earlier matches over later matches. All matches are case insensitive.\nFor each token in the match, the feature is encoded in BIOES annotation (Begin, Inside, Outside, End, Single), indicating the position of the token in the matched entry. In other words, the B label will not appear in a suffix-only partial match, and the E label will not appear in a prefix-only partial match.\nAs shown in Table 10, we found that this more sophisticated method outperforms the method presented by Collobert et al. (2011b), which simply uses exact matching and marks tokens with on/off instead of BIOES annotation."}, {"heading": "2.2 Character-level Features", "text": "Figure 2 shows the CNN that extracts a fixed-length feature vector from the characters of a single word. For each character in a word, features are encoded using lookup tables and concatenated, then passed through convolution and max layers. The resulting vector is concatenated with other word-level feature vectors described above."}, {"heading": "2.2.1 Character Embeddings", "text": "We randomly initialized a lookup table with values drawn from an uniform distribution with range [\u22120.5, 0.5] to output a character embedding of 25 dimensions. The character set includes all unique characters in the CoNLL-2003 dataset (which includes upper and lower case letters, numbers, and punctuations) plus the special tokens PADDING and UNKNOWN. The PADDING token is used for the CNN, and the UNKNOWN token is used all other characters (which appear in OntoNotes). The same set of random embeddings were used for all experiments. We chose these settings for convenience and did not experiment with other settings."}, {"heading": "2.2.2 Character Type Feature", "text": "A lookup table was used to output a 4- dimensional vector representing the type of the character (upper case, lower case, punctuation, other).\nPadding P o Padding Char\nEmbedding\nType\ni c a s s\nConvolution\nMax over time\nChar features\nFigure 2: The convolutional neural network extracts character features from each word. The character embedding and the character type feature vector are computed through lookup tables. Then, they are concatenated and passed into the CNN."}, {"heading": "2.2.3 Extracting Features Using a Convolutional Neural Network", "text": "For each word we employ a convolution and a max layer to extract a new feature vector from the per-character feature vectors described above. Words are padded with a number of special PADDING characters on both sides depending on the window size of the CNN.\nHyper-parameters of the CNN include the window size and the output vector size."}, {"heading": "2.3 Sequence-labelling with Bidirectional LSTM", "text": "Following the speech-recognition framework outlined by Graves et al. (2013), we employed a stacked10 bi-directional recurrent neural network\n10For each direction (forward and backward), the input is fed into multiple layers of LSTM units connected in sequence\nwith long short-term memory (LSTM) units to transform the extracted word features into a distribution of named entity tag scores. This network configuration will be referred to as BLSTM throughout the remainder of the paper. Figure 3 and Figure 4 illustrates the network in detail.\nThe extracted features of each word is fed into a forward LSTM network and a backward LSTM network. The output of each recurrent network at each time step is fed through a linear layer and a logsoftmax layer to decode into log-probabilities for each tag category. These two vectors are then simply added together to produce the final output.\nWe tried minor variants of output layer architecture and selected the one that performed the best in preliminary experiments."}, {"heading": "2.4 Training and Inference", "text": ""}, {"heading": "2.4.1 Implementation", "text": "We implement the neural network using the torch7 library (Collobert et al., 2011a). Training and inference are done on a per-sentence level. The initial states of the LSTM are zero vectors. Except for the character and word embeddings whose initialization have been described previously, all lookup tables are randomly initialized with values drawn from the standard normal distribution."}, {"heading": "2.4.2 Objective Function and Inference", "text": "We train our network to maximize the sentencelevel log-likelihood as presented by Collobert et al. (2011b).\nFirst, we define a tag-transition matrix A where Ai,j represents the score of jumping from tag i to tag j in successive words, and A0,i as the score for\n(i.e. LSTM units in the second layer takes in the output of the first layer, and so on); the number of layers is a tuned hyperparameter. Figure 3 shows only one unit for simplicity.\nWe saw paintings of Picasso Word\nEmbedding\nCapitalization\nLexicon Features (one per category)\nChar Features\nLSTM LSTM LSTM LSTM LSTM\nLSTM LSTM LSTM LSTM LSTM\nOut Out Out Out Out\n0\n0\nForward LSTM\nBackward LSTM\nOutput Layers\nOutput\nFigure 3: The (unrolled) BLSTM for tagging named entities. Multiple lookup tables compute the word embedding, capitalization, and lexicon feature vectors. The CNN (Figure 2) computes the char feature vector. For each word, these vectors are concatenated and fed to the BLSTM network, and then the output layers (Figure 4).\nstarting with tag i. This matrix of parameters are also learned. Define \u03b8 as the set of parameters for the neural network, and \u03b8\u2032 = \u03b8 \u222a {Ai,j \u2200i, j} as the set of all parameters to be trained. Given an example sentence, [x]T1 , of length T , and define [f\u03b8]i,t as the score outputted by the neural network for the tth word and ith tag, then the score of a sequence of tags [i]T1 is given as the sum of network and transition scores:\nS([x]T1 , [i] T 1 , \u03b8 \u2032) = T\u2211 t=1 ( A[i]t\u22121,[i]t + [f\u03b8][i]t,t ) Then, letting [y]T1 be the true tag sequence, the sentence-level log-likelihood is obtained by normalizing the above score over all possible tag-sequences [j]T1 using a softmax:\nlogP ([y]T1 | [x]T1 , \u03b8\u2032)\n= S([x]T1 , [y] T 1 , \u03b8 \u2032)\u2212 log \u2211 \u2200[j]T1 eS([x] T 1 ,[j] T 1 ,\u03b8 \u2032)\nLSTM LSTM\nForward Backward\nLinear\nLog-Softmax\nAdd\nFigure 4: The output layers (\u201cOut\u201d in Figure 3) to decode LSTM output into a score for each tag category, which can be interpreted as log-probabilities of the tag being correct\nThis objective function and its gradients can be efficiently computed by dynamic programming (Collobert et al., 2011b).\nAt inference time, given neural network outputs [f\u03b8]i,t we use the Viterbi algorithm to find the tag sequence [i]T1 that maximizes the score S([x]T1 , [i] T 1 , \u03b8 \u2032)."}, {"heading": "2.4.3 Learning Algorithm", "text": "Training is done by mini-batch stochastic gradient descent (SGD) with a fixed learning rate. Each minibatch consists of multiple sentences with the same number of tokens.\nAs shown in Table 8 and Table 9, we found that applying dropout to the input and output nodes of each LSTM layer (Pham et al., 2014) was quite effective in reducing overfitting.\nWe explored other more sophisticated optimization algorithms such as momentum (Nesterov, 1983), AdaDelta (Zeiler, 2012), and RMSProp (Hinton et al., 2012), and in preliminary experiments they did not meaningfully improve upon plain SGD."}, {"heading": "2.4.4 Tagging Scheme", "text": "The output tags are annotated with BIOES (which stand for Begin, Inside, Outside, End, Single) as this scheme has been reported to outperform others such as BIO (Ratinov and Roth, 2009)."}, {"heading": "3 Evaluation", "text": "Evaluation was performed on the well-established CoNLL-2003 NER shared task dataset (Tjong Kim Sang and De Meulder, 2003) and the much larger but less-studied OntoNotes 5.0 dataset (Hovy et al., 2006). Table 2 gives an overview of these two different datasets.\nWe ran each experiment multiple times and report the average and standard deviation of at least 7 successful trials. Since for unknown reasons, sometimes the model fails to converge, in all experiments we exclude trials where the final F1-score on a subset of training data falls below a certain threshold."}, {"heading": "3.1 Dataset Preprocessing", "text": "For all datasets, we performed the following preprocessing:\n\u2022 All sequences of digits 0-9 are replaced by a single \u201c0\u201d.\n\u2022 Before training starts, sentences are grouped by sentence length and separated into minibatches, which are then shuffled.\nIn addition, for the OntoNotes dataset, in order to handle the Date, Time, Money, Percent, Quantity, Ordinal, and Cardinal named entity tags, we split tokens before and after every digit."}, {"heading": "3.2 CoNLL 2003 Dataset", "text": "Table 3 shows the chosen hyper-parameters for all experiments involving the CoNLL-2003 dataset. We tuned the hyper-parameters on the development set by random search, then trained the model on both the training and development sets.\nWe excluded all trials where the final F1 score on the development set was less than 95. There was no ambiguity in selecting the threshold as every trial scored either above 99 or below 90.\nWe trained the models for a large number of epochs because while tuning on the development set, we found that for some initializations, the model appears to plateau around 20-30 epochs, and yet slowly improves starting at epoch 50 and beyond."}, {"heading": "3.3 OntoNotes 5.0 Dataset", "text": "Following Durrett and Klein (2014), we applied our model to the portion of the CoNLL-2012 shared task dataset (Pradhan et al., 2012) with gold-standard named entity annotations; the New Testaments portion was excluded for lacking gold-standard annotations.\nFor hyper-parameters, due to time constraints it was infeasible to do a fresh random search across the full parameter space. Thus, we simply used the best setting from CoNLL 2003 experiments and tuned the number of epochs and learning rate based on development set performance. Table 3 shows the final hyper-parameters for all experiments involving this dataset.\nWe excluded all trials where the final F1 score on the last 5,000 sentences of the training set was less than 80; every trial scored either above 80 or below 50."}, {"heading": "4 Results and Discussion", "text": "Table 4 shows the results for all datasets. Our best model is competitive with other state of the art systems on the CoNLL-2003 dataset, and for the OntoNotes dataset, to the best of our knowledge we\n11OntoNotes results taken from (Durrett and Klein, 2014) 12It was unclear whether or not they evaluated their system on the CoNLL-2012 split of the OntoNotes dataset. 13Numbers taken from the original paper (Luo et al., 2015). While the precision, recall, and F1 scores are clearly inconsistent, it is unclear in which way they are incorrect.\nhave surpassed the previous highest reported results on all of precision, recall, and F1-score. In particular, word embeddings and character-level features (produced by the CNN) contributed most of the performance, suggesting that when given enough data, the neural network is able to automatically learn the relevant features for NER without feature engineering."}, {"heading": "4.1 Character-level CNNs vs. Character Type and Capitalization Features", "text": "Comparison of Table 5 and Table 6 shows that on CoNLL-2003, BLSTM-CNN models significantly14 outperform the BLSTM models when given the same feature set. This effect is much smaller and not\n14Wilcoxon rank sum test, p < 0.05 when comparing the four BLSTM models with the corresponding BLSTM-CNN models utilizing the same feature set. The Wilcoxon rank sum test was selected for its robustness against small sample sizes when the distribution is unknown.\nstatistically significant on OntoNotes when capitalization features are added. Notably, with the presence of the character-level CNN, neither the character type nor the capitalization feature provide any improvements at all. These results suggest that the character-level CNN can replace hand-crafted character class features."}, {"heading": "4.2 Word Embeddings", "text": "Table 4 and Table 7 show that we obtain a large, significant15 improvement when trained word embeddings are used, as opposed to random embeddings, regardless of the additional features used. This is\n15Wilcoxon rank sum test, p < 0.001\nconsistent with results reported previously by Collobert et. al. (2011b).\nTable 7 compares the performance of different word embeddings in our best model in Table 4 (BLSTM-CNN + emb + lex). For CoNLL-2003, publicly available GloVe and Google embeddings are about one point behind Collobert\u2019s embeddings. For OntoNotes, GloVe embeddings perform close to Collobert embeddings while Google embeddings are again one point behind. In addition, 300 dimensional embeddings present no significant improvement over 50 dimensional embeddings \u2013 a result previously reported by Turian et al. (2010).\nOne possible reason that Collobert embeddings perform better than other publicly available embeddings on CoNLL-2003 is that they are trained on the Reuters RCV-1 corpus, the source of CoNLL2003 dataset, whereas the other embeddings are not. On the other hand, we suspect that Google\u2019s embeddings perform poorly because of vocabulary mismatch \u2013 in particular, Google\u2019s embeddings were trained in a case-sensitive manner, and embeddings for many common punctuations and symbols were not provided. To test these hypotheses, we performed experiments with new word embeddings trained using GloVe and word2vec, with vocabulary list and corpus similar to Collobert et. al. (2011b).\nAs shown in Table 7, our GloVe embeddings improved significantly16 over publicly available embeddings on CoNLL-2003, and our word2vec embeddings improved significantly17 over Google\u2019s embeddings on OntoNotes.\nDue to time constraints we did not perform new hyper-parameter searches with any of the word embeddings. As word embedding quality depends on hyper-parameter choice during their training (Pennington et al., 2014), and also, in our NER neural network, hyper-parameter choice is likely sensitive to the type of word embeddings used, optimizing them all will likely produce better results and provide a fairer comparison of word embedding quality."}, {"heading": "4.3 Effect of Dropout", "text": "Table 8 and Table 9 compares the result with and without dropout for each dataset. All other hyperparameters and features remain the same as our best model Table 4. In both datasets, dropout is essential for state of the art performance, and the improvement is statistically significant18; without dropout, performance on training data improves while test data performance suffers."}, {"heading": "4.4 Lexicon Features", "text": "Table 5 and Table 6 show that on the CoNLL-2003 dataset, addition of lexicon features appears to pro-\n16Wilcoxon rank sum test, p < 0.01 17Wilcoxon rank sum test, p < 0.001 18Wilcoxon rank sum test, p < 0.01 for both CoNLL-2003\nand OntoNotes.\nvide a small improvement19 in all models. Unfortunately such an improvement does not occur for the OntoNotes, most likely because our lexicon is not populated for many of the tags in the OntoNotes tagset due to the lack of corresponding DBpedia categories. Another interpretation of these results is that our character-level CNN model is learning much of the same knowledge that is encoded in the NE lexicons20.\nFor lexicon matching, we also tried the method outlined by Collobert et al. (2011b), which involves performing exact matching for each category and turning \u201con\u201d the corresponding lexicon feature when a match is found. Table 10 shows that their more simplistic method provides no improvement at all over not using a lexicon. We hypothesize that our model is able to learn character-level features that perform better than Collobert et al.\u2019s inflexible lexical matching."}, {"heading": "5 Related Research", "text": "Named entity recognition is a task with a long history in NLP. In this section, we summarize the works that are most relevant to our research, namely those we make direct comparisons with and those that our approach drew inspiration from."}, {"heading": "5.1 Named Entity Recognition", "text": "Most recent approaches to NER has been characterized by the use of CRF models or other models like SVMs and averaged perceptron models, where performance is heavily dependent on feature engineering. Ratinov and Roth (2009) used nonlocal features, gazetteer extracted from Wikipedia, and Brown-cluster-like word representations, and achieved an F1 score of 90.80 on CoNLL-2003.\n19Not statistically significant 20This should not come as a great surprise given the amount of information about proper noun status that is conveyed by capitalization in the English language.\nLin and Wu (2009) surpassed them without using a gazetteer, by instead using phrase features obtained by performing k-means clustering over a private database of search engine query logs. Passos et al. (2014) obtained nearly the same performance using only public data by training phrase vectors in their lexicon-infused SkipGram model.\nTraining an NER system together with related tasks such as entity linking has recently been shown to improve the state of the art. Durrett and Klein (2014) combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors. Their system achieved state of the art results on the OntoNotes dataset, but they did not evaluate on the CoNLL-2003 dataset due to lack of coreference annotations. Luo et al. (2015) achieved the current state of the art results on CoNLL-2003 by training a joint model over the NER and entity linking tasks, the pair of tasks whose inter-dependencies contributed the most to the work of Durrett and Klein (2014)."}, {"heading": "5.2 NER with Neural Networks", "text": "While many approaches involve CRF models, there has also been a long history of research applying neural networks to NER. Early attempts were hindered by lack of computational power, scalable learning algorithms, and high quality word embeddings.\nPetasis et al. (2000) used a feed-forward neural network with one hidden layer on NER and achieved state-of-the-art results on the MUC6 dataset. Their approach used one-hot encodings for POS tag and gazetteer tags for each word, with no word embeddings.\nHammerton (2003) attempted NER with a singledirection LSTM network and a combination of 64-dimensional word vectors trained using selforganizing maps, as well as context vectors obtained using principle component analysis. However, while our method optimizes log-likelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function. Hammerton\u2019s (2003) reported results were only slightly above baseline models.\nMuch later, with the advent of neural word embeddings, Collobert et al. (2011b) presented SENNA, which employs a feed-forward deep neu-\nral network and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL. We build on their approach, sharing the word embeddings, feature encoding method, and objective functions.\nRecently, dos Santos et al. (2015) presented their CharWNN network, which augments the neural network of Collobert et al. (2011b) with character level CNNs, and they reported improved performance on Spanish and Portuguese NER. We have successfully incorporated character-level CNNs into our RNN model21.\nOur method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al. (2015). Both of them employed the same objective function and similar bidirectional networks, with the former using a Bi-directional LSTM without CNNs and the latter using character-level CNNs with standard RNNs. Our method differs from Huang et al. (2015) in that we employ very little feature engineering and instead take advantage of character level CNNs to automatically extract features from a word. Unlike Labeau et al. (2015), we employed the more powerful LSTM, which we found to outperform standard RNNs in preliminary experimentation as well as being easier to train. In addition, while Labeau et al. (2015) reported near stateof-the-art performance in German POS tagging with no word embeddings at all, we found that even with character-level CNNs, word embeddings are crucial to NER performance."}, {"heading": "6 Conclusion", "text": "We have shown that our neural network model, which incorporates a bidirectional LSTM and a character-level CNN and which benefits from robust training through dropout, achieves state-of-the-art results in named entity recognition with little feature engineering. Our model improves over previous best reported results on the large OntoNotes dataset, indicating that the model is capable of learning complex relationships from large amounts of\n21Attempts at character-level RNNs did not perform as well as CNNs despite promising results in language model applications (Karpathy et al., 2015).\ndata, and achieves competitive performance on the smaller CoNLL 2003 dataset.\nPreliminary evaluation of our partial matching lexicon algorithm suggests that performance could be further improved through more flexible application of existing lexicons, but that our character-level CNN models learn much of the information that is encoded in lexicons. Evaluation of existing word vectors suggests that the domain of training data text is as important as the word vector training algorithm.\nMore effective construction and application of lexicons and word vectors are areas that require more research. In the future, we would also like to extend our model to perform similar tasks such as extended tagset NER and entity linking."}, {"heading": "Acknowledgments", "text": "This research was supported by Honda Research Institute Japan, Co.Ltd. The authors would like to thank Collobert et al. (2011b) for releasing the SENNA source code and word vectors, the torch7 framework contributors, and Andrey Karpathy for the reference LSTM implementation."}], "references": [{"title": "DBpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "Springer.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "A convolutional neural network for modelling sentences", "author": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet."], "venue": "BigLearn, NIPS Workshop, number EPFL-CONF-192376.", "citeRegEx": "Collobert et al\\.,? 2011a", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011b", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["C\u0131cero dos Santos", "Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro."], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop, page 25.", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Greg Durrett", "Dan Klein."], "venue": "Transactions of the Association for Computational Linguistics, 2:477\u2013490.", "citeRegEx": "Durrett and Klein.,? 2014", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural computation, 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler."], "venue": "Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347\u2013352. IEEE.", "citeRegEx": "Goller and Kuchler.,? 1996", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton."], "venue": "Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4, pages 172\u2013175. Association for Computational Linguistics.", "citeRegEx": "Hammerton.,? 2003", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Lecture 6e: rmsprop: divide the gradient by a running average of its recent magnitude", "author": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky."], "venue": "Neural Networks for Machine Learning. http: //www.cs.toronto.edu/ \u0303tijmen/csc321/", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "OntoNotes: the 90% solution", "author": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."], "venue": "Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers, pages 57\u201360. Association", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "CoRR, abs/1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "CoRR, abs/1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Matthieu Labeau", "Kevin L\u00f6ser", "Alexandre Allauzen."], "venue": "Proceedings of the 2015 Conference", "citeRegEx": "Labeau et al\\.,? 2015", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Phrase clustering for discriminative learning", "author": ["Dekang Lin", "Xiaoyun Wu."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2,", "citeRegEx": "Lin and Wu.,? 2009", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Joint entity recognition and disambiguation", "author": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 879\u2013888, Lisbon, Portugal, September. Associ-", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky."], "venue": "Proc. of the 2011 ASRU Workshop, pages 196\u2013201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov."], "venue": "Soviet Mathematics Doklady, 27(2):372\u2013376.", "citeRegEx": "Nesterov.,? 1983", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "Proceedings of CoNLL-2014, page 78.", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Symbolic and neural learning for named-entity recognition", "author": ["G Petasis", "S Petridis", "G Paliouras", "V Karkaletsis", "SJ Perantonis", "CD Spyropoulos."], "venue": "Symposium on Computational Intelligence and Learning, Chios, Greece, pages 58\u201366. Citeseer.", "citeRegEx": "Petasis et al\\.,? 2000", "shortCiteRegEx": "Petasis et al\\.", "year": 2000}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pages 285\u2013", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."], "venue": "Joint Conference on EMNLP and CoNLL-Shared Task, pages 1\u201340. Association for", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147\u2013155. Association for Computational Linguistics.", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Learning representations by back-propagating errors", "author": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams."], "venue": "Nature, pages 323\u2013533.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142\u2013147.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computa-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 27, "context": "High performance approaches have been dominated by applying statistical models such as CRF, SVM, or perceptron models to hand-crafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015).", "startOffset": 143, "endOffset": 206}, {"referenceID": 22, "context": "High performance approaches have been dominated by applying statistical models such as CRF, SVM, or perceptron models to hand-crafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015).", "startOffset": 143, "endOffset": 206}, {"referenceID": 18, "context": "High performance approaches have been dominated by applying statistical models such as CRF, SVM, or perceptron models to hand-crafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015).", "startOffset": 143, "endOffset": 206}, {"referenceID": 3, "context": "(2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text \u2013 an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amount of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al.", "startOffset": 319, "endOffset": 369}, {"referenceID": 20, "context": "(2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text \u2013 an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amount of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al.", "startOffset": 319, "endOffset": 369}, {"referenceID": 28, "context": ", 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al., 1986).", "startOffset": 77, "endOffset": 101}, {"referenceID": 3, "context": "However, Collobert et al. (2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text \u2013 an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amount of data (Collobert and Weston, 2008; Mikolov et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 4, "context": "Unfortunately there are many limitations to the model proposed by Collobert et al. (2011b). First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word \u2013 an approach that discards useful long-distance relations between words.", "startOffset": 66, "endOffset": 91}, {"referenceID": 9, "context": "A well-studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network (RNN) (Goller and Kuchler, 1996).", "startOffset": 142, "endOffset": 168}, {"referenceID": 10, "context": "Recently, RNNs have shown great success in diverse NLP tasks such as speech recognition (Graves et al., 2013), machine translation (Cho et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 2, "context": ", 2013), machine translation (Cho et al., 2014), and language modeling (Mikolov et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 19, "context": ", 2014), and language modeling (Mikolov et al., 2011).", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": "The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned (Gers et al., 2000).", "startOffset": 134, "endOffset": 153}, {"referenceID": 10, "context": "For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model (Graves et al., 2013).", "startOffset": 268, "endOffset": 289}, {"referenceID": 2, "context": ", 2013), machine translation (Cho et al., 2014), and language modeling (Mikolov et al., 2011). The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned (Gers et al., 2000). For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model (Graves et al., 2013). While LSTMs have been studied in the past for the NER task by Hammerton (2003), the lack of computational power (which led to the use of very small models) and quality word embeddings ar X iv :1 51 1.", "startOffset": 30, "endOffset": 620}, {"referenceID": 1, "context": "(2011b) also applied CNNs to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures (Blunsom et al., 2014).", "startOffset": 157, "endOffset": 179}, {"referenceID": 3, "context": "Collobert et al. (2011b) also applied CNNs to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures (Blunsom et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the exact match only approach of Collobert et al. (2011b). Extensive evaluation shows that our proposed method performs competitively on the CoNLL-2003 NER shared task and outperforms all known state-of the-art systems on the OntoNotes 5.", "startOffset": 213, "endOffset": 238}, {"referenceID": 10, "context": "Instead of a feed-forward network, we use the more powerful recurrent neural network; in particular, we choose the bidirectional long-short term memory (LSTM) network, which has been used successfully in other sequence labelling tasks such as speech recognition (Graves et al., 2013).", "startOffset": 262, "endOffset": 283}, {"referenceID": 16, "context": ", 2015) and German POStagging (Labeau et al., 2015).", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": "Our neural network is inspired by the work of Collobert et al. (2011b), where feature vectors are computed by lookup tables and concatenated together, and then fed into a multi-layer network.", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": "Our best model uses the publicly available 50dimensional word embeddings released by Collobert et al. (2011b)1, which were trained on Wikipedia and Reuters RCV-1 corpus.", "startOffset": 85, "endOffset": 110}, {"referenceID": 23, "context": "We also experimented with two other sets of published embeddings, namely Stanford\u2019s GloVe embeddings2 trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014) and Google\u2019s word2vec embeddings3 trained on 100 billion words from Google News (Mikolov et al.", "startOffset": 157, "endOffset": 182}, {"referenceID": 20, "context": ", 2014) and Google\u2019s word2vec embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013).", "startOffset": 88, "endOffset": 110}, {"referenceID": 23, "context": "In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation4 of the word2vec (Mikolov et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 20, "context": ", 2014) program and an in-house re-implementation4 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 dataset as well.", "startOffset": 67, "endOffset": 89}, {"referenceID": 4, "context": "Following Collobert et al. (2011b), all words are lower-cased before passing through the lookup table to convert to their corresponding embeddings.", "startOffset": 10, "endOffset": 35}, {"referenceID": 5, "context": "As capitalization information is erased during lookup of the word embedding, we evaluate Collobert\u2019s method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo (Collobert et al., 2011b).", "startOffset": 253, "endOffset": 278}, {"referenceID": 17, "context": "word2vec/ We used our in-house reimplementation to train word vectors because it uses distributed processing to train much quicker than the publicly-released implementation of word2vec and its performance on the word analogy task was higher than reported by Mikolov et al. (2013). While Collobert et al.", "startOffset": 258, "endOffset": 280}, {"referenceID": 4, "context": "While Collobert et al. (2011b) used Wikipedia text from 2007, we used Wikipedia text from 2011.", "startOffset": 6, "endOffset": 31}, {"referenceID": 27, "context": "Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 22, "context": "Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 7, "context": "Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 18, "context": "Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 0, "context": "For each of the four categories (Person, Organization, Location, Miscellaneous) defined by the CoNLL 2003 NER shared task, we compiled a list of known named entities from DBpedia (Auer et al., 2007), by extracting all descendants of DBpedia types corresponding to the CoNLL categories7.", "startOffset": 179, "endOffset": 198}, {"referenceID": 4, "context": "As shown in Table 10, we found that this more sophisticated method outperforms the method presented by Collobert et al. (2011b), which simply uses exact matching and marks tokens with on/off instead of BIOES annotation.", "startOffset": 103, "endOffset": 128}, {"referenceID": 10, "context": "Following the speech-recognition framework outlined by Graves et al. (2013), we employed a stacked10 bi-directional recurrent neural network", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": "We implement the neural network using the torch7 library (Collobert et al., 2011a).", "startOffset": 57, "endOffset": 82}, {"referenceID": 4, "context": "We train our network to maximize the sentencelevel log-likelihood as presented by Collobert et al. (2011b).", "startOffset": 82, "endOffset": 107}, {"referenceID": 5, "context": "This objective function and its gradients can be efficiently computed by dynamic programming (Collobert et al., 2011b).", "startOffset": 93, "endOffset": 118}, {"referenceID": 25, "context": "As shown in Table 8 and Table 9, we found that applying dropout to the input and output nodes of each LSTM layer (Pham et al., 2014) was quite effective in reducing overfitting.", "startOffset": 113, "endOffset": 132}, {"referenceID": 21, "context": "We explored other more sophisticated optimization algorithms such as momentum (Nesterov, 1983), AdaDelta (Zeiler, 2012), and RMSProp (Hinton et al.", "startOffset": 78, "endOffset": 94}, {"referenceID": 31, "context": "We explored other more sophisticated optimization algorithms such as momentum (Nesterov, 1983), AdaDelta (Zeiler, 2012), and RMSProp (Hinton et al.", "startOffset": 105, "endOffset": 119}, {"referenceID": 12, "context": "We explored other more sophisticated optimization algorithms such as momentum (Nesterov, 1983), AdaDelta (Zeiler, 2012), and RMSProp (Hinton et al., 2012), and in preliminary experiments they did not meaningfully improve upon plain SGD.", "startOffset": 133, "endOffset": 154}, {"referenceID": 27, "context": "The output tags are annotated with BIOES (which stand for Begin, Inside, Outside, End, Single) as this scheme has been reported to outperform others such as BIO (Ratinov and Roth, 2009).", "startOffset": 161, "endOffset": 185}, {"referenceID": 13, "context": "0 dataset (Hovy et al., 2006).", "startOffset": 10, "endOffset": 29}, {"referenceID": 26, "context": "Following Durrett and Klein (2014), we applied our model to the portion of the CoNLL-2012 shared task dataset (Pradhan et al., 2012) with gold-standard named entity annotations; the New Testaments portion was excluded for lacking gold-standard annotations.", "startOffset": 110, "endOffset": 132}, {"referenceID": 7, "context": "Following Durrett and Klein (2014), we applied our model to the portion of the CoNLL-2012 shared task dataset (Pradhan et al.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "OntoNotes results taken from (Durrett and Klein, 2014) It was unclear whether or not they evaluated their system on the CoNLL-2012 split of the OntoNotes dataset.", "startOffset": 29, "endOffset": 54}, {"referenceID": 18, "context": "Numbers taken from the original paper (Luo et al., 2015).", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.", "startOffset": 4, "endOffset": 29}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.", "startOffset": 4, "endOffset": 64}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.", "startOffset": 4, "endOffset": 104}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.10 - Ratinov and Roth (2009)11 91.", "startOffset": 4, "endOffset": 138}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.10 - Ratinov and Roth (2009)11 91.20 90.50 90.80 82.00 84.95 83.45 Lin and Wu (2009) - 90.", "startOffset": 4, "endOffset": 194}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.10 - Ratinov and Roth (2009)11 91.20 90.50 90.80 82.00 84.95 83.45 Lin and Wu (2009) - 90.90 - Passos et al. (2014)12 - 90.", "startOffset": 4, "endOffset": 225}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.10 - Ratinov and Roth (2009)11 91.20 90.50 90.80 82.00 84.95 83.45 Lin and Wu (2009) - 90.90 - Passos et al. (2014)12 - 90.90 - 82.24 Durrett and Klein (2014) - 85.", "startOffset": 4, "endOffset": 268}, {"referenceID": 4, "context": "19) Collobert et al. (2011b) - 88.67 - Collobert et al. (2011b) + lexicon - 89.59 - Huang et al. (2015) - 90.10 - Ratinov and Roth (2009)11 91.20 90.50 90.80 82.00 84.95 83.45 Lin and Wu (2009) - 90.90 - Passos et al. (2014)12 - 90.90 - 82.24 Durrett and Klein (2014) - 85.22 82.89 84.04 Luo et al. (2015)13 91.", "startOffset": 4, "endOffset": 306}, {"referenceID": 30, "context": "In addition, 300 dimensional embeddings present no significant improvement over 50 dimensional embeddings \u2013 a result previously reported by Turian et al. (2010).", "startOffset": 140, "endOffset": 161}, {"referenceID": 23, "context": "As word embedding quality depends on hyper-parameter choice during their training (Pennington et al., 2014), and also, in our NER neural network, hyper-parameter choice is likely sensitive to the type of word embeddings used, optimizing them all will likely produce better results and provide a fairer comparison of word embedding quality.", "startOffset": 82, "endOffset": 107}, {"referenceID": 4, "context": "For lexicon matching, we also tried the method outlined by Collobert et al. (2011b), which involves performing exact matching for each category and turning \u201con\u201d the corresponding lexicon feature when a match is found.", "startOffset": 59, "endOffset": 84}, {"referenceID": 27, "context": "Ratinov and Roth (2009) used nonlocal features, gazetteer extracted from Wikipedia, and Brown-cluster-like word representations, and achieved an F1 score of 90.", "startOffset": 0, "endOffset": 24}, {"referenceID": 17, "context": "Lin and Wu (2009) surpassed them without using a gazetteer, by instead using phrase features obtained by performing k-means clustering over a private database of search engine query logs.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "Lin and Wu (2009) surpassed them without using a gazetteer, by instead using phrase features obtained by performing k-means clustering over a private database of search engine query logs. Passos et al. (2014) obtained nearly the same performance using only public data by training phrase vectors in their lexicon-infused SkipGram model.", "startOffset": 0, "endOffset": 209}, {"referenceID": 7, "context": "Durrett and Klein (2014) combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Durrett and Klein (2014) combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors. Their system achieved state of the art results on the OntoNotes dataset, but they did not evaluate on the CoNLL-2003 dataset due to lack of coreference annotations. Luo et al. (2015) achieved the current state of the art results on CoNLL-2003 by training a joint model over the NER and entity linking tasks, the pair of tasks whose inter-dependencies contributed the most to the work of Durrett and Klein (2014).", "startOffset": 0, "endOffset": 331}, {"referenceID": 7, "context": "Durrett and Klein (2014) combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors. Their system achieved state of the art results on the OntoNotes dataset, but they did not evaluate on the CoNLL-2003 dataset due to lack of coreference annotations. Luo et al. (2015) achieved the current state of the art results on CoNLL-2003 by training a joint model over the NER and entity linking tasks, the pair of tasks whose inter-dependencies contributed the most to the work of Durrett and Klein (2014).", "startOffset": 0, "endOffset": 560}, {"referenceID": 4, "context": "Much later, with the advent of neural word embeddings, Collobert et al. (2011b) presented SENNA, which employs a feed-forward deep neu-", "startOffset": 55, "endOffset": 80}, {"referenceID": 4, "context": "Recently, dos Santos et al. (2015) presented their CharWNN network, which augments the neural network of Collobert et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 4, "context": "(2015) presented their CharWNN network, which augments the neural network of Collobert et al. (2011b) with character level CNNs, and they reported improved performance on Spanish and Portuguese NER.", "startOffset": 77, "endOffset": 102}, {"referenceID": 14, "context": "Our method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 14, "context": "Our method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al. (2015). Both of them employed the same objective function and similar bidirectional networks, with the former using a Bi-directional LSTM without CNNs and the latter using character-level CNNs with standard RNNs.", "startOffset": 106, "endOffset": 196}, {"referenceID": 14, "context": "Our method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al. (2015). Both of them employed the same objective function and similar bidirectional networks, with the former using a Bi-directional LSTM without CNNs and the latter using character-level CNNs with standard RNNs. Our method differs from Huang et al. (2015) in that we employ very little feature engineering and instead take advantage of character level CNNs to automatically extract features from a word.", "startOffset": 106, "endOffset": 446}, {"referenceID": 14, "context": "Our method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al. (2015). Both of them employed the same objective function and similar bidirectional networks, with the former using a Bi-directional LSTM without CNNs and the latter using character-level CNNs with standard RNNs. Our method differs from Huang et al. (2015) in that we employ very little feature engineering and instead take advantage of character level CNNs to automatically extract features from a word. Unlike Labeau et al. (2015), we employed the more powerful LSTM, which we found to outperform standard RNNs in preliminary experimentation as well as being easier to train.", "startOffset": 106, "endOffset": 622}, {"referenceID": 14, "context": "Our method is most similar to the recent BiLSTM-CRF model for POS-tagging, chunking, and NER presented by Huang et al. (2015) and the BiRNN model for POS-tagging presented by Labeau et al. (2015). Both of them employed the same objective function and similar bidirectional networks, with the former using a Bi-directional LSTM without CNNs and the latter using character-level CNNs with standard RNNs. Our method differs from Huang et al. (2015) in that we employ very little feature engineering and instead take advantage of character level CNNs to automatically extract features from a word. Unlike Labeau et al. (2015), we employed the more powerful LSTM, which we found to outperform standard RNNs in preliminary experimentation as well as being easier to train. In addition, while Labeau et al. (2015) reported near stateof-the-art performance in German POS tagging with no word embeddings at all, we found that even with character-level CNNs, word embeddings are crucial to NER performance.", "startOffset": 106, "endOffset": 807}, {"referenceID": 15, "context": "Attempts at character-level RNNs did not perform as well as CNNs despite promising results in language model applications (Karpathy et al., 2015).", "startOffset": 122, "endOffset": 145}, {"referenceID": 4, "context": "The authors would like to thank Collobert et al. (2011b) for releasing the SENNA source code and word vectors, the torch7 framework contributors, and Andrey Karpathy for the reference LSTM implementation.", "startOffset": 32, "endOffset": 57}], "year": 2015, "abstractText": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects wordand character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing exact match approaches. Extensive evaluation shows that, given only tokenized text, publicly available word vectors, and an automatically constructed lexicon from open sources, our system is able to surpass the reported state-ofthe-art on the OntoNotes 5.0 dataset by 2.35 F1 points and achieves competitive results on the CoNLL 2003 dataset, rivaling systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.", "creator": "LaTeX with hyperref package"}}}