{"id": "1511.08411", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological Similarity", "abstract": "text segmentation ( ts ) aims at dividing long text into coherent segments which thus reflect the subtopic structure of executing the text. it is beneficial to many natural language processing tasks, integrating such functions as relational information retrieval ( ir ) and document summarisation. current approaches to text segmentation are similar in that they all use word - frequency metrics to equally measure the similarity between that two regions of linear text, so that a document is segmented efficiently based on the lexical cohesion principles between its words. various nlp tasks are now moving towards the semantic web and web ontologies, such / as ontology - based ir systems, to capture the conceptualizations ideally associated with user needs and contents. assessing text tree segmentation based partially on lexical cohesion between words is hence not sufficient anymore for such tasks. this paper proposes ontoseg, a novel approach to improving text segmentation based on the possible ontological similarity between text blocks. the proposed method uses ontological similarity to explore conceptual relations involved between text segments and a hierarchical agglomerative clustering ( hac ) algorithm, to represent the text as a tree - like hierarchy that therefore is conceptually randomly structured. the rich structure of the created consonant tree further allows the segmentation synthesis of text in relatively a linear fashion at various levels of granularity. the concept proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text vector segmentation is very promising. also we enhance the theoretical proposed method by combining ontological similarity with lexical similarity and the results show an enhancement greatly of the segmentation sequence quality.", "histories": [["v1", "Thu, 26 Nov 2015 15:10:18 GMT  (1166kb)", "http://arxiv.org/abs/1511.08411v1", "10 pages, IEEE ICDMW 2015 (SENTIRE Workshop)"]], "COMMENTS": "10 pages, IEEE ICDMW 2015 (SENTIRE Workshop)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mostafa bayomi", "killian levacher", "m rami ghorab", "s\\'eamus lawless"], "accepted": false, "id": "1511.08411"}, "pdf": {"name": "1511.08411.pdf", "metadata": {"source": "CRF", "title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological Similarity", "authors": ["Mostafa Bayomi", "Killian Levacher", "M.Rami Ghorab", "S\u00e9amus Lawless"], "emails": ["seamus.lawless}@scss.tcd.ie"], "sections": [{"heading": null, "text": "into coherent segments which reflect the subtopic structure of the text. It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation. Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words. Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents. Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks. This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured. The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity. The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising. Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality.\nKeywords\u2014Text Segmentation; Ontological similarity; Lexical\nCohesion; Vector Space Model\nI. INTRODUCTION\nText segmentation is the process of placing boundaries within text to create segments according to some taskdependent criterion. An example of text segmentation is topical segmentation, which aims to segment a text according to the subjective definition of what constitutes a topic. Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5]. In Information Retrieval, a document is segmented into distinct topics and only the topical segments relevant to the user\u2019s needs are retrieved. Segmentation not only provides more accurate information to the user, but also reduces the user\u2019s burden to read the whole document. In document summarisation, a document is segmented into topics\nand then each topic is summarized independently. This process guarantees that the final summary covers all the key topics in the document.\nThere are different approaches to text segmentation in the literature. Some approaches segment text linearly [6] and others segment it hierarchically[7]. TextTiling, for example, is a well-known linear text segmentation method proposed by Hearst [8] that measures the lexical similarity between text blocks. Text blocks are the smallest units that constitute the text. They range from one sentence [9] to multiple sentences (paragraphs) [10]. TextTiling uses a sliding window and follows the peaks and valleys of the similarity curve to determine where to segment a text. Utiyama and Isahara [11] segmented all possible partitions using dynamic programming and used the probability distribution of the words to rank and select the best segments.\nThese methods are similar in that they all use word frequency metrics to measure the similarity between two regions of text so that a document is segmented at the points where the connections between the regions of words are the weakest, which means that the obtained segments from these approaches are segmented based on the lexical relationship between words in the text. As mentioned before, text segmentation is an essential step for many NLP tasks; these NLP tasks are moving now towards the Semantic Web and the use of ontologies. In Information Retrieval, for example, systems that are based on keywords provide limited capabilities to capture the conceptualizations associated with user needs and contents. In order to solve these limitations, the idea of semantic searches, based on the conceptual meaning of text, has been the focus of a wide body of research and many ontology-based IR systems have been developed [12]. In such systems, whereby text is segmented based solely on the relation between words, such method represents a limitation to capture the conceptualizations associated with user needs. Hence, a need for segmenting and representing text based on the ontological relation between its constituents arises. In this paper, we propose OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. In contrast to traditional text segmentation approaches which used lexical-based similarity between words, we use ontology-based similarity to assess the relatedness between text blocks. A Hierarchical Agglomerative Clustering (HAC) approach is then applied to cluster similar blocks\ntogether. The output is a hierarchy that is constructed based on how text blocks (one or more sentence) are conceptually related to each other. Our experiments demonstrate that segmenting text based on the ontological similarity is applicable with a low error rate.\nThis research has three contributions:\n1- Segmenting text based on the ontological similarity between text blocks (as opposed to lexical similarity); this method is intended for use in ontology-based NLP tasks.\n2- A method aimed at enhancing the quality of segments produced when no ontological relation between text blocks exists is also presented\n3- Evaluating the quality of text segmentation using the ontology-based similarity.\nThe rest of the paper is organized as follows. In Section II, recent related work in the text segmentation literature is reviewed. Section III presents the proposed ontological text segmentation model. Section IV describes the experiments and the dataset that were used to evaluate the proposed method. Section V describes the evaluation process and the evaluation metrics used therein. Results and findings are discussed in Section VI. Section VII concludes the paper with some future research directions.\nII. RELATED WORK\nVarious synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].\nFurthermore, segmentation tasks have been categorised from\ndifferent points of view as:\n1) Content-based and Discourse-based. 2) Supervised and Unsupervised. 3) Linear and Hierarchical. 4) Borderline sentences detection methods:\na) Similarity based methods. b) Graphical methods. c) Lexical chain based methods."}, {"heading": "A. Content-based and Discourse-based", "text": "Content-based approaches focus on the story content and resolve the segmentation problem by relying on some measure of the difference in word usage on the two sides of a potential boundary: the larger the difference, the more indicative of a boundary. A well-known content-based approach example is TextTiling proposed by Hearst [8]. TextTiling is content-based text segmentation algorithm that uses a sliding window approach to segment a text. The calculation is accomplished by two vectors containing the number of occurring terms of each block. The similarities between adjacent blocks within the text are computed to detect topic changes. The computed similarities are smoothed, and used to identify topic boundaries by a cutoff function.\nOn the other hand, the discourse-based techniques focus on story structure or discourse. These approaches make use of prosodic features such as pause duration as well as lexical features such as the presence of certain cue phrases that tend to appear near the segment boundaries. An example of discourse-based approaches is the Hidden Markov Model (HMM) segmentation method [17] that models \u201cmarker words\u201d, or words which predict a topic change."}, {"heading": "B. Supervised and Unsupervised.", "text": "A supervised text segmentation approach called divSeg was introduced by Song et al. [18], where they apply an iterative approach that splits text at its weakest point in terms of the lexical connectivity strength between two adjacent parts. After they found the weakest point in the text, their approach produces a deep and narrow binary tree. The tree is then flattened into a broad and shallow hierarchy through supervised learning of a document set or explicit input of how a text should be segmented. Hsueh et al. [19] described a supervised hierarchical topic segmentation approach that trains separate classifiers for topic and sub-topic segmentation.\nOn the other hand, Eisenstein and Barzilay[14] proposed a Bayesian approach to unsupervised topic segmentation. They showed that lexical cohesion between text segments can be placed in a Bayesian context by modelling the words in each topic segment. TextTiling [8] and C99 [20] are also considered unsupervised linear topic segmentation algorithms."}, {"heading": "C. Linear and Hierarchical", "text": "If we look at the text segmentation from the text representation perspective, we can divide it into linear and hierarchical approaches. Linear text segmentation deals with the sequential analysis of topical changes where segments are non-overlapping and sequential. It has been argued that this sequence model is sufficient for many purposes [8]. An early linear text segmentation algorithm was the TextTiling approach introduced by Hearst [10] in 1997. Galley et al. [21] proposed LcSeg, a TextTiling-based algorithm that uses tf-idf term weights, which improves text segmentation results. Another well-known linear text segmentation algorithm is C99 introduced by Choi [20]. C99 segments a text by combining a rank matrix, transformed from the sentence-similarity matrix, and divisive clustering. Choi et al. [22] introduced another enhanced version of C99 by applying Latent concept modelling to the similarity metric. They showed that using a Latent Semantic Analysis (LSA) based metric could improve the segmentation accuracy. Utiyama and Isahara [11] introduced probabilistic approaches using Dynamic Programming (DP) called U00. DP can be used to efficiently find paths of minimum cost in a graph. DP is used in text segmentation to represent each possible segment (e.g. every sentence boundary) as an edge providing a cost function that penalizes common vocabulary across segment boundaries. Misra et al. [23] used Latent Dirichlet Allocation (LDA) topic model to linearly segment a text into semantically coherent segments.\nHierarchical text segmentation concerns itself with finding more fine grained subtopic structures in texts. The first\nhierarchical algorithm was proposed by Yaari [7]. Yaari used paragraphs as an elementary units for his algorithm and he measured the cohesion between them using cosine similarity. An agglomerative clustering approach is then applied to induce a dendrogram over paragraphs; the dendrogram is subsequently transformed into a hierarchical segmentation. A hierarchical Bayesian algorithm based on LDA is introduced by Eisenstein [24]."}, {"heading": "D. Borderline sentences detection methods", "text": "There are three main approaches to detect borderline\nsentences within text [2]:\n1) Similarity based methods: represent text blocks as\nvectors and then measure the proximity by using (most of the\ntime) the cosine of the angle between these vectors. The C99\nalgorithm [20] for example uses a similarity matrix to generate\na local classification of sentences and isolate topical segments.\n2) Graphical methods: represent terms frequencies and\nuse these representations to identify topical segments (which\nare dense dot clouds on the graphic). The DotPlotting\nalgorithm [16] is the most common example of the use of a\ngraphical approach of text segmentation.\n3) Lexical chains based methods: the notion of lexical\nchains was first proposed by Morris and Hirst [25] to chain\nsemantically related words together via a thesaurus. A chain\nlinks multiple occurrences of a term in the document and is\nconsidered broken when there are too much sentences between\ntwo occurrences of a term. Segmenter [26] uses this method\nfor text segmentation with a subtle adjustment as it determines\nthe number of necessary sentences to break a chain in function\nof the syntactical category of the term.\nAll the aforementioned approaches have focused on the similarity (or dissimilarity) between text blocks based on the words that constitute the text. Even the approaches that relied on semantic analysis only applied a shallow semantic parsing of text to discover different kinds of relationships between two words, including synonymy (the same meaning) and hyponymy (where one word is a more specific instance of another). Our research therefore focuses on semantically mining text and applying deep semantic analysis of text to discover the relation between its constituents.\nIn our approach we measure the similarity between text blocks based on the ontology-based semantic similarity. The ontology-based semantic similarity relates to computing the similarity between conceptually similar but not necessarily lexically similar terms. Semantic similarity has been widely used in many research fields such as: (1) Information Retrieval: to improve accuracy of current Information Retrieval techniques and semantic indexing [12]. (2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31]. (3) Knowledge management: such as thesauri generation [32], information extraction [33], semantic annotation [34] and ontology merging and learning [35], in\nwhich new concepts should be discovered or acquired from text in order to relate them to already existing ones.\nOntology-based similarity can be classified into three main\napproaches:\n1) Edge-counting approaches: where a straightforward\nmethod to calculate similarity between two concepts is to\ncompute the minimum path length connecting their\ncorresponding ontological nodes via is-a links [36]. The longer\nthe path, the more semantically far the terms are.\n2) Feature-based approaches: on the contrary to edge-\ncounting approaches, feature-based approaches assess\nsimilarity between concepts as a function of their properties\n[28]. They take into account common and noncommon\nfeatures of the compared terms.\n3) Information Content (IC) based approaches: these\napproaches are associated with the appearance probabilities of\neach concept in the taxonomy computed from their\noccurrences in a given corpus. IC of a term is computed\naccording to the negative log of its probability of occurrence.\nIn this manner, infrequent words are considered more\ninformative than common ones. In this research, we rely on an Edge-counting approach proposed by Wu and Palmer [36] as its performance is deemed better than other methods [28].\nIII. SEGMENTATION BY HIERARCHICAL AGGLOMERATIVE CLUSTERING\nThe segmentation process proposed in this paper consists of three phases:\n1) Semantic annotation.\n2) Calculating similarity between text blocks (sentences or paragraphs).\n3) Hierarchical Agglomerative Clustering (HAC)."}, {"heading": "A. Semantic annotation:", "text": "In this phase, the text is semantically annotated using a named entity recognition algorithm and text entities are extracted. Each entity is then mapped to its class or classes in an ontology and the text is represented as a sentence-based vector-space. This vector space is then used as an input to the following phase. Several ontologies exist nowadays, some of them are domain-specific ontologies (such as the MeSH1 ontology of medical and biomedical terms), while others are cross-domain (such as DBpedia2). As we are not focusing on a specific domain, in this research we use DBpedia ontology as the underlying knowledge base, as opposed to a domainspecific alternative. DBpedia Spotlight3 is used as the named entity recognition system to extract entities from the targeted text. DBpedia Spotlight is a tool for automatically annotating mentions of DBpedia resources in text, providing a solution for linking unstructured information sources to the Linked Open Data cloud through DBpedia. DBpedia Spotlight recognizes\n1 http://www.nlm.nih.gov/mesh 2 http://dbpedia.org/ 3 https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki\nentities that have been mentioned in text and subsequently matches these entities to their classes in DBpedia ontology. For each annotated entity in the text, the classes that match this entity are extracted. For example, Barack Obama, as an entity, matches with DBpedia classes: [\u201cPerson\u201d, \u201cAgent\u201d, \u201cOfficeholder\u201d]. Since the elementary blocks for the proposed approach are sentences, each sentence in the text is represented as a vector of entities, and each entity is represented by a set of classes that match the entity from DBpedia. A sentence based vector space is built and a similarity between its adjacent vectors is measured as discussed in the following subsection."}, {"heading": "B. Similarity Computation:", "text": "The key idea proposed in this research consists in treating the segmentation of text based on the ontological similarity between its blocks. A text block is the elementary unit to the segmentation algorithm, which could be one sentence or multiple sentences (paragraphs).\nWe measure the similarity between text units based on two similarity measures: (1) Ontological similarity and (2) Lexical similarity."}, {"heading": "1) Ontological similarity:", "text": "To measure the ontological similarity between two text blocks, we measure the similarity between the classes of their entities using the is-a relation. In ontology structure, the is-a relations group the classes according to how they are conceptually related to each other. Given a pair of two classes, c1 and c2, a well-known method with intuitive explicitness for assessing their similarity is to calculate the distance between these classes in an ontology hierarchy; the shorter the distance, the higher the similarity. In the case that multiple paths between the nodes exist, the shortest distance of all paths is used.\nSeveral measures have been developed for measuring similarity between two concepts in a taxonomy. Out of these, we choose the measure proposed by Wu and Palmer [36] because it has shown performance improvements over other methods [28]. The principle behind Wu and Palmer\u2019s similarity computation is based on the edge-counting method, whereby the similarity of two concepts is defined by how closely they are related in the hierarchy, i.e., their structural relations. Given two concepts c1 and c2, the conceptual similarity between them is:\n ConSim(c1,c2) = 2*N/(N1+N2) \nwhere N is the distance between the closest common ancestor (CS) of c1 and c2 and the taxonomy root, and N1 and N2 are the distances between the taxonomy root on one hand and c1 and c2 on the other hand respectively.\nThe similarity between two entities can be defined as a summation of weighted similarities between pairs of classes in each of the entities. Given two entities E1 and E2, the similarity between them is:\n \nwhere m and n are the two sets of classes that E1 and E2\nhave respectively.\nEquation (2) calculates the similarity between two entities, where each entity belongs to one or more classes. For example, Barack Obama as an entity is mapped to three DBpedia classes: [\u201cPerson\u201d, \u201cAgent\u201d, \u201cOfficeholder\u201d], and George Bush also is mapped to three DBpedia classes: [\u201cPerson\u201d, \u201cAgent\u201d, and \u201cOfficeholder\u201d]. Hence, although the two entities are not lexically similar, or even close to each other, they are deemed ontologically similar. This is the idea behind the ontological similarity: it measures the similarity between entities according to the conceptual characteristics which they share. As another example of how ontological similarity differentiates between entities, consider Michael Jackson as an entity that is mapped to four DBpedia classes: [\u201cPerson\u201d, \u201cAgent\u201d, \u201cArtist\u201d, \u201cMusicalArtist\u201d]. Intuitively, the two entities Barack Obama and George Bush are more ontologically similar to each other than either of them is to Michael Jackson.\nOn a text-block level, the similarity between two blocks can be defined as a summation of weighted similarities between pairs of entities in each of the units.\nGiven two text blocks B1 and B2, which have a set of entities a and b respectively, the similarity between B1 and B2 is:\n  "}, {"heading": "2) Lexical similarity:", "text": "Lexical similarity has been used widely in the literature in text segmentation [8], [20], and as its name suggests, it splits text into segments that are lexically coherent. Lexical cohesion refers to the connectivity between two portions of text in terms of word relationships.\nAlthough text blocks might share ontological similarities between each other, it may be the case that ontological similarity alone is not sufficient to measure how text blocks are coherent with each other. This is due to the following reasons: 1- Text blocks might not contain any entities at all.\n2- The entity extraction algorithm may not discover some entities in the text block.\n3- The extracted entities from a text block may not be sufficient to reflect the similarity between text blocks.\n4- The used ontology may not cover all the text mentions.\nThus, the lexical overlap between text blocks should be part of the overall similarity measure. As a result, we enrich our\nsimilarity measure by obtaining the lexical similarity between text blocks and combine it with the ontological similarity. To measure the lexical similarity between text blocks, first, stopwords are removed from the text as they are generally assumed to be of less, or no, informational value. Then the remaining words are stemmed and each block is represented by a lexical frequency vector. A lexical vector cosine similarity is calculated. It is defined as the cosine of the angle between two vectors v and w such that:\n  "}, {"heading": "C. Hierarchical Agglomerative Clustering (HAC)", "text": "Hierarchical clustering algorithms have been studied extensively in the clustering literature [37]. The general concept of agglomerative clustering is to successively merge documents into clusters based on their similarity with one another. The agglomerative clustering technique could be transferred from document level into text level, where the clustering process is done between text blocks, within a document (as opposed to across whole documents) [7]. When applying Hierarchical Agglomerative Clustering on text blocks the algorithm successively agglomerates blocks that are coherent to each other, thus forming a text structure.\nThe idea behind using HAC in text segmentation is that it is a bottom-up clustering approach, which means that it starts from the smallest chunks and then builds the text hierarchy by merging text blocks (clusters) based on how near or similar they are to each other. In contrast, the top-down (divisive) clustering approach starts from the full document and then divides the text into smaller blocks based on how far (i.e. how different) they are from each other. Hence, the output of the bottom-up approach can be regarded as hierarchically coherent tree. Thus, the method of Hierarchical Agglomerative Clustering for text is useful to support a variety of search methods because it naturally converts text into a tree-like hierarchy and provides different levels of granularity for the underlying content; this can then easily be leveraged for the search process.\nUnlike general HAC for clustering documents, where at each stage the proximity of the newly merged object to all other available segments is computed, on text level we compute only the similarity of the text block to its two neighbours. This is because we require that the linear order in the text be preserved in the structure. The implication on complexity is that while general HAC algorithm for documents takes an order of O(N2) steps, it takes only O(N) on text level.\nThe algorithm successively clusters \u201ccoherent\u201d segments based on the accumulation between the ontological and lexical similarity scores between text blocks, which guarantees the ontological and lexical cohesion between agglomerated\nsegments. The HAC algorithm for text segmentation, based on blocks as the elementary segments, is shown in Fig. 1.\nConceptually, the process of agglomerating blocks into successively higher levels of clusters creates a cluster hierarchy (dendrogram) for which the leaf nodes correspond to individual blocks, and the internal nodes correspond to the merged groups of clusters. When two groups are merged, a new node is created in this tree corresponding to this larger merged group. The two children of this node correspond to the two groups of blocks which have been merged to it. Fig. 2 shows the resulted dendrogram from the algorithm for a sample text."}, {"heading": "D. From hierarchical into linear representation", "text": "The hierarchical text segmentation produces a tree that can be used as a visual illustration of the underlying hierarchical structure of a document. Fig. 3 depicts a tree representation of a sample text of 10 sentences. The benefit of this tree is that it represents different levels of granularity of the document, which in turn means that the document can be segmented into different segmentation levels. This is a powerful criterion in the hierarchical representation of text. In contrast to linear representation, in each level of the structure (tree) segmentation with different levels of details could be obtained and can be usefully applied to many other tasks\u2019 needs.\nIn order to convert a hierarchical representation into a linear representation a threshold corresponding to the number of the segments needed is set and the level that contains the corresponding number of nodes in the tree is extracted. If this number is not represented in one of the tree levels, a flattening process is applied to the largest nodes. For example, suppose that the specified number is 10 segments, and in one of the tree levels the number of nodes (segments) is seven nodes. As now we need three more segments, for the largest three nodes\nFig. 2. Sentences dendrogram of a sample text.\nFig. 3 A tree representation for a text from 10 sentences\n(large in number of blocks) they are flattened by obtaining the two subsequent nodes that constitute this large node, i.e. we go down a level in the tree for those three large segments. This method of flattening the tree guarantees that the coherency between the obtained segments is preserved.\nIV. EXPERIMENTAL SETUP\nThe output from the proposed approach is a tree that represents the text hierarchy. As depicted in Fig 2., each level in the tree represents a level of granularity for the text where each node, in that level, represents a segment that contains coherent blocks. As mentioned before, a linear representation of text can be obtained from such a tree, which means that our method can be evaluated as a linear text segmentation method. In this experiment, we evaluated the efficiency of our approach on Choi\u2019s dataset4 [20]. This dataset has been widely used in linear text segmentation evaluation [38][39]. The dataset\n4Choi\u2019s C99 release and the dataset are available here : http://web.archive.org/web/20040810103924/http://www.cs.man.ac.uk/~mary/ choif/software.html\nconsists of documents made up of ten concatenated text segments. Each segment consists of the first n sentences of a randomly selected document from the Brown Corpus. The dataset is divided into four subsets and are listed in the table below. There are a total of 700 text documents.\nEach document in the dataset is processed and two vector space models are generated: the ontological and the lexical. Since the elementary text blocks to our method consists of sentences, each sentence in the ontological vector space is represented as a vector of sets of DBpedia classes where each set represents an entity that is extracted from the sentence. These sets of classes are used to measure the ontological similarity between sentence vectors according to (1), (2), and (3). To build the lexical vector space, first the stopwords are removed from the text and then the remaining terms are stemmed; after this, each sentence is subsequently represented as a term-frequency vector. The lexical similarity between adjacent vectors is then determined by calculating the cosine similarity between them (4).\nA HAC algorithm is then applied on the obtained vector space models. For the ontological vector space, an ontological similarity score is calculated between each vector and its two neighbours. A lexical similarity score is also obtained for the lexical vector space. The final similarity score between two adjacent sentences is the combination of their ontological similarity and lexical similarity scores. For each set of three neighbouring sentences, the middle sentence is merged with the one that is most similar to it from the other two (e.g. sentence B is merged with C if the similarity score between B and C is higher than the score between A and B). When the two neighbours are merged together they form a new text block (cluster) and two new vectors (ontological and lexical) are defined based on the new block to be used in the next iteration of the algorithm. Iteratively, the algorithm applies the same process between adjacent blocks until it merges all text blocks in one single cluster and a tree representation of the text is produced. A linear segmentation is then produced as mentioned before (Section III D) where the threshold is set to 10 as each document in Choi\u2019s dataset is consisting of 10 segments.\nSince the main contribution of this research is to segment text based on the ontological similarity between its blocks, we first evaluate the quality of the produced segments based on the ontological similarity only. After that, we examine the impact of adding the lexical similarity to the ontological similarity using different weights for the two similarity measures.\nThe size of the elementary text blocks is considered a critical step in the segmentation process. Yaari [7] used paragraphs as the elementary blocks for his segmentation algorithm and affirms that the size of a paragraph, as opposed\nto a sentence, contains sufficient lexical information for the proximity test. Also Hearst et al. [8] measured the cosine similarity between text blocks where text blocks are consisting of fixed number of sentences (window). As a result, we examine the quality of the produced segments, using the ontological similarity only or the combination between the ontological and the lexical similarity, using varying window sizes: from one to four sentences. According to the aforementioned considerations, we conducted four experimental runs (in each run, we used varying window sizes (1 to 4)):\n1) Experiment 1: in the first run we use the ontological\nsimilarity only.\n2) Experiment 2: in the second run, we use the\ncombination between the ontological and lexical similarity\nscores with \u03b1 = 0.3, where \u03b1 specifies the weight of each of\nthe two similarity measures. Let Osim and Lsim be the\nontological and the lexical similarity scores respectively; the\nfinal score between two sentences (or blocks) S1 and S2 is:\n Sim(S1,S2) =\u03b1 * Lsim + (1- \u03b1) * Osim  \nHence, \u03b1 = 0.3 means that the ontological similarity score\nweight is 0.7 and the lexical score weight is 0.3.\n3) Experiment 3: in this run treat both similarity scores\nequally, i.e. \u03b1 = 0.5.\n4) Experiment 4: in this run we give a higher weight to the\nlexical similarity by setting \u03b1=0.3.\nV. EVALUATION\nIt is common to evaluate text segmentation systems by Pk and / or WindowDiff measures. Pk and WindowDiff are penalty measurement metrics, which means that lower scores indicate higher segmentation accuracy. Pk was proposed by Beeferman et al. [40] as a measure that expresses a probability of segmentation error. To calculate Pk, we take a window of fixed width k, which is usually set to half of the average segment length in the reference partition, and move it across the segmented text, at each step examining whether the hypothesized segmentation is correct about the separation (or not) of the two ends of the window. Pk metric is defined as:\n    \nwhere \u03b4ref (i, j) is an indicator function whose value is one if sentences i and j belong to the same segment and zero otherwise. Similarly, \u03b4hyp (i, j) is one if the two sentences are hypothesized as belonging to the same segment and zero otherwise. The \u2295 operator is the XOR operator. The function Dk is the distance probability distribution that uniformly concentrates all its mass on the sentences which have a distance of k.\nWindowDiff [41] is stricter as it not only decides whether there is a mismatch between the hypothesized partition and the\nreference partition, it also counts the difference of the number of segment boundaries in the given window between the two partitions. Thus, the results of WindowDiff are generally higher than those of Pk metric. WindowDiff is defined as:\n   \nwhere ref is the correct segmentation for reference, hyp is the segmentation produced by the model, K is the number of sentences in the text, k is the size of the sliding window and b(i, j) is the number of boundaries between sentences i and j.\nVI. RESULTS\nWe evaluated our approach using the WindowDiff error metric. TABLE II shows the results of experiment 1 (using only the ontological similarity) while applying different window sizes, from 1 to 4 sentences per text block. From the results we can see that the error rates are not high for all the subsets (range from 0.15 to 0.32), which means that generating text segments based on the ontological relation between its constituents is feasible with low error rates. It can also be noticed that varying the window size does not increase the quality of the segmentation; in contrast, it decreases the quality for some subsets. Fig. 4 depicts the impact of the window size on the quality of the produced segments.\nThe lowest error rates can be seen in the 9-11 subset (0.15 for all windows), while the highest error rates can be seen in the 3-5 subset. Intuitively, this implies that as the length of the reference segments (i.e. the real segments from the original text) increases, the efficiency of text segmentation increases. This implication reinforces the feasibility of our approach. This is because, as mentioned before, long segments exhibit more interlinking conceptual relations than short segments. TABLE III shows the results of experiments 2, 3, and 4 where we evaluated the hybrid approach that combines the ontological and lexical similarities using different weights.\nTABLE II. ONTOLOGICAL SIMILARITY ERROR RATES (WD) FOR DIFFERENT WINDOW SIZES\nRange of n Window\n3-11 3-5 6-8 9-11\nW = 1 0.21 0.32 0.20 0.15\nW = 2 0.21 0.32 0.21 0.15\nW = 3 0.21 0.34 0.21 0.15\nW = 4 0.22 0.34 0.21 0.15\nFig. 4. Error rates of the Ontological Similarity using different window sizes\nThe results of experiment 2 indicate that when \u03b1 = 0.3, the error rates of the segmentation in all the subsets are less than the error rates using the ontological similarity only (TABLE II). In experiments 3 and 4, we notice that as \u03b1 increases (0.5 and 0.7 respectively), the error rates decrease. According to (5), when \u03b1 increases, the lexical similarity weight is more than the ontological similarity weight, which means that combining the lexical similarity with the ontological similarity enhances the quality of the produced segments.\nFurthermore, it is noticed that, as in experiment 1, when the window size increases, the error rate also increases which means that the segmentation quality decreases. The chart in Fig. 5 illustrates that increasing the window size increases the error rate with \u03b1 = 0.3. Charts for setting \u03b1 = 0.5 and 0.7 are in APPENDIX I. Fig. 6 depicts the error rates for the four experiments using window = 1 (charts for using other windows are in APPENDIX I). To the best of our knowledge, there is no segmentation approach that uses ontological similarity to segment text. Therefore, it is not possible to compare the evaluation scores of our approach to those of a similar approach. Nevertheless we can compare it with state-of-the-art approaches based on the segmentation quality in general.\nAs we evaluated the performance of our approach using WindowDiff, we also evaluated it with Pk. The approaches that we compare our approach with were evaluated also with the Pk metric. Furthermore, these approaches were evaluated against the same dataset that we use in our experiments (Choi\u2019s dataset). Examples of such approaches are: TextTiling [8], C99 [20], and U00 [11]. TABLE IV presents a comparison of the performance of our approach compared to these approaches where number of segments needed is provided.5\nAlthough OntoSeg (i.e. our segmentation approach that is based on ontological similarity) does not produce the best scores, the results show that it \u2013as a novel method in text segmentation\u2013 is generally performing as good as current state-of-the-art approaches. In other words, the experimental results show that using ontological similarity in text segmentation is very promising and also that text segmentation can be performed in a way that does not depend on text (lexical) characteristics. This renders OntoSeg an approach that lends itself well to Ontology-based NLP tasks.\nVII. CONCLUSION AND FUTURE WORK\nText Segmentation (TS) is an essential pre-step for many Natural Language Processing (NLP) tasks, such as Information Retrieval and Text Summarisation. As these tasks are moving towards the Semantic Web and the use of Ontologies (e.g. ontology-based IR systems), this generates a need to segment text in a way that suits these ontology-based tasks. In this paper we presented a new approach to text segmentation based on the ontological similarity between text blocks. The proposed approach uses a Hierarchical Agglomerative Clustering (HAC) approach to iteratively cluster text segments that are deemed to be ontologically\n5 The results were brought from Utiyama and Isahara [11] & Riedl and Biemann [39] papers.\nsimilar to each other. The output is a tree-like hierarchy of the text. We showed that the produced hierarchy is beneficial in producing hierarchical text segments with different levels of granularity, and also in producing linear text segments by flattening the obtained tree. The results of our experiments showed that using ontological similarity (even on its own) performs successful segmentation with low error rates; this reflects that the ontological segmentation approach has good potential for being used in modern ontology-based systems. We also enhanced the proposed approach by combining the lexical similarity with the ontological similarity; to this end, the experimental results showed that this combination enhanced the produced segments.\nMoving forward, viable future work may involve examining a number of factors that can enhance the segmentation process. For example, it is expected that the choice of the knowledge base ontology to use definitely affects the segmentation quality; the richness of the ontology reflects the richness of the semantic annotation of text. Also the ontology-based similarity approach represents an important factor in enhancing the segmentation quality. As mentioned before, there are different approaches to measure the similarity between two concepts in an ontology, of which we used the edge-counting based approach. For the other approaches that rely on concept properties and Information Content (IC), they measure the similarity between concepts from different perspectives, and provide, for a concept, a better understanding of its semantics. Using these approaches in the similarity measurement may contribute to improving the segmentation quality.\nACKNOWLEDGMENT\nThis work is supported by Science Foundation Ireland (Grant 12/CE/I2267) as part of CNGL Centre for Global Intelligent Content (www.cngl.ie) at Trinity College Dublin.\nREFERENCES\n[1] F. Llopis, A. Ferr\u00e1ndez, and J. Vicedo, \u201cText Segmentation for Efficient Information Retrieval,\u201d in Computational Linguistics and Intelligent Text Processing SE - 39, vol. 2276, A. Gelbukh, Ed. Springer Berlin Heidelberg, 2002, pp. 373\u2013380.\n[2] V. Prince and A. Labadi\u00e9, \u201cText Segmentation Based on Document Understanding for Information Retrieval,\u201d in Natural Language Processing and Information Systems SE - 26, vol. 4592. Springer Berlin Heidelberg, 2007, pp. 295\u2013304.\n[3] B. K. Boguraev and M. S. Neff, \u201cDiscourse segmentation in aid of document summarization,\u201d System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International Conference on. IEEE, 2000.\n[4] S. Tellex, B. Katz, J. Lin, A. Fernandes, and G. Marton, \u201cQuantitative Evaluation of Passage Retrieval Algorithms for Question Answering,\u201d in Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, 2003, pp. 41\u201347.\n[5] C. Beck, A. Streicher, and A. Zielinski, \u201cUsing Text Segmentation Algorithms for the Automatic Generation of E-Learning Courses,\u201d Lexical and Computational Semantics ( SEM 2014), p. 132, 2014.\n[6] Kazantseva and S. Szpakowicz, \u201cLinear Text Segmentation Using Affinity Propagation,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2011, p. 284\u2013293.\n[7] Y. Yaari, \u201cSegmentation of Expository Texts by Hierarchical Agglomerative Clustering,\u201d In Proceedings of RANLP\u201997.\n[8] M. A. Hearst, \u201cMulti-paragraph Segmentation of Expository Text,\u201d in Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 9\u201316.\n[9] N. Ye, J. Zhu, Y. Zheng, M. Ma, H. Wang, and B. Zhang, \u201cA Dynamic Programming Model for Text Segmentation Based on Min-Max Similarity,\u201d in Information Retrieval Technology SE - 14, vol. 4993, Springer Berlin Heidelberg, 2008, pp. 141\u2013152.\n[10] M. A. Hearst, \u201cTextTiling: Segmenting Text into Multi-paragraph Subtopic Passages,\u201d Computational linguistics., vol. 23, no. 1, pp. 33\u2013 64, 1997.\n[11] M. Utiyama and H. Isahara, \u201cA Statistical Model for Domainindependent Text Segmentation,\u201d in Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, 2001, pp. 499\u2013 506.\n[12] G. Hliaoutakis, Angelos, G. Varelas, Giannis (, E. Voutsakis, E. Petrakis, G.M., and E. Milios, \u201cInformation Retrieval by Semantic Similarity,\u201d In Proceedings of International Journal on Semantic Web and Information Systems (IJSWIS), vol. 2, no. 3, 2006.\n[13] M. Purver, \u201cTopic segmentation,\u201d In Proceedings of the Spoken language understanding: systems for extracting semantic information from speech, pp. 291\u2013317, 2011.\n[14] J. Eisenstein and R. Barzilay, \u201cBayesian Unsupervised Topic Segmentation,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2008, pp. 334\u2013343.\n[15] Labadi\u00e9 and V. Prince, \u201cFinding Text Boundaries and Finding Topic Boundaries: Two Different Tasks?,\u201d in Advances in Natural Language Processing SE - 25, vol. 5221. Springer Berlin Heidelberg, 2008, pp. 260\u2013271.\n[16] J. C. Reynar, \u201cAn Automatic Method of Finding Topic Boundaries,\u201d in Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 331\u2013333.\n[17] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang, \u201cTopic Detection and Tracking Pilot Study: Final Report,\u201d in Proceedings of the DARPA Broadcast News Transcription and Understanding Worksho, 1998, pp. 194\u2013218.\n[18] F. Song, W. Darling, A. Duric, and F. Kroon, \u201cAn Iterative Approach to Text Segmentation,\u201d in Advances in Information Retrieval SE - 63, vol. 6611. Springer Berlin Heidelberg, 2011, pp. 629\u2013640.\n[19] P.-Y. Hsueh, J. D. Moore, and S. Renals, \u201cAutomatic segmentation of multiparty dialogue,\u201d in EACL, 2006.\n[20] F. Y. Y. Choi, \u201cAdvances in Domain Independent Linear Text Segmentation,\u201d in Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, 2000, pp. 26\u201333.\n[21] M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing, \u201cDiscourse Segmentation of Multi-party Conversation,\u201d in Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, 2003, pp. 562\u2013569.\n[22] F. Y. Y. Choi, P. Wiemer-Hastings, and J. Moore, \u201cLatent Semantic Analysis for Text Segmentation,\u201d in In Proceedings of Conference on Empirical Methods in Natural Language, 2001, pp. 109\u2013117.\n[23] H. Misra, F. Yvon, J. M. Jose, and O. Cappe, \u201cText Segmentation via Topic Modeling: An Analytical Study,\u201d in Proceedings of the 18th ACM Conference on Information and Knowledge Management, 2009, pp. 1553\u20131556.\n[24] J. Eisenstein, \u201cHierarchical Text Segmentation from Multi-scale Lexical Cohesion,\u201d in Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2009, pp. 353\u2013361.\n[25] J. Morris and G. Hirst, \u201cLexical Cohesion Computed by Thesaural Relations As an Indicator of the Structure of Text,\u201d Computational linguistics, vol. 17, no. 1, pp. 21\u201348, 1991.\n[26] M.-Y. Kan, J. L. Klavans, and K. McKeown, \u201cLinear Segmentation and Segment Significance,\u201d CoRR, vol. cs.CL/9809, 1998.\n[27] P. Resnik and Philip, \u201cSemantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language,\u201d Journal of Artificial Intelligent. Res.(JAIR), vol. 11, pp. 95\u2013130, 1999.\n[28] D. Lin, \u201cAn information-theoretic definition of similarity.,\u201d in ICML, 1998, vol. 98, pp. 296\u2013304.\n[29] E. Cambria, J. Fu, F. Bisio, and S. Poria, \u201cAffectiveSpace 2: Enabling affective intuition for concept-level sentiment analysis,\u201d in TwentyNinth AAAI Conference on Artificial Intelligence, pp. 508\u2013514, 2015.\n[30] E. Cambria, P. Gastaldo, F. Bisio, and R. Zunino, \u201cAn ELM-based model for affective analogical reasoning,\u201d Neurocomputing, vol. 149, pp. 443\u2013455, Feb. 2015.\n[31] A. Budanitsky and G. Hirst, \u201cEvaluating WordNet-based Measures of Lexical Semantic Relatedness,\u201d Computational Linguistics, vol. 32, no. 1, pp. 13\u201347, 2006.\n[32] J. R. Curran, \u201cEnsemble Methods for Automatic Thesaurus Extraction,\u201d in Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language - Volume 10, 2002, pp. 222\u2013229.\n[33] J. Atkinson, A. Ferreira, and E. Aravena, \u201cDiscovering implicit intention-level knowledge from natural-language texts,\u201d KnowledgeBased Systems., vol. 22, no. 7, pp. 502\u2013508, 2009.\n[34] D. S\u00e1nchez, D. Isern, and M. Millan, \u201cContent annotation for the semantic web: an automatic web-based approach,\u201d Knowledge and Information Systems, vol. 27, no. 3, pp. 393\u2013418, 2011.\n[35] M. Gaeta, F. Orciuoli, and P. Ritrovato, \u201cAdvanced ontology management system for personalised e-Learning,\u201d Knowledge-Based Systems, vol. 22, no. 4, pp. 292\u2013301, 2009.\n[36] Z. Wu and M. Palmer, \u201cVerbs Semantics and Lexical Selection,\u201d in Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 133\u2013138.\n[37] K. Jain, R. C. Dubes, and others, Algorithms for clustering data, vol. 6. Prentice hall Englewood Cliffs, 1988.\n[38] L. Du, W. L. Buntine, and M. Johnson, \u201cTopic Segmentation with a Structured Topic Model.,\u201d in HLT-NAACL, 2013, pp. 190\u2013200.\n[39] M. Riedl and C. Biemann, \u201cText segmentation with topic models,\u201d J. Lang. Technol. Computational Linguistics, vol. 27, no. 1, pp. 47\u201369, 2012.\n[40] D. Beeferman, A. Berger, and J. Lafferty, \u201cStatistical Models for Text Segmentation,\u201d Machine Learning, vol. 34, no. 1\u20133, pp. 177\u2013210, 1999.\n[41] L. Pevzner and M. A. Hearst, \u201cA Critique and Improvement of an Evaluation Metric for Text Segmentation,\u201d Computational Linguistics, vol. 28, no. 1, pp. 19\u201336, Mar. 2002.\nAPPENDIX I\nwindow sizes with \u03b1 = 0.5\nwindow sizes with \u03b1 = 0.7"}], "references": [{"title": "Text Segmentation for Efficient Information Retrieval", "author": ["F. Llopis", "A. Ferr\u00e1ndez", "J. Vicedo"], "venue": "Computational Linguistics and Intelligent Text Processing SE - 39, vol. 2276, A. Gelbukh, Ed. Springer Berlin Heidelberg, 2002, pp. 373\u2013380.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Text Segmentation Based on Document Understanding for Information Retrieval", "author": ["V. Prince", "A. Labadi\u00e9"], "venue": "Natural Language Processing and Information Systems SE - 26, vol. 4592. Springer Berlin Heidelberg, 2007, pp. 295\u2013304.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Discourse segmentation in aid of document summarization", "author": ["B.K. Boguraev", "M.S. Neff"], "venue": "System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International Conference on. IEEE, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering", "author": ["S. Tellex", "B. Katz", "J. Lin", "A. Fernandes", "G. Marton"], "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, 2003, pp. 41\u201347.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Using Text Segmentation Algorithms for the Automatic Generation of E-Learning Courses", "author": ["C. Beck", "A. Streicher", "A. Zielinski"], "venue": "Lexical and Computational Semantics ( SEM 2014), p. 132, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Linear Text Segmentation Using Affinity Propagation", "author": ["Kazantseva", "S. Szpakowicz"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2011, p. 284\u2013293.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation of Expository Texts by Hierarchical Agglomerative Clustering", "author": ["Y. Yaari"], "venue": "Proceedings of RANLP\u201997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "Multi-paragraph Segmentation of Expository Text", "author": ["M.A. Hearst"], "venue": "Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 9\u201316.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "A Dynamic Programming Model for Text Segmentation Based on Min-Max Similarity", "author": ["N. Ye", "J. Zhu", "Y. Zheng", "M. Ma", "H. Wang", "B. Zhang"], "venue": "Information Retrieval Technology SE - 14, vol. 4993, Springer Berlin Heidelberg, 2008, pp. 141\u2013152.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages", "author": ["M.A. Hearst"], "venue": "Computational linguistics., vol. 23, no. 1, pp. 33\u2013 64, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "A Statistical Model for Domainindependent Text Segmentation", "author": ["M. Utiyama", "H. Isahara"], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, 2001, pp. 499\u2013 506.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Information Retrieval by Semantic Similarity", "author": ["G. Hliaoutakis", "Angelos", "G. Varelas", "Giannis (", "E. Voutsakis", "G.M.E. Petrakis", "E. Milios"], "venue": "Proceedings of International Journal on Semantic Web and Information Systems (IJSWIS), vol. 2, no. 3, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Topic segmentation", "author": ["M. Purver"], "venue": "Proceedings of the Spoken language understanding: systems for extracting semantic information from speech, pp. 291\u2013317, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian Unsupervised Topic Segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2008, pp. 334\u2013343.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding Text Boundaries and Finding Topic Boundaries: Two Different Tasks", "author": ["Labadi\u00e9", "V. Prince"], "venue": "Advances in Natural Language Processing SE - 25, vol. 5221. Springer Berlin Heidelberg, 2008, pp. 260\u2013271.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An Automatic Method of Finding Topic Boundaries", "author": ["J.C. Reynar"], "venue": "Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 331\u2013333.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Topic Detection and Tracking Pilot Study: Final Report", "author": ["J. Allan", "J. Carbonell", "G. Doddington", "J. Yamron", "Y. Yang"], "venue": "Proceedings of the DARPA Broadcast News Transcription and Understanding Worksho, 1998, pp. 194\u2013218.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "An Iterative Approach to Text Segmentation", "author": ["F. Song", "W. Darling", "A. Duric", "F. Kroon"], "venue": "Advances in Information Retrieval SE - 63, vol. 6611. Springer Berlin Heidelberg, 2011, pp. 629\u2013640.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic segmentation of multiparty dialogue", "author": ["P.-Y. Hsueh", "J.D. Moore", "S. Renals"], "venue": "EACL, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Advances in Domain Independent Linear Text Segmentation", "author": ["F.Y.Y. Choi"], "venue": "Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, 2000, pp. 26\u201333.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Discourse Segmentation of Multi-party Conversation", "author": ["M. Galley", "K. McKeown", "E. Fosler-Lussier", "H. Jing"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, 2003, pp. 562\u2013569.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent Semantic Analysis for Text Segmentation", "author": ["F.Y.Y. Choi", "P. Wiemer-Hastings", "J. Moore"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language, 2001, pp. 109\u2013117.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Text Segmentation via Topic Modeling: An Analytical Study", "author": ["H. Misra", "F. Yvon", "J.M. Jose", "O. Cappe"], "venue": "Proceedings of the 18th ACM Conference on Information and Knowledge Management, 2009, pp. 1553\u20131556.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical Text Segmentation from Multi-scale Lexical Cohesion", "author": ["J. Eisenstein"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2009, pp. 353\u2013361.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Lexical Cohesion Computed by Thesaural Relations As an Indicator of the Structure of Text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational linguistics, vol. 17, no. 1, pp. 21\u201348, 1991.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1991}, {"title": "Linear Segmentation and Segment Significance", "author": ["M.-Y. Kan", "J.L. Klavans", "K. McKeown"], "venue": "CoRR, vol. cs.CL/9809, 1998.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["P. Resnik", "Philip"], "venue": "Journal of Artificial Intelligent. Res.(JAIR), vol. 11, pp. 95\u2013130, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "ICML, 1998, vol. 98, pp. 296\u2013304.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "AffectiveSpace 2: Enabling affective intuition for concept-level sentiment analysis", "author": ["E. Cambria", "J. Fu", "F. Bisio", "S. Poria"], "venue": "Twenty- Ninth AAAI Conference on Artificial Intelligence, pp. 508\u2013514, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "An ELM-based model for affective analogical reasoning", "author": ["E. Cambria", "P. Gastaldo", "F. Bisio", "R. Zunino"], "venue": "Neurocomputing, vol. 149, pp. 443\u2013455, Feb. 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Computational Linguistics, vol. 32, no. 1, pp. 13\u201347, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble Methods for Automatic Thesaurus Extraction", "author": ["J.R. Curran"], "venue": "Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language - Volume 10, 2002, pp. 222\u2013229.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Discovering implicit intention-level knowledge from natural-language texts", "author": ["J. Atkinson", "A. Ferreira", "E. Aravena"], "venue": "Knowledge- Based Systems., vol. 22, no. 7, pp. 502\u2013508, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Content annotation for the semantic web: an automatic web-based approach", "author": ["D. S\u00e1nchez", "D. Isern", "M. Millan"], "venue": "Knowledge and Information Systems, vol. 27, no. 3, pp. 393\u2013418, 2011.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Advanced ontology management system for personalised e-Learning", "author": ["M. Gaeta", "F. Orciuoli", "P. Ritrovato"], "venue": "Knowledge-Based Systems, vol. 22, no. 4, pp. 292\u2013301, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Verbs Semantics and Lexical Selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, 1994, pp. 133\u2013138.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1994}, {"title": "Topic Segmentation with a Structured Topic Model", "author": ["L. Du", "W.L. Buntine", "M. Johnson"], "venue": "HLT-NAACL, 2013, pp. 190\u2013200.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Text segmentation with topic models", "author": ["M. Riedl", "C. Biemann"], "venue": "J. Lang. Technol. Computational Linguistics, vol. 27, no. 1, pp. 47\u201369, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical Models for Text Segmentation", "author": ["D. Beeferman", "A. Berger", "J. Lafferty"], "venue": "Machine Learning, vol. 34, no. 1\u20133, pp. 177\u2013210, 1999.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "A Critique and Improvement of an Evaluation Metric for Text Segmentation", "author": ["L. Pevzner", "M.A. Hearst"], "venue": "Computational Linguistics, vol. 28, no. 1, pp. 19\u201336, Mar. 2002. APPENDIX I Fig. 7. The error rate of the Ontological and Lexical similarities for different window sizes with \u03b1 = 0.5  Fig. 8. The error rate of the Ontological and Lexical similarities for different window sizes with \u03b1 = 0.7 Fig. 9. The error rates for the four experiments with window = 2. Fig. 10. The error rates for the four experiments with window = 3. Fig. 11. The error rates for the four experiments with window = 4.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 3, "context": "Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "Text segmentation algorithms are widely used as an essential step in many Natural Language Processing (NLP) tasks, such as Information Retrieval [1] [2], document summarisation [3], Question answering [4] and Automatic generation of ELearning Courses [5].", "startOffset": 251, "endOffset": 254}, {"referenceID": 5, "context": "Some approaches segment text linearly [6] and others segment it hierarchically[7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Some approaches segment text linearly [6] and others segment it hierarchically[7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "TextTiling, for example, is a well-known linear text segmentation method proposed by Hearst [8] that measures the lexical similarity between text blocks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "They range from one sentence [9] to multiple sentences (paragraphs) [10].", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "They range from one sentence [9] to multiple sentences (paragraphs) [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Utiyama and Isahara [11] segmented all possible partitions using dynamic programming and used the probability distribution of the words to rank and select the best segments.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "In order to solve these limitations, the idea of semantic searches, based on the conceptual meaning of text, has been the focus of a wide body of research and many ontology-based IR systems have been developed [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 5, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 144, "endOffset": 147}, {"referenceID": 12, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 172, "endOffset": 176}, {"referenceID": 14, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 220, "endOffset": 224}, {"referenceID": 15, "context": "Various synonyms in the literature are used to refer to text segmentation such as: Linear Text Segmentation [6], Hierarchical Text Segmentation [7], Topic Segmentation [13][14], Text Boundaries or Boundary Determination [15], and Topic Boundaries [16].", "startOffset": 247, "endOffset": 251}, {"referenceID": 7, "context": "A well-known content-based approach example is TextTiling proposed by Hearst [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 16, "context": "An example of discourse-based approaches is the Hidden Markov Model (HMM) segmentation method [17] that models \u201cmarker words\u201d, or words which predict a topic change.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "[18], where they apply an iterative approach that splits text at its weakest point in terms of the lexical connectivity strength between two adjacent parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] described a supervised hierarchical topic segmentation approach that trains separate classifiers for topic and sub-topic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "On the other hand, Eisenstein and Barzilay[14] proposed a Bayesian approach to unsupervised topic segmentation.", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "TextTiling [8] and C99 [20] are also considered unsupervised linear topic segmentation algorithms.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "TextTiling [8] and C99 [20] are also considered unsupervised linear topic segmentation algorithms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "It has been argued that this sequence model is sufficient for many purposes [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "An early linear text segmentation algorithm was the TextTiling approach introduced by Hearst [10] in 1997.", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "[21] proposed LcSeg, a TextTiling-based algorithm that uses tf-idf term weights, which improves text segmentation results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Another well-known linear text segmentation algorithm is C99 introduced by Choi [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "[22] introduced another enhanced version of C99 by applying Latent concept modelling to the similarity metric.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Utiyama and Isahara [11] introduced probabilistic approaches using Dynamic Programming (DP) called U00.", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "[23] used Latent Dirichlet Allocation (LDA) topic model to linearly segment a text into semantically coherent segments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "hierarchical algorithm was proposed by Yaari [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 23, "context": "A hierarchical Bayesian algorithm based on LDA is introduced by Eisenstein [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 1, "context": "There are three main approaches to detect borderline sentences within text [2]:", "startOffset": 75, "endOffset": 78}, {"referenceID": 19, "context": "algorithm [20] for example uses a similarity matrix to generate", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "algorithm [16] is the most common example of the use of a", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "chains was first proposed by Morris and Hirst [25] to chain", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "Segmenter [26] uses this method", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "Semantic similarity has been widely used in many research fields such as: (1) Information Retrieval: to improve accuracy of current Information Retrieval techniques and semantic indexing [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 26, "context": "(2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "(2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "(2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31].", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "(2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 30, "context": "(2) Natural Language Processing tasks: there are several tasks such as word sense disambiguation [27], synonym detection [28], sentiment analysis [29], analogical reasoning for sentiment analysis [30] or automatic spelling error detection and correction [31].", "startOffset": 254, "endOffset": 258}, {"referenceID": 31, "context": "(3) Knowledge management: such as thesauri generation [32], information extraction [33], semantic annotation [34] and ontology merging and learning [35], in which new concepts should be discovered or acquired from text in order to relate them to already existing ones.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "(3) Knowledge management: such as thesauri generation [32], information extraction [33], semantic annotation [34] and ontology merging and learning [35], in which new concepts should be discovered or acquired from text in order to relate them to already existing ones.", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "(3) Knowledge management: such as thesauri generation [32], information extraction [33], semantic annotation [34] and ontology merging and learning [35], in which new concepts should be discovered or acquired from text in order to relate them to already existing ones.", "startOffset": 109, "endOffset": 113}, {"referenceID": 34, "context": "(3) Knowledge management: such as thesauri generation [32], information extraction [33], semantic annotation [34] and ontology merging and learning [35], in which new concepts should be discovered or acquired from text in order to relate them to already existing ones.", "startOffset": 148, "endOffset": 152}, {"referenceID": 35, "context": "corresponding ontological nodes via is-a links [36].", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "In this research, we rely on an Edge-counting approach proposed by Wu and Palmer [36] as its performance is deemed better than other methods [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "In this research, we rely on an Edge-counting approach proposed by Wu and Palmer [36] as its performance is deemed better than other methods [28].", "startOffset": 141, "endOffset": 145}, {"referenceID": 35, "context": "Out of these, we choose the measure proposed by Wu and Palmer [36] because it has shown performance improvements over other methods [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Out of these, we choose the measure proposed by Wu and Palmer [36] because it has shown performance improvements over other methods [28].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "2) Lexical similarity: Lexical similarity has been used widely in the literature in text segmentation [8], [20], and as its name suggests, it splits text into segments that are lexically coherent.", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "2) Lexical similarity: Lexical similarity has been used widely in the literature in text segmentation [8], [20], and as its name suggests, it splits text into segments that are lexically coherent.", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "The agglomerative clustering technique could be transferred from document level into text level, where the clustering process is done between text blocks, within a document (as opposed to across whole documents) [7].", "startOffset": 212, "endOffset": 215}, {"referenceID": 19, "context": "In this experiment, we evaluated the efficiency of our approach on Choi\u2019s dataset [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 36, "context": "This dataset has been widely used in linear text segmentation evaluation [38][39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "This dataset has been widely used in linear text segmentation evaluation [38][39].", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "Yaari [7] used paragraphs as the elementary blocks for his segmentation algorithm and affirms that the size of a paragraph, as opposed", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "[8] measured the cosine similarity between text blocks where text blocks are consisting of fixed number of sentences (window).", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "[40] as a measure that expresses a probability of segmentation error.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "WindowDiff [41] is stricter as it not only decides whether there is a mismatch between the hypothesized partition and the reference partition, it also counts the difference of the number of segment boundaries in the given window between the two partitions.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Examples of such approaches are: TextTiling [8], C99 [20], and U00 [11].", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Examples of such approaches are: TextTiling [8], C99 [20], and U00 [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "Examples of such approaches are: TextTiling [8], C99 [20], and U00 [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "5 The results were brought from Utiyama and Isahara [11] & Riedl and Biemann [39] papers.", "startOffset": 52, "endOffset": 56}, {"referenceID": 37, "context": "5 The results were brought from Utiyama and Isahara [11] & Riedl and Biemann [39] papers.", "startOffset": 77, "endOffset": 81}], "year": 2015, "abstractText": "Text segmentation (TS) aims at dividing long text into coherent segments which reflect the subtopic structure of the text. It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation. Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words. Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents. Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks. This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured. The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity. The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising. Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality. Keywords\u2014Text Segmentation; Ontological similarity; Lexical Cohesion; Vector Space Model", "creator": "Microsoft\u00ae Word 2013"}}}