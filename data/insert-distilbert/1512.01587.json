{"id": "1512.01587", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text", "abstract": "we advance the state of the art in biomolecular interaction extraction with three significant contributions : ( i ) we show that deep, objective abstract meaning representations ( amr ) significantly improve in the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface - and syntax - based features ; ( ii ) in fine contrast with previous approaches that infer relations on a sentence - by - sentence basis, we expand our framework to enable consistent predictions over sets of sentences ( documents ) ; ( iii ) we shall further modify and expand a graph dynamic kernel learning analytics framework to enable concurrent exploitation of automatically induced amr ( semantic ) and dependency temporal structure ( syntactic ) representations. our experiments show that our approach yields interaction extraction systems specifically that are more robust in environments where generally there is a significant mismatch between training and test conditions.", "histories": [["v1", "Fri, 4 Dec 2015 22:58:29 GMT  (356kb,D)", "http://arxiv.org/abs/1512.01587v1", "Appearing in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)"]], "COMMENTS": "Appearing in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR cs.IT cs.LG math.IT", "authors": ["sahil garg", "aram galstyan", "ulf hermjakob", "daniel marcu"], "accepted": true, "id": "1512.01587"}, "pdf": {"name": "1512.01587.pdf", "metadata": {"source": "CRF", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text", "authors": ["Sahil Garg", "Aram Galstyan", "Ulf Hermjakob"], "emails": ["marcu}@isi.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\n1 Recent advances in genomics and proteomics have significantly accelerated the rate of uncovering and accumulating new biomedical knowledge. Most of this knowledge is available only via scientific publications, which necessitates the development of automated and semi-automated tools for extracting useful biomedical information from unstructured text. In particular, there has been a significant body of research on identifying biological entities (proteins, genes, chemical compounds) and interactions between those entities from bio-medical papers [22, 19, 35, 6]. Despite the recent progress, current methods for biomedical knowledge extraction suffer from a number of important shortcomings. First of all, existing methods rely heavily on shallow analysis techniques that severely limit their scope. For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12]. Second, most existing methods focus on single-sentence extraction, which makes them very susceptible to noise. And finally, owing to the enormous diversity of research topics in biomedical literature and the high cost of data annotation, there is often significant mismatch between training and testing corpora, which reflects poorly on generalization ability of existing methods [35].\nIn this paper, we present a novel algorithm for extracting biomolecular interactions from unstructured text that addresses the above challenges. Contrary to the previous works, the extraction task considered here is less restricted and spans a much more diverse corpus of biomedical articles. These more realistic settings present some important technical problems for which we provide explicit solutions.\nOur specific contributions are as follows:\n\u2022 We propose a graph-kernel based algorithm for extracting biomolecular interactions from Abstract Meaning Representation, or AMR. To the best of our knowledge, this is the first attempt of using deep semantic parsing for biomedical knowledge extraction task. \u2022 We provide a multi-sentence generalization of the algorithm by defining Graph Distribution Kernels (GDK), which enables us to perform document-level extraction. \u2022 We suggest a hybrid extraction method that utilizes both AMRs and syntactic parses given by Stanford Dependency Graphs (SDGs). Toward this goal, we develop a linear algebraic formulation for learning vector space embedding of edge labels in AMRs and SDGs to define similarity measures between AMRs and SDGs.\nWe conduct an exhaustive empirical evaluation of the proposed extraction system on 45+ research articles on cancer (approximately 3k sentences), containing approximately 20,000 positive-negative labeled biomolecular interactions2. Our results indicate that the joint extraction method that leverages both AMRs and SDGs parses significantly improves the extraction accuracy, and is more robust to mismatch between training and test conditions.\n1This work has been published previously [15]. 2The code and the data are available at https://github.com/sgarg87/big mech isi gg\nar X\niv :1\n51 2.\n01 58\n7v 1\n[ cs\n.C L\n] 4\nD ec\n2 01\n5"}, {"heading": "II. PROBLEM STATEMENT", "text": "Consider the sentence \u201cAs a result, mutant Ras proteins accumulate with elevated GTP-bound proportion\u201d, which describes a \u201cbinding\u201d interaction between a protein \u201cRas\u201d and a small-molecule \u201cGTP\u201d. We want to extract this interaction.\nIn our representation, which is motivated by BioPAX [12], an interaction refers to either i) an entity effecting state change of another entity; or ii) an entity binding/dissociating with another entity to form/break a complex while, optionally, also influenced by a third entity. An entity can be of any type existent in a bio pathway, such as protein, complex, enzyme, etc, although here we refer to an entity of all valid types simply as a protein. The change in state of an entity or binding type is simply termed as \u201cinteraction type\u201d in this work. In some cases, entities are capable of changing their state on their own or bind to an instance of its own (self-interaction). Such special cases are also included. Some examples of interaction types are shown in Table I.\nBelow we describe our approach for extracting above-defined interactions from natural language parses of sentences in a research document."}, {"heading": "III. EXTRACTING INTERACTIONS FROM AN AMR", "text": ""}, {"heading": "A. AMR Biomedical Corpus", "text": "Abstract Meaning Representation, or AMR, is a semantic annotation of single/multiple sentences [3]. In contrast to syntactic parses, in AMR, entities are identified, typed and their semantic roles are annotated. AMR maps different syntactic constructs to same conceptual term. For instance, \u201cbinding\u201d, \u201cbound\u201d, \u201dbond\u201d correspond to the same concept \u201cbind-01\u201d. Because one AMR representation subsumes multiple syntactic representations, we hypothesize that AMRs have higher utility for extracting biomedical interactions.\nWe trained an English-to-AMR parser [31] on two manually annotated corpora: i) a corpus of 17k general domain sentences including newswire and web text as published by the Linguistic Data Consortium; and ii) 3.4k systems biology sentences, including in-domain PubMedCentral papers and the BEL BioCreative corpus. As part of building the bio-specific AMR corpus, we extended the PropBank-based framesets used in AMR by 45 bio-specific frames such as \u201cphosphorylate-01\u201d, \u201cimmunoblot01\u201d and extended the list of AMR standard named entities by 15 types such as \u201cenzyme\u201d, \u201cpathway\u201d. It is important to note that these extensions are not specific to biomolecular interactions, and cover more general cancer biology concepts."}, {"heading": "B. Extracting Interactions", "text": "Fig. 1 depicts a manual AMR annotation of a sentence, which has two highlighted entity nodes with labels \u201cRAS\u201d and \u201cGTP\u201d. These nodes also have entity type annotations, \u201cenzyme\u201d and \u201csmall-molecule\u201d respectively; the concept node with a node label \u201cbind-01\u201d corresponds to an interaction type \u201cbinding\u201d (from the \u201cGTP-bound\u201d in the text). The interaction \u201cRAS-binds-GTP\u201d is extracted from the highlighted subgraph under the \u201cbind\u201d node. In the subgraph, relationship between the interaction node \u201cbind-01\u201d and the entity nodes, \u201cRas\u201d and \u201cGTP\u201d, is defined through two edges with edge labels \u201cARG1\u201d and \u201cARG2\u201d respectively. Additionally, in the subgraph, we assign roles \u201cinteraction-type\u201d, \u201cprotein\u201d, \u201cprotein\u201d to the nodes \u201cbind-01\u201d, \u201cRas\u201d, \u201cGTP\u201d respectively (roles presented with different colors in the subgraph).\nGiven an AMR graph, as in Fig. 1, we first identify potential entity nodes (proteins, molecules, etc) and interaction nodes (bind, activate, etc). Next, we consider all permutations to generate a set of potential interactions according to the format defined above. For each candidate interaction, we extract the corresponding shortest path subgraph. We then project the subgraph to a tree structure 3 with the interaction node as root and also possibly the protein nodes (entities involved in the interaction) as leaves.\nOur training set consists of tuples {Gai , Ii, li}ni=1, where Gai is an AMR subgraph constructed such that it can represent an extracted candidate interaction Ii with interaction node as root and proteins nodes as leaves typically; and l = {0, 1} is a binary label indicating whether this subgraph contains Ii or not. Given a training set, and a new sample AMR subgraph Ga\u2217 for interaction I\u2217, we would like to infer whether I\u2217 is valid or not. We address this problem by developing a graph-kernel based approach."}, {"heading": "C. Semantic Embedding Based Graph Kernel", "text": "We propose an extension of the contiguous subtree kernel [38, 11] for mapping the extracted subgraphs (tree structure) to an implicit feature space. Originally, this kernel uses an identity function on two node labels when calculating the similarity between those two nodes. We instead propose to use vector space embedding of the node labels [10, 26], and then define a sparse RBF kernel on the node label vectors. Similar extensions of convolution kernels have been been suggested previously[25, 33].\nConsider two graphs Gi and Gj rooted at nodes Gi.r and Gj .r, respectively, and let Gi.c and Gj .c be the children nodes of the corresponding root nodes. Then the kernel between Gi and Gj is defined as follows:\nK(Gi, Gj) = { 0 if k(i, j) = 0 k(i, j) +Kc(Gi.c, Gj .c) otherwise ,\nwhere k(i, j) \u2261 k(Gi.r, Gj .r) is the similarity between the root nodes, whereas Kc(Gi.c, Gj .c) is the recursive part of the kernel that measures the similarity of the children subgraphs. Furthermore, the similarity between root nodes x and y is defined as follows:\nk(x, y) = kw(x, y) 2(kw(x, y) 2 + ke(x, y) + kr(x, y))\nkw(x, y) = exp\n( (wTxwy \u2212 1)\n\u03b2\n)( (wTxwy \u2212 \u03b1)\n(1\u2212 \u03b1) ) +\nke(x, y) = I(ex = ey), kr(x, y) = I(rx = ry) .\n(1)\nHere (\u00b7)+ denotes the positive part; I(\u00b7) is the indicator function; wx,wy are unit vector embeddings of node labels 4; ex, ey represent edge labels (label of an edge from a node\u2019s parent to it is the node\u2019s edge label); rx, ry are roles of nodes (such as protein, catalyst, concept, interaction-type); \u03b1 is a threshold parameter on the cosine similarity (wTxwy) to control sparsity [16]; and \u03b2 is the bandwidth.\nThe recursive part of the kernel, Kc, is defined as follows: Kc(Gi.c, Gj .c) = \u2211\ni,j:l(i)=l(j)\n\u03bbl(i) \u2211\ns=1,\u00b7\u00b7\u00b7 ,l(i)\nK(Gi[i[s]], Gj [j[s]]) \u220f\ns=1,\u00b7\u00b7\u00b7 ,l(i)\nk(Gi[i[s]].r, Gj [j[s]].r),\nwhere i, j are contiguous children subsequences under the respective root nodes Gi.r, Gj .r; \u03bb \u2208 (0, 1) is a tuning parameter; and l(i) is the length of sequence i = i1, \u00b7 \u00b7 \u00b7 , il; Gi[i[s]] is a sub-tree rooted at i[s] index child node of Gi.r. Here, we propose to sort children of a node based on the corresponding edge labels. This helps in distinguishing between two mirror image trees.\nThis extension is a valid kernel function (Zelenko et al.). Next, we generalize the dynamic programming approach of Zelenko et al. for efficient calculation of this extended kernel.\n3This can be done via so called inverse edge labels; see [3, section 3]. 4Learned using word2vec software [26] on over one million PubMed articles.\n1) Dynamic programming for computing convolution graph kernel: In the convolution kernel presented above, the main computational cost is due to comparison of children sub-sequences. Since different children sub-sequences of a given root node partially overlap with each other, one can use dynamic programming to avoid redundant computations, thus reducing the cost. Toward this goal, we use the following decomposition of the kernel Kc :\nKc(Gi.c, Gj .c) = \u2211 p,q Cp,q ,\nwhere Cp,q refers to the similarity between sub-sequences starting at indices p, q respectively in Gi.c and Gj .c. To calculate Cp,q via dynamic programming, let us introduce\nLp,q = max l ( l\u220f s=0 k(Gi[i[p+ s]].r, Gj [j[q + s]].r) 6= 0 ) .\nFurthermore, let us denote kp,q = k(Gi[i[p]].r, Gj [j[q]].r), and Kp,q = K(Gi[i[p]], Gj [j[q]]). We then evaluate Cp,q in a recursive manner using the following equations.\nLp,q = { 0 if kp,q = 0 Lp+1,q+1 + 1 otherwise\n(2)\nCp,q = { 0 if kp,q = 0 \u03bb(1\u2212\u03bbL(p,q))\n1\u2212\u03bb Kp,qkp,q + \u03bbCp+1,q+1 otherwise (3)\nLm+1,n+1 = 0, Lm+1,n = 0, Lm,n+1 = 0\nCm+1,n+1 = 0, Cm+1,n = 0, Cm,n+1 = 0 , (4)\nwhere m,n are number of children under the root nodes Gi.r and Gj .r respectively. Note that for graphs with cycles, the above dynamic program can be transformed into a linear program. There are a couple of practical considerations during the kernel computations. First of all, the kernel depends on two tunable parameters \u03bb and \u03b1. Intuitively, decreasing \u03bb discounts the contributions of longer child sub-sequences. The parameter \u03b1, on the other hand, controls the tradeoff between computational cost and accuracy. Based on some prior tuning we found that our results are not very sensitive to the parameters. In the experiments below we set \u03bb = 0.99 and \u03b1 = 0.4. Also, consistent with previous studies, we normalize the graph kernel (e.g., kernel similarity K(Gi, Gj) is divided by the normalization term\u221a K(Gi, Gi)K(Gj , Gj)) to increase accuracy."}, {"heading": "IV. GRAPH DISTRIBUTION KERNEL- GDK", "text": "Often an interaction is mentioned more than once in the same research paper, which justifies a document-level extraction, where one combines evidence from multiple sentences. The prevailing approach to document-level extraction is to first perform inference at sentence level, and then combine those inferences using some type of an aggregation function for a final documentlevel inference [32, 7]. For instance, in [7], the inference with the maximum score is chosen. We term this baseline approach as \u201cMaximum Score Inference\u201d, or MSI. Here we advocate a different approach, where one uses the evidences from multiple sentences jointly, for a collective inference.\nLet us assume an interaction Im is supported by km sentences, and let {Gm1, \u00b7 \u00b7 \u00b7 , Gmkm} be the set of relevant AMR subgraphs extracted from those sentences. We can view the elements of this set as samples from some distribution over the graphs, which, with a slight abuse of notation, we denote as Gm. Consider now interactions I1, \u00b7 \u00b7 \u00b7 , Ip, and let G1, \u00b7 \u00b7 \u00b7 ,Gp be graph distributions representing these interactions.\nThe graph distribution kernel (GDK), K(Gi,Gj), for a pair Gi,Gj is defined as follows:\nK(Gi,Gj) = exp(\u2212Dmm(Gi,Gj)); Dmm(Gi,Gj) = ki\u2211\nr,s=1\nK(Gir, Gis)\nk2i + kj\u2211 r,s=1 K(Gjr, Gjs) k2j \u2212 2 ki,kj\u2211 r,s=1 K(Gir, Gjs) kikj\nHere Dmm is the Maximum Mean Discrepancy (MMD), a valid l2 norm, between a pair of distributions Gi,Gj [17]; K(., .) is the graph kernel defined in Section III-C (though, not restricted to this specific kernel). As the term suggests, maximum mean discrepancy represents the discrepancy between the mean of graph kernel features (features implied by kernels) in samples of distributions Gi and Gj . Now, since Dmm is the l2 norm on the mean feature vectors, K(Gp,Gq) is a valid kernel function.\nWe note that MMD metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5]. For our purpose, we prefer using this divergence metric over others (such as KL-D divergence) for the following reasons: i) Dmm(., .) is a \u201ckernel trick\u201d based formulation, nicely fitting with our settings since we do not have explicit features representation of the graphs but only kernel density on the graph samples. Same is true for KL-D estimation with kernel density method. ii) Empirical estimate of Dmm(., .) is a valid l2 norm distance. Therefore, it is straightforward to derive the graph distribution kernel K(Gi,Gj) from Dmm(Gi,Gj) using a function such as RBF. This is not true for divergence metrics such as KL-D, Renyi [34]; iii) It is suitable for compactly supported distributions (small number of samples) whereas methods, such as k-nearest neighbor estimation of KL-D, are not suitable if the number of samples in a distribution is too small [36]; iv) We have seen the most consistent results in our extraction experiments using this metric as opposed to the others.\nFor the above mentioned reasons, here we focus on MMD as our primary metric for computing similarities between graph distributions. The proposed GDK framework, however, is very general and not limited to a specific metric. Next, we briefly describe two other metrics that can be used with GDK.\na) GDK with Kullback-Leibler divergence: While MMD represents maximum discrepancy between the mean features of two distributions, the Kullback-Leibler divergence (KL-D) is a more comprehensive (and fundamental) measure of distance between two distributions5. For defining kernel KKL in terms of KL-D, however, we have two challenges. First of all, KL-D is not a symmetric function. This problem can be addressed by using a symmetric version of the distance in the RBF kernel,\nKKL(Gi,Gj) = exp(\u2212[DKL(Gi||Gj) +DKL(Gj ||Gi)])\nwhere DKL(Gi||Gj) is the KL distance of the distribution Gi w.r.t. the distribution Gj . And second, even the symmetric combination of the divergences is not a valid Euclidian distance. Hence, KKL is not guaranteed to be a positive semi-definite function. This issue can be dealt in a practical manner as nicely discussed in [34]. Namely, having computed the Gram matrix using KKL, we can project it onto a positive semi-definite one by using linear algebraic techniques, e.g., by discarding negative eigenvalues from the spectrum.\nSince we do not know the true divergence, we approximate it with its empirical estimate from the data, DKL(Gi||Gj) \u2248 D\u0302KL(Gi||Gj). While there are different approaches for estimating divergences from samples [36], here we use kernel density estimator as shown below:\nD\u0302KL(Gi||Gj) = 1\nki ki\u2211 r=1 log 1 ki\n\u2211ki s=1K(Gir, Gis)\n1 kj \u2211kj s=1K(Gir, Gjs)\nb) GDK with cross kernels: Another simple way to evaluate similarity between two distributions is to take the mean of cross-kernel similarities between the corresponding two sample sets:\nK(Gi,Gj) = ki,kj\u2211 r,s=1 K(Gir, Gjs) kikj\nNote that this metric looks quite similar to the MMD. As demonstrated in our experiments, however, MMD does better, presumably because it accounts for the mean kernel similarity between samples of the same distribution.\nHaving defined the graph distribution kernel-GDK, K(., .), our revised training set consists of tuples {Gi, Ii, li}ni=1 with Gai1, \u00b7 \u00b7 \u00b7 , Gaiki sample sub-graphs in Gi. For inferring an interaction I\u2217, we evaluate GDK between a test distribution G\u2217 and the train distributions {G1, \u00b7 \u00b7 \u00b7 ,Gn}, from their corresponding sample sets. Then, one can apply any \u201ckernel trick\u201d based classifier."}, {"heading": "V. CROSS REPRESENTATION SIMILARITY", "text": "In the previous section, we proposed a novel algorithm for document-level extraction of interactions from AMRs. Looking forward, we will see in our experiments (Section VI) that AMRs yield better extraction accuracy compared to SDGs. This result suggests that using deep semantic features is very useful for the extraction task. On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9]. Thus, one can ask whether the joint use of semantic (AMRs) and syntactic (SDGs) parses can improve extraction accuracy further.\nThere are some intuitive observations that justify the joint approach: i) shallow syntactic parses may be sufficient for correctly extracting a subset of interactions; ii) semantic parsers might make mistakes that are avoidable in syntactic ones. For instance, in machine translation based semantic parsers [31, 2], hallucinating phrasal translations may introduce an interaction/protein in a parse that is non-existent in true semantics; iii) over fit of syntactic/semantic parsers can vary from each other in a test corpus depending upon the data used in their independent trainings.\n5Recall that the KL divergence between distributions p and q is defined as DKL(p||q) = Ep(x)[log p(x) q(x) ]\nAbstract Meaning Representation\n(a / activate-01 :ARG0 (s / protein :name (n1 / name :op1 \"RAS\")) :ARG1 (s / protein :name (n2 / name :op1 \"B-RAF\"))\nStanford Typed Dependency\nnsubj(activates-2, RAS-1) root(ROOT-0, activates-2) acomp(activates-2, B-RAF-3)\nFig. 2. AMR and SDG parses of \u201cRAS activates B-RAF.\u201d\nIn this setting, in each evidence sentence, a candidate interaction Ii is represented by a tuple \u03a3i = {Gai , Gsi} of sub-graphs Gai and G s i which are constructed from AMR and SDG parses of a sentence respectively. Our problem is to classify the interaction jointly on features of both sub-graphs. This can be further extended for the use of multiple evidence sentences. We now argue that the graph-kernel framework outlined above can be applied to this setting as well, with some modifications.\nLet \u03a3i and \u03a3j be two sets of points. To apply the framework above, we need a valid kernel K(\u03a3i,\u03a3j) defined on the joint space. One way of defining this kernel would be using similarity measures between AMRs and SDGs separately, and then combining them e.g., via linear combination. However, here we advocate a different approach, where we flatten the joint representation. Each candidate interaction is represented as a set of two points in the same space. This projection is a valid operation as long as we have a similarity measure between Gai and G s i (correlation between the two original dimensions). This is rather problematic since AMRs and SDGs have non-overlapping edge labels (although the space of node labels of both representations coincide). To address this issue, for inducing this similarity measure, we next develop our approach for edge-label vector space embedding.\nLet us understand what we mean by vector space embedding of edge-labels. In Fig. 2, we have an AMR and a SDG parse of \u201cRAS activates B-RAF\u201d. \u201cARG0\u201d in the AMR and \u201cnsubj\u201d in SDG are conveying that \u201cRAS\u201d is a catalyst of the interaction \u201cactivation\u201d; \u201cARG1\u201d and \u201cacomp\u201d are meaning that \u201cB-RAF\u201d is activated. In this sentence, \u201cARG0\u201d and \u201cnsubj\u201d are playing the same role though their higher dimensional roles, across a diversity set of sentences, would vary. Along these lines, we propose to embed these high dimensional roles in a vector space, termed as \u201cedge label vectors\u201d."}, {"heading": "A. Consistency Equations for Edge Vectors", "text": "We now describe our unsupervised algorithm that learns vector space embedding of edge labels. The algorithm works by imposing linear consistency conditions on the word vector embeddings of node labels. While we describe the algorithm using AMRs, it is directly applicable to SDGs as well.\n1) Linear algebraic formulation: In our formulation, we first learn subspace embedding of edge labels (edge label matrices) and then transform it into vectors by flattening. Let us see the AMR in Fig. 2 again. We already have word vectors embedding for terms \u201cactivate\u201d, \u201cRAS\u201d, \u201cB-RAF\u201d, denoted as wactivate, wras, wbraf respectively; a word vector wi \u2208 Rm\u00d71. Let embedding for edge labels \u201cARG0\u201d and \u201cARG1\u201d be Aarg0, Aarg1; Ai \u2208 Rm\u00d7m. In this AMR, we define following linear algebraic equations.\nwactivate = A T arg0wras,wactivate = A T arg1wbraf ATarg0wras = A T arg1wbraf\nThe edge label matrices ATarg0, A T arg1 are linear transformations on the word vectors wras, wbraf , establishing linear consistencies between the word vectors along the edges. One can define such a set of equations in each parent-children nodes sub-graph in a given set of manually annotated AMRs (and so applies to SDGs independent of AMRs). Along these lines, for a pair of edge labels i, j in AMRs, we have generalized equations as below.\nY i = XiAi, Y j = XjAj , Z ij i Ai = Z ij j Aj\nHere Ai,Aj are edge labels matrices. Considering ni occurrences of edge labels i, we correspondingly have word vectors from the ni child node labels stacked as rows in matrix Xi \u2208 Rni\u00d7m; and Y i \u2208 Rni\u00d7m from the parent node labels. There would be a subset of instances, nij <= ni, nj where edge labels i and j has same parent node (occurrence of pairwise relationship between i and j). This gives Ziji \u2208 Rnij\u00d7m and Z ij j \u2208 Rnij\u00d7m, subsets of word vectors in Xi and Xj respectively (along rows). Along these lines, neighborhood of edge label i is defined to be: N (i) : j \u2208 N (i) s.t. nij > 0. From the above pairwise\nlinear consistencies, we derive linear dependencies of an Ai with its neighbors Aj : j \u2208 N (i), while also applying least square approximation.\nXTi Y i + \u2211\nj\u2208N (i)\nZiji T Zijj Aj = (X T i Xi + \u2211 j\u2208N (i) Ziji T Ziji )Ai\nExploiting the block structure in the linear program, we propose an algorithm that is a variant of \u201cGauss-Seidel\u201d method [13, 28].\nAlgorithm 1. (a) Initialize: A\n(0) i = (X T i Xi) \u22121XTi Y i.\n(b) Iteratively update A(t+1)i until convergence:\nA (t+1) i = XTi Xi + \u2211 j\u2208N (i) Ziji T Ziji \u22121 XTi Y i + \u2211 j\u2208N (i) Ziji T Zijj A (t) j  (c) Set the inverse edge label matrices:\nAiinv = A \u22121 i .\nTheorem 6.2 in [13][p. 287, chapter 6] states that the Gauss-Seidel method converges if the linear transformation matrix in a linear program is strictly row diagonal dominant [28]. In our formulation, diagonal blocks dominate the non-diagonal ones row-wise. Thus, Algorithm 1 should converge to an optimum.\nUsing Algorithm 1, we learned edge label matrices in AMRs and SDGs independently on corresponding AMRs and SDGs annotations from 2500 bio-sentences (high accuracy auto-parse for SDGs). Convergence was fast for both AMRs and SDGs (log error drops from 10.14 to 10.02 for AMRs, and from 30 to approx. 10 for SDGs).\nNext, we flatten an edge label matrix Ai \u2208 Rm\u00d7m to a corresponding edge label vector 6 ei \u2208 Rm 2\u00d71, and then redefine\nke(x, y) in (1) using the sparse RBF kernel.\nke(x, y) = exp\n( eTx ey \u2212 1\n\u03b2\n)( eTx ey \u2212 \u03b1\n1\u2212 \u03b1 ) +\nThis redefinition enables to define kernel similarity between AMRs and SDGs. One can either use our original formulation where a single AMR/SDG sub-graph is classified using training sub-graphs from both AMRs and SDGs, and then the inference with maximum score-MSI [7] is chosen. Another option, preferable, is to consider the set {Gai , Gsi} as samples of a graph distribution Gi representing an interaction Ii. Generalizing it further, Gi has samples set {Gai1, \u00b7 \u00b7 \u00b7 , Gaikai , G s i1, \u00b7 \u00b7 \u00b7 , Gsiksi }, containing kai , k s i number of sub-graphs in AMRs and SDGs respectively from multiple sentences in a document, all for classifying Ii. With this graph distribution representation, we can apply our GDK from Section IV and then infer using a \u201ckernel trick\u201d based classifier. This final formulation gives the best results in our experiments discussed next."}, {"heading": "VI. EXPERIMENTAL EVALUATION", "text": "We evaluated the proposed algorithm on two data sets."}, {"heading": "A. Data Sets", "text": "1) PubMed45: This dataset has 400 manual and 3k auto parses of AMRs (and 3.4k auto parses of SDGs)7; AMRs auto-parses are from 45 PubMed articles on cancer. From the 3.4k AMRs, we extract 25k subgraphs representing 20k interactions (valid/invalid); same applies to SDGs. This is our primary data for the evaluation.\nWe found that for both AMR and SGD based methods, a part of the extraction error can be attributed to poor recognition of named entities. To minimize this effect, and to isolate errors that are specific to the extraction methods themselves, we follow the footsteps of the previous studies, and take a filtered subset of the interactions (approx. 10k out of 20k). We refer to this data subset as \u201cPubMed45\u201d and the super set as \u201cPubMed45-ERN\u201d (for entity recognition noise).\n2) AIMed: This is a publicly available dataset8, which contains about 2000 sentences from 225 abstracts. In contrast to PubMed45, this dataset is very limited as it describes only whether a given pair of proteins interact or not, without specifying the interaction type. Nevertheless, we find it useful to include this dataset in our evaluation since it enables us to compare our results with other reported methods.\n6alternatives for kernel directly on the matrices instead of the flattening can be more accurate, that we plan to explore in the future 7not the same 2.5k sentences used in learning edge label vectors 8http://corpora.informatik.hu-berlin.de"}, {"heading": "B. Evaluation settings", "text": "In a typical evaluation scenario, validation is performed by random sub-sampling of labeled interactions (at sentence level) for a test subset, and using the rest as a training set. This sentence-level validation approach is not always appropriate for extracting protein interactions [35], since interactions from a single/multiple sentences in a document can be correlated. Such correlations can lead to information leakage between training and test sets (artificial match, not encountered in real settings). For instance, in [27], the reported F1 score from the random validation in the AIMed data is approx. 0.5. Our algorithm, even using SDGs, gives 0.66 F1 score in those settings. However, the performance drops significantly when an independent test document is processed. Therefore, for a realistic evaluation, we divide data sets at documents level into approx. 10 subsets such that there is minimal match between a subset, chosen as test set, and the rest of sub sets used for training a kernel classifier. In the PubMed45 data sets, the 45 articles are clustered into 11 subsets by clustering PubMed-Ids (training data also includes gold annotations). In AIMed, abstracts are clustered into 10 subsets on abstract-ids. In each of 25 independent test runs (5 for AIMed data) on a single test subset, 80% interactions are randomly sub sampled from the test subset and same percent from the train data.\nFor the classification, we use the LIBSVM implementation of Kernel Support Vector Machines [8] with the sklearn python wrapper 9. Specifically, we used settings { probability = True, C = 1, class weight = auto}. In our data, we have a class \u201cswap\u201d in addition to the two binary classes (\u201cvalid\u201d, \u201cinvalid\u201d). The \u201cswap\u201d class means that an interaction is invalid as such but swapping of entity roles in the interaction makes it valid. For the analysis purpose however, we focus on F1 scores only for the positive class, i.e. class \u201cvalid\u201d."}, {"heading": "C. Evaluation Results", "text": "We categorize all methods evaluated below as follows: i) Sentence Level Inference-SLI 10; ii) document level using Maximum Score Inference-MSI [7]; and iii) document-level inference on all the subgraphs using our Graph Distribution Kernel (GDK). In each of the categories, AMRs, SDGs are used independently, and then jointly. Edge label vectors are used only when AMRs and SDGs are jointly used, referred as \u201cAMR-SDG\u201d.\nTable II shows the F1 score statistics for all the experiments. In addition, the mean of precision and recall values are presented as (precision, recall) tuples in the same table. For most of the following discussion, we focus on F1 scores only to keep the exposition simple.\n9http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html 10Note that even for the sentence level inference, the training/test division is done on document level.\nBefore going into detailed discussion of the results, we make the following two observations. First, we can see that, in all methods (including our GDK and baselines), we obtain much better accuracy using AMRs compared to SDGs. This result is remarkable, especially taking into account the fact that the accuracy of semantic parsing is still significantly lower when compared to syntactic parsing. And second, observe that the overall accuracy numbers are considerably lower for the PubMed45-ERN data, compared to the filtered data PubMed45.\nLet us focus on document-level extraction using MSI. We do not see much improvement in numbers compared to SLI for our PubMed45 data. On the other hand, even this simple MSI technique works for the restricted extraction settings in the AIMed data. MSI works for AIMed data probably because there are multiple sub-graph evidences with varying interaction types (root node in subgraphs), even in a single sentence, all representing same protein-protein pair interaction. This high number of evidences at document level, should give a boost in performance even using MSI.\nNext, we consider document-level extraction using the proposed GDK method with the MMD metric. Comparing against the baseline SLI, we see a significant improvement for all data sets and in both AMRs and SDGs (although the improvement in PubMed45-ERN is relatively small). The effect of the noise in entity recognition can be a possible reason why GDK does not work so well in this data compared to the other two data sets. Here, we also see that: a) GDK method performs better than the document level baseline MSI; and b) AMRs perform better than SDGs with GDK method also.\nLet us now consider the results of extraction using both AMRs and SDGs jointly. Here we evaluate MSI and GDK, both using our edge label vectors. Our primary observation here is that the joint inference using both AMRs and SDGs improves the extraction accuracy across all datasets. Furthremore, in both PubMed45 datasets, the proposed GDK method is a more suitable choice for the joint inference on AMRs and SDGs. As we can see, comparing to GDK for AMRs only, F1 points increment from 0.35 to 0.38 for the PubMed45-ERN data, and from 0.51 to 0.57 for the PubMed45 data. For the AIMed dataset, on the other hand, the best result (F1 score of 0.55) is obtained when one uses the baseline MSI for the joint inference on AMRs and SDGs.\nTo get more insights, we now consider (mean-precision, mean-recall) tuples shown in the Table II. The general trend is that the AMRs lead to higher recall compared to the SDGs. In the PubMed45-ERN data set, this increase in the recall is at cost of a drop in the precision values. Since the entity types are noisy in this data set, this drop in the precision numbers is not completely surprising (note that the F1 scores still increase). With the use of the GDK method in the same data set, however, the precision drop (SDGs to AMRs) becomes negligible, while the recall still increases significantly. In the data set PubMed45 (the one without noise in the entity types), both the precision and recall are generally higher for the AMRs compared to the SDGs. Again, there is an exception for the GDK approach, for which the recall decreases slightly. However, the corresponding precision almost doubles.\nFor a more fine-grained comparison between the methods, we plot F1 score for each individual test set in Fig. 3. Here, we compare the baselines, \u201cAMR (MSI)\u201d, \u201cSDG (MSI)\u201d against the \u201cAMR-SDG (GDK)\u201d in our data (and, \u201cAMR-SDG (MSI)\u201d for \u201cAIMed\u201d). We see a general trend, across all test subsets, of AMRs being more accurate than SDGs and the joint use of two improving even upon AMRs. Though, there are some exceptions where the difference is marginal between the three. In our cross checking, we find that such exceptions are when there is relatively more information leakage between train-test, i.e. less train-test divergence. We use Maximum Mean Discrepancy-MMD for evaluating this train-test divergence (originally used for defining GDK in Section IV. We find that our GDK technique is more suitable when MMD > 0.01 (MMD is normalized metric for a normalized graph kernel).\nThe results for the GDK method described above are specific to the MMD metric. We also evaluated GDK using two other metrics (KL-D and cross kernels), specifically on \u201cPubMed45-ERN\u201d dataset, as presented in Table III. Here, as in Table II, we also present (mean-precision, mean-recall) tuples. We can see that MMD and KL-D metrics, both, perform equally well for AMR whereas MMD does better in case of SDG. CK (cross kernels), which is a relatively naive approach, also performs reasonably well, although for the AMRs it performs worse compared to MMD and KL-D. For the precision and recall numbers in the Table III, we see similar trends as reported in Table II. We observe that the recall numbers increase for the AMRs compared to the SDGs (the metric CK is an exception with negligible increase). Also, comparing KL-D against MMD, we see the former favors (significantly) higher precision, albeit at the expense of lower recall values."}, {"heading": "VII. RELATED WORK", "text": "There have been different lines of work for extracting protein extractions. Pattern-matching based systems (either manual or semi-automated) usually yield high precision but low recall [20, 22, 19, 18]. Kernel-based methods based on various convolution kernels have also been developed for the extraction task [35, 1, 27]. Some approaches work on string rather than parses [27]. The above mentioned works either rely on text or its shallow parses, none using semantic parsing for the extraction task. Also, most works consider only protein-protein interactions while ignoring interaction types. Some recent works used distant supervision to obtain a large data set of protein-protein pairs for their experiments [23].\nDocument-level extraction has been explored in the past [32, 7]. These works classify at sentence level and then combine the inferences whereas we propose to infer jointly on all the sentences at document level.\nPreviously, the idea of linear relational embedding has been explored in [29], where triples of concepts and relation types between those concepts are (jointly) embedded in some latent space. Neural networks have also been employed for joint embedding [4]. Here we advocate for a factored embedding where concepts (node labels) are embedded first using plain text, and then relations (edge labels) are embedded in a linear sub-space."}, {"heading": "VIII. CONCLUSION", "text": "In summary, we have developed and validated a method for extracting biomolecular interactions that, for the first time, uses deep semantic parses of biomedical text (AMRs). We have presented a novel algorithm, which relies on Graph Distribution Kernels (GDK) for document-level extraction of interactions from a set of AMRs in a document. GDK can operate on both\nAMR and SDG parses of sentences jointly. The rationale behind this hybrid approach is that while neither parsing is perfect, their combination can yield superior results. Indeed, our experimental results suggest that the proposed approach outperforms the baselines, especially in the practically relevant scenario when there is a noticeable mismatch between the training and test sets.\nTo facilitate the joint approach, we have proposed a novel edge vector space embedding method to assess similarity between different types of parses. We believe this notion of edge-similarly is quite general and will have applicability for a wider class of problems involving graph kernels. As a future work, we intend to validate this framework on a number of problems such as improving accuracy in AMRs parsing with SDGs."}, {"heading": "IX. ACKNOWLEDGEMENTS", "text": "This work was sponsored by the DARPA Big Mechanism program (W911NF-14-1-0364). It is our pleasure to acknowledge fruitful discussions with Michael Pust, Kevin Knight, Shuyang Gao, Linhong Zhu, Neal Lawton, Emilio Ferrara, and Greg Ver Steeg. We are also grateful to anonymous reviewers for their valuable feedback."}], "references": [{"title": "All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning", "author": ["Antti Airola", "Sampo Pyysalo", "Jari Bj\u00f6rne", "Tapio Pahikkala", "Filip Ginter", "Tapio Salakoski"], "venue": "BMC Bioinformatics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Semantic parsing as machine translation", "author": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark"], "venue": "In Proc. of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": "In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["Karsten M Borgwardt", "Arthur Gretton", "Malte J Rasch", "Hans-Peter Kriegel", "Bernhard Sch\u00f6lkopf", "Alex J Smola"], "venue": "Bioinformatics, 22:e49\u2013e57,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Comparative experiments on learning information extractors for proteins and their interactions", "author": ["Razvan Bunescu", "Ruifang Ge", "Rohit J Kate", "Edward M Marcotte", "Raymond J Mooney", "Arun K Ramani", "Yuk Wah Wong"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Integrating co-occurrence statistics with information extraction for robust retrieval of protein interactions from medline", "author": ["Razvan Bunescu", "Raymond Mooney", "Arun Ramani", "Edward Marcotte"], "venue": "In Proc. of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Libsvm: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Vector space models of lexical meaning", "author": ["Stephen Clark"], "venue": "Handbook of Contemporary Semantics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Dependency tree kernels for relation extraction", "author": ["Aron Culotta", "Jeffrey Sorensen"], "venue": "In Proc. of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "The biopax community standard for pathway data sharing", "author": ["Emek Demir", "Michael P Cary", "Suzanne Paley", "Ken Fukuda", "Christian Lemer", "Imre Vastrik", "Guanming Wu", "Peter D\u2019Eustachio", "Carl Schaefer", "Joanne Luciano"], "venue": "Nature Biotechnology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Applied numerical linear algebra", "author": ["James W Demmel"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["J. Flanigan", "S. Thomson", "J. Carbonell", "C. Dyer", "N.A. Smith"], "venue": "In Proc. of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Extracting biomolecular interactions using semantic parsing of biomedical text", "author": ["Sahil Garg", "Aram Galstyan", "Ulf Hermjakob", "Daniel Marcu"], "venue": "In Proc. of AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Compactly supported correlation functions", "author": ["Tilmann Gneiting"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "JMLR, 13:723\u2013773,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A domain-independent rule-based framework for event extraction", "author": [], "venue": "ACL-IJCNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Gene mention normalization and interaction extraction with context models and sentence motifs", "author": ["J\u00f6rg Hakenberg", "Conrad Plake", "Loic Royer", "Hendrik Strobelt", "Ulf Leser", "Michael Schroeder"], "venue": "Genome Biol,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Opendmap: an open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression", "author": ["Lawrence Hunter", "Zhiyong Lu", "James Firby", "William A Baumgartner", "Helen L Johnson", "Philip V Ogren", "K Bretonnel Cohen"], "venue": "BMC Bioinformatics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Maximum mean discrepancy imitation learning", "author": ["Beomjoon Kim", "Joelle Pineau"], "venue": "In Proc. of RSS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Overview of the protein-protein interaction annotation extraction task of biocreative ii", "author": ["Martin Krallinger", "Florian Leitner", "Carlos Rodriguez-Penagos", "Alfonso Valencia"], "venue": "Genome biology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Large-scale extraction of gene interactions from full-text literature using deepdive", "author": ["Emily K Mallory", "Ce Zhang", "Christopher R\u00e9", "Russ B Altman"], "venue": "Bioinformatics, page btv476,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Simple algorithms for complex relation extraction with applications to biomedical ie", "author": ["Ryan McDonald", "Fernando Pereira", "Seth Kulick", "Scott Winters", "Yang Jin", "Pete White"], "venue": "In Proc. of ACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Syntactic/semantic structures for textual entailment recognition", "author": ["Yashar Mehdad", "Alessandro Moschitti", "Fabio Massimo Zanzotto"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proc. of NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Subsequence kernels for relation extraction", "author": ["Raymond J Mooney", "Razvan C Bunescu"], "venue": "In Proc. of NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Convergence of block iterative methods applied to sparse least-squares problems", "author": ["W Niethammer", "J De Pillis", "RS Varga"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1984}, {"title": "Learning distributed representations by mapping concepts and relations into a linear space", "author": ["Alberto Paccanaro", "Geoffrey E Hinton"], "venue": "In Proc. of ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Transfer learning via dimensionality reduction", "author": ["Sinno Jialin Pan", "James T Kwok", "Qiang Yang"], "venue": "In Proc. of AAAI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Using syntax-based machine translation to parse english into abstract meaning representation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May"], "venue": "In Proc. of EMNLP,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Evidence combination in biomedical natural-language processing", "author": ["Marios Skounakis", "Mark Craven"], "venue": "In Proc. of BIOKDD,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "A walk-based semantically enriched tree kernel over distributed word representations", "author": ["Shashank Srivastava", "Dirk Hovy", "Eduard H Hovy"], "venue": "In Proc. of EMNLP,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Kernels on sample sets via nonparametric divergence estimates", "author": ["Dougal J Sutherland", "Liang Xiong", "Barnab\u00e1s P\u00f3czos", "Jeff Schneider"], "venue": "arXiv preprint arXiv:1202.0302,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "A comprehensive benchmark of kernel methods to extract protein\u2013protein interactions from literature", "author": ["Domonkos Tikk", "Philippe Thomas", "Peter Palaga", "J\u00f6rg Hakenberg", "Ulf Leser"], "venue": "PLoS Comput Biol,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Divergence estimation for multidimensional densities via-nearestneighbor distances", "author": ["Qing Wang", "Sanjeev R Kulkarni", "Sergio Verd\u00fa"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Building a semantic parser overnight", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": "In Proc. of ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Kernel methods for relation extraction", "author": ["Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella"], "venue": "JMLR, 3:1083\u20131106,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}], "referenceMentions": [{"referenceID": 21, "context": "In particular, there has been a significant body of research on identifying biological entities (proteins, genes, chemical compounds) and interactions between those entities from bio-medical papers [22, 19, 35, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 18, "context": "In particular, there has been a significant body of research on identifying biological entities (proteins, genes, chemical compounds) and interactions between those entities from bio-medical papers [22, 19, 35, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 34, "context": "In particular, there has been a significant body of research on identifying biological entities (proteins, genes, chemical compounds) and interactions between those entities from bio-medical papers [22, 19, 35, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 5, "context": "In particular, there has been a significant body of research on identifying biological entities (proteins, genes, chemical compounds) and interactions between those entities from bio-medical papers [22, 19, 35, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 0, "context": "For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 26, "context": "For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 19, "context": "For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12].", "startOffset": 252, "endOffset": 264}, {"referenceID": 23, "context": "For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12].", "startOffset": 252, "endOffset": 264}, {"referenceID": 11, "context": "For instance, most existing approaches focus on whether there is an interaction between a pair of proteins while ignoring the interaction types [1, 27], whereas other more advanced approaches cover only a small subset of all possible interaction types [20, 24, 12].", "startOffset": 252, "endOffset": 264}, {"referenceID": 34, "context": "And finally, owing to the enormous diversity of research topics in biomedical literature and the high cost of data annotation, there is often significant mismatch between training and testing corpora, which reflects poorly on generalization ability of existing methods [35].", "startOffset": 269, "endOffset": 273}, {"referenceID": 14, "context": "1This work has been published previously [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "In our representation, which is motivated by BioPAX [12], an interaction refers to either i) an entity effecting state change of another entity; or ii) an entity binding/dissociating with another entity to form/break a complex while, optionally, also influenced by a third entity.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Abstract Meaning Representation, or AMR, is a semantic annotation of single/multiple sentences [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 30, "context": "We trained an English-to-AMR parser [31] on two manually annotated corpora: i) a corpus of 17k general domain sentences including newswire and web text as published by the Linguistic Data Consortium; and ii) 3.", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "We propose an extension of the contiguous subtree kernel [38, 11] for mapping the extracted subgraphs (tree structure) to an implicit feature space.", "startOffset": 57, "endOffset": 65}, {"referenceID": 10, "context": "We propose an extension of the contiguous subtree kernel [38, 11] for mapping the extracted subgraphs (tree structure) to an implicit feature space.", "startOffset": 57, "endOffset": 65}, {"referenceID": 9, "context": "We instead propose to use vector space embedding of the node labels [10, 26], and then define a sparse RBF kernel on the node label vectors.", "startOffset": 68, "endOffset": 76}, {"referenceID": 25, "context": "We instead propose to use vector space embedding of the node labels [10, 26], and then define a sparse RBF kernel on the node label vectors.", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": "Similar extensions of convolution kernels have been been suggested previously[25, 33].", "startOffset": 77, "endOffset": 85}, {"referenceID": 32, "context": "Similar extensions of convolution kernels have been been suggested previously[25, 33].", "startOffset": 77, "endOffset": 85}, {"referenceID": 15, "context": "Here (\u00b7)+ denotes the positive part; I(\u00b7) is the indicator function; wx,wy are unit vector embeddings of node labels 4; ex, ey represent edge labels (label of an edge from a node\u2019s parent to it is the node\u2019s edge label); rx, ry are roles of nodes (such as protein, catalyst, concept, interaction-type); \u03b1 is a threshold parameter on the cosine similarity (wxwy) to control sparsity [16]; and \u03b2 is the bandwidth.", "startOffset": 382, "endOffset": 386}, {"referenceID": 25, "context": "4Learned using word2vec software [26] on over one million PubMed articles.", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": "The prevailing approach to document-level extraction is to first perform inference at sentence level, and then combine those inferences using some type of an aggregation function for a final documentlevel inference [32, 7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 6, "context": "The prevailing approach to document-level extraction is to first perform inference at sentence level, and then combine those inferences using some type of an aggregation function for a final documentlevel inference [32, 7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 6, "context": "For instance, in [7], the inference with the maximum score is chosen.", "startOffset": 17, "endOffset": 20}, {"referenceID": 16, "context": "Here Dmm is the Maximum Mean Discrepancy (MMD), a valid l2 norm, between a pair of distributions Gi,Gj [17]; K(.", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "We note that MMD metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5].", "startOffset": 106, "endOffset": 121}, {"referenceID": 20, "context": "We note that MMD metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5].", "startOffset": 106, "endOffset": 121}, {"referenceID": 29, "context": "We note that MMD metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5].", "startOffset": 106, "endOffset": 121}, {"referenceID": 4, "context": "We note that MMD metric has attracted a considerable attention in the machine learning community recently [17, 21, 30, 5].", "startOffset": 106, "endOffset": 121}, {"referenceID": 33, "context": "This is not true for divergence metrics such as KL-D, Renyi [34]; iii) It is suitable for compactly supported distributions (small number of samples) whereas methods, such as k-nearest neighbor estimation of KL-D, are not suitable if the number of samples in a distribution is too small [36]; iv) We have seen the most consistent results in our extraction experiments using this metric as opposed to the others.", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "This is not true for divergence metrics such as KL-D, Renyi [34]; iii) It is suitable for compactly supported distributions (small number of samples) whereas methods, such as k-nearest neighbor estimation of KL-D, are not suitable if the number of samples in a distribution is too small [36]; iv) We have seen the most consistent results in our extraction experiments using this metric as opposed to the others.", "startOffset": 287, "endOffset": 291}, {"referenceID": 33, "context": "This issue can be dealt in a practical manner as nicely discussed in [34].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "While there are different approaches for estimating divergences from samples [36], here we use kernel density estimator as shown below:", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9].", "startOffset": 118, "endOffset": 136}, {"referenceID": 13, "context": "On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9].", "startOffset": 118, "endOffset": 136}, {"referenceID": 36, "context": "On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9].", "startOffset": 118, "endOffset": 136}, {"referenceID": 1, "context": "On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9].", "startOffset": 118, "endOffset": 136}, {"referenceID": 8, "context": "On the other hand, the accuracy of semantic (AMR) parsing is not as good as the accuracy of shallow parsers like SDGs [31, 14, 37, 2, 9].", "startOffset": 118, "endOffset": 136}, {"referenceID": 30, "context": "For instance, in machine translation based semantic parsers [31, 2], hallucinating phrasal translations may introduce an interaction/protein in a parse that is non-existent in true semantics; iii) over fit of syntactic/semantic parsers can vary from each other in a test corpus depending upon the data used in their independent trainings.", "startOffset": 60, "endOffset": 67}, {"referenceID": 1, "context": "For instance, in machine translation based semantic parsers [31, 2], hallucinating phrasal translations may introduce an interaction/protein in a parse that is non-existent in true semantics; iii) over fit of syntactic/semantic parsers can vary from each other in a test corpus depending upon the data used in their independent trainings.", "startOffset": 60, "endOffset": 67}, {"referenceID": 12, "context": "Exploiting the block structure in the linear program, we propose an algorithm that is a variant of \u201cGauss-Seidel\u201d method [13, 28].", "startOffset": 121, "endOffset": 129}, {"referenceID": 27, "context": "Exploiting the block structure in the linear program, we propose an algorithm that is a variant of \u201cGauss-Seidel\u201d method [13, 28].", "startOffset": 121, "endOffset": 129}, {"referenceID": 12, "context": "2 in [13][p.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "287, chapter 6] states that the Gauss-Seidel method converges if the linear transformation matrix in a linear program is strictly row diagonal dominant [28].", "startOffset": 152, "endOffset": 156}, {"referenceID": 6, "context": "One can either use our original formulation where a single AMR/SDG sub-graph is classified using training sub-graphs from both AMRs and SDGs, and then the inference with maximum score-MSI [7] is chosen.", "startOffset": 188, "endOffset": 191}, {"referenceID": 34, "context": "This sentence-level validation approach is not always appropriate for extracting protein interactions [35], since interactions from a single/multiple sentences in a document can be correlated.", "startOffset": 102, "endOffset": 106}, {"referenceID": 26, "context": "For instance, in [27], the reported F1 score from the random validation in the AIMed data is approx.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "For the classification, we use the LIBSVM implementation of Kernel Support Vector Machines [8] with the sklearn python wrapper 9.", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Evaluation Results We categorize all methods evaluated below as follows: i) Sentence Level Inference-SLI 10; ii) document level using Maximum Score Inference-MSI [7]; and iii) document-level inference on all the subgraphs using our Graph Distribution Kernel (GDK).", "startOffset": 162, "endOffset": 165}, {"referenceID": 19, "context": "Pattern-matching based systems (either manual or semi-automated) usually yield high precision but low recall [20, 22, 19, 18].", "startOffset": 109, "endOffset": 125}, {"referenceID": 21, "context": "Pattern-matching based systems (either manual or semi-automated) usually yield high precision but low recall [20, 22, 19, 18].", "startOffset": 109, "endOffset": 125}, {"referenceID": 18, "context": "Pattern-matching based systems (either manual or semi-automated) usually yield high precision but low recall [20, 22, 19, 18].", "startOffset": 109, "endOffset": 125}, {"referenceID": 17, "context": "Pattern-matching based systems (either manual or semi-automated) usually yield high precision but low recall [20, 22, 19, 18].", "startOffset": 109, "endOffset": 125}, {"referenceID": 34, "context": "Kernel-based methods based on various convolution kernels have also been developed for the extraction task [35, 1, 27].", "startOffset": 107, "endOffset": 118}, {"referenceID": 0, "context": "Kernel-based methods based on various convolution kernels have also been developed for the extraction task [35, 1, 27].", "startOffset": 107, "endOffset": 118}, {"referenceID": 26, "context": "Kernel-based methods based on various convolution kernels have also been developed for the extraction task [35, 1, 27].", "startOffset": 107, "endOffset": 118}, {"referenceID": 26, "context": "Some approaches work on string rather than parses [27].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "Some recent works used distant supervision to obtain a large data set of protein-protein pairs for their experiments [23].", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "Document-level extraction has been explored in the past [32, 7].", "startOffset": 56, "endOffset": 63}, {"referenceID": 6, "context": "Document-level extraction has been explored in the past [32, 7].", "startOffset": 56, "endOffset": 63}, {"referenceID": 28, "context": "Previously, the idea of linear relational embedding has been explored in [29], where triples of concepts and relation types between those concepts are (jointly) embedded in some latent space.", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "Neural networks have also been employed for joint embedding [4].", "startOffset": 60, "endOffset": 63}], "year": 2015, "abstractText": "We advance the state of the art in biomolecular interaction extraction with three contributions: (i) We show that deep, Abstract Meaning Representations (AMR) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surfaceand syntax-based features; (ii) In contrast with previous approaches that infer relations on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents); (iii) We further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced AMR (semantic) and dependency structure (syntactic) representations. Our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions.", "creator": "LaTeX with hyperref package"}}}