{"id": "1512.01728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "Similarity Learning via Adaptive Regression and Its Application to Image Retrieval", "abstract": "we study the problem of similarity learning and its application to image retrieval with large - scale data. the similarity between pairs of images can be measured by the distances between their high dimensional representations, and the problem of parameter learning the appropriate similarity graph is often addressed by discrete distance metric learning. however, distance metric learning requires the learned metric to be a psd matrix, which furthermore is computational expensive and not necessary for retrieval ranking problem. on the other hand, the bilinear model is shown to be more flexible for large - scale image retrieval task, hence, we adopt it to learn a matrix parameter for estimating pairwise similarities under the regression framework. by adaptively updating the target matrix in regression, we can mimic both the hinge loss, which is more appropriate for similarity learning problem. although the regression problem can have the closed - form solution, raising the computational cost can be very expensive. the computational challenges come from two aspects : the number of images can be very large and image features have high dimensional dimensionality. we address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee. for the high dimensional issue, we address it by taking low rank assumption and manually applying alternating method to obtain the partial matrix, which has a global optimal solution. empirical studies on real network world image datasets ( i. e., caltech and imagenet ) demonstrate the effectiveness and efficiency of the proposed method.", "histories": [["v1", "Sun, 6 Dec 2015 02:56:32 GMT  (157kb)", "http://arxiv.org/abs/1512.01728v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi qian", "inci m baytas", "rong jin", "anil jain", "shenghuo zhu"], "accepted": false, "id": "1512.01728"}, "pdf": {"name": "1512.01728.pdf", "metadata": {"source": "CRF", "title": "Similarity Learning via Adaptive Regression and Its Application to Image Retrieval", "authors": ["Qi Qian", "Inci M. Baytas", "Rong Jin", "Shenghuo Zhu"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n01 72\n8v 1\n[ cs\n.L G\n] 6\nD ec\n2 01"}, {"heading": "Introduction", "text": "Learning pairwise similarity is important for machine learning tasks, e.g., classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al. 2012; Xing et al. 2002), ranking (Chechik et al. 2010; Lim and Lanckriet 2014), etc. Distance metric learning (DML), which aims to learn the appropriate distance (i.e., similarity) for any given pair of instances, has been studied for decades and various methods have been proposed to estimate the similarity. Given the learned metric M , most of the DML methods compute the similarity in the form of Mahalanobis distance (Mahalanobis 1936): disM (xi,xj) =\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n(xi\u2212xj)\u22a4M(xi\u2212xj), where M has to be the positive semidefinite (PSD) matrix. The PSD constraint guarantees that the similarity value is nonnegative and symmetric, which is the requirement for distance based classification (e.g., knearest neighbor) and clustering (e.g., k-means) methods.\nFor certain applications, however, the properties of nonnegativity and symmetry are not necessary. Ranking tasks, for example, only require to output a list of examples according to the query and hence the similarity value could be arbitrary (non metric). Additionally, the task of image retrieval is known to be non-symmetric according to human judgement (Tversky 1977). In this scenario, the similarity can be measured by the similarity function in bilinear form (Guo and Ying 2014; Chechik et al. 2010): SimM (xi,xj) = x \u22a4 i Mxj , which computes the inner product between two examples. Without the PSD constraint as required in DML, the matrix M in the bilinear model can be non-symmetric and the cost of optimization decreases from O(d3) (i.e., PSD projection) to O(d2), where d is the dimensionality of data. We will adopt the bilinear model in this work.\nGiven the bilinear model, an appropriate similarity function can be learned over pairwise constraints. There are two challenges when applying it directly to large-scale image data. First, the number of constraints is quadratic in the number of images (i.e., n). Although most of existing DML methods apply stochastic strategy to access one pair at each iteration for efficiency, it is impossible for them to utilize all the pairs, when n is large (e.g., the cost is at least O(n2d2) and up to O(n3d3)), which introduces the risk of the suboptimal solution. Second, images tend to have high dimensional features to capture the image content. Since the size of M is d\u00d7 d, it becomes difficult to store it when d is very large. Further, with large number of parameters, it is easy to encounter the overfitting issue.\nIn this paper, we attempt to learn a good similarity function by solving a matrix regression problem (Yi et al. 2012). Unlike the conventional matrix regression, the proposed method adaptively update the target metric at each iteration to mimic the hinge loss, which is more appropriate for real world applications. Besides, different from stochastic methods, the approach is deterministic and has the closed-form solution, which is very efficient for optimization and can obtain information from all pairs of data. To adapt it for large-\nscale high dimensional image retrieval, we further explore the randomized algorithm for data compressing and the low rank assumption. Our contributions can be summarised as follows.\n\u2022 We develop an adaptive strategy for updating the target matrix in regression, which is more flexible and appropriate for learning a similarity function with pairwise constraints.\n\u2022 To further improve the scalability for large-scale problem, we develop a randomized algorithm to compress the data without sacrificing the performance. Our theoretical analysis shows that if the dataset matrix is of low rank with rank rD \u226a min{n, d}, then only O(rD log(rD)) examples are sufficient to obtain a good similarity function, which is much better over the previous result (i.e., O(d2) (Drineas, Mahoney, and Muthukrishnan 2006)). It is important when the data matrix or the target matrix cannot be loaded into the memory.\n\u2022 To alleviate the high dimensional challenge, we take the low rank assumption (Weinberger and Saul 2009) that the rank of the optimal M is significantly smaller than the dimensionality of data (i.e., r \u226a d). Then, the low rank component of M can be optimized by an alternating method efficiently. Although the corresponding optimization problem is non-convex, the recent improvement in the alternating method shows that the solution is globally optimal with the geometric convergence rate (Jain, Netrapalli, and Sanghavi 2012).\nWe conduct empirical studies on real world image datasets; comparison with the state-of-the-art methods verify both the effectiveness and efficiency of the proposed method."}, {"heading": "Related Work", "text": ""}, {"heading": "Distance Metric Learning", "text": "Many methods have been developed to estimate pairwise similarity. Some of them can be categorized as distance metric learning, where a distance metric is learned to increase the pairwise distance involving examples from different classes and reduce the pairwise distance involving examples from the same class. The representative methods include Xing\u2019s method (Xing et al. 2002), POLA (ShalevShwartz, Singer, and Ng 2004), ITML (Davis et al. 2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014). ITML learns a metric according to the pairwise constraints, where the distance between pairs from the same class should be smaller than a pre-defined threshold and the distance between pairs from different classes should be larger than a second threshold. LMNN is developed with triplet constraints and a metric is learned to make sure that pairs from the same classes are separated from the pairs from different classes with a large margin. PSD constraint is adopted by these methods to make sure that the learned similarity matrix is symmetric and nonnegative. All of these DML methods have demonstrated some success in real applications. However, PSD projection is time consum-\ning while its property may be not necessary in certain applications."}, {"heading": "Similarity Learning with Bilinear Model", "text": "Besides these DML methods, the bilinear model is developed for learning a similarity without the PSD constraint. Different from the similarity defined on the distance of two examples, the bilinear model computes the inner product of two examples by the learned similarity function. OASIS (Chechik et al. 2010) successfully applied it to the ranking problem, which requires that the more relevant examples have larger similarity and a hinge loss is adopted for a safety margin. The corresponding optimization problem is solved by an online learning method for large-scale image retrieval task. Compared with DML, the learned matrix is not symmetric, which offers more flexibility in real world applications. Although OASIS could handle millions of images, the computational cost is still O(d2) at each iteration. Moreover, online learning cannot enumerate all triplet constraints due to its huge number (i.e., O(n3)), which may lead to a suboptimal solution. In this paper, we propose a matrix regression model for large-scale high dimensional image retrieval task."}, {"heading": "Our Approach", "text": ""}, {"heading": "Adaptive Regression for Similarity Learning", "text": "Given a set of training images X \u2208 Rd\u00d7n, similarity learning with bilinear model aims to learn a good matrix M for estimating pairwise similarity as\nSimM (xi,xj) = x \u22a4 i Mxj\nWith the target matrix Y \u2208 Rn\u00d7n, M can be learned via solving a matrix regression problem\nmin M\u2208Rd\u00d7d\n\u2016X\u22a4MX \u2212 Y \u20162F (1)\nwhere Y can be generated by the label information and defined as\nYi,j =\n{\n1 : label(xi) = label(xj) 0 : otherwise\n(2)\nThis formulation is popular for similarity learning and widely used in different tasks (Yi et al. 2012; Feng, Jin, and Jain 2013). It has a closed-form solution for the matrix M :\nM\u2217 = (X \u2020)\u22a4Y X\u2020 (3)\nwhere X\u2020 is the pseudo inverse of X . However, the constraints in the similarity learning appear as\nx\u22a4i Mxj \u2265 \u03b41; x\u22a4i Mxk \u2264 \u03b42 where xi and xj are from the same class and the label of xk is different. \u03b41 and \u03b42 are pre-defined thresholds. The corresponding definition for the target matrix should be\nYi,j =\n{\n\u2265 \u03b41 : label(xi) = label(xj) \u2264 \u03b42 : otherwise\nand the corresponding optimiztion problem with the squared hinge loss is\nmin M\u2208Rd\u00d7d\n\u2211\ni,j\n[\u03b41 \u2212 x\u22a4i Mxj ]2+ + \u2211\ni,k\n[x\u22a4i Mxk \u2212 \u03b42]2+ (4)\nThe problem in Eqn. 4 can be solved by SGD but will suffer from the problem of suboptimal solution.\nTherefore, we take an alternating update strategy, which puts less penalty on the pairs that have been separated well and focuses on the hard pairs of examples, for the target matrix to mimic the hinge loss as summarized in Alg. 1. The proposed method, on the one hand, can approximates the hinge loss, which is more appropriate for real world application. On the other hand, it enjoys the light computational cost from the closed-form solution of the conventional regression task.\nAlgorithm 1 Adaptive Regression for Similarity Learning\n1: Input: Dataset X \u2208 Rd\u00d7n, # Iterations T , thresholds \u03b41 and \u03b42 2: Initialize M0 as an identity matrix 3: for k = 1, \u00b7 \u00b7 \u00b7 , T do 4: Y\u0302 = X\u22a4Mk\u22121X\n5: Y =\n{\nmax{Y\u0302i,j , \u03b41} : label(xi) = label(xj) min{Y\u0302i,j , \u03b42} : otherwise\n6: Learn Mk = minM\u2208Rd\u00d7d \u2016X\u22a4MX \u2212 Y \u20162F 7: end for 8: return MT"}, {"heading": "Large-scale Challenge", "text": "For obtaining the closed-form solution at each iteration, it has to compute the pseudo inverse of X , which costs O(min{n2d, d2n}). Besides, solving the regression problem in Alg. 1 requires the appearance of the n \u00d7 n target matrix Y . When the number of examples is extremely large, this is not affordable to maintain X and Y in the memory. Therefore, we try to reduce the number of examples, which is equivalent to the number of constraints, at each iteration by developing a randomized algorithm.\nGiven a general regression problem\nmin M\n\u2016AMB\u22a4 \u2212 Y \u20162F (5)\nwhere A = X\u22a4 and B = X in our application, we will solve a problem with compressed examples instead\nmin M\n\u2016S\u22a41 AMB\u22a4S2 \u2212 S\u22a41 Y S2\u20162F (6)\nwhere S1, S2 \u2208 Rn\u00d7m are two random matrices and the resulting target matrix is only m\u00d7m. The following theorem shows that the solution from problem (6) is close to the solution in the original problem in (5).\nTheorem 1. Let M be the optimal solution of problem (5) and M\u0302 be the solution for problem (6). Let R be the residual from M in problem (5). Let \u03b4Amin and \u03b4 A max represent the\nminimal and maximal singular value of the matrix A, respectively. S1, S2 \u2208 Rn\u00d7m are two normalized Gaussian random matrices and the rank of the data matrix A and B is rD . Then, with a probability 1\u2212 \u03b4, we have\n\u2016M \u2212 M\u0302\u20162 \u2264 \u2016R\u20162\n\u03b4Amin\u03b4 B min\n(\n\u03b52(1 + \u03b5)\n1\u2212 \u03b5\n+2 \u03b5(1 + \u03b5)3/2\u221a\n1\u2212 \u03b5 + 2\n\u221a 2\u03b5(1 + \u03b5)3/2) )\nprovided\nm \u2265 (rD + 1) log(48rD/\u03b4) c\u03b52\nFurthermore, if we take a similar assumption as in (Drineas, Mahoney, and Muthukrishnan 2006) that R is bounded by the estimate of the optimal solution by a factor \u03b3 \u2208 (0, 1)\n\u2016Y \u2212AMB\u22a4\u20162 \u2264 \u03b3\u2016AMB\u22a4\u20162 we have\n\u2016M \u2212 M\u0302\u20162 \u2264 \u03b3\u03b4Amax\u03b4 B max\u2016M\u20162\n\u03b4Amin\u03b4 B min\n( \u03b52(1 + \u03b5)\n1\u2212 \u03b5\n+2 \u03b5(1 + \u03b5)3/2\u221a\n1\u2212 \u03b5 + 2\n\u221a 2\u03b5(1 + \u03b5)3/2)\nRemark 2 We only require O(rD log(rD)) random projections which is significantly smaller than the result in (Drineas, Mahoney, and Muthukrishnan 2006), where the sampling number is at least O(d2). Compared with the cost of obtaining X\u2020 (i.e., O(min{n2d, nd2})), the cost of computing (XS)\u2020 is only O(r2Dd+ rDnd), which is feasible for large-scale problems.\nRemark 3 Although we use Gaussian random matrices, the essential requirement for the random matrix is E[SS\u22a4] = I . Therefore, it can be a matrix with each column randomly selected from { \u221a n/me1, \u00b7 \u00b7 \u00b7 , \u221a\nn/men}, which is equivalent to uniformly sampled columns of the data matrix. Compared with random projection, it saves the computational cost of compressing the dataset (i.e. O(rDnd) in random projection) and the total cost of the algorithm is only O(r2Dd+ rrDd). Besides, we can show that Lemma 1. Let S \u2208 Rn\u00d7m be a random sampling matrix, where each column is uniformly sampled from { \u221a n/me1, \u00b7 \u00b7 \u00b7 , \u221a\nn/men}, with a probability 1 \u2212 \u03b4. We have \u2016SS\u22a4 \u2212 I\u20162 \u2264 \u03b5 provided\nm \u2265 (n\u2212 1) logn/\u03b4 c\u03b52\nwhere constant c is at least 1/3.\nAlg. 2 summarizes the randomized variant of the proposed method.\nAlgorithm 2 Randomized Regression for Similarity Learning\n1: Input: Dataset X \u2208 Rd\u00d7n, # Iterations T , m 2: Initialize M0 as an identity matrix 3: for k = 1, \u00b7 \u00b7 \u00b7 , T do 4: Generate two random matrices S1, S2 \u2208 Rn\u00d7m 5: Compute the pairs selected by the random matrices\nY\u0302S = X \u22a4Mk\u22121X\n6: YS =\n{\nmax{Y\u0302i,j , \u03b41} : label(xi) = label(xj) min{Y\u0302i,j , \u03b42} : otherwise\n7: Learn Mk = minM\u2208Rd\u00d7d \u2016S\u22a41 X\u22a4MXS2 \u2212 YS\u20162F 8: end for 9: return M = LTR\u22a4T"}, {"heading": "High Dimensional Challenge", "text": "To avoid overfitting and alleviate the storage challenge, the learned matrix is constrained to be of low rank and the corresponding problem is\nmin M\u2208Rd\u00d7d,rank(M)=r\n\u2016X\u22a4MX \u2212 Y \u20162F\nwhere r is the rank of the learned matrix and r \u226a d. Although this problem has the closed-form solution (Yu and Schuurmans 2011), the solution is only available for Frobenius norm and the computational cost is expensive (i.e., O(dn2 + n3)).\nInspired by the idea of alternating optimization (Jain, Netrapalli, and Sanghavi 2012; Netrapalli, Jain, and Sanghavi 2013), we can decompose M as M = LR\u22a4, where L,R \u2208 Rd\u00d7r. Then the original problem is equivalent to\nmin L,R\u2208Rd\u00d7r\n\u2016X\u22a4LR\u22a4X \u2212 Y \u20162F (7)\nNote that L and R can be different since M is not a PSD matrix. The problem in (7) can be solved via the alternating method. Since the component \u201cX\u2020\u201d can be reused at each iteration, it is just computed once at the beginning of the algorithm and the cost of each iteration is only O(ndr + n2r), which is affordable for high dimensional data.\nAlg. 3 shows the algorithm with data compressing. Each subproblem (i.e., Steps 6 and 9) has a closed-form solution and only a partial random projection is needed since the size of X\u22a4R or X\u22a4L is only n \u00d7 r and the cost of computing pseudo inverse is light.\nCorollary 2. Assume that the rank of the optimal solution M\u2217 for the Problem (1) is r and the linear measurement A(M) = X\u22a4MX satisfies restricted isometry property (RIP) (Recht, Fazel, and Parrilo 2010). When n < d, we have that the solution from Alg. 1 is globally optimal with a geometric convergence rate\n\u2016M\u2217 \u2212 LTRT \u2016F \u2264 e\u2212T/2\u2016M\u2217\u2016F\nProof. It is directly from the Theorem 2.2 in (Jain, Netrapalli, and Sanghavi 2012).\nAlgorithm 3 Alternating Regression for Similarity Learning\n1: Input: Dataset X \u2208 Rd\u00d7n, # Iterations T , m 2: Initialize L0 and R0 using a randomly sampled subset 3: for k = 1, \u00b7 \u00b7 \u00b7 , T do 4: Generate a random matrix S \u2208 Rn\u00d7m 5: Compute the corresponding target matrix YS 6: Learn Lk = minL\u2208Rd\u00d7r \u2016S\u22a4X\u22a4LR\u22a4k\u22121X \u2212 YS\u20162F 7: Generate a random matrix S \u2208 Rn\u00d7m 8: Compute the corresponding target matrix YS 9: Learn Rk = minR\u2208Rd\u00d7r \u2016X\u22a4LkR\u22a4XS \u2212 YS\u20162F\n10: end for 11: return M = LTR\u22a4T"}, {"heading": "Theoretical Analysis", "text": ""}, {"heading": "Proof of Theorem 1", "text": "The key of our proof is from the following corollary.\nCorollary 3. (Zhang et al. 2013) Let S \u2208 Rr\u00d7m be a standard Gaussian random matrix. Then, for any 0 < \u03b5 \u2264 1/2, with a probability 1\u2212 \u03b4, we have\n\u2225 \u2225 \u2225 \u2225 1\nm SS\u22a4 \u2212 I\n\u2225 \u2225 \u2225 \u2225\n2\n\u2264 \u03b5\nprovided\nm \u2265 (r + 1) log(2r/\u03b4) c\u03b52\nwhere constant c is at least 1/4.\nThe main idea of the proof is similar to that in (Drineas, Mahoney, and Muthukrishnan 2006). Before the proof, we first give some useful lemmas.\nLemma 2. Given a rank r matrix A = UA\u03a3AV \u22a4A \u2208 Rn\u00d7d and a normalized Gaussian random matrix S \u2208 Rn\u00d7m, which is normalized by \u221a m, with a probability 1 \u2212 \u03b4, we have |1\u2212 \u03b42i (S\u22a4UA)| \u2264 \u03b5\nand rank(S\u22a4UA) = rank(UA)\nwhere m = O(r log(r)) and \u03b4i(S\u22a4UA) is the i-th singular value of S\u22a4U .\nLemma 3. Let A and S are the matrices defined in Lemma 2, with a probability 1\u2212 \u03b4, we have\n(S\u22a4A)\u2020 = VA\u03a3 \u22121 A (S \u22a4UA) \u2020\nLemma 4. Let \u2126 = (S\u22a4UA)\u2020S\u22a4 \u2212 (S\u22a4UA)\u22a4S\u22a4, where UA and S are the matrices in Lemma 2 and X\u2020 is the pseudo inverse of matrix X , with a probability 1\u2212 2\u03b4, we have\n\u2016\u2126\u20162 \u2264 \u03b5 \u221a 1 + \u03b5\u221a 1\u2212 \u03b5\nWe skip the proofs for these lemmas since the proof is similar as in (Drineas, Mahoney, and Muthukrishnan 2006). Now, we show the key steps of the proof for Theorem 1 due to the space limitation.\nWe try to bound the difference between M and M\u0302 as\n\u2016M \u2212 M\u0302\u20162 = \u2016A\u2020Y (B\u22a4)\u2020 \u2212 (S\u22a41 A)\u2020S\u22a41 Y S2(B\u22a4S2)\u2020\u20162 where M and M\u0302 are the solutions for problems (5) and (6), respectively. According to Lemma 3, with a probability 1\u2212 2\u03b4, we have\n\u2016M \u2212 M\u0302\u20162 \u2264 1\n\u03b4Amin\u03b4 B min\n\u2016(S\u22a41 UA)\u2020S\u22a41 RS2(U\u22a4BS2)\u2020\u20162\nwhere the residual R = Y \u2212 AMB\u22a4 = U\u22a5AU\u22a5\u22a4A Y + UAU \u22a4 A Y U \u22a5 BU \u22a5\u22a4 B\nLet \u2126A = (S\u22a41 UA) \u2020S\u22a41 \u2212 (S\u22a41 UA)\u22a4S\u22a41 and \u2126B =\nS2(B \u22a4S2) \u2020 \u2212 S2(B\u22a4S2)\u22a4. Then \u2016M \u2212 M\u0302\u20162 \u2264\n1\n\u03b4Amin\u03b4 B min\n\u2016(\u2126A + U\u22a4AS1S\u22a41 )R(\u2126B + S2S\u22a42 UB)\u20162\nFirst, we can proof that with a probability 1\u2212 2\u03b4, it is \u2016U\u22a4ASS\u22a4\u20162 \u2264 1 + \u03b5 (8)\nUsing the similar procedure, with a probability 1 \u2212 3\u03b4, we have\n\u2016UAU\u22a4A \u2212 UAU\u22a4ASS\u22a4\u20162 \u2264 \u221a \u03b52 + 2\u03b5 (9)\nSince R has two components, we investigate the pattern as\n\u2016U\u22a4AS1S\u22a41 U\u22a5AU\u22a5\u22a4A Y \u20162 = \u2016UAU\u22a4AS1S\u22a41 U\u22a5AU\u22a5\u22a4A Y \u20162 = \u2016UAU\u22a4AU\u22a5AU\u22a5\u22a4A Y \u2212 UAU\u22a4AS1S\u22a41 U\u22a5AU\u22a5\u22a4A Y \u20162 \u2264 \u2016UAU\u22a4A \u2212 UAU\u22a4AS1S\u22a41 \u20162\u2016U\u22a5AU\u22a5\u22a4A Y \u20162 (10)\nBy taking Eqn. 8-10 back to the Eqn. 8, and applying union bound, with a probability 1\u2212 24\u03b4 we have\n\u2016M \u2212 M\u0302\u20162 \u2264 \u2016R\u20162\n\u03b4Amin\u03b4 B min\n( \u03b52(1 + \u03b5) 1\u2212 \u03b5 + 2 \u03b5(1 + \u03b5)3/2\u221a 1\u2212 \u03b5 )\n+ (1 + \u03b5)\n\u221a \u03b52 + 2\u03b5\n\u03b4Amin\u03b4 B min\n(\u2016U\u22a5AU\u22a5\u22a4A Y \u20162\n+\u2016UAU\u22a4A Y U\u22a5BU\u22a5\u22a4B \u20162) We finish the proof with\n2\u2016R\u20162 > \u2016U\u22a5AU\u22a5\u22a4A Y \u20162 + \u2016UAU\u22a4A Y U\u22a5BU\u22a5\u22a4B \u20162 (11) Furthermore, if we take a similar assumption as\nin (Drineas, Mahoney, and Muthukrishnan 2006)\n\u2016R\u20162 = \u2016Y \u2212 UAU\u22a4AY UBU\u22a4B \u20162 \u2264 \u03b3\u2016UAU\u22a4A Y UBU\u22a4B \u20162 where \u03b3 \u2208 (0, 1), we have\n\u2016M \u2212 M\u0302\u20162 \u2264 \u03b3\u03b4Amax\u03b4 B max\u2016M\u20162\n\u03b4Amin\u03b4 B min\n( \u03b52(1 + \u03b5)\n1\u2212 \u03b5\n+2 \u03b5(1 + \u03b5)3/2\u221a\n1\u2212 \u03b5 + 2\n\u221a 2\u03b5(1 + \u03b5)3/2)\ndue to\n\u2016M\u20162 = \u2016A\u2020Y B\u2020\u22a4\u20162 \u2265 1\n\u03b4Amax\u03b4 B max\n\u2016UAU\u22a4A Y UBU\u22a4B \u20162"}, {"heading": "Proof of Lemma 1", "text": "Proof. We define the random variable Qi as\nQi = nqiq \u22a4 i \u2212 I\nwhere qi is randomly selected from {e1, \u00b7 \u00b7 \u00b7 , en} with probability 1/n. It is obvious that Qi is self-adjoint, E[Qi] = 0 and \u03b4max \u2264 n\u2212 1. Furthermore, we have E[Q2i ] = n 2E[qiq \u22a4 i qiq \u22a4 i ]\u2212 2nE[qiq\u22a4i ] + I = (n\u2212 1)I Therefore, \u03c32 = \u2016 \u2211m\ni E[Q 2 i ]\u20162 = m(n\u2212 1). According to\nBernstein\u2019s inequality (Recht 2011), we have\nPr{\u03b4max( m \u2211\ni=1\nQi) \u2265 \u03b5} \u2264 n exp( \u2212\u03b52/2\nm(n\u2212 1) + (n\u2212 1)\u03b5/3)\nWith a similar procedure, we can bound \u03b4max(\u2212 \u2211\ni Qi). Combining them together, with a probability 1 \u2212 \u03b4, we\nhave \u2016SS\u22a4 \u2212 I\u20162 \u2264 \u03b5\nprovided\nm \u2265 (n\u2212 1) logn/\u03b4 c\u03b52\nwhere each column of S is uniformly sampled from { \u221a n/me1, \u00b7 \u00b7 \u00b7 , \u221a n/men} and constant c \u2265 1/3."}, {"heading": "Experiments", "text": "Four state-of-the-art similarity learning algorithms are included in comparison to verify the effectiveness of the proposed method. \u2022 LMNN (Weinberger and Saul 2009): DML methods with\ntriplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML meth-\nods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al. 2010): Bilinear model for similar-\nity learning; \u2022 SLR: Similarity learning with adaptive regression and al-\nternating solver. .\nBesides, Euclid is the baseline with Euclidean distance directly. We use the implementations provided by the authors with the recommended parameter settings. We set the number of constraints for FRML and OASIS as N = 106 to fully explore the information contained in the data. Note that this number is still much less than n2. Since LMNN and FRML can optimize the low rank part of the metric (i.e., L) directly, we set the rank of the metric as 100which is also used by our method. Thresholds \u03b41 and \u03b42 are set to be 1 and 0, respectively, while the number of iterations is set as T = 10. All experiments are implemented on a server with 64GB memory and 12\u00d7 2.4GHz CPUs.\nWe evaluate the learned metric on a ranking task, which is the same as in (Chechik et al. 2010). For the DML methods, the ranking list is given according to the distance to the query in the increasing order while that for the bilinear model is output by the similarity in the descending order. The mean-average-precision (mAP) is used to evaluate the ranking performance. Below is a description of the image datasets used in our experiments."}, {"heading": "Caltech101", "text": "Caltech101 is a benchmark image dataset which consists of 101 different objects (Li, Fergus, and Perona 2007). We extract LLC features (Yang et al. 2009) as representations for each image and reduce the dimension to 1, 000 by PCA to make the comparison with other methods. The final set contains 8, 677 examples with 1, 000 features. We randomly select 70% for training and the rest for testing. The experiments are repeated 5 times with different splits (of the same proportion between training and test sets) and average results with standard deviation are reported in Table 1.\nFirst, it is obvious that both distance based and similarity based methods outperforms the Euclidean distance. Second, we observe that methods using the bilinear model (i.e., OASIS and SLR) are better than the distance based method. This is because the bilinear model is more flexible for ranking. Second, the proposed method outperforms other methods significantly, which is because SLR can visit all pairs and hence no information is lost. Fig. 1 shows the examples of retrieved images. The metric learned by SLR can return the right images while the metrics of other methods make certain mistakes.\nTo demonstrate the advantage of adaptive regression, we compare SLR to the variant with the fixed target matrix as in Eqn. 2, which is denoted as \u201cSLR-Fixed\u201d. We also include the whole metric computed from the closed-form solution as in Eqn. 3, which is denoted as \u201cSLR-Whole\u201d for comparison, and the result is summarized in Fig. 2. It is observed that SLR is even better than SLR-Fixed and SLR-Whole, which verifies the effectiveness of the adaptive regression. Moreover, we find that SLR-Fixed almost performs the same as SLR-Whole, and it confirms that the alternating method can achieve global optimal solution for the non-convex problem. Finally, SLR and SLR-Fixed converges to a good solution in only 2 iterations, which is consistent with the analysis of the alternating method.\nWe further conduct the experiments to illustrate the influence of data compressing. We denote SLR with Gaussian random projection and alternating target matrix (i.e., in Alg. 3) as \u201cSLR-G-*\u201d and column sampling as \u201cSLR-C-*\u201d.\nThe number \u2217 denotes the dimension after compressing and results are shown in Fig. 3. We can find that the performance of SLR-G and SLR-C can approach that of SLR, which verifies our analysis in Theorem 1.\nImageNet50\nImageNet50 consists of 50 randomly selected categories from the ImageNet dataset (Russakovsky et al. 2014). There are 101, 687 images with 1, 000 features in the dataset and the experimental settings are the same as for the Caltech101 dataset except that we reduce the number of SLR\u2019s iterations to 5 due to the large size of training set. Since the whole target metric is too large, we use the result of SLR-C-20k to stand for that of SLR. Table 2 summarizes the results from different methods. A similar phenomenon, as for Caltech101 can be observed, where SLR achieves the best ranking performance compared to state-of-the-art methods.\nTo further verify the efficiency of the proposed method,\nwe compare the run time of methods in the experiments shown in Table 3. It is observed that the proposed method is more efficient than other methods. We attribute it to the fact that SLR costs O(nd2) to go through all examples while the other methods take O(Nd2) to learn a good metric, where N is the number of constraints and N \u226b n."}, {"heading": "Conclusions", "text": "In this work, we use bilinear model to rank large-scale images. The challenge of large-scale data is alleviated by writing the learning problem as a matrix regression task, which has a closed-form solution. To further compress data, the randomized variant of the proposed method is developed. We address the challenge from high dimensionality by applying alternating method with low rank assumption. Unlike most of previous methods with low rank assumption, the solution of the proposed method is guaranteed to be global optimal. Different from many previous methods that use stochastic algorithm for efficiency, our method can go through all pairs of images without information lost. In the future, we plan to apply our randomized algorithm to the dataset with the huge number of images that cannot be handled by a single machine. We will also explore our method in other applications (e.g., high dimensional document data)."}], "references": [{"title": "Similarity learning for provably accurate sparse linear classification", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ICML.", "citeRegEx": "Bellet et al\\.,? 2012", "shortCiteRegEx": "Bellet et al\\.", "year": 2012}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "JMLR 11:1109\u20131135.", "citeRegEx": "Chechik et al\\.,? 2010", "shortCiteRegEx": "Chechik et al\\.", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, 209\u2013 216.", "citeRegEx": "Davis et al\\.,? 2007", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SODA, 1127\u20131136.", "citeRegEx": "Drineas et al\\.,? 2006", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Large-scale image annotation by efficient and robust kernel metric learning", "author": ["Z. Feng", "R. Jin", "A.K. Jain"], "venue": "ICCV, 1609\u20131616.", "citeRegEx": "Feng et al\\.,? 2013", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Guaranteed classification via regularized similarity learning", "author": ["Z. Guo", "Y. Ying"], "venue": "Neural Computation 26(3):497\u2013522.", "citeRegEx": "Guo and Ying,? 2014", "shortCiteRegEx": "Guo and Ying", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "CoRR abs/1212.0467.", "citeRegEx": "Jain et al\\.,? 2012", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["F. Li", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding 106(1):59\u201370.", "citeRegEx": "Li et al\\.,? 2007", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["D. Lim", "G.R.G. Lanckriet"], "venue": "ICML, 1980\u20131988.", "citeRegEx": "Lim and Lanckriet,? 2014", "shortCiteRegEx": "Lim and Lanckriet", "year": 2014}, {"title": "On the generalized distance in statistics", "author": ["P.C. Mahalanobis"], "venue": "Proceedings of the National Institute of Sciences (Calcutta) 2:49\u201355.", "citeRegEx": "Mahalanobis,? 1936", "shortCiteRegEx": "Mahalanobis", "year": 1936}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "NIPS, 2796\u20132804.", "citeRegEx": "Netrapalli et al\\.,? 2013", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review 52(3):471\u2013501.", "citeRegEx": "Recht et al\\.,? 2010", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "JMLR 12:3413\u20133430.", "citeRegEx": "Recht,? 2011", "shortCiteRegEx": "Recht", "year": 2011}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "ICML.", "citeRegEx": "Shalev.Shwartz et al\\.,? 2004", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Features of similarity", "author": ["A. Tversky"], "venue": "Psychological Review 84(4):327\u2013352.", "citeRegEx": "Tversky,? 1977", "shortCiteRegEx": "Tversky", "year": 1977}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR 10:207\u2013244.", "citeRegEx": "Weinberger and Saul,? 2009", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "NIPS, 505\u2013512.", "citeRegEx": "Xing et al\\.,? 2002", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T.S. Huang"], "venue": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, 1794\u20131801.", "citeRegEx": "Yang et al\\.,? 2009", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Semi-crowdsourced clustering: Generalizing crowd labeling by robust distance metric learning", "author": ["J. Yi", "R. Jin", "A.K. Jain", "S. Jain", "T. Yang"], "venue": "NIPS, 1781\u20131789.", "citeRegEx": "Yi et al\\.,? 2012", "shortCiteRegEx": "Yi et al\\.", "year": 2012}, {"title": "Rank/norm regularization with closed-form solutions: Application to subspace clustering", "author": ["Y. Yu", "D. Schuurmans"], "venue": "UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, 778\u2013785.", "citeRegEx": "Yu and Schuurmans,? 2011", "shortCiteRegEx": "Yu and Schuurmans", "year": 2011}, {"title": "Recovering the optimal solution by dual random projection", "author": ["L. Zhang", "M. Mahdavi", "R. Jin", "T. Yang", "S. Zhu"], "venue": "COLT, 135\u2013157.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al.", "startOffset": 17, "endOffset": 70}, {"referenceID": 19, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al. 2012; Xing et al. 2002), ranking (Chechik et al.", "startOffset": 83, "endOffset": 117}, {"referenceID": 17, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al. 2012; Xing et al. 2002), ranking (Chechik et al.", "startOffset": 83, "endOffset": 117}, {"referenceID": 1, "context": "2002), ranking (Chechik et al. 2010; Lim and Lanckriet 2014), etc.", "startOffset": 15, "endOffset": 60}, {"referenceID": 8, "context": "2002), ranking (Chechik et al. 2010; Lim and Lanckriet 2014), etc.", "startOffset": 15, "endOffset": 60}, {"referenceID": 9, "context": "Given the learned metric M , most of the DML methods compute the similarity in the form of Mahalanobis distance (Mahalanobis 1936): disM (xi,xj) =", "startOffset": 112, "endOffset": 130}, {"referenceID": 15, "context": "Additionally, the task of image retrieval is known to be non-symmetric according to human judgement (Tversky 1977).", "startOffset": 100, "endOffset": 114}, {"referenceID": 5, "context": "In this scenario, the similarity can be measured by the similarity function in bilinear form (Guo and Ying 2014; Chechik et al. 2010): SimM (xi,xj) = x \u22a4 i Mxj , which computes the inner product between two examples.", "startOffset": 93, "endOffset": 133}, {"referenceID": 1, "context": "In this scenario, the similarity can be measured by the similarity function in bilinear form (Guo and Ying 2014; Chechik et al. 2010): SimM (xi,xj) = x \u22a4 i Mxj , which computes the inner product between two examples.", "startOffset": 93, "endOffset": 133}, {"referenceID": 19, "context": "In this paper, we attempt to learn a good similarity function by solving a matrix regression problem (Yi et al. 2012).", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "\u2022 To alleviate the high dimensional challenge, we take the low rank assumption (Weinberger and Saul 2009) that the rank of the optimal M is significantly smaller than the dimensionality of data (i.", "startOffset": 79, "endOffset": 105}, {"referenceID": 17, "context": "The representative methods include Xing\u2019s method (Xing et al. 2002), POLA (ShalevShwartz, Singer, and Ng 2004), ITML (Davis et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "2002), POLA (ShalevShwartz, Singer, and Ng 2004), ITML (Davis et al. 2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 55, "endOffset": 74}, {"referenceID": 16, "context": "2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 12, "endOffset": 38}, {"referenceID": 8, "context": "2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 49, "endOffset": 73}, {"referenceID": 1, "context": "OASIS (Chechik et al. 2010) successfully applied it to the ranking problem, which requires that the more relevant examples have larger similarity and a hinge loss is adopted for a safety margin.", "startOffset": 6, "endOffset": 27}, {"referenceID": 19, "context": "This formulation is popular for similarity learning and widely used in different tasks (Yi et al. 2012; Feng, Jin, and Jain 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 20, "context": "Although this problem has the closed-form solution (Yu and Schuurmans 2011), the solution is only available for Frobenius norm and the computational cost is expensive (i.", "startOffset": 51, "endOffset": 75}, {"referenceID": 21, "context": "(Zhang et al. 2013) Let S \u2208 R be a standard Gaussian random matrix.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "According to Bernstein\u2019s inequality (Recht 2011), we have", "startOffset": 36, "endOffset": 48}, {"referenceID": 16, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al.", "startOffset": 7, "endOffset": 33}, {"referenceID": 8, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al.", "startOffset": 80, "endOffset": 104}, {"referenceID": 1, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al. 2010): Bilinear model for similarity learning; \u2022 SLR: Similarity learning with adaptive regression and alternating solver.", "startOffset": 174, "endOffset": 195}, {"referenceID": 1, "context": "We evaluate the learned metric on a ranking task, which is the same as in (Chechik et al. 2010).", "startOffset": 74, "endOffset": 95}, {"referenceID": 18, "context": "We extract LLC features (Yang et al. 2009) as representations for each image and reduce the dimension to 1, 000 by PCA to make the comparison with other methods.", "startOffset": 24, "endOffset": 42}, {"referenceID": 13, "context": "ImageNet50 consists of 50 randomly selected categories from the ImageNet dataset (Russakovsky et al. 2014).", "startOffset": 81, "endOffset": 106}], "year": 2015, "abstractText": "We study the problem of similarity learning and its application to image retrieval with large-scale data. The similarity between pairs of images can be measured by the distances between their high dimensional representations, and the problem of learning the appropriate similarity is often addressed by distance metric learning. However, distance metric learning requires the learned metric to be a PSD matrix, which is computational expensive and not necessary for retrieval ranking problem. On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem. Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee. For the high dimensional issue, we address it by taking low rank assumption and applying alternating method to obtain the partial matrix, which has a global optimal solution. Empirical studies on real world image datasets (i.e., Caltech and ImageNet) demonstrate the effectiveness and efficiency of the proposed method.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}