{"id": "1512.01768", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "Want Answers? A Reddit Inspired Study on How to Pose Questions", "abstract": "questions form an integral part of our everyday communication, both offline and online. getting responses to our questions from others is fundamental to satisfying our information need and in extending our knowledge boundaries. a question may may be represented using various factors such as social, syntactic, semantic, etc. we hypothesize that representing these factors contribute with varying degrees towards getting responses from others for a given question. we perform a thorough empirical study to measure effects of mapping these factors using a novel question and answer dataset obtained from the website named reddit. com. to the best of penetrating our knowledge, this is the first such analysis of its kind on this important topic. we also use a sparse nonnegative matrix factorization technique robust to continually automatically induce interpretable semantic factors from the question dataset. we also document various patterns on response prediction we observe during our analysis in the data. for instance, we found that preference - probing questions are scantily popularly answered. our analysis method is robust to capture such latent response factors. we hope to make our code and datasets publicly available upon publication of the paper.", "histories": [["v1", "Sun, 6 Dec 2015 10:31:12 GMT  (97kb)", "http://arxiv.org/abs/1512.01768v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["danish", "yogesh dahiya", "partha talukdar"], "accepted": false, "id": "1512.01768"}, "pdf": {"name": "1512.01768.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yogesh Dahiya", "Partha Talukdar"], "emails": ["danish037@gmail.com", "yogeshd2612@gmail.com", "ppt@serc.iisc.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n01 76\n8v 1\n[ cs\n.C L\n] 6\nD ec\n2 01\nI. INTRODUCTION\nQuestions and the responses they elicit are a ubiquitous and fundamental part of our everyday communication. Through such Questions and Answers (QA), we quench our curiosities, clarify doubts, validate our ideas, and seek advice, among others. It has been established that questions form an integral part in our quest to extend our knowledge boundaries [1]. It has also been observed that useful responses correspond to good questions [2]. This raises the following challenge: what factors constitute a good question which is more likely to elicit a response?\nImportance of asking right questions in specific settings have been previously explored, e.g., in classroom [3], and in corporate environment [4]. However, most of these studies either had no empirical evaluation at all or\notherwise consisted of very small samples.\nAlong with the growth of the World Wide Web (WWW), many large online QA sites, such as Yahoo Answers, Stack Overflow, Quora, etc., have been successful in connecting responders to inquirers who post questions on these sites. Such online QA forums may be categorized as Single Inquirer Multiple Responders (SIMR), where a question from a single user may be responded to by multiple other responders. Prior research have used datasets from these sites to analyze which response to a question is most likely to be selected as the best response [5] . However, analyzing factors of a question which are likely to elicit a response has been outside the scope of such prior work.\nTo address these shortcomings, in this paper we present an empirical analysis to determine factors of a question which are more likely to elicit a response. We make use of the IAmA subreddit of the popular Internet website Reddit.com. In each discussion thread of this online forum, a celebrity answers questions submitted by anonymous users. Thus, dataset from this subreddit may be categorized as Multiple Inquirers Single Responder (MISR). Such MISR datasets provide an ideal starting point to identify response-eliciting factors of a question, as the undesirable confounds produced due to the presence of multiple responders in SIMR datasets are not present in such MISR datasets.\nWe make the following contribution:\n\u2022 We address the important problem of automatically identifying response-eliciting factors of a question. We explore effectiveness of various factors, viz., orthographic, temporal, syntactic and also semantic of the question. To the best of our knowledge, this is the first such analysis of its kind.\n\u2022 We make use of a novel dataset, questions and responses from the IAmA subreddit of reddit.com. This MISR dataset provides additional benefits compared to SIMR datasets which have been explored in previous related research.\n\u2022 We provide a sparse, non-negative matrix factorization-based framework to automatically induce semantic factors of a question collection. Through extensive experiments on real datasets, we demonstrate that such factorizationbased technique results in significantly more interpretable factors compared to standard topic modeling techniques, such as Latent Dirichlet Allocation (LDA).\n\u2022 We hope to make all the code and datasets used in the paper publicly available upon publication of the paper."}, {"heading": "II. RELATED WORK", "text": "Studies on questioning techniques date back to Socrates [6], [7], who encouraged a systematic, disciplined, and deep questioning of fundamental concepts, theories, issues and problems. Socratic questioning is widely adopted in education and psychotherapy. Under the Socratic Questioning scheme [8], questions are grouped as follows: i) Clarifying questions, ones seeking further explanation, ii) Challenging the assumptions, questions that challenge the constraints, iii) Argument based questions, ones that reason behind the underlying theory or seek evidence, iv) Alternate viewpoints, questions that analyze the given scenario with an altogether different perspective, v) Implication and Consequence based questions\nSince Socrates, many different taxonomies have been discussed. Bloom\u2019s revised taxonomy given by Krathwohl [9] is based upon dividing questions into levels such that the amount of mental activity required to respond increases after each level. Their categories are \u2014 remembering, understanding, applying, analyzing, evaluating and creating. Nam et al.[10] group questions into Factual, Procedural, Opinion-oriented, Task-oriented and Advice related categories.\nRole of Socratic Techniques in thinking, teaching and learning has also been explored [11]. Hypothetical questions too have been studied independently and have been found to foster creativity [12]. While there has been considerable thought given over such demarcations and\nquestion formulation techniques, none of them are supported by any large datasets as most of the experiments were performed in a typical classroom sized setting.\nUnlike the mentioned qualitative analysis, Whittaker et al.[13] adopted a data-centric approach and uncovered the general demographic patterns among large samples of Usenet newsgroups. Some amount of research is also done on different QA forums like Yahoo! answers, e.g., [5] have proposed solutions to predict whether a particular answer will be chosen best by the inquirer or not.\nAlthough the aforementioned taxonomies are helpful in understanding general questioning paradigms, we are more curious about the qualities of a question that are more likely to generate a response. To the best of our knowledge, there have been no attempts to study questions with the objective of maximizing response rate.\nVariety of interesting questions have been studied using the Reddit conversation network. It has been used to understand how people react to online discussions[14], and to model the most reportable events in stories[15]. Domestic abuse analysis in [16] also was based upon Reddit.\nAn empirical case study to understand factors underlying successful favor requests online were studied in [17]. Like in that paper, we also make use of a subreddit as our primary dataset. Even though the setting explored in [17] is different than ours, this is probably the closest paper in motivation and spirit.\nUnlike other analysis on online forums including Reddit and Yahoo! answers, our dataset is unique as it falls into MISR category, where there is just one responder but multiple inquirers. To the best of our knowledge, this is a first attempt to understand any such dataset."}, {"heading": "III. DATASET", "text": "Reddit is the 26th most popular website, with about 36 million user accounts. It also comprises of over 9,000 subreddits which are sub-forums within reddit, these subreddits are focussed towards specific topics. Subreddits span diverse categories like News, Sports, Machine Learning etc. Reddit is also a home of subreddits like: ELIF (Explain like I\u2019m five), TIL (Today I learnt), AMA(Ask Me Anything) etc.\nVarious celebrities and noteworthy personalities have used reddit as a means to interact with the popular\ninternet crowd, such conversations fall under the AskMe-Anything and its variant subreddits. IAmA, AMA and casualama are 3 of the popular Ask-Me-Anything variants. IAmA is reserved for distinguished personalities, with an exception for people who have a truly interesting and unique event to take questions about. The other two AMA\u2019s are open to a more wider audience for sharing their life events and allowing other reddit users to ask questions related to those events.\nIAmA\u2019s is one of the most popular subreddits that has featured notable politicians, actors, directors, authors, businessmen, athletes and musicians. IAmA posts gain a lot of attention, and thousands of questions are asked in each IAmA post. But owing to time constraints, not all questions are answered. This gives us a good ground to understand and analyze what gets answered and what not.\nIn particular, we study four popular categories of celebrities \u2014 actors, authors, directors and politicians. In each category we analyse the top 50 upvoted posts, which aggregate over 110,000 questions, and the average reply rate is 10.16%. Since some questions arrive after the celebrity has moved out of the conversation, we ignore all the questions after the last successfully answered question. Reddit allows for threaded conversations, where users can comment over other comments. But to avoid any bias from the discourse of the comments in such threads, we ignore questions in deep threaded conversations and constrain ourselves to questions posted at the topmost level only. Since some comments also get posted at the topmost level, we only consider comments that have a question mark in them. Table I throws some light over the statistics about the questions we considered as a part of our study."}, {"heading": "IV. SUCCESS FACTORS OF QUESTIONS", "text": "In this section, we study various factors of questions that can result in healthy response rates. The factors we\nconsider range from orthographic, temporal, social, and syntactical, to semantic aspects.\nA. Orthographic Factors\nLength: Do short questions win over their longer variants, as the responder may not be interested in comprehending and then answering long questions? Or, are longer questions better as they offer more context? Are shorter and crisper questions more direct and focused and have a better chance at getting answered? We analyze the impact of question on response rate to answer the aforementioned question.\nB. Temporal Factors\nTime of Question: Does the time of asking question play any role in determining the response rate? We hypothesize that questions that are asked early on have far less competing questions and hence should have better chances of soliciting response.\nWe capture temporal information in two ways: (1) we note the fraction of questions answered in the IAmA before a given question is posted as an estimate of the time of question; (2) we use the fraction of time elapsed in the IAmA as another indicator of the time of the question. In most cases, we see that the time features complement each other.\nC. Social Factors\nPoliteness: Are polite questions more likely to generate a response? Or, is it the case that the default level of politeness expressed in the IAmA dataset already sufficient, and hence any additional politeness in the question is unlikely to positively affect response rate?\nPoliteness has been actively explored in the recent past in a variety of others research settings[18][19]. We employ the model introduced by Danescu-NiculescuMizil et al. [20] to measure politeness level of questions. This model bases its politeness score on the occurrences of greetings, apologies and hedges in the question.\nD. Syntactic Factors\nSyntactic: We ask whether questions that are simply formulated have better chances of getting answered? Syntactic features, such as parse tree depth, verb phrase depth, and their ratios [21], etc., have been used in past research as proxies for sentence complexity. In fact, such features have also been recently used to study syntactic\ncomplexity of reddit comments [15]. After generating constituent parse trees from the Stanford Corenlp package [22], we employ 16 such features to capture the essence of syntactic complexity in a given question.\nWe look at a few simple and a few complex sentences from the IAmA by President Barack Obama in Table II. We demonstrate how our features capture the varied levels of complexity. Since there can be various sentences and sub-questions in a given question, we calculate the average, maximum and minimum values of parse tree depths and verb phrase depths. It is because of such statistical aggregation techniques that we end up with 16 syntax features, but the basis of these features rest upon \u2014 parse tree of the sentence, verb phrase subtree and their ratios.\nE. Forum Factors\nRedundancy: Is a question which is very similar to already asked (or answered) questions in a given IAmA forum less likely to get a response? We think that is indeed the case and include factors in our analysis to account for question redundancy. Consider the questions in Table III asked to a popular Chef.\nAs the first few questions were not answered in the series of the above mentioned questions, it is nearly certain that the responder is not interesting in any such questions. By accounting for redundancy we hope to tackle similar and frequent scenarios.\nWe estimated the redundancy score of a given question as the maximum similarity score achieved with any of the other questions previously asked in the same IAmA.\nRelevance: For each IAmA, the responder usually posts a description to set the tone of the IAmA. We ask whether questions which are more aligned to the posted description more likely to receive a response? The posted descriptions usually carry information about the celebrity responder\u2019s current affiliation and engagements, and hence the hypothesis is that questions which are\nin line with such descriptions should outweigh other questions. In other words, relevant questions should attract more responses from the responder.\nFor both the relevance and redundancy factors, we came up with our own novel extension of Jaccard Similarity to account for sentence similarities. For two given sets A and B, the Jaccard Similarity is given by\nJ(A,B) = |A \u2229B|\n|A \u222aB| (1)\nFor our case, let A and B be sets of words corresponding to the two questions to be compared. Strictly, A\u2229B would translate to the count of the words matched across the sets of A and B. But consider the following two sentences: \u2013 How far is your workplace from your house? \u2013 How far is your office from your home?\nWith the strict definition, we would not be able to capture that the two sentences are completely similar, for all practical purposes. Hence we consider the Glove embeddings[23], and synset hierarchies to extend the scope of our matching. Two words are considered same, if (1) the two words are synonyms to each other and (2) if one word lies in top-K nearest neighbours of the other word in Glove embedding space.\nThis technique helps us to capture similarity of pairs like <home, house> and <office, workplace> and hence helps us better estimate the similarity of two sentences.\nF. Semantic Factors\nThe factors described so far consider various aspects of the questions being analyzed. However, none of them explicitly look at the semantic content of the question and perform analysis based on the semantic type of the question. For example, given questions of the following form posed to actors, \u201dwhat is your favorite movie?\u201d, \u201dwhat is your favorite book?\u201d, etc. we would like to group all such preference-probing questions into one category and then determine the response rate for such types of questions from actor responders. However, such categorization of questions are not readily available as we only have the list of question, and no additional annotation on top of them.\nIdeally, we would like to discover such categorical structure in the data automatically. Topic modeling techniques such as Latent Dirichlet Allocation (LDA)\n[24] may be employed to discover such latent structure in the question dataset. Given a set of questions, such techniques will induce topics as probability distribution over words. Ultimately, each question is going to be represented in terms of such induced topics. We note that interpretability, i.e., coherence among questions which share a given topic with high weights, is of paramount importance here as all subsequent response-rate analysis are going to be hinged on the label or meaning of each topic. Unfortunately, as we shall see in Section V, topics induced by LDA don\u2019t achieve the desired level of interpretability.\nTo overcome this limitation, we explore other latent factorization methods. Recently, Non-Negative Sparse Embedding (NNSE) [25], [26] has been proposed which tends to induce effective as well as interpretable embeddings. In order to apply NNSE to our question dataset, we first represent the data as a co-occurrence matrix X where rows correspond to questions and columns correspond to words. Each question is additionally augmented with word sense-restricted synsets from Wordnet. The effect after the synset extension from Wordnet can be seen in Table IV This extended co-occurrence matrix X is usually of very high dimension (e.g., 100k x 1m). We first reduce dimensionality of the matrix using sparse SVD. The number of dimensions in the SVD space is selected based on knee-plot analysis of eigenvalues obtained during SVD decomposition. The rank r approximation Xn\u00d7r obtained from SVD is then factorized into two matrices using NNSE, which minimize the following objective.\nargmin A,D\n1\n2\nn\u2211\ni=1\n\u2016 Xi,: \u2212Ai,: \u00d7D \u2016 2\nst : Di,:D T i,: \u2264 1,\u22001 \u2264 i \u2264 k\nAi,j \u2265 0, 1 \u2264 i \u2264 n, 1 \u2264 k\n\u2016 Ai: \u20161\u2264 \u03bb1, 1 \u2264 i \u2264 n\nwhere n is the number of questions, and k is the resulting number of latent factors induced by NNSE. We note that NNSE imposes non-negativity and sparsity penalty on the rows of matrix A. Though the objective represents a non-convex system, but when we solve for A with a fixed D (and vice versa) the loss function is convex. In such scenarios Alternating Minimization has been established to converge to a local optima [27], [25]. The solution for A is found with LARS implementation[28] of LASSO regression with non-negativity constrains; and D is found via gradient descent methods. The SPAMS package may be used for this optimization [29]. At the end of this process, Ai,j represents the membership weight of question i belonging to latent factor j."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we evaluate impact of various factors discussed in Section IV on response rate of questions from different domains.\nA. Is Response Rate Predictable?\nDatasets: We experiment with four popular domains \u2014 actors, authors, director and politicians. These do-\nmains covered more than 110,000 questions, and only about 10% of them generated a response. Statistics of the IAmA datasets are presented in Table I.\nMetric & Classifier: In order to measure response rate predictive power of a subset of factors, we train a L2 and L1 regularized (i.e., elastic net) classifier using only those subset of factors. Hyperparameters of the classifier is tuned using over a development set using grid search. We use area under the receiver operating characteristics\ncurve (ROC AUC) of the classifier on held out test data as our metric. This metric essentially measures how well the classifier ranks a randomly chosen positive question over a randomly chosen negative question. Please note that the dataset is highly skewed with significantly more negative questions than positive ones. This measure provides a balanced metric while accounting for the data skew.\nBaselines: To evaluate the strength and decisiveness of our probable factors, we test our system against\nthe random and bag-of-words (BoW) baselines. In the Random Baseline, each question is randomly given one of the two labels \u2014 answered or not answered.\nThe bag of words model comprises of each and every word in the vocabulary as a feature, hence aggregating up to thousands of features for every questions. Due to the large number of features, this Unigram model performs reasonably well, but it doesn\u2019t help us in answering our general question of \u2014 Which factors help a question get answered? \u2013 because the unigram features don\u2019t generalize to the factors that we are interested in evaluating.\nExperimental results comparing performance of the classifier with different features on multiple datasets are presented in Table VI. Based on this table, we discuss predictive capabilities of various factors below. Please refer to Section IV for description of the factors and how we computed them.\n1) Orthographic Factors: From Table VI, we observe that the length of the questions (measured in terms of numbers of tokens in the question), the only orthographic factor feature we considered, plays practically no role in influencing response rate. This is evident from the fact that the classifier with length as the only feature achieves AUC of 0.51 on average across all four domains\ncompared to AUC of 0.5 of the random classifier.\n2) Syntactic Factors: From Table VI, we clearly see that syntax-based features add very little little predictive power to the classifier (0.52 vs 0.50 of random). Though our syntax features are rigorous enough to capture the nuances of complexity (e.g., see Table II), but the responses to questions don\u2019t heavily depend on the complexity of the sentence. We observed that combining syntax with orthographic features also didn\u2019t increase predictive power.\n3) Temporal Factors: We find that temporal features play a significant role in the response rate. This is evident from Table VI where the classifier with temporal factor features achieves a significantly higher AUC score of 0.66 compared to random 0.5. As we had hypothesized earlier, questions that are asked early tend to be replied more often than others.\nIn addition to classifier\u2019s AUC score, we measured effect of temporal factors using Alternative Precision (AP) as well. For questions in a given domain, AP is computed over two ordering of the questions in that domain: (1) ordering of all questions based on the value of the temporal factor features; and (2) randomly shuffled question sequences. Percentage AP gains of the featurebased ranking over the random ranking (AP averaged over thousand trials) are summarized in Table V. From this, we observe the clear trend that temporal factor features significantly aid in response prediction, sometimes with gains as high as 218%. We think that the responder is initially exposed to far lesser number of questions compared to a situation in the middle or towards the end of the IAmA when the number of questions demanding his or her attention are huge.\n4) Forum Factors: Redundancy: Our dataset consists of prominent celebrities, and they gain undeniably high attention among Reddit users. Due to large participation, the number of similar questions is high, as many users wish to know similar facts, preferences, likings and happenings. Redundancy comes out as one of the most promising factors in understanding questions that get answered. Examples of a few redundant questions are shown in Section IV-E.\nThe original, and genuine questions, which are identified by our redundant factor feature, are heavily preferred over questions that are redundant and stale. This is established by the fact the classifier which accounts for redundancy achieves a significantly higher AUC score of 0.66 compared to the random baseline.\nRelevance: Relevance of the question, with the post description by the celebrity responder, show only faint signals with the response rate. The description given by the celebrities is usually very short to capture the variety of questions. Hence we don\u2019t see any meaningful dependencies between relevance and response rate (0.52 AUC).\nOverall, with all the three forum features included, the classifier achieves an AUC score of 0.68.\n5) Politeness: Politeness, a seemingly important cue for demystifying question qualities, surprisingly, didn\u2019t come out as a strong predictor of response rate. In Table VI the classifier with politeness forum factor feature achieves an AUC score of only 0.52. We have observed that the Reddit culture is very informal, frank and open. Hence, making requests extra polite might not help while framing questions in such scenarios. Of all domains, politeness is most important in the case of prominent politicians.\n6) Unigram: In addition to the factors mentioned above, we also experimented with the bag-of-wordsbased unigram model. As mentioned previously, in this case, each token of the question was added a feature. From Table VI, we observe that the unigram model achieves an AUC of 0.68 which is significantly better than the random baseline of 0.5. However, the Unigram model uses 13704 features (averaged across all four domains). It is encouraging to note that performance of this Unigram model with thousands of features is superseded by the classifier using only 4 form factor features (AUC 0.65 vs 0.68) in the response prediction task.\nB. Do Induced Semantic Factors Help Discover Response Trends?\nSo far, we have tried to handcraft the seemingly most important factors but we can never account for patterns other than what we are looking for. In any large dataset as ours, creating an exhaustive set that can capture all such factors is humanly impossible. Also for each factor, we need to train a system that can well detect and measure it in an unknown question. In such scenarios, the need to automatically discover latent dimensions is essential. As mentioned in Section IV-F, we use LDA and NNSE to induce semantic factors present in the question dataset. First we shall present comparisons between interpretability of factors induced by these two\nmethods. Subsequently, we shall measure the response predictive power of these induced semantic factors.\nLDA vs NNSE: We reiterate that finding latent factors that are interpretable is not just a luxury but a bare necessity in our setting as we need to understand what kind of latent semantic factors play a role in maximizing response rate. For this, we compared the latent factors induced by LDA and NNSE, examples of which are in Table VII. In this table, four randomly selected latent factors induced each by LDA and NNSE are shown. Also, for each latent factor, top two most active questions in that dimension are shown. For easy reference, the main theme of each question is manually marked in bold. From this table, we observe that NNSE is able to produce much more interpretable latent semantic factors compared LDA. Such lack of interpretability in LDA topics was also observed in another prior work [17]. Given the interpretability advantage with NNSE, we use the latent factors induced by this method in subsequent analysis.\nHaving successfully induced interpretable semantic factors using NNSE which have good number of questions attached to them, we analyzed the dimensions of questions with extremely high and extremely low reply rates. Please note that such latent factors are induced separately for each domain. Experimental results comparing NNSE latent factors in three domains, overall response rate in the domain, response rate over questions in the factor, and examples of top questions in each such factor are shown in Table VIII. Based on this table, we list below a few trends. We point out that this analysis and trend recognition would have been impossible without the ability to automatically induce interpretable semantic factors.\n1) Actors: We found that adulation techniques worked well in eliciting a response for actors: 15.88% response rate in Actor latent factor 524 in Table VIII compared to domain response rate of 5.19%. Based on the top questions in this factor, we can easily identify that this is a fan-related factor. Authors seem to reply more if the inquirer describes himself as a huge fan or if he expresses some liking for their movies and role. We also learnt that actors weren\u2019t very comfortable when it came to questions diving into their non-camera life (Actor factor 880). Also many actors were evasive when asked about their favorite actors, movies, meals etc (Author factor 852).\n2) Politicians: We observe that Politicians were prompt in clarifying all fund related issues pertaining to their campaigns (Politician factor 927 in Table VIII). Whereas not many politicians seemed to be happy in taking questions on wage rise and the job situations in the country (Politician factor 304).\n3) Author: We observe that many users inquired authors about how they can pursue a career in writing, even more asked about writing advices. We found that such questions were generously replied: 36.53% response rate in factor 742 of the Author domain, compared to domain response rate of 17.62%. Also, authors answered a lot of questions that questioned about their ideas, thoughts and preferences (Author factor 136). However, they were a little less responsive when asked about inspiration (factor 4) or favorites (factor 118). This might be attributed to the fact that questions of these types are extremely frequently posed to authors, and due to the redundancy, they may answer only a few of them (please note that the response rate in these factors are not 0).\nC. Summary of Results\nFrom Section V-A, we observe that all our designed factors in conjunction beat the Random and the Bagof-words baseline for all the domains. We also use far less features compared to the thousands of features in BoW (Unigram). This clearly demonstrates that we have arrived at a good mix of concise factors that are helpful in understanding response rate.\nFrom Section V-B, we see that our technique was able to capture some hard to find semantic factors that resulted in high reply rates. This also allowed us to identify factors in questions that are scantily replied."}, {"heading": "VI. CONCLUSION", "text": "Question-Answering forms an integral part of our everyday communication. While some questions elicit a lot of responses, many others go unanswered. In this paper, we present a large-scale empirical analysis to identify factors underlying response-eliciting questions. To the best of our knowledge, this is the first such analysis of its kind. In particular, we focus on the Multiple Inquirers Single Responder(MISR) online setting where there are multiple users asking questions to a single responder, and where the responder has a choice to not answer any particular question. We used a novel dataset from the website Reddit.com, and considered several factors underlying questions, viz., orthographic,\ntemporal, syntactic, and semantic. For semantic features, we used a sparse non-negative matrix factorization technique to automatically identify interpretable latent factors. Because of this automated analysis, we are able to observe a few interesting and non-trivial trends. For instance we observed that all the advice related questions were generously entertained by Authors, as long as they carried some context about their writing pursuits. Similarly Actors were keen on making people aware about the behind-the-scene events, whenever asked. These trends are hard to capture otherwise, as designing a system to detect such particular cases requires training over large annotated corpus.\nAs part of future work, we hope to explore other factorization techniques, e.g., hierarchical latent factors, for even more effective and interpretable latent factors. Additionally, we hope to use the insights gained in this study to explore how an existing question may be rewritten to elicit response from voluntary responders. We hope to make all the datasets and code publicly available upon publication of the paper."}, {"heading": "VII. ACKNOWLEDGEMENTS", "text": "This research is supported in part by gifts from Google Research and Accenture Technology Labs."}], "references": [{"title": "Learning concepts by asking questions", "author": ["C. Sammut", "R.B. Banerji"], "venue": "Machine learning: An artificial intelligence approach, vol. 2, pp. 167\u2013192, 1986.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Finding high-quality content in social media", "author": ["E. Agichtein", "C. Castillo", "D. Donato", "A. Gionis", "G. Mishne"], "venue": "Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008, pp. 183\u2013194.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Guiding knowledge construction in the classroom: Effects of teaching children how to question and how to explain", "author": ["A. King"], "venue": "American educational research journal, vol. 31, no. 2, pp. 338\u2013368, 1994.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "How to ask better questions", "author": ["J. Ross"], "venue": "Harvard Business Review Blogs. Viitattu, vol. 8, p. 2010, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge sharing and yahoo answers: everyone knows something", "author": ["L.A. Adamic", "J. Zhang", "E. Bakshy", "M.S. Ackerman"], "venue": "Proceedings of the 17th international conference on World Wide Web. ACM, 2008, pp. 665\u2013674.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Critical thinking: The art of socratic questioning", "author": ["R. Paul", "L. Elder"], "venue": "Journal of Developmental Education, vol. 31, no. 1, p. 36, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "What is socratic questioning?", "author": ["T.A. Carey", "R.J. Mullan"], "venue": "Psychotherapy: Theory, Research, Practice, Training,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Thinker\u2019s Guide to the Art of Socratic Questioning", "author": ["R. Paul", "L. Elder"], "venue": "Foundation Critical Thinking,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A taxonomy for learning, teaching, and assessing: A revision of Bloom\u2019s taxonomy of educational objectives", "author": ["L.W. Anderson", "D.R. Krathwohl", "B.S. Bloom"], "venue": "Allyn & Bacon,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Questions in, knowledge in?: a study of naver\u2019s question answering community", "author": ["K.K. Nam", "M.S. Ackerman", "L.A. Adamic"], "venue": "Proceedings of the SIGCHI conference on human factors in computing systems. ACM, 2009, pp. 779\u2013 788.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "The role of socratic questioning in thinking, teaching, and learning", "author": ["L. Elder", "R. Paul"], "venue": "The Clearing House, vol. 71, no. 5, pp. 297\u2013301, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Hypotheticals in cognitive psychotherapy: Creative questions, novel answers, and therapeutic change", "author": ["C.F. Newman"], "venue": "Journal of Cognitive Psychotherapy, vol. 14, no. 2, pp. 135\u2013 147, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "The dynamics of mass interaction", "author": ["S. Whittaker", "L. Terveen", "W. Hill", "L. Cherny"], "venue": "From Usenet to CoWebs. Springer, 2003, pp. 79\u201391.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling reportable events as turning points in narrative", "author": ["J. Ouyang", "K. McKeown"], "venue": "EMNLP, 20015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "An analysis of domestic abuse discourse on reddit", "author": ["N.S.C.O.A. Ray", "P.C.M. Homan"], "venue": "anxiety, vol. 4183, p. 23300.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2330}, {"title": "How to ask for a favor: A case study on the success of altruistic requests", "author": ["T. Althoff", "C. Danescu-Niculescu-Mizil", "D. Jurafsky"], "venue": "ICWSM, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Brief report gratitude and prosocial behaviour: An experimental test of gratitude", "author": ["J.-A. Tsang"], "venue": "Cognition & Emotion, vol. 20, no. 1, pp. 138\u2013148, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Gratitude and prosocial behavior helping when it costs you", "author": ["M.Y. Bartlett", "D. DeSteno"], "venue": "Psychological science, vol. 17, no. 4, pp. 319\u2013325, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "A computational approach to politeness with application to social factors", "author": ["C. Danescu-Niculescu-Mizil", "M. Sudhof", "D. Jurafsky", "J. Leskovec", "C. Potts"], "venue": "ACL, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, 2003, pp. 423\u2013430.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55\u201360.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), vol. 12, pp. 1532\u20131543, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding.", "author": ["B. Murphy", "P.P. Talukdar", "T.M. Mitchell"], "venue": "in COLING,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "A compositional and interpretable semantic space", "author": ["A. Fyshe", "L. Wehbe", "P.P. Talukdar", "B. Murphy", "T.M.  Mitchell"], "venue": "Proceedings of the NAACL-HLT, Denver, USA, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 19\u201360, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse coding and dictionary learning for image analysis", "author": ["F. Bach", "J. Mairal", "J. Ponce", "G. Sapiro"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "It has been established that questions form an integral part in our quest to extend our knowledge boundaries [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "It has also been observed that useful responses correspond to good questions [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": ", in classroom [3], and in corporate environment [4].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": ", in classroom [3], and in corporate environment [4].", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Prior research have used datasets from these sites to analyze which response to a question is most likely to be selected as the best response [5] .", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "Studies on questioning techniques date back to Socrates [6], [7], who encouraged a systematic, disciplined, and deep questioning of fundamental concepts, theories, issues and problems.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Studies on questioning techniques date back to Socrates [6], [7], who encouraged a systematic, disciplined, and deep questioning of fundamental concepts, theories, issues and problems.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Under the Socratic Questioning scheme [8], questions are grouped as follows: i) Clarifying questions, ones seeking further explanation, ii) Challenging the assumptions, questions that challenge the constraints, iii) Argument based questions, ones that reason behind the underlying theory or seek evidence, iv) Alternate viewpoints, questions that analyze the given scenario with an altogether different perspective, v) Implication and Consequence based questions", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Bloom\u2019s revised taxonomy given by Krathwohl [9] is based upon dividing questions into levels such that the amount of mental activity required to respond", "startOffset": 44, "endOffset": 47}, {"referenceID": 9, "context": "[10] group questions into Factual, Procedural, Opinion-oriented, Task-oriented and Advice related categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Role of Socratic Techniques in thinking, teaching and learning has also been explored [11].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Hypothetical questions too have been studied independently and have been found to foster creativity [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "[13] adopted a data-centric approach and uncovered the general demographic patterns among large samples of Usenet newsgroups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": ", [5] have proposed solutions to predict whether a particular answer will be chosen best by the inquirer or not.", "startOffset": 2, "endOffset": 5}, {"referenceID": 13, "context": "and to model the most reportable events in stories[15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "Domestic abuse analysis in [16] also was based upon Reddit.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "An empirical case study to understand factors underlying successful favor requests online were studied in [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Even though the setting explored in [17] is different than ours, this is probably the closest paper in motivation and spirit.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "Politeness has been actively explored in the recent past in a variety of others research settings[18][19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Politeness has been actively explored in the recent past in a variety of others research settings[18][19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "[20] to measure politeness level of questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "formulated have better chances of getting answered? Syntactic features, such as parse tree depth, verb phrase depth, and their ratios [21], etc.", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "complexity of reddit comments [15].", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "After generating constituent parse trees from the Stanford Corenlp package [22], we employ 16 such features to capture the essence of syntactic complexity in a given question.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "embeddings[23], and synset hierarchies to extend the scope of our matching.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "[24] may be employed to discover such latent structure in the question dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Recently, Non-Negative Sparse Embedding (NNSE) [25], [26] has been proposed which tends to induce effective as well as interpretable embeddings.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "Recently, Non-Negative Sparse Embedding (NNSE) [25], [26] has been proposed which tends to induce effective as well as interpretable embeddings.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "In such scenarios Alternating Minimization has been established to converge to a local optima [27], [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "In such scenarios Alternating Minimization has been established to converge to a local optima [27], [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "The solution for A is found with LARS implementation[28] of LASSO regression with non-negativity constrains; and D is found via gradient descent methods.", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "The SPAMS package may be used for this optimization [29].", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Such lack of interpretability in LDA topics was also observed in another prior work [17].", "startOffset": 84, "endOffset": 88}], "year": 2015, "abstractText": "Questions form an integral part of our everyday communication, both offline and online. Getting responses to our questions from others is fundamental to satisfying our information need and in extending our knowledge boundaries. A question may be represented using various factors such as social, syntactic, semantic, etc. We hypothesize that these factors contribute with varying degrees towards getting responses from others for a given question. We perform a thorough empirical study to measure effects of these factors using a novel question and answer dataset from the website Reddit.com. To the best of our knowledge, this is the first such analysis of its kind on this important topic. We also use a sparse nonnegative matrix factorization technique to automatically induce interpretable semantic factors from the question dataset. We also document various patterns on response prediction we observe during our analysis in the data. For instance, we found that preference-probing questions are scantily answered. Our method is robust to capture such latent response factors. We hope to make our code and datasets publicly available upon publication of the paper.", "creator": "LaTeX with hyperref package"}}}