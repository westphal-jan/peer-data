{"id": "1512.02011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies", "abstract": "comparisons using deep neural quantum nets as transition function approximator for reinforcement learning tasks systems have recently been shown to be surprisingly very powerful for solving problems approaching real - world complexity. using these results as also a benchmark, we discuss the tremendous role that the discount factor may play in the quality variability of the learning process of a deep q - channel network ( inverse dqn ). ideally when the discount factor progressively naturally increases markedly up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. when used in conjunction with a varying learning confidence rate, we empirically show that it outperforms original dqn on several experiments. we relate this phenomenon with imagining the instabilities of neural networks perceived when they are used in an approximate dynamic programming setting. we also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with setting the exploration / exploitation dilemma.", "histories": [["v1", "Mon, 7 Dec 2015 12:25:18 GMT  (954kb,D)", "http://arxiv.org/abs/1512.02011v1", "NIPS 2015 Deep Reinforcement Learning Workshop"], ["v2", "Wed, 20 Jan 2016 10:33:00 GMT  (959kb,D)", "http://arxiv.org/abs/1512.02011v2", "NIPS 2015 Deep Reinforcement Learning Workshop"]], "COMMENTS": "NIPS 2015 Deep Reinforcement Learning Workshop", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vincent fran\\c{c}ois-lavet", "raphael fonteneau", "damien ernst"], "accepted": false, "id": "1512.02011"}, "pdf": {"name": "1512.02011.pdf", "metadata": {"source": "CRF", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies", "authors": ["Vincent Fran\u00e7ois-Lavet", "Raphael Fonteneau"], "emails": ["v.francois@ulg.ac.be", "raphael.fonteneau@ulg.ac.be", "dernst@ulg.ac.be"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning is a learning paradigm aiming at learning optimal behaviors while interacting within an environment [2]. One of the main challenge met when designing reinforcement learning algorithms is the fact that the state space may be very large or continuous, potentially leading to the fact that state(-action) value functions may not be represented comprehensively (for instance, using lookup tables) [3]. Recently, it has been empirically demonstrated that using deep neural networks as a function approximator may be very powerful in situations with high-dimensional sensory inputs [1]. However, one of the main drawback of using (deep) neural networks as function approximator is that they may potentially become unstable when combined with a Q-Learning-type recursion [4].\nIn this paper, we propose to discuss the role that the discount factor may play in the stability and convergence of deep reinforcement learning algorithms. We empirically show that an increasing discount factor has the potential to improve the quality of the learning process. We also discuss the link between the discount factor influence, the instabilities of (deep) neural networks and the vanilla exploration/exploitation dilemma.\nA motivation for this work comes from empirical studies about the attentional and cognitive mechanisms in delay of gratification. One well known experiment in the domain was a series of studies in which a child was offered a choice between one small reward provided immediately or two small rewards if they waited for a short period (\"marshmallow experiment\" [5]). The capacity to wait longer for the preferred rewards seems to develop markedly only at about ages 3-4. By 5 years old, most children are able to demonstrate better self-control by gaining control over their immediate\nar X\niv :1\n51 2.\n02 01\n1v 1\n[ cs\n.L G\n] 7\ndesires. According to this theory, it seems plausible that following immediate desires at a young age is a better way to develop its abilities and that delaying strategies is only advantageous afterwards when pursuing longer term goals. Similarly, reinforcement learning may also have an advantage of starting to learn by maximizing rewards on a short-term horizon and progressively giving more weights to delayed rewards.\nThe remaining of the paper is organized the following: we first recall the main equations used in the deep reinforcement learning problem formulation originally introduced in [1]. We then discuss the factors that influence the stability of (deep) neural networks. In light of this analysis, we suggest the possibility to modify the discount factor along the way to convergence up to its final value in order to speed up the learning process. To illustrate our approach, we use the benchmark proposed in [1]. In this context, we then discuss the role of the learning rate as well as the level of exploration."}, {"heading": "2 Problem Formulation", "text": "We consider tasks in which an agent interacts with an environment so as to maximize expected future rewards\nQ\u2217(s, a) = max \u03c0 E[rt, \u03b3rt+1, \u03b32rt+2 + . . . |st = s, at = a, \u03c0] (1)\nwhich is the maximum sum of rewards r discounted by \u03b3 at each time-step t, achievable by a behaviour stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1] where \u03c0(s, a) denotes the probability that action a may be chosen by policy \u03c0 in state s.\nIn this paper, the task is described by a time-invariant stochastic discrete-time system whose dynamics can be described by the following equation:\nst+1 = f(st, at, wt) (2)\nwhere for all t, st is an element of the state space S, the action at is an element of the action space A and the random disturbance wt is an element of the disturbance space W generated by a timeinvariant conditional probability distribution wt \u223c P (.|s, a). In our experiments, the dynamics will be given by the Atari emulator, the state space will be based on observed pixels of the screen and the action space is the set of legal game actions A = {1, ...,K}. The (unique) solution of the Bellman equation for the Q-value function [6] is given by:\nQ\u2217(s, a) = (HQ\u2217)(s, a) (3)\nwhere H is an operator mapping any function K : S \u00d7A\u2192 R and defined as follows:\n(HK)(s, a) = E[r(s, a, w) + \u03b3max a\u2032\u2208A\nK(f(s, a\u2032, w), a\u2032)] (4)\nFor most problems approaching real-world complexity, we need to learn a parameterized value function Q(s, a; \u03b8k). General function-approximation system such as neural networks are well suited to deal with high-dimensional sensory inputs. In addition, they work readily online, i.e. they can make use of additional samples obtained as learning happens by using only one supervised learning problem. In the case where a neural network Q(s, a; \u03b8k) is used to aim at convergence towards Q\u2217(s, a), the parameters \u03b8k may be updated by stochastic gradient descent (or a variant), updating the current value Q(s, a; \u03b8k) towards a target value Y Q k = r(s, a, w) + \u03b3 argmaxa\u2032\u2208AQ(f(s, a \u2032, w), a\u2032; \u03b8\u2212k ) where \u03b8\u2212k refers to parameters from some previous Q-network. The Q-learning update when using the squared-loss amounts in updating the weights :\n\u03b8k+1 = \u03b8k + \u03b1(Y Q k \u2212Q(s, a; \u03b8k))\u2207\u03b8kQ(s, a; \u03b8k) (5)\nwhere \u03b1 is a scalar step size called the learning rate. A sketch of the algorithm is given in Figure 1."}, {"heading": "3 Instabilities of the online neural fitted Q-learning", "text": "The Q-learning rule from Equation 5 can be directly implemented online using a neural network (as is the case for the Deep Q-Network). However, due to the generalization and extrapolation abilities of neural nets, they can build unpredictable changes at different places in the state-action space. It is\nknown that errors may be propagated and that this may even become unstable. Additional care must therefore be taken since it can not be guaranteed that the current estimation for the accumulated costs always underestimates the optimal cost, and therefore convergence is not assured [7, 8]. These convergence problems are verified experimentally and it has been reported that convergence may be slow or even unreliable with this online update rule [9].\nWe offer an analysis based on the complexity of the policy class [10] via a parallel with machine learning. High complexity machine learning techniques have the ability to represent their training set well, but at risk of overfitting. In contrast, models with lower complexity do not tend to overfit, but may fail to capture important features. In the reinforcement learning setting, an equivalence with the model complexity of the learning method may be seen in the policy class complexity. Specifically, in the context of online neural fitted reinforcement learning, the discount factor as well as the neural network architecture control the number of possible policies. Similarly to the bias-variance tradeoff observed in supervised learning, reinforcement learning also faces a trade-off between large policy class complexity and low policy class complexity.\nIt is already known that the optimal solution may actually be found in the set of policy with a planning horizon \u03b3 smaller than the evaluation horizon \u03b3eval specified by the problem formulation in case of only inaccurate information on the actual dynamics is available a priori (e.g. in the case where a small number of tuples is available) [10]. It is also well known that the longer the planning horizon, the greater the computational expense of computing an optimal policy [11].\nIn the case of neural fitted value learning, the difficulty to target a high policy class complexity is severe because targeting a high discount factor leads to propagation of errors and instabilities. Some practical ways to prevent instabilities make use of a replay memory, clipping the error term, a separate target Q-network in Equation 5 and a convolutional network architecture [1]. Using the double Q-learning algorithm also help reducing overestimations of Q-value function caused by the generalization exageration of the regression method [12].\nIn this paper, we investigate the tradeoff between performance of the targeted policy versus stability and speed of the learning process. We investigate the possibility to reduce instabilities during learning by working on an adaptive discount factor so as to soften the errors through learning. The goal is to target a high policy class complexity while reducing error propagations during the Deep Q learning iterations."}, {"heading": "4 Experiments", "text": "The deep Q-learning algorithm described in [1] is used as a benchmark. All hyperparameters are kept identical if not stated differently. The main modification comes from the discount factor which is increased at every epoch (250 000 steps) with the following formula:\n\u03b3k+1 = 1\u2212 0.98(1\u2212 \u03b3k) (6)\nThe learned policies are then evaluated for 125000 steps with an -greedy policy identical to the original benchmark with test = 0.05. The reported scores are the highest average episode score of each simulation where these evaluation episodes were not truncated at 5 min. Each game is simulated 5 times for each configuration with varying seeds and results are reported in Figure 2. It can be observed that by simply using an increasing discount factor, learning is faster for four out of the five tested games and similar for the remaining game. We conclude that by starting with a low discount factor, we obtain faster policy improvement thanks to less instability. In addition, this also provides more robustness with respect to neural network weights initialisation."}, {"heading": "4.1 Convergence of the neural network", "text": "We now discuss the stability of DQN. We use the experimental rule from Equation 6 and either let \u03b3 increase or keep it constant when it attains 0.99 (at 20M steps). This is illustrated on Figure 3. It is shown that increasing \u03b3 without additional care degrades severely the score obtained beyond \u03b3 \u2248 0.99. By looking at the average V value, it can be seen that overestimation is particularly severe which causes the poor policies."}, {"heading": "4.2 Further improvement with an adaptive learning rate", "text": "Since instabilities are only severe when the discount factor is high, we now study the possibility to use a more aggressive learning rate in the neural network when working at a low discount factor because potential errors would have less impact at this stage. The learning rate is then reduced along with the increasing discount factor so as to end up with a stable neural Q-learning function.\nWe start with a learning rate of 0.005, i.e. twice as big as the one considered in the original benchmark and we use the following simple rule at every epoch:\n\u03b1k+1 = 0.98\u03b1k\nWith \u03b3 that follows Equation 6 and is kept constant when it attains 0.99, we manage to improve further the score obtained on all of the games tested. Results are reported in Figure 4 and an illustration is given for two different games in Figure 5. It can be noted that the value function V decreases when \u03b3 is hold fixed and when the learning rate is lowered which is a sign of a decrease of the overestimations of the Q-value function."}, {"heading": "4.3 Exploration / Exploitation Dilemma", "text": "Errors in the policy may also have positive impacts since it increases exploration [13]. When using a lower discount factor, it actually decreases exploration and opens up the risk of falling in a local optimum in the value iteration learning. This is observed as the agent gets repeatedly a score lower than the optimal while being unable to discover some parts of the state space.\nIn this case, an actor-critic-type algorithm that increases the level of exploration may allow to overcome this problem. We believe that an actor critic agent that also manages adaptively the level of exploration is important to further improve deep reinforcement learning algorithms. In order to illustrate this, a simple rule has been applied in the case of the game seaquest as can be seen on Figure 6. This rule adapts the exploration during the training process in the -greedy action selection until the agent was able to get out of the local optimum."}, {"heading": "4.4 Towards an actor-critic algorithm", "text": "Following the ideas discussed above, Figure 7 represents the general update scheme that we propose to further improve the performance of deep reinforcement learning algorithms."}, {"heading": "5 Conclusion", "text": "This paper introduced an approach to speed-up the convergence and improve the quality of the learned Q-function in deep reinforcement learning algorithms. It works by adapting the discount factor and the learning rate along the way to convergence. We used the deep Q-learning algorithms for Atari 2600 computer games as a benchmark and our approach showed improved performances for the 6 tested games. These results motivate further experiments, in particular it would be interesting to develop an automatic way to adapt online the discount factor along with the learning rate and possibly the level of exploration. It would also be of interest to combine this approach with the recent advances in deep reinforcement learning : the massively parallel architecture \"Gorilla\" [14],\nthe double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17]."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Reinforcement learning and dynamic programming using function approximators, volume 39", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter", "Damien Ernst"], "venue": "CRC press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Cognitive and attentional mechanisms in delay of gratification", "author": ["Walter Mischel", "Ebbe B Ebbesen", "Antonette Raskoff Zeiss"], "venue": "Journal of personality and social psychology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Approximate solutions to markov decision processes", "author": ["Geoffrey J Gordon"], "venue": "Robotics Institute,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The dependence of effective planning horizon on model accuracy", "author": ["Nan Jiang", "Alex Kulesza", "Satinder Singh", "Richard Lewis"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A sparse sampling algorithm for nearoptimal planning in large markov decision processes", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Y Ng"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "Silver David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Deep recurrent Q-learning for partially observable MDPs", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Reinforcement learning is a learning paradigm aiming at learning optimal behaviors while interacting within an environment [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "One of the main challenge met when designing reinforcement learning algorithms is the fact that the state space may be very large or continuous, potentially leading to the fact that state(-action) value functions may not be represented comprehensively (for instance, using lookup tables) [3].", "startOffset": 288, "endOffset": 291}, {"referenceID": 0, "context": "Recently, it has been empirically demonstrated that using deep neural networks as a function approximator may be very powerful in situations with high-dimensional sensory inputs [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "However, one of the main drawback of using (deep) neural networks as function approximator is that they may potentially become unstable when combined with a Q-Learning-type recursion [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "One well known experiment in the domain was a series of studies in which a child was offered a choice between one small reward provided immediately or two small rewards if they waited for a short period (\"marshmallow experiment\" [5]).", "startOffset": 229, "endOffset": 232}, {"referenceID": 0, "context": "The remaining of the paper is organized the following: we first recall the main equations used in the deep reinforcement learning problem formulation originally introduced in [1].", "startOffset": 175, "endOffset": 178}, {"referenceID": 0, "context": "To illustrate our approach, we use the benchmark proposed in [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "which is the maximum sum of rewards r discounted by \u03b3 at each time-step t, achievable by a behaviour stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1] where \u03c0(s, a) denotes the probability that action a may be chosen by policy \u03c0 in state s.", "startOffset": 131, "endOffset": 137}, {"referenceID": 5, "context": "Additional care must therefore be taken since it can not be guaranteed that the current estimation for the accumulated costs always underestimates the optimal cost, and therefore convergence is not assured [7, 8].", "startOffset": 206, "endOffset": 212}, {"referenceID": 6, "context": "Additional care must therefore be taken since it can not be guaranteed that the current estimation for the accumulated costs always underestimates the optimal cost, and therefore convergence is not assured [7, 8].", "startOffset": 206, "endOffset": 212}, {"referenceID": 7, "context": "These convergence problems are verified experimentally and it has been reported that convergence may be slow or even unreliable with this online update rule [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "We offer an analysis based on the complexity of the policy class [10] via a parallel with machine learning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "in the case where a small number of tuples is available) [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "It is also well known that the longer the planning horizon, the greater the computational expense of computing an optimal policy [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "Some practical ways to prevent instabilities make use of a replay memory, clipping the error term, a separate target Q-network in Equation 5 and a convolutional network architecture [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 10, "context": "Using the double Q-learning algorithm also help reducing overestimations of Q-value function caused by the generalization exageration of the regression method [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "The deep Q-learning algorithm described in [1] is used as a benchmark.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "Errors in the policy may also have positive impacts since it increases exploration [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "It would also be of interest to combine this approach with the recent advances in deep reinforcement learning : the massively parallel architecture \"Gorilla\" [14],", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 152, "endOffset": 156}], "year": 2017, "abstractText": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.", "creator": "LaTeX with hyperref package"}}}