{"id": "1512.04466", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Semisupervised Autoencoder for Sentiment Analysis", "abstract": "in this paper, we investigate the usage of autoencoders in modeling textual data. traditional reference autoencoders suffer from at least two aspects : scalability difficulty with the high dimensionality of vocabulary size and dealing with task - irrelevant words. we address this problem by introducing supervision via the loss function of autoencoders. in particular, we first train a linear classifier on the narrowly labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. to reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with laplace closure approximation. we show that our choice of loss function can be rationalized from the perspective of bregman divergence, which justifies the soundness of our model. we evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to performance classification accuracy. we also show that our statistical model is able to take advantage of unlabeled dataset and get improved performance. we further show something that our model successfully only learns highly optimal discriminative feature maps, which explains arguably its superior performance.", "histories": [["v1", "Mon, 14 Dec 2015 19:09:53 GMT  (25kb)", "http://arxiv.org/abs/1512.04466v1", "To appear in AAAI 2016"]], "COMMENTS": "To appear in AAAI 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": true, "id": "1512.04466"}, "pdf": {"name": "1512.04466.pdf", "metadata": {"source": "CRF", "title": "Semisupervised Autoencoder for Sentiment Analysis", "authors": ["Shuangfei Zhai", "Zhongfei (Mark) Zhang"], "emails": ["szhai2@binghamton.edu", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n04 46\n6v 1\n[ cs\n.L G\n] 1\n4 D\nec 2"}, {"heading": "Introduction", "text": "In machine learning, documents are usually represented as Bag of Words (BoW), which nicely reduces a piece of text with arbitrary length to a fixed length vector. Despite its simplicity, BoW remains the dominant representation in many applications including text classification. There has also been a large body of work dedicated to learning useful representations for textual data (Turney and Pantel 2010; Blei, Ng, and Jordan 2003; Deerwester et al. 1990; Mikolov et al. 2013; Glorot, Bordes, and Bengio 2011). By exploiting the co-occurrence pattern of words, one can learn a low dimensional vector that forms a compact and meaningful representation for a document. The new representation is often found useful for subsequent tasks such as topic visualization and information retrieval. In this paper, we investigate the application of one of the most popular representation learning methods, namely autoencoders\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n(Bengio 2009), to learn task-dependent representations for textual data. Our model differs from most of the existing work as it naturally incorporates label information into its objective function, which allow the learned representation to be directly coupled with the task of interest.\nIn this paper we focus on a specific class of task in text mining: Sentiment Analysis (SA). We further focus on a special case of SA as a binary classification problem, where a given piece of text is either of positive or negative attitude. This problem is interesting largely due to the emergence of online social networks, where people consistently express their opinions about certain subjects. Also, it is easy to obtain a large amount of clean labeled data for SA by crawling reviews from websites such as IMDB or Amazon. Thus, SA is an ideal benchmark for evaluating text classification models (and features).\nAutoencoders have attracted a lot of attention in recent years as a building block of Deep Learning (Bengio 2009). They act as the feature learning methods by reconstructing inputs with respect to a given loss function. In a neural network implementation of autoencoders, the hidden layer is taken as the learned feature. While it is often trivial to obtain good reconstructions with plain autoencoders, much effort has been devoted on regularizations in order to prevent them against overfitting (Bengio 2009; Vincent et al. 2008; Rifai et al. 2011b). However, little attention has been devoted to the loss function, which we argue is critical for modeling textual data. The problem with the commonly adopted loss functions (squared Euclidean distance and element-wise KL Divergence, for instance) is that they try to reconstruct all dimensions of input independently and undiscriminatively. However, we argue that this is not the optimal approach when our interest is text classification. The reason is two folds. First, it is well known that in natural language the distribution of word occurrences follows the power-law. This means that a few of the most frequent words will account for most of the probability mass of word occurrences. An immediate result is that the Autoencoder puts most of its effort on reconstructing the most frequent words well but (to a certain extent) ignores the less frequent ones. This may lead to a bad performance especially when the class distribution is not well captured by merely the frequent words. For sentiment analysis, this problem is especially severe because it is obvious that the truly useful features (words or phrases\nexpressing a clear polarity) only occupy a small fraction of the whole vocabulary; and reconstructing irrelevant words such as \u2019actor\u2019 or \u2019movie\u2019 very well is not likely to help learn more useful representations to classify the sentiment of movie reviews. Second, explicitly reconstructing all the words in an input text is expensive, because the latent representation has to contain all aspects of the semantic space carried by the words, even if they are completely irrelevant. As the vocabulary size can easily reach the range of tens of thousands even for a moderate sized dataset, the hidden layer size has to be chosen very large to obtain a reasonable reconstruction, which causes a huge waste of model capacity and makes it difficult to scale to large problems.\nIn fact, the reasoning above applies to all the unsupervised learning methods in general, which we argue is one of the most important problems to address in order to learn task-specific representations. This naturally leads us to the semisupervised approach, where label information is introduced to guide the feature learning procedure. In particular, we propose a novel loss function for training autoencoders that are directly coupled with the classification task. We first train a linear classifier on BoW, then a Bregman Divergence (Banerjee et al. 2004) is derived as the loss function of a subsequent autoencoder. The new loss function gives the autoencoder the information about directions along which the reconstruction should be accurate, and where larger reconstruction errors are tolerated. Informally, this can be considered as a weighting of words based on their correlations with the class label: predictive words should be given large weights in the reconstruction even they are not frequent words, and vice versa. Furthermore, to reduce the bias introduced by the linear classifier, we take a Bayesian view by defining a posterior distribution on the weights of the classifier. We then approximate the posterior with Laplace approximation and derive the marginalized loss function for the autoencoder. We show that our model successfully learns features that are highly discriminative with respect to class labels, and also outperform all the competing methods evaluated by classification accuracy. Moreover, the derived loss can also be applied to unlabeled data, which allows the model to learn further better representations."}, {"heading": "Model", "text": ""}, {"heading": "Denoising Autoencoders", "text": "Autoencoders learn functions that can reconstruct the inputs. They are typically implemented as a neural network with one hidden layer, and one can extract the activation of the hidden layer as the new representation. Mathematically, we are given a collection of data points X = {xi}, xi \u2208 Rd, i \u2208 [1,m], the objective function of an autoencoder is thus:\nmin \u2211\ni\nD(x\u0303i, xi)\ns.t. hi = g(Wxi + b), x\u0303i = f(W \u2032hi + b \u2032)\n(1)\nwhere W \u2208 Rk\u00d7d, b \u2208 Rk,W \u2032 \u2208 Rd\u00d7k, b\u2032 \u2208 Rd are the parameters to be learned; D is a loss function, such as the squared Euclidean Distance \u2016x\u0303\u2212x\u201622; g and f are predefined nonlinear functions, which we set as g(x) = max(0, x),\nf(x) = (1+exp(\u2212x))\u22121 in this paper; hi is the learned representation; x\u0303i is the reconstruction. A common approach is to use tied weights by setting W = W \u2032; this usually works better as it speeds up learning and prevents overfitting at the same time. For this reason, we always use tied weights in this paper.\nAutoencoders transform an unsupervised learning problem to a supervised one by the self reconstruction criteria. This enables one to use all the tools developed for supervised learning such as back propagation to efficiently train the autoencoders. Moreover, thanks to the nonlinear functions f and g, autoencoders are able to learn non-linear and possibly overcomplete representations, which give the model much more expressive power than their linear counter parts such as PCA (LSA) (Deerwester et al. 1990).\nIn this paper, we adopt one of the most popular variants of autoencoders, namely Denoising Autoencoder. Denoising Autoencoder works by reconstructing the input from a noised version of itself. The intuition is that a robust model should be able to reconstruct the input well even in the presence of noises, due to the high correlation among features. For example, imagine deleting or adding a few words from/to a document, the semantics should still remain unchanged, thus the autoencoder should learn a consistent representation from all the noisy inputs. In the high level, Denoising Autoencoders are equivalent to ordinary autoencoders trained with dropout (Srivastava et al. 2014), which has been shown as an effective regularizer for (deep) neural networks. Formally, let q(x\u0304|x) be a predefined noising distribution, and x\u0304 be a noised sample of x: x\u0304 \u223c q(x\u0304|x). The objective function takes the form of sum of expectations over all the noisy samples:\nmin \u2211\ni\nEq(x\u0304i|xi)D(x\u0303i, xi)\ns.t. hi = g(Wx\u0304i + b), x\u0303i = f(W \u2032hi + b \u2032)\n(2)\nwhere we have slightly overloaded the notation to let x\u0303i denote the reconstruction calculated from the noised input x\u0304i. While the marginal objective function requires infinite many noised samples per data point, in practice it is sufficient to simulate it stochastically. That is, for each example seen in the stochastic gradient descent training, we randomly sample a x\u0304i from q(x\u0304i|xi) and calculate the gradient with ordinary back propagation."}, {"heading": "Loss Function as Bregman Divergence", "text": "We then discuss the proper choice of the loss function D in (2) as a specific form of Bregman Divergence. Bregman Divergence (Banerjee et al. 2004) generalizes the notion of distance in a d dimensional space. To be concrete, given two data points x\u0303, x \u2208 Rd and a convex function f(x) defined on Rd, the Bregman Divergence of x\u0303 from x with respect to f is:\nDf (x\u0303, x) = f(x\u0303)\u2212 (f(x) +\u2207f(x) T (x\u0303\u2212 x)). (3)\nNamely, Bregman Divergence measures the distance between two points x\u0303, x as the deviation between the function value of f and the linear approximation of f around x at x\u0303.\nTwo of the most commonly used loss functions for autoencoders are the squared Euclidean distance and elementwise KL divergence. It is not difficult to verify that they both fall into this family by choosing f as the squared \u21132 norm and the sum of element-wise entropy respectively. What the two loss functions have in common is that they make no distinction among dimensions of the input. In other words, each dimension of the input is pushed to be reconstructed equally well. While autoencoders trained in this way have been shown to work very well on image data, learning much more interesting and useful features than the original pixel intensity features, they are less appropriate for modeling textual data. The reason is two folds. First, textual data are extremely sparse and high dimensional, where the dimensionality is equal to the vocabulary size. To maintain all the information of the input in the hidden layer, a very large layer size must be adopted, which makes the training cost extremely large. Second, ordinary autoencoders are not able to deal with the power law of word distributions, where a few of the most frequent words account for most of the word occurrences. As a result, frequent words naturally gain favor to being reconstructed accurately, and rare words tend to be reconstructed with less precision. This problem is also analogous to the imbalanced classification setting. This is especially problematic when frequent words carry little information about the task of interest, which is not uncommon. Examples include stop words (the, a, this, from) and topic related terms (movie, watch, actress) in a movie review sentiment analysis task."}, {"heading": "Semisupervised Autoencoder with Bregman Divergence", "text": "To address the problems mentioned above, we propose to introduce supervision to the training of autoencoders. To achieve this, we first train a linear classifier on Bag of Words, and then use the weight of the learned classifier to define a new loss function for the autoencoder. Now let us first describe our choice of loss function, and then elaborate the motivation later:\nD(x\u0303, x) = (\u03b8T (x\u0303\u2212 x))2. (4)\nwhere \u03b8 \u2208 Rd are the weights of the linear classifier, and we have omitted the bias for simplicity. Before we delve into more details, note that Equation (4) is a valid distance, as it is non-negative and reaches zeros if and only if x\u0303 = x. Moreover, the reconstruction error is only measured after projecting on \u03b8; this guides the reconstruction to be accurate only along directions where the linear classifier is sensitive to. Note also that Equation (4) on the one hand uses label information (\u03b8 has been trained with labeled data), on the other hand no explicit labels are directly referred to (only requires xi). Thus one is able to train an autoencoder on both labeled and unlabeled data with the loss function in Equation (4). This subtlety distinguishes our method from pure supervised or unsupervised learning, and allows us to enjoy the benefit from both worlds.\nAs a design choice, we consider SVM with squared hinge loss (SVM2) and \u21132 regularization as the linear classifier, but other classifiers such as Logistic Regression can be used\nand analyzed similarly. Let us denote {xi}, xi \u2208 Rd as the collection of samples, and {yi}, yi \u2208 {1,\u22121} as the class labels; the objective function SVM2 is:\nL(\u03b8) = \u2211\ni\n(max(0, 1\u2212 yi\u03b8 Txi)) 2 + \u03bb\u2016\u03b8\u20162. (5)\nHere \u03b8 \u2208 Rd is the weight; \u03bb is the weight decay parameter. Equation (5) is continuous and differentiable everywhere with respect to \u03b8, so the model can be easily trained with stochastic gradient descent. The next (and most critical) step of our approach is to transfer label information from the linear classifier to the autoencoder. With this in mind, we examine the loss induced by each sample as a function of the input, while with \u03b8 fixed:\nf(xi) = (max(0, 1\u2212 yi\u03b8 Txi)) 2 (6)\nNote that f(xi) is defined on the input space Rd, which should be contrasted with L(\u03b8) in Equation (5) which is a function of \u03b8. We are interested in f(xi) because if we consider moving each input xi to x\u0303i, f(xi) indicates the direction along which the loss is sensitive to. If we think of x\u0303 as the reconstruction of xi obtained from an autoencoder, a good x\u0303i should be in a way such that the deviation of x\u0303i from xi is small evaluated by f(xi). In other words, we would like x\u0303i to still be correctly classified by the pretrained linear classifier. Therefore, f(xi) should be a much better function to evaluate the deviation of two samples. if we can derive a Bregman Divergence from f(xi) and use it as the loss function of the subsequent autoencoder training, the autoencoder should be guided to give reconstruction errors that do not confuse the classifier. Note that f(xi) is a quadratic function of xi whenever f(xi) > 0, so we only need to derive the Hessian matrix in order to achieve the Bregman Divergence. The Hessian follows as:\nH(xi) =\n{\n\u03b8\u03b8T , if 1\u2212 yi\u03b8Txi > 0 0, otherwise.\n(7)\nRecall that for a quadratic function with Hessian matrix H , the Bregman Divergence is simply (x\u0303\u2212 x)TH(x\u0303\u2212 x); then we have:\nD(x\u0303i, xi) =\n{\n(\u03b8T (x\u0303i \u2212 xi))2, if 1\u2212 yi\u03b8Txi > 0 0, otherwise (8)\nIn words, Equation (8) says that we measure the reconstruction loss for difficult examples (those that satisfy 1 \u2212 yi\u03b8\nTxi > 0) with Equation (4); and there is no reconstruction loss at all for easy examples. This discrimination is undesirable, because in this case the Autoencoder would completely ignore easy examples, and there is no way to guarantee that the x\u0303i can be correctly classified. Actually, this split is just an artifact of the hinge loss and the asymmetrical property of Bregman Divergence. Hence, we perform a simple correction by ignoring the condition in Equation (8), which basically pretends that all the examples induce a loss. This directly yields the loss function as in Equation (4)."}, {"heading": "The Bayesian Marginalization", "text": "In principle, one may directly apply Equation (4) as the loss function in place of the squared Euclidean distance and train an autoencoder. However, doing so might introduce a bias brought by one single classifier. As a remedy, we resort to the Bayesian approach, which defines a probability distribution over \u03b8. Although SVM2 is not a probabilistic classifier like Logistic Regression, we can borrow the idea of Energy Based Model (Bengio 2009) and use L(\u03b8) as the negative log likelihood of the following distribution:\np(\u03b8) = exp(\u2212\u03b2L(\u03b8)) \u222b\nexp(\u2212\u03b2L(\u03b8))d\u03b8 (9)\nwhere \u03b2 > 0 is the temperature parameter which controls the shape of the distribution p. Note that the larger \u03b2 is, the sharper p will be. In the extreme case, p(\u03b8) is reduced to a uniform distribution as \u03b2 approaches 0, and collapses into a single \u03b4 function as \u03b2 goes to positive infinity.\nGiven p(\u03b8), we rewrite Equation (4) as an expectation over \u03b8:\nD(x\u0303, x) = E\u03b8\u223cp(\u03b8)(\u03b8 T (x\u0303 \u2212 x))2 =\n\u222b\n(\u03b8T (x\u0303\u2212 x))2p(\u03b8)d\u03b8.\n(10)\nObviously there is now no closed form expression for D(x\u0303, x). To solve it one could use sampling methods such as MCMC, which provides unbiased estimates of the expectation but could be slow in practice. Instead, we use the Laplace approximation, which approximates p(\u03b8) by a Gaussian distribution p\u0303(\u03b8) = N (\u03b8\u0302,\u03a3). As estimating the full covariance matrix is prohibitive, we further constrain \u03a3 to be diagonal. The benefit of doing so is that the expectation can now be computed directly in closed form. To see this, by simply replacing p(\u03b8) with p\u0303(\u03b8) in Equation (11):\nD(x\u0303, x) =E\u03b8\u223cp\u0303(\u03b8)(\u03b8 T (x\u0303\u2212 x))2\n=(x\u0303\u2212 x)TE\u03b8\u223cp\u0303(\u03b8)(\u03b8\u03b8 T )(x\u0303\u2212 x)\n=(x\u0303\u2212 x)T (\u03b8\u0302\u03b8\u0302T +\u03a3)(x\u0303\u2212 x)\n=(\u03b8\u0302T (x\u0303\u2212 x))2 + (\u03a3 1 2 (x\u0303\u2212 x))T (\u03a3 1\n2 (x\u0303 \u2212 x)). (11)\nwhere D now involves two parts, corresponding to the mean and variance term of the Gaussian distribution respectively. Now let us derive p\u0303(\u03b8) for p(\u03b8). In Laplace approximation, \u03b8\u0302 is chosen as the mode of p(\u03b8), which is exactly the solution to the SVM2 optimization problem. For \u03a3, we have:\n\u03a3 =(diag( \u22022L(\u03b8)\n\u2202\u03b82 ))\u22121\n= 1\n\u03b2 (diag(\n\u2211\ni\nI(1\u2212 yi\u03b8 Txi > 0)x 2 i ))\n\u22121 (12)\nHere we have overridden diag but letting it denote a diagonal matrix induced either by a square matrix or a vector; I is the indicator function; (\u00b7)\u22121 denotes matrix inverse. Interestingly, the second term in Equation (11) is now equivalent\nto the squared Euclidean distance after performing elementwise normalizing the input using all difficult examples. The effect of this normalization is that the reconstruction errors of frequent words are down weighted; on the other hand, discriminative words are given higher weights as they would occur less frequently in difficult examples. Note that it is important to use a relatively large \u03b2 in order to avoid the variance term dominating the mean term. In other words, we need to ensure p(\u03b8) to be reasonable peaked around \u03b8\u0302 to effective take advantage of label information."}, {"heading": "Experiments", "text": ""}, {"heading": "Datasets", "text": "We evaluate our model on six Sentiment Analysis benchmarks. The first one is the IMDB dataset 1 (Maas et al. 2011), which consists of movie reviews collected from IMDB. The IMDB dataset is one of the largest sentiment analysis dataset that is publicly available; it also comes with an unlabeled set which allows us to evaluate semisupervised learning methods. The rest five datasets are all collected from Amazon 2(Blitzer, Dredze, and Pereira 2007), which corresponds to the reviews of five different products: books, DVDs, music, electronics, kitchenware. All the six datasets are already tokenized as either uni-gram or bi-gram features. For computational reasons, we only select the words that occur in at least 30 training examples. We summarize the statistics of datasets in Table 1."}, {"heading": "Methods", "text": "\u2022 Bag of Words (BoW). Instead of using the raw word counts directly, we take a simple step of data normalization:\nxi,j = log(1 + ci,j)\nmaxj log(1 + ci,j) (13)\nwhere ci,j denotes the number of occurrences of the jth word in the ith document, xi,j denotes the normalized count. We choose this normalization because it preserves the sparsity of the Bag of Words features; also each feature element is normalized to the range [0, 1]. Note that the very same normalized Bag of Words features are fed into the autoencoders.\n1http://ai.stanford.edu/ amaas/data/sentiment/ 2http://www.cs.jhu.edu/ mdredze/datasets/sentiment/\n\u2022 Denoising Autoencoder (DAE) (Vincent et al. 2008). This refers to the regular Denoising Autoencoder defined in Equation (1) with squared Euclidean distance loss: D(x\u0303, x) = \u2016x\u0303 \u2212 x\u201622. This is also used in (Glorot, Bordes, and Bengio 2011) on the Amazon datasets for domain adaptation. We use ReLu max(0, x) as the activation function, and Sigmoid as the decoding function.\n\u2022 Denoising Autoencoder with Finetuning (DAE+) (Vincent et al. 2008). This denotes the common approach to continue training an DAE on labeled data by replacing the decoding part of DAE with a Softmax layer.\n\u2022 Feedforward Neural Network (NN). This is the standard fully connected neural network with one hidden layer and random initialization. We use the same activation function as that in Autoencoders, i.e., ReLU.\n\u2022 Logistic Regression with Dropout (LrDrop) (Wager, Wang, and Liang 2013). This is a model where logistic regression is regularized with the marginalized dropout noise. LrDrop differs from our approach as it uses feature noising as an explicit regularization. Another difference is that our model is able to learn nonlinear representations, not merely a classifier, and thus is potentially able to model more complicated patterns in data.\n\u2022 Semisupervised Bregman Divergence Autoencoder (SBDAE). This corresponds to our model with Denoising Autoencoder as the feature learner. The training process is roughly equivalent to training on BoW followed by the training of DAE, except that the loss function of DAE is replaced with the loss function defined in Equation (11). We cross validate \u03b2 from the set {104, 105, 106, 107, 108} (note that larger \u03b2 corresponds to weaker Bayesian regularization).\n\u2022 Semisupervised Bregman Divergence Autoencoder with Finetuning (SBDAE+).\nNote that except for BoW and LrDrop, all the other methods require a predefined dimensionality of representation. We use fixed sizes on all the datasets. For SBDAE and NN, a small hidden size is sufficient, so we use 200. For DAE, we observe that it benefits from very large hidden sizes; however, due to computational constraints, we take 2000. For BoW, DAE, SBDAE, we use SVM2 as the classifier. All the models are trained with mini-batch Stochastic Gradient Descent with momentum of 0.9."}, {"heading": "Results", "text": "We first summarize the results as in classification error rate in Table 2. First of all, our model consistently beats BoW with a margin, and it achieves the best results on four (larger) datasets out of six. On the other hand, DAE, DAE+ and NN all fail to outperform BoW, although they share the same architecture as nonlinear classifiers. This suggests that SBDAE be able to learn a much better nonlinear feature transformation function by training with a more informed objective (than that of DAE). Moreover, note also that finetuning on labeled set (DAE+) significantly improves the perfor-\nmance of DAE, which is ultimately on a par with training a neural net with random initialization (NN). However, finetuning offers little help to SBDAE, as it is already implicitly guided by labels during the training.\nLrDrop is the second best method that we have tested. Thanks to the usage of dropout regularization, it consistently outperforms BoW, and achieves the best results on two (smaller) datasets. Compared with LrDrop, it appears that our model works better on large datasets (\u2248 10K words, more than 10K training examples) than smaller ones. This indicates that in high dimensional spaces with sufficient samples, SBDAE benefits from learning a nonlinear feature transformation that disentangles the underlying factors of variation, while LrDrop is incapable of doing so due to its nature as a linear classifier.\nAs the training of the autoencoder part of SBDAE does not require the availability of labels, we also try incorporating unlabeled data after learning the linear classifier in SBDAE. As shown in Table 2, doing so further improves the performance over using labeled data only. This justifies that it is possible to bootstrap from a relatively small amount of labeled data and learn better representations with more unlabeled data with SBDAE.\nTo gain more insights of the results, we further visualize the filters learned by SBDAE and DAE on the IMDB dataset in Table 3. In particular, we show the top 5 most activated and deactivated words of the first 8 filters (corresponding to the first 8 rows of W ) of SBDAE and DAE, respectively. First of all, it seems very difficult to make sense of the filters of DAE as they are mostly common words with no clear co-occurrence pattern. By comparison, if we look at the filters from SBDAE, they are mostly sensitive to words that demonstrate clear polarity. In particular, all the 8 filters seem to be most activated by certain negative words, and are most deactivated by certain positive words. In this way, the activation of each filter of SBDAE is much more indicative of the polarity than that of DAE, which explains the better performance of SBDAE over DAE. Note that this difference only comes from reweighting the reconstruction errors in a certain way, with no explicit usage of labels."}, {"heading": "Related Work and Discussion", "text": "Our work falls into the general category of learning representations for text data. In particular, there have been a lot of efforts that try to learn compact representations for either words or documents (Turney and Pantel 2010; Blei, Ng, and Jordan 2003; Deerwester et al. 1990; Mikolov et al. 2013; Le and Mikolov 2014; Maas et al. 2011). LDA (Blei, Ng, and Jordan 2003) explicitly learns a set of topics, each of which is defined as a distribution on words; a document is thus represented as the posterior distribution on topics, which is a fixed-length, non-negative vector. Closely related are matrix factorization models such as LSA (Deerwester et al. 1990) and Non-negative Matrix Factorization (NMF) (Xu, Liu, and Gong 2003). While LSA factorizes the doc-term matrix via Singular Value Decomposition, NMF learns non-negative basis and coefficient vectors. Similar to these efforts, our model also works\ndirectly on the doc-term matrix. However, thanks to the usage of autoencoder, the representation for documents are calculated instantly via direct matrix product, which eliminates the need of expensive inference. Our work also distinguishes itself from other work as a semisupervised representation learning model, where label information can be effectively leveraged.\nRecently, there has also been an active thread of research on learning word representations. Notably, (Mikolov et al. 2013) shows that we can learn interesting word embeddings via very simple architecture on a large amount of unlabeled dataset. Moreover, (Le and Mikolov 2014) proposed to jointly learn representations for sentences and paragraphs together with words in a similar unsupervised fashion. While our work does not explicitly model the representations for words, it is straightforward to incorporate this idea by adding an additional linear layer at the bottom of the autoencoder.\nFrom the perspective of machine learning methodology, our approach resembles the idea of layer-wise pretraining in deep Neural Networks (Bengio 2009). Our model differs from the traditional training procedure of autoencoders\nin that we effectively utilize the label information to guide the representation learning. Related idea has been proposed in (Socher et al. 2011), where they train Recursive autoencoders on sentences jointly with prediction of sentiment. Due to the delicate recursive architecture, their model only works on sentences with given parsing trees, and could not generalize to documents. MTC (Rifai et al. 2011a) is another work that models the interaction of autoencoders and classifiers. However, their training of autoencoders is purely unsupervised, the interaction comes into play by requiring the classifier to be invariant along the tangents of the learned data manifold. It is not difficult to see that the assumption of MTC would not hold when the class labels did not align well with the data manifold, which is a situation our model does not suffer from."}, {"heading": "Conclusion", "text": "In this paper, we have proposed a novel extension to autoencoders for learning task-specific representations for textual data. We have generalized the traditional autoencoders by relaxing their loss function to the Bregman Divergence, and then derived a discriminative loss function from the label\ninformation. Experiments on text classification benchmarks have shown that our model significantly outperforms Bag of Words, traditional Denoising Autoencoder, and other competing methods. We have also qualitatively visualized that our model successfully learns discriminative features, which unsupervised methods fail to do."}, {"heading": "Acknowledgments", "text": "This work is supported in part by NSF (CCF-1017828)."}, {"heading": "2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest", "text": "Group of the ACL, 151\u2013161.\n[Srivastava et al. 2014] Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15(1):1929\u20131958.\n[Turney and Pantel 2010] Turney, P. D., and Pantel, P. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Intell. Res. (JAIR) 37:141\u2013188.\n[Vincent et al. 2008] Vincent, P.; Larochelle, H.; Bengio, Y.; and Manzagol, P. 2008. Extracting and composing robust features with denoising autoencoders. In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, 1096\u2013 1103.\n[Wager, Wang, and Liang 2013] Wager, S.; Wang, S. I.; and Liang, P. 2013. Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., 351\u2013359.\n[Xu, Liu, and Gong 2003] Xu, W.; Liu, X.; and Gong, Y. 2003. Document clustering based on non-negative matrix factorization. In SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, July 28 - August 1, 2003, Toronto, Canada, 267\u2013273."}], "references": [{"title": "Clustering with bregman divergences", "author": ["Banerjee"], "venue": "In Proceedings of the Fourth SIAM International Conference on Data Mining, Lake Buena Vista, Florida,", "citeRegEx": "Banerjee,? \\Q2004\\E", "shortCiteRegEx": "Banerjee", "year": 2004}, {"title": "Latent dirichlet allocation", "author": ["Ng Blei", "D.M. Jordan 2003] Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3:993\u20131022", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Dredze Blitzer", "J. Pereira 2007] Blitzer", "M. Dredze", "F. Pereira"], "venue": "ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester"], "venue": "JASIS", "citeRegEx": "Deerwester,? \\Q1990\\E", "shortCiteRegEx": "Deerwester", "year": 1990}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q.V. Mikolov 2014] Le", "T. Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Maas"], "venue": "Proceedings of the Conference,", "citeRegEx": "Maas,? \\Q2011\\E", "shortCiteRegEx": "Maas", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["Rifai"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Rifai,? \\Q2011\\E", "shortCiteRegEx": "Rifai", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Rifai,? \\Q2011\\E", "shortCiteRegEx": "Rifai", "year": 2011}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "P.D. Pantel 2010] Turney", "P. Pantel"], "venue": "J. Artif. Intell. Res", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["Wang Wager", "S. Liang 2013] Wager", "S.I. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["Liu Xu", "W. Gong 2003] Xu", "X. Liu", "Y. Gong"], "venue": "In SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.", "creator": "LaTeX with hyperref package"}}}