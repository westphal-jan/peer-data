{"id": "1512.04906", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Strategies for Training Large Vocabulary Neural Language Models", "abstract": "training neural network language models over large vocabularies is still computationally very costly compared to count - based computation models such today as kneser - ney. at the same time, neural model language models are gaining popularity for many applications software such as speech context recognition and machine translation processing whose success depends predominantly on scalability. we present a systematic case comparison of strategies to securely represent problems and train comparatively large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. we further extend self normalization to be a proper causal estimator of inference likelihood and introduce an efficient variant of softmax. we evaluate each method on three popular benchmarks, mostly examining performance on rare words, the speed / accuracy trade - off and complementarity constraints to kneser - ney.", "histories": [["v1", "Tue, 15 Dec 2015 19:29:01 GMT  (252kb,D)", "http://arxiv.org/abs/1512.04906v1", "12 pages; journal paper; under review"]], "COMMENTS": "12 pages; journal paper; under review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["wenlin chen", "david grangier", "michael auli"], "accepted": true, "id": "1512.04906"}, "pdf": {"name": "1512.04906.pdf", "metadata": {"source": "CRF", "title": "Strategies for Training Large Vocabulary Neural Language Models", "authors": ["Wenlin Chen", "David Grangier", "Michael Auli"], "emails": ["wenlinchen@wustl.edu", "grangier@fb.com", "michaelauli@fb.com"], "sections": [{"heading": null, "text": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney."}, {"heading": "1 Introduction", "text": "Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013). Furthermore, models similar in architecture to neural language models have been proposed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015).\n\u2020Work done while Wenlin was an intern at Facebook.\nLanguage models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010), or log-bilinear models (Mnih and Hinton, 2010). We experiment with a simple but proven feed-forward neural network model similar to Bengio et al. (2003). Our focus is not the model architecture or how the context can be represented but rather how to efficiently deal with large output vocabularies, a problem common to all approaches to neural language modeling and related tasks such as machine translation and language generation.\nPractical training speed for these models quickly decreases as the vocabulary grows. This is due to three combined factors. First, model evaluation and gradient computation become more time consuming, mainly due to the need of computing normalized probabilities over a large vocabulary. Second, large vocabularies require more training data in order to observe enough instances of infrequent words which increases training times. Third, a larger training set often allows for higher capacity models which requires more training iterations.\nIn this paper we provide an overview of popular strategies to model large vocabularies for language modeling. This includes the classical softmax over all output classes, hierarchical softmax which introduces latent variables, or clusters, to simplify normalization, target sampling which only considers a random subset of classes for normalization, noise contrastive estimation which discriminates between genuine data points and samples from a noise distri-\nar X\niv :1\n51 2.\n04 90\n6v 1\n[ cs\n.C L\n] 1\n5 D\nec 2\nbution, and infrequent normalization, also referred as self-normalization, which computes the partition function at an infrequent rate. We also extend selfnormalization to be a proper estimator of likelihood. Furthermore, we introduce differentiated softmax, a novel variation of softmax which assigns more capacity to frequent words and which we show to be faster and more accurate than softmax (\u00a72).\nOur comparison assumes a reasonable budget of one week for training models. We evaluate on three well known benchmarks differing in the amount of training data and vocabulary size, that is Penn Treebank, Gigaword and the recently introduced Billion Word benchmark (\u00a73).\nOur results show that conclusions drawn from small datasets do not always generalize to larger settings. For instance, hierarchical softmax is less accurate than softmax on the small vocabulary Penn Treebank task but performs best on the very large vocabulary Billion Word benchmark, because hierarchical softmax is the fastest method for training and can perform more training updates in the same period of time. Furthermore, our results with differentiated softmax demonstrate that assigning capacity where it has the most impact allows to train better models in our time budget (\u00a74).\nUnlike traditional count-based models, our neural models benefit less from more training data because the computational complexity of training is much higher, exceeding our time budget in some cases. Finally, our analysis shows clearly that Kenser-Ney count-based language models are very competitive on rare words, contrary to the common belief that neural models are better on infrequent words (\u00a75)."}, {"heading": "2 Modeling Large Vocabularies", "text": "We first introduce our basic language model architecture with a classical softmax and then describe various other methods including a novel variation of softmax."}, {"heading": "2.1 Softmax Neural Language Model", "text": "Our feed-forward neural network implements an ngram language model, i.e., it is a parametric function estimating the probability of the next word wt given n \u2212 1 previous context words, wt\u22121, . . . , wt\u2212n+1. Formally, we take as input a sequence of discrete\nindexes representing the n \u2212 1 previous words and output a vocabulary-sized vector of probability estimates, i.e.,\nf : {1, . . . , V }n\u22121 \u2192 [0, 1]V ,\nwhere V is the vocabulary size. This function results from the composition of simple differentiable functions or layers.\nSpecifically, f composes an input mapping from discrete word indexes to continuous vectors, a succession of linear operations followed by hyperbolic tangent non-linearities, plus one final linear operation, followed by a softmax normalization.\nThe input layer maps each context word index to a continuous d0-dimensional vector. It relies on a parameter matrix W 0 \u2208 RV\u00d7d0 to convert the input\nx = [wt\u22121, . . . , wt\u2212n+1] \u2208 {1, . . . , V }n\u22121\nto n\u2212 1 vectors of dimension d0. These vectors are concatenated into a single (n\u2212 1)\u00d7 d0 matrix,\nh0 = [W 0wt\u22121 ; . . . ;W 0 wt\u2212n+1 ] \u2208 Rn\u22121\u00d7d0 .\nThis state h0 is considered as a (n\u2212 1)\u00d7 d0 vector by the next layer. The subsequent states are computed through k layers of linear mappings followed by hyperbolic tangents, i.e.\n\u2200i = 1, . . . , k, hi = tanh(W ihi\u22121 + bi) \u2208 Rdi\nwhere W i \u2208 Rdi\u00d7di\u22121 , b \u2208 Rdi are learnable weights and biases and tanh denotes the component-wise hyperbolic tangent.\nFinally, the last layer performs a linear operation followed by a softmax normalization, i.e.,\nhk+1 =W k+1hk + bk+1 \u2208 RV (1)\nand y = 1\nZ exp(hk+1) \u2208 [0, 1]V (2)\nwhere Z = V\u2211 j=1 exp(hk+1j ).\nand exp denotes the component-wise exponential. The network output y is therefore a vocabulary-sized vector of probability estimates. We use the standard\ncross-entropy loss with respect to the computed log probabilities\n\u2202 log yi\n\u2202hk+1j = \u03b4ij \u2212 yj\nwhere \u03b4ij = 1 if i = j and 0 otherwise The gradient update therefore increases the score of the correct output hk+1i and decreases the score of all other outputs hk+1j for j 6= i.\nA downside of the classical softmax formulation is that it requires computation of the activations for all output words (see Equation 2). When grouping multiple input examples into a batch, Equation 1 amounts to a large matrix-matrix product of the form W k+1Hk where W k+1 \u2208 RV\u00d7dk , Hk = [hk1; . . . ;h k l ] \u2208 Rdk\u00d7l, where l is the number of input examples in a batch. For example, typical settings for the gigaword corpus (\u00a73) are a vocabulary of size V = 100, 000, with output word embedding size dk = 1024 and batch size of l = 500 examples. This gives a very large matrix-matrix product of 100, 000\u00d7 1024 by 1024\u00d7 500. The rest of the network involves matrix-matrix operations whose size is determined by the batch size and the layer dimensions, both are typically much smaller than the vocabulary size, ranging for hundreds to a couple of thousands. Therefore, the output layer dominates the complexity of the entire network.\nThis computational burden is high even for Graphics Processing Units (GPUs). GPUs are well suited for matrix-matrix operation when matrix dimensions are in the thousands, but become less efficient with dimensions over 10, 000. The size of the output matrix is therefore a bottleneck during training. Previous work suggested tackling these products by sharding them across multiple GPUs (Sutskever et al., 2014), which introduces additional engineering challenges around inter-GPU communication. This paper focuses on orthogonal algorithmic solutions which are also relevant to parallel training."}, {"heading": "2.2 Hierarchical Softmax", "text": "Hierarchical Softmax (HSM) organizes the output vocabulary into a tree where the leaves are the words and the intermediate nodes are latent variables, or classes (Morin and Bengio, 2005). The tree has potentially many levels and there is a unique path from\nthe root to each word. The probability of a word is the product of the probabilities of the latent variables along the path from the root to the leaf, including the probability of the leaf. If the tree is perfectly balanced, this can reduce the complexity fromO(V ) to O(log V ).\nWe experiment with a version that follows Goodman (2001) and which has been used in Mikolov et al. (2011b). Goodman proposed a two-level tree which first predicts the class of the next word ct and then the actual word wt given context x\np(wt|x) = p(ct|x) p(wt|ct, x) (3) If the number of classes is O( \u221a V ) and each class has the same number of members, then we only need to compute O(2 \u221a V ) outputs. This is a good strategy in practice as it yields weight matrices for clusters and words whose largest dimension is less than \u223c 1, 000, a setting for which GPUs are fast.\nA popular strategy clusters words based on frequency. It slices the list of words sorted by frequency into clusters that contain an equal share of the total unigram probability. We pursue this strategy and compare it to random class assignment and to clustering based on word embedding features. The latter applies k-means over word embeddings obtained from Hellinger PCA over co-occurrence counts (Lebret and Collobert, 2014). Alternative word representations (Brown et al., 1992; Mikolov et al., 2013) are also relevant but an extensive study of word clustering techniques is beyond the scope of this work."}, {"heading": "2.3 Differentiated Softmax", "text": "This section introduces a novel variation of softmax that assigns variable capacity per word in the output layer. The weight matrix of the final layer W k+1 \u2208 Rdk\u00d7V stores output embeddings of size dk for the V words the language model may predict: W k+11 ; . . . ;W k+1 V . Differentiated softmax (DSoftmax) varies the dimension of the output embeddings dk across words depending on how much model capacity is deemed suitable for a given word. In particular, it is meaningful to assign more parameters to frequent words than to rare words. By definition, frequent words occur more of ten in the training data than rare words and therefore allow to fit more parameters.\nIn particular, we define partitions of the output vocabulary based on word frequency and the words in each partition share the same embedding size. For example, we may partition the frequency ordered set of output word ids, O = {1, . . . , V }, into AdA = {1, . . . ,K} andBdB = {K+1, . . . , V } s.t. A \u222a B = O \u2227 A \u2229 B = \u2205, where dA and dB are different output embedding sizes andK is a word id.\nPartitioning results in a sparse final weight matrix W k+1 which arranges the embeddings of the output words in blocks, each one corresponding to a separate partition (Figure 1). The size of the final hidden layer hk is the sum of the embedding sizes of the partitions. The final hidden layer is effectively a concatenation of separate features for each partition which are used to compute the dot product with the corresponding embedding type in W k+1. In practice, we compute separate matrix-vector products, or in batched form, matrix-matrix products, for each partition in W k+1 and hk.\nOverall, differentiated softmax can lead to large speed-ups as well as accuracy gains since we can greatly reduce the complexity of computing the output layer. Most significantly, this strategy speeds up both training and inference. This is in contrast to hierarchical softmax which is fast during training but requires even more effort than softmax for computing the most likely next word."}, {"heading": "2.4 Target Sampling", "text": "Sampling-based methods approximate the softmax normalization (Equation 2) by selecting a number of impostors instead of using all outputs. This can significantly speed-up each training iteration, depending on the size of the impostor set.\nWe follow Jean et al. (2014) who choose as impostors all positive examples in a mini-batch as well as a subset of the remaining words. This subset is sampled uniformly and its size is chosen by cross-validation. A downside of sampling is that the (downsampled) final weight matrix W k+1 (Equation 1) keeps changing between mini-batches. This is computationally costly and the success of sampling hinges on being to estimate a good model while keeping the number of samples small."}, {"heading": "2.5 Noise Contrastive Estimation", "text": "Noise contrastive estimation (NCE) is another sampling-based technique (Hyva\u0308rinen, 2010; Mnih and Teh, 2012). Contrary to target sampling, it does not maximize the training data likelihood directly. Instead, it solves a two-class problem of distinguishing genuine data from noise samples. The training algorithm samples a word w given the preceding context x from a mixture\nP (w|x) = 1 k + 1 Ptrain(w|x) + k k + 1 Pnoise(w|x)\nwherePtrain is the empirical distribution of the training set and Pnoise is a known noise distribution which is typically a context-independent unigram distribution fitted on the training set. The training algorithm fits the model P\u0302 (w|x) to recover whether a mixture sample came from the data or the noise distribution, this amounts to minimizing the binary cross-entropy\n\u2212y log P\u0302 (y = 1|w, x)\u2212 (1\u2212y) log P\u0302 (y = 0|w, x)\nwhere y is a binary variable indicating whether the current sample originates from the data (y = 1) or the noise (y = 0) and P\u0302 (y = 1|w, x) = P\u0302 (w|x) P\u0302 (w|x)+kPnoise(w|x)\n, P\u0302 (y = 0|w, x) = 1 \u2212 P\u0302 (y = 1|w, x) are the model estimates of the corresponding posteriors.\nThis formulation still involves a softmax over the vocabulary to compute P\u0302 (w|x). However, Mnih\nand Teh (2012) suggest to forego the normalization step and simply consider replacing P\u0302 (w|x) with unnormalized exponentiated scores which makes the complexity of training independent of the vocabulary size. At test time, the softmax normalization is reintroduced to obtain a proper distribution."}, {"heading": "2.6 Infrequent Normalization", "text": "Andreas and Klein (2015) also propose to relax score normalization. Their strategy (here referred to as WeaknormSQ) associates unnormalized likelihood maximization with a penalty term that favors normalized predictions. This yields the following loss over the training set T\nL(2)\u03b1 = \u2212 \u2211\n(w,x)\u2208T\ns(w|x) + \u03b1 \u2211\n(w,x)\u2208T\n(logZ(x))2\nwhere s(w|x) refers to the unnormalized score of word w given context x and Z(x) =\u2211\nw exp(s(w|x)) refers to the partition function for context x. For efficient training, the second term can be down-sampled\nL(2)\u03b1,\u03b3 = \u2212 \u2211 (w,x) \u2208train s(w|x) + \u03b1 \u03b3 \u2211 (w,x) \u2208train\u03b3 (logZ(x))2\nwhere T\u03b3 is the training set sampled at rate \u03b3. A small rate implies computing the partition function only for a small fraction of the training data.\nThis work extends this strategy to the case where the log partition term is not squared (Weaknorm), i.e.,\nL(1)\u03b1,\u03b3 = \u2212 \u2211 (w,x) \u2208train s(w|x) + \u03b1 \u03b3 \u2211 (w,x) \u2208train\u03b3 logZ(x)\nFor \u03b1 = 1, this loss is an unbiased estimator of the negative log-likelihood of the training data L(2)1 = \u2212\u2211(w,x)\u2208train s(w|x)\u2212 logZ(x)."}, {"heading": "2.7 Other Methods", "text": "Fast locality-sensitive hashing has been used to approximate the dot-product between the final hidden layer activation hk and the output word embedding (Vijayanarasimhan et al., 2014). However, during training, there is a high overhead for re-indexing the embeddings and test time speed-ups virtually vanish as the batch size increases due to the efficiency of matrix-matrix products."}, {"heading": "3 Experimental Setup", "text": "This section describes the data used in our experiments, our evaluation methodology and our validation procedure.\nDatasets Our experiments are performed over three datasets of different sizes: Penn Treebank (PTB), WMT11-lm (billionW) and English Gigaword, version 5 (gigaword). Penn Treebank is a wellestablished dataset for evaluating language models (Marcus et al., 1993). It is the smallest dataset with a benchmark setting relying on 1 million tokens and a vocabulary size of 10, 000 (Mikolov et al., 2011a). The vocabulary roughly corresponds to words occurring at least twice in the training set. The WMT11-lm corpus has been recently introduced as a larger corpus to evaluate language models and their impact on statistical machine translation (Chelba et al., 2013). It contains close to a billion tokens and a vocabulary of about 800,000 words, which corresponds to words with more than 3 occurrences in the training set.1 This dataset is often referred as the billion word benchmark. Gigaword (Parker et al., 2011) is the largest corpus we consider with 5 billion tokens of newswire data. Even though it has been used for language modeling previously (Heafield, 2011), there is no standard train/test split or vocabulary for this set. We split the data according to time: the training set covers the period 1994\u20132009 and the test data covers 2010. The vocabulary consists of the 100, 000 most frequent words, which roughly corresponds to words with more than 100 occurrences in the training data. Table 1 summarizes data set statistics.\nEvaluation Performance is evaluated in terms of perplexity over the test set. For PTB and billionW,\n1We use the version distributed by Tony Robinson at http://tiny.cc/1billionLM .\nwe report perplexity results on a per sentence basis, i.e., the model does not use context words across sentence boundaries and we score the endof-sentence marker. This is the standard setting for these benchmarks. On gigaword, we do not segment the data into sentences and the model uses contexts crossing sentence boundaries and the evaluation does not include end-of-sentence markers.\nOur baseline is an interpolated Kneser-Ney (KN) language model and we use the KenLM toolkit to train 5-gram models without pruning (Heafield, 2011). For our neural models, we train 11-gram language models for gigaword, billionW and a 6-gram language model for the smaller PTB. The parameters of the models are the weights W i and the biases bi for i = 0, . . . , k + 1. These parameters are learned by maximizing the log-likelihood of the training data relying on stochastic gradient descent (SGD) (LeCun et al., 1998).\nValidation The hyper-parameters of the model are the number of layers k and the dimension of each layer di,\u2200i = 0, . . . , k. These parameters are set by cross-validation, i.e., the parameters which maximize the likelihood over a validation set (subset of the training data excluded from sampling during SGD optimization). We also cross-validate the number of clusters and as well as the clustering technique for hierarchical softmax, the number of frequency bands and their allocated capacity for differentiated softmax, the number of distractors for target sampling, the noise/data ratio for NCE, as well as the regularization rate and strength for infrequent normalization. Similarly, the SGD parameters, i.e., learning rate and mini-batch size, are also set to maximize validation accuracy.\nTraining Time We train for 168 hours (one week) on the large datasets (billionW, gigaword) and 24 hours (one day) for Penn Treebank. We select the hyper-parameters which yield the best validation perplexity after the allocated time and report the perplexity of the resulting model on the test set. This training time is a trade-off between being able to do a comprehensive exploration of the various settings for each method and good accuracy."}, {"heading": "4 Results", "text": "Looking at test results (Table 2) and learning paths on the validation sets (Figures 2, 3, and 4) we can see a clear trend: the competitiveness of softmax diminishes with the vocabulary size. Softmax does very well on the small vocabulary Penn Treebank corpus, but it does very poorly on the larger vocabulary billionW corpus. Faster methods such as sampling, hierarchical softmax, and infrequent normalization (Weaknorm and WeaknormSQ) are much better in the large-vocabulary setting of billionW.\nD-Softmax is performing very well on all data sets and shows that assigning higher capacity where it benefits most results in better models. Target sampling performs worse than softmax on gigaword but better on billionW. Hierarchical softmax performs very poorly on Penn Treebank which is in stark contrast to billionW where it does very well. Noise contrastive estimation has good accuracy on billionW, where speed is essential to achieving good accuracy.\nOf all the methods, hierarchical softmax processes most training examples in a given time frame (Table 3). Our test time speed comparison assumes that we would like to find the highest scoring next word, instead rescoring an existing string. This scenario requires scoring all output words and DSoftmax can process nearly twice as many tokens per second than the other methods whose complex-\nity is then similar to softmax."}, {"heading": "4.1 Softmax", "text": "Despite being our baseline, softmax ranks among the most accurate methods on PTB and it is second best on gigaword after D-Softmax (with WeaknormSQ performing similarly). For billionW, the extremely large vocabulary makes softmax training too slow to compete with faster alternatives. However, of all the methods softmax has the sim-\nplest implementation and it has no additional hyperparameters compared to other methods."}, {"heading": "4.2 Target Sampling", "text": "Figure 5 shows that target sampling is most accurate when the distractor set represents a large fraction of the vocabulary, i.e. more than 30% on gigaword (billionW best setting is even higher with 50%). Target sampling is asymptotically faster and therefore performs more iterations than softmax in the same time. However, it makes less progress in terms of perplexity reduction per iteration compared to softmax. Overall, it is not much better than softmax. A reason might be that the sampling procedure chooses distractors independently from context, or current model performance. This does not favor sampling distractors the model incorrectly considers likely given the current context. These distrac-\ntors would yield high gradient that could make the model progress faster."}, {"heading": "4.3 Hierarchical Softmax", "text": "Hierarchical softmax is very efficient for large vocabularies and it is the best method on billionW. On the other hand, HSM is performing poorly on small vocabularies as seen on Penn Treebank.\nWe found that a good word clustering structure helps learning: when each cluster contains words occurring in similar contexts, cluster likelihoods are easier to learn; when the cluster structure is uninformative, cluster likelihoods converge to the uniform distribution. This adversely affects accuracy since words can never have higher probability than their clusters (cf. Equation 3).\nOur experiments group words into a two level hierarchy and compare four clustering strategies over billionW and gigaword (\u00a72.2). Random clustering shuffles the vocabulary and splits it into equally sized partitions. Frequency-based clustering first orders words based on the number of their occurrences and assigns words to clusters such that each cluster represents an equal share of frequency counts (Mikolov et al., 2011b). K-means runs the well-know clustering algorithm on Hellinger PCA word embeddings. Weighted k-means is similar but weights each word by its frequency.\nRandom clustering performs worst (Table 4) followed by frequency-based clustering but k-means does best; weighted k-means performs similarly than its unweighted version. In our initial experiments, pure k-means performed very poorly because the most significant cluster captured up to 40% of\nthe word frequencies in the data. We resorted to explicitly capping the frequency-budget of each cluster to \u223c 10% which brought k-means to the performance of weighted k-means."}, {"heading": "4.4 Differentiated Softmax", "text": "D-Softmax is the best technique on gigaword, and the second best on billionW, after HSM. On PTB it ranks among the best techniques whose perplexities cannot be reliably distinguished. The variablecapacity scheme of D-Softmax can assign large embeddings to frequent words, while keeping computational complexity manageable through small embeddings for rare words.\nUnlike for hierarchical softmax, NCE or Weaknorm, the computational advantage of D-Softmax is preserved at test time (Table 3). D-Softmax is the fastest technique at test time, while ranking among the most accurate methods. This speed advantage is due to the low dimensional representation of rare words which negatively affects the model accuracy on these words (Table 5)."}, {"heading": "4.5 Noise Contrastive Estimation", "text": "For language modeling we found NCE difficult to use in practice. In order to work with large neural networks and large vocabularies, we had to dissociate the number of noise samples from the data to noise ratio in the modeled mixture. For instance, a data/noise ratio of 1/50 gives good performance in our experiments but estimating only 50 noise sample posteriors per data point is wasteful given the cost of network evaluation. Moreover, this setting does not allow frequent sampling of every word in a large vocabulary. Our setting considers more noise samples and up-weights the data sample. This allows to set the data/noise ratio independently from the number of noise samples.\nOverall, NCE results are better than softmax only for billionW, a setting for which softmax is very slow due to the very large vocabulary. Why does NCE perform so poorly? Figure 6 shows entropy on the validation set versus the NCE loss for several models. The results clearly show that similar NCE loss values can result in very different validation entropy. Although NCE might make sense for other metrics, it is not among the best techniques for minimizing perplexity."}, {"heading": "4.6 Infrequent Normalization", "text": "Infrequent normalization (Weaknorm and WeaknormSQ) performs better than softmax on billionW and comparably to softmax on Penn Treebank and gigaword (Table 2). The speedup from skipping partition function computations is substantial. For instance, WeaknormSQ on billionW evaluates the partition only on 10% of the examples. In one week, the model is evaluated and updated on 868M tokens (with 86.8M partition evaluations) compared to 156M tokens for softmax.\nAlthough referred to as self-normalizing in the literature (Andreas and Klein, 2015), the trained models still needs to be normalized after training. The partition cannot be considered as a constant and varies greatly between data samples. On billionW, the 10th to 90th percentile range was 9.4 to 10.3 on the natural log scale, i.e., a ratio of 2.5 for Wea-\nknormSQ. It is worth noting that the squared regularizer version of infrequent normalization (WeaknormSQ) is highly sensitive to the regularization parameter. We often found regularization strength to be either too low (collapse) or too high (blow-up) after a few days of training. We added an extra unit to our model in order to bound predictions, which yields more stable training and better generalization performance. We bounded unnormalized predictions within the range [\u221210,+10] by using x\u2192 10 tanh(x/5)). We also observed that for the non-squared version of the technique (Weaknorm), a regularization strength of 1 was the best setting. With this choice, the loss is an unbiased estimator of the data likelihood."}, {"heading": "5 Analysis", "text": "This section discusses model capacity, model initialization, training set size and performance on rare words."}, {"heading": "5.1 Model Capacity", "text": "Training neural language models over large corpora highlights that training time, not training data, is the main factor limiting performance. The learning curves on gigaword and billionW indicate that most models are still making progress after one week. Training time has therefore to be taken into account when considering increasing capacity. Figure 7 shows validation perplexity versus the number of iterations for a week of training. This figure indicates that a softmax model with 1024 hidden units in the last layer could perform better than the 512- hidden unit model with a longer training horizon. However, in the allocated time, 512 hidden units yield the best validation performance. D-softmax shows that it is possible to selectively increase capacity, i.e. to allocate more hidden units to the representation of the most frequent words at the expense of rarer words. This captures most of the benefit of a larger softmax model while staying within a reasonable training budget."}, {"heading": "5.2 Effect of Initialization", "text": "Several techniques for pre-training word embeddings have been recently proposed (Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al.,\n2014). Our experiments use Hellinger PCA (Lebret and Collobert, 2014), motivated by its simplicity: it can be computed in a few minutes and only requires an implementation of parallel co-occurrence counting as well as fast randomized PCA. We consider initializing both the input word embeddings and the output matrix from PCA embeddings.\nFigure 8 shows that PCA is better than random for initializing both input and output word representations; initializing both from PCA is even better. The results show that even after a week of training, the initial conditions still impact the validation perplexity. This trend is not specific to softmax and similar outcomes have been observed for other strategies. After a week of training, we observe only for HSM that the random initialization of the output matrix can reach performance comparable to PCA initialization."}, {"heading": "5.3 Training Set Size", "text": "Large training sets and a fixed training time introduce competition between slower models with more capacity and observing more training data. This trade-off only applies to iterative SGD optimization and it does not apply to classical count-based mod-\nels, which visit the training set once and then solve training in closed form.\nWe compare Kneser-Ney and softmax, trained for one week, with gigaword on differently sized subsets of the training data. For each setting we take care to include all data from the smaller subsets. Figure 9 shows that the performance of the neural model improves very little on more than 500M tokens. In order to benefit from the full training set we would require a much higher training budget, faster hardware, or parallelization.\nScaling training to large datasets can have a significant impact on perplexity, even when data from the distribution of interest is limited. As an illustration, we adapted a softmax model trained on billionW to Penn Treebank and achieved a perplexity of 96 - a far better result than with any model we\ntrained from scratch on PTB (cf. Table 2)."}, {"heading": "5.4 Rare Words", "text": "How well are neural models performing on rare words? To answer this question we computed entropy across word frequency bands of the vocabulary for Kneser-Ney and neural models, that is we report entropy for the 4, 000 most frequent words, then the next most frequent 16, 000 words and so on. Table 5 shows that Kneser-Ney is very competitive on rare words, contrary to the common belief that neural models are better on infrequent words. For frequent words, neural models are on par or better than Kneser-Ney. This highlights that the two approaches complement each other, as observed in our combination experiments (Table 2).\nAmong the neural strategies, D-Softmax excels on frequent words but performs poorly on rare ones. This is because D-Softmax assigns more capacity to frequent words at the expense of rare ones. Overall, hierarchical softmax is the best neural technique for rare words since it is very fast. Hierarchical softmax does more iterations than the other techniques and observes the occurrences of every rare words several times."}, {"heading": "6 Conclusions", "text": "This paper presents the first comprehensive analysis of strategies to train large vocabulary neural language models. Large vocabularies are a challenge for neural networks as they need to compute the partition function over the entire vocabulary at each evaluation.\nWe compared classical softmax to hierarchical softmax, target sampling, noise contrastive\nestimation and infrequent normalization, commonly referred to as self-normalization. Furthermore, we extend infrequent normalization, or selfnormalization, to be a proper estimator of likelihood and we introduce differentiated softmax, a novel variant of softmax which assigns less capacity to rare words in order to reduce computation.\nOur results show that methods which are effective on small vocabularies are not necessarily the best on large vocabularies. In our setting, target sampling and noise contrastive estimation failed to outperform the softmax baseline. Overall, differentiated softmax and hierarchical softmax are the best strategies for large vocabularies. Compared to classical Kneser-Ney models, neural models are better at modeling frequent words, but they are less effective for rare words. A combination of the two is therefore very effective.\nFrom this paper, we conclude that there is still a lot to explore in training from a combination of normalized and unnormalized objectives. We also see parallel training and better rare word modeling as promising future directions."}, {"heading": "7 Acknowledgments", "text": "Do not number the acknowledgment section. Do not include this section when submitting your paper for review."}], "references": [{"title": "When and why are log-linear models selfnormalizing", "author": ["Andreas", "Klein2015] Jacob Andreas", "Dan Klein"], "venue": "In Proc. of NAACL", "citeRegEx": "Andreas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Brian Kingsbury", "author": ["Ebru Arisoy", "Tara N. Sainath"], "venue": "and Bhuvana Ramabhadran.", "citeRegEx": "Arisoy et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Jauvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Jenifer C", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra"], "venue": "Lai.", "citeRegEx": "Brown et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Phillipp Koehn", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants"], "venue": "and Tony Robinson.", "citeRegEx": "Chelba et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Alexander M", "author": ["Sumit Chopra", "Jason Weston"], "venue": "Rush.", "citeRegEx": "Chopra et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Schwartz", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar"], "venue": ", and John Makhoul.", "citeRegEx": "Devlin et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Classes for Fast Maximum Entropy Training", "author": ["Joshua Goodman"], "venue": "In Proc. of ICASSP", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["Kenneth Heafield"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann Aapo Hyv\u00e4rinen"], "venue": "In Proc. of AISTATS", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2010\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2010}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Alexandre Allauzen", "author": ["Hai-Son Le"], "venue": "and Fran\u00e7ois Yvon.", "citeRegEx": "Le et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Genevieve Orr", "author": ["Yann LeCun", "Leon Bottou"], "venue": "and Klaus-Robert Mueller.", "citeRegEx": "LeCun et al.1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Mary Ann Marcinkiewicz", "author": ["Mitchell P. Marcus"], "venue": "and Beatrice Santorini.", "citeRegEx": "Marcus et al.1993", "shortCiteRegEx": null, "year": 1993}, {"title": "2010", "author": ["Tom\u00e1\u0161 Mikolov", "Karafi\u00e1t Martin", "Luk\u00e1\u0161 Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "Recurrent Neural Network based Language Model. In Proc. of INTERSPEECH, pages 1045\u2013", "citeRegEx": "Mikolov et al.2010", "shortCiteRegEx": null, "year": 1048}, {"title": "Extensions of Recurrent Neural Network Language Model", "author": ["Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Greg Corrado", "author": ["Tom\u00e1\u0161 Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["Mnih", "Hinton2010] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Proc. of NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "In Proc. of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proc. of AISTATS", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Ke Chen", "author": ["Robert Parker", "David Graff", "Junbo Kong"], "venue": "and Kazuaki Maeda.", "citeRegEx": "Parker et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Anthony Rousseau", "author": ["Holger Schwenk"], "venue": "and Mohammed Attik.", "citeRegEx": "Schwenk et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Jian-Yun Nie1", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell"], "venue": "Jianfeng Gao, and Bill Dolan.", "citeRegEx": "Sordoni et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Victoria Fossum", "author": ["Ashish Vaswani", "Yinggong Zhao"], "venue": "and David Chiang.", "citeRegEx": "Vaswani et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rajat Monga", "author": ["Sudheendra Vijayanarasimhan", "Jonathon Shlens"], "venue": "and Jay Yagnik.", "citeRegEx": "Vijayanarasimhan et al.2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.", "creator": "LaTeX with hyperref package"}}}