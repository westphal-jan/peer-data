{"id": "1512.05004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model Sampling Effects", "abstract": "in this preliminary study, we examine whether random samples from within given library territory of congress when classification outline areas yield significantly different topic models. we first find that models of existing subsamples can equal the topic similarity of models over the whole corpus. equally as the sample size increases, topic distance decreases and subsequent topic overlap increases. the requisite subsample size ratio differs by field and slightly by number of topics. while this study focuses fundamentally on only five areas, generally we find significant differences in the behavior of these areas that can only be investigated with large corpora like the hathi trust.", "histories": [["v1", "Tue, 15 Dec 2015 23:07:58 GMT  (155kb,D)", "http://arxiv.org/abs/1512.05004v1", "HTRC ACS Technical Report; 8 pages, 3 figures"], ["v2", "Wed, 3 Feb 2016 21:12:17 GMT  (157kb,D)", "http://arxiv.org/abs/1512.05004v2", "HTRC ACS Technical Report; 10 pages, 3 figures"], ["v3", "Mon, 13 Feb 2017 15:48:16 GMT  (110kb,D)", "http://arxiv.org/abs/1512.05004v3", "2016 International Conference on Computational Social Science (IC2S2), June 23-26, 2016. 3 pages"]], "COMMENTS": "HTRC ACS Technical Report; 8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.IR", "authors": ["jaimie murdock", "jiaan zeng", "colin allen"], "accepted": false, "id": "1512.05004"}, "pdf": {"name": "1512.05004.pdf", "metadata": {"source": "CRF", "title": "Towards Cultural-Scale Models of Full Text", "authors": ["Jaimie Murdock", "Jiaan Zeng", "Colin Allen"], "emails": ["jammurdo@indiana.edu"], "sections": [{"heading": null, "text": "Keywords\u2014 digital libraries; topic modeling; topic alignment; random sampling; Hathi Trust; Library of Congress Classification Outline (LCCO).\nLarge-scale digital libraries, such as the Hathi Trust1, give a window into a much greater quantity of textual data than ever before [1]. These data raise new challenges for analysis and interpretation. The constant, dynamic addition and revision of works in digital libraries mean that any study aiming to characterize the evolution of culture using large-scale digital libraries must have an awareness of the implications of corpus sampling. Failing to recognize that any large digital library is merely a sample of a larger set of books published within the culture can lead to unintentionally strong claims about socio-linguistics [2]. New methodologies also require careful consideration for humanistic implications [3].\nOne methodology with rapid uptake in the study of cultural evolution is probabilistic topic modeling [4]. Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7]. Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9].\nResearchers need confidence in sampling methods used to construct topic models intended to represent very large portions of the HathiTrust collection. For example, topic modeling every book categorized under the Library of Congress Classification Outline (LCCO)2 as \u201cPhilosophy\u201d (call numbers B1-5802) is impractical, as any library will be incomplete. However, if it can be shown that models built from different random samples are highly similar to one another, then the project of having a topic model that is sufficiently representative of the entire HT collection may become tractable.\n1http://hathitrust.org/ 2https://www.loc.gov/catdir/cpso/lcco/\nar X\niv :1\n51 2.\n05 00\n4v 1\n[ cs\n.D L\n] 1"}, {"heading": "Methods", "text": ""}, {"heading": "LCCO Sampling", "text": "We implemented a random sampling web service that provides the following query interfaces:\n\u2022 sampling(Category, number): It takes a category and the number of random samples as input, and returns a list of book ID which are randomly generated. For example, given category \u201cDQ78-210\u201d and 3, the web service returns \u201cgri.ark:/13960/t50g6cm2v\u2014uc1.31822038210555\u2014uva.x030577307\u201d where the book ID is separated by the pipe symbol;\n\u2022 id(Category): It takes a category as input and returns a list of book ID which has all the books under such a category;\n\u2022 idTotal(Category): It takes a category as input and returns the total number of books under such a category.\nFigure 1 shows an example of the LCCO hierarchy stored in the HathiTrust Solr Index."}, {"heading": "Corpus Download", "text": "Each subject area was downloaded from the HathiTrust on 19 October 2015 using the HathiTrust Data API through the InPhO Topic Explorer interface. The selected areas can be found in Table 1."}, {"heading": "Topic Modeling", "text": "LDA topic modeling [10] represents the current state of the art for extracting meaningful data from digitized texts. We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12]. All corpuses have the NLTK English stoplist removed. Additionally, all words occurring more than 50000 times and less than 15 times removed from the corpus.\nA reference model is trained on the whole subject area. Multiple other spanning models are trained on the whole subject area. For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research. Finally, multiple subcorpus models are trained on different portions of the whole corpus, selected randomly."}, {"heading": "Topic alignment", "text": "A topic alignment is a function that maps one topic in M1 to a topic in M2. For this analysis, M2 is always the reference model, while M1 is either a spanning or subcorpus model. The alignment function is not\nrequired to be bijective: multiple topics from M1 may map to the same topic in M2 (not injective) and not all topics in M2 are required to have an analog in M1 (not surjective).\nBasic Alignment \u2014 The basic alignment simply computes the Jensen-Shannon distance (JSD) between each topic and all topics in M2. Each topic is simply matched to the closest topic, regardless of whether that topic in M2 is matched to another topic in M1.\nMathematically, this function is neither injective nor surjective, so an inverse alignment from M2 to M1 is not guaranteed to produce identical results.\nNa\u0131\u0308ve Alignment \u2014 The na\u0131\u0308ve alignment also computes JSD between each topic and all topics in M2. However, if the closest match in M2 has already been selected by a topic in M1, the algorithm proceeds to the next closest topic until it finds one that has not yet been selected.\nMathematically, for equal numbers of topics in M1 and M2, the na\u0131\u0308ve alignment is bijective."}, {"heading": "Results", "text": "In this preliminary study, we examined five areas of the LCCO. These five areas were selected to give examples in arts, humanities, sciences, and engineering and at different levels of granularity. For each subject area, three classes of models were trained at k = {20, 40, 60, 80}:\n1. LDA topic modeling was applied to all volumes in the subject area to generate the reference model. 2. Additional models were trained over all volumes to generate multiple spanning models. 3. Models were trained over random samples of the subject area to generate subcorpus models. A basic alignment was performed between each spanning or subcorpus model to the reference model. Each alignment\u2019s fitness score is calculated as the average Jensen-Shannon distance [14] between the word distribution of each topic pair. The results of the alignment are plotted against the sample size in Figure 2 for nb: Art Sculpture. Figure 3 shows the alignments for all five subject areas for k = {20, 40, 60, 80}.\nUsing the basic alignment, multiple topics in the spanning or subcorpus model may map to the same topic in the reference model and not all topics in the reference model need have an analog in the spanning or subcorpus model. We calculate the percentage of reference model topics captured by a basic alignment of the spanning or subcorpus model. Figure 4 shows topic capture for all five subject areas."}, {"heading": "Discussion and Conclusions", "text": "In this preliminary study we wished to answer \u201cDo random samples from within given LCCO categories yield significantly different TMs?\u201d We find that the approach of principled subsampling can work, based on the existence of subcorpus models that have topic distances less than or equal to spanning models, relative to a reference model.\nWe highlight several phenomena for further investigation: 1. Topic distance decreases with sample size. (See Figures 2 and 3.) 2. Topic overlap increases with sample size. (See Figure 4.) 3. Different areas may have different characteristics, so a single sample size proportion may not be\npossible. (See Figures 3 and 4.) In particular, the behavior of different areas may be tied to the \u201ccognitive extent\u201d of the discipline [15]. While we only used five subject areas, Figure 4 suggests differing levels of cognitive extent. We hypothesize that in fields such as Art Sculpture, History of Ethics, and Bridge Engineering, a canon has emerged that is common to the structure of many foundational textbooks in the discipline. For example, discussions of the History of Ethics are likely to discuss Aristotle, Kant, Hume, and Mill, regardless of the specific speciality. Discussions of Art Sculpture are very likely to discuss Michelangelo and Rodin, regardless of other influences. Discussions of bridge engineering are likely to start with a review of classical mechanics before diving into the equations necessary for a suspension bridge. If this hypothesis is true, there may be two types of classifications: those for emergent fields, and those for canonized fields. This is a new empirical question only made possible by corpora the size of the HathiTrust."}, {"heading": "Acknowledgments", "text": "The work in this report was supported by a Hathi Trust Research Center (HTRC) Advanced Collaborative Support (ACS) Grant. We thank Miao Chen for her project management of the ACS grants.We thank Justin Stamets for assistance with corpus curation."}, {"heading": "Art Sculpture topic alignments", "text": ""}], "references": [{"title": "Quantitative Analysis of Culture", "author": ["Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "The Google Books Team", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant", "Steven Pinker", "Martin A Nowak", "Erez Lieberman Aiden"], "venue": "Using Millions of Digitized Books. Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution", "author": ["Eitan Adam Pechenick", "Christopher M Danforth", "Peter Sheridan Dodds"], "venue": "PLoS ONE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Theorizing Research Practices We Forgot to Theorize", "author": ["Ted Underwood"], "venue": "Twenty Years Ago. Representations,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Probabilistic topic models", "author": ["David M. Blei"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "The Emergence of Literary Diction", "author": ["Ted Underwood", "Jordan Sellers"], "venue": "Journal of Digital Humanities,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us", "author": ["Andrew Goldstone", "Ted Underwood"], "venue": "New Literary History,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Trawling in the Sea of the Great Unread: Sub-corpus topic modeling and Humanities research", "author": ["Timothy R. Tangherlini", "Peter Leonard"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Cloud computing data capsules for non-consumptiveuse of texts", "author": ["Jiaan Zeng", "Guangchen Ruan", "Alexander Crowell", "Atul Prakash", "Beth Plale"], "venue": "In Proceedings of the 5th ACM Workshop on Scientific Cloud Computing, ScienceCloud", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Topic Exploration with the HTRC Data Capsule for Non-Consumptive Research", "author": ["Jaimie Murdock", "Jiaan Zeng", "Robert H McDonald"], "venue": "In JCDL \u201915 Proceedings of the 15th ACM/IEEE-CS joint conference on Digital libraries,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Visualization techniques for topic model checking", "author": ["Jaimie Murdock", "Colin Allen"], "venue": "Proceedings of 29th Association for the Advancement of Artificial Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Finding scientific topics", "author": ["Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Navigating the Local Modes of Big Data: The Case of Topic Models. In Data Analytics in Social Science, Government, and Industry", "author": ["Margaret Roberts", "Brandon Stewart", "Dustin Tingley"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Divergence Measures Based on the Shannon Entropy", "author": ["Jianhua Lin"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Large-scale digital libraries, such as the Hathi Trust1, give a window into a much greater quantity of textual data than ever before [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Failing to recognize that any large digital library is merely a sample of a larger set of books published within the culture can lead to unintentionally strong claims about socio-linguistics [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "New methodologies also require careful consideration for humanistic implications [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "One methodology with rapid uptake in the study of cultural evolution is probabilistic topic modeling [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9].", "startOffset": 102, "endOffset": 108}, {"referenceID": 8, "context": "Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9].", "startOffset": 102, "endOffset": 108}, {"referenceID": 9, "context": "Topic Modeling LDA topic modeling [10] represents the current state of the art for extracting meaningful data from digitized texts.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research.", "startOffset": 122, "endOffset": 125}, {"referenceID": 12, "context": "For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research.", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "Each alignment\u2019s fitness score is calculated as the average Jensen-Shannon distance [14] between the word distribution of each topic pair.", "startOffset": 84, "endOffset": 88}], "year": 2017, "abstractText": "In this preliminary study, we examine whether random samples from within given Library of Congress Classification Outline areas yield significantly different topic models. We find that models of subsamples can equal the topic similarity of models over the whole corpus. As the sample size increases, topic distance decreases and topic overlap increases. The requisite subsample size differs by field and by number of topics. While this study focuses on only five areas, we find significant differences in the behavior of these areas that can only be investigated with large corpora like the Hathi Trust. Keywords\u2014 digital libraries; topic modeling; topic alignment; random sampling; Hathi Trust; Library of Congress Classification Outline (LCCO). Large-scale digital libraries, such as the Hathi Trust1, give a window into a much greater quantity of textual data than ever before [1]. These data raise new challenges for analysis and interpretation. The constant, dynamic addition and revision of works in digital libraries mean that any study aiming to characterize the evolution of culture using large-scale digital libraries must have an awareness of the implications of corpus sampling. Failing to recognize that any large digital library is merely a sample of a larger set of books published within the culture can lead to unintentionally strong claims about socio-linguistics [2]. New methodologies also require careful consideration for humanistic implications [3]. One methodology with rapid uptake in the study of cultural evolution is probabilistic topic modeling [4]. Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7]. Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9]. Researchers need confidence in sampling methods used to construct topic models intended to represent very large portions of the HathiTrust collection. For example, topic modeling every book categorized under the Library of Congress Classification Outline (LCCO)2 as \u201cPhilosophy\u201d (call numbers B1-5802) is impractical, as any library will be incomplete. However, if it can be shown that models built from different random samples are highly similar to one another, then the project of having a topic model that is sufficiently representative of the entire HT collection may become tractable. http://hathitrust.org/ https://www.loc.gov/catdir/cpso/lcco/ 1 ar X iv :1 51 2. 05 00 4v 1 [ cs .D L ] 1 5 D ec 2 01 5 Methods LCCO Sampling We implemented a random sampling web service that provides the following query interfaces: \u2022 sampling(Category, number): It takes a category and the number of random samples as input, and returns a list of book ID which are randomly generated. For example, given category \u201cDQ78-210\u201d and 3, the web service returns \u201cgri.ark:/13960/t50g6cm2v\u2014uc1.31822038210555\u2014uva.x030577307\u201d where the book ID is separated by the pipe symbol; \u2022 id(Category): It takes a category as input and returns a list of book ID which has all the books under such a category; \u2022 idTotal(Category): It takes a category as input and returns the total number of books under such a category. Figure 1 shows an example of the LCCO hierarchy stored in the HathiTrust Solr Index. Corpus Download Each subject area was downloaded from the HathiTrust on 19 October 2015 using the HathiTrust Data API through the InPhO Topic Explorer interface. The selected areas can be found in Table 1. Topic Modeling LDA topic modeling [10] represents the current state of the art for extracting meaningful data from digitized texts. We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12]. All corpuses have the NLTK English stoplist removed. Additionally, all words occurring more than 50000 times and less than 15 times removed from the corpus. A reference model is trained on the whole subject area. Multiple other spanning models are trained on the whole subject area. For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research. Finally, multiple subcorpus models are trained on different portions of the whole corpus, selected randomly. Topic alignment A topic alignment is a function that maps one topic in M1 to a topic in M2. For this analysis, M2 is always the reference model, while M1 is either a spanning or subcorpus model. The alignment function is not LCCO Subject Heading # HT Vols nb Art Sculpture 801 tg Bridge Engineering 799 bj71-1185 History of Ethics 898 hd6050-6305 Classes of Labor 1255 tn600-799 Metallurgy 938 Table 1: LCCO Areas Sampled \u2014 Areas in the Library of Congress Classification Outline (LCCO) and their representation in the HathiTrust Digital Library.", "creator": "LaTeX with hyperref package"}}}