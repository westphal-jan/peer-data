{"id": "1512.05947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Complexity and Approximation of the Fuzzy K-Means Problem", "abstract": "the fuzzy $ k $ - means problem is a generalization of the classical $ k $ - means problem to soft clusterings, i. e. clusterings where each points potentially belongs to each infinite cluster to some degree. although popular in archaeological practice, prior to this eponymous work the fuzzy $ k $ - do means problem has not wholly been studied from a complexity theoretic or algorithmic perspective. originally we show that optimal solutions for fuzzy $ k $ - means means consequently cannot, in that general, be expressed precisely by radicals over the input value points. surprisingly, this already holds for very simple inputs and in one - dimensional space. hence, \" one cannot expect to compute optimal solutions like exactly. we give the first $ ( % 1 + \\ epsilon ) $ - approximation algorithms for the fuzzy $ k $ - means problem. first, we originally present a deterministic error approximation algorithm whose runtime is polynomial in $ n $ and linear in the dimension $ d $ of the input record set, given that $ k $ is constant, i. e. a primitive polynomial time precision approximation algorithm given a fixed $ k $., we achieve this result by uniformly showing that for each soft clustering there exists almost a hard clustering with comparable properties. second, by using techniques known from coreset constructions for the $ z k $ - means problem, we develop a deterministic approximation algorithm that runs in time almost linear in $ * n $ but exponential in the dimension $ d $. we complement on these results with a randomized algebraic algorithm which imposes some natural restrictions on the random input set constraint and whose runtime computation is comparable to computing some of the most efficient approximation algorithms for $ k $ - means, i. e. linear in the number of points and the dimension, but mainly exponential in the number of clusters.", "histories": [["v1", "Fri, 18 Dec 2015 13:35:59 GMT  (56kb)", "http://arxiv.org/abs/1512.05947v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["johannes bl\\\"omer", "sascha brauer", "kathrin bujna"], "accepted": false, "id": "1512.05947"}, "pdf": {"name": "1512.05947.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kathrin Bujna"], "emails": ["kathrin.bujna}@uni-paderborn.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n05 94\n7v 1\n[ cs\n.L G\n] 1\n8 D\nec 2\n01 5"}, {"heading": "1 Introduction", "text": "Clustering is a widely used technique in unsupervised machine learning. Simply speaking, its goal is to group similar objects. This problem occurs in a wide range of practical applications in many fields such as image analysis, information retrieval, and bioinformatics. We call a grouping of objects into a given number of clusters a hard clustering if each object is assigned to exactly one cluster. A popular example for a hard clustering problem is the well known K-means problem. In contrast, in a soft clustering each object belongs to each cluster with a certain degree of membership. There is a continuous generalization of the K-means problem that leads to a such a soft clustering problem, known as the fuzzy K-means problem.\n1.1 Fuzzy K-Means\n[1] was the first to present a fuzzy K-means objective function, which was later extended by [2]. Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].\nFuzzy K-Means Problem Let X = {(x1, w1), . . . , (xN , wN )} be a set of data points xn \u2208 RD weighted by wn \u2208 R\u22650. We want to group X into some predefined number of clusters K. These clusters are represented by mean vectors {\u00b51, . . . , \u00b5K} \u2282 RD. In a fuzzy clustering, each data point xn belongs to each cluster, represented by a \u00b5k, with a certain membership value rnk \u2208 [0, 1]. The fuzzy K-means problem has an additional parameter, the so-called fuzzifier m \u2208 N>1, which is chosen in advance and is not subject to optimization. In simple terms, the fuzzifier m determines how much clusters are allowed to overlap, i.e. how soft the clustering is.\nProblem 1 (Fuzzy K-means). Given X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 R\u22650, K \u2265 1 and m \u2265 2, find C = {\u00b5k}k\u2208[K] \u2282 RD and R = {rnk}n\u2208[N ],k\u2208[K] \u2282 [0, 1] minimizing\n\u03c6 (m) X (C,R) =\nN\u2211\nn=1\nK\u2211\nk=1\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 ,\nsubject to: \u2211K\nk=1 rnk = 1 for all n \u2208 [N ]. We denote the costs of an optimal solution by \u03c6OPT(X,K,m).\nFor m = 1 this problem would coincide with the classical K-means problem, while for m \u2192 \u221e the memberships converge to a uniform distribution and the centers converge to the center of the data set X . Our problem definition is a generalization of the original definition presented in [2] in that we consider weighted data sets. By setting all weights to 1, we obtain the original definition.\nFuzzy K-Means (FM) Algorithm The most widely used heuristic for the fuzzy K-means problem is an alternating optimization algorithm known as fuzzy K-means (FM) algorithm. It is defined by the following first-order optimality conditions [5]: Fixing the means {\u00b5k}k\u2208[K], optimal memberships are given by\nrnk = \u2016xn \u2212 \u00b5k\u2016\n\u2212 2 m\u22121\n2 \u2211K\nl=1 \u2016xn \u2212 \u00b5l\u2016 \u2212 2 m\u22121 2\n(1)\nif xn 6= \u00b5l for all l \u2208 [K]. If xn coincides with some of the \u00b5l, then the membership of xn can be distributed arbitrarily among those \u00b5l with \u00b5l = xn. Fixing the memberships {rnk}n,k, the optimal means are given by\n\u00b5k =\n\u2211N n=1 r m nkwnxn\n\u2211N n=1 r m nkwn\n. (2)\nA major downside of the FM algorithm is that there are no guarantees on the quality of computed solutions.\nObservation 1 The FM algorithm converges to a (local) minimum or a saddle point that can be arbitrarily poor compared to an optimal solution. Such points can be reached by the FM algorithm even if it is initialized with points from the given point set\nA proof of this observation can be found in Section 7.8."}, {"heading": "1.2 Related Work", "text": "Although the fuzzy K-means problem appears in a wide range of practical applications, so far there has been no complexity classification. To the best of our knowledge, there are no hardness results for the fuzzy K-means problem. It is not even known whether it lies in NP. The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].\nTwo problems that are closely related to the fuzzy K-means problem are the K-means and the K-median problem. The complexity of the K-means problem is well-studied. For fixed K and D, there is a polynomial time algorithm solving the problem optimally [8]. The K-means problem is NP-complete, even if K or D is fixed to 2 [9] [10]. Furthermore, assuming P 6=NP, there is no PTAS for the K-means problem for arbitrary K and D [11]. However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + \u01eb)-approximation algorithm with runtime polynomial in N and D [13]. The K-median problem is a variant of the K-means problem that uses the Euclidean instead of the squared Euclidean distance. Just as the K-means problem, the K-median problem is NP-hard, even for D = 2 [14]. However, it is known that the optimal solutions to the K-median problem have an inherently different structure than the solutions to the K-means problem. Even in the plane, optimal solutions of the 1-median problem are in general not expressable by radicals over Q [15].\nMany practical applications make use of the fuzzy K-means (FM) algorithm, which does not yield any approximation guarantees. However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function. Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point. Furthermore, it is known that the algorithm converges locally, i.e. started sufficiently close to a minimizer, the iteration sequence converges to that particular minimizer [18]. However, to the best of our knowledge, there are no theoretical results on approximation algorithms for the fuzzy K-means problem."}, {"heading": "1.3 Overview", "text": "The following technical part of the paper is divided in three parts. In Section 2 we give an overview on our results. In Section 2.1 we formally state our result that the fuzzy K-means problem is not solvable by radicals. In Section 2.2 we present our results on approximation algorithms. In Section 3 give an overview on our algorithmic techniques. In Section 4 we outline the analysis of our approximation algorithms. The interested reader can find fully detailed proofs in Section 6."}, {"heading": "2 Our Contribution", "text": "We initiate the complexity theoretical and algorithmic study of the fuzzy K-means problem."}, {"heading": "2.1 Complexity", "text": "We say that a fuzzy K-means solution is not solvable by radicals if neither means nor memberships can be expressed in terms of (+,\u2212, \u00b7, /, q\u221a ) over the domain of the input.\nTheorem 1. The fuzzy K-means problem for m = 2, K = 2, D \u2265 1, X \u2282 N and |X | \u2265 6 is in general not optimally solvable by radicals over Q. That is, neither the coordinates of the mean vectors nor the membership values can be expressed in terms of (+,\u2212, \u00b7, /, q\u221a ).\nThis result is an application of the technique used by Bajaj [15] who proved the same result for the K-median problem. Notably our result already holds for m = 2 and in one-dimensional space. For instance, we show that an optimal solution of the fuzzy 2-means problem (with m = 2) for the set X = {\u22123,\u22122,\u22121, 1, 2, 3} is not solvable by radicals over Q. In contrast, the K-means and K-median problem can both even be solved efficiently for D = 1. As for m = 2, it is noteworthy that in this case the first-order optimality conditions for means and memberships (cf. Equations (1) and (2)) lead to rationals in the input domain, respectively. A consequence of the inexpressibility by radicals is that no algorithm can solve the fuzzy K-means problem optimally if it only uses arithmetic operations and root extraction to obtain the zeroes of an algebraic equation. A more detailed discussion of the implications of unsolvability by radicals can be found in [15]."}, {"heading": "2.2 Approximation Algorithms", "text": "We present the first (1 + \u01eb)-approximation algorithms for the fuzzy K-means problem.\nA PTAS For Fixed K and m We present the first PTAS for the fuzzy K-means problem, assuming a constant number of clusters K and a constant fuzzifier m. That is, for any given \u01eb \u2208 [0, 1], our algorithm computes an (1+ \u01eb)-approximation to the fuzzy K-means problem in time polynomial in the number of points N and dimension D.\nTheorem 2. There is a deterministic algorithm that, given X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 Q\u22650, K \u2208 N, m \u2208 N, and \u01eb \u2208 (0, 1], computes a solution (C,R) such that\n\u03c6 (m) X (C,R) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m) .\nThe algorithms\u2019 runtime is bounded by D \u00b7 NO ( K2 log(K)\u00b7 1 \u01eb ( m log(m\u01eb )+log ( wmax wmin )))\n, where wmax = maxn\u2208[N ]wn and wmin = minn\u2208[N ]wn.\nThe main idea behind our result is to exploit the existence of a hard clusters that exhibit characteristics similar to those of a fuzzy clusters. By combining this result with a sampling technique which is well known from the K-means problem and applying exhaustive search, we obtain the algorithm.\nA Fast Deterministic (1 + \u01eb)-Approximation Algorithm By using a completely different technique, we obtain a deterministic algorithm whose runtime almost linearly depends on N . On the negative side, we have to give up the linear dependence on the dimension D for this.\nTheorem 3. There is a deterministic algorithm that, given X = {xn}n\u2208[N ] \u2282 RD, K \u2208 N, m \u2208 N, and \u01eb \u2208 (0, 1], computes a solution (C,R) such that\n\u03c6 (m) X (C,R) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m) .\nThe algorithms\u2019 runtime is bounded by N (log(N)) K KO(K 2D log(1/\u01eb)m).\nThe runtime of this algorithm is not comparable with the runtime of the algorithm from Theorem 2. However, comparing the terms N log(N)K and NO(K\n2 log(K)) with one another, we find that the runtime of the algorithm from Theorem 2 depends much stronger on K than the runtime of our algorithm from Theorem 3. For instance, assuming K2D = O (log(N)/log log(N)), the runtime of our algorithm from Theorem 3 is still polynomial in N , i.e. NO(log( 1 \u01eb )), assuming that m is constant. In comparison, the runtime of our PTAS from Theorem 2 would then be exponential in N . Hence, Theorem 2 and Theorem 3 complement each other.\nThe idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20]. That is, we construct a small set of good candidate means. After generating this candidate set, our algorithm simply tests all these candidates and chooses the best one.\nA Fast Randomized (1 + \u01eb)-Approximation Algorithm Last, we show that there is a randomized algorithm with runtime linear in N and D. However, in return for this speedup, this algorithm has some requirement on the input sets. More precisely, our algorithm from Theorem 4 approximates the best ( \u03b1 \u2211N n=1 wn, K ) -balanced solution.\nDefinition 1 ((B,K)-balanced). Let X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 R\u22650. Given a solution with memberships R = {rnl}n\u2208[N ],l\u2208[L], we denote the weight of the lth fuzzy cluster by\nRl := N\u2211\nn=1\nrmnlwn . (3)\nWe say that the memberships R are (B,K)-balanced if\nL \u2264 K and B \u2264 min l\u2208[L] Rl .\nAn optimal (B,K)-balanced solution has smallest cost among all solutions with (B,K)-balanced membership values. An optimal (0,K)-balanced solution is an optimal solution to the fuzzy Kmeans problem.\nTheorem 4. There is a randomized algorithm that, given X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 Q\u22650, K \u2208 N, m \u2208 N, \u01eb \u2208 (0, 1], and \u03b1 \u2208 (0, 1], computes (C,R) such that with constant probability\n\u03c6 (m) X (C,R) \u2264 (1 + \u01eb)\u03c6 (m) X\n(\nCoptB,K , R opt B,K\n)\n,\nwhere (\nCoptB,K , R opt B,K\n)\nis an optimal (B,K)-balanced solution with\nB = \u03b1 N\u2211\nn=1\nwn .\nThe algorithms\u2019 runtime is bounded by N \u00b7D \u00b7 2O(K2\u00b7 1\u01eb log( 1\u03b1\u01eb )).\nWe can boost the probability of success to an arbitrary 1 \u2212 \u03b4 by simply repeating the algorithm log(1/\u03b4) times. Observe that the running time basically coincides with the running time of an algorithm that applies the superset sampling technique to the K-means problem [21].\nThe restriction B = \u03b1\u2211Nn=1 wn can also be seen as a restriction on the deviation between the single cluster weights Rk and the average cluster weight Ravg = 1/K \u00b7 \u2211K k=1 Rk. More precisely, using the Cauchy-Schwarz inequality,\n1\nKm\u22121 \u00b7\nN\u2211\nn=1\nwn \u2264 K \u00b7Ravg = N\u2211\nn=1\n( K\u2211\nk=1\nrmnk\n)\nwn \u2264 N\u2211\nn=1\nwn .\nHence, if B \u2265 \u03b1\u2211Nn=1 wn, then for all k \u2208 [K] we have Rk/Ravg \u2208 [\u03b1K,Km]. For example, by choosing \u03b1 = 2\u2212O(K) we allow a deviation of a factor 2\u2212O(K) and obtain a runtime that is still linear in N and becomes exponential in K3.\nThe main idea behind this algorithm is the same as behind the PTAS described in Theorem 2. Knowing that for given fuzzy clusters there exist a hard clusters with similar characteristics, we apply a sampling technique known from the K-means problem. However, instead of combining this technique with exhaustive search, we directly apply the sampling technique to obtain our randomized algorithm from Theorem 4. In other words, the PTAS from Theorem 2 can be seen as a de-randomized version of this algorithm."}, {"heading": "3 Our Main Techniques", "text": "In this section, we describe the techniques that we use to prove Theorems 2, 4, and 3. To this end, we use the following notation.\nDefinition 2 (Induced Solution). Let X \u2282 RD \u00d7 R. Membership values R induce the solution (C\u0303, R) where C\u0303 contains the corresponding optimal mean vectors (cf. Equation (2)). Mean vectors C induce the solution (C, R\u0303) where R\u0303 contains the corresponding optimal membership values (cf. Equation (1)). We denote the costs of the induced solutions by \u03c6 (m) X (R) and \u03c6 (m) X (C), respectively.\nObserve that for all means C = {\u00b5k}k\u2208[K] and memberships R = {rnk}n\u2208[N ],k\u2208[K] we have\n\u03c6 (m) X (C) \u2264 \u03c6 (m) X (C,R) and \u03c6 (m) X (R) \u2264 \u03c6 (m) X (C,R) . (4)\n3.1 Structure of the Fuzzy K-Means Problem\nThere are two aspects of the structure of the fuzzy K-means problem that we exploit extensively. First, there is a coarse but still useful relation between the K-means and the fuzzy K-means cost function. Recall that optimal solutions of the fuzzy K-means problem seem to have a substantially different structure than optimal solutions of the K-means problem (cf. Section 2.1). Nonetheless, the fuzzy K-means and K-means cost of solutions induced the same set of mean vectors differ by at most a factor of Km\u22121. We use this result when transfering the ideas behind the coreset construction of [19] in order to obtain a candidate set of means.\nDefinition 3 (K-means). For X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 R\u22650 and C = {\u00b5k}k\u2208[K] \u2282 RD we define kmX (C) := \u2211N n=1 wn mink\u2208[K] \u2016xn \u2212 \u00b5k\u2016 2 2.\nLemma 1. Let X \u2282 RD \u00d7 R\u22650, m \u2208 N, and C \u2282 RD with |C| = K. Then,\n1\nKm\u22121 kmX(C) \u2264 \u03c6(m)X (C) \u2264 kmX(C) .\nProof. Obviously, \u03c6 (m) X (C) \u2264 kmX(C). Let X = {(xn, wn)}n\u2208[N ]. Let {rnk}n,k be the optimal memberships induced by C = {\u00b5k}k\u2208[K]. Using the Cauchy-Schwarz inequality, 1Km\u22121 \u00b7 kmX(C) \u2264 \u2211N\nn=1\n( \u2211K\nk=1 r m nk\n)\nwn\n( mink\u2208[K] \u2016xn \u2212 \u00b5k\u201622 ) \u2264 \u03c6(m)X (C).\nSecond, we can ignore fuzzy clusters with too small a weight. For each optimal fuzzy K-means solution, there exists a fuzzy L-means solution with L \u2264 K clusters such that each cluster has a certain minimum weight B while the cost are only at most a factor (1 + \u01eb) worse than the cost of the optimal solution. Recall that we denote such clusterings as (B,K)-balanced (cf. Definition 1). More precisely, B only depends on \u01eb, m, K, and the smallest weight of a point in X . This result becomes important when we apply sampling techniques. When sampling points from X , we can only expect to sample points from a certain cluster if this cluster is large enough.\nLemma 2. Let X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 R, m \u2208 N, K \u2208 N, and \u01eb \u2208 [0, 1]. There exist (B,K)-balanced membership values R such that\n\u03c6 (m) X (R) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m)\nwhere\nB = ( \u01eb\n4mK2\n)m\n\u00b7 min n\u2208[N ] wn .\nProof. Consider an arbitrary but fixed solution {\u00b5k}k\u2208[K]. Let {rnk}n,k be the optimal membership values induced by {\u00b5k}k\u2208[K]. Assume that for some l \u2208 [K] we have Rl = \u2211N n=1 r m nlwn \u2264 ( \u01eb 4mK2\n)m \u00b7 minn\u2208[N ]wn. Thus, we have rnl \u2264 \u01eb4mK2 for all n \u2208 [N ].\nConsider an arbitrary n \u2208 [N ]. Since \u2211Kk=1 rnk = 1 and rnl \u2264 1K , there exists some k(n) \u2208 [K] with k(n) 6= l such that rnk(n) \u2265 1K . Since rnl \u2264 \u01eb4mK2 , we have rnl \u2264 \u01eb4mK rnk(n). Hence,\n(rnk(n) + rnl) m \u2264\n(\n1 + \u01eb\n4mK\n)m rmnk(n) \u2264 ( 1 + \u01eb\n2K\n)\n\u00b7 rmnk(n) . (5)\nDue to the optimality of the rnl and rnk(n) (cf. Equation (1)) and since rnl \u2264 rnk(n), we have \u2225 \u2225xn \u2212 \u00b5k(n) \u2225 \u2225 2 2 \u2264 \u2016xn \u2212 \u00b5l\u201622 . (6)\nHence,\n\u03c6 (m) X ({\u00b5k}k\u2208[K] \\ {\u00b5l})\n\u2264 N\u2211\nn=1\n\u2211\nk\u2208[K] k 6=l,k(n)\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 + N\u2211\nn=1\n(rnk(n) + rnl) mwn \u2225 \u2225xn \u2212 \u00b5k(n) \u2225 \u2225 2\n2\n\u2264 N\u2211\nn=1\n\u2211\nk\u2208[K] k 6=l,k(n)\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 + N\u2211\nn=1\n(\n1 + \u01eb\n2K\n)\nrmnk(n)wn \u2225 \u2225xn \u2212 \u00b5k(n) \u2225 \u2225 2 2 (by Eq. (5))\n\u2264 ( 1 + \u01eb\n2K\n)\n\u03c6 (m) X ({\u00b5k}k) . (by Eq. (6))\nNow consider an optimal solution {\u00b5k}k\u2208[K]. Let B \u2282 {\u00b5k}k\u2208[K] be the set containing the means \u00b5l where Rl \u2264 ( \u01eb\n4mK2 )m \u00b7minn\u2208[N ]wn. Note that B \u2264 K \u2212 1. Let L = K \u2212 |B|. Then, by the above there exists a set of membership values r\u0303L = {r\u0303nl}n\u2208[N ],l\u2208[L] such that\n\u03c6 (m) X ({\u00b5k}k\u2208[K] \\B, r\u0303L) \u2264\n(\n1 + \u01eb\n2K\n)K\n\u03c6OPT(X,K,m) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m)\nand \u2211N\nn=1(r\u0303nl) mwn \u2265\n( \u01eb\n4mK2 )m \u00b7 minn\u2208[N ]wn for all l \u2208 [L]. Finally, observe that \u03c6(m)X (r\u0303L) \u2264 \u03c6 (m) X ({\u00b5k}k\u2208[K] \\B, r\u0303L)."}, {"heading": "3.2 Sampling Techniques", "text": "Our algorithms from Theorems 2 and 4 both use sampling techniques. First, we show that there exist hard clusters suitably imitating soft clusters. Second, we show how to construct a candidate set that contains good approximations of the means of these (unknown) hard clusters.\nRelating Fuzzy to Hard Clusters A fundamental result is that, given fuzzy clusters with not too small a weight, there always exist hard clusters that exhibit characteristics similar to those of the fuzzy clusters.\nDefinition 4 (Cost of a Fuzzy Cluster, Hard Clusters). Let X = {(xn, wn)}n\u2208[N ] \u2282 RD\u00d7R and K \u2208 N. Given memberships {rnk}n\u2208[N ],k\u2208[K] and induced means {\u00b5k}k\u2208[K], we let\n\u03c6 (m) X,k({rnk}n) :=\nN\u2211\nn=1\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 .\nfor all k \u2208 [K]. For all hard clusters C \u2282 X, C 6= \u2205, we define\nw(C) := \u2211\n(wn,xn)\u2208C wn, \u00b5(C) :=\n\u2211\n(wn,xn)\u2208C wn \u00b7 xn w(C)\nand\nkm(C) := \u2211\n(wn,xn)\u2208C wn \u2016xn \u2212 \u00b5(C)\u201622 .\nTheorem 5 (Existence of Similar Hard Clusters). Let X = {(xn, wn)}n\u2208[n] \u2282 RD\u00d7R\u22650 and \u01eb \u2208 (0, 1]. Let {rnk}n,k be memberships values, and let {\u00b5k}k be the corresponding optimal mean vectors.\nIf mink\u2208[K] Rk \u2265 16Kwmax/\u01eb, where wmax = maxn\u2208[N ]wn, then there exist pairwise disjoint sets C1, . . . , CK \u2286 X such that for all k \u2208 [K]\nw(Ck) \u2265 1\n2 Rk , (7)\n\u2016\u00b5(Ck)\u2212 \u00b5k\u201622 \u2264 \u01eb\n2Rk \u00b7 \u03c6(m)X,k ({rnk}n) and (8)\nkm(Ck) \u2264 4K \u00b7 \u03c6(m)X,k ({rnk}n) . (9)\nProof (Idea of the Proof in Section 7). To prove this theorem, we apply the probabilistic method. Consider a random process that samples a hard assignment for each (xn, wn) \u2208 X independently at random by assigning (xn, wn) \u2208 X to the kth cluster with probability rmnk. This assignment can be considered a binary random variable znk \u2208 {0, 1} with expected value E[znk] = rmnk. We compare the resulting hard clusters Ck = {(xn, wn) \u2208 X | znk = 1} with the fuzzy clusters CFk = {(xn, wn \u00b7 rmnk) \u2208 X | xn \u2208 X}. With positive probability, the weights, means, and costs of the constructed hard clusters satisfy the given properties.\nUnfortunately, to the best of our knowledge, the hard clusters Ck do not exhibit any structure, e.g. are not necessarily convex and do not necessarily cover X . In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means \u00b5(Ck) well. It is not clear, how other techniques which do not solely rely on sampling can be applied. For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap. The hard clusters Ck whose existence we can prove do not necessarily have this property.\nSuperset Sampling From Theorem 5, we know that there exist hard clusters similar to given fuzzy clusters. Hence, means that are sufficiently close to the means of the hard clusters induce a solution that is also close to the solution given by the fuzzy clusters. The superset sampling technique introduced by [8] [13] can be used to find such means. More precisely, we can construct a candidate set containing good approximations of the means of unknown hard clusters if these hard clusters do not have too small a weight compared to the weight of the give point set.\nTheorem 6. There is a randomized algorithm that, given X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 Q\u22650, K \u2208 N, \u01eb \u2208 (0, 1], and \u03b1 \u2208 (0, 1], constructs a set T \u2282 (RD)K in time\nO ( D \u00b7 ( N + 2 K \u01eb \u00b7log( 1\u03b1\u01eb)\u00b7log(log(K)) ))\nsuch that for an arbitrary but fixed set {Ck}k\u2208[K] of (unknown) sets Ck \u2286 X, with constant probability, there exists a (\u00b5\u0303k)k\u2208[K] \u2208 T such that for all k \u2208 [K] where\nw(Ck) \u2265 \u03b1 \u00b7 N\u2211\nn=1\nwn\nwe have\n\u2016\u00b5\u0303k \u2212 \u00b5(Ck)\u201622 \u2264 \u01eb\nw(Ck) km(Ck) .\nWe apply this result in two different ways: Firstly, we apply the superset sampling technique directly to obtain a randomized approximation algorithm. That is, we generate the candidate set T and determine the candidate with the smallest fuzzy K-means costs. Secondly, we use exhaustive search to obtain a deterministic approximation algorithm. That is, we generate all candidates that the algorithm from Theorem 6 might possibly generate and choose the best of these candidates. Note that the latter approach does not require that the weights of the fuzzy clusters make up a certain fraction of the weight of the point set."}, {"heading": "4 Proof Sketches", "text": ""}, {"heading": "4.1 Relating Fuzzy to Hard Clusters (Theorems 2 and 4)", "text": "The following proposition is the basis for the proofs of Theorems 2 and 4.\nProposition 1. There is a randomized algorithm that, given X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 Q\u22650, K \u2208 N , m \u2208 N, \u01eb \u2208 (0, 1], and \u03b1 \u2208 (0, 1], computes mean vectors C \u2282 RD, |C| = K, such that with probability at least 1/2 we have\n\u03c6 (m) X (C) \u2264 (1 + \u01eb)\u03c6 (m) X\n(\nRoptB,K\n)\n,\nwhere the memberships RoptB,K induce an optimal (B,K)-balanced solution with\nB = max { \u03b1 \u00b7 N\u2211\nn=1\nwn, 16Kwmax\n\u01eb\n}\nand wmax = max n\u2208[N ] wn .\nThe algorithms\u2019 runtime is bounded by ND \u00b7 2O(K2/\u01eb\u00b7log(1/(\u03b1\u01eb))). There is a deterministic version of this algorithm that, given X = {(xn, wn)}n\u2208[N ] \u2282 RD\u00d7Q\u22650, K \u2208 N , m \u2208 N, and \u01eb \u2208 (0, 1], computes mean vectors which induce an (1+ \u01eb)-approximation to an optimal (B, K)-balanced solution where B = 16Kwmax\u01eb . The runtime of this algorithm is bounded by D \u00b7NO(K2/\u01eb).\nProof. First, we prove that there is a randomized algorithm as described in the theorem. Fix an optimal (B,K)-balanced solution with memberships {rnl}n\u2208[N ],l\u2208[L]. Let {\u00b5l}l\u2208[L] be the optimal mean vectors induced by the fixed {rnl}n\u2208[N ],l\u2208[L].\nBy Theorem 5, we know that there exist hard clusters {Cl}l\u2208[L] \u2286 X similar to the fuzzy clusters given by the {rnl}n\u2208[N ],l\u2208[L]. Due to Equation (7) and since Rl = \u2211N n=1 r m nlwn \u2265 \u03b1 \u00b7 \u2211N n=1 wn, we have w(Cl) \u2265 Rl/2 \u2265 \u03b1/2 \u00b7 \u2211N n=1 wn.\nNow, apply Theorem 6. Consider the set T that is generated as in Theorem 6 with \u01eb set to \u01eb/(16K) and \u03b1 set to \u03b1/2. With probability at least 1/2, the set T contains a candidate (\u00b5\u0303k)k\u2208[K] such that for all l \u2208 [L] we have\n\u2016\u00b5\u0303l \u2212 \u00b5(Cl)\u201622 \u2264 \u01eb\n16Kw(Cl) km(Cl) . (10)\nWe can upper bound the cost of the means {\u00b5\u0303k}k\u2208[K] by\n\u03c6 (m) X ({\u00b5\u0303k}k\u2208[K]) \u2264 \u03c6 (m) X ({\u00b5\u0303l}l\u2208[L]) (since ({\u00b5\u0303l}l\u2208[L] \u2286 {\u00b5\u0303k}k\u2208[K])\n\u2264 \u03c6(m)X ({\u00b5\u0303l}l\u2208[L], {rnl}n\u2208[N ],l\u2208[L]) \u2264 \u03c6(m)X ({rnl}n\u2208[N ],l\u2208[L]) + L\u2211\nl=1\nRl \u2016\u00b5l \u2212 \u00b5\u0303l\u201622\n\u2264 \u03c6(m)X ({rnl}n\u2208[N ],l\u2208[L]) + 2 L\u2211\nl=1\nRl\n(\n\u2016\u00b5l \u2212 \u00b5(Cl)\u201622 + \u2016\u00b5(Cl)\u2212 \u00b5\u0303l\u2016 2 2\n)\n,\nwhere the second to last inequality is well-known (a proof can be found in Section 6.1 (Lem. 3)) and where the last inequality is due to the fact that \u2200a, b \u2208 R : (a+ b)2 \u2264 2a2 + 2b2.\nDue to Equation (8), we obtain\nL\u2211\nl=1\nRl \u2016\u00b5l \u2212 \u00b5(Cl)\u201622 \u2264 L\u2211\nl=1\nRl \u00b7 \u01eb\n2Rl \u00b7 \u03c6(m)X,l ({rnk}n) =\n\u01eb 2 \u00b7 \u03c6(m)X ( {rnl}n\u2208[N ],l\u2208[L] ) .\nFurthermore, we have\nL\u2211\nl=1\nRl \u2016\u00b5(Cl)\u2212 \u00b5\u0303l\u201622 \u2264 L\u2211\nl=1\nRl \u00b7 \u01eb\n16Kw(Cl) \u00b7 km(Cl) (by Equation (10))\n\u2264 \u01eb 2 \u00b7 \u03c6(m)X ({rnl}n\u2208[N ],l\u2208[L]) . (by Equations (7), (9))\nTherefore, with probability 1/2, there exists a candidate tuple (\u00b5\u0303k)k\u2208[K] \u2208 T that induces the desired approximate solution of the given optimal (B,K)-balanced solution {rnl}n\u2208[N ],l\u2208[L]. Hence, the candidate with the smallest fuzzy K-means cost among all the possible candidates satisfies the approximation bound as well. This concludes the proof of the existence of the randomized algorithm.\nTo obtain a deterministic algorithm, we use exhaustive search. Using the same argument as above, but setting \u03b1 to minl\u2208[L] Rl/ \u2211N n=1 wn, one can obtain the desired approximation with positive probability. The set T contains tuples of means of multi-sets, which are subsets of X with size 32K/\u01eb (cf. Section 7.4). Hence, by testing all combinations of means of all possible multisets, which are subsets of X with size 32K/\u01eb, we obtain a candidate set containing the desired approximation. Note that there are at most N32K/\u01eb such subsets. Again, the candidate with the smallest cost yields the desired approximation as well.\nProof (Proof of Theorem 2). Let {rnl}n\u2208[N ],l\u2208[L] be the memberships from Lemma 2 with \u01eb replaced by \u01eb/4. Then, we have \u03c6\n(m) X ({rnl}n\u2208[N ],l\u2208[L]) \u2264 (1 + \u01eb/4)\u03c6OPT(X,K,m).\nLet X \u2032 be the point set containing c = \u2308 2 \u00b7 16m+1mmK2m+1wmax/(wmin\u01ebm+1) \u2309 copies of each\npoint. From Lemma 2 we know that for all l \u2208 [L] we have \u2211Nn=1 rmnlwn \u2265 ( \u01eb/(16mK2) )m \u00b7 wmin. Hence, for all l \u2208 [L] we have \u2211\nxn\u2208X\u2032 r m nlwn = c\n\u2211\nxn\u2208X r m nlwn \u2265 c\n( \u01eb/(16mK2) )m wmin \u2265\n32Kwmax/\u01eb. Thus, we can apply Proposition 1 (with X replaced by X \u2032, \u01eb replaced by \u01eb/2, and {rnl}n\u2208[N ],l\u2208[L] replaced by a set containing c copies of the memberships in {rnl}n\u2208[N ],l\u2208[L]) and obtain means {\u00b5\u0303k}k\u2208[K]. Observe that for all C \u2282 RD we have \u03c6(m)X\u2032 (C) = c \u00b7 \u03c6 (m) X (C). Thus, \u03c6 (m) X ({\u00b5\u0303k}k\u2208[K]) \u2264 (1 + \u01eb/2)\u03c6 (m) X ({rnl}n\u2208[N ],l\u2208[L]) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m).\nSince we apply the algorithm from Lemma 2 to a set containing c copies of the points in X ,\nits runtime is bounded by D \u00b7 (c \u00b7 N)O ( K2 \u01eb ) . Finally, note that (c \u00b7 N)O ( K2 \u01eb ) \u2286 NO ( log(c)\u00b7K2 \u01eb )\n\u2286 N O (( m log(K)+m log(m)+m log( 1\u01eb )+log ( wmax wmin )) \u00b7K2 \u01eb ) \u2286 NO ( K2 log(K)\u00b7 1 \u01eb ( m log(m\u01eb )+log ( wmax wmin ))) .\nProof (Proof of Theorem 4). Construct a point set X \u2032 that contains c = \u230816K/(\u03b1\u01eb)\u2309 copies of each point in X . Fix an arbitrary k \u2208 [K]. Since B \u2265 \u03b1\u2211Nn=1 wn, we have \u2211 xn\u2208X r m nkwn \u2265 \u03b1 \u2211\nxn\u2208X wn. By definition of X \u2032, \u2211\nxn\u2208X\u2032 r m nkwn =\n\u2211 xn\u2208X c \u00b7 wnrmnk \u2265 c \u00b7 \u03b1 \u2211 xn\u2208X wn = \u03b1 \u2211\nxn\u2208X\u2032 wn. Also note that c \u00b7 \u03b1\u2211xn\u2208X wn \u2265 c \u00b7 \u03b1 \u00b7 wmax \u2265 16Kwmax/\u01eb. Hence, we can apply Proposition 1 to X \u2032\nto find {\u00b5\u0303k}k approximating the {\u00b5k}k induced by (copies of) the memberships in {rnk}n,k, with constant probability. Observe that for all C \u2282 RD, \u03c6(m)X\u2032 (C) = c \u00b7 \u03c6 (m) X (C). Hence, \u03c6 (m) X ({\u00b5\u0303k}k) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m). Since we apply the algorithm from Lemma 2 to a set containing c copies of the points in X , it requires runtime (c\u00b7N)\u00b7D\u00b72O(K2/\u01eb\u00b7log(1/(\u03b1\u01eb))). Finally, note that c = 2O(log(c)) \u2286 2O(log(K)+log(1/(\u03b1\u01eb))) \u2286 2O(K 2/\u01eb\u00b7log(1/(\u03b1\u01eb))), assuming that \u03b1\u01eb \u2264 1/2."}, {"heading": "4.2 Candidate Set Search for Mean Vectors (Proof of Theorem 3)", "text": "Using ideas behind the coreset construction of [19], we can construct a candidate set of mean vectors. The algorithm that creates and tests all these candidates and finally chooses the best candidates satisfies the properties from Theorem 3.\nTheorem 7 (Candidate Set). Let X \u2282 RD, K \u2208 N, and \u01eb \u2208 (0, 1]. There exists a set G \u2282 RD with size\n|G| = O ( KmD+1\u01eb\u2212Dm log ( mK\n\u01eb\n)\nlog(N)\n)\nthat contains {\u00b5k}k\u2208[K] \u2282 G with\n\u03c6 (m) X ({\u00b5k}k\u2208[K]) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m) .\nThe set G can be computed in time O(N(log(N))K\u01eb\u22122K2D +NKD |G|). Proof (Sketch of Proof in Section 7.5). The idea behind the coreset construction of [19] can be used to construct a candidate set of mean vectors. Part of the construction is a constant factor approximation of the K-means problem. To this end, we use the deterministic algorithm presented in [23], which requires time O ( N(log(N))K\u01eb\u22122K 2D ) ."}, {"heading": "5 Future Work & Open Problems", "text": "A goal of further research is to examine whether Theorem 5 can be transferred to other soft clustering problems. In particular, we hope to obtain a constant factor approximation algorithm for the maximum likelihood estimation problem for mixtures of Gaussian distributions, e.g. by using the results from [24].\nIt is an open question whether we are able to classify hardness of approximation of fuzzy Kmeans. We conjecture that, just as the classicalK-means problem, if P 6=NP, then there is no PTAS for the fuzzy K-means problem for arbitrary K and D."}, {"heading": "6 Full Proofs", "text": ""}, {"heading": "6.1 Preliminaries", "text": "In this section we introduce some notation and lemmata that are used throughout the rest of this appendix.\nHard Clusters and K-Means Costs The following definition restates some of the notation already presented in Definition 4.\nDefinition 5 (Hard Clusters). For weighted point sets C \u2282 RD \u00d7 R\u22650 , we let\nw(C) := \u2211\n(xn,wn)\u2208C wn ,\n\u00b5(C) :=\n\u2211\n(xn,wn)\u2208C wnxn\nw(C) and\nkm(C) := \u2211\n(xn,wn)\u2208C wn \u2016xn \u2212 \u00b5(C)\u201622 .\nThe following lemma is well known (e.g. used in the proof of Theorem 2 in [8]).\nLemma 3. Let C \u2282 RD \u00d7 R be a weighted point set and \u00b5 \u2208 RD. Then, \u2211\n(xn,wn)\u2208C wn \u2016xn \u2212 \u00b5\u201622 = km(C) + w(C) \u2016\u00b5\u2212 \u00b5(C)\u2016 2 2 .\nProof.\n\u2211\n(xn,wn)\u2208C wn \u2016xn \u2212 \u00b5\u201622 = \u2211\n(xn,wn)\u2208C wn \u2016xn \u2212 \u00b5(C) + \u00b5(C)\u2212 \u00b5\u201622\n= \u2211\n(xn,wn)\u2208C wn \u3008xn \u2212 \u00b5(C) + \u00b5(C) \u2212 \u00b5, xn \u2212 \u00b5(C) + \u00b5(C) \u2212 \u00b5\u3009 (scalar product)\n= \u2211\n(xn,wn)\u2208C wn (\u3008xn \u2212 \u00b5(C), xn \u2212 \u00b5(C)\u3009 + 2 \u3008xn \u2212 \u00b5(C), \u00b5(C) \u2212 \u00b5\u3009+ \u3008\u00b5(C)\u2212 \u00b5, \u00b5(C)\u2212 \u00b5\u3009)\n(bilinearity and symmetry)\n= \u2211\n(xn,wn)\u2208C wn\n(\n\u2016xn \u2212 \u00b5(C)\u201622 + 2 \u3008xn \u2212 \u00b5(C), \u00b5(C) \u2212 \u00b5\u3009+ \u2016\u00b5(C)\u2212 \u00b5\u2016 2 2\n)\n= \u2211\n(xn,wn)\u2208C wn \u2016xn \u2212 \u00b5(C)\u201622 +\n\u2211\n(xn,wn)\u2208C wn \u3008xn \u2212 \u00b5(C), \u00b5(C) \u2212 \u00b5\u3009+ w(C) \u2016\u00b5(C)\u2212 \u00b5\u201622\nwhere due to the bilinarity of the scalar product and by the definition of \u00b5(C)\n\u2211\n(xn,wn)\u2208C wn \u3008xn \u2212 \u00b5(C), \u00b5(C) \u2212 \u00b5\u3009 =\n\u2329 \u2211\n(xn,wn)\u2208C wn(xn \u2212 \u00b5(C)), \u00b5(C) \u2212 \u00b5\n\u232a\n=\n\u2329\n \u2211\n(xn,wn)\u2208C wnxn\n \u2212 w(C) \u00b7 \u00b5(C), \u00b5(C) \u2212 \u00b5 \u232a\n= \u30080, \u00b5(C)\u2212 \u00b5\u3009 = 0\nwhich yields the claim.\nFurthermore, the cluster costs km(C) can be expressed via pairwise distances.\nLemma 4. Let C \u2282 RD \u00d7 R be a weighted point set. Then,\nkm(C) = 1\n2 \u2211\n(xn,wn)\u2208C wn\n\u2211\n(xn,wn)\u2208C\n\u2211\n(xl,wl)\u2208C wnwl \u2016xn \u2212 xl\u201622 .\nDefinition 6 (K-Means Costs (unweighted)). For unweighted point sets X \u2282 RD, x \u2208 RD, and finite sets M \u2282 RD we let\nd(x,M) := min m\u2208M\n\u2016x\u2212m\u20162 ,\nkmX(M) := \u2211\nx\u2208X d(x,M)2 =\n\u2211 x\u2208X min m\u2208M \u2016x\u2212m\u201622 and\nkmX,K := min M\u2282RD\n|M|=K\nkmX(M) .\nDefinition 7 (Induced Partition (unweighted)). We say {Ck}Kk=1 is a partition of X induced by C \u2282 RD if Ck \u2286 {xn \u2208 X | \u2200l 6= k : \u2016xn \u2212 \u00b5k\u20162 \u2264 \u2016xn \u2212 \u00b5l\u20162} and X = \u222a\u0307 K k=1Ck.\nSome Useful Technical Lemmas Besides, we make extensive use of the following simple technical lemmata.\nLemma 5. For all a, b, c \u2208 RD we have\n\u2016c\u2212 a\u201622 \u2212 \u2016c\u2212 b\u2016 2 2 \u2264 \u2016a\u2212 b\u2016 2 2 + 2 \u2016a\u2212 b\u20162 \u2016c\u2212 b\u20162\nProof.\n\u2016a\u2212 c\u201622 \u2212 \u2016b\u2212 c\u2016 2 2 \u2264 \u2223 \u2223 \u2223\u2016a\u2212 c\u201622 \u2212 \u2016b\u2212 c\u2016 2 2 \u2223 \u2223 \u2223\n= \u2223 \u2223 \u2223\u2016a\u2212 b+ b\u2212 c\u201622 \u2212 \u2016b\u2212 c\u2016 2 2 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223\u2016a\u2212 b\u201622 + 2 \u3008a\u2212 b, b\u2212 c\u3009 \u2223 \u2223 \u2223\n\u2264 \u2016a\u2212 b\u201622 + 2 |\u3008a\u2212 b, b\u2212 c\u3009| \u2264 \u2016a\u2212 b\u201622 + 2 \u2016a\u2212 b\u20162 \u2016b\u2212 c\u20162 (Cauchy-Schwarz)\nLemma 6. Let \u01eb \u2208 [0, 1], c > 1, and m \u2208 N. Then, for all i \u2208 [m] it holds (\n1 + \u01eb\n2mc\n)i\n\u2264 1 + i \u00b7 \u01eb mc .\nLemma 7. For all a, b \u2208 R we have\n1. 2ab \u2264 a2 + b2, 2. (a+ b)2 \u2264 2(a2 + b2) and 3. (a+ b + c)2 \u2264 3(a2 + b2 + c2)."}, {"heading": "7 Stochastic Fuzzy Clustering (Proof of Theorem 5)", "text": "In this section, we first describe a random process that, given some fuzzy K-means clusters, creates K hard clusters. We define different quantities that describe the different clusters and derive probabilistic bounds on the similarity between them with respect to these quantities."}, {"heading": "7.1 Setting and Random Process", "text": "In the following we consider arbitrary but fixed memberships {rnk}n,k. These membership values induce fuzzy clusters. We say that the kth fuzzy cluster has weight Rk, mean \u00b5k, and cost \u03c6 (m) X,k({rnk}n), where the \u00b5k are the optimal means with respect to the given memberships. Recall that by Equation (2), Equation (3), and Definition 4 we have\nRk =\nN\u2211\nn=1\nrmnkwn ,\n\u00b5k =\n\u2211N n=1 r m nkwnxn\n\u2211N n=1 r m nkwn\n, and\n\u03c6 (m) X,k({rnk}n) =\nN\u2211\nn=1\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 . (11)\nWe consider the following random process that aims to imitate the fuzzy clustering. Given the fixed memberships {rnk}n,k, the process samples an assignment for each (xn, wn) \u2208 X independently at random. Formally, we describe these assignments by random variables (znk) \u2208 {0, 1}K with\n\u2211K k=1 znk \u2208 {0, 1}. They are sampled according to the following distribution:\n1. With probability rmnk, the process assigns xn to the k th cluster. That is, Pr(znk = 1) = r m nk. 2. The process does not assign xn to any cluster at all with probability 1 \u2212 \u2211K k=1 r m nk. That is,\nPr(\u2200k \u2208 [K] : znk = 0) = 1\u2212 \u2211K k=1 r m nk.\nThis process constructs hard clusters {Ck}k\u2208[K] with Ck = {(xn, wn) \u2208 X |znk = 1} \u2286 X that do not necessarily cover X . Using Definition 5, we can conclude that\nw(Ck) =\nN\u2211\nn=1\nznkwn ,\n\u00b5(Ck) = \u2211N n=1 znkwnxn w(Ck) , and\nkm(Ck) =\nN\u2211\nn=1\nznkwn \u2016xn \u2212 \u00b5(Ck)\u201622 .\nNote that these quantities are random variables defined by the random process. All of them depend on the binary random variables znk."}, {"heading": "7.2 Proximity", "text": "By definition, the binary random variables znk have the property\nE [znk] = Pr (znk = 1) = r m nk .\nIn the following, we use Chebyshev\u2019s and Markov\u2019s inequality to give concentration bounds on the difference between weights, means, and costs of the fuzzy clusters and the hard clusters constructed by the random process, respectively. One might suspect that Chernoff bounds yield better results. Unfortunately, these bounds do not directly measure the differences between the means and costs in terms of the fuzzy K-means costs, respectively. Hence, it is not clear how Chernoff bounds can be applied here.\nLet \u03bb, \u03bd \u2208 R>1 be constants. Lemma 8 (Weights). For all k \u2208 [K] we have\nPr (|w(Ck)\u2212Rk| \u2265 \u03bb\u03b7k) \u2264 1\n\u03bb2 ,\nwhere\n\u03b7k =\n\u221a \u221a \u221a \u221a N\u2211\nn=1\nrmnk(1\u2212 rmnk)w2n . (12)\nProof. Since w(Ck) = \u2211N n=1 znkwn, we have E [w(Ck)] = \u2211N n=1 E [znk]wn = \u2211N n=1 r m nkwn = Rk. Furthermore, since {znk}n is a set of independent random variables, we have\nVar (w(Ck)) =\nN\u2211\nn=1\nVar (znk)w 2 n =\nN\u2211\nn=1\nrmnk (1\u2212 rmnk)w2n = \u03b72k .\nThe claim is a direct consequence of Chebyshev\u2019s inequality.\nNote that numerator and denominator of\n\u2016\u00b5(Ck)\u2212 \u00b5k\u201622 =\n\u2225 \u2225 \u2225 \u2211N n=1 znkwn(xn \u2212 \u00b5k) \u2225 \u2225 \u2225 2\n2\nw(Ck)2 (13)\nare both random variables depending on the same random variables znk. Lemma 8 already gives a bound on the denominator w(Ck). Next, we bound the numerator.\nLemma 9 (Means).\nPr (\u2225 \u2225 \u2225 \u2211N n=1znkwn(xn \u2212 \u00b5k) \u2225 \u2225 \u2225 2\n2 \u2265 \u03bd\u03c4k\n)\n\u2264 1 \u03bd ,\nwhere\n\u03c4k = N\u2211\nn=1\nrmnk (1\u2212 rmnk)w2n \u2016xn \u2212 \u00b5k\u201622 . (14)\nProof. Let Mk = \u2225 \u2225 \u2225 \u2211N n=1 znkwn(xn \u2212 \u00b5k) \u2225 \u2225 \u2225 2\n2 . Observe that\nMk =\n\u2329 N\u2211\nn=1\nznkwn(xn \u2212 \u00b5k), N\u2211\nn=1\nznkwn(xn \u2212 \u00b5k) \u232a\n=\nN\u2211\nn=1\nN\u2211\no=1\nznkzokwnwo \u3008xn \u2212 \u00b5k, xo \u2212 \u00b5k\u3009 .\nBecause the expectation is linear, we obtain\nE[Mk] =\nN\u2211\nn=1\nN\u2211\no=1\nE [znkzok]wnwo \u3008(xn \u2212 \u00b5k), (xo \u2212 \u00b5k)\u3009\n=\nN\u2211\nn=1\nE [ z2nk ] w2n \u2016xn \u2212 \u00b5k\u201622 +\n\u2211 o 6=n E [znkzok]wnwo \u3008(xn \u2212 \u00b5k), (xo \u2212 \u00b5k)\u3009 .\nRecall that the znk are independent binary random variables, hence for all n, o \u2208 [N ], n 6= o,\nE [ z2nk ] =Pr (znk = 1) = r m nk\nE [znkzok] =Pr (znkzok = 1) = r m nkr m ok .\nThus,\nE[Mk] =\nN\u2211\nn=1\nrmnkw 2 n \u2016xn \u2212 \u00b5k\u201622 +\n\u2211 o 6=n rmnkr m okwnwo \u3008(xn \u2212 \u00b5k), (xo \u2212 \u00b5k)\u3009\n= N\u2211\nn=1\nrmnkw 2 n \u2016xn \u2212 \u00b5k\u201622 \u2212 r2mnk \u2016xn \u2212 \u00b5k\u2016 2 2 +\nN\u2211\no=1\nrmnkr m okwnwo \u3008(xn \u2212 \u00b5k), (xo \u2212 \u00b5k)\u3009\n=\nN\u2211\nn=1\nrmnk(1 \u2212 rmnk)w2n \u2016xn \u2212 \u00b5k\u201622 + rmnkwn \u2329 (xn \u2212 \u00b5k), N\u2211\no=1 rmokwo(xo \u2212 \u00b5k) \ufe38 \ufe37\ufe37 \ufe38\n=0\n\u232a\n= N\u2211\nn=1\nrmnk(1 \u2212 rmnk)w2n \u2016xn \u2212 \u00b5k\u201622 = \u03c4k .\nApplying Markov\u2019s inequality yields the claim.\nFinally, we can bound the cluster-wise cost as follows.\nLemma 10 (Cost). For all k \u2208 [K] we have\nPr ( km(Ck) \u2265 \u03bd \u00b7 \u03c6(m)X,k({rnk}n) ) \u2264 1 \u03bd\nProof. Observe that, by definition of \u00b5(Ck) and Lemma 3,\nkm(Ck) =\nN\u2211\nn=1\nznkwn \u2016xn \u2212 \u00b5(Ck)\u201622 \u2264 N\u2211\nn=1\nznkwn \u2016xn \u2212 \u00b5k\u201622\nThe expectation of the upper bound evaluates to\nE\n[ N\u2211\nn=1\nznkwn \u2016xn \u2212 \u00b5k\u201622\n]\n=\nN\u2211\nn=1\nrmnkwn \u2016xn \u2212 \u00b5k\u201622 = \u03c6 (m) X,k({rnk}n) .\nApplying Markovs\u2019s inequality yields the claim.\nNow, we can formally prove the existence of hard clusters imitating given fuzzy clusters.\nCorollary 1. Let X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 R\u22650 be a weighted point set. Let {rnk}n,k be the memberships of a fuzzy K-means solution for X with corresponding optimal means {\u00b5k}k. Then, there exist pairwise disjoint subsets {Ck}k\u2208[K] of X such that for all k \u2208 [K]\n|w(Ck)\u2212Rk| \u2264 \u221a 4K \u00b7 \u03b7k (15)\n\u2016\u00b5(Ck)\u2212 \u00b5k\u201622 \u2264 4K\n(Rk \u2212 \u221a 4K\u03b7k)2 \u00b7 \u03c4k (16)\nkm(Ck) \u2264 4K \u00b7 \u03c6(m)X,k({rnk}n) , (17)\nwhere \u03b7k = \u221a \u2211N n=1 r m nk(1\u2212 rmnk)w2n and \u03c4k = \u2211N n=1 r m nk (1\u2212 rmnk)w2n \u2016xn \u2212 \u00b5k\u2016 2 2.\nProof. Recall that the binary random variables znk indicate hard clusters {Ck}k\u2208[K] by means of Ck = {(xn, wn) \u2208 X |znk = 1}. If we apply Lemma 8 through 10 with \u03bb = \u221a 4K and \u03bd = \u03bb2, we can take the union bound and obtain that the inequalities stated in the lemmata hold simultaneously with probability strictly larger than 0. Finally, using Equation (13) yields the claim."}, {"heading": "7.3 Proof of Theorem 5", "text": "We apply Corollary 1 to the given membership values {rnk}n,k and the point set\nX\u0302 =\n{(\nxn, wn wmax\n) \u2223 \u2223 \u2223 \u2223 (xn, wn) \u2208 X } .\nLet k \u2208 [K]. Note that the cluster weight R\u0302k with respect to X\u0302 is given by\nR\u0302k =\nN\u2211\nn=1\nrmnk ( wn wmax ) = 1 wmax Rk .\nUsing mink\u2208[K] Rk \u2265 16Kwmax/\u01eb, we can conclude\n\u01eb \u00b7 R\u0302k \u2265 16K . (18)\nObserve that\n\u03b7k =\n\u221a \u221a \u221a \u221a N\u2211\nn=1\nrmnk(1\u2212 rmnk) (\nwn wmax\n)2\n\u2264\n\u221a \u221a \u221a \u221a N\u2211\nn=1\nrmnk wn wmax =\n\u221a\nR\u0302k . (19)\nDue to Inequality (18), we have\n\u221a 4K \u2264\n\u221a 16K\n2 <\n\u221a\n\u01eb \u00b7 R\u0302k 2 <\n\u221a\nR\u0302k 2 . (20)\nUsing Inequality (20) and (19), we can conclude\n\u221a 4K \u00b7 \u03b7k <\nR\u0302k 2 . (21)\nHence, Inequality (15) of Corollary 1 yields Inequality (7) of Theorem 5. Note that the optimal mean vectors \u00b5k with respect to X and {rnk}n,k coincide with the corresponding optimal mean vectors with respect to X\u0302 and {rnk}n,k (cf. Equation (2)). Next, we have\n\u03c4k = N\u2211\nn=1\nrmnk (1\u2212 rmnk) (\nwn wmax\n)2\n\u2016xn \u2212 \u00b5k\u201622 \u2264 1\nwmax \u03c6 (m) X,k({rnk}n) .\nUsing Inequality (21), we obtain\nR\u0302k \u2212 \u221a 4K\u03b7k \u2265\nR\u0302k 2 > 0 .\nDue to Inequality (20), we have\n4K \u2264 \u01ebR\u0302k 4 .\nHence, 4K\n(R\u0302k \u2212 \u221a 4K\u03b7k)2 \u2264 \u01eb R\u0302k = \u01eb \u00b7 wmax Rk .\nTherefore, Inequality (16) of Corollary 1 yields Inequality (8) of Theorem 5. Finally, recall that the optimal mean vectors for X and {rnk}n,k coincide with those for X\u0302 and {rnk}n,k. Thus, \u03c6(m)X\u0302,k({rnk}n) = 1 wmax \u03c6 (m) X,k({rnk}n). Analogously, for all C \u2282 X and C\u0302 \u2282 X\u0302 with {(\nx, wwmax\n)\u2223 \u2223 \u2223(x,w) \u2208 C }\n= C\u0302 we have km(C\u0302) = 1wmax km(C). Hence, Inequality (17) of Corollary 1\nyields Inequality (9) of Theorem 5. \u2293\u2294"}, {"heading": "7.4 Superset Sampling (Proof of Theorem 6)", "text": "Recall that, in Section 7, we argued on the existence of hard clusters which approximate optimal fuzzy clusters well. In this section we consider the problem of finding a good approximation to the means of such unknown hard clusters.\nFirst, consider the problem of finding the mean of a single unknown cluster C. That is, given a set X \u2282 RD and some unknown subset C \u2282 X , we want to find a good approximation to \u00b5(C). If we assume that C contains at least a constant fraction of the points of X , then this problem can be solved via the superset sampling technique [8]. The main idea behind this technique is that the mean of a small uniform sample of a set is, with high probability, already a good approximation to the mean of the whole set. Knowing that C contains a constant fraction of points from X , we can obtain a uniform sample of C by sampling a uniform multiset S from X and inspecting all subsets of S. Thereby, we obtain a set of candidate means, i.e. the means of all subsets of S, including (with certain probability) one candidate that is a good approximation to the mean of C. Formally, using [21], we directly obtain the following lemma.\nLemma 11 (Superset Sampling [21]). Let X \u2282 RD, \u03b1 \u2208 (0, 1], and \u01eb \u2208 (0, 1]. Let S \u2282 X be a uniform sample multiset of size at least 4/(\u03b1\u01eb).\nConsider an arbitrary but fixed (unknown) subset C \u2282 X with |C| \u2265 \u03b1 |X |. With probability at least 1/10 there exists a subset C\u2032 \u2282 S with |C\u2032| = \u23082/\u01eb\u2309 satisfying\n\u2016\u00b5(C) \u2212 \u00b5(C\u2032)\u201622 \u2264 \u01eb |C| \u2211\nx\u2208C \u2016x\u2212 \u00b5(C)\u201622 .\nwhere for any finite set S \u2282 RD we set \u00b5(S) := \u2211 x\u2208S x\n|S| .\nAs a consequence, for weighted point sets X with weights in Q, we obtain a good approximation of the mean of X by sampling each (unweighted) point with probability proportional to its weight.\nCorollary 2 (Weighted Superset Sampling). Let X = {(xn, wn)}n\u2208[N ] \u2282 RD \u00d7 Q\u22650, \u03b1 \u2208 (0, 1], and \u01eb \u2208 (0, 1]. Let W = \u2211Nn=1 wn. Let S \u2282 {(xn, 1)}n\u2208[N ] be a sample multiset of size at least 4/(\u03b1\u01eb), where each point xn \u2208 X is sampled with probability wn/W .\nConsider an arbitrary but fixed (unknown) subset C \u2282 X with w(C) \u2265 \u03b1W . With probability at least 1/10 there exists a subset C\u2032 \u2282 S with |C\u2032| = \u23082/\u01eb\u2309 satisfying\n\u2016\u00b5(C) \u2212 \u00b5(C\u2032)\u201622 \u2264 \u01eb\nw(C) km(C) .\nProof. Let \u03c9 be a common denominator of all {wn}n\u2208[N ]. Let X\u0302 \u2282 RD be the multiset containing wn \u00b7\u03c9 copies of each xn with (xn, wn) \u2208 X , and let C\u0302 \u2282 RD be the multiset containing wn \u00b7\u03c9 copies of each xn with (xn, wn) \u2208 C. Note that sampling a point uniformly at random from X\u0302 yields the same distribution on the points as sampling a point from X with probability wn/W . Furthermore, if C \u2282 X with w(C) \u2265 \u03b1W , then \u02c6|C| = w(C) \u2265 \u03b1W = \u03b1 \u02c6|X |. Hence, applying the previous lemma directly yields the claim.\nGiven Corollary 2, we can finally prove Theorem 6.\nProof (Proof of Theorem 6).\nLet R = \u230810 log(2K)\u2309 and let W = \u2211Nn=1 wn. For each r \u2208 [R], sample an unweighted multiset Sr \u2282 X of size at least 4/(\u03b1\u01eb) by choosing a point xn \u2208 X with probability wn/W . Define the candidate set T as\nT :=\n{\n\u00b5(S\u2032) \u2223 \u2223 \u2223 \u2223 S\u2032 \u2282 Sr, |S\u2032| = \u23082/\u01eb\u2309, r \u2208 [R] }K .\nNext, we prove that T and its construction have the desired properties.\nLet M = {\u00b5(S\u2032) | S\u2032 \u2282 Sr, |S\u2032| = \u23082/\u01eb\u2309, r \u2208 [R]} and fix an arbitrary k \u2208 [K] with w(Ck) \u2265 \u03b1W . By Lemma 2, there is, with probability p := 1\u2212 (9/10)R, a candidate \u00b5\u0303k \u2208 M satisfying\n\u2016\u00b5(Ck)\u2212 \u00b5\u0303k\u201622 \u2264 \u01eb\nw(Ck) km(Ck) .\nSince R \u2265 10 ln(2K), we have that (9/10)R \u2264 1/(2K). Hence, p \u2265 1\u2212 1/(2K). By taking the union bound, we obtain that, with probability at least 1/2, T = MK contains a tuple (\u00b5\u0303k)k\u2208[K] where for each k \u2208 [K] the vector \u00b5\u0303k is close to \u00b5(Ck) if w(Ck) \u2265 \u03b1W . Finally, we analyze the time needed to construct T . We have to sample R multisets of size \u23084/(\u03b1\u01eb)\u2309 from X , which needs running time O(R \u00b7 (1/(\u03b1\u01eb)) \u00b7N). Each of the multisets contains at most (4/(\u03b1\u01eb))2/\u01eb subsets of size \u23082/\u01eb\u2309. We compute the the means of all these subsets, which needs time O(1/\u01eb) per subset. Then, by combining all these means we obtain the candidate set of tuples T . Consequently, this set has size\n|T | \u2264 ( R \u00b7 ( 4\n\u03b1\u01eb\n)1/\u01eb )K\n= 2 1 \u01eb \u00b7log( 4\u03b1\u01eb )\u00b7log(R) .\nHence, we can bound the running time of our construction by\nO (\nR \u00b7 1 \u01eb\n(\nN + 2 K \u01eb \u00b7log( 1\u03b1\u01eb )\u00b7log(R)\n))\n."}, {"heading": "7.5 Candidate Set of Means (Proof of Theorem 7)", "text": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.\nConstruction We are given X = {xn}n\u2208[N ] \u2282 RD and K \u2208 N. Let A = {ak}k\u2208[K] \u2282 RD be an \u03b1-approximation of the K-means problem with respect to X , i.e.\nkmX(A) \u2264 \u03b1 \u00b7 kmX,K . (22)\nFurthermore, let\nR :=\n\u221a\nkmX(A)\n\u03b1N , (23)\nB(x, r) := { y \u2208 RD | \u2016x\u2212 y\u20162 \u2264 r } , (24)\n\u03a6 :=\n\u2308 1\n2\n( log(\u03b1N) +m \u00b7 log ( 64\u03b1mK2\n\u01eb\n))\u2309\n, (25)\nU := K\u22c3\nk=1\nB(ak, 2\u03a6R) , (26)\nLk,j :=\n{\nB(ak, R), if j = 0 B(ak, 2jR) \\ B(ak, 2j\u22121R) if j \u2265 1 , (27)\n\u03ba := \u03b1Km\u22121 , and (28)\nb := 1208 (29)\nfor all k \u2208 [K], j \u2208 {0, . . . , \u03a6}, x \u2208 RD, and r \u2208 R. Construct an axis-parallel grid with side length\n\u03c1j = 2j\u01ebR\nb\u03ba \u221a D\nto partition Lk,j into cells. Inside each Lk,j pick the center of the cell as its representative point. Denote by G be the set of all representative points.\nProof of Theorem 7 In Section 7.5, we show that there exists C\u0303 = {\u00b5\u0303l}l\u2208[L] \u2286 U with L \u2208 [K] and\n\u03c6 (m) X (C\u0303) \u2264\n(\n1 + \u01eb\n2\n)\n\u03c6OPT(X,K,m) .\nSince C\u0303 = {\u00b5\u0303l}l\u2208[L] \u2286 U , there our construction from Section 7.5 defines a representative \u00b5\u0303\u2032l \u2208 G for each \u00b5\u0303l \u2208 C\u0303.\nIn Section 7.5, we consider arbitrary sets C = U with representatives C\u2032 \u2286 G. We show that K-means costs of C and C\u2032 are similar. Given this result, in Section 7.5, we show that the fuzzy K-means costs of C and C\u2032 are similar. More precisely, we show that if \u03b1 \u2264 2, then\n\u2223 \u2223 \u2223\u03c6\n(m) X (C)\u2212 \u03c6 (m) X (C\n\u2032) \u2223 \u2223 \u2223 \u2264 \u01eb\n4 \u03c6 (m) X (C) .\nIn particular, this holds for C\u0303 and its representatives C\u0303\u2032 \u2282 G. From these results we can conclude that\n\u03c6 (m) X (C\u0303\n\u2032) \u2264 ( 1 + \u01eb\n4\n)\n\u03c6 (m) X (C\u0303) \u2264\n(\n1 + \u01eb\n2\n)(\n1 + \u01eb\n4\n)\n\u03c6OPT(X,K,m) \u2264 (1 + \u01eb)\u03c6OPT(X,K,m) .\nFinally, observe that for all M \u2282 RD it holds \u03c6(m)X (C\u0303\u2032\u222aM) \u2264 \u03c6 (m) X (C\u0303 \u2032). Thus, by testing all subsets {\u00b5\u2032k}k\u2208[K] \u2282 G, we find a solution that is at least as good as C\u0303\u2032 and hence a (1+ \u01eb)-approximation.\nPreliminaries In the following proof, we make extensive use of the following lemmata, as well as the lemmata given in Section 6.1.\nCorollary 3. Let X \u2282 RD \u00d7 R\u22650 and C \u2282 RD with |C| = K. If kmX(C) \u2264 \u03b3 kmX,K , then \u03c6 (m) X (C) \u2264 (\u03b3 \u00b7Km\u22121)\u03c6OPT(X,K,m). If \u03c6 (m) X (C) \u2264 \u03b3\u03c6OPT(X,K,m), then kmX(C) \u2264 (\u03b3 \u00b7Km\u22121) kmX,K\nProof. Use Lemma 1.\nLemma 12. For all C \u2282 RD, |C| = K, we have\nN \u00b7 R2 = N\u2211\nn=1\nd(xn, A) 2 = kmX(A) \u2264 \u03b1 \u00b7 kmX,K \u2264 \u03ba \u00b7 \u03c6OPT(X,K,m) \u2264 \u03ba \u00b7 \u03c6 (m) X (C) .\nProof. Use Definition 6, Equation (23), Equation (22), and Corollary 3.\nLemma 13. For each \u00b5 \u2208 U with representative \u00b5\u2032 \u2208 G it holds\n\u2016\u00b5\u2212 \u00b5\u2032\u20162 \u2264 2\u01eb b\u03ba (d (\u00b5\u0303, A) +R) \u2264 2\u01eb b\u03ba (\u2016x\u2212 \u00b5\u0303\u20162 + d(x,A) +R) .\nand\n\u2016\u00b5\u2212 \u00b5\u2032\u201622 \u2264 12\u01eb2\nb2\u03ba2\n( \u2016x\u2212 \u00b5\u0303\u201622 + d(x,A)2 +R2 )\nfor all x \u2208 RD and \u00b5\u0303 \u2208 {\u00b5, \u00b5\u2032}.\nProof. By construction, some Lk,j contains \u00b5 and its representative \u00b5 \u2032. Moreover, \u00b5 and \u00b5\u2032 are contained in the same grid cell with side length \u03c1j . Hence, \u2016\u00b5\u2212 \u00b5\u2032\u20162 \u2264 2 j\u01ebR b\u03ba . By construction of Lk,j , we have min{d(\u00b5,A), d(\u00b5\u2032, A)} \u2265 2j\u22121R for all xn \u2208 Lk,j with j \u2265 1. For j = 0 we know that \u2016\u00b5\u2212 \u00b5\u2032\u20162 \u2264 2 0\u01ebR b\u03b1 = \u01ebR b\u03ba . Applying Lemma 7 and observing that for all x, y \u2208 RD and C \u2282 RD we have d(x,C) \u2264 \u2016x\u2212 y\u20162 + d(y, C), yields the claim.\nExistence of an (1 + \u01eb2 )-Approximation in U\nClaim. There exists C\u0303 = {\u00b5\u0303k}k \u2282 U with\n\u03c6 (m) X (C\u0303) \u2264\n(\n1 + \u01eb\n2\n)\n\u03c6OPT(X,K,m) .\nIn the following, we prove Claim 7.5.\nClaim.\n\u22c3\nx\u2208X B(x, r) \u2286\nK\u22c3\nk=1\nB(ak, 2\u03a6R) = U\nwhere\nr =\n\u221a (\n1 + \u01eb\n2\n)(8mK2\n\u01eb\n)m\nkmX(A) .\nProof. Towards a contradiction, assume that there exists an x \u2208 X with B(x, r) * U . By definition of U , this implies that for all k \u2208 [K] we have B(x, r) * B(ak, 2\u03a6R). Hence,\nd(x,A)\n\u2265 2\u03a6R\u2212 r = 2\u03a6 \u221a kmX(A)\n\u03b1N \u2212 \u221a ( 1 + \u01eb 2 )(8mK2 \u01eb )m kmX(A) (by Equation (23))\n= \u221a\nkmX(A)\n(\n2\u03a6 \u221a 1 \u03b1N \u2212 \u221a ( 1 + \u01eb 2 )(8mK2 \u01eb )m ) .\nObserve that by Equation (25), we have\n\u03a6 = 1\n2\n( log(\u03b1N) +m \u00b7 log ( 64\u03b1mK2\n\u01eb\n))\n= log (\u221a \u03b1N ) + log\n(\u221a( 64\u03b1mK2\n\u01eb\n)m )\n\u2265 log (\u221a \u03b1N ) + log\n(\u221a\n2 \u00b7 2\u03b1 \u00b7 2 ( 8mK2\n\u01eb\n)m )\n= log ( \u221a \u03b1N \u00b7\n(\u221a\n2 \u00b7 2\u03b1 \u00b7 2 ( 8mK2\n\u01eb\n)m ))\n\u2265 log ( \u221a \u03b1N \u00b7 ( \u221a 2\u03b1+\n\u221a\n2\n( 8mK2\n\u01eb\n)m ))\n(since \u221a a+ b \u2264 2 \u221a ab for all a, b \u2265 1)\n\u2265 log ( \u221a \u03b1N \u00b7 ( \u221a 2\u03b1+ \u221a ( 1 + \u01eb\n2\n)(8mK2\n\u01eb\n)m ))\n. (since \u01eb < 1)\nHence, d(x,A) \u2264 \u221a kmX(A) \u221a 2\u03b1 .\nUsing Definition 6, we can conclude\nkmX(A) \u2265 d(x,A)2 \u2265 2\u03b1 kmX(A) \u2265 2\u03b1kmX,K > \u03b1 kmX,K ,\nwhich contradicts Equation (22).\nClaim. Let C\u0303 = {\u00b5\u0303k}k\u2208[K] \u2282 RD such that\n\u03c6 (m) X (C\u0303) \u2264\n(\n1 + \u01eb\n4K\n)\u2113\n\u03c6OPT(X,K,m) .\nLet {C\u0303k}k\u2208[K] be a partition of X induced by C\u0303. For all k \u2208 [K] we have\n(\u00b5k /\u2208 U \u2227 Ck = \u2205) \u21d2 \u03c6(m)X (C\u0303 \\ {\u00b5k}) \u2264 ( 1 + \u01eb\n4K\n)\u2113+1\n\u03c6 (m) X (C\u0303) .\nProof. Let {rnk}n,k be the optimal responsibilities induced by C\u0303. Using Claim 7.5 and \u00b5k /\u2208 U , we obtain\n(\n1 + \u01eb\n4K\n)\u2113\n\u03c6OPT(X,K,m) \u2265 \u03c6 (m) X (C\u0303) \u2265 \u03c6 (m) X,k(C\u0303) =\nN\u2211\nn=1\nrmnk \u2016xn \u2212 \u00b5k\u201622 \u2265 ( N\u2211\nn=1\nrmnk\n)\nr2 ,\nwhere r is defined as in Claim 7.5. Hence,\nN\u2211\nn=1\nrmnk \u2264 ( 1 + \u01eb4K )\u2113 \u03c6OPT(X,K,m)\nr2\n\u2264 ( 1 + \u01eb4K )\u2113 kmX(A)\nr2 (Lemma 1)\n=\n( 1 + \u01eb4K )K kmX(A)\n( 1 + \u01eb2 ) ( 8mK2 \u01eb )m kmX(A)\n(Claim 7.5)\n\u2264 ( 1 + \u01eb2 )\n( 1 + \u01eb2 ) ( 8mK2 \u01eb\n)m (Lemma 6)\n\u2264 ( \u01eb\n8mK2\n)m\n.\nConsequently,\nrnk \u2264 \u01eb\n8mK2 . (30)\nSince Ck = \u2205, for all n \u2208 [N ] there exists an l(n) \u2208 [K] with l(n) 6= k and\nrn,l(n) \u2265 1\nK .\nUsing Equation (30), we can conclude\nrnk \u2264 \u01eb 8mK2 \u2264 \u01eb 8mK rn,l(n) .\nUsing Lemma 6, we obtain ( rnk + rn,l(n) )m \u2264 (( 1 + \u01eb\n8Km\n)\nrn,l(n)\n)m \u2264 ( 1 + \u01eb\n4K\n)\nrmn,l(n) .\nHence, we have\n\u03c6 (m) X (C\u0303 \\ {\u00b5\u0303k}) \u2264\nN\u2211\nn=1\n\u2211\nl 6=l(n),k rmnk \u2016xn \u2212 \u00b5\u0303l\u201622 +\n( rn,l(n) + rnk )m \u2225 \u2225xn \u2212 \u00b5\u0303l(n) \u2225 \u2225 2\n2\n\u2264 N\u2211\nn=1\n\u2211\nl 6=l(n),k rmnk \u2016xn \u2212 \u00b5\u0303l\u201622 +\n(\n1 + \u01eb\n4K\n)\nrmn,l(n) \u2225 \u2225xn \u2212 \u00b5\u0303l(n) \u2225 \u2225 2\n2\n\u2264 ( 1 + \u01eb\n4K\n)\n\u03c6 (m) X (C\u0303)\n\u2264 ( 1 + \u01eb\n4K\n)\u2113+1\n\u03c6OPT(X,K,m)\nwhich yields the claim.\nClaim. There exists L \u2208 [K] and C\u0303 = {\u00b5\u0303l}l\u2208[L] \u2282 RD satisfying the following properties: 1. \u03c6\n(m) X (C\u0303) \u2264 ( 1 + \u01eb2 ) \u03c6OPT(X,K,m)\n2. Let {C\u0303l}l\u2208[L] be a partition of X induced by C. For all l \u2208 [L] we have \u00b5l /\u2208 U \u21d2 C\u0303l 6= \u2205 .\nProof. Consider an arbitrary but fixed optimal fuzzy K-means solution O = {ok}Kk=1. There are at most K \u2212 1 means in O that are not in U and where the corresponding clusters Ok are empty. By repeatedly applying Claim 7.5, we obtain a solution O\u0303 with \u2223 \u2223 \u2223O\u0303 \u2223 \u2223 \u2223 \u2264 K satisfying the second\nproperty in the claim. Furthermore, we have \u03c6 (m) X (O\u0303) \u2264 (1 + \u01eb4K )K\u03c6OPT(X,K,m) \u2264 ( 1 + \u01eb2 ) \u03c6OPT(X,K,m), where the last inequality is due to Lemma 6. This yields the claim.\nProof (Proof of Claim 7.5). Consider the solution C\u0303 = {\u00b5\u0303l}l\u2208[L] from Claim 7.5. Assume \u00b5\u0303l /\u2208 U . Then, by the second property of Claim 7.5, the corresponding cluster C\u0303l is not empty. Hence,\n\u03c6 (m) X (C\u0303) \u2265\n1\nKm\u22121 \u00b7 km(C\u0303) (Lemma 1 and L \u2265 K)\n\u2265 1 Km\u22121 \u00b7 \u2211\nx\u2208C\u0303k\n\u2016xn \u2212 \u00b5\u0303k\u201622\n\u2265 1 Km\u22121 \u00b7 \u2211\nx\u2208C\u0303k\nr2 (by Claim 7.5)\n\u2265 1 Km\u22121 \u00b7 r2 (Ck 6= \u2205)\n\u2265 1 Km\u22121 \u00b7 ( 1 + \u01eb 2\n)(2mK2\n\u01eb\n)m\nkmX(A)\n> ( 1 + \u01eb\n2\n)\n\u03c6OPT(X,K,m) , (by Lemma 1)\nwhich is a contradiction to the first property of Claim 7.5. Hence, from the properties of Claim 7.5, we can conclude that C\u0303 \u2282 U .\nNotation For the following proofs fix an arbitrary solution\nC = {\u00b5k}k\u2208[K] \u2282 U \u2282 RD . Let {rnk}n,k be the optimal responsibilities induced by C. We denote the representative of \u00b5k \u2208 C by \u00b5\u2032k \u2208 G, and the set of all these representatives by\nC\u2032 := {\u00b5\u20321, . . . , \u00b5\u2032K | \u00b5\u2032k is representative of \u00b5k \u2208 C for all k \u2208 [K]} .\nCloseness with respect to the K-Means Problem\nClaim. kmX(C\n\u2032) \u2264 \u03b3 kmX(C) , where\n\u03b3 = 1 + 72\u01eb\nbKm\u22121 .\nIn the following, we prove Claim 7.5 To this end, denote by \u02d9 \u22c3 k\u2208[K]Ck = X and \u02d9\u22c3 k\u2208[K]C \u2032 k = X\nthe partitions (ties broken arbitrarily) induced by C and C\u2032, respectively.\nClaim. If kmX(C) > kmX(C \u2032), then\n|kmX(C) \u2212 kmX(C\u2032)| \u2264 K\u2211\nk=1\n|C\u2032k| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\n\u2211\nxn\u2208C\u2032k\n\u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 ,\notherwise\n|kmX(C) \u2212 kmX(C\u2032)| \u2264 K\u2211\nk=1\n|Ck| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\n\u2211\nxn\u2208Ck \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u2016 2 2 .\nProof. If kmX(C) \u2265 kmX(C\u2032), then\n|kmX(C) \u2212 kmX(C\u2032)| = K\u2211\nk=1\n\u2211\nxn\u2208Ck \u2016xn \u2212 \u00b5k\u201622 \u2212\n\u2211\nxn\u2208C\u2032k\n\u2016xn \u2212 \u00b5\u2032k\u2016 2 2\n\u2264 K\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n\u2016xn \u2212 \u00b5k\u201622 \u2212 \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 ({Ck}k induced by C)\n\u2264 K\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n\u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2 \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 (Lemma 5)\nIf kmX(C) < kmX(C \u2032), then the term can be bounded analogously. This yields the claim.\nClaim. K\u2211\nk=1\n|Ck| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n36\u01eb2\nb2\u03ba kmX(C)\nProof.\nK\u2211\nk=1\n|Ck| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n12\u01eb2 b2\u03ba\nK\u2211\nk=1\n\u2211\nxn\u2208Ck\n( \u2016xn \u2212 \u00b5k\u201622 + d(xn, A)2 +R2 )\n(Lemma 13)\n= 12\u01eb2\nb2\u03ba2\n( K\u2211\nk=1\n\u2211\nxn\u2208Ck \u2016xn \u2212 \u00b5k\u201622 +\nN\u2211\nn=1\nd(xn, A) 2 +NR2\n)\n\u2264 12\u01eb 2 b2\u03ba2 ( kmX(C) + kmX(A) +NR 2 ) \u2264 12\u01eb 2\nb2\u03ba2 (kmX(C) + 2\u03b1 kmX,K) (Lemma 12)\n\u2264 36\u01eb 2\nb2\u03ba kmX(C) (since \u03b1 \u2265 1)\nClaim.\nK\u2211\nk=1\n|C\u2032k| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n12\u01eb2 b2\u03ba2 (kmX (C \u2032) + 2\u03b1 \u00b7 kmX(C))\nProof.\nK\u2211\nk=1\n|C\u2032k| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n12\u01eb2 b2\u03ba2\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n(\n\u2016xn \u2212 \u00b5\u2032k\u2016 2 2 + d(xn, A)\n2 +R2 )\n(Lemma 13)\n= 12\u01eb2\nb2\u03ba2\n\n\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n\u2016xn \u2212 \u00b5\u2032k\u2016 2 2 +\nN\u2211\nn=1\nd(xn, A) 2 +NR2\n\n\n\u2264 12\u01eb 2 b2\u03ba2 ( kmX(C \u2032) + kmX(A) +NR 2 ) \u2264 12\u01eb 2\nb2\u03ba2 (kmX(C\n\u2032) + 2\u03b1 \u00b7 kmX,K) (Lemma 12)\n\u2264 12\u01eb 2\nb2\u03ba2 (kmX(C\n\u2032) + 2\u03b1 \u00b7 kmX(C))\nClaim.\n2\nK\u2211\nk=1\n\u2211\nxn\u2208Ck \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162 \u2264\n24\u01eb\nbKm\u22121 kmX(C)\nProof.\n2 K\u2211\nk=1\n\u2211\nxn\u2208Ck\n\u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162\n\u2264 2\u01eb b\u03ba\nK\u2211\nk=1\n\u2211\nxn\u2208Ck 2 (\u2016x\u2212 \u00b5k\u20162 + d(xn, A) +R) \u2016xn \u2212 \u00b5k\u20162 (Lemma 13)\n\u2264 6\u01eb b\u03ba\nK\u2211\nk=1\n\u2211\nxn\u2208Ck\n(\n\u2016x\u2212 \u00b5k\u201622 + d(xn, A)2 +R2 + \u2016xn \u2212 \u00b5k\u2016 2 2\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba ( 2 kmX(C) + kmX(A) +NR 2 ) \u2264 6\u01eb b\u03ba (2 kmX(C) + 2\u03b1 kmX,K) (Lemma 12) \u2264 24\u01eb bKm\u22121 kmX(C) (since \u03b1 \u2265 1)\nClaim.\n2\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n\u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162 \u2264 12\u01eb\nb\u03ba (kmX(C\n\u2032) + \u03b1 \u00b7 kmX(C))\nProof.\n2\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n\u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162\n\u2264 2\u01eb b\u03ba\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n2 (\u2016x\u2212 \u00b5\u2032k\u20162 + d(xn, A) +R) \u2016xn \u2212 \u00b5\u2032k\u20162 (Lemma 13)\n\u2264 6\u01eb b\u03ba\nK\u2211\nk=1\n\u2211\nxn\u2208C\u2032k\n(\n\u2016x\u2212 \u00b5\u2032k\u2016 2 2 + d(xn, A) 2 +R2 + \u2016xn \u2212 \u00b5\u2032k\u2016 2 2\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba ( 2 kmX(C \u2032) + kmX(A) +NR 2 ) \u2264 6\u01eb b\u03ba (2 kmX(C \u2032) + 2\u03b1 kmX,K) (Lemma 12) \u2264 12\u01eb b\u03ba (kmX(C \u2032) + \u03b1 kmX(C))\nProof (Proof of Claim 7.5). If kmX(C) > kmX(C \u2032), then by Claim 7.5, 7.5, and 7.5 we have\n0 \u2264 kmX(C) \u2212 kmX(C\u2032)\n\u2264 K\u2211\nk=1\n|C\u2032k| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\n\u2211\nxn\u2208C\u2032k\n\u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u2016 2 2\n\u2264 12\u01eb 2\nb2\u03ba2 (kmX (C\n\u2032) + 2\u03b1 \u00b7 kmX(C)) + 12\u01eb\nb\u03ba (kmX(C\n\u2032) + 2 \u00b7 kmX(C))\n\u2264 24\u01eb b\u03ba (kmX (C \u2032) + 2\u03b1 \u00b7 kmX(C))\nHence,\n(\n1\u2212 48\u03b1\u01eb b\u03ba\n) kmX(C)\u2212 ( 1 + 24\u01eb\nb\u03ba\n)\nkmX(C \u2032) \u2264 0\n(\n1 + 24\u01eb\nb\u03ba\n) (kmX(C) \u2212 kmX(C\u2032)) \u2264 ( 24\u01eb\nb\u03ba +\n48\u03b1\u01eb\nb\u03ba\n)\nkmX(C)\nkmX(C)\u2212 kmX(C\u2032) \u2264 ( 24\u01eb\nb\u03ba +\n48\u03b1\u01eb\nb\u03ba\n)\n/\n(\n1 + 24\u01eb\nb\u03ba\n)\nkmX(C)\n\u2264 72\u01eb bKm\u22121 kmX(C) .\nIf km(C\u2032) > km(C), then by Claim 7.5, 7.5 and 7.5 we obtain\n0 \u2264 kmX(C) \u2212 kmX(C\u2032)\n\u2264 K\u2211\nk=1\n|Ck| \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\n\u2211\nxn\u2208Ck \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u2016 2 2\n\u2264 36\u01eb 2\nb2\u03ba kmX(C) +\n24\u01eb\nbKm\u22121 kmX(C)\n\u2264 60\u01eb bKm\u22121 kmX(C)\nCloseness with respect to the Fuzzy K-Means Problem\nClaim. If \u03b1 \u2264 2, then \u2223 \u2223 \u2223\u03c6\n(m) X (C)\u2212 \u03c6 (m) X (C\n\u2032) \u2223 \u2223 \u2223 \u2264 \u01eb\n4 \u03c6 (m) X (C)\nIn the following we prove Claim 7.5. To this end, let {rnk}n,k and {r\u2032nk}n,k be the optimal responsibilities with respect to C and C\u2032, respectively. Then, let\nE := \u2223 \u2223 \u2223\u03c6\n(m) X (C)\u2212 \u03c6 (m) X (C\n\u2032) \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223 \u2223 N\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016xn \u2212 \u00b5k\u201622 \u2212 (r\u2032nk)m \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 \u2223 \u2223 \u2223 \u2223 \u2223 .\nClaim.\nE \u2264 max { N\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162 ,\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162\n}\n.\nProof. If the first term in E is larger than the second, then\nE = N\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016xn \u2212 \u00b5k\u201622 \u2212 (r\u2032nk)m \u2016xn \u2212 \u00b5\u2032k\u2016 2 2\n\u2264 N\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m ( \u2016xn \u2212 \u00b5k\u201622 \u2212 \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 )\n(rnk optimal wrt. C)\n\u2264 N\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m ( \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2 \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162 ) , (Lemma 5)\nAnalogously, if the second term in E is larger than the first, then\nE \u2264 N\u2211\nn=1\nK\u2211\nk=1\nrmnk\n(\n\u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2 \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162\n)\n.\nThis yields the claim.\nClaim.\n\u03c6 (m) X (C \u2032) \u2264 \u03b3Km\u22121 \u00b7 \u03c6(m)X (C)\nProof. Using Claim 7.5 and Lemma 1, we obtain\n\u03c6 (m) X (C \u2032) \u2264 kmX(C\u2032) \u2264 \u03b3 kmX(C) \u2264 \u03b3Km\u22121 \u00b7 \u03c6(m)X (C) .\nClaim.\nK\u2211\nk=1\nmax\n{ N\u2211\nn=1\nrmnk,\nN\u2211\nn=1\n(r\u2032nk) m\n}\n\u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n36(\u03b3 + 2\u03b1)\u01eb2\nb2\u03b1 \u03c6 (m) X (C)\nProof. Observe that\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n12\u01eb2 b2\u03ba2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk\n(\n\u2016xn \u2212 \u00b5k\u201622 + d (xn, A) 2 +R2\n)\n(Lemma 13)\n\u2264 12\u01eb 2\nb2\u03ba2\n(\n\u03c6 (m) X (C) +\nN\u2211\nn=1\nd (xn, A) 2 +NR2\n)\n\u2264 36\u01eb 2\nb2\u03ba \u03c6 (m) X (C) (Lemma 12)\nSimilarly,\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 \u2264\n12\u01eb2 b2\u03ba2\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m ( \u2016xn \u2212 \u00b5\u2032k\u2016 2 2 + d (xn, A) 2 +R2 ) (Lemma 13)\n\u2264 12\u01eb 2\nb2\u03ba2\n(\n\u03c6 (m) X (C\n\u2032) + N\u2211\nn=1\nd (xn, A) 2 +NR2\n)\n\u2264 12\u01eb 2\nb2\u03ba2\n(\n\u03c6 (m) X (C \u2032) + 2\u03ba\u03c6(m)X (C) )\n(Lemma 12)\n\u2264 12\u01eb 2\nb2\u03ba2\n(\n\u03b3Km\u22121\u03c6(m)X (C) + 2\u03ba\u03c6 (m) X (C)\n)\n(Claim 7.5)\n\u2264 12(\u03b3 + 2\u03b1)\u01eb 2\nb2\u03b12 \u03c6 (m) X (C) .\nClaim.\n2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162 \u2264 24\u01eb\nb \u03c6 (m) X (C)\nProof.\n2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162\n\u2264 2\u01eb b\u03ba\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk2 (\u2016\u00b5k \u2212 xn\u20162 + d(xn, A) +R) \u2016xn \u2212 \u00b5k\u20162 (Lemma 12)\n\u2264 2\u01eb b\u03ba\n( N\u2211\nn=1\nK\u2211\nk=1\nrmnk (\u2016\u00b5k \u2212 xn\u20162 + d(xn, A) +R) 2 +\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016xn \u2212 \u00b5k\u201622\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba\n( N\u2211\nn=1\nK\u2211\nk=1\nrmnk\n( \u2016\u00b5k \u2212 xn\u201622 + d(xn, A)2 +R2 ) + \u03c6 (m) X (C)\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba\n(\n\u03c6 (m) X (C) +\nN\u2211\nn=1\nd(xn, A) 2 +NR2 + \u03c6 (m) X (C)\n)\n\u2264 24\u01eb b \u03c6 (m) X (C) (Lemma 12)\nClaim.\n2\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162 \u2264\n12(\u03b3 + \u03b1)\u01eb\nb\u03b1 \u03c6 (m) X (C)\nProof.\n2\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162\n\u2264 2\u01eb b\u03ba\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m2 (\u2016\u00b5\u2032k \u2212 xn\u20162 + d(xn, A) +R) \u2016xn \u2212 \u00b5\u2032k\u20162 (Lemma 12)\n\u2264 2\u01eb b\u03ba\n( N\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m (\u2016\u00b5\u2032k \u2212 xn\u20162 + d(xn, A) +R) 2 +\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) \u2016xn \u2212 \u00b5\u2032k\u2016 2 2\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba\n( N\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m ( \u2016\u00b5\u2032k \u2212 xn\u2016 2 2 + d(xn, A) 2 +R2 ) + \u03c6 (m) X (C \u2032)\n)\n(Lemma 7)\n\u2264 6\u01eb b\u03ba\n(\n\u03c6 (m) X (C\n\u2032) + N\u2211\nn=1\nd(xn, A) 2 +NR2 + \u03c6 (m) X (C \u2032)\n)\n\u2264 6\u01eb b\u03ba\n(\n2\u03c6 (m) X (C \u2032) + 2\u03ba\u03c6(m)X (C) )\n(Lemma 12)\n\u2264 12(\u03b3 + \u03b1)\u01eb b\u03b1 \u03c6 (m) X (C) (Claim 7.5)\nProof (Proof of Claim 7.5).\nE \u2264 max { N\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162 ,\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 + 2\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162\n}\n\u2264 max { N\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2 ,\nN\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u2016 2 2\n}\n+\nmax\n{\n2\nN\u2211\nn=1\nK\u2211\nk=1\nrmnk \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5k\u20162 , 2 N\u2211\nn=1\nK\u2211\nk=1\n(r\u2032nk) m \u2016\u00b5k \u2212 \u00b5\u2032k\u20162 \u2016xn \u2212 \u00b5\u2032k\u20162\n}\n\u2264 36(\u03b3 + 2\u03b1)\u01eb 2\nb2\u03b1 \u03c6 (m) X (C) +\n24(\u03b3 + \u03b1)\u01eb\nb \u03c6 (m) X (C) (By Claims 7.5, 7.5 and 7.5)\n\u2264 60(\u03b3 + 2\u03b1)\u01eb b \u03c6 (m) X (C)\nwhere \u03b3 = 1 + 72\u01ebbKm\u22121 (cf. Claim 7.5). Since b = 1208 and \u03b1 \u2264 2, we have E \u2264 \u01eb4\u03c6 (m) X (C), which yields the claim.\nUpper Bound on the Size of G In the following, we upper bound |G| analogously to [20]. Recall from Section 7.5 that each Lk,j is partitioned into an axis-parallel grid with side length \u03c1j = 2j\u01ebR b\u03ba \u221a D . Hence, the volume of each grid cell is\nVj =\n( 2j\u01ebR\nb\u03ba \u221a D\n)D\n.\nFurthermore, observe that the distance between a point x \u2208 Lk,j and mean vector ak is at most 2jR+ \u03c1j < 2\nj+1R. Hence, the grid cell that contains x is contained in B(ak, 2j+1R). Each of these balls has a volume of\nvol(B(ak, 2j+1R)) = \u03c0D/2(2j+1R)D\n\u0393 (D/2 + 1) .\nConsequently, the number of grid cells in each Lk,j is bounded by\nvol(B(ak, 2j+1R)) Vj =\n( \u03c0D/2(2j+1R)D\n\u0393 (D/2 + 1)\n)( b\u03ba \u221a D\n2j\u01ebR\n)D\n\u2264 ( \u03c0 D/2(2j+1R)D\n(D/4e)D/2\n)( b\u03ba \u221a D\n2j\u01ebR\n)D\n=\n( 2b\u03ba\n\u01eb\n)D\n(4e\u03c0) D/2 \u2264\n( 12b\u03ba\n\u01eb\n)D\n.\nOverall, we obtain\n|G| \u2264 \u03a6\u2211\nj=0\nK\u2211\nk=1\nvol(B(ak, 2j+1R)) Vj\n\u2264 K ( log(\u03b1N) +m log ( 64\u03b1mK2\n\u01eb\n))( 12b\u03ba\n\u01eb\n)D\n= O ( KmD+1\u01eb\u2212Dm log (mK/\u01eb) log(N) ) ."}, {"heading": "7.6 Unsolvability by Radicals (Proof of Theorem 1)", "text": "Consider the fuzzy 2-means instance with m = 2 and X = {\u22123,\u22122,\u22121, 1, 2, 3} \u2282 R. Let {\u00b5\u22171, \u00b5\u22172} \u2282 R be the means of an optimal solution.\nClaim. sgn(\u00b5\u22171) 6= sgn(\u00b5\u22172)\nProof. Assume w.l.o.g. that \u00b5\u22171, \u00b5 \u2217 2 \u2264 0. Let {rnk}n,k be the memberships induced by {\u00b5\u22171, \u00b5\u22172}. Using the Cauchy-Schwarz inequality, we conclude\n\u03c6OPT(X,2,2) = \u03c6 (2) X (\u00b5 \u2217 1, \u00b5 \u2217 2) \u2265 r231(3\u2212 \u00b51)2 + r232(3\u2212 \u00b52)2 \u2265\n1 2 32 > 4 .\nThis contradicts the fact that, due Lemma 1, we have \u03c6OPT(X,2,2) \u2264 kmX({2,\u22122}) = \u2211\nx\u2208X min{(x\u2212 2)2, (x+ 2)2} = 4. This yields the claim.\nObservation 2 \u00b5\u22171 and \u00b5 \u2217 2 lie inside the convex hull of the point set X.\nFrom this observation and Claim 7.6, we can conclude that the optimal solution {\u00b5\u22171, \u00b5\u22172} satisfies the following equations:\n\u2202\u03c6 (2) X ({\u00b51, \u00b5\u22172})\n\u2202\u00b51 (\u00b5\u22171) = 0 ,\n\u2202\u03c6 (2) X ({\u00b5\u22171, \u00b52})\n\u2202\u00b52 (\u00b5\u22172) = 0\n3 \u2265 \u00b5\u22171 > 0 , 0 > \u00b5\u22172 \u2265 \u22123.\nOne can check that the only pair of real values satisfying all of the equations above are the two real roots of the polynomial\ng(x) = 3x12 + 84x10 + 490x8 \u2212 292x6 \u2212 8981x4 \u2212 17640x2 \u2212 11664.\nThe interested reader can reproduce this result using the CAS MapleTM1 and the worksheet provided in Section 7.7.\nNote, that the roots of the polynomial\nh(x) = 3x6 + 84x5 + 490x4 \u2212 292x3 \u2212 8981x2 \u2212 17640x\u2212 11664\nare the square roots of the roots of g. By using the following well-known results from algebra, we can show that the roots of h, and hence also the roots of g, are not solvable by radicals over Q.\n1 Maple is a trademark of Waterloo Maple Inc.\nDefinition 8. We call a prime p good for a polynomial f \u2208 Q[x] if p does not divide the discriminant of f .\nLemma 14 ([15]). Let f \u2208 Q[x] with deg(f) = n > 2 and deg(f) = 0 mod 2. If there are good primes p1, p2, p3 for f such that\n1. f mod p1 is an irreducible polynomial of degree n, 2. f mod p2 factors into a linear polynomial and an irreducible polynomial of degree n\u2212 1, and 3. f mod p3 factors into a linear polynomial, an irreducible polynomial of degree 2 and an irre-\nducible polynomial of degree n\u2212 3,\nthen Gal(f) \u223c= Sn.\nLemma 15 ([25]). Let f \u2208 Q[x]. If the equation f(x) = 0 is solvable by radicals over Q, then the Galois group of f is a solvable group.\nLemma 16 ([25]). The symmetric group Sn is not solvable for n \u2265 5 .\nCorollary 4. The equation h(x) = 0 is not solvable by radicals over Q.\nProof. Since the discriminant of h is D(h) = 231 \u00b737 \u00b752 \u00b773 \u00b776637866514129, we can conclude that 11, 17, and 89 are good primes for h. We factor h modulo these good primes\nh = 3 \u00b7 (x6 + 6x5 + 2x4 + 9x3 + 2x2 + 5x+ 6) mod 11 h = 3 \u00b7 (x5 + 3x4 + 9x3 + 12x2 + 10x+ 7) \u00b7 (x+ 8) mod 17 h = 3 \u00b7 (x3 + 17x2 + 50x+ 17) \u00b7 (x2 + 9x+ 27) \u00b7 (x + 2) mod 89.\nFrom Lemma 14 we obtain Gal(h) \u223c= S6. Applying Lemmata 15 and 16 yields the claim.\n7.7 MapleTMWorksheet\nThe following worksheet was developed using MapleTM 13.0. It can be downloaded at https://www-old.cs.uni-paderb In the worksheet we use the following formulation of the fuzzy 2-means objective function with m = 2.\nObservation 3 For all {\u00b5k}k \u2282 RD and X = {xn}n\u2208[N ] \u2282 RD with xn 6= \u00b5k for all k \u2208 [K] and n \u2208 [N ], we have\n\u03c6 (m) X ({\u00b5k}k) =\nN\u2211\nn=1\nK\u2211\nk=1\n(\n\u2016xn \u2212 \u00b5k\u2016\u221222 \u2211K\nl=1 \u2016xn \u2212 \u00b5l\u2016 \u22122 2\n)2\n\u2016xn \u2212 \u00b5k\u201622 = N\u2211\nn=1\n1 \u2211K\nk=1 \u2016xn \u2212 \u00b5k\u2016 \u22122 2\n,\nwhere the first equality is due to Equation 1."}, {"heading": "7.8 Arbitrarily Poor Local Minima (Proof of Observation 1)", "text": "It is known that the FM algorithm converges to a stationary point of the objective function that is either a saddlepoint or a (local) minimum [5]. We show that there are instances for which this point is arbitrarily poor compared to an optimal solution.\nClaim. Let m \u2208 N, D \u2265 2, and K = 2. Choose an arbitrary c \u2208 R. Then, there exists a point set Xa and initial point set I \u2282 Xa, |I| = 2, satisfying the following properties: If the FM algorithm is initialized with I, then in each round it computes a solution whose cost are at least c \u00b7 \u03c6OPT(X,K,m).\nProof. Consider the unweighted instances\nXa := {(a, 1), (\u2212a, 1), (\u2212a,\u22121), (a,\u22121)} \u2282 R2,\nwhere a \u2208 R with a > 1.\nClaim. Let {rnk}n,k be a solution to the fuzzy 2-means problem with respect to Xa. Then, for all xn \u2208 Xa we have rmn1 + rmn2 \u2265 ( 1 2 )m . Proof. Since rn1 + rn2 = 1, we know max{rn1, rn2} \u2265 1/2 and thus rmn1 + rmn2 \u2265 ( 1 2 )m .\nClaim. An optimal fuzzy 2-means clustering of Xa costs at least 1\n2m\u22121 and at most 4.\nProof. Observe that the means of every optimal solution lie in the convex hull of the input points. Consider an arbitrary solution {\u00b51, \u00b52} inside the rectangle spanned by Xa. There are two points in Xa for which the distance to both means is at least 1. Using Claim 7.8, we conclude that the costs of any solution can be lower bounded by 2 \u00b7\n( 1 2 )m \u00b7 1 = 12m\u22121 . Finally, observe that for \u00b51 = (\u2212a, 0), \u00b52 = (a, 0) we have \u03c6OPT(Xa,2,m) < \u2016x1 \u2212 \u00b52\u2016 2 2+\u2016x2 \u2212 \u00b52\u2016 2 2+\n\u2016x3 \u2212 \u00b52\u201622 + \u2016x4 \u2212 \u00b51\u2016 2 2 = 4.\nHowever, the FM algorithm might compute arbitrarily poor solutions, even if it is initialized with points from the point set.\nClaim. If the FM algorithm is started on Xa with {(a, 1), (a,\u22121)} as initial centers, then it computes a solution that has at least cost a 2\n2m+1\u03c6 (OPT ) (Xa,2,m) .\nProof. Let x1 = (a, 1), x2 = (\u2212a, 1), x3 = \u2212x1, x4 = \u2212x2, \u00b51 = (a, 1), and \u00b52 = (a,\u22121). First, we show that if the FM algorithm is initialized with means (\u00b5\u03031, \u00b5\u03032) that lie on a line parallel to the y-axis, then it computes means that also lie on a line parallel to the y-axis. Given (\u00b5\u03031, \u00b5\u03032), the algorithm computes memberships where r11 = r42, r21 = r32, r31 = r22 and r41 = r12. Hence,\n(\u00b5\u03031)x = rm11a\u2212 rm21a\u2212 rm31a+ rm41a\nrm11 + r m 21 + r m 31 + r m 41\n= rm42a\u2212 rm32a\u2212 rm22a+ rm12a\nrm42 + r m 32 + r m 22 + r m 12\n= (\u00b5\u03032)x .\nNext, we lower bound the cost of means {\u00b5\u03031, \u00b5\u03032} that lie on a line parallel to the y-axis. Observe that there are always at least 2 points in Xa that have distance at least a\n2 from both means. Without loss of generality, we can assume that these points are x2 and x3. Denote by {rnk}n,k the optimal responsibilities induced by {\u00b5\u03031, \u00b5\u03032}. Then,\n\u03c6({\u00b5\u03031, \u00b5\u03032} = 4\u2211\nn=1\n2\u2211\nk=1\nrmnk \u2016xn \u2212 \u00b5\u0303k\u201622 \u2265 a2 2\u2211\nk=1\nrm2k + r m 3k \u2265\na2\n2m\u22121 \u2265 a\n2\n2m+1 \u03c6 (OPT ) (Xa,2,m) ,\nwhere the second last inequality follows from Claim 7.8 and the last inequality follows from Claim 7.8.\nApplying Claim 7.8 with a := \u23082m\u221ac\u2309 yields the claim."}], "references": [{"title": "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact WellSeparated Clusters", "author": ["J.C. Dunn"], "venue": "Journal of Cybernetics 3(3)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "FCM: The fuzzy c-means clustering algorithm", "author": ["J. Bezdek", "R. Ehrlich", "W. Full"], "venue": "Computers & Geosciences 10(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "A multiresolution image segmentation technique based on pyramidal segmentation and fuzzy clustering", "author": ["M. Rezaee", "P. van der Zwet", "B. Lelieveldt", "R. van der Geest", "J. Reiber"], "venue": "Image Processing, IEEE Transactions on 9(7)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Fuzzy C-means method for clustering microarray data", "author": ["D. Demb\u00e9l\u00e9", "P. Kastner"], "venue": "Bioinformatics 19(8)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Convergence theory for fuzzy c-means: Counterexamples and repairs", "author": ["J. Bezdek", "R. Hathaway", "M. Sabin", "W. Tucker"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on 17(5)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1987}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C. Bishop"], "venue": "SpringerVerlag New York, Inc., Secaucus, NJ, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["D. Mackay"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering: (Extended Abstract)", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "Proceedings of the Tenth Annual Symposium on Computational Geometry. SCG \u201994, New York, NY, USA, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Technical report, Department of Computer Science and Engineering, University of California, San Diego", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "The Hardness of Approximation of Euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A. Sinop"], "venue": "31st Annual Symposium on Computational Geometry, SOCG\u201915.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Proceedings of the Eighteenth Annual Symposium on Computational Geometry. SCG \u201902, ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "A simple linear time (1+ \u03b5)-approximation algorithm for geometric k-means clustering in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Proceedings-Annual Symposium on Foundations of Computer Science, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Complexity of Some Common Geometric Location Problems", "author": ["N. Megiddo", "K. Supowit"], "venue": "SIAM J. Comput. 13(1)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1984}, {"title": "The Algebraic Degree of Geometric Optimization Problems", "author": ["C. Bajaj"], "venue": "Discrete Comput. Geom. 3(2)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "A contribution to convergence theory of fuzzy c-means and derivatives", "author": ["F. Hoppner", "F. Klawonn"], "venue": "Fuzzy Systems, IEEE Transactions on 11(5)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimality tests for fixed points of the fuzzy c-means algorithm", "author": ["T. Kim", "J. Bezdek", "R. Hathaway"], "venue": "Pattern Recognition 21(6)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "Local convergence of the fuzzy c-means algorithms", "author": ["R. Hathaway", "J. Bezdek"], "venue": "Pattern Recognition 19(6)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1986}, {"title": "Coresets for k-Means and k-Median Clustering and their Applications", "author": ["S. Har-peled", "S. Mazumdar"], "venue": "In Proc. 36th Annu. ACM Sympos. Theory Comput.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM J. Comput. 39(3)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering for Metric and Nonmetric Distance Measures", "author": ["M. Ackermann", "J. Bl\u00f6mer", "C. Sohler"], "venue": "ACM Trans. Algorithms 6(4)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. SODA \u201907, Society for Industrial and Applied Mathematics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "On approximate geometric k -clustering", "author": ["J. Matousek"], "venue": "Discrete & Computational Geometry 24(1)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "A theoretical and experimental comparison of the EM and SEM algorithm", "author": ["J. Bl\u00f6mer", "K. Bujna", "D. Kuntze"], "venue": "22nd International Conference on Pattern Recognition, ICPR 2014, Stockholm, Sweden, August 24-28, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Algebra", "author": ["T. Hungerford"], "venue": "Graduate Texts in Mathematics. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1974}], "referenceMentions": [{"referenceID": 0, "context": "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "In a fuzzy clustering, each data point xn belongs to each cluster, represented by a \u03bck, with a certain membership value rnk \u2208 [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 0, "context": "Given X = {(xn, wn)}n\u2208[N ] \u2282 R \u00d7 R\u22650, K \u2265 1 and m \u2265 2, find C = {\u03bck}k\u2208[K] \u2282 R and R = {rnk}n\u2208[N ],k\u2208[K] \u2282 [0, 1] minimizing", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "Our problem definition is a generalization of the original definition presented in [2] in that we consider weighted data sets.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "It is defined by the following first-order optimality conditions [5]: Fixing the means {\u03bck}k\u2208[K], optimal memberships are given by", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "For fixed K and D, there is a polynomial time algorithm solving the problem optimally [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "The K-means problem is NP-complete, even if K or D is fixed to 2 [9] [10].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "Furthermore, assuming P 6=NP, there is no PTAS for the K-means problem for arbitrary K and D [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + \u01eb)-approximation algorithm with runtime polynomial in N and D [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + \u01eb)-approximation algorithm with runtime polynomial in N and D [13].", "startOffset": 203, "endOffset": 207}, {"referenceID": 12, "context": "Just as the K-means problem, the K-median problem is NP-hard, even for D = 2 [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Even in the plane, optimal solutions of the 1-median problem are in general not expressable by radicals over Q [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.", "startOffset": 17, "endOffset": 20}, {"referenceID": 14, "context": "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "started sufficiently close to a minimizer, the iteration sequence converges to that particular minimizer [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "This result is an application of the technique used by Bajaj [15] who proved the same result for the K-median problem.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "A more detailed discussion of the implications of unsolvability by radicals can be found in [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "That is, for any given \u01eb \u2208 [0, 1], our algorithm computes an (1+ \u01eb)-approximation to the fuzzy K-means problem in time polynomial in the number of points N and dimension D.", "startOffset": 27, "endOffset": 33}, {"referenceID": 17, "context": "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "Observe that the running time basically coincides with the running time of an algorithm that applies the superset sampling technique to the K-means problem [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "We use this result when transfering the ideas behind the coreset construction of [19] in order to obtain a candidate set of means.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Let X = {(xn, wn)}n\u2208[N ] \u2282 R \u00d7 R, m \u2208 N, K \u2208 N, and \u01eb \u2208 [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 7, "context": "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means \u03bc(Ck) well.", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means \u03bc(Ck) well.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "The superset sampling technique introduced by [8] [13] can be used to find such means.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "The superset sampling technique introduced by [8] [13] can be used to find such means.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "2 Candidate Set Search for Mean Vectors (Proof of Theorem 3) Using ideas behind the coreset construction of [19], we can construct a candidate set of mean vectors.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The idea behind the coreset construction of [19] can be used to construct a candidate set of mean vectors.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "To this end, we use the deterministic algorithm presented in [23], which requires time O ( N(log(N))K\u01eb\u22122K D ) .", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "by using the results from [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "used in the proof of Theorem 2 in [8]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Let \u01eb \u2208 [0, 1], c > 1, and m \u2208 N.", "startOffset": 8, "endOffset": 14}, {"referenceID": 7, "context": "If we assume that C contains at least a constant fraction of the points of X , then this problem can be solved via the superset sampling technique [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 19, "context": "Formally, using [21], we directly obtain the following lemma.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Lemma 11 (Superset Sampling [21]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Upper Bound on the Size of G In the following, we upper bound |G| analogously to [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Lemma 14 ([15]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Lemma 15 ([25]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Lemma 16 ([25]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "8 Arbitrarily Poor Local Minima (Proof of Observation 1) It is known that the FM algorithm converges to a stationary point of the objective function that is either a saddlepoint or a (local) minimum [5].", "startOffset": 199, "endOffset": 202}], "year": 2015, "abstractText": "The fuzzy K-means problem is a generalization of the classical K-means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although popular in practice, prior to this work the fuzzy K-means problem has not been studied from a complexity theoretic or algorithmic perspective. We show that optimal solutions for fuzzy K-means cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for very simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first (1+ \u01eb)-approximation algorithms for the fuzzy K-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in N and linear in the dimension D of the input set, given that K is constant, i.e. a polynomial time approximation algorithm given a fixed K. We achieve this result by showing that for each soft clustering there exists a hard clustering with comparable properties. Second, by using techniques known from coreset constructions for the K-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in N but exponential in the dimension D. We complement these results with a randomized algorithm which imposes some natural restrictions on the input set and whose runtime is comparable to some of the most efficient approximation algorithms for Kmeans, i.e. linear in the number of points and the dimension, but exponential in the number of clusters.", "creator": "LaTeX with hyperref package"}}}