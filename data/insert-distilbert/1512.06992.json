{"id": "1512.06992", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "On the Differential Privacy of Bayesian Inference", "abstract": "we study how to communicate findings of bayesian inference technologies to third parties, while preserving the strong guarantee of differential privacy. our main contributions are with four different algorithms for private bayesian inference variants on proba - bilistic graphical models. these include two mechanisms for adding delay noise to the bayesian updates, coupled either directly to the posterior boundary parameters, or to their fourier transform so as to preserve update consistency. we also utilise a recently introduced robust posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete bayesian networks ; additionally and we introduce a maximum - a - posteriori private mechanism. our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. worked examples and experiments with bayesian na { \\ \" i } ve bayes and bayesian linear regression consistently illustrate the application of our protection mechanisms.", "histories": [["v1", "Tue, 22 Dec 2015 09:22:39 GMT  (35kb,D)", "http://arxiv.org/abs/1512.06992v1", "AAAI 2016, Feb 2016, Phoenix, Arizona, United States"]], "COMMENTS": "AAAI 2016, Feb 2016, Phoenix, Arizona, United States", "reviews": [], "SUBJECTS": "cs.AI cs.CR cs.LG math.ST stat.ML stat.TH", "authors": ["zuhe zhang", "benjamin i p rubinstein", "christos dimitrakakis"], "accepted": true, "id": "1512.06992"}, "pdf": {"name": "1512.06992.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on probabilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na\u00efve Bayes and Bayesian linear regression illustrate the application of our mechanisms."}, {"heading": "1 Introduction", "text": "We consider the problem faced by a statistician B who analyses data and communicates her findings to a third party A . While B wants to learn as much as possible from the data, she doesn\u2019t want A to learn about any individual datum. This is for example the case where A is an insurance agency, the data are medical records, and B wants to convey the efficacy of drugs to the agency, without revealing the specific illnesses of individuals in the population. Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al. (2007); Zhang et al. (2014) due to the impact on individual privacy by real-world data analytics.\nIn our setting, we assume that B is using Bayesian inference to draw conclusions from observations of a system of random variables by updating a prior distribution on parameters (i.e., latent variables) to a posterior. Our goal is to release an approximation to the posterior that preserves privacy. We adopt the formalism of differential privacy to characterise how easy it is for A to discover facts about the individual data from the aggregate posterior. Releasing the posterior permits external parties to make further inferences at will. For example, a third-party pharmaceutical might use the released posterior as a prior on the efficacy of drugs, and update it with their own patient data. Or they could form a predictive posterior for classification or regression, all while preserving differential privacy of the original data.\nOur focus in this paper is Bayesian inference in probabilistic graphical models (PGMs), which are popular as a tool for modelling conditional independence assumptions. Similar to the effect on statistical and computational efficiency of non-private inference, a central tenet of this paper is that independence structure should impact privacy. Our mechanisms and theoretical bounds are the first to establish such a link between PGM graph structure and privacy.\nMain Contributions. We develop the first mechanisms for Bayesian inference on the flexible PGM framework (cf. Table 1). We propose two posterior perturbation mechanisms for networks with likelihood functions from exponential families and conjugate\nar X\niv :1\n51 2.\n06 99\n2v 1\n[ cs\n.A I]\n2 2\nD ec\n2 01\n5\npriors, that add Laplace noise Dwork et al. (2006) to posterior parameters (or their Fourier coefficients) to preserve privacy. The latter achieves stealth through consistent posterior updates. For general Bayesian networks, posteriors may be non-parametric. In this case, we explore a mechanism Dimitrakakis et al. (2014) which samples from the posterior to answer queries\u2014no additional noise is injected. We complement our study with a maximum a posteriori estimator that leverages the exponential mechanism McSherry and Talwar (2007). Our utility and privacy bounds connect privacy and graph/dependency structure, and are complemented by illustrative experiments with Bayesian na\u00efve Bayes and linear regression.\nRelated Work. Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009).\nProbabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses.\nLittle attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds.\nThis paper is the first to develop mechanisms for differential privacy under the general framework of Bayesian inference on multiple, dependent r.v.\u2019s. Our mechanisms consider graph structure and include a purely Bayesian approach that only places conditions on the prior. We show how the (stochastic) Lipschitz assumptions of Dimitrakakis et al. (2014) lift to graphs of r.v.\u2019s, and bound KL-divergence when releasing an empirical posterior based on a modified prior. While Chaudhuri, Monteleoni, and Sarwate (2011) achieve privacy in regularised Empirical Risk Minimisation through objective randomisation, we do so through conditions on priors. We develop an alternate approach that uses the additive-noise mechanism of Dwork et al. (2006) to perturb posterior parameterisations; and we apply techniques due to Barak et al. (2007), who released marginal tables that maintain consistency in addition to privacy, by adding Laplace noise in the Fourier domain. Our motivation is novel: we wish to guarantee privacy against omniscient attackers and stealth against unsuspecting third parties."}, {"heading": "2 Problem Setting", "text": "Consider a Bayesian statistician B estimating the parameters \u03b8 of some family of distributions F\u0398 = { p\u03b8 : \u03b8 \u2208 \u0398 } on a system of r.v.\u2019s X = {Xi : i \u2208 I }, where I is an index set, with observations denoted xi \u2208 Xi, where Xi is the sample space of Xi. B has a prior distribution1 \u03be on \u0398 reflecting her prior belief, which she updates on an observation x to obtain posterior\n\u03be(B | x) = \u222b B p\u03b8 (x) d\u03be (\u03b8)\n\u03c6(x) , \u2200B \u2208 S\u0398\nwhere \u03c6(x) , \u222b\n\u0398 p\u03b8 (x) d\u03be (\u03b8). Posterior updates are iterated over an i.i.d. dataset D \u2208 D = ( \u220f i Xi)n to \u03be(\u00b7 | D).\nB\u2019s goal is to communicate her posterior distribution to a third party A , while limiting the information revealed about the original data. From the point of view of the data provider, B is a trusted party.2 However, she may still inadvertently reveal information. We assume that A is computationally unbounded, and has knowledge of the prior \u03be and the family F\u0398. To guarantee that A can gain little additional information about D from their communication, B uses Bayesian inference to learn from the data, and a differentially-private posterior to ensure disclosure to A is carefully controlled."}, {"heading": "2.1 Probabilistic Graphical Models", "text": "Our main results focus on PGMs which model conditional independence assumptions with joint factorisation\np\u03b8(x) = \u220f i\u2208I p\u03b8 (xi | x\u03c0i) , x\u03c0i = {xj : j \u2208 \u03c0i } ,\nwhere \u03c0i are the parents of the i-th variable in a Bayesian network\u2014a directed acyclic graph with r.v.\u2019s as nodes.\nExample 1. For concreteness, we illustrate some of our mechanisms on systems of Bernoulli r.v.\u2019s Xi \u2208 {0, 1}. In that case, we represent the conditional distribution of Xi given its parents as Bernoulli with parameters \u03b8i,j \u2208 [0, 1] :\n(Xi | X\u03c0i = j) \u223c Bernoulli(\u03b8i,j) . 1Precisely, a probability measure on a \u03c3-algebra (\u0398i,S\u0398i ). 2Cryptographic tools for untrusted B do not prevent information leakage to A cf. e.g., Pagnin et al.\n(2014).\nThe choice of conjugate prior \u03be(\u03b8) = \u220f i,j \u03bei,j(\u03b8i,j) has Beta marginals with parameters \u03b1i,j , \u03b2i,j , so that:\n(\u03b8i,j | \u03b1i,j = \u03b1, \u03b2i,j = \u03b2) \u223c Beta(\u03b1, \u03b2) .\nGiven observation x, the updated posterior Beta parameters are \u03b1i,j := \u03b1i,j + xi and \u03b2i,j := \u03b2i,j + (1\u2212 xi) if x\u03c0i = j."}, {"heading": "2.2 Differential Privacy", "text": "B communicates to A by releasing information about the posterior distribution, via randomised mechanism M that maps dataset D \u2208 D to a response in set Y . Dwork et al. (2006) characterise when such a mechanism is private:\nDefinition 1 (Differential Privacy). A randomised mechanismM : D \u2192 Y is ( , \u03b4)-DP if for any neighbouring D, D\u0303 \u2208 D, and measurable B \u2286 Y:\nP[M(D) \u2208 B] \u2264 e P[M(D\u0303) \u2208 B] + \u03b4,\nwhere D = (xi)ni=1, D\u0303 = (x\u0303 i)ni=1 are neighbouring if x i 6= x\u0303i for at most one i.\nThis definition requires that neighbouring datasets induce similar response distributions. Consequently, it is impossible for A to identify the true dataset from bounded mechanism query responses. Differential privacy assumes no bounds on adversarial computation or auxiliary knowledge."}, {"heading": "3 Privacy by Posterior Perturbation", "text": "One approach to differential privacy is to use additive Laplace noise (Dwork et al., 2006). Previous work has focused on the addition of noise directly to the outputs of a non-private mechanism. We are the first to apply Laplace noise to the posterior parameter updates."}, {"heading": "3.1 Laplace Mechanism on Posterior Updates", "text": "Under the setting of Example 1, we can add Laplace noise to the posterior parameters. Algorithm 1 releases perturbed parameter updates for the Beta posteriors, calculated simply by counting. It then adds zero-mean Laplace-distributed noise to the updates \u2206\u03c9 = (\u00b7 \u00b7 \u00b7 ,\u2206\u03b1i,j ,\u2206\u03b2i,j , \u00b7 \u00b7 \u00b7 ). This is the final dependence on D. Finally, the perturbed updates \u2206\u03c9\u2032 are truncated at zero to rule out invalid Beta parameters and are upper truncated at n. This yields an upper bound on the raw updates and facilitates an application of McDiarmid\u2019s bounded-differences inequality (cf. Lemma A.1 in the Appendix) in our utility analysis. Note that this truncation only improves utility (relative to the utility pre-truncation), and does not affect privacy.\nAlgorithm 1 Laplace Mechanism on Posterior Updates 1: Input data D; graph I, {\u03c0i | i \u2208 I}; parameter > 0 2: calculate posterior updates: \u2206\u03b1i,j ,\u2206\u03b2i,j for all i \u2208 I, j \u2208 {0, 1}|\u03c0i|\n3: perturb updates: \u2206\u03b1\u2032i,j , \u2206\u03b1i,j + Lap ( 2|I| ) ,\n\u2206\u03b2\u2032i,j , \u2206\u03b2i,j + Lap ( 2|I| ) .\n4: truncate: Z(1)i,j , 1[0,n](\u2206\u03b1 \u2032 i,j), Z (2) i,j , 1[0,n](\u2206\u03b2 \u2032 i,j) 5: output Zi,j = (Z (1) i,j , Z (2) i,j )\nPrivacy. To establish differential privacy of our mechanism, we must calculate a Lipschitz condition for the vector \u2206\u03c9 called global sensitivity Dwork et al. (2006).\nLemma 1. For any neighbouring datasets D, D\u0303, the corresponding updates \u2206\u03c9,\u2206\u03c9\u0303 satisfy \u2016\u2206\u03c9 \u2212\u2206\u03c9\u0303\u20161 \u2264 2|I|.\nProof. By changing the observations of one datum, at most two counts associated with each Xi can change by 1.\nCorollary 1. Algorithm 1 preserves -differential privacy.\nProof. Based on Lemma 1, the intermediate \u2206\u03c9\u2032 preserve -differential privacy Dwork et al. (2006). Since truncation depends only on \u2206\u03c9\u2032, theZ preserves the same privacy.\nUtility on Updates. Before bounding the effect on the posterior of the Laplace mechanism, we demonstrate a utility bound on the posterior update counts.\nProposition 1. With probability at least 1 \u2212 \u03b4, for \u03b4 \u2208 (0, 1), the update counts computed by Algorithm 1 are close to the non-private counts\n\u2016\u2206\u03c9 \u2212\u2206\u03c9\u2032\u2016\u221e \u2264 2|I| ln\n( 2m\n\u03b4\n) ,\nwhere m = \u2211 i\u2208I 2 |\u03c0i|.\nThis bound states that w.h.p., none of the updates can be perturbed beyondO(|I|2/ ). This implies the same bound on the deviation between \u2206\u03c9 and the revealed truncated Z.\nUtility on Posterior. We derive our main utility bounds for Algorithm 1 in terms of posteriors, proved in the Appendix. We abuse notation, and use \u03be to refer to the prior density; its meaning will be apparent from context. Given priors \u03bei,j(\u03b8i,j) = Beta (\u03b1i,j , \u03b2i,j), the posteriors on n observations are\n\u03bei,j(\u03b8i,j |D) = Beta(\u03b1i,j + \u2206\u03b1i,j , \u03b2i,j + \u2206\u03b2i,j) .\nThe privacy-preserving posterior parametrised by the output of Algorithm 1 is \u03be\u2032i,j(\u03b8i,j |D) = Beta ( \u03b1i,j + Z (1) i,j , \u03b2i,j + Z (2) i,j ) .\nIt is natural to measure utility by the KL-divergence between the joint product posteriors \u03be(\u03b8|D) and \u03be\u2032(\u03b8|D), which is the sum of the component-wise divergences, with each having known closed form. In our analysis, the divergence is a random quantity, expressible as the sum \u2211m i,j fi,j(Zi,j), where the randomness is due to the added noise. We demonstrate this r.v. is not too big, w.h.p.\nTheorem 1. Let m = \u2211 i\u2208I 2\n|\u03c0i|. Assume that Zi,j are independent and f is a mapping from Zm to R: f(\u00b7 \u00b7 \u00b7 , zi,j , \u00b7 \u00b7 \u00b7 ) , \u2211 i,j fi,j(zi,j). Given \u03b4 > 0, we have\nP f(Z) \u2265 E(f(Z)) + \u22121\n2 \u2211 i,j ci,j ln \u03b4\n 12  \u2264 \u03b4\nwhere ci,j \u2264 (2n + 1) ln[(\u03b1i,j + n + 1) + (\u03b2i,j + n + 1)) and E(fi,j(Zi,j)] \u2264 n ln((\u03b1i,j + \u2206\u03b1i,j)(\u03b2i,j + \u2206\u03b2i,j)) = U . Moreover, when n \u2265 b = 2|I| , the bound for expectation can be refined as the following\nln[(\u03b1i,j + n+ 1)(\u03b2i,j + n+ 1)]\n( n\n2 exp\n( \u2212 n\n2|I|\n)) .\nThe loss of utility measured by KL-divergence is no more than O (mn lnn) [ 1\u2212 exp ( \u2212 n\n2|I|\n)] + \u221a \u2212O (mn lnn) ln \u03b4\nwith probability at least 1\u2212 \u03b4.\nNote thatm depends on the structure of the network: bounds are better for networks with an underlying graph having smaller average in-degree."}, {"heading": "3.2 Laplace Mechanism in the Fourier Domain", "text": "Algorithm 1 follows Kerckhoffs\u2019s Principle Kerckhoffs (1883) of \u201cno security through obscurity\u201d: differential privacy defends against a mechanism-aware attacker. However additional stealth may be required in certain circumstances. An oblivious observer will be tipped off to our privacy-preserving activities by our independent perturbations, which are likely inconsistent with one-another (e.g., noisy counts forX1, X2 and X2, X3 will say different things about X2). To achieve differential privacy and stealth, we turn to Barak et al. (2007)\u2019s study of consistent marginal contingency table release. This section presents a particularly natural application to Bayesian posterior updates.\nDenote by h \u2208 R{0,1}|I| the contingency table over r.v.\u2019s I induced by D: i.e., for each combination of variables j \u2208 {0, 1}|I|, component or cell hj is a non-negative count of the observations in D with characteristic j. Geometrically h is a real-valued\nfunction over the |I|-dimensional Boolean hypercube. Then the parameter delta\u2019s of our first mechanism correspond to cells of (|\u03c0i|+ 1)-way marginal contingency tables C\u03c0i(h) where vector \u03c0i , \u03c0i+ei and the projection/marginalisation operator is defined as (\nCj(h) ) \u03b3 , \u2211\n\u03b7:\u3008\u03b7,j\u3009=\u03b3\nh\u03b7 . (1)\nWe wish to release these statistics as before, however we will not represent them under their Euclidean coordinates but instead in the Fourier basis {f j : j \u2208 {0, 1}|I|} where\nf j\u03b3 , (\u22121)\u3008\u03b3,j\u30092\u2212|I|/2 .\nDue to this basis structure and linearity of the projection operator, any marginal contingency table must lie in the span of few projections of Fourier basis vectors Barak et al. (2007):\nTheorem 2. For any table h \u2208 R{0,1}|I| and set of variables j \u2208 {0, 1}|I|, the marginal table on j satisfies Cj(h) = \u2211 \u03b3 j \u3008f\u03b3 , h\u3009Cj(f\u03b3).\nThis states that marginal j lies in the span of only those (projected) basis vectors f\u03b3 with \u03b3 contained in j. The number of values needed to update Xi is then 2|\u03c0i|+1, potentially far less than suggested by (1). To release updates for two r.v.\u2019s i, j \u2208 I there may well be significant overlap \u3008\u03c0i, \u03c0j\u3009; we need to release once, coefficients \u3008f\u03b3 , h\u3009 for \u03b3 in the downward closure of variable neighbourhoods:\nNI , \u22c3 i\u2208I \u22c3 j \u03c0i j .\nPrivacy. By (Barak et al., 2007, Theorem 6) we can apply Laplace additive noise to release these Fourier coefficients.\nCorollary 2. For any > 0, releasing for each \u03b3 \u2208 NI the Fourier coefficient \u3008f\u03b3 , h\u3009+ Lap ( 2|NI | \u221212\u2212|I|/2 ) (and Algorithm 2) preserves -differential privacy.\nRemark 1. Since |NI | \u2264 |I|21+maxi\u2208I indeg(i), at worst we have noise scale\n|I|22+maxi indeg(i)\u2212|I|/2/ .\nThis compares favourably with Algorithm 1\u2019s noise scale provided no r.v. is child to more than half the graph. Moreover the denser the graph\u2014the more overlap between nodes\u2019 parents and the less conditional independence assumed\u2014the greater the reduction in scale. This is intuitively appealing.\nConsistency. What is gained by passing to the Fourier domain, is that the perturbed marginal tables of Corollary 2 are consistent: anything in the span of projected Fourier basis vectors corresponds to some valid contingency table on I with (possibly negative) real-valued cells Barak et al. (2007).\nAlgorithm 2 Laplace Mechanism in the Fourier Domain 1: Input data D; graph I, {\u03c0i | i \u2208 I}; prior parameters \u03b1,\u03b2 0; parameters t, > 0\n2: define contingency table h \u2208 R{0,1}|I| on D 3: define downward closure NI = \u22c3 i\u2208I \u22c3 j \u03c0i j 4: for \u03b3 \u2208 NI do 5: Fourier coefficient z\u03b3 = \u3008f\u03b3 , h\u3009+ Lap ( 2|NI | 2|I|/2\n) 6: end for 7: increment first coefficient z0 \u2190 z0 + 4t|NI | 2\n2|I|/2\n8: for i \u2208 I do 9: project marginal for Xi as hi = \u2211 \u03b3 \u03c0i z\u03b3C\n\u03c0i(f\u03b3) 10: for j \u03c0i do 11: output posterior param ( \u03b1ij + h i ei+j , \u03b2ij + h i j ) 12: end for 13: end for\nNon-negativity. So far we have described the first stage of Algorithm 2. The remainder yields stealth by guaranteeing releases that are non-negative w.h.p. We adapt an idea of Barak et al. (2007) to increase the coefficient of Fourier basis vector f0, affecting a small increment to each cell of the contingency table. While there is an exact minimal amount that would guarantee non-negativity, it is data dependent. Thus our efficient O (|NI |)-time approach is randomised.\nCorollary 3. For t > 0, adding 4t|NI |2 \u221212\u2212k/2 to f0\u2019s coefficient induces a nonnegative table w.p. \u2265 1\u2212 exp(\u2212t).\nParameter t trades off between the probability of non-negativity and the resulting (minor) loss to utility. In the rare event of negativity, re-running Algorithm 2 affords another chance of stealth at the cost of privacy budget . We could alternatively truncate to achieve validity, sacrificing stealth but not privacy.\nUtility. Analogous to Proposition 1, each perturbed marginal is close to its unperturbed version w.h.p.\nTheorem 3. For each i \u2208 I and \u03b4 \u2208 (0, 1), the perturbed tables in Algorithm 2 satisfy with probability at least 1\u2212 \u03b4:\n\u2225\u2225C\u03c0i(h)\u2212 hi\u2225\u2225 1 \u2264 4|NI | ( 2|\u03c0i| log |NI | \u03b4 + t|NI | ) .\nNote that the scaling of this bound is reasonable since the table hi involves 2|\u03c0i|+1\ncells."}, {"heading": "4 Privacy by Posterior Sampling", "text": "For general Bayesian networks, B can release samples from the posterior Dimitrakakis et al. (2014) instead of perturbed samples of the posterior\u2019s parametrisation. We now develop a calculus of building up (stochastic) Lipschitz properties of systems of r.v.\u2019s that are locally (stochastic) Lipschitz. Given smoothness of the entire network, differential privacy and utility of posterior sampling follow."}, {"heading": "4.1 (Stochastic) Lipschitz Smoothness of Networks", "text": "The distribution family {p\u03b8 : \u03b8 \u2208 \u0398} on outcome space S, equipped with pseudo metric3 \u03c1, is Lipschitz continuous if\nAssumption 1 (Lipschitz Continuity). Let d(\u00b7, \u00b7) be a metric on R. There exists L > 0 such that, for any \u03b8 \u2208 \u0398:\nd(p\u03b8(x), p\u03b8(y)) \u2264 L\u03c1(x, y),\u2200x, y \u2208 S.\nWe fix the distance function d to be the absolute log-ratio (cf. differential privacy). Consider a general Bayesian network. The following lemma shows that the individual Lipschitz continuity of the conditional likelihood at every i \u2208 I implies the global Lipschitz continuity of the network.\nLemma 2. If there exists L = (L1, \u00b7 \u00b7 \u00b7 , L|I|) \u2265 0 such that \u2200i \u2208 I, \u2200x,y \u2208 X =\u220f|I| i=1 Xi we have d(p\u03b8(xi|x\u03c0i), p\u03b8(yi|y\u03c0i)) \u2264 Li\u03c1i(xi, yi), then d(p\u03b8(x), p\u03b8(y)) \u2264 \u2016L\u2016\u221e\u03c1(x,y) where \u03c1(x,y) = \u2211|I| i=1 \u03c1i(xi, yi).\nNote that while Lipschitz continuity holds uniformly for some families e.g., the exponential distribution, this is not so for many useful distributions such as the Bernoulli. In such cases a relaxed assumption requires that the prior be concentrated on smooth regions.\nAssumption 2 (Stochastic Lipschitz Continuity). Let the set of L-Lipschitz \u03b8 be\n\u0398L ,\n{ \u03b8 \u2208 \u0398 : sup\nx,y\u2208S {d(p\u03b8(x), p\u03b8(y))\u2212 L\u03c1(x, y)} \u2264 0 } Then there exists constants c, L0 > 0 such that, \u2200L \u2265 L0: \u03be(\u0398L) \u2265 1\u2212 e\u2212cL.\nLemma 3. For the conditional likelihood at each node i \u2208 I, define the set \u0398i,L of parameters for which Lipschitz continuity holds with Lipschitz constant L. If \u2203c = (c1, \u00b7 \u00b7 \u00b7 , c|I|) such that \u2200i, L \u2265 L0, \u03be(\u0398i,L) \u2265 1 \u2212 e\u2212ciL, then \u03be(\u0398L) \u2265 1 \u2212 e\u2212c\n\u2032L, where c\u2032 = mini\u2208I ci \u2212 ln |I|/L0 when |I| \u2264 eL0 mini\u2208I ci .\nTherefore, (Dimitrakakis et al., 2015, Theorem 7) asserts differential privacy of the Bayesian network\u2019s posterior.\n3Meaning that \u03c1(x, y) = 0 does not necessarily imply x = y.\nTheorem 4. Differential privacy is satisfied using the log-ratio distance, for all B \u2208 S\u0398 and x,y \u2208 X :\n1. Under the conditions in Lemma 2:\n\u03be(B | x) \u2264 exp{2L\u03c1(x,y)}\u03be(B | y)\ni.e., the posterior \u03be is (2\u2016L\u2016\u221e, 0)-differentially private under pseudo-metric \u03c1(x,y).\n2. Under the conditions in Lemma 3, if \u03c1(x,y) \u2264 (1 \u2212 \u03b4)c uniformly for all x,y for some \u03b4 \u2208 (0, 1):\n|\u03be(B | x)\u2212 \u03be(B | y)| \u2264 \u221a M\n2 \u00b7max{\u03c1(x,y), 1}\nwhereM = ( \u03ba c + L0( 1 1\u2212e\u2212\u03c9 + 1) + lnC + ln ( e\u2212L0\u03b4c(e\u2212\u03c9(1\u2212\u03b4) \u2212 e\u2212\u03c9)\u22121 + eL0(1\u2212\u03b4)c )) C;\nconstants \u03ba = 4.91081 and \u03c9 = 1.25643; C = \u220f|I| i Ci; and\nCi = sup x\u2208X p\u03b8?i,MLE(xi | x\u03c0i)\u222b \u0398i p\u03b8i(xi | x\u03c0i)d\u03be(\u03b8i) ,\nthe ratio between the maximum and marginal likelihoods of each likelihood function. Note that M = O ((\n1 c + lnC + L0\n) C ) i.e., the posterior \u03be is ( 0, \u221a\nM 2\n) -\ndifferentially private under pseudo-metric \u221a \u03c1 for \u03c1(x,y) \u2265 1."}, {"heading": "4.2 MAP by the Exponential Mechanism", "text": "As an application of the posterior sampler, we now turn to releasing MAP point estimates via the exponential mechanism McSherry and Talwar (2007), which samples responses from a likelihood exponential in some score function. By selecting a utility function that is maximised by a target non-private mechanism, the exponential mechanism can be used to privately approximate that target with high utility. It is natural then to select as our utility u the posterior likelihood \u03be (\u00b7|D). This u is maximised by the MAP estimate.\nAlgorithm 3 Mechanism for MAP Point Estimates 1: Input data D; prior \u03be (\u00b7); appropriate smoothness parameters c, L,M > 0; param-\neters distance r > 0, privacy > 0 2: calculate posterior \u03be (\u03b8|D)\n3: set \u2206 = {\u221a Lr , if Lipschitz continuous\u221a 0.5M , if stochastic Lipschitz\n4: output \u03b8\u0302 sampled \u221d exp ( \u03be(\u03b8|D)\n2\u2206\n) \u03be (\u03b8)\nFormally, Algorithm 3, under the assumptions of Theorem 4, outputs response \u03b8 with probability proportional to exp( u(D, \u03b8)/2\u2206) times a base measure \u00b5(\u03b8). Here\nBayesian Linear Regression: Census Data\n\u2206 is a Lipschitz coefficient for u with sup-norm on responses and pseudo-metric \u03c1 on datasets as in the previous section. Providing the base measure is non-trivial in general, but for discrete finite outcome spaces can be uniform McSherry and Talwar (2007). For our mechanism to be broadly applicable, we can safely take \u00b5(\u03b8) as \u03be (\u03b8).4\nCorollary 4. Algorithm 3 preserves -differential privacy wrt pseudo-metric \u03c1 up to distance r > 0.\nProof. The sensitivity of the posterior score function corresponds to the computed \u2206 (Dimitrakakis et al., 2015, Theorem 6) under either Lipschitz assumptions. The result then follows from (McSherry and Talwar, 2007, Theorem 6).\nUtility for Algorithm 3 follows from McSherry and Talwar (2007), and states that the posterior likelihood of responses is likely to be close to that of the MAP.\nLemma 4. Let \u03b8? = max\u03b8 \u03be (\u03b8|D) with maximizer the MAP estimate, and let St = {\u03b8 \u2208 \u0398 : \u03be (\u03b8|D) > \u03b8? \u2212 t} for t > 0. Then P(Sc2t) \u2264 exp(\u2212 t)/\u03be (St)."}, {"heading": "5 Experiments", "text": "Having proposed a number of mechanisms for approximating exact Bayesian inference in the general framework of probabilistic graphical models, we now demonstrate our approaches on two simple, well-known PGMs: the (generative) na\u00efve Bayes classifier, and (discriminative) linear regression. This section, with derivations in the Appendix, illustrates how our approaches are applied, and supports our extensive theoretical results with experimental observation. We focus on the trade-off between privacy and utility (accuracy and MSE respectively), which involves the (private) posterior via a predictive posterior distribution in both case studies.\n4In particular the base measure guarantees we have a proper density function: if u(D, \u03b8) is bounded by M , then we have normalising constant \u222b \u03b8 exp( u(D, \u03b8))\u00b5(\u03b8)d\u03b8 \u2264 exp(M ) <\u221e."}, {"heading": "5.1 Bayesian Discrete Na\u00efve Bayes", "text": "An illustrative example for our mechanisms is a Bayesian na\u00efve Bayes model on Bernoulli class Y and attribute variables Xi, with full conjugate Beta priors. This PGM directly specialises the running Example 1. We synthesised data generated from a na\u00efve Bayes model, with 16 features and 1000 examples. Of these we trained our mechanisms on only 50 examples, with uniform Beta priors. We formed predictive posteriors for Y |X from which we thresholded at 0.5 to make classification predictions on the remaining, unseen test data so as to evaluate classification accuracy. The results are reported in Figure 1, where average performance is taken over 100 repeats to account for randomness in train/test split, and randomised mechanisms.\nThe small size of this data represents a challenge in our setting, since privacy is more difficult to preserve under smaller samples Dwork et al. (2006). As expected, privacy incurs a sacrifice to accuracy for all private mechanisms.\nFor both Laplace mechanisms that perturb posterior updates, note that the dBoolean attributes and class label (being sole parent to each) yields nodes |I| = d+1 and downward closure size |NI | = 2d+ 2. Following our generic mechanisms, the noise added to sufficient statistics is independent on training set size, and is similar in scale. t was set for the Fourier approach, so that stealth was achieved 90% of the time\u2014those times that contributed to the plot. Due to the small increments to cell counts for Fourier, necessary to achieve its additional stealth property, we expect a small decrease to utility which is borne out in Figure 1.\nFor the posterior sampler mechanism, while we can apply Assumption 2 to a BernoulliBeta pair to obtain a generalised form of ( , \u03b4)-differential privacy, we wish to compare with our -differentially-private mechanisms and so choose a route which satisfies Assumption 1 as detailed in the Appendix. We trim the posterior before sampling, so that probabilities are lower-bounded. Figure 1 demonstrates that for small , the minimal probability at which to trim is relatively large resulting in a poor approximate posterior. But past a certain threshold, the posterior sampler eventually outperforms the other private mechanisms."}, {"heading": "5.2 Bayesian Linear Regression", "text": "We next explore a system of continuous r.v.\u2019s in Bayesian linear regression, for which our posterior sampler is most appropriate. We model label Y as i.i.d. Gaussian with known-variance and mean a linear function of features, and the linear weights endowed with multivariate Gaussian prior with zero mean and spherical covariance. To satisfy Assumption 1 we conservatively truncate the Gaussian prior (cf. the Appendix), and sample from the resulting truncated posterior; form a predictive posterior; then compute mean squared error. To evaluate our approach we used the U.S. census records dataset from the Integrated Public Use Microdata Series Minnesota Population Center (2009) with 370k records and 14 demographic features. To predict Annual Income, we train on 10% data with the remainder for testing. Figure 2 displays MSE under varying prior precision b (inverse of covariance) and weights with bounded norm 10/ \u221a b (chosen conservatively). As expected, more concentrated prior (larger b) leads to worse MSE for both mechanisms, as stronger priors reduce data influence. Compared with linear\nregression, private regression suffers only slightly worse MSE. At the same time the posterior sampler enjoys increasing privacy (that is proportional to the bounded norm as given in the Appendix)."}, {"heading": "6 Conclusions", "text": "We have presented a suite of mechanisms for differentially-private inference in graphical models, in a Bayesian framework. The first two perturb posterior parameters to achieve privacy. This can be achieved either by performing perturbations in the original parameter domain, or in the frequency domain via a Fourier transform. Our third mechanism relies on the choice of a prior, in combination with posterior sampling. We complement our mechanisms for releasing the posterior, with private MAP point estimators. Throughout we have proved utility and privacy bounds for our mechanisms, which in most cases depend on the graph structure of the Bayesian network: naturally, conditional independence affects privacy. We support our new mechanisms and analysis with applications to two concrete models, with experiments exploring the privacyutility trade-off.\nAcknowledgements. This work was partially supported by the Swiss National Foundation grant \u201cSwiss Sense Synergy\u201d CRSII2-154458."}, {"heading": "A Proofs for Laplace Mechanism on Posterior Updates", "text": "A.1 Proof of Proposition 1 Let us denote the event of a Laplace sample exceeding z > 0 in absolute value as Ak, k \u2208 1, \u00b7 \u00b7 \u00b7 , 2m. Consider the probability of an event that none of the 2m i.i.d. Laplace noise we add to each count exceed z > 0 in absolute value:\n1\u2212 P[\u222a2mk=1{Ak}] \u2265 1\u2212 2m\u2211 k=1 P[Ai]\n= 1\u2212 2m exp(\u2212z /2|I|)).\nTo make sure this probability is no smaller than 1 \u2212 \u03b4, we need z to be at most to 2|I| ln(\n2m \u03b4 ).\nA.2 Proof of Theorem 1 Lemma A.1. (McDiarmid\u2019s inequality) Suppose that random variablesZ1, \u00b7 \u00b7 \u00b7 ,Zm \u2208 Z are independent, f is a mapping from Zm to R. For z1, \u00b7 \u00b7 \u00b7 , zn, z\u2032k \u2208 Z , if f satisfies |f(z1, ...,zm)\u2212 f(z1, ...,zk\u22121, z\u2032k, zk+1, ..., zm)| \u2264 ck Then\nPr[f(z1, ...,zm)\u2212 Ef \u2265 t] \u2264 exp ( \u22122t2\u2211 k c 2 k )\nTo prove Theorem 1, we need the following statements.\nLemma A.2. For constants t \u2265 0 and a, (a+ t) ln(a+ t)\u2212 a ln a \u2264 t ln(a+ t) + t.\nProof. This follows from applying the mean value theorem to the function x ln(x) on the interval [a, a+ t].\nWe need to assume that \u03b1i,j and \u03b2i,j are larger than the only turning point of the \u0393 function which is between 1 and 2; \u03b1i,j , \u03b2i,j \u2265 2 is sufficient.5\nLemma A.3. For Zi,j \u2208 Z ,\nfi,j(Zi,j) \u2264 \u2206\u03b1i,j ln(\u03b1i,j + \u2206\u03b1i,j) + \u2206\u03b2i,j ln(\u03b2i,j + \u2206\u03b2i,j)\n\u2212 Z(1)i,j ln(\u03b1i,j + \u2206\u03b1i,j \u2212 1)\n\u2212 Z(2)i,j ln(\u03b2i,j + \u2206\u03b2i,j \u2212 1) .\nProof. By monotonicity of the \u0393 function,\nln\n( \u0393(\u03b1ij + \u2206\u03b1ij)\n\u0393(\u03b1ij + Zij) ) \u2264 ln ( \u0393(\u03b1ij + \u2206\u03b1ij)\n\u0393(\u03b1ij) ) \u2264 ln ( \u0393(\u03b1ij) \u220f\u2206\u03b1ij\u22121 r=0 (\u03b1ij + r)\n\u0393(\u03b1ij)\n)\n= \u2206\u03b1ij\u22121\u2211 r=0 ln(\u03b1ij + r) .\n5To cover more priors, we could assume that \u03b1i,j is bounded away from zero, and that \u0393 at this parameter is maximum below 2 and proceed from there for the second case.\nand by inequalities ln(x\u2212 1) \u2264 \u03c8(x) \u2264 ln(x), we have\nfij(Zij) \u2264 \u2206\u03b1ij\u22121\u2211 r=0 ln(\u03b1ij + r) + \u2206\u03b2ij\u22121\u2211 r=0 ln(\u03b2ij + r)\n+ (\u2206\u03b1ij \u2212 Z(1)ij )\u03c8(\u03b1ij + \u2206\u03b1ij)\n+ (\u2206\u03b2ij \u2212 Z(2)ij )\u03c8(\u03b2ij + \u2206\u03b2ij) \u2264 (\u03b1ij + 2\u2206\u03b1ij) ln(\u03b1ij + \u2206\u03b1ij) + (\u03b2ij + 2\u2206\u03b2ij) ln(\u03b2ij + \u2206\u03b2ij)\n\u2212 \u03b1ij ln\u03b1ij \u2212 \u03b2ij ln\u03b2ij \u2212\u2206\u03b1ij \u2212\u2206\u03b2ij \u2212 Z(1) ln(\u03b1ij + \u2206\u03b1ij \u2212 1)\n\u2212 Z(2)ij ln(\u03b2ij + \u2206\u03b2ij \u2212 1) \u2264 \u2206\u03b1ij ln(\u03b1ij + \u2206\u03b1ij) + \u2206\u03b2ij ln(\u03b2ij + \u2206\u03b2ij)\n\u2212 Z(1)ij ln(\u03b1ij + \u2206\u03b1ij \u2212 1)\n\u2212 Z(2)ij ln(\u03b2ij + \u2206\u03b2ij \u2212 1)\nThe last inequality follows from\n\u2206\u03b1ij\u22121\u2211 r=0 ln(\u03b1ij + r) < \u222b \u2206\u03b1ij 0 ln(\u03b1ij + x)dx\n= (\u03b1ij + \u2206\u03b1ij) ln(\u03b1ij + \u2206\u03b1ij)\n\u2212\u03b1ij ln\u03b1ij \u2212\u2206\u03b1ij .\nLemma A.4. For zi,j , z\u2032i,j \u2208 Z , f satisfies\n|fi,j(zi,j)\u2212 fi,j(z\u2032i,j)| \u2264 (2n+ 1)[ln(\u03b1i,j + n+ 1) + (\u03b2i,j + n+ 1)]\nProof. \u2223\u2223fij(zij)\u2212 fij(z\u2032ij)\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223ln \u0393 ( \u03b1ij + z \u2032(1) ij ) \u0393 ( \u03b1ij + z (1) ij ) + ln \u0393 ( \u03b2ij + z \u2032(2) ij ) \u0393 ( \u03b2ij + z (2) ij\n) + ( z \u2032(1) ij \u2212 z (1) ij ) \u03c8(\u03b1ij + \u2206\u03b1ij)\n+ ( z \u2032(2) ij \u2212 z (2) ij ) \u03c8(\u03b2ij + \u2206\u03b2ij) \u2223\u2223\u2223\u2223\u2223 \u2264 ln \u0393(\u03b1ij + n)\n\u0393(\u03b1ij) + ln\n\u0393(\u03b2ij + n)\n\u0393(\u03b2ij)\n+ n ln(\u03b1ij + \u2206\u03b1ij) + n ln(\u03b2ij + \u2206\u03b2ij) \u2264 n\u2211 r=0 ln(\u03b1ij + r) + n\u2211 r=0 ln(\u03b2ij + r)\n+ n ln(\u03b1ij + \u2206\u03b1ij) + n ln(\u03b2ij + \u2206\u03b2ij)\n\u2264(\u03b1ij + n+ 1) ln(\u03b1ij + n+ 1)\u2212 \u03b1ij ln\u03b1ij + (\u03b2ij + n+ 1) ln(\u03b2ij + n+ 1)\u2212 \u03b2ij ln\u03b2ij \u2212 2n\u2212 2 + n (ln(\u03b1ij + n) + ln(\u03b2ij + n))\n\u2264(2n+ 1) (ln(\u03b1ij + n+ 1) + ln(\u03b2ij + n+ 1))\nThe distribution ofZ(1)ij is given by,  P(Z(1)ij = 0) = P(Y + \u2206\u03b1ij \u2264 0) P(Z(1)ij = n) = P(Y + \u2206\u03b1ij \u2265 n) P(Z(1)ij \u2264 z) = P(Y \u2264 z \u2212\u2206\u03b1ij) z \u2208 (0, n) Then we have\nE ( Z\n(1) ij ) =\n\u222b n 0 [1\u2212 FY (z \u2212\u2206\u03b1ij)]dz + nP(Y + \u2206\u03b1ij \u2265 n)\n= \u222b \u2206\u03b1ij 0 [ 1\u2212 1 2 exp ( z \u2212\u2206\u03b1ij b )] dz\n+ \u222b n \u2206\u03b1ij 1 2 exp ( \u2206\u03b1ij \u2212 z b ) dz + n 2 exp ( \u2206\u03b1ij \u2212 n b ) =\u2206\u03b1ij + b\n2 exp\n( \u2212\u2206\u03b1ij\nb\n) + n\u2212 b\n2 exp\n( \u2206\u03b1ij \u2212 n\nb ) By the same argument, the expectation of Z(2)ij is given by \u2206\u03b2ij + b 2 exp ( \u2212\u2206\u03b2ijb ) +\nn\u2212b 2 exp\n( \u2206\u03b2ij\u2212n\nb\n) .\nBy plugging E ( Z\n(1) i,j\n) and E ( Z\n(2) i,j\n) in Lemma A.3 with have\nE(fij(Zij)) \u2264 (\u03b1ij + 2\u2206\u03b1ij) ln(\u03b1ij + \u2206\u03b1ij) + (\u03b2ij + 2\u2206\u03b2ij) ln(\u03b2ij + \u2206\u03b2ij)\n\u2212 \u03b1ij ln\u03b1ij \u2212 \u03b2ij ln\u03b2ij \u2212\u2206\u03b1ij \u2212\u2206\u03b2ij \u2212 ln(\u03b1ij + \u2206\u03b1ij \u2212 1)E ( Z (1) ij ) \u2212 ln(\u03b2ij + \u2206\u03b2ij \u2212 1)E ( Z (2) ij\n) \u2264 n ln[(\u03b1ij + \u2206\u03b1ij)(\u03b2ij + \u2206\u03b2ij)].\nWhen n \u2265 b, this can be refined as O (n lnn) [ 1\u2212 exp ( \u2212 n\n2|I| )] since E(Z(1)i,j ) and E(Z (2) i,j ) are lower bounded by n 2 exp(\u2212 n b ). Therefore, by McDiarmid\u2019s difference inequality we have\nP \u2211 i,j fi,j \u2212 \u2211 i,j E(fi,j) \u2265 \u221a \u22121 2 ln \u03b4 \u2211 i,j ci,j  \u2264 \u03b4, where ci,j is the RHS in Lemma A.4.\nA.3 Proof of Theorem 3 We follow the proof of (Barak et al., 2007, Theorem 7). If X \u223c Lap (b) then by the CDF of the Laplace P (|X| > R) = exp(\u2212R/b) whereR > 0. By the union bound for {Xj}j\u2208NI\ni.i.d.\u223c Lap (b), we have w.h.p. none is large P (\u2200j \u2208 NI , |Xj | \u2264 b log(|NI |/\u03b4)) \u2265 1\u2212 \u03b4 for \u03b4 \u2208 (0, 1). Since \u2016f j\u20161 = 2k/2 for each j \u2286 I it follows with probability at least 1\u2212\u03b4, that \u2200j \u2208 NI\\{\u2205},\n\u2225\u2225zjf j \u2212 \u3008f j , h\u3009f j\u2225\u22251 \u2264 2|NI | log |NI |\u03b4 . For f0 the additional increment comes at an additional cost of 4t|NI |2/ . Putting everything together, we note that 2|\u03c0i|+1 Fourier coefficients represent hi including f0."}, {"heading": "B Posterior Sampling", "text": "B.1 Proof of Lemma 2\nd(p\u03b8(x), p\u03b8(y)) = \u2223\u2223\u2223\u2223\u2223\u2223log |I|\u220f i=1 p\u03b8(xi|x\u03c0i) p\u03b8(yi|y\u03c0i) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264\n|I|\u2211 i=1 \u2223\u2223\u2223\u2223log p\u03b8(xi|x\u03c0i)p\u03b8(yi|y\u03c0i) \u2223\u2223\u2223\u2223\n= |I|\u2211 i=1 d(p\u03b8(xi|x\u03c0i), p\u03b8(yi|y\u03c0i)) \u2264 |I|\u2211 i=1 Li\u03c1i(xi, yi) \u2264 \u2016L\u2016\u221e\u2016\u03c1(x,y)\u20161.\nB.2 Proof of Lemma 3 Define\n\u0398i,L =\n{ \u03b8 \u2208 \u0398 : sup\nx,y\u2208X {d(p\u03b8(xi|x\u03c0i), p\u03b8(yi|y\u03c0i))\n\u2212 L\u03c1i(xi, yi)} \u2264 0 } .\nBy taking \u03c1(x, y) = \u2211 i \u03c1i(xi, yi), we have\n|I|\u22c2 i=1 \u0398\u0303i,L\n= { \u03b8 \u2208 \u0398 : sup\nxi,yi\u2208Xi {d(p\u03b8(xi|x\u03c0i), p\u03b8(yi|y\u03c0i))\n\u2264 L\u03c1i(xi, yi)},\u2200i \u2208 I }\n\u2286 \u03b8 \u2208 \u0398 : supxi,yi\u2208Xi  |I|\u2211 i=1 d(p\u03b8(xi|x\u03c0i), p\u03b8(yi|y\u03c0i))\n\u2264 L |I|\u2211 i=1 \u03c1i(xi, yi)  \n\u2286{\u03b8 \u2208 \u0398 : sup{d(p\u03b8(x), p\u03b8(y))\u2212 L\u03c1(x, y)} \u2264 0} =\u0398L\nTherefore, we have that the set of \u03b8 \u2208 \u0398 satisfying the Stochastic Lipschitz continuity for conditional likelihood of every i \u2208 I in the Bayesian network is a subset of the set of \u03b8 satisfying the global Stochastic Lipschitz continuity for same L.\nNote that ( \u22c2|I| i=1 \u0398i,L) c = \u22c3|I| i=1(\u0398i,L)\nc and \u03be((\u0398i,L)c) = 1 \u2212 \u03be(\u0398i,L) \u2264 e\u2212ciL. Then we have\n\u03be[(\u2229|I|i=1\u0398i,L) c] \u2264 |I|\u2211 i=1 \u03be(\u0398i,L) c) \u2264 |I|\u2211 i=1 e\u2212ciL.\nTherefore, we have\n\u03be(\u0398L) \u2265 \u03be(\u2229|I|i=1\u0398i,L) \u2265 1\u2212 |I|\u2211 i=1 e\u2212ciL \u2265 1\u2212Ne\u2212mini ciL.\nTake c\u2032 = ln |I|min{ci}|I|i=1, we have \u03be(\u0398L) \u2265 1\u2212 e\u2212c \u2032L."}, {"heading": "C Bayesian Na\u00efve Bayes", "text": "We review the derivation of the na\u00efve Bayes predictive posterior for two cases applied in our experiments.\nC.1 Closed-Form Beta-Bernoulli When the r.v.\u2019s are all Bernoulli\u2019s with Beta conjugate priors:\nP(Y = y|X = x) \u221d \u222b\n\u0398\np\u03b8 (y) d\u220f i=1 p\u03b8 (xi|y) \u03be (\u03b8) d\u03b8.\nThe integral decouples into the product of (where \u03b1, \u03b2 refer to the y posterior)\u222b \u0398 p\u03b8 (y) \u03be (\u03b8) d\u03b8\n= \u222b 1 0 \u03b8\u03b1+y\u22121(1\u2212 \u03b8)\u03b2+(1\u2212y)\u22121 B(\u03b1, \u03b2) d\u03b8 = B(\u03b1+ y, \u03b2 + 1\u2212 y)\nB(\u03b1, \u03b2) \u00d7 \u222b 1\n0\n\u03b8\u03b1+y\u22121(1\u2212 \u03b8)\u03b2+(1\u2212y)\u22121\nB(\u03b1+ y, \u03b2 + 1\u2212 y) d\u03b8\n= B(\u03b1+ y, \u03b2 + 1\u2212 y)\nB(\u03b1, \u03b2)\n= \u0393(\u03b1+ y)\u0393(\u03b2 + 1\u2212 y)\n\u0393(\u03b1+ \u03b2 + 1)\n\u0393(\u03b1+ \u03b2)\n\u0393(\u03b1)\u0393(\u03b2)\n= \u03b1y\u03b21\u2212y\n\u03b1+ \u03b2 .\nand terms (where \u03b1, \u03b2 refer to the xi | y posterior)\u222b 1 0 \u03b8\u03b1+xi\u22121(1\u2212 \u03b8)\u03b2+(1\u2212xi)\u22121 B(\u03b1, \u03b2) d\u03b8\n= \u03b1xi\u03b21\u2212xi\n\u03b1+ \u03b2 ,\ncomputed in the same way.\nC.2 Sampling Given an empirical CDF sampled from our posterior sampling mechanism, we can approximate by posterior sampling:\n\u2022 Repeat many times for both y = 0, y = 1:\n\u2013 Sample \u03b8\u0302y, \u03b8\u0302x1,y, . . . , \u03b8\u0302xd,y \u2013 Plug-in the sampled parameters and fixed r.v.\u2019s into the product of densities\nto obtain an unnormalised probability estimate\n\u2022 Average the obtained estimates, for each y = 0, y = 1\n\u2022 Normalise\nWe modify the above slightly so that we sample from a truncated posterior. This allows us to assume a minimal probability \u03c9 assigned to any sub-event in the na\u00efve Bayes network, so that the joint distribution satisfies Assumption 1. Trivially in particular this yields a differential privacy level given by = 2 log(1/\u03c9). Given a desired privacy budget we can therefore select \u03c9 = exp(\u2212 /2). We then simply rejection sample when sampling above, to obtain samples from the truncated posterior. This is the posterior sampler algorithm used in the na\u00efve Bayes experiments."}, {"heading": "D Bayesian Linear Regression", "text": "Let us denote a set of observationsD = {(x1, y1), . . . , (xn, yn)}where xi = (x(1)i , . . . , x (d) i ) \u2208 Rd, yi \u2208 R. In the model we assume that Yi are independent given xw. Recall that a normal linear regression model with i.i.d Gaussian noise is given as follows,\nyi = d\u2211 j=1 x (j) i w (j) + i, i \u223c N(0, \u03c32).\nThe normal likelihood function, as a product of likelihoods for each of the individual components of y = (y1, . . . , yn), is given by\npw(y|x,w;\u03c32) = 1 (2\u03c0\u03c32)n/2 e\u2212 1 2\u03c32 (y\u2212xw)T (y\u2212xw).\nGiven observations D, we are interested in computing the sensitivity (in terms of data/observation) of this likelihood, that is supw,D,D\u2032 | ln pw(D) pw(D\u2032)\n|. Note that ln pw(D)pw(D\u2032) = ln \u220f i pw(xi,yi)\u220f i pw(x \u2032 i,y \u2032 i) = \u2211 i ln pw(xi,yi) pw(x\u2032i,y \u2032 i)\n. For simplicity, assume that the precision of Y is 1. let fw(D) denote the log-\nlikelihood, we have |fw(D)\u2212 fw(D\u2032)| \u2264 \u2211 i |fw(xi, yi)\u2212 fw(x\u2032i, y\u2032i).|\nNote that by mean value theorem, we have\nfw(xi, yi)\u2212 fw(x\u2032i, y\u2032i) = \u2207fw((1\u2212 c)(x(1)i , . . . , x (d) i , yi)\n+c(x \u2032(1) i , . . . , x \u2032(d) i , y \u2032 i)) \u00b7(x(1)i \u2212 x \u2032(1) i , . . . , x (d) i \u2212 x \u2032(d) i , yi \u2212 y \u2032 i)\nTherefore by Cauchy-Schwarz inequality we have:\n\u2206fw(xi, yi) \u2264 ||\u2207fw||2||(\u2206xi,\u2206yi)||2\n\u2206fw(D,D \u2032) \u2264 n\u2211 i=1 \u2206fw(xi, yi)\n\u2264 ||\u2207fw||2 n\u2211 i=1 ||(\u2206xi,\u2206yi)||2\nNote that dfw(xi, yi)/dx (j) i = 1\n2\u03c32 (x\n(j) i w Tw \u2212 yiw(j))\ndfw(xi, yi)/dyi = 1\n2\u03c32 (yi \u2212 wTxTi )\nRecall that in linear regression, it is common to assume that every tuple (xi, yi) in the database satisfies ||xi||2 \u2264 1 and ||yi||2 \u2264 1, we have\n||\u2207fw||2\n\u2264 1 2\u03c32 n\u2211 i=1 yi \u2212 wTxTi + d\u2211 j=1 ( x (j) i ||w||2 \u2212 yiw (j) )\n\u2264 n 2\u03c32 (1 + 2||w||1 + d||w||2) \u2264 n 2\u03c32 (1 + (d+ 2)||w||1)\nHence the log-likelihood of normal regression satisfies Assumption 1 for \u03c1(D,D\u2032) =\u2211n i=1 ||(\u2206xi,\u2206yi)||2 under the condition that w is bounded.\nFor normal linear regression with bounded w, it is natural to choose a prior of w with truncated normal density, that is\np(w) \u221d N(0,\u039b\u22121)1{||w||2 \u2264 1}\n(In experiments we vary the norm bound for truncation with \u039b. Our argument extends immediately.) As we show below, this truncated normal prior is still a conjugate prior for Gaussian likelihood.\nLemma D.1. The truncated Gaussian prior and the Gaussian likelihood of linear regression is a conjugate pair and the resulted posterior is a truncated Gaussian distribution.\nProof. By Bayes\u2019s rule,\np(w|D) \u221d p(D|w)p(w) \u221d N(w|\u00b5n,\u03a3n)1{||w||2 \u2264 1}\nwhere \u00b5n = (XTX + \u03c32\u039b)\u22121XT y and \u03a3n = \u03c32(XTX + \u03c32\u039b)\u22121.\nTherefore the posterior BAPS (Bayesian Posterior Sampling) on p(w|D) is 2L(w)differentially private, where L(w) = n2\u03c32 (1 + 2||w||1 + d||w||2)."}], "references": [{"title": "Privacy, accuracy, and consistency too: A holistic solution to contingency table release", "author": ["B. Barak", "K. Chaudhuri", "C. Dwork", "S. Kale", "F. McSherry", "K. Talwar"], "venue": "PODS \u201907, 273\u2013282.", "citeRegEx": "Barak et al\\.,? 2007", "shortCiteRegEx": "Barak et al\\.", "year": 2007}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["K. Chaudhuri", "D. Hsu"], "venue": "ICML\u201912.", "citeRegEx": "Chaudhuri and Hsu,? 2012", "shortCiteRegEx": "Chaudhuri and Hsu", "year": 2012}, {"title": "Privacy-preserving logistic regression", "author": ["K. Chaudhuri", "C. Monteleoni"], "venue": "NIPS\u201908, 289\u2013296.", "citeRegEx": "Chaudhuri and Monteleoni,? 2008", "shortCiteRegEx": "Chaudhuri and Monteleoni", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research 12(Mar):1069\u20131109.", "citeRegEx": "Chaudhuri et al\\.,? 2011", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["K. Chaudhuri", "A. Sarwate", "K. Sinha"], "venue": "NIPS\u201912, 989\u2013997.", "citeRegEx": "Chaudhuri et al\\.,? 2012", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Robust and private Bayesian inference", "author": ["C. Dimitrakakis", "B. Nelson", "A. Mitrokotsa", "B. Rubinstein"], "venue": "ALT\u201914.", "citeRegEx": "Dimitrakakis et al\\.,? 2014", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2014}, {"title": "Differential privacy in a Bayesian setting through posterior sampling", "author": ["C. Dimitrakakis", "B. Nelson", "Z. Zhang", "A. Mitrokotsa", "B. Rubinstein"], "venue": "Technical Report 1306.1066, arXiv.", "citeRegEx": "Dimitrakakis et al\\.,? 2015", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2015}, {"title": "Local privacy and statistical minimax rates", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "Technical Report 1302.3203, arXiv.", "citeRegEx": "Duchi et al\\.,? 2013", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Differential privacy for statistics: What we know and what we want to learn", "author": ["C. Dwork", "A. Smith"], "venue": "Journal of Privacy and Confidentiality 1(2):135\u2013154.", "citeRegEx": "Dwork and Smith,? 2009", "shortCiteRegEx": "Dwork and Smith", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "TCC\u201906, 265\u2013284.", "citeRegEx": "Dwork et al\\.,? 2006", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "A practical differentially private random decision tree classifier", "author": ["G. Jagannathan", "K. Pillaipakkamnatt", "R. Wright"], "venue": "ICDM\u201909, 114\u2013121.", "citeRegEx": "Jagannathan et al\\.,? 2009", "shortCiteRegEx": "Jagannathan et al\\.", "year": 2009}, {"title": "La cryptographie militaire", "author": ["A. Kerckhoffs"], "venue": "Journal des sciences militaires IX:5\u2013 83 January; 161\u2013191, February.", "citeRegEx": "Kerckhoffs,? 1883", "shortCiteRegEx": "Kerckhoffs", "year": 1883}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "FOCS\u201907, 94\u2013103.", "citeRegEx": "McSherry and Talwar,? 2007", "shortCiteRegEx": "McSherry and Talwar", "year": 2007}, {"title": "Differentially-private learning and information theory", "author": ["D. Mir"], "venue": "Proceedings of the 2012 Joint EDBT/ICDT Workshops, 206\u2013210. ACM.", "citeRegEx": "Mir,? 2012", "shortCiteRegEx": "Mir", "year": 2012}, {"title": "On the leakage of information in biometric authentication", "author": ["E. Pagnin", "C. Dimitrakakis", "A. Abidin", "A. Mitrokotsa"], "venue": "Indocrypt 2014.", "citeRegEx": "Pagnin et al\\.,? 2014", "shortCiteRegEx": "Pagnin et al\\.", "year": 2014}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["B.I.P. Rubinstein", "P.L. Bartlett", "L. Huang", "N. Taft"], "venue": "Journal of Privacy and Confidentiality", "citeRegEx": "Rubinstein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2012}, {"title": "Privacy for free: Posterior sampling and stochastic gradient monte carlo", "author": ["Y.-X. Wang", "S. Fienberg", "A. Smola"], "venue": "Blei, D., and Bach, F., eds., ICML\u201915, 2493\u2013 2502.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Probabilistic inference and differential privacy", "author": ["O. Williams", "F. McSherry"], "venue": "NIPS\u201910, 2451\u20132459.", "citeRegEx": "Williams and McSherry,? 2010", "shortCiteRegEx": "Williams and McSherry", "year": 2010}, {"title": "Bayesian inference under differential privacy", "author": ["Y. Xiao", "L. Xiong"], "venue": "arXiv preprint arXiv:1203.0617.", "citeRegEx": "Xiao and Xiong,? 2012", "shortCiteRegEx": "Xiao and Xiong", "year": 2012}, {"title": "Functional mechanism: regression analysis under differential privacy", "author": ["J. Zhang", "Z. Zhang", "X. Xiao", "Y. Yang", "M. Winslett"], "venue": "Proc. VLDB Endowment 5(11):1364\u20131375.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Privbayes: Private data release via bayesian networks", "author": ["J. Zhang", "G. Cormode", "C.M. Procopiuc", "D. Srivastava", "X. Xiao"], "venue": "SIGMOD\u201914, 1423\u20131434.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "The differential privacy of Bayesian inference", "author": ["S. Zheng"], "venue": "Bachelor\u2019s thesis, Harvard College.", "citeRegEx": "Zheng,? 2015", "shortCiteRegEx": "Zheng", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al.", "startOffset": 69, "endOffset": 132}, {"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al.", "startOffset": 69, "endOffset": 185}, {"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al.", "startOffset": 69, "endOffset": 213}, {"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al. (2007); Zhang et al.", "startOffset": 239, "endOffset": 259}, {"referenceID": 0, "context": "Such requirements of privacy are of growing interest in the learning Chaudhuri and Hsu (2012); Duchi, Jordan, and Wainwright (2013), theoretical computer science Dwork and Smith (2009); McSherry and Talwar (2007) and databases communities Barak et al. (2007); Zhang et al. (2014) due to the impact on individual privacy by real-world data analytics.", "startOffset": 239, "endOffset": 280}, {"referenceID": 7, "context": "priors, that add Laplace noise Dwork et al. (2006) to posterior parameters (or their Fourier coefficients) to preserve privacy.", "startOffset": 31, "endOffset": 51}, {"referenceID": 5, "context": "In this case, we explore a mechanism Dimitrakakis et al. (2014) which samples from the posterior to answer queries\u2014no additional noise is injected.", "startOffset": 37, "endOffset": 64}, {"referenceID": 5, "context": "In this case, we explore a mechanism Dimitrakakis et al. (2014) which samples from the posterior to answer queries\u2014no additional noise is injected. We complement our study with a maximum a posteriori estimator that leverages the exponential mechanism McSherry and Talwar (2007). Our utility and privacy bounds connect privacy and graph/dependency structure, and are complemented by illustrative experiments with Bayesian na\u00efve Bayes and linear regression.", "startOffset": 37, "endOffset": 278}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al.", "startOffset": 130, "endOffset": 162}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al.", "startOffset": 130, "endOffset": 196}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al.", "startOffset": 130, "endOffset": 239}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al.", "startOffset": 130, "endOffset": 281}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009).", "startOffset": 130, "endOffset": 327}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy.", "startOffset": 130, "endOffset": 386}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy.", "startOffset": 130, "endOffset": 474}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy.", "startOffset": 130, "endOffset": 595}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses.", "startOffset": 130, "endOffset": 708}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements.", "startOffset": 130, "endOffset": 1147}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds.", "startOffset": 130, "endOffset": 1211}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds.", "startOffset": 130, "endOffset": 1307}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds.", "startOffset": 130, "endOffset": 1405}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds. This paper is the first to develop mechanisms for differential privacy under the general framework of Bayesian inference on multiple, dependent r.v.\u2019s. Our mechanisms consider graph structure and include a purely Bayesian approach that only places conditions on the prior. We show how the (stochastic) Lipschitz assumptions of Dimitrakakis et al. (2014) lift to graphs of r.", "startOffset": 130, "endOffset": 1786}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds. This paper is the first to develop mechanisms for differential privacy under the general framework of Bayesian inference on multiple, dependent r.v.\u2019s. Our mechanisms consider graph structure and include a purely Bayesian approach that only places conditions on the prior. We show how the (stochastic) Lipschitz assumptions of Dimitrakakis et al. (2014) lift to graphs of r.v.\u2019s, and bound KL-divergence when releasing an empirical posterior based on a modified prior. While Chaudhuri, Monteleoni, and Sarwate (2011) achieve privacy in regularised Empirical Risk Minimisation through objective randomisation, we do so through conditions on priors.", "startOffset": 130, "endOffset": 1949}, {"referenceID": 1, "context": "Many individual learning algorithms have been adapted to maintain differential privacy, including regularised logistic regression Chaudhuri and Monteleoni (2008), the SVM Rubinstein et al. (2012); Chaudhuri, Monteleoni, and Sarwate (2011), PCA Chaudhuri, Sarwate, and Sinha (2012), the functional mechanism Zhang et al. (2012) and trees Jagannathan, Pillaipakkamnatt, and Wright (2009). Probabilistic graphical models have been used to preserve privacy. Zhang et al. (2014) learned a graphical model from data, in order to generate surrogate data for release; while Williams and McSherry (2010) fit a model to the response of private mechanisms to clean up output and improve accuracy. Xiao and Xiong (2012) similarly used Bayesian credible intervals to increase the utility of query responses. Little attention has been paid to private inference in the Bayesian setting. We seek to adapt Bayesian inference to preserve differential privacy when releasing posteriors. Dimitrakakis et al. (2014; 2015) introduce a differentially-private mechanism for Bayesian inference based on posterior sampling\u2014a mechanism on which we build\u2014 while Zheng (2015) considers further refinements. Wang, Fienberg, and Smola (2015) explore Monte Carlo approaches to Bayesian inference using the same mechanism, while Mir (2012) was the first to establish differential privacy of the Gibbs estimator McSherry and Talwar (2007) by minimising risk bounds. This paper is the first to develop mechanisms for differential privacy under the general framework of Bayesian inference on multiple, dependent r.v.\u2019s. Our mechanisms consider graph structure and include a purely Bayesian approach that only places conditions on the prior. We show how the (stochastic) Lipschitz assumptions of Dimitrakakis et al. (2014) lift to graphs of r.v.\u2019s, and bound KL-divergence when releasing an empirical posterior based on a modified prior. While Chaudhuri, Monteleoni, and Sarwate (2011) achieve privacy in regularised Empirical Risk Minimisation through objective randomisation, we do so through conditions on priors. We develop an alternate approach that uses the additive-noise mechanism of Dwork et al. (2006) to perturb posterior parameterisations; and we apply techniques due to Barak et al.", "startOffset": 130, "endOffset": 2175}, {"referenceID": 0, "context": "(2006) to perturb posterior parameterisations; and we apply techniques due to Barak et al. (2007), who released marginal tables that maintain consistency in addition to privacy, by adding Laplace noise in the Fourier domain.", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": "Sampler 7 (2L, 0) if Lipschitz; or (0, \u221a M/2) stochastic Lipschitz expected utility functional wrt posterior O ( \u03b7 + \u221a ln(1/\u03b4)/N ) Dimitrakakis et al. (2015)", "startOffset": 131, "endOffset": 158}, {"referenceID": 14, "context": ", Pagnin et al. (2014).", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "Dwork et al. (2006) characterise when such a mechanism is private: Definition 1 (Differential Privacy).", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "One approach to differential privacy is to use additive Laplace noise (Dwork et al., 2006).", "startOffset": 70, "endOffset": 90}, {"referenceID": 9, "context": "To establish differential privacy of our mechanism, we must calculate a Lipschitz condition for the vector \u2206\u03c9 called global sensitivity Dwork et al. (2006). Lemma 1.", "startOffset": 136, "endOffset": 156}, {"referenceID": 9, "context": "To establish differential privacy of our mechanism, we must calculate a Lipschitz condition for the vector \u2206\u03c9 called global sensitivity Dwork et al. (2006). Lemma 1. For any neighbouring datasets D, D\u0303, the corresponding updates \u2206\u03c9,\u2206\u03c9\u0303 satisfy \u2016\u2206\u03c9 \u2212\u2206\u03c9\u0303\u20161 \u2264 2|I|. Proof. By changing the observations of one datum, at most two counts associated with each Xi can change by 1. Corollary 1. Algorithm 1 preserves -differential privacy. Proof. Based on Lemma 1, the intermediate \u2206\u03c9\u2032 preserve -differential privacy Dwork et al. (2006). Since truncation depends only on \u2206\u03c9\u2032, theZ preserves the same privacy.", "startOffset": 136, "endOffset": 528}, {"referenceID": 10, "context": "2 Laplace Mechanism in the Fourier Domain Algorithm 1 follows Kerckhoffs\u2019s Principle Kerckhoffs (1883) of \u201cno security through obscurity\u201d: differential privacy defends against a mechanism-aware attacker.", "startOffset": 62, "endOffset": 103}, {"referenceID": 0, "context": "To achieve differential privacy and stealth, we turn to Barak et al. (2007)\u2019s study of consistent marginal contingency table release.", "startOffset": 56, "endOffset": 76}, {"referenceID": 0, "context": "Due to this basis structure and linearity of the projection operator, any marginal contingency table must lie in the span of few projections of Fourier basis vectors Barak et al. (2007): Theorem 2.", "startOffset": 166, "endOffset": 186}, {"referenceID": 0, "context": "What is gained by passing to the Fourier domain, is that the perturbed marginal tables of Corollary 2 are consistent: anything in the span of projected Fourier basis vectors corresponds to some valid contingency table on I with (possibly negative) real-valued cells Barak et al. (2007).", "startOffset": 266, "endOffset": 286}, {"referenceID": 0, "context": "We adapt an idea of Barak et al. (2007) to increase the coefficient of Fourier basis vector f, affecting a small increment to each cell of the contingency table.", "startOffset": 20, "endOffset": 40}, {"referenceID": 5, "context": "For general Bayesian networks, B can release samples from the posterior Dimitrakakis et al. (2014) instead of perturbed samples of the posterior\u2019s parametrisation.", "startOffset": 72, "endOffset": 99}, {"referenceID": 12, "context": "2 MAP by the Exponential Mechanism As an application of the posterior sampler, we now turn to releasing MAP point estimates via the exponential mechanism McSherry and Talwar (2007), which samples responses from a likelihood exponential in some score function.", "startOffset": 154, "endOffset": 181}, {"referenceID": 10, "context": "Providing the base measure is non-trivial in general, but for discrete finite outcome spaces can be uniform McSherry and Talwar (2007). For our mechanism to be broadly applicable, we can safely take \u03bc(\u03b8) as \u03be (\u03b8).", "startOffset": 108, "endOffset": 135}, {"referenceID": 5, "context": "The sensitivity of the posterior score function corresponds to the computed \u2206 (Dimitrakakis et al., 2015, Theorem 6) under either Lipschitz assumptions. The result then follows from (McSherry and Talwar, 2007, Theorem 6). Utility for Algorithm 3 follows from McSherry and Talwar (2007), and states that the posterior likelihood of responses is likely to be close to that of the MAP.", "startOffset": 79, "endOffset": 286}, {"referenceID": 9, "context": "The small size of this data represents a challenge in our setting, since privacy is more difficult to preserve under smaller samples Dwork et al. (2006). As expected, privacy incurs a sacrifice to accuracy for all private mechanisms.", "startOffset": 133, "endOffset": 153}], "year": 2015, "abstractText": "We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on probabilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na\u00efve Bayes and Bayesian linear regression illustrate the application of our mechanisms.", "creator": "LaTeX with hyperref package"}}}