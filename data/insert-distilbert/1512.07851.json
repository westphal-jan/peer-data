{"id": "1512.07851", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2015", "title": "Context-Based Prediction of App Usage", "abstract": "in this paper we propose a fundamentally new algorithm for dynamically predicting a set of apps that the conscious user is likely to use. each a set line of app user icons ( usually four ) are first presented on a special dock, called prediction bar, and dynamically change according to the user's habits at a carefully given time, location, and device state ( headphone connected, bluetooth active, and so on ). the goal of the algorithm is, given the context information, to actively help the user navigate to the desired app as well as to provide a personalized feeling. we propose an efficient online algorithm that is executed on the device, which is based on the passive - aggressive online algorithmic framework, adapted to maximize the auc at each round. we lastly concluded the paper with a large scale empirical study on the feedback performance of the algorithm on a subjective real users'data.", "histories": [["v1", "Thu, 24 Dec 2015 16:27:40 GMT  (473kb,D)", "https://arxiv.org/abs/1512.07851v1", null], ["v2", "Mon, 25 Jan 2016 19:39:40 GMT  (473kb,D)", "http://arxiv.org/abs/1512.07851v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joseph keshet", "adam kariv", "arnon dagan", "dvir volk", "joey simhon"], "accepted": false, "id": "1512.07851"}, "pdf": {"name": "1512.07851.pdf", "metadata": {"source": "CRF", "title": "Context-Based Prediction of App Usage", "authors": ["Joseph Keshet", "Adam Kariv", "Arnon Dagan", "Dvir Volk", "Joey Simhon"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Smartphones are one of the most widely used devices nowadays. It is estimated that today about half the adult population owns a smartphone. The average American actively uses a smartphone more than two hours every day, and nearly 80% of smartphone-owners check messages, news or other services within 15 minutes of getting up.\nOne of the things that made smartphones ubiquitous is their ability to execute countless apps. Those apps take the advantage of the device\u2019s high computation power, constant internet connection, and features like location services, and use them to handle various tasks. Google Play store offers 1.5 million apps for Android users. On the average, there are 97 installed apps on typical smartphone according to the logs of EverythingMe, and similarly, there are 96 installed apps according to Yahoo Aviate\u2019s logs [1]. The high number of installed apps and the limited number of app icons that can be displayed on the device\u2019s screen, requires a new paradigm to address their visibility to the user.\nIn this paper we propose a new algorithm for dynamically predicting a set of apps that the user is likely to use. A set of app icons (usually four) are presented on a special dock, called Prediction Bar, and dynamically change according to the user\u2019s habits at a given time, location, and device state (headphone connected, bluetooth active, and so on). For example, a user often uses the app Evernote to take notes during the morning at her office, but never at home or during the weekend. Every Saturday morning she goes to the farmers market and then uses BestParking to help her find a nearby parking place. Every now and then she uses Facebook. The time, location and the device state are all considered as the context of the user\u2019s device. The goal of the algorithm is, given the context information,\n\u2217J. Keshet is with the Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel, 52900. \u2020A. Kariv, A.Dagan, D. Volk and J. Simhon are with EverythingMe and Doat Media Ltd., Tel-Aviv,\nIsrael, 64921.\nar X\niv :1\n51 2.\n07 85\n1v 2\n[ cs\n.L G\n] 2\n5 Ja\nn 20\n16\nto actively help the user navigate to the desired app as well as to provide a personalized feeling.\nThe algorithm proposed here is based on learning the user\u2019s preferences in an online fashion. The algorithm keeps a weight vector (a set of parameters) for each of the installed apps. The user\u2019s contextual information is represented as a vector of real numbers called feature primitive that is mapped to a high dimensional abstract space, to have a more meaningful representation for the learning process. The projection of the mapped feature primitives onto any of the app\u2019s weight vector is the score associated with that app. At a given context the algorithm selects the set of apps that attain the highest score, and presents them to the user as icons in the Prediction Bar. Then the user clicks on one of the apps, either in the Prediction Bar, or elsewhere. If the clicked app is not presented in the Prediction Bar, or is in the Prediction Bar, but with a low score, then the algorithm updates the weight vector associated with the clicked app. The algorithm also updates all the weight vectors of the apps that where mistakenly predicted with a higher score than the clicked app.\nOne way to assess the performance of the algorithm is by checking the precision of the prediction, that is, the number of times the user clicks on the apps displayed in the Prediction Bar relative to the number of times she clicks on apps displayed anywhere (including the Prediction Bar). While this might sound a very reasonable measure of performance, it seems that this evaluation metric tends to predict the most frequent apps and ignores rarely used apps. It prefers, for example, the prediction of frequently used apps, like Facebook, at any time and location, over the prediction of apps like GateGuru, which the user uses only at airports, or BestParking, which the user uses every Saturday morning when she searches for a parking spot. An empirical study conducted by EverythingMe showed that the naive paradigm that constantly presents the most frequent apps in the Prediction Bar, attains the highest precision (see Section 4). This, of course, does not serve our goals of helping the user navigate to the desired app or of providing a personalized feeling.\nAs we shall see from the empirical analysis, we prefer to assess the quality of the prediction using a different metric, namely, the area under the receiver operating characteristic curve (AUC). This measure of performance tends to prefer predictions with a high true positive rate and a low false positive rate, which means that a mis-prediction of any app has the same cost \u2013 no matter how frequent the app is.\nWe propose an efficient online algorithm that is executed on the device. It is based on the Passive-Aggressive online algorithmic framework [3], adapted to maximize the AUC at each round. While there exists algorithms to maximize the AUC, such as [12, 19], they are different from the algorithm proposed here in several aspects. Firstly, the algorithms [12, 19] are batch algorithms. In the batch setting the input to the algorithm is a training set of labeled examples, and the output of the algorithm is a hypothesis that should perform well on an unseen data that drawn from the same distribution of the training set. In the online setting, which we are interested in this paper, the algorithm constantly adapts the hypothesis, and there is partition of the data into a training set or a test set. The online learning algorithms works in rounds, where at each round the algorithm get as an input a feature vector that represents the device context and has to predict the next apps the user will use. Based on the feedback from the user, the algorithm updates its hypothesis. The online algorithm does not need to be evaluated on a test set of unseen data, but on the next behavior of the user. Hence online algorithm inherently supports a drifting hypothesis, that is, it support a drift or a change of the user\u2019s preferences over time. Secondly, the algorithms [12, 19] are based on structural support vector machine (SSVM) [22]. They assume that the training set is given as pairs of a feature vector and a binary label. At every round, they need a loss-augment inference with the AUC loss, which is computationally heavier than a\nsimple inference. Our algorithm, on the other hand, assumes that each example is composed of pairs of two feature vectors: one that represents a context in which the app is used and the other represents a context where the app is not used. This might be more efficient than the used of loss-augmented inference, and leads to a strongly consistent preditor when converted to a batch algorithm. Since we are interested in the online setting in the paper, we defer the theoretical comparison to a different paper.\nA important issue in practical implementation of the algorithm is how to initiate the Prediction Bar for new users, when we do not have any of their app preferences. We will discussion on this topic in the Section 5. The interested reader can find more ideas in [1].\nThe contribution of this paper is the following: (i) a new machine learning online algorithm for ranking the set of installed apps so as to maximized the AUC; (ii) theoretical analysis of the algorithm; (iii) a large scale empirical study on the performance of the algorithm on a real users\u2019 data.\nThe paper is organized as follows. In Section 2 we present previous work on predicting app usage. In Section 3 we formally state the notation and the problem definition. Then in Section 4 we describe the motivation of using the AUC as an evaluation metric. We continue with a detailed derivation of the algorithm and present its theoretical analysis in Section 5. The features used in the prediction are discussed in Section 6. Experimental results are presented in Section 7. We conclude the paper with a discussion in Section 8."}, {"heading": "2 Related Work", "text": "The problem of predicting the app the user is about to use has been recently addressed by many research groups. The work in [11] was one of the first. The authors proposed an app predictor based Bayesian Networks and contextual information such as time, location, and the user profile. They evaluated their results using average prediction rate on the IDIAP/Nokia MDC dataset [13], which contains a small group of 38 users. Similarly, in [27] the author proposed a light-weighted Bayesian methods to predict the next app based on the app usage history. They also evaluated their result on the MDC dataset. Yahoo Aviate team used Bayesian Networks as a learning algorithm and evaluated it on a larger set of 480 active users, and compare their system to other standard learning algorithm [1]. They are one of the few groups who study the cold-start problem, when no data is available on the user. In [10] the temporal user\u2019s behavior was also taken into account by using an HMM-based sequence prediction.\nMost works are based on contextual features related to the time, location, phone state and environment. Some authors [25, 15], however, proposed to identify the usage patterns and user rating, without taking into account usage context. Interestingly, in [15] the authors propose to detect the periodicity patterns of usage by the Fourier transform, and then scoring them by Chebyshev\u2019s inequality.\nSeveral authors based their prediction on similar users or a general group of users. Both [25, 17] are based on the adaptation of a collaborative filtering algorithm as an app prediction algorithm. The work presented in [24] leverages the user-specific models by patterns of community app behaviors, guided by user similarities.\nRelated works also include predictions of users behavior with mobile devices. See, for example, [23, 8, 6, 5, 16], and the many refereces therein. Another set of works concerns a smart caching mechanism for fast app pre-loading [26, 18].\nAll the works above have been focused on the maximizing the average precision or the prediction rate. In that sense our work is unique, as it proposes a new theoretically founded algorithm for the task of app prediction. Most of the works has been focused on generative\nmodels (Bayesian Networks and HMM) and our work is based on discriminative models (large margin and kernel methods). We would like to note that we found it difficult to directly compare our algorithm to previous work. The main reason is that there is no single benchmark for this tasks, and the few dataset which exists are either restricted to non-profit organizations (IDIAP/Nokia MDC dataset) or not available online (AppJoy dataset)."}, {"heading": "3 Problem Setting", "text": "In this section we set the notation and formally define the problem of online prediction of app usage. Our goal is to predict the most probable apps the user will use given her location, activity, time, device status, and so on. The input to the system, therefore, is a feature primitive, x \u2208 X, that represents the user\u2019s contextual information as a vector of n real numbers, where X \u2282 Rn is the domain of possible contextual information. The concrete representation and description of the features are discussed in Section 6.\nWe denote the set of user\u2019s installed apps by A, and their number by K = |A|. The Prediction Bar presents a set of k apps (currently k=4), and it is assumed that at a given context the user may click on one of the k apps (k \u2264 K) with a high probability. Our goal is to find a function that outputs the set of k apps the user is about to click on, given the input context. The predicted apps should reflect their relevance according to the user\u2019s preference at the given context.\nWe assume that there is a score function, f : X \u00d7 A \u2192 R, that assigns a score to every app a \u2208 A given the user\u2019s context x \u2208 X. High score of f(x, a) means that app a is relevant at context x. We define the set of all scores as F (x,A) = {f(x, a) | a \u2208 A}. Define the k-sort function as a function that returns a vector of the top k ordered scores, given the set of all scores, namely, sortk : F (x,A)\u2192 Rk. The argument of the k-sort function is the set of apps corresponding to the top k ordered scores. The prediction of the set of k best apps is given as\na\u0302 = arg sortk F (x,A), (1)\nwhere a\u0302 \u2208 Ak is a vector of k apps, ordered such that the most relevant app is first, the second most relevant is second, and so on.\nThe performance of the prediction is measured by a cost function \u03b3(a, a\u0302) between the clicked app a \u2208 A and a set of k predicted apps a\u0302, which checks the existence of the clicked app a within the set a\u0302 and returns a positive real number, namely, \u03b3 : A\u00d7 Ak \u2192 R+. For example, the 0-1 cost function is defined to be 0 if a \u2208 a\u0302 and 1 otherwise, that is,\n\u03b3(a, a\u0302) = I[a /\u2208 a\u0302], (2)\nwhere I[\u03c0] is an indicator function and it equals to 1 if the predicate \u03c0 holds and 0 otherwise. If k = 1 this cost function reduces to the standard 0-1 binary loss function. In Section 4 we show that using the 0-1 cost function as a measure of performance favors the set of k most frequently used apps over other types of predictions, and we then present the reason to introduce other cost functions.\nWe propose an online learning algorithm for predicting a set of k apps. The online algorithm maintains a set of parameters. We denote the parameters the t-th round by \u03b8t, and indicating that by adding a subscript to the prediction function f\u03b8t . Each round corresponds to a click on an app. After each such click the algorithm updates it parameters and generates a new predictor f\u03b8t+1 . Our goal is to estimate the parameters so as to maximize the cumulative AUC along its run."}, {"heading": "4 Automatic App Usage Prediction", "text": "We briefly review the device screen organization. A schematic screenshot of the device is depicted in Figure 1. In this particular device there are two Workspaces. A Workspace is a virtual screen where the user can place app icons and app folders. The user can change Workspaces by swiping the screen left or right. The primary Workspace is also called the Home Screen. At the bottom of all Workspaces there is a set of 5 icons which is called the App Dock. The App Dock is composed of 4 user defined app icons, while the middle icon opens the App Tray - a folder which contains all of the installed apps. On the first Workspace, above the App Dock, there is another set of 5 icons which are called the Prediction Bar. Out of those 5 icons, 4 represent apps that are automatically predicted according to the context information.\nApps that are located on the Home Screen or on the App Dock are considered as highly available apps and they should not be considered to be predicted in the Prediction Bar. By presenting to the user apps which are already located in the same screen, we create un-necessary duplication and offer no value at all. This characteristic has an interesting implication. These highly available app clicks we avoid also tend to be the ones that are most used by the user. About 35% of app clicks are done on application shortcuts which are placed on the Home Screen or the App Dock; these clicks are distributed among only 4.6 apps on average. The rest of the clicks, i.e., the ones which occur in other locations, are distributed among 17.6 apps on average. This means that our prediction algorithm needs to focus on the 2nd-tier apps, and not on the mostly used apps. Hence, from now on we will restrict ourselves to the prediction of those apps which are not on the Home Screen.\nIn order to generate the automatic prediction, the device keeps track of all app clicks. For each such event the device records the time of the event, the device location (both exact coordinates as well as an indication of a recurring location), and hardware related signals (e.g., headphones connected indication, current Wi-Fi network, Bluetooth devices,\netc.). Along with these fields, the position of the app on the device\u2019s screen is also recorded. Specifically we record both the relative location of the app\u2019s icon on the display (e.g., first row and third column of the secondary workspace), and if it was clicked on a special location (e.g. App Tray, App Dock, Prediction Bar, a search result, a shortcut inside a folder, etc.) \u2014 all those form the user\u2019s context feature vector x.\nOne of the most trivial ways to build the Prediction Bar is to constantly predict the k most frequent apps. We name this method the k Most Frequently Used, or kMFU. Formally, we define the score function for an app a as follows:\nfkMFU(x, a) = { 1 if a \u2208 AkMFU 0 otherwise , (3)\nwhere AkMFU is the set of k most used installed apps. A bit more dynamic method combines the most frequent apps and the most recent apps and is called the Frecency algorithm. This algorithm was originally used for cache management [14], and adapted to an algorithm for predicting users\u2019 behavior [8]. Denote by tai the time of the i-th click on app a, where there were overall n\na clicks on this app in the last T days. Then the score function of an app a is defined as follows:\nfFrecency(x, a) = na\u2211 i=1 p t\u2212tai T , (4)\nwhere p and T are fixed parameters (in our setting p = 0.1 and T = 60 days performed best).\nBefore we considered designining a new online learning algorithm, we tested the kMFU and the Frecency algorithms on our data as follows (full details in Section 7). We chose uniformly at random a set of 1000 devices that contains a total of 5,181,312 app clicks. For each device we executed both kMFU and Frecency and tried to predict a set of k = 4 apps. We checked the average 0-1 cost as in (2) of each algorithm on each device and averaged the results on all devices. This is the average precision of the algorithm, averaged on the rounds and the devices.\nWe found out that both kMFU and Frecency algorithms had very high precision (see Figure 2a). This, however, does not serve our goal: the kMFU algorithm simply puts the same 4 apps in the Prediction Bar, and does not predict the next app or gives a personalized feeling. The Frecency algorithm tends to predict frequent used apps and \u201cforgets\u201d, for example, bi-weekly or weekly app click patterns.\nMeasuring the performance of the algorithm using precision is biased toward the frequency of the app usage. In our case a better evaluation metric would be the area under the receiver operating characteristic curve (AUC) [7]. This measure of performance tends to prefer predictions with a high true positive rate and a low false positive rate, which means that a mis-prediction of any app has the same cost no matter how frequent the app is. In the next section we derive an online algorithm that predict the next app and aims at maximizing the AUC metric."}, {"heading": "5 Learning Apparatus", "text": "We saw in the previous section that the the usage of 0-1 cost function, \u03b3(a, a\u0302) = I[a /\u2208 a\u0302], to assess the performance favors the set of k most used apps over other predictions. This lead to a static prediction, which does not reflect the user\u2019s instantaneous context-based app usage. We turn to describe a different measure of evaluation.\nThe receiver operating characteristic curve (ROC) is a graphical plot of the true positive rate (a \u2208 a\u0302) as a function of the false positive rate (a /\u2208 a\u0302). The points on the curve are obtained by sweeping the decision threshold from the most positive confidence value to the most negative one. Hence, the choice of the threshold represents a trade-off between different operational settings, corresponding to cost functions weighting false positive and false negative errors differently. Assuming a flat prior over all cost functions, it is appropriate to select the system maximizing the averaged performance over all settings, which corresponds to the model maximizing the area under the ROC curve (AUC). In the following we describe a large margin approach which aims at predicting a set of apps which achieves a high AUC.\nLet us denote by Xa,+ the set of contexts (location, times, WiFi networks, etc.) where app a is clicked. Similarly denote by Xa,\u2212 the set of contexts where app a is never clicked. The AUC can be defined as [2, 9]\nAUC = P[f\u03b8(xa,+, a) > f\u03b8(xa,\u2212, a)], (5)\nwhere xa,+ \u2208 Xa,+ and xa,\u2212 \u2208 Xa,\u2212. That is, the AUC is the probability that the confidence for app a in a context that it is clicked is higher than the confidence where it is not clicked. The probability is over a triplet (xa,+,xa,\u2212, a) drawn from a fixed unknown distribution \u03c1. Our goal is find the parameters \u03b8 that maximizes the AUC, namely\n\u03b8\u2217 = arg max \u03b8 P[f\u03b8(xa,+, a) > f\u03b8(xa,\u2212, a)], (6)\nor equivalently,\n\u03b8\u2217 = arg max \u03b8\nE [ I [ f\u03b8(x a,+, a) > f\u03b8(x a,\u2212, a) ]] , (7)\nwhere the expectation is with respect to the distribution \u03c1. Since the distribution \u03c1 is unknown, this optimization problem cannot be solved directly. For methodological reasons we will first discuss the batch setting, where a training set of m examples are sampled from the distribution \u03c1. We replace the expectation with the sample average and add a regularization factor to avoid overfitting. The objective is now\nmin \u03b8\n1\nm m\u2211 i=1 I [ f\u03b8(x ai,+ i , ai) < f\u03b8(x ai,\u2212 i , ai) ] + \u03bb\u2126(\u03b8), (8)\nwhere \u03bb is a trade-off parameter between the loss term and the regularization. Conventionally, we replaced the max operation with a min operation while changing the direction of the inequality sign. Still, we cannot optimize this objective since the summands are indicator functions, a combinatorial quantity that is hard to be minimized directly. Different learning algorithms use various surrogate loss function to approximate this function. Here we focused on large margin based algorithms, which replaces it with a convex upper bound surrogate loss called hinge loss we describe in the next subsection."}, {"heading": "5.1 A Large Margin Algorithm for App Ranking", "text": "Building on techniques used for learning multiclass [4] and structured prediction classifiers [21, 22], our set of prediction functions distills to a classifier in this vector-space which is aimed at separating the relevant apps from irrelevant ones. We focus on the following set of linear prediction functions:\nf\u03b8(x, a) = \u03b8 \u00b7\u03d5(x, a), (9)\nwhere \u03b8 \u2208 Rd are the model parameters , and \u03d5 : X \u00d7 A \u2192 Rd is a set of functions called feature functions or feature maps. Each feature function gets as input a feature primitive x\nand an app a and returns a scalar which, intuitively, should be correlated with whether the app a is used in context of x. For example, one such function might be how many times the app a was clicked in a radius of 50 meters around the location xyz. Refer to Section 6 for mode details.\nMethods like support vector machines (SVMs) replace the summands of (8) with their corresponding convex upper bound called hinge loss. Denote [\u03c0]+ = max{\u03c0, 0}, then in our case:\nI [ \u03b8 \u00b7\u03d5(xai,+i , ai) < \u03b8 \u00b7\u03d5(x ai,\u2212 i , ai) ] (10)\n\u2264 [ 1\u2212 \u03b8 \u00b7\u03d5(xai,+i , ai) + \u03b8 \u00b7\u03d5(x ai,\u2212 i , ai) ] +\n(11) \u2264 [ 1\u2212 \u03b8 \u00b7\u03d5(xai,+i , ai) + maxa \u03b8 \u00b7\u03d5(x ai,\u2212 i , a) ] + . (12)\nWe define the hinge surrogate loss as follows:\n`(xai,+i ,x ai,\u2212 i , ai,\u03b8) = [ 1\u2212 \u03b8 \u00b7\u03d5(xai,+i , ai) + maxa \u03b8 \u00b7\u03d5(x ai,\u2212 i , a) ] + . (13)\nUsing this surrogate loss instead of the cost in the objective (8), and using the `2 regularization, we get the structural SVM algorithm for maximizing the AUC in our setting\nmin \u03b8\n1\nm m\u2211 i=1 `(xai,+i ,x ai,\u2212 i , ai,\u03b8) + \u03bb 2 \u2016\u03b8\u20162. (14)\nThis is a convex function in its parameters and its solution can be found using the cutting plane method [22] or by stochastic sub-gradient descent [20]. In out setting we are interested in optimizing the set of parameters for each user separately on the device. Moreover we would like to handle scenarios where the users preferences changes over time. Hence we turn to online learning algorithm."}, {"heading": "5.2 An On-line Algorithm", "text": "We now describe an online algorithm for learning the parameters, while maximizing the AUC instantaneously. It is a variant of the Passive-Aggressive algorithm [3] for maximizing the AUC which we call AUC-PA.\nThe online algorithm works in rounds, and updates its parameters every round. Set the initial parameters to \u03b80 = 0. While the model extends prediction for every context, a learning round is defined by the event of app click. At the t-th round, when the device is at context xt, the algorithm predicts a set of k apps a\u0302t using the set of parameters \u03b8t\u22121\na\u0302t = arg sort k {\u03b8t\u22121 \u00b7\u03d5(xt, a) | a \u2208 A}. (15)\nThe set of predicted apps a\u0302kt are presented to the user. The user clicks on an app at either from the set of k presented apps or from the whole set of the n installed apps A.\nFor each app a we keep a set Xa,\u2212 of randomly chosen context, where the app a is never clicked by the user. Once we know the user clicked the app at, we extract uniformly at random xat,\u2212 \u2208 Xat,\u2212. We then predict the most relevant app given the new feature primitive\na\u0302\u2212t = arg max a\u2208A \u03b8t\u22121 \u00b7\u03d5(xat,\u2212, a). (16)\nThe model then suffers a loss `(xat,+t ,x at,\u2212 t , at,\u03b8t\u22121), defined in (13) or equivalently\n`(xai,+i ,x ai,\u2212 i , ai,\u03b8t\u22121) = 1\u2212 \u03b8t\u22121 \u00b7\u03d5(x ai,+ i , ai) + \u03b8t\u22121 \u00b7\u03d5(x ai,\u2212 i , a\u0302 \u2212 t ), (17)\nand updates the parameters as follows\n\u03b8t = arg min \u03b8\n`(xat,+t ,x at,\u2212 t , at,\u03b8) +\n\u03bb 2 \u2016\u03b8 \u2212 \u03b8t\u22121\u20162. (18)\nThe solution of this optimization problem is [3]\n\u03b8t = \u03b8t\u22121 + \u03c4t [ \u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t ) ] , (19)\nwhere\n\u03c4t = min\n{ 1\n\u03bb ,\n`(xat,+t ,x at,\u2212 t , at,\u03b8t\u22121)\n\u2016\u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t )\u20162\n} . (20)\nThe overall algorithm is described in Algorithm 1.\nAlgorithm 1 The AUC-PA algorithm\n1: input: parameter \u03bb 2: init: \u03b80 = 0 3: for t = 1, 2, . . . do 4: new user\u2019s context xt 5: predict a\u0302t = arg sort\nk {\u03b8t\u22121 \u00b7\u03d5(xt, a) | a \u2208 A} 6: the user clicked on at 7: infer a\u0302\u2212t = arg maxa\u2208A \u03b8t\u22121 \u00b7\u03d5(xat,\u2212, a) 8: suffer loss `(xai,+i ,x ai,\u2212 i , ai,\u03b8t\u22121)\n9: set \u03c4t = min { 1 \u03bb , `(x at,+ t ,x at,\u2212 t ,at,\u03b8t\u22121)\n\u2016\u03d5(xat,+,at)\u2212\u03d5(xat,\u2212,a\u0302\u2212t )\u20162 } 10: update: \u03b8t = \u03b8t\u22121 + \u03c4t [ \u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t )\n] 11: end for"}, {"heading": "5.3 Analysis", "text": "We show that our online algorithm attains a high cumulative AUC after T rounds, defined as follows\nA\u0302UC = 1\nT T\u2211 t=1 I [ \u03b8t \u00b7\u03d5(xat,+t , at) > \u03b8t \u00b7\u03d5(x at,\u2212 t , at) ] , (21)\nwhere \u03b81, . . . ,\u03b8T are the weight vectors obtained by the algorithm. The examination of the cumulative AUC is of great interest as it provides an estimator for the generalization performance. Note that at each round the algorithm can be considered as receives new example (xat,+t ,x at,\u2212 t , at) and predicts an app that is best suitable to x at,\u2212 t using the previous weight vector \u03b8t\u22121. Only after the prediction a\u0302 \u2212 t is made the algorithm suffers loss. The cumulative AUC is a weighted sum of the performance of the algorithm on the next unseen training example and hence it is a good estimation to the performance of the algorithm on unseen data during training.\nThe following theorem states a competitive bound. It compares the cumulative AUC of the weight vectors series, {\u03b81, . . . ,\u03b8T }, resulted from the online algorithm to the best\nfixed weight vector, \u03b8?, chosen in hindsight, and essentially proves that, for any sequence of examples, our algorithms cannot do much worse than the best fixed weight vector. Formally, it shows that the cumulative area above the curve, 1 \u2212 A\u0302UC, is smaller than the weighted average loss `(xat,+t ,x at,\u2212 t , at,\u03b8t\u22121) of the best fixed weight vector \u03b8\n? and its weighted complexity. That is, the cumulative AUC of the iterative training algorithm is going to be high, given that the loss of the best solution is small, the complexity of the best solution is small and that there are reasonable number of rounds, T .\nTheorem 1. Let {(xat,+t ,x at,\u2212 t , at)}Tt=1 be a set of training examples and assume that we\nhave \u2016\u03d5(x, a)\u2016 \u2264 1/ \u221a\n2 for all x and a. Let \u03b8? be the best weight vector selected under some optimization criterion by observing all instances in hindsight. Then,\n1\u2212 A\u0302UC \u2264 \u03bb T \u2016\u03b8?\u20162 + 2 T T\u2211 t=1 `(xat,+t ,x at,\u2212 t , at,\u03b8 ?). (22)\nwhere \u03bb \u2264 1 and A\u0302UC is the cumulative AUC defined in (21).\nProof. Denote by `t(\u03b8) the instantaneous loss the weight vector \u03b8 suffers on the t-th round, that is, `t(\u03b8) = `(x at,+ t ,x at,\u2212 t , at,\u03b8t\u22121). The proof of the theorem relies on Lemma 1 and Theorem 4 in [3]. Lemma 1 in [3] implies that,\nT\u2211 t=1 \u03c4t ( 2`i(\u03b8t\u22121)\u2212 \u03c4t\u2016\u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t )\u20162 \u2212 2`t(\u03b8?) ) \u2264 \u2016\u03b8?\u20162. (23)\nNow if the algorithm makes a prediction mistake and the predicted confidence of app a\u0302\u2212 in xat,\u2212t is higher than the confidence of the app at in x at,+ T then `t(\u03b8t\u22121) \u2265 1. Using the\nassumption that \u2016\u03d5(x, a)\u2016 \u2264 1/ \u221a 2, which in turn means that\n\u2016\u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t )\u20162 \u2264 1,\nand the definition of \u03c4t given in (20), we conclude that if a prediction mistake occurs then it holds that\n\u03c4t`t(\u03b8t\u22121) \u2265 min {\n`t(\u03b8t\u22121)\n\u2016\u03d5(xat,+, at)\u2212\u03d5(xat,\u2212, a\u0302\u2212t )\u20162 ,\n1\n\u03bb } \u2265 min { 1, 1\n\u03bb\n} = 1.\nSumming over all the prediction mistakes made on the entire set of examples and taking into account that \u03c4t`t(\u03b8t\u22121) is always non-negative, we have\nT\u2211 t=1 \u03c4t`t(\u03b8t\u22121) \u2265 T\u2211 t=1 I [ \u03b8t\u22121 \u00b7\u03d5(xat,+t , at) \u2264 \u03b8t\u22121 \u00b7\u03d5(x at,\u2212 t , at) ] . (24)\nAgain using the definition of \u03c4t, we know that \u03c4t`t(\u03b8 ?) \u2264 `t(\u03b8?)/\u03bb and that \u03c4t\u2016\u2206\u03d5t\u20162 \u2264\n`t(\u03b8t\u22121). Plugging these two inequalities and (24) into (23) we get\nT\u2211 t=1 I [ \u03b8t\u22121 \u00b7\u03d5(xat,+t , at) \u2264 \u03b8t\u22121 \u00b7\u03d5(x at,\u2212 t , at) ] \u2264 \u03bb\u2016\u03b8?\u20162 + 2 T\u2211 t=1 `t(\u03b8 ?). (25)\nThe theorem follows by replacing the sum over prediction mistakes to a sum over prediction hits and plugging the definition of the cumulative AUC given in (21)."}, {"heading": "6 Feature Primitives and Feature Functions", "text": "All our features are based on basic primitives that are measured from the device. The feature primitives denoted x \u2208 X \u2282 Rn. They are the device time and date, the device location, and hardware related signals (e.g., headphones connected indication, current Wi-Fi network, Bluetooth devices, etc.).\nOn the top of the feature primitives, we design a set of feature functions which allow us to incorporate into the feature design the presumed app. The idea is that each feature function takes as input a vector of feature primitives x, which describes the context information, and an app a, and returns a scalar which should be related to whether the app a corresponds to the the user\u2019s preference at context x. The feature functions map the context vector x along with a proposed app a to a vector of fixed dimension in Rd.\nOur feature functions have two sets of representations: contextual features and appdependent features. Before turning to describe the actual feature, we define them formally. The set of contextual features is composed of a set of non-linear functions of the feature primitives x and denoted as \u03c8(x), where \u03c8 : X\u2192 Rd\u03c8 , where d\u03c8 is the number of contextual features. The set of app-dependent features is a non-linear functions of the feature primitives x and the app a and denoted \u03c6(x, a), where \u03c6 : X \u00d7 A \u2192 Rd\u03c6 , where d\u03c6 is the number of app-dependent features.\nThe new score function is of the form:\nf\u03b8(x, a) = w \u00b7 \u03c6(x, a) +wa \u00b7\u03c8(x), (26)\nwhere w \u2208 Rd\u03c6 and wa \u2208 Rd\u03c8 for all a \u2208 A are the weight vectors that replace \u03b8. While this function looks somewhat different from the score in (9), it is straightforward to show the form in (9) is more general. Moreover, the context features could all have been represented as app-dependent features. However, in our setting it is more efficient and convenient to keep both set of feature representations. This allows us to manage memory of the context features in a sparse way."}, {"heading": "6.1 Contextual Features", "text": "Contextual features represent the state of the device, and they are expressed as a set of functions over the feature primitives, such as day-of-week, headphones connected, and knownlocation are used and describe in detail below. While the contextual feature do not depend on any specific app, they are weighed from each app separately. It means that we expect to have a high weight for when the feature day-of-week equals Saturday for the app BestParking.\nWe describe now the context feature vector \u03c8(x) that are common to all apps. The first set of features is time-based features and includes the following features\n1. hour-of-day: {0, 1, . . . , 23}\n2. day-of-week: {Mon, Tue, . . . , Sun}\n3. part-of-day: {dawn,morning, noon, afternoon, . . .}\n4. weekend: {yes, no}, where the weekend days are country-specific\nThe second set is location-based features. These features are based in the notion of known location. Known location is defined as a recurring location, that was visited within 50 meters in the last month.\n5. ID of the known location\n6. is the current location known: {yes, no}\n7. just entered a known location: defined by a decay scoring function 10\u2212t/15, where t is the time since entering the location in minutes.\n8. just left known: same function, but t is the time of a transition into unknown locations.\nThe last set of context feature is hardware-based features.\n9. headphones connected: {yes, no}\n10. headphones just connected: defined using the decay function above\n11. headphones just disconnected: defined using the decay function above\n12. Wi-Fi network connected: {yes, no}\n13. Wi-Fi network SSID\n14. Wi-Fi just connected: defined using the decay function above\n15. Wi-Fi just disconnected: defined using the decay function above\n16. Bluetooth network connected: {yes, no}\n17. Bluetooth network SSID\n18. Bluetooth just connected: defined using the decay function above\n19. Bluetooth just disconnected: defined using the decay function above\nThe last feature models the dependency of the current app given the most 5 frecent apps.\n20. The 5 scores corresponds to the 5 most frecent apps according to the frecency predictor in (4), where T = 1 hour."}, {"heading": "6.2 App-dependent Features", "text": "The second representation of features includes non-linear function functions that are computed for each app separately. We have three feature function.\nLet Ta be the set of set of time-samples that the app a was used. Those are absolute time values. The first set feature functions scale the current time t relative to the previous time-stamps of the app a:\n\u03c61,h(x, a) = 1 |Ta| \u2211 ts\u2208Ta 0.5 ( 1 + e\u2212(t\u2212ts) 2/2h2 ) for h in {1, 1.5, 3} days. This feature function gives high score to recent apps for which t\u2212 ts is small in the order of h.\nThe next set of feature functions is similar, but it refers to the relative times within a day. That is, if an app a is clicked every day around 9:30am, the score would be high if it is used around the same time again,\n\u03c62,h(x, a) = 1 |Ta| \u2211 ts\u2208Ta e\u2212\u2206SecOfDay(t,ts) 2/2h2\nfor h in {60, 600, 1500} seconds. The function \u2206SecOfDay(t, ts) is the difference between absolute times t and ts, translated to seconds of day.\nThe third set of feature functions score the apps based on the locations it is used. Let l be the current location in terms of latitude-longitude. Let La be the set of the locations the app a was used (latitude-longitude). Let \u2206LatLong(l, ls) be the distance in meters between the location l and the location ls, then,\n\u03c63,h(x, a) = 1 |La| \u2211 ls\u2208La e\u2206LatLong(l,ls) 2/2h2\nfor h in {50, 200, 1000} meters."}, {"heading": "7 Experimental Results", "text": "In this section we present the performance of our algorithm against other algorithms with different evaluation metrics. We start by comparing the accuracy of the proposed algorithm to other algorithms and with different evaluation metrics. Then, we verify our results with different number of users on different time periods. We continue with an experiment that check the accuracy and the convergence of the online algorithm over time. We conclude with results on the influence of the app ranking on its accuracy.\nThe data was collected from a set of 1000 randomly selected users which were active users of EverythingMe Launcher for 180 days. This set containing 5,181,312 app-click entries.\nOn each of these data-sets, we ran the three prediction algorithms - kMFU (using k = 4), Frecency (using p = 0.1, T = 60days) and AUC-PA (using C = 0.02)."}, {"heading": "7.1 App prediction performance", "text": "We start by comparing the performance of the online algorithm with the kMFU algorithm and the Frecency algorithm. We randomly selected 1000 devices and extracted their data for time span of 180 days. Specifically we extracted the context information at times which correspond to app-click events. Then, for each such event, we extract a vector of feature primitives and feature functions and predict a set of 4 apps using kMFU, Frecency and AUC-PA. We compared the predictions of the algorithms with the actual app that was clicked. The performance results are given in Figure 2. We present 3 evaluation metrics: (a) precision \u2013 for each device we count the times the predicted app is also the clicked app, and then average over all devices ; (b) per-app precision \u2013 for each device and for each app we checked the precision, then averaged over all apps and devices; (c) AUC \u2013 for each device we computed the AUC and averaged over devices. Note that AUC cannot be computed to kMFU, since it constantly predicts the same k apps and only them.\nRecall that the kMFU algorithm always predicts the same k apps \u2014 the most frequent ones. As expected it has the best precision compared to Freceny and AUC-PA. On the other hand when comparing the per-app precision and the AUC, the AUC-PA outperforms kMFU and Frecency."}, {"heading": "7.2 Prediction Quality Over Time", "text": "In the next experiment we analyze the quality of the prediction over the entire time period. We use the same randmoly selected group of 1000 devices, and calculate the (a) per-app precision and (b) AUC in a sliding window of one week. That is, each point in the following\ngraph represents a time period of one week starting 3 days beforehand and ending 3 days afterwords.\nWe consider the performance of the algorithms over since it is installed on the device and on, that is we would like to understand the behavior over time and how it converges over time. This is very important issue, mainly in order to understand how fast prediction would be relevant for new users (the cold start problem).\nWe can see that all algorithms reach stable performance in the first couple of days, and maintain the same performance throughout the entire time period. AUC-PA outperforms Frecency and kMFU consistently both in the AUC as well as with the per-app precision.\nSomething that\u2019s worth noting is the fact that as time passes, the prediction algorithms need to take into consideration more apps. According to the test data, the average amount of distinct apps used by a user accumulated over 30 days is 30.23, after 90 days is 57.92 and after 180 days, 76.14. A simple explanation for this increase is that most apps have a limited lifetime on user\u2019s devices. Some apps are installed, used for some time and then forgotten or uninstalled. Other apps are simply used very rarely.\nBoth kMFU and Frecency ignore the lesser-used apps by design, as both favour the more used apps. It is therefore interesting to see that in spite the increase in the amount of possible predictions AUC-PA keeps a stable per-app precision rate. This demonstrates the ability of the prediction algorithm to adapt to a moving hypothesis."}, {"heading": "7.3 Performance versus the usage-rank", "text": "This final experiment demonstrates the ability of AUC-PA to predict from the long-tail of lesser-used apps. As previously stated, the average user will use on average 17.6 2nd-tier apps during a single month (i.e. ones which are not positioned on the App Dock or the Home Screen). We therefore assign for each app and device a usage-rank - 0 for the most used app, 1 for the second most used app and so on. Next, we average on all devices the prediction precision of all of the n-ranked apps.\nAs expected, kMFU performs very well on ranks 0-3 and very badly on the rest. This is a direct result of the fact that it always returns the k most used apps, k = 4 in our setting. Frecency performs slightly worse for the most used apps and slightly better for the\nlesser ranked apps, although by the time we reach the 10th place performace is quite poor. AUC-PA shows significantly better results for the apps with lower ranks, while taking a hit in the top-ranked apps."}, {"heading": "8 Discussion", "text": "In this paper we presented an algorithm for predicting the set of apps the user will probably use, based on the device\u2019s contextual information (time, location, etc.). The algorithm was designed to optimize the user\u2019s personalization and to promote the prediction of less used apps in an appropriate context by aiming at the maximization of AUC, rather than simply the raw clicks. The algorithm runs efficiently on the device in an online fashion and constantly updates its hypothesis to handle changes in user\u2019s preference over time.\nIn a set of experiments we showed that the algorithm attains high performance when evaluated using AUC or when evaluated using the normalized precision (frequently and rarely used apps are weighted the same). We also showed that the algorithm converges, on the average, within few days. Last we showed that the algorithm attains a high prediction rate for those apps which are ranked below the most used apps.\nFuture work is focused on finding new features to improve performance in general. More specifically, we are interested in including features that indicate the user is located in a special point-of-interest, like a school, an airport, or a shopping mole. Such features, for example, will allow the algorithm to support a different set of app preferences at those location. Another possible direction which we are investigating now is the option to suggest the user to install a new app that would fits its preference in a given context. Another interesting direction that we will explore in future work is a sequential modeling of app usage.\n0 10 20 30 40 50 0.\n0 0.\n2 0.\n4 0.\n6 0.\n8 1. 0 App usage rank (#clicks out of #total clicks) A ve ra ge p re ci si on \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf"}], "references": [{"title": "Predicting the next app that you are going to use", "author": ["Ricardo Baeza-Yates", "Di Jiang", "Fabrizio Silvestri", "Beverly Harrison"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The area above the ordinal dominance graph and the area below the receiver operating characteristic graph", "author": ["Donald Bamber"], "venue": "Journal of mathematical psychology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1975}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Where and what: Using smartphones to predict next locations and applications in daily life", "author": ["Trinh Minh Tri Do", "Daniel Gatica-Perez"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Where to go from here? mobility prediction from instantaneous information", "author": ["Vincent Etter", "Mohamed Kafsi", "Ehsan Kazemi", "Matthias Grossglauser", "Patrick Thiran"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "An introduction to ROC analysis", "author": ["Tom Fawcett"], "venue": "Pattern recognition letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Accessrank: predicting what users will do next", "author": ["Stephen Fitchett", "Andy Cockburn"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["James A Hanley", "Barbara J McNeil"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Toward personalized context recognition for mobile users: A semisupervised bayesian HMM approach", "author": ["Baoxing Huai", "Enhong Chen", "Hengshu Zhu", "Hui Xiong", "Tengfei Bao", "Qi Liu", "Jilei Tian"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Predicting mobile application usage using contextual information", "author": ["Ke Huang", "Chunhui Zhang", "Xiaoxiao Ma", "Guanling Chen"], "venue": "In Proceedings of the 2012 ACM Conference on Ubiquitous Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A support vector method for multivariate performance measures", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "The mobile data challenge: Big data for mobile computing research", "author": ["Juha K Laurila", "Daniel Gatica-Perez", "Imad Aad", "Olivier Bornet", "Trinh-Minh-Tri Do", "Olivier Dousse", "Julien Eberle", "Markus Miettinen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1924}, {"title": "On the existence of a spectrum of policies that subsumes the least recently used (lru) and least frequently used (lfu) policies", "author": ["Donghee Lee", "Jongmoo Choi", "Jong-Hun Kim", "Sam H Noh", "Sang Lyul Min", "Yookun Cho", "Chong Sang Kim"], "venue": "In ACM SIGMETRICS Performance Evaluation Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Mining temporal profiles of mobile applications for usage prediction", "author": ["Zhung-Xun Liao", "Po-Ruey Lei", "Tsu-Jou Shen", "Shou-Chung Li", "Wen-Chih Peng"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Mining GPS data for mobility patterns: A survey", "author": ["Miao Lin", "Wen-Jing Hsu"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Which app will you use next?: collaborative filtering with interactional context", "author": ["Nagarajan Natarajan", "Donghyuk Shin", "Inderjit S Dhillon"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Practical prediction and prefetch for faster access to applications on mobile phones", "author": ["Abhinav Parate", "Matthias B\u00f6hmer", "David Chu", "Deepak Ganesan", "Benjamin M Marlin"], "venue": "In Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Learning structured models with the auc loss and its generalizations", "author": ["Nir Rosenfeld", "Ofer Meshi", "Amir Globerson", "Daniel Tarlow"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical programming,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Contextual patterns in mobile service usage", "author": ["Hannu Verkasalo"], "venue": "Personal and Ubiquitous Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Preference, context and communities: a multifaceted approach to predicting smartphone app usage patterns", "author": ["Ye Xu", "Mu Lin", "Hong Lu", "Giuseppe Cardone", "Nicholas Lane", "Zhenyu Chen", "Andrew Campbell", "Tanzeem Choudhury"], "venue": "In Proceedings of the 2013 International Symposium on Wearable Computers,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "AppJoy: personalized mobile application discovery", "author": ["Bo Yan", "Guanling Chen"], "venue": "In Proceedings of the 9th international conference on Mobile systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Fast app launching for mobile devices using predictive user context", "author": ["Tingxin Yan", "David Chu", "Deepak Ganesan", "Aman Kansal", "Jie Liu"], "venue": "In Proceedings of the 10th international conference on Mobile systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Prophet: What app you wish to use next", "author": ["Xun Zou", "Wangsheng Zhang", "Shijian Li", "Gang Pan"], "venue": "In Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct publication,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "On the average, there are 97 installed apps on typical smartphone according to the logs of EverythingMe, and similarly, there are 96 installed apps according to Yahoo Aviate\u2019s logs [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 2, "context": "It is based on the Passive-Aggressive online algorithmic framework [3], adapted to maximize the AUC at each round.", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "While there exists algorithms to maximize the AUC, such as [12, 19], they are different from the algorithm proposed here in several aspects.", "startOffset": 59, "endOffset": 67}, {"referenceID": 18, "context": "While there exists algorithms to maximize the AUC, such as [12, 19], they are different from the algorithm proposed here in several aspects.", "startOffset": 59, "endOffset": 67}, {"referenceID": 11, "context": "Firstly, the algorithms [12, 19] are batch algorithms.", "startOffset": 24, "endOffset": 32}, {"referenceID": 18, "context": "Firstly, the algorithms [12, 19] are batch algorithms.", "startOffset": 24, "endOffset": 32}, {"referenceID": 11, "context": "Secondly, the algorithms [12, 19] are based on structural support vector machine (SSVM) [22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 18, "context": "Secondly, the algorithms [12, 19] are based on structural support vector machine (SSVM) [22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 21, "context": "Secondly, the algorithms [12, 19] are based on structural support vector machine (SSVM) [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "The interested reader can find more ideas in [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "The work in [11] was one of the first.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "They evaluated their results using average prediction rate on the IDIAP/Nokia MDC dataset [13], which contains a small group of 38 users.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Similarly, in [27] the author proposed a light-weighted Bayesian methods to predict the next app based on the app usage history.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Yahoo Aviate team used Bayesian Networks as a learning algorithm and evaluated it on a larger set of 480 active users, and compare their system to other standard learning algorithm [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "In [10] the temporal user\u2019s behavior was also taken into account by using an HMM-based sequence prediction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Some authors [25, 15], however, proposed to identify the usage patterns and user rating, without taking into account usage context.", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "Some authors [25, 15], however, proposed to identify the usage patterns and user rating, without taking into account usage context.", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "Interestingly, in [15] the authors propose to detect the periodicity patterns of usage by the Fourier transform, and then scoring them by Chebyshev\u2019s inequality.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "Both [25, 17] are based on the adaptation of a collaborative filtering algorithm as an app prediction algorithm.", "startOffset": 5, "endOffset": 13}, {"referenceID": 16, "context": "Both [25, 17] are based on the adaptation of a collaborative filtering algorithm as an app prediction algorithm.", "startOffset": 5, "endOffset": 13}, {"referenceID": 23, "context": "The work presented in [24] leverages the user-specific models by patterns of community app behaviors, guided by user similarities.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "See, for example, [23, 8, 6, 5, 16], and the many refereces therein.", "startOffset": 18, "endOffset": 35}, {"referenceID": 7, "context": "See, for example, [23, 8, 6, 5, 16], and the many refereces therein.", "startOffset": 18, "endOffset": 35}, {"referenceID": 5, "context": "See, for example, [23, 8, 6, 5, 16], and the many refereces therein.", "startOffset": 18, "endOffset": 35}, {"referenceID": 4, "context": "See, for example, [23, 8, 6, 5, 16], and the many refereces therein.", "startOffset": 18, "endOffset": 35}, {"referenceID": 15, "context": "See, for example, [23, 8, 6, 5, 16], and the many refereces therein.", "startOffset": 18, "endOffset": 35}, {"referenceID": 25, "context": "Another set of works concerns a smart caching mechanism for fast app pre-loading [26, 18].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "Another set of works concerns a smart caching mechanism for fast app pre-loading [26, 18].", "startOffset": 81, "endOffset": 89}, {"referenceID": 13, "context": "This algorithm was originally used for cache management [14], and adapted to an algorithm for predicting users\u2019 behavior [8].", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "This algorithm was originally used for cache management [14], and adapted to an algorithm for predicting users\u2019 behavior [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "In our case a better evaluation metric would be the area under the receiver operating characteristic curve (AUC) [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "The AUC can be defined as [2, 9] AUC = P[f\u03b8(x, a) > f\u03b8(x, a)], (5) where x \u2208 X and xa,\u2212 \u2208 Xa,\u2212.", "startOffset": 26, "endOffset": 32}, {"referenceID": 8, "context": "The AUC can be defined as [2, 9] AUC = P[f\u03b8(x, a) > f\u03b8(x, a)], (5) where x \u2208 X and xa,\u2212 \u2208 Xa,\u2212.", "startOffset": 26, "endOffset": 32}, {"referenceID": 3, "context": "1 A Large Margin Algorithm for App Ranking Building on techniques used for learning multiclass [4] and structured prediction classifiers [21, 22], our set of prediction functions distills to a classifier in this vector-space which is aimed at separating the relevant apps from irrelevant ones.", "startOffset": 95, "endOffset": 98}, {"referenceID": 20, "context": "1 A Large Margin Algorithm for App Ranking Building on techniques used for learning multiclass [4] and structured prediction classifiers [21, 22], our set of prediction functions distills to a classifier in this vector-space which is aimed at separating the relevant apps from irrelevant ones.", "startOffset": 137, "endOffset": 145}, {"referenceID": 21, "context": "1 A Large Margin Algorithm for App Ranking Building on techniques used for learning multiclass [4] and structured prediction classifiers [21, 22], our set of prediction functions distills to a classifier in this vector-space which is aimed at separating the relevant apps from irrelevant ones.", "startOffset": 137, "endOffset": 145}, {"referenceID": 21, "context": "This is a convex function in its parameters and its solution can be found using the cutting plane method [22] or by stochastic sub-gradient descent [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "This is a convex function in its parameters and its solution can be found using the cutting plane method [22] or by stochastic sub-gradient descent [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "It is a variant of the Passive-Aggressive algorithm [3] for maximizing the AUC which we call AUC-PA.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "The solution of this optimization problem is [3]", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "The proof of the theorem relies on Lemma 1 and Theorem 4 in [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Lemma 1 in [3] implies that,", "startOffset": 11, "endOffset": 14}], "year": 2016, "abstractText": "There are around a hundred installed apps on an average smartphone. The high number of apps and the limited number of app icons that can be displayed on the device\u2019s screen requires a new paradigm to address their visibility to the user. In this paper we propose a new online algorithm for dynamically predicting a set of apps that the user is likely to use. The algorithm runs on the user\u2019s device and constantly learns the user\u2019s habits at a given time, location, and device state. It is designed to actively help the user to navigate to the desired app as well as to provide a personalized feeling, and hence is aimed at maximizing the AUC. We show both theoretically and empirically that the algorithm maximizes the AUC, and yields good results on a set of 1,000 devices.", "creator": "LaTeX with hyperref package"}}}