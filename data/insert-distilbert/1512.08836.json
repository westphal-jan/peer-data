{"id": "1512.08836", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "Learning to Filter with Predictive State Inference Machines", "abstract": "latent state space models are one of the truly most fundamental and widely used tools for modeling dynamical intelligent systems. traditional maximum likelihood estimation ( mle ) hypothesis based approaches aim to maximize the probable likelihood objective, which is non - convex even due to such latent states. while subsequent non - convex optimization methods like em can learn similar models conditions that actually locally optimize the likelihood estimated objective, using the first locally optimal model for an inference task such measures as bayesian filtering usually does not have performance guarantees. in this work, we propose a method that considers the inference procedure on the dynamical system as a composition of predictors. naturally instead of optimizing a given parametrization of latent states, we learn predictors for its inference in predictive belief plan space, where we sometimes can use sufficient features composing of observations for supervision of our learning algorithm. further we further show easily that our algorithm, the predictive state inference machine, has theoretical performance guarantees on the inference task. empirical verification across several of dynamical system benchmarks ranging from a simulated 3d helicopter to recorded telemetry traces from a robot showcase the abilities of training inference machines.", "histories": [["v1", "Wed, 30 Dec 2015 03:17:00 GMT  (412kb,D)", "https://arxiv.org/abs/1512.08836v1", null], ["v2", "Mon, 30 May 2016 17:20:32 GMT  (1065kb,D)", "http://arxiv.org/abs/1512.08836v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wen sun", "arun venkatraman", "byron boots", "j andrew bagnell"], "accepted": true, "id": "1512.08836"}, "pdf": {"name": "1512.08836.pdf", "metadata": {"source": "META", "title": "Learning to Filter with Predictive State Inference Machines", "authors": ["Wen Sun", "Arun Venkatraman", "Byron Boots", "J. Andrew Bagnell"], "emails": ["WENSUN@CS.CMU.EDU", "ARUNVENK@CS.CMU.EDU", "BBOOTS@CC.GATECH.EDU", "DBAGNELL@RI.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Data driven approaches to modeling dynamical systems is important in applications ranging from time series forecasting for market predictions to filtering in robotic systems. The classic generative approach is to assume that each observation is correlated to the value of a latent state and then model the dynamical system as a graphical model, or latent state space model, such as a Hidden Markov Model (HMM). To learn the parameters of the model from observed data, Maximum Likelihood Estimation (MLE) based methods attempt to maximize the likelihood of the observations with respect to the parameters. This approach has proven to be highly successful in some applications (Coates et al., 2008; Roweis & Ghahramani,\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n1999), but has at least two shortcomings. First, it may be difficult to find an appropriate parametrization for the latent states. If the model is parametrized incorrectly, the learned model may exhibit poor performance on inference tasks such as Bayesian filtering or predicting multiple time steps into the future. Second, learning a latent state space model is difficult. The MLE objective is non-convex and finding the globally optimal solution is often computationally infeasible. Instead, algorithms such as ExpectationMaximization (EM) are used to compute locally optimal solutions. Although the maximizer of the likelihood objective can promise good performance guarantees when it is used for inference, the locally optimal solutions returned by EM typically do not have any performance guarantees.\nSpectral Learning methods are a popular alternative to MLE for learning models of dynamical systems (Boots, 2012; Boots et al., 2011; Hsu et al., 2009; Hefny et al., 2015). This family of algorithms provides theoretical guarantees on discovering the global optimum for the model parameters under the assumptions of infinite training data and realizability. However, in the non-realizable setting \u2014 i.e. model mismatch (e.g., using learned parameters of a Linear Dynamical System (LDS) model for a non-linear dynamical system) \u2014 these algorithms lose any performance guarantees on using the learned model for filtering or other inference tasks. For example, Kulesza et al. (2014) shows when the model rank is lower than the rank of the underlying dynamical system, the inference performance of the learned model may be arbitrarily bad.\nBoth EM and spectral learning suffer from limited theoretical guarantees: from model mismatch for spectral methods, and from computational hardness for finding the global optimality of non-convex objectives for MLE-based methods. In scenarios where our ultimate goal is to infer some quantity from observed data, a natural solution is to skip the step of learning a model, and instead directly optimize the inference procedure. Toward this end, we generalize the supervised message-passing Inference Machine approach\nar X\niv :1\n51 2.\n08 83\n6v 2\n[ cs\n.L G\n] 3\n0 M\nay 2\nof Ross et al. (2011b); Ramakrishna et al. (2014); Lin et al. (2015). Inference machines do not parametrize the graphical model (e.g., design of potential functions) and instead directly train predictors that use incoming messages and local features to predict outgoing messages via black-box supervised learning algorithms. By combining the model and inference procedure into a single object \u2014 an Inference Machine \u2014 we directly optimize the end-to-end quality of inference. This unified perspective of learning and inference enables stronger theoretical guarantees on the inference procedure: the ultimate task that we care about.\nOne of the principal limitations of inference machines is that they require supervision. If we only have access to observations during training, then there is no obvious way to apply the inference machine framework to graphical models with latent states. To generalize Inference Machines to dynamical systems with latent states, we leverage ideas from Predictive State Representations (PSRs) (Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Hefny et al., 2015). In contrast to latent variable representations of dynamical systems, which represent the belief state as a probability distribution over the unobserved state space of the model, PSRs instead maintain an equivalent belief over sufficient features of future observations.\nWe propose PREDICTIVE STATE INFERENCE MACHINES (PSIMs), an algorithm that treats the inference procedure (filtering) on a dynamical system as a composition of predictors. Our procedure takes the current predictive state and the latest observation from the dynamical system as inputs and outputs the next predictive state (Fig. 1). Since we have access to the observations at training, this immediately brings the supervision back to our learning problem \u2014 we quantify the loss of the predictor by measuring the likelihood that the actual future observations are generated from the predictive state computed by the learner. PSIM allows us to treat filtering as a general supervised learning problem handed-off to a black box learner of our choosing. The complexity of the learner naturally controls the tradeoff between computational complexity and prediction accuracy. We provide two algorithms to train a PSIM. The first algorithm learns a sequence of non-stationary filters which are provably consistent in the realizable case. The second algorithm is more data efficient and learns a stationary filter which has reduction-style performance guarantees.\nThe three main contributions of our work are: (1) we provide a reduction of unsupervised learning of latent state space models to the supervised learning setting by leveraging PSRs; (2) our algorithm, PSIM, directly minimizes error on the inference task\u2014closed loop filtering; (3) PSIM works for general non-linear latent state space models and guarantees filtering performance even in agnostic setting."}, {"heading": "2. Related Work", "text": "In addition to the MLE-based approaches and the spectral learning approaches mentioned in Sec. 1, there are several supervised learning approaches related to our work. Data as Demonstrator (DaD) (Venkatraman et al., 2015) applies the Inference Machine idea to fully observable Markov chains, and directly optimizes the open-loop forward prediction accuracy. In contrast, we aim to design an unsupervised learning algorithm for latent state space models (e.g., HMMs and LDSs) to improve the accuracy of closed loop prediction\u2013Bayesian filtering. It is unclear how to apply DaD to learning a Bayesian filter. Autoregressive models (Wei, 1994) on k-th order fully observable Markov chains (AR-k) use the most recent k observations to predict the next observation. The AR model is not suitable for latent state space models since the beliefs of latent states are conditioned on the entire history. Learning mappings from entire history to next observations is unreasonable and one may need to use a large k in practice. A large k, however, increases the difficulty of the learning problem (i.e., requires large computational and samples complexity).\nIn summary, our work is conceptually different from DaD and AR models in that we focus on unsupervised learning of latent state space models. Instead of simply predicting next observation, we focus on predictive state\u2014a distribution of future observations, as an alternative representation of the beliefs of latent states."}, {"heading": "3. Preliminaries", "text": "We consider uncontrolled discrete-time time-invariant dynamical systems. At every time step t, the latent state of the dynamical system, st \u2208 Rm, stochastically generates an observation, xt \u2208 Rn, from an observation model P (xt|st). The stochastic transition model P (st+1|st) computes the predictive distribution of states at t+ 1 given the state at time t. We define the belief of a latent state st+1 as the distribution of st+1 given all the past observations up to time step t: {x1, ..., xt}, which we denote as ht."}, {"heading": "3.1. Belief Propagation in Latent State Space Models", "text": "Let us define bt as the belief P (st|ht\u22121). When the transition model P (st+1|st) and observation model P (xt|st) are known, the belief bt can be computed by a special-case of message passing called forward belief propagation:\nbt+1 = 1\nP (xt|ht\u22121) \u222b st btP (st+1|st)P (xt|st)dst. (1)\nThe above equation essentially maps the belief bt and the current observation xt to the next belief bt+1.\nConsider the following linear dynamical system:\nst+1 = Ast + s, s \u223c N (0, Q), xt = Cst + x, x \u223c N (0, R), (2)\nwhere A \u2208 Rm\u00d7m is the transition matrix, C \u2208 Rn\u00d7m is the observation matrix, and Q \u2208 Rm\u00d7m and R \u2208 Rn\u00d7n are noise covariances. The Kalman Filter (Van Overschee & De Moor, 2012) update implements the belief update in Eq. 1. Since P (st|ht\u22121) is a Gaussian distribution, we simply use the mean s\u0302t and the covariance \u03a3t to represent P (st|ht\u22121). The Kalman Filter update step can then be viewed as a function that maps (s\u0302t,\u03a3t) and the observation xt to (s\u0302t+1,\u03a3t+1), which is a nonlinear map.\nGiven the sequences of observations {xt}t generated from the linear dynamical system in Eq. 2, there are two common approaches to recover the parameters A,C,Q,R. Expectation-Maximization (EM) attempts to maximize the likelihood of the observations with respect to parameters (Roweis & Ghahramani, 1999), but suffers from locally optimal solutions. The second approach relies on Spectral Learning algorithms to recover A,C,Q,R up to a linear transformation (Van Overschee & De Moor, 2012).1 Spectral algorithms have two key characteristics: 1) they use an observable state representation; and 2) they rely on method-of-moments for parameter identification instead of likelihood. Though spectral algorithms can promise global optimality in certain cases, this desirable property does not hold under model mismatch (Kulesza et al., 2014). In this case, using the learned parameters for filtering may result in poor filtering performance."}, {"heading": "3.2. Predictive State Representations", "text": "Recently, predictive state representations and observable operator models have been used to learn from, filter on, predict, and simulate time series data (Jaeger, 2000; Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Boots & Gordon, 2011; Hefny et al., 2015). These models provide a compact and complete description of a dynamical system that is easier to learn than latent variable models, by representing state as a set of predictions of observable quantities such as future observations.\nIn this work, we follow a predictive state representation (PSR) framework and define state as the distribution of ft = [x T t , ..., x T t+k\u22121]\nT \u2208 Rkn, a k-step fixed-sized time window of future observations {xt, ..., xt+k\u22121} (Hefny et al., 2015). PSRs assume that if we can predict everything about ft at time-step t (e.g., the distribution of ft), then we also know everything there is to know about the state of a dynamical system at time step t (Singh et al., 2004). We\n1Sometimes called subspace identification (Van Overschee & De Moor, 2012) in the linear time-invariant system context.\nassume that systems we consider are k-observable2 for k \u2208 N+: there is a bijective function that maps P (st|ht\u22121) to P (ft|ht\u22121). For convenience of notation, we will present our results in terms of k-observable systems, where it suffices to select features from the next k observations.\nFollowing Hefny et al. (2015), we define the predictive state at time step t as E[\u03c6(ft)|ht\u22121] where \u03c6 is some feature function that is sufficient for the distribution P (ft|ht\u22121). The expectation is taken with respect to the distribution P (ft|ht\u22121): E[\u03c6(ft)|ht\u22121] =\u222b ft \u03c6(ft)P (ft|ht\u22121)dft. The conditional expectation can be understood as a function of which the input is the random variable ht\u22121. For example, we can set E[\u03c6(f)|ht\u22121] = E[f, ffT |ht\u22121] if P (ft|ht\u22121) is a Gaussian distribution (e.g., linear dynamical system in Eq. 2 ); or we can set \u03c6(f) = [xt \u2297 ... \u2297 xt+k\u22121] if we are working on a discrete models (discrete latent states and discrete observations), where xt is an indicator vector representation of the observation and \u2297 is the tensor product. Therefore, we assume that there exists a bijective function mapping P (f |ht\u22121) to E[\u03c6(ft)|ht\u22121]. For any test f \u2032t , we can compute the probability of P (f \u2032t |ht\u22121) by simply using the predictive state E[\u03c6(ft)|ht\u22121]. Note that the mapping from E[\u03c6(ft)|ht\u22121] to P (f \u2032t |ht\u22121) is not necessarily linear. To filter from the current predictive state E[\u03c6(ft)|ht\u22121] to the next state E[\u03c6(ft+1)|ht] conditioned on the most recent observation xt (see Fig. 1 for an illustration), PSRs additionally define an extended state E[\u03b6(ft, xt+k)|ht\u22121] =\u222b (ft,xt+k)\n\u03b6(ft, xt+k)P (ft, xt+k|ht\u22121)d(ft, xt+k), where \u03b6 is another feature function for the future observations ft and one more observation xt+k. PSRs explicitly assume there exists a linear relationship between E[\u03c6(ft)|ht\u22121] and E[\u03b6(ft, xt+k)|ht\u22121], which can be learned by Instrumental Variable Regression (IVR) (Hefny et al., 2015). PSRs then additionally assume a nonlinear conditioning operator that can compute the next predictive state with the extended state and the latest observation as inputs.\n2This assumption allows us to avoid the cryptographic hardness of the general problem (Hsu et al., 2009)."}, {"heading": "4. Predictive State Inference Machines", "text": "The original Inference Machine framework reduces the problem of learning graphical models to solving a set of classification or regression problems, where the learned classifiers mimic message passing procedures that output marginal distributions for the nodes in the model (Langford et al., 2009; Ross et al., 2011b; Bagnell et al., 2010). However, Inference Machines cannot be applied to learning latent state space models (unsupervised learning) since we do not have access to hidden states\u2019 information.\nWe tackle this problem with predictive states. By using an observable representation for state, observations in the training data can be used for supervision in the inference machine. More formally, instead of tracking the hidden state st, we focus on the corresponding predictive state E[\u03c6(ft)|ht\u22121]. Assuming that the given predictive state E[\u03c6(ft)|ht\u22121] can reveal the probability P (ft|ht\u22121), we use the training data ft to quantify how good the predictive state is by computing the likelihood of ft. The goal is to learn an operator F (the green box in Fig. 1) which deterministically passes the predictive states forward in time conditioned on the latest observation:\nE[\u03c6(ft+1)|ht] = F ( E[\u03c6(ft)|ht\u22121], xt ) , (3)\nsuch that the likelihood of the observations {ft}t being generated from the sequence of predictive states {E[\u03c6(ft)|ht\u22121]}t is maximized. In the standard PSR framework, the predictor F can be regarded as the composition of the linear mapping (from predictive state to extended state) and the conditioning operator. Below we show if we can correctly filter with predictive states, then this is equivalent to filtering with latent states as in Eq. 1."}, {"heading": "4.1. Predictive State Propagation", "text": "The belief propagation in Eq. 1 is for latent states st. We now describe the corresponding belief propagation for updating the predictive state from E[\u03c6(ft)|ht\u22121] to E[\u03c6(ft+1)|ht] conditioned on the new observation xt. Since we assume that the mapping from P (st|ht\u22121) to P (ft|ht\u22121) and the mapping from P (ft|ht\u22121) to E[\u03c6(ft)|ht\u22121] are both bijective, there must exist a bijective map q and its inverse q\u22121 such that q(P (st|ht\u22121)) = E[\u03c6(ft)|ht\u22121] and q\u22121(E[\u03c6(ft)|ht\u22121]) = P (st|ht\u22121),3 then the message passing in Eq. 1 is also equivalent to:\nE[\u03c6(ft+1)|ht] = q(P (st+1|ht)) (4) = q ( \u222b\nst\nP (st|ht\u22121)P (st+1|st)P (xt|st) P (xt|ht\u22121)\ndst )\n= q ( \u222b\nst\nq\u22121(E[\u03c6(ft)|ht\u22121])P (st+1|st)P (xt|st) P (xt|ht\u22121)\ndst )\n3The composition of two bijective functions is bijective.\nEq. 4 explicitly defines the map F that takes the inputs of E[\u03c6(ft)|ht\u22121] and xt and outputs E[\u03c6(ft+1)|ht]. This map F could be non-linear since it depends the transition model P (st+1|st), observation model P (xt|st) and function q, which are all often complicated, non-linear functions in real dynamical systems. We do not place any parametrization assumptions on the transition and observation models. Instead, we parametrize and restrict the class of predictors to encode the underlying dynamical system and aim to find a predictor F from the restricted class. We call this framework for inference the PREDICTIVE STATE INFERENCE MACHINE (PSIM).\nPSIM is different from PSRs in the following respects: (1) PSIM collapses the two steps of PSRs (predict the extended state and then condition on the latest observation) into one step\u2014as an Inference Machine\u2014for closed-loop update of predictive states; (2) PSIM directly targets the filtering task and has theoretical guarantees on the filtering performance; (3) unlike PSRs where one usually needs to utilize linear PSRs for learning purposes (Boots et al., 2011), PSIM can generalize to non-linear dynamics by leveraging non-linear regression or classification models.\nImagine that we can perform belief propagation with PSIM in predictive state space as shown in Eq. 4, then this is equivalent to classic filter with latent states as shown in Eq. 1. To see this, we can simply apply q\u22121 on both sides of the above equation Eq. 4, which exactly reveals Eq. 1. We refer readers to the Appendix for a detailed case study of the stationary Kalman Filter, where we explicitly show this equivalence. Thanks to this equivalence, we can learn accurate inference machines, even for partially observable systems. We now turn our focus on learning the map F in the predictive state space."}, {"heading": "4.2. Learning Non-stationary Filters with Predictive States", "text": "For notational simplicity, let us define trajectory as \u03c4 , which is sampled from a unknown distribution D\u03c4 . We denote the predictive state as mt = E[\u03c6(ft)|ht\u22121]. We use m\u0302t to denote an approximation of mt. Given a predictive state mt and a noisy observation ft conditioned on the history ht\u22121, we let the loss function4 d(mt, ft) = \u2016mt \u2212 \u03c6(ft)\u201622. This squares loss function can be regarded as matching moments. For instance, in the stationary Kalman filter setting, we could set mt = E[ft|ht\u22121] and d(mt, ft) = \u2016mt \u2212 ft\u201622 (matching the first moment).\n4Squared loss in an example Bregman divergence of which there are others that are optimized by the conditional expectation (Banerjee et al., 2005). We can design d(mt, ft) as negative log-likelihood, as long as it can be represented as a Bregman divergence (e.g., negative log-likelihood of distributions in exponential family).\nAlgorithm 1 PREDICTIVE STATE INFERENCE MACHINE (PSIM) with Forward Training\n1: Input: M independent trajectories \u03c4i, 1 \u2264 i \u2264M ; 2: Set m\u03021 = 1M \u2211M i=1 \u03c6(f i 1); 3: Set m\u0302i1 = m\u03021 for trajectory \u03c4i, 1 \u2264 i \u2264M ; 4: for t = 1 to T do 5: For each trajectory \u03c4i, add the input zit = (m\u0302 i t, x i t) to\nDt as feature variables and the corresponding f it+1 to Dt as the targets;\n6: Train a hypothesis Ft on Dt to minimize the loss d(F (z), f) over Dt; 7: For each trajectory \u03c4i, roll out F1, ..., Ft along the trajectory (Eq. 6) to compute m\u0302it+1; 8: end for 9: Return: the sequence of hypothesis {Ft}Nt=1.\nWe first present a algorithm for learning non-stationary filters using Forward Training (Ross & Bagnell, 2010) in Alg. 1. Forward Training learns a non-stationary filter for each time step. Namely, at time step t, forward training learns a hypothesis Ft that approximates the filtering procedure at time step t: m\u0302t+1 = Ft(m\u0302t, xt), where m\u0302t is computed by Ft\u22121(m\u0302t\u22121, xt\u22121) and so on. Let us define m\u0302it as the predictive state computed by rolling out F1, .., Ft\u22121 on trajectory \u03c4i to time step t \u2212 1. We define f it as the next k observations starting at time step t on trajectory \u03c4i. At each time step t, the algorithm collects a set of training data Dt, where the feature variables zt consist of the predictive states m\u0302it from the previous hypothesis Ft\u22121 and the local observations xit, and the targets consist of the corresponding future observations f it+1 across all trajectories \u03c4i. It then trains a new hypothesis Ft over the hypothesis class F to minimize the loss over dataset Dt. PSIM with Forward Training aims to find a good sequence of hypotheses {Ft} such that:\nmin F1\u2208F,...FT\u2208F E\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) ] , (5)\ns.t. m\u0302\u03c4t+1 = Ft(m\u0302 \u03c4 t , x \u03c4 t ),\u2200t \u2208 [1, T \u2212 1], (6)\nwhere m\u03021 = arg minm \u2211M t=1 d(m, f i 1), which is equal to 1 T \u2211T i=1 \u03c6(f i i ). Let us define \u03c9t as the joint distribution of feature variables zt and targets ft+1 after rolling out F1, ..., Ft\u22121 on the trajectories sampled from D\u03c4 . Under this definition, the filter error defined above is equivalent to 1T \u2211T t=1 E(z,f)\u223c\u03c9t [ d(Ft(z), f) ] . Note essentially the dataset Dt collected by Alg. 1 at time step t forms a finite sample estimation of \u03c9t.\nTo analyze the consistency of our algorithm, we assume every learning problem can be solved perfectly (risk minimizer finds the Bayes optimal) (Langford et al., 2009). We\nfirst show that under infinite many training trajectories, and in realizable case \u2014 the underlying true filters F \u22171 , ..., F \u2217 T are in the hypothesis class F , Alg. 1 is consistent: Theorem 4.1. With infinite many training trajectories and in the realizable case, if all learning problems are solved perfectly, the sequence of predictors F1, F2, ..., FT from Alg. 1 can generate exact predictive states E[\u03c6(f\u03c4t )|h\u03c4t\u22121] for any trajectory \u03c4 \u223c D\u03c4 and 1 \u2264 t \u2264 T .\nWe include all proofs in the appendix. Next for the agnostic case, we show that Alg. 1 can still achieve a reasonable upper bound. Let us define t = minF\u223cF E(z,f)\u223c\u03c9t [d(F (z), f)], which is the minimum batch training error under the distribution of inputs resulting from hypothesis class F . Let us define max = maxt{ t}. Under infinite many training trajectories, even in the model agnostic case, we have the following guarantees for filtering error for Alg. 1:\nTheorem 4.2. With infinite many training trajectories, for the sequence {Ft}t generated by Alg. 1, we have: E\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) ] = 1 T \u2211 t t \u2264 max.\nTheorem. 4.2 shows that the filtering error is upperbounded by the average of the minimum batch training errors from each step. If we have a rich class of hypotheses and small noise (e.g., small Bayes error), t could be small.\nTo analyze finite sample complexity, we need to split the dataset into T disjoint sets to make sure that the samples in the datasetDt are i.i.d (see details in Appendix). Hence we reduce forward training to T independent passive supervised learning problems. We have the following agnostic theoretical bound: Theorem 4.3. With M training trajectories, for any F \u2217t \u2208F ,\u2200t, we have with probability at least 1\u2212 \u03b4:\nE\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) ] \u2264 E\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(F \u2217t (m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1)\n] + 4\u03bdR\u0304(F) + 2 \u221a T ln(T/\u03b4)\n2M , (7)\nwhere v = supF,z,f 2\u2016F (z) \u2212 f\u20162, R\u0304(F) = 1T \u2211T t=1Rt(F)) andRt(F) is the Rademacher number of F under \u03c9t.\nAs one might expect, the learning problem becomes harder as T increases, however our finite sample analysis shows the average filtering error grows sublinearly as O\u0303( \u221a T ).\nAlthough Alg. 1 has nice theoretical properties, one shortcoming is that it is not very data efficient. In practice, it\nis possible that we only have small number of training trajectories but each trajectory is long (T is big). This means that we may have few training data samples (equal to the number of trajectories) for learning hypothesis Ft. Also, instead of learning non-stationary filters, we often prefer to learn a stationary filter such that we can filter indefinitely. In the next section, we present a different algorithm that utilizes all of the training data to learn a stationary filter."}, {"heading": "4.3. Learning Stationary Filters with Predictive States", "text": "The optimization framework for finding a good stationary filter F is defined as:\nmin F\u2208F\nE\u03c4\u223cD\u03c4 1\nT T\u2211 t=1 d(F (m\u0302t, xt), ft+1), (8)\ns.t m\u0302t+1 = F (m\u0302t, xt),\u2200t \u2208 [1, T \u2212 1], (9)\nwhere m\u03021 = arg minm \u2211M t=1 d(m, f i 1) = 1 T \u2211T i=1 \u03c6(f i i ). Note that the above objective function is non-convex, since m\u0302t is computed recursively and in fact is equal to F (...F (F (m\u03021, x1), x2)...), where we have t \u2212 1 nested F . As we show experimentally, optimizing this objective function via Back-Propagation likely leads to local optima. Instead, we optimize the above objective function using an iterative approach called Dataset Aggregation (DAgger) (Ross et al., 2011a) (Alg. 2). Due to the non-convexity of the objective, DAgger also will not promise global optimality. But as we will show, PSIM with DAgger gives us a sound theoretical bound for filtering error.\nGiven a trajectory \u03c4 and hypothesis F , we define m\u0302\u03c4,Ft as the predictive belief generated by F on \u03c4 at time step t. We also define z\u03c4,Ft to represent the feature variables (m\u0302\u03c4,Ft , x \u03c4 t ). At iteration n, Alg. 2 rolls out the predictive states using its current hypothesis Fn (Eq. 9) on all the given training trajectories (Line. 2). Then it collects all the feature variables {(m\u0302i,Fnt , xit)}t,i and the corresponding target variables {f it+1}t,i to form a new datasetD\u2032n and aggregates it to the original dataset Dn\u22121. Then a new hypothesis Fn is learned from the aggregated dataset Dn by minimizing the loss d(F (z), f) over Dn.\nAlg. 2 essentially utilizes DAgger to optimize the nonconvex objective in Eq. 8. By using DAgger, we can guarantee a hypothesis that, when used during filtering, performs nearly as well as when performing regression on the aggregate dataset DN . In practice, with a rich hypothesis class F and small noise (e.g., small Bayes error), small regression error is possible. We now analyze the filtering performance of PSIM with DAgger below.\nLet us fix a hypothesis F and a trajectory \u03c4 , we define \u03c9F,\u03c4 as the uniform distribution of (z, f): \u03c9F,\u03c4 = U [ (z\u03c4,F1 , f \u03c4 2 ), ..., (z \u03c4,F T , f \u03c4 T+1) ] . Now we\nAlgorithm 2 PREDICTIVE STATE INFERENCE MACHINE (PSIM) with DAgger\n1: Input: M independent trajectories \u03c4i, 1 \u2264 i \u2264M ; 2: InitializeD0 \u2190 \u2205 and initalize F0 to be any hypothesis\nin F ; 3: Initialize m\u03021 = 1M \u2211M i=1 \u03c6(f i 1) 4: for n = 0 to N do 5: Use Fn to perform belief propagation (Eq. 9) on trajectory \u03c4i, 1 \u2264 i \u2264M 6: For each trajectory \u03c4i and each time step t, add the\ninput zit = (m i,Fn t , x i t) encountered by Fn to D \u2032 n+1 as feature variables and the corresponding f it+1 to D\u2032n+1 as the targets ;\n7: Aggregate dataset Dn+1 = Dn \u222aD\u2032n+1; 8: Train a new hypothesis Fn+1 on Dn+1 to minimize the loss d(F (m,x), f); 9: end for 10: Return: the best hypothesis F\u0302 \u2208 {Fn}n on validation trajectories.\ncan rewrite the filtering error in Eq. 8 as L(F ) = E\u03c4 [Ez,f\u223c\u03c9F,\u03c4 [d(F (z), f)]|\u03c4 ]. Let us define the loss function for any predictor F at iteration n of Alg. 2 as:\nLn(F ) = E\u03c4 [Ez,f\u223c\u03c9Fn,\u03c4 [d(F (z), f)]|\u03c4 ]. (10)\nAs we can see, at iteration n, the datasetD\u2032n that we collect forms an empirical estimate of the loss Ln:\nL\u0302n(F ) = 1\nM M\u2211 \u03c4=1 Ez,f\u223c\u03c9Fn,\u03c4 ( d(F (z), f) ) . (11)\nWe first analyze the algorithm under the assumption that M = \u221e, L\u0302n(F ) = Ln(F ). Let us define Regret \u03b3N as: 1N \u2211N n=1 Ln(Fn) \u2212 minF\u2208F 1N \u2211N n=1 Ln(F ) \u2264 \u03b3N . We also define the minimum average training error N = minF\u2208F 1N \u2211N n=1 Ln(F ). Alg. 2 can be regarded as running the Follow the Leader (FTL) (Cesa-Bianchi et al., 2004; Shalev-Shwartz & Kakade, 2009; Hazan et al., 2007) on the sequence of loss functions {Ln(F )}Nn=1. When the loss function Ln(F ) is strongly convex with respect to F , FTL is no-regret in a sense that limN\u2192\u221e \u03b3N = 0. Applying Theorem 4.1 and its reduction to no-regret learning analysis from (Ross et al., 2011a) to our setting, we have the following guarantee for filtering error:\nCorollary 4.4. (Ross et al., 2011a) For Alg. 2, there exists a predictor F\u0302 \u2208 {Fn}Nn=1 such that:\nL(F\u0302 ) = E\u03c4 [ Ez,f\u223c\u03c9F\u0302 ,\u03c4 (d(F\u0302 (z), f))|\u03c4 ] \u2264 \u03b3N + N .\nAs we can see, under the assumption that Ln is strongly convex, as N \u2192 \u221e, \u03b3N goes to zero. Hence the filtering error of F\u0302 is upper bounded by the minimum batch training\nerror that could be achieved by doing regression on DN within classF . In general the term N depends on the noise of the data and the expressiveness of the hypothesis class F . Corollary. 4.4 also shows for fully realizable and noisefree case, PSIM with DAgger finds the optimal filter that drives the filtering error to zero when N \u2192\u221e. The finite sample analysis from (Ross et al., 2011a) can also be applied to PSIM. Let us define \u0302N = minF\u2208F 1N L\u0302n(F ), \u03b3\u0302N \u2265 1N \u2211N n=1 L\u0302n(Fn) \u2212\nminF\u2208F 1N \u2211N n=1 L\u0302n(F ), we have:\nCorollary 4.5. (Ross et al., 2011a) For Alg. 2, there exists a predictor F\u0302 \u2208 {Fn}Nn=1 such that with probability at least 1\u2212 \u03b4:\nL(F\u0302 ) = E\u03c4 [ Ez,f\u223c\u03c9\nF\u0302 ,\u03c4 (d(F\u0302 (z), f))|\u03c4\n] \u2264 \u03b3\u0302N + \u0302N\n+ Lmax(\n\u221a 2 ln(1/\u03b4)\nMN ). (12)"}, {"heading": "5. Experiments", "text": "We evaluate the PSIM on a variety of dynamical system benchmarks. We use two feature functions: \u03c61(ft) = [xt, ..., xt+k\u22121], which stack the k future observations together (hence the message m can be regarded as a prediction of future k observations (x\u0302t, .., x\u0302t+k\u22121)), and \u03c62(ft) = [xt, ..., xt+k\u22121, xt2, ..., x2t+k\u22121], which includes second moments (hence m represents a Gaussian distribution approximating the true distribution of future observations). To measure how good the computed predictive states are, we extract x\u0302i from m\u0302t, and evaluate \u2016x\u0302i \u2212 xi\u201622, the squared distance between the predicted observation x\u0302i and the corresponding true observation xi. We implement PSIM with DAgger using two underlying regression methods: ridge linear regression (PSIM-Lineard) and linear ridge regression with Random Fourier Features (PSIMRFFd) (Rahimi & Recht, 2007)5. We also test PSIM with back-propagation for linear regression (PSIM-Linearb). We compare our approaches to several baselines: Autoregressive models (AR), Subspace State Space System Identification (N4SID) (Van Overschee & De Moor, 2012), and PSRs implemented with IVR (Hefny et al., 2015).\n5With RFF, PSIM approximately embeds the distribution of ft into a Reproducing Kernel Hilbert Space."}, {"heading": "5.1. Synthetic Linear Dynamical System", "text": "First we tested our algorithms on a synthetic linear dynamical system (Eq. 2) with a 2-dimensional observation x. We designed the system such that it is exactly 2-observable. The sequences of observations are collected from the linear stationary Kalman filter of the LDS (Boots, 2012; Hefny et al., 2015). The details of the LDS are in Appendix.\nSince the data is collected from the stationary Kalman filter of the 2-observable LDS, we set k = 2 and use \u03c61(ft) = [xt, xt+1]. Note that the 4-dimensional predictive state E[\u03c61(ft)|ht] will represent the exact conditional distribution of observations (xt, xt+1) and therefore is equivalent to P (st|ht\u22121) (see the detailed case study for LDS in Appendix). With linear ridge regression, we test PSIM with forward training, PSIM with DAgger, and AR models (AR-k) with different lengths (k steps of past observations) of history on this dataset. For each method, we compare the average filtering error e to eF which is computed by using the underlying linear filter F of the LDS.\nFig. 2 shows the convergence trends of PSIM with DAgger, PSIM with Forward Training, and AR as the number of training trajectories N increases. The prediction error for AR with k = 5, 10, 20 is too big to fit into the plot. PSIM with DAgger performs much better with few training data while Forward Training eventually slightly surpasses DAgger with sufficient data. The AR-k models need long histories to perform well given data gnereated by latent state space models, even for this 2-observable LDS. Note AR-\n35 performs regression in a 70-dimensional feature space (35 past observations), while PSIM only uses 6-d features (4-d predictive state + 2-d current observation). This shows that predictive state is a more compact representation of the history and can reduce the complexity of learning problem."}, {"heading": "5.2. Real Dynamical Systems", "text": "We consider the following three real dynamical systems: (1) Robot Drill Assembly: the dataset consists of 96 sensor telemetry traces, each of length 350, from a robotic manipulator assembling the battery pack on a power drill. The 13 dimensional noisy observations consist of the robot arm\u2019s 7 joint torques as well as the the 3D force and torque vectors. Note the fixed higher level control policy for the drill assembly task is not given in the observations and must be learned as part of the dynamics; (2) Human Motion Capture: the dataset consists of 48 skeletal tracks of 300 timesteps each from a Vicon motion capture system from three human subjects performing walking actions. The observations consist of the 3D positions of the various skeletal parts (e.g. upperback, thorax, clavicle, etc.); (3) Video Textures: the datasets consists of one video of flag waving and the other one of waves on a beach.\nFor these dynamical systems, we do not test PSIM with Forward Training since our benchmarks have a large number of time steps per trajectory. Throughout the experiments, we set k = 5 for all datasets except for video textures, where we set k = 3. For each dataset, we randomly pick a small number of trajectories as a validation set for parameter tuning (e.g., ridge, rank for N4SID and IVR, band width for RFF). We partition the whole dataset into ten folds, train all algorithms on 9 folds and test on 1 fold. For the feature function \u03c61, the average one-step filtering errors and its standard deviations across ten folds are shown in Tab. 1. Our approaches outperforms the two baselines across all datasets. Since the datasets are generated from complex dynamics, PSIM with RFF exhibits better performance than PSIM with Linear. This experimentally supports our theorems suggesting that with powerful regressors, PSIM could perform better. We implement PSIM with back-propagation using Theano with several\ntraining approaches: gradient descent with step decay, RMSProp (Tieleman & Hinton, 2012) and AdaDelta (Zeiler, 2012) (see Appendix. E). With random initialization, backpropagation does not achieve comparable performance, except on the flag video, due to local optimality.We observe marginal improvement by using back-propogation to refine the solution from DAgger. This shows PSIM with DAgger finds good models by itself (details in Appendix. E). We also compare these approaches for multi-step look ahead (Fig. 3). PSIM consistently outperforms the two baselines.\nTo show predictive states with larger k encode more information about latent states, we additionally run PSIM with k = 1 using \u03c61 . PSIM (DAgger) with k = 5 outperforms k = 1 by 5% for robot assembly dataset, 6% for motion capture, 8% for flag and 32% for beach video. Including belief over longer futures into predictive states can thus capture more information and increase the performance.\nFor feature function \u03c62 and k = 5, with linear ridge regression, the 1-step filter error achieved by PSIM with DAgger across all datasets are: 2.05\u00b1 0.08 on Robot Drill Assembly, 5.47\u00b1 0.42 on motion capture, 154.02\u00b1 9.9 on beach video, and 1.27e3\u00b1 13e1 on flag video. Comparing to the results shown in the PSIM-Lineard in column of Table. 1, we achieve slightly better performance on all datasets, and noticeably better performance on the beach video texture."}, {"heading": "6. Conclusion", "text": "We introduced PREDICTIVE STATE INFERENCE MACHINES, a novel approach to directly learn to filter with latent state space models. Leveraging ideas from PSRs, PSIM reduces the unsupervised learning of latent state space models to a supervised learning setting and guarantees filtering performance for general non-linear models in both the realizable and agnostic settings."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported in part by: DARPA ALIAS contract number HR0011-15-C-0027 and National Science Foundation Graduate Research Fellowship Grant No. DGE-1252522. The authors also thank Geoff Gordon for valuable discussions."}, {"heading": "A. Proof of Theorem. 4.1", "text": "Proof. We prove the theorem by induction. We start from t = 1. Under the assumption of infinite many training trajectories, m\u03021 is exactly equal to m1, which is E\u03c4 (\u03c6(f1)) (no observations yet, conditioning on nothing).\nNow let us assume at time step t, we have all computed m\u0302\u03c4j equals to m \u03c4 j for 1 \u2264 j \u2264 t on any trajectory \u03c4 . Under the assumption of infinite training trajectories, minimizing the empirical risk over Dt is equivalent to minimizing the true risk E\u03c4 [d(F (m\u03c4t , x\u03c4t ), f\u03c4t+1)]. Since we use sufficient features for distribution P (ft|ht\u22121) and we assume the system is k-observable, there exists a underlying deterministic map, which we denote as F \u2217t here, that maps m\u03c4t and x\u03c4t to m\u03c4t+1 (Eq. 4 represents F \u2217t ). Without loss of generality, for any \u03c4 , conditioned on the history h\u03c4t , we have that for a noisy observation f\u03c4t :\n\u03c6(f\u03c4t+1)|h\u03c4t = E[\u03c6(f\u03c4t+1)|h\u03c4t ] + (13) = m\u03c4t+1 + (14)\n= F \u2217t (m \u03c4 t , x \u03c4 t ) + , (15)\nwhere E[ ] = 0. Hence we have that F \u2217t is the operator of conditional expectation E[ ( \u03c6(ft+1)|ht ) |mt, xt], which exactly computes the predictive state mt+1 = E[\u03c6(f\u03c4t+1)|h\u03c4t ], given m\u03c4t and x\u03c4t on any trajectory \u03c4 .\nSince the loss d is a squared loss (or any other loss that can be represented by Bregman divergence), the minimizer of the true risk will be the operator of conditional expectation E[ ( \u03c6(ft+1)|ht ) |mt, xt]. Since it is equal to F \u2217 and we have F \u2217 \u2208 F due to the realizable assumption, the risk minimization at step t exactly finds F \u2217t . Using m\u0302\u03c4t (equals to m\u03c4t based on the induction assumption for step t), and x\u03c4t , the risk minimizer F \u2217 then computes the exact m\u03c4t+1 for time step t + 1. Hence by the induction hypothesis, we prove the theorem."}, {"heading": "B. Proof of Theorem. 4.2", "text": "Under the assumption of infinitely many training trajectories, we can represent the objective as follows:\nE\u03c4\u223cD 1\nT T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) = 1 T T\u2211 t=1 E(z,f)\u223c\u03c9t [ d(Ft(z), f) ] (16)\nNote that each Ft is trained by minimizing the risk:\nFt = arg min F\u223cF\nE(z,f)\u223c\u03c9t [ d(F (z), f) ] . (17)\nSince we define t = minF\u223cF E(z,f)\u223c\u03c9t [ d(F (z), f) ] , we have:\nE\u03c4\u223cD 1\nT T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) = 1 T T\u2211 t=1 E(z,f)\u223c\u03c9t [ d(Ft(z), f) ] \u2264 1 T \u2211 t t. (18)\nDefining max = maxt{ t}, we prove the theorem."}, {"heading": "C. Proof of Theorem. 4.3", "text": "Proof. Without loss of generality, let us assume the loss d(F (z), f) \u2208 [0, 1]. To derive generalization bound using Rademacher complexity, we assume that \u2016F (z)\u20162 and \u2016f\u20162 are bounded for any z, f, F \u2208 F , which makes sure that d(F (z), f) will be Lipschitz continuous with respect to the first term F (z)6.\nGiven M samples, we further assume that we split M samples into T disjoint sets S1, ..., ST , one for each training process of Fi, for 1 \u2264 i \u2264 T . The above assumption promises that the data St for training each filter Ft is i.i.d. Note that each Si now contains M/T i.i.d trajectories.\nSince we assume that at time step t, we use St (rolling out F1, ..., Ft\u22121 on trajectories in St) for training Ft, we can essentially treat each training step independently: when learning Ft, the training data z, f are sampled from \u03c9t and are i.i.d.\nNow let us consider time step t. With the learned F1, ..., Ft\u22121, we roll out them on the trajectories in St to get MT i.i.d samples of (z, f) \u223c \u03c9t. Hence, training Ft on these MT i.i.d samples becomes classic empirical risk minimization problem. Let us define loss class as L = {lF : (z, f) \u2192 d(F (z), f) : F \u2208 F}, which is determined by F and d. Without loss of generality, we assume l(z, f) \u2208 [0, 1], \u2200l \u2208 L. Using the uniform bound from Rademacher theorem (Mohri et al., 2012), we have for any F \u2208 F , with\n6Note that in fact for the squared loss, d is 1-smooth with respect to its first item. In fact we can remove the boundness assumption here by utilizing the existing Rademacher complexity analysis for smooth loss functions (Srebro et al., 2010).\nprobability at least 1\u2212 \u03b4\u2032:\nEz,f\u223c\u03c9t [d(F (z), f)]\u2212 T\nM \u2211 i d(F (zi), f i)| (19)\n\u2264 2Rt(L) + \u221a T ln(1/\u03b4\u2032)\n2M , (20)\nwhere Rt(L) is Rademacher complexity of the loss class L with respect to distribution \u03c9t. Since we have Ft is the empirical risk minimizer, for any F \u2217t \u2208 F , we have with probability at least 1\u2212 \u03b4\u2032:\nEz,f\u223c\u03c9t [d(Ft(z), f)] \u2264 Ez,f\u223c\u03c9t [d(F \u2217 t (z i), f i)] + 4Rt(L) + 2 \u221a T ln(1/\u03b4\u2032)\n2M . (21)\nNow let us combine all time steps together. For any F \u2217t \u2208 F , \u2200t, with probability at least (1\u2212 \u03b4\u2032)T , we have:\nE\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(Ft(m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) ] = 1 T T\u2211 t=1 Ez,f\u223cdt [ d(Ft(z), f) ] \u2264 1 T T\u2211 t=1 Ez,f\u223c\u03c9t [d(F \u2217 t (z), f)] + 4R\u0304(L) + 2 \u221a T ln(1/\u03b4\u2032) 2M\n= E\u03c4\u223cD\u03c4 [ 1 T T\u2211 t=1 d(F \u2217t (m\u0302 \u03c4 t , x \u03c4 t ), f \u03c4 t+1) ] + 4R\u0304(L) + 2\n\u221a T ln(1/\u03b4\u2032)\n2M , (22)\nwhere R\u0304(L) = (1/T ) \u2211T t=1Rt(L) is the average Rademacher complexity. Inequality. 22 is derived from the fact the event that the above inequality holds can be implied by the event that Inequality. 21 holds for every time step t (1 \u2264 t \u2264 T ) independently. The probability of Inequality. 21 holds for all t is at least (1\u2212 \u03b4\u2032)T .\nNote that in our setting d(F (z), f) = \u2016F (z)\u2212f\u201622, and under our assumptions that \u2016F (z)\u20162 and \u2016f\u20162 are bounded for any z, f, F \u2208 F , d(F (z), f) is Lipschitz continuous with respect to its first item with Lipschitz constant equal to \u03bd, which is supF,z,f 2\u2016F (z) \u2212 f\u20162. Hence, from the composition property of Rademacher number (Mohri et al., 2012), we have:\nRt(L) \u2264 \u03bdRt(F), \u2200t. (23)\nIt is easy to verify that for T \u2265 1, \u03b4\u2032 \u2208 (0, 1), we have (1\u2212 \u03b4\u2032)T \u2265 1\u2212 T\u03b4\u2032. Let 1\u2212 T\u03b4\u2032 = 1\u2212 \u03b4, and solve for \u03b4\u2032, we get \u03b4\u2032 = \u03b4/T . Substitute Eq. 23 and \u03b4\u2032 = \u03b4/T into Eq. 22, we prove the theorem.\nNote that the above theorem shows that for fixed number training examples, the generalization error increase as O\u0303( \u221a T ) (sublinear with respect to T )."}, {"heading": "D. Case Study: Stationary Kalman Filter", "text": "To better illustrate PSIM, we consider a special dynamical system in this section. More specifically, we focus on the stationary Kalman filter (Boots, 2012; Hefny et al., 2015) 7:\nst+1 = Ast + s, s \u223c N (0, Q), xt = Cst + x, x \u223c N (0, R). (24)\nAs we will show, the Stationary Kalman Filter allows us to explicitly represent the predictive states (sufficient statistics of the distributions of future observations are simple). We will also show that we can explicitly construct a bijective map between the predictive state space and the latent state space, which further enables us to explicitly construct the predictive state filter. We will show that the predictive state filter is closely related to the original filter in the latent state space.\nThe k-observable assumption here essentially means that the observability matrix: O = [ C CA CA2 ... CAk\u22121 ]> is full (column) rank. Now let us define P (st|ht\u22121) = N (s\u0302t,\u03a3s), and P (ft|ht\u22121) = N (f\u0302t,\u03a3f ). Note that \u03a3s is a constant for a stationary Kalman filter (the Kalman gain is converged). Since \u03a3f is purely determined by \u03a3s, A, C, R, Q, it is also a constant. It is clear now\n7For a well behaved system, the filter will become stationary (Kalman gain converges) after running for some period of time. Our definition here is slightly different from the classic Kalman filter: we focus on filtering from P (st|ht\u22121) (without conditioning on the observation xt generated from st) to P (st+1|ht), while traditional Kalman filter usually filters from P (st|ht) to P (st+1|ht+1).\nthat f\u0302t = Os\u0302t. When the Kalman filter becomes stationary, it is enough to keep tracking s\u0302t. Note that here, given s\u0302t, we can compute f\u0302t; and given f\u0302t, we can reveal s\u0302t as O\u2020f\u0302t, where O\u2020 is the pseudo-inverse of O. This map is bijective since O is full column rank due to the k-observability.\nNow let us take a look at the update of the stationary Kalman filter:\ns\u0302t+1 = As\u0302t \u2212A\u03a3sCT (C\u03a3sCT +R)\u22121(Cs\u0302t \u2212 xt) = As\u0302t \u2212 L(Cs\u0302t \u2212 xt), (25)\nwhere we define L = A\u03a3sCT (C\u03a3sCT +R)\u22121. Here due to the stationary assumption, \u03a3s keeps constant across time steps. Multiple O on both sides and plug in O\u2020O, which is an identity, at proper positions, we have:\nf\u0302t+1 = Os\u0302t+1 = OA(O\u2020O)s\u0302t \u2212OL(CO\u2020Os\u0302t \u2212 xt)\n= OAO\u2020f\u0302t \u2212OL(CO\u2020f\u0302t \u2212 xt) = A\u0303f\u0302t \u2212 L\u0303(C\u0303f\u0302t \u2212 xt) (26)\n= [ A\u0303\u2212 L\u0303C\u0303 L\u0303 ] [f\u0302t xt ] , (27)\nwhere we define A\u0303 = OAO\u2020, C\u0303 = CO\u2020 and L\u0303 = OL. The above equation represents the stationary filter update step in predictive state space. Note that the deterministic map from (f\u0302t,\u03a3f ) and xt to (f\u0302t+1,\u03a3f ) is a linear map (F defined in Sec. 4 is a linear function with respect to f\u0302t and xt). The filter update in predictive state space is very similar to the filter update in the original latent state space except that predictive state filter uses operators (A\u0303, C\u0303, Q\u0303) that are linear transformations of the original operators (A,C,Q).\nWe can do similar linear algebra operations (e.g., multiply O and plug in O\u2020O in proper positions) to recover the stationary filter in the original latent state space from the stationary predictive state filter. The above analysis leads to the following proposition: Proposition D.1. For a linear dynamical system with k-observability, there exists a filter in predictive state space (Eq. 27) that is equivalent to the stationary Kalman filter in the original latent state space (Eq. 25).\nWe just showed a concrete bijective map between the filter with predictive states and the filter with the original latent states by utilizing the observability matrix O. Though we cannot explicitly construct the bijective map unless we know the parameters of the LDS (A,B,C,Q,R), we can see that learning the linear filter shown in Eq. 27 is equivalent to learning the original linear filter in Eq. 25 in a sense that the predictive beliefs filtered from Eq. 27 encodes as much information as the beliefs filtered from Eq. 25 due to the existence of a bijective map between predictive states and the beliefs for latent states.\nD.1. Collection of Synthetic Data\nWe created a linear dynamical system with A \u2208 R3\u00d73, C \u2208 R2\u00d73, Q \u2208 R3\u00d73, R \u2208 R2\u00d72. The matrix A is full rank and its largest eigenvalue is less than 1. The LDS is 2-observable. We computed the constance covariance matrix \u03a3s, which is a fixed point of the covariance update step in the Kalman filter. The initial distribution of s0 is set to N (1,\u03a3s). We then randomly sampled 50000 observation trajectories from the LDS. We use half of the trajectories for training and the left half for testing."}, {"heading": "E. Additional Experiments", "text": "With linear regression as the underlying filter model: m\u0302t+1 = W [m\u0302Tt , xTt ]T , where W is a 2-d matrix, we compare PSIM with backpropagation using the solutions from DAgger as initialization to PSIM with DAgger, and PSIM with back-propagation with random initialization. We implemented PSIM with Back-propagation in Theano (Bastien et al., 2012). For random initialization, we uniformly sample non-zero small matrices to avoid gradient blowing up. For training, we use mini-batch gradient descent where each trajectory is treated as a batch. We tested several different gradient descent approaches: regular gradient descent with step decay, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012). We report the best performance from the above approaches. When using the solutions from PSIM with DAgger as an initialization for back-propagation, we use the same setup. We empirically find that RMSProp works best across all our datasets for the inference machine framework, while regular gradient descent generally performs the worst.\nTab. 2 shows the results of using different training methods with ridge linear regression as the underlying model.\nAdditionally, we test back-propagation for PSIM with Kernel Ridge regression as the underlying model: m\u0302t+1 = W\u03b7(m\u0302t, xt), where \u03b7 is a pre-defined, deterministic feature function that maps (m\u0302t, xt) to a reproducing kernel Hilbert space approximated with Random\nFourier Features (RFF). Essentially, we lift the inputs (m\u0302t, xt) into a much richer feature space (a scaled, and transition invariant feature space) before feeding it to the next module. The results are shown in Table. 3. As we can see, with RFF, back-propagation achieves better performance than back-propagation with simple linear regression (PSIM-Linear (Bp)). This is expected since using RFF potentially captures the non-linearity in the underlying dynamical systems. On the other hand, PSIM with DAgger achieves better results than back-propagation across all the datasets. This result is consistent with the one from PSIM with ridge linear regression.\nOverall, several interesting observations are: (1) back-propagation with random initialization achieves reasonable performance (e.g., good performance on flag video compared to baselines), but worse than the performance of PSIM with DAgger. PSIM back-propagation is likely stuck at locally optimal solutions in some of our datasets; (2) PSIM with DAgger and Back-propagation can be symbiotically beneficial: using back-propagation to refine the solutions from PSIM with DAgger improves the performance. Though the improvement seems not significant over the 400 epochs we ran, we do observe that running more epochs continues to improve the results; (3) this actually shows that PSIM with DAgger itself finds good filters already, which is not surprising because of the strong theoretical guarantees that it has."}], "references": [{"title": "Learning deep inference machines", "author": ["Bagnell", "J Andrew", "Grubb", "Alex", "Munoz", "Daniel", "Ross", "Stephane"], "venue": "The Learning Workshop,", "citeRegEx": "Bagnell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2010}, {"title": "On the optimality of conditional expectation as a bregman predictor", "author": ["Banerjee", "Arindam", "Guo", "Xin", "Wang", "Hui"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Spectral Approaches to Learning Predictive Representations", "author": ["Boots", "Byron"], "venue": "PhD thesis,", "citeRegEx": "Boots and Byron.,? \\Q2012\\E", "shortCiteRegEx": "Boots and Byron.", "year": 2012}, {"title": "Predictive state temporal difference learning", "author": ["Boots", "Byron", "Gordon", "Geoffrey J"], "venue": "In NIPS,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Cesa-Bianchi", "Nicolo", "Conconi", "Alex", "Gentile", "Claudio"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Learning for control from multiple demonstrations", "author": ["Coates", "Adam", "Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Coates et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan", "Elad", "Agarwal", "Amit", "Kale", "Satyen"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Supervised learning for dynamical system learning", "author": ["Hefny", "Ahmed", "Downey", "Carlton", "Gordon", "Geoffrey J"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hefny et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hefny et al\\.", "year": 2015}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Hsu", "Daniel", "M. Kakade", "Sham", "Zhang", "Tong"], "venue": "In COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Jaeger", "Herbert"], "venue": "Neural Computation,", "citeRegEx": "Jaeger and Herbert.,? \\Q2000\\E", "shortCiteRegEx": "Jaeger and Herbert.", "year": 2000}, {"title": "Low-rank spectral learning", "author": ["Kulesza", "Alex", "Rao", "N Raj", "Singh", "Satinder"], "venue": "In Proceedings of the 17th Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kulesza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2014}, {"title": "Learning nonlinear dynamic models", "author": ["Langford", "John", "Salakhutdinov", "Ruslan", "Zhang", "Tong"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Deeply learning the messages in message passing inference", "author": ["Lin", "Guosheng", "Shen", "Chunhua", "Reid", "Ian", "van den Hengel", "Anton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Predictive representations of state", "author": ["Littman", "Michael L", "Sutton", "Richard S", "Singh", "Satinder"], "venue": "In NIPS,", "citeRegEx": "Littman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2001}, {"title": "Foundations of machine learning", "author": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Random features for largescale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Pose machines: Articulated pose estimation via inference machines", "author": ["Ramakrishna", "Varun", "Munoz", "Daniel", "Hebert", "Martial", "Bagnell", "James Andrew", "Sheikh", "Yaser"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Ramakrishna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramakrishna et al\\.", "year": 2014}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "J. Andrew"], "venue": "In AISTATS, pp", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to noregret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "J.Andrew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross", "Stephane", "Munoz", "Daniel", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In CVPR,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "A unifying review of linear gaussian models", "author": ["Roweis", "Sam", "Ghahramani", "Zoubin"], "venue": "Neural computation,", "citeRegEx": "Roweis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 1999}, {"title": "Mind the duality gap: Logarithmic regret algorithms for online optimization", "author": ["Shalev-Shwartz", "Shai", "Kakade", "Sham M"], "venue": "In NIPS, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Predictive state representations: A new theory for modeling dynamical systems", "author": ["Singh", "Satinder", "James", "Michael R", "Rudary", "Matthew R"], "venue": "In UAI,", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Optimistic rates for learning with a smooth loss", "author": ["Srebro", "Nathan", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "arXiv preprint arXiv:1009.3896,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Subspace identification for linear systems: TheoryImplementationApplications", "author": ["Van Overschee", "Peter", "De Moor", "BL"], "venue": "Springer Science & Business Media,", "citeRegEx": "Overschee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Overschee et al\\.", "year": 2012}, {"title": "Improving multi-step prediction of learned time series models", "author": ["Venkatraman", "Arun", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": null, "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Time series analysis", "author": ["Wei", "William Wu-Shyong"], "venue": "Addison-Wesley publication,", "citeRegEx": "Wei and Wu.Shyong.,? \\Q1994\\E", "shortCiteRegEx": "Wei and Wu.Shyong.", "year": 1994}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "F}, which is determined by F and d. Without loss of generality, we assume l(z", "author": [], "venue": "{lF : (z,", "citeRegEx": "\u2208,? \\Q2012\\E", "shortCiteRegEx": "\u2208", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Spectral Learning methods are a popular alternative to MLE for learning models of dynamical systems (Boots, 2012; Boots et al., 2011; Hsu et al., 2009; Hefny et al., 2015).", "startOffset": 100, "endOffset": 171}, {"referenceID": 11, "context": "Spectral Learning methods are a popular alternative to MLE for learning models of dynamical systems (Boots, 2012; Boots et al., 2011; Hsu et al., 2009; Hefny et al., 2015).", "startOffset": 100, "endOffset": 171}, {"referenceID": 10, "context": "Spectral Learning methods are a popular alternative to MLE for learning models of dynamical systems (Boots, 2012; Boots et al., 2011; Hsu et al., 2009; Hefny et al., 2015).", "startOffset": 100, "endOffset": 171}, {"referenceID": 4, "context": "Spectral Learning methods are a popular alternative to MLE for learning models of dynamical systems (Boots, 2012; Boots et al., 2011; Hsu et al., 2009; Hefny et al., 2015). This family of algorithms provides theoretical guarantees on discovering the global optimum for the model parameters under the assumptions of infinite training data and realizability. However, in the non-realizable setting \u2014 i.e. model mismatch (e.g., using learned parameters of a Linear Dynamical System (LDS) model for a non-linear dynamical system) \u2014 these algorithms lose any performance guarantees on using the learned model for filtering or other inference tasks. For example, Kulesza et al. (2014) shows when the model rank is lower than the rank of the underlying dynamical system, the inference performance of the learned model may be arbitrarily bad.", "startOffset": 114, "endOffset": 679}, {"referenceID": 18, "context": "of Ross et al. (2011b); Ramakrishna et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 18, "context": "(2011b); Ramakrishna et al. (2014); Lin et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 15, "context": "(2014); Lin et al. (2015). Inference machines do not parametrize the graphical model (e.", "startOffset": 8, "endOffset": 26}, {"referenceID": 16, "context": "To generalize Inference Machines to dynamical systems with latent states, we leverage ideas from Predictive State Representations (PSRs) (Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Hefny et al., 2015).", "startOffset": 137, "endOffset": 219}, {"referenceID": 25, "context": "To generalize Inference Machines to dynamical systems with latent states, we leverage ideas from Predictive State Representations (PSRs) (Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Hefny et al., 2015).", "startOffset": 137, "endOffset": 219}, {"referenceID": 4, "context": "To generalize Inference Machines to dynamical systems with latent states, we leverage ideas from Predictive State Representations (PSRs) (Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Hefny et al., 2015).", "startOffset": 137, "endOffset": 219}, {"referenceID": 10, "context": "To generalize Inference Machines to dynamical systems with latent states, we leverage ideas from Predictive State Representations (PSRs) (Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Hefny et al., 2015).", "startOffset": 137, "endOffset": 219}, {"referenceID": 29, "context": "Data as Demonstrator (DaD) (Venkatraman et al., 2015) applies the Inference Machine idea to fully observable Markov chains, and directly optimizes the open-loop forward prediction accuracy.", "startOffset": 27, "endOffset": 53}, {"referenceID": 13, "context": "Though spectral algorithms can promise global optimality in certain cases, this desirable property does not hold under model mismatch (Kulesza et al., 2014).", "startOffset": 134, "endOffset": 156}, {"referenceID": 16, "context": "Recently, predictive state representations and observable operator models have been used to learn from, filter on, predict, and simulate time series data (Jaeger, 2000; Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Boots & Gordon, 2011; Hefny et al., 2015).", "startOffset": 154, "endOffset": 272}, {"referenceID": 25, "context": "Recently, predictive state representations and observable operator models have been used to learn from, filter on, predict, and simulate time series data (Jaeger, 2000; Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Boots & Gordon, 2011; Hefny et al., 2015).", "startOffset": 154, "endOffset": 272}, {"referenceID": 4, "context": "Recently, predictive state representations and observable operator models have been used to learn from, filter on, predict, and simulate time series data (Jaeger, 2000; Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Boots & Gordon, 2011; Hefny et al., 2015).", "startOffset": 154, "endOffset": 272}, {"referenceID": 10, "context": "Recently, predictive state representations and observable operator models have been used to learn from, filter on, predict, and simulate time series data (Jaeger, 2000; Littman et al., 2001; Singh et al., 2004; Boots et al., 2011; Boots & Gordon, 2011; Hefny et al., 2015).", "startOffset": 154, "endOffset": 272}, {"referenceID": 10, "context": ", xt+k\u22121} (Hefny et al., 2015).", "startOffset": 10, "endOffset": 30}, {"referenceID": 25, "context": ", the distribution of ft), then we also know everything there is to know about the state of a dynamical system at time step t (Singh et al., 2004).", "startOffset": 126, "endOffset": 146}, {"referenceID": 10, "context": "PSRs explicitly assume there exists a linear relationship between E[\u03c6(ft)|ht\u22121] and E[\u03b6(ft, xt+k)|ht\u22121], which can be learned by Instrumental Variable Regression (IVR) (Hefny et al., 2015).", "startOffset": 168, "endOffset": 188}, {"referenceID": 10, "context": "Following Hefny et al. (2015), we define the predictive state at time step t as E[\u03c6(ft)|ht\u22121] where \u03c6 is some feature function that is sufficient for the distribution P (ft|ht\u22121).", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "This assumption allows us to avoid the cryptographic hardness of the general problem (Hsu et al., 2009).", "startOffset": 85, "endOffset": 103}, {"referenceID": 14, "context": "The original Inference Machine framework reduces the problem of learning graphical models to solving a set of classification or regression problems, where the learned classifiers mimic message passing procedures that output marginal distributions for the nodes in the model (Langford et al., 2009; Ross et al., 2011b; Bagnell et al., 2010).", "startOffset": 274, "endOffset": 339}, {"referenceID": 0, "context": "The original Inference Machine framework reduces the problem of learning graphical models to solving a set of classification or regression problems, where the learned classifiers mimic message passing procedures that output marginal distributions for the nodes in the model (Langford et al., 2009; Ross et al., 2011b; Bagnell et al., 2010).", "startOffset": 274, "endOffset": 339}, {"referenceID": 4, "context": "PSIM is different from PSRs in the following respects: (1) PSIM collapses the two steps of PSRs (predict the extended state and then condition on the latest observation) into one step\u2014as an Inference Machine\u2014for closed-loop update of predictive states; (2) PSIM directly targets the filtering task and has theoretical guarantees on the filtering performance; (3) unlike PSRs where one usually needs to utilize linear PSRs for learning purposes (Boots et al., 2011), PSIM can generalize to non-linear dynamics by leveraging non-linear regression or classification models.", "startOffset": 444, "endOffset": 464}, {"referenceID": 1, "context": "Squared loss in an example Bregman divergence of which there are others that are optimized by the conditional expectation (Banerjee et al., 2005).", "startOffset": 122, "endOffset": 145}, {"referenceID": 14, "context": "To analyze the consistency of our algorithm, we assume every learning problem can be solved perfectly (risk minimizer finds the Bayes optimal) (Langford et al., 2009).", "startOffset": 143, "endOffset": 166}, {"referenceID": 6, "context": "2 can be regarded as running the Follow the Leader (FTL) (Cesa-Bianchi et al., 2004; Shalev-Shwartz & Kakade, 2009; Hazan et al., 2007) on the sequence of loss functions {Ln(F )}n=1.", "startOffset": 57, "endOffset": 135}, {"referenceID": 9, "context": "2 can be regarded as running the Follow the Leader (FTL) (Cesa-Bianchi et al., 2004; Shalev-Shwartz & Kakade, 2009; Hazan et al., 2007) on the sequence of loss functions {Ln(F )}n=1.", "startOffset": 57, "endOffset": 135}, {"referenceID": 10, "context": "We compare our approaches to several baselines: Autoregressive models (AR), Subspace State Space System Identification (N4SID) (Van Overschee & De Moor, 2012), and PSRs implemented with IVR (Hefny et al., 2015).", "startOffset": 190, "endOffset": 210}, {"referenceID": 10, "context": "The sequences of observations are collected from the linear stationary Kalman filter of the LDS (Boots, 2012; Hefny et al., 2015).", "startOffset": 96, "endOffset": 129}], "year": 2016, "abstractText": "Latent state space models are a fundamental and widely used tool for modeling dynamical systems. However, they are difficult to learn from data and learned models often lack performance guarantees on inference tasks such as filtering and prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE (PSIM), a data-driven method that considers the inference procedure on a dynamical system as a composition of predictors. The key idea is that rather than first learning a latent state space model, and then using the learned model for inference, PSIM directly learns predictors for inference in predictive state space. We provide theoretical guarantees for inference, in both realizable and agnostic settings, and showcase practical performance on a variety of simulated and real world robotics benchmarks.", "creator": "LaTeX with hyperref package"}}}