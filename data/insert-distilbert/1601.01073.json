{"id": "1601.01073", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "abstract": "we propose traditional multi - coupled way, multilingual neural machine translation. the proposed approach enables basically a single neural translation model to translate between theoretically multiple languages, with a number of parameters that ultimately grows only linearly with the number codes of languages. this is made possible by having a single attention mechanism that transmission is shared across all language pairs. we train the proposed multi - way, multilingual linguistics model on ten language pairs from wmt'15 simultaneously and observe clear performance improvements over models trained on only one language pair. in particular, we observe that the proposed model significantly improves with the computer translation quality model of low - resource language pairs.", "histories": [["v1", "Wed, 6 Jan 2016 04:00:50 GMT  (77kb,D)", "http://arxiv.org/abs/1601.01073v1", null]], "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["orhan firat", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1601.01073"}, "pdf": {"name": "1601.01073.pdf", "metadata": {"source": "CRF", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["orhan.firat@ceng.metu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation It has been shown that a deep (recurrent) neural network can successfully learn a complex mapping between variablelength input and output sequences on its own. Some of the earlier successes in this task have, for instance, been handwriting recognition (Bottou et al., 1997; Graves et al., 2009) and speech recognition (Graves et al., 2006; Chorowski et al., 2015). More recently, a general framework of encoderdecoder networks has been found to be effective at learning this kind of sequence-to-sequence mapping by using two recurrent neural networks (Cho et al., 2014b; Sutskever et al., 2014).\nA basic encoder-decoder network consists of two recurrent networks. The first network, called an encoder, maps an input sequence of variable length\ninto a point in a continuous vector space, resulting in a fixed-dimensional context vector. The other recurrent neural network, called a decoder, then generates a target sequence again of variable length starting from the context vector. This approach however has been found to be inefficient in (Cho et al., 2014a) when handling long sentences, due to the difficulty in learning a complex mapping between an arbitrary long sentence and a single fixed-dimensional vector.\nIn (Bahdanau et al., 2014), a remedy to this issue was proposed by incorporating an attention mechanism to the basic encoder-decoder network. The attention mechanism in the encoder-decoder network frees the network from having to map a sequence of arbitrary length to a single, fixed-dimensional vector. Since this attention mechanism was introduced to the encoder-decoder network for machine translation, neural machine translation, which is purely based on neural networks to perform full end-to-end translation, has become competitive with the existing phrase-based statistical machine translation in many language pairs (Jean et al., 2015; Gulcehre et al., 2015; Luong et al., 2015b).\nMultilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language. This kind of mapping is strictly specific to a given language pair, and it is not trivial to extend this mapping to work on multiple pairs of languages.\nA system based on neural machine translation, on the other hand, can be decomposed into two mod-\nar X\niv :1\n60 1.\n01 07\n3v 1\n[ cs\n.C L\n] 6\nJ an\n2 01\nules. The encoder maps a source sentence into a continuous representation, either a fixed-dimensional vector in the case of the basic encoder-decoder network or a set of vectors in the case of attentionbased encoder-decoder network. The decoder then generates a target translation based on this source representation. This makes it possible conceptually to build a system that maps a source sentence in any language to a common continuous representation space and decodes the representation into any of the target languages, allowing us to make a multilingual machine translation system.\nThis possibility is straightforward to implement and has been validated in the case of basic encoderdecoder networks (Luong et al., 2015a). It is however not so, in the case of the attention-based encoder-decoder network, as the attention mechanism, or originally called the alignment function in (Bahdanau et al., 2014), is conceptually language pair-specific. In (Dong et al., 2015), the authors cleverly avoided this issue of language pair-specific attention mechanism by considering only a one-tomany translation, where each target language decoder embedded its own attention mechanism. Also, we notice that both of these works have only evaluated their models on relatively small-scale tasks, making it difficult to assess whether multilingual neural machine translation can scale beyond lowresource language translation.\nMulti-Way, Multilingual Neural Machine Translation In this paper, we first step back from the currently available multilingual neural translation systems proposed in (Luong et al., 2015a; Dong et al., 2015) and ask the question of whether the attention mechanism can be shared across multiple language pairs. As an answer to this question, we propose an attention-based encoder-decoder network that admits a shared attention mechanism with multiple encoders and decoders. We use this model for all the experiments, which suggests that it is indeed possible to share an attention mechanism across multiple language pairs.\nThe next question we ask is the following: in which scenario would the proposed multi-way, multilingual neural translation have an advantage over the existing, single-pair model? Specifically, we consider a case of the translation between a low-\nresource language pair. The experiments show that the proposed multi-way, multilingual model generalizes better than the single-pair translation model, when the amount of available parallel corpus is small. Furthermore, we validate that this is not only due to the increased amount of target-side, monolingual corpus.\nFinally, we train a single model with the proposed architecture on all the language pairs from the WMT\u201915; English, French, Czech, German, Russian and Finnish. The experiments show that it is indeed possible to train a single attention-based network to perform multi-way translation."}, {"heading": "2 Background: Attention-based Neural Machine Translation", "text": "The attention-based neural machine translation was proposed in (Bahdanau et al., 2014). It was motivated from the observation in (Cho et al., 2014a) that a basic encoder-decoder translation model from (Cho et al., 2014b; Sutskever et al., 2014) suffers from translating a long source sentence efficiently. This is largely due to the fact that the encoder of this basic approach needs to compress a whole source sentence into a single vector. Here we describe the attention-based neural machine translation.\nNeural machine translation aims at building a single neural network that takes as input a source sequence X = (x1, . . . , xTx) and generates a corresponding translation Y = ( y1, . . . , yTy ) . Each symbol in both source and target sentences, xt or yt, is an integer index of the symbol in a vocabulary.\nThe encoder of the attention-based model encodes a source sentence into a set of context vectors C = {h1,h2, . . . ,hTx}, whose size varies w.r.t. the length of the source sentence. This context set is constructed by a bidirectional recurrent neural network (RNN) which consists of a forward RNN and reverse RNN. The forward RNN reads the source sentence from the first token until the last one, resulting in the forward context vectors{\u2212\u2192 h 1, . . . , \u2212\u2192 h Tx } , where\n\u2212\u2192 h t = \u2212\u2192 \u03a8 enc (\u2212\u2192 h t\u22121,Ex [xt] ) ,\nand Ex \u2208 R|Vx|\u00d7d is an embedding matrix containing row vectors of the source symbols. The\nreverse RNN in an opposite direction, resulting in{\u2190\u2212 h 1, . . . , \u2190\u2212 h Tx } , where\n\u2190\u2212 h t = \u2190\u2212 \u03a8 enc (\u2190\u2212 h t+1,Ex [xt] ) .\n\u2212\u2192 \u03a8 enc and \u2190\u2212 \u03a8 enc are recurrent activation functions such as long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRU, (Cho et al., 2014b)). At each position in the source sentence, the forward and reverse context vectors are concatenated to form a full context vector, i.e.,\nht = [\u2212\u2192 h t; \u2190\u2212 h t ] . (1)\nThe decoder, which is implemented as an RNN as well, generates one symbol at a time, the translation of the source sentence, based on the context set returned by the encoder. At each time step t in the decoder, a time-dependent context vector ct is computed based on the previous hidden state of the decoder zt\u22121, the previously decoded symbol y\u0303t\u22121 and the whole context set C.\nThis starts by computing the relevance score of each context vector as\net,i = fscore(hi, zt\u22121,Ey [y\u0303t\u22121]), (2)\nfor all i = 1, . . . , Tx. fscore can be implemented in various ways (Luong et al., 2015b), but in this work, we use a simple single-layer feedforward network. This relevance score measures how relevant the i-th context vector of the source sentence is in deciding the next symbol in the translation. These relevance scores are further normalized:\n\u03b1t,i = exp(et,i)\u2211Tx j=1 exp(et,j) , (3)\nand we call \u03b1t,i the attention weight. The time-dependent context vector ct is then the weighted sum of the context vectors with their weights being the attention weights from above:\nct = Tx\u2211 i=1 \u03b1t,ihi. (4)\nWith this time-dependent context vector ct, the previous hidden state zt\u22121 and the previously decoded symbol y\u0303t\u22121, the decoder\u2019s hidden state is updated by\nzt = \u03a8dec (zt\u22121,Ey [y\u0303t\u22121] , ct) , (5)\nwhere \u03a8dec is a recurrent activation function. The initial hidden state z0 of the decoder is initialized based on the last hidden state of the reverse RNN:\nz0 = finit (\u2190\u2212 h Tx ) , (6)\nwhere finit is a feedforward network with one or two hidden layers.\nThe probability distribution for the next target symbol is computed by\np(yt = k|y\u0303<t, X) \u221d egk(zt,ct,E[y\u0303t\u22121]), (7)\nwhere gk is a parametric function that returns the unnormalized probability for the next target symbol being k.\nTraining this attention-based model is done by maximizing the conditional log-likelihood\nL(\u03b8) = 1 N N\u2211 n=1 Ty\u2211 t=1 log p(yt = y (n) t |y (n) <t , X (n)),\nwhere the log probability inside the inner summation is from Eq. (7). It is important to note that the ground-truth target symbols y(n)t are used during training. The entire model is differentiable, and the gradient of the log-likelihood function with respect to all the parameters \u03b8 can be computed efficiently by backpropagation. This makes it straightforward to use stochastic gradient descent or its variants to train the whole model jointly to maximize the translation performance."}, {"heading": "3 Multi-Way, Multilingual Translation", "text": "In this section, we discuss issues and our solutions in extending the conventional single-pair attentionbased neural machine translation into multi-way, multilingual model.\nProblem Definition We assume N > 1 source languages { X1, X2, . . . , XN } and M > 1 tar-\nget languages { Y 1, Y 2, . . . , YM } , and the availability of L \u2264 M \u00d7 N bilingual parallel corpora {D1, . . . , DL}, each of which is a set of sentence pairs of one source and one target languages. We use s(Dl) and t(Dl) to indicate the source and target languages of the l-th parallel corpus.\nFor each parallel corpus l, we can directly use the log-likelihood function from Eq. (7) to\ndefine a pair-specific log-likelihood Ls(Dl),t(Dl). Then, the goal of multi-way, multilingual neural machine translation is to build a model that maximizes the joint log-likelihood function L(\u03b8) = 1L \u2211L l=1 Ls(Dl),t(Dl)(\u03b8). Once the training is over, the model can do translation from any of the source languages to any of the target languages included in the parallel corpora."}, {"heading": "3.1 Existing Approaches", "text": "Neural Machine Translation without Attention In (Luong et al., 2015a), the authors extended the basic encoder-decoder network for multitask neural machine translation. As they extended the basic encoder-decoder network, their model effectively becomes a set of encoders and decoders, where each of the encoder projects a source sentence into a common vector space. The point in the common space is then decoded into different languages.\nThe major difference between (Luong et al., 2015a) and our work is that we extend the attentionbased encoder-decoder instead of the basic model. This is an important contribution, as the attentionbased neural machine translation has become de facto standard in neural translation literatures recently (Jean et al., 2014; Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015b; Sennrich et al., 2015a), by opposition to the basic encoder-decoder.\nThere are two minor differences as well. First, they do not consider multilinguality in depth. The authors of (Luong et al., 2015a) tried only a single language pair, English and German, in their model. Second, they only report translation perplexity, which is not a widely used metric for measuring translation quality. To more easily compare with other machine translation approaches it would be important to evaluate metrics such as BLEU, which counts the number of matched n-grams between the generated and reference translations.\nOne-to-Many Neural Machine Translation The authors of (Dong et al., 2015) earlier proposed a multilingual translation model based on the attention-based neural machine translation. Unlike this paper, they only tried it on one-to-many translation, similarly to earlier work by (Collobert et al., 2011) where one-to-many natural language processing was done. In this setting, it is trivial to extend the\nsingle-pair attention-based model into multilingual translation by simply having a single encoder for a source language and pairs of a decoder and attention mechanism (Eq. (2)) for each target language. We will shortly discuss more on why, with the attention mechanism, one-to-many translation is trivial, while multi-way translation is not."}, {"heading": "3.2 Challenges", "text": "A quick look at neural machine translation seems to suggest a straightforward path toward incorporating multiple languages in both source and target sides. As described earlier already in the introduction, the basic idea is simple. We assign a separate encoder to each source language and a separate decoder to each target language. The encoder will project a source sentence in its own language into a common, language-agnostic space, from which the decoder will generate a translation in its own language.\nUnlike training multiple single-pair neural translation models, in this case, the encoders and decoders are shared across multiple pairs. This is computationally beneficial, as the number of parameters grows only linearly with respect to the number of languages (O(L)), in contrary to training separate single-pair models, in which case the number of parameters grows quadratically (O(L2).)\nThe attention mechanism, which was initially called a soft-alignment model in (Bahdanau et al., 2014), aligns a (potentially non-contiguous) source phrase to a target word. This alignment process is largely specific to a language pair, and it is not clear whether an alignment mechanism for one language pair can also work for another pair.\nThe most naive solution to this issue is to have O(L2) attention mechanisms that are not shared across multiple language pairs. Each attention mechanism takes care of a single pair of source and target languages. This is the approach employed in (Dong et al., 2015), where each decoder had its own attention mechanism.\nThere are two issues with this naive approach. First, unlike what has been hoped initially with multilingual neural machine translation, the number of parameters again grows quadratically w.r.t. the number of languages. Second and more importantly, having separate attention mechanisms makes it less likely for the model to fully benefit from having mul-\ntiple tasks (Caruana, 1997), especially for transfer learning towards resource-poor languages.\nIn short, the major challenge in building a multiway, multilingual neural machine translation is in avoiding independent (i.e., quadratically many) attention mechanisms. There are two questions behind this challenge. The first one is whether it is even possible to share a single attention mechanism across multiple language pairs. The second question immediately follows: how can we build a neural translation model to share a single attention mechanism for all the language pairs in consideration?"}, {"heading": "4 Multi-Way, Multilingual Model", "text": "We describe in this section a proposed multiway, multilingual attention-based neural machine translation. The proposed model consists of N encoders {\u03a8nenc}Nn=1 (see Eq. (1)), M decoders {(\u03a8mdec, gm, fminit)}Mm=1 (see Eqs. (5)\u2013(7)) and a shared attention mechanism fscore (see Eq. (2) in the single language pair case).\nEncoders Similarly to (Luong et al., 2015b), we have one encoder per source language, meaning that a single encoder is shared for translating the language to multiple target languages. In order to handle different source languages better, we may use for each source language a different type of encoder, for instance, of different size (in terms of the number of recurrent units) or of different architecture (con-\nvolutional instead of recurrent.) This allows us to efficiently incorporate varying types of languages in the proposed multilingual translation model.\nThis however implies that the dimensionality of the context vectors in Eq. (1) may differ across source languages. Therefore, we add to the original bidirectional encoder from Sec. 2, a linear transformation layer consisting of a weight matrix Wnadp and a bias vector bnadp, which is used to project each context vector into a common dimensional space:\nhnt = W n adp [\u2212\u2192 h t; \u2190\u2212 h t ] + bnadp, (8)\nwhere Wnadp \u2208 Rd\u00d7(dim \u2212\u2192 h t+dim \u2190\u2212 h t) and bnadp \u2208 Rd.\nIn addition, each encoder exposes two transformation functions \u03c6natt and \u03c6 n init. The first transformer \u03c6natt transforms a context vector to be compatible with a shared attention mechanism:\nh\u0303nt = \u03c6 n att(h n t ). (9)\nThis transformer can be implemented as any type of parametric function, and in this paper, we simply apply an element-wise tanh to hnt .\nThe second transformer \u03c6ninit transforms the first context vector hn1 to be compatible with the initializer of the decoder\u2019s hidden state (see Eq. (6)):\nh\u0302n1 = \u03c6 n init(h n 1 ). (10)\nSimilarly to \u03c6natt, it can be implemented as any type of parametric function. In this paper, we use a feedforward network with a single hidden layer and share one network \u03c6init for all encoder-decoder pairs, like attention mechanism.\nDecoders We first start with an initialization of the decoder\u2019s hidden state. Each decoder has its own parametric function \u03d5minit that maps the last context vector h\u0302nTx of the source encoder from Eq. (10) into the initial hidden state:\nzm0 = \u03d5 m init(h\u0302 n Tx) = \u03d5 m init(\u03c6 n init(h n 1 ))\n\u03d5minit can be any parametric function, and in this paper, we used a feedforward network with a single tanh hidden layer.\nEach decoder exposes a parametric function \u03d5matt that transforms its hidden state and the previously decoded symbol to be compatible with a shared attention mechanism. This transformer is a parametric function that takes as input the previous hidden state zmt\u22121 and the previous symbol y\u0303 m t\u22121 and returns a vector for the attention mechanism:\nz\u0303mt\u22121 = \u03d5 m att ( zmt\u22121,E m y [ y\u0303mt\u22121 ]) (11)\nwhich replaces zt\u22121 in Eq. 2. In this paper, we use a feedforward network with a single tanh hidden layer for each \u03d5matt.\nGiven the previous hidden state zmt\u22121, previously decoded symbol y\u0303mt\u22121 and the time-dependent context vector cmt , which we will discuss shortly, the decoder updates its hidden state:\nzt = \u03a8dec ( zmt\u22121,E m y [ y\u0303mt\u22121 ] , fmadp(c m t ) ) ,\nwhere fmadp affine-transforms the time-dependent context vector to be of the same dimensionality as the decoder. We share a single affine-transformation layer fmadp, for all the decoders in this paper.\nOnce the hidden state is updated, the probability distribution over the next symbol is computed exactly as for the pair-specific model (see Eq. (7).)\nAttention Mechanism Unlike the encoders and decoders of which there is an instance for each language, there is only a single attention mechanism, shared across all the language pairs. This shared mechanism uses the attention-specific vectors h\u0303nt and z\u0303mt\u22121 from the encoder and decoder, respectively.\nThe relevance score of each context vector hnt is computed based on the decoder\u2019s previous hidden state zmt\u22121 and previous symbol y\u0303 m t\u22121:\nem,nt,i =fscore ( h\u0303nt , z\u0303 m t\u22121, y\u0303 m t\u22121 ) These scores are normalized according to Eq. (3) to become the attention weights \u03b1m,nt,i .\nWith these attention weights, the time-dependent context vector is computed as the weighted sum of the original context vectors: cm,nt = \u2211Tx i=1 \u03b1 m,n t,i h n i .\nSee Fig. 1 for the illustration."}, {"heading": "5 Experiment Settings", "text": ""}, {"heading": "5.1 Datasets", "text": "We evaluate the proposed multi-way, multilingual translation model on all the pairs available from WMT\u201915\u2013English (En)\u2194 French (Fr), Czech (Cs), German (De), Russian (Ru) and Finnish (Fi)\u2013, totalling ten directed pairs. For each pair, we concatenate all the available parallel corpora from WMT\u201915 and use it as a training set. We use newstest-2013 as a development set and newstest-2015 as a test set, in all the pairs other than Fi-En. In the case of Fi-En, we use newsdev-2015 and newstest-2015 as a development set and test set, respectively.\nData Preprocessing Each training corpus is tokenized using the tokenizer script from the Moses decoder. The tokenized training corpus is cleaned following the procedure in (Jean et al., 2015). Instead of using space-separated tokens, or words, we use sub-word units extracted by byte pair encoding, as recently proposed in (Sennrich et al., 2015b). For each and every language, we include 30k sub-word symbols in a vocabulary. See Table 1 for the statistics of the final, preprocessed training corpora.\nEvaluation Metric We mainly use BLEU as an evaluation metric using the multi-bleu script from Moses. BLEU is computed on the tokenized text after merging the BPE-based sub-word symbols. We further look at the average log-probability assigned\nto reference translations by the trained model as an additional evaluation metric, as a way to measure the model\u2019s density estimation performance free from any error caused by approximate decoding."}, {"heading": "5.2 Two Scenarios", "text": "Low-Resource Translation First, we investigate the effect of the proposed multi-way, multilingual model on low-resource language-pair translation. Among the five languages from WMT\u201915, we choose En, De and Fi as source languages, and En and De as target languages. We control the amount of the parallel corpus of each pair out of three to be 5%, 10%, 20% and 40% of the original corpus. In other words, we train four models with different sizes of parallel corpus for each language pair (EnDe, De-En, Fi-En.)\nAs a baseline, we train a single-pair model for each multi-way, multilingual model. We further finetune the single-pair model to incorporate the target-side monolingual corpus consisting of all the target side text from the other language pairs (e.g., when a single-pair model was trained on Fi-En, the target-side monolingual corpus consists of the target sides from De-En.) This is done by the recently proposed deep fusion (Gulcehre et al., 2015). The latter is included to tell whether any improvement from the multilingual model is simply due to the increased amount of target-side monolingual corpus.\nLarge-scale Translation We train one multi-way, multilingual model that has five encoders and five decoders, corresponding to the five languages from WMT\u201915; En, Fr, De, Cs, Ru, Fi\u2192 En, Fr, De, Cs, Ru, Fi. We use the full corpora for all of them."}, {"heading": "5.3 Model Architecture", "text": "Each symbol, either source or target, is projected on a 620-dimensional space. The encoder is a bidirectional recurrent neural network with 1,000 gated recurrent units (GRU) in each direction, and the decoder is a recurrent neural network with also 1,000 GRU\u2019s. The decoder\u2019s output function gk from Eq. (7) is a feedforward network with 1,000 tanh hidden units. The dimensionalities of the context vector hnt in Eq. (8), the attention-specific context vector h\u0303nt in Eq. (9) and the attention-specific decoder hidden state h\u0303mt\u22121 in Eq. (11) are all set to\n1,200. We use the same type of encoder for every source language, and the same type of decoder for every target language. The only difference between the single-pair models and the proposed multilingual ones is the numbers of encoders N and decoders M . We leave those multilingual translation specific components, such as the ones in Eqs. (8)\u2013(11), in the single-pair models in order to keep the number of shared parameters constant."}, {"heading": "5.4 Training", "text": "Basic Settings We train each model using stochastic gradient descent (SGD) with Adam (Kingma and Ba, 2015) as an adaptive learning rate algorithm. We use the initial learning rate of 2 \u00b7 10\u22124 and leave all the other hyperparameters as suggested in (Kingma and Ba, 2015). Each SGD update is computed using a minibatch of 80 examples, unless the model is parallelized over two GPUs, in which case we use a minibatch of 60 examples. We only use sentences of length up to 50 symbols. We clip the norm of the gradient to be no more than 1 (Pascanu et al., 2012). All training runs are early-stopped based on BLEU on the development set. As we observed in preliminary experiments better scores on the development set when finetuning the shared parameters and output layers of the decoders in the case of multilingual models, we do this for all the multilingual models. During finetuning, we clip the norm of the gradient to be no more than 5.\nSchedule As we have access only to bilingual corpora, each sentence pair updates only a subset of the parameters. Excessive updates based on a single language pair may bias the model away from the other pairs. To avoid it, we cycle through all the language pairs, one pair at a time, in Fi En, De En, Fr En, Cs En, Ru En order.1\nModel Parallelism The size of the multilingual model grows linearly w.r.t. the number of languages. We observed that a single model that handles five source and five target languages does not fit in a single GPU during training. We address this by distributing computational paths according to different translation pairs over multiple GPUs. The shared pa-\n1 indicates simultaneous updates on two GPUs.\nrameters, mainly related to the attention mechanism, is duplicated on both GPUs. The implementation was based on the work in (Ding et al., 2014)."}, {"heading": "6 Results and Analysis", "text": "Low-Resource Translation It is clear from Table 2 that the proposed model (Multi) outperforms the single-pair one (Single) in all the cases. This is true even when the single-pair model is strengthened with a target-side monolingual corpus (Single+DF). This suggests that the benefit of generalization from having multiple languages goes beyond that of simply having more target-side monolingual corpus. The performance gap grows as the size of target parallel corpus decreases.\nLarge-Scale Translation In Table 3, we observe that the proposed multilingual model outperforms or is comparable to the single-pair models for the majority of the all ten pairs/directions considered. This happens in terms of both BLEU and average logprobability. This is encouraging, considering that there are twice more parameters in the whole set of single-pair models than in the multilingual model.\nIt is worthwhile to notice that the benefit is more apparent when the model translates from a foreign language to English. This may be due to the fact that all of the parallel corpora include English as either a source or target language, leading to a better parameter estimation of the English decoder. In the future, a strategy of using a pseudo-parallel corpus to increase the amount of training examples for the decoders of other languages (Sennrich et al., 2015a) should be investigated to confirm this conjecture."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed multi-way, multilingual attention-based neural machine translation. The proposed approach allows us to build a single neural network that can handle multiple source and target languages simultaneously. The proposed model is a step forward from the recent works on multilingual neural translation, in the sense that we support attention mechanism, compared to (Luong et al., 2015a) and multi-way translation, compared to (Dong et al., 2015). Furthermore, we evaluate the proposed model on large-scale experiments, using the full set of parallel corpora from WMT\u201915.\nWe empirically evaluate the proposed model in large-scale experiments using all five languages from WMT\u201915 with the full set of parallel corpora and also in the settings with artificially controlled amount of the target parallel corpus. In both of the settings, we observed the benefits of the proposed multilingual neural translation model over having a set of single-pair models. The improvement was especially clear in the cases of translating low-resource language pairs.\nWe observed the larger improvements when translating to English. We conjecture that this is due to a higher availability of English in most parallel corpora, leading to a better parameter estimation of the English decoder. More research on this phenomenon in the future will result in further improvements from using the proposed model. Also, all the other techniques proposed recently, such as ensembling and large vocabulary tricks, need to be tried together with the proposed multilingual model to improve the translation quality even further. Finally, an interesting future work is to use the proposed model to translate between a language pair not included in a set of training corpus."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merrie\u0308nboer et al., 2015). We acknowledge the support of the following organizations for research funding and computing support: NSERC, Samsung, IBM, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs, CIFAR and TUBITAK-2214a."}], "references": [{"title": "2014", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2015}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "David WardeFarley", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "1997", "author": ["L\u00e9on Bottou", "Yoshua Bengio", "Yann Le Cun"], "venue": "Global training of document processing systems using graph transformer networks. In Computer Vision and Pattern Recognition,", "citeRegEx": "Bottou et al.1997", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder\u2013 Decoder approaches", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Eighth Workshop on Syntax,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Graham W", "author": ["Weiguang Ding", "Ruoyan Wang", "Fei Mao"], "venue": "Taylor.", "citeRegEx": "Ding et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dianhai Yu", "author": ["Daxiang Dong", "Hua Wu", "Wei He"], "venue": "and Haifeng Wang.", "citeRegEx": "Dong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Faustino Gomez", "author": ["Alex Graves", "Santiago Fern\u00e1ndez"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Horst Bunke", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Holger Schwenk", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Gulcehre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "2014", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "On using very large target vocabulary for neural machine translation. In ACL", "citeRegEx": "Jean et al.2014", "shortCiteRegEx": null, "year": 2015}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "The International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc VV Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Jan Chorowski", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley"], "venue": "and Yoshua Bengio.", "citeRegEx": "van Merri\u00ebnboer et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT\u201915 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "creator": "LaTeX with hyperref package"}}}