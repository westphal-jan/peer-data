{"id": "1601.01530", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2016", "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling", "abstract": "recurrent neural network ( rnn ) methodology and one of potentially its specific architectures, long short - term memory ( lstm ), have been widely used for sequence labeling. in this paper, we first enhance lstm - based sequence region labeling to explicitly model label dependencies. then we propose another enhancement to incorporate the global information spanning over the whole input sequence. the latter proposed processing method, encoder - labeler lstm, first encodes the whole input sequence sequence length into a fixed length vector with the output encoder lstm, and then uses this encoded vector as the initial labeling state of another lstm for sequence labeling. combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. in the experiments of conducting a slot filling task, which is an associated essential component of natural language understanding, with using the standard atis corpus, we achieved the state - of - the - art f1 - score of 95. 66 %.", "histories": [["v1", "Thu, 7 Jan 2016 13:32:31 GMT  (157kb)", "http://arxiv.org/abs/1601.01530v1", null], ["v2", "Tue, 23 Aug 2016 02:06:45 GMT  (605kb)", "http://arxiv.org/abs/1601.01530v2", null], ["v3", "Mon, 29 Aug 2016 00:41:29 GMT  (603kb)", "http://arxiv.org/abs/1601.01530v3", "Accepted in EMNLP 2016"], ["v4", "Wed, 31 Aug 2016 00:30:10 GMT  (512kb)", "http://arxiv.org/abs/1601.01530v4", "Accepted in EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gakuto kurata", "bing xiang", "bowen zhou", "mo yu"], "accepted": true, "id": "1601.01530"}, "pdf": {"name": "1601.01530.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gakuto@jp.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com", "gflfof@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n01 53\n0v 1\n[ cs\n.C L\n] 7\nJ an"}, {"heading": "1 Introduction", "text": "Natural language understanding (NLU) is an essential component of natural human computer interaction and typically consists of identifying the intent of the users (intent classification) and extracting the associated semantic slots (slot filling) (De Mori et al., 2008). We focus on the latter slot filling task in this paper.\nSlot filling can be framed as a sequential labeling problem in which the most probable semantic slot labels are estimated for each word of the given word sequence. Slot filling is a traditional task and tremendous efforts have been done, especially since the 1980s when the\nDefense Advanced Research Program Agency (DARPA) Airline Travel Information System (ATIS) projects started (Price, 1990). Following the success of deep learning (Hinton et al., 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al., 2013; Yao et al., 2014a; Mesnil et al., 2015). The RNN/LSTM-based slot filling has been extended to be combined with explicit modeling of label dependencies (Yao et al., 2014b; Liu and Lane, 2015).\nIn this paper, we first enhance the LSTM-based slot filling to explicitly model label dependencies by feeding the output label of the previous time step to the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN. Then we further extend the LSTMbased slot filling to consider sentence-level information. In the field of machine translation, an encoder-decoder LSTM has been gaining attention (Sutskever et al., 2014), where the encoder LSTM encodes the global information spanning over the whole input sentence in its last hidden state. Inspired by this idea, we propose an encoderlabeler LSTM that leverages the encoder LSTM for slot filling. First, we encode the input sentence into a fixed length vector by the encoder LSTM. Then, we predict the slot label sequence by the labeler LSTM whose hidden state is initialized with the encoded vector by the encoder LSTM. With this encoder-labeler LSTM, we can predict the label sequence while taking the sentence-level information into consideration. By combining explicit model-\ning of label dependencies and the proposed encoderlabeler LSTM, slot filling with jointly considering label dependencies and sentence-level information becomes possible.\nThe main contributions of this paper are twofolds: (1) Proposed an encoder-labeler LSTM to leverage sentence-level information for natural language understanding. (2) Achieved the state-of-theart F1-score of 95.66% in the slot filling task of the standard ATIS corpus."}, {"heading": "2 Proposed Method", "text": "We first revisit the LSTM for slot filling and enhance this to explicitly model label dependencies. Then we explain the proposed encoder-labeler LSTM."}, {"heading": "2.1 LSTM for Slot Filling", "text": "LSTM is a specific RNN architecture and is easier to train thanks to its internal memory cells and gates. Figure 1(a) shows a typical LSTM for slot filling and we call this as labeler LSTM(W) where words are fed to the LSTM (Yao et al., 2014a).\nSlot filling is a sequential labeling task to map a sequence of T words xT1 to a sequence of T slot labels yT1 . Each word xt is represented with a V dimensional one-hot-vector where V is the vocabulary size and is transferred to de dimensional continuous space by the word embedding matrix E \u2208 R de\u00d7V as Ext. Instead of simply feeding Ext into the LSTM, Context Window is a widely used technique to jointly consider k preceding and succeeding words as Ext+k\nt\u2212k \u2208 Rde(2k+1). The LSTM has the\narchitecture based on Jozefowicz et al. (2015) that does not have peephole connections and yields the hidden state sequence hT1 . For each time step t, the posterior probabilities for each slot label are calculated by the softmax layer over the hidden state ht. The word embedding matrix E, LSTM parameters, and softmax layer parameters are estimated to minimize the negative log likelihood over the correct la-\nbel sequences with Back-Propagation Through Time (BPTT) (Williams and Peng, 1990)."}, {"heading": "2.2 Explicit Modeling of Label Dependency", "text": "A shortcoming of the labeler LSTM(W) is that it does not consider label dependencies. To explicitly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in Figure 1(b), where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN. For model training, one-hot-vector of ground truth label of previous time step is fed to the hidden state of current time step and for evaluation, left-to-right beam search is used."}, {"heading": "2.3 Encoder-labeler LSTM for Slot Filling", "text": "We propose two types of the encoder-labeler LSTM that uses the labeler LSTM(W) and the labeler LSTM(W+L). Figure 1(d) shows the encoderlabeler LSTM(W). The encoder LSTM, to the left of the dotted line, reads through the input sentence backward. Its last hidden state contains the encoded information of the input sentence. The labeler LSTM(W), to the right of the dotted line, is the same with the labeler LSTM(W) explained in Section 2.1, except that its hidden state is initialized with the last hidden state of the encoder LSTM. The labeler LSTM(W) predicts the slot label conditioned on the encoded information by the encoder LSTM, which means that slot filling is conducted with taking sentence-level information into consideration. Figure 1(e) shows the encoder-labeler LSTM(W+L), which uses the labeler LSTM(W+L) and predicts the slot label considering sentence-level information and label dependencies jointly.\nModel training is basically the same as with the baseline labeler LSTM(W), as shown in Section 2.1, except that the error in the labeler LSTM is propa-\ngated to the encoder LSTM with BPTT. This encoder-labeler LSTM is motivated by the encoder-decoder LSTM that has been applied to machine translation (Sutskever et al., 2014), graphemeto-phoneme conversion (Yao and Zweig, 2015), and so on. The difference is that the proposed encoderlabeler LSTM accepts the same input sequence twice while the usual encoder-decoder LSTM accepts the input sequence once in the encoder. Note that the LSTMs for encoding and labeling are different in the encoder-labeler LSTM, but the same word embedding matrix is used both for the encoder and labeler since the same input sequence is fed twice."}, {"heading": "3 Experiments", "text": "We first explain the experimental setup. Then we report the results to confirm the improvement by the proposed encoder-labeler LSTM. Finally, we compare our results with the published results while discussing the related works."}, {"heading": "3.1 Experimental Setup", "text": "We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its semantic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences.\nThe performance was measured by the F1-score: F1 = 2\u00d7Precision\u00d7Recall P recision+Recall , where precision is the ratio of the correct labels in the system\u2019s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979).\nThe ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sentences. The unique number of slot labels is 127 and the vocabulary size is 572. In the following experiments, we randomly selected 80% of the original training data to train the model and used the remaining 20% as the heldout data (Mesnil et al., 2015). We reported the F1-score on the evaluation data with hyper-parameters that achieved the best F1-score on the heldout data.\nFor training, we randomly initialized parameters in accordance with the normalized initialization (Glorot and Bengio, 2010). We used ADAM for learning rate control (Kingma and Ba, 2014) and\ndropout for generalization with a dropout rate of 0.5 (Srivastava et al., 2014; Zaremba et al., 2014)."}, {"heading": "3.2 Improvement by Encoder-labeler LSTM", "text": "We conducted experiments to compare the labeler LSTM(W) (Section 2.1), the labeler LSTM(W+L) (Section 2.2), and the encoder-labeler LSTM (Section 2.3). As for yet another baseline, we tried the encoder-decoder LSTM as shown in Figure 1(c)1.\nFor all architectures, we set the initial learning rate to 0.001 (Kingma and Ba, 2014) and the dimension of word embeddings to de = 30. We changed the number of hidden units in the LSTM, dh \u2208 {100, 200, 300}2 , and the size of the context window, k \u2208 {0, 1, 2}3 . We used backward encoding for the encoder-decoder LSTM and the encoder-labeler LSTM as suggested in Sutskever et al. (2014). For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder (Sutskever et al., 2014) with beam sizes of 1, 2, 4, and 8 for evaluation where the best F1-score was reported. During 100 training epochs, we reported the F1-score on the evaluation data with the epoch when the F1-score for the heldout data was maximized. Table 1 shows the results.\nBy comparing labeler LSTM(W) and labeler LSTM(W+L), we found improvement by explicitly modeling label dependencies. The proposed encoder-labeler LSTM(W) and encoderlabeler LSTM(W+L) both outperformed the labeler LSTM(W) and labeler LSTM(W+L) by considering sentence-level information. It is noteworthy that the sentence-level information used by the proposed encoder-labeler LSTM(W+L) is effective on top of the label dependencies modeled by the labeler LSTM(W+L).\nContrary to expectations, F1-score by the encoder-labeler LSTM(W+L) was worse than that by the encoder-labeler LSTM(W). A possible reason is that the model structure of the encoder-labeler LSTM(W+L) is more complex than that of the encoder-labeler LSTM(W) and optimization is more difficult with the limited training data of ATIS.\n1Length of the output label sequence is equal to that of the input word sequence in a slot filling task. Therefore, ending symbol for slot sequence is not necessary.\n2When using deep architecture later in this section, dh was tuned for each layer.\n3In our preliminary experiments with using the labeler LSTM(W), F1-scores deteriorated with k \u2265 3.\nFor the encoder-labeler LSTM(W) which was better than the encoder-labeler LSTM(W+L), we tried the deep architecture of 2 LSTM layers (Encoderlabeler deep LSTM(W)). We also trained the corresponding labeler deep LSTM(W). As in Table 1, we obtained improvement from 94.91% to 95.47% by the proposed encoder-labeler deep LSTM(W).\nLastly, F1-score by the encoder-decoder LSTM was worse than that by the labeler LSTM(W). Since the slot label is closely related with the input word, the encoder-decoder LSTM was not an appropriate approach for the slot filling task."}, {"heading": "3.3 Comparison with Published Results", "text": "Table 2 summarizes the recently published results on the ATIS slot filling task and compares them with the results from the proposed methods.\nRecent research has been focusing on RNN and its extensions. Yao et al. (2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.98%). Bi-directional RNN was not effective after using context window in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN. Peng and Yao (2015) proposed RNN-EM that used an external memory architecture to improve the memory capability of RNN.\nMany studies have been also conducted to explicitly model the label dependencies. Xu and Sarikaya (2013) proposed CNN-CRF that explicitly models the dependencies of the output from CNN. Mesnil et al. (2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs. Liu and Lane (2015) used the output label for the previous word to model label dependencies\n(RNN-SOP). By comparing with these methods, the main difference of our proposed encoder-labeler LSTM is the use of encoder LSTM to leverage sentence-level information. For our encoder-labeler LSTM(W) and encoder-labeler deep LSTM(W), we further conducted hyper-parameter search with a random search strategy (Bergstra and Bengio, 2012). We tuned the dimension of word embeddings, de \u2208 {30, 50, 75}, number of hidden states in each layer, dh \u2208 {100, 150, 200, 250, 300}, size of context window, k \u2208 {0, 1, 2}, and initial learning rate sampled from uniform distribution in range [0.0001, 0.01]. To the best of our knowledge, the previously published best F1-score was 95.25%4 (Peng and Yao, 2015). Our encoder-labeler LSTM(W) and encoder-labeler deep LSTM(W) achieved 95.40% and 95.66% F1-score, respectively, both outperforming the previously published F1-score as shown in Table 2.\nNote some of the previous results used whole training data for model training while others used randomly selected 80% of data for model training and the remaining 20% for hyper-parameter tuning. Our results are based on the latter setup."}, {"heading": "4 Conclusion", "text": "We enhanced the LSTM-based slot filling to model label dependencies and proposed an encoder-labeler LSTM to leverage sentence-level information. By combining these methods, slot filling with jointly considering label dependencies and sentence-level\n4 There are other published results that achieved better F1scores by using other information on top of word features. Vukotic et al. (2015) achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. Yao et al. (2013) and Yao et al. (2014a) used NE features in addition to word features and obtained improvement with both the RNN and LSTM.\ninformation became possible. We obtained the stateof-the-art F1-score in a slot filling task with using the standard ATIS corpus. Our future work includes evaluation on other data sets."}], "references": [{"title": "Learning deep architectures for ai. Foundations and trends R", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "author": ["Dahl et al.1994] Deborah A Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate Hunicke-Smith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg"], "venue": "In Proc. HLT,", "citeRegEx": "Dahl et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 1994}, {"title": "Spoken language understanding", "author": ["Fr\u00e9d\u00e9ric Bechet", "Dilek Hakkani-Tur", "Michael McTear", "Giuseppe Riccardi", "Gokhan Tur"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Mori et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mori et al\\.", "year": 2008}, {"title": "Use of kernel deep convex networks and end-to-end learning for spoken language understanding", "author": ["Deng et al.2012] Li Deng", "Gokhan Tur", "Xiaodong He", "Dilek Hakkani-Tur"], "venue": "In Proc. SLT,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proc AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Jordan.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proc. ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "ADAM: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Recurrent neural network structured output prediction for spoken language understanding", "author": ["Liu", "Lane2015] Bing Liu", "Ian Lane"], "venue": "In Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language", "author": ["Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Recurrent neural networks with external memory for language understanding", "author": ["Peng", "Yao2015] Baolin Peng", "Kaisheng Yao"], "venue": "arXiv preprint arXiv:1506.00195", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["Patti Price"], "venue": "In Proc. DARPA Speech and Natural Language Workshop,", "citeRegEx": "Price.,? \\Q1990\\E", "shortCiteRegEx": "Price.", "year": 1990}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Raymond", "Riccardi2007] Christian Raymond", "Giuseppe Riccardi"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Raymond et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raymond et al\\.", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "What is left to be understood in ATIS", "author": ["Tur et al.2010] Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "venue": "In Proc. SLT,", "citeRegEx": "Tur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["Christian Raymond", "Guillaume Gravier"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Vukotic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vukotic et al\\.", "year": 2015}, {"title": "Combining statistical and knowledge-based spoken language understanding in conditional models", "author": ["Wang et al.2006] Ye-Yi Wang", "Alex Acero", "Milind Mahajan", "John Lee"], "venue": "In Proc. COLING-ACL,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Peng1990] Ronald J Williams", "Jing Peng"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "Convolutional neural network based triangular", "author": ["Xu", "Sarikaya2013] Puyang Xu", "Ruhi Sarikaya"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["Yao", "Zweig2015] Kaisheng Yao", "Geoffrey Zweig"], "venue": "Proc. INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Yao et al.2014a] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Proc. SLT,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent conditional random field for language understanding", "author": ["Yao et al.2014b] Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao"], "venue": "In Proc. ICASSP,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Slot filling is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Information System (ATIS) projects started (Price, 1990).", "startOffset": 217, "endOffset": 230}, {"referenceID": 7, "context": "Following the success of deep learning (Hinton et al., 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 39, "endOffset": 74}, {"referenceID": 0, "context": "Following the success of deep learning (Hinton et al., 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 39, "endOffset": 74}, {"referenceID": 5, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 9, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 25, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al., 2013; Yao et al., 2014a; Mesnil et al., 2015).", "startOffset": 254, "endOffset": 312}, {"referenceID": 13, "context": ", 2006; Bengio, 2009), Recurrent Neural Network (RNN) (Elman, 1990; Jordan, 1997) and one of its specific architectures, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), have been widely used since they can capture temporal dependencies (Yao et al., 2013; Yao et al., 2014a; Mesnil et al., 2015).", "startOffset": 254, "endOffset": 312}, {"referenceID": 18, "context": "In the field of machine translation, an encoder-decoder LSTM has been gaining attention (Sutskever et al., 2014), where the encoder LSTM encodes the global information spanning over the whole input sentence in its last hidden state.", "startOffset": 88, "endOffset": 112}, {"referenceID": 13, "context": "In this paper, we first enhance the LSTM-based slot filling to explicitly model label dependencies by feeding the output label of the previous time step to the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 202, "endOffset": 223}, {"referenceID": 13, "context": "In this paper, we first enhance the LSTM-based slot filling to explicitly model label dependencies by feeding the output label of the previous time step to the hidden state of the current time step, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 202, "endOffset": 247}, {"referenceID": 10, "context": "The LSTM has the architecture based on Jozefowicz et al. (2015) that does not have peephole connections and yields the hidden state sequence hT1 .", "startOffset": 39, "endOffset": 64}, {"referenceID": 13, "context": "To explicitly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in Figure 1(b), where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 235, "endOffset": 256}, {"referenceID": 13, "context": "To explicitly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in Figure 1(b), where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as Mesnil et al. (2015) and Liu and Lane (2015) tried with RNN.", "startOffset": 235, "endOffset": 280}, {"referenceID": 18, "context": "This encoder-labeler LSTM is motivated by the encoder-decoder LSTM that has been applied to machine translation (Sutskever et al., 2014), graphemeto-phoneme conversion (Yao and Zweig, 2015), and so on.", "startOffset": 112, "endOffset": 136}, {"referenceID": 15, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 2, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 21, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 19, "context": "1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010).", "startOffset": 98, "endOffset": 167}, {"referenceID": 13, "context": "In the following experiments, we randomly selected 80% of the original training data to train the model and used the remaining 20% as the heldout data (Mesnil et al., 2015).", "startOffset": 151, "endOffset": 172}, {"referenceID": 17, "context": "5 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 28, "context": "5 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 18, "context": "For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder (Sutskever et al., 2014) with beam sizes of 1, 2, 4, and 8 for evaluation where the best F1-score was reported.", "startOffset": 126, "endOffset": 150}, {"referenceID": 18, "context": "We used backward encoding for the encoder-decoder LSTM and the encoder-labeler LSTM as suggested in Sutskever et al. (2014). For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder (Sutskever et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012).", "startOffset": 124, "endOffset": 143}, {"referenceID": 22, "context": "Yao et al. (2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.", "startOffset": 125, "endOffset": 166}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.98%). Bi-directional RNN was not effective after using context window in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN.", "startOffset": 125, "endOffset": 368}, {"referenceID": 4, "context": "(2013) used RNN and outperformed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation comparing with their single-directional RNN (94.98%). Bi-directional RNN was not effective after using context window in this task. Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN. Peng and Yao (2015) proposed RNN-EM that used an external memory architecture to improve the memory capability of RNN.", "startOffset": 125, "endOffset": 453}, {"referenceID": 25, "context": "Liu and Lane (2015) used the output label for the previous word to model label dependencies F1-score RNN (Yao et al., 2013) 94.", "startOffset": 105, "endOffset": 123}, {"referenceID": 13, "context": "35 Bi-directional RNN (Mesnil et al., 2015) 94.", "startOffset": 22, "endOffset": 43}, {"referenceID": 13, "context": "89 Hybrid RNN (Mesnil et al., 2015) 95.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": "Mesnil et al. (2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs. Liu and Lane (2015) used the output label for the previous word to model label dependencies F1-score RNN (Yao et al.", "startOffset": 37, "endOffset": 90}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. Yao et al. (2013) and Yao et al.", "startOffset": 0, "endOffset": 138}, {"referenceID": 20, "context": "Vukotic et al. (2015) achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. Yao et al. (2013) and Yao et al. (2014a) used NE features in addition to word features and obtained improvement with both the RNN and LSTM.", "startOffset": 0, "endOffset": 161}], "year": 2016, "abstractText": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1score of 95.66%.", "creator": "LaTeX with hyperref package"}}}