{"id": "1601.01887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2016", "title": "Research Project: Text Engineering Tool for Ontological Scientometry", "abstract": "the number of scientific papers grows exponentially in many analytical disciplines. the share of online available papers grows as well. at the same time, the period of subscription time for a paper to loose at chance to be cited anymore shortens. the decay of the citing rate shows similarity to ultradiffusional processes as for other online contents in social networks. the distribution of papers per author file shows similarity to the distribution of posts per net user in social networks. the rate of uncited papers for generating online available papers grows while some papers'go viral'in terms of being openly cited. summarized, the practice of scientific publishing moves towards the domain of social networks. the goal of assisting this project iteration is to create a text engineering tool, which can semi - automatically categorize a paper according again to its type of contribution and extract relationships between them into an ontological database. semi - automatic categorization means that the mistakes made by automatic pre - categorization and relationship - extraction will be corrected through a wikipedia - like front - end by volunteers from general public. this tool should not only help researchers and the general public to find relevant supplementary material and peers faster, but also secondly provide more information for research funding agencies.", "histories": [["v1", "Fri, 8 Jan 2016 14:29:44 GMT  (93kb)", "http://arxiv.org/abs/1601.01887v1", "5 pages, 2 figure"]], "COMMENTS": "5 pages, 2 figure", "reviews": [], "SUBJECTS": "cs.CL cs.DL", "authors": ["rustam tagiew"], "accepted": false, "id": "1601.01887"}, "pdf": {"name": "1601.01887.pdf", "metadata": {"source": "CRF", "title": "Text Engineering Tool for Ontological Scientometry", "authors": ["Rustam Tagiew"], "emails": ["yepkio@mail.ru"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n01 88\n7v 1\n[ cs\n.C L\n] 8\nJ an\n2 01\nKeywords\u2014Scientometry, Bibliometry, Social Networks, Information Retrieval, Ontology, Semantic Technology, Formal Concept Analysis\nI. INTRODUCTION \u201cFor scientists there are moments, when they read about a paper they were interested in. They are curious about, how the person got exactly to that results and then they find a link from the paper to the data. And then they find out that they can just reproduce the processing of the results immediately. I think that is the step, which a lot of funding agencies pushing towards I know certainly in the US. Pushing to say, if we are funding you, you have to put your data out there, you should put it out there in a standard format, because we paid you to make the data. You may have published a paper, which is based on some results you\u2019ve noticed. Somebody else may wanna do a very much of a transverse cut for your and anybody else\u2019s data. We need to be able to get a reuse of the data we funded.\u201d Tim Berners-Lee [1]\nThere are different types of contributions in science, which all are pressed into text format having title, abstract, body and references. It can be a formula to calculate a sequence of integers for a certain phenomenon. It can be a set of experimentally proven properties for a new chemical element.\nIt can be a machine learning algorithm. It can be an evaluation of a dataset from an experiment. Obviously, some papers originate from a significant practical work beyond reviewing literature and composing the actual text. And papers exist, which do not comprehend such effort. Papers without comprehended practical work either introduce a new theory or are at least inferential from previous papers.\nSome of the practical work behind a paper can be directly manifested as digital content, without being presented as a paper. For instance, an algorithm from machine learning can be integrated into machine learning libraries, a chemical formula with a set of properties can be entered into chemical databases, a dataset from a subject research experiment can be uploaded to data repositories and a footage of a ball lightning including its spectrogram [2] can be published on a pod-cast website. This content is also known as supplementary material. In many cases the supplementary material is of much higher scientific use than the paper text itself. Traditionally, publishing supplementary material alone does not benefit a scientific career and therefore scientists have to go through publishing an obligatory paper.\nMost of world research funding and academic position allocation is led by statistics of citations \u2013 present day\u2019s scientometry. Published supplementary material still plays a marginal role. This situation is harshly criticized by scientists from diverse disciplines [3], [4]. The main arguments are the alienation of scientific work from its purpose and the negligence of the practical component. Every scientist has his own representation of publications in his field and his own view on ranking of the relevant research, which does not fit scientometric figures. Since the return to the less transparent, less exact and more time consuming alternative of manual content comparison is not possible, the desire for a better scientometry formed. \u2018Altmetrics\u2019 is the present sublimation of this desire and suggests taking more indicators into account than only citations. Those indicators are driven from web data as views and citations in other media.\nScientific papers are a content targeted for human readers and therefore noisily formated and require data mining techniques for extraction of statistics. Nevertheless, certain facts are already known [5], [6]. The number of scientific papers grows exponentially in many disciplines. The share of online available papers grows as well. At the same time, the period of time for a paper to loose at chance to be cited anymore shortens. The decay of citing rate shows similarity to ultradiffusional processes as for other online contents in social networks. The distribution of papers per\nauthor shows similarity to the distribution of posts per user in social networks. The rate of uncited papers for online available papers grows while some papers \u2018go viral\u2019 in terms of being cited. Summarized, the practice of scientific research moves towards the domain of unstructured social networks vulnerable to hypes and content repetition.\nThere is a lot of effort in all disciplines to create clear taxonomy of research subjects and to introduce standard keywords. Nevertheless, computer scientists from the \u2018neuronal nets\u2019 community reinvent approaches from the \u2018optimization\u2019 community and economists redo experiments from psychology. Given the exponential growth at number of papers, researchers in different categories may end up doing the same thing without knowing about each other. Researchers use different words to describe the same semantics \u2013 the practical work they conducted. The sheer exponentially growing number of publications requires more and more of automated assistance in semantical analysis of the publications, in their structuring and in guidance of researchers and funding agencies."}, {"heading": "II. TERMINOLOGY", "text": "A piece of scientific research, which is published, is a publication. It can be a paper, which is of format having title, abstract, body and references. It can also be a dataset, a coded algorithm or an audiovisual sample \u2013 suplementary material. It can also be a combination of these things. A scientific document is a paper in any device independent file format like PDF, PS, DVI and so on. The metadata of paper is represented by its BIBTEX entry plus the list of cited publications."}, {"heading": "III. DESCRIPTION OF THE OBJECTIVE", "text": "The objective is to represent every scientist\u2019s knowledge about practical research in his field as an ontology assisted by text classification. There are two sides of semantics for this ontology \u2013 the human and the machine one. The human semantics is the description of research and the practical work behind it. The machine semantics is a document classifier and a set of rules for processing of metadata into relationships between papers.\nThe design of the semi-automatically created ontological database for scientific documents is relatively simple, as you can see in Fig.1. Scientific documents and their metadata are prepared and offered under conditions by many commercial and non-commercial entities such as Citeseerx, Mendeley, Thomas Reuters (TR) web of science, Scopus and ResearchGate. There are also domain-specific repositories such as DBLP and RePEc/IDEAS. The preparation of metadata is by no means trivial [7], leads to noisy results, and requires its own scientific community in between of \u2018information retrieval\u2019 and \u2018scientometrics\u2019 [8].\nFrom the acquired documents, text and features are to be extracted and the documents are to be clustered on their base. The method of clustering is to be semi-supervised [9] \u2013 the clustering results are to be evaluated on a partial set of manually made sample clusters. Unsupervised clustering may lead to unwanted results such as clusters of different\nproficiency in english e.g.. Creating a full set of sample clusters for every type of paper would require too much of manual work For the supervised classification as we will see in section V. The results of cluster evaluation will reflect on text/feature extraction as well as on clustering method.\nRecent development of open source text engineering tools is promising and will play a crucial role in the task of clustering. For instance, GATE (general architecture for text engineering) is a free open source software agglomerate [10], which includes libraries such as KEA (keyphrase extraction) [11], which in turn uses WEKA (Waikato data mining library) [12], which is an aggregation of machine learning algorithms.\nOnce the types of the papers are determined, the metadata can tell much more than just (co)citation and (co)authorship. Now the type of a citation can defined. For instance, when a computer science paper evaluating algorithms cites a paper about chemical reactions, the type of citation will be different to the case, where another chemical paper cites it. In the first case the paper is cited for its dataset, and in the second, for its insight. The application of formal concept analysis [13] will be considered as a framework for the relationships between the types of papers.\nOne can expect that the information ingested into the ontological database will contain mistakes. For instance, some papers will be assigned to wrong types. This problem is\neasily solved by wikipedia-like front-end. Like on Wikipedia, volunteers will correct the database. A subset of volunteers would be the researchers themselves, because some of them are interested in having their research being appropriately presented. The system to be chosen for such a front-end is yet to be chosen from these available open source.\nAlthough the design is simple, the amount of its purposes is huge. The ontological database should be able to answer questions on the relationships of a certain papers to other papers. It should also be able to answer the question about the type of supplementary material the authors could be asked for. It should allow machines to communicate with and guide scientists in a more detailed way than just displaying them their h-index [14].\nThe main scientific contribution lies in the development of the scientometric ontology and its human and machine semantics. The development of machine semantics implies contributions in the domain of document clustering."}, {"heading": "IV. ACCESSING SUPPLEMENTARY MATERIAL", "text": "In scientific tradition supplementary material of a paper can be accessed by sending a request to the authors. This tradition is based on the idea that any insight should be reviewed by colleagues based on supplementary material. Having a scientometric ontology, where every paper has a type, one would easily form a query to get names of authors, which have a similar supplementary material, and address them all in the request for the supplementary materials.\nThe less traditional way is the publication of the supplementary material together with the paper. The second approach obviously provides a faster access. The authors hereby have to agree with a certain type of license, whose disadvantages may overweight the academic incentive for some of the authors.\nFor standalone publications of datasets as a subtype of supplementary material, the academic incentive is recently declared by Data Citation Synthesis Group through a list of justifying reasons [15]. VegBank [16] is an example for an online repository, where obligatory citable datasets of a certain type are stored. A scientometric ontology from the realm of papers accompanied by supplementary materials would provide a reasonable structure for standalone material publications."}, {"heading": "V. CREATING CLUSTERS MANUALLY", "text": "Creating clusters manually is a tricky and very domain dependent procedure. We present it on two examples.\nThe first example is about human behavior and decision making research. There are two different communities, which struggle with two sides of the same problem. Those are the data scientists dealing with huge noisy datasets of human behavior from web and behavioral economists gathering clean small but insightful datasets while conducting experiments on human subjects [17]. The datasets of data scientists are mostly confidential and results are not always published online. There is a certain interest to integrate the domain knowledge and datasets from behavioral economics into web mining. Since the website ExLab (exlab.bus.ucf.edu) for supplementary\nmaterials of experimental human subject research is defunct, there is no other way to accessing the data than to email the authors. One can define a new type \u2018laboratory behavior study\u2019=Labbehavior in the scientometric ontology. Labbehavior has following description in natural language:\nIt is a publication with experimental behavioral data being involved. It it should not be a paper based only on poll data \u2013 subject actions should be recorded and not assumed based on poll ratings. The subjects are humans or higher primates \u2013 no studies with computer agents participating all along. The indicated discipline is either economics, psychology, social sciences or computer science. It is neither a pure summary paper nor a paper based on a pure field study nor on a pure field experiment. It should not be a study of cognitive abilities. It can be a study of behavior under bio-chemical influence such as through hormones and drugs. It should not be a pure game theoretical paper.\nNow, let us create a set of objects of type Labbehavior. First step would be to get the names of main authors of behavioral economics from Wikipedia. Here are the names of the authors:\nErnst Fehr, Dan Ariely, Urs Fischbacher, Colin Camerer, Simon G\u00e4chter, Amos Tversky, Armin Falk, Reinhard Selten, Daniel Kahneman, Uri Gneezy, B. Douglas Bernheim, George Loewenstein, Matthew Rabin, Herbert A. Simon, Vernon L. Smith, Larry Summers, Michael Taillard, Richard Thaler, John Quiggin, Margaret McConne, Werner De Bondt, Roy Baumeister, Ed Diener, Ward Edwards, Gerd Gigerenzer, George Katona, Steven Lea, Walter Mischel, Drazen Prelec, Paul Slovic, Malcolm Baker, Nicholas Barberis, Gunduz Caginalp, David Hirshleifer, Andrew Lo, Michael Mauboussin, Terrance Odean, Richard L. Peterson, Charles Plott, Hersh Shefrin, Robert Shille, Andrei Shleifer, Robert Vishny\nThen we add about 500 names from the z-Tree emailing list. z-Tree is a popular software for conducting experiments. Having this list of names, Google Scholar and Citeseerx can be used to crawl the documents, mostly in the PDF format. The crawled documents are the top 10-40 results for every name and first 100 for the term \u2018z-Tree\u2019. Running Mendeley on these documents gives a list of noisy BIBTEX entries including the abstracts. \u2018Noisy\u2019 means random mistakes in the entries. The results of the combination pdftotext [18] and the ParsCit library [19] are noisier. Abstracts are not always sufficient to classify a paper \u2013 a closer look into the documents is sometimes required. Any volunteer for manual sorting should be warned \u2013 this task requires a firm control of own attention, since every paper can be a surprise and distract attention to further reading. It is like sorting a library. For instance, papers on negotiation skills under influence of drugs and even papers on supernatural powers appeared among the crawled documents. Fig.2 shows a word stem cloud of the abstracts in the created Labbehaviour cluster of ca. 1000 documents. The results of application of pdftotext on documents from this cluster can be downloaded from here:\npsycholog subject problem\nindividu\nuse\nmani\npayoff\nempir\nincreasmodel reciproc\ndesign\nreport\nco nt\nro l evalu\nhowev\nmake set\ngood will\neconom person\nwork\nobserv\nresult\nlevel\naffect\ninvestig interact pe rf or m cooper risk\nba se\ncognit\noffer\ncondit\nexplain\nefficivalu\nimport equilibrium peopl\nro le\nshow\ncost\nta ke\nincent right\nevid\ntime\nhigh\ndetermin\nexamin\nbehavior\ndiscuss\nle ss\nrelat\npropos te\nrm di ffe r\nlearnreward\nunderstand\neffect\nchoic\nhigher\nprefer\nth eo\nre t\nreserv\nsuggest\ngame\ninfluenc\ngroup\nparticip\npolici\nnew\nm ea\nsu r\ndevelop\nexperiment\nstrategi\nchoos\nsocial\nquestion well\nstrateg\nplay\ndecis\nexperi\noutcom\nstrong\ninc\npattern\nchang\nauction\nsignific\npredict\navers\nimpact\nconsist\nbid\nactiv indic\nrate\npunish\nrespons\nty pe\nresearch market\npr ov\nid\ntreatment\nexpect\nte st\nlaboratori\nstudi larg\ncontribut even\nhuman\nco m\npa rinform\ncompetit\ntheori\napproach\nmotiv\naction\nmechan\nse lf ration\nlead\npresent trust\nprice\nposit\nta sk\nplayer\ndepend\nmay\nnon\npublic low\nreduc\nlike\nprocess\ngeneral\nfind\nagent\nwhether\ndata\ncopy.com/WQc7UO9ICzorxAzz. The second example is from computer science. For the discipline of machine learning, the papers describing algorithms form the type MLalgo. BIBTEX entries of ca. 100 such papers can be easily derived from the source code of WEKA library. The results of application of pdftotext on documents from this cluster can be downloaded from here: copy.com/OBnaf11bWnL4LcQm."}, {"heading": "VI. RELATED WORK", "text": "Let us call the type for the related papers to the clustering part of this research proposal as ScienceDocCl. The semantics\nof ScienceDocCl is:\nIt is a publication with clusters being calculated on scientific documents. The presented method should be scalable up to 5M of papers as currently available on Citeseerx. It should not be clustering based on (co)citations or other metadata. The clusters of scientific documents should be formed according to the type of performed practical work, and not according to the topic of research.\nEvaluating clustering algorithms on sets of documents has been a research subject since decades [9]. One paper [20] is especially interesting, since it evaluates clustering algorithms on 2M of scientific documents. The best result of clustering could be impressively depicted by graph visualization software. Unfortunately, it was a topic oriented clustering. Clustering according to the type of performed practical work will be a bigger challenge \u2013 it requires a deeper understanding of the language, than bag of words statistics.\nThe website of semantic web journal [21], [22] presents a related work to the wikipedia-like web-interface upon an ontological database. It is indeed a scientometric ontology with information about the peer-review process for every paper. Nevertheless, there is no way to correct or augment this knowledge base by internet users."}, {"heading": "VII. TIME & COST PLAN", "text": "The main practical work for this research proposal will be represented by building a \u2018Big Data\u2019 infrastructure, massively (24/7) lunching parallel runs with diverse clustering approaches and recording the performance of generated classifiers on the manually created sample clusters. This would require a certain investition into hardware. There is a certain trend towards GPU in text clustering [23].\nThe collection of scientific documents and their metadata is not a fully trivial task. There is a cheap way of downloading that data from Citeseerx, which will give very noisy metadata. And there is a more expensive way of buying cleaner data from sources like Thomas Reuters (TR) web of science.\nFinally, a quest for a user-edited ontology website system will run in parallel. Regarding the current pace of progress, such systems will be developed in a year or two."}], "references": [{"title": "The Semantic Web of Data", "author": ["T. Berners-Lee"], "venue": "youtube.com/watch?v=HeUrEh-nqtU, 2008, 04 : 32.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "First recorded scientific video of ball lightning", "author": ["J. Cen", "P. Yuan", "S. Xue"], "venue": "youtube.com/watch?v=VXm3zDM_v80, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The mismeasurement of science", "author": ["P.A. Lawrence"], "venue": "Current Biology, vol. 17, no. 15, pp. 583\u2013585, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "How scientometry is killing science", "author": ["A.M.C. \u015eeng\u00f6r"], "venue": "GSA Today, pp. 44\u201345, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention decay in science", "author": ["P.D.B. Parolo", "R. Kumar", "R. Ghosh", "B.A. Huberman", "K. Kaski"], "venue": "2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Electronic publication and the narrowing of science and scholarship.", "author": ["J.A. Evans"], "venue": "Science (New York, N.Y.),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "CiteSeerX : AI in a Digital Library Search Engine", "author": ["J. Wu", "K. Williams", "H. hsuan Chen", "M. Khabsa", "C. Caragea", "A. Ororbia", "D. Jordan", "C.L. Giles"], "venue": "pp. 2930\u20132937, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Scientometrics and information retrieval: weak-links revitalized", "author": ["P. Mayr", "A. Scharnhorst"], "venue": "Scientometrics, vol. 102, no. 3, pp. 2193\u20132199, 2014. [Online]. Available: http://link.springer.com/10.1007/s11192-014-1484-3", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of text clustering algorithms", "author": ["C. Aggarwal", "C. Zhai"], "venue": "Mining Text Data, 2012. [Online]. Available: http://link.springer.com/chapter/10.1007/978-1-4614-3223-4_4", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy and rough formal concept analysis: a survey", "author": ["J. Poelmans", "D.I. Ignatov", "S.O. Kuznetsov", "G. Dedene"], "venue": "Int. J. General Systems, vol. 43, no. 2, pp. 105\u2013134, 2014. [Online]. Available: http://dx.doi.org/10.1080/03081079.2013.862377", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "An index to quantify an individual\u2019s scientific research output", "author": ["E.J. Hirsch"], "venue": "Proc. Nat. Acad. Sci., vol. 46, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint Declaration of Data Citation Principles", "author": ["D.C.S. Group"], "venue": "force11.org/datacitation, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimental economics for web mining", "author": ["R. Tagiew", "D.I. Ignatov", "F. Amroush"], "venue": "CoRR, vol. abs/1412.4726, 2014. [Online]. Available: http://arxiv.org/abs/1412.4726", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering more than two million biomedical publications: Comparing the accuracies of nine text-based similarity approaches", "author": ["K.W. Boyack", "D. Newman", "R.J. Duhon", "R. Klavans", "M. Patek", "J.R. Biberstine", "B. Schijvenaars", "A. Skupin", "N. Ma", "K. B\u00f6rner"], "venue": "PLoS ONE, vol. 6, no. 3, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A linked-data-driven and semantically-enabled journal portal for scientometrics", "author": ["Y. Hu", "K. Janowicz", "G. McKenzie", "K. Sengupta", "P. Hitzler"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 8219 LNCS, no. PART 2, pp. 114\u2013129, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Data-intensive document clustering on graphics processing unit (GPU) clusters", "author": ["Y. Zhang", "F. Mueller", "X. Cui", "T. Potok"], "venue": "Journal of Parallel and Distributed Computing, vol. 71, no. 2, pp. 211\u2013224, 2011. [Online]. Available: http://dx.doi.org/10.1016/j.jpdc.2010.08.002", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "\u201d Tim Berners-Lee [1]", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "For instance, an algorithm from machine learning can be integrated into machine learning libraries, a chemical formula with a set of properties can be entered into chemical databases, a dataset from a subject research experiment can be uploaded to data repositories and a footage of a ball lightning including its spectrogram [2] can be published on a pod-cast website.", "startOffset": 326, "endOffset": 329}, {"referenceID": 2, "context": "This situation is harshly criticized by scientists from diverse disciplines [3], [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "This situation is harshly criticized by scientists from diverse disciplines [3], [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Nevertheless, certain facts are already known [5], [6].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Nevertheless, certain facts are already known [5], [6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "The preparation of metadata is by no means trivial [7], leads to noisy results, and requires its own scientific community in between of \u2018information retrieval\u2019 and \u2018scientometrics\u2019 [8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The preparation of metadata is by no means trivial [7], leads to noisy results, and requires its own scientific community in between of \u2018information retrieval\u2019 and \u2018scientometrics\u2019 [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "The method of clustering is to be semi-supervised [9] \u2013 the clustering results are to be evaluated on a partial set of manually made sample clusters.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "The application of formal concept analysis [13] will be considered as a framework for the relationships between the types of papers.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "It should allow machines to communicate with and guide scientists in a more detailed way than just displaying them their h-index [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "For standalone publications of datasets as a subtype of supplementary material, the academic incentive is recently declared by Data Citation Synthesis Group through a list of justifying reasons [15].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Those are the data scientists dealing with huge noisy datasets of human behavior from web and behavioral economists gathering clean small but insightful datasets while conducting experiments on human subjects [17].", "startOffset": 209, "endOffset": 213}, {"referenceID": 8, "context": "Evaluating clustering algorithms on sets of documents has been a research subject since decades [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 13, "context": "One paper [20] is especially interesting, since it evaluates clustering algorithms on 2M of scientific documents.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "The website of semantic web journal [21], [22] presents a related work to the wikipedia-like web-interface upon an ontological database.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "There is a certain trend towards GPU in text clustering [23].", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "The number of scientific papers grows exponentially in many disciplines. The share of online available papers grows as well. At the same time, the period of time for a paper to loose at chance to be cited anymore shortens. The decay of the citing rate shows similarity to ultradiffusional processes as for other online contents in social networks. The distribution of papers per author shows similarity to the distribution of posts per user in social networks. The rate of uncited papers for online available papers grows while some papers \u2018go viral\u2019 in terms of being cited. Summarized, the practice of scientific publishing moves towards the domain of social networks. The goal of this project is to create a text engineering tool, which can semi-automatically categorize a paper according to its type of contribution and extract relationships between them into an ontological database. Semi-automatic categorization means that the mistakes made by automatic pre-categorization and relationship-extraction will be corrected through a wikipedia-like front-end by volunteers from general public. This tool should not only help researchers and the general public to find relevant supplementary material and peers faster, but also provide more information for research funding agencies. Keywords\u2014Scientometry, Bibliometry, Social Networks, Information Retrieval, Ontology, Semantic Technology, Formal Concept Analysis", "creator": "LaTeX with hyperref package"}}}