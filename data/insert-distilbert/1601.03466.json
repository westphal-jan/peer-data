{"id": "1601.03466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Dynamic Privacy For Distributed Machine Learning Over Network", "abstract": "privacy - preserving distributed machine learning becomes increasingly important due to the rapid growth of amount of data and the importance of distributed crowd learning. this paper develops conceptual algorithms to provide privacy - preserving learning for classification problem using the regularized empirical risk minimization ( erm ) objective function in a distributed fashion. we use the definition of differential privacy, developed by dwork et al. privacy to capture the notion of privacy protection of our algorithm. we provide two methods. finally we first propose the dual variable perturbation } if which perturbs the dual variable before next intermediate minimization of augmented lagrange function over the classifier in every admm iteration. in the second method, we apply the output perturbation to the primal variable before releasing it to neighboring nodes. we call himself the second method primal variable perturbation. under certain conditions differing on the convexity and differentiability of the loss function and regularizer, our algorithms is proved to provide differential privacy through the entire learning process. we also provide theoretical results for the accuracy of the algorithm, and prove that both underlying algorithms converges in distribution. the theoretical return results show that the dual variable perturbation outperforms the primal case. the tradeoff between privacy and accuracy is examined in the numerical limit experiment. our experiment shows that both algorithms performs similar in managing the privacy - accuracy tradeoff, and primal variable perturbaiton is slightly better than the dual case.", "histories": [["v1", "Thu, 14 Jan 2016 02:20:46 GMT  (636kb,D)", "http://arxiv.org/abs/1601.03466v1", "33 pages, 8 figures"], ["v2", "Mon, 15 Feb 2016 05:20:29 GMT  (3223kb,D)", "http://arxiv.org/abs/1601.03466v2", "22 pages, 8 figures Corrected typos. Revised argument in section 4, results unchanged"], ["v3", "Wed, 9 Mar 2016 21:51:12 GMT  (3297kb,D)", "http://arxiv.org/abs/1601.03466v3", "15 pages, 5 figures Corrected typos. Revised argument in section 3, 4, and Appendix, results unchanged"]], "COMMENTS": "33 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tao zhang", "quanyan zhu"], "accepted": false, "id": "1601.03466"}, "pdf": {"name": "1601.03466.pdf", "metadata": {"source": "CRF", "title": "Dynamic Privacy For Distributed Machine Learning Over Network", "authors": ["Tao Zhang", "Quanyan Zhu"], "emails": ["t.z.1992.nyc@gmail.com", "quanyan.zhu@nyu.edu"], "sections": [{"heading": null, "text": "Privacy-preserving distributed machine learning becomes increasingly important due to the rapid growth of amount of data and the importance of distributed learning. This paper develops algorithms to provide privacy-preserving learning for classification problem using the regularized empirical risk minimization (ERM) objective function in a distributed fashion. We use the definition of differential privacy, developed by Dwork et al. privacy to capture the notion of privacy of our algorithm. We provide two methods. We first propose the dual variable perturbation, which perturbs the dual variable before next intermediate minimization of augmented Lagrange function over the classifier in every ADMM iteration. In the second method, we apply the output perturbation to the primal variable before releasing it to neighboring nodes. We call the second method primal variable perturbation. Under certain conditions on the convexity and differentiability of the loss function and regularizer, our algorithms is proved to provide differential privacy through the entire learning process. We also provide theoretical results for the accuracy of the algorithm, and prove that both algorithms converges in distribution. The theoretical results show that the dual variable perturbation outperforms the primal case. The tradeoff between privacy and accuracy is examined in the numerical experiment. Our experiment shows that both algorithms performs similar in managing the privacy-accuracy tradeoff, and primal variable perturbaiton is slightly better than\nthe dual case."}, {"heading": "1. Introduction", "text": "Distributed machine learning has become increasingly important due to the rapid growth of amount of data and the increasing of model complexity. In practice, the amount of training data can range from 1T B to 1PB [2]. With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5]. In centralized learning, these training data are shared by all the nodes participating in the learning process via centralized collection, and the parameters are available to all these nodes. In many cases, especially for the statistical learning, all nodes must frequently use the shared data and parameters in order to improve the parameters during the learning process. The centralized learning is not encouraged due to several aspects such as high computational complexity, scalability, and communication overhead, to name a few. As a result, decentralization of the dataset as well as distributed algorithms become more and more important.\nThe main goal of distributed learning is to decentralize the problem to multiple local subproblems. There are many ways to establish the decentralization, and the alternating direction method of multiplier (ADMM) is a well suited algorithm to deal with large scale distributed optimization problems. ADMM algorithm trains the model purely based on the information exchange among the neighboring nodes, rather than the en-\nar X\niv :1\n60 1.\n03 46\n6v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\ntire network, and it has been proved that ADMM for convex optimization problem is convergent to the centralized problem under some specific conditions [6].\nMany benefits have raised in the field of distributed machine learning. Google, eBay, Linkedin and Apple were among the corporations to take advantage of the massive data collected from their customers or users. They use technology like machine learning to improve decision making, reducing cost, provide new products and services. The benefits of distributed machine learning are undeniable, but it also presents serious privacy issues; there are possible internal and external attacks to the training data, which are stored in digital databases, such as social network data, web search histories, financial information, and medical records.\nThe general ADMM-based distributed learning has a certain level of privacy by avoiding the centralized collection of training datasets. Indeed, the decentralization has avoided the direct sharing of local dataset that contains sensitive information. However, deleting or anonymizing the sensitive information from the training dataset may reduce the accuracy of the learning model; even if the accuracy is not affected, some sensitive information can be still re-identified from the remaining information. These kinds of attacks have been studied in many works; for example, the adversary can use some background knowledge and cross correlation with other databeses to extract the private information [32, 30]. Other examples such as when the dataset has certain structural features an attacker is able to learn from the private model. These attackers can be from the outside as well as inside of the learing network.\nIn this paper, we focus on the ADMM-based distributed machine learning on the problem of classification. We use the empirical risk minimization (ERM) to construct the objective function of the problem. The ERM method use the dataset to construct an approximation of the expected risk , which is usually referred to the empirical risk. The classifier is chosen by minimizing the empirical risk. In this paper, we regularize the ERM with an additional term, the regularizer, in the empricial loss function to avoid overfitting,\nwhich means that although the minimum of the empirical risk can be close to zero, the expected risk we are interested in can be very large.\nOur goal is to develop an learning algorithm that can preserve the privacy of training data in every local node from both the internal and the external attackers during the entire learning process. Specifically, we develop randomized algorithms that can provide privacy in terms of \u03b1differential privacy [4, 9] while keeping the learning procedure accurate. Our algorithms hold for loss functions and regularizers that satisfy specific conditions of convexity and differentiability. For training, we propose two privacy preserving estimates of the regularized ERM-beased optimization. The first is primal variable perturbation; this is based on the output perturbation developed by Dwork et al. [4], which adds noise to the output of the non-private regularized ERM algorithm. In our method, we add noise to the intermediate updated primal variable of each node of ADMMbased distributed algorithm before sharing this primal variable to neighboring nodes. We call the second case dual variable perturbation, in which we perturb the dual variable of every node at each ADMM iteration before next iteration.\nOur results are applicable to general ERM optimization problems, and we use numerical experiments to the classification problem based on logistic regression. Differential privacy model aims to ensure that even if the adversary has knowledge of all the dataset except one data point, the adversary should not be able to distinguish whether an individual datapoint is present or absent, by adding randomness to the output of the algorithm; thus, the differential privacy not only aims to protect the specific data points present in the dataset but also all the possible datapoints for that dataset. Since there are no conditions ofr the dataset for the purpose of privacy preservation, the randomness incurs a cost in the performance while guaranteeing the differential privacy. Therefore, managing the tradeoff between privacy and accuracy is critical. Under the assumption that the data points in the dataset are drawn from an unknown but fixed distribution, we prove the accuracy of the distributed learning algorithm in terms of the privacy parameters. Another impor-\ntant issue is the convergence of ADMM. There are many convergences results for ADMM discussed in literature. Based on the accuracy analysis, we also discuss the convergence of our private ADMM method.\nThe contributions of this paper are shown as follows:\n\u2022 We derive a method, dual variable perturbation, in which we add randomness to the dual variable before the next update of the primal variable. The differential privacy is guaranteed for every ADMM iteration as well as the final trained output. \u2022 Based on the output perturbation developed by Dwork et al. [4], we develop a private ADMM-based distributed algorithm for regulatized ERM, which applies primal variable perturbation. In this technique, the randomness rises when every node transmits the primal parameter to the corresponding neighboring nodes. It is guaranteed to provide differential privacy for the every intermediate update. For the final update, we apply the dual variable perturbation in order to increase the accuracy. \u2022 We provide the theoretical guarantees of accuracy of both algorithms with L2 regularization. Based on the accuracy analysis, we also show that both algorithms are convergent in distribution with different probability densities. \u2022 We implement our methods by experiments on a dateset of UCI Machine Learning Repositories [16]. We provide a method to select the optimal privacy parameter \u03b1 by solving an optimization problem given a specific utility function of privacy. The test results show that both the algorithm performs similarly, but the primal variable perturbation slightly outperforms the dual variable perturbation. However, theoretical analysis shows that dual variable perturbation has higher probability of accuracy and better sample requirement than does\nthe primal case. Both algorithms are suitable for the both types of attacks we are interested in."}, {"heading": "1.1. Related Work", "text": "There has been a significant amount of literature on the distributed classification learning algorithms. These works mainly focus on either enhancing the efficiency of the learning model, or on producing a global classifier from multiple distributed local classifier trained at the corresponding individual node. In the first kind of these works, researchers focus on making the distributed algorithm suitable to datasets of very large size; some ([14]) use MapReduce to explore the performance improvements. The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]). Our distributed algorithm is based on ADMM, in which the centralized problem acts as a group of coupled distributed convex optimization subproblems with the consensus constaints on the primal parameters.\nResearch on privacy has been studied in a significant number of works since at least [20]. Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35]. Simple anonymization approaches are ineffective. Individual information can be re-identified by simply using a small amount of side information [32,16]. In privacy-preserving data mining research, the privacy can be pried through, for example, composition attacks, in which case the adversary have some prior knowledge. Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.\nThe idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details). The main perturbation techniques can be summarized into two basic classes. One is input perturbation, in which the training datasets are randomly modified prior to\nlearning. The other one is output perturbation, where the exact solution is obtained from the true datasets but the noisy randomized version of the solution is released. There exist some inherent limitations for these two methods. Since Agrawal and Srikant\u2019s work in [50], increasing number of work studies the limitations and applicability of noise perturbation, and the definitions of privacy started to expand. In Dwork et al.\u2019s basic definition of privacy [4], \u03b5-indistinguishability or differential privacy, a change in a single entry of the dataset incurs a small change in the distribution from the view of any adversary via a specific measure of distance in a worst-case scenario.\nDifferential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4]. Differential privacy is immune to the composition attacks mentioned above [30]. Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8]. Moreover, Wasserman and Zhou study the differential privacy more statistically. A body of exist literature also studies the differential-private machine learning. For example, Kasiviswanathan et al. derives a general method for probabilistically approximately correct (PAC, [47]) in [46]. Other examples includes the work of Blum et al. in [9] that provides a method to deliver the dataset differentially privately. Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]). There are two main approaches used in differential privacy: perturbation by Laplace noise, and other exponential mechanism. In this paper, we focus on the Laplace noise perturbation. Laplace noise perturbation, especially the Laplace noise addition, is the primary method for the differential privacy.\nThe rest of the paper is organized as follows. Section 2 outlines the centralized ERM objective function and then shows the equivalent distributed form; the corresponding privacy concerns are described. In Section 3, algorithms are produced, and the analysis of privacy guarantee is provided. Section 4 discusses the tradeoff between privacy and accuracy, and the convergence of the algo-\nrithms. Finally, Section 5 and 6 present numerical experiments and concluding remarks."}, {"heading": "2. Problem Statement", "text": "Consider a connected network shown in Figure 1, which contains P nodes described by one undirected graph G(P,E ) with the set of nodes P = {1,2,3, ...,P}, and edges E represened by lines denoting the linkes between connected nodes. A particular node p \u2208P only exchanges information between its neighboring node j \u2208Np, where j \u2208Np is the set of all neighboring nodes of node p, and Np = |Np| is the number of neighboring nodes of node p. The network is connected but not necessary fully connected; there can be local cycles (e.g. local central node p and i, j,k \u2208Np). In a connected network, there must exist a path i1, i2, i3, ..., im\u22121, im of length at least 1 connecting node i1 and im. Each node p contains a dataset Dp defined as follows:\nDp = {(xip,yip)\u2282 X\u00d7Y : i = 0,1, ...,Bp},\nof size Bp with data vector xip \u2208 X \u2286Rd , and the corresponding label yip \u2208Y = {\u22121,1}. The entire network therefore has a set of data as:\nD\u0302 = \u22c3 p\u2208P Dp.\nThe target of the centralized classification algorithm is to find a classifier f : X \u2192 Y using all available data D\u0302 that enables each node in the network to classify any data x\u2032 input to a label y\u2032 \u2208 {\u22121,1}.\nSuppose that D\u0302 is available to the fusion center node, then we can choose the global classifier f : X \u2192 Y that minimizes the following centralized regularized emprical risk minimization problem (CR-ERM)\nmin f\nZC( f |D\u0302) := CR\nBp\nP\n\u2211 p=1\nBp \u2211 i=1 L\u0302 (yip, f T xip)+\u03c1R( f ),\n(1) where CR \u2264 Bp is a regularization parameter, and \u03c1 > 0 is the parameter that controls the impact of the regularizer. The loss function L\u0302 (yip, f T xip) : Rd \u2192 R, is used to measure the quality of the classifier trained. In this paper, we focus on the specific loss function:\nL\u0302 (yip, f T xip) = L (yip f T xip)\nThe function R( f ) is a regularizer that prevent overfitting. We aim to solve the centralized optimization problem (1) in a distributed fashion while achieving the same performance as in the centralized case. The decentralized equivalent enables node p to contribute by optimizing only the p-dependent terms of the objective function without exchanging any training data to other nodes p\u2032 6= p. In this paper, we have the following assumptions\nAssumption 1. - The loss function L is strictly convex and doubly differentiable of f with |L \u2032| \u2264 1 and |L \u2032\u2032| \u2264 c1, where c1 is a constant. Both L and L \u2032 are continuous.\nAssumption 2. - The regularizer function R(\u00b7) is continuous differentiable and 1-strongly convex. Both R(\u00b7) and \u2207R(\u00b7) are continuous.\nAssumption 3. - We assume that \u2016xip\u2016\u2264 1. Since yip \u2208 {\u22121,1}, then |yip|= 1."}, {"heading": "2.1. Distributed ERM", "text": "To solve (1) in a distributed way, we first reform the objective function. The global variable f in CR-ERM is coupling the problem over the network. To decouple, we replace f by P copies of f ; thus the global variable becomes auxiliary pernode variables { fp}Pp=1. Consensus constraints are required to force necessary global consistency\ncondition f1 = f2 = ... = fP since the network is connected. Let ZD denote ZD({ fp}p\u2208P |D\u0302) be the decentralized objective function. An equivalent distributed form of the CR-ERM is\nmin { fp}Pp=1\nZD := CR\nBp\nP\n\u2211 p=1\nBp \u2211 i=1 L (yip f Tp xip)+ P \u2211 p=1 \u03c1R( fp).\ns.t. fp = f j, p = 1, ...,P, j \u2208Np. (2)\nNow the problem (2) can be solved distributively by using the alternative direction method of multiplier (ADMM).\nAccording to Lemma1 in [1], if { fp}Pp=1 represnet a feasible solution of (2) and the network is connected, then problems (1) and (2) are equivalent, that is, f = fp, p = 1, ...,P, where f is a feasible solution of (1).\nIn order to solve (2) by ADMM, we use the redundant variables {w jp} to assist to decouple fp of node p from its neighors j \u2208Np. Thus the distrubted regularized emprical risk minimization problem (DR-ERM) becomes\nmin { fp}Pp=1 ZD.\ns.t. fp = wp j,wp j = f j, p = 1, ...,P, j \u2208Np (3)\nThen the node-p-based individual objective function of (3) is\nZp( fp|Dp) := CR\nBp\nBp \u2211 i=1 L (yip f Tp xip)+\u03c1R( fp).\nThe augmented Lagrange funciton associated with the distributed optimization problem is:\nLD({ fp},{wp j},{\u03bb kp j}) =ZD + P\n\u2211 p=1 \u2211 i\u2208Np\n( \u03bb api )T ( fp\u2212wpi)\n+ P\n\u2211 p=1 \u2211 i\u2208Np\n( \u03bb bpi )T (wpi\u2212 fi)\n+ \u03b7 2\nP\n\u2211 p=1 \u2211 i\u2208Np (\u2016 fp\u2212wpi \u20162\n+ \u2016 wpi\u2212 fi \u20162). (4)\nThe distributed iterations solving (3) are:\n{ fp(t+1)}Pp=1 = arg min { fp}Pp=1\nLD ( { fp},{wp j(t)},{\u03bb kp j(t)} ) ,\n(5)\n{wp j(t +1)}Pp=1 = arg min {wp j}Pp=1\nLD ( { fp(t +1)},{wp j}, {\u03bb kp j(t)} ) ,\n(6)\n\u03bb ap j(t +1) = \u03bb a p j(t)+\u03b7( fp(t +1)\u2212wp j(t +1)),\np \u2208P, j \u2208Np, (7)\n\u03bb bp j(t +1) = \u03bb b p j(t)+\u03b7(wp j(t +1)\u2212 fp(t +1))\u2032\np \u2208P, j \u2208Np. (8)\nThe general ADMM convergence is shown in Appendix A. Since the iterations (5)-(8) are proved to have the general form of ADMM iterations (see Appendix I), then the convergence of the decentralized regularized ERM is guaranteed.\nFrom (4), the augmented Lagrange function is linear-quadratic in wpi; thus, there is a closed form of wpi(t +1) at each iteration. Then we can replace wpi terms in (5), (7), (8) by its closed expression. Moreover, by initializing the dual variables \u03bb kp j = 0d\u00d7d , and let \u03bbp(t) = \u2211 j\u2208Np \u03bb k p j, p\u2208P , j \u2208Np, k = a, b, we then can combine (7) and (8) into one update. As a result, the update procedures (5) to (8) can be further simplified through replacing wpi by its corresponding closed form in (4). The simplified ADMM iteration is shown as follows, due to Lemma 3 of [1].\nLet LN(t) denotes LN({ fp},{ fp(t)},{\u03bbp(t)}), and\nLN(t) =ZD +2 P\n\u2211 p=1 \u03bbp(t)T fp\n+\u03b7 P\n\u2211 p=1 \u2211 i\u2208Np \u2016 fp\u2212\n1 2 ( fp(t)+ fi(t)) \u20162 .\nThe ADMM iterations (5)-(8) can be reduced to\n{ fp(t+1)}Pp=1 = arg min { fp}Pp=1 LN({ fp},{ fp(t)},{\u03bbp(t)}),\n(9)\n\u03bbp(t +1) = \u03bbp(t)+ \u03b7 2 \u2211j\u2208Np [ fp(t +1)\u2212 f j(t +1)].\n(10) We denote the node-p-based nonprivate augmented Lagrange function\nLN p({ fp},{ fp(t)},{\u03bbp(t)}) as LN p(t):\nLN p(t) = CR\nBp\nBp \u2211 i=1 L (yip f Tp xip)+\u03c1R( fp)+2\u03bbp(t) T fp\n+\u03b7 \u2211 i\u2208Np \u2016 fp\u2212\n1 2 ( fp(t)+ fi(t)) \u20162 .\n(11) Thus, every node p updates fp(t + 1) at each iteration as follows\nfp(t +1) = argmin fp LN p(t).\nAlgorithm 1 Distributed ERM Required:Randomly initialize fp,\u03bbp = 0d\u00d71 for every p Inputs:D\u0302 1: for t = 0,1,2,3, ... Do 2: for p = 1,2,3, ...P Do 3: Compute fp(t +1) via (9). 4: end for 5: for p = 1,2,3, ...P Do 6: Broadcast fp(t +1) to all neighbors j \u2208Np 7: end for 8: for p = 1,2,3, ...P Do 9: Compute \u03bbp(t +1) via (10) 10: end for 11: end for Outputs: f \u2217\nADMM-based distributed ERM iterations (9) to (10) is illustrated in Figure 2 and summarized in Algorithm 1. Every node p \u2208P updates its local d\u00d71 estimates fp(t) and \u03bbp(t). At iteration t +1, node p updates the local fp(t +1) through\n(9). Next, node p broadcasts the latest fp(t+1) to all its neighboring nodes j \u2208Np. Iteration t + 1 finishes as each node updates the \u03bbp(t + 1) via (10).\nEvery iteration of our algorithm is still a minimization problem similar to the centralized problem (1). However, the number of variables participating in solving (9) per node per iteration, which is Np, is much smaller than that in the centralized problem, which is \u2211Pp=1 Np. There are several methods to solve (9). For instance, projected gradient method, Newton method, and Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) method that approximates the Newton method, to name a few.\nADMM based distributed machine learning has benefits due to high scalability, economic communication, and a certain level of privacy. The privacy arised is mainly due to the local parameter exchange among neighboring nodes instead of centralized communication. Dually, the parameter of each node is anonymous to the non-neighboring nodes. However, the neighboring nodes can access to the parameter without privacy protection; also, as shown in Section 1, simple anonymization is not good enough because it is still possible for adversary to extract the sensitive information with side information about the target."}, {"heading": "2.2. Privacy Concerns", "text": "Although the data stored at each node is not exchanged during the entire ADMM algorithm, the potential privacy risk still exists. Suppose the dataset Dp stored at node p contains sensitive information in data point (xi,yi) that is not allowed to be released to other nodes in the network or anyone else outside.\nIn the distributed version of algorithm, node p optimizes only the p-dependent parts of the centralized problem. Let K : Rd \u2192 R be the randomized version of Algorithm 1, and let { f \u2217p}p\u2208P be the output of K at all the nodes. Then { f \u2217p}p\u2208P is random. Let Ktp be the node-p-dependent subalgorithm of K at iteration t, and let fp(t) be the output of Ktp(Dp) at iteration t inputing Dp. fp(t) is random at each t.\nConsider an adversary, who knows all the data about node p except for the (xip,yip). The adversary is able to extract much additional information about (xip,yip) by observing the output of the algorithm. For the adversary inside the network, the sensitive information can even be leaked at any iteration of the training process. Therefore, it is necessary to develop a privacy preserved distributed ADMM algorithm for classification problem. We consider two types of attacks: \u2022 Type 1: This attack is from adversaries\noutside the network, who do not have access to the intermediate ADMM iteration. The attack observes the output { f \u2217p}p\u2208P of algorithm K and aims to extract additional information of the private data point of the training dataset.\n\u2022 Type 2: This attack is from the adversaries that can get access to the intermediate ADMM iterations. This attack aims to obtain additional information about the private data point of the the training dataset by observing the intermediate output fp(t) of Ktp for all p \u2208P .\nWe denote our privacy of distributed network based on the definition of differential privacy in [4]. Specifically, we require that a change of any single data point in the dataset might only change the distribution of the output of the algorithm slightly, which is visible to the adversary; this is done by adding randomness to the output of the algorithm. Let Dp and D\u2032p be two datasets differing in one data points; i.e., let (xip,yip)\u2282Dp, and (x\u2032ip,y \u2032 ip)\u2282D\u2032p, then (xip,yip) 6= (x\u2032ip,y\u2032ip). In other words, their Hamming Distance\nHd(Dp,D\u2032p) = Bp\n\u2211 i=0 1{i : xi 6= x\u2032i} (12)\nequals 1: i.e. Hd(Dp,D\u2032p) = 1.\nDefinition 1. (Networked \u03b1p-Differential Privacy) Consider a network consisits of P nodes P = 1, 2, ...P, and each node p has a training dataset Dp, and D\u0302 = \u22c3 p\u2208P Dp. Let K : Rd\u2192R be a randomized version of Algorithm 1. K outputs { f \u2217p}p\u2208P , where f \u2217p =K(Dp) is the corresponding output at node p. Let D\u2032p be any dataset with\nHd(D\u2032p,Dp) = 1, and let g \u2217 p = K(D \u2032 p). Then, K is networked \u03b1p-differential private, if for any datasets D\u2032p for all p \u2208P , known by the adversary of Type 1 attack, and for all possible sets of the outcomes S \u2286 R, the following inequality holds:\nPr[ f \u2217p \u2208 S]\u2264 e\u03b1p \u00b7Pr[g\u2217p \u2208 S]. (13)\nThe probability is taken over f \u2217p the output of K(\u00b7) at each node p \u2208P . The privacy raised is called networked \u03b1p-differential Privacy.\nDefinition 1 specifies the privacy required against Type 1 attack. More specifically, networked \u03b1p-differential private algorithms can prevent adversaries from obtaining much additional information by simply observing the output of the algorithm. This is because that no matter how the adversaries adjust the dataset D\u2032p (Hd(D \u2032 p,Dp) = 1), the distribution of output can only change slightly.\nFor the privacy preserved against Type 2 attack, we have the following definition.\nDefinition 2. (Dynamic \u03b1 tp-Differential Privacy) Consider a network consisits of P nodes P = 1, 2, ...P, and each node p has a training dataset Dp, and D\u0302 = \u22c3 p\u2208P Dp. Let K : Rd \u2192 R be a randomized version of Algorithm 1. Let Ktp be the node-p-dependent sub-algorithm of K, optimizating ADMM iteration at t and outputing fp(t). Let D\u2032p be any dataset with Hd(D \u2032 p,Dp) = 1, and let gp(t) = Ktp(D \u2032 p). We say that the algorithm K is dynamic \u03b1 tp-differential private if for any dataset D\u2032p for all p\u2208P known by the adversary of Type 2 attack, and for all possible sets of the outcomes S\u2286R, the following inequality holds:\nPr[ fp(t) \u2208 S]\u2264 e\u03b1 t p \u00b7Pr[gp(t) \u2208 S], (14)\nfor all time t during a learning process. The probability is taken over fp(t), the output of Ktp. The privacy raised for algorithm K is called dynamic \u03b1 tp-differential Privacy.\nDefinition 2 provides the privacy against Type 2 attack. Dually, in dynamic \u03b1 tp-differential private algorithms, adversaries of Type 2 attack cannot extract much additional information by observing the intermediate updates of fp(t). This is because\nthat inputing any D\u2032p with Hd(D \u2032 p,Dp) = 1 to the algorithm, the distribution of the output will not change much if any one data point is changed in D\u2032p.\nClearly, the algorithm with ADMM iterations shown in (9) to (11) is neither networked \u03b1pdifferential private nor dynamic \u03b1 tp-differential private. This is because the intermedate and final optimal output fp\u2019s are deterministic given dataset Dp. For D\u2032p with Hd(Dp,D \u2032 p) = 1, the classifier will change completely, and the probability density Pr([ fp|D\u2032p]) = 0, which leads to the ratio of probabilities Pr[ fp|Dp]Pr[ fp|D\u2032p] \u2192 \u221e.\nIn order to provide the differential privacies defnined in Definition 1 and 2, we propose two algorithms, dual variable perturbation and primal variable perturbation, which are described in Section 3.1 and 3.2, respectively. Both algorithms can provide the two types of differential privacy defined in Section 2.2. However, we modify the primal variable perturbation by replacing the last ADMM iteration with dual variable perturbation in order to improve the accruacy of the final outputed classifier."}, {"heading": "3. Dynamic Private Preserving", "text": "In this section, we describe two algorithms that provide networked and dynamic \u03b1-differential privacy defined in Section 2.2, respectively."}, {"heading": "3.1. Dual Variable Perturbation", "text": "In order to provide differential privacy defined in Definition 1 and 2, we introduce our first private algorithm, dual variable perturbation, in which we perturb the dual variable {\u03bbp(t)}Pp=1 with a random noise vector \u03b5p(t), which has the probability density function:\nK (\u03b5)\u223c e\u2212\u03b6p(t)\u2016\u03b5\u2016, (15)\nwhere \u03b6p(t) is a parameter related to the value of \u03b1p(t). Let \u00b5p(t) = \u03bbp(t)+ \u03b5p(t) be the perturbed dual variable. Now the corresponding nodep-based augmented Lagrange function LN p(t) becomes Ldual ( fp, fp(t),\u00b5p(t + 1),{ fi(t)}i\u2208Np ) .\nUse Ldual(t) to denote Ldual (\nfp, fp(t),\u00b5p(t + 1),{ fi(t)}i\u2208Np ) , and we have\nLdual(t) = CR\nBp\nBp \u2211 i=1 L (yip f Tp xip)+\u03c1R( fp)\n+2\u00b5p(t +1)T fp + \u03a6 2 \u2016 fp \u20162\n+\u03b7 \u2211 i\u2208Np \u2016 fp\u2212\n1 2 ( fp(t)+ fi(t)) \u20162,\n(16) where \u03a62 \u2016 fp \u2016\n2 is an additional penalizer. As a result, the minimization of Ldual(t) becomes random. We slightly change the iterations (9) to (10) as follows:\n\u00b5p(t +1) = \u03bbp(t)+ CR\n2Bp \u03b5p(t +1), (17)\nfp(t +1) = argmin fp Ldual(t), (18)\n\u03bbp(t +1) = \u03bbp(t)+ \u03b7 2 \u2211j\u2208Np [ fp(t +1)\u2212 f j(t +1)].\n(19) We perturb the dual variable \u03bbp(t) via an additional variable \u00b5p in (19). This is because the dual variable is not exchanged during the training and is only used within the corresponding node; thus the direct perturbation to \u03bbp will affect the accuracy by the accumulated noise and is not necessary. We have the following theorem.\nTheorem 1. Let \u03b1\u0302 = \u03b1p(t) \u2212 ln ( 1 +\nc1 Bp CR ( \u03c1+2\u03b7Np ))2. If \u03b1\u0302 > 0, then \u03a6 = 0; else, let \u03a6 = c1Bp\nCR (e\u03b1p(t)/4\u22121)\n\u2212\u03c1\u22122\u03b7Np, and as a result\n\u03b1\u0302 = \u03b1p(t)/2. Under Assumption 1, 2 and 3, if the distributed classification optimization problem with objective function (2) can be solved by Algorithm 2, then the algorithm A1 solving this distributed problem is dynamic \u03b1-differential private with \u03b1p(t) for each node p \u2208P at time\nt. The ratio of conditional probabilities of fp(t) is bounded as:\nQ( fp(t)|D) Q( fp(t)|D\u2032p) \u2264 e\u03b1p(t), (20)\nwhere Q( fp(t)|D) and Q( fp(t)|D\u2032p) are the probability density functions of fp(t) given dataset D and D\u2032p, respectively, and Hd(D,D \u2032 p) = 1.\nProof: See Appendix B\nAlgorithm 2 Dual Variable Perturbation Required:Randomly initialize fp,\u03bbp = 0d\u00d71 for every p Inputs:D\u0302,{[\u03b1p(1),\u03b1p(2), ...]}Pp=1 1: for t = 0,1,2,3,... Do 2: for p = 1,2,3,...P Do 3: Let \u03b1\u0302 = \u03b1p(t)\u2212 ln ( 1+ c1Bp\nCR\n( \u03c1+2\u03b7Np ))2. 4: If \u03b1\u0302 > 0, then \u03a6 = 0, else, \u03a6 =\nc1 Bp CR (e\u03b1p(t)/4\u22121) \u2212\n\u03c1\u22122\u03b7Np and \u03b1\u0302 = \u03b1p(t)/2. 5: Draw noise \u03b5p(t) according to (15) with \u03b6p(t) = \u03b1\u0302 6: Compute \u00b5p(t +1) via (17) 7: Compute fp(t +1) via (15) 8: with augmented Lagrange function as (16). 9: end for 10: for p = 1,2,3,...P Do 11: Broadcast fp(t+1) to all neighbors j \u2208Np 12: end for 13: for p = 1,2,3,...P Do 14: Compute \u03bbp(t +1) via (16) 15: end for 16: end for Outputs: { f \u2217p}Pp=1\nThe algorithm corresponding to Theorem 1 is illustrated in Figure 3 (a) and (c), and summarized in Algorithm 2. All nodes have its corresponding value of \u03c1 . Every node p \u2208P updates its local estimates \u00b5p(t), fp(t) and \u03bbp(t) at time t; at time t+1, node p first perturbs the dual variable \u03bbp(t) obtained at time t to get \u00b5p(t + 1) via (17), and then uses training dataset Dp to compute fp(t+1) via (18). Next, node p distributes fp(t + 1) to all its neighboring nodes. The (t + 1)-th update finishes when each node has updated its local\n\u03bbp(t + 1) via (19). The final iteration is exactly the same as the intermeidate iterations.\nTheorem 1 links Algorithm 1 with Definition 2. We observe that an algorithm satisfies Definition 2 also satisfies Definition 1 for any individual node p\u2208P . Therefore, any distributed algorithm that is dynamic \u03b1-differential private is also networked \u03b1-differential private. Thus, we have the following corollary.\nCorollary 1.1. If each node in the network chooses the same privacy parameter \u03b1p(t) = \u03b1\u2217(t) for all p \u2208P at each time, then the algorithm meets Theorem 1 also provide networked \u03b1-differential privacy with \u03b1 = \u03b1\u2217(t).\nCorollary 1.1 can be proved by directly substituting \u03b1p(t) = \u03b1\u2217(t) for all p \u2208P .\nThe definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset. However, in many cases, the sensitive information may be contained in more than one data point. Actually, Theorem 1 can be extended to deal with the dataset that has multiple sensitive data entries.\nCorollary 1.2. Let D and D\u2032p be two datasets with Hd(D,D\u2032p) = c2, c2 \u2265 1. Then an algorithm meets Theorem 1 can compute a fp(t) that has the following bounded ratio of condtional densities:\nQ( fp(t)|D) Q( fp(t)|D\u2032p) \u2264 ec2\u03b1 \u2032p(t), (21)\nProof: See Appendix C.\nHowever, the output distribution must change corresponging to the a change of multiple data entries; as a result, the level of privacy has to decrease, especially for large c2 and large BP."}, {"heading": "3.2. Primal Variable Perturbation", "text": "In this case, we perturb the primal variable { fp(t + 1)}Pp=0 before releasing this variable to the neighboring nodes of each local node. This algorithm can also provide differential privacy defined in Definition 1 and 2. Let\nthe node-p-based augmented Lagrange function Lprim ( fp, fp(t),\u03b5p(t),\u03bbp(t),{Vi(t)}i\u2208Np ) be represented as Lprim(t):\nLprim(t) = CR\nBp\nBp \u2211 i=1 L (yip f Tp xip)+\u03c1R( fp)+2\u03bbp(t) T fp\n+\u03b7 \u2211 i\u2208Np \u2016 fp\u2212\n1 2 ( fp(t)+Vi(t)\u2212 \u03b5p(t)) \u20162 .\nWe use the un-perturbed primal fp(t) obtained at time t in the augmented Lagrange function and subtract the noise vector \u03b5p(t) added at time t in order to reduce the noise in the minimization in (22); \u03b5p(t) is static at time t +1. The privacy of releasing primal variable is not affected.\nThe following iterations specify the corresponding ADMM iterations.\nThe distributed iteration provideing dynamic \u03b1p(t)-differential privacy at time t is\nfp(t +1) = argmin fp Lprim(t), (22)\nVp(t +1) = fp(t +1)+ \u03b5p(t +1), (23)\n\u03bbp(t +1) = \u03bbp(t)+ \u03b7 2 \u2211j\u2208Np [Vp(t +1)\u2212Vj(t +1)], (24) where \u03b5p(t + 1) is the random noise vector with the density function (15). The aurgmented Lagrange function is (11). When the ADMM iteration meets the stop time, we input D\u0302, and the latest { fp(t)}p and {\u03bbp(t)}p obtained from (22) and (24), respectively, to Algorithm 1 to iterate (17) to (19) one time.\nThe following theorem states the result of primal variable perturbation.\nTheorem 2. Under Assumption 1, 2 and 3, if the distributed classification optimization problem with objective function (2) can be solved by Algorithm 3 with \u03b6p(t) = \u03c1Bp\u03b1p(t) 2 , then the algorithm A2 solving this distributed problem is dynamic \u03b1-differential private with \u03b1p(t) for each node p \u2208P at time t. The ratio of conditional probabilities of fp(t) is bounded as in (20).\nProof: See Appendix D.\nCorollary 1.1 and 1.2 also hold for Theorem 2.\nCorollary 2.1. If all the nodes have the same privacy parameter \u03b1\u2217(t) at each time, then the algorithm meets Theorem 2 also provide networked \u03b1-differential privacy with \u03b1 = \u03b1\u2217(t).\nSimilar to Corollary 1.1, Corollary 2.1 can be proved by substituting \u03b1 = \u03b1\u2217(t) for all p \u2208P .\nCorollary 2.2. Let D and D\u2032p be two datasets with Hd(D,D\u2032p) = c2, c2 \u2265 1. Any algorithm satisfies Theorem 2 can produce a private fp(t), which has the following bounded ratio of condtional densities at each iteration:\nQ( fp(t)|D) Q( fp(t)|D\u2032p) \u2264 ec2\u03b1 \u2032p(t). (25)\nThe proof of Corollary 2.2 is the same as that of Corollary 1.2 in Appendix C.\nAlgorithm 3 Primal Variable Perturbation Required:Randomly initialize fp,\u03bbp = 0d\u00d71 for every p Inputs:D\u0302,{[\u03b1p(1),\u03b1p(2), ...]}Pp=1 1: for t = 0,1,2,3,... Do 2: for p = 1,2,3,...P Do 3: Draw noise \u03b5p(t) according to (15) with\n\u03b6p(t) = \u03c1Bp\u03b1p(t)\n2CR 4: Compute fp(t +1) via (22) 5: with augmented Lagrange function as (11). 6: Compute Vp(t +1) via (23) 7: end for 8: for p = 1,2,3,...P Do 9: Broadcast Vp(t +1) to all neighbors j \u2208Np 10: end for 11: for p = 1,2,3,...P Do 12: Compute \u03bbp(t +1) via (16) 13: end for 14: if t = stop time 15 Input D\u0302, and the latest { fp(t)}p and {\u03bbp(t)}p obtained in above Step 4 and 12, respectively, to Algorithm 1 to\niterate Step 1 once. 16: end for Outputs: { f \u2217p}Pp=1\nThe algorithm associated with Theorem 2 is illustrated in Figure 3 (b)-(c), and is summarized in Algorithm 3. Each node p \u2208P updates fp(t),\nVp(t) and \u03bbp(t) at time t. Then, at time t + 1, training dataset is used to compute fp(t + 1) via (22), which is then perturbed to obtain Vp(t +1) via (23). Next, Vp(t + 1) is distributed to all the neighboring nodes of node p. Finally, \u03bbp(t+1) is updated via (24). The final iteration follows the dual variable perturbation."}, {"heading": "4. Accuracy and Convergence Analysis", "text": "In this section, we discuss the accuracy of Algorithm 1 and 2. We establish performance bounds for regularization functions with L2 norm. Our analysis is based on the following assumptions:\nAssumption 4. - The data points {(xpi,ypi)} Bp i=1 are drawn i.i.d. from a fixed but unknown probability distribution Pxy(xpi,ypi).\nAssumption 5. - \u03b5p(t) is drawn from (15) with the same \u03b1p(t) = \u03b1(t) for all p \u2208P .\nWe define the expected loss as\nC\u0302( fp) :=CRE(x,y)\u223cPxy(L (y f T x)).\nLet Z\u0302 be the expected objective as\nZ\u0302( fp) := C\u0302( fp)+\u03c1R( fp).\nWe also defined the constrained objectives for perturbed ADMM-based algorithms. Let \u03b5 pi(t) = \u03b5p(t)\u2212 \u03b5i(t), for i \u2208 Np. Specifically, at each iteration t, we define:\nZdual( fp, t|Dp) := Zp( fp|Dp)+ CR\nBp \u03b5p(t)T fp,\nZprim( fp, t|Dp) := Zp( fp|Dp)\n\u2212\u03b7 \u2211 i\u2208Np\n( ( fp\u2212\n1 2 ( fp(t)+ fi(t))T\n\u00b7 (\u03b5 pi(t))+ 1 4 ( \u03b5 pi(t) )2) .\nLet \u03b5 tp = \u03b5p(t), the noise vector generated at time t. The objective Zdual( fp, t|Dp) (respectively, Zprim( fp, t|Dp)) is the corresponding nodep based objective function for the Algorithm 1 (respectively, Algorithm 2) if we fix the noise as \u03b5 tp generated at time t for Ldual( fp, t|Dp) throughout the entire ADMM process.\nLet f\u0302p(t + 1), f nonp (t + 1) and f \u2217 p(t + 1) be the population optimum, (non-private) empirical optimum, and private (empirical) optimum, respectively, defined at iteration t +1 as:\nf\u0302p(t +1) = argmin fp Z\u0302( fp),\nf nonp (t +1) = argminfp Zp( fp, t|Dp),\nf \u2217p(t +1) = argminfp Z( fp, t|Dp),\nwhere Z represents Zdual or Zprim, respectively. Let Fp(t + 1) = argmin fp LnonP( fp, t|Dp)be the updated non-private classifier at iteration t + 1. From Theorem 9 (see Appendix A), the sequence {Fp(t +1)} is bounded and converges to an optimal value f nonp (t + 1) as time t \u2192 \u221e. Thus,there exists a constant \u2206non(t) such that:\nC\u0302(Fp(t))\u2212C\u0302( f non(t))\u2264 \u2206non(t).\nLet fp(t+1) be the minimizer of the corresponding augmented Lagrange function of Zpriv at time t. Since both Zdual( fp, t|Dp) and Zprim( fp, t|Dp) are real and convex; similarly, the sequence { fp(t)} is bounded and fp(t) converges to f \u2217p(t), which is a limit point of fp(t), and there exists a constant \u2206privp (t)=\u2206dualp (t) or \u2206 prim p (t) given noise vector \u03b5p(t) such that\nC\u0302( fp(t))\u2212C\u0302( f \u2217p(t))\u2264 \u2206privp (t).\nWe will show that the performance of the algorithm can depend on the number of data points, Bp, of the dataset Dp, for all p\u2208P . Let f 0p(t) be a reference classifier at time t with the expected loss as C\u0302\u2217 = C\u0302( f 0p(t)). Specifically, the performance of the algorithm is measured by the Bp, which is a function of \u2016 f 0p(t) \u2016 required to obtain a classifier fp(t) that minimizes the expected loss within some accuracy:\nC\u0302( fp(t))\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206privp (t).\nwhere \u03b1acc is the optimization accuracy. We say that every learned fp(t) is \u03b1acc-optimal if it satisfies the above inequality. First, we provide the theorem about the performance of the non-private ADMM-based algorithm.\nTheorem 3. Let R( fp(t))= 12 \u2016 fp(t) \u2016 2, and f 0p(t) such that C\u0302( f 0p(t)) = C \u2217 E(t) for all p \u2208 P at\ntime t, and a real number \u03b4 > 0. Let Fp(t + 1) = argmin fp LnonP( fp, t|Dp) be the output of Algorithm 1. If Assumption 1 and 4 are satisfied, then there exists a constant \u03b2non such that if the number of data points, Bp in Dp = { (xip,yip) \u2282\nR d\u00d7{\u22121,1} } satisfy:\nBp > \u03b2non max ({CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ) ,\nthen Fp(t +1) satisfies: P ( C\u0302(Fp(t+1))\u2264 C\u0302\u2217(t+1)+\u03b1acc+\u2206non(t) ) \u2265 1\u2212\u03b4 .\nProof: See Appendix E.\nWe now establish the performance bounds for Algorithm 1, dual variable perturbation, which is summarized in the following theorem.\nTheorem 4. Let R( fp(t))= 12 \u2016 fp(t) \u2016 2, and f 0p(t) such that C\u0302( f 0p(t)) =C \u2217 E(t) for all p \u2208P , and a real number \u03b4 > 0. If Assumption 1, 4 and 5 are satisfied, then there exists a constant \u03b2dual such that if the number of data points, Bp in Dp ={ (xip,yip)\u2282Rd\u00d7{\u22121,1} } satisfy:\nBp > \u03b2dual max ({\u2016 f 0p(t +1) \u2016 d ln( d\u03b4 ) \u03b1acc\u03b1p(t) } t=1 ,\n{CRc1 \u2016 f 0p(t +1) \u20162 \u03b1acc\u03b1p(t) } t=1 ,\n{CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ) ,\nthen f \u2217p(t +1) satisfies:\nP ( C\u0302( f \u2217p(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22122\u03b4 .\nProof: See Appendix F.\nCorollary 4.1. Let fp(t + 1) = argminLdual( fp, t|Dp) be the intermediate updated classifier of Algorithm 2 and let f 0p(t) be a reference classifier such that C\u0302( f 0p(t) = C\u0302\n\u2217(t). If all the conditions of Theorem 3 are satisfied, then, fp(t +1) satisfies\nP ( C\u0302( fp(t+1))\u2264 C\u0302\u2217(t)+\u03b1acc+\u2206dualp (t) ) \u2265 1\u22122\u03b4 .\nProof: The following holds for fp(t) and f \u2217p(t)\nC\u0302( fp(t))\u2212C\u0302( f \u2217p(t))\u2264 \u2206dualp (t).\nFrom Theorem 3, P ( C\u0302( f \u2217p(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22122\u03b4 .\nTherefore, we can have: P ( C\u0302( fp(t +1))\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206dualp ) \u2265 1\u22122\u03b4 .\nTheorem 4 and Corollary 4.1 can guarantee the privacy defined in both Definition 1 and 2. The following theorem is used to analyze the performance bound of un-perturbed classifier fp(t +1) in (22), which minimizes Lprim(t) that involves noise vectors from Vp(t) perturbed at the previous iteration.\nTheorem 5. Let R( fp(t))= 12 \u2016 fp(t) \u2016 2, and f 0p(t) such that C\u0302( f 0p(t)) = C \u2217 E(t), and a real number \u03b4 > 0. From Assumption 1, we have the loss function L (\u00b7) is convex and differentiable with L \u2032(\u00b7) \u2264 1. If Assumption 4 and 5 are satisfied, then there exists a constant \u03b2 Aprim such that if the number of data points, Bp in Dp = { (xip,yip) \u2282\nR d\u00d7{\u22121,1} } satisfies:\nBp > \u03b2 Aprim max ({CR \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 ) \u03b12acc\u03b1p(t) } t=1 ,\n{CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ) ,\nthen f \u2217p(t +1) satisfies: P ( C\u0302( f \u2217p(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22122\u03b4 .\nProof: See Appendix G.\nTheorem 6. Let R( fp(t)) = 12 \u2016 fp(t) \u2016 2, and f 0p(t) such that C\u0302( f 0 p(t)) = C \u2217 E(t), and a real number \u03b4 > 0. Let f \u2217p(t + 1) = argminZprim(t) be \u03b1acc-accurate according to Theorem 4. From Assumption 1 we have that the loss function L (\u00b7) is convex and differentiable with L \u2032(\u00b7) \u2264 1, and we also assume that L \u2032 satisfies:\n|L \u2032(a)\u2212L \u2032(b)| \u2264 c4|a\u2212b|\nfor all pairs (a,b) with a constant c4. If Assumption 4 and 5 are satisfied, then there exists a constant \u03b2 Bprim such that if the number of data points, Bp in Dp = { (xip,yip) \u2282 Rd \u00d7{\u22121,1}\n} satisfies:\nBp >\u03b2 Bprim max ({CR \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 )\n\u03b12acc\u03b1p(t)\n} t=1\n,{CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ,\n{4CB \u2016 f 0(t +1) \u2016 d( ln( d\u03b4 ))2 \u03b1acc\u03b1p(t) } t=1 ,\n{4 \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 ) \u03b12acc\u03b1p(t) } t=1 ,\n{4(CR) 32 \u2016 f 0p(t +1) \u20162 d ln( d\u03b4 ) \u03b13/2acc \u03b1p(t) } t=1 ) ,\nthen V \u2217p (t +1) = f \u2217 p(t +1)+ \u03b5p(t +1) satisfies: P ( C\u0302(V \u2217p (t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22123\u03b4 .\nProof: See Appendix H.\nCorollary 6.1. Let fp(t + 1) = argminLprim( fp, t|Dp) be the intermediate updated classifier of Algorithm 3, and let f 0p(t) be a reference classifier such that C\u0302( f 0p(t) = C\u0302\n\u2217(t). If all the conditions of Theorem 5 are satisfied, then, Vp(t +1) = fp(t +1)+ \u03b5p(t +1) satisfies\nP ( C\u0302(Vp(t+1))\u2264 C\u0302\u2217(t)+\u03b1acc+\u2206primp (t) ) \u2265 1\u22123\u03b4 .\nProof: Since\nC\u0302( fp(t))\u2212C\u0302( f \u2217p(t))\u2264 \u2206primp (t),\nthen\nC\u0302(Vp(t))\u2212C\u0302(V \u2217p (t))\u2264 \u2206primp (t).\nFrom Theorem 5, V \u2217p (t +1) satisfies P ( C\u0302(V \u2217p (t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22123\u03b4 .\nTherefore, we have: P ( C\u0302(Vp(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc+\u2206primp (t) ) \u2265 1\u22123\u03b4 .\nSince at the last iteration of primal variable perturbation we use the same iteration as that\nof the dual variable perturbation, Theorem 6 and Corollary 6.1 only guarantee the dynamic \u03b1 tpdifferential privacy for primal variable perturbation. As a result, we combine the conditions of Theorem 4 and 6 to guarantee the networked \u03b1pdifferential privacy. Thus, we have the following corollary.\nCorollary 6.2. Let f \u2217p be the final output classifier of Algorithm 3 of node p, and let f 0p(t) be a reference classifier such that C\u0302( f 0p(t) = C\u0302\n\u2217(t). If all the conditions of Theorem 4 and 6 are satisfied, then, f \u2217p satisfies\nP ( C\u0302( f \u2217p)\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206dualp (t) ) \u2265 1\u22125\u03b4 .\nProof: We need all the conditions of Theorem 6 to be satisfied in order to guarantee the privacy during the intermediate iterations. All the conditions of Theorem 4 are satisfied so that the networked \u03b1p-differential privacy is provided. Combining Theorem 4 and 6 gives the probability no less than 1\u22125\u03b4 .\nClearly, the privacy rises by trading the accuracy. It is essential to manage the tradeoff between the privacy and accuracy in order to establish both the privacy and accuracy with at least satisfied level.\nAnother important issue we care about is the convergence of the Algorithm 1 and 2. Our analysis based on the assumption that all the conditions of Theorem 3 to 5 are satisfed. As shown in Appendix A, the non-private ADMM algorithm is convergent. In our private algorithms, the augmented Lagrange function (11) and (16) are solvable since both of them are convex. Also, the matrix A = Id is an identity matrix in our case, thus AT A is nonsingular. Theorem 9 shows that the non-private ADMM-based optimizaiton is convergent. However, our algorithms do not necessarily converge to one optimum classifier for all the nodes; different node can have different value of convergent classifier, but all of them have similar performance.\nWe first analysis the convergence of dual variable perturbation. We summarize the convergence analysis in the following theorem.\nProposition 7. Let f 0p(t) be a reference classifier such that C\u0302( f 0p) = C \u2217 E(t) for all node p \u2208P at\ntime t. If all the conditions of Theorem 4 are satisfied, then fp(t) = argminLdual( fp, t \u2212 1|Dp) is convergent in distribution with probability \u2264 1\u22122\u03b4 . Proof: See Appendix J.\nThe convergence of primal variable perturbation only consider the primal variable fp(t+1) at each time before perturbation. It is summarized in the folowing theorem.\nProposition 8. Let R( fp(t)) = 12 \u2016 fp(t) \u2016 2, and f 0p(t) such that C\u0302( f 0 p(t)) = C \u2217 E(t), and a real number \u03b4 > 0. If all the conditions of Theorem 6 is satisfied, then fp(t) = argminLprim( fp, t|Dp) is convergent in distribution with probability 1\u22123\u03b4 . Proof: See Appendix K."}, {"heading": "5. Numerical Experiments", "text": "In this section, we test Algorithm 2 and 3 with real world training dataset. Consider the following examples. The classification method used is logistic regression. Potential application scenarios include but nor limited to the following two.\nExample 5.1. (Potential Customer Classification) Consider a network of P companies agreed to collaborate to develop an algorithm that can classify the target customers by predicting their annual incomes based on thier information such as age, sex, occupation, and education. Suppose Dp is the customer data records stored at company p. The learning process of the algorithm is based on all available datasets {Dp}Pp=1, rather than company p alone. The company p learns the model only by its own training dataset Dp, and there is no data exchange between different companies. The intermediate updated classifier fp(t) is the only shared information. Moreover, company p only communicate with its neighboring companies. The companies want to increase the privacy level of the algorithm, and make sure the final algorithm and also the learning process preserves the privacy of the sensitive information against other companies in this network as well as other parties from outside.\nExample 5.2. (International Collaborative AntiTerrorist) Consider a group of countries with\ncorresponding datasets D containing intelligence about the terrorism. All the countries are willing to collaborate in order to classify jointly possible terrorist entering their countries. However, the confidential information involved in the intelligence prevents each countries to open access to the dataset of other countries. In this case, differential privacy model can preserve the confidential intelligence while producing an accurate classifier of terrorist."}, {"heading": "5.1. Privacy Preserved Logistic Regression", "text": "In the experiments, we use our algorithm to develop a dynamic differential private logisitic regression. The logistic regression has the loss function:\nLLR(yip f T xip) = log(1+ exp(\u2212yip f Tp xip)). (26) The first derivative and teh second derivative are:\nL \u2032LR = \u2212yipxip\n1+ exp(yip f Tp xip)\nL \u2032\u2032LR = y2ipxipx T ip\n(1+ exp(yip f Tp xip)(1+ exp(\u2212yip f Tp xip) ,\nwhich can be bounded as |L \u2032LR| \u2264 1 and L \u2032\u2032LR\u2264 14 , respectively, according to Assumption 3. Therefore, the loss function of logistic regression satisfies the conditions shown in Assumption 1. In this case, R(Fp) = 12 \u2016 fp \u2016\n2, and c1 = 14 . And we can directly apply the loss function LLR to Theorem 1 and 2 with R( f ) = 12 \u2016 fp \u2016\n2, and c1 = 14 , and then it can provide \u03b1p(t)-differential privacy for any p \u2208P at time t = 1, 2, ... of a distributed logistic regression problem."}, {"heading": "5.2. Pre-Processing", "text": "We test our algorithms to this example. The classification method used is logistic regression. We simulate the customer information by Adult dataset from UCI Machine Learning Repository [11], which contains demographic information such as age, sex, education, occupation, marital status, and native country. There are 48842 data samples. The prediction task is to determine\nwhether a person\u2019s annul income is greater than $50k.\nIn order to process the Adult dataset to our algorithm, we remove all the missing data points, and follow the data cleaning process of [12]. Also, we convert the categorial attributes to a binary vector. For other non-numerical descriptive attributes such as different countries in the category of native country, we replace them by their own frequency of occurance in the corresponding category. Moreover, each column is first normalized to make sure the maximum value is 1; then each row is normalized so that the L2 norm of each data sample is at most 1."}, {"heading": "5.3. Privacy-Accuracy Tradeoff", "text": "In this experiment, we study the privacyaccuracy tradeoff of Algorithm 2 and 3. The privacy is quantified by the value of \u03b1p(t).\nWhen \u03b1p(t) becomes larger, the ratio of densities of the classifier fp(t) on two different data sets is larger, which implies a higher belief of the adversary when one data point in data set D is changed; thus, it provides lower privacy. However, the accuracy of the algorithm increases as \u03b1p(t) becomes larger. As shown in Figure 4, larger \u03b1p(t) gives better convergence of the algorithms; moreover, from Figure 4, we can see that the dual variable perturbation is slightly more robust to noise than is the primal case given the same value of \u03b1p(t). When \u03b1p(t) is small, the model is more private but less accurate. Therefore, the utilities of privacy and accuracy shoud satisfy the following assumption:\nAssumption 6. - the utilities of privacy and accuracy should be monotonic with respect to\n\u03b1p(t) but in different directions, say decreasingly and increasingly, respectively.\nAs a result, The quality of classifier is measured by the empirical loss C(t) = C R\nBp \u2211 Bp i=1 L (yip fp(t) T xip). Given the dataset Dp and a \u03b1p(tx) at a specific time tx, there exists a corresponding fp(tx) minimizing (16). Thus, there must be a function Lacc() to capture relationship between \u03b1p(t) and C(t): Lacc(\u03b1p(t)) = C(t). The function Lacc is obtained by curve fitting given the experimental data points (\u03b1p(t),C(t)). Let Upriv(\u03b1p(t)) be the utility of privacy. Besides the decreasing monotonicity, Upriv(\u03b1p(t)) should be convex and doubly differentiable function of \u03b1p(t).\nGiven the privacy utility function Upriv(\u03b1p(t)), there exists an optimal value of \u03b1\u2217p(t) that minimizes the following problem:\nminJ (t) = Lacc(\u03b1p(t))\u2212Upriv(\u03b1p(t)) s.t. 0 < \u03b1p(t)\u2264 \u03b1U , 0\u2264 Lacc(\u03b1p(t))\u2264 c3\n(27) where \u03b1U and c3 are the threshold values for \u03b1p(t) and Lacc, respectively, beyond which is considered as non-private and non-accurate, respectively. The above discussion is summarized in the following definition.\nDefinition 3. (Optimal Private) If there exists a value of privacy parameter \u03b1\u2217p(t) that minimizes (35):\n\u03b1\u2217p(t) = arg min\u03b1p(t) J (t)\ns.t. \u03b1L \u2264 \u03b1p(t)\u2264 \u03b1U , 0\u2264 Lacc(\u03b1p(t))\u2264 c3 (28) then by choosing thie value as the privacy prarmeter, every iteration of Algorithm 1 and 2 for each node p \u2208P is optimal private.\nFor training the classifier, we tried a few fixed values of \u03c1 and test the empirical loss Lep(t) of the classifier. Then, we selected the value of \u03c1 that minimizes the empirical loss for a fixed \u03b1p (0.3 in this experiment). We also test the nonprivate version of Algorithm, and the corresponding minimum value of \u03c1 is obtained as the control. We chose the corresponding optimal value of the regularization prarmeter \u03c1 for each algorithm as shown in Table 1.\nTable 2 shows the values of CR chosen for each algorithm and the non-private case. Figure (4) shows the convergence of dual and primal\nvariable perturbations at different value of \u03b1p(t). Larger values of \u03b1p yields better convergence for both perturbations. Moreover, the dual variable perturbation has smaller variance of empirical loss than does the primal perturbation. However, larger \u03b1p incurs poorer privacy. This tradeoff is discussed below.\nThe utility function of privacy is chosen according to the specification in Section 4 as:\nUpriv(\u03b1p(t)) = \u03c9p1 \u00b7 ln \u03c9p2\n\u03c9p3\u03b1p(t)+\u03c9p4\u03b12p(t) ,\n(29) where \u03c9p1 and \u03c9p2 are two positive constants. Taking the derivatives and double derivatives with respective to \u03b1p(t),\nU \u2032priv(\u03b1p(t)) =\u2212 \u03c9p1 ( \u03c9p3 +2\u03c9p4\u03b1p(t) ) \u03c9p3\u03b1p(t)+\u03c9p4\u03b12p(t) ,\nU \u2032\u2032priv(\u03b1p(t))= \u22122\u03c9p1\u03c9p4 +\u03c9p1\n( \u03c9p3 +2\u03c9p4\u03b1p(t) )2 \u03b12p(t)+\u03c9p3 .\nFor \u03b1p(t) > 0, U \u2032priv(\u03b1p(t)) < 0 and U \u2032\u2032priv(\u03b1p(t)) > 0, which imply decreasingly\nmonotonicity and convexity, respectively. The function Lacc(\u03b1p(t)) is determined by data fitting from {(\u03b1p(t),Lep(t)}t=0. In our experiment, we choose \u03c9p1 = 0.02, \u03c9p2 = 6, \u03c9p3 = 9, \u03c9p4 = 1.\nFigure 5 shows the privacy-accuracy tradeoff of dual variable perturbation at different iterations. From curve fitting, we model the function\nLacc(\u03b1p(t)) = c4 \u00b7 e\u2212c5\u03b1p(t)+ c6 (30)\nwhere c4, c5,c6 and are three non-negative constant. From the experimental results, we determine c4 = 0.2, c5 = 25, c6 = min{Lep(t1)}t1=0; these values are applicable for all iteraions.\nFigure 6 presents the privacy-accuracy tradeoff of primal perturbation at different iterations. We model the function Lacc the same as (38). From the plots in Figure 5, we can see that the experimental results of Lacc(\u03b1p(t)) given {\u03b1p(t)} for primal variable perturbation experimences more oscillation than the dual variable perturbation does. For iteration t > 1, c4 = 20, c5 = 20, c6 = 181 \u2211 100 t=20 Lep(t). Figure 7 and 8 compare the privacy-accuracy tradeoff of dual and primal variable perturbations in terms of empirical loss and misclassification error rate, respectively. As shown, the reaction of empirical loss of dual variable perturbation is more stable than the primal variable perturbation for most values of \u03b1p(t). Moreover, the dual perturbation gives better error rate for most of \u03b1p, which implies better management of tradeoff between privacy and accuracy.\nWe determine \u03b1U = 1, c3 = 0.135. Let \u03b1\u2217p be the value such that the corresponding is optimal private. Substitute (37) and (38) to (35), and we then take derivative of T in (35) with respect to \u03b1p(t), and set it to 0 at \u03b1\u2217p:\n\u03c9p1(\u03c9p3\u22122\u03c9p4\u03b1p(t)) =c4c5(\u03c9p3\u03b1p(t) +\u03c9p4\u03b12p(t)) \u00b7 e\u2212c5\u03b1p(t).\nThe optimum value of \u03b1p(t) at each time t is obtained by soving the above equation.\nFigure 7 and 8 shows the privacy-accuracy tradeoff of the final optimum classifier in terms of empirical loss and misclassification error rate (MER). The MER is determined by the fraction of times the trained classifier predict a wrong label. We can see that primal variable peroforms slightly\nbetter than dual variable perturbation with respect to the empirical loss."}, {"heading": "6. Conclusion", "text": "This work developed two ADMM-based algorithms to solve a centralized regularized ERM in a distributed fashion while providing \u03b1-differential privacy for the ADMM iterations as well as the final trained output. Thus, the sensitive information stored in the training dataset at each node is protected against both the internal and the external adversares.\nBased on distributed training datasets, Algorithm 1 perturbs the dual variable \u03bbp(t) for every\nnode p \u2208P at iteration t; For the next iteration, t + 1, the perturbed version of \u03bbp(t) is involved in the update of primal variable fp(t + 1). Thus the perturbation created at time t provides privacy at time t + 1. In Algorithm 2, we perturb the primal variable fp(t), whose noisy version is then released to the neighboring nodes. In this case, the perturbation added at time t make the training process private at time t. Moreover, since the primal variables are shared among all the nieghboring nodes, at time t, the noise directly involved in the optimization of parameter update comes from multiple nodes; as a result, the updated variable has more randomness than the dual perturbation case.\nIn general, the accuracy decreases as privacy requirements are more stringent. The tradeoff between the privacy and accuracy is studied. Our experiments on real data from UCI Machine Learning Repository show that dual variable perturbation is more robust to the noise than the primal variable perturbation. The dual variable perturbation outperforms the primal case at balancing the privacy-accuracy tradeoff as well as learning quality.\nHowever, there are several conditions for the loss function and the regularizer function, which are summarized in Assumption 1 to 3. The conditions for dual variable perturbation and primal variable perturbation are similar except that the loss funciton is required to be bounded doubly differentiable for dual variable perturbation. Thus, for the loss functions and regularizer functions satisfing Assumption 1 to 3, we recommand the dual variable perturbation algorithm, which can obtain more accurate results while keep the \u03b1differential privacy to a good level."}, {"heading": "Appendix A. Alternating Direction Method of Multipliers", "text": "Consider a convex optimization problem:\nmin x g1(x)+g2(Ax) s.t. x \u2208 S1, Ax \u2208 S2, (31)\nwhere g1 : Rs1 \u2192 R and g2 : Rs2 \u2192 R1 are both convex functions, A \u2208 Rs2\u00d7s1 is a matrix, S1 \u2208 Rs1\nand S2 \u2208 Rs2 are two non-empty polyhedral sets. Using an additional auxiliary variable v \u2208 Rs2 yields an equivalent form of (33) as:\nmin x g1(x)+g2(v).\ns.t. Ax = v\nx \u2208 S1, v \u2208 S2\n(32)\nThe corresponding augmented Lagrange function of (34) is:\nL(x,v,\u03bb ) =g1(x)+g2(v)+\u03bb T (Ax\u2212 v)\n+ \u03b7 2 \u2016 Ax\u2212 v \u20162,\n(33)\nwhere \u03bb \u2208 Rs2 is the Lagrange multiplier corresponding to the constraints Ax = v, and \u03b7 > 0 is a penalty parameter that controls the effect of constraints violation in (34). The ADMM first minimizes L(x,v,\u03bb ) with respect to primal variable x, and then keeping the value of x fixed at the just computed value, with respect to the auxiliary variable v. After that, the dual variable \u03bb is updated in a gradient ascending manner. Specifically, ADMM iterates at time t +1 is:\nx(t +1) = min x L(x,v(t),\u03bb (t)), (34)\nv(t +1) = min x L(x(t +1),v,\u03bb (t)), (35)\n\u03bb (t +1) = \u03bb (t)+\u03b7(Ax(t +1)\u2212 v(t +1)), (36)\nThe following theorem states the convergence of ADMM.\nTheorem 9. ([51]) Assume (33) is solvable, and either AT A is nonsingular or S1 is bounded. Then \u2022 a sequence {x(t),v(t),\u03bb (t)} generated\nby ADMM iterations (36) to (38) is bounded, and every limit points of {x(t)} is an optimal solution of (33): {x(t),v(t)} converges to a solution of (33).\n\u2022 {\u03bb} converges to a solution of the dual problem:\nmin \u03bb\u2208S2 G1(\u03bb )+G2(\u03bb ), (37)\nwhere\nG1(\u03bb ) = inf x\ng1(x)+\u03bb T Ax,\nG2(\u03bb ) = inf v\ng2(v)\u2212\u03bb T v."}, {"heading": "Appendix B. Proof of Theorem 1", "text": "Proof: (Theorem 1) Let fp(t + 1) be the optimal primal variable with zero duality gap. From the Assumption 1 and 2, we know that both the loss funciton L and the regularizer R(\u00b7) are differentiable and convex, and by using the Karush-Kuhn-Tucker (KKT) optimality condition (stationarity), we have\n0 = CR\nBp\nBp \u2211 i=1 yipL \u2032(yip fp(t +1)T xip)xip +\u03c1\u2207R( fp)\n+2 ( CR\n2Bp \u03b5p(t)+\u03bbp(t)\n) +(\u03a6+2\u03b7Np) fp(t +1)\n\u2212\u03b7 \u2211 i\u2208Np ( fp(t)+ fi(t)),\nfrom which we can establish the relationship between the noise \u03b5p(t) and the optimal primal variable fp(t +1) as:\n\u03b5p(t) =\u2212 Bp\n\u2211 i=1 yipL \u2032(yip fp(t +1)T xip)xip\u2212 Bp CR \u03c1\u2207R( fp)\n\u2212 2Bp CR \u03bbp(t)\u2212 Bp CR (\u03a6+2\u03b7Np) fp(t +1) + Bp\u03b7 CR \u2211i\u2208Np ( fp(t)+ fi(t)).\n(38) Under Assumption 1, the augmented Lagrange function Ldual(t) is strictly convex, thus there is a unique value of fp(t + 1) for fixed \u03b5p(t) and dataset Dp. The equation (38) shows that for any value of fp(t + 1), we can find a unique value of \u03b5p(t) such that fp(t + 1) is the minimizer of Ldual . Therefore, given a dataset Dp, the relation between \u03b5p(t) and fp(t +1) is bijective.\nLet Dp and D\u2032p be two datasets with Hd(Dp,D\u2032p) = 1, (xi,yi) \u2208 Dp and (x\u2032i,y\u2032i) \u2208 D\u2032p are the corresponding two different data points. Let two matrices J f (\u03b5p(t)|Dp) and J f (\u03b5 \u2032p(t)|D\u2032p) denote the Jacobian matrices of mapping from fp(t + 1) to \u03b5p(t) and \u03b5 \u2032p(t), respectively. Then, transformation from noise fp(t + 1) to \u03b5p(t) by\nJacobian yields:\nQ( fp(t +1)|Dp) Q( fp(t +1)|D\u2032p) = q(\u03b5p(t)|Dp) q(\u03b5 \u2032p(t)|D\u2032p) |det(J f (\u03b5p(t)|Dp))|\u22121 |det(J f (\u03b5 \u2032p(t)|D\u2032p))|\u22121 ,\n(39)\nwhere q(\u03b5p(t)|Dp) and q(\u03b5 \u2032p(t)|D\u2032p) are the densities of \u03b5p(t) and \u03b5 \u2032p(t), respectively, given fp(t + 1) when the datasets are Dp and D\u2032p, respectively.\nTherefore, in order to prove the ratio of conditional densities of optimal primal variable is bounded as:\nQ( fp(t)|D) Q( fp(t)|D\u2032p) \u2264 e\u03b1p(t),\nwe have to show:\nq(\u03b5p(t)|Dp) q(\u03b5 \u2032p(t)|D\u2032p) \u00b7 |det(J f (\u03b5p(t)|Dp))|\u22121 |det(J f (\u03b5 \u2032p(t)|D\u2032p))|\u22121\n\u2264 e\u03b1p(t).\nWe first bound the ratio of the determinant of Jacobian matrices, and then the ratio of conditional densities of the noise vectors.\nLet xa be the a-th element of the vector x, and (a,b). Let E \u2208Rd\u00d7d be a matrix, then let E(a,b) denote the (a,b)-th entry of the matrix E. Thus, the (m,n)-th entry of J f (\u03b5p(t)) is:\nJ f (\u03b5p(t))(m,n) =\u2212 Bp\n\u2211 i=1 (y2i L \u2032\u2032(yi fp(t +1)T xi)x (m) i x (n) i\n\u2212 Bp CR \u03c1\u22072R( fp(t +1))(m,n) \u2212 Bp CR (\u03a6+2\u03b7Np)1( j = k).\nLet J0f (xi,yi)= (y2i L \u2032\u2032(yi fp(t+1)T xi)xixTi , thn the Jacobian matrix can be expressed as:\nJ f (\u03b5p(t)|Dp) =\u2212 Bp\n\u2211 i=1 J0f (xi,yi)\u2212 Bp CR \u03c1\u22072R( fp(t +1))\n\u2212 Bp CR (\u03a6+2\u03b7Np)Id .\nLet M = J0f (x\u2032i,y\u2032i) \u2212 J0f (xi,yi), and H = \u2212J f (\u03b5p(t)|Dp), and thus J f (\u03b5p(t)|D\u2032p) = \u2212(M+ H). Let h j(W) be the j-th largest eigenvalue of\na symmetric matrix W \u2208Rd\u00d7d with rank \u03b8 Then we have the following fact:\ndet(I+W) = \u03b8\n\u220f j (1+h j(W)).\nSince the matrix xixTi has rank 1, then matrix M has rank at most 2; thus matrix H\u22121M has rank at most 2; therefore, we have:\ndet(H+M) = det(H) \u00b7det(I+H\u22121M) = det(H) \u00b7 (1+h1(H\u22121M))(1+h2(H\u22121M).\nThus, the ratio of determinants of the Jacobian matrices can be expressed as:\n|det(J f (\u03b5p(t)|Dp))|\u22121\n|det(J f (\u03b5 \u2032p(t)|D\u2032p))|\u22121 = |det(H+M)| |det(H)|\n=|det(I+H\u22121M)| =(1+h1(H\u22121M))(1+h2(H\u22121M) =|1+h1(H\u22121M)+h2(H\u22121M) +h1(H\u22121M)h2(H\u22121M)|.\nBased on Assumption 2, all the eigenvalues of \u22072R( fp(t + 1)) is greater than 1 [32]. Thus, from Assumption 1, matrix H has all eigenvalues at least BpCR ( \u03c1 + \u03a6 + 2\u03b7Np ) . Therefore, |h1(H\u22121M)| \u2264 |hi(M)|Bp CR ( \u03c1+\u03a6+2\u03b7Np\n) . Let \u03c3i(M) be the non-negative singular value of the symmetric matrix M. According to [3], we have the inequality\n\u2211 i |hi(M)| \u2264\u2211 i \u03c3i(M). (40)\nThus, we have\n|h1(M)|+ |h2(M)| \u2264 \u03c31(M)+\u03c32(M).\nLet \u2016 X \u2016\u03a3= \u2211i \u03c3i be the trace norm of X. Then according to the trace norm inequality, we have:\n\u2016M \u2016\u03a3\u2264\u2016 J0(x\u2032i,y\u2032i) \u2016\u03a3 + \u2016 \u2212J0(xi,yi) \u2016\u03a3 .\nAs a result, based on the upper bounds from Assumption 1 and 3, we have:\n|h1(M)|+ |h2(M)| \u2264\u2016 J0(x\u2032i,y\u2032i) \u2016\u03a3 + \u2016 \u2212J0(xi,yi) \u2016\u03a3 \u2264 |(y2i L \u2032\u2032(yi fp(t +1)T xi)|\u00b7 \u2016 xi \u2016 + |(y\u20322i L \u2032\u2032(y\u2032i fp(t +1)T x\u2032i)|\u00b7 \u2016 x\u2032i \u2016 \u2264 2c1,\nwhich follows h1(M)h2(M) \u2264 c21. Finally, the ratio of determinants of Jacobian matrices is bounded as:\n|det(J f (\u03b5p(t)|Dp))|\u22121\n|det(J f (\u03b5 \u2032p(t)|D\u2032p))|\u22121 \u2264 (1+ c1Bp CR ( \u03c1 +\u03a6+2\u03b7Np ) ) )2\n= e\u03b1 , (41)\nwhere \u03b1 = ln (\n1+ c1Bp CR ( \u03c1+\u03a6+2\u03b7Np ))2. Now, we bound the ratio of densities of \u03b5p(t). Let sur(E) be the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) \u00b7 Ed\u22121. We can write:\nq(\u03b5p(t)|Dp) q(\u03b5 \u2032p|D\u2032p) = K (\u03b5p(t))\n\u2016\u03b5p(t)\u2016d\u22121 sur(\u2016\u03b51(t)\u2016)\nK (\u03b5 \u2032p(t)) \u2016\u03b5 \u2032p(t)\u2016d\u22121\nsur(\u2016\u03b5 \u2032p(t)\u2016)\n\u2264 e\u03b6p(t)(\u2016\u03b5 \u2032p(t)\u2016\u2212\u2016\u03b5p(t)\u2016)\n\u2264 e\u03b1\u0302 ,\n(42)\nwhere \u03b1\u0302 is a constant satisfying the above inequality. Since we want to bound the ratio of densities of fp(t +1)\nQ( fp(t +1)|Dp) Q( fp(t +1)|D\u2032p) \u2264 e\u03b1p(t),\nwe need \u03b1\u0302 \u2264 \u03b1p(t)\u2212\u03b1. For non-negative \u03a6, let \u03b1\u0302 = \u03b1p(t)\u2212 ln ( 1+ c1\nBp CR ( \u03c1 +2\u03b7Np ))2. If \u03b1\u0302 > 0, then we fix \u03a6= 0, and thus \u03b1\u0302 =\u03b1p(t)\u2212 \u03b1 . Otherwise, let \u03a6 = c1Bp\nCR (e\u03b1p(t)/4\u22121)\n\u2212\u03c1 \u2212 2\u03b7Np,\nand \u03b1\u0302 = \u03b1p(t)2 which makes \u03b1\u0302 =\u03b1p(t)\u2212\u03b1 . Therefore, we can have\n|det(J f (b1|Dp))|\u22121\n|det(J f (b2|D\u2032p))|\u22121 \u2264 e\u03b1p(t)\u2212\u03b1\u0302 .\nFrom the upper bounds stated in Assumption 1 and 3, the l2 norm of the difference of \u03b51 and \u03b52 can be bounded as:\n\u2016 \u03b5 \u2032p(t)\u2212 \u03b5p(t) \u2016= Bp\n\u2211 i=1 \u2016 yipL \u2032(y\u2032ip fp(t +1)T x\u2032ip)x\u2032ip\n\u2212 (yipL \u2032(yip fp(t +1)T xip)xip \u2016 \u22642.\nThus,\u2016 \u03b5 \u2032p(t) \u2016 \u2212 \u2016 \u03b5p(t) \u2016\u2264\u2016 \u03b5 \u2032p(t)\u2212 \u03b5p(t) \u2016\u2264 2. Therefore, by selecting \u03b6p(t) = \u03b1\u03022 , we can bound the ratio of conditional densities of fp(t +1) as:\nQ( fp(t +1)|Dp) Q( fp(t +1)|D\u2032p) \u2264 e\u03b1p(t),\nand prove that the dual variable perturbation can provide \u03b1p(t)-differential privacy."}, {"heading": "Appendix C. Proof of Corollary 1.2", "text": "Proof: (Corollary 1.2) We prove this corollary by induction. For c2 = 1, it is true since this is exactly the case of Theorem 1. Suppose Corollary 1.2 is held for Hd(Dp,D\u2032p)= c2. Let Hd(Dp,D \u2032 p)= c2+1. Clearly, there must exist a dataset D\u2032\u2032p such that Hd(Dp,D\u2032\u2032p) = 1, and Hd(D \u2032 p,D \u2032\u2032 p) = c2. Thus, from (13), we have:\nQ( fp(t)|Dp) Q( fp(t)|D\u2032p) = Q( fp(t)|Dp) Q( fp(t)|D\u2032\u2032p) \u00b7 Q( fp(t)|D\u2032\u2032p) Q( fp(t)|D\u2032p)\n\u2264 e\u03b1p(t)ec2\u03b1p(t) = e(c2+1)\u03b1p(t). (43)\nTherefore, the induction hypothesis is true and Corollary 1.2 is proven."}, {"heading": "Appendix D. Proof of Theorem 2", "text": "Proof: (Theorem 2) Let Dp and D\u2032p be two datasets with Hd(Dp,D\u2032p) = 1. Since only Vp(t) is released, then our target is to prove the following:\nQ(Vp(t +1)|Dp) Q(Vp(t +1)|D\u2032p) \u2264 e\u03b1p(t). (44)\nFrom (23), we have:\nQ(Vp(t +1)|Dp) Q(Vp(t +1)|D\u2032p) = K (\u03b5p(t)) K (\u03b5 \u2032p(t)) = e\u2212\u03b6p(t)\u2016\u03b5p(t)\u2016 e\u2212\u03b6p(t)\u2016\u03b5 \u2032 p(t)\u2016 .\n(45) Therefore, in order to make the model to provide \u03b1p(t)-differential privacy, we need to find a \u03b6p(t) that satisfies\n\u03b6p(t)(\u2016 \u03b5p(t) \u2016 \u2212 \u2016 \u03b5 \u2032p(t) \u2016)\u2264 \u03b1p(t). (46)\nLet V A = argminVp Lprim(t|Dp), and V B = argminVp Lprim(t|D\u2032p), where Lprim(t|D) is the\naugmented Lagrange function for primal variable perturbation given dataset D.\nLet F , G be defined at each node p \u2208P as:\nF(Vp(t)) = Lprim(t|Dp),\nG(Vp(t)) = Lprim(t|D\u2032p)\u2212Lprim(t|Dp).\nThus, G(Vp) = C R Bp \u2211 Bp i=1(L (y \u2032 ipV T p x \u2032 ip) \u2212 L (yipV Tp xip)). According to Assumption 2, we can imply that Lprim(t|Dp) = F(Vp(t)) and Lprim(t|D\u2032p) = F(Vp(t)) + G(Vp(t)) are both \u03c1-strong convex. Differentating G(Vp(t)) with respect to Vp(t) gives:\n\u2207G(Vp) = CR\nBp (y\u2032ipL \u2032(y\u2032ipV T p x \u2032 ip)x \u2032 ip \u2212 (yipL (yipV Tp xip)xip.\nFrom Assumption 1 and 3, \u2016 \u2207G(Vp) \u2016\u2264 2C R\nBp .\nFrom definitions of V A and V B, we have:\n\u2207F(V A) = \u2207F(V B)+\u2207F(V B) = 0\nFrom Lemma 14 in [52] and the fact that F(\u00b7) is \u03c1-strongly convex, weh have the following inequality:\n\u3008\u2207F(V A)\u2212F(V B),V A\u2212V B\u3009 \u2265 \u03c1 \u2016V A\u2212V B \u20162;\ntherefore, Cauchy-Schwarz inequality yields:\n\u2016V A\u2212V B \u2016 \u00b7 \u2016 \u2207G(V B) \u2016 \u2265 (V A\u2212V B)T \u2207G(V B) = \u3008\u2207F(V A)\u2212F(V B),V A\u2212V B\u3009 \u2265 \u03c1 \u2016V A\u2212V B \u20162 .\nDividing both sides by \u03c1 \u2016V A\u2212V B \u2016 gives:\n\u2016V A\u2212V B \u2016\u2264 1 \u03c1 \u2016 \u2207G(V B) \u2016\u2264 2C\nR\n\u03c1Bp . (47)\nFrom (23), we have\n\u2016V A\u2212V B \u2016\u2264 1 \u03c1 \u2016 \u2207G(V B) \u2016=\u2016 \u03b5p(t)\u2212 \u03b5 \u2032p(t) \u2016 .\nThus, we can bound\n\u03b6p(t)(\u2016 \u03b5p(t) \u2016 \u2212 \u2016 \u03b5 \u2032p(t) \u2016)\u2264 \u03b6p(t)(\u2016 \u03b5p(t)\u2212 \u03b5 \u2032p(t) \u2016)\n\u2264 2C R\nBp\u03c1 \u03b6p(t)\nTherefore, by choosing \u03b6p(t) = \u03c1Bp\u03b1p(t)\n2CR , the inequality (43) holds; thus primal variable perturbation is dynamic \u03b1p-differential private at each node p."}, {"heading": "Appendix E. Proof of Theorem 3", "text": "Proof: (Theorem 3) Let\nf\u0302p(t +1) = argmin Z\u0302( fp, t),\nf\u0303p(t +1) = argminZp( fp, t|Dp),\nand let f nonp (t +1) be the estimated optimum that is practical result of the algorithm. We assume that f nonp (t+1) is very close to the actually so that Zp( f nonp (t+1), t|Dp)\u2212Zp( f\u0303p(t+1), t|Dp)\u2248 0. For the non-private ERM, Shalev-Shwartz and Srebro in [53] show that for a specific reference classifier f0(t + 1) at time t + 1 such that C\u0302( f 0(t + 1)) = C\u2217E, we have:\nC\u0302( f nonp (t +1)) =C\u0302 \u2217 + ( Z\u0302( f nonp (t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t) ) + ( Z\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t)\n) +\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f nonp (t +1) \u20162 .\nFrom Sridharan et al. [54], we have, with probability at least 1\u2212\u03b4 Z\u0302( f nonp (t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t) \u2264 2 ( Zp( f nonp (t +1), t|Dp)\u2212Zp( f\u0303p(t +1), t|Dp)\n) +O ( CR\nln( 1\u03b4 ) Bp\u03c1\n) .\nSince Zp( f nonp (t+1), t|Dp)\u2212Zp( f\u0303p(t+1), t|Dp)\u2248 0, then\nZ\u0302( f nonp (t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t)\n\u2264 O (\nCR ln( 1\u03b4 ) Bp\u03c1\n) .\nIf we choose \u03c1 \u2264 \u03b1acc\u2016 f 0p (t+1)\u20162 , then\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f nonp (t +1) \u20162\u2264 \u03b1acc 2 .\nThus C\u0302( f nonp (t +1))\u2264C\u0302\u2217+O (\nCR ln( 1\u03b4 ) Bp\u03c1\n) +\n\u03b1acc 2 .\nTherefore, we can find the value of Bp by solving\nO (\nCR ln( 1\u03b4 ) Bp\u03c1\n) +\n\u03b1acc 2 \u2264 \u03b1acc\nWe get:\nBp > \u03b2non max\n( CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 )\n\u03b12acc\n) .\nIf we determine different reference classifier f 0p(t + 1) at different time, then we need to find the maximum value across the time and among different value of \u2016 f 0p(t +1) \u2016:\nBp > \u03b2non max ({CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ) .\nLet Fp(t +1) = argmin fp Lnon( fp, t|Dp). Since\nC\u0302(Fp(t +1)) = C\u0302( f nonp (t +1))+\u2206 non(t),\nthen\nC\u0302(Fp(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc +\u2206non(t),\nwith probability no less than 1\u2212\u03b4 ."}, {"heading": "Appendix F. Proof of Theorem 4", "text": "Proof: (Theorem 4) First we define the following optimal variables:\nf\u0302p(t +1) = argmin Z\u0302( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf \u2217p(t +1) = argminZdual( fp, t|Dp),\nand as defined in Theorem 3, C\u0302( f 0p(t + 1)) = C\u0302 \u2217 at time t + 1. We use the analysis of ShalevShwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows:\nC\u0302( f \u2217p(t +1)) =C\u0302( f 0 p(t +1)) + ( Z\u0302( f \u2217p(t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t) ) + ( Z\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t)\n) +\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f \u2217p(t +1) \u20162 .\n(48) Now we bound each terms in the right hand side of (47) as follows. From Assumption 1, we have\nL \u2032 \u2264 c1. By choosing Bp > 5c1CR\u2016 f 0p (t+1)\u20162\n\u03b1acc\u03b1p(t) , and \u03c1 > \u03b1acc2\u2016 f 0p (t+1)\u20162 , and since \u03b1p(t)\u2264 1, we have:\n\u03b1\u0302 =\u03b1p(t)\u2212 ln ( 1+ c1\nBp CR ( \u03c1 +2\u03b7Np ))2 >\u03b1p(t)\u2212 ln(1+ c1CR\nBp\u03c1 )2\n>\u03b1p(t)\u2212 ln(1+ 2\u03b1p(t)\n5 )2\n>\u03b1p(t)\u2212 4\u03b1p(t)\n5 = \u03b1p(t) 5 .\nThen, according to Algorithm 1, we choose the corresponding \u03b6p(t) = \u03b1p(t) 4 because \u03b1\u0302 > 0. Let \u039b be the event\n\u039b := {\nZp( f \u2217p(t +1), t|Dp)\u2264 Zp( f nonp (t +1), t|Dp)\n+ 16d2\n( ln( d\u03b4 ) )2 \u03c1B2p\u03b1p(t)2 } .\nSince \u03b1\u0302 > \u03b1p(t)2 > 0, and applying Lemma 11 yields:\nP\u03b5p(t) ( \u039b ) \u2265 1\u2212\u03b4 .\nFrom the work of Sridharan et al. in [54], the following inequality holds with probability 1\u2212\u03b4\nZ\u0302( f \u2217p(t +1))\u2212 Z\u0302( f\u0302p(t +1))\u2264 2 ( Zp( f \u2217p(t +1), t|Dp)\n\u2212Zp( f nonp (t +1), t|Dp) )\n+O ( ln( 1\u03b4 )\nBp\u03c1\n)\n\u2264 32d2\n( ln( d\u03b4 ) )2 \u03c1B2p\u03b1p(t)2\n+O ( ln( 1\u03b4 )\nBp\u03c1\n) .\nThe big-O notation hides only fixed numerical constants, which depend on the derivative of the loss function and the upper bounds of the data points shown in Assumption 3. Combining the above two processes, Z\u0302( f \u2217p(t +1))\u2212 Z\u0302( f\u0302p(t +1)) is bounded as shown above with probability 1\u2212 2\u03b4 .\nFrom the definitions of f 0p(t+1) and f\u0302p(t+1), we can get Z\u0302( f\u0302p(t + 1), t)\u2212 Z\u0302( f 0p(t + 1), t) < 0.\nSince P\u2265 1, then by selecting \u03c1 = \u03b1acc\u2016 f 0p (t+1)\u20162 , we can bound\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f \u2217p(t +1) \u20162\u2264 \u03b1acc 2 .\nTherefore, from (47), we have:\nC\u0302( f \u2217p(t +1))\u2264C\u2217E + 32d2\n( ln( d\u03b4 ) )2 \u03c1B2p\u03b1p(t)2\n+O (\nCR ln( 1\u03b4 ) Bp\u03c1\n) +\n\u03b1acc 2 ,\nwith \u03c1 = 6\u03b1acc\u2016 f 0p (t+1)\u20162 . The lower bounds of Bp is determined by solving the following:\n32d2 ( ln( d\u03b4 ) )2\n\u03c1B2p\u03b1p(t)2 +O\n( CR\nln( 1\u03b4 ) Bp\u03c1\n) +\n\u03b1acc 2 \u2264 \u03b1acc.\nLemma 10. Let Z be a gamma random variable with density function \u0393(k,\u03b8), where k is an integer, and let \u03b4 > 0. Then we have:\nP(Z < k\u03b8 ln( k \u03b4 ))\u2265 1\u2212\u03b4 .\nProof: (Lemma 10) Since Z is a gamma random variable \u0393(k,\u03b8), then we can express Z as follows:\nZ = k\n\u2211 i=1 Zi,\nwhere {Zi}ki=1 are independent exponential random variable with density function Exp( 1\u03b8 ); thus, for each Zi we have:\nP(Zi \u2264 \u03b8 ln( k \u03b4 )) = 1\u2212 \u03b4 k .\nSince Zii=1 are independent, we have:\nP(Z < k\u03b8 ln( k \u03b4 )) =\nk\n\u220f i=1 P(Zi \u2264 \u03b8 ln( k \u03b4 ))\n= (1\u2212 \u03b4 k )k \u2265 1\u2212\u03b4 .\nLemma 11. Let \u03b1\u0302 > 0, and f \u2217p(t + 1) = argminZdual( fp, t|Dp), and f nonp (t + 1) =\nargminZp( fp, t|Dp). Let \u039b be the event \u039b := {\nZp( f \u2217p(t +1), t|Dp)\u2264 Zp( f nonp (t +1), t|Dp)\n+ 16d2\n( ln( d\u03b4 ) )2 \u03c1B2p\u03b1p(t)2 } .\nUnder Assumption 1 and 2, we have: P\u03b5p(t) ( \u039b ) \u2265 1\u2212\u03b4 .\nThe probability P\u03b5p(t) is taken over the noise vector \u03b5p(t).\nProof: (Lemma 11) Since \u03b1\u0302 > 0, \u03a6 = 0; then f \u2217p(t + 1) = argminZdual( fp, t|Dp) can be expressed as:\nf \u2217p(t +1) = argmin ( Zp( fp, t|Dp)+2\u03b5p(t)T fp ) .\nThus, we have:\nZp( f \u2217p(t +1), t|Dp)\u2264 Zp( f nonp (t +1), t|Dp)\n+ CR\nBp \u03b5p(t)T ( f nonp (t +1)\u2212 f \u2217p(t +1)).\nFirstly, we bound the l2-norm \u2016 f nonp (t + 1)\u2212 f \u2217p(t + 1) \u2016. We use the similar procedure to establish (46) in Appendix D by setting F(Y ) = Zp(Y, t|Dp) and G(Y ) = C R\nBp \u03b5p(t); thus, based on\nAssumption 1 and 2, we have:\n\u2016 f nonp (t +1)\u2212 f \u2217p(t +1) \u2016\u2264 1 \u03c1 \u2016 \u2207 ( 2\u03b5p(t)T fp ) \u2016\n\u2264 CR \u2016 \u03b5p(t) \u2016\nBp\u03c1 .\nCauchy-Schwarz inequality yields:\nZp( f \u2217p(t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp) \u2264\u2016 Zp( f \u2217p(t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp) \u2016\n\u2264 2 Bp \u2016 \u03b5p(t)T ( f nonp (t +1)\u2212 f \u2217p(t +1) \u2016\n\u2264 ( CR )2 \u2016 \u03b5p(t) \u20162\nB2p\u03c1 .\nSince the noise vector \u03b5p(t) is drawn from\nK (\u03b5)\u223c e\u2212\u03b6p(t)\u2016\u03b5\u2016,\nthen \u2016 \u03b5p(t) \u2016 is drawn from \u0393(d, 1\u03b6p(t)) = \u0393(d, 2 \u03b1\u0302 ). Then by using Lemma 10 with \u2016 \u03b5p(t) \u2016\u2264 2d ln( d\u03b4 )\n\u03b1\u0302 , we have:\nLnonP( f \u2217p(t +1), t|Dp)\u2212LnonP( f nonp (t +1), t|Dp) \u2264 4d2 ( ln( d\u03b4 ) )2\n\u03c1B2p\u03b1p(t)2 .\nwith probability no less than 1\u2212\u03b4 ."}, {"heading": "Appendix G. Proof of Theorem 5", "text": "Proof: (Theorem 5) Similar to the proof of Theorem 4 in Appendix F, we define the following optimal variables:\nf\u0302p(t +1) = argminZE( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf \u2217p(t +1) = argminZprim( fp, t|Dp).\nLet C\u0302( f 0p(t + 1)) = C\u0302 \u2217 at time t + 1. We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows,\nC\u0302( f \u2217p(t +1)) =C\u0302( f 0 p(t +1)) + ( Z\u0302( f \u2217p(t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t) ) + ( Z\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t)\n) +\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f \u2217p(t +1) \u20162 .\n(49) According to Theorem 2, we choose \u03b6p(t) = \u03c1Bp\u03b1p(t) 2CR > 0. Thus, applying Lemma 14, we have:\nZp( f \u2217p(t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp) \u2264 16 ( CR )2\u03b72N2pd2( ln( d\u03b4 ))2 \u03c13B2p\u03b1p(t)2 ,\nwith probability no smaller than 1\u2212\u03b4 . Then we use the result of Sridharan et al. in [54], with\nprobability no smaller than 1\u2212\u03b4 : Z\u0302( f \u2217p(t +1))\u2212Z\u0302( f\u0302p(t +1))\u2264 2 ( Zp( f \u2217p(t +1), t|Dp)\n\u2212Zp( f \u2217p(t +1), t|Dp) )\n+O ( ln( d\u03b4 )\nBp\u03c1 ) \u2264\n32 ( CR )2\u03b72N2pd2( ln( d\u03b4 ))2 \u03c13B2p\u03b1p(t)2\n+O ( ln( 1\u03b4 )\nBp\u03c1\n) .\nCombining the above two processes, we have the probability no smaller than 1\u22122\u03b4 .\nIn order to bound the last two terms in (48), we select \u03c1 = \u03b1acc\u2016 f 0p (t+1)\u20162 ; as a result,\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016 f \u2217p(t +1) \u20162\u2264 \u03b1acc 2 .\nFrom the definitions of f\u0302p(t + 1) and f 0p(t + 1), we have:\nZ\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t)\u2264 0.\nThe value of Bp is determined such that\nC\u0302( f \u2217p(t +1))\u2264 C\u0302\u2217+\u03b1acc.\nTherefore, we find the bounds of Bp by solving 32 ( CR )2\u03b72N2pd2( ln( d\u03b4 ))2 \u03c13B2p\u03b1p(t)2 +O (CR ln( 1\u03b4 ) Bp\u03c1 ) + \u03b1acc 2\n\u2264 \u03b1acc,\nwith \u03c1 = \u03b1acc\u2016 f 0p (t+1)\u20162 .\nLemma 12. Let f and g be two probability density functions. If there exists a constant c6 such that f (x) = c6g(x) for all x \u2208Rd , then:\nf (x) = g(x).\nProof: (Lemma 12) From the property of probability density function, we have:\n1 = \u222b \u221e \u2212\u221e f (x)dx\n=c6 \u00b7 \u222b \u221e \u2212\u221e g(x)dx =c6.\nTherefore, c6 = 1, and f (x) = g(x).\nLemma 13. Let {Z j}Kj=1 be independent gamma random variables with density \u0393(\u03b2 j,h). Then Z = \u2211Kj=1 is a gamma random variable with \u0393(\u2211Kj \u03b2 j,h)\nProof: (Lemma 13) We prove Lemma 11 by induction. First, we show it is true for K = 2. Let g(\u00b7) = \u0393(\u03b21 + \u03b22,h), and fZ1+Z2(z) be the joint probability density of Z1 and Z2. Then, we have fZ1+Z2(z) = 0 = g(z) for all z < 0. Let r > 0, then\nfZ1+Z2(r) =( fZ1 \u2217 fz2)(r) = \u222b r\n0 fZ1(x)\u2217 fZ2(r\u2212 x)dx\n= 1\n\u0393(\u03b21)\u0393(\u03b22) \u222b r 0 he\u2212hx(hx)\u03b21\u22121heh(r\u2212x)\n\u00b7 ( h(r\u2212 x)) )\u03b22\u22121dx = h\u03b21+\u03b22e\u2212hr\n\u0393(\u03b21)\u0393(\u03b22) \u222b r 0 x\u03b21\u22121(r\u2212 x)\u03b22\u22121dx\n= he\u2212hrh\u03b21+\u03b22\u22121\n\u0393(\u03b21)\u0393(\u03b22) \u222b 1 0 (ry)\u03b21\u22121 ( r(1\u2212 y) )\u03b22\u22121rdy = he\u2212hr(rh)\u03b21+\u03b22\u22121\n\u0393(\u03b21 +\u03b22) \u00b7 \u0393(\u03b21 +\u03b22) \u0393(\u03b21)\u0393(\u03b22)\n\u00b7 \u222b 1\n0 y\u03b21\u22121(1\u2212 y)\u03b22\u22121dy\n=g(r) \u00b7 c6, (50)\nwhere c6 = \u0393(\u03b21+\u03b22) \u0393(\u03b21)\u0393(\u03b22) \u222b 1 0 y \u03b21\u22121(1 \u2212 y)\u03b22\u22121dy is a constant. From Lemma 11, we prove that fZ1+Z2(z) = g(z).\nNow we assume it is also true for K = K. We next prove it is also true for K\u2032 = K + 1. Let f K(z)= f\u2211Kj=1(z), and g\nK(z)=\u0393(\u2211Kj=1 \u03b2 j,h). Then we have:\nf K+1(z) = ( f K(z)\u2217 fZK+1)(z).\nBy replacing fZ1(z) by f K(z), fZ2(z) by fZK+1(z), \u03b21 by \u2211Kj=1 \u03b2 j, and \u03b22 by \u03b2K+1 in (47), we can prove\nf K+1(z) = gK+1(z) \u00b7 c7,\nwhere c7 = \u0393(\u2211Kj=1 \u03b2 j+\u03b2K+1)\n\u0393(\u2211Kj=1 \u03b2 j)\u0393(\u03b2K+1)\n\u222b 1 0 y \u2211Kj=1 \u03b2 j\u22121(1 \u2212\ny)\u03b2K+1\u22121dy is a constant. Thus form Lemma 12, f K+1(z) = gK+1(z) Therefore, by induction, Lemma 13 is proved.\nThe following Lemma is analogous to Lemma 11\nLemma 14. Let \u03b6p(t) > 0, and f \u2217p(t + 1) = argminZprim( fp, t|Dp), and f nonp (t + 1) = argminZp( fp, t|Dp). Suppose that the noise vector \u03b5t(t) generated at time t has the same value of \u03b1p(t) for all p \u2208P . Let \u039b be the event\n\u039b := {\nZp( f \u2217p(t +1), t|Dp)\u2264 Zp( f nonp (t +1), t|Dp)\n+ 16 ( CR )2\u03b72N2pd2( ln( d\u03b4 ))2 \u03c13B2p\u03b1p(t)2 } .\nIf the loss function L is convex and differentiable with |L | \u2264 1, then we have:\nP\u03b5p(t) ( \u039b ) \u2265 1\u2212\u03b4 .\nThe probability P\u03b5p(t) is taken over the noise vector \u03b5p(t).\nProof: (Lemma 14) Let \u03b5 pi(t) = \u03b5p(t)\u2212 \u03b5i(t) with probability density P\u03b5 pi . Let f \u2217 p(t + 1) = argminZprim( fp, t|Dp), and it can be expressed as:\nf \u2217p(t +1) =argmin ( Zp( fp, t|Dp)\n\u2212\u03b7 \u2211 i\u2208Np\n( ( fp\u2212\n1 2 ( fp(t)+ fi(t))T\n\u00b7 (\u03b5 pi(t))+ 1 4 ( \u03b5 pi(t) )2) .\nThus, we have:\nZp( f \u2217p(t +1), t|Dp) \u2264 Zp( f nonp (t +1), t|Dp) \u2212\u03b7 \u2211\ni\u2208Np ( f nonp (t +1)\u2212 f \u2217p(t +1))T \u00b7 \u03b5 pi.\nFirstly, we bound the l2-norm \u2016 f nonp (t + 1)\u2212 f \u2217p(t + 1) \u2016. We use the similar procedure to establish (46) in Appendix D by setting F(\u00b7) = Zp(Z, t|Dp) and G(Z) = \u03b7 \u2211i\u2208Np ( \u03b5 pi )T (\u00b7);\nthus,based on Assumption 1 and 2, we have:\n\u2016 f nonp (t +1)\u2212 f \u2217p(t +1) \u2016 \u2264 1 \u03c1 \u2016 \u2211\ni\u2208Np \u2207(\u03b7Np( f \u2217p(t +1)) T \u03b5 pi) \u2016\n\u2264 \u2211 i\u2208Np\n\u03b7 \u2016 \u03b5 pi(t) \u2016 \u03c1\n= \u2211 i\u2208Np\n\u03b7 ( \u2016 \u03b5p(t)\u2212 \u03b5 j(t) \u2016 ) \u03c1\n\u2264 \u2211 i\u2208Np\n\u03b7 ( \u2016 \u03b5p(t) \u2016+ \u2016 \u03b5 j(t) \u2016 ) \u03c1 .\nSince \u03b1p(t) is the same for all p \u2208P at time t; thus \u03b6 j(t) = \u03c1Bp\u03b1p(t) 2CR for all j \u2208P . Since \u03b5 j(t) is drawn from (15), then, \u2016 \u03b5p(t) \u2016 is gamma with \u0393(d, 1\u03b6p(t)) for all p \u2208P . Let\n\u2016 \u03b5pi \u2016\u2295=\u2016 \u03b5p(t) \u2016+ \u2016 \u03b5i(t) \u2016 .\nThus\n\u2016 f nonp (t +1)\u2212 f \u2217p(t +1) \u2016 \u2264 \u2211 i\u2208Np\n\u03b7 ( \u2016 \u03b5pi \u2016\u2295 ) \u03c1\n= \u03b7Np\n( \u2016 \u03b5pi \u2016\u2295 ) \u03c1 .\nCauchy-Schwarz inequality yields:\nZp( f \u2217p(t +1), t|Dp)\u2212Zp( f \u2217p(t +1), t|Dp) \u2264\u2016 Zp( f \u2217p(t +1), t|Dp)\u2212Zp( f \u2217p(t +1), t|Dp) \u2016\n\u2264 \u03b72N2p\n( \u2016 \u03b5pi \u2016\u2295 )2 \u03c1 ,\nand from Lemma 12 we have the P\u2016\u03b5 p j\u2016 = \u0393(2d, 2C R\n\u03c1Bp\u03b1p(t)). Applying Lemma 10 with \u2016\n\u03b5 p j(t) \u2016\u2295\u2264 4C Rd ln( d\u03b4 )\n\u03c1Bp\u03b1p(t) yields:\nZp( f \u2217p(t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp) \u2264 16 ( CR )2\u03b72N2pd2( ln( d\u03b4 ))2 \u03c13B2p\u03b1p(t)2\nwith probability no smaller than 1\u2212\u03b4"}, {"heading": "Appendix H. Proof of Theorem 6", "text": "Proof: (Theorem 6) Again, we define the following optimal variables:\nf\u0302p(t +1) = argminZE( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf \u2217p(t +1) = argminZprim( fp, t|Dp),\nV \u2217p (t +1) = f \u2217 p(t +1)+ \u03b5p(t).\nNow we make f 0p(t +1) such that C\u0302( f \u2217 p(t +1)) = C\u0302\u2217(t + 1) be the reference at time t + 1. We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows,\nC\u0302(V \u2217p (t +1)) =C\u0302( f 0 p(t +1)) + ( Z\u0302(V \u2217p (t +1), t)\u2212 Z\u0302( f\u0302p(t +1), t) ) + ( Z\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t)\n) +\n\u03c1 2 \u2016 f 0p(t +1) \u20162 \u2212 \u03c1 2 \u2016V \u2217p (t +1) \u20162 .\n(51) If R( fp(t)) = 12 \u2016 fp(t) \u2016\n2, then \u2016\u22072R( fp(t)) \u2016\u2264 1. Thus, we can apply Lemma 15 with \u03c4 = 1:\nZprim(V \u2217p (t +1), t|Dp)\u2212Zprim( f \u2217p(t +1), t|Dp) \u2264 4 ( CR )2d2(\u03c1 + c4CR)( ln( d\u03b4 ))2\n\u03c12B2p\u03b1p(t)2 ,\nwith probability \u2265 1\u2212 \u03b4 over the noise. In the proof of Theorem 5, we have, with probability 1\u2212\u03b4 :\nZp( f \u2217p(t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp)\n\u2264 4\u03b72N2pd2\n( ln( d\u03b4 ) )2 \u03c13B2p\u03b1p(t)2 .\nTherefore, with probability 1\u22122\u03b4 , we have\nZp(V \u2217p (t +1), t|Dp)\u2212Zp( f nonp (t +1), t|Dp)\n\u2264 4\u03b72N2pd2\n( ln( d\u03b4 ) )2 \u03c13B2p\u03b1p(t)2 + 4d2 ( \u03c1 + c4 )( ln( d\u03b4 ) )2 \u03c12B2p\u03b1p(t)2 .\nSridharan et al. in [54] shows, with probability 1\u2212\u03b4 :\nZ\u0302(V \u2217p (t +1))\u2212 Z\u0302( f\u0302p(t +1)) \u2264 2 ( Zprim(Vp(t +1), t|Dp)\u2212Zprim( f \u2217p(t +1), t|Dp) )\n+O (\nCR ln( d\u03b4 ) Bp\u03c1\n)\n\u2264 8 ( CR )2d2(\u03c1 + c4CR)( ln( d\u03b4 ))2\n\u03c12B2p\u03b1p(t)2 +\n8\u03b72N2pd2 ( ln( d\u03b4 ) )2\n\u03c13B2p\u03b1p(t)2 +O (\nCR ln( 1\u03b4 ) Bp\u03c1\n) .\nCombining the above two processes, we have the probability no smaller than 1\u22123\u03b4 . Since f\u0302p(t + 1) = argmin Z\u0302( fp, t), then( Z\u0302( f\u0302p(t +1), t)\u2212 Z\u0302( f 0p(t +1), t) \u2264 0. For the last two terms, we select \u03c1 = \u03b1acc\u2016 f 0p (t+1)\u20162 in order to make them bounded by \u03b1acc2 .\nThe value of Bp is determined by solving\n8 ( CR )2d2(\u03c1 + c4CR)( ln( d\u03b4 ))2\n\u03c12B2p\u03b1p(t)2 +\n8\u03b72N2pd2 ( ln( d\u03b4 ) )2\n\u03c13B2p\u03b1p(t)2 +O (\nCR ln( 1\u03b4 ) Bp\u03c1\n) +\n\u03b1acc 2 = \u03b1acc,\nwith \u03c1 = \u03b1acc\u2016 f 0p (t+1)\u20162 , such that\nP ( C\u0302(V \u2217p (t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22123\u03b4 .\nWe get:\nBp =max ({4CB \u2016 f 0(t +1) \u2016 d( ln( d\u03b4 ))2 \u03b1acc\u03b1p(t) } t=1 ,\n{4 \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 ) \u03b12acc\u03b1p(t) } t=1 ,\n{4(CR) 32 \u2016 f 0p(t +1) \u20162 d ln( d\u03b4 ) \u03b13/2acc \u03b1p(t) } t=1 ) .\nHowever, the accuracy of V \u2217p (t+1) depends on f \u2217p(t +1), thus we also have to make\nP ( C\u0302( f \u2217p(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc ) \u2265 1\u22122\u03b4 .\nCombining the result of Theorem 5, we have Bp >\u03b2 Bprim max ({CR \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 )\n\u03b12acc\u03b1p(t)\n} t=1\n,{CR \u2016 f 0p(t +1) \u20162 ln( 1\u03b4 ) \u03b12acc } t=1 ,\n{4CB \u2016 f 0(t +1) \u2016 d( ln( d\u03b4 ))2 \u03b1acc\u03b1p(t) } t=1 ,\n{4 \u2016 f 0p(t +1) \u20163 \u03b7Npd ln( d\u03b4 ) \u03b12acc\u03b1p(t) } t=1 ,\n{4(CR) 32 \u2016 f 0p(t +1) \u20162 d ln( d\u03b4 ) \u03b13/2acc \u03b1p(t) } t=1 ) .\nAs a result, the value of Bp is determined by taking the intersection of\nLemma 15. Assume R( fp(t)) is doubly differentiable w.r.t. fp(t) with \u2016 \u22072R( fp(t)) \u2016\u2264 \u03c4 for all fp(t). Suppose the loss function L is differentiable, L \u2032 is continuous, and satisfies\n|L \u2032(a)\u2212L \u2032(b)| \u2264 c4|a\u2212b|\nfor all pairs (a,b) with a constant c4. Let f \u2217p(t + 1) = argminZprim( fp, t|Dp), and V \u2217p (t + 1) = f \u2217p(t + 1) + \u03b5p(t), where the noise vector \u03b5p(t) is drawn from (15) with the same \u03b1p(t) for all p \u2208P at time t. Let \u039b be the event\n\u039b := {\nZprim(V \u2217p (t +1), t|Dp)\u2264 Zprim( f \u2217p(t +1), t|Dp)\n+ 4 ( CR )2d2(\u03c1\u03c4 + c4CR)( ln( d\u03b4 ))2\n\u03c12B2p\u03b1p(t)2 .\nUnder Assumption 1 and 2, we have: P\u03b5p(t) ( \u039b ) \u2265 1\u2212\u03b4 .\nThe probability P\u03b5p(t) is taken over the noise vector \u03b5p(t).\nProof: (Lemma 15) From Assumption 3, we know that the data points in dataset Dp satisfy: \u2016 xip \u2016\u2264 1, and |yip|= 1. From Assumption 1 and 2, R(\u00b7) and L are differentiable. Suporse R(\u00b7) is doubly differentiable and \u22072R(\u00b7)\u2264 \u03c4 . Let 0\u2264\n\u03d5 \u2264 1, then the Mean Value Theorem and CauchySchwarz inequality give:\nZprim(V \u2217p (t +1), t|Dp)\u2212Zprim( f \u2217p(t +1), t|Dp) = (V \u2217p (t +1)\u2212 f \u2217p(t +1))T \u2207Zprim ( \u03d5 f \u2217p(t +1)\n+(1\u2212\u03d5)V \u2217p (t +1) ) \u2264\u2016V \u2217p (t +1)\u2212 f \u2217p(t +1) \u2016\n\u00b7 \u2016 \u2207Zprim ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1) ) \u2016 .\nLet \u03b5 pi(t) = \u03b5p(t)\u2212 \u03b5i(t). From the definition of Zprim( fp, t|Dp), we have:\nZprim( fp, t|Dp) =Zp( fp, t|Dp)\n\u2212\u03b7 \u2211 i\u2208Np\n( ( fp\u2212\n1 2 ( fp(t)+ fi(t))T \u00b7 (\u03b5 pi(t))\n+ 1 4 (\u03b5 pi(t))2\n) .\nTaking the derivative of Zprim w.r.t. fp gives\n\u2207Zprim( fp, t|Dp) = CR\nBp\nBp \u2211 i=1 yipL \u2032(yip f Tp xip)xip\n+\u03c1\u2207R( fp)\u2212\u03b7 \u2211 j\u2208Np \u03b5 pi(t).\nSince \u2207Zprim( f \u2217p(t +1), t|Dp) = 0, then we have:\n\u2207Zprim ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1)|Dp )\n= \u2207Zprim( f \u2217p(t +1), t|Dp) \u2212\u03c1 ( \u2207R( f \u2217p(t +1))\u2212\u2207R ( \u03d5 f \u2217p(t +1)\n+(1\u2212\u03d5)V \u2217p (t +1) ))\n\u2212C R\nBp\nBp \u2211 i=1\n( yip ( L \u2032(yip f \u2217p(t +1) T xip)\n\u2212L \u2032(yip ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1) )T xip))xip). Let\nT =yip ( L \u2032(yip f \u2217p(t +1) T xip)\n\u2212L \u2032(yip ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1) )T xip))xip. Based on the condition on the loss function:\n|L \u2032(a)\u2212L \u2032(b)| \u2264 c4|a\u2212b|,\nwe can bound T as: T \u2264|yip| \u2016 xip \u2016 \u00b7 |L \u2032(yip f \u2217p(t +1)T xip)\n\u2212L \u2032(yip ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1) )T xip)| \u2264|yip| \u2016 xip \u2016 \u00b7c4 \u00b7 |yip(1\u2212\u03d5)( f \u2217p(t +1)\u2212V \u2217p (t +1))T xip| \u2264c4 \u00b7 (1\u2212\u03d5)|yip|2 \u2016 xip \u20162\u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 \u2264c4 \u00b7 (1\u2212\u03d5) \u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 .\nSince we assume R(\u00b7) is doubly differentiable, we then apply the Mean Value Theorem:\n\u2016 \u2207R( f \u2217p(t +1))\u2212\u2207R ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1) ) \u2016 \u2264 (1\u2212\u03d5) \u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 \u00b7 \u2016 \u22072R(\u03be ) \u2016,\nwhere \u03be \u2208Rd . Therefore, we have: \u2207Zprim ( \u03d5 f \u2217p(t +1)+(1\u2212\u03d5)V \u2217p (t +1)|Dp )\n\u2264 (1\u2212\u03d5) \u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 \u00b7\u03c1\u00b7 \u2016 \u22072R(\u03be ) \u2016 +CRc4 \u00b7 (1\u2212\u03d5) \u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016\n\u2264 (1\u2212\u03d5)\u00b7 \u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 ( \u03c1\u03c4 +CRc4 )\n\u2264\u2016 f \u2217p(t +1)\u2212V \u2217p (t +1) \u2016 ( \u03c1\u03c4 +CRc4 ) .\nSince f \u2217p(t + 1)\u2212V \u2217p (t + 1) = \u03b5p(t), with density \u0393(d, 2C R\n\u03c1Bp\u03b1p(t)) then we can apply Lemma 10 to \u2016 f \u2217p(t+1)\u2212V \u2217p (t+1) \u2016. Thus, with \u2016 f \u2217p(t+1)\u2212 V \u2217p (t +1) \u2016\u2264 2CRd ln( d\u03b4 ) \u03c1Bp\u03b1p(t) , we have:\nZprim(V \u2217p (t +1), t|Dp)\u2212Zprim( f \u2217p(t +1), t|Dp) \u2264 4 ( CR )2d2(\u03c1\u03c4 + c4CR)( ln( d\u03b4 ))2\n\u03c12B2p\u03b1p(t)2 ,\nwith probability no less than 1\u2212\u03b4 .\nAppendix I. Proof that iterations (5) to (8) are convergent ADMM algorithm shown in Appendix A\nThe goal here is to cast (3) in the form of (31) and show that updates (5)-(8) correspond to (33)-(35) in Appendix A. We first reform the constraints { fp = wp j, wp j = f j}p\u2208P, j\u2208Np to A f = w, where f = [ f1, f2, ..., fP]T . For all the\nnodes in the network, the constraint fp = wp j can be written as:\n{ f1 = w1 j} j\u2208N1 ...\n{ fP = wP j} j\u2208NP .\n(52)\nLet w = [{wT1 j} j\u2208N1 , ...,{w T P j} j\u2208NP ] T\nand let A be a block-diagonal matrix with diagonal\nAi = [Id , ...,Id ]T .\nThus,\nA = \u2223\u2223\u2223\u2223\u2223\u2223\u2223 A1 . . . AP \u2223\u2223\u2223\u2223\u2223\u2223\u2223 . Therefore, we can write (51) in the form of matrix and vector as:\nA f = w. (53)\nLet |E | be the number of links in the network. Then there are \u2211Pi=1 Np = 2|E |, where the factor 2 of 2|E | is from the fact that there are two constraints between two nodes: fp = wp j and f j = w jp. Then matrix A \u2208 R2d|E |\u00d7dP, and Ai \u2208 RdNp\u00d7dNp .\nNow we consider the constraint fp = w jp. We can also list it acorss the nodes as:\n{ f1 = w j1} j\u2208N1 ...\n{ fP = w jP} j\u2208NP .\n(54)\nSimilarly, let\nw1 = [{wTj1} j\u2208N1 , ...,{w T jP} j\u2208NP ] T .\nand then we can write (53) in the form as:\nA f = w1. (55)\nIt can be observed that replacing each wi j in w by w ji gives w1. Now we express w1 in terms of w. Let Sw be a 2|E |\u00d72|E | matrix defined as:\nSw = [{s1 j} j\u2208N1 , ...,{sP j} j\u2208NP ],\nwhere sp j = [ ( s1p j )T , ..., ( sPp j )T ]T\nis a 2|E | \u00d7 1 indictor vector. Let \u03b4 (\u00b7, \u00b7) be the Kronecker\u2019s delta. Then\nsap j = [{\u03b4 (p\u2212b, j\u2212b)}b\u2208Na ] T .\nThus, we can write w1 in terms of w as: w1 = ( Sw\u2297 Id ) w. (56)\nTherefore, (54) can be written as: A f = ( Sw\u2297 Id ) w, (57)\nwhere \u2297 denotes Kronecker product. Let A1 = [ AT AT ]T , and S1 = [IT2d|E |(Sw \u2297 Id )T ]T . Then, we can combine (54) and (56) as:\nA1 f = S1w. (58)\nThus, we can re-write (3) as:\nmin Zdec = CR\nBp\nP\n\u2211 p=1\nBp \u2211 i=1 L (yip f Tp xip)+ P \u2211 p=1 \u03c1R( fp)\ns.t. A1 f = S1w. (59)\nNow, let\ng1( f ) = CR\nBp\nP\n\u2211 p=1\nBp \u2211 i=1 L (yip f Tp xip)+ P \u2211 p=1 \u03c1R( fp)\ng2(w) = 0 S1 = RdP\nS2 = { w \u2208 R4d|E ||w = S1w\u2032for somew\u2032 \u2208 R2d|E | } .\nThus, problem (3) has the type of (31). Therefore, the ADMM-based algorithm with updates (5)- (8) is convergent according to Theorem 9 in Appendix A."}, {"heading": "Appendix J. Proof of Proposition 7", "text": "Proof: (Proposition 7) According to Corollary 4.1, fp(t) is \u03b1acc-optimal at each time t, and\nP ( C\u0302( fp(t))\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206dualp (t) ) \u2265 1\u22122\u03b4 ,\nand from Theorem 3, we have P ( C\u0302(Fp(t))\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206non(t) ) \u2265 1\u2212\u03b4 .\nThen, we have: P ( C\u0302( fp(t))\u2264 C\u0302(Fp(t))+\u2206dualp (t)\u2212\u2206non(t) ) \u2265 1\u22122\u03b4 .\nIt also holds for t\u2192\u221e when the Fp(t) converges to f nonp (t + 1). Therefore, fp(t) performs similar to f nonp (t), and the error between them is caused by the noise {\u03b5p(t)}.\nTaking the gradient of Ldual (16) and setting it to 0 at fp(t) give (37) in Appendix:\n\u03b5p(t) =\u2212 Bp\n\u2211 i=1 yipL \u2032(yip fp(t +1)T xip)xip\u2212 Bp CR \u03c1\u2207R( fp)\n\u2212 2Bp CR \u03bbp(t)\u2212 Bp CR (\u03a6+2\u03b7Np) fp(t +1) + Bp\u03b7 CR \u2211i\u2208Np ( fp(t)+ fi(t)).\nFollowing the similar argument in the proof of Theorem 1 in Appendix B, we claim that the relation between \u03b5p(t) and fp(t +1) is bijective.\nLet J f (\u03b5p(t)|Dp) be the Jacobian matrix transforming from fp(t+1)\u2192 \u03b5p(t) as (See Appendix B for more details):\nJ f (\u03b5p(t)|Dp) =\u2212 Bp\n\u2211 i=1 J0f (xi,yi)\u2212 Bp CR \u03c1\u22072R( fp(t +1))\n\u2212 Bp CR (\u03a6+2\u03b7Np)Id .\nBy transformation through Jacobian, we have:\nQ( fp(t)|Dp)\n=K (\u03b5p(t)) \u2016 \u03b5p(t) \u2016d\u22121\nsur(\u2016 \u03b5p(t) \u2016) |det(J f (\u03b5p(t)|Dp))|\u22121\n=K (\u03b5p(t)) 1\nsur(\u2016 1 \u2016) |det(J f (\u03b5p(t)|Dp))|\u22121,\nwhere sur(E) is the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) \u00b7 Ed\u22121. Since \u03b5p(t) is Laplace random variable with density K , and |det(J f (\u03b5p(t)|Dp))| is a bounded function of fp(t + 1). Thus, Q( fP(t)|Dp) is a bounded density function. Therefore, with probability greater than 1\u22122\u03b4 , Algorithm 2 converges in distribution."}, {"heading": "Appendix K. Proof of Proposition 8", "text": "Proof: (Proposition 8) From Corollary 6.1, we have: P ( C\u0302(Vp(t +1))\u2264 C\u0302\u2217(t +1)+\u03b1acc+\u2206primp (t) ) \u2265 1\u22123\u03b4 ,\nand from Theorem 3, P ( C\u0302(Fp(t))\u2264 C\u0302\u2217(t)+\u03b1acc +\u2206non(t) ) \u2265 1\u2212\u03b4 , then, P ( C\u0302(Vp(t +1))\u2264 C\u0302(Fp(t))+\u2206primp (t)\u2212\u2206non(t)\n) \u2265 1\u22123\u03b4 .\nIt also holds for t\u2192\u221e when the Fp(t) converges to f nonp (t + 1). Therefore, Vp(t) performs similar to f nonp (t), and the error between them is caused by the noise {\u03b5p(t)}.\nLet fp(t + 1) = argminLprim(t) with zero duality gap, and let \u03b5 pi(t) = \u03b5p(t)\u2212 \u03b5i(t). Under the Assumption 1 and 2, using the Karush-KuhnTucker (KKT) optimality condition (stationarity), we have\n0 = CR\nBp\nBp \u2211 i=1 yipL \u2032(yip fp(t +1)T xip)xip +\u03c1\u2207R( fp)\n\u2212\u03b7 \u2211 i\u2208Np \u03b5 pi(t).\nLet \u03b5 p(t) = \u2211i\u2208Np \u03b5 pi(t). Then we can establish the relationship between the noise \u03b5 pi(t) and the optimal primal variable fp(t +1) as:\n\u03b5 p(t) = CR\nBp\u03b7\nBp \u2211 i=1 yipL \u2032(yip fp(t +1)T xip)xip\n+ \u03c1 \u03b7 \u2207R( fp).\nAgain, following the similar argument in the proof of Theorem 1 in Appendix B, we claim that there is bijection between \u03b5p(t) and fp(t +1).\nLet J1f (\u03b5 p(t)|Dp) be the Jacobian matrix transforming from fp(t + 1) to \u03b5 p(t) based on the above equation. Let J1f (\u03b5 p(t)|Dp)(a,b) be the (a,b) entry of matrix J1f (\u03b5 p(t)|Dp), then\nJ1f (\u03b5 p(t)|Dp)(a,b)\n= CR\nBp\u03b7\nBp \u2211 i=1 y2ipL \u2032\u2032(yip fp(t +1)T xip)x (a) ip x (b) ip\n+ \u03c1 \u03b7 \u22072R( fp)(a,b).\nThus,\nJ1f (\u03b5 p(t)|Dp) =\nCR\nBp\u03b7\nBp \u2211 i=1 y2ipL \u2032\u2032(yip fp(t +1)T xip)xipxTip\n+ \u03c1 \u03b7 \u22072R( fp).\nWe now find the probability density function of \u03b5 pi(t). Since \u03b5p(t) and \u03b5i(t) are independent, then their joint density function Ppi(z) is:\nPpi(z) = 1 \u03ba\ne\u2212 ( \u03b6p(t)+\u03b6 j(t) ) \u2016z\u2016,\nwhere \u03ba is a normalizing constant. Since \u03b1p(t) is fixed for all nodes at time t, then all the nodes have the same value of \u03b6p(t) = \u03b6 (t). Then\nPpi(\u03b5p(t),\u03b5i(t)) = 1 \u03ba\ne\u22122\u03b6 (t) ( \u2016\u03b5p(t)\u2016\u2212\u2016\u03b5i(t)\u2016 ) .\nThen the cumulative distribution function of \u03b5 pi(t) is F\u03b5 pi(t)(z) =P(\u03b5 pi \u2264 z)\n= \u222b \u221e\n\u221e \u222b \u221e \u03b5 p\u2212z Ppi(\u03b5p(t),\u03b5i(t))Ppi(z)d\u03b5p(t)d\u03b5 j(t).\nThus, the density function of \u03b5 pi(t) is\nPpi(z) = dF\u03b5 pi(t)(z) dz .\nTherefore, the density function of \u03b5 p(t) = \u2211i\u2208Np \u03b5 pi(t) can be expressed as:\nP\u03b5 p(t)(z) = Np\n\u220f i\u2208Np *Ppi(z),\nwhere \u220fNpi\u2208Np * is the Np-fold convolution. By transformation through Jacobian, we have:\nQA( fp(t +1)|Dp) =P\u03b5 p(t)(\u03b5p(t)) \u2016 \u03b5p(t) \u2016d\u22121\nsur(\u2016 \u03b51(t) \u2016) \u00b7 |det(J1f (\u03b5 p(t)|Dp))|\u22121 = P\u03b5 p(t)(\u03b5p(t)) 1 sur(\u2016 1 \u2016) \u00b7 |det(J1f (\u03b5 p(t)|Dp))|\u22121,\nwhere sur(E) is the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) \u00b7 Ed\u22121. |det(J1f (\u03b5 p(t)|Dp))| is a bounded function of fp(t +1).\nSince Vp(t + 1) = fp(t + 1) + \u03b5p(t + 1) and fp(t +1) and \u03b5p(t +1) are independent, then we can find the probability density function, Pt+1Vp , of Vp(t +1) as:\nPt+1Vp (z) = (Q A( fP(t +1)|Dp)\u2217K )(z).\nTherefore, with probability greater than 1\u2212 3\u03b4 , Algorithm 2 converges in distribution."}], "references": [{"title": "Consensus-based distributed support vector machines", "author": ["P.A. Forero", "A. Cano", "G. Giannakis"], "venue": "JMLR, 11:1663\u20131707,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Scaling Distributed Machine Learning with the Parameter Server", "author": ["Mu Li", "Dave Andersen", "Alex Smola", "Junwoo Park", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene Shekita", "Bor-Yiing Su"], "venue": "Operating Systems Design and Implementation (OSDI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Singular Value Inequalities: New Approaches to Conjectures", "author": ["Peter Chilstrom"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "Proc. of the Third Theory of Cryptography Conference \u2013 TCC 2006,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Sibyl: A system for large scale supervised machine learning", "author": ["K. Canini"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Distributed training strategies for the structured perceptron", "author": ["McDonald", "Ryan", "Hall", "Keith", "Mann", "Gideon"], "venue": "In NAACL HLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Differentially private approximation algorithms", "author": ["A. Gupta", "K. Ligett", "F. McSherry", "A. Roth", "K. Talwar"], "venue": "In Proceedings of the 2010 ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "Proceedings of the 39th STOC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["B. Barak", "K. Chaudhuri", "C. Dwork", "S. Kale", "F. McSherry", "K. Talwar"], "venue": "In Proceedings of the 26th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Differentially private empirical risk minimization", "author": ["K Chaudhuri", "C Monteleoni", "AD Sarwate"], "venue": "Journal of machine learning research: JMLR 12,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In Proceedings of the 41st ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms", "author": ["Collins", "Michael"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Singular Value Inequalities: New Approaches to Conjectures", "author": ["P. Chilstrom"], "venue": "UNF Theses and Dissertations", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants", "author": ["Bauer", "Eric", "Kohavi", "Ron"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Bundle methods for regularized risk minimization", "author": ["Teo", "Choon Hui", "S.V.N. Vishwanthan", "Smola", "Alex J", "Le", "Quoc V"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A statistical framework for differential privacy", "author": ["L. Wasserman", "S. Zhou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["C. Dwork", "K. Kenthapadi", "F. McSherry", "I. Mironov", "M. Naor"], "venue": "In Serge Vaudenay, editor, EUROCRYPT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Towards a methodology for statistical disclosure control", "author": ["T. Dalenius"], "venue": "Statistik Tidskrift,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1977}, {"title": "Privacy-preserving data mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "SIGMOD Record,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Limiting privacy breaches in privacy preserving data mining", "author": ["A. Evfimievski", "J. Gehrke", "R. Srikant"], "venue": "In Proceedings of the 22nd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "l-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam"], "venue": "In Proceedings of the 22nd International Conference on Data Engineering (ICDE),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Privacy preserving mining of association rules", "author": ["A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke"], "venue": "Information Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Multiplicative noise for masking continuous data", "author": ["J. Kim", "W. Winkler"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "The price of privacy and the limits of LP decoding", "author": ["C. Dwork", "F. McSherry", "K. Talwar"], "venue": "Proceedings of the 39th STOC,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "Proceedings of the 48th FOCS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Practical privacy: the SuLQ framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "Proceedings of the 24th PODS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Differential privacy with compression", "author": ["S. Zhou", "K. Ligett", "L. Wasserman"], "venue": "In Proceedings of the 2009 International Symposium on Information Theory, Seoul, South Korea,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Robust deanonymization of large sparse datasets (how to break anonymity of the netflix prize dataset)", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "In Proceedings of 29th IEEE Symposium on Security and Privacy,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Cryptographic techniques for privacypreserving data mining", "author": ["B. Pinkas"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}, {"title": "Secure multiparty computation of approximations", "author": ["J. Feigenbaum", "Y. Ishai", "T. Malkin", "K. Nissim", "M.J. Strauss", "R.N. Wright"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "etc.On the Linear Convergence of the ADMM in Decentralized Consensus Optimization", "author": ["Wei Shi", "Qing Ling"], "venue": "IEEE TRANSACTIONS ON SIG- NAL PROCESSING,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Wherefore art thou R3579X? Anonymized social networks, hidden patterns, and structural steganography", "author": ["L. Backstrom", "C. Dwork", "J. Kleinberg"], "venue": "In Proceedings of the 16th International World Wide Web Conference,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Learning your identity and disease from research papers: information leaks in genome wide association study", "author": ["R. Wang", "Y.F. Li", "X. Wang", "H. Tang", "X.-Y. Zhou"], "venue": "In ACM Conference on Computer and Communications Security,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Privacy-preserving classification of vertically partitioned data via random kernels", "author": ["O.L. Mangasarian", "E.W. Wild", "G. Fung"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Multiplicative noise for masking continuous data", "author": ["J. Kim", "W. Winkler"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2003}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["C. Dwork", "K. Nissim"], "venue": "Proc. CRYPTO,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "A General Analysis of the Convergence of ADMM", "author": ["R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan"], "venue": "International Conference on Machine Learning", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "What can we learn privately", "author": ["S.A. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In Proc. of FOCS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "A theory of the learnable", "author": ["L.G. VALIANT"], "venue": "Communications of the ACM", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1984}, {"title": "Securitycontrol methods for statistical databases: a comparative study", "author": ["N.R. Adam", "J.C. Wortmann"], "venue": "ACM Computing Surveys,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1989}, {"title": "Secure statistical databases with random sample queries", "author": ["Dorothy E. Denning"], "venue": "ACM Transactions on Database Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1980}, {"title": "Privacy-preserving data mining", "author": ["Rakesh Agrawal", "Ramakrishnan Srikant"], "venue": "SIGMOD Conference,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2000}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1997}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "SVM optimization : Inverse dependence on training set size", "author": ["S. Shalev-Shwartz", "N. Srebro"], "venue": "In The 25th International Conference on Machine Learning (ICML),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Fast rates for regularized objectives", "author": ["K. Sridharan", "N. Srebro", "S. Shalev-Shwartz"], "venue": "In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "In practice, the amount of training data can range from 1T B to 1PB [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5].", "startOffset": 94, "endOffset": 100}, {"referenceID": 4, "context": "With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5].", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "tire network, and it has been proved that ADMM for convex optimization problem is convergent to the centralized problem under some specific conditions [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 30, "context": "These kinds of attacks have been studied in many works; for example, the adversary can use some background knowledge and cross correlation with other databeses to extract the private information [32, 30].", "startOffset": 195, "endOffset": 203}, {"referenceID": 3, "context": "Specifically, we develop randomized algorithms that can provide privacy in terms of \u03b1differential privacy [4, 9] while keeping the learning procedure accurate.", "startOffset": 106, "endOffset": 112}, {"referenceID": 8, "context": "Specifically, we develop randomized algorithms that can provide privacy in terms of \u03b1differential privacy [4, 9] while keeping the learning procedure accurate.", "startOffset": 106, "endOffset": 112}, {"referenceID": 3, "context": "[4], which adds noise to the output of the non-private regularized ERM algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], we develop a private ADMM-based distributed algorithm for regulatized ERM, which applies primal variable perturbation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "\u2022 We implement our methods by experiments on a dateset of UCI Machine Learning Repositories [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In the first kind of these works, researchers focus on making the distributed algorithm suitable to datasets of very large size; some ([14]) use MapReduce to explore the performance improvements.", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).", "startOffset": 146, "endOffset": 149}, {"referenceID": 19, "context": "Research on privacy has been studied in a significant number of works since at least [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].", "startOffset": 89, "endOffset": 99}, {"referenceID": 19, "context": "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].", "startOffset": 89, "endOffset": 99}, {"referenceID": 20, "context": "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].", "startOffset": 89, "endOffset": 99}, {"referenceID": 31, "context": "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].", "startOffset": 126, "endOffset": 134}, {"referenceID": 30, "context": "Individual information can be re-identified by simply using a small amount of side information [32,16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 15, "context": "Individual information can be re-identified by simply using a small amount of side information [32,16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 24, "context": "Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.", "startOffset": 59, "endOffset": 67}, {"referenceID": 25, "context": "Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.", "startOffset": 59, "endOffset": 67}, {"referenceID": 44, "context": "The idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details).", "startOffset": 90, "endOffset": 94}, {"referenceID": 43, "context": "The idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details).", "startOffset": 104, "endOffset": 108}, {"referenceID": 45, "context": "Since Agrawal and Srikant\u2019s work in [50], increasing number of work studies the limitations and applicability of noise perturbation, and the definitions of privacy started to expand.", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "\u2019s basic definition of privacy [4], \u03b5-indistinguishability or differential privacy, a change in a single entry of the dataset incurs a small change in the distribution from the view of any adversary via a specific measure of distance in a worst-case scenario.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 96, "endOffset": 115}, {"referenceID": 10, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 96, "endOffset": 115}, {"referenceID": 26, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 96, "endOffset": 115}, {"referenceID": 27, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 96, "endOffset": 115}, {"referenceID": 28, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 96, "endOffset": 115}, {"referenceID": 3, "context": "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 42, "context": "derives a general method for probabilistically approximately correct (PAC, [47]) in [46].", "startOffset": 75, "endOffset": 79}, {"referenceID": 41, "context": "derives a general method for probabilistically approximately correct (PAC, [47]) in [46].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "in [9] that provides a method to deliver the dataset differentially privately.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 8, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 9, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 26, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 27, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 28, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 39, "context": "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).", "startOffset": 137, "endOffset": 163}, {"referenceID": 0, "context": "According to Lemma1 in [1], if { fp}p=1 represnet a feasible solution of (2) and the network is connected, then problems (1) and (2) are equivalent, that is, f = fp, p = 1, .", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "The simplified ADMM iteration is shown as follows, due to Lemma 3 of [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "We denote our privacy of distributed network based on the definition of differential privacy in [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.", "startOffset": 80, "endOffset": 91}, {"referenceID": 11, "context": "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "We simulate the customer information by Adult dataset from UCI Machine Learning Repository [11], which contains demographic information such as age, sex, education, occupation, marital status, and native country.", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "In order to process the Adult dataset to our algorithm, we remove all the missing data points, and follow the data cleaning process of [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 46, "context": "([51]) Assume (33) is solvable, and either AT A is nonsingular or S1 is bounded.", "startOffset": 1, "endOffset": 5}, {"referenceID": 30, "context": "Based on Assumption 2, all the eigenvalues of \u22072R( fp(t + 1)) is greater than 1 [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "According to [3], we have the inequality", "startOffset": 13, "endOffset": 16}, {"referenceID": 47, "context": "From Lemma 14 in [52] and the fact that F(\u00b7) is \u03c1-strongly convex, weh have the following inequality:", "startOffset": 17, "endOffset": 21}, {"referenceID": 48, "context": "For the non-private ERM, Shalev-Shwartz and Srebro in [53] show that for a specific reference classifier f0(t + 1) at time t + 1 such that \u0108( f 0(t + 1)) = C\u2217E, we have: \u0108( f non p (t +1)) =\u0108 \u2217", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "[54], we have, with probability at least 1\u2212\u03b4 \u1e90( f non p (t +1), t)\u2212 \u1e90( f\u0302p(t +1), t) \u2264 2 ( Zp( f non p (t +1), t|Dp)\u2212Zp( f\u0303p(t +1), t|Dp) )", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "We use the analysis of ShalevShwartz and Srebro in [53] (also see the work of Chaudhuri et al.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "in [11]), and have the follows:", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "in [54], the following inequality holds with probability 1\u2212\u03b4", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "in [11]), and have the follows,", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "in [54], with", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "in [11]), and have the follows,", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "in [54] shows, with probability 1\u2212\u03b4 :", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Privacy-preserving distributed machine learning becomes increasingly important due to the rapid growth of amount of data and the importance of distributed learning. This paper develops algorithms to provide privacy-preserving learning for classification problem using the regularized empirical risk minimization (ERM) objective function in a distributed fashion. We use the definition of differential privacy, developed by Dwork et al. privacy to capture the notion of privacy of our algorithm. We provide two methods. We first propose the dual variable perturbation, which perturbs the dual variable before next intermediate minimization of augmented Lagrange function over the classifier in every ADMM iteration. In the second method, we apply the output perturbation to the primal variable before releasing it to neighboring nodes. We call the second method primal variable perturbation. Under certain conditions on the convexity and differentiability of the loss function and regularizer, our algorithms is proved to provide differential privacy through the entire learning process. We also provide theoretical results for the accuracy of the algorithm, and prove that both algorithms converges in distribution. The theoretical results show that the dual variable perturbation outperforms the primal case. The tradeoff between privacy and accuracy is examined in the numerical experiment. Our experiment shows that both algorithms performs similar in managing the privacy-accuracy tradeoff, and primal variable perturbaiton is slightly better than the dual case.", "creator": "LaTeX with hyperref package"}}}