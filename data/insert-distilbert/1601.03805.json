{"id": "1601.03805", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Matrix Neural Networks", "abstract": "traditional conceptual neural networks assume vectorial analyst inputs as the network is arranged as layers of single line of computing units called neurons. this special structure requires extending the non - vectorial intelligence inputs such as matrices to be converted into vectors. changing this process can be problematic. firstly firstly, the spatial information among elements of the data may be lost during vectorisation. secondly, the solution space becomes uncomfortably very large which repeatedly demands specific very special treatments to the network parameters demanded and high computational cost. to explicitly address illustrate these issues, we propose matrix neural networks ( matnet ), which takes matrices directly as inputs. each neuron senses summarised information through bilinear mapping from lower layer units in exactly fairly the same way as the classic feed forward neural networks. under this general structure, back prorogation and gradient correlation descent combination tests can be utilised to obtain network parameters efficiently. furthermore, it can be conveniently extended for multimodal inputs. we finally apply spatial matnet to mnist handwritten digits classification and image super resolution tasks to show its effectiveness. without too much tweaking matnet achieves comparable performance as the state - by of - of the - art communication methods in both tasks with essentially considerably reduced complexity.", "histories": [["v1", "Fri, 15 Jan 2016 03:33:35 GMT  (3636kb,D)", "http://arxiv.org/abs/1601.03805v1", "20 pages, 5 figures"], ["v2", "Fri, 9 Dec 2016 01:47:07 GMT  (3652kb,D)", "http://arxiv.org/abs/1601.03805v2", "20 pages, 5 figures"]], "COMMENTS": "20 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junbin gao", "yi guo", "zhiyong wang"], "accepted": false, "id": "1601.03805"}, "pdf": {"name": "1601.03805.pdf", "metadata": {"source": "CRF", "title": "Matrix Neural Networks", "authors": ["Junbin Gao", "Yi Guo"], "emails": ["junbin.gao@sydney.edu.au", "y.guo@westernsydney.edu.au", "zhiyong.wang@sydney.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Neural networks especially deep networks [13, 19] have attracted a lot of attention recently due to their superior performance in several machine learning tasks such as face recognition, image understanding and language interpretation. The applications of neural netowrks go far beyond artificial intelligence domain, stretching to autonomous driving systems [1, 18], pharmaceutical\nar X\niv :1\n60 1.\n03 80\n5v 1\nresearch [32, 33], neuroscience [5, 10, 29, 37, 38] among others. Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4]. Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].\nEvolving from the simplest perceptron [26] to the most sophisticated deep learning neural networks [19], the basic structure of the most widely used neural networks remains almost the same, i.e. hierarchical layers of computing units (called neurons) with feed forward information flow from previous layer to the next layer [6]. Although there is no restriction on how the neurons should be arranged spatially, traditionally they all line in a row or a column just like elements in a vector. The benefit of this is apparently the ease of visualisation of networks as well as the convenience of deduction of mathematical formulation of information flow. As a consequence, vectors are naturally the inputs for the neural networks. This special structure requires the non-vectorial inputs especially matrices (e.g. images) to be converted into vectors. The usual way of vectorising a matrix or multi mode tensor is simply concatenating rows or columns into a long vector if it is a matrix or flatten everything to one dimension if it is a tensor. We are mostly interested in matrices and therefore we restrict our discussion on matrices from now on. Unfortunately this process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Images especially nature images have very strong spatial correlations among pixels. Any sort of vectorisation will certainly result in the loss of such correlation. Moreover, the interpretability is heavily compromised. This renders the neural networks as \u201cblack boxes\u201d as what is going on inside the network is not interpretable by human operator as the information encoded in the parameters or neurons deviates from the form we would normally percept from the very beginning if we take images as an example. Secondly, the solution space becomes very large which demands very special treatments to the network parameters. There are many adverse effects. First, the chance of reaching a meaningful local minimum is reduced due to large domain for sub-optimum. Second, the success of training relies heavily on human intervention, pretraining, special initialisation, juggling parameters of optimisation algorithms and so on. This situation becomes even worse with the growth of the depth of the networks. This is the well known model complexity against learning capacity dilemma [35]. Third, if the spatial information among elements in matrices has to be utilised by the network, one has to resort to either specially designed connection configuration among neurons if it is possible or priors on the network parameters as regularisation which may cripple back prorogation based optimisation because spatial connection means coupling. For large scale problems e.g. big data, this may not be viable at all. Fourth, the computational cost is very high which requires massive computation platforms.\nTo address the issues discussed above, we propose matrix neural networks or MatNet for short, which takes matrices directly as inputs. Therefore the input layer neurons form a matrix, for example, each neuron corresponds to a pixel in a grey scale image. The upper layers are also but not limited to matrices. This is an analogy to the neurons in retina sensing visual signal which are organised in layers of matrix like formation [25]. It is worth of pointing out that the convolutional neural network (ConvNet) [8, 20] works on images (matrices) directly. However, the major difference between ConvNet and MatNet is that ConvNet\u2019s input layers are feature extraction layers consisting of filtering and pooling and its core is still the traditional vector based neural network. While in MatNet matrices are passing through each layer without vectorisation at all. To achieve this, each neuron in MatNet senses summarised information through bilinear mapping from immediate previous layer units\u2019 outputs plus an offset term. Then the neuron activates complying with the pre-specified activation function e.g. sigmoid, tanh, and rectified linear unit (reLU) [21] to generate its output for the next layer. It is exactly\nthe same way as the classic feed forward neural networks. Obviously the bilinear mapping is the key to preserve matrix structure. It is also the key for the application of simple back prorogation to train the network. This will become very clear after we formulate the MatNet model in the next section. In order not to disturb the flow, we leave the derivation of the gradients to appendix where interested readers can find the details.\nTo demonstrate the usefulness of the proposed MatNet, we will test it in two image processing tasks, the well-known MNIST handwritten digits classification and image super resolution. For digits classification, it is just a direct application MatNet to normalised images with given class labels, where MatNet acts as a classifier. However, for image super resolution, MatNet needs some adaptation, i.e. an \u201cadd-on\u201d to accommodate multimodal inputs. As we will show in Section 3, this process is straightforward with great possibility to embrace other modalities such as natural languages for image understanding [40] and automated caption generation [34]. As shown in Section 4, MatNet can achieve comparable classification rate as those sophisticated deep learning neural networks. We need to point out that MatNet is not optimised for this task and the choices of the key network parameters such as the number of layers and neurons are somewhat arbitrary. Surprisingly for super resolution task, MatNet has superior results already in terms of peak signal to noise ratio (PSNR) compared to the state-of-the-art methods such as the sparse representation (SR) [39]. Once again, this result can be further optimised and we will discuss some further developments that will be carried out in near future in Section 5."}, {"heading": "2 Matrix Neural Network Model", "text": "The basic model of a layer of MatNet is the following bilinear mapping\nY = \u03c3(UXV T +B) + E, (2.1)\nwhere U , V , B and E are matrices with compatible dimensions, U and V are connection weights, B is the offset of current layer, \u03c3(\u00b7) is the activation function acting on each element of matrix and E is the error."}, {"heading": "2.1 Network Structure", "text": "The MatNet consists multiple layers of neurons in the form of (2.1). Let X(l) \u2208 RIl\u00d7Jl be the matrix variable at layer l where l = 1, 2, . . . , L, L + 1. Layer 1 is the input layer that takes matrices input directly and Layer L + 1 is the output layer. All the other layers are hidden layers. Layer l is connected to Layer l + 1 by\nX(l+1) = \u03c3(U (l)X(l)V (l)T +B(l)). (2.2)\nwhere B(l) \u2208 RIl+1\u00d7Jl+1 , U (l) \u2208 RIl+1\u00d7Il and V (l) \u2208 RJl+1\u00d7Jl , for l = 1, 2, ..., L \u2212 1. For the convenience of explanation, we define\nN (l) = U (l)X(l)V (l)T +B(l) (2.3)\nfor l = 1, 2, ..., L. Hence X(l+1) = \u03c3(N (l)).\nThe shape of the output layer is determined by the functionality of the network, i.e. regression or classification, which in turn determines the connections from Layer L. We discuss in the following three cases.\n\u2022 Case 1: Normal regression network. The output layer is actually a matrix variable as O = X(L+1). The connection between layer L and the output layer is defined as (2.2) with l = L.\n\u2022 Case 2: Classification network I. The output layer is a multiple label (0-1) vector o = (o1, ..., oK) where K is the number of classes. In o, all elements are 0 but one 1. The final connection is then defined by\nok = exp(ukX (L)vTk + tbk)\u2211K k\u2032=1 exp(uk\u2032X (L)vTk\u2032 + tbk\u2032) , (2.4)\nwhere k = 1, 2, ...,K, U = [uT1 , ....,u T K ] T \u2208 RK\u00d7IL and V = [vT1 , ....,vTK ]T \u2208 RK\u00d7JL . That is both uk and vk are rows of matrices U and V , respectively. Similar to (2.3), we denote\nnk = ukX (L)vTk + tbk. (2.5)\n(2.4) is the softmax that is frequently used in logistic regression [16]. Note that in (2.4), the matrix form is maintained. However, one can flatten the matrix for the output layer leading to the third case.\n\u2022 Case 3: Classification network II. The connection of Layer L to the output layer can be defined as the following\nN (L) k = vec(X (L))Tuk + tbk (2.6)\nok = exp(N (L) k )\u2211K\nk\u2032=1 exp(N (L) k\u2032 )\n(2.7)\nwhere vec() is the vectorisation operation on matrix and uk is a column vector with compatible length. This makes Case 2 a special case of Case 3.\nAssume that we are given a training dataset D = {(Xn, Yn)}Nn=1 for regression or D = {(Xn, tn)}Nn=1 for classification problems respectively. Then we define the following loss functions\n\u2022 Case 1: Regression problem\u2019s loss function is defined as\nL = 1\nN N\u2211 n=1 1 2 \u2016Yn \u2212X(L+1)n \u20162F . (2.8)\n\u2022 Cases 2&3: Classification problem\u2019s cross entropy loss function is defined as\nL = \u2212 1 N N\u2211 n=1 K\u2211 k=1 tnk log(onk). (2.9)\nNote that the selection of cost function is mainly from the consideration of the convenience of implementation. Actually, MatNet is open to any other cost functions as long as the gradient with respect to unknown variables can be easily obtained.\nFrom Eq. (2.2) we can see that the matrix form is well preserved in the information passing right from the input layer. By choosing the shape of U (l), V (l) and B(l) accordingly, one can\nreshape the matrices in hidden layers. In traditional neural networks with vectors input, Eq. (2.2) actually becomes\nx(2) = \u03c3(W (1)vec(X(1)) + b(1)) (2.10)\nwhere x(2) and b(1) are column vectors with compatible lengths. If we vectorise the first hidden layer of MatNet we obtain\nvec(X(2)) = \u03c3((V (1) > \u2297 U (1))vec(X(1)) + vec(B(1))), (2.11)\nwhere A\u2297B is the Kronecker product between matrix A and B and we used the identity\nvec(AXB) = (BT \u2297A)vec(X).\nIt is clear that by choosing W (1) in traditional neural networks such that W (1) = V (1) > \u2297 U (1), it is possible to mimic MatNet and it is also true for other layers. Therefore, MatNet is a special case of traditional neural networks. However, V (l) > \u2297 U (l) has significantly less degrees of freedom than W (l), i.e. Il+1Il + Jl+1Jl v.s. Il+1IlJl+1Jl. The reduction of the solution space brought by the bilinear mapping in Eq. (2.2) is apparent. The resultant effects and advantages include less costly training process, less local minima, easier to handle and most of all, direct and intuitive interpretation. The first three comes immediately from the shrunk solution space. The improved interpretability comes from the fact that U (l) and V (l) work on the matrices directly which normally correspond to input images. Therefore, the functions of U (l) and V (l) becomes clearer, i.e. the linear transformation applied on matrices. This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet."}, {"heading": "2.2 Optimisation", "text": "We collect all the unknown variables i.e. the network parameters of each layer here. They are U (l), V (l), B(l) for l = 1, . . . , L, and uk and tbk for the output layer. Write the parameters of each layer as \u0398(l). From Eq. (2.2) one can easily see that the information is passing in the exactly the same way of the traditional fee forward neural networks. The underlining mechanism is the bilinear mapping in (2.3), which preserves the matrix form throughout the network. This suggests that the optimisation used in traditional neural networks, i.e. back propagation (BP) and gradient descent combination can be used for MatNet. All we need to do is to obtain the derivative of the cost function w.r.t \u0398(l), which can be passed backwards the network.\nSince we proposed both regression and classification network models, the derivatives differ slightly in these two cases due to different cost functions while the back propagation is exactly the same. The details about the gradients and back propagation are in the appendix for better flow of the paper. Once the gradients are computed, then any gradient descent algorithm such as the limited memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) [11] can be readily used to find the sub-optimum given an initialisation. Normally, the network is initialised by random numbers to break symmetry. When the number of layers of a MatNet is 3, this strategy is good enough. However, if MatNet contains many layers, i.e. forming a deep network, then the complexity of the model increases drastically. It requires more training samples. Meanwhile some constraints will be helpful for faster convergence or better solution."}, {"heading": "2.3 Regularisation", "text": "Although MatNet has reduced solution space heavily by using the bilinear mapping in (2.3) already, some techniques routinely used in traditional neural networks can still be used to\nfurther constrain the solution towards the desired pattern. The first is the weight decay, i.e. clamping the size of the weights on the connections, mainly U (l) and V (l). Normally we use Frobenius norm of a matrix for this purpose, that is to incorporate\n\u03bb \u2211 l (\u2016U (l)\u20162F + \u2016V (l)\u20162F ),\nwhere \u03bb is a nonnegative regularisation parameter and the summation of Frobenius norms includes the output layer as well.\nOne may immediately think of the sparsity constraint on the weights to cut off some connections between layers similar to the DropConnect in [36]. It turns out that it is not trivial to incorporate sparsity constraint manifested by sparsity encouraging norms such as `1 norm favourably used in sparse regressions [30]. The dropping in [36] in implemented by a 0/1 mask sampled from Bernoulli distribution. Here we discuss another type of sparsity which is much easier to be incorporated into MatNet. This is the situation when we have an over supply of neurons in hidden layers. In this case, the neural network may be able to discover interesting structure in the data with less number of neurons.\nRecall that X (l) n in (2.2) denotes the activation at hidden unit l in the network. let\n\u03c1(l) = 1\nN N\u2211 n=1 X(l)n (2.12)\nbe the average activations of hidden layer l (averaged over the training set). Through (approximately) enforcing the constraint elementwise\n\u03c1 (l) ij = \u03c1,\none can achieve sparsity in reducing the number of neurons [28]. Therefore, \u03c1 is called a sparsity parameter, typically a small value close to zero, e.g. \u03c1 = 0.05. In words, the constraint requires the average activation of each hidden neuron to be close to a small given value. To satisfy this constraint, some hidden units\u2019 activations must be close to 0.\nTo implement the above equality constraint, we need a penalty term penalising the elements of \u03c1(l) deviating significantly from \u03c1. The deviation is quantified as the following akin to Kullback-Leibler divergence or entropy [7]:\nRl = sum\n( \u03c1 log \u03c1\n\u03c1(l) + (1\u2212 \u03c1) log 1\u2212 \u03c1 1\u2212 \u03c1(l)\n) (2.13)\nwhere sum(M) summing over all the elements in matrix M ; log and / are applied to matrix elementwise. To screen out neurons that are not necessary, we add the following extra term in the cost function of MatNet\n\u03b2 L\u2211 l=2 Rl.\nThe gradient of this term is detailed in the appendix."}, {"heading": "3 Multimodal Matrix Neural Networks", "text": "We have the basics of MatNet from above discussion. Now we proceed to extending MatNet to multimodal case for image super resolution application. The extension is as straightforward\nas including more than one input matrix at the same time at input layer. Conceptually, we have more than one input layer standing side by side for different modalities and they all send the information to the shared hidden layers through separate connections [22]. It turns out for super resolution, three layer MatNet is sufficient, i.e., input layer, hidden layer and output layer, and it works on autoencoder [14] mode meaning a regression MatNet reproducing the input in output layer. This requires that the output layer has the same amount of modalities as the input layer. Although we showcase only a three layer regression multimodal MatNet, it is not difficult to extend to other type of multimodal MatNet with multiple hidden layers using the same methodology.\nAssumeD modalities as matrices in consideration denoted byXj \u2208 RKj1\u00d7Kj2 (j = 1, 2, ..., D). Similarly there are D output matrix variables of the same sizes. Denote by X = (X1, ..., XD). In the hidden layer, we only have one matrix variable H \u2208 RK1\u00d7K2 . The transformation from input layer to hidden layer is defined by the following multiple bilinear mapping with the activation function \u03c3 (sigmoid or any other activation function)\nH = \u03c3( D\u2211 j=1 UjX jV Tj +B) (3.1)\nand from hidden layer to output layer by\nX\u0302j = \u03c3(RjHS T j + Cj), j = 1, 2, ..., D. (3.2)\nWe call H the encoder for data X . For a given set of training data D = {Xj}Ni=1 with Xi = (X1i , ..., XDi ), the corresponding hidden variable is denoted by Hi. The objective function to be minimised for training an MatNet autoencoder is defined by\nL = 1\n2N N\u2211 i=1 D\u2211 j=1 \u2016X\u0302ji \u2212X j i \u2016 2 F . (3.3)\nL is a function of all the parameters W = {Uj , Vj , Rj , Sj , Cj , B}Dj=1. We leave the derivation of the gradients of multimodal MatNet autoencoder to the appendix. It is very similar to those of the original MatNet and therefore the the same BP scheme can be utilised for optimisation."}, {"heading": "4 Experimental Evaluation", "text": "In this section, we apply MatNet to MNIST handwritten digits classification and image super resolution. The network settings are somewhat arbitrary, or in other words, we did not optimise the number of layers and neurons in each layer in these tests. For handwritten digits recognition, MatNet was configured as a classification network, i.e. the output layer was a vector of softmax functions as in Eq. (2.6) and (2.7) of length 10 (for 10 digits). It contained 2 hidden layers, each with 20\u00d720 and 16\u00d716 neurons. As the numbers of layers and neurons were very conservative, we turned off sparsity constraint. MatNet was onboarded with unit norm manifold constraint. Hence we excluded the weights decay. For super resolution task, the only hidden layer was of size 10 \u00d7 10, therefore, only 3 layer MatNet. The activation function in both networks was sigmoid."}, {"heading": "4.1 MNIST Handwritten Digits Classification", "text": "The MNIST handwritten digits database is available at http://yann.lecun.com/exdb/mnist/. The entire database contains 60,000 training samples and 10,000 testing samples, and each digit is a 28\u00d7 28 gray scale image. We use all training samples for modeling and test on all testing samples. Figure 1 shows the weights, U (l) and V (l), and bias B(l) in hidden layers. Figure 2 shows the first 100 test digits, and hidden layer outputs. The check board effects can be seen from the the hidden layer output in Figure 2(b). The final classification rate is 97.3%, i.e. error rate of 2.7%, which is inferior to the best MNIST performance by DropConnect with error rate 0.21%. However, as we stated earlier, MatNet has much less computational complexity."}, {"heading": "4.2 Image Super Resolution", "text": "For image super resolution, we need to use the multimodal MatNet detailed in Section 3. The training is the following. From a set of high resolution images, we downsample them by bicubic\ninterpolation to the ratio of 1/s where s is the target up-scaling factor. In this experiment, s = 2. From these down scaled images, we sampled patches, say 15, from their feature images, i.e. first and second derivatives along x and y direction, 4 feature images for each. These are the modalities from X2 to X5. We also sampled the same size patches from the original high resolution images as X1. See Eq. (3.1). These data were fed into multimodal MatNet for training.\nTo obtain a high resolution image we used the following procedure. First upscale the image by bicubic interpolation to the ratio of s and convert it to YCbCr space. The luminance component is then the working image on which the same size patches are sampled by sliding window as new input X1. Obtain 4 feature images from this working image on which patches are sampled exactly the same way to form X2 to X5. Feed these to a well trained multimodal MatNet to get high resolution image patches from network output. The high resolution patches are then merged together by averaging pixels in patches. This gives us the high resolution luminance image, which is in turn combined with up-scaled, chrominance images, Cb and Cr images, simply by bicubic interpolation, to form final high resolution image in YCbCr space. For better display, it is converted to RGB format as final image.\nWe applied MatNet to the data set used in SR [39], both for training and testing. There are 69 images for training. The patch size was 15\u00d7 15. We randomly sampled 10,000 patches altogether from all images for training. Some additional parameters for MatNet are \u03bb = 0.001, \u03c1 = 0.05 and \u03b2 = 1. So we turned on weight decay and sparsity constraints but left out the manifold constraint. Figure 3 shows the network parameters learnt from the data, from which we can observe the scaling changing filters in the weights for high resolution patches.\nFig. 4 shows the results on two testing images. Multimodal MatNet has comparable performance as SR, the state-of-the-art super resolution method, evaluated by PSNR: for Lena image, multimodal MatNet, SR and bicubic interpolation achieved PSNR 33.966dB, 35.037dB and 32.795dB respectively; for kingfisher image, they had PSNR 36.056dB, 36.541dB and 34.518dB respectively. We applied to a number of images of similar size (256 \u00d7 256) and we observed similar scenario. Fig. 5 (a) shows the all the test images, including the two in Fig. 4, and PSNR\u2019s obtained by different methods is shown in Fig. 5 (b). MatNet is very close to SR in terms of PSNR, especially for image 5 and 8."}, {"heading": "5 Discussion", "text": "We proposed a matrix neural network (MatNet) in this paper, which takes matrices input directly without vectorisation. The most prominent advantage of MatNet over the traditional\nvector based neural works is that it reduces the complexity of the optimisation problem drastically, while manages to obtain comparable performance as the state-of-the-art methods. This has been demonstrated in applications of MNIST handwritten digits classification and image super resolution.\nAs we mentioned several times in the text, MatNet was not specially optimised for the tasks we showed in experiment section. There is a lot of potentials for further improvement. Many techniques used for deep networks can be readily applied to MatNet with appropriate adaptation, e.g. reLU activation function, max-pooling, etc., which certainly become our future research."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Backpropagation Algorithm for Regression", "text": "We will work out the derivative formulas for all the parameters \u0398 = {U (l), V (l), B(l)}Ll=1. We use the following useful formulas\nvec(AXB) = (BT \u2297A)vec(X), \u2202AXB T\n\u2202X :=\n\u2202vec(AXBT )\n\u2202vec(X) = B \u2297A,\nwhere vec(M) transforms a matrix into a column vector along columns of the matrix and \u2297 is the Kronecker product operator. Also we will use to denote the elementwise product of two vectors or two matrices. In the following derivative formula for matrix valued functions, we use the tradition \u2202A\u2202B = [ Aij \u2202Bkl\n](ij,kl) \u2208 R(I\u00d7J)\u00d7(K\u00d7L) for matrix variables A \u2208 RI\u00d7J and B \u2208 RK\u00d7L. From (2.2), we can see that, for all l = 1, 2, ..., L\nX(l+1)n = \u03c3(N (l) n ),\nwhere n refers to the n-th item corresponding to the training dataset.\nWe are interested in the derivative of the regression loss function (2.8) with respect to N (l) n .\nL is the function of N (l) n via its intermediate variable N (l+1) n . Hence the chain rule gives\nvec( \u2202L\n\u2202N (l) n\n)T = vec( \u2202L\n\u2202N (l+1) n\n)T \u2202N\n(l+1) n\n\u2202N (l) n\n. (6.1)\nNote that\nN (l+1)n = U (l+1)X(l+1)V (l+1)T +B(l+1) = U (l+1)\u03c3(N (l)n )V (l+1)T +B(l+1)\nAs the sigmoid function \u03c3 is applied elementwise to the matrix, it is easy to show that\n\u2202N (l+1) n\n\u2202\u03c3(N (l) n )\n= \u2202vec\n( N (l+1) n ) \u2202vec ( \u03c3(N (l) n ) ) = V (l+1) \u2297 U (l+1). (6.2)\nA direct calculation leads to\n\u2202\u03c3(N (l) n )\n\u2202N (l) n\n= diag(vec(\u03c3\u2032(N (l)))). (6.3)\nTaking (6.2) and (6.3) into (6.1) gives, with a transpose,\nvec( \u2202L\n\u2202N (l) n\n) = diag(vec(\u03c3\u2032(N (l)n )))(V (l+1)T \u2297 U (l+1)T )vec( \u2202L\n\u2202N (l+1) n\n)\n= diag(vec(\u03c3\u2032(N (l)n )))vec\n( U (l+1)T \u2202L\n\u2202N (l+1) n\nV (l+1)\n)\n= vec(\u03c3\u2032(N (l)n ))vec\n( U (l+1)T \u2202L\n\u2202N (l+1) n\nV (l+1)\n)\n= vec ( \u03c3\u2032(N (l)n ) ( U (l+1)T \u2202L\n\u2202N (l+1) n\nV (l+1) )) .\nFinally we have proved that\n\u2202L\n\u2202N (l) n\n= ( U (l+1)T \u2202L\n\u2202N (l+1) n\nV (l+1) ) \u03c3\u2032(N (l)n ). (6.4)\nFrom (2.8) we have\n\u2202L\n\u2202N (L) n\n= (\u03c3(N (L)n )\u2212 Yn) \u03c3\u2032(N (L)n ). (6.5)\nHence both (6.4) and (6.5) jointly define the backpropagation algorithm. Let us denote\n\u03b4 (l) n = \u2202L\n\u2202N (l) n\n.\nNow consider the derivatives with respect to parameters. Take U (l) as an example:\nvec\n( \u2202L\n\u2202U (l)\n)T = N\u2211 n=1 vec ( \u2202L \u2202N (l) n )T \u2202N (l) n \u2202U (l)\n= N\u2211 n=1 vec\n( \u2202L\n\u2202N (l) n\n)T ( V (l)X(l)Tn \u2297 IIl+1 ) .\nThis gives\n\u2202L\n\u2202U (l) = N\u2211 n=1 \u2202L \u2202N (l) n V (l)X(l)Tn = N\u2211 n=1 \u03b4(l)n V (l)X(l)Tn . (6.6)\nSimilarly\n\u2202L\n\u2202V (l) = N\u2211 n=1 \u03b4(l)Tn U (l)X(l)n . (6.7)\n\u2202L\n\u2202B(l) = N\u2211 n=1 \u03b4(l)n (6.8)\nThen we have the following algorithm, for l = L\u2212 1, ..., 1,\n\u03b4(L)n = (\u03c3(N (L) n )\u2212 Yn) \u03c3\u2032(N (L)n ). (6.9)\n\u2202L\n\u2202U (l) = N\u2211 n=1 \u03b4(l)n V (l)X(l)Tn (6.10)\n\u2202L\n\u2202V (l) = N\u2211 n=1 \u03b4(l)Tn U (l)X(l)n (6.11)\n\u2202L\n\u2202B(l) = N\u2211 n=1 \u03b4(l)n (6.12)\n\u03b4(l)n = ( U (l+1)T \u03b4(l+1)n V (l+1) ) \u03c3\u2032(N (l)n ) (6.13)\n\u03c3\u2032(N (l)n ) = \u03c3(N (l) n ) \u00b7 (1\u2212 \u03c3(N (l)n )) = X(l+1)n \u00b7 (1\u2212X(l+1)n ) (6.14)\nwhere \u03c3(N (l) n ) = X (l+1) n is actually the output of layer l + 1."}, {"heading": "6.2 Backpropagation Algorithm for Classification", "text": "The only difference between regression and classification mnnet is in the last layer where the output at layer L+ 1 is a vector of dimension K. That is the connection between this output layer and layer L is between a vector and the matrix variable X(L) of dimensions IL \u00d7 JL.\nAccording to (2.7), we have the following two cases for calculating \u2202onk \u2202N (L)\nnk\u2032 :\nCase 1: k = k\u2032. Then\n\u2202onk\n\u2202N (L) nk\n=\n(\u2211K k\u2032=1 exp(N (L) nk\u2032 ) ) exp(N (L) nk )\u2212 exp(N (L) nk ) exp(N\n(L) nk )(\u2211K\nk\u2032=1 exp(N (L) nk\u2032 ) )2 = onk(1\u2212 onk)\nCase 2: k 6= k\u2032. Then\n\u2202onk\n\u2202N (L) nk\u2032\n= \u2212 exp(N (L)nk ) exp(N (L) nk\u2032 )(\u2211K\nk\u2032=1 exp(N (L) nk\u2032 ) )2 = \u2212onkonk\u2032 Combining the above cases results in\n\u03b4 (L) nk =\n\u2202L\n\u2202N (L) nk\n= \u2212 \u2202 \u2202N\n(L) nk K\u2211 k\u2032=1 tnk\u2032 log onk\u2032\n= \u2212tnk 1\nonk onk(1\u2212 onk) + K\u2211 k\u2032 6=k tnk\u2032 1 onk\u2032 onkonk\u2032\n= onk \u2212 tnk.\nFor our convenience, denote\n\u03b4(L) = OK \u2212 TK = [onk \u2212 tnk]nk \u2208 RN\u00d7K .\nFinally we want to calculate \u03b4 (L\u22121) n =\n\u2202L\n\u2202N (L\u22121) n\nwhere N (L\u22121) n is a matrix, i.e., the output\nbefore sigmoid in layer L. In other lower layers, the formulas will be the same as the regression case. From (2.9), we have, noting that N (L) nk = vec(\u03c3(N (L\u22121) n )Tuk + tbk ((2.6) and (2.7)),\nvec\n( \u2202L\n\u2202N (L\u22121) n\n) =\nK\u2211 k=1 \u2202L \u2202N (L) nk \u2202N (L) nk \u2202N (L\u22121) n = K\u2211 k=1 \u03b4 (L) nk diag(vec(\u03c3 \u2032(N (L\u22121)n )))uk.\nFor each uk, we convert it into a matrix, denoted by Uk, according to the position of elements X (L) n , and formulate a third-order tensor U such that U(:, :, k) = Uk. Then\n\u03b4(L\u22121)n = \u2202L\n\u2202N (L\u22121) n = K\u2211 k=1 \u03b4 (L) nk (\u03c3 \u2032(N (L\u22121)n ) Uk) = \u03c3\u2032(N (L\u22121)n ) (U\u00d73\u03b4(L)n ) (6.15)\nAgain, according to both (2.6) and (2.7), it is easy to see that\n\u2202onk \u2202uk\u2032\n= \u2212 exp(N (L)nk ) exp((N (L) nk\u2032 )vec(X (L) n )(\u2211K\nk\u2032=1 exp(N (L) nk\u2032 ) )2 = \u2212onkonk\u2032vec(X(L)n ). The second case of k = k\u2032 is actually\n\u2202onk \u2202uk =\n(\u2211K k\u2032=1 exp(N (L) nk\u2032 ) ) exp(N (L) nk )vec(X (L) n )\u2212 exp(N (L)nk ) exp((N (L) nk )vec(X\n(L) n )(\u2211K\nk\u2032=1 exp(N (L) nk\u2032 ) )2 = onk(1\u2212 onk)vec(X(L)n )\nHence, for each k = 1, 2, ...,K,\n\u2202L\n\u2202uk = \u2212 N\u2211 n=1 K\u2211 k\u2032=1 tnk 1 onk\u2032 \u2202onk\u2032 \u2202uk\n= \u2212 N\u2211\nn=1  K\u2211 k\u2032 6=k tnk\u2032 1 onk\u2032 (\u2212onk\u2032)onkvec(X(L)n ) + tnk 1 onk onk(1\u2212 onk)vec(X(L)n )  = \u2212\nN\u2211 n=1\n\u2212  K\u2211\nk\u2032 6=k tnk\u2032\n onk + tnk(1\u2212 onk)  vec(X(L)n )\n= \u2212 N\u2211\nn=1\n[\u2212 (1\u2212 tnk) onk + tnk(1\u2212 onk)] vec(X(L)n )\n= N\u2211 n=1 (onk \u2212 tnk)vec(X(L)n )\nIf we formulate a matrix U = [u1,u2, ...,uK ], then\n\u2202L \u2202U = N\u2211 n=1 vec(XL)n )[on1 \u2212 tn1, on2 \u2212 tn2, ..., onK \u2212 tnK ]\n= X(L)\u03b4(L) (6.16)\nwhere X(L) = [vec(X L) 1 ), vec(X L) 2 ), ..., vec(X L) N )] \u2208 R(IL\u00d7JL)\u00d7N .\nSimilar to \u2202onk \u2202N (L)\nnk\u2032 , we have\n\u2202onk \u2202tbk = onk(1\u2212 onk) and \u2202onk \u2202tbk = \u2212onkonk\u2032 (k 6= k\u2032). (6.17)\nSo it is easy to show\n\u2202L\n\u2202tbk = N\u2211 n=1 (onk \u2212 tnk), , that is \u2202L \u2202tb = sum(OK \u2212 TK).\nThe entire backpropagation is to combine (6.10) to (6.14), and (6.15) to (6.17)."}, {"heading": "6.3 Sparsity", "text": "We repeat the sparsity penalty Rl here.\nRl = sum\n( \u03c1 log \u03c1\n\u03c1(l) + (1\u2212 \u03c1) log 1\u2212 \u03c1 1\u2212 \u03c1(l)\n) (6.18)\nwhere sum(M) means the sum of all the elements of matrix M , and log and / are applied to matrix elementwise.\nIf we applied the sparsity constraints on all the layers excepts for input and output layers, the objective function (of regression) defined in (2.4) can be sparsely regularised as\nL\u2032 = L+ \u03b2 L\u2211 l=2 Rl = N\u2211 n=1 1 2 \u2016Yn \u2212X(L+1)n \u20162F + \u03b2 L\u2211 l=2 Rl. (6.19)\nThen, by noting that Rj (j < l + 1) is irrelevant to N (l) n ,\n\u2202L\u2032\n\u2202N (l) n\n= \u2202\n\u2202N (l) n\n(L+ \u03b2 L\u2211 j>l+1 Rj) + \u03b2 \u2202 \u2202N (l) n Rl+1\n= \u2202L\u2032\n\u2202N (l) n\n+ \u03b2 \u2202\n\u2202N (l) n\nRl+1\n= \u2202L\u2032\n\u2202N (l+1) n\n\u2202N (l+1) n\n\u2202N (l) n\n+ \u03b2 \u2202\n\u2202N (l) n\nRl+1\n= [ U (l+1)T \u2202L\u2032\n\u2202N (l+1) n\nV (l+1) ] \u03c3\u2032(N (l)n ) + \u03b2 \u2202\n\u2202N (l) n\nRl+1\nBy using the similar technique, we can prove that\n\u2202\n\u2202N (l) n\nRl+1 = [ \u2212 \u03c1 \u03c1(l+1) + 1\u2212 \u03c1 1\u2212 \u03c1(l+1) ] \u03c3\u2032(N (l)n )\nHence the backpropagation defined in (6.4) can be re-defined as\n\u03b4 \u2032(l) n = [ U (l+1)T \u03b4 \u2032(l+1) n V (l+1) + \u03b2 ( \u2212 \u03c1 \u03c1(l) + 1\u2212 \u03c1 1\u2212 \u03c1(l) )] \u03c3\u2032(N (l)n )\nThe above can be easily implemented into BP scheme as explained in previous section."}, {"heading": "6.4 BP Algorithm for Multimodal MatNet Autoencoder", "text": "To train the multimodal MatNet autoencoder, we need to work out the derivatives of L with respect to all the parameters. First, we define the derivative of L with respect to the output layer variables\n\u03b42ij = X\u0302 j i \u2212X j i .\nNow we back-propagate these derivatives from output layer to the hidden layer according to the network structure and define\n\u03b41i = D\u2211 j=1 Sj(\u03b4 2 ij \u03c3\u2032(RjYiSTj + Cj))RTj = D\u2211 j=1 RTj (\u03b4 2 ij \u03c3\u2032(X\u0302 j i ))Sj\nThen it is not hard to prove that\n\u2202L\n\u2202Rj =\n1\nN N\u2211 i=1 (\u03b42ij \u03c3\u2032(X\u0302 j i ))SjY T i (6.20)\n\u2202L \u2202Sj = 1 N N\u2211 i=1 (\u03b42ij \u03c3\u2032(X\u0302 j i )) TRjYi (6.21)\n\u2202L\n\u2202Cj =\n1\nN N\u2211 i=1 (\u03b42ij \u03c3\u2032(X\u0302 j i )) (6.22)\nand\n\u2202L\n\u2202Uj =\n1\nN N\u2211 i=1 (\u03b41i \u03c3\u2032(Yi))VjXTi (6.23)\n\u2202L \u2202Vj = 1 N N\u2211 i=1 (\u03b41i \u03c3\u2032(Yi))TUjXi (6.24)\n\u2202L \u2202B = 1 N N\u2211 i=1 (\u03b41i \u03c3\u2032(Yi)) (6.25)\nThe algorithm implementation is straighforward. In the forward sweeping, from the input, we can get all Yi and X\u0302 j i , then in the backward sweep, all the \u03b4\u2019s can be calculated, then all the derivatives can be obtained from the above formula."}, {"heading": "6.5 Sparsity in Multimodal MatNet Autoencoder", "text": "If we applied the sparsity constraint on the hidden layer, the objective function defined in (3.3) becomes\nL\u2032 = L+ \u03bbRy = 1\n2N N\u2211 i=1 D\u2211 j=1 \u2016X\u0302ji \u2212X j i \u2016 2 F + \u03b2Ry. (6.26)\nAs Ry is independent of Rj , Sj , Cj , then \u2202L\u2032\n\u2202Rj = \u2202L\u2202Rj ,\n\u2202L\u2032 \u2202Sj = \u2202L\u2202Sj and \u2202L\u2032 \u2202Cj = \u2202L\u2202Cj . We can\nprove that \u2202Ry \u2202Yi = 1 N [ \u2212\u03c1 \u03c1 + 1\u2212 \u03c1 1\u2212 \u03c1 ] :, 1 N \u03b4(\u03c1).\nThen we have\n\u2202L\u2032 \u2202Uj = 1 N N\u2211 i=1 ((\u03b41i + \u03b2\u03b4(\u03c1)) \u03c3\u2032(Yi))VjXTi (6.27) \u2202L\u2032 \u2202Vj = 1 N N\u2211 i=1 ((\u03b41i + \u03b2\u03b4(\u03c1)) \u03c3\u2032(Yi))TUjXi (6.28) \u2202L\u2032\n\u2202B =\n1\nN N\u2211 i=1 ((\u03b41i + \u03b2\u03b4(\u03c1)) \u03c3\u2032(Yi)) (6.29)"}], "references": [{"title": "Pedestrian detection with a large-field-of-view deep network", "author": ["Anelia Angelova", "Alex Krizhevsky", "Vincent Vanhoucke"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Symmetry-invariant optimization in deep networks", "author": ["V. Badrinarayanan", "B. Mishra", "R. Cipolla"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep networks for motor control functions", "author": ["Max Berniker", "Konrad P Kording"], "venue": "Frontiers in computational neuroscience,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "John S Denker", "D Henderson", "Richard E Howard", "W Hubbard", "Lawrence D Jackel"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "From deep learning to episodic memories: Creating categories of visual experiences", "author": ["Jigar Doshi", "Zsolt Kira", "Alan Wagner"], "venue": "In Proceedings of the Third Annual Conference on Advances in Cognitive Systems ACS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "An improved implementation of the lbfgs algorithm for automatic history matching", "author": ["Guohua Gao", "Albert C Reynolds"], "venue": "In SPE Annual Technical Conference and Exhibition. Society of Petroleum Engineers,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Introduction to special issue on neuromorphic computing", "author": ["Dan Hammerstrom", "Vijaykrishnan Narayanan"], "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "P.R. Salakhutdinov"], "venue": "Science, 313:504\u2013507,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Tensor principal component analysis via sum-of-squares proofs", "author": ["Samuel B Hopkins", "Jonathan Shi", "David Steurer"], "venue": "arXiv preprint arXiv:1507.03269,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Applied logistic regression, volume 398", "author": ["David W Hosmer Jr.", "Stanley Lemeshow", "Rodney X Sturdivant"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Jianxiong Xiao"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-Eighth International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data", "author": ["P. Paatero", "U. Tapper"], "venue": "values. Environmetrics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["Ning Qiao", "Hesham Mostafa", "Federico Corradi", "Marc Osswald", "Fabio Stefanini", "Dora Sumislawska", "Giacomo Indiveri"], "venue": "Frontiers in neuroscience,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Rodieck. The vertebrate retina: principles of structure and function", "author": ["W Robert"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}, {"title": "The perceptron - a perceiving and recognizing automaton", "author": ["Frank Rosenblatt"], "venue": "Technical report, Cornell Aeronautical Laboratory,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1957}, {"title": "The brain", "author": ["Robert F. Service"], "venue": "chip. Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sparse autoencoders for word decoding from magnetoencephalography", "author": ["Michelle Shu", "Alona Fyshe"], "venue": "In Proceedings of the third NIPS Workshop on Machine Learning and Interpretation in NeuroImaging (MLINI),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Deep learning in diagnosis of brain disorders", "author": ["Heung-Il Suk", "Dinggang Shen"], "venue": "In Recent Progress in Brain and Cognitive Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["Robert Tibshirani"], "venue": "Journal of Royoal Statistical Society,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Creation of a deep convolutional auto-encoder in caffe", "author": ["Volodymyr Turchenko", "Artur Luczak"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Aiding drug design with deep neural networks", "author": ["Thomas Unterthiner", "Andreas Mayr", "G\u00fcnter Klambauer", "Marvin Steijaert", "J\u00f6rg K Wegner", "Hugo Ceulemans", "Sepp Hochreiter"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Deep learning for drug target prediction", "author": ["Thomas Unterthiner", "Andreas Mayr", "G\u00fcnter Klambauer", "Marvin Steijaert", "J\u00f6rg K Wegner", "Hugo Ceulemans", "Sepp Hochreiter"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Common subspace for model and similarity: Phrase learning for caption generation from images", "author": ["Yoshitaka Ushiku", "Masataka Yamaguchi", "Yusuke Mukuta", "Tatsuya Harada"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "The Nature of Statistical Learning", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Encoding voxels with deep learning", "author": ["Panqu Wang", "Vicente Malave", "Ben Cipollini"], "venue": "The Journal of Neuroscience,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "The emergence of face-selective units in a model that has never seen a face.yaminscohenhongkanwisherdicarlo2015", "author": ["Daniel Yamins", "Michael Cohen", "Ha Hong", "Nancy Kanwisher", "James Di- Carlo"], "venue": "Journal of vision,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Image super-resolution via sparse representation", "author": ["Jianchao Yang", "John Wright", "Thomas Huang", "Yi Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Neural self talk: Image understanding via continuous questioning and answering", "author": ["Yezhou Yang", "Yi Li", "Cornelia Fermuller", "Yiannis Aloimonos"], "venue": "arXiv preprint arXiv:1512.03460,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Fast nonnegative matrix/tensor factorization based on low-rank approximation", "author": ["Guoxu Zhou", "A Cichocki", "Shengli Xie"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Neural networks especially deep networks [13, 19] have attracted a lot of attention recently due to their superior performance in several machine learning tasks such as face recognition, image understanding and language interpretation.", "startOffset": 56, "endOffset": 64}, {"referenceID": 0, "context": "The applications of neural netowrks go far beyond artificial intelligence domain, stretching to autonomous driving systems [1, 18], pharmaceutical", "startOffset": 123, "endOffset": 130}, {"referenceID": 17, "context": "The applications of neural netowrks go far beyond artificial intelligence domain, stretching to autonomous driving systems [1, 18], pharmaceutical", "startOffset": 123, "endOffset": 130}, {"referenceID": 30, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 9, "endOffset": 17}, {"referenceID": 31, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 9, "endOffset": 17}, {"referenceID": 4, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 9, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 27, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 35, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 36, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 16, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 145, "endOffset": 153}, {"referenceID": 29, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 145, "endOffset": 153}, {"referenceID": 3, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 22, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 25, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 24, "context": "Evolving from the simplest perceptron [26] to the most sophisticated deep learning neural networks [19], the basic structure of the most widely used neural networks remains almost the same, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "hierarchical layers of computing units (called neurons) with feed forward information flow from previous layer to the next layer [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 33, "context": "This is the well known model complexity against learning capacity dilemma [35].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "This is an analogy to the neurons in retina sensing visual signal which are organised in layers of matrix like formation [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "It is worth of pointing out that the convolutional neural network (ConvNet) [8, 20] works on images (matrices) directly.", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "It is worth of pointing out that the convolutional neural network (ConvNet) [8, 20] works on images (matrices) directly.", "startOffset": 76, "endOffset": 83}, {"referenceID": 19, "context": "sigmoid, tanh, and rectified linear unit (reLU) [21] to generate its output for the next layer.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "As we will show in Section 3, this process is straightforward with great possibility to embrace other modalities such as natural languages for image understanding [40] and automated caption generation [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 32, "context": "As we will show in Section 3, this process is straightforward with great possibility to embrace other modalities such as natural languages for image understanding [40] and automated caption generation [34].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "Surprisingly for super resolution task, MatNet has superior results already in terms of peak signal to noise ratio (PSNR) compared to the state-of-the-art methods such as the sparse representation (SR) [39].", "startOffset": 202, "endOffset": 206}, {"referenceID": 15, "context": "4) is the softmax that is frequently used in logistic regression [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 21, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 39, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 10, "context": "Once the gradients are computed, then any gradient descent algorithm such as the limited memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) [11] can be readily used to find the sub-optimum given an initialisation.", "startOffset": 137, "endOffset": 141}, {"referenceID": 34, "context": "One may immediately think of the sparsity constraint on the weights to cut off some connections between layers similar to the DropConnect in [36].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "It turns out that it is not trivial to incorporate sparsity constraint manifested by sparsity encouraging norms such as `1 norm favourably used in sparse regressions [30].", "startOffset": 166, "endOffset": 170}, {"referenceID": 34, "context": "The dropping in [36] in implemented by a 0/1 mask sampled from Bernoulli distribution.", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Through (approximately) enforcing the constraint elementwise \u03c1 (l) ij = \u03c1, one can achieve sparsity in reducing the number of neurons [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "The deviation is quantified as the following akin to Kullback-Leibler divergence or entropy [7]:", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "Conceptually, we have more than one input layer standing side by side for different modalities and they all send the information to the shared hidden layers through separate connections [22].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": ", input layer, hidden layer and output layer, and it works on autoencoder [14] mode meaning a regression MatNet reproducing the input in output layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "We applied MatNet to the data set used in SR [39], both for training and testing.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity.", "creator": "LaTeX with hyperref package"}}}