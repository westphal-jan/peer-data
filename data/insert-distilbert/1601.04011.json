{"id": "1601.04011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization", "abstract": "we tighten the sample complexity of empirical risk minimization ( erm ) associated populations with a class of generalized linear models that include linear and logistic regression. in particular, we conclude that erm attains the optimal sample complexity for performing linear regression. our analysis relies on a rather new invariant notion of stability, called preconditioned stability, which may alternatively be independently of independent fundamental interest.", "histories": [["v1", "Fri, 15 Jan 2016 17:32:44 GMT  (17kb)", "https://arxiv.org/abs/1601.04011v1", null], ["v2", "Tue, 2 Feb 2016 11:46:18 GMT  (18kb)", "http://arxiv.org/abs/1601.04011v2", null], ["v3", "Tue, 11 Oct 2016 12:29:09 GMT  (16kb)", "http://arxiv.org/abs/1601.04011v3", null], ["v4", "Sun, 16 Apr 2017 12:15:33 GMT  (16kb)", "http://arxiv.org/abs/1601.04011v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon gonen", "shai shalev-shwartz"], "accepted": false, "id": "1601.04011"}, "pdf": {"name": "1601.04011.pdf", "metadata": {"source": "CRF", "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization", "authors": ["Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n04 01\n1v 4\n[ cs\n.L G\n] 1\n6 A"}, {"heading": "1 Introduction", "text": "Central to statistical learning theory is the notion of (algorithmic) stability. Since being introduced by [4], deep connections between the generalization ability and the algorithmic stability of a learning algorithm have been established. It was shown by [22, 18] that stability characterizes learnability. Furthermore, in expectation, some notion of stability is exactly equal to the generalization error of an algorithm (namely, to the gap between true loss and train loss).\nFor generalized linear learning problems, a prominent geometric property which upper bounds the stability rate is the condition number of the loss function. While uniform convergence bounds ([21][Chapter 4]) mostly yield bounds that scale with 1{?n, where n is the size of the sample, well-conditioned problems admit faster (stability) rates that scale linearly with 1{n. The caveat is that typical (large-scale) machine learning problems are ill-conditioned. While we defer the precise definition of the condition number to the next part, let us mention that the condition number is controlled by two related quantities corresponding to both the choice of the loss function and the choice of the coordinate system. In a nutshell, our paper establishes the following result:\n\u2217School of Computer Science, The Hebrew University, Jerusalem, Israel \u2020School of Computer Science, The Hebrew University, Jerusalem, Israel\nThe average stability of ERM is invariant to the choice of the coordinate system.\nWhile this observation admits a one-line proof, it has far-reaching implications. In particular, in this paper we use this observation to establish fast rates for empirical risk minimization.\nThe rest of the paper is organized as follows. In Section 2 we define the setting and proceed to provide basic definitions and results in stability analysis. In Section 3 we state and prove our main result. Section 4 discusses the implications to linear regression as well as improved bounds on the stability of SGD. Related work is discussed in Section 5."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Setup", "text": "We consider the problem of minimizing the risk associated with generalized linear model :\nmin wPW\nLpwq :\u201c Epx,yq\u201eDr\u03c6ypwJxqs . (1)\nHere, both the domain W and the instance space X are assumed to be compact and convex subsets of Rd. We denote by D an arbitrary probability distribution defined over X \u02c6 Y. Each element y in the label set Y induces a twice differentiable1 loss function of the form \u03c6y : twJx : w P W, x P X u \u00d1 R`. We make the following assumptions on the loss function:\n(A1) For each y P Y, \u03c6y is \u03c1-Lipschitz, i.e., |\u03c61ypzq| \u010f \u03c1 for all z. (A2) For each y P Y, \u03c6y is \u03b1-strongly convex, i.e., \u03c62ypzq \u011b \u03b1 for all z.\nOur main example is the following formulation of linear regression ([19]).\nExample 1 (Linear Regression:) Let X be any compact and convex subset of Rd and Y be an interval of the form r\u00b4Y, Y s. The domain W is given by\nW \u201c tw P Rd : p@x P X q |wJx| \u010f Y u .\nFor all y P Y, let \u03c6y be the square loss, \u03c6ypzq \u201c 12pz \u00b4 yq2. Note that for any y P Y and z P twJx : w P W, x P X u,\n|\u03c61ypzq| \u201c 1 2 |2pz \u00b4 yq| \u010f |z| ` |y| \u010f 2Y, }\u03c62ypzq} \u201c 1 .\nHence, the assumptions (A1-2) are satisfied with \u03c1 \u201c 2Y and \u03b1 \u201c 1.\nMore generally, our setting captures all known exp-concave functions ([13]). A twicecontinuously differentiable function f : W \u00d1 R is said to be \u03b1\u0304-exp-concave if \u22072fpwq \u013e \u03b1\u0304\u2207fpwq\u2207fpwqJ for all w P W.\nLemma 1 Consider a risk of the form (1) that satisfies the assumptions (A1-2). Then, for any px, yq P X \u02c6 Y, the function w P W \u00de\u00d1 \u03c6ypwJxq is \u03b1{\u03c12-exp concave.\n1As we do not require smoothness of the loss function, our results can easily be extended to continuous but non-differentiable functions.\nProof Fix a pair px, yq P X \u02c6Y. The gradient and the Hessian of the map \u2113pwq \u201c \u03c6ypwJxq are given by \u2207\u2113pwq \u201c \u03c61pwJxqx, \u22072\u2113pwq \u201c \u03c62pwJxqxxJ . (2) By assumption |\u03c61pwJxq| \u010f \u03c1 and \u03c62pwJxq \u011b \u03b1, hence \u2113 is \u03b1{\u03c12-exp concave.\nA learning algorithm A receives as an input a training sequence (a.k.a. sample) of n i.i.d. pairs, S \u201c ppxi, yiqqni\u201c1 \u201e Dn, and outputs a predictor, ApSq P W. The empirical risk function, L\u0302 : W \u00d1 R, is defined as\nL\u0302Spwq \u201c L\u0302pwq \u201c 1\nn\nn \u00ff i\u201c1 \u03c6yipwJxiq loooomoooon\n:\u201c\u2113\u0302ipwq\n. (3)\nIn this paper we focus on the ERM algorithm, whose output is a minimizer of the empirical risk.2 We denote the output of the ERM by w\u0302pSq, or simply w\u0302 when S is understood from the context. The generalization error and the excess risk of w\u0302 are defined by Lpw\u0302q \u00b4 L\u0302pwq and Lpw\u0302q \u00b4 Lpw\u2039q, respectively. For ERM, it is immediate that any upper bound on the generalization error translates into the same bound on the excess risk.\nRemark 1 While we mostly focus on exact ERM, it should be emphasized that our results are easily extended to any algorithm that approximately minimizes the empirical risk. The formulation of Lemma 2 below highlights this idea."}, {"heading": "2.2 Stability", "text": "In this section we review basic definitions and results on stability. For completeness, we also provide proofs of the stated results.\nLet S \u201c ppxi, yiqqni\u201c1 be a training sequence. For every i P rns, let w\u0302i be a minimizer of the risk w.r.t. Sztpxi, yiqu, namely,\nw\u0302i P argmin wPW\n1\nn\u00b4 1 \u00ff j\u2030i \u2113\u0302jpwq .\nThe average stability of ERM is defined as\n\u2206pS,Wq \u201c 1 n\nn \u00ff i\u201c1 p\u2113\u0302ipw\u0302iq \u00b4 \u2113\u0302ipw\u0302qq . (4)\nWe omit the dependency on W when it is clear from the context. The next lemma shows that the expected generalization error of the ERM is equal to the expected average stability.\nLemma 2 Let A be a possibly randomized algorithm and denote by w\u0302 its output. The generalization error of A satisfies\nES\u201eDn\u00b41rLpw\u0302q \u00b4 L\u0302pw\u0302qs \u201c ES\u201eDnr\u2206pSqs . (5) 2The compactness of W implies that both the true and the empirical risks admit minimizers.\nFurthermore, if A satisfies, for every sample S, ErL\u0302pw\u0302qs \u010f minwPW L\u0302pwq ` \u01eb, where the expectation is with respect to A\u2019s own randomization, then the excess risk of A is bounded by\nES\u201eDn\u00b41rLpw\u0302q \u00b4 Lpw\u2039qs \u010f ES\u201eDnr\u2206pSqs ` \u01eb . (6)\nProof Since w\u0302i does not depend on the i.i.d. pair pxi, yiq,\nES\u201eDnr\u2113\u0302ipw\u0302iqs \u201c ES\u201eDn\u00b41rLpw\u0302pSqqs , i \u201c 1, . . . , n .\nBy linearity of expectation, we obtain\nES\u201eDn\n\u00ab\n1\nn\nn \u00ff i\u201c1 \u2113\u0302ipw\u0302iq\nff\n\u201c ES\u201eDn\u00b41rLpw\u0302pSqqs .\nTherefore,\nEr\u2206pSqs \u201c ES\u201eDn \u00ab 1\nn\nn \u00ff i\u201c1 \u2113\u0302ipw\u0302iq\nff \u00b4ES\u201eDn \u00ab 1\nn\nn \u00ff i\u201c1 \u2113\u0302ipw\u0302q\nff\n\u201c ES\u201eDn\u00b41rLpw\u0302qs\u00b4ES\u201eDnrL\u0302pw\u0302qs .\nThis establishes the first claim. Next, by assumption, for every S, ErL\u0302pw\u0302qs \u010f L\u0302pw\u2039q ` \u01eb. Hence,\nES\u201eDnrL\u0302pw\u0302qs \u010f ES\u201eDnrL\u0302pw\u2039qs ` \u01eb \u201c Lpw\u2039q ` \u01eb .\nCombining this inequality with the first claim, concludes the proof."}, {"heading": "2.2.1 Stability of Well-conditioned Objectives", "text": "Lemma 2 motivates us to derive an upper bound on the average stability. A key quantity that governs \u2206pSq is the condition number of the objective. We next provide exact definitions and discuss this relation.\nFix a training sequence S. We denote the empirical covariance matrix by\nC\u0302 :\u201c C\u0302pSq \u201c 1 n\nn \u00ff i\u201c1 xix J i .\nThe (average) empirical condition number of C\u0302 is defined as\n\u03bapC\u0302q \u201c trpC\u0302q \u03bbminpC\u0302q ,\nwhere trpC\u0302q is the trace of C\u0302 and \u03bbminpC\u0302q is the smallest nonzero eigenvalue of C\u0302. We define the functional condition number as the ratio between the squared Lipschitz parameter and the strong convexity parameter:\n\u03bap\u03c6q \u201c \u03c1 2\n\u03b1 .\nFinally, we define the condition number of the objective as the product between the empirical and the functional condition number:\n\u03ba \u201c \u03bapC\u0302q\u03bap\u03c6q .\nLemma 3 For every training sequence S,\n\u2206pSq \u010f 2\u03ba n \u201c 2\u03bapC\u0302q\u03bap\u03c6q n \u201c 2\u03c1 2 \u03b1n \u03bapC\u0302q . (7)\nTo the best of our knowledge, this result has only been proved in the context of regularized loss minimization (e.g., the bound on the uniform stability in [21][Corollary 13.6]). Inspecting the proofs, one can notice that the role of regularization is merely to ensure the strong convexity of the objective. This simple observation is crucial for our development.\nProof (of Lemma 3) We first assume that C\u0302 is of full rank. Note that for all w, the Hessian of L\u0302 at w is given by\n\u22072L\u0302pwq \u201c 1 n\nn \u00ff i\u201c1 \u03c62pwJxiqxixJi \u013e 1 n n \u00ff i\u201c1 \u03b1xix J i \u201c \u03b1 C\u0302 . (8)\nIn particular, L\u0302 is strongly convex and w\u0302 is uniquely defined. Denote the strong convexity parameter of L\u0302 by \u00b5\u0302. We also denote the Lipschitz parameter of each \u2113\u0302i by \u03c1\u0302i and define \u03c1\u03022 \u201c 1\nn \u0159n i\u201c1 \u03c1\u0302 2\ni . We will shortly derive upper and lower bounds on these parameters, but first let us relate them to the average stability.\nFix some i P rns and let \u2206\u0302i \u201c \u2113\u0302ipw\u0302iq \u00b4 \u2113\u0302ipw\u0302q (we do not assume that w\u0302i is uniquely defined). The \u03c1\u0302i-Lipschitzness of \u2113\u0302i yields the bound\n\u2206\u0302i \u010f \u03c1\u0302i}w\u0302i \u00b4 w\u0302} .\nThe \u00b5\u0302-strong convexity of L\u0302 implies (e.g. using [20][Lemma 2.8]) that\n\u00b5\u0302 2 }w\u0302i \u00b4 w\u0302}2 \u010f L\u0302pw\u0302iq \u00b4 L\u0302pw\u0302q .\nOn the other hand, since w\u0302i minimizes the risk over Sztpxi, yiqu, we have that\nL\u0302pw\u0302iq \u00b4 L\u0302pw\u0302q \u201c \u0159 j\u2030ip\u2113\u0302jpw\u0302iq \u00b4 \u2113\u0302jpw\u0302qq n ` \u2113\u0302ipw\u0302iq \u00b4 \u2113\u0302ipw\u0302q n \u010f 0` \u2206\u0302i n .\nCombining the bounds, we conclude the following bound for every i P rns:\n\u2206\u03022i \u010f \u03c1\u03022i }w\u0302i \u00b4 w\u0302}2 \u010f 2\u03c1\u03022i \u00b5\u0302 pL\u0302pw\u0302iq \u00b4 L\u0302pw\u0302qq \u010f 2\u03c1\u03022i n\u00b5\u0302 \u2206\u0302i .\nDividing by \u2206\u0302i (we may assume w.l.o.g. that \u2206\u0302i \u0105 0), we obtain\n\u2206\u0302i \u010f 2\u03c1\u03022i n\u00b5\u0302 . (9)\nLet us remark that at this point, we can deduce a bound of maxiPrns 2\u03c1\u03022i n\u00b5\u0302\non the uniform stability. This matches the bound in [21][Corollary 13.6]. We next proceed to establish the claimed bound on the average stability.\nBy averaging (9) over i \u201c 1, . . . , n , we obtain\n\u2206\u0302 \u201c 1 n\nn \u00ff i\u201c1 \u2206\u0302i \u010f\n\u02dc\n1\nn\nn \u00ff i\u201c1 \u03c1\u03022i\n\u00b8\n2 n\u00b5\u0302 \u201c 2\u03c1\u0302\n2\nn\u00b5\u0302 . (10)\nIt remains to derive bounds on \u03c1\u0302 and \u00b5\u0302. Note that\n}\u2207\u2113\u0302ipwq}2 \u201c }\u03c61pwJxiqxi}2 \u010f \u03c12}xi}2 \u201c \u03c12trpxixJi q .\nHence, \u03c1\u03022i \u010f \u03c12trpxixJi q. By averaging, we obtain that \u03c1\u03022 \u010f \u03c12trpC\u0302q. Next, using (8) we obtain that \u00b5\u0302 \u011b \u03b1\u03bbdpC\u0302q. By substituting the bounds on \u03c1\u03022 and \u00b5\u0302 in (10), we conclude the desired bound.\nNote that if C\u0302 is not of full rank, we can replace each vector x P Rd with UJx, where the columns of U form an orthonormal basis for spanptx1, . . . , xnuq, without affecting \u2206\u0302, \u2206\u03021, . . . , \u2206\u0302n (this modification is only for the sake of the analysis). As a result, the new covariance matrix is of full rank and its eigenvalues are \u03bb1pC\u0302q, . . . , \u03bbminpC\u0302q. Repeating the above arguments, we conclude the proof.\nLet us specify the bound to linear regression as formulated in Example (1). As \u03b1 \u201c 1 and \u03c1 \u201c 2Y , the functional condition number is 4Y 2. Hence, the average stability is bounded by\n\u2206pSq \u010f 4Y 2\nn \u03ba\u0302pC\u0302q . (11)\nUsing Lemma 2 we deduce the same bound on the excess risk. The weakness of this bound stems from the fact that empirically, the empirical condition number tends to be huge (e.g., see the empirical study in [8]).\nIn the next section we show that the (dependence on the) empirical condition number associated with our arbitrary choice of coordinate system can be replaced by the empirical condition number obtained by an optimal preconditioning."}, {"heading": "3 Preconditioned Stability", "text": "We are now in position to describe our main result. Let P be a (symmetric) positive definite matrix, SP be the training set obtained by replacing every xj with x\u0303j \u201c P\u00b41{2xj , and WP \u201c P 1{2W. We call P\u00b41{2 a preconditioner. Recall the definition of average stability from Equation (4). Our main theorem is:\nTheorem 1 For any training sequence S and positive definite matrix P ,\n\u2206pSP ,WP q \u201c \u2206pS,Wq .\nIn words, the average stability is invariant to the choice of the coordinate system.\nProof The crucial observation is that the empirical risk minimization with respect to SP over the domain WP is equivalent to the ERM w.r.t. S over the domain W in the following sense. For any pair pw, w\u0303 \u201c P 1{2wq P W \u02c6WP and any j P rns, the prediction pw\u0303qJx\u0303j is equal to the prediction wJxj . Therefore, the empirical risks L\u0302Sppw\u0303q and L\u0302Spwq are equal. By associating the corresponding minimizers of the empirical risk (i.e., w\u0302 is associated with P 1{2w\u0302 and for any i P rns, w\u0302i is associated with P 1{2w\u0302i), we conclude our proof. Theorem 1 tells us that we can analyze the stability of SP instead of the stability of S. Crucially, this is true for every P , even one that is chosen based on S. Therefore, the expected suboptimality is upper bounded by the expected value of the quantity, infP\u01050\u2206pSP ,WP q, which we refer to as the preconditioned average stability. Equipped with this observation, we next choose P that leads to a minimal condition number, and consequently obtain a tighter bound on the excess risk.\nNote that for every P \u0105 0, the empirical covariance matrix that corresponds to the preconditioned training sequence, SP , is\n1\nn\nn \u00ff i\u201c1 pP\u00b41{2xiqpP\u00b41{2xiqJ \u201c P\u00b41{2\n\u02dc\n1\nn\nn \u00ff i\u201c1 xix J i\n\u00b8\nP\u00b41{2 \u201c P\u00b41{2C\u0302P\u00b41{2 .\nWhen C\u0302 is of full rank, by choosing P \u201c C\u0302, we obtain that\n\u03bapP\u00b41{2C\u0302P\u00b41{2 looooooomooooooon\nI\nq \u201c trpIq \u03bbminpIq \u201c d .\nIf C\u0302 is not of full rank, we can add arbitrary \u201cnoise\u201d in directions that do not lie in the column space of C\u0302. For example, by choosing P \u201c C\u0302 ` \u03b4pI \u00b4 C\u0302C\u0302:q, (where \u03b4 can be any positive scalar), we obtain that \u03bapP\u00b41{2C\u0302P\u00b41{2q \u201c rankpC\u0302q \u010f d. It is easy to see that in both cases, we obtain the minimal value of \u03bapP\u00b41{2C\u0302P\u00b41{2q over all matrices P \u0105 0. Combining this bound with Lemma 2 and Lemma 3, we arrive at the following conclusion.\nCorollary 1 Consider the optimization problem (1), where for all y P Y, \u03c6y is \u03c1-Lipschitz and \u03b1-strongly convex. The expected excess risk of empirical risk minimization is bounded by\nES\u201eDn\u00b41rLpw\u0302q \u00b4 Lpw\u2039qs \u010f ES\u201eDnr\u2206pSqs \u201c ES\u201eDnr inf P\u01050\n\u2206pSP qs \u010f 2\u03c12 d\n\u03b1n .\nRemark 2 Note that the theorem holds for any algorithm; We only use the fact that the prediction itself is invariant to preconditioning (for any algorithm).\nUsing Lemma 2, we can deduce similar bound holds for approximate ERM."}, {"heading": "4 Some Implications", "text": ""}, {"heading": "4.1 Linear Regression", "text": "We start by specifying our bounds to linear regression (Example (1)).\nCorollary 2 (Linear Regression) Consider linear regression as formulated in Example (1). The expected excess risk of ERM is bounded by\nES\u201eDn\u00b41rLpw\u0302q \u00b4 Lpw\u2039qs \u010f \u2206pSq \u010f 4Y 2d\nn .\nComparing the bounds in (11) and Corollary 2, we see that the dependence on \u03ba\u0302pC\u0302q is replaced by the optimal empirical condition number, \u03ba\u0302pIq \u201c d. As we mentioned above, this gap tends to be huge in practice.\nAs we discuss in Section 5, standard bounds for this setting depend on the geometry of X and W. On the contrary, it follows from the generalized Cauchy-Schwarz inequality that for any choice of a norm } \u00a8 } on Rd, our bound applies to the sets3\nX \u201c B}\u00a8} \u201c tx P Rd : }x} \u010f 1u, W \u201c Y B}\u00a8}\u2039 :\u201c tw P Rd : }w}\u2039 \u010f Y u (12)"}, {"heading": "4.2 The Average Stability of Stochastic Gradient Descent", "text": "One of the most widely used algorithms in machine learning is Stochastic Gradient Descent (SGD). Besides its computational simplicity, its popularity stems also from its generalization abilities ([21][Section 14.5]). Recently, [9] studied the (uniform) stability of SGD in various settings. Following our notation, theorem 3.9 of their paper implies a bound of maxi 2\u03c1\u03022i \u03b3n on the uniform stability, where \u03b3 is the strong convexity of the entire objective, and for any i P rns, \u03c1\u0302i is the Lipschitz parameter of \u2113\u0302i. As the proof of Lemma 3 reveals, \u03b3 can be bounded by \u03b1\u03ba\u0302pC\u0302q and \u03c1\u0302i is at most \u03c1\u03022}xi}2. In particular, the bound depends on the choice of the coordinate system.\nAs implied by [5][Theorem 3.2], SGD can be viewed in our context as an (approximate) ERM. Hence, the average stability of SGD is invariant to the choice of the coordinate system and the stability rate of SGD is bounded as in Corollary 1."}, {"heading": "5 Related Work", "text": ""}, {"heading": "5.1 Slower rates", "text": "One of the most direct techniques for establishing bounds on the excess risk is via analyzing the Rademacher complexity ([3]) of the associated class of predictors. In our setting, these techniques have been employed by [11] to establish bounds of order 1{?n on the generalization error of ERM. We refer to these rates as slower due to the inferior dependence on the sample size n.\n3In fact, under mild additional assumptions on X , any two sets X and W that satisfy our assumptions can be presented in this way. Assume that X is symmetric (i.e., x P X iff \u00b4x P X ) and 0 P intpX q. Then it is known ([7]) that X induces a norm on Rd through the Minkowsky functional\n}x} :\u201c ppxq \u201c inf tt P R : x P tBu .\nIt is immediate that the closed unit ball tx : }x} \u010f 1u is X itself. Therefore, the dual norm is simply the support function of X\n}w}\u2039 \u201c max xPX w J x .\nIt follows that X and W can be described as in (12)."}, {"heading": "5.2 Dependence on Norm", "text": "Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.\nUnlike our fast rates, the exact bounds depend on the geometry of the set X and W. For example: a) If both X and W are the Euclidean unit ball in Rd, then the obtained bound scales with 1{?n. b) If X is the unit \u21138-ball and W is the \u21131-ball, then the obtained bound scales with a logpdq{n."}, {"heading": "5.3 Lower bounds on the excess risk", "text": "Lower bounds for stochastic minimization of exp-concave functions have been studied in [16]. For our setting, theorem 2 in this paper implies a bound of \u2126pd{nq on the excess risk of any algorithm.\nFor the special case of linear regression with\nX \u201c tx P Rd : }x}2 \u010f 1u, W \u201c tw P Rd : }w}2 \u010f Bu, Y \u201c r\u00b4Y, Y s (13)\n[23] proved the lower bound \u2126 \u00b4\nmintY 2, B2`dY 2 n , BY? n u \u00af on the generalization error of ERM.\nThe left-most term is trivially attained by the predictor w \u201c 0. The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]). Last, the right term is attained by ERM, as implied by the aforementioned upper bound of [11].\nIt is left open whether the middle term in the lower bound is attained by ERM. Note that if B \u201c \u03c9p ? dY q, then the middle term in the above lower bound is asymptotically larger than our upper bound. However, since in the setting of [23] (Equation (13)) the magnitude of the predictions is not uniformly upper bounded by Y , no contradiction arises."}, {"heading": "5.4 Stability and Regularization", "text": "Previous work ([4, 22]) studied the rate of uniform stability in various settings. For our setting, their bounds on the expected risk are identical to the bound in Lemma 3. As we explained above, these fast rates are often worse than the so-called slower rates due to the dependence on the empirical condition number. The standard approach for tackling this problem is add a regularization term. By adding the regularization term \u03bb}w}2 to the objective, one effectively increases the eigenvalues of C\u0302 by \u03bb, and consequently, the overall condition number is decreased. However, as explained in [21][Section 13.4], this modification usually does not preserve the fast rates.4"}, {"heading": "5.5 Stability and Exp-concavity", "text": "Informally, exp-concavity can be seen as a local and weaker form of strong convexity. Indeed, the Online Newton Step (ONS) of [10], which has been designed for online minimization\n4Namely, when tuning \u03bb, we need to ensure that any \u01eb{2-approximate minimizer with respect to the regularized objective is also an \u01eb-approximate minimizer with respect to the unregularized objective. As explained in [21][Section 13.3], by optimally controlling this tradeoff, we no longer obtain fast rates (i.e., the stability rate scales with 1{?n rather than 1{n).\nof exp-concave functions, achieves improved (logarithmic) regret bounds that resemble the regret bounds for strongly convex functions ([10]). The online-to-batch analysis of [16] yields a bound on the excess risk that coincides with our bounds up to logarithmic factors. The main shortcoming of the ONS algorithm is that it employs expensive iterations (the runtime per iteration scales at least quadratically with d). Hence, it is natural to ask whether there exist simpler algorithms that achieve fast rates.\nThis question was answered affirmatively by [15]. This work, which is most closely related to our work, considers the minimization of a risk of the form F pwq \u201c Erfpw,Zqs, where for any z, fp\u00a8, zq is \u03b2\u0304-smooth5 and \u03b1\u0304-exp-concave function. They established fast rates for any algorithm that minimizes the regularized risk L\u0302pwq ` 1\nn Rpwq, where Rpwq\nis assumed to be a 1-strongly convex function (e.g., one can set Rpwq \u201c 1 2 }w}2). While exp-concavity is weaker than strong convexity, [15][section 4.2] interprets exp-concavity as strong convexity in the (local) norm induced by the outer products of the gradients and the regularization term. In other words, the problem is well-conditioned with respect to this local norm. Note that their formulation is more general in the sense that they do not assume a GLM structure. However, it should be emphasized that all the known exp-concave functions in machine learning are of the form (1)).\nThe above interpretation of [15] inspired us to make one step forward and directly show that regularization is not required as long as a related (preconditioned) problem is well conditioned. Besides the obvious importance of showing the insignificance of regularization in this context, we believe that the notion of preconditioned stability and its relation to the excess risk make these ideas more transparent and simplify the proofs.\nThe upper bound of [15] on the excess risk scales with 24\u03b2d \u03b1\u0304n \u201c 24\u03b2d\u03c12 \u03b1n\n(recall that the exp-concavity parameter \u03b1\u0304 is equal to \u03b1{\u03c12). Note that our analysis does not assume smoothness of the loss. This resolves the question raised by [15] regarding the necessity of the smoothness assumption. Note that for linear regression, the smoothness is 1, making our bounds identical to the bounds of [15] for this special case.\nAs discussed in [15], it is difficult to translate bounds on the average stability into high-probability bounds (while preserving the fast rate and introducing only logarithmic dependence on 1{\u03b4)."}, {"heading": "5.6 Other Techniques and High-Probability Bounds", "text": "The bound on the expected excess risk in Corollary 1 can be obtained by using two additional techniques. Both of these techniques also yield high probability bounds. We next survey the corresponding results.\nA recent follow-up work by [17] established a bound of O\u0303pd logpnq ` logp1{\u03b4q{nq on the excess risk of ERM, where \u03b4 is the confidence parameter.6. He also showed how to get rid of the logpnq factor by boosting the confidence of ERM. The proof is centered around a Bernstein condition which holds due to the exp-concavity assumption.\nAnother alternative, is to bound the excess risk by the local Rademacher complexity (LRC) of the associated class of predictors (e.g., using Corollary 5.3 in [2]). In our setting, one can derive bounds on the LRC (e.g., using [14]) which coincide with our bounds.\n5That is, the maximal eigenvalue of the Hessian of f at any point w is at most \u03b2. 6The dependence on the exp-concavity parameter as well as the diameter of the loss function is hidden.\nAll of these techniques employ arguably heavy machinery and lack the geometric interpretation, which is nicely captured by our notion of preconditioned stability."}, {"heading": "Acknowledgments", "text": "We thank Iliya Tolstikhin for pointing out the alternative proof of Corollary 1 using local Rademacher complexities."}], "references": [{"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["Katy S Azoury", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Local rademacher complexities", "author": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A course in functional analysis, volume 96", "author": ["John B Conway"], "venue": "Springer Science & Business Media,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Solving ridge regression using sketched preconditioned svrg", "author": ["Alon Gonen", "Francesco Orabona", "Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1602.02350,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["Sham M Kakade", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["Michael Kearns", "Dana Ron"], "venue": "Neural Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "In Computational Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Oracle inequalities in empirical risk minimization and sparse recovery problems: Lecture notes", "author": ["V Koltchinskii"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast rates for exp-concave empirical risk minimization", "author": ["Tomer Koren", "Kfir Levy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Lower and upper bounds on the generalization of stochastic exponentially concave optimization", "author": ["Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "From exp-concavity to variance control: O (1/n) rates and onlineto-batch conversion with high probability", "author": ["Nishant A Mehta"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Beyond logarithmic bounds in online learning", "author": ["Francesco Orabona", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The sample complexity of learning linear predictors with the squared loss", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1406.5143,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Competitive on-line statistics", "author": ["Volodya Vovk"], "venue": "International Statistical Review,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.", "startOffset": 65, "endOffset": 72}, {"referenceID": 3, "context": "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.", "startOffset": 65, "endOffset": 72}, {"referenceID": 8, "context": "b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Since being introduced by [4], deep connections between the generalization ability and the algorithmic stability of a learning algorithm have been established.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "It was shown by [22, 18] that stability characterizes learnability.", "startOffset": 16, "endOffset": 24}, {"referenceID": 17, "context": "It was shown by [22, 18] that stability characterizes learnability.", "startOffset": 16, "endOffset": 24}, {"referenceID": 20, "context": "While uniform convergence bounds ([21][Chapter 4]) mostly yield bounds that scale with 1{?n, where n is the size of the sample, well-conditioned problems admit faster (stability) rates that scale linearly with 1{n.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Our main example is the following formulation of linear regression ([19]).", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "More generally, our setting captures all known exp-concave functions ([13]).", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": ", the bound on the uniform stability in [21][Corollary 13.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "using [20][Lemma 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "This matches the bound in [21][Corollary 13.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": ", see the empirical study in [8]).", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "Besides its computational simplicity, its popularity stems also from its generalization abilities ([21][Section 14.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Recently, [9] studied the (uniform) stability of SGD in various settings.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "As implied by [5][Theorem 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "1 Slower rates One of the most direct techniques for establishing bounds on the excess risk is via analyzing the Rademacher complexity ([3]) of the associated class of predictors.", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "In our setting, these techniques have been employed by [11] to establish bounds of order 1{?n on the generalization error of ERM.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Then it is known ([7]) that X induces a norm on R through the Minkowsky functional }x} :\u201c ppxq \u201c inf tt P R : x P tBu .", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "3 Lower bounds on the excess risk Lower bounds for stochastic minimization of exp-concave functions have been studied in [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "For the special case of linear regression with X \u201c tx P R : }x}2 \u010f 1u, W \u201c tw P R : }w}2 \u010f Bu, Y \u201c r \u0301Y, Y s (13) [23] proved the lower bound \u03a9 \u0301", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 23, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 5, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 10, "context": "Last, the right term is attained by ERM, as implied by the aforementioned upper bound of [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "However, since in the setting of [23] (Equation (13)) the magnitude of the predictions is not uniformly upper bounded by Y , no contradiction arises.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.", "startOffset": 46, "endOffset": 53}, {"referenceID": 21, "context": "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.", "startOffset": 46, "endOffset": 53}, {"referenceID": 20, "context": "However, as explained in [21][Section 13.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "Indeed, the Online Newton Step (ONS) of [10], which has been designed for online minimization Namely, when tuning \u03bb, we need to ensure that any \u01eb{2-approximate minimizer with respect to the regularized objective is also an \u01eb-approximate minimizer with respect to the unregularized objective.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "As explained in [21][Section 13.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "of exp-concave functions, achieves improved (logarithmic) regret bounds that resemble the regret bounds for strongly convex functions ([10]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "The online-to-batch analysis of [16] yields a bound on the excess risk that coincides with our bounds up to logarithmic factors.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "This question was answered affirmatively by [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "While exp-concavity is weaker than strong convexity, [15][section 4.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "The above interpretation of [15] inspired us to make one step forward and directly show that regularization is not required as long as a related (preconditioned) problem is well conditioned.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "The upper bound of [15] on the excess risk scales with 24\u03b2d \u1fb1n \u201c 24\u03b2d\u03c1 \u03b1n (recall that the exp-concavity parameter \u1fb1 is equal to \u03b1{\u03c12).", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "This resolves the question raised by [15] regarding the necessity of the smoothness assumption.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "Note that for linear regression, the smoothness is 1, making our bounds identical to the bounds of [15] for this special case.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "As discussed in [15], it is difficult to translate bounds on the average stability into high-probability bounds (while preserving the fast rate and introducing only logarithmic dependence on 1{\u03b4).", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "A recent follow-up work by [17] established a bound of \u00d5pd logpnq ` logp1{\u03b4q{nq on the excess risk of ERM, where \u03b4 is the confidence parameter.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "3 in [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": ", using [14]) which coincide with our bounds.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.", "creator": "LaTeX with hyperref package"}}}