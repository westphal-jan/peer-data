{"id": "1601.04149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2016", "title": "$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images", "abstract": "in this paper, we design a deep dual - domain ( $ \\ mathbf { d ^ 3 } $ ) based fast restoration model to remove artifacts of jpeg compressed images. it leverages the large learning capacity of deep networks, as well as the problem - specific expertise that was hardly incorporated locally in the past design of deep architectures. for the latter, we take into closer consideration both the prior knowledge of the jpeg compression scheme, and the successful practice of the sparsity - based dual - query domain approach. we further design the one - step sparse inference ( 1 - si ) memory module, characterised as an efficient and light - weighted direct feed - forward approximation of sparse coding. extensive experiments verify the superiority of the long proposed $ d ^ 3 $ model over several state - powers of - \u2018 the - art methods. specifically, our best model is capable of outperforming the latest deep model for around 1 * db in psnr, and is 30 times faster.", "histories": [["v1", "Sat, 16 Jan 2016 10:38:43 GMT  (2035kb,D)", "http://arxiv.org/abs/1601.04149v1", null], ["v2", "Fri, 1 Apr 2016 03:19:10 GMT  (1527kb,D)", "http://arxiv.org/abs/1601.04149v2", null], ["v3", "Sat, 9 Apr 2016 19:25:08 GMT  (1527kb,D)", "http://arxiv.org/abs/1601.04149v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhangyang wang", "ding liu", "shiyu chang", "qing ling", "yingzhen yang", "thomas s huang"], "accepted": false, "id": "1601.04149"}, "pdf": {"name": "1601.04149.pdf", "metadata": {"source": "CRF", "title": "D: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images", "authors": ["Zhangyang Wang", "Ding Liu", "Shiyu Chang", "Qing Ling", "Thomas S. Huang"], "emails": ["t-huang1}@illinois.edu", "qingling@mail.ustc.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In visual communication and computing systems, the most common cause of image degradation is arguably compression. Lossy compression, such as JPEG [26] and HEVC-MSP [4], is widely adopted in image and video codecs for saving both bandwidth and in-device storage. It exploits inexact approximations for representing the encoded content compactly. Inevitably, it will introduce undesired complex artifacts, such as blockiness, ringing effects, and blurs. They are usually caused by the discontinuities arising from batch-wise processing, the loss of highfrequency components by coarse quantization, and so on. These artifacts not only degrade perceptual visual quality, but also adversely affect various low-level image processing routines that take compressed images as input [12].\nAs practical image compression methods are not information theoretically optimal [25], the resulting compression code streams still possess residual redundancies, which makes the restoration of the original signals possible. Different from general image restoration problems, compression artifact restoration has problem-specific properties that\ncan be utilized as powerful priors. For example, JPEG compression first divides an image into 8 \u00d7 8 pixel blocks, followed by discrete cosine transformation (DCT) on every block. Quantization is applied on the DCT coefficients of every block, with pre-known quantization levels [26]. Moreover, the compression noises are more difficult to model than other common noise types. In contrast to the tradition of assuming noise to be white and signal independent [2], the non-linearity of quantization operations makes quantization noises non-stationary and signal-dependent.\nVarious approaches have been proposed to suppress compression artifacts. Early works [6, 23] utilized filteringbased methods to remove simple artifacts. Data-driven methods were then considered to avoid inaccurate empirical modeling of compression degradations. Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions. In [25], Liu et.al. proposed a sparse coding process carried out jointly in the DCT and pixel domains, to simultaneously exploit residual redundancies of JPEG code streams and sparsity properties of latent images. More recently, Dong et. al. [12] first introduced deep learning techniques [22] into this problem, by specifically adapting their SR-CNN model in [13]. However, it does not incorporate much problem-specific prior knowledge.\nThe time constraint is often stringent in image or video codec post-processing scenarios. Low-complexity or even real-time attenuation of compression artifacts is highly desirable [29]. The inference process of traditional approaches, for example, sparse coding, usually involves iterative optimization algorithms, whose inherently sequential structure as well as the data-dependent complexity and latency often constitute a major bottleneck in the computational efficiency [15]. Deep networks benefit from the feed-forward structure and enjoy much faster inference. However, to maintain their competitive performances, deep networks show demands for increased width (numbers of filters) and depth (number of layers), as well as smaller strides, all leading to growing computational costs [17].\n1\nar X\niv :1\n60 1.\n04 14\n9v 1\n[ cs\n.C V\n] 1\n6 Ja\nn 20\n16\nIn the paper, we focus on removing artifacts in JPEG compressed images. Our major innovation is to explicitly combine both the prior knowledge in the JPEG compression scheme and the successful practice of dual-domain sparse coding [25], for designing a task-specific deep architecture. Furthermore, we introduce a One-Step Sparse Inference (1-SI) module, that acts as a highly efficient and light-weighted approximation of the sparse coding inference [11]. 1-SI also reveals important inner connections between sparse coding and deep learning. The proposed model, named Deep Dual-Domain (D3) based fast restoration, proves to be more effective and interpretable than general deep models. It gains remarkable margins over several state-of-the-art methods, in terms of both restoration performance and time efficiency."}, {"heading": "2. Related Work", "text": "Our work is inspired by the prior wisdom in [25]. Most previous works restored compressed images in either the pixel domain [2] or the DCT domain [26] solely. However, an isolated quantization error of one single DCT coefficient is propagated to all pixels of the same block. An aggressively quantized DCT coefficient can further produce structured errors in the pixel-domain that correlate to the latent signal. On the other hand, the compression process sets most high frequency coefficients to zero, making it impossible to recover details from only the DCT domain. In view of their complementary characteristics, the dualdomain model was proposed in [25]. While the spatial redundancies in the pixel domain were exploited by a learned dictionary [2], the residual redundancies in the DCT domain were also utilized to directly restore DCT coefficients. In this way, quantization noises were suppressed without propagating errors. The final objective (see Section 3.1) is a combination of DCT- and pixel-domain sparse representations, which could cross validate each other.\nTo date, deep learning [22] has shown impressive results on both high-level and low-level vision problems. The SR-\nCNN proposed by Dong et al. [13] showed the great potential of end-to-end trained networks in image super resolution (SR). Their recent work [12] proposed a four-layer convolutional network that was tuned based on SR-CNN, named Artifacts Reduction Convolutional Neural Networks (AR-CNN), which was effective in dealing with various compression artifacts.\nIn [15], the authors leveraged fast trainable regressors and constructed feed-forward network approximations of the learned sparse models. By turning sparse coding into deep networks, one may expect faster inference, larger learning capacity, and better scalability. Similar views were adopted in [30] to develop a fixed-complexity algorithm for solving structured sparse and robust low rank models. The paper [18] summarized the methodology of \u201cdeep unfolding\u201d. Very recently, [34] proposed deeply improved sparse coding for SR, which can be incarnated as an end-to-end neural network. Our task-specific architecture shares similar spirits with these works.\n3. Deep Dual-Domain (D3) based Restoration"}, {"heading": "3.1. Sparsity-based Dual-Domain Formulation", "text": "We first review the sparsity-based dual-domain restoration model established in [25]. Considering a training set of JPEG compressed images, pixel-domain blocks {xi} \u2208 Rm (vectorized from a \u221a m \u00d7 \u221a m patch; m = 64 for JPEG) are drawn for training, along with their (quantized) DCT coefficient blocks {yi}\u2208 Rm. For each testing input (JPEG-coded) xt \u2208 Rm, two dictionaries \u03a6 \u2208 Rm\u00d7p\u03a6 and \u03a8 \u2208 Rm\u00d7p\u03a8 (p\u03a6 and p\u03a8 denote the dictionary sizes) are constructed from training data {yi} and {xi}, in the DCT and pixel domains, respectively, via locally adaptive feature selection and projection. The following optimization model is then solved during the testing stage:\nmin{\u03b1,\u03b2} ||yt \u2212\u03a6\u03b1||22 + \u03bb1||\u03b1||1 +\u03bb2||T\u22121\u03a6\u03b1\u2212\u03a8\u03b2||22 + \u03bb3||\u03b2||1, s.t. qL \u03a6\u03b1 qU .\n(1)\nwhere yt \u2208 Rm is the DCT coefficient block for xt. \u03b1 \u2208 Rp\u03a6 and \u03b2 \u2208 Rp\u03a8 are sparse codes in the DCT and pixel domains, respectively. T\u22121 denotes the inverse discrete cosine transform (IDCT) operator. \u03bb1, \u03bb2 and \u03bb3 are positive scalars. One noteworthy point is the inequality constraint, where qL and qU represents the (pre-known) quantization intervals according to the JPEG quantization table [26]. The constraint incorporates the important side information and further confines the solution space. Finally, \u03a8\u03b2 provides an estimate of the original uncompressed pixel block x\u0302t.\nSuch a sparsity-based dual-domain model (1) exploits residual redundancies (e,g, inter-DCT-block correlations) in the DCT domain without spreading errors into the pixel domain, and at the same time recovers high-frequency information driven by a large training set. However, note that the inference process of (1) relies on iterative algorithms, and is computational expensive. Also in (1), the three parameters \u03bb1, \u03bb2 and \u03bb3 have to be manually tuned. The authors of [25] simply set them all equal, which may hamper the performance. In addition, the dictionaries \u03a6 and \u03a8 have to be individually learned for each patch, which allows for extra flexibility but also brings in heavy computation load.\n3.2. D3: A Feed-Forward Network Formulation\nIn training, we have the pixel-domain blocks {xi} after JPEG compression, as well as their corresponding original blocks {x\u0302i}. During testing, for an input compressed block xt, our goal is to estimate the original x\u0302t, using the redundancies in both DCT and pixel domains, as well as JPEG prior knowledge.\nAs illustrated in Fig. 1, the input xt is first transformed into its DCT coefficient block yt, by feeding through the constant 2-D DCT matrix layer T . The subsequent two layers aim to enforce DCT domain sparsity, where we refer to the concepts of analysis and synthesis dictionaries in sparse coding [16]. The Sparse Coding (SC) Analysis Module 1 is implemented to solve the following type of sparse inference problem in the DCT domain (\u03bb is a positive coefficient):\nmin\u03b1 1 2 ||yt \u2212\u03a6\u03b1|| 2 2 + \u03bb||\u03b1||1. (2)\nThe Sparse Coding (SC) Synthesis Module 1 outputs the DCT-domain sparsity-based reconstruction in (1), i.e., \u03a6\u03b1.\nThe intermediate output \u03a6\u03b1 is further constrained by an auxiliary loss, which encodes the inequality constraint in (1): qL \u03a6\u03b1 qU . We design the following signaldependent, box-constrained [21] loss:\nLB(\u03a6\u03b1, x) = ||[\u03a6\u03b1\u2212 qU (x)]+||22 + ||[qL(x)\u2212\u03a6\u03b1]+||22. (3) Note it takes not only \u03a6\u03b1, but also x as inputs, since the actual JPEG quantization interval [qL, qU ] depends on x. The operator [ ]+ keeps the nonnegative elements unchanged while setting others to zero. Eqn. (3) will thus only penalize the coefficients falling out of the quantization interval.\nAfter passing through the constant IDCT matrix layer T\u22121, the DCT-domain reconstruction \u03a6\u03b1 is transformed back to the pixel domain for one more sparse representation. The SC Analysis Module 2 solves (\u03b3 is a positive coefficient):\nmin\u03b2 1 2 ||T \u22121\u03a6\u03b1\u2212\u03a8\u03b2||22 + \u03b3||\u03b2||1, (4)\nwhile the SC Synthesis Module 2 produces the final pixeldomain reconstruction \u03a8\u03b2. Finally, the L2 loss between \u03a8\u03b2 and x\u0302i is enforced.\nNote that in the above, we try to correspond the intermediate outputs of D3 with the variables in (1), in order to help understand the close analytical relationship between the proposed deep architecture with the sparse coding-based model. That does not necessarily imply any exact numerical equivalence, since D3 allows for end-to-end learning of all parameters (including \u03bb in (2) and \u03b3 in (4)). However, we will see in experiments that such enforcement of the specific problem structure improves the network performance and efficiency remarkably. In addition, the above relationships remind us that the deep model could be well initialized from the sparse coding components."}, {"heading": "3.3. One-Step Sparse Inference Module", "text": "The implementation of SC Analysis and Synthesis Modules appears to be the core of D3. While the synthesis process is naturally feed-forward by multiplying the dictionary, it is less straightforward to transform the sparse analysis (or inference) process into a feed-forward network.\nWe take (2) as an example, while the same solution applies to (4). Such a sparse inference problem could be solved by the iterative shrinkage and thresholding algorithm (ISTA) [5], each iteration of which updates as follows:\n\u03b1k+1 = s\u03bb(\u03b1 k + \u03a6T (yt \u2212\u03a6\u03b1k)), (5)\nwhere \u03b1k denotes the intermediate result of the k-th iteration, and where s\u03bb is an element-wise shrinkage function (u is a vector and ui is its i-th element, i = 1, 2, ..., p):\n[s\u03bb(u)]i = sign(ui)[|ui| \u2212 \u03bbi]+. (6)\nThe learned ISTA (LISTA) [15] parameterized encoder further proposed a natural network implementation of ISTA. The authors time-unfolded and truncated (5) into a fixed number of stages (more than 2), and then jointly tuned all parameters with training data, for a good feed-forward approximation of sparse inference. The similar unfolding methodology has been lately exploited in [18], [30], [31].\nIn our work, we launch a more aggressive approximation, by only keeping one iteration of (5), leading to a OneStep Sparse Inference (1-SI) Module. Our major motivation lies in the same observation as in [12] that overly deep networks could adversely affect the performance in low-level\nvision tasks. Note that we have two SC Analysis modules where the original LISTA applies, and two more SC Synthesis modules (each with one learnable layer). Even only two iterations are kept as in [15], we end up with a six-layer network, that suffers from both difficulties in training [12] and fragility in generalization [32] for this task.\nA 1-SI module takes the following simplest form:\n\u03b1 = s\u03bb(\u03a6yt), (7)\nwhich could be viewed as first passing through a fullyconnected layer (\u03a6), followed by neurons that take the form of s\u03bb. We further rewrite (6) as [34] did1:\n[s\u03bb(u)]i = \u03bbi \u00b7 sign(ui)(|ui|/\u03bbi \u2212 1)+ = \u03bbis1(ui/\u03bbi) (8) Eqn. (8) indicates that the original neuron with trainable thresholds can be decomposed into two linear scaling layers plus a unit-threshold neuron. The weights of the two scaling layers are diagonal matrices defined by \u03b8 and its elementwise reciprocal, respectively. The unit-threshold neuron s1 could in essence be viewed as a double-sided and translated variant of ReLU [22].\nA related form to (7) was obtained in [11] on a different case of non-negative sparse coding. The authors studied its connections with the soft-threshold feature for classification, but did not correlate it with network architectures."}, {"heading": "3.4. Model Overview", "text": "By plugging in the 1-SI module (7), we are ready to obtain the SC Analysis and Synthesis Modules, as in Fig. 2. By comparing Fig. 2 with Eqn. (2) (or (4)), it is easy to notice the analytical relationships between DA and \u03a6T (or \u03a8T ), DS and \u03a6 (or \u03a8), as well as \u03b8 and \u03bb (or \u03b3). In fact, those network hyperparamters could be well initialized from the sparse coding parameters, which could be obtained easily. The entire D3 model, consisting of four learnable fully-connected weight layers (except for the diagonal layers), are then trained from end to end 2.\n1In (8), we slightly abuse notations, and set \u03bb to be a vector of the same dimension as u, in order for extra element-wise flexibility.\n2From the analytical perspective, DS is the transpose of DA, but we untie them during training for larger learning capability.\nIn Fig. 2, we intentionally do not combine \u03b8 into DA layer (also 1/\u03b8 into DS layer ), for the reason that we still wish to keep \u03b8 and 1/\u03b8 layers tied as element-wise reciprocal. That proves to have positive implications in our experiments. If we absorb the two diagonal layers into DA and DS , Fig. 2 is reduced to two fully connected weight matrices, concatenated by one layer of hidden neurons (8). However, keeping the \u201cdecomposed\u201d model architecture facilitates the incorporation of problem-specific structures."}, {"heading": "3.5. Complexity Analysis", "text": "From the clear correspondences between the sparsitybased formulation and the D3 model, we immediately derive the dimensions of weight layers, as in Table 1."}, {"heading": "3.5.1 Time Complexity", "text": "During training, deep learning with the aid of gradient descent scales linearly in time and space with the number of train samples. We are primarily concerned with the time complexity during testing (inference), which is more relevant to practical usages. Since all learnable layers in theD3 model are fully-connected, the inference process of D3 is nothing more than a series of matrix multiplications. The multiplication times are counted as: p\u03a6m (DA in Stage I) + 2p\u03a6 (two diagonal layers) + p\u03a6m (DS in Stage I) + p\u03a8m (DA in Stage II) + 2p\u03a8 (two diagonal layers) + p\u03a8m (DS in Stage II). The 2D DCT and IDCT each takes 12m log(m) multiplications [26] . Therefore, the total inference time complexity of D3 is:\nCD3 = 2(p\u03a6 + p\u03a8)(m+ 1) +m log(m) \u2248 2m(p\u03a6 + p\u03a8). (9)\nThe complexity could also be expressed as O(p\u03a6 + p\u03a8). It is obvious that the sparse coding inference [25] has dramatically higher time complexity. We are also interested in the inference time complexity of other competitive deep models, especially AR-CNN [12]. For their fully convolutional architecture, the total complexity [17] is:\nCconv = \u2211d l=1 nl\u22121 \u00b7 s2l \u00b7 nl \u00b7m2l , (10)\nwhere l is the layer index, d is the total depth, nl is the number of filters in the l-th layer, sl is the spatial size of the filter, and ml is the spatial size of the output feature map.\nThe theoretical time complexities in (9) and (10) do not represent the actual running time, as they depend on different configurations and can be sensitive to implementations\nand hardware. Yet, our actual running time scales nicely with those theoretical results."}, {"heading": "3.5.2 Parameter Complexity", "text": "The total number of free parameters in D3 is:\nND3 = 2p\u03a6m+ p\u03a6 + 2p\u03a8m+ p\u03a8 = 2(p\u03a6 + p\u03a8)(m+ 1). (11)\nAs a comparison, the AR-CNN model [12] contains:\nNconv = \u2211d l=1 nl\u22121 \u00b7 nl \u00b7 s2l . (12)"}, {"heading": "3.6. Remark: A Hidden Gem", "text": "A hidden gem in the design process of D3 is the discovery of inner connections between popular deep network modules and well-studied sparse coding algorithms. The current model concerns solving conventional sparse coding (2). By enforcing an extra nonnegative constraint on \u03b1, (2) becomes the non-negative sparse coding problem that is found to be meaningful in many visual recognition tasks [2]. Following the same routine in Section 3.3, the only change of 1-SI module occurs in the thresholding operator :\n\u03b1 = r\u03bb(\u03a6yt), where [r\u03bb(u)]i = [ui \u2212 \u03bbi]+. (13)\nThe new r\u03bb is exactly a ReLU [22] neuron with a translation bias \u03bb. Similarly, if we substitute the formulation (2) with the convolutional sparse coding model [7], DA in Fig. 2 will be replaced by a convolutional layer.\nThe above analogies provide us with some new clues on interpreting and designing deep architectures. Deep networks constituted by popular modules (fully-connected and convolutional layers, ReLU, etc) could be viewed as a hierarchy of roughly-solved sparse coding models, and further enables end-to-end training. Although a more in-depth discussion is beyond the focus of this paper, we hope those observations can evoke more interests from the community."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Implementation and Setting", "text": "We use the disjoint training set (200 images) and test set (200 images) of BSDS500 database [3], as our training set; its validation set (100 images) is used for validation, which follows [12]. For training the D3 model, we first divide each original image into overlapped 8 \u00d7 8 patches, and subtract the pixel values by 128 as in the JPEG mean shifting process. We then perform JPEG encoding on them by MATLAB JPEG encoder with a specific quality factor Q, to generate the corresponding compressed samples. Whereas JPEG works on non-overlapping patches, we emphasize that the training patches are overlapped and extracted from arbitrary positions. For a testing image, we sample 8 \u00d7 8 blocks with a stride of 4, and apply the D3 model in a patch-wise manner. For a patch that misaligns with the original JPEG block boundaries, we find its most similar coding block from its 16 \u00d7 16 local neighborhood, whose quantization intervals are then applied to the mis-\naligned patch. We find this practice effective and important for removing blocking artifacts and ensuring the neighborhood consistency. The final result is obtained via aggregating all patches, with the overlapping regions averaged.\nThe proposed networks are implemented using the cudaconvnet package [22]. We apply a constant learning rate of 0.01, a batch size of 128, with no momentum. Experiments run on a workstation with 12 Intel Xeon 2.67GHz CPUs and 1 GTX680 GPU. The two losses, LB and L2, are equally weighted. For the parameters in Table 1, m is fixed as 64. We try different values of p\u03a6 and p\u03a8 in experiments.\nBased on the solved Eqn. (1), one could initialize DA, DS , and \u03b8 from \u03a6, \u03a6T and \u03bb in the DCT domain block of Fig. 1, and from \u03a8, \u03a8T and \u03b3 in the pixel domain block, respectively. In practice, we find such an initializa-\ntion strategy has a marginal yet positive impact on the performances. What is more, it usually leads to faster training convergences.\nWe test the quality factor Q = 5, 10, and 20. For each Q, we train a dedicated model. We further find the easy-hard transfer suggested by [12] useful. As images of low Q values (heavily compressed) contain more complex artifacts, it is helpful to use the features learned from images of high Q values (lightly compressed) as a starting point. In practice, we first train the D3 model on JPEG compressed images with Q = 20 (the highest quality). We then initialize the Q = 10 model with the Q = 20 model, and similarly, initialize Q = 5 model from the Q = 10 one."}, {"heading": "4.2. Restoration Performance Comparison", "text": "We include the following two relevant, state-of-the-art methods for comparison:\n\u2022 Sparsity-based Dual-Domain Method (S-D2) [25] could be viewed as the \u201cshallow\u201d counterpart of D3. It has outperformed most traditional methods [25], such as BM3D [10] and DicTV [8], with which we thus do not compare again. The algorithm has a few parameters to be manually tuned. Especially, their dictionary atoms are adaptively selected by a nearest-neighbour type algorithm; the number of selected atoms varies for every testing patch. Therefore, the parameter complexity of S-D2 cannot be exactly computed.\n\u2022 AR-CNN has been the latest deep model resolving the JPEG compression artifact removal problem. In [12], the authors show its advantage over SA-DCT [14], RTF [19], and SR-CNN [13]. We adopt the default network configuration in [12]: s1 = 9, s2 = 7, s3 = 1, s4 = 5; n1 = 64, n2 = 32, n3 = 16, n4 = 1. The authors adopted the easy-hard transfer in training.\nFor D3, we test p\u03a6 = p\u03a8 = 128 and 256 3. The resulting D3 models are denoted as D3-128 and D3-256, respectively. In addition, to verify the superiority of our task-specific design, we construct a fully-connected Deep Baseline Model\n3from the common experiences of choosing dictionary sizes [2]\n(D-Base), of the same complexity with D3-256, named DBase-256. It consists of four weight matrices of the same dimensions as D3-256\u2019s four trainable layers4. D-Base-256 utilizes ReLU [22] neurons and the dropout technique.\nWe use the 29 images in the LIVE1 dataset [28] (converted to the gray scale) to evaluate both the quantitative and qualitative performances. Three quality assessment criteria: PSNR, structural similarity (SSIM) [33], and PSNR-B [35], are evaluated, the last of which is designed specifically to assess blocky images. The averaged results on the LIVE1 dataset are list in Table 2.\nCompared to S-D2, both D3-128 and D3-256 gain remarkable advantages, thanks to the end-to-end training as deep architectures. As p\u03a6 and p\u03a8 grow from 128 to 256, one observes clear improvements in PSNR/SSIM/PSNR-B. D3-256 has outperformed the state-of-the-art ARCNN, for around 1 dB in PSNR. Moreover, D3-256 also demonstrates a notable performance margin over D-Base-256, although they possess the same number of parameters. We thus verify our important argument that D3 benefits from its taskspecific architecture inspired by the sparse coding process (1), rather than just the large learning capacity of generic deep models. The parameter numbers of different models are compared in the last row of Table 2. It is impressive to see that D3-256 also takes less parameters than AR-CNN.\n4The diagonal layers contain a very small portion of parameters and are ignored here.\nWe display three groups of visual results, on Bike, Monarch and Parrots images, when Q = 5, in Figs. 3, 4 and 5, respectively. AR-CNN tends to generate oversmoothness, such as in the edge regions of butterfly wings and parrot head. S-D2 is capable of restoring sharper edges and textures. The D3 models further reduce the unnatural artifacts occurring in S-D2 results. Especially, while D3128 results still suffer from a small amount of visible ringing artifacts, D3-256 not only shows superior in preserving details, but also suppresses artifacts well.\nIn addition, we re-train the D3-256 model without the intermediate loss LB (3), in the Q = 5 case. We find that the resulting model is less capable in generating sharp details, accompanied with degraded PSNR/SSIM/PSNR-B. That verifies the significance of the LB loss, without which the energy tends to focus more on the low-frequency bands."}, {"heading": "4.3. Running Time Comparison", "text": "The image or video codecs desire highly efficient compression artifact removal algorithms as the post-processing tool. Traditional TV and digital cinema business uses frame rate standards such as 24p (i.e., 24 frames per second), 25p, and 30p. Emerging standards require much higher rates. For example, high-end High-Definition (HD) TV systems adopt 50p or 60p; the Ultra-HD (UHD) TV standard advocates 100p/119.88p/120p; the HEVC format could reach the maximum frame rate of 300p [1]. To this end, higher time efficiency is as desirable as improved performances.\nWe compare the averaged testing times of AR-CNN and\nthe proposed D3 models in Table 3, on 29 images in the LIVE29 dataset, all using the same machine and software environment 5. The time costs of AR-CNN and D3-256 running on each individual image are also profiled in Fig. 6. Our best model, D3-256, takes approximately 12 ms per image; that is more than 30 times faster than AR-CNN. The speed difference is NOT mainly caused by the different implementations. Both being completely feed-forward, AR-CNN relies on the time-consuming convolution operations while ours takes only a few matrix multiplications. That is in accordance with the theoretical time complexities computed from (9) and (10), too. As a result, D3-256 is able to process 80p image sequences (or even higher). To our best knowledge, D3 is the fastest among all state-ofthe-art algorithms, and proves to be a practical choice for HDTV industrial usage."}, {"heading": "5. Conclusion", "text": "We introduce the Deep Dual-Domain (D3) based fast restoration model to remove artifacts in JPEG compressed images. The successful combination of both JPEG prior knowledge and sparse coding expertise helps our proposed deep architecture to be highly effective and efficient. In the future, we aim to further reduce the complexity of D3, as well as extend our model to more related applications, such as denoising and super-resolution.\nReferences [1] https://en.wikipedia.org/wiki/Frame_\nrate/. [2] M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algo-\nrithm for designing overcomplete dictionaries for sparse representation. TSP, 54(11):4311\u20134322, 2006.\n[3] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. TPAMI, 33(5):898\u2013916, 2011.\n[4] E. A. Ayele and S. Dhok. Review of proposed high efficiency video coding (hevc) standard. International Journal of Computer Applications, 59(15):1\u20139, 2012.\n[5] T. Blumensath and M. E. Davies. Iterative thresholding for sparse approximations. Journal of Fourier Analysis and Applications, 14(5-6):629\u2013654, 2008.\n[6] K. Bredies and M. Holler. A total variation-based jpeg decompression model. SIAM Journal on Imaging Sciences, 5(1):366\u2013393, 2012.\n[7] H. Bristow, A. Eriksson, and S. Lucey. Fast convolutional sparse coding. In CVPR, pages 391\u2013398. IEEE, 2013.\n[8] H. Chang, M. K. Ng, and T. Zeng. Reducing artifacts in jpeg decompression via a learned dictionary. TSP, 62(3):718\u2013 728, 2014.\n[9] I. Choi, S. Kim, M. S. Brown, and Y.-W. Tai. A learningbased approach to reduce jpeg artifacts in image matting. In ICCV, pages 2880\u20132887. IEEE, 2013.\n[10] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. TIP, 16(8):2080\u20132095, 2007.\n[11] M. Denil and N. de Freitas. Recklessly approximate sparse coding. arXiv preprint arXiv:1208.0959, 2012.\n[12] C. Dong, Y. Deng, C. C. Loy, and X. Tang. Compression artifacts reduction by a deep convolutional network. ICCV, 2015.\n[13] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In ECCV, pages 184\u2013199. Springer, 2014.\n[14] A. Foi, V. Katkovnik, and K. Egiazarian. Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images. TIP, 16(5):1395\u20131411, 2007.\n[15] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In ICML, pages 399\u2013406, 2010.\n[16] S. Gu, L. Zhang, W. Zuo, and X. Feng. Projective dictionary pair learning for pattern classification. In NIPS, pages 793\u2013 801, 2014.\n[17] K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, pages 5353\u20135360, 2015.\n[18] J. R. Hershey, J. L. Roux, and F. Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.\n[19] J. Jancsary, S. Nowozin, and C. Rother. Loss-specific training of non-parametric image restoration models: A new state of the art. In ECCV, pages 112\u2013125. Springer, 2012.\n[20] C. Jung, L. Jiao, H. Qi, and T. Sun. Image deblocking via sparse representation. Signal Processing: Image Communication, 27(6):663\u2013677, 2012.\n[21] D. Kim, S. Sra, and I. S. Dhillon. Tackling box-constrained optimization via a new projected quasi-newton approach. SIAM Journal on Scientific Computing, 32(6):3548\u20133563, 2010.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097\u20131105, 2012.\n[23] K. Lee, D. S. Kim, and T. Kim. Regression-based prediction for blocking artifact reduction in jpeg-compressed images. TIP, 14(1):36\u201348, 2005.\n[24] X. Liu, G. Cheung, X. Wu, and D. Zhao. Inter-block soft decoding of jpeg images with sparsity and graph-signal smoothness priors. In ICIP. IEEE, 2015.\n[25] X. Liu, X. Wu, J. Zhou, and D. Zhao. Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain. In CVPR, 2015.\n[26] W. B. Pennebaker and J. L. Mitchell. JPEG: Still image data compression standard. Springer Science & Business Media, 1993.\n[27] R. Rothe, R. Timofte, and L. Van Gool. Efficient regression priors for reducing image compression artifacts. In IEEE ICIP, 2015.\n[28] H. R. Sheikh, Z. Wang, L. Cormack, and A. C. Bovik. Live image quality assessment database release 2, 2005.\n[29] M.-Y. Shen and C.-C. Jay Kuo. Real-time compression artifact reduction via robust nonlinear filtering. In ICIP, volume 2, pages 565\u2013569. IEEE, 1999.\n[30] P. Sprechmann, A. Bronstein, and G. Sapiro. Learning efficient sparse and low rank models. TPAMI, 2015.\n[31] P. Sprechmann, R. Litman, T. B. Yakar, A. M. Bronstein, and G. Sapiro. Supervised sparse analysis and synthesis operators. In NIPS, pages 908\u2013916, 2013.\n[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1):1929\u20131958, 2014.\n[33] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600\u2013612, 2004.\n[34] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deeply improved sparse coding for image super-resolution. ICCV, 2015.\n[35] C. Yim and A. C. Bovik. Quality assessment of deblocked images. TIP, 20(1):88\u201398, 2011."}], "references": [{"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "TSP, 54(11):4311\u20134322", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "TPAMI, 33(5):898\u2013916", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Review of proposed high efficiency video coding (hevc) standard", "author": ["E.A. Ayele", "S. Dhok"], "venue": "International Journal of Computer Applications, 59(15):1\u20139", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Iterative thresholding for sparse approximations", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Journal of Fourier Analysis and Applications, 14(5-6):629\u2013654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "A total variation-based jpeg decompression model", "author": ["K. Bredies", "M. Holler"], "venue": "SIAM Journal on Imaging Sciences, 5(1):366\u2013393", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast convolutional sparse coding", "author": ["H. Bristow", "A. Eriksson", "S. Lucey"], "venue": "CVPR, pages 391\u2013398. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing artifacts in jpeg decompression via a learned dictionary", "author": ["H. Chang", "M.K. Ng", "T. Zeng"], "venue": "TSP, 62(3):718\u2013 728", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A learningbased approach to reduce jpeg artifacts in image matting", "author": ["I. Choi", "S. Kim", "M.S. Brown", "Y.-W. Tai"], "venue": "ICCV, pages 2880\u20132887. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "TIP, 16(8):2080\u20132095", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Recklessly approximate sparse coding", "author": ["M. Denil", "N. de Freitas"], "venue": "arXiv preprint arXiv:1208.0959,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Compression artifacts reduction by a deep convolutional network", "author": ["C. Dong", "Y. Deng", "C.C. Loy", "X. Tang"], "venue": "ICCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "ECCV, pages 184\u2013199. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images", "author": ["A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "TIP, 16(5):1395\u20131411", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "ICML, pages 399\u2013406", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Projective dictionary pair learning for pattern classification", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "NIPS, pages 793\u2013 801", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "CVPR, pages 5353\u20135360", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J.R. Hershey", "J.L. Roux", "F. Weninger"], "venue": "arXiv preprint arXiv:1409.2574", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Loss-specific training of non-parametric image restoration models: A new state of the art", "author": ["J. Jancsary", "S. Nowozin", "C. Rother"], "venue": "ECCV, pages 112\u2013125. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Image deblocking via sparse representation", "author": ["C. Jung", "L. Jiao", "H. Qi", "T. Sun"], "venue": "Signal Processing: Image Communication, 27(6):663\u2013677", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Tackling box-constrained optimization via a new projected quasi-newton approach", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "SIAM Journal on Scientific Computing, 32(6):3548\u20133563", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression-based prediction for blocking artifact reduction in jpeg-compressed images", "author": ["K. Lee", "D.S. Kim", "T. Kim"], "venue": "TIP, 14(1):36\u201348", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Inter-block soft decoding of jpeg images with sparsity and graph-signal smoothness priors", "author": ["X. Liu", "G. Cheung", "X. Wu", "D. Zhao"], "venue": "ICIP. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain", "author": ["X. Liu", "X. Wu", "J. Zhou", "D. Zhao"], "venue": "CVPR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "JPEG: Still image data compression standard", "author": ["W.B. Pennebaker", "J.L. Mitchell"], "venue": "Springer Science & Business Media", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient regression priors for reducing image compression artifacts", "author": ["R. Rothe", "R. Timofte", "L. Van Gool"], "venue": "IEEE ICIP", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["H.R. Sheikh", "Z. Wang", "L. Cormack"], "venue": "C. Bovik. Live image quality assessment database release 2", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time compression artifact reduction via robust nonlinear filtering", "author": ["M.-Y. Shen", "C.-C. Jay Kuo"], "venue": "ICIP, volume 2, pages 565\u2013569. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A. Bronstein", "G. Sapiro"], "venue": "TPAMI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised sparse analysis and synthesis operators", "author": ["P. Sprechmann", "R. Litman", "T.B. Yakar", "A.M. Bronstein", "G. Sapiro"], "venue": "NIPS, pages 908\u2013916", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 15(1):1929\u20131958", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "TIP, 13(4):600\u2013612", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Deeply improved sparse coding for image super-resolution", "author": ["Z. Wang", "D. Liu", "J. Yang", "W. Han", "T. Huang"], "venue": "ICCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Quality assessment of deblocked images", "author": ["C. Yim", "A.C. Bovik"], "venue": "TIP, 20(1):88\u201398", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 24, "context": "Lossy compression, such as JPEG [26] and HEVC-MSP [4], is widely adopted in image and video codecs for saving both bandwidth and in-device storage.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "Lossy compression, such as JPEG [26] and HEVC-MSP [4], is widely adopted in image and video codecs for saving both bandwidth and in-device storage.", "startOffset": 50, "endOffset": 53}, {"referenceID": 10, "context": "These artifacts not only degrade perceptual visual quality, but also adversely affect various low-level image processing routines that take compressed images as input [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "As practical image compression methods are not information theoretically optimal [25], the resulting compression code streams still possess residual redundancies, which makes the restoration of the original signals possible.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Quantization is applied on the DCT coefficients of every block, with pre-known quantization levels [26].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "In contrast to the tradition of assuming noise to be white and signal independent [2], the non-linearity of quantization operations makes quantization noises non-stationary and signal-dependent.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Early works [6, 23] utilized filteringbased methods to remove simple artifacts.", "startOffset": 12, "endOffset": 19}, {"referenceID": 21, "context": "Early works [6, 23] utilized filteringbased methods to remove simple artifacts.", "startOffset": 12, "endOffset": 19}, {"referenceID": 6, "context": "Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions.", "startOffset": 66, "endOffset": 84}, {"referenceID": 7, "context": "Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions.", "startOffset": 66, "endOffset": 84}, {"referenceID": 18, "context": "Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions.", "startOffset": 66, "endOffset": 84}, {"referenceID": 22, "context": "Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions.", "startOffset": 66, "endOffset": 84}, {"referenceID": 25, "context": "Sparsitybased image restoration approaches have been discussed in [8, 9, 20, 24, 27] to produce sharpened images, but they are often accompanied with artifacts along edges, and unnatural smooth regions.", "startOffset": 66, "endOffset": 84}, {"referenceID": 23, "context": "In [25], Liu et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[12] first introduced deep learning techniques [22] into this problem, by specifically adapting their SR-CNN model in [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[12] first introduced deep learning techniques [22] into this problem, by specifically adapting their SR-CNN model in [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "[12] first introduced deep learning techniques [22] into this problem, by specifically adapting their SR-CNN model in [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "Low-complexity or even real-time attenuation of compression artifacts is highly desirable [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "The inference process of traditional approaches, for example, sparse coding, usually involves iterative optimization algorithms, whose inherently sequential structure as well as the data-dependent complexity and latency often constitute a major bottleneck in the computational efficiency [15].", "startOffset": 288, "endOffset": 292}, {"referenceID": 15, "context": "However, to maintain their competitive performances, deep networks show demands for increased width (numbers of filters) and depth (number of layers), as well as smaller strides, all leading to growing computational costs [17].", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "Our major innovation is to explicitly combine both the prior knowledge in the JPEG compression scheme and the successful practice of dual-domain sparse coding [25], for designing a task-specific deep architecture.", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "Furthermore, we introduce a One-Step Sparse Inference (1-SI) module, that acts as a highly efficient and light-weighted approximation of the sparse coding inference [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "Our work is inspired by the prior wisdom in [25].", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "Most previous works restored compressed images in either the pixel domain [2] or the DCT domain [26] solely.", "startOffset": 74, "endOffset": 77}, {"referenceID": 24, "context": "Most previous works restored compressed images in either the pixel domain [2] or the DCT domain [26] solely.", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "In view of their complementary characteristics, the dualdomain model was proposed in [25].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "While the spatial redundancies in the pixel domain were exploited by a learned dictionary [2], the residual redundancies in the DCT domain were also utilized to directly restore DCT coefficients.", "startOffset": 90, "endOffset": 93}, {"referenceID": 20, "context": "To date, deep learning [22] has shown impressive results on both high-level and low-level vision problems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "[13] showed the great potential of end-to-end trained networks in image super resolution (SR).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Their recent work [12] proposed a four-layer convolutional network that was tuned based on SR-CNN, named Artifacts Reduction Convolutional Neural Networks (AR-CNN), which was effective in dealing with various compression artifacts.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "In [15], the authors leveraged fast trainable regressors and constructed feed-forward network approximations of the learned sparse models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Similar views were adopted in [30] to develop a fixed-complexity algorithm for solving structured sparse and robust low rank models.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "The paper [18] summarized the methodology of \u201cdeep unfolding\u201d.", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "Very recently, [34] proposed deeply improved sparse coding for SR, which can be incarnated as an end-to-end neural network.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "We first review the sparsity-based dual-domain restoration model established in [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "One noteworthy point is the inequality constraint, where q and q represents the (pre-known) quantization intervals according to the JPEG quantization table [26].", "startOffset": 156, "endOffset": 160}, {"referenceID": 23, "context": "The authors of [25] simply set them all equal, which may hamper the performance.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "The subsequent two layers aim to enforce DCT domain sparsity, where we refer to the concepts of analysis and synthesis dictionaries in sparse coding [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "We design the following signaldependent, box-constrained [21] loss:", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Such a sparse inference problem could be solved by the iterative shrinkage and thresholding algorithm (ISTA) [5], each iteration of which updates as follows:", "startOffset": 109, "endOffset": 112}, {"referenceID": 13, "context": "The learned ISTA (LISTA) [15] parameterized encoder further proposed a natural network implementation of ISTA.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "The similar unfolding methodology has been lately exploited in [18], [30], [31].", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "The similar unfolding methodology has been lately exploited in [18], [30], [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "The similar unfolding methodology has been lately exploited in [18], [30], [31].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Our major motivation lies in the same observation as in [12] that overly deep networks could adversely affect the performance in low-level", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Even only two iterations are kept as in [15], we end up with a six-layer network, that suffers from both difficulties in training [12] and fragility in generalization [32] for this task.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Even only two iterations are kept as in [15], we end up with a six-layer network, that suffers from both difficulties in training [12] and fragility in generalization [32] for this task.", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Even only two iterations are kept as in [15], we end up with a six-layer network, that suffers from both difficulties in training [12] and fragility in generalization [32] for this task.", "startOffset": 167, "endOffset": 171}, {"referenceID": 32, "context": "We further rewrite (6) as [34] did1:", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "The unit-threshold neuron s1 could in essence be viewed as a double-sided and translated variant of ReLU [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A related form to (7) was obtained in [11] on a different case of non-negative sparse coding.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "The 2D DCT and IDCT each takes 12m log(m) multiplications [26] .", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "It is obvious that the sparse coding inference [25] has dramatically higher time complexity.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "We are also interested in the inference time complexity of other competitive deep models, especially AR-CNN [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "For their fully convolutional architecture, the total complexity [17] is:", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "(11) As a comparison, the AR-CNN model [12] contains:", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "By enforcing an extra nonnegative constraint on \u03b1, (2) becomes the non-negative sparse coding problem that is found to be meaningful in many visual recognition tasks [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 20, "context": "The new r\u03bb is exactly a ReLU [22] neuron with a translation bias \u03bb.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "Similarly, if we substitute the formulation (2) with the convolutional sparse coding model [7], DA in Fig.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "We use the disjoint training set (200 images) and test set (200 images) of BSDS500 database [3], as our training set; its validation set (100 images) is used for validation, which follows [12].", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "We use the disjoint training set (200 images) and test set (200 images) of BSDS500 database [3], as our training set; its validation set (100 images) is used for validation, which follows [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "The proposed networks are implemented using the cudaconvnet package [22].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "We further find the easy-hard transfer suggested by [12] useful.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "\u2022 Sparsity-based Dual-Domain Method (S-D) [25] could be viewed as the \u201cshallow\u201d counterpart of D.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "It has outperformed most traditional methods [25], such as BM3D [10] and DicTV [8], with which we thus do not compare again.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "It has outperformed most traditional methods [25], such as BM3D [10] and DicTV [8], with which we thus do not compare again.", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "It has outperformed most traditional methods [25], such as BM3D [10] and DicTV [8], with which we thus do not compare again.", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "In [12], the authors show its advantage over SA-DCT [14], RTF [19], and SR-CNN [13].", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [12], the authors show its advantage over SA-DCT [14], RTF [19], and SR-CNN [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "In [12], the authors show its advantage over SA-DCT [14], RTF [19], and SR-CNN [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "In [12], the authors show its advantage over SA-DCT [14], RTF [19], and SR-CNN [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "We adopt the default network configuration in [12]: s1 = 9, s2 = 7, s3 = 1, s4 = 5; n1 = 64, n2 = 32, n3 = 16, n4 = 1.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "3from the common experiences of choosing dictionary sizes [2] (D-Base), of the same complexity with D-256, named DBase-256.", "startOffset": 58, "endOffset": 61}, {"referenceID": 20, "context": "D-Base-256 utilizes ReLU [22] neurons and the dropout technique.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "We use the 29 images in the LIVE1 dataset [28] (converted to the gray scale) to evaluate both the quantitative and qualitative performances.", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "Three quality assessment criteria: PSNR, structural similarity (SSIM) [33], and PSNR-B [35], are evaluated, the last of which is designed specifically to assess blocky images.", "startOffset": 70, "endOffset": 74}, {"referenceID": 33, "context": "Three quality assessment criteria: PSNR, structural similarity (SSIM) [33], and PSNR-B [35], are evaluated, the last of which is designed specifically to assess blocky images.", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "In this paper, we design a Deep Dual-Domain (D) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and lightweighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed D model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster.", "creator": "LaTeX with hyperref package"}}}