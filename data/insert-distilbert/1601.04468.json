{"id": "1601.04468", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation", "abstract": "we present an approach to structured response prediction from bandit feedback, called smooth bandit structured prediction, where only the value of a task loss balance function at initially a single predicted point, instead of a correct structure, is observed in learning. we present an application to discriminative reranking in statistical machine translation ( smt ) where the learning recovery algorithm only has access to a 1 - bleu loss evaluation of a predicted translation instead of altogether obtaining a gold standard reference translation. in our experiment bandit feedback is obtained by successfully evaluating bleu on reference translations without suddenly revealing them to the algorithm. this can be thought of as a simulation of interactive machine translation where an smt system is personalized by a user who specifically provides single percentage point feedback to predicted translations. our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning.", "histories": [["v1", "Mon, 18 Jan 2016 11:09:02 GMT  (30kb)", "http://arxiv.org/abs/1601.04468v1", "In Proceedings of MT Summit XV, 2015. Miami, FL"]], "COMMENTS": "In Proceedings of MT Summit XV, 2015. Miami, FL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["artem sokolov", "stefan riezler", "tanguy urvoy"], "accepted": false, "id": "1601.04468"}, "pdf": {"name": "1601.04468.pdf", "metadata": {"source": "CRF", "title": "Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation", "authors": ["Artem Sokolov", "Stefan Riezler", "Tanguy Urvoy"], "emails": ["sokolov@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de", "tanguy.urvoy@orange.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n04 46\n8v 1\n[ cs\n.C L"}, {"heading": "1 Introduction", "text": "Learning from bandit1 feedback describes an online learning scenario, where on each of a sequence of rounds, a learning algorithm makes a prediction, and receives partial information in terms of feedback to a single predicted point. In difference to the full information supervised scenario, the learner does not know what the correct prediction looks like, nor what would have happened if it had predicted differently. This scenario has (financially) important real world applications such as online advertising (Chapelle et al., 2014) that showcases a tradeoff between exploration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term). Crucially, in this scenario it is unrealistic to expect more detailed feedback than a user click on the displayed ad. Similar to the online advertising scenario, there are many potential applications of bandit learning to NLP situations where feedback is limited for various reasons. For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Post-editing feedback clearly is limited by its high cost and by the required expertise of users, however, current approaches force the full information supervised scenario onto the problem of learning from user post-edits.\n1The name is inherited from a model where in each round a gambler pulls an arm of a different slot machine (\u201cone-armed bandit\u201d), with the goal of maximizing his reward relative to the maximal possible reward, without apriori knowledge of the optimal slot machine.\nBandit learning would allow to learn from partial user feedback that is easier and faster to obtain than full information. An example where user feedback is limited by a time constraint is simultaneous translation of a speech input stream (Cho et al., 2013). Clearly, it is unrealistic to expect user feedback that goes beyond a one-shot user quality estimate of the predicted translation in this scenario. Another example is SMT domain adaptation where the translations of a large out-of-domain model are re-ranked based on bandit feedback on in-domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures.\nThe goal of this paper is to develop algorithms for structured prediction from bandit feedback, tailored to NLP problems. We investigate possibilities to \u201cbanditize\u201d objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP. Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005). Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011). Our case is a non-convex optimization problem, which we analyze in the simple and elegant framework of pseudogradient adaptation that allows us to show convergence of the presented algorithm (Polyak and Tsypkin, 1973; Polyak, 1987).\nThe central contributions of this paper are:\n\u2022 An algorithm for minimization of expected loss for structured prediction from bandit feedback, called Bandit Structured Prediction.\n\u2022 An analysis of convergence of our algorithm in the stochastic optimization framework of pseudogradient adaptation.\n\u2022 An experimental evaluation on structured learning in SMT. Our experiment follows a simulation design that is standard in bandit learning, namely by simulating bandit feedback by evaluating task loss functions against gold standard structures without revealing them to the learning algorithm.\nAs a disclaimer, we would like to note that improvements over traditional full-information structured prediction cannot be expected from learning from partial feedback. Instead, the goal is to investigate learning situations in which full information is not available. Similarly, a comparison between our approach and dueling bandits (Yue and Joachims, 2009) is skewed towards the latter approach that has access to two-point feedback instead of one-point feedback as in our case. While it has been shown that querying the loss function at two points leads to convergence results that closely resemble bounds for the full information case (Agarwal et al., 2010), such feedback is clearly twice as expensive and, depending on the application, might not be elicitable from users."}, {"heading": "2 Related Work", "text": "Stochastic Approximation. Online learning from bandit feedback dates back to Robbins (1952) who formulated the task as a problem of sequential decision making. His analysis, as ours, is in a stochastic setting, i.e., certain assumptions are made on the probability distribution of feedback and its noisy realization. Stochastic approximation covers bandit feedback as noisy observations which only allow to compute noisy gradients that equal true gradients\nin expectation. While the stochastic approximation framework is quite general, most theoretical analyses of convergence and convergence rate are based on (strong) convexity assumptions (Polyak and Juditsky, 1992; Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011, 2013) and thus not applicable to our case.\nNon-Stochastic Bandits. Auer et al. (2002) initiated an active area of research on nonstochastic bandit learning, i.e., no statistical assumptions are made on the distribution of feedback, including models of feedback as a malicious choice of an adaptive adversary. The adversarial bandit setting has been extended to take context or side information into account, using models based on general linear classifiers (Auer et al., 2002; Langford and Zhang, 2007; Chu et al., 2011). However, they formalize a multi-class classification problem that is not easily scalable to general exponentially large structured output spaces. Furthermore, most theoretical analyses rely on online (strongly) convex optimization (Flaxman et al., 2005; Shalev-Shwartz, 2012) thus limiting the applicability to our case.\nNeurodynamic Programming. Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d. Both areas are dealing with non-convex objectives that lead to stochastic iterative algorithms. Interestingly, the available analyses of non-convex optimization for neural networks and reinforcement learning in Bertsekas and Tsitsiklis (1996), Sutton et al. (2000), or Bottou (2004) rely heavily on Polyak and Tsypkin (1973)\u2019s pseudogradient framework. We apply their simple and elegant framework directly to give asymptotic guarantees for our algorithm.\nNLP Applications. In the area of NLP, recently algorithms for response-based learning have been proposed to alleviate the supervision problem by extracting supervision signals from taskbased feedback to system predictions. For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning. Saluja and Zhang (2014) present an incorporation of binary feedback into an latent structured SVM for discriminative SMT training. NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al. (2015). Their model differs from ours in that it is structured as a sequence of states at which actions and rewards are computed, however, the theoretical foundation of both types of models can be traced back to Polyak and Tsypkin (1973)\u2019s pseudogradient framework ."}, {"heading": "3 Expected Loss Minimization under Full Information", "text": "The expected loss learning criterion for structured prediction is defined as a minimization of the expectation of a task loss function with respect to the conditional distribution over structured outputs (Gimpel and Smith, 2010; Yuille and He, 2012). More formally, let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let \u2206y : Y \u2192 [0, 1] quantify the loss \u2206y(y\u2032) suffered for making errors in predicting y\u2032 instead of y; as a rule, \u2206y(y\n\u2032) = 0 iff y = y\u2032. Then, for a data distribution p(x, y), the learning criterion is defined as minimization of the expected loss\nEp(x,y)pw(y\u2032|x) [\u2206y(y \u2032)] =\n\u2211\nx,y\np(x, y) \u2211\ny\u2032\u2208Y(x)\n\u2206y(y \u2032)pw(y \u2032|x). (1)\nAssume further that output structures given inputs are distributed according to an underlying Gibbs distribution (a.k.a. conditional exponential or log-linear model)\npw(y|x) = exp(w\u22a4\u03c6(x, y))/Zw(x),\nwhere \u03c6 : X \u00d7 Y \u2192 Rd is a joint feature representation of inputs and outputs, w \u2208 Rd is a weight vector, and Zw(w) is a normalization constant.\nThe natural rule for prediction or inference is according to the minimum Bayes risk principle\ny\u0302w(x) = argmin y\u2208Y(x)\n\u2211\ny\u2032\u2208Y(x)\n\u2206y(y \u2032)pw(y \u2032|x). (2)\nThis requires an evaluation of \u2206y(y\u2032) over the full output space, which is standardly avoided in practice by performing inference according to a maximum a posteriori (MAP) criterion (which equals criterion (2) for the special case of \u2206y(y\u2032) = 1[y 6= y\u2032] where 1[s] evaluates to 1 if statement s is true, 0 otherwise)\ny\u0302w(x) = argmax y\u2208Y(x)\npw(y|x) (3)\n= argmax y\u2208Y(x)\nw\u22a4\u03c6(x, y).\nFurthermore, since it is unfeasible to take expectations over the full space X \u00d7 Y to perform minimization of objective (1), in the full information case the data distribution p(x, y) is approximated by the empirical distribution p\u0303(x, y) = 1\nT\n\u2211T\nt=0 1[x = xt]1[y = yt] for i.i.d. training data {(xt, yt)}Tt=0. This yields the objective\nEp\u0303(x,y)pw(y\u2032|x) [\u2206y(y \u2032)] =\n1\nT\nT \u2211\nt=0\n\u2211\ny\u2032\u2208Y(xt)\n\u2206yt(y \u2032)pw(y \u2032|xt). (4)\nWhile being continuous and differentiable, the expected loss criterion is typically nonconvex. For example, in SMT, expected loss training for the standard task loss BLEU leads to highly non-convex optimization problems. Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4):\n\u2207Ep\u0303(x,y)pw(y\u2032|x) [\u2206y(y\u2032)]\n= Ep\u0303(x,y)\n[\nEpw(y\u2032|x)[\u2206y(y \u2032)\u03c6(x, y\u2032)]\u2212 Epw(y\u2032|x)[\u2206y(y\u2032)] Epw(y\u2032|x)[\u03c6(x, y\u2032)]\n]\n= Ep\u0303(x,y)pw(y\u2032|x)\n[\n\u2206y(y \u2032)(\u03c6(x, y\u2032)\u2212 Epw(y\u2032|x)[\u03c6(x, y\u2032)])\n]\n."}, {"heading": "4 Bandit Structured Prediction", "text": "Bandit feedback in structured prediction means that the gold standard output structure y, with respect to which the objective function is evaluated, is not revealed to the learner. Thus we can neither calculate the gradient of the objective function (4) nor evaluate the task loss \u2206 as in the full information case. A solution to this problem is to pass the evaluation of the loss function to the user, i.e, we access the loss directly through user feedback without assuming existence of a fixed reference y. We indicate this by dropping the subscript y in \u2206(y\u2032). Assuming a fixed,\nAlgorithm 1 Bandit Structured Prediction\n1: Input: sequence of learning rates \u03b3t 2: Initialize w0 3: for t = 0, . . . , T do 4: Observe xt 5: Calculate Epwt (y\u2032|xt)[\u03c6(xt, y\n\u2032)] 6: Sample y\u0303t \u223c pwt(y\u2032|xt) 7: Obtain feedback \u2206(y\u0303t) 8: Update wt+1 = wt \u2212 \u03b3t \u2206(y\u0303t)(\u03c6(xt, y\u0303t)\u2212 Epwt (y\u2032|xt)[\u03c6(xt, y \u2032)])\nunknown distribution p(x) over input structures, we can formalize the following new objective for expected loss minimization in a bandit setup\nJ(w) = Ep(x)pw(y\u2032|x) [\u2206(y \u2032)] (5)\n= \u2211\nx\np(x) \u2211\ny\u2032\u2208Y(x)\n\u2206(y\u2032)pw(y \u2032|x).\nOptimization of this objective is then as follows:\n1. We assume a sequence of input structures xt, t = 1, . . . , T that are generated by a fixed, unknown distribution p(x).\n2. We use a Gibbs distribution estimate as a sampling distribution to perform simultaneous exploration / exploitation on output structures (Abernethy and Rakhlin, 2009).\n3. We use feedback to the sampled output structures to construct a parameter update rule that is an unbiased estimate of the true gradient of objective (5)."}, {"heading": "4.1 Algorithm", "text": "Algorithm 1 implements these ideas as follows: We assume as input a given learning rate schedule (line 1) and a deterministic initialization w0 of the weight vector (line 2). For each random i.i.d. input structure xt, we calculate the expected feature count (line 5). This can be done exactly, provided the underlying graphical model permits a tractable calculation, or for intractable models, with MCMC sampling. We then sample an output structure y\u0303t from the Gibbs model (line 6). If the number of output options is small, this is done by sampling from a multinomial distribution. Otherwise, we use a Perturb-and-MAP approach (Papandreou and Yuille, 2011), restricted to unary potentials, to obtain an approximate Gibbs sample without waiting for the MC chain to mix. Finally, an update in the negative direction of the instantaneous gradient, evaluated at the input structure xt (line 8), is performed.\nIntuitively, the algorithm compares the sampled feature vector to the average feature vector, and performs a step into the opposite direction of this difference, the more so the higher the loss of the sampled structure is. In the extreme case, if the sampled structure is correct (\u2206(y\u0303t) = 0), no update is performed."}, {"heading": "4.2 Stochastic Approximation Analysis", "text": "The construction of the update in Algorithm 1 as a stochastic realization of the true gradient allows us to analyze the algorithm as a stochastic approximation algorithm. We show how our case can be fit in the pseudogradient adaptation framework of Polyak and Tsypkin (1973) which gives asymptotic guarantees for non-convex and convex objectives. They characterize an\niterative process\nwt+1 = wt \u2212 \u03b3t st (6)\nwhere \u03b3t \u2265 0 is a learning rate, wt and st are vectors in Rd with fixed w0, and the distribution of st depends on w0, . . . , wt. For a given lower bounded and differentiable function J(w) with Lipschitz continuous gradient \u2207J(w), that is, for all w,w\u2032, there exists L \u2265 0, such that\n\u2016\u2207J(w + w\u2032)\u2212\u2207J(w)\u2016 \u2264 L\u2016w\u2032\u2016, (7) the vector st in process (6) is said to be a pseudogradient of J(w) if\n\u2207J(wt)\u22a4E[st] \u2265 0, (8) where the expectation is taken over all sources of randomness. Intuitively, the pseudogradient st is on average at an acute angle with the true gradient, meaning that \u2212st is on average a direction of decrease of the functional J(w).\nIn order to show convergence of the iterative process (6), besides conditions (7) and (8), only mild conditions on boundedness of the pseudogradient\nE[\u2016st\u20162] < \u221e, (9) and on the use of a decreasing learning rate satisfying\n\u03b3t \u2265 0, \u221e \u2211\nt=0\n\u03b3t = \u221e, \u221e \u2211\nt=0\n\u03b32t < \u221e, (10)\nare necessary. Under the exclusion of trivial solutions such as st = 0, the following convergence assertion can be made:\nTheorem 1 (Polyak and Tsypkin (1973), Thm. 1) Under conditions (7)\u2013(10), for any w0 in process (6):\nJ(wt) \u2192 J\u2217 a.s., and lim t\u2192\u221e \u2207J(wt)\u22a4E(st) = 0.\nThe significance of the theorem is that its conditions can be checked easily, and it applies to a wide range of cases, including non-convex functions, in which case the convergence point J\u2217 is a critical point of J(w). The convergence analysis of Theorem 1 can be applied to Algorithm 1 as follows: First note that we can define our functional J(w) with respect to expectations over the full space of X as J(w) = Ep(x)pw(y\u2032|x)[\u2206(y\n\u2032)]. This means, convergence of the algorithm can be understood directly as a generalization result that extends to unseen data. In order to show this result, we have to verify conditions (7)\u2013(10). It is easy to show that condition (7) holds for our functional J(w). Next we match the update in Algorithm 1 to a vector\nst = \u2206(y\u0303t)(\u03c6(xt, y\u0303t)\u2212 Epwt (y\u2032|xt)[\u03c6(xt, y \u2032)]).\nTaking the expectation of st yields Ep(x)pwt (y\u2032|x)[st] = \u2207J(wt) such that condition (8) is satisfied by\n\u2207J(wt)\u22a4Ep(x)pwt(y\u2032|x)[st] = \u2016\u2207J(wt)\u2016 2 \u2265 0.\nAssuming \u2016\u03c6(x, y\u2032)\u2016 \u2264 R and \u2206(y\u2032) \u2208 [0, 1] for all x, y\u2032, condition (9) is satisfied by Ep(x)pwt (y\n\u2032|x)[\u2016st\u20162] \u2264 4R2. For a decreasing learning rate, e.g., \u03b3t = 1/t, condition (10) holds, such that convergence to a critical point of the expected risk follows according to Theorem 1.\nAlgorithm Structured Dueling Bandits\n1: Input: \u03b3, \u03b4, w0 2: for t = 0, . . . , T do 3: Observe xt 4: Sample unit vector ut uniformly 5: Set w\u2032t = wt + \u03b4ut 6: Compare \u2206(y\u0302wt(xt)) to \u2206(y\u0302w\u2032t(xt)) 7: if w\u2032t wins then 8: wt+1 = wt + \u03b3ut 9: else\n10: wt+1 = wt"}, {"heading": "5 Structured Dueling Bandits", "text": "For purposes of comparison, we present an extension of Yue and Joachims (2009)\u2019s dueling bandits algorithm to structured prediction problems. The original algorithm is not specifically designed for structured prediction problems, but it is generic enough to be applicable to such problems when the quality of a parameter vector can be proxied through loss evaluation of an inferred structure.\nThe Structured Dueling Bandits algorithm compares a current weight vector wt with a neighboring point w\u2032t along a direction ut, performing exploration (controlled by \u03b4, line 5) by probing random directions, and exploitation (controlled by \u03b3, line 8) by taking a step into the winning direction. The comparison step in line 6 is adapted to structured prediction from the original algorithm of Yue and Joachims (2009) by comparing the quality of wt and w\u2032t via an evaluation of the losses \u2206(y\u0302wt(xt)) and \u2206(y\u0302w\u2032t(xt)) of the structured arms corresponding to MAP prediction (3) under wt and w\u2032t, respectively.\nFurther, note that the Structured Dueling Bandit algorithm requires access to a two-point feedback instead of a one-point feedback as in case of Bandit Structured Prediction (Algorithm 1). It has been shown that two-point feedback leads to convergence results that are close to those for learning from full information Agarwal et al. (2010). However, two-point feedback is twice as expensive as one-point feedback, and most importantly, such feedback might not be elicitable from users in real-world situations where feedback is limited by time- and resourceconstraints. This limits the range of applications of Dueling Bandits to real-world interactive scenarios."}, {"heading": "6 Experiments", "text": "Our experimental design follows the standard of simulating bandit feedback by evaluating task loss functions against gold standard structures without revealing them to the learner. We compare the proposed Structured Bandit Prediction algorithm to Structured Dueling Bandits, and report results by test set evaluations of the respective loss functions under MAP inference. Furthermore, we evaluate models at different iterations according to their loss on the test set in order to visualize the empirical convergence behavior of the algorithms.\nAll experiments with bandit algorithms perform online learning for parameter estimation, and apply early stopping to choose the last model in a learning sequence for online-to-batch conversion at test time. Final results for bandit algorithms are averaged over 5 independent runs.\nIn this experiment, we present bandit learning for the structured 1 \u2212 BLEU loss used in SMT. The setup is a reranking approach to SMT domain adaptation where the k-best list of an out-of-domain model is re-ranked (without re-decoding) based on bandit feedback from in-\ndomain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures.\nWe use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daume\u0301 and Jagarlamudi, 2011). We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013). The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al., 2013).\nThe out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain and 1 in-domain language model, 1 word penalty, 5 translation model features) that are tuned with MERT (Och, 2003) on a dev set of Europarl data (dev2006, 2,000 sentences). The full-information in-domain SMT model gives an upper bound by MERT tuning the out-ofdomain model on in-domain development data (nc-dev2007, 1,057 sentences). MERT runs for both baseline models were repeated 7 times and median results are reported.\nLearning under bandit feedback started at the learned weights of the out-of-domain median model. It uses the parallel NewsCommentary data (news-commentary, 43,194 sentences) to simulate bandit feedback, by evaluating the sampled translation against the gold standard reference using as loss function \u2206 a smoothed per-sentence 1\u2212BLEU (by flooring zero n-gram\ncounts to 0.01). The meta-parameters of Dueling Bandits and Bandit Structured Prediction were adjusted by online optimization of cumulative per-sentence 1\u2212BLEU on a small in-domain dev set (nc-devtest2007, 1,064 parallel sentences). The final results are obtained by online-tobatch conversion where the model trained for 100 epochs on 43,194 in-domain training data is evaluated on a separate in-domain test set (nc-test2007, 2,007 sentences).\nTable 1 shows that the results for Bandit Structured Prediction and Dueling Bandits are very close, however, both are significant improvements over the out-of-domain SMT model that even includes an in-domain language model. We show the standard evaluation of the corpusBLEU metric evaluated under MAP inference. The range of possible improvements is given by the difference of the BLEU score of the in-domain model and the BLEU score of the out-ofdomain model \u2013 nearly 3 BLEU points. Bandit learning can improve the out-of-domain baseline by about 1.26 BLEU points (Bandit Structured Prediction) and by about 1.52 BLEU points (Dueling Bandits). All result differences are statistically significant at a p-value of 0.0001, using an Approximate Randomization test (Riezler and Maxwell, 2005; Clark et al., 2011). Figure 1 shows that per-sentence BLEU is a difficult metric to provide single-point feedback, yielding a non-smooth progression of loss values against iterations for Bandit Structured Prediction. The progression of loss values is smoother and empirical convergence speed is faster for Dueling Bandits since it can exploit preference judgements instead of having to trust real-valued feedback."}, {"heading": "7 Discussion", "text": "We presented an approach to Bandit Structured Prediction that is able to learn from feedback in form of an evaluation of a task loss function for single predicted structures. Our experimental evaluation showed promising results, both compared to Structured Dueling Bandits that employ two-point feedback, and compared to full information scenarios where the correct structure is revealed.\nOur approach shows its strength where correct structures are unavailable and two-point feedback is infeasible. In future work we would like to apply bandit learning to scenarios with limited human feedback such as the interactive SMT applications discussed above. In such scenarios, per-sentence BLEU might not be the best metric to quantify feedback. We will instead investigate feedback based on HTER (Snover et al., 2006), or based on judgements according to Likert scales (Likert, 1932)."}, {"heading": "Acknowledgements", "text": "This research was supported in part by DFG grant RI-2221/2-1 \u201cGrounding Statistical Machine Translation in Perception and Action\u201d."}], "references": [{"title": "An efficient bandit algorithm", "author": ["J. Abernethy", "A. Rakhlin"], "venue": null, "citeRegEx": "Abernethy and Rakhlin,? \\Q2009\\E", "shortCiteRegEx": "Abernethy and Rakhlin", "year": 2009}, {"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback", "author": ["A. Agarwal", "O. Dekel", "L. Xiao"], "venue": "Conference on Learning Theory (COLT), Haifa, Israel.", "citeRegEx": "Agarwal et al\\.,? 2010", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing, 32(1):48\u201377.", "citeRegEx": "Auer et al\\.,? 2002", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Large-scale expected BLEU training of phrase-based reordering models", "author": ["M. Auli", "M. Galley", "J. Gao"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Auli et al\\.,? 2014", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems (NIPS), Granada, Spain.", "citeRegEx": "Bach and Moulines,? 2011", "shortCiteRegEx": "Bach and Moulines", "year": 2011}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems (NIPS), Lake Tahoe, CA, USA.", "citeRegEx": "Bach and Moulines,? 2013", "shortCiteRegEx": "Bach and Moulines", "year": 2013}, {"title": "Online adaptation to post-edits for phrase-based statistical machine translation", "author": ["N. Bertoldi", "P. Simianer", "M. Cettolo", "K. W\u00e4schle", "M. Federico", "S. Riezler"], "venue": "Machine Translation, 29:309\u2013339.", "citeRegEx": "Bertoldi et al\\.,? 2014", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2014}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Bousquet, O. and von Luxburg, U., editors, Advanced Lectures on Machine Learning, pages 146\u2013168.", "citeRegEx": "Bottou,? 2004", "shortCiteRegEx": "Bottou", "year": 2004}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, Suntec, Singapore.", "citeRegEx": "Branavan et al\\.,? 2009", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to search better than your teacher", "author": ["Chang", "K.-W.", "A. Krishnamurthy", "A. Agarwal", "H. Daume", "J. Langford"], "venue": "International Conference on Machine Learning (ICML), Lille, France.", "citeRegEx": "Chang et al\\.,? 2015", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Simple and scalable response prediction for display advertising", "author": ["O. Chapelle", "E. Manavaglu", "R. Rosales"], "venue": "ACM Transactions on Intelligent Systems and Technology, 5(4).", "citeRegEx": "Chapelle et al\\.,? 2014", "shortCiteRegEx": "Chapelle et al\\.", "year": 2014}, {"title": "A real-world system for simultaneous translation of German lectures", "author": ["E. Cho", "C. F\u00fcgen", "T. Hermann", "K. Kilgour", "M. Mediani", "C. Mohr", "J. Niehues", "K. Rottman", "C. Saam", "S. St\u00fcker", "A. Waibel"], "venue": "Interspeech, Lyon, France.", "citeRegEx": "Cho et al\\.,? 2013", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "Contextual bandits with linear payoff functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), Fort Lauderdale, FL, USA.", "citeRegEx": "Chu et al\\.,? 2011", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["J. Clark", "C. Dyer", "A. Lavie", "N. Smith"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL\u201911), Portland, OR.", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Domain adaptation for machine translation by mining unseen words", "author": ["H. Daum\u00e9", "J. Jagarlamudi"], "venue": "Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), Portland, OR, USA.", "citeRegEx": "Daum\u00e9 and Jagarlamudi,? 2011", "shortCiteRegEx": "Daum\u00e9 and Jagarlamudi", "year": 2011}, {"title": "Learning from post-editing: Online model adaptation for statistical machine translation", "author": ["M. Denkowski", "C. Dyer", "A. Lavie"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL), Gothenburg, Sweden.", "citeRegEx": "Denkowski et al\\.,? 2014", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Atlanta, GA, USA.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "ACM-SIAM Symposium on Discrete Algorithms (SODA), Philadelphia, PA.", "citeRegEx": "Flaxman et al\\.,? 2005", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Softmax-margin training for structured log-linear models", "author": ["K. Gimpel", "N.A. Smith"], "venue": "Technical Report CMU-LTI-10-008, Carnegie Mellon University, Pittsburgh, PA, USA.", "citeRegEx": "Gimpel and Smith,? 2010", "shortCiteRegEx": "Gimpel and Smith", "year": 2010}, {"title": "Learning from natural instructions", "author": ["D. Goldwasser", "D. Roth"], "venue": "Machine Learning, 94(2):205\u2013232.", "citeRegEx": "Goldwasser and Roth,? 2013", "shortCiteRegEx": "Goldwasser and Roth", "year": 2013}, {"title": "Human effort and machine learnability in computer aided translation", "author": ["S. Green", "S.I. Wang", "J. Chuang", "J. Heer", "S. Schuster", "C.D. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar.", "citeRegEx": "Green et al\\.,? 2014", "shortCiteRegEx": "Green et al\\.", "year": 2014}, {"title": "Maximum expected BLEU training of phrase and lexicon translation models", "author": ["X. He", "L. Deng"], "venue": "Meeting of the Association for Computational Linguistics (ACL), Jeju Island, Korea.", "citeRegEx": "He and Deng,? 2012", "shortCiteRegEx": "He and Deng", "year": 2012}, {"title": "Scalable modified KneserNey language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Meeting of the Association for Computational Linguistics (ACL), Sofia, Bulgaria.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Birch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "ACL Demo and Poster Sessions, Prague, Czech Republic.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Experiments in domain adaptation for statistical machine translation", "author": ["P. Koehn", "J. Schroeder"], "venue": "Workshop on Statistical Machine Translation, Prague, Czech Republic.", "citeRegEx": "Koehn and Schroeder,? 2007", "shortCiteRegEx": "Koehn and Schroeder", "year": 2007}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems (NIPS). Vancouver, Canada.", "citeRegEx": "Langford and Zhang,? 2007", "shortCiteRegEx": "Langford and Zhang", "year": 2007}, {"title": "A technique for the measurement of attitudes", "author": ["R. Likert"], "venue": "Archives of Psychology, 140:5\u201355.", "citeRegEx": "Likert,? 1932", "shortCiteRegEx": "Likert", "year": 1932}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19(4):1574\u20131609.", "citeRegEx": "Nemirovski et al\\.,? 2009", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL), Edmonton, Canada.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "IEEE International Conference on Computer Vision (ICCV), Barcelona, Spain.", "citeRegEx": "Papandreou and Yuille,? 2011", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "Introduction to Optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software, Inc., New York.", "citeRegEx": "Polyak,? 1987", "shortCiteRegEx": "Polyak", "year": 1987}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855.", "citeRegEx": "Polyak and Juditsky,? 1992", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Pseudogradient adaptation and training algorithms", "author": ["B.T. Polyak", "Y.Z. Tsypkin"], "venue": "Automation and remote control: a translation of Avtomatika i Telemekhanika, 34(3):377\u2013397.", "citeRegEx": "Polyak and Tsypkin,? 1973", "shortCiteRegEx": "Polyak and Tsypkin", "year": 1973}, {"title": "On some pitfalls in automatic evaluation and significance testing for MT", "author": ["S. Riezler", "J. Maxwell"], "venue": "Proceedings of the ACL-05 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, MI.", "citeRegEx": "Riezler and Maxwell,? 2005", "shortCiteRegEx": "Riezler and Maxwell", "year": 2005}, {"title": "Response-based learning for grounded machine translation", "author": ["S. Riezler", "P. Simianer", "C. Haas"], "venue": "Meeting of the Association for Computational Linguistics (ACL), Baltimore, MD, USA.", "citeRegEx": "Riezler et al\\.,? 2014", "shortCiteRegEx": "Riezler et al\\.", "year": 2014}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Statistical Society, 55:527\u2013535.", "citeRegEx": "Robbins,? 1952", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Online discriminative learning for machine translation with binary-valued feedback", "author": ["A. Saluja", "Y. Zhang"], "venue": "Machine Translation, 28:69\u201390.", "citeRegEx": "Saluja and Zhang,? 2014", "shortCiteRegEx": "Saluja and Zhang", "year": 2014}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, 4(2):107\u2013194.", "citeRegEx": "Shalev.Shwartz,? 2012", "shortCiteRegEx": "Shalev.Shwartz", "year": 2012}, {"title": "Minimum risk annealing for training log-linear models", "author": ["D.A. Smith", "J. Eisner"], "venue": "International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL), Sydney, Australia.", "citeRegEx": "Smith and Eisner,? 2006", "shortCiteRegEx": "Smith and Eisner", "year": 2006}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "Conference of the Association for Machine Translation in the Americas (AMTA), Cambridge, MA, USA.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A coactive learning view of online structured prediction in statistical machine translation", "author": ["A. Sokolov", "S. Riezler", "S.B. Cohen"], "venue": "Proceedings of the Conference on Computational Natural Language Learning (CoNLL), Beijing, China.", "citeRegEx": "Sokolov et al\\.,? 2015", "shortCiteRegEx": "Sokolov et al\\.", "year": 2015}, {"title": "Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control", "author": ["J.C. Spall"], "venue": "Wiley.", "citeRegEx": "Spall,? 2003", "shortCiteRegEx": "Spall", "year": 2003}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Advances in Neural Information Processings Systems (NIPS), Vancouver, Canada.", "citeRegEx": "Sutton et al\\.,? 2000", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Max-margin parsing", "author": ["B. Taskar", "D. Klein", "M. Collins", "D. Koller", "C. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.", "citeRegEx": "Taskar et al\\.,? 2004", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, 5:1453\u20131484.", "citeRegEx": "Tsochantaridis et al\\.,? 2005", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "A comparison of update strategies for large-scale maximum expected bleu training", "author": ["J. Wuebker", "S. Muehr", "P. Lehnen", "S. Peitz", "H. Ney"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT), Denver, CO, USA.", "citeRegEx": "Wuebker et al\\.,? 2015", "shortCiteRegEx": "Wuebker et al\\.", "year": 2015}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), Montreal, Canada.", "citeRegEx": "Yue and Joachims,? 2009", "shortCiteRegEx": "Yue and Joachims", "year": 2009}, {"title": "Probabilistic models of vision and max-margin methods", "author": ["A. Yuille", "X. He"], "venue": "Frontiers of Electrical and Electronic Engineering, 7(1).", "citeRegEx": "Yuille and He,? 2012", "shortCiteRegEx": "Yuille and He", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "This scenario has (financially) important real world applications such as online advertising (Chapelle et al., 2014) that showcases a tradeoff between exploration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term).", "startOffset": 93, "endOffset": 116}, {"referenceID": 6, "context": "For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014).", "startOffset": 112, "endOffset": 179}, {"referenceID": 16, "context": "For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014).", "startOffset": 112, "endOffset": 179}, {"referenceID": 21, "context": "For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014).", "startOffset": 112, "endOffset": 179}, {"referenceID": 12, "context": "An example where user feedback is limited by a time constraint is simultaneous translation of a speech input stream (Cho et al., 2013).", "startOffset": 116, "endOffset": 134}, {"referenceID": 29, "context": "We investigate possibilities to \u201cbanditize\u201d objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP.", "startOffset": 77, "endOffset": 136}, {"referenceID": 39, "context": "We investigate possibilities to \u201cbanditize\u201d objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP.", "startOffset": 77, "endOffset": 136}, {"referenceID": 19, "context": "We investigate possibilities to \u201cbanditize\u201d objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP.", "startOffset": 77, "endOffset": 136}, {"referenceID": 44, "context": "Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005).", "startOffset": 234, "endOffset": 284}, {"referenceID": 45, "context": "Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005).", "startOffset": 234, "endOffset": 284}, {"referenceID": 18, "context": "Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al.", "startOffset": 180, "endOffset": 224}, {"referenceID": 38, "context": "Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al.", "startOffset": 180, "endOffset": 224}, {"referenceID": 42, "context": ", 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011).", "startOffset": 82, "endOffset": 145}, {"referenceID": 28, "context": ", 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011).", "startOffset": 82, "endOffset": 145}, {"referenceID": 4, "context": ", 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011).", "startOffset": 82, "endOffset": 145}, {"referenceID": 33, "context": "Our case is a non-convex optimization problem, which we analyze in the simple and elegant framework of pseudogradient adaptation that allows us to show convergence of the presented algorithm (Polyak and Tsypkin, 1973; Polyak, 1987).", "startOffset": 191, "endOffset": 231}, {"referenceID": 31, "context": "Our case is a non-convex optimization problem, which we analyze in the simple and elegant framework of pseudogradient adaptation that allows us to show convergence of the presented algorithm (Polyak and Tsypkin, 1973; Polyak, 1987).", "startOffset": 191, "endOffset": 231}, {"referenceID": 47, "context": "Similarly, a comparison between our approach and dueling bandits (Yue and Joachims, 2009) is skewed towards the latter approach that has access to two-point feedback instead of one-point feedback as in our case.", "startOffset": 65, "endOffset": 89}, {"referenceID": 1, "context": "While it has been shown that querying the loss function at two points leads to convergence results that closely resemble bounds for the full information case (Agarwal et al., 2010), such feedback is clearly twice as expensive and, depending on the application, might not be elicitable from users.", "startOffset": 158, "endOffset": 180}, {"referenceID": 29, "context": "Stochastic Approximation. Online learning from bandit feedback dates back to Robbins (1952) who formulated the task as a problem of sequential decision making.", "startOffset": 2, "endOffset": 92}, {"referenceID": 32, "context": "While the stochastic approximation framework is quite general, most theoretical analyses of convergence and convergence rate are based on (strong) convexity assumptions (Polyak and Juditsky, 1992; Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011, 2013) and thus not applicable to our case.", "startOffset": 169, "endOffset": 265}, {"referenceID": 42, "context": "While the stochastic approximation framework is quite general, most theoretical analyses of convergence and convergence rate are based on (strong) convexity assumptions (Polyak and Juditsky, 1992; Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011, 2013) and thus not applicable to our case.", "startOffset": 169, "endOffset": 265}, {"referenceID": 28, "context": "While the stochastic approximation framework is quite general, most theoretical analyses of convergence and convergence rate are based on (strong) convexity assumptions (Polyak and Juditsky, 1992; Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011, 2013) and thus not applicable to our case.", "startOffset": 169, "endOffset": 265}, {"referenceID": 2, "context": "The adversarial bandit setting has been extended to take context or side information into account, using models based on general linear classifiers (Auer et al., 2002; Langford and Zhang, 2007; Chu et al., 2011).", "startOffset": 148, "endOffset": 211}, {"referenceID": 26, "context": "The adversarial bandit setting has been extended to take context or side information into account, using models based on general linear classifiers (Auer et al., 2002; Langford and Zhang, 2007; Chu et al., 2011).", "startOffset": 148, "endOffset": 211}, {"referenceID": 13, "context": "The adversarial bandit setting has been extended to take context or side information into account, using models based on general linear classifiers (Auer et al., 2002; Langford and Zhang, 2007; Chu et al., 2011).", "startOffset": 148, "endOffset": 211}, {"referenceID": 18, "context": "Furthermore, most theoretical analyses rely on online (strongly) convex optimization (Flaxman et al., 2005; Shalev-Shwartz, 2012) thus limiting the applicability to our case.", "startOffset": 85, "endOffset": 129}, {"referenceID": 38, "context": "Furthermore, most theoretical analyses rely on online (strongly) convex optimization (Flaxman et al., 2005; Shalev-Shwartz, 2012) thus limiting the applicability to our case.", "startOffset": 85, "endOffset": 129}, {"referenceID": 2, "context": "Auer et al. (2002) initiated an active area of research on nonstochastic bandit learning, i.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d.", "startOffset": 0, "endOffset": 32}, {"referenceID": 7, "context": "Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d. Both areas are dealing with non-convex objectives that lead to stochastic iterative algorithms. Interestingly, the available analyses of non-convex optimization for neural networks and reinforcement learning in Bertsekas and Tsitsiklis (1996), Sutton et al.", "startOffset": 0, "endOffset": 387}, {"referenceID": 7, "context": "Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d. Both areas are dealing with non-convex objectives that lead to stochastic iterative algorithms. Interestingly, the available analyses of non-convex optimization for neural networks and reinforcement learning in Bertsekas and Tsitsiklis (1996), Sutton et al. (2000), or Bottou (2004) rely heavily on Polyak and Tsypkin (1973)\u2019s pseudogradient framework.", "startOffset": 0, "endOffset": 409}, {"referenceID": 7, "context": "Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d. Both areas are dealing with non-convex objectives that lead to stochastic iterative algorithms. Interestingly, the available analyses of non-convex optimization for neural networks and reinforcement learning in Bertsekas and Tsitsiklis (1996), Sutton et al. (2000), or Bottou (2004) rely heavily on Polyak and Tsypkin (1973)\u2019s pseudogradient framework.", "startOffset": 0, "endOffset": 427}, {"referenceID": 7, "context": "Bertsekas and Tsitsiklis (1996) cover optimization for neural networks and reinforcement learning under the name of \u201cneurodynamic programming\u201d. Both areas are dealing with non-convex objectives that lead to stochastic iterative algorithms. Interestingly, the available analyses of non-convex optimization for neural networks and reinforcement learning in Bertsekas and Tsitsiklis (1996), Sutton et al. (2000), or Bottou (2004) rely heavily on Polyak and Tsypkin (1973)\u2019s pseudogradient framework.", "startOffset": 0, "endOffset": 469}, {"referenceID": 18, "context": "For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning.", "startOffset": 13, "endOffset": 40}, {"referenceID": 18, "context": "For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning.", "startOffset": 13, "endOffset": 258}, {"referenceID": 18, "context": "For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning.", "startOffset": 13, "endOffset": 484}, {"referenceID": 18, "context": "For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning. Saluja and Zhang (2014) present an incorporation of binary feedback into an latent structured SVM for discriminative SMT training.", "startOffset": 13, "endOffset": 699}, {"referenceID": 9, "context": "NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al. (2015). Their model differs from ours in that it is structured as a sequence of states at which actions and rewards are computed, however, the theoretical foundation of both types of models can be traced back to Polyak and Tsypkin (1973)\u2019s pseudogradient framework .", "startOffset": 72, "endOffset": 118}, {"referenceID": 9, "context": "NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al. (2015). Their model differs from ours in that it is structured as a sequence of states at which actions and rewards are computed, however, the theoretical foundation of both types of models can be traced back to Polyak and Tsypkin (1973)\u2019s pseudogradient framework .", "startOffset": 72, "endOffset": 349}, {"referenceID": 19, "context": "The expected loss learning criterion for structured prediction is defined as a minimization of the expectation of a task loss function with respect to the conditional distribution over structured outputs (Gimpel and Smith, 2010; Yuille and He, 2012).", "startOffset": 204, "endOffset": 249}, {"referenceID": 48, "context": "The expected loss learning criterion for structured prediction is defined as a minimization of the expectation of a task loss function with respect to the conditional distribution over structured outputs (Gimpel and Smith, 2010; Yuille and He, 2012).", "startOffset": 204, "endOffset": 249}, {"referenceID": 27, "context": "Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al.", "startOffset": 90, "endOffset": 101}, {"referenceID": 27, "context": "Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al.", "startOffset": 90, "endOffset": 126}, {"referenceID": 21, "context": "Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 3, "context": "Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 3, "context": "Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4):", "startOffset": 147, "endOffset": 189}, {"referenceID": 0, "context": "We use a Gibbs distribution estimate as a sampling distribution to perform simultaneous exploration / exploitation on output structures (Abernethy and Rakhlin, 2009).", "startOffset": 136, "endOffset": 165}, {"referenceID": 30, "context": "Otherwise, we use a Perturb-and-MAP approach (Papandreou and Yuille, 2011), restricted to unary potentials, to obtain an approximate Gibbs sample without waiting for the MC chain to mix.", "startOffset": 45, "endOffset": 74}, {"referenceID": 29, "context": "2 Stochastic Approximation Analysis The construction of the update in Algorithm 1 as a stochastic realization of the true gradient allows us to analyze the algorithm as a stochastic approximation algorithm. We show how our case can be fit in the pseudogradient adaptation framework of Polyak and Tsypkin (1973) which gives asymptotic guarantees for non-convex and convex objectives.", "startOffset": 4, "endOffset": 311}, {"referenceID": 31, "context": "Under the exclusion of trivial solutions such as st = 0, the following convergence assertion can be made: Theorem 1 (Polyak and Tsypkin (1973), Thm.", "startOffset": 117, "endOffset": 143}, {"referenceID": 46, "context": "5 Structured Dueling Bandits For purposes of comparison, we present an extension of Yue and Joachims (2009)\u2019s dueling bandits algorithm to structured prediction problems.", "startOffset": 84, "endOffset": 108}, {"referenceID": 46, "context": "5 Structured Dueling Bandits For purposes of comparison, we present an extension of Yue and Joachims (2009)\u2019s dueling bandits algorithm to structured prediction problems. The original algorithm is not specifically designed for structured prediction problems, but it is generic enough to be applicable to such problems when the quality of a parameter vector can be proxied through loss evaluation of an inferred structure. The Structured Dueling Bandits algorithm compares a current weight vector wt with a neighboring point w t along a direction ut, performing exploration (controlled by \u03b4, line 5) by probing random directions, and exploitation (controlled by \u03b3, line 8) by taking a step into the winning direction. The comparison step in line 6 is adapted to structured prediction from the original algorithm of Yue and Joachims (2009) by comparing the quality of wt and w t via an evaluation of the losses \u2206(\u0177wt(xt)) and \u2206(\u0177w\u2032 t(xt)) of the structured arms corresponding to MAP prediction (3) under wt and w t, respectively.", "startOffset": 84, "endOffset": 838}, {"referenceID": 1, "context": "It has been shown that two-point feedback leads to convergence results that are close to those for learning from full information Agarwal et al. (2010). However, two-point feedback is twice as expensive as one-point feedback, and most importantly, such feedback might not be elicitable from users in real-world situations where feedback is limited by time- and resourceconstraints.", "startOffset": 130, "endOffset": 152}, {"referenceID": 25, "context": "We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum\u00e9 and Jagarlamudi, 2011).", "startOffset": 163, "endOffset": 219}, {"referenceID": 15, "context": "We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum\u00e9 and Jagarlamudi, 2011).", "startOffset": 163, "endOffset": 219}, {"referenceID": 17, "context": "We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013).", "startOffset": 105, "endOffset": 124}, {"referenceID": 24, "context": "The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 23, "context": ", 2007) and a 4-gram language model (Heafield et al., 2013).", "startOffset": 36, "endOffset": 59}, {"referenceID": 29, "context": "The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain and 1 in-domain language model, 1 word penalty, 5 translation model features) that are tuned with MERT (Och, 2003) on a dev set of Europarl data (dev2006, 2,000 sentences).", "startOffset": 201, "endOffset": 212}, {"referenceID": 34, "context": "0001, using an Approximate Randomization test (Riezler and Maxwell, 2005; Clark et al., 2011).", "startOffset": 46, "endOffset": 93}, {"referenceID": 14, "context": "0001, using an Approximate Randomization test (Riezler and Maxwell, 2005; Clark et al., 2011).", "startOffset": 46, "endOffset": 93}, {"referenceID": 40, "context": "We will instead investigate feedback based on HTER (Snover et al., 2006), or based on judgements according to Likert scales (Likert, 1932).", "startOffset": 51, "endOffset": 72}, {"referenceID": 27, "context": ", 2006), or based on judgements according to Likert scales (Likert, 1932).", "startOffset": 59, "endOffset": 73}], "year": 2016, "abstractText": "We present an approach to structured prediction from bandit feedback, called Bandit Structured Prediction, where only the value of a task loss function at a single predicted point, instead of a correct structure, is observed in learning. We present an application to discriminative reranking in Statistical Machine Translation (SMT) where the learning algorithm only has access to a 1 \u2212 BLEU loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. In our experiment bandit feedback is obtained by evaluating BLEU on reference translations without revealing them to the algorithm. This can be thought of as a simulation of interactive machine translation where an SMT system is personalized by a user who provides single point feedback to predicted translations. Our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning.", "creator": "LaTeX with hyperref package"}}}