{"id": "1601.06602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection", "abstract": "we present a possibly novel algorithm for anomaly detection on very large data datasets and data streams. the method, named expected similarity estimation ( expose ), is kernel - based and able to efficiently compute the similarity between new data points and the distribution sequence of regular occurrence data. the estimator is formulated as an inner product mechanism with a reproducing kernel hilbert space embedding information and makes no assumption about controlling the type or block shape of the underlying data distribution. we cannot show that efficiently offline ( batch ) swarm learning with expose can be consciously done in linear continuous time and online ( incremental ) learning takes constant time per instance and model update. furthermore, expose tools can make predictions in constant time, while learning it requires only constant memory. in addition and we propose different methodologies for concept drift adaptation on evolving data quality streams. on several interacting real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while never being significant faster than techniques with the same discriminant power.", "histories": [["v1", "Mon, 25 Jan 2016 13:56:59 GMT  (3428kb,D)", "https://arxiv.org/abs/1601.06602v1", null], ["v2", "Mon, 18 Apr 2016 12:37:33 GMT  (3431kb,D)", "http://arxiv.org/abs/1601.06602v2", null], ["v3", "Mon, 6 Jun 2016 13:48:17 GMT  (3431kb,D)", "http://arxiv.org/abs/1601.06602v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["markus schneider", "wolfgang ertel", "fabio ramos"], "accepted": false, "id": "1601.06602"}, "pdf": {"name": "1601.06602.pdf", "metadata": {"source": "CRF", "title": "EXPECTED SIMILARITY ESTIMATION FOR LARGE-SCALE BATCH AND STREAMING ANOMALY DETECTION", "authors": ["markus schneider\u03b1", "wolfgang ertel"], "emails": [], "sections": [{"heading": null, "text": "We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (EXPoSE), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with EXPoSE can be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, EXPoSE can make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches."}, {"heading": "1 Introduction", "text": "What is an anomaly? An anomaly is an element whose properties differ from the majority of other elements under consideration which are called the normal data. \u201cAnomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. These non-conforming patterns are often referred to as anomalies [. . .]\u201d (Chandola et al, 2009).\nTypical applications of anomaly detection are network intrusion detection, credit card fraud detection, medical diagnosis and failure detection in industrial environments. For example, systems which detect unusual network behavior can be used to complement or replace traditional intrusion detection methods which are based on experts\u2019 knowledge in order to defeat the increasing number of attacks on computer based networks (Kumar, 2005). Credit card transactions which differ significantly from the usual shopping behavior of the card owner can indicate that the credit card was stolen or a compromise of data associated with the account occurred (Aleskerov et al,\n\u03b1 Institute of Neural Information Processing, University of Ulm, Germany \u03b2 Institute for Artificial Intelligence, Ravensburg-Weingarten University of Applied Sciences, Germany \u03b3 School of Information Technologies, The University of Sydney, Australia This is an extended and revised version of a preliminary conference report that was presented in the International\nJoint Conference on Neural Networks 2015 (Schneider et al, 2015). The final publication is available at Springer via http://dx.doi.org/10.1007/s10994-016-5567-7\nar X\niv :1\n60 1.\n06 60\n2v 3\n[ cs\n.L G\n] 6\nJ un\n2 01\n1997). The diagnosis of radiographs can be supported by automated systems to detect breast cancers in mammographic image analysis (Spence et al, 2001). Unplanned downtime of production lines caused by failing components is a serious concern in many industrial environments. Here anomaly detection can be used to detect unusual sensor information to predict possible faults and enabling condition-based maintenance (Zhang et al, 2011). Novelty detection can be used to detect new interesting or unusual galaxies in astronomical data such as the Sloan Digital Sky Survey (Xiong et al, 2011).\nObtaining labeled training data for all types of anomalies is often too expensive. Imagine the labeling has to be done by a human expert or is obtained through costly experiments (Hodge and Austin, 2004). In some applications anomalies are also very rare as in air traffic safety or space missions. Hence, the problem of anomaly detection is typically unsupervised, however it is implicitly assumed that the dataset contains only very few anomalies. This assumption is reasonable since it is quite often possible to collect large amounts of data for the normal state of a system as, for example usual credit card transactions or network traffic of a system not under attack.\nThe computational complexity and memory requirements of classical algorithms become the limiting factor when applied to large-scale datasets as they occur nowadays. To solve this problem we propose a new anomaly detection algorithm called EXPected Similarity Estimation (EXPoSE). As explained later in detail, the EXPoSE anomaly detection classifier\n\u03b7(z) = \u3008\u03c6(z),\u00b5[P]\u3009\ncalculates a score (the likelihood of z belonging to the class of normal data) using the inner product between a feature map \u03c6 and the kernel mean map \u00b5[P] of the distribution of normal data P. We will show that this inner product can be evaluated in constant time, while \u00b5[P] can be estimated in linear time, has constant memory consumption and is designed to solve very large-scale anomaly detection problems.\nMoreover, we will see that the proposed EXPoSE classifier can be learned incrementally making it applicable to online and streaming anomaly detection problems. Learning on data streams directly is unavoidable in many applications such as network traffic monitoring, video surveillance and document feeds as data arrives continuously in fast streams with a volume too large or impractical to store.\nOnly a few anomaly detection algorithms can be applied to large-scale problems and even less are applicable to streaming data. The proposed EXPoSE anomaly detector fills this gap.\nour main contributions are:\n\u2022 We present an efficient anomaly detection algorithm, called EXPected Similarity Estimation (EXPoSE), with O(n) training time, O(1) prediction time and only O(1) memory requirements with respect to the dataset size n.\n\u2022 We show that EXPoSE is especially suitable for parallel and distributed processing which makes it scalable to very large problems.\n\u2022 We demonstrate how EXPoSE can be applied to online and streaming anomaly detection, while requiring only O(1) time for a model update, O(1) time per prediction and O(1) memory.\n\u2022 We introduce two different approaches which allow EXPoSE to be efficiently used with the most common techniques for concept drift adaptation.\n\u2022 We evaluate EXPoSE on several real datasets, including surveillance, image data and network intrusion detection.\nThis paper is organised as follows: We first provide a formal problem description including a definition of batch and streaming anomaly detection. Section 3 provides an overview of related work and a comparison of these techniques. Section 4 introduces the EXPoSE anomaly detection algorithm along with the necessary theoretical framework. Subsequently we show in Section 5 how EXPoSE can be applied to streaming anomaly detection problems. The key to EXPoSE s computational performance is subject to Section 6. In Section 7 we empirically compare EXPoSE with several state of the art anomaly detectors."}, {"heading": "2 Problem Definition", "text": "Even though there is a vast amount of literature on anomaly detection, there is no unique definition of what anomalies are and what exactly anomaly detection is. In this section we will state the problem of anomaly detection in batch and streaming applications.\nDefinition 1 (Input Space): The input space for an observation X is a measurable space1 (X, X ) containing all values that X might take. We denote the realization after measurement of the random variable X with X = x. \u00ab\nWe make no assumptions about the nature of the input space X which can consist of simple numerical vectors, but also can contain images, video data or trajectories of vehicles and people. We assume that there is a true (but unknown) distribution PX : X \u2192 [0, 1] of the data. Definition 2 (Output/Label Space): In anomaly detection an observation X = x can belong to the class of normal data cn or can be an anomaly ca. This is called label of the observation and denoted by the random variable Y. The collection of all labels is given by the measurable space (Y, Y ) called label space or output space (Fig. 2). \u00ab\nThe distribution of the observation x \u2208 X is stochastic and depends on the label Y and hence is distributed according to PX|Y .\nDefinition 3 (Prediction/Decision Space): Based on the outcome X = x of an observation, the objective of an anomaly detection algorithm is to make a prediction \u03d1 \u2208 Q, where the measurable space (Q, Q) is called the prediction space or sometimes decision space. \u00ab\n1A measurable space is a tuple (X, X ), where X is a nonempty set and X is a \u03c3-algebra of its subsets. We refer the reader unfamiliar with this topic to Kallenberg (2006) for an overview.\nThe prediction space Q is not necessarily equal to label space Y. Especially in anomaly detection and classification many algorithms calculate a probability or a score for a label. Such a score is called anomaly score if it quantifies the likelihood of x belonging to ca and normal score if it determines the degree of certainty to which x belongs to cn.\nScoring based algorithms are more flexible than techniques which assign hard class labels since anomalies can be ranked and prioritized according their score or a domain specific discrimination threshold can be applied to separate anomalies from normal data. For example we can define a mapping \u03c4 : (Q, Q)\u2192 (Y, Y ) as\n\u03c4\u03b8(\u03d1) = { cn if \u03d1 > \u03b8 ca else\nbased on the threshold \u03b8. Such a domain specific threshold depends on the costs of false positives (an anomaly is reported when the observation is normal) and false negatives (no anomaly is reported when the observation is anomalous).\nDefinition 4 (Classifier/Predictor): A measurable function \u03b7 : (X, X )\u2192 (Q, Q) is called a classifier or predictor. \u00ab\nA classifier calculates a prediction for an observation X = x. In the context of anomaly detection our goal is to find a good predictor which can distinguish normal from anomalous data. However, the distribution PX \u2297PY is typically unknown and hence we have to build a classifier solely based on observations. The estimation of such a functional relationship between the input space X and the prediction space Q is called learning or training.\nDefinition 5 (Batch Anomaly Detection): In unsupervised batch learning we have access to n \u2208 N unlabeled independent realizations (x1, . . . , xn), identically distributed according to \u2297ni=1PX which form a training set. In anomaly detection we make the following assumptions:\n\u2022 The objective is to estimate a predictor \u03b7 based on an unlabeled training set.\n\u2022 The training set contains mostly normal instances from PX|Y=cn and only a few anomalies as, by definition, anomalies are rare events.\n\u2022 It is assumed that the algorithm has complete access to all n elements of the dataset at once.\n\u2022 We may have access to a small labeled fraction of the training data to configure our algorithm. \u00ab\nDefinition 6 (Online & Streaming Anomaly Detection): In contrast to batch learning, where the full dataset (x1, . . . , xn) is permanently available, online learning algorithms observe each xt, (1 6 t 6 n) only once and in a sequential order. Typically these algorithms have limited memory and thus can only store a small set of previously observed samples. Hence it is necessary to continuously update the prediction function based on the new observations\n(xt,\u03b7t) 7\u2192 \u03b7t+1\nto build a new predictor. This can be generalized to streaming anomaly detection where data arrives in a possible infinite sequence x1, x2, x3, . . . of observations. Moreover the input and label space distributions may evolve over time, a problem known as concept drift. (Formally we now have to consider the stochastic processes {Xt}t\u2208N, {Yt}t\u2208N and their corresponding distributions). It is therefore necessary that an algorithm can adapt to the changes e.g. by forgetting outdated information while incorporating new knowledge (Gama et al, 2014). These classes of algorithms assume that more recent observations carry more relevant information than older data. In summary streaming anomaly detection has the following key characteristics:\n\u2022 The data stream is possible infinite which requires the algorithm to learn incrementally since it is not possible to store the whole stream.\n\u2022 Most instances in the data stream belong to the class of normal data and anomalies are rare.\n\u2022 The stream can evolve over time, forcing algorithms to adapt to changes in the data distribution.\n\u2022 Only a small time frame at the beginning of the stream is available to configure the algorithm\u2019s parameter.\n\u2022 We have to instantly make a prediction \u03b7t(xt) as soon as an observation xt is available. This requires that predictions can be made fast. \u00ab"}, {"heading": "3 Related Work", "text": "Many approaches from statistics and machine learning can be used for anomaly detection (Chandola et al, 2009; Gupta et al, 2014), but only a few are applicable on high-dimensional, large-scale problems, where a vast amount of information has to be processed. We review several algorithms with focus on their computational complexity and memory requirements."}, {"heading": "3.1 DISTRIBUTION BASED MODELS", "text": "One of the oldest, statistical methods for anomaly detection is the kernel (or Parzen) density estimator (kde). With O(n) time for predictions the kde is too slow for large amounts of data and known to be problematic in the case of increasing data dimensionality (Gretton et al, 2012). Fitting parametric distributions such as the Normal, Gamma, etc. is problematic since in general, the underlying data distribution is unknown. Therefore, a mixture of Gaussians is often used as a surrogate for the true distribution as, for example, done by SmartSifter (Yamanishi et al, 2004). SmartSifter can handle multivariate data with both, continuous and categorical observations. The\nmain disadvantage of this approach is the high number of parameters required for the mixture model which grows quadratically with the dimension (Tax, 2001)."}, {"heading": "3.2 DISTANCE BASED MODELS", "text": "Distance based models are popular since most of them are easy to implement and interpret. Knorr et al. (Knorr and Ng, 1998; Knorr et al, 2000) labels an observation as a distance based outlier (anomaly) if at least a fraction of points in the dataset have a distance of more than a threshold (based on the fraction) to this point. The authors proposed two simple algorithms which have both O(n2) runtime and a cell-based version which runs linear in n, but exponential with the dimension d. Ramaswamy et al (2000) argues that the threshold can be difficult to determine and proposes an outlier score which is simply the distance from a query point to its kth nearest neighbor. The algorithm is called KNNOutlier and suffers from the problem of efficient nearest neighbor search. If the input space is of low dimension and n is much larger than 2d then finding 1 nearest neighbor in a k-d tree with randomly distributed points takes O(logn) time on average. However this does not hold in high dimensions, where such a tree is not better than an exhaustive search with O(n) (Goodman and O\u2019Rourke, 2004). Also the algorithm proposed by Ramaswamy et al. is only used to identify the top outliers in a given dataset. An alternative algorithm was proposed by Angiulli et al. (Angiulli and Pizzuti, 2002) using the sum of distances from its k-nearest neighbors. Ott et al (2014) simultaneously perform clustering and anomaly detection in an integer programming optimization task.\nPopular approaches from data mining for distance based novelty detection on streams are olindda (Spinosa et al, 2007) and its extension minas (Faria et al, 2013) which both represent normal data as a union of spheres obtained by clustering. This representation becomes problematic if data within one cluster exhibits high variance since then the decision boundary becomes too large to detect novelties. Both algorithms are designed to incorporate novel classes into their model of normal data and hence barely applicable to anomaly detection.\nThe STream OutlieR Miner (storm) (Angiulli and Fassetti, 2010, 2007) offers an efficient solution to the problem of distance-based outlier detection over windowed data streams using a new data structure called Indexed Stream Buffer. Continuous Outlier Detection (cod) (Kontaki et al, 2011) aims to further improve the efficiency of storm by reducing the number of range queries."}, {"heading": "3.3 DENSITY BASED MODELS", "text": "Nearest neighbor data description (Tax, 2001) approximates a local density while using only distances to its first neighbor. The algorithm is very simple and often used as a baseline. It is also relatively slow approaching O(n) per prediction. More sophisticated is the local density based approach called Local Outlier Factor (lof) (Breunig et al, 2000). It considers a point to be an anomaly if there are only relatively few other points in its neighborhood. lof was extended to work on data streams (Pokrajac, 2007), however both (the batch and incremental approach) are relatively slow with training time between O(n logn) and O(n2) and O(n) memory consumption.\nThe angle based outlier detection for high-dimensional data (abod) proposed by Kriegel and Zimek (Kriegel and Zimek, 2008) is able to outperform lof, however requires O(n2) time per prediction with the exact model and O(n+ k2) if the full dataset is replaced by the k-nearest neighbors of the query point (FastAbod)."}, {"heading": "3.4 CLASSIFICATION & TREE BASED MODELS", "text": "The One-class support vector machine (oc-svm) (Sch\u00f6lkopf et al, 2001; Tax and Duin, 2004) is a kernel based method which attempts to find a hyperplane such that most of the observations are separated from the origin with maximum margin. This approach does not scale very well to large datasets where predictions have to be made with high frequency. As, Steinwart (2003) showed that the number of support vectors can grow linearly in the size of the dataset. There exist One-class support vector machines which can be learned incrementally (Gretton and Desobry, 2003).\nHoeffding Trees (Domingos and Hulten, 2000) are anytime decision trees to mine high-speed data streams. The Hoeffding Trees algorithm is not applicable to solve the unsupervised anomaly detection problem considered in this work since it requires the availability of class labels. Streaming Half-Space-Trees (hsta) (Tan et al, 2011) randomly construct a binary tree structure without any data. It selects a dimension at random and splits it in half. Each tree then counts the number of instances from the training set at each node referred to as \u201cmass\u201d. The score for a new instance is then proportional to the mass in the leaf in which new instance hits after passing down the tree. Obviously, an ensemble of such trees can be built-in constant time and the training is linear in n. However, randomly splitting a very high-dimensional space will not yield in a tree sufficiently fine-grained for anomaly detection. The RS-Forest (Wu et al, 2014) is a modification of hsta in which each dimension is not splitted in half, but at a random cut-point. Also the assumption that \u201c[. . .] once each instance is scored, streaming RS-Forest will receive the true label of the instance [. . .]\u201d (Wu et al, 2014) does not always hold. The Isolation Forest (iForest) is an algorithm which uses a tree structure to isolate instances (Liu et al, 2012). The anomaly score is based on the path length to an instance. iForests achieve a constant training time and space complexity by sub-sampling the training set to a fixed size. The characteristics of the most relevant anomaly detection algorithms is summarized in Table 1. All complexities are given with respect to the dataset size n in high-dimensional spaces.\nMost methods discussed do not scale to very large problems since either the training time is non-linear with the number of samples or the time to make a single prediction increases with the dataset size (stream length). We now present a novel anomaly detection algorithm to overcome these problems."}, {"heading": "4 Expected Similarity Estimation", "text": "As before, let X be a random variable taking values in a measurable space (X, X ). We are primarily interested in the distribution of normal data PX|Y=cn for which we will simply use the shorthand notation P in the remainder of this work. Next we introduce some definitions which are necessary in the following.\nA Hilbert space (H, \u3008\u00b7, \u00b7\u3009) of functions f : X \u2192 R is said to be a reproducing kernel Hilbert space (rkhs) if the evaluation functional \u03b4\u0304x : f 7\u2192 f(x) is continuous. A function k : X\u00d7X\u2192 R which satisfies the reproducing property\n\u3008f,k(x, \u00b7)\u3009 = f(x) and in particular \u3008k(x, \u00b7),k(y, \u00b7)\u3009 = k(x,y)\nis called reproducing kernel ofH (Steinwart and Christmann, 2008).2 The map \u03c6 : X\u2192 H, \u03c6 : x 7\u2192 k(x, \u00b7) with the property that\nk(x,y) = \u3008\u03c6(x),\u03c6(y)\u3009\nis called feature map.\nThroughout this work we assume that the reproducing kernel Hilbert space (H, \u3008\u00b7, \u00b7\u3009) is separable such that \u03c6 is measurable. We therefore assume that the input space X is a separable topological space and the kernel k on X is continuous, which is sufficient for H to be separable (Steinwart and Christmann, 2008, Lemma 4.33).\nAs mentioned in the introduction, EXPoSE calculates a score which can be interpreted as the likelihood of an instance z \u2208 X belonging to the distribution of normal data P. It uses a kernel function k to measure the similarity between instances of the input space X.\nDefinition 7 (Expected Similarity Estimation): The expected similarity of z \u2208 X with respect to the (probability) distribution P is defined as\n\u03b7(z) = E [\u03c6(z)] = \u222b X k(z, x)dP(x),\nwhere k : X\u00d7X\u2192 R is a reproducing kernel. \u00ab Intuitively the query point z is compared to all other points of the distribution P. We will show that this equation can be rewritten as an inner product between the feature map \u03c6(z) and the kernel mean map \u00b5[P] of P. This reformulation is of central importance and will enable us to efficiently compute all quantities of interest. Given a reproducing kernel k, the kernel mean map can be used to embed a probability measure into a rkhs where it can be manipulated efficiently. It is defined as follows.\nDefinition 8 (Kernel Embedding): Let P be a Borel probability measure on X. The kernel embedding or kernel mean map \u00b5[P] of P is defined as\n\u00b5[P] = \u222b X k(x, \u00b7)dP(x),\nwhere k is the associated continuous, bounded and positive-definite kernel function.\u00ab\nWe assume that the kernel k is bounded in expectation i. e.\u222b X \u221a k(x, x)dP(x) <\u221e,\n2The notation k(x, \u00b7) indicates that the second function argument is not bound to a variable.\nsuch that \u00b5[P] exists for all Borel probability measures P (Sejdinovic et al, 2013, Page 8). This is a weaker assumption than k being bounded. We can now continue to formulate the central theorem of our work.\nTheorem 1: Let (H, \u3008\u00b7, \u00b7\u3009) be a rkhs with reproducing kernel k : X\u00d7 X \u2192 R. The expected similarity of z \u2208 X with respect to the distribution P can be expressed as\n\u03b7(z) = \u222b X k(z, x)dP(x)\n= \u3008\u03c6(z),\u00b5[P]\u3009,\nwhere \u00b5[P] is the kernel embedding of P. \u00ab\nThis reformulation has several desirable properties. At this point we see how the EXPoSE classifier can make prediction in constant time. After the kernel mean map \u00b5[P] of P is learned, EXPoSE only needs to calculate a single inner product in H to make a prediction. However there are some crucial aspects to consider i. e. in Hilbert spaces, integrals and continuous linear forms are not in general interchangeable. In the proof of Theorem 1 we will thus use the weak integral and show that it coincides with the strong integral.\nDefinition 9 (Strong Integral): Let (X, X , P) be a \u03c3-finite measure space and let \u03c6 : X\u2192 H be measurable. Then \u03c6 is strong integrable (Bochner integrable) over a set D \u2208X if and only if its norm \u2016\u03c6\u2016 is Lebesgue integrable over D, that is,\u222b\nD \u2016\u03c6\u2016dP(x) <\u221e. If \u03c6 is strong integrable over each D \u2208X we say that \u03c6 is strong integrable. (Aliprantis and Border, 2006, Theorem 11.44) \u00ab\nDefinition 10 (Weak Integral): Let (X, X , P) be a \u03c3-finite measure space. A function \u03c6 : X\u2192 H is weakly integrable over a set D \u2208X if there exists some \u03bb \u2208 H satisfying\n\u3008f, \u03bb\u3009 = \u222b D \u3008f,\u03c6(x)\u3009dP(x)\nfor each f \u2208 H. The weak integral is denoted by\n\u03bb = \u222e D \u03c6dP(x)\nand the unique element \u03bb \u2208 H is called weak integral of \u03c6 over D. If the integral exists for each D \u2208X we say that \u03c6 is weakly integrable. (Aliprantis and Border, 2006, Section 11.10) \u00ab\nA sufficient condition therefore is provided by the following lemma.\nLemma 1: If \u03c6 is strong (Bochner) integrable then \u03c6 is weak (Pettis) integrable and the two integrals coincide. (Aliprantis and Border, 2006, Theorem 11.50) \u00ab\nWe are now in the position to proof Theorem 1.\nProof of Theorem 1: By definition of the feature map \u03c6 we have\u222b X k(x, z)dP(x) = \u222b X \u3008\u03c6(z),\u03c6(x)\u3009dP(x)\nBy the assumption that k is bounded in expectation it follows that\u222b X \u2016\u03c6(x)\u2016dP(x) = \u222b X \u221a k(x, x)dP(x) <\u221e\nand therefore \u03c6 is strongly integrable and hence weakly integrable (Lemma 1). By definition of the weak integral we get for all z \u2208 X\u222b\nX\n\u3008\u03c6(z),\u03c6(x)\u3009dP(x) = \u2329 \u03c6(z), \u222e X \u03c6(x)dP(x) \u232a\n= \u2329 \u03c6(z), \u222b X \u03c6(x)dP(x) \u232a = \u3008\u03c6(z),\u00b5[P]\u3009\nfor all probability measures P.\nIn anomaly detection, we cannot assume to know the distribution of normal data P. However we assume to have access to n \u2208 N independent realizations (x1, . . . , xn) sampled from P. It is common in statistics to estimate P with the empirical distribution\nPn = 1\nn n\u2211 i=1 \u03b4xi ,\nwhere \u03b4x is the Dirac measure. The empirical distribution Pn can also be used to construct an approximation \u00b5[Pn] of \u00b5[P] as\n\u00b5[P] \u2248 \u00b5[Pn] = 1\nn n\u2211 i=1 \u03c6(xi)\nwhich is called empirical kernel embedding (Smola et al, 2007). This is an efficient estimate since it can be shown (Schneider, 2016) that under the assumption \u2016\u03c6(X)\u2016 6 c with c > 0 the difference between \u00b5[P] and \u00b5[Pn] is in probability\nP (\u2225\u2225\u00b5[P] \u2212 \u00b5[Pn]\u2225\u2225 > ) 6 2 exp(\u2212 n 2\n8c2 ) for all > 0.\nAs a consequence we can substitute \u00b5[P] with \u00b5[Pn] whenever the distribution P is not directly accessible yielding\n\u03b7(z) = \u3008\u03c6(z),\u00b5[Pn]\u3009\n= \u2329 \u03c6(z), 1\nn n\u2211 i=1 \u03c6(xi) \u232a\nas the (empirical) EXPoSE anomaly detector. The empirical kernel embedding \u00b5[Pn] is responsible for the linear training computational complexity of EXPoSE. We will call \u00b5[Pn] the EXPoSE model. One of the important observations is, that EXPoSE makes no assumption about the type or shape of the data distribution P as such assumption can be wrong, causing erroneous predictions. This is an advantage over other statistical approaches that try to approximate the distribution directly with parametric models."}, {"heading": "4.1 PARALLEL & DISTRIBUTED PROCESSING", "text": "Parallel and distributed data processing is the key to scalable machine learning algorithms. The formulation of EXPoSE as \u03b7(z) = \u3008\u03c6(z),\u00b5[Pn]\u3009 is especially appealing for this kind of operations. We can use a spmd (single program, multiple data) technique to achieve parallelism. On of the first programming paradigms on this line is Google\u2019s MapReduce for processing large data sets on a cluster (Dean and Ghemawat, 2008).\nAssume a partition of the dataset (x1, . . . , xn) into m 6 n distinct collections s1, . . . , sm which can be distributed on different computational nodes. Obviously, the feature map \u03c6 can be applied in parallel to all instances in x1, . . . , xn. We also note that the partial sums\np(si) = \u2211 x\u2208si \u03c6(x)\ncan be calculated without any communication or data-sharing across concurrent computations. Solely the partial sums p(si), which are elements of H, need to be transmitted and combined as\n\u00b5[Pn] = 1\nn m\u2211 i=1 p(si)\nby a central processing node.\nSUMMARY\nIn this section we derived the EXPoSE anomaly detection algorithm. We showed how EXPoSE can be expressed as an inner product \u3008\u03c6(z),\u00b5[Pn]\u3009 between the kernel mean map of P and the feature mapping of a query point z \u2208 X for which we need to make a prediction. Evaluating this inner product takes constant time while estimating the model \u00b5[Pn] can be done in linear time and with constant memory. We will explain the calculation of \u03c6 in more detail in Section 6 and will explore now how EXPoSE can be learned incrementally and applied to large-scale data streams."}, {"heading": "5 Online & Streaming EXPoSE", "text": "In this section we will show how EXPoSE can be used for online and streaming anomaly detection. To recap, a data stream is an often infinite sequence of observations (x1, x2, x3, . . . ), where xt \u2208 X is the instance arriving at time t. A source of such data can be, for example, continuous sensor readings from an engine or a video stream from surveillance cameras.\nDomingos and Hulten (2001) identified the following requirement for algorithms operating on \u201cthe high-volume, open-ended data streams we see today\u201d.\n\u2022 Require small constant time per instance.\n\u2022 Use only a fixed amount of memory, independent of the number of past instances.\n\u2022 Build a model using at most one scan over the data.\n\u2022 Make a usable predictor available at any point in time.\n\u2022 Ability to deal with concept drift.\n\u2022 For streams without concept drift, produce a predictor that is equivalent (or nearly identical) to the one that would be obtained by an offline (batch) learning algorithm.\nIn this section we will show that the online version of EXPoSE fulfills all requirements, starting with the last item of the list.\nProposition 1: The EXPoSE model \u00b5[Pn] can be learned incrementally, where each model update can be performed in O(1) time and memory. \u00ab\nProof: Given a stream (x1, x2, x3, . . . ) of observations and let \u00b5[P1] = \u03c6(x1). Whenever a new observation xt is made at t > 1, the new model \u00b5[Pt] can be incrementally calculated as\n\u00b5[Pt] = 1\nt t\u2211 i=1 \u03c6(xi)\n= \u00b5[Pt\u22121] + 1\nt\n( \u03c6(xt) \u2212 \u00b5[Pt\u22121] ) using the previous model \u00b5[Pt\u22121].\nWe see that online learning of EXPoSE does neither increase the computational complexity nor the memory requirements of EXPoSE. We also emphasize that online learning yields the exact same model as the EXPoSE offline learning procedure."}, {"heading": "5.1 LEARNING ON EVOLVING DATA STREAMS", "text": "Sometimes it can be expected that the underlying distribution of the stream evolves over time. This is a property known as concept drift (Sadik and Gruenwald, 2014). For example in environmental monitoring, the definition of \u201cnormal temperature\u201d changes naturally with seasons. We can also expect that human behavior changes over time which requires us to redefine what anomalous actions are. In Fig. 3 we illustrate the difference between incremental learning as in Proposition 1 and a model which adapts itself to changes in the underlying distribution. In the following we will use wt to denote the EXPoSE model at time t since the equation wt = \u00b5[Pt] will not necessarily hold when concept drift adaptation is implemented.\nIn this work we are not concerned with the detection of concept drift (Gama, 2010), but we will show how EXPoSE can be used efficiently with the most common approaches to concept drift adaption which either utilize windowing or forgetting mechanisms (Gama et al, 2014)."}, {"heading": "5.1.1 WINDOWING", "text": "Windowing is a straight forward technique which uses a buffer (the window) of l \u2208N previous observations. Whenever a new observation is added to the window, the oldest one is discarded. We can efficiently implement windowing for EXPoSE as follows.\nProposition 2: Concept drift adaption on data streams using a sliding window mechanism can be implemented for EXPoSE with O(1) time and O(l) memory consumption, where l \u2208N is the window size. \u00ab Proof: Given a data stream (x1, x2, x3 . . . ) and the window size l. For t < l we set wt = 1 t \u2211t i=1 and use the incremental update\nwt = 1\nl t\u2211 i=t\u2212l+1 \u03c6(xi)\n= wt\u22121 + 1\nl \u03c6(xt) \u2212\n1 l \u03c6(xt\u2212l),\nwhenever t > l.\nThe downside of a sliding window mechanism is the requirement to keep the past l \u2208N events in memory. Also the sudden discard of a data point can lead to abrupt changes in predictions of the classifier which is sometimes not desirable. Another question is how to choose the correct window size. A shorter sliding window allows the algorithm to react faster to changes and requires less memory though the available data might not be representative or noise has too much negative impact. On the other hand a wider window may take too long to adapt to concept drift. The window size is therefore often dynamically adjusted (Widmer and Kubat, 1996) or multiple competing windows are used (Lazarescu et al, 2004)."}, {"heading": "5.1.2 GRADUAL FORGETTING (DECAY)", "text": "The problems of sliding window approaches can be avoided if a forgetting mechanism is applied, where the influence of older data gradually vanishes. Typically a parameter can be used to control the tradeoff between fast adaptation to new observations and robustness against noise in the data. We can realize such a forgetting mechanism for EXPoSE by replacing the factor 1t in Proposition 1 by a constant \u03b3 \u2208 [0, 1) yielding\nwt = { \u03c6(xt) for t = 1 \u03b3\u03c6(xt) + (1\u2212 \u03b3)wt\u22121, for t > 1\nwhere, with \u03b3 = 0, no new observations are integrated into the model. This operation can be performed in constant time as summarized in the next proposition.\nProposition 3: Concept drift adaptation on data streams using a forgetting mechanism can be implemented for EXPoSE in O(1) time and memory. \u00ab\nProof: This is a direct consequence from Proposition 1.\nIn general, weighting with a fixed \u03b3 or using a static window size is called blind adaptation since the model does not utilize information about changes in the environment (Gama, 2010). The alternative is informed adaptation where one could, for example, use an external change detector (Gama et al, 2014) and weight new samples more if a concept drift was detected. We could also apply more sophisticated decay rules making \u03b3 a function of t or xt.\nA summary of characteristics for each proposed online learning variant of EXPoSE is listed in Table 2 and a general discussion can be found in literature, e.g. the work of Gama (2010)."}, {"heading": "5.2 PREDICTIONS ON DATA STREAMS", "text": "We introduced three different approaches to learn the model for EXPoSE on data streams. One incremental (online) learning approach and two evolving techniques. In order to make a prediction as a new observation is made we have to normalize the calculated predicted score. This is necessary as the score would continuously change, even if exactly the same data would be observed again. This problem is not present in the batch version of EXPoSE since the model does not change anymore at the time we make predictions. To avoid this problem we divide by the total volume\u222b\nX \u222b X k(x,y)dP(x)dP(y) = \u3008\u00b5[P],\u00b5[P]\u3009\n\u2248 \u3008wt,wt\u3009\nyielding\n\u03b7(z) = \u3008\u03c6(z),wt\u3009 \u2016wt\u20162 ,\nas the EXPoSE classifier. We emphasize that the calculation of the normalization constant does not change the limiting behavior of runtime and memory we derived earlier in this section since we have constant time access to wt anyway."}, {"heading": "6 Approximate Feature Maps", "text": "We showed in the previous part how EXPoSE can be expressed as an inner product \u3008\u03c6(z),\u00b5[Pn]\u3009 between the kernel mean map and the feature map of a query point z \u2208 X and derived a similar expression for the incremental and streaming variants of EXPoSE. However, the feature map \u03c6 (and hence \u00b5[Pn]) can not always be calculated explicitly as \u03c6(z) = k(\u00b7, z). One possible solution is to resort to approximate feature maps which we review in this section. The key idea behind approximate feature maps\nis to find a function \u03c6\u0302 such that\nk(x, z) \u2248 \u3008\u03c6\u0302(x), \u03c6\u0302(z)\u3009\nand \u03c6\u0302(x) \u2208 Rr for some r \u2208N. We will see, that this can be done efficiently."}, {"heading": "6.1 RANDOM KITCHEN SINKS", "text": "A way to efficiently create a feature map \u03c6 from a kernel k is known as random kitchen sinks (Rahimi and Recht, 2007, 2008). The random kitchen sinks (rks) approximation is based on Bochner\u2019s theorem for translation invariant kernels (such as Laplace, Mat\u00e9rn, Gaussian rbf, etc.) and states that such a kernel can be represented as\nk(x,y) = \u222b z \u03c6?z(x)\u03c6z(y)\u03bb(z) with \u03c6z(x) = e i\u3008z,x\u3009,\nwhere \u03c6? is the conjugate transpose of \u03c6. A Monte Carlo approximation of this integral can then be used to estimate the expression above as\nk(x,y) \u2248 1 r r\u2211 i=1 \u03c6?zi(x)\u03c6zi(y) = \u3008\u03c6\u0302(x), \u03c6\u0302(y)\u3009 with zi \u223c \u03bb.\nFor kernels such as the Gaussian rbf k(x,y) = exp(\u221212\u2016x\u2212 y\u2016 2/\u03c32), the measure \u03bb can be found with the help of the inverse Fourier transform yielding\n\u03c6\u0302(x) = 1\u221a r exp(iZx) with Zij \u223c N(0,\u03c32) and Z \u2208 Rr\u00d7d\nwhere d is the input space dimension. The parameter r \u2208N determines the number of kernel expansions and is typically around 20,000. Larger r result in better kernel approximations as the Monte Carlo approach becomes more accurate (Fig. 4). Recently Le et al (2013) proposed an approximation of Z such that the product Zx can be calculated in O(r logd) time complexity while requiring only O(r) storage."}, {"heading": "6.2 NYSTR\u00d6M\u2019S APPROXIMATION", "text": "An alternative to random kitchen sinks are Nystr\u00f6m methods (Williams and Seeger, 2001) which project the data into a subspace Hr \u2282 H spanned by r 6 n randomly chosen elements \u03c6(x1), . . . ,\u03c6(xr).\nThe Nystr\u00f6m feature map \u03c6\u0302 is then given by \u03c6\u0302(x) = (\u03c6\u03021(x), . . . , \u03c6\u0302r(x)) with\n\u03c6\u0302i(x) = 1\u221a \u03bbi r\u2211 j=1 ujik(xj, x), 1 6 i 6 r,\nwhere \u03bbi and ui denote the i-th eigenvalue and the i-th eigenvector of kernel matrix K \u2208 Rr\u00d7r with Ki,j = k(xi, xj). The Nystr\u00f6m approximation needs in general less basis functions, r, than the rks approach (typically around 1000). However the approximation is data dependent and hence becomes erroneous if the underlying distribution changes or when we are not able to get independent samples from the dataset. This is a problem for online learning and streaming applications with concept drift. We therefore suggest to avoid the Nystr\u00f6m feature map in this context.\nRandom kitchen sinks and the Nystr\u00f6m approximation the most common feature map approximations. We refer to the corresponding literature for a discussion of other approximate feature maps such as (Li et al, 2010; Vedaldi and Zisserman, 2012; Kar and Karnick, 2012), which can be used as well for EXPoSE."}, {"heading": "6.3 EXPoSE & APPROXIMATE FEATURE MAPS", "text": "Recall from the previous sections that EXPoSE uses the inner product \u3008\u03c6(z),\u00b5[Pn]\u3009 to calculate the score and make predictions. Using an approximate feature map \u03c6\u0302, it is now possible to explicitly represent the feature function and consequently also the mean map \u00b5[Pn] as\n\u03b7(z) \u2248 \u2329 \u03c6\u0302(z), 1\nn n\u2211 i=1 \u03c6\u0302(xi) \u232a\n(1)\nfor the EXPoSE classifier.\nWe emphasize that with an efficient approximation of \u03c6, as showed here, the training time of this algorithm is linear in the number of samples n and an evaluation of \u03b7(z) for predictions takes only constant time. Moreover we need only O(r) memory to store the model which is also independent of n and the input dimension d."}, {"heading": "7 Experimental Evaluation", "text": "In this section we show in several experiments how EXPoSE compares to other state of the art anomaly detection techniques in prediction and runtime performances. We first explain which statistical test are used to compare the investigated algorithms."}, {"heading": "7.1 STATISTICAL COMPARISON OF ALGORITHMS", "text": "When comparing multiple (anomaly detection) algorithms over multiple datasets one cannot simply compare the raw numbers obtained from the area under receiver\noperating characteristic (auc) or precision-recall curves. Webb (2000) warns against averaging these numbers: \u201cIt is debatable whether error rates in different domains are commensurable, and hence whether averaging error rates across domains is very meaningful\u201d (Webb, 2000).\nAs Dem\u0161ar (2006) points out, it is also dangerous to use tests which are designed to compare a pair of algorithms for more than two: \u201cA common example of such questionable procedure would be comparing seven algorithms by conducting all 21 paired t-tests [. . .]. When so many tests are made, a certain proportion of the null hypotheses is rejected due to random chance, so listing them makes little sense.\u201d (Dem\u0161ar, 2006)\nDem\u0161ar suggests to use the Friedman test with the corresponding post-hoc Nemenyi test for comparison of more classifiers over multiple data sets. A methodology we summarize in the following."}, {"heading": "7.1.1 THE FRIEDMAN TEST", "text": "The Friedman test (Friedman, 1937) is a non-parametric statistical test which ranks algorithms for each dataset individually starting from 1 as the best rank. Its purpose is to examine whether there is a significant difference between the performances of the individual algorithms. Lets assume we compare k algorithms on m datasets and let rij be the rank of the j-th algorithm on the i-th dataset. We use r\u0304j to denote the average rank of algorithm j given by r\u0304j = m\u22121 \u2211 i rij. The Friedman statistic\n\u03c72F = 12m\nk(k+ 1) ( k\u2211 j=1 r\u03042j \u2212 k(k+ 1)2 4 )\nis undesirably conservative and therefore Iman and Davenport (1980) suggest to use\nFF = (m\u2212 1)\u03c72F\nm(k\u2212 1) \u2212 \u03c72F\nwhich is distributed according to the F-distribution with k \u2212 1 and (k\u2212 1)(m\u2212 1) degrees of freedom. If the null-hypothesis (all algorithms are equivalent) is rejected one can proceed with a post-hoc test."}, {"heading": "7.1.2 THE NEMENYI TEST", "text": "The Nemenyi test (Nemenyi, 1963) is a post-hoc test to compare all (anomaly detection) algorithms with each other. Hereby the performance of two algorithms is significantly different if their average ranks differ by at least\ncd = q\u03b1\n\u221a k(k+ 1)\n6m ,\ncalled the critical difference. Here q\u03b1 is the Studentised range statistic divided by \u221a 2.\nDem\u0161ar (2006) also suggests to visually represent the results of the Nemenyi test in a critical difference diagram as in Fig. 5. In this diagram we compare 5 algorithms on 20 datasets against each other. Algorithms not connected by a bar have a significantly different performance."}, {"heading": "7.2 BATCH ANOMALY DETECTION", "text": "The aim of this experiment is to compare EXPoSE against iForest, oc-svm, lof, kde and FastAbod in terms of anomaly detection performance and processing time in a learning task without concept drift. In order to be comparable, we follow Liu et al (2012) and perform an outlier selection task with the objective to identify anomalies in a given dataset."}, {"heading": "7.2.1 DATASETS", "text": "For performance analysis and evaluation we take the following datasets which are often used in literature for comparison of anomaly detection algorithms as for example in (Sch\u00f6lkopf et al, 2001; Tax and Duin, 2004; Liu et al, 2012). We use several smaller benchmark datasets with known anomaly classes such as Ionosphere, Arrhythmia, Pima, Satellite, Shuttle, (Lichman, 2013), Biomed and Wisconsin Breast Cancer (Breastw) (Tax and Duin, 2004). These datasets are set up as described in (Liu et al, 2012) where all nominal and binary attributes are removed.\nThe larger datasets are the Kdd Cup 99 network intrusion data (KddCup) and Forest Cover Type (ForestCover). For KddCup instances we follow the setup of (Yu et al, 2003) and obtain a total of 127 attributes. Furthermore, we add two high-dimensional image datasets mnist and the Google Street View House Numbers (svhn) (Netzer et al, 2011). We use the scaled version of mnist (Chang and Lin, 2011) and create hog features (Vondrick et al, 2013) for svhn. The methodology suggested by (Sch\u00f6lkopf et al, 2001; Tax, 2001) is used to create anomaly detection datasets from mnist and svhn in the following way. We take all images of digit 1 from mnist as normal instances. The images of the remaining digits (2, 3, . . . , 9) are used as anomalies. We then create a dataset comprising all normal instances and a random subset anomalies such that anomalies account for 1% of the elements in the set. We repeat this process for mnist images of digits 2, 3, . . . , 9 and do the same with the 9 digit classes of svhn to create 18 anomaly detection datasets. The subset of anomalies is independently sampled for each repetition of an experiment. Table 3 provides an overview of the dataset properties and how the anomaly classes are defined.\nIn the experiment we provide a dedicated labeled random subset of 1% or 2000 instances (whichever is smaller) to configure the algorithms parameters. We emphasize that this subset is not used to evaluate the predictive performance. The parameter configuration is done by a pattern search (Torczon, 1997) using cross-validation. Examples of parameters being optimized are the number of nearest neighbors in lof and FastAbod, the kernel bandwidth of EXPoSE, kde and oc-svm or the number of trees for iForest. We do not optimize over different distance metrics and various kernels functions, but use the most common Euclidean distance and squared exponential\nkernel, respectively. However, we remark that the choice of these functions pose a possibility to include domain and expert knowledge into the system. Each experiment is repeated 5 times and their auc scores are used to perform the Friedman test. If not stated otherwise we use EXPoSE in combination with Nystr\u00f6m\u2019s approximation for batch anomaly detection and random kitchen sinks in the streaming experiments as discussed in Section 6."}, {"heading": "7.2.2 EVALUATION", "text": "The average scores for each experiment are reported in Table 4, whereas the runtimes are provided in Table 5. Some algorithms failed on the larger datasets. For example lof was not able to process the KddCup dataset due to the high memory requirements of the tree data structure. However the advantage of a tree data structure for nearest neighbor lookup in low dimensions can be seen when comparing the runtime of lof on ForestCover and mnist. Even though the ForestCover dataset has more than 2.5 times the size of mnist, it takes only a fraction of the time to be processed. This advantage vanishes in higher dimensions. kde and FastAbod exhibit a good anomaly detection performance on small datasets, however fail as soon as we apply them to medium-sized problems.\nWith the auc values we can perform the Friedman and post-hoc Nemenyi tests. The Friedman test confirms a statistical significant difference between the performances of the individual algorithms at a p-value of 0.05. From the critical difference diagram in Fig. 6 we observe that EXPoSE performs significant better than iForest, FastAbod and kde. While no significant difference in terms of anomaly detection between EXPoSE, oc-svm and lof can be confirmed, EXPoSE is several orders of magnitude faster on large-scale, high-dimensional datasets."}, {"heading": "7.3 STREAMING ANOMALY DETECTION", "text": "In this set of experiments we compare the streaming variants of EXPoSE against hsta, storm and cod. All of these algorithms are blind methods as they adapt their model at regular intervals without knowing if a concept drift occurred or not. They can be combined with a concept drift detector to make the adaptation informed (Gama, 2010).\nThe evaluation of streaming algorithms is not as straightforward as the rating of batch learning techniques. There are two accepted techniques proposed in literature.\n\u2022 Using a dedicated subset of the data (holdout) and evaluate the algorithm at regular time intervals. The holdout set must reflect the respective stream properties and therefore has to evolve with the stream in case of a concept drift.\n\u2022 Making a prediction as the instance becomes available (prequential)3. A performance metric can then be applied based on the prediction and the actual label of the instance. Since predictions are made on the stream directly there are no special actions which have to be taken in case of concept drift.\nIf possible, the holdout method is preferable since it is an unbiased risk estimator and we can use a balanced test set with the same number of normal instances and anomalies. This is a disadvantage of the prequential method since, by definition, the data stream contains only a few anomalies. This is problematic since storm and cod assign hard class labels and, in contrast to auc, the classification accuracy is highly sensitive to unbalanced data. We will therefore use the balanced accuracy defined as\n0.5 \u00b7 true positives true positives + false negatives + 0.5 \u00b7 true negatives true negatives + false positives ,\nwhich compensates the unequal class distribution."}, {"heading": "7.3.1 DATASETS", "text": "There exist only a few non-synthetic datasets for anomaly detection with concept drift. Most of them are based on multi-class datasets, where each class represents a single concept. For example we use the svhn dataset and stream 9000 randomly sampled instances of the digits 1 to 9 in sequence, such that the 1000 instances of digit 1 appear first, then 1000 instances of digit 2 until digit 9 (see Fig. 8). Every 25 time steps we calculate the accuracy using the holdout method for a dedicated random test set which contains 500 instances of the normal class and 500 instances of anomalies.\n3Prequential originates from predictive and sequential (Dawid, 1984).\nHere, the normal class is the digit which is streamed at time step t and anomalies are all other classes. Likewise we proceed with the Satellite and Shuttle datasets.\nSimilar, Ho (2005) proposed the three digit data stream (tdds) which contains four different concepts. Each concept consists of three digits of the usps handwritten digits dataset as described4. After all instances of concept 1 are processed, the stream switches to the second concept and so on until concept 4. We randomly induce 1% anomalies to each concept and use the prequential method for evaluation to calculate the balanced accuracy.\nAll datasets presented so far contain one or more sudden (abrupt) concept drifts. Bifet et al (2009) proposed a methodology to introduce a smooth (incremental) drift between two concepts. The instances of the concepts from two classes under consideration are sampled according to a Bernoulli distribution where the class probability smoothly changes from one class to the other according to a sigmoid function (Fig. 7). The concept drift occurs at t0 and w is the length of the drift interval. During this interval the instances of both concepts belong to the class of normal data. We apply this methodology to usps and create the smooth digit drift (sdd) dataset. We start with digit 1 and then smoothly change to digit 2 at t0 = 500 using w = 100. The next drift to digit 3 occurs at t0 = 1000 and we repeat this until digit 9. As before, we randomly add 1% anomalies to each concept and use the prequential method for evaluation. We summarized the dataset characteristics in Table 6."}, {"heading": "7.3.2 EVALUATION", "text": "In the following we will denote EXPoSE with a sliding window (Section 5.1.1) and EXPoSE with gradual forgetting (Section 5.1.2) by w-EXPoSE and \u03b3-EXPoSE, respectively.\nA sliding window of length 100 demonstrated to obey an appropriate trade off between drift adaptation and model accuracy. We therefore use this length for all algorithms and all datasets except \u03b3-EXPoSE. A change of the window length affects w-EXPoSE, cod, storm and hsta in the same way. This is not unexpected as the window size determines the number of instances available to the algorithm. The first 100 instances of each stream are used to configure algorithm parameters via cross-validation using pattern search.\nA detailed illustration of the svhn experiment is shown in Fig. 8. The predictive performance of all algorithms is relatively similar. It can be observed that, as the stream changes from one digit to another, the accuracy suddenly drops which indicates that the current model is not valid anymore. After a short period of time, the model adapts and the accuracy recovers. EXPoSE performs on average better than cod,\n4See (Ho, 2005) for a detailed description of the tdds dataset.\nstorm and hsta. A possible interpretation of this result is the sound foundation in probability theory of our approach. The suboptimal performance of hsta indicates the random binary trees constructed by hsta are not sufficiently fine-grained for this high-dimensional datasets. This interpretation is supported by the experiments with the low-dimensional Shuttle and Satellite data, where hsta performs better.\nThe average over all accuracies of the individual experiments can be found in Table 7. The only statistical significance (at p < 0.05) is observed between \u03b3-EXPoSE and cod. We could not confirm a significant difference between the other algorithms as illustrated in the critical difference diagram (Fig. 9).\nAlthough these results are promising we recommend to combine the techniques presented here with a concept drift detection technique to make informed model updates (Gama, 2010)."}, {"heading": "8 Conclusion", "text": "We proposed a new algorithm, EXPoSE, to perform anomaly detection on very large-scale datasets and streams with concept drift. Although anomaly detection is a problem of central importance in many applications, only a few algorithms are scalable to the vast amount of data we are often confronted with.\nThe EXPoSE anomaly detection classifier calculates a score (the likelihood of a query point belonging to the class of normal data) using the inner product between a feature map and the kernel embedding of probability measures. The kernel embedding technique provides an efficient way to work with probability measures without the necessity to make assumptions about the underlying distributions.\nDespite its simplicity EXPoSE obeys a linear computational complexity for learning and can make predictions in constant time while it requires only constant memory. When applied incrementally or online, a model update can also be performed in constant time. We demonstrated that EXPoSE can be used as an efficient anomaly detection algorithm with the same predictive performance as the best state of the art methods while being significant faster than techniques with the same discriminant power."}], "references": [{"title": "Cardwatch: A neural network based database mining system for credit card fraud detection", "author": ["E Aleskerov", "B Freisleben", "B Rao"], "venue": null, "citeRegEx": "Aleskerov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Aleskerov et al\\.", "year": 1997}, {"title": "Infinite dimensional analysis: a hitchhiker\u2019s guide", "author": ["CD Aliprantis", "K Border"], "venue": null, "citeRegEx": "Aliprantis and Border,? \\Q2006\\E", "shortCiteRegEx": "Aliprantis and Border", "year": 2006}, {"title": "Detecting distance-based outliers in streams of data", "author": ["F Angiulli", "F Fassetti"], "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Angiulli and Fassetti,? \\Q2007\\E", "shortCiteRegEx": "Angiulli and Fassetti", "year": 2007}, {"title": "Distance-based outlier queries in data streams: the novel task and algorithms. Data Mining and Knowledge Discovery", "author": ["F Angiulli", "F Fassetti"], "venue": null, "citeRegEx": "Angiulli and Fassetti,? \\Q2010\\E", "shortCiteRegEx": "Angiulli and Fassetti", "year": 2010}, {"title": "Fast outlier detection in high dimensional spaces", "author": ["F Angiulli", "C Pizzuti"], "venue": "PKDD, Springer,", "citeRegEx": "Angiulli and Pizzuti,? \\Q2002\\E", "shortCiteRegEx": "Angiulli and Pizzuti", "year": 2002}, {"title": "New ensemble methods for evolving data streams", "author": ["A Bifet", "G Holmes", "B Pfahringer", "R Kirkby", "R Gavald\u00e0"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bifet et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bifet et al\\.", "year": 2009}, {"title": "LOF: identifying density-based local outliers", "author": ["MM Breunig", "HP Kriegel", "RT Ng", "J Sander"], "venue": "ACM Sigmod Record, ACM,", "citeRegEx": "Breunig et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Breunig et al\\.", "year": 2000}, {"title": "Anomaly detection: A survey", "author": ["V Chandola", "A Banerjee", "V Kumar"], "venue": "ACM Computing Surveys", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "LIBSVM: A library for support vector machines", "author": ["CC Chang", "CJ Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Present position and potential developments: Some personal views: Statistical theory: The prequential approach", "author": ["AP Dawid"], "venue": "Journal of the Royal Statistical Society Series A (General) pp 278\u2013292,", "citeRegEx": "Dawid,? \\Q1984\\E", "shortCiteRegEx": "Dawid", "year": 1984}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J Dean", "S Ghemawat"], "venue": "Communications of the ACM 51(1):107\u2013113,", "citeRegEx": "Dean and Ghemawat,? \\Q2008\\E", "shortCiteRegEx": "Dean and Ghemawat", "year": 2008}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J Dem\u0161ar"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Dem\u0161ar,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar", "year": 2006}, {"title": "Mining high-speed data streams", "author": ["P Domingos", "G Hulten"], "venue": "Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Domingos and Hulten,? \\Q2000\\E", "shortCiteRegEx": "Domingos and Hulten", "year": 2000}, {"title": "Catching up with the Data: Research Issues in Mining Data Streams", "author": ["P Domingos", "G Hulten"], "venue": "DMKD", "citeRegEx": "Domingos and Hulten,? \\Q2001\\E", "shortCiteRegEx": "Domingos and Hulten", "year": 2001}, {"title": "Novelty detection algorithm for data streams multi-class problems", "author": ["ER Faria", "J Gama", "AC Carvalho"], "venue": "Proceedings of the 28th Annual ACM Symposium on Applied Computing,", "citeRegEx": "Faria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Faria et al\\.", "year": 2013}, {"title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "author": ["M Friedman"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Friedman,? \\Q1937\\E", "shortCiteRegEx": "Friedman", "year": 1937}, {"title": "Knowledge discovery from data streams", "author": ["J Gama"], "venue": null, "citeRegEx": "Gama,? \\Q2010\\E", "shortCiteRegEx": "Gama", "year": 2010}, {"title": "A (2014) A survey on concept drift adaptation", "author": ["J Gama", "I \u017dliobaite", "A Bifet", "M Pechenizkiy", "Bouchachia"], "venue": "ACM Computing Surveys (CSUR) 46(4):44,", "citeRegEx": "Gama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gama et al\\.", "year": 2014}, {"title": "Handbook of discrete and computational geometry, 2nd edn. CRC press", "author": ["JE Goodman", "J O\u2019Rourke"], "venue": null, "citeRegEx": "Goodman and O.Rourke,? \\Q2004\\E", "shortCiteRegEx": "Goodman and O.Rourke", "year": 2004}, {"title": "On-line one-class support vector machines. an application to signal segmentation", "author": ["A Gretton", "F Desobry"], "venue": "In: Acoustics, Speech, and Signal Processing,", "citeRegEx": "Gretton and Desobry,? \\Q2003\\E", "shortCiteRegEx": "Gretton and Desobry", "year": 2003}, {"title": "A (2012) A kernel twosample test", "author": ["A Gretton", "KM Borgwardt", "MJ Rasch", "B Sch\u00f6lkopf", "Smola"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Outlier Detection for Temporal Data: A Survey", "author": ["M Gupta", "J Gao", "CC Aggarwal", "J Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 26(9):2250\u20132267,", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "A martingale framework for concept change detection in time-varying data streams", "author": ["SS Ho"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Ho,? \\Q2005\\E", "shortCiteRegEx": "Ho", "year": 2005}, {"title": "A survey of outlier detection methodologies", "author": ["VJ Hodge", "J Austin"], "venue": "Artificial Intelligence Review 22(2):85\u2013126,", "citeRegEx": "Hodge and Austin,? \\Q2004\\E", "shortCiteRegEx": "Hodge and Austin", "year": 2004}, {"title": "Approximations of the critical region of the Friedman statistic. Communications in Statistics-Theory and Methods", "author": ["RL Iman", "JM Davenport"], "venue": null, "citeRegEx": "Iman and Davenport,? \\Q1980\\E", "shortCiteRegEx": "Iman and Davenport", "year": 1980}, {"title": "Random feature maps for dot product kernels", "author": ["P Kar", "H Karnick"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Kar and Karnick,? \\Q2012\\E", "shortCiteRegEx": "Kar and Karnick", "year": 2012}, {"title": "Algorithms for mining distance-based outliers in large datasets", "author": ["EM Knorr", "RT Ng"], "venue": "Proceedings of the International Conference on Very Large Data Bases,", "citeRegEx": "Knorr and Ng,? \\Q1998\\E", "shortCiteRegEx": "Knorr and Ng", "year": 1998}, {"title": "Distance-based outliers: algorithms and applications", "author": ["EM Knorr", "RT Ng", "V Tucakov"], "venue": "The VLDB Journal\u2014The International Journal on Very Large Data Bases", "citeRegEx": "Knorr et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Knorr et al\\.", "year": 2000}, {"title": "Continuous monitoring of distance-based outliers over data streams", "author": ["M Kontaki", "A Gounaris", "AN Papadopoulos", "K Tsichlas", "Y Manolopoulos"], "venue": "Data Engineering (ICDE),", "citeRegEx": "Kontaki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kontaki et al\\.", "year": 2011}, {"title": "Angle-based outlier detection in high-dimensional data", "author": ["HP Kriegel", "A Zimek"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kriegel and Zimek,? \\Q2008\\E", "shortCiteRegEx": "Kriegel and Zimek", "year": 2008}, {"title": "Parallel and distributed computing for cybersecurity", "author": ["V Kumar"], "venue": "IEEE Distributed Systems Online 6(10):1,", "citeRegEx": "Kumar,? \\Q2005\\E", "shortCiteRegEx": "Kumar", "year": 2005}, {"title": "Using multiple windows to track concept drift. Intelligent data analysis", "author": ["MM Lazarescu", "S Venkatesh", "HH Bui"], "venue": null, "citeRegEx": "Lazarescu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lazarescu et al\\.", "year": 2004}, {"title": "Fastfood: approximating kernel expansions in loglinear time", "author": ["Q Le", "T Sarl\u00f3s", "AJ Smola"], "venue": "Proceedings of the international conference on machine learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F Li", "C Ionescu", "C Sminchisescu"], "venue": "Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Isolation-based anomaly detection", "author": ["FT Liu", "KM Ting", "ZH Zhou"], "venue": "ACM Transactions on Knowledge Discovery from Data 6(1):3,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "A (2011) Reading digits in natural images with unsupervised feature learning", "author": ["Y Netzer", "T Wang", "A Coates", "A Bissacco", "B Wu", "Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On Integrated Clustering and Outlier Detection", "author": ["L Ott", "L Pang", "FT Ramos", "S Chawla"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Ott et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2014}, {"title": "Incremental local outlier detection for data streams", "author": ["D Pokrajac"], "venue": "IEEE Symposium on Computational Intelligence and Data Mining pp 504\u2013515,", "citeRegEx": "Pokrajac,? \\Q2007\\E", "shortCiteRegEx": "Pokrajac", "year": 2007}, {"title": "Random features for large-scale kernel machines. In: Advances in neural information processing", "author": ["A Rahimi", "B Recht"], "venue": null, "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A Rahimi", "B Recht"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rahimi and Recht,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "Efficient algorithms for mining outliers from large data", "author": ["S Ramaswamy", "R Rastogi", "K Shim"], "venue": "sets. In: ACM SIGMOD Record, ACM,", "citeRegEx": "Ramaswamy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ramaswamy et al\\.", "year": 2000}, {"title": "Research issues in outlier detection for data streams", "author": ["S Sadik", "L Gruenwald"], "venue": "ACM SIGKDD Explorations Newsletter 15(1):33\u201340,", "citeRegEx": "Sadik and Gruenwald,? \\Q2014\\E", "shortCiteRegEx": "Sadik and Gruenwald", "year": 2014}, {"title": "Probability Inequalities for Kernel Embeddings in Sampling without Replacement", "author": ["M Schneider"], "venue": "Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Schneider,? \\Q2016\\E", "shortCiteRegEx": "Schneider", "year": 2016}, {"title": "Expected Similarity Estimation for Large Scale Anomaly Detection", "author": ["M Schneider", "W Ertel", "G Palm"], "venue": "In: International Joint Conference on Neural Networks,", "citeRegEx": "Schneider et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 2015}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B Sch\u00f6lkopf", "JC Platt", "J Shawe-Taylor", "AJ Smola", "RC Williamson"], "venue": "Neural computation 13(7):1443\u20131471,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing", "author": ["D Sejdinovic", "B Sriperumbudur", "A Gretton", "K Fukumizu"], "venue": "The Annals of Statistics", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "A Hilbert space embedding for distributions", "author": ["AJ Smola", "A Gretton", "L Song", "B Sch\u00f6lkopf"], "venue": "Algorithmic Learning Theory, Springer,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Detection, synthesis and compression in mammographic image analysis with a hierarchical image probability model", "author": ["C Spence", "L Parra", "P Sajda"], "venue": "Mathematical Methods in Biomedical Image Analysis,", "citeRegEx": "Spence et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Spence et al\\.", "year": 2001}, {"title": "OLINDDA: a cluster-based approach for detecting novelty and concept drift in data streams", "author": ["EJ Spinosa", "AP de Leon F de Carvalho", "J Gama"], "venue": "Proceedings of the 2007 ACM symposium on Applied computing,", "citeRegEx": "Spinosa et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Spinosa et al\\.", "year": 2007}, {"title": "Sparseness of support vector machines", "author": ["I Steinwart"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Steinwart,? \\Q2003\\E", "shortCiteRegEx": "Steinwart", "year": 2003}, {"title": "Support vector machines", "author": ["I Steinwart", "A Christmann"], "venue": null, "citeRegEx": "Steinwart and Christmann,? \\Q2008\\E", "shortCiteRegEx": "Steinwart and Christmann", "year": 2008}, {"title": "Fast anomaly detection for streaming data", "author": ["SC Tan", "KM Ting", "TF Liu"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Citeseer,", "citeRegEx": "Tan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2011}, {"title": "On the convergence of pattern search algorithms", "author": ["V Torczon"], "venue": "SIAM Journal on optimization", "citeRegEx": "Torczon,? \\Q1997\\E", "shortCiteRegEx": "Torczon", "year": 1997}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A Vedaldi", "A Zisserman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34(3):480\u2013492,", "citeRegEx": "Vedaldi and Zisserman,? \\Q2012\\E", "shortCiteRegEx": "Vedaldi and Zisserman", "year": 2012}, {"title": "A (2013) Hoggles: Visualizing object detection features", "author": ["C Vondrick", "A Khosla", "T Malisiewicz", "Torralba"], "venue": "Computer Vision (ICCV),", "citeRegEx": "Vondrick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2013}, {"title": "Multiboosting: A technique for combining boosting and wagging. Machine learning", "author": ["GI Webb"], "venue": null, "citeRegEx": "Webb,? \\Q2000\\E", "shortCiteRegEx": "Webb", "year": 2000}, {"title": "Learning in the presence of concept drift and hidden contexts. Machine learning", "author": ["G Widmer", "M Kubat"], "venue": null, "citeRegEx": "Widmer and Kubat,? \\Q1996\\E", "shortCiteRegEx": "Widmer and Kubat", "year": 1996}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C Williams", "M Seeger"], "venue": "Proceedings of the 14th Annual Conference on Neural Information Processing Systems, MIT Press,", "citeRegEx": "Williams and Seeger,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "RS-Forest: A Rapid Density Estimator for Streaming Anomaly Detection", "author": ["K Wu", "K Zhang", "W Fan", "A Edwards", "PS Yu"], "venue": "Data Mining (ICDM),", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Hierarchical Probabilistic Models for Group Anomaly Detection", "author": ["L Xiong", "B Poczos", "J Schneider", "A Connolly", "J VanderPlas"], "venue": "AISTATS", "citeRegEx": "Xiong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2011}, {"title": "On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms. Data Mining and Knowledge Discovery", "author": ["K Yamanishi", "JI Takeuchi", "G Williams", "P Milne"], "venue": null, "citeRegEx": "Yamanishi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2004}, {"title": "Classifying large data sets using SVMs with hierarchical clusters", "author": ["H Yu", "J Yang", "J Han"], "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Yu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2003}, {"title": "A probabilistic fault detection approach: application to bearing fault detection", "author": ["B Zhang", "C Sconyers", "C Byington", "R Patrick", "ME Orchard", "G Vachtsevanos"], "venue": "IEEE Transactions on Industrial Electronics 58(5):2011\u20132018,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 30, "context": "For example, systems which detect unusual network behavior can be used to complement or replace traditional intrusion detection methods which are based on experts\u2019 knowledge in order to defeat the increasing number of attacks on computer based networks (Kumar, 2005).", "startOffset": 253, "endOffset": 266}, {"referenceID": 23, "context": "Imagine the labeling has to be done by a human expert or is obtained through costly experiments (Hodge and Austin, 2004).", "startOffset": 96, "endOffset": 120}, {"referenceID": 26, "context": "(Knorr and Ng, 1998; Knorr et al, 2000) labels an observation as a distance based outlier (anomaly) if at least a fraction of points in the dataset have a distance of more than a threshold (based on the fraction) to this point.", "startOffset": 0, "endOffset": 39}, {"referenceID": 18, "context": "However this does not hold in high dimensions, where such a tree is not better than an exhaustive search with O(n) (Goodman and O\u2019Rourke, 2004).", "startOffset": 115, "endOffset": 143}, {"referenceID": 4, "context": "(Angiulli and Pizzuti, 2002) using the sum of distances from its k-nearest neighbors.", "startOffset": 0, "endOffset": 28}, {"referenceID": 21, "context": "(Knorr and Ng, 1998; Knorr et al, 2000) labels an observation as a distance based outlier (anomaly) if at least a fraction of points in the dataset have a distance of more than a threshold (based on the fraction) to this point. The authors proposed two simple algorithms which have both O(n2) runtime and a cell-based version which runs linear in n, but exponential with the dimension d. Ramaswamy et al (2000) argues that the threshold can be difficult to determine and proposes an outlier score which is simply the distance from a query point to its kth nearest neighbor.", "startOffset": 1, "endOffset": 411}, {"referenceID": 2, "context": "(Angiulli and Pizzuti, 2002) using the sum of distances from its k-nearest neighbors. Ott et al (2014) simultaneously perform clustering and anomaly detection in an integer programming optimization task.", "startOffset": 1, "endOffset": 103}, {"referenceID": 37, "context": "lof was extended to work on data streams (Pokrajac, 2007), however both (the batch and incremental approach) are relatively slow with training time between O(n logn) and O(n2) and O(n) memory consumption.", "startOffset": 41, "endOffset": 57}, {"referenceID": 29, "context": "The angle based outlier detection for high-dimensional data (abod) proposed by Kriegel and Zimek (Kriegel and Zimek, 2008) is able to outperform lof, however requires O(n2) time per prediction with the exact model and O(n+ k2) if the full dataset is replaced by the k-nearest neighbors of the query point (FastAbod).", "startOffset": 97, "endOffset": 122}, {"referenceID": 19, "context": "There exist One-class support vector machines which can be learned incrementally (Gretton and Desobry, 2003).", "startOffset": 81, "endOffset": 108}, {"referenceID": 12, "context": "Hoeffding Trees (Domingos and Hulten, 2000) are anytime decision trees to mine high-speed data streams.", "startOffset": 16, "endOffset": 43}, {"referenceID": 45, "context": "As, Steinwart (2003) showed that the number of support vectors can grow linearly in the size of the dataset.", "startOffset": 4, "endOffset": 21}, {"referenceID": 50, "context": "A function k : X\u00d7X\u2192 R which satisfies the reproducing property \u3008f,k(x, \u00b7)\u3009 = f(x) and in particular \u3008k(x, \u00b7),k(y, \u00b7)\u3009 = k(x,y) is called reproducing kernel ofH (Steinwart and Christmann, 2008).", "startOffset": 160, "endOffset": 192}, {"referenceID": 42, "context": "This is an efficient estimate since it can be shown (Schneider, 2016) that under the assumption \u2016\u03c6(X)\u2016 6 c with c > 0 the difference between \u03bc[P] and \u03bc[Pn] is in probability P (\u2225\u2225\u03bc[P] \u2212 \u03bc[Pn]\u2225\u2225 > ) 6 2 exp(\u2212 n 2 8c2 )", "startOffset": 52, "endOffset": 69}, {"referenceID": 10, "context": "On of the first programming paradigms on this line is Google\u2019s MapReduce for processing large data sets on a cluster (Dean and Ghemawat, 2008).", "startOffset": 117, "endOffset": 142}, {"referenceID": 12, "context": "Domingos and Hulten (2001) identified the following requirement for algorithms operating on \u201cthe high-volume, open-ended data streams we see today\u201d.", "startOffset": 0, "endOffset": 27}, {"referenceID": 41, "context": "This is a property known as concept drift (Sadik and Gruenwald, 2014).", "startOffset": 42, "endOffset": 69}, {"referenceID": 16, "context": "In this work we are not concerned with the detection of concept drift (Gama, 2010), but we will show how EXPoSE can be used efficiently with the most common approaches to concept drift adaption which either utilize windowing or forgetting mechanisms (Gama et al, 2014).", "startOffset": 70, "endOffset": 82}, {"referenceID": 56, "context": "The window size is therefore often dynamically adjusted (Widmer and Kubat, 1996) or multiple competing windows are used (Lazarescu et al, 2004).", "startOffset": 56, "endOffset": 80}, {"referenceID": 16, "context": "In general, weighting with a fixed \u03b3 or using a static window size is called blind adaptation since the model does not utilize information about changes in the environment (Gama, 2010).", "startOffset": 172, "endOffset": 184}, {"referenceID": 16, "context": "the work of Gama (2010).", "startOffset": 12, "endOffset": 24}, {"referenceID": 57, "context": "An alternative to random kitchen sinks are Nystr\u00f6m methods (Williams and Seeger, 2001) which project the data into a subspace Hr \u2282 H spanned by r 6 n randomly chosen elements \u03c6(x1), .", "startOffset": 59, "endOffset": 86}, {"referenceID": 53, "context": "We refer to the corresponding literature for a discussion of other approximate feature maps such as (Li et al, 2010; Vedaldi and Zisserman, 2012; Kar and Karnick, 2012), which can be used as well for EXPoSE.", "startOffset": 100, "endOffset": 168}, {"referenceID": 25, "context": "We refer to the corresponding literature for a discussion of other approximate feature maps such as (Li et al, 2010; Vedaldi and Zisserman, 2012; Kar and Karnick, 2012), which can be used as well for EXPoSE.", "startOffset": 100, "endOffset": 168}, {"referenceID": 55, "context": "Webb (2000) warns against averaging these numbers: \u201cIt is debatable whether error rates in different domains are commensurable, and hence whether averaging error rates across domains is very meaningful\u201d (Webb, 2000).", "startOffset": 203, "endOffset": 215}, {"referenceID": 11, "context": "\u201d (Dem\u0161ar, 2006) Dem\u0161ar suggests to use the Friedman test with the corresponding post-hoc Nemenyi test for comparison of more classifiers over multiple data sets.", "startOffset": 2, "endOffset": 16}, {"referenceID": 53, "context": "Webb (2000) warns against averaging these numbers: \u201cIt is debatable whether error rates in different domains are commensurable, and hence whether averaging error rates across domains is very meaningful\u201d (Webb, 2000).", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "As Dem\u0161ar (2006) points out, it is also dangerous to use tests which are designed to compare a pair of algorithms for more than two: \u201cA common example of such questionable procedure would be comparing seven algorithms by conducting all 21 paired t-tests [.", "startOffset": 3, "endOffset": 17}, {"referenceID": 15, "context": "The Friedman test (Friedman, 1937) is a non-parametric statistical test which ranks algorithms for each dataset individually starting from 1 as the best rank.", "startOffset": 18, "endOffset": 34}, {"referenceID": 24, "context": "is undesirably conservative and therefore Iman and Davenport (1980) suggest to use", "startOffset": 42, "endOffset": 68}, {"referenceID": 11, "context": "Dem\u0161ar (2006) also suggests to visually represent the results of the Nemenyi test in a critical difference diagram as in Fig.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "We use the scaled version of mnist (Chang and Lin, 2011) and create hog features (Vondrick et al, 2013) for svhn.", "startOffset": 35, "endOffset": 56}, {"referenceID": 52, "context": "The parameter configuration is done by a pattern search (Torczon, 1997) using cross-validation.", "startOffset": 56, "endOffset": 71}, {"referenceID": 16, "context": "They can be combined with a concept drift detector to make the adaptation informed (Gama, 2010).", "startOffset": 83, "endOffset": 95}, {"referenceID": 9, "context": "3Prequential originates from predictive and sequential (Dawid, 1984).", "startOffset": 55, "endOffset": 68}, {"referenceID": 22, "context": "Similar, Ho (2005) proposed the three digit data stream (tdds) which contains four different concepts.", "startOffset": 9, "endOffset": 19}, {"referenceID": 22, "context": "Similar, Ho (2005) proposed the three digit data stream (tdds) which contains four different concepts. Each concept consists of three digits of the usps handwritten digits dataset as described4. After all instances of concept 1 are processed, the stream switches to the second concept and so on until concept 4. We randomly induce 1% anomalies to each concept and use the prequential method for evaluation to calculate the balanced accuracy. All datasets presented so far contain one or more sudden (abrupt) concept drifts. Bifet et al (2009) proposed a methodology to introduce a smooth (incremental) drift between two concepts.", "startOffset": 9, "endOffset": 543}, {"referenceID": 22, "context": "EXPoSE performs on average better than cod, 4See (Ho, 2005) for a detailed description of the tdds dataset.", "startOffset": 49, "endOffset": 59}, {"referenceID": 16, "context": "Although these results are promising we recommend to combine the techniques presented here with a concept drift detection technique to make informed model updates (Gama, 2010).", "startOffset": 163, "endOffset": 175}], "year": 2016, "abstractText": "We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (EXPoSE), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with EXPoSE can be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, EXPoSE can make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches.", "creator": "LaTeX with hyperref package"}}}