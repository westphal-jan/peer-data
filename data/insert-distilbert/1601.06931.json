{"id": "1601.06931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "Fisher Motion Descriptor for Multiview Gait Recognition", "abstract": "the predominant goal of this paper is largely to identify individuals by analyzing their gait. instead of using binary silhouettes just as input data ( as often done in many previous works ) we alternatively propose and evaluate the use of motion descriptors based correctly on three densely sampled orthogonal short - term trajectories. thus we take advantage of state - of - the - art the people proximity detectors method to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. the single local motion features ( indirectly described by the divergence - curl - shear descriptor ) extracted on the different spatial areas sampled of the person are combined into a single high - activity level gait descriptor by manually using the fisher vector encoding. the underlying proposed approach, coined pyramidal fisher motion, is fully experimentally validated on ` casia'dataset ( parts b and c ), ` tum gaid'dataset, ` cmu mobo'dataset and referencing the recent ` ava multiview gait'dataset. the results show that this new approach achieves state - of - the - web art results in the problem of gait recognition, allowing to collectively recognize walking people from diverse functional viewpoints on both single and multiple camera setups, wearing entirely different clothes, carrying bags, walking at too diverse speeds and not limited to straight walking paths.", "histories": [["v1", "Tue, 26 Jan 2016 09:05:26 GMT  (13202kb,D)", "http://arxiv.org/abs/1601.06931v1", "This paper extends with new experiments the one published at ICPR'2014"]], "COMMENTS": "This paper extends with new experiments the one published at ICPR'2014", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["f m castro", "m j mar\\'in-jim\\'enez", "n guil", "r mu\\~noz-salinas"], "accepted": false, "id": "1601.06931"}, "pdf": {"name": "1601.06931.pdf", "metadata": {"source": "CRF", "title": "Fisher Motion Descriptor for Multiview Gait Recognition", "authors": ["F.M. Castro", "M.J. Ma\u0155\u0131n-Jim\u00e9nezb", "N. Guil", "R. Mu\u00f1oz-Salinas"], "emails": ["fcastro@uma.es", "mjmarin@uco.es", "nguil@uma.es"], "sections": [{"heading": null, "text": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2]. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6]. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths."}, {"heading": "1. Introduction", "text": "The term gait refers to the way each person walks. Actually, humans are good recognizing people at a distance thanks to their gait [7], what provides a good (non invasive) way to identify people without requiring their cooperation, in contrast to other biometric approaches as iris or fingerprint analysis. Some potential applications are access control in special areas (e.g. military bases or governmental facilities) or smart video surveillance (e.g. bank offices or underground stations), where it is crucial to identify potentially dangerous people without their cooperation. Although great effort has been put into this problem in recent years [8], it is still far from solved.\nPopular approaches for gait recognition require the computation of the binary silhouettes of people [9], usually, by applying some background segmentation technique. However, this is a clear limitation in presence of dynamic backgrounds and/or non static cameras, where noisy segmentations are obtained. To deal with these limitations, we propose the use of descriptors based on the local motion of points. These kind of descriptors have become recently popular in the field of human action recognition [10]. The main idea is to build local motion descriptors from densely sampled points. Then, these local descriptors are aggregated into higher level descriptors by using histogram-based techniques (e.g. Bag of Words [11]).\nTherefore, our research question is: could we identify people by using only local motion features as represented in Fig. 1? We represent in Fig. 1 the local trajectories (in green) of image points (in red)\n\u2217Corresponding author Department of Computing and Numerical Analysis, Escuela Polite\u0301cnica Superior, Campus de Rabanales, Co\u0301rdoba, Spain, Tlf. 0034 957218980\nEmail addresses: fcastro@uma.es (F.M. Castro), mjmarin@uco.es (M.J. Mar\u0301\u0131n-Jime\u0301nez ), nguil@uma.es (N. Guil), rmsalinas@uco.es (R. Mun\u0303oz-Salinas)\nPreprint submitted to Technical Report \u2013 University of Cordoba January 27, 2016\nar X\niv :1\n60 1.\n06 93\n1v 1\n[ cs\n.C V\n] 2\n6 Ja\nn 20\n16\nbelonging to four different people. Our goal is to use each set of local trajectories (or tracklets) to build a high-level descriptor that allows to identify individuals. In this paper we introduce a new gait descriptor, coined Pyramidal Fisher Vector, that combines the potential of recent human action recognition descriptors with the rich representation provided by Fisher Vectors encoding [2]. A thorough experimental evaluation is carried out on \u2018CASIA Gait\u2019 dataset (sets B and C), \u2018CMU MoBo\u2019 dataset, \u2018TUM GAID\u2019 dataset and the recent \u2018AVA Multiview Gait\u2019 dataset. The variety of the employed datasets will allow us to show the robustness of our gait recognition method in presence of challenging situations: poor silhouette segmentation, occlusion of body parts, strong changes in body scale, complex subject trajectories and changes in clothing. And the most important point, we will show that a discriminative classifier based on our features can handle multiple viewpoints at test time, removing the limitation of using several camera viewpoints simultaneously, even on curved walking paths. Comparison with previous works has also been performed, showing that our approach outperforms previous techniques in most of scenarios, demonstrating that the new paradigm can compete successfully with current state-of-the-art gait recognition methods.\nThis paper is organized as follows. After presenting the related work, we describe our proposed framework for gait recognition in Sec. 2. Sections 3\u20137 are devoted to the experimental results. An overall discussion of the results is expounded in Sec. 8 And, finally, the conclusions are presented in Sec. 9."}, {"heading": "1.1. Related work", "text": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images. For example, in [8] we can find a survey on this problem summarizing some of the most popular approaches. Some of them use explicit geometrical models of human bodies, whereas others use only image features. A sequence of binary silhouettes of the body is adopted in many works as input data. In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [9]. The key idea is to compute a temporal averaging of the binary silhouette of the target subject. Liu et al. [16], to improve the gait recognition performance, propose the computation of HOG descriptors from popular gait descriptors as the GEI and the Chrono-Gait Image (CGI). In [17], the authors try to find the minimum number of gait cycles needed to carry out a successful recognition by using the GEI descriptor. Martin-Felez and Xiang [18] [19], using GEI as the basic gait descriptor, propose a new ranking model for gait recognition. This new formulation of the problem allows to leverage training data from different datasets, thus, improving the recognition performance. In [20], Akae et al. propose a temporal super resolution approach to deal with low frame-rate videos for gait recognition. They achieve impressive results by using binary silhouettes of people at a rate of 1-fps. Hu proposes in [21] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI. In addition, the same author defines in [22] a method to identify camera viewpoints at test time from patch distribution features. Recently, Lai et al. [23] proposed a novel discriminant subspace learning method (Sparse Bilinear Discriminant Analysis) that extends methods based on matrix-representation discriminant analysis to sparse\ncases, obtaining competitive results on gait recognition. In many works it is assumed that the target person follows a straight path, however, Iwashita et al. [24] explicitly focus on curved trajectories. Although, curved trajectories is not an specific goal in our paper, we show results of our proposed method on unconstrained trajectory paths, highlighting that the kind of trajectory is not a limitation for our proposal.\nOn the other hand, human action recognition (HAR) is related to gait recognition in the sense that the former also focuses on human motion, but tries to categorize such motion into categories of actions as walking, jumping, boxing, etc. In HAR, the work of Wang et al. [10] is a key reference. They introduce the use of short-term trajectories of densely sampled points for describing human actions, obtaining state-of-theart results in the HAR problem. The dense trajectories are described with the Motion Boundary Histogram. Then, they describe the video sequence by using the Bag of Words (BOW) model [11]. Finally, they use a non-linear SVM with \u03c72-kernel for classification. In parallel, Perronnin and Dance [25] introduced a new way of histogram-based encoding for sets of local descriptors for image categorization: the Fisher Vector (FV) encoding. In FV, instead of just counting the number of occurrences of a visual word (i.e. quantized local descriptor) as in BOW, the concatenation of gradient vectors of a Gaussian Mixture is used. Thus, obtaining a larger but richer representation of the image.\nBorrowing ideas from the HAR and the image categorization communities, we propose in this paper a new approach for gait recognition that combines low-level motion descriptors, extracted from short-term point trajectories, with a multi-level gait encoding based on Fisher Vectors: the Pyramidal Fisher Motion (PFM) gait descriptor. In this context, the work of Gong et al. [26] is similar to ours in the sense that they propose a method that uses dense local spatio-temporal features and a Fisher-based representation rearranged as tensors. However, there are some significant differences: (i) instead of dealing with a single camera viewpoint, we integrate in our system several camera viewpoints; (ii) instead of using all the local features available in the sequence, we use a person detector to focus only on the ones related to the target subject; and, (iii) the information provided by the person detector enables a richer representation by including coarse geometrical information through a spatial grid defined on the person bounding-box.\nA conference version of this paper was presented in [27]. In this current version, three important improvements have been introduced: (i) we use a new person detector that employs a combination of full-body and upper-body detectors, resulting in a robust tool when occlusions happen in body parts; (ii) to recover broken tracks of detections, we use a histogram-based linking process, obtaining longer tracks that allow a better representation of the gait; and, (iii) three new databases have been included in the experimental results section: \u2018CMU Motion of Body\u2019 dataset, the \u2018CASIA Gait\u2019 dataset and the \u2018TUM GAID\u2019 dataset.\nOn this set of databases we have conducted a variety of significant experiments to evaluate the robustness of our method: (i) to evaluate the minimum number of frames necessary to identify a subject at test time ; (ii) to take advantage of information of multiple cameras to build a classifier able to identify subjects under multiple views; and, (iii) to identify subjects by employing their upper or lower body. Moreover, we have designed specific experiments to carry out a thorough comparison with previous methods and to conclude that our method outperforms state-of-the-art gait recognition approaches under most of scenarios."}, {"heading": "2. Proposed framework", "text": "In this section we present our proposed framework to address the problem of gait recognition. Fig. 2 summarizes the pipeline of our approach. We start by computing local motion descriptors from tracklets of densely sampled points on the whole scene (Sec. 2.1). Since, we do not assume a static background, we run a person detector to remove the point trajectories that are not related to people (Sec. 2.2). In addition, we spatially divide the person regions to aggregate the local motion descriptors into mid-level descriptors (Sec. 2.3). Finally, a discriminative classifier is used to identify the subjects (Sec. 2.4)."}, {"heading": "2.1. Motion-based features", "text": "The first step of our pipeline is to compute densely sampled trajectories. Those trajectories are computed by following the approach of Wang et al. [10]. Firstly, dense optical flow F = (ut, vt) is computed [28] on a\ndense grid (i.e. step size of 5 pixels and over 8 scales). Then, each point pt = (xt, yt) at frame t is tracked to the next frame by median filtering as follows:\npt+1 = (xt+1, yt+1) = (xt, yt) + (M \u2217 F )|(x\u0304t,y\u0304t) (1)\nwhere M is the kernel of median filtering and (x\u0304t, y\u0304t) is the rounded position of pt. To minimize drifting effect, the tracking is limited to L frames. We use L = 15 as in [1]. As a postprocessing step, noisy and uninformative trajectories (e.g. excessively short or showing sudden large displacements) are removed. These short-term trajectories (or tracklets) are represented in Fig. 2 by green lines for each considered point (in red).\nOnce the local trajectories are computed, they are described with the Divergence-Curl-Shear (DCS) descriptor proposed by Jain et al. [1], which is computed as follows: div(pt) = \u2202u(pt) \u2202x + \u2202v(pt) \u2202y curl(pt) = \u2212\u2202u(pt) \u2202y + \u2202v(pt) \u2202x hyp1(pt) = \u2202u(pt) \u2202x \u2212 \u2202v(pt) \u2202y\nhyp2(pt) = \u2202u(pt) \u2202y + \u2202v(pt) \u2202x\n(2)\nAs described in [1], the divergence is related to axial motion, expansion and scaling effects, whereas the curl is related to rotation in the image plane. From the hyperbolic terms (hyp1,hyp2), we can compute the magnitude of the shear as:\nshear(pt) = \u221a hyp21(pt) + hyp 2 2(pt) (3)\nThen, those kinematic features are combined in pairs as in [1] to get the final motion descriptors."}, {"heading": "2.2. People detection and tracking", "text": "We follow a tracking-by-detection strategy as in [29]: we detect people with the detection framework of Felzenszwalb et al. [30]; and, then, we apply the clique partitioning algorithm of Ferrari et al. [31] to group detections into tracks. Short tracks with low-scored detections are considered as false positives and are discarded for further processing. We are able to recover broken tracks by linking non-overlapping (in time) tracks that are similar enough based on histograms of color (i.e. 3\u00d716-bins histograms on RGB space and \u03c72 distance). To obtain the color histogram of a track, we compute the average of all histograms of\neach bounding-box that compose a track. Finally, we compare the similarity of tracks through \u03c72 distance. This is especially useful when the detector misses the person for a period of time. In addition, to remove false positives generated by static objects, we measure the displacement of the detection along the sequence. Thus, discarding those tracks showing a static behaviour.\nThe person tracks finally kept are used to filter out the trajectories that are not related to people: we only keep the tracklets that pass through, at least, one bounding-box of any person track. In this way, we can focus on the trajectories that should contain information about the gait. People detection. On each video frame, we run both a pedestrian detector [30] (i.e. full body) and an upper-body detector [32]. Since the pedestrian detector favors the detection of people standing with the legs at rest, the idea of using the upper-body detector is to be able to detect people holding poses not covered by the pedestrian detector (see Fig. 3.a), or people with the legs partially occluded.\nInspired by the work of Kla\u0308ser [33], after running both detectors (Fig. 3.a\u2013b), we transform the upperbody bounding-boxes (BB) into pedestrian-like BB.\nGiven an upper-body BB (x, y, w, h) with center (x, y), width w and height h, its transformed BB (x\u2032, y\u2032, w\u2032, h\u2032) is given by the following equations:\nx\u2032 = x+ \u00b5x \u00b7 h; y\u2032 = y + \u00b5y \u00b7 h;w\u2032 = \u00b5w \u00b7 w;h\u2032 = \u00b5h \u00b7 h\nWhere \u00b5x, \u00b5y, \u00b5w and \u00b5h are parameters learnt from training samples as the mean value of relative locations (xr, yr) and scales (wr, hr). In particular, we use the following relations:\nxr = xf \u2212 xu hu ; yr = yf \u2212 yu hu ;wr = wf/wu;hr = hf/hu\nWhere xr and yr are the scale-invariant relative coordinates between the center of a full-body BB (xf , yf , wf , hf ) and the center of an upper-body BB (xu, yu, wu, hu); and wr and hr are the relative widths and heights of the BBs.\nSince we have now two classes of detections, upper-bodies (UB) and full-bodies (FB), it is more than likely that an UB is found inside a FB. Therefore, we define a procedure similar to the Kla\u0308ser\u2019s one [33] to combine UB and FB detections. Firstly, all the UB detections are geometrically transformed into FBs, and the detection scores of both classes are scaled to the same range to make them comparable. Then, for each FB detection, (i) we select from all the unused UBs the one that overlaps more (in terms of intersection-overunion, IoU) with the current FB detection (recall that the geometrical transformation has been previously applied to the UB); (ii) if the IoU is greater than a combining threshold \u03c4C , then, the selected UB is marked as \u2018used\u2019 and a new BB is added to the new detection set S, where such BB is defined as the one with the largest area from the FB and the transformed UB; (iii) if a combination has been done, we assign a new score Sc to the resulting BB as Sc = Sf \u00b7 Su \u00b7 IoU , where Sf and Su are the original detection scores of\nthe FB and the UB, respectively. Finally, the unused UBs (transformed) and the non combined FBs are included into set S.\nNote that more than one BB in S could now cover a significant part of the same image region, therefore, a non-maxima-suppression (NMS) procedure is applied to the new set of BBs to obtain the subset of BBs that will be used for further processing. An example of the resulting BBs is shown in Fig. 3.c, where from four window detections we end up with only two final BBs with FB-like aspect-ratio and new scores."}, {"heading": "2.3. Pyramidal Fisher Motion Descriptor", "text": "Fisher Motion. As described above, our low-level features are based on motion properties extracted from person-related local trajectories. In order to build a person-level gait descriptor, we need to summarize the local features. We propose here the use of Fisher Vectors (FV) encoding [2].\nThe FV, that can be seen as an extension of the Bag of Words (BOW) representation [11], builds on top of a Gaussian Mixture Model (GMM), where each Gaussian corresponds to a visual word. Whereas in BOW, an image is represented by the number of occurrences of each visual word, in FV an image is described by a gradient vector computed from a generative probabilistic model. The dimensionality of FV is 2ND, where N is the number of Gaussians in the GMM, and D is the dimensionality of the local motion descriptors xt. For example, in our case, the dimensionality of the local motion descriptors is D = 318\n1, if we use N = 100 Gaussians, then, the FV would have 63600 dimensions. In this paper, we will use the term Fisher Motion (FM) to refer to the FV computed on a video from low-level motion features.\nAssuming that our local motion descriptors {xt \u2208 RD, t = 1 . . . T} of a video V are generated independently by a GMM p(x|\u03bb) with parameters \u03bb = {wi, \u00b5i,\u03a3i, i = 1 . . . N}, we can represent V by the following gradient vector [25]:\nG\u03bb(V ) = 1\nT T\u2211 t=1 \u2207\u03bb log p(xt|\u03bb) (4)\nwhere T is the total number of local descriptors and \u2207\u03bb denotes the gradient operator with respect to \u03bb. Following the proposal of [2], to compare two videos V and W , a natural kernel on these gradients is the Fisher Kernel: K(V,W ) = G\u03bb(V ) TF\u22121\u03bb G\u03bb(W ), where F\u03bb is the Fisher Information Matrix. As F\u03bb is symmetric and positive definite, it has a Cholesky decomposition F\u22121\u03bb = L T \u03bbL\u03bb, and K(V,W ) can be rewritten as a dot-product between normalized vectors \u0393\u03bb with: \u0393\u03bb(V ) = L\u03bbG\u03bb(V ). Then, \u0393\u03bb(V ) is known as the Fisher Vector of video V. As stated in [2], the capability of description of the FV can be improved by applying it a signed square-root followed by L2 normalization. So, we adopt this finding for our descriptor. Pyramidal representation. We borrow from [34] the idea of building a pyramidal representation of the gait motion. Since each bounding-box covers the whole body of a single person, we propose to spatially divide the BB into cells. Then, a Fisher vector is computed inside each cell of the spatio-temporal grid. We can build a pyramidal representation by combining different grid configurations. Then, the final feature vector, used to represent a time interval, is computed as the concatenation of the cell-level Fisher vectors from all the levels of the pyramid.\n12D coordinates: 30; Div+Curl: 96; Curl+Shear: 96; Div+Shear: 96"}, {"heading": "2.4. Classification", "text": "The last stage of our pipeline is to train a discriminative classifier to distinguish between the different human gaits. Since, this is a multiclass problem, we train P binary Support Vector Machines (SVM) [35] (as many as different people) in a one-vs-all strategy. Although the \u03c72 kernel is a popular choice for BOW-based descriptors, a linear kernel is typically enough for FV, due to the rich feature representation that it provides."}, {"heading": "2.5. Implementation details", "text": "For people detection, we use the code published by the authors of [30]. For computing the local motion features, we use the code published by the authors of [1]. The Fisher Vector encoding and the classification is carried out by using the code included in the library VLFeat2."}, {"heading": "3. Overview of the experiments", "text": "In order to validate our approach, we carry out diverse experiments on four datasets: \u201cAVA Multi-View Dataset\u201d, \u201cCMU MoBo Dataset\u201d, \u201cCASIA B and C Datasets\u201d and \u201cTUM GAID Dataset\u201d. With these experiments we try to answer, among others, the following questions: a) is the combination of trajectorybased features with FV a valid approach for gait recognition? ; b) can we learn different camera viewpoints in a single classifier? ; c) can we improve the recognition rate by spatially dividing the human body region? ; d) what is the effect of using PCA-based dimensionality reduction on the recognition performance? ; e) what is the influence of the sequence length in the recognition performance? ; f) is it necessary to use the DCS descriptor as a whole or can we use just some of its components? ; and, g) can the proposed model generalize well on unrestricted walk trajectories?\nThe subsequent sections present the experiments and results obtained on the different datasets."}, {"heading": "4. Experimental results on AVAMVG", "text": "The first dataset where we perform our experiments is the \u201cAVA Multi-View Dataset for Gait Recognition\u201d (AVAMVG) [6]. In AVAMVG 20 subjects perform 10 walking trajectories in an indoor environment. Each trajectory is recorded by 6 color cameras placed around a room that is crossed by the subjects during the performance. Fig. 4 shows the scenario from the six available camera viewpoints. Note that depending on the viewpoint and performed trajectory, people appear at diverse scales, even showing partially occluded body parts. In particular, the 3rd and 6th camera viewpoints represented in Fig. 4 are more likely to show partially visible bodies most of the time than the other four cameras. Therefore, in our experiments, and without loss of generality, we will use only four cameras (i.e. Cam00,Cam01,Cam03,Cam04 ). Trajectories 1 to 3 follow a linear path, whereas the remaining seven trajectories are curved. The released videos have a resolution of 640\u00d7 480 pixels. Each video has around 375 frames, where only approximately one third of the frames contains visible people."}, {"heading": "4.1. Experimental setup", "text": "Since we have multiple viewpoints of each instance (i.e. pair subject\u2013trajectory), we can assign a single label to it by majority voting on the viewpoints. This approach helps to deal with labels wrongly assigned to individual viewpoints. Note that instead of training an independent classifier (see Sec. 2.4) per camera viewpoint, we train a single classifier with samples obtained from different camera viewpoints, allowing the classifier to learn the relevant gait features of each subject from multiple viewpoints. In order to increase the amount of training samples, we generate their mirror sequences, thus, doubling the amount of samples available during learning.\nWe describe below the different experiments performed to give answer to the questions stated at Sec. 3. After the description of the experiments, the achieved results are discussed in Sec. 4.2.\n2VLFeat library is available at http://www.vlfeat.org/\nExperiment A: baseline. We use the popular Bag of Words approach (BOW) [11] as baseline, which is compared to our approach. For this experiment, we use trajectories 1, 2 and 3 (i.e. straight path). We use a leave-one-out strategy on the trajectories (i.e. two for training and one for test). We sample dictionary sizes in the interval [500, 4000] for BOW3, and in the interval [50, 200] for PFM. Both BOW and PFMs have a single level with two rows and one column (i.e. concatenation of two descriptors: half upper-body and half lower-body). The results of this experiment are included in Tab. 1: rows \u2018BOW\u2019 and \u2018PFM\u2019. Experiment B: half body features. Focusing on PFM, we compare in Tab. 1 four configurations of the PFM on trajectories 1, 2 and 3: a) no spatial partition of the body (row \u2018PFM-FB\u2019); b) using only the top half of the body (row \u2018PFM-H1\u2019); c) using only the bottom half of the body (row \u2018PFM-H2\u2019); and, d) using the concatenation of the top and bottom half of the body (row \u2018PFM\u2019). Experiment C: dimensionality reduction. Since the dimensionality of PFM is typically large, we evaluate in this experiment the impact of dimensionality reduction on the final recognition performance. We run Principal Component Analysis (PCA) both on the original low-level features (318 dimensions), and on the PFM vectors. We use the PFM descriptor, as in experiment A, on trajectories 1, 2 and 3. The results of this experiment are included in Tab. 1: row \u2018PFM+PCAL50\u2019 indicates that the low-level feature vectors have been compressed to 50 dimensions; row \u2018PFM+PCAH256\u2019 indicates that the final PFM feature vectors have been compressed to 256 dimensions; row \u2018PFM+PCAL100+PCAH256\u2019 means that the lowlevel features have been intially compressed to 100 dimensions and the final PFM vectors to 256 dimensions; and, finally, row \u2018PFM+PCAL50+PCAH256+pyr\u2019 corresponds to a two level pyramid with the compression indicated in the name. Experiment D: influence of sequence length. The goal of this experiment is to evaluate the influence of the sequence length in the recognition process. For this purpose, at test time, we use only a single subsequence of T frames4extracted around the middle of the sequence. In Tab. 2, each row corresponds to a different number of frames in the range [15, 50]. The dictionary size has been fixed to 150 components.\n3Larger dictionary sizes for BOW did not show any significative improvement. In contrast, the computational time increased enormously.\n4Note that a buffer of L = 15 video frames is needed to compute the set of dense trajectories. Therefore, a value of +14 actual video frames has to be added to T . For example, T \u2032 = 15 + 14 = 29, around one second.\nExperiment E: testing on a single camera. The previous results assume a multicamera enviroment at test time to combine information from diverse viewpoints. However, in many real situations, only a viewpoint is available at test time. In this experiment we present a breakdown of the recognition rates per camera. The results of this experiment are summarized in Tab.3. We use here the configuration \u2018PFM-len50\u2019 learnt during the previous experiment (see Tab. 2). Each row corresponds to a different camera viewpoint (see Fig. 4). Experiment F: feature selection. In the previous experiments, our local motion vectors had 318 dimensions, obtained as the concatenation of four kind of features (see Sec. 2.1): normalized coordinates, Div+Curl (DC), Curl+Shear (CS) and Div+Shear (DS). The goal of this experiment is to evaluate the contribution of each type of local feature in the gait recognition process. In this case, we learn independent dictionaries per feature type, instead of a single dictionary of concatenated local features as done before. Then, we concatenate the resulting feature-specific FVs. Tab. 4 summarizes the results of this experiment. Each row corresponds to a different configuration of the PFM descriptor, where \u2018ftxxxx \u2019 indicates the set of features used, encoded in the xxxx string as follows (from left to right): normalized coordinates, DC, CS and DS. Value 1 means used whereas value 0 means not used. For example, string \u2018ft0110\u2019 means that only DC and CS descriptors have been used. Experiment G: training on straight paths and testing on curved paths. In this experiment, we use the PFM descriptor as in experiment F. We use trajectories 1 to 3 for training, and trajectories 4 to 10 for testing. Note that in the latter sequences, the subjects perform curved trajectories, thus, changing their viewpoint (with regard to a given camera). The results of this experiment are summarized in Tab. 5, where each column correspond to a different test trajectory. For each trajectory, 20 multiview sequences are evaluated (one per subject), corresponding to 80 individual camera viewpoints5."}, {"heading": "4.2. Results", "text": "The results shown in Tab. 1 correspond to experiments A, B and C (see Sec. 4.1) and have been obtained by training on two of the three straight trajectories ({1, 2, 3}) and testing on the remaining one (e.g.\n5Trajectory #08 is not available for subject rafa; and, trajectory #10 is not available for subject angel.\n\u2018Trj=1+2 \u2019 indicates training on trajectories #1 and #2, then, testing on trajectory #3). Therefore, each model is trained with 160 samples (i.e. 20 subjects \u00d7 4 cameras \u00d7 2 trajectories) and tested on 80 samples. Each column \u2018Trj=X+Y \u2019 contains the percentage of correct recognition per partition at instance level (i.e. combining the four viewpoints by majority voting) and, in parenthesis, at video level (i.e. supposing that each camera viewpoint is an independent sample); column \u2018Avg \u2019 contains the average on the three partitions. Column K refers to the number of centroids used for quantizing the low-level features in each FM descriptor. Row \u2018BOW\u2019 corresponds to the baseline approach (see Sec. 4.1). Row \u2018PFM-FB\u2019 corresponds to the PFM on the full body (no spatial partitions). Rows \u2018PFM-H1\u2019and \u2018PFM-H2\u2019 correspond to PFM on the top half and on the bottom half of the body, respectively. Row \u2018PFM\u2019 corresponds to a single-level PFM obtained by the concatenation of the descriptors extracted from both the top and bottom half of the body. Row \u2018PFM+PCAL50\u2019 corresponds to our proposal but reducing with PCA the dimensionality of the low-level motion descriptors to 50 before building our PFM (i.e. final gait descriptor with K = 150 is 15000-dimensional). Row \u2018PFM+PCAH256\u2019 corresponds to our proposal but reducing with PCA the dimensionality of our final PFM descriptor to 256 dimensions before learning the classifiers (i.e. final gait descriptor is 256-dimensional). Row \u2018PFM+PCAL100+PCAH256\u2019 corresponds to our proposal but reducing both the dimensionality of the low-level descriptors and the final PFM descriptor (i.e. final gait descriptor is 256-dimensional). Row \u2018PFM+PCAL50+PCAH256+pyr\u2019 corresponds to a two-level pyramidal configuration where the first level has no spatial partitions and the second level is obtained by dividing the bounding box in two parts along the vertical axis, as done previously. In addition, PCA is applied to both the low-level descriptors and the final PFM vector.\nThe results shown in Tab. 5 correspond to experiment D (see Sec. 4.1) and have been obtained by training on trajectories {1, 2, 3} (all in the same set), and testing on trajectories {4, 5, 6, 7, 8, 9, 10} (see corresponding columns). As done in the previous experiments, different configurations of PFM have been evaluated. Each entry of the table contains the percentage of correct recognition in the multiview setup and, in parenthesis, the recognition per video. From the seven tested trajectories, only on trajectory number #06 a perfect recognition rate was achieved on the multiview setup. In contrast, the trajectory number #05 resulted to be one of the hardest when trying to classify per individual cameras, although the use of the majority voting strategy on the multiview setup clearly contributed to boost the recognition rate (e.g. from 83.1 to 95). In our opinion, the main difficulty when dealing with this kind of curved trajectories is the fragmentation of the person tracks due to partial occlusions (i.e. body parts temporally out of camera\u2019s field of view), what in turn implies the loss of dense tracks and, therefore, less motion features available for characterizing the gait of the subject.\nWith regard to the number of frames needed to recognize a person with the proposed framework, we can see in Tab. 2 that with the use of local motion features from 15 consecutive frames, the recognition rate in the multiview setup is nearly perfect (98.3%). Although, in a monocular setup, such configuration reaches a modest 86.3%. If we increase the number of used frames up to 50, we obtain an average of 100% in multiview and 96.3% per camera.\nIf we focus on the results obtained by each of the four used cameras, we can see in Tab. 3 that camera \u2018Cam03\u2019 yields a perfect recognition rate (100%) on average. Note that for such camera the viewpoint of the person\u2019s trajectory is nearly profile, thus allowing a suitable computation of point tracks for long time\nintervals. In contrast, the lowest rate is obtained while working only with camera \u2018Cam01\u2019 (91.7%). In our opinion, it is due to the fact that only during a short period of time (around 1 second), the whole body of people is fully visible (i.e. neither the head nor the feet are out of the field of view).\nRecall that the trajectories of the densely sampled points, used as low level features, are described by using the descriptor DCS (Sec. 2.1), which is compound of four subtypes of features. In Tab. 4 we can see the effect of removing, at a time, some of the subtypes. The results show that the weakest subtype of feature is the normalized coordinates as shown in row \u2018PFM-ft1000\u2019. In addition to selecting only part of the descriptor, the results of the bottom rows of the table show the effect of dimensionality reduction with PCA. In contrast to the results previously reported in Tab. 1 where the whole vector of low level features was reduced to a fixed size (e.g. \u2018PCAL100\u2019), in this case, the subtypes are indepedently reduced to a fraction of the original size. For example, \u2018PCALx40\u2019 indicates that only the 40% of the dimensions are kept."}, {"heading": "5. Experimental results on MoBo", "text": "The second dataset where we carry out our experiments is the \u201cCMU Motion of Body\u201d (MoBo) database [5]. MoBo contains video sequences of 25 subjects performing four different walking patterns on a treadmill: slow walk, fast walk, incline walk and walking with a ball. It has been recorded from six camera viewpoints. Fig. 5 shows six video frames from the dataset. Video resolution is 480\u00d7 640 pixels."}, {"heading": "5.1. Experimental setup", "text": "We run here experiments similar to the ones presented above to evaluate further our proposed framework for gait recognition. Note that, in contrast to AVAMVG dataset where people actually displace along the room, in MoBo dataset there is no actual displacement of the person body as people are walking on a treadmill. Therefore, there will not be available motion trajectories associated to the body as a whole. In addition, there is a set of videos where people do not move arms freely as they are holding a ball, thus removing the motion pattern associated to arms swing. In our experiments, we will use only four of the cameras (i.e. Cam01,Cam02,Cam04,Cam05 ). As done in the previous section, if multiple cameras are available during testing, a majority voting strategy is followed to deliver a single label per subject. Experiment A: training on multiple walking patterns. In this experiment we use, at a time, three of the walking patterns for training and the remaining one for testing (i.e. leave-one-out on the walking patterns). Therefore, we report the average of the four recognition rates. The results of this experiment are summarized in Tab. 6 where each row correspond to a different configuration. We combine different subtypes of DCS features with diverse PCA-based dimensionality reductions and dictionary sizes. Experiment B: influence of sequence length. The goal of this experiment is to evaluate the influence of the sequence length in the recognition process. For this purpose, at test time, we use only a subsequence of T frames extracted around the middle of the sequence. In Tab. 7, each row corresponds to a different\nnumber of frames in the range [5, 40]. The dictionary size has been fixed to either 50 or 100 (column \u2018K\u2019) on the configuration \u2018PFM+PCALx20+PCAH256-ft0110\u2019. Experiment C: testing on a single camera. The previous results assume a multicamera enviroment at test time to combine information from diverse viewpoints. However, in many real situations, only a viewpoint is available at test time. In this experiment we present a breakdown of the recognition rates per camera. The results of this experiment are summarized in Tab.8. We use here the configuration \u2018PFM+PCALx20+PCAH256-ft0110\u2019 with K = 100 learnt during the previous experiment (see Tab. 7). Each row corresponds to a different camera viewpoint. Experiment D: training on a single walking pattern. In this experiment, we train the models on a single walking pattern, at a time, and then we test on the other patterns. Tab. 9 summarizes the results of this experiment. Rows correspond to training walking patterns, whereas columns correspond to test patterns. This table allows a comparison with some previously published results on this dataset."}, {"heading": "5.2. Results", "text": "The results shown in Tab. 6 correspond to Experiment A. Since the number of possible combinations of the parameters is so big, we show in the table only a subset of the most representative ones. All the configurations (i.e. rows) not containing the suffix \u2018pyr\u2019 correspond to a single level PFM with two vertical partitions (i.e. concatenation of lower- and upper-body FM descriptors). Note that one of the best configurations (in terms of mean accuracy) is \u2018PFM+PCALx40+PCAH128-ft0110\u2019 with K = 50, what means that the selected low level motion features (DC and CS) have been initially reduced up to the 40% of their original dimensionality, and the final PFM descriptor has been reduced to 128 dimensions before the classification stage. In such case, we obtain a perfect mean recognition rate in the multiview setup and a mean 99.8% of accuracy per viewpoint. This indicates a clear success of the proposed descriptor in this dataset with low-dimensional feature vectors. Focusing on the dimensionality reduction of the final PFM vector, reducing it up to 64 dimensions only implies a small decrease in the accuracy (3% in the multiview case) while obtaining a more compact representation of the gait. The most extreme case of compression evaluated in this experiment is represented in row \u2018PFM+PCALx10+PCAH064-ft0110\u2019 (with K = 50). However, the accuracy worsens less than 10%. It is worth mentioning that a perfect mean recognition accuracy can be achieved in the multiview setup by using only the low level descriptor CS (row \u2018PFM+PCALx20+PCAH128-ft0010\u2019). Since so high accuracy is obtained without using more than one level in the PFM, the results reported in the bottom rows of Tab. 6 (including suffix \u2018pyr\u2019), where two levels are used, are included just for the completeness of the evaluation.\nWith regard to Experiment B, where the influence of the sequence length in the performance is evaluated, the results included in Tab. 7 show that a perfect mean recognition rate can be achieved using features of\n20 consecutive frames. In addition, by using only 10 frames, the accuracy only decreases to approximately 95% of correct recognition.\nThe results obtained per camera for the case \u2018PFM+PCALx20+PCAH256-ft0110-len20\u2019 (included in Tab. 7) are summarized in Tab. 8. Note that cameras Cam01 and Cam04 achieve a mean perfect recognition rate, what means that not all viewpoints are needed for an accurate identification of the individuals with PFM. As previously observed in the experiments on \u2018AVAMVG\u2019, profile viewpoints favor the recognition process.\nThe results shown in Tab. 9 correspond to Experiment D. In particular, we have used the following configuration: features DC and CS; K = 100 for the dictionaries; and single level PFM with two vertical partitions. No PCA compression has been performed in this experiment. To increase the number of training samples, the video sequences have been split into subsequences of length 100 frames, with an overlap of 25 frames. In this case, the worst recognition results are obtained when the training stage is carried out on the ball sequences. This result is reasonable since the classifiers have never seen the motion of the arms that is present in the other walking types (s, f and i). To put our results in context, the authors of [36] report the results of training on slow walk and testing on both fast walk and ball, obtaining 92% and 96% of accuracy, respectively. In our case, we obtain the same accuracy on fast walk (column \u2018Tst=f\u2019) and improve on the ball case (column \u2018Tst=b\u2019). We can also compare with the results published in [37], where in one case they train on slow walk and test on ball, obtaining 91.7%; and a second case where they train on fast walk and test on slow walk, obtaining 96% of accuracy. In our case, we obtain 100% and 92%, respectively. Comparing with the results reported in [38], we outperform the cases of training on slow walk and testing on ball, and training on fast walk and testing on slow walk, obtaining 100% and 92% of accuracy, respectively. In the rest of cases, we obtain similar results. In [39], the authors only perform the experiments in cases of training on slow walk and testing on fast walk and training on fast walk and testing on slow walk, obtaining 100% and 92% respectively. In our case, we obtain a 92% in both cases. In the recent paper [40], the reported results on this dataset only use the combination of training on fast walk and testing on slow walk, achieving\na recognition rate of 88%, in contrast to our 92%."}, {"heading": "6. Experimental results on CASIA", "text": "The third dataset where we perform our experiments is \u201cCASIA Gait Dataset\u201d [41], parts B (CASIAB) [3] and C (CASIA-C) [42].\nIn CASIA-B 124 subjects perform walking trajectories in an indoor environment. The action is captured from 11 viewpoints. Three situations are considered: normal walk (nm), wearing a coat (cl), and carrying a bag (bg). The first camera viewpoint included in the dataset is 0o and the last one is 180o, the intermediate ones are separated by 18o. Some examples can be seen in the top row of Fig. 6. In CASIA-C 153 subjects perform walking trajectories in an outdoor environment during night. The action is captured from a single viewpoint with an infrared camera. Four situations are considered: normal walk (fn), fast walk (fq), slow walk (fs) and carrying a bag (fb). Some examples can be seen in the bottom row of Fig. 6. In both sets, video resolution is 320\u00d7 240 pixels."}, {"heading": "6.1. Experimental setup", "text": "We run here experiments similar to the ones presented above to evaluate the performance of our proposed framework on videos with low resolution, in comparison to the ones previously used in our experiments. In addition, this a more challenging dataset due to the larger amount of people, different scenarios (i.e. wearing coats and carrying bags), and outdoor and night conditions.\nWe use the full length video sequences to build single layer PFM descriptors, unless otherwise stated.\nExperiment A: training on a single camera and testing on different cameras. The goal of this experiment is to evaluate the capacity of generalization of the proposed system to changes in camera viewpoint. This experiment is carried out on CASIA-B. We use a model trained on sequences 1 to 4 of the \u2018nm\u2019 scenario to classify sequences 5 and 6 of the same scenario. The recognition percentages for this experiment are summarized in table Tab. 10. Each row correspond to a training camera viewpoint, whereas the columns correspond to test camera viewpoints.\nExperiment B: robustness to clothing and carrying objects. The goal of this experiment is to evaluate the robustness of our proposed system to changes in shape due to changing clothing and carrying bags. This experiment is carried out on CASIA-B. We use a model trained on sequences 1 to 4 of the \u2018nm\u2019 scenario to classify sequences 1 and 2 of scenarios \u2018cl\u2019 (wearing a coat) and \u2018bg\u2019 (carrying a bag). The recognition percentages for this experiment are summarized in tables Tab. 11 and Tab. 12. Each row correspond to a training camera viewpoint, whereas the columns correspond to test camera viewpoints.\nExperiment C: training and testing on multiple cameras. CASIA-B In this experiment, firstly, we use several camera viewpoints to train a single model, and we test on different viewpoints independently. The results of this experiment are presented in Tab. 14. We show in the rows different number of cameras used during training. In the case we use all cameras for training (from 0o to 180o) the mean recognition accuracy over all the test camera viewpoints is 99.7% for the \u2018nm\u2019 scenario. We also evaluate in this experiment the behaviour of the system if multiple viewpoints are available at test time. Thus, combining the opinion of the individual viewpoints. The results are summarized in Tab. 15, where each row corresponds to a different test scenario (e.g. \u2018nm-bg\u2019 means training on \u2018nm\u2019 sequences and testing on \u2018bg\u2019 sequences). Column \u2018Acc\u2019 shows the percentage of recognition achieved when the individual opinions of the test cameras (column \u2018Test cams\u2019) are combined by majority voting. Experiment D: gait recognition during night. The goal of this experiment is to evaluate the performance of our proposed PFM descriptor on infrared images taken outdoors. For this purpose, we use CASIA-C dataset. Since the previously used person detector (Sec. 2.2) showed a poor performance on the infrared images of CASIA-C, we carried out background segmentation to define the bounding-box of the persons in these sequences. For that purpose, we learnt a Gaussian Mixture Model from 40 video frames. We use the implementation of [59] included in Matlab. For each video frame, a bounding-box is fitted to the obtained foreground pixels, ensuring a fixed aspect ratio of 1 : 3. The remaining stages remain as explained in Sec. 2.2.\nThe bottom row of Tab. 17 shows the recognition percentages achieved by our system. For each subject, two sequences from subset \u2018fn\u2019 (normal walk) are used for training, and the remaining sequences are used for testing. Note that, for example, row RSM [58] used three sequences per subject during training, instead of the two we use. In our case, the PFM setup for column \u2018fn\u2019 is PCAL100+PCAH128+K50; for column \u2018fs\u2019 is PCAL150+PCAH256+K150; for column \u2018fq\u2019 is PCAL100+PCAH256+K50; and, for column \u2018fb\u2019 is PCAL150+PCAH128+K50. For the case \u2018fs\u2019, dictionary size K can be reduced to 100 when using two layers in PFM. Experiment E: effect of person detection on the system performance. The goal of this experiment is to evaluate the impact of the person detection module (Sec. 2.2) on the final performance of our system. In particular, instead of using our detection module, we use binary silhouettes, obtained through a GMM-\nbased background segmentation (as done with CASIA-C), to filter out the initially estimated dense tracks. We have tried this for the hardest scenario of CASIA-B, wearing coats (\u2018cl\u2019). The results are included as a row of the Tab. 14. It is indicated with the keyword \u2018sil\u2019. Although it is not included in the table, we also tried for this experiment the binary silhouettes provided by the authors of the dataset, but we obtained lower results \u2013 we realised that some silhouettes were missing. Additionally, in Tab. 13, our accuracy 85.5% for \u2018cl\u2019 increased up to 88.3% when using background segmentation."}, {"heading": "6.2. Results", "text": "The results shown in table Tab. 10 correspond to experiment A. We can see that in most cases, by training and testing on the same camera, we can obtain an almost perfect identification of the subjects. This value decreases when testing on different cameras. Although, for similar viewpoints, in some cases we can find drops in accuracy lower than 5% (e.g. 90o vs. 72o and 108o).\nIf the previously trained system is tested with people wearing coats (\u2018cl\u2019), we can see in Tab. 11 that testing on the same camera where it was trained achieves an average accuracy of 86.7% (i.e. mean of the diagonal). We can see in Fig. 6 (top row) some examples of people wearing a coat that clearly show the difficulty of this scenario, where legs of some people are occluded up to the ankle (i.e. 17 people wear that kind of coats, and seven hold their hands inside the pockets). In the case of people wearing bags (\u2018bg\u2019), the average accuracy obtained from the diagonal of Tab. 12 is 97.8%. To put our results in context with other works, Tab. 13 contains the results of training and testing on only camera 90o for the three scenarios. Bottom row (PFM) shows our best results for each scenario, along with the average performance. Note how our approach improves on the state-of-the-art average from 94.1% to 95.2%.\nIn the previous experiments, an independent classifier was trained per viewpoint. In contrast, Tab. 14 shows that a single classifier can be trained including several viewpoints (Experiment C ). Column \u2018Training cams\u2019 indicates the cameras used during training, whereas the subsequent columns indicate the test cameras. Note that for the scenarios \u2018nm\u2019 and \u2018bg\u2019 the recognition rate is nearly perfect for all the test viewpoints. In all cases, the lowest scores are located on frontal and back viewpoints (i.e. 0o and 180o) \u2013 where dense tracklets present very small displacements and, therefore, are less discriminative \u2013 decreasing the average performance (column \u2018Avg\u2019). In the case of scenario \u2018cl\u2019, the average accuracy increases when using a twolevels PFM, as shown in row \u2018K100-pyr\u2019. These results are comparable to state-of-the-art ones, as shown in Tab. 16, where our approach improves on the best known result, up to our knowledge, from 96.2% to 99.7%, using even less training sequences (four per person instead of five). The results of the second part\nof Experiment C can be seen in Tab. 15. We can see that the majority voting strategy on the test cameras allows to achieve higher recognition rates, as previously shown on the other datasets. For example, in the case \u2018cl\u2019, if we consider that each viewpoint is an independent video sequence (monocular case), we obtain an accuracy of 75%, but this value grows up to 83.1% in the multiview setup.\nWith regard to Experiment E, in the case of the scenario \u2018cl\u2019 (see row with \u2018K100-sil\u2019 of Tab. 14) the average recognition improves from 71.7% to 76.2% (same PFM setup but K = 100). What, in our opinion, indicates that our person detector has a good behaviour in general, but should be improved for the cases where people wear clothing that deforms the expected shape of a person (from the point of view of the full-body person detector).\nFinally, we focus on CASIA-C. The results of Experiment D, summarized in row \u2018PFM\u2019 of Tab. 17, indicate that our proposed descriptor is suitable for outdoors and infrared images, achieving perfect recognition results on two out of four situations, despite the difficulty of this kind of images as can be seen in the bottom row of Fig. 6. If we compare with previous state-of-the-art approaches6, we can see that our system establishes the new state-of-the-art on two scenarios of this dataset, showing similar results on the other two."}, {"heading": "7. Experimental results on TUM GAID", "text": "The fourth dataset where we perform our experiments is \u201cTUM Gait from Audio, Image and Depth (GAID) database\u201d [4].\nIn TUM GAID 305 subjects perform two walking trajectories in an indoor environment. The first trajectory is performed from left to right and the second one from right to left. Therefore, both sides of the subjects are recorded. Two recording sessions were performed, one in January, where subjects wear heavy jackets and mostly winter boots, and the second in April, were subjects wear different clothes. Some examples can be seen in the Fig. 7. Top row shows three subjects recorded in the first session. Each one is recorded in a different walking condition (normal walk, carrying a backpack and wearing coating shoes). Bottom row shows the same three subjects recorded in the second session under the same walking conditions as in the first session.\nHereinafter the following nomenclature is used to refer every of the four walking conditions considered: normal walk (N ), carrying a backpack of approximately 5 kg (B), wearing coating shoes (S ), as used in clean rooms for hygiene conditions, and elapsed time (TN-TB-TS ).\nEach subject of the dataset is composed of: six sequences of normal walking (N1, N2, N3, N4, N5, N6 ), two sequences carrying a bag (B1, B2 ) and two sequences wearing coating shoes (S1, S2 ). In addition, 32\n6Rows of Tab. 17 have been imported from the publication [58].\nsubjects were recorded in both sessions (i.e. January and April) so they have 10 additional sequences (TN1, TN2, TN3, TN4, TN5, TN6, TB1, TB2, TS1, TS2 ).\nThe action is captured by a Microsoft Kinect sensor which provides a video stream, a depth stream and four-channel audio. Video and depth are recorded at a resolution of 640 \u00d7 480 pixels with a frame rate of approximately 30 fps. The four-channel audio is sampled with 24 bits at 16 kHz. In our experiments, we will only use the video stream.\nIn [4], Hofmann et al. designed the recommended experiments that should be performed in the database. For that purpose, they split the database in 3 partitions: 100 subjects for training and building models, 50 subjects for validation and 155 subjects for testing. Finally, a set of experiments (Sec. 7.1) are proposed for validating the robustness of the algorithms against different factors."}, {"heading": "7.1. Experimental setup", "text": "We run here the experiments proposed by the authors of the dataset at [4]. Note that we only perform the experimentation in the RGB video stream, leaving depth and audio streams for future improvements of our algorithm.\nIn our experiments, we use the training and validation sets combined as in [4] for building the dictionary. The test set is used for training and testing the person-specific classifiers.\nIn order to increase the amount of training samples, we generate their mirror sequences, thus, doubling the amount of samples available during learning. Experiment A: identification over different walking conditions\nIn this experiment we use the four first sequences of normal walking (N1, N2, N3 N4 ) for training and the sequences N5, N6, B1, B2, S1, S2 of normal walking, bag and coating shoes respectively, for testing. Therefore, we build the classifier using normal walking, whereas, test is performed over different walking conditions (normal, bag and coating shoes).\nThe results of this experiment are summarized in Tab. 18 where each row correspond to a different configuration. Experiment B: identification over temporal test set. The goal of this experiment is to evaluate the influence of recording in different seasons with changes in illumination, clothes, etc.\nIn this experiment, we use for training the first four sequences of normal walking recorded during the first session (N1, N2, N3 N4 ), and, for testing, sequences TN1, TN2, TB1, TB2, TS1, TS2 of normal walking, bag and coating shoes, respectively, recorded during the second session.\nThe results of this experiment are summarized in Tab. 19, where each row corresponds to a different configuration. For this experiment we have used a temporal partitioning to increase the number of samples due to the low number of subjects available for training (16 subjects). All temporal partitions have an overlap of 10 frames between partitions. Further details are presented below."}, {"heading": "7.2. Results", "text": "The results shown in table Tab. 18 correspond to experiment A. We can see that all cases present high recognition rates (\u2265 99%), indicating the robustness of our method \u2013 different values of the parameters only present maximum variations of \u00b13%.\nIn the case of normal gait (N ) we can see that lower values of K are better due to the similarity of samples since both train and test sequences have the same walking conditions. Otherwise, in cases of carrying a bag\n(B) and wearing coating shoes (S ), higher values of K are necessary as sequences have different walking conditions and the algorithm requires larger dictionaries for representing the gait information.\nThe best case for normal gait (N ) is PFM+PCAL100+PCAH256 with K = 200 where FM achieves an accuracy of 99.7% (only one sequence is mismatched). For the remaining cases, the best configuration is the same but with a bigger dictionary (K = 600) where FM reaches 99% in both cases, B and S.\nIn table Tab. 19 we can see the results of experiment B. In this experiment, we obtain lower accuracy due to high variability between train and test sequences. Moreover, the low number of samples available for training the classifier makes harder to obtain discriminant information needed for the identification of subjects. To avoid this lack, we split the original training sequences into independent subsequences of XX frames with O frames of overlap. Thus, from one sequence we can obtain more subsamples that allow us to train a better classifier. Note that in Tab. 19, the rows follow the pattern PFM+PCAL200+PCAH256+lenXX where XX corresponds to the frames of the partition. In our case, the best configuration is XX = 45 and O = 10 because partitions with a lower number of frames produces overfitting due to the high number of subsamples produced. On the other hand, partitions with higher number of frames produce only two subsamples with huge differences in the number of frames. If the row does not follow this pattern, it indicates that we have used the full sequence without partitions (e.g. bottom row of the table).\nThe best case for normal gait (TN ) is PFM+PCAL150+PCAH256 with K = 600 and without time partition because train and test samples are similar. In this case, FM achieves an accuracy of 78.1%. For the case of carrying a bag (TB) the best configuration is PFM+PCAL200+PCAH256+T45+O10 with K = 400 where FM reaches an accuracy of 62.0%. Finally for wearing coating shoes case (TS ) the best result is 54.9% with the configuration PFM+PCAL200+PCAH256+T45+O10 with K = 600. As we can see in the results, time partition is useful for experiments where the variability of training and test samples is high. The same reasoning applies to the dictionary size as Tables 18 and 19 indicate: big dictionaries allow the algorithm to achieve better results in experiments over different conditions because a richer representation is obtained and consequently, a better generalization.\nTo put our results in context with other works, Tab. 20 contains results from experiments A and B. In particular, bottom row (PFM) shows our best results for each scenario, taken from Tabs 18 and 19, along with the average performance. The first method (SDL) is specialized in temporal identification and the authors only report experiments for the case TN, so we cannot obtain an average accuracy. Note how our approach improves on the state-of-the-art average from 88.2% to 96.0%. The lowest accuracy is reached in TS where we obtain a value only 2.1% lower than best result. In the rest of cases (excluding specialized method in TN ) our method outperforms or obtains similar results to the state-of-the-art."}, {"heading": "8. Final Discussion", "text": "We summarize here our main overall findings based on the experimental results obtained on the datasets. First of all, the results presented in tables 1, 6, 16 and 20 indicate that the proposed pipeline is a valid approach for gait recognition, obtaining a 100% of correct recognition on the multiview setup on both AVAMVG and CMU MoBo datasets, and a 99.7% on CASIA-B. In addition, the FV-based formulation surpasses the BOW-based one, as stated by other authors in the problem of image categorization [2]. Moreover, the large dimensionality of the PFM can be drastically reduced by applying PCA, without worsening the final performance. For example, we can see in Tab. 1 that reducing the dimensions of the low-level motion descriptors to 100, and the final PFM to 256, allows to achieve a similar recognition rate but decreasing significantly the computational complexity (\u2248 \u00d7370 smaller with K = 150).\nIf we focus on the idea of spatially dividing the human body for computing different gait descriptors, the results in Tab. 1 show that the most discriminantive features are localized on the lower-body (row \u2018PFM-H2\u2019), what confirms our intuition (i.e. gait is mostly defined by the motion of the legs). In addition, although in a slight manner (see values in parenthesis), the upper-body features (row \u2018PFM-H1\u2019) contribute to the definition of the gait as well.\nFocusing on Tab. 5, we can observe that PFM generalizes fairly well, as derived from the results obtained when testing on curved trajectories. Note that this kind of situations clearly benefits from the use of multiple cameras, as indicated by the low results yielded by single cameras and improved when combined. Dealing with changing body viewpoints and deformations of the body parts highlights the importance of having a good person tracker able to properly group the person detections along time. Actually, the results reported in this work on curved trajectories improve on the ones published in the conference version [27], thanks to the new stage that links broken tracks of persons (see Sec. 2.2).\nWith regard to the use of more than one level in PFM, we can see in Tab. 1 and Tab. 6 that similar results are obtained with the single- and two-level configurations. Although we tried an additional third level in the pyramid, the recognition rate did not increase. This fact indicates that, for most situations, is enough to use just a single vertical partition of the person\u2019s bounding-box to obtain very accurate results. However, for very complicated scenarios, as wearing coats on low resolution videos as CASIA-B, using two levels shows benefits as shown in Tab. 14.\nConcerning the contribution of the subtypes of descriptors in DCS, the experimental results suggest that (i) not all of them are strictly necessary, (ii) the normalized coordinates can be safely omitted, and (iii) in most cases the use of just the combination of div+curl with curl+shear is enough to achieve a very high recognition accuracy as shown in Tab. 4 and Tab. 6.\nAlthough we have defined the gait recognition problem in a multiple-camera setup, the results reported in tables 3, 8, 14 and 18 for the single camera case indicate that the proposed method is also valid for monocular environments. Thus, widening the range of application of our approach. A known limitation of our approach is the handle of trajectories perpendicular to the camera plane (i.e. perfectly frontal or backwards body viewpoint), where informative enough point trajectories cannot be computed. See for example, the case \u2018cl\u2019 in Tab. 14. In our opinion, for such particular cases, the addition of shape-based features could help.\nWith regard to changes in appearance of people, we can say from the results on CASIA-B and TUM GAID, that our system is able to deal very well with people wearing bags, although improvement is needed with strong changes in clothing. In addition, although our system was not initially designed to deal with outdoor infrared images, the results of CASIA-C clearly indicates that our PFM descriptor offers state-ofthe-art results on that kind of data.\nIn summary, we can conclude that the proposed PFM allows to identify subjects by their gait by using as basis local motion (i.e. short-term trajectories) and coarse structural information (i.e. spatial divisions on the person bounding-box). Moreover, PFM does not need either segmenting or aligning the gait cycle of each subject as done in previous works."}, {"heading": "8.1. Speed of the proposed system", "text": "To have an idea of the average speed of our system, we break down the time processing of the different stages comprising it. We have run this experiments on a state-of-the-art desktop computer with a CPU at\n3.47 GHz and 24 GB of RAM. The non-parallel code is mostly written in Matlab with some pieces of code written in C++. The average time, in seconds, needed to process a video sequence of 50 frames from CASIAB (320 \u00d7 240 pixels) is as follows: a) dense tracks computation, 8.95; b) person detection, 54.2; c) person tracking plus tracklets filtering, 0.62; d) PFM computation, 0.23; and, e) SVM classification, 0.02. Which makes a total of around 64 seconds for that kind of 50-frames video sequence. Clearly, the computational bottleneck is located on the person detection module. However, a GPU based person detector7 could be used instead. In the latter case, the system could achieve around 5 fps. A future improvement could be the speed-up of the dense tracking module by restricting the computation of the tracklets to smaller image regions guided by the person detector previously run, instead of processing the whole image frame and, then, removing unuseful tracklets, as currently done in this work."}, {"heading": "9. Conclusions", "text": "We have presented a new approach for recognizing human gait in video sequences. Our method builds a motion-based representation of the human gait by combining densely sampled local features and Fisher vectors: the Pyramidal Fisher Motion.\nThe results show that PFM allows to obtain a high recognition rate on a multicamera setup on the evaluated datasets: AVAMVG, CMU MoBo and CASIA (sets B and C) and on a single camera setup like TUM GAID. In the case of AVAMVG, a perfect identification of the individuals is achieved when we combine information from different cameras and the subjects follow a straight path. In addition, our pipeline shows a good behaviour on unconstrained paths, as shown by the experimental results \u2013 the model is trained on subjects performing straight walking trajectories and tested on curved trajectories. In the case of CMU MoBo, we have seen as our method is able to deal with differences in speed, as well as with cases where the movement of the arms is not available (i.e. holding an object with both hands). With regard to the PFM configuration, we have observed that it is beneficial to decorrelate (by using PCA) both the low-level motion features and the final PFM descriptor in order to achieve high recognition results and, in turn, decreasing the computational burden at test time \u2013 the classification with a linear SVM is extremely fast on 256-dimensional vectors. The experimental results also show that a single camera viewpoint is enough for recognition in many cases, even using just a short time interval of the video sequence \u2013 a PFM computed on around one second length sequence allows a perfect recognition on MoBo from a single viewpoint.\nFurthermore, the experiments on CASIA-B, CASIA-C and TUM GAID show that our system scales properly with the number of subjects, is able to handle changes in appearance and speed, as well as, is able to deal with recordings taken indoors, outdoors and during night.\nSince we use a person detector to localize the subjects, the proposed system in not restricted to deal with scenarios with static backgrounds. Moreover, the motion features used in this paper can be easily adapted to non static cameras by removing the global affine motion as proposed by Jain et al. in [1].\nIn conclusion, PFM enables a new way of tackling the problem of gait recognition on single and multiple viewpoint scenarios, removing the need of using people segmentation as mostly done so far."}, {"heading": "Acknowledgments", "text": "This work has been partially funded by the Research Projects TIN2012-32952 and BROCA, both financed by FEDER and the Spanish Ministry of Science and Technology; and by project TIC-1692 (Junta de Andaluc\u0301\u0131a). We also thank David Lo\u0301pez for his help with the setup of the AVAMVG dataset. Portions of the research in this paper use the CASIA Gait Database collected by Institute of Automation, Chinese Academy of Sciences.\n7The HOG-based person detector available in OpenCV library can run at 60 pfs on our computers."}], "references": [{"title": "Better exploiting motion for better action recognition", "author": ["M. Jain", "H. Jegou", "P. Bouthemy"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "in: Proceedings of the European Conference on Computer Vision (ECCV)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A framework for evaluating the effect of view angle", "author": ["S. Yu", "D. Tan", "T. Tan"], "venue": "clothing and carrying condition on gait recognition, in: Proceedings of the International Conference on Pattern Recognition, Vol. 4", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "The TUM Gait from Audio", "author": ["M. Hofmann", "J. Geiger", "S. Bachmann", "B. Schuller", "G. Rigoll"], "venue": "Image and Depth (GAID) database: Multimodal recognition of subjects and traits, Journal of Visual Communication and Image Representation 25 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The CMU Motion of Body (MoBo) Database", "author": ["R. Gross", "J. Shi"], "venue": "Tech. Rep. CMU-RI-TR-01-18, Robotics Institute ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "The AVA multi-view dataset for gait recognition", "author": ["D. L\u00f3pez-Fern\u00e1ndez", "F. Madrid-Cuevas", "A. Carmona-Poyato", "M. Ma\u0155\u0131n-Jim\u00e9nez", "R. Mu\u00f1oz Salinas"], "venue": "in: Activity Monitoring by Multiple Distributed Sensing, Lecture Notes in Computer Science", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing friends by their walk: Gait perception without familiarity cues", "author": ["J.E. Cutting", "L.T. Kozlowski"], "venue": "Bulletin of the psychonomic society 9 (5) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "A survey on visual surveillance of object motion and behaviors", "author": ["W. Hu", "T. Tan", "L. Wang", "S. Maybank"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on 34 (3) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Individual recognition using gait energy image", "author": ["J. Han", "B. Bhanu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 28 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "in: Proceedings of the International Conference on Computer Vision (ICCV), Vol. 2", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Similar gait action recognition using an inertial sensor", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 48 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The largest inertial sensor-based gait database and performance evaluation of gait-based personal authentication", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 47 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A cascade fusion scheme for gait and cumulative foot pressure image recognition", "author": ["S. Zheng", "K. Huang", "T. Tan", "D. Tao"], "venue": "Pattern Recognition 45 (10) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Infrared gait recognition based on wavelet transform and support vector machine", "author": ["Z. Xue", "D. Ming", "W. Song", "B. Wan", "S. Jin"], "venue": "Pattern Recognition 43 (8) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple HOG templates for gait recognition", "author": ["Y. Liu", "J. Zhang", "C. Wang", "L. Wang"], "venue": "in: Proceedings of the International Conference on Pattern Recognition, IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring the effects of video length on gait recognition", "author": ["R. Martin-Felez", "J. Ortells", "R. Mollineda"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Gait recognition by ranking", "author": ["R. Mart\u0301\u0131n-F\u00e9lez", "T. Xiang"], "venue": "in: Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Uncooperative gait recognition by learning to rank, Pattern Recognition", "author": ["R. Mart\u0301\u0131n-F\u00e9lez", "T. Xiang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Video from nearly still: an application to low frame-rate gait recognition", "author": ["N. Akae", "A. Mansur", "Y. Makihara", "Y. Yagi"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Enhanced gabor feature based classification using a regularized locally tensor discriminant model for multiview gait recognition", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 23 (7) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview gait recognition based on patch distribution features and uncorrelated multilinear sparse local discriminant canonical correlation analysis", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 24 (4) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Human gait recognition via sparse discriminant projection learning", "author": ["Z. Lai", "Y. Xu", "Z. Jin", "D. Zhang"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 24 (10) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Identification of people walking along curved trajectories", "author": ["Y. Iwashita", "K. Ogawara", "R. Kurazume"], "venue": "Pattern Recognition Letters 48 (0) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Fisher tensor decomposition for unconstrained gait recognition", "author": ["W. Gong", "M. Sapienza", "F. Cuzzolin"], "venue": "in: Proc. of Tensor Methods for Machine Learning, Workshop of the European Conference of Machine Learning", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Pyramidal Fisher Motion for multiview gait recognition", "author": ["F.M. Castro", "M. Ma\u0155\u0131n-Jim\u00e9nez", "R. Medina-Carnicer"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "in: Proc. of Scandinavian Conf. on Image Analysis, Vol. 2749", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "2D articulated human pose estimation and retrieval in (almost) unconstrained still images", "author": ["M. Eichner", "M.J. Ma\u0155\u0131n-Jim\u00e9nez", "A. Zisserman", "V. Ferrari"], "venue": "International Journal of Computer Vision 99 (2) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time affine region tracking and coplanar grouping", "author": ["V. Ferrari", "T. Tuytelaars", "L. Van Gool"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Detecting people looking at each other in videos", "author": ["M. Marin-Jimenez", "A. Zisserman", "M. Eichner", "V. Ferrari"], "venue": "International Journal of Computer Vision 106 (3) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Human Detection and Action Recognition in Video Sequences - Human Character Recognition in TV-Style Movies", "author": ["A. Kl\u00e4ser"], "venue": "Master thesis, Bonn-Rhein-Sieg University of Applied Sciences ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "N", "author": ["M. Ma\u0155\u0131n-Jim\u00e9nez"], "venue": "P\u00e9rez de la Blanca, M. Mendoza, Human action recognition from simple feature pooling, Pattern Analysis and Applications 17 (1) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "F", "author": ["E. Osuna", "R. Freund"], "venue": "Girosi, Support Vector Machines: training and applications., Tech. Rep. AI-Memo 1602, MIT ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1997}, {"title": "Silhouette-based human identification from body shape and gait", "author": ["R. Collins", "R. Gross", "J. Shi"], "venue": "in: Proceedings of the International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Appearance-based gait recognition using independent component analysis", "author": ["J. Liang", "Y. Chen", "H. Hu", "H. Zhao"], "venue": "in: Proc. Int. Conf. on Advances in Natural Computation", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Silhouette-based gait recognition using procrustes shape analysis and elliptic fourier descriptors", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 45 (9) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Gait recognition based on improved dynamic bayesian networks", "author": ["C. Chen", "J. Liang", "X. Zhu"], "venue": "Pattern Recognition 44 (4) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Gait probability image: An information-theoretic model of gait representation", "author": ["C.P. Lee", "A.W. Tan", "S.C. Tan"], "venue": "Journal of Visual Communication and Image Representation 25 (6) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient night gait recognition based on template matching", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Active energy image plus 2DLPP for gait recognition", "author": ["E. Zhang", "Y. Zhao", "W. Xiong"], "venue": "Signal Processing 90 (7) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Incremental learning for video-based gait recognition with LBP flow", "author": ["M. Hu", "Y. Wang", "Z. Zhang", "D. Zhang", "J. Little"], "venue": "Cybernetics, IEEE Transactions on 43 (1) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Human identification using temporal information preserving gait template", "author": ["C. Wang", "J. Zhang", "L. Wang", "J. Pu", "X. Yuan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust view-invariant multiscale gait recognition", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 48 (3) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition 47 (11) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "The humanid gait challenge problem: Data sets", "author": ["S. Sarkar", "P.J. Phillips", "Z. Liu", "I.R. Vega", "P. Grother", "K.W. Bowyer"], "venue": "performance, and analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (2) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Gait curves for human recognition", "author": ["A. DeCann", "A. Ross"], "venue": "backpack detection, and silhouette correction in a nighttime environment, in: SPIE conference on Biometric Technology for Human Identification", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Walker recognition without gait cycle estimation", "author": ["D. Tan", "S. Yu", "K. Huang", "T. Tan"], "venue": "in: S.-W. Lee, S. Li (Eds.), Advances in Biometrics, Vol. 4642 of Lecture Notes in Computer Science", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Orthogonal diagonal projections for gait recognition", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the IEEE International Conference on Image Processing, Vol. 1", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Gait recognition using wavelet packet silhouette representation and transductive support vector machines", "author": ["F. Dadashi", "B. Araabi", "H. Soltanian-Zadeh"], "venue": "in: Image and Signal Processing, 2009. CISP \u201909. 2nd International Congress on", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Uniprojective features for gait recognition", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Advances in Biometrics, Vol. 4642 of Lecture Notes in Computer Science", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing night walkers based on one pseudoshape representation of gait", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic gait recognition using weighted binary pattern on video", "author": ["W. Kusakunniran", "Q. Wu", "H. Li", "J. Zhang"], "venue": "in: Advanced Video and Signal Based Surveillance. AVSS \u201909. Sixth IEEE International Conference on", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Speed-invariant gait recognition based on procrustes shape analysis using higher-order shape configuration", "author": ["W. Kusakunniran", "Q. Wu", "J. Zhang", "H. Li"], "venue": "in: Proceedings of the IEEE International Conference on Image Processing", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Gait recognition across various walking speeds using higher order shape configuration based on a differential composition model", "author": ["W. Kusakunniran", "Q. Wu", "J. Zhang", "H. Li"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 42 (6) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "A robust speed-invariant gait recognition system for walker and runner identification", "author": ["Y. Guan", "C. Li"], "venue": "in: Biometrics (ICB), 2013 International Conference on", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved adaptive background mixture model for real-time tracking with shadow detection", "author": ["P. KaewTraKulPong", "R. Bowden"], "venue": "in: Video-Based Surveillance Systems, Springer", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "Dynamic distance-based shape features for gait recognition", "author": ["T. Whytock", "A. Belyaev", "N. Robertson"], "venue": "Journal of Mathematical Imaging and Vision 50 (3) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 2, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "Actually, humans are good recognizing people at a distance thanks to their gait [7], what provides a good (non invasive) way to identify people without requiring their cooperation, in contrast to other biometric approaches as iris or fingerprint analysis.", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "Although great effort has been put into this problem in recent years [8], it is still far from solved.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Popular approaches for gait recognition require the computation of the binary silhouettes of people [9], usually, by applying some background segmentation technique.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "These kind of descriptors have become recently popular in the field of human action recognition [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Bag of Words [11]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "In this paper we introduce a new gait descriptor, coined Pyramidal Fisher Vector, that combines the potential of recent human action recognition descriptors with the rich representation provided by Fisher Vectors encoding [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 11, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 158, "endOffset": 166}, {"referenceID": 12, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 158, "endOffset": 166}, {"referenceID": 13, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 204, "endOffset": 208}, {"referenceID": 7, "context": "For example, in [8] we can find a survey on this problem summarizing some of the most popular approaches.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 15, "context": "[16], to improve the gait recognition performance, propose the computation of HOG descriptors from popular gait descriptors as the GEI and the Chrono-Gait Image (CGI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], the authors try to find the minimum number of gait cycles needed to carry out a successful recognition by using the GEI descriptor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Martin-Felez and Xiang [18] [19], using GEI as the basic gait descriptor, propose a new ranking model for gait recognition.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "Martin-Felez and Xiang [18] [19], using GEI as the basic gait descriptor, propose a new ranking model for gait recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "In [20], Akae et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Hu proposes in [21] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "In addition, the same author defines in [22] a method to identify camera viewpoints at test time from patch distribution features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "[23] proposed a novel discriminant subspace learning method (Sparse Bilinear Discriminant Analysis) that extends methods based on matrix-representation discriminant analysis to sparse", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] explicitly focus on curved trajectories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] is a key reference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Then, they describe the video sequence by using the Bag of Words (BOW) model [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "In parallel, Perronnin and Dance [25] introduced a new way of histogram-based encoding for sets of local descriptors for image categorization: the Fisher Vector (FV) encoding.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "[26] is similar to ours in the sense that they propose a method that uses dense local spatio-temporal features and a Fisher-based representation rearranged as tensors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "A conference version of this paper was presented in [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Firstly, dense optical flow F = (ut, vt) is computed [28] on a", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "We use L = 15 as in [1].", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "[1], which is computed as follows: \uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 div(pt) = \u2202u(pt)", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "As described in [1], the divergence is related to axial motion, expansion and scaling effects, whereas the curl is related to rotation in the image plane.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Then, those kinematic features are combined in pairs as in [1] to get the final motion descriptors.", "startOffset": 59, "endOffset": 62}, {"referenceID": 28, "context": "We follow a tracking-by-detection strategy as in [29]: we detect people with the detection framework of Felzenszwalb et al.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "[31] to group detections into tracks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "full body) and an upper-body detector [32].", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "Inspired by the work of Kl\u00e4ser [33], after running both detectors (Fig.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "Therefore, we define a procedure similar to the Kl\u00e4ser\u2019s one [33] to combine UB and FB detections.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "We propose here the use of Fisher Vectors (FV) encoding [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "The FV, that can be seen as an extension of the Bag of Words (BOW) representation [11], builds on top of a Gaussian Mixture Model (GMM), where each Gaussian corresponds to a visual word.", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "N}, we can represent V by the following gradient vector [25]:", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "Following the proposal of [2], to compare two videos V and W , a natural kernel on these gradients is the Fisher Kernel: K(V,W ) = G\u03bb(V ) TF\u22121 \u03bb G\u03bb(W ), where F\u03bb is the Fisher Information Matrix.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "As stated in [2], the capability of description of the FV can be improved by applying it a signed square-root followed by L2 normalization.", "startOffset": 13, "endOffset": 16}, {"referenceID": 32, "context": "We borrow from [34] the idea of building a pyramidal representation of the gait motion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "Since, this is a multiclass problem, we train P binary Support Vector Machines (SVM) [35] (as many as different people) in a one-vs-all strategy.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "For computing the local motion features, we use the code published by the authors of [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "The first dataset where we perform our experiments is the \u201cAVA Multi-View Dataset for Gait Recognition\u201d (AVAMVG) [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "We use the popular Bag of Words approach (BOW) [11] as baseline, which is compared to our approach.", "startOffset": 47, "endOffset": 51}, {"referenceID": 47, "context": "We sample dictionary sizes in the interval [500, 4000] for BOW, and in the interval [50, 200] for PFM.", "startOffset": 84, "endOffset": 93}, {"referenceID": 14, "context": "2, each row corresponds to a different number of frames in the range [15, 50].", "startOffset": 69, "endOffset": 77}, {"referenceID": 47, "context": "2, each row corresponds to a different number of frames in the range [15, 50].", "startOffset": 69, "endOffset": 77}, {"referenceID": 4, "context": "The second dataset where we carry out our experiments is the \u201cCMU Motion of Body\u201d (MoBo) database [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "number of frames in the range [5, 40].", "startOffset": 30, "endOffset": 37}, {"referenceID": 38, "context": "number of frames in the range [5, 40].", "startOffset": 30, "endOffset": 37}, {"referenceID": 34, "context": "To put our results in context, the authors of [36] report the results of training on slow walk and testing on both fast walk and ball, obtaining 92% and 96% of accuracy, respectively.", "startOffset": 46, "endOffset": 50}, {"referenceID": 35, "context": "We can also compare with the results published in [37], where in one case they train on slow walk and test on ball, obtaining 91.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "Comparing with the results reported in [38], we outperform the cases of training on slow walk and testing on ball, and training on fast walk and testing on slow walk, obtaining 100% and 92% of accuracy, respectively.", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "In [39], the authors only perform the experiments in cases of training on slow walk and testing on fast walk and training on fast walk and testing on slow walk, obtaining 100% and 92% respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In the recent paper [40], the reported results on this dataset only use the combination of training on fast walk and testing on slow walk, achieving", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "The third dataset where we perform our experiments is \u201cCASIA Gait Dataset\u201d [41], parts B (CASIAB) [3] and C (CASIA-C) [42].", "startOffset": 98, "endOffset": 101}, {"referenceID": 39, "context": "The third dataset where we perform our experiments is \u201cCASIA Gait Dataset\u201d [41], parts B (CASIAB) [3] and C (CASIA-C) [42].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "AEI+2DLPP [43] 124 3 3-nm 2-bg-cl 98.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "GEI [3] 124 4 2 97.", "startOffset": 4, "endOffset": 7}, {"referenceID": 41, "context": "8 iHMM [44] 84 5 1 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 42, "context": "7 CGI [45] 124 1 1 88.", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": "3 VI-MGR [46] 124 4 2 100 89.", "startOffset": 9, "endOffset": 13}, {"referenceID": 44, "context": "SDL [47] 124 3 3-nm 2-bg-cl 98.", "startOffset": 4, "endOffset": 8}, {"referenceID": 56, "context": "We use the implementation of [59] included in Matlab.", "startOffset": 29, "endOffset": 33}, {"referenceID": 55, "context": "Note that, for example, row RSM [58] used three sequences per subject during training, instead of the two we use.", "startOffset": 32, "endOffset": 36}, {"referenceID": 41, "context": "IF+iHMM [44] 84 5 1 98.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "GEI+PCA+LDA [44] [48] 84 5 1 96.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "GEI+PCA+LDA [44] [48] 84 5 1 96.", "startOffset": 17, "endOffset": 21}, {"referenceID": 46, "context": "Method #subjects fn fs fq fb Gait Curves [49] 153 91.", "startOffset": 41, "endOffset": 45}, {"referenceID": 47, "context": "5 NDDP [50] 153 98.", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "0 ODP [51] 153 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 49, "context": "0 WPSR [52] 153 93.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "0 HTI [42] 46 94.", "startOffset": 6, "endOffset": 10}, {"referenceID": 50, "context": "0 HDP [53] 153 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 40, "context": "0 AEI [43] 153 88.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "7 Pseudoshape [54] 153 98.", "startOffset": 14, "endOffset": 18}, {"referenceID": 52, "context": "7 WBP [55] 153 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 53, "context": "1 HSC [56] 50 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 54, "context": "0 DCM [57] 120 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 55, "context": "0 RSM [58] 153 100 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "The fourth dataset where we perform our experiments is \u201cTUM Gait from Audio, Image and Depth (GAID) database\u201d [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 55, "context": "17 have been imported from the publication [58].", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "In [4], Hofmann et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "We run here the experiments proposed by the authors of the dataset at [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "In our experiments, we use the training and validation sets combined as in [4] for building the dictionary.", "startOffset": 75, "endOffset": 78}, {"referenceID": 44, "context": "SDL [47] - - - 96.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "GEI [4] 99.", "startOffset": 4, "endOffset": 7}, {"referenceID": 57, "context": "SEIM [60] 99.", "startOffset": 5, "endOffset": 9}, {"referenceID": 57, "context": "GVI [60] 99.", "startOffset": 4, "endOffset": 8}, {"referenceID": 57, "context": "SVIM [60] 98.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "RSM [58] 100.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "In addition, the FV-based formulation surpasses the BOW-based one, as stated by other authors in the problem of image categorization [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 26, "context": "Actually, the results reported in this work on curved trajectories improve on the ones published in the conference version [27], thanks to the new stage that links broken tracks of persons (see Sec.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2]. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6]. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths.", "creator": "LaTeX with hyperref package"}}}