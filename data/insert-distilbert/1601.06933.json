{"id": "1601.06933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "A Novel Memetic Feature Selection Algorithm", "abstract": "feature selection is a problem of finding efficient features among all features in which the final feature set can improve accuracy and reduce complexity. in feature selection algorithms search identification strategies are key information aspects. since feature selection is an np - hard problem ; therefore heuristic algorithms have been properly studied to solve this problem. in this paper, we have proposed discovering a method based it on memetic algorithm to find an efficient feature subset for solving a classification loss problem. it incorporates a filter method in the genetic algorithm formulation to improve classification performance and accelerates the search in identifying core candidate feature subsets. particularly, the method adds slowly or actively deletes a feature from a candidate feature subset based on the multivariate feature information. empirical study on commonly data compared sets of images the university of california, carson irvine shows that the proposed method significantly outperforms existing methods.", "histories": [["v1", "Tue, 26 Jan 2016 09:07:08 GMT  (233kb)", "http://arxiv.org/abs/1601.06933v1", "M., Montazeri, H. R. Naji, M. Montazeri, A. Faraahi. A novel memetic feature selection algorithm. In Information and Knowledge Technology (IKT), 2013 5th Conference on. 2013"]], "COMMENTS": "M., Montazeri, H. R. Naji, M. Montazeri, A. Faraahi. A novel memetic feature selection algorithm. In Information and Knowledge Technology (IKT), 2013 5th Conference on. 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohadeseh montazeri", "hamid reza naji", "mitra montazeri", "ahmad faraahi"], "accepted": false, "id": "1601.06933"}, "pdf": {"name": "1601.06933.pdf", "metadata": {"source": "CRF", "title": "A Novel Memetic Feature Selection Algorithm", "authors": ["Mohadeseh Montazeri", "Hamid Reza Naji", "Ahmad Faraahi"], "emails": ["mohadeseh_montazeri@yahoo.com", "mmontazeri@eng.uk.ac.ir", "hamidnaji@ieee.org", "afaraahi@pnu.ac.ir"], "sections": [{"heading": null, "text": "In this paper, we have proposed a method based on memetic algorithm to find an efficient feature subset for a classification problem. It incorporates a filter method in the genetic algorithm to improve classification performance and accelerates the search in identifying core feature subsets. Particularly, the method adds or deletes a feature from a candidate feature subset based on the multivariate feature information. Empirical study on commonly data sets of the university of California, Irvine shows that the proposed method outperforms existing methods.\nKeywords-Feature Selection; Memetic Algorithms; Meta-\nHeuristic Algorithms; Local search\nI. INTRODUCTION Most of real world classification problems require supervised [1] or semi-supervised [2] learning which use class information to establish a model to predict unseen instance. In these models, the underlying class probabilities and class conditional probabilities are unknown [3], and some instances are associated with a class label. In real-world situations, relevant features are often unknown a priori. Therefore, many candidate features are introduced to better represent the domain. In theory, more features should provide more discriminating power, but in practice, with a limited amount of training data, excessive features will not only significantly slow down the learning process, but also cause the classifier to over-fit the training data as uninformative features may confuse the learning algorithm. Feature Selection is the problem of selecting informative feature among all features in\nwhich a selected feature subset has lower cardinality and have higher accuracy. Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].\nIn general, a feature selection algorithm consists of four basic steps: subset generation, subset evaluation, stopping criterion, and result validation [11]. Subset generation is a search procedure. Basically, it generates subsets of features for evaluation. Let N denote the number of features in the original data set, then the total number of candidate subset is 2N which makes exhaustive search through the feature space infeasible with even moderate N. Each subset generated by the generation procedure needs to be evaluated by a certain evaluation criterion and compared with the previous best one with respect to this criterion. If it is found to be better, then it replaces the previous best subset. Without a suitable stopping criterion the feature selection process may run exhaustively before it stops. The selected best feature subset needs to be validated on both the selected subset and the original set and comparing the results using artificial data sets and/or realworld data sets.\nResearchers have studied various aspects of feature selection. There are two key aspects of feature selection: feature evaluation and search strategies. Feature evaluation is how to measure the goodness of a feature subset [12, 13]. There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.\nThe filters approach is based on the intrinsic properties of the data. The essence of filters is to seek the relevant features and eliminate the irrelevant ones. This method finds efficient features in one of two ways: univariate method and\n978-1-4673-6490-4/13/$31.00 \u00a92013 IEEE\nmultivariate method. In univariate method, the idea here is to compute some simple score S(i) that measures how informative each feature xi is about the class labels y. using this method have three problems. First problem, features that are not individually relevant should become deleted but they may become relevant in the context of others [20]; second problem, always the relevant feature is not useful one because of possible redundancies [21]. Third problem, when the features were ranked according to their scores S(i), if the number of effective feature is not determine, decide how many features is difficult and time consuming. Therefore, the second method in filter approach is attended. In this method, it takes into account feature dependencies. This method potentially achieves better results because they consider feature dependence [21] but it is obvious, they need search strategies in feature space to find the best feature subset. All filter based methods are fast and allow them to be used with any learner. An important disadvantage of filter methods is that they ignore the interaction with the classifier therefore they have low accuracy.\nBeside this method, wrapper approach is placed. Wrapper approach is a procedure that \u201cwraps\u201d around a learning algorithm, and accuracy of this learning algorithm is to be used to evaluate the goodness of different feature subsets. It is slow to execute because the learning algorithm is called repeatedly, but it has high accuracy. Another class is embedded methods which incorporate feature subset generation and evaluation in the training algorithm. This method like wrapper approach does not separate the learning from the feature selection part but incorporating knowledge about the specific structure of the classification or regression function. In actual, the structure of the class of functions under consideration plays a crucial role. Recently, hybrid models are proposed to combine the advantages of both filter models.\nFilters and wrappers differ mostly by the evaluation criterion. Both filter and wrapper methods can make use of search strategies to explore the space of all possible feature combinations that is usually too large to be explored exhaustively. Therefore, another important aspect i.e. search strategies [12] were studied by researchers.\nHeuristic search employs heuristics in conducting search. Due to polynomial complexity, it can be implemented very faster than previous searches. These methods do not guarantee the optimality or feasibility. Therefore many researches have been done to increase and guarantee the optimality.\nA meta-heuristic algorithm, which is kind of heuristic algorithm, by tightening a focus on good solutions and improving upon them (intensification), and to encourage the exploration of the solution space by broadening the focus of the search into new areas (diversification) can search solution space effectively [23]. Exploration and Exploitation are two competing goals govern the design of global search methods. Exploration is important to ensure global reliability, i.e., every part of the domain is searched enough to provide a reliable estimate of the global optimum; exploitation is also important\nsince it concentrates the search effort around the best solutions found so far by searching their neighborhoods to produce better solutions [37]. Meta-heuristc is capable of global exploration and locating new regions of the solution space to identify potential candidates, but there is no further focus on the exploitation aspect when a potential region is identified [24]. Thus, Memetic Algorithms (MA) which incorporate local improvement search into meta-heuristics, were proposed [25]. Experimental studies have been shown that a hybrid of a meta-heuristic and a local search is capable of more efficient search capabilities [26].\nIn this paper, we propose a feature selection algorithm based on memetic algorithm (MFS). The goal of our method is to improve classification performance and accelerate the search to identify important feature subsets. In particular, the filter method tunes the population of Genetic Algorithm (GA) solutions by adding or deleting features based on multivariate feature information. Hence, our focus is on filter methods that are able to assess the goodness of one feature in the context of others. We denote popular filter method, Pearson Correlation Coefficient, as our filter method in this correspondence. Furthermore, we investigate the balance between exploitation and exploration. Empirical study of our method on most commonly used data sets from the University of California, Irvine (UCI) repository [27] indicates that it outperforms recent existing methods. The rest of this article is organized as follow: Section 2 describes memetic algorithm. In Section 3, we explain MFS. The experimental results and conclusion are presented in Section 4 and 5, respectively.\nII. MEMETIC ALGORITHM Memetic algorithms [25] are population-based metaheuristic search approaches that have been receiving increasing attention in the recent years. They are inspired by Neo-Darwinian\u2019s principles of natural evolution and Dawkins\u2019 notion of a meme defined as a unit of cultural evolution that is capable of local refinements. Generally, Memetic algorithms may be regarded as a marriage between a population-based global search and local improvement procedures. It has shown to be a successful and popular method for solving optimization problems in many contexts [39].\nFig.1 represents general framework of memetic algorithm. As you can see, two important searches have done on a problem, global and local search. With these two searches, exploration and exploitation have done properly and hence solution space is searched effectively. Nevertheless, Metaheuristc algorithms generally suffer from excessively slow convergence to locate a precise enough solution because of their failure to exploit local information.\nMemetic algorithm often limits the practicality of Metaheuristc algorithms on many large-scale real world problems where the computational time is a crucial consideration.\nIII. MFS Figure. 2 show the procedure of MFS. In the first step, the GA population is initialized randomly which each chromosome is encoding a candidate feature subset. Then, on the elite chromosomes, a local search or meme is applied. The mechanism of local improvement can be reaching a local optimum or improving the solution. Genetic operators are then used to generate the next population. This process repeats until the stopping conditions are satisfied.\nThe local search performs a search on complete solutions to improve their solutions. It causes improving the quality of the candidate solution at hand. The goal of local search is to produce a better candidate solution at each step to increase exploitation.\nA. Chromosome Encoding In the MFS, encoding solution is a binary string with a length equal to the total number of features. Each bit encodes a single feature. A bit of \u201c1\u201d implies the corresponding feature is selected and \u201c0\u201d is not so that the length of the chromosome is N.\nB. Fitness Function To evaluate the goodness of each feature subset which is generated by each chromosome, we use accuracy of classification. It can be defined based on (1):\n( )Fit J fs= (1) Where fs denotes the corresponding selected feature subset encoded in solution, J compute the goodness of feature subset.\nIn this paper, it is accuracy of classification. Note that if two solutions have the same fitness, which one has smaller feature number is selected.\nC. Evolutionary Operations Evolutionary operations are selection, crossover and mutation. In selection, we use a rank based elitism roulette"}, {"heading": "Procedure of MFS", "text": ""}, {"heading": "1 Begin", "text": "2 Initialize: Randomly initialize population of feature subset, initialize E and others; 3 While (stop if condition is not satisfied) 4 Evaluate fitness of all feature subset encoded in the population; 5 Find E best feature subset in the population and put them into elite pop; 6 For (each subset in elite pop )\n7 Perform local search and replace it with new feature subset; 8 End For 9 Evaluate fitness of new solutions which is generated by local search; 10 Select the best solution based on fitness function; 11 Perform evolutionary operators, i.e. selection, crossover, mutation; 12 End While 13 End\nFigure 2. The procedure of MFS.\nwheel selection which is based on the fitness of chromosomes. It should ensure that fitter chromosomes have better chance to survive. We use one-point crossover such that if two parent chromosomes C1 and C2 are selected, they perform crossover operation with a crossover probability Pc to generate two new chromosomes Off1 and Off2 with exchanging information in a randomly cut point. In mutation operator selects some positions with a mutation probability Pm randomly and invert genes at these positions.\nD. The Local Search Improvement Procedure Our local search iterate over each feature of candidate feature subset. At each step, one feature is deleted or added (it depend to candidate feature subset). New feature subset is evaluated, if an improvement is achieved, new feature subset is accepted as the current one. Then the iteration continues with another feature. This process iterates for L (LS steps) times.\nFigure. 3 show the procedure of our local search procedure. In fact, both delete and add features are possible.\nE. Feature Subset Evaluation filter(FE) In local search there is a filter evaluation function to evaluate solution which is generated during the local search procedure. In our method, for improving our work\neffectively, we used Pearson\u2019s correlation coefficient (3) which is a famous subset evaluation filter.\nCorrelation coefficients are the simplest approach to feature relevance measurements. The linear correlation coefficient of Pearson is very popular in statistics and represents the normalized measure of the strength of linear relationship between variables [28]. For random variable X with value x and random variable Y with value y it is defined as (2)."}, {"heading": "Procedure of Local Search", "text": ""}, {"heading": "1 Begin", "text": "2 Input: Elite population; 3 Initialize: K; 4 For (each feature subset in elite population(E), Ei) 5 For (number of K) 6 Ebest = Ei; 7 Add or delete each feature in Ei; 8 calculate filter evaluation of improved feature\nsubset fs using FE(fs);\n( )1 cf k\nff\nkr J\nk k k r = + \u2212\n9 If (FE(fs)>FE(Ebest)) 10 Ebest = fs; 11 Else 12 change feature subset in original format; 13 End If 14 Replace Ei with Ebest; 15 End For 16 End For 17 End\nWhich is equal to \u00b1 1 if X and Y are linearly dependent and zero if they are completely uncorrelated. Some random variables may be correlated positively, and some negatively.\nIf a group of k features variables has already been existed, correlation coefficients may be used to estimate correlation between this group and the class variable, including intercorrelations between the features. Relevance of a group of features grows with the correlation between features and classes, and decreases with growing inter-correlation. These ideas have been discussed in theory of psychological measurements [29] and in the literature on decision making and aggregating opinions [30]. In 1964, Ghiselli [29] proposed (3):\n(3)\nWhere cfr is the average correlation coefficient between these k features and the output variables and the average between different features as ffr . This formula is obtained from Pearson\u2019s correlation coefficient with all variables standardized. It has been used in the Correlation-based Feature Selection (CFS) algorithm [31].\nIV. EXPERIMENTAL RESULT AND DISCUSSION In this section, our experimental result is carried out to show the effectiveness of our method. In The following subsections, a brief description of dataset benchmark is given, and then our simulations results and comparison with literature works are presented and discussed.\nA. Database Description and preprocessing We use 6 benchmark datasets which are frequently used in literatures. They are from the University of California, Irvine (UCI) repository [27]. Table I shows description of these datasets. They are both nominal and numerical data.\nSince some of these datasets have missing values or continues values in uncontrolled rang, they have a preprocessing step before they are used. For missing values, we replaced them with the most frequently used values for\nnominal and numeric features. To control the range of continues features we normalize them in rang [0, 1].\n- K-Fold Cross Validation: This metric is used to estimate how accurately a predictive model will perform in practice and represents the probability that an instance was classified correctly. In k-fold cross-validation, the original sample is randomly partitioned into k subsamples. A single subsample is retained as the test data and the remaining k 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data.\nTABLE II. DESCRIPTION OF DATASETS.\nNo. Database N Number of instances Number of classes\n1. Lymphography 18 148 4\n2. isolet 617 1559 26\n3. Synthetic 60 600 6\n4. Audiology 69 226 24\n5. Dermatology 34 366 6\n6. Musk clean1 166 476 2 Then k results from the folds then can be averaged to produce a single estimation. In our study, due to being randomness, run 10 times and at each time a 10-fold cross validation which is commonly used is used [35], and the final results were their average values (10-10 fold CV).\nB. Performance evaluation In this section, we present an experimental study of MFS on commonly used UCI data sets. We employed a population size of 30 and generation number is 200. Crossover rate, Pc, and mutation rate. Pm, are 0.6 and 0.1, respectively. Table II shows the best and average accuracy of 5 runs of MFS on defined databases. Because MFS is a random search algorithm, different results may be obtained at every run. We have run this algorithm on 5 runs and record average of them.\nC. Comparison Of literature Works We empirically evaluated the performance of MFS by comparing with recently methods, Ref. [40] and Ref. [41]. We have compared our method with two typical feature selectors: ReliefF and IG. ReliefF [42] is a popular instance-based feature weighting algorithms and Information Gain (IG) measures the decrease in entropy when the features are presented [43]. They are all well-known methods and have excellent performance. For ReliefF, we use 5 neighbors and 30 instances throughout the experiments as suggested by RobnikSikonja and Kononenko [44], which is also used in the literature [45]. The results of comparisons are reported in table III. In each row best results are bolded. As we can see in this table, in most cases the presented method (MFS) has better results which are considerable in some databases.\nTABLE I.\n[4] Mitra, P., C.A. Murthy, and S.K. Pal, Unsupervised feature selection using feature similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002. 24(3): p. 301\u2013312.\n[5] Liu, H., H. Motoda, and L. Yu. Feature selection with selective sampling. in In Proceedings of the Nineteenth International Conference on Machine Learning. 2002b.\n[6] Robnik-Sikonja, M. and I. Kononenko, Theoretical and empirical analysis of Relief and ReliefF. Machine Learning, 2003. 53.\n[7] Kim, Y., W. Street, and F. Menczer. Feature selection for unsupervised learning via evolutionary search. in In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2000.\n[8] Dash, M., et al. Feature selection for clustering \u2013 a filter solution. in In Proceedings of the Second International Conference on Data Mining. 2002.\n[9] Hastie, T., R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Springer, 2001.\n[10] Miller, A.J., Subset Selection in Regression. 0002-Revised ed. English, ed. 2. 2002: CRC Press. 256.\n[11] Dash, M. and H. Liu, Feature selection methods for classifications. Intelligent Data Analysis: An International Journal, 1997. 1(3).\n[12] Doak, J., An evaluation of feature selection methods and their application to computer security. 1992: Davis CA: University of California, Department of Computer Science.\n[13] Liu, H. and H. Motoda, Less Is More, in Feature Extraction, Construction and Selection: A Data Mining Perspective H.L.a. H, Editor. 1998, Kluwer Academic. p. 3 - 12.\n[14] Siedlecki, W. and J. Sklansky, On automatic feature selection. International Journal of Pattern Recognition and Artificial Intelligence, 1988. 2: p. 197\u2013220.\n[15] Fayyad, U.M. and K.B. Irani. The attribute selection problem in decision tree generation. in In AAAI-92, Proceedings of the Ninth National Conference on Artificial Intelligence. 1992.\n[16] Liu, H. and R. Setiono. A probabilistic approach to feature selection - a filter solution. in International Conference on Machine Learning. 1996. Bari, Italy: Morgan Kaufmann Publishers.\n[17] Dy, J.G. and C.E. Brodley. Feature subset selection and order identification for unsupervised learning. in Seventeenth International Conference on Machine Learning. 2000.\n[18] John, G.H., R. Kohavi, and K. Pfleger. Irrelevant feature and the subset selection problem. in Eleventh International Conference Machine Learning. 1994.\n[19] Caruana, R. and D. Freitag. Greedy attribute selection. in In Proceedings of International Conference on Machine Learning (ICML-94). 1994. Menlo Park, California: AAAI Press / The MIT Press.\n[20] Guyon, I., et al., Feature Extraction Foundations and Applications. 2006: Springer.\n[21] Yu, L. and H. Liu, Efficient Feature Selection via Analysis of Relevance and Redundancy. Journal of Machine Learning Research, 2004: p. 1205\u20131224.\n[22] Langley, P. Selection of relevant features in machine learning. in AAAI Fall Symposium on Relevance. 1994: AAAI Press.\n[23] O'Brien, R.F.J., Ant Algorithm Hyperheuristic Approaches for Scheduling Problems. 2007, Nottingham.\n[24] Ang, J.H., K.C. Tan., and A.A. Mamun, An evolutionary memetic algorithm for rule extraction Expert Systems with Applications 2010. 37 p. 1302\u20131315\n[25] Moscato, P., On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts: Towards Memetic Algorithms, in Tech. Rep. Caltech Concurrent Computation Program, Report. 1989, California Institute of Technology: Pasadena, California, USA.\n[26] Merz, P. and B. Freisleben. A comparison of memetic algorithms, Tabu search, and ant colonies for the quadratic assignment problem. in congress on evolutionary computation 1999.\n[27] D. J. Newman, S.H., C. L. Blake, and C. J. Merz, UCI repository of machine learning databases. 1998.\n[28] Press, W.H., et al., Numerical recipes in C. The art of scientific computing. 1988, Cambridge, UK: Cambridge University Press.\n[29] Ghiselli, E.E., Theory of psychological measurement. 1964, New York: McGraw-Hill Education.\n[30] Hogarth, R.M., Methods for aggregating opinions. Decision making and change in human affairs 1977: Dordrecht, Holland: Reidel Publishing Co.\n[31] Hall, M.A., Correlation-based Feature Subset Selection for Machine Learning, in Department of Computer Science. 1999, University of Waikato: Waikato, N.Z.\n[35] GJ, M.K.A.D., C. Ambroise Analyzing microarray gene expression data. 2005: Wiley Series in Probability and Statistics.\n[37] Ong Y. S., Keane A. J., Meta-Lamarckian Learning in Memetic Algorithms, IEEE Transaction On Evolutionary Computation, VOL. 8, NO. 2, April 2004.\n[39] Ishibuchi, H., Yoshida, T., Murata, T., Balance between Genetic Search and Local Search in Memetic Algorithms for Multiobjective Permutation Flowshop Scheduling, IEEE Transactions on Evolutionary Computation, Vol.7, No.2, pp.204-223, April, 2003.\n[40] Sun, X., Liu, Y., Li, J., Zhu, J., Chen, H., & Liu, X, Feature evaluation and selection with cooperative game theory, pattern recognition, 2012.\n[41] Liu H., Sun J., Liu L., Zhang H., Feature selection with dynamic mutual information. Pattern Recognition 42(7): 1330- 1339 2009.\n[42] Kononenko, I. , Estimating attributes: analysis and extensions of relief, in: Proceedings of the 1994 European Conference on Machine Learning, 1994, pp. 171\u2013182.\n[43] Lee, C., Lee, G.G., Information gain and divergence based feature selection for machine learning based text categorization, Information Processing & Management 42 (1) (2006) 155\u2013165.\n[44] M. Robnik-Sikonja, I. Kononenko, Theoretical and empirical analysis of ReliefF and RReliefF, Machine Learning 53 (1) 23\u201369 2003.\n[45] Yu, L., Liu, H., Efficient feature selection via analysis of relevance and redundancy, Journal of Machine Learning Research 5 1205\u20131224 2004."}], "references": [{"title": "Introduction to Machine Learning 2004: The MIT", "author": ["E. Alpaydin"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Shouraki, Kernel-based metric learning for semi-supervised clustering", "author": ["M.S. Baghshah", "S.B"], "venue": "Neurocomputing 2010", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "stork, pattern classification Secon Edition ed", "author": ["R.O. Duda", "P.E. Hart", "D.G"], "venue": "CV Ref", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Unsupervised feature selection using feature similarity", "author": ["P. Mitra", "C.A. Murthy", "S.K. Pal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Feature selection with selective sampling", "author": ["H. Liu", "H. Motoda", "L. Yu"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Theoretical and empirical analysis of Relief and ReliefF", "author": ["M. Robnik-Sikonja", "I. Kononenko"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Feature selection for unsupervised learning via evolutionary search", "author": ["Y. Kim", "W. Street", "F. Menczer"], "venue": "Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Feature selection for clustering \u2013 a filter solution", "author": ["M Dash"], "venue": "Proceedings of the Second International Conference on Data Mining", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Subset Selection in Regression", "author": ["A.J. Miller"], "venue": "0002-Revised ed. English, ed", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Feature selection methods for classifications", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis: An International Journal,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "An evaluation of feature selection methods and their application to computer security", "author": ["J. Doak"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}, {"title": "Less Is More, in Feature Extraction, Construction and Selection: A Data Mining Perspective H.L.a", "author": ["H. Liu", "H. Motoda"], "venue": "H, Editor", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "On automatic feature selection", "author": ["W. Siedlecki", "J. Sklansky"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "The attribute selection problem in decision tree generation", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "Proceedings of the Ninth National Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "A probabilistic approach to feature selection - a filter solution", "author": ["H. Liu", "R. Setiono"], "venue": "in International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Feature subset selection and order identification for unsupervised learning", "author": ["J.G. Dy", "C.E. Brodley"], "venue": "in Seventeenth International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Irrelevant feature and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "Eleventh International Conference Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "Greedy attribute selection", "author": ["R. Caruana", "D. Freitag"], "venue": "Proceedings of International Conference on Machine Learning (ICML-94)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Feature Extraction Foundations and Applications", "author": ["I Guyon"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Efficient Feature Selection via Analysis of Relevance and Redundancy", "author": ["L. Yu", "H. Liu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Selection of relevant features in machine learning", "author": ["P. Langley"], "venue": "AAAI Fall Symposium on Relevance", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Ant Algorithm Hyperheuristic Approaches for Scheduling Problems", "author": ["R.F.J. O'Brien"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "An evolutionary memetic algorithm for rule extraction Expert Systems with Applications", "author": ["J.H. Ang", "K.C. Tan", "A.A. Mamun"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts: Towards Memetic Algorithms, in Tech", "author": ["P. Moscato"], "venue": "Rep. Caltech Concurrent Computation Program,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "A comparison of memetic algorithms, Tabu search, and ant colonies for the quadratic assignment problem. in congress on evolutionary computation", "author": ["P. Merz", "B. Freisleben"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Numerical recipes in C", "author": ["Press", "W.H"], "venue": "The art of scientific computing", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Methods for aggregating opinions. Decision making and change in human affairs 1977: Dordrecht, Holland: Reidel Publishing Co", "author": ["R.M. Hogarth"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1977}, {"title": "Correlation-based Feature Subset Selection for Machine Learning, in Department of Computer Science", "author": ["M.A. Hall"], "venue": "IEEE Transaction On Evolutionary Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Most of real world classification problems require supervised [1] or semi-supervised [2] learning which use class information to establish a model to predict unseen instance.", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "Most of real world classification problems require supervised [1] or semi-supervised [2] learning which use class information to establish a model to predict unseen instance.", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "In these models, the underlying class probabilities and class conditional probabilities are unknown [3], and some instances are associated with a class label.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 153, "endOffset": 159}, {"referenceID": 5, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 153, "endOffset": 159}, {"referenceID": 6, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 173, "endOffset": 179}, {"referenceID": 7, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 173, "endOffset": 179}, {"referenceID": 8, "context": "Feature selection has been an active and fruitful field of research and development for decades in statistical pattern recognition [4], machine learning [5, 6], data mining [7, 8] and statistics [9, 10].", "startOffset": 195, "endOffset": 202}, {"referenceID": 9, "context": "In general, a feature selection algorithm consists of four basic steps: subset generation, subset evaluation, stopping criterion, and result validation [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Feature evaluation is how to measure the goodness of a feature subset [12, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 11, "context": "Feature evaluation is how to measure the goodness of a feature subset [12, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 24, "endOffset": 31}, {"referenceID": 14, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 24, "endOffset": 31}, {"referenceID": 15, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 51, "endOffset": 58}, {"referenceID": 16, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 51, "endOffset": 58}, {"referenceID": 17, "context": "There are filter models [14-16] and wrapper models [17-19] with different emphases on dimensionality reduction or accuracy enhancement.", "startOffset": 51, "endOffset": 58}, {"referenceID": 18, "context": "First problem, features that are not individually relevant should become deleted but they may become relevant in the context of others [20]; second problem, always the relevant feature is not useful one because of possible redundancies [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": "First problem, features that are not individually relevant should become deleted but they may become relevant in the context of others [20]; second problem, always the relevant feature is not useful one because of possible redundancies [21].", "startOffset": 236, "endOffset": 240}, {"referenceID": 19, "context": "This method potentially achieves better results because they consider feature dependence [21] but it is obvious, they need search strategies in feature space to find the best feature subset.", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "search strategies [12] were studied by researchers.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "A meta-heuristic algorithm, which is kind of heuristic algorithm, by tightening a focus on good solutions and improving upon them (intensification), and to encourage the exploration of the solution space by broadening the focus of the search into new areas (diversification) can search solution space effectively [23].", "startOffset": 313, "endOffset": 317}, {"referenceID": 22, "context": "Meta-heuristc is capable of global exploration and locating new regions of the solution space to identify potential candidates, but there is no further focus on the exploitation aspect when a potential region is identified [24].", "startOffset": 223, "endOffset": 227}, {"referenceID": 23, "context": "Thus, Memetic Algorithms (MA) which incorporate local improvement search into meta-heuristics, were proposed [25].", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "Experimental studies have been shown that a hybrid of a meta-heuristic and a local search is capable of more efficient search capabilities [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "Memetic algorithms [25] are population-based metaheuristic search approaches that have been receiving increasing attention in the recent years.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "The linear correlation coefficient of Pearson is very popular in statistics and represents the normalized measure of the strength of linear relationship between variables [28].", "startOffset": 171, "endOffset": 175}, {"referenceID": 26, "context": "These ideas have been discussed in theory of psychological measurements [29] and in the literature on decision making and aggregating opinions [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "It has been used in the Correlation-based Feature Selection (CFS) algorithm [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "To control the range of continues features we normalize them in rang [0, 1].", "startOffset": 69, "endOffset": 75}], "year": 2013, "abstractText": "Feature selection is a problem of finding efficient features among all features in which the final feature set can improve accuracy and reduce complexity. In feature selection algorithms search strategies are key aspects. Since feature selection is an NP-Hard problem; therefore heuristic algorithms have been studied to solve this problem. In this paper, we have proposed a method based on memetic algorithm to find an efficient feature subset for a classification problem. It incorporates a filter method in the genetic algorithm to improve classification performance and accelerates the search in identifying core feature subsets. Particularly, the method adds or deletes a feature from a candidate feature subset based on the multivariate feature information. Empirical study on commonly data sets of the university of California, Irvine shows that the proposed method outperforms existing methods. Keywords-Feature Selection; Memetic Algorithms; MetaHeuristic Algorithms; Local search", "creator": "'Certified by IEEE PDFeXpress at 05/05/2013 10:06:26 PM'"}}}