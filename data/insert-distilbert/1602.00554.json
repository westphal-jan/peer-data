{"id": "1602.00554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "Graph-based Predictable Feature Analysis", "abstract": "we propose a new method intended for resolving the unsupervised automated extraction of predictable detection features from high - dimensional time - series, where high predictability is understood very generically as low variance decreases in the distribution of the next data flow point given exactly the current one. we show how this semantic objective can be be understood in terms of observed graph embedding as well as reporting how it corresponds to the information - theoretic measure of excess entropy in special cases. experimentally, we compare the approach to two other algorithms for the extraction of various predictable features, namely foreca and pfa, and show how it is able to outperform them in certain settings.", "histories": [["v1", "Mon, 1 Feb 2016 15:11:48 GMT  (239kb)", "https://arxiv.org/abs/1602.00554v1", null], ["v2", "Thu, 11 May 2017 12:41:25 GMT  (286kb)", "http://arxiv.org/abs/1602.00554v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bj\\\"orn weghenkel", "asja fischer", "laurenz wiskott"], "accepted": false, "id": "1602.00554"}, "pdf": {"name": "1602.00554.pdf", "metadata": {"source": "CRF", "title": "Graph-based Predictable Feature Analysis", "authors": ["Bj\u00f6rn Weghenkel", "Asja Fischer", "Laurenz Wiskott"], "emails": ["bjoern.weghenkel@rub.de", "fischer@iro.umontreal.ca", "laurenz.wiskott@rub.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n00 55\n4v 2\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "When we consider the problem of an agent (artificial or biological) interacting with its environment, its signal processing is naturally embedded in time. In such a scenario, a feature\u2019s ability to predict the future is a necessary condition for it to be useful in any behaviorally relevant way: A feature that does not hold information about the future is out-dated the moment it is processed and any action based on such a feature can only be expected to have random effects.\nAs an practical example, consider a robot interacting with its environment. When its stream of sensory input is high-dimensional (e.g., the pixel values from a camera), we are interested in mapping this input to a lower-dimensional representation to make subsequent machine learning steps and decision making more robust and efficient. At this point, however, it is crucial not to throw away\n\u2217bjoern.weghenkel@rub.de \u2020fischer@iro.umontreal.ca \u2021laurenz.wiskott@rub.de\ninformation that the input stream holds about the future as any subsequent decision making will depend on this information. The same holds for time series like video, weather, or business data: When performing classification or regression on the learned features, or when the data is modelled for instance by a (hidden) Markov model, we are mostly interested in features that have some kind of predictive power.\nStandard algorithms for dimensionality reduction (DR), like PCA, however, are designed to preserve properties of the data that are not (or at least not explicitly) related to predictability and thus are likely to waste valuable information that could be extracted from the data\u2019s temporal structure. In this paper we will therefore focus on the unsupervised learning of predictable features for high-dimensional time series, that is, given a sequence of data points in a high-dimensional vector space we are looking for the projection into a sub-space which makes predictions about the future most reliable.\nWhile aspects of predictability are (implicitly) dealt with through many different approaches in machine learning, only few algorithms have addressed this problem of finding subspaces for multivariate time series suited for predicting the future. The recently proposed forecastable component analysis (ForeCA) [10] is based on the idea that predictable signals can be recognized by their low entropy in the power spectrum while white noise in contrast would result in a power spectrum with maximal entropy. Predictable feature analysis (PFA) [17] focuses on signals that are well predictable through autoregressive processes. Another DR approach that was not designed to extract predictable features but explicitly takes into account the temporal structure of the data is slow feature analysis (SFA) [26]. Still, the resulting slow features can be seen as a special case of predictable features [6]. For reinforcement learning settings, predictive projections [20] and robotic priors [13] learn mappings where actions applied to similar states result in similar successor states. Also, there are recent variants of PCA that at least allow for weak statistical dependence between samples [11].\nAll in all, however, the field of unsupervised learning of predictable subspaces for time series is largely unexplored. Our contribution consists of a new measure of the predictability of learned features as well as of an algorithm for learning those. The proposed measure has the advantage of being very generic, of making only few assumptions about the data at hand, and of being easy to link to the information-theoretic quantity of predictive information [3], that is, the mutual information between past and future. The proposed algorithm, graph-based predictable feature analysis (GPFA), not only shows very competitive results in practice but also has the advantage of being very flexible, and of allowing for a variety of future extensions. Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12]. Moreover, GPFA could make use of potential speed-ups like spectral regression [4], include additional label information in its graph like in [8], or could be applied to non-vectorial data like text. Kernelization and other approaches to use GPFA in a non-linear way are discussed in Section 5.\nThe remaining paper is structured as follows. In Section 2 we derive the GPFA algorithm. We start by introducing a new measure of predictability (Section 2.1), a consistent estimate for it (Section 2.2), and a simplified version\nof the estimate which is used by the proposed algorithm as an intermediate step (Section 2.3). Then the link to the graph embedding framework is established in Sections 2.4 and 2.5. After describing three useful heuristics in Section 2.6, the core algorithm is summarized in Section 2.7 and an iterated version of the algorithm is described in Section 2.8. Afterwards the algorithm is analyzed with respect to its objective\u2019s close relation to predictive information (Section 2.9) and with respect to its time complexity (Section 2.10). Section 3 summarizes the most closely related approaches for predictable feature learning\u2014namely SFA, ForeCA, and PFA\u2014and Section 4 describes experiments on different datasets. We end with a discussion of limitations, open questions and ideas which shall be conducted by future research in Section 5 and with a conclusion in Section 6."}, {"heading": "2 Graph-based Predictable Feature Analysis", "text": "Given is a time series xt \u2208 RN , t = 1, . . . , S, as training data that is assumed to be generated by a stationary stochastic process (Xt)t of order p. The goal of GPFA is to find a lower-dimensional feature space for that process by means of an orthogonal transformation A \u2208 RN\u00d7M , leading to projected random variables Y t = A T Xt with low average variance given the state of the p previous time steps. We use X (p) t to denote the concatenation (X T t , . . . ,X T t\u2212p+1)\nT of the p predecessor of Xt+1 to simplify notation. The corresponding state values are vectors in RN \u00b7p and denoted by x (p) t ."}, {"heading": "2.1 Measuring predictability", "text": "We understand the predictability of the learned feature space in terms of the variance of the projected random variables Y t in this space: The lower their average variance given their p-step past, the higher the predictability. We measure this through the expected covariance matrix of Y t+1 given Y (p) t and minimize it in terms of its trace, i.e., we minimize the sum of variances in all principal directions. Formally, we look for the projection matrix A leading to a projected stochastic process (Y t)t with minimum\ntrace(E Y\n(p) t\n[cov(Y t+1|Y (p)t )]) . (1)\nFor simplicity, we refer to this as \u201cminimizing the variance\u201d in the following. When we make the generally reasonable assumption of p(Y t+1|Y (p)t = y (p) t ) being Gaussian, that makes the learned features a perfect fit to be used in combination with least-squares prediction models1. For non-Gaussian conditional distributions we assume the variance to function as an useful proxy for quantifying the uncertainty of the next step. Note, however, that assuming Gaussianity for the conditional distributions does not imply or require Gaussianity of Xt or of the joint distributions p(Xs,Xt), s 6= t, which makes the predictability measure applicable to a wide range of stochastic processes.\n1Note that in the Gaussian case the covariance not only covers the distribution\u2019s second\nmoments but is sufficient to describe the higher-order moments as well."}, {"heading": "2.2 Estimating predictability", "text": "In practice, the expected value in (1) can be estimated by sampling a time series y1, . . . ,yS from the process (Y t)t. However, the empirical estimate for the covariance matrices cov(Y t+1|Y (p)t = y (p) t ), with y (p) t \u2208 RM\u00b7p, is not directly available because there might be only one sample of Y (p) t+1 with previous state value y (p) t . Therefore we calculate a k-nearest neighbor (kNN) estimate instead. Intuitively, the sample size is increased by also considering the k points that are most similar (e.g., in terms of Euclidean distance) to y (p) t , assuming that a distribution p(Y t+1|Y (p)t = y\u2032 (p) t ) is similar to p(Y t+1|Y (p) t = y (p) t ) if y \u2032(p) t is close to y (p) t . In other words, we group together signals that are similar in their past p steps. To that end, a set K(p)t is constructed, containing the indices of all k nearest neighbors of y\n(p) t (plus the 0-st neighbor, t itself), i.e.,\nK(p)t := {i | y (p) i is kNN of y (p) t , i = 1, . . . , S} \u222a {t}. The covariance is finally estimated based on the successors of these neighbors. Formally, the k-nearest neighbor estimate of (1) is given by\ntrace(\u3008cov({yi+1 | i \u2208 K(p)t })\u3009t) , (2)\nwhere \u3008\u00b7\u3009t denotes the average over t. Note that the distance measure used for the k nearest neighbors does not necessarily need to be Euclidean. Think for instance of \u201cperceived similarities\u201d of words or faces.\nWhile we introduce the kNN estimate here to assess the uncertainty inherent in the stochastic process, we note that it may be of practical use in a deterministic setting as well. For a deterministic dynamical system the kNN estimate includes nearby points belonging to nearby trajectories in the dataset. Thus, the resulting feature space may be understood as one with small divergence of neighboring trajectories (as measured through the Lyapunov exponent, for instance)."}, {"heading": "2.3 Simplifying predictability", "text": "Finding the transformation A that leads to the most predictable (Y t)t in the sense of (1) becomes difficult through the circumstance that the predictability can only be evaluated after A has been fixed. The circular nature of this optimization problem motivates the iterated algorithm described in Section 2.8. As a helpful intermediate step we define a weaker measure of predictability that is conditioned on the input Xt instead of the features Y t and has a closed-form solution, namely minimizing\ntrace(E X\n(p) t\n[cov(Y t+1|X(p)t )])\nvia its k-nearest neighbor estimate\ntrace(\u3008cov({yi+1 | i \u2208 K\u0303(p)t })\u3009t) . (3)\nAnalogous to K(p)t , the set K\u0303 (p) t contains the indices of the k nearest neighbors of x (p) t plus t itself. Under certain mild mixing assumptions for the stochastic process, the text-book results on k-nearest neighbor estimates can be applied\nto auto-regressive time series as well [5]. Thus, in the limit of S \u2192 \u221e, k \u2192 \u221e, k/S \u2192 0, the estimated covariance\ncov({yi+1 | i \u2208 K\u0303(p)t }) = \u3008yi+1yTi+1\u3009i\u2208K\u0303(p)t \u2212 \u3008yi+1\u3009i\u2208K\u0303(p)t \u3008yi+1\u3009 T i\u2208K\u0303 (p) t\nconverges to\nE[Y t+1Y T t+1|X (p) t = x (p) t ]\u2212 E[Y t+1|X (p) t = x (p) t ]E[Y t+1|X (p) t = x (p) t ] T ,\ni.e., it is a consistent estimator of cov(Y t+1|X(p)t = x (p) t ).\nWhen measuring predictability, one assumption made about the process (Xt)t in the following is that it is already white, i.e., E[Xt] = 0 and cov(Xt) = I for all t. Otherwise components with lower variance would tend to have higher predictability per se."}, {"heading": "2.4 Predictability as graph", "text": "Instead of optimizing objective (3) directly, we reformulate it such that it can be interpreted as the embedding of an undirected graph on the set of training samples. Consider the graph to be represented by a symmetric connection matrix W = (Wij)ij \u2208 RS\u00d7S with weights Wij = Wji > 0 whenever two nodes corresponding to vectors xi and xj from the training sequence are connected by an edge {xi,xj}. Further assume an orthogonal transformation A \u2208 RN\u00d7M for that graph with M \u226a N that minimizes\nS \u2211\ni,j=1\nWij\u2016ATxi \u2212ATxj\u20162 = S \u2211\ni,j=1\nWij\u2016yi \u2212 yj\u20162 . (4)\nIntuitively, this term becomes small if the projections of points connected in the graph (i.e., nodes for which Wij > 0) are close to each other, while there is no penalty for placing the projections of unconnected points far apart.\nThrough a proper selection of the weights Wij , the transformation A can be used to maximize predictability in the sense of minimizing (3). This becomes clear by noting that the trace of the sample covariance\ncov({yi+1 | i \u2208 K\u0303(p)t }) = \u3008yi+1yTi+1\u3009i\u2208K\u0303(p)t \u2212 \u3008yi+1\u3009i\u2208K\u0303(p)t \u3008yi+1\u3009 T i\u2208K\u0303 (p) t\n= \u3008(yi+1 \u2212 yi+1)(yi+1 \u2212 yi+1)T \u3009i\u2208K\u0303(p)t ,\nwith yi+1 = \u3008yi+1\u3009i\u2208K\u0303(p)t being the sample mean, can always be formulated via pairwise differences of samples, since\ntrace(\u3008(yi+1 \u2212 yi+1)(yi+1 \u2212 yi+1)T \u3009i\u2208K\u0303(p)t )\n= \u3008(yi+1 \u2212 yi+1)T (yi+1 \u2212 yi+1)\u3009i\u2208K\u0303(p)t = \u3008yTi+1yi+1\u3009i\u2208K\u0303(p)t \u2212 \u3008yi+1\u3009 T i\u2208K\u0303 (p) t\n\u3008yj+1\u3009j\u2208K\u0303(p)t = \u3008yTi+1yi+1 \u2212 yTi+1yj+1\u3009i,j\u2208K\u0303(p)t = 1\n2 \u3008yTi+1yi+1 \u2212 2yTi+1yj+1 + yTj+1yj+1\u3009i,j\u2208K\u0303(p)t\n= 1\n2 \u3008\u2016yi+1 \u2212 yj+1\u20162\u3009i,j\u2208K\u0303(p)t .\n(5)\nThus, by incrementing weights2 of the edges {yi+1,yj+1} for all i, j \u2208 K\u0303(p)t , t = p, . . . , S \u2212 1, minimizing (4) directly leads to the minimization of (3).\nNote that for the construction of the graph, the data actually does not need to be represented by points in a vector space. Data points also could, for instance, be words from a text corpus as long as there are either enough samples per word or there is an applicable distance measure to determine \u201cneighboring words\u201d for the k-nearest neighbor estimates."}, {"heading": "2.5 Graph embedding", "text": "To find the orthogonal transformation A = (a1, a2, . . . , aM ) \u2208 RN\u00d7M that minimizes (4), let the training data be concatenated in X = (x1,x2, . . . ,xS) \u2208 R N\u00d7S , and let D \u2208 RS\u00d7S be a diagonal matrix with Dii = \u2211\nj Wij being the sum of edge weights connected to node xi. Let further L := D \u2212 W be the graph Laplacian. Then, the minimization of (4) can be re-formulated as a minimization of\n1\n2\nS \u2211\ni,j=1\nWij\u2016ATxi \u2212ATxj\u20162\n= 1\n2\nS \u2211\ni,j=1\nWij trace((A Txi \u2212ATxj)(ATxi \u2212ATxj)T )\n= trace(\nS \u2211\ni=1\nATxiDiix T i A\u2212\nS \u2211\ni,j=1\nATxiWijx T j A)\n= trace(ATX(D\u2212W)XTA) = trace(ATXLXTA) = M \u2211\ni=1\naTi XLX T ai . (6)\nThe ai that minimize (6) are given by the first (\u201csmallest\u201d) M eigenvectors of the eigenvalue problem\nXLXTa = \u03bba . (7)\nSee [12] for the analogous derivation of the one-dimensional case that was largely adopted here as well as for a kernelized version of the graph embedding."}, {"heading": "2.6 Additional heuristics", "text": "The following three heuristics proved to be useful for improving the results in practice.\nNormalized graph embedding\nFirst, in the context of graph embedding, the minimization of aTXLXTa described in the section above is often solved subject to the additional constraint aTXDXTa = 1 (see for instance [12, 25]). Through this constraint the projected data points are normalized with respect to their degree of connectivity in every component Y = XTa, i.e., YTDY = 1. Objective function and constraint can be combined in the Lagrange function aTXLXTa \u2212 \u03bb(aTXDXTa \u2212 1). Then\n2All edge weights are initialized with zero.\nthe solution is given by the \u201csmallest\u201d eigenvectors of the generalized eigenvalue problem\nXLXTa = \u03bbXDXT a . (8)\nSolving this generalized eigenvalue problem instead of (7) tended to improve the results for GPFA.\nMinimizing variance of the past\nSecond, while not being directly linked to the above measure of predictability, results benefit significantly when the variance of the past is minimized simultaneously to that of the future. To be precise, additional edges {yi\u2212p,yj\u2212p} are added to the graph for all i, j \u2208 K\u0303(p)t , t = p + 1, . . . , S. The proposed edges here have the effect of mapping states with similar futures to similar locations in feature space. In other words, states are represented with respect to what is expected in the next steps (not with respect to their past). Conceptually this is related to the idea of causal states [19], where all (discrete) states that share the same conditional distribution over possible futures are mapped to the same causal state (also see [22] for a closely related formulation in interactive settings).\nStar-like graph structure\nAs a third heuristic, the graph above can be simplified by replacing the sample mean yi+1 in the minimization objective\ntrace(\u3008cov({yi+1 | i \u2208 K\u0303(p)t })\u3009t) = trace(\u3008(yi+1 \u2212 yi+1)(yi+1 \u2212 yi+1)T \u3009i\u2208K\u0303(p)t )\n= \u3008\u2016yi+1 \u2212 yi+1\u20162\u3009i\u2208K\u0303(p)t by yt+1. This leads to\n\u3008\u2016yi+1 \u2212 yt+1\u20162\u3009i\u2208K\u0303(p)t , (9)\ninducing a graph with star-like structures. It is constructed by adding (undirected) edges {yi+1,yt+1} for all i \u2208 K\u0303(p)t . Analogously, edges for reducing the variance of the past are given by {yi\u2212p,yt\u2212p} for i \u2208 K\u0303(p)t .\nWe refer to the resulting algorithms as GPFA (1) and GPFA (2), corresponding to the graphs defined through (5) and (9), respectively. See Figure 1 for an illustration of both graphs. The differences in performance are empirically evaluated in Section 4."}, {"heading": "2.7 Algorithm", "text": "In the following, the core algorithm is summarized step by step, where training data x1, . . . ,xS is assumed to be white already or preprocessed accordingly (in that case, the same transformation has to be taken into account during subsequent feature extractions). Lines starting with (1) and (2) indicate the steps for GPFA (1) and GPFA (2), respectively.\n1. Calculate neighborhood\nFor every x (p) t , t = p, . . . , S, calculate index set K\u0303 (p) t of k nearest neighbors (plus t itself).\n2. Construct graph (future)\nInitialize connection matrix W to zero. For every t = p, . . . , S \u2212 1, add edges, according to either\n(1) Wi+1,j+1 \u2190 Wi+1,j+1 + 1 \u2200i, j \u2208 K\u0303(p)t or (2) Wi+1,t+1 \u2190 Wi+1,t+1 + 1 and\nWt+1,i+1 \u2190 Wt+1,i+1 + 1 \u2200i \u2208 K\u0303(p)t \\ {t}.\n3. Construct graph (past)\nFor every t = p+ 1, . . . , S, add edges, according to either\n(1) Wi\u2212p,j\u2212p \u2190 Wi\u2212p,j\u2212p + 1 \u2200i, j \u2208 K\u0303(p)t or (2) Wi\u2212p,t\u2212p \u2190 Wi\u2212p,t\u2212p + 1 and\nWt\u2212p,i\u2212p \u2190 Wt\u2212p,i\u2212p + 1 \u2200i \u2208 K\u0303(p)t \\ {t}.\n4. Linear graph embedding\nCalculate L and D as defined in Section 2.5.\nFind the first (\u201csmallest\u201d) M solutions to XLXTa = \u03bbXDXT a and normalize them, i.e., \u2016a\u2016 = 1."}, {"heading": "2.8 Iterated GPFA", "text": "As shown in Section 2.4, the core algorithm above produces features (Y t)t with low trace(E X\n(p) t [cov(Y t+1|X(p)t )]). In many cases these features may already be predictable in themselves, that is, they have a low trace(E\nY (p) t [cov(Y t+1|Y (p)t )]). There are, however, cases where the results of both objectives can differ significantly (see Figure 2 for an example of such a case). Also, the k-nearest neighbor estimates of the covariances become increasingly unreliable in higherdimensional spaces.\nTherefore, we propose an iterated version of the core algorithm as a heuristic to address these problems. First, an approximation of the desired covariances cov(Y t+1|Y (p)t = y (p) t ) can be achieved by rebuilding the graph according to neighbors of y (p) t , not x (p) t . This in turn may change the whole optimization problem, which is the reason to repeat the whole procedure several times. Second, calculating the sample covariance matrices based on the k nearest neighbors of y (p) t \u2208 RM\u00b7p instead of x (p) t \u2208 RN \u00b7p counteracts the problem of unreliable k-nearest neighbor estimates in high-dimensional spaces, since M \u00b7 p \u226a N \u00b7 p. The resulting (iterated) GPFA algorithm works like this:\na) Calculate neighborhoods K\u0303(p)t of x (p) t for t = p, . . . , S \u2212 1.\nb) Perform steps 2\u20134 of GPFA as described in Section 2.7.\nc) Calculate projections yt = A Txt for t = 1, . . . , S.\nd) Calculate neighborhoods3 K(p)t of y (p) t for t = p, . . . , S \u2212 1.\ne) Start from step b), using K(p)t instead of K\u0303 (p) t .\nwhere steps b) to e) are either repeated for R iterations or until convergence. While we can not provide a theoretical guarantee for the iterative process to converge, it did so in practice in all of our experiments (see Section 4). Also note that in general there is no need for the dimensionality M of the intermediate projections yt \u2208 RM to be the same as for the final feature space."}, {"heading": "2.9 Relationship to predictive information", "text": "Predictive information\u2014that is, the mutual information between past states and future states\u2014has been used as a natural measure of how well-predictable a stochastic process is (e.g., [2] and [19]). In this section we discuss under which conditions the objective of GPFA corresponds to extracting features with maximal predictive information.\nConsider again the stationary stochastic process (Xt)t of order p and its extracted features Y t = A T Xt. Their predictive information is given by\nI(Y t+1;Y (p) t ) = H(Y t+1)\u2212H(Y t+1|Y (p) t ) , (10)\nwhere H(Y t+1) = E[\u2212 log p(Y t+1)] denotes the entropy and\nH(Y t+1|Y (p)t ) = EY t+1,Y (p)t [\u2212 log p(Y t+1|Y (p) t )]\n3Of course, this step is not necessary for the last iteration.\ndenotes the conditional entropy of (Y t+1)t given its past. If we assume Y t+1 to be normally distributed\u2014which can be justified by the fact that it corresponds to a mixture of a potentially high number of distributions from the original high-dimensional space\u2014then its differential entropy is given by H(Y t+1) = 1 2 log{(2\u03c0e)M}+ log{| cov(Y t+1)|} and is thus a strictly increasing function of the determinant of its covariance. Now recall that (Xt)t is assumed to have zero mean and covariance I. Thus, cov(Y t+1) = I holds independently of the selected transformation A which makes H(Y t+1) independent of A too.\nWhat remains for the maximization of (10) is the minimization of the term H(Y t+1|Y (p)t ). Again assuming Gaussian distributions, the differential conditional entropy is given by\nH(Y t+1|Y (p)t ) = 1\n2 log{(2\u03c0e)M}\n+ E Y\n(p) t\n[log{| cov(Y t+1|Y (p)t )|}] . (11)\nWhen we consider the special case of the conditional covariance cov(Y t+1|Y (p)t = y (p) t ) =: \u03a3Y t+1|Y (p)t being the same for every value that Y (p) t may take, then the expected value reduces to log{|\u03a3 Y t+1|Y (p) t\n|} and (11) becomes minimal for the projection for which the resulting determinant |\u03a3\nY t+1|Y (p) t | is minimized. Furthermore, under this assumption, (1) can be written as\ntrace(E Y\n(p) t [cov(Y t+1|Y (p)t )]) = trace(\u03a3\nY t+1|Y (p) t\n)\n= trace(AT\u03a3 Xt+1|Y (p) t A) .\nThus, it becomes clear that \u03a3 Y t+1|Y (p) t with minimal trace is constructed by selecting the principle directions from the N \u00d7N matrix \u03a3 Xt+1|Y (p) t that correspond to the M smallest eigenvalues. Thereby the determinant |\u03a3 Y t+1|Y (p) t | is\nminimized as well, since\u2014like for the trace\u2014its minimization only depends on the selection of the smallest eigenvalues. Thus, GPFA produces features with the maximum predictive information under this assumption of a prediction error\n\u03a3 Y t+1|Y (p) t\nindependent of the value of Y (p) t (and to the degree that the iterated\nheuristic in Section 2.8 minimizes (1)).\nFor the general case of different cov(Y t+1|Y (p)t ) for different values of Y (p) t\nwe have the following equality for the last term in (11):\nE Y (p) t\n[log{| cov(Y t+1|Y (p)t )|}]\n= E Y\n(p) t\n[trace(log{cov(Y t+1|Y (p)t )})]\n= trace(E Y\n(p) t\n[log{cov(Y t+1|Y (p)t )}]) . (12)\nThis corresponds to GPFA\u2019s objective (1) with logarithmically weighted covariances. Such a weighting intuitively makes sense from the perspective that the predictive information expresses how many bits of uncertainty (that is, variance) are removed through knowing about the feature\u2019s past. Since the number of bits only grows logarithmically with increasing variance, the weight of events with low uncertainty is disproportionally large, which could be accounted for in the objective function if the goal would be low coding length instead of low future variance."}, {"heading": "2.10 Time complexity", "text": "In the following section we derive GPFA\u2019s asymptotic time complexity in dependence of the number of training samples S, input dimensions N , process order p, output dimensions M , number of iterations R, as well as the neighborhood size k.\n2.10.1 k-nearest-neighbor search\nThe first computationally expensive step of the GPFA is the k-nearest-neighbor search. When we naively assume a brute-force approach, it can be realized in O(NpS). This search is repeated for each of the S data points and for each of the R iterations (in N \u00b7 p dimensions for the first iteration and in M \u00b7 p for all others). Thus, the k-nearest-neighbor search in the worst case has a time complexity of O(NpS2 +RMpS2) . Of course, more efficient approaches to k-nearest-neighbor search exist."}, {"heading": "2.10.2 Matrix multiplications", "text": "The second expensive step consists of the matrix multiplications in (8) to calculate the projected graph Laplacians. For a multiplication of two dense matrices of size l \u00d7 m and m \u00d7 n we assume a computational cost of O(lmn). If the first matrix is sparse, with L being the number of non-zero elements, we assume O(Ln). This gives us a complexity of O(N2S + LN) for the left-hand side of (8). For GPFA (1) there is a maximum of L = 2k2S non-zero elements (corresponding to the edges added to the graph, which are not all unique), for\nGPFA (2) there is a maximum of L = 2kS. The right-hand side of (8) then can be ignored since it\u2019s complexity of O(N2S + SN) is completely dominated by the left-hand side. Factoring in the number of iterations R, we finally have computational costs of O(RN2S +RLN) with L = k2S for GPFA (1) and L = kS for GPFA (2)."}, {"heading": "2.10.3 Eigenvalue decomposition", "text": "For solving the eigenvalue problem (8) R times we assume an additional time complexity of O(RN3). This is again a conservative guess because only the first M eigenvectors need to be calculated."}, {"heading": "2.10.4 Overall time complexity", "text": "Taking together the components above, GPFA has a time complexity of O(NpS2 + RMpS2 + RN2S + RLN + RN3) with L = k2S for GPFA (1) and L = kS for GPFA (2). In terms of the individual variables, that is: O(S2), O(N3), O(M), O(p), O(R), and O(k2) or O(k) for GPFA (1) or GPFA (2), respectively."}, {"heading": "3 Related methods", "text": "In this section we briefly summarize the algorithms most closely related to GPFA, namely SFA, ForeCA, and PFA."}, {"heading": "3.1 SFA", "text": "Although SFA originally has been developed to model aspects of the visual cortex, it has been successfully applied to different problems in technical domains as well (see [7] for a short overview), like, for example, state-of-the art age-estimation [9]. It is one of the few DR algorithms that considers the temporal structure of the data. In particular, slowly varying signals can be seen as a special case of predictable features [6]. It is also possible to reformulate the slowness principle implemented by SFA in terms of graph embedding, for instance to incorporate label information into the optimization problem [8].\nAdopting the notation from above, SFA finds an orthogonal transformation A \u2208 RN\u00d7M such that the extracted signals yt = ATxt have minimum temporal variation \u3008\u2016yt+1 \u2212 yt\u20162\u3009t. The input vectors xt\u2014and thus yt as well\u2014are assumed to be white."}, {"heading": "3.2 ForeCA", "text": "In case of ForeCA [10], (Xt)t is assumed to be a stationary second-order process and the goal of the algorithm is finding an extraction vector a such that the projected signals Yt = a T Xt are as forecastable as possible, that is, having a low entropy in their power spectrum. Like SFA, ForeCA has the advantage of being completely model- and parameter-free.\nFor the formal definition of forecastability, first consider the signal\u2019s autocovariance function \u03b3Y (l) = E(Yt \u2212 \u00b5Y )E(Yt\u2212l \u2212 \u00b5Y ), with \u00b5Y being the mean\nvalue and the corresponding autocorrelation function \u03c1Y (l) = \u03b3Y (l)/\u03b3Y (0). The spectral density of the process can be calculated as the Fourier transform of the autocorrelation function, i.e., as\nfY (\u03bb) = \u221e \u2211\nj=\u2212\u221e\n\u03c1Y (j)e ij\u03bb ,\nwith i = \u221a \u22121 being the imaginary unit.\nSince fY (\u03bb) \u2265 0 and \u222b \u03c0\n\u2212\u03c0 fY (\u03bb)d\u03bb = 1, the spectral density can be interpreted as a probability density function and thus its entropy calculated as\nH(Yt) = \u2212 \u222b \u03c0\n\u2212\u03c0\nfY (\u03bb) log(fY (\u03bb))d\u03bb .\nFor white noise the spectral density becomes uniform with entropy log(2\u03c0). This motivates the definition of forecastability as\n\u2126(Yt) := 1\u2212 H(Yt)\nlog(2\u03c0) ,\nwith values between 0 (white noise) and \u221e (most predictable). Since \u2126(Yt) = \u2126(aTXt) is invariant to scaling and shifting, Xt can be assumed to be white, without loss of generality. The resulting optimization problem\nargmax a \u2126(aTXt)\nthen is solved by an EM-like algorithm that uses weighted overlapping segment averaging (WOSA) to estimate the spectral density of a given (training) time series. By subsequently finding projections which are orthogonal to the already extracted ones, the approach can be employed for finding projections to higher dimensional subspaces as well. For details about ForeCA see [10]."}, {"heading": "3.3 PFA", "text": "The motivation behind PFA is finding an orthogonal transformationA \u2208 RN\u00d7M as well as coefficient matrices Bi \u2208 RM\u00d7M , with i = 1 . . . p, such that the linear, autoregressive prediction error of order p,\n\u3008\u2016ATxt \u2212 p \u2211\ni=1\nBiA Txt\u2212i\u20162\u3009t ,\nis minimized. However, this is a difficult problem to optimize because the optimal values of A and Bi mutually depend on each other. Therefore the solution is approached via a related but easier optimization problem: Let \u03b6t := (x T t\u22121, . . . ,x T t\u2212p)\nT \u2208 RN \u00b7p be a vector containing the p-step history of xt. Let further W \u2208 RN\u00d7N \u00b7p contain the coefficients that minimize the error of predicting xt from its own history, i.e., \u3008\u2016xt \u2212 W\u03b6t\u20162\u3009t. Then minimizing \u3008\u2016ATxt \u2212ATW\u03b6t\u20162\u3009t with respect to A corresponds to a PCA (in the sense of finding the directions of smallest variance) on that prediction error. Minimizing this prediction error however does not necessarily lead to features yt = A\nTxt that are best for predicting their own future because the calculated prediction\nwas based on the history of xt, not yt alone. Therefore an additional heuristic is proposed that is based on the intuition that the inherited errors of K times repeated autoregressive predictions create an even stronger incentive to avoid unpredictable components. Finally,\nK \u2211\ni=0\n\u3008\u2016ATxt \u2212ATWVi\u03b6t\u20162\u3009t\nis minimized with respect to A, where V \u2208 RN \u00b7p\u00d7N \u00b7p contains the coefficients that minimize the prediction error \u3008\u2016\u03b6t+1 \u2212V\u03b6t\u20162\u3009t.\nLike the other algorithms, PFA includes a preprocessing step to whiten the data. So far, PFA has been shown to work on artificially generated data. For further details about PFA see [17]."}, {"heading": "4 Experiments", "text": "We conducted experiments4 on different datasets to compare GPFA to SFA, ForeCA, and PFA. As a baseline, we compared the features extracted by all algorithms to features that were created by projecting into an arbitrary (i.e., randomly selected) M -dimensional subspace of the data\u2019s N -dimensional vector space.\nFor all experiments, first the training set was whitened and then the same whitening transformation was applied to the test set. After training, the learned projection was used to extract the most predictable M -dimensional signal from the test set with each of the algorithms. The extracted signals were evaluated in terms of their empirical predictability (2). The neighborhood size used for this evaluation is called q in the following to distinguish it from the neighborhood size k used during the training of GPFA. Since there is no natural choice for the different evaluation functions that effectively result from different q, we arbitrarily chose q = 10 but also include plots on how results change with the value of q. The size of training and test set will be denoted by Strain and Stest, respectively. The plots show mean and standard deviation for 50 repetitions of each experiment.5"}, {"heading": "4.1 Toy example (\u201cpredictable noise\u201d)", "text": "We created a small toy data set to demonstrate performance differences of the different algorithms. The data set contains a particular kind of predictable signals which are challenging to identify for most algorithms. Furthermore, the example is suited to get an impression for running time constants of the different algorithms that are not apparent from the big O notation in Section 2.10.\nFirst, a two-dimensional signal\nxt =\n(\n\u03bet \u03bet\u22121\n)\n(13)\n4GPFA and experiments have been implemented in Python 2.7. Code and datasets will be published upon acceptance. 5Note that while the algorithms themselves do not depend on any random effects, the data set generation does.\nwas generated with \u03bet being normally distributed noise. Half of the variance in this sequence can be predicted when xt\u22121 is known (i.e., p = 1), making the noise partly predictable. This two-dimensional signal was augmented with N\u22122 additional dimensions of normally distributed noise to create the full data set. We generated such data sets with up to Strain = 800 training samples, a fixed test set size of Stest = 100, and with up to N = 100 input dimensions and extracted M = 2 components with each of the algorithms. If not varied themselves during the experiment, values were fixed to Strain = 700 training samples, N = 10 input dimensions, and k = 10 neighbors for the training of GPFA. The results of PFA did not change significantly with number of iterations K, which was therefore set to K = 0.\nFigure 3 shows the predictability of the signals extracted by the different algorithms and how it varies in Strain, N , and k. Only ForeCA and GPFA are able to distinguish the two components of predictable noise from the unpredictable ones, as can be seen from reaching a variance of about 1, which corresponds to the variance of the two generated, partly predictable components. As Figure 3b shows, the performance of both versions of GPFA (as of all other algorithms) declines with a higher number of input dimensions (but for GPFA (2) less than for GPFA (1)). At this point, a larger number of training samples is necessary to produce more reliable results (experiments not shown). The results do not differ much with the choice of k though.\nAs the runtime plots of the experiments reveal (see Figure 4), ForeCA scales especially badly in the number of input dimensions N , so that it becomes very computationally expensive to be applied to time series with more than a few dozen dimensions. For that reason we excluded ForeCA from the remaining, high-dimensional experiments."}, {"heading": "4.2 Auditory data", "text": "In the second set of experiments we focused on short-time Fourier transforms (STFTs) of audio files. Three public domain audio files (a silent film piano soundtrack, ambient sounds from a bar, and ambient sounds from a forest) were re-sampled to 22kHz mono. The STFTs were calculated with the Python library stft with a frame length of 512 and a cosine window function, resulting in three datasets with 26147, 27427, and 70433 frames, respectively, each\nwith 512 dimensions (after discarding complex-conjugates and representing the remaining complex values as two real values each). For each repetition of the experiment, Strain = 10000 successive frames were picked randomly as training set and Stest = 5000 distinct and successive frames were picked as test set. PCA was calculated for each training set to preserve 99% of the variance and this transformation was applied to training and test set alike.\nThe critical parameters p and k, defining the assumed order of the process and the neighborhood size respectively, were selected through cross-validation to be a good compromise between working well for all values of M and also not treating one of the algorithms unfavourably. PFA and GPFA tend to benefit from the same values for p. The number of iteration R for GPFA was found to be not very critical and was set to R = 50. The iteration parameter K of PFA was selected by searching for the best result in {0 . . .10}, leaving all other parameters fixed.\nThe central results can be seen in Figures 5-7f in terms of the predictability of the components extracted by the different algorithms in dependence of their dimensionality M . The other plots show how the results change with the individual parameters. Increasing the number of past time steps p tends to improve the results first but may let them decrease later (see Figures 5-7a). Presumably, because higher numbers of p make the models more prone to overfitting. The neighborhood size k had to be selected carefully for each of the different datasets. While its choice was not critical on the first dataset, the second dataset benefited from low values for k and the third one from higher values (see Figures 5-7b). Similar, the neighborhood size q for calculating the final predictability of the results had different effects for different datasets (see Figures 5-7c). At this point it\u2019s difficult to favor one value over another, which is why we kept q fixed to q = 10. As expected, results tend to improve with increasing numbers of training samples Strain (see Figures 5-7d). Similarly, results first improve with the number of iterations R for GPFA and then remain stable (see Figures 5-7e). We take this as evidence for the viability of the iteration heuristic motivated in Section 2.8.\nTo gauge the statistical reliability of the results, we applied the Wilcoxon signed-rank test, testing the null hypothesis that the results for different pairs of algorithms actually come from the same distribution. We tested this hypothesis for each data set for the experiment with default parameters, i.e., for the results shown in Figures 5-7f with M = 5. As can be seen from the p-values in Table 1, the null hypothesis can be rejected with certainty in many cases, which confirms that GPFA (2) learned the most predictable features on two of three datasets.\nFor GPFA (1) the results are clear for the first dataset as well for the second in comparison to PFA. It remains a small probability, however, that the advantage compared to SFA on the second dataset is only due to chance. For the large third dataset, all algorithms produce relatively similar results with high variance between experiments. It depends on the exact value of M if SFA or GPFA produced the best results. For M = 5 GPFA happened to find slightly more predictable results (not highly significant though as can be seen in Table 1). But in general we don\u2019t see a clear advantage of GPFA on the third dataset."}, {"heading": "4.3 Visual data", "text": "A third experiment was conducted on a visual dataset. We modified the simulator from the Mario AI challenge [14] to return raw visual input in gray-scale without text labels. The raw input was scaled from 320\u00d7240 down to 160\u00d7120 dimensions and then the final data points were taken from a small window of 20\u00d7 20 = 400 pixels at a position where much of the game dynamics happened (see Figure 8 for an example). As with the auditory datasets, for each experiment Strain = 10000 successive training and Stest = 5000 non-overlapping test frames were selected randomly and PCA was applied to both, preserving 99% of the variance. Eventually, M predictable components were extracted by each of the algorithms and evaluated with respect to their predictability (2). Parameters p and k again were selected from a range of candidate values to yield the best results (see Figures 9a-b).\nTwo things are apparent from the results as shown in Figure 9. First, the choice of parameters was less critical compared to the auditory datasets. And second, all compared algorithms show quite similar results in terms of their predictability. GPFA only is able to find features slightly more predictable than those of SFA for higher values of M (see Figure 9f). Again, this observation is highly significant with a Wilcoxon p-value of 0.00 for M = 12."}, {"heading": "5 Discussion and Future work", "text": "In the previous section we saw that GPFA produced the most predictable features on a toy example with a certain kind of predictable noise as well as on two auditory datasets. However, on a third auditory dataset as well as on a visual dataset, GPFA did not show a clear advantage compared to SFA. This matches our experience with other visual datasets (not shown here). We hypothesize that SFA\u2019s assumption of the most relevant signals being the slow ones may especially suited for the characteristics of visual data. This also matches the fact that SFA originally was designed for and already proved to work well for signal extraction from visual data sets. A detailed analysis of which algorithm and corresponding measure of predictability is best suited for what kind of data or domain remains a subject of future research.\nIn practical terms we conclude that GPFA (2) has some advantages over GPFA (1). First, its linear time complexity in k (see Section 2.10.2) makes a notable difference in practice (see Section 4.1). Second, GPFA (2) consistently produced better results (see Section 4) which is a bit surprising given that the fully connected graph of GPFA (1) is theoretically more sound and also matches the actual evaluation criterion (2). Our intuition here is that it is beneficial to give yt+1 a central role in the graph because it is a more reliable estimate of the true mean of p(Y t+1|Y t = yt) than the empirical mean of all data points\n(stemming from different distributions) in the fully connected graph. In the form described above, GPFA performs linear feature extraction. However, we are going to point out three strategies to extend the current algorithm for non-linear feature extraction. The first strategy is very straight-forward and can be applied to the other linear feature extractors as well: In a preprocessing step, the data is expanded in a non-linear way, for instance through all polynomials up to a certain order. Afterwards, application of a linear feature extractor implicitly results in non-linear feature extraction. This strategy is usually applied to SFA, often in combination with hierarchical stacking of SFA nodes which further increases the non-linearities while at the same time regularizing spatially (on visual data) [7].\nThe other two approaches to non-linear feature extraction build upon the graph embedding framework. We already mentioned above that kernel versions of graph embedding are readily available [27, 4]. Another approach to non-linear graph embedding was described for an algorithm called hierarchical generalized SFA: A given graph is embedded by first expanding the data in a non-linear way and then calculating a lower-dimensional embedding of the graph on the expanded data. This step is repeated\u2014each time with the original graph\u2014 resulting in an embedding for the original graph that is increasingly non-linear with every repetition (see [21] for details).\nRegarding the analytical understanding of GPFA, we have shown in Section 2.9 under which assumptions GPFA can be understood as finding the features with the highest predictive information, for instance when the underlying process is assumed to be deterministic but its states disturbed by independent Gaussian noise. If we generally had the goal of minimizing the coding length of the extracted signals (which would correspond to high predictive information) rather than minimizing their next-step variance, then the covariances in GPFA\u2019s main objective (1) needed to be weighted logarithmically. Such an adoption, however, would not be straight forward to include into the graph structure.\nAnother information-theoretic concept relevant in this context (besides predictive information) is that of information bottlenecks [24]. Given two random variables A and B, an information bottleneck is a compressed variable T that solves the problem minp(t|a) I(A;T )\u2212\u03b2I(T ;B). Intuitively, T encodes as much\ninformation from A about B as possible while being restricted in complexity. When this idea is applied to time series such that A represents the past and B the future, then T can be understood as encoding the most predictable aspects of that time series. In fact, SFA has been shown to implement a special case of such a past-future information bottleneck for Gaussian variables [6]. The relationship between GPFA and (past-future) information bottlenecks shall be investigated in the future.\nIn Section 2.6 we introduced the heuristic of reducing the variance of the past in addition that of the future. Effectively this groups together parts of the feature space that have similar expected futures. This property may be especially valuable for interactive settings like reinforcement learning. When you consider an agent navigating its environment, it is usually less relevant to know which way it reached a certain state but rather where it can go to from there. That\u2019s why state representations encoding the agent\u2019s future generalize better and allow for more efficient learning of policies than state representations that encode the agent\u2019s past [15, 16]. To better address interactive settings, multiple actions may incorporated into GPFA by conditioning the kNN search on actions, for instance. Additional edges in the graph could also allow grouping together features with similar expected rewards. We see such extension of GPFA as an interesting avenue of future research."}, {"heading": "6 Conclusion", "text": "We presented graph-based predictable feature analysis (GPFA), a new algorithm for unsupervised learning of predictable features from high-dimensional time series. We proposed to use the variance of the conditional distribution of the next time point given the previous ones to quantify the predictability of the learned representations and showed how this quantity relates to the informationtheoretic measure of predictive information. As demonstrated, searching for the projection that minimizes the proposed predictability measure can be reformulated as a problem of graph embedding. Experimentally, GPFA produced very competitive results, especially on auditory STFT datasets, which makes it a promising candidate for every problem of dimensionality reduction (DR) in which the data is inherently embedded in time."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Predictive information", "author": ["William Bialek", "Naftali Tishby"], "venue": "e-print arXiv:cond-mat/9902341,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Predictability, complexity, and learning", "author": ["William Bialek", "Ilya Nemenman", "Naftali Tishby"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["Deng Cai", "Xiaofei He", "Jiawei Han"], "venue": "In Seventh IEEE International Conference on Data Mining (ICDM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Non parametric time series analysis and prediction: Uniform almost sure convergence of the window and k-nn autoregression estimates", "author": ["G\u00e9rard Collomb"], "venue": "Statistics: A Journal of Theoretical and Applied Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1985}, {"title": "Predictive coding and the slowness principle: An information-theoretic approach", "author": ["Felix Creutzig", "Henning Sprekeler"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Slow feature analysis: Perspectives for technical applications of a versatile learning algorithm", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "Ku\u0308nstliche Intelligenz [Artificial Intelligence],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "How to solve classification and regression problems on high-dimensional data with a supervised extension of slow feature analysis", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improved graphbased SFA: Information preservation complements the slowness principle", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "e-print arXiv:1601.03945,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Forecastable component analysis", "author": ["Georg Goerg"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Principal component analysis on non-gaussian dependent data", "author": ["Fang Han", "Han Liu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "Partha Niyogi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Learning state representations with robotic priors", "author": ["Rico Jonschkowski", "Oliver Brock"], "venue": "Autonomous Robots,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The Mario AI benchmark and competitions", "author": ["Sergey Karakovskiy", "Julian Togelius"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS 2001),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Using predictive representations to improve generalization in reinforcement learning", "author": ["Eddie J. Rafols", "Mark B. Ring", "Richard S. Sutton", "Brian Tanner"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Predictable feature analysis", "author": ["Stefan Richthofer", "Laurenz Wiskott"], "venue": "e-print arXiv:1311.2503,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Computational mechanics: Pattern and prediction, structure and simplicity", "author": ["Cosma Rohilla Shalizi", "James P. Crutchfield"], "venue": "Journal of Satistical Physics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Predictive projections", "author": ["Nathan Sprague"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On the relation of slow feature analysis and Laplacian eigenmaps", "author": ["Henning Sprekeler"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Information-theoretic approach to interactive learning", "author": ["Susanne Still"], "venue": "Europhysics Letters,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B. Tenenbaum", "Vin de Silva", "John C. Langford"], "venue": "Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C. Pereira", "William Bialek"], "venue": "e-print arXiv:physics/0004057,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["Laurenz Wiskott", "Terrence Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Hong-Jiang Zhang", "Qiang Yang", "Stephen Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "The recently proposed forecastable component analysis (ForeCA) [10] is based on the idea that predictable signals can be recognized by their low entropy in the power spectrum while white noise in contrast would result in a power spectrum with maximal entropy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Predictable feature analysis (PFA) [17] focuses on signals that are well predictable through autoregressive processes.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Another DR approach that was not designed to extract predictable features but explicitly takes into account the temporal structure of the data is slow feature analysis (SFA) [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 5, "context": "Still, the resulting slow features can be seen as a special case of predictable features [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 19, "context": "For reinforcement learning settings, predictive projections [20] and robotic priors [13] learn mappings where actions applied to similar states result in similar successor states.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "For reinforcement learning settings, predictive projections [20] and robotic priors [13] learn mappings where actions applied to similar states result in similar successor states.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Also, there are recent variants of PCA that at least allow for weak statistical dependence between samples [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "The proposed measure has the advantage of being very generic, of making only few assumptions about the data at hand, and of being easy to link to the information-theoretic quantity of predictive information [3], that is, the mutual information between past and future.", "startOffset": 207, "endOffset": 210}, {"referenceID": 26, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 235, "endOffset": 239}, {"referenceID": 0, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 297, "endOffset": 300}, {"referenceID": 3, "context": "Moreover, GPFA could make use of potential speed-ups like spectral regression [4], include additional label information in its graph like in [8], or could be applied to non-vectorial data like text.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Moreover, GPFA could make use of potential speed-ups like spectral regression [4], include additional label information in its graph like in [8], or could be applied to non-vectorial data like text.", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "to auto-regressive time series as well [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "(7) See [12] for the analogous derivation of the one-dimensional case that was largely adopted here as well as for a kernelized version of the graph embedding.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Normalized graph embedding First, in the context of graph embedding, the minimization of aXLXa described in the section above is often solved subject to the additional constraint aXDXa = 1 (see for instance [12, 25]).", "startOffset": 207, "endOffset": 215}, {"referenceID": 24, "context": "Normalized graph embedding First, in the context of graph embedding, the minimization of aXLXa described in the section above is often solved subject to the additional constraint aXDXa = 1 (see for instance [12, 25]).", "startOffset": 207, "endOffset": 215}, {"referenceID": 18, "context": "Conceptually this is related to the idea of causal states [19], where all (discrete) states that share the same conditional distribution over possible futures are mapped to the same causal state (also see [22] for a closely related formulation in interactive settings).", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "Conceptually this is related to the idea of causal states [19], where all (discrete) states that share the same conditional distribution over possible futures are mapped to the same causal state (also see [22] for a closely related formulation in interactive settings).", "startOffset": 205, "endOffset": 209}, {"referenceID": 1, "context": ", [2] and [19]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 18, "context": ", [2] and [19]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "1 SFA Although SFA originally has been developed to model aspects of the visual cortex, it has been successfully applied to different problems in technical domains as well (see [7] for a short overview), like, for example, state-of-the art age-estimation [9].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "1 SFA Although SFA originally has been developed to model aspects of the visual cortex, it has been successfully applied to different problems in technical domains as well (see [7] for a short overview), like, for example, state-of-the art age-estimation [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 5, "context": "In particular, slowly varying signals can be seen as a special case of predictable features [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "It is also possible to reformulate the slowness principle implemented by SFA in terms of graph embedding, for instance to incorporate label information into the optimization problem [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "2 ForeCA In case of ForeCA [10], (Xt)t is assumed to be a stationary second-order process and the goal of the algorithm is finding an extraction vector a such that the projected signals Yt = a T Xt are as forecastable as possible, that is, having a low entropy in their power spectrum.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "For details about ForeCA see [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "For further details about PFA see [17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "We modified the simulator from the Mario AI challenge [14] to return raw visual input in gray-scale without text labels.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "This strategy is usually applied to SFA, often in combination with hierarchical stacking of SFA nodes which further increases the non-linearities while at the same time regularizing spatially (on visual data) [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 26, "context": "We already mentioned above that kernel versions of graph embedding are readily available [27, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "We already mentioned above that kernel versions of graph embedding are readily available [27, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 20, "context": "This step is repeated\u2014each time with the original graph\u2014 resulting in an embedding for the original graph that is increasingly non-linear with every repetition (see [21] for details).", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "Another information-theoretic concept relevant in this context (besides predictive information) is that of information bottlenecks [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "In fact, SFA has been shown to implement a special case of such a past-future information bottleneck for Gaussian variables [6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "That\u2019s why state representations encoding the agent\u2019s future generalize better and allow for more efficient learning of policies than state representations that encode the agent\u2019s past [15, 16].", "startOffset": 185, "endOffset": 193}, {"referenceID": 15, "context": "That\u2019s why state representations encoding the agent\u2019s future generalize better and allow for more efficient learning of policies than state representations that encode the agent\u2019s past [15, 16].", "startOffset": 185, "endOffset": 193}], "year": 2017, "abstractText": "We propose graph-based predictable feature analysis (GPFA), a new method for unsupervised learning of predictable features from high-dimensional time series, where high predictability is understood very generically as low variance in the distribution of the next data point given the previous ones. We show how this measure of predictability can be understood in terms of graph embedding as well as how it relates to the informationtheoretic measure of predictive information in special cases. We confirm the effectiveness of GPFA on different datasets, comparing it to three existing algorithms with similar objectives\u2014namely slow feature analysis, forecastable component analysis, and predictable feature analysis\u2014to which GPFA shows very competitive results.", "creator": "LaTeX with hyperref package"}}}