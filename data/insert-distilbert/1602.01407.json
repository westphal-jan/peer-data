{"id": "1602.01407", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "abstract": "second - order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the residual curvature parameters of the loss function. unfortunately, the exact natural gradient is more impractical to compute for large models, and often most approximations either require an often expensive iterative decomposition procedure or make crude approximations to the curvature. we present kronecker factors for convolution ( kfc ), a tractable integral approximation to the fisher matrix for convolutional networks based on a structured probabilistic model searching for the distribution over backpropagated derivatives. similarly to the recently proposed kronecker - factored approximate curvature ( k - fac ), but each block of the approximate fisher matrix decomposes as per the kronecker product transforms of quite small matrices, allowing for easily efficient inversion. consequently kfc captures important curvature information while still yielding comparably efficient updates to stochastic learning gradient trajectory descent ( sgd ). we show proof that the updates are invariant to commonly better used reparameterizations, such as fourier centering of efficiently the activations. in our modeling experiments, approximate natural gradient descent with kfc was able to train convolutional networks several times larger faster than carefully tuned sgd. furthermore, it was able to train the networks in slightly 10 - thousand 20 more times fewer total iterations quickly than sgd, suggesting its potential applicability in a distributed setting.", "histories": [["v1", "Wed, 3 Feb 2016 18:45:07 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v1", null], ["v2", "Mon, 23 May 2016 22:44:56 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["roger b grosse", "james martens"], "accepted": true, "id": "1602.01407"}, "pdf": {"name": "1602.01407.pdf", "metadata": {"source": "CRF", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "authors": ["Roger Grosse"], "emails": ["rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Despite advances in optimization, most neural networks are still trained using variants of stochastic gradient descent (SGD) with momentum. It has been suggested that natural gradient descent (Amari, 1998) could greatly speed up optimization because it accounts for the geometry of the optimization landscape and has desirable invariance properties. (See Martens (2014) for a review.) Unfortunately, computing the exact natural gradient is intractable for large networks, as it requires solving a large linear system involving the Fisher matrix, whose dimension is the number of parameters (potentially tens of millions for modern architectures). Approximations to the natural gradient typically either impose very restrictive structure on the Fisher matrix (e.g. LeCun et al., 1998; Le Roux et al., 2008) or require expensive iterative procedures to compute each update, analogously to quasi-Newton methods (e.g. Martens, 2010). An ongoing challenge has been to develop a curvature matrix approximation which reflects enough structure to yield high-quality updates, while introducing minimal computational overhead beyond the standard gradient computations.\nar X\niv :1\n60 2.\n01 40\n7v 1\n[ st\nat .M\nL ]\n3 F\nMuch progress in machine learning in the past several decades has been driven by the development of structured probabilistic models whose independence structure allows for efficient computations, yet which still capture important dependencies between the variables of interest. In our case, since the Fisher matrix is the covariance of the backpropagated log-likelihood derivatives, we are interested in modeling the distribution over these derivatives. The model must support efficient computation of the inverse covariance, as this is what\u2019s required to compute the natural gradient. Recently, the Factorized Natural Gradient (FANG) (Grosse & Salakhutdinov, 2015) and Kronecker-Factored Approximate Curvature (K-FAC) (Martens & Grosse, 2015) methods exploited probabilistic models of the derivatives to efficiently compute approximate natural gradient updates. In its simplest version, K-FAC approximates each layer-wise block of the Fisher matrix as the Kronecker product of two much smaller matrices. These (very large) blocks can then be can be tractably inverted by inverting each of the two factors. K-FAC was shown to greatly speed up the training of deep autoencoders. However, its underlying probabilistic model assumed fully connected networks with no weight sharing, rendering the method inapplicable to two architectures which have recently revolutionized many applications of machine learning \u2014 convolutional networks (LeCun et al., 1989; Krizhevsky et al., 2012) and recurrent neural networks (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014).\nWe introduce Kronecker Factors for Convolution (KFC), an approximation to the Fisher matrix for convolutional networks. Most modern convolutional networks have trainable parameters only in convolutional and fully connected layers. Standard K-FAC can be applied to the latter; our contribution is a factorization of the Fisher blocks corresponding to convolution layers. KFC is based on a structured probabilistic model of the backpropagated derivatives where the activations are modeled as independent of the derivatives, the activations and derivatives are spatially homogeneous, and the derivatives are spatially uncorrelated. Under these approximations, we show that the Fisher blocks for convolution layers decompose as a Kronecker product of smaller matrices (analogously to K-FAC), yielding tractable updates.\nKFC yields a tractable approximation to the Fisher matrix of a conv net. It can be used directly to compute approximate natural gradient descent updates, as we do in our experiments. One could further comibne it with the adaptive step size, momentum, and damping methods from the full K-FAC algorithm (Martens & Grosse, 2015). It could also potentially be used as a pre-conditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014). We show that the approximate natural gradient updates are invariant to widely used reparameterizations of a network, such as whitening or centering of the activations.\nWe have evaluated our method on training conv nets on object recognition benchmarks. In our experiments, KFC was able to optimize conv nets several times faster than carefully tuned SGD with momentum, in terms of both training and test error. Furthermore, it was able to train the networks in 10-20 times fewer iterations, suggesting its usefulness in the context of highly distributed training algorithms."}, {"heading": "2 Background", "text": "In this section, we outline the K-FAC method as previously formulated for standard fully-connected feed-forward networks without weight sharing (Martens & Grosse, 2015). Each layer of a fully\nconnected network computes activations as:\ns` = W`a\u0304`\u22121 (1)\na` = \u03c6`(s`), (2)\nwhere ` \u2208 {1, . . . , L} indexes the layer, s` denotes the inputs to the layer, a` denotes the activations, W\u0304` = (b` W`) denotes the matrix of biases and weights, a\u0304` = (1 a > ` ) > denotes the activations with a homogeneous dimension appended, and \u03c6` denotes a nonlinear activation function (usually applied coordinate-wise). (Throughout this paper, we will use the index 0 for all homogeneous coordinates.) We will refer to the values s` as pre-activations. By convention, a0 corresponds to the inputs x and aL corresponds to the prediction z made by the network. For convenience, we concatenate all of the parameters of the network into a vector \u03b8 = (vec(W1) >, . . . , vec(WL) >)>, where vec denotes the Kronecker vector operator which stacks the columns of a matrix into a vector. We denote the function computed by the network as f(x,\u03b8) = aL.\nTypically, a network is trained to minimize an objective h(\u03b8) given by L(y, f(x,\u03b8)) as averaged over the training set, where L(y, z) is a loss function. The gradient \u2207h of h(\u03b8), which is required by most optimization methods, is estimated stochastically using mini-batches of trianing examples. (We will often drop the explicit \u03b8 subscript when the meaning is unambiguous.) For each case, \u2207\u03b8h is usually computed using automatic-differentiation aka backpropagation (Rumelhart et al., 1986; LeCun et al., 1998), which can be thought of as comprising two steps: first computing the pre-activation derivatives \u2207s`h for each layer, and then computing \u2207W`h = (\u2207s`h)a\u0304>`\u22121.\nFor the remainder of this paper, we will assume the network\u2019s prediction f(x,\u03b8) determines the value of the parameter z of a distribution Ry|z over y, and the loss function is the corresponding negative log-likelihood L(y, z) = \u2212 log r(y|z)."}, {"heading": "2.1 Second-order optimization of neural networks", "text": "Second-order optimization methods are based on computing a direction which approximately minimizes a quadratic approximation to the objective. This can also be viewed as approximately solving a linear system Cv = \u2207\u03b8h, where C is a matrix which represents the curvature of the cost function. The original and most well-known example is Newton\u2019s method, where C is chosen to be the Hessian matrix; this isn\u2019t appropriate in the non-convex setting because of the well-known problem that it searches for critical points rather than local optima (e.g. Pascanu et al., 2014). Therefore, it is more common to use natural gradient (Amari, 1998) or updates based on the generalized GaussNewton matrix (Schraudolph, 2002), which are guaranteed to produce descent directions because the curvature matrix C is positive semidefinite.\nIn natural gradient descent, C is defined to be the Fisher information matrix F, which is given by\nF = E x\u223cpdata y\u223cRy|f(x,\u03b8)\n[ D\u03b8(D\u03b8)> ] , (3)\nwhere pdata denotes the training distribution, Ry|f(x,\u03b8) denotes the model\u2019s predictive distribution, and D\u03b8 = \u2207\u03b8L(y, f(x,\u03b8)) is the log-likelihood gradient. For the remainder of this paper, all expectations are with respect to this distribution (which we term the model\u2019s distribution), so we will leave off the subscripts. (In this paper, we will use the D notation for log-likelihood derivatives; gradients of other functions will be written out explicitly.) In the case where Ry|z corresponds to an exponential family model with \u201cnatural\u201d parameters given by z, F is equivalent to the generalized Gauss-Newton matrix (Martens, 2014), which is an approximation of the Hessian which has also\nseen extensive use in various neural-network optimization methods (e.g. Martens, 2010; Vinyals & Povey, 2012).\nF is an n \u00d7 n matrix, where n is the number of parameters and can be in the tens of millions for modern deep architectures. Therefore, it is impractical to represent F explicitly in memory, let alone solve the linear system exactly. There are two general strategies one can take to find a good search direction. First, one can impose a structure on F enabling tractable inversion; for instance LeCun et al. (1998) approximates it as a diagonal matrix, TONGA (Le Roux et al., 2008) uses a more flexible low-rank-within-block-diagonal structure, and factorized natural gradient (Grosse & Salakhutdinov, 2015) imposes a directed Gaussian graphical model structure.\nAnother strategy is to approximately minimize the quadratic approximation to the objective using an iterative procedure such as conjugate gradient; this is the approach taken in Hessianfree optimization (Martens, 2010), a type of truncated Newton method (e.g. Nocedal & Wright, 2006). Conjugate gradient (CG) is defined in terms of matrix-vector products Fv, which can be computed efficiently and exactly using the method outlined by Schraudolph (2002). While iterative approaches can produce high quality search directions, they can be very expensive in practice, as each update may require tens or even hundreds of CG iterations to reach an acceptable quality, and each of these iterations is comparable in cost to an SGD update.\nWe note that these two strategies are not mutually exclusive. In the context of iterative methods, simple (e.g. diagonal) curvature approximations can be used as preconditioners, where the iterative method is implicitly run in a coordinate system where the curvature is less extreme. It has been observed that a good choice of preconditioner can be crucial to obtaining good performance from iterative methods (Martens, 2010; Chapelle & Erhan, 2011; Vinyals & Povey, 2012). Therefore, improved tractable curvature approximations such as the one we develop could likely be used to improve iterative second-order methods."}, {"heading": "2.2 Kronecker-factored approximate curvature", "text": "Kronecker-factored approximate curvature (K-FAC; Martens & Grosse, 2015) is a recently proposed optimization method for neural networks which can be seen as a hybrid of the two approximation strategies: it uses a tractable approximation to the Fisher matrix F, but also uses an optimization strategy which behaves locally like conjugate gradient. This section gives a conceptual summary of the aspects of K-FAC relevant to the contributions of this paper; a precise description of the full algorithm is given in Appendix A.2.\nThe block-diagonal version of K-FAC (which is the simpler of the two versions, and is what we will present here) is based on two approximations to F which together make it tractable to invert. First, weight gradients in different layers are assumed to be uncorrelated, which corresponds to F being block diagonal, with one block per layer:\nF \u2248 E[vec(DW\u03041) vec(DW\u03041) >] 0 . . .\n0 E[vec(DW\u0304L) vec(DW\u0304L)>]  (4) This approximation by itself is insufficient, because each of the blocks may still be very large. (E.g., if a network has 1,000 units in each layer, each block would be of size 106 \u00d7 106.) For the second approximation, observe that\nE [ D[W\u0304`]ijD[W\u0304`]i\u2032j\u2032 ] = E [D[s`]i[a\u0304`\u22121]jD[s`]i\u2032 [a\u0304`\u22121]j\u2032 ] . (5)\nIf we approximate the activations and pre-activation derivatives as independent, this can be decomposed as E [ D[W\u0304`]ijD[W\u0304`]i\u2032j\u2032 ] \u2248 E [D[s`]iD[s`]i\u2032 ]E [[a\u0304`\u22121]j [a\u0304`\u22121]j\u2032 ]. This can be written algebraically as a decomposition into a Kronecker product of two smaller matrices:\nE[vec(W\u0304`) vec(W\u0304`)>] \u2248 \u03a8`\u22121 \u2297 \u0393` , F\u0302`, (6)\nwhere \u03a8`\u22121 = E[a\u0304`\u22121a\u0304>`\u22121] and \u0393` = E[s`s>` ] denote the second moment matrices of the activations and pre-activation derivatives, respectively. Call the block diagonal approximate Fisher matrix, with blocks given by Eqn. 6, F\u0302. The two factors are estimated online from the empirical moments of the model\u2019s distribution using exponential moving averages.\nTo invert F\u0302, we use the facts that (1) we can invert a block diagonal matrix by inverting each of the blocks, and (2) the Kronecker product satisfies the identity (A\u2297B)\u22121 = A\u22121 \u2297B\u22121:\nF\u0302\u22121 = \u03a8 \u22121 0 \u2297 \u0393 \u22121 1 0 . . .\n0 \u03a8\u22121L\u22121 \u2297 \u0393 \u22121 L  (7) We do not represent F\u0302\u22121 explicitly, as each of the blocks is quite large. Instead, we keep track of each of the Kronecker factors.\nThe approximate natural gradient F\u0302\u22121\u2207h can then be computed as follows:\nF\u0302\u22121\u2207h =  vec ( \u0393\u221211 (\u2207W\u03041h)\u03a8 \u22121 0 ) ...\nvec ( \u0393\u22121L (\u2207W\u0304Lh)\u03a8 \u22121 L\u22121\n)  (8)\nWe would often like to add a multiple of the identity matrix to F for two reasons. First, many networks are regularized with weight decay, which corresponds to a penalty of \u03bb\u03b8>\u03b8, for some parameter \u03bb. Following the interpretation of F as a quadratic approximation to the curvature, it would be appropriate to use F +\u03bbI to approximate the curvature of the regularized objective. The second reason is that the approximate natural gradient update introduces two sources of error: F\u0302 might not be a good approximation of F, and the quadratic approximation might not adequately match the true objective function. To deal with this, it is customary to damp the updates by adding \u03b3I to the approximate curvature, for some small value \u03b3, before solving the linear system.\nTherefore, we would ideally like to compute [ F\u0302 + (\u03bb+ \u03b3)I ]\u22121 \u2207h.\nUnfortunately, adding (\u03bb+\u03b3)I breaks the Kronecker factorization structure. While it is possible to exactly solve the damped system (see Appendix A.2), it is often preferable to approximate\nF\u0302 + (\u03bb+ \u03b3)I in a way that maintains the factorizaton structure. Martens & Grosse (2015) pointed out that F\u0302` + (\u03bb+ \u03b3)I \u2248 ( \u03a8`\u22121 + \u03c0` \u221a \u03bb+ \u03b3 I ) \u2297 ( \u0393` + 1\n\u03c0`\n\u221a \u03bb+ \u03b3 I ) . (9)\nWe will denote this damped approximation as F\u0302 (\u03b3) ` = \u03a8 (\u03b3) `\u22121 \u2297\u0393 (\u03b3) ` . Mathematically, \u03c0` can be any positive scalar, but Martens & Grosse (2015) suggest the formula\n\u03c0` = \u221a \u2016\u03a8`\u22121 \u2297 I\u2016 \u2016I\u2297 \u0393`\u2016 , (10)\nwhere \u2016 \u00b7 \u2016 denotes some matrix norm, as this value minimizes the norm of the residual in Eqn. 9. In this work, we use the trace norm \u2016B\u2016 = tr B. The approximate natural gradient \u2207\u0302h is then computed as:\n\u2207\u0302h , [F\u0302(\u03b3)]\u22121\u2207h =  vec ( [\u0393 (\u03b3) 1 ] \u22121(\u2207W\u03041h)[\u03a8 (\u03b3) 0 ] \u22121 ) ...\nvec (\n[\u0393 (\u03b3) L ] \u22121(\u2207W\u0304Lh)[\u03a8 (\u03b3) L\u22121]\n\u22121 )  (11)\nThe algorithm as presented by Martens & Grosse (2015) has many additional elements which are orthogonal to the contributions of this paper. For concision, a full description of the algorithm is relegated to Appendix A.2."}, {"heading": "2.3 Convolutional networks", "text": "Convolutional networks require somewhat crufty notation when the computations are written out in full. In our case, we are interested in computing correlations of derivatives, which compounds the notational difficulties. In this section, we summarize the notation we use. (Table 1 lists all convolutional network notation used in this paper.) In sections which focus on a single layer of the network, we drop the explicit layer indices.\nA convolution layer takes as input a layer of activations {aj,t}, where j \u2208 {1, . . . , J} indexes the input map and t \u2208 T indexes the spatial location. (Here, T is the set of spatial locations, which is typically a 2-D grid. For simplicity, we assume convolution is performed with a stride of 1 and padding equal to R, so that the set of spatial locations is shared between the input and output feature maps.) This layer is parameterized by a set of weights wi,j,\u03b4 and biases bi, where i \u2208 {1, . . . , I} indexes the output map, j indexes the input map, and \u03b4 \u2208 \u2206 indexes the spatial offset (from the center of the filter). If the filters are of size (2R+1)\u00d7 (2R+1), then we would have \u2206 = {\u2212R, . . . , R}\u00d7{\u2212R, . . . , R}. We denote the numbers of spatial locations and spatial offsets as |T | and |\u2206|, respectively. The convolution layer computes a set of pre-activations {si,t} as follows:\nsi,t = \u2211 \u03b4\u2208\u2206 wi,j,\u03b4aj,t+\u03b4 + bi, (12)\nwhere bi denotes the bias parameter. The activations are defined to take the value 0 outside of T . The pre-activations are passed through a nonlinearity such as ReLU to compute the output layer activations, but we have no need to refer to this explicitly when analyzing a single layer. (For simplicity, we assume operations such as pooling and response normalization are implemented as separate layers.)\nPre-activation derivatives Dsi,t are computed during backpropagation. One then computes weight derivatives as:\nDwi,j,\u03b4 = \u2211 t\u2208T aj,t+\u03b4Dsi,t. (13)"}, {"heading": "2.3.1 Efficient implementation and vectorized notation", "text": "For modern large-scale vision applications, it\u2019s necessary to implement conv nets efficiently for a GPU (or some other parallel architecture). We provide a very brief overview of the low-level efficiency issues which are relevant to K-FAC. We base our discussion on the Toronto Deep Learning\nConvNet (TDLCN) package (Srivastava, 2015), whose convolution kernels we use in our experiments. Like many modern implementations, this implementation follows the approach of Chellapilla et al. (2006), which reduces the convolution operations to large matrix-vector products in order to exploit memory locality and efficient parallel BLAS operators. We describe the implementation explicitly, as it is important that our proposed algorithm be efficient using the same memory layout (shuffling operations are extremely expensive). As a bonus, these vectorized operations provide a convenient high-level notation which we will use throughout the paper.\nThe ordering of arrays in memory is significant, as it determines which operations can be performed efficiently without requiring (very expensive) transpose operations. The activations are stored as a M \u00d7 |T | \u00d7 J array A\u0303`\u22121, where M is the mini-batch size, |T | is the number of spatial locations, and J is the number of feature maps.1 This can be interpreted as an M |T | \u00d7 J matrix. (We must assign orderings to T and \u2206, but this choice is arbitrary.) Similarly, the weights are stored as an I \u00d7 |\u2206| \u00d7 J array W`, which can be interpreted either as an I \u00d7 |\u2206|J matrix or a I|\u2206| \u00d7 J matrix without reshuffling elements in memory. We will almost always use the former interpretation, which we denote W`; the I|\u2206| \u00d7 J matrix will be denoted W\u0306`.\nThe naive implementation of convolution, while highly parallel in principle, suffers from poor memory locality. Instead, efficient implementations typically use what we will term the expansion operator and denote J\u00b7K. This operator extracts the patches surrounding each spatial location and flattens them into vectors. These vectors become the rows of a matrix. For instance, JA\u0303`\u22121K is a M |T | \u00d7 J |\u2206| matrix, defined as\nJA\u0303`\u22121KtM+m, j|\u2206|+\u03b4 = [A\u0303`\u22121](t+\u03b4)M+m, j = a(m)j,t+\u03b4, (14)\nfor all entries such that t+ \u03b4 \u2208 T . All other entries are defined to be 0. Here, m indexes the data instance within the mini-batch.\nIn TDLCN, the forward pass is computed as A\u0303` = \u03c6(S\u0303`) = \u03c6 ( JA\u0303`\u22121KW>` + 1b>` ) , (15)\nwhere \u03c6 is the nonlinearity, applied elementwise, 1 is a vector of ones, and b is the vector of biases. In backpropagation, the activation derivatives are computed as:\nDA\u0303`\u22121 = JDS\u0303`KW\u0306`. (16)\nFinally, the gradient for the weights is computed as\nDW` = DS\u0303>` JA\u0303`\u22121K (17)\nThe matrix products are computed using the cuBLAS function cublasSgemm. In practice, the expanded matrix JA\u0303`\u22121K may be too large to store in memory. In this case, a subset of the rows of JA\u0303`\u22121K are computed and processed at a time.\nWe will also use the |T | \u00d7 J matrix A`\u22121 and the |T | \u00d7 I matrix S` to denote the activations and pre-activations for a single training case. A`\u22121 and S` can be substituted for A\u0303`\u22121 and S\u0303` in Eqns. 15-17.\nFor fully connected networks, it is often convenient to append a homogeneous coordinate to the activations so that the biases can be folded into the weights (see Section 2.2). For convolutional\n1The first index of the array is the least significant in memory.\nlayers, there is no obvious way to add extra activations such that the convolution operation simulates the effect of biases. However, we can achieve an analogous effect by adding a homogeneous coordinate (i.e. a column of all 1\u2019s) to the expanded activations. We will denote this JA\u0303`\u22121KH . Similarly, we can prepend the bias vector to the weights matrix: W\u0304` = (b` W`). The homogeneous coordinate is not typically used in conv net implementations, but it will be convenient for us notationally. For instance, the forward pass can be written as:\nA\u0303` = \u03c6 ( JA\u0303`\u22121KHW\u0304>` ) (18)\nTable 1 summarizes all of the conv net notation used in this paper."}, {"heading": "3 Kronecker factorization for convolution layers", "text": "We begin by assuming a block-diagonal approximation to the Fisher matrix like that of K-FAC, where each block contains all the parameters relevant to one layer (see Section 2.2). (Recall that\nthese blocks are typically too large to invert exactly, or even represent explicitly, which is why the further Kronecker approximation is required.) The Kronecker factorization from K-FAC applies only to fully connected layers. Convolutional networks introduce several kinds of layers not found in fully connected feed-forward networks: convolution, pooling, and response normalization. Since pooling and response normalization layers don\u2019t have trainable weights, they are not included in the Fisher matrix. However, we must deal with convolution layers. In this section, we present our main contribution, an approximate Kronecker factorization for the blocks of F\u0302 corresponding to convolution layers. In the tradition of fast food puns (Ranzato & Hinton, 2010; Yang et al., 2014), we call our method Kronecker Factors for Convolution (KFC).\nFor this section, we focus on the Fisher block for a single layer, so we drop the layer indices. Recall that the Fisher matrix F = E [ D\u03b8(D\u03b8)> ] is the covariance of the log-likelihood gradient under the model\u2019s distribution. (In this paper, all expectations are with respect to the model\u2019s distribution unless otherwise specified.) By plugging in Eqn. 13, the entries corresponding to weight derivatives are given by:\nE[Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = E [(\u2211 t\u2208T aj,t+\u03b4Dsi,t )(\u2211 t\u2032\u2208T aj\u2032,t\u2032+\u03b4\u2032Dsi\u2032,t\u2032 )]\n(19)\nTo think about the computational complexity of computing the entries directly, consider the second convolution layer of AlexNet (Krizhevsky et al., 2012), which has 48 input feature maps, 128 output feature maps, 27\u00d7 27 = 729 spatial locations, and 5\u00d7 5 filters. Since there are 128\u00d7 48\u00d7 5\u00d7 5 = 245760 weights and 128 biases, the full block would require 2458882 \u2248 60.5 billion entries to represent explicitly, and inversion is clearly impractical.\nRecall that K-FAC approximation for classical fully connected networks can be derived by approximating activations and pre-activation derivatives as being statistically independent (this is the IAD approximation below). Deriving an analogous Fisher approximation for convolution layers will require some additional approximations.\nHere are the approximations we will make in deriving our Fisher approximation:\n\u2022 Independent activations and derivatives (IAD). The activations are independent of the pre-activation derivatives, i.e. {aj,t} \u22a5\u22a5 {Dsi,t\u2032}.\n\u2022 Spatial homogeneity (SH). The first-order statistics of the activations are independent of spatial location. The second-order statistics of the activations and pre-activation derivatives at any two spatial locations t and t\u2032 depend only on t\u2032 \u2212 t. This implies there are functions M , \u2126 and \u0393 such that:\nE [aj,t] = M(j) (20) E [aj,taj\u2032,t\u2032 ] = \u2126(j, j\u2032, t\u2032 \u2212 t) (21) E [Dsi,tDsi\u2032,t\u2032 ] = \u0393(i, i\u2032, t\u2032 \u2212 t). (22)\nNote that E[Dsi,t] = 0 under the model\u2019s distribution, so Cov (Dsi,t,Dsi\u2032,t\u2032) = E [Dsi,tDsi\u2032,t\u2032 ].\n\u2022 Spatially uncorrelated derivatives (SUD). The pre-activation derivatives at any two distinct spatial locations are uncorrelated, i.e. \u0393(i, i\u2032, \u03b4) = 0 for \u03b4 6= 0.\nWe believe SH is fairly innocuous, as one is implicitly making a spatial homogeneity assumption when choosing to use convolution in the first place. SUD perhaps sounds like a more severe\napproximation, but in fact appeared to describe the model\u2019s distribution quite well in the networks we investigated; this is analyzed empirially in Section 5.1.\nWe now show that combining the above three approximations yields a Kronecker factorization of the Fisher blocks. For simplicity of notation, assume the data are two-dimensional, so that the offsets can be parameterized with indices \u03b4 = (\u03b41, \u03b42) and \u03b4 \u2032 = (\u03b4\u20321, \u03b4 \u2032 2), and denote the dimensions of the activations map as (T1, T2). The formulas can be generalized to data dimensions higher than 2 in the obvious way.\nTheorem 1. Combining approximations IAD, SH, and SUD yields the following factorization:\nE [Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u03b2(\u03b4, \u03b4\u2032) \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) \u0393(i, i\u2032, 0), E [Dwi,j,\u03b4Dbi\u2032 ] = \u03b2(\u03b4)M(j) \u0393(i, i\u2032, 0)\nE [DbiDbi\u2032 ] = |T |\u0393(i, i\u2032, 0) (23)\nwhere\n\u03b2(\u03b4) , (T1 \u2212 |\u03b41|) (T2 \u2212 |\u03b42|) \u03b2(\u03b4, \u03b4\u2032) , (T1 \u2212max(\u03b41, \u03b4\u20321, 0) + min(\u03b41, \u03b4\u20321, 0)) \u00b7 (T2 \u2212max(\u03b42, \u03b4\u20322, 0) + min(\u03b42, \u03b4\u20322, 0)) (24)\nProof. See Appendix B.\nTo talk about how this fits in to the block diagonal approximation to the Fisher matrix F, we now restore the explicit layer indices and use the vectorized notation from Section 2.3.1. The above factorization yields a Kronecker factorization of each block, which will be useful for computing their inverses (and ultimately our approximate natural gradient). In particular, if F\u0302` \u2248 E[vec(DW\u0304`) vec(DW\u0304`)>] denotes the block of the approximate Fisher for layer `, Eqn. 23 yields our KFC factorization of F\u0302` into a Kronecker product of smaller factors:\nF\u0302` = \u2126`\u22121 \u2297 \u0393`, (25)\nwhere\n[\u2126`\u22121]j|\u2206|+\u03b4, j\u2032|\u2206|+\u03b4\u2032 , \u03b2(\u03b4, \u03b4 \u2032) \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)\n[\u2126`\u22121]j|\u2206|+\u03b4, 0 = [\u2126`\u22121]0, j|\u2206|+\u03b4 , \u03b2(\u03b4)M(j)\n[\u2126`\u22121]0, 0 , |T | [\u0393`]i,i\u2032 , \u0393(i, i \u2032, 0). (26)\n(We will derive much simpler formulas for \u2126`\u22121 and \u0393` in the next section.) Using this factorization, the rest of the K-FAC algorithm can be carried out without modification. For instance, we can compute the approximate natural gradient using a damped version of F\u0302 analogously to Eqns. 9 and 11 of Section 2.2:\nF\u0302 (\u03b3) ` = \u2126 (\u03b3) `\u22121 \u2297 \u0393 (\u03b3) ` (27) , ( \u2126`\u22121 + \u03c0` \u221a \u03bb+ \u03b3 I ) \u2297 ( \u0393` + 1\n\u03c0`\n\u221a \u03bb+ \u03b3 I ) . (28)\n\u2207\u0302h = [F\u0302(\u03b3)]\u22121\u2207h =  vec ( [\u0393 (\u03b3) 1 ] \u22121(\u2207W\u03041h)[\u2126 (\u03b3) 0 ] \u22121 ) ...\nvec (\n[\u0393 (\u03b3) L ] \u22121(\u2207W\u0304Lh)[\u2126 (\u03b3) L\u22121]\n\u22121 )  (29)\nReturning to our running example of AlexNet, W\u0304` is a I \u00d7 (J |\u2206| + 1) = 128 \u00d7 1201 matrix. Therefore the factors \u2126`\u22121 and \u0393` are 1201\u00d7 1201 and 128\u00d7 128, respectively. These matrices are small enough that they can be represented exactly and inverted in a reasonable amount of time, allowing us to efficiently compute the approximate natural gradient direction using Eqn. 29."}, {"heading": "3.1 Estimating the factors", "text": "Since the true covariance statistics are unknown, we estimate them empirically by sampling from the model\u2019s distribution, similarly to Martens & Grosse (2015). To sample derivatives from the model\u2019s distribution, we select a mini-batch, sample the outputs from the model\u2019s predictive distribution, and backpropagate the derivatives.\nWe need to estimate the Kronecker factors {\u2126`}L\u22121`=0 and {\u0393`}L`=1. Since these matrices are defined in terms of the autocovariance functions \u2126 and \u0393, it would appear natural to estimate these functions empirically. Unfortunately, if the empirical autocovariances are plugged into Eqn. 26, the resulting \u2126` may not be positive semidefinite. This is a problem, since negative eigenvalues in the approximate Fisher could cause the optimization to diverge (a phenomenon we have observed in practice). An alternative which at least guarantees PSD matrices is to simply ignore the boundary effects, taking \u03b2(\u03b4, \u03b4\u2032) = \u03b2(\u03b4) = |T | in Eqn. 26. Sadly, we found this to give very inaccurate covariances, especially for higher layers, where the filters are of comparable size to the activation maps.\nInstead, we estimate each \u2126` directly using the following fact:\nTheorem 2. Under assumption SH, \u2126` = E [ JA`K>HJA`KH ] (30)\n\u0393` = 1 |T | E [ DS>` DS` ] . (31)\n(The J\u00b7K notation is defined in Section 2.3.1.) Proof. See Appendix B.\nUsing this result, we define the empirical statistics for a given mini-batch:\n\u2126\u0302` = 1\nM JA\u0303`K>HJA\u0303`KH\n\u0393\u0302` = 1\nM |T | DS\u0303>` DS\u0303` (32)\nSince the estimates \u2126\u0302` and \u0393\u0302` are computed in terms of matrix inner products, they are always PSD matrices. Importantly, because JA\u0303`K and DS\u0303` are the same matrices used to implement the convolution operations (Section 2.3.1), the computation of covariance statistics enjoys the same memory locality properties as the convolution operations.\nAt the beginning of training, we estimate {\u2126`}L\u22121`=0 and {\u0393`}L`=1 from the full dataset (or a large subset) using Eqn. 32. Subsequently, we maintain exponential moving averages of these matrices, where these equations are applied to each mini-batch, i.e.\n\u2126` \u2190 \u03be\u2126` + (1\u2212 \u03be)\u2126\u0302` \u0393` \u2190 \u03be\u0393` + (1\u2212 \u03be)\u0393\u0302`, (33)\nwhere \u03be is a parameter which determines the timescale for the moving average."}, {"heading": "3.2 Using KFC in optimization", "text": "So far, we have defined an approximation F\u0302(\u03b3) to the Fisher matrix F which can be tractably inverted. This can be used in any number of ways in the context of optimization, most simply by using \u2207\u0302h = [F\u0302(\u03b3)]\u22121\u2207h as an approximation to the natural gradient F\u22121\u2207h. Alternatively, we could use it in the context of the full K-FAC algorithm, or as a preconditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014).\nIn our experiments, we explored two particular instantiations of KFC in optimization algorithms. First, in order to provide as direct a comparison as possible to standard SGD-based optimization, we used \u2207\u0302h in the context of a generic approximate natural gradient descent procedure; this procedure is like SGD, except that \u2207\u0302h is substituted for the Euclidean gradient. Additionally, we used momentum, update clipping, and parameter averaging \u2014 all standard techniques in the context of stochastic optimization.2 One can also view this as a preconditioned SGD method, where F\u0302(\u03b3) is used as the preconditioner. Therefore, we refer to this method in our experiments as KFC-pre (to distinguish it from the KFC approximation itself). This method is spelled out in detail in Appendix A.1.\nWe also explored the use of F\u0302(\u03b3) in the context of K-FAC, which (in addition to the techniques of Section 2.2), includes methods for adaptively changing the learning rate, momentum, and damping parameters over the course of optimization. The full algorithm is given in Appendix A.2. Our aim was to measure how KFC can perform in the context of a sophisticated and well-tuned second-order optimization procedure. We found that the adaptation methods tended to choose stable values for the learning rate, momentum, and damping parameters, suggesting that these could be replaced with fixed values (as in KFC-pre). Since both methods performed similarly, we report results only for KFC-pre. We note that this finding stands in contrast with the autoencoder experiments of Martens & Grosse (2015), where the adapted parameters varied considerably over the course of optimization.\nWith the exception of inverting the Kronecker factors, all of the heavy computation for our methods was performed on the GPU. We based our implementation on CUDAMat (Mnih, 2009) and the convolution kernels provided by the Toronto Deep Learning ConvNet (TDLCN) package (Srivastava, 2015). Full details on our GPU implementation and other techniques for minimizing computational overhead are given in Appendix A.3."}, {"heading": "4 Theoretical analysis", "text": ""}, {"heading": "4.1 Invariance", "text": "Natural gradient descent is motivated partly by way of its invariance to reparameterization: regardless of how the model is parameterized, the updates are equivalent to the first order. Approximations to natural gradient don\u2019t satisfy full invariance to parameterization, but certain approximations have been shown to be invariant to more limited, but still fairly broad, classes of transformations. Ollivier (2015) showed that one such approximation was invariant to (invertible) affine transformations of individual activations. This class of transformations includes replacing sigmoidal with tanh activation functions, as well as the centering transformations discussed in the next section. Martens & Grosse (2015) showed that K-FAC is invariant to a broader class of reparameterizations: affine\n2Our SGD baseline used momentum and parameter averaging as well. Clipping was not needed for SGD, for reasons explained in Appendix A.1.\ntransformations of the activations (considered as a group), both before and after the nonlinearity. In addition to affine transformations of individual activations, this class includes transformations which whiten the activations to have zero mean and unit covariance. The transformations listed here have all been used to improve optimization performance (see next section), so these invariance properties provide an interesting justification of approximations to natural gradient methods. I.e., to the extent that these transformations help optimization, approximate natural gradient descent methods can be expected to achieve such benefits automatically.\nFor convolutional layers, we cannot expect an algorithm to be invariant to arbitrary affine transformations of a given layer\u2019s activations, as such transformations can change the set of functions which are representable. (Consider for instance, a transformation which permutes the spatial locations.) However, we show that the KFC updates are invariant to homogeneous, pointwise affine transformations of the activations, both before and after the nonlinearity. This is perhaps an overly limited statement, as it doesn\u2019t use the fact that the network accounts for spatial correlations. However, it still accounts for a broad set of transformations, such as normalizing activations to be zero mean and unit variance either before or after the nonlinearity.\nTo formalize this, recall that a layer\u2019s activations are represented as a |T | \u00d7 J matrix and are computed from that layer\u2019s pre-activations by way of an elementwise nonlinearity, i.e. A` = \u03c6`(S`). We replace this with an activation function \u03c6\u2020` which additionally computes affine transformations before and after the nonlinearity. Such transformations can be represented in matrix form:\nA\u2020` = \u03c6 \u2020 `(S \u2020 `) = \u03c6`(S \u2020 `U` + 1c > ` )V` + 1d > ` , (34)\nwhere U` and V` are both invertible. For convenience, the inputs to the network can be treated as an activation function \u03c60 which takes no arguments. We also assume the final layer outputs are not transformed, i.e. VL = I and dL = 0. KFC is invariant to this class of transformations:\nTheorem 3. Let N be a network with parameter vector \u03b8 and activation functions {\u03c6`}L`=0. Given activation functions {\u03c6\u2020`}L`=0 defined as in Eqn. 34, there exists a parameter vector \u03b8 \u2020 such that a network N \u2020 with parameters \u03b8\u2020 and activation functions {\u03c6\u2020`}L`=0 computes the same function as N . The KFC updates on N and N \u2020 are equivalent, in that the resulting networks compute the same function.\nProof. See Appendix B.\nInvariance to affine transformations also implies approximate invariance to smooth nonlinear transformations; see Martens (2014) for further discussion."}, {"heading": "4.2 Relationship with other algorithms", "text": "Other neural net optimization methods have been proposed which attempt to correct for various statistics of the activations or gradients. Perhaps the most commonly used are algorithms which attempt to adapt learning rates for individual parameters based on the variance of the gradients (LeCun et al., 1998; Duchi et al., 2011; Tieleman & Hinton, 2012; Zeiler, 2013; Kingma & Ba, 2015). These can be thought of as diagonal approximations to the Hessian or the Fisher matrix.3\n3Some of these methods use the empirical Fisher matrix, which differs from the proper Fisher matrix in that the targets are taken from the training data rather than sampled from the model\u2019s predictive distribution. The empirical Fisher matrix is less closely related to the curvature than is the proper one (Martens, 2014).\nAnother class of approaches attempts to reparameterize a network such that its activations have zero mean and unit variance, with the goals of preventing covariate shift and improving the conditioning of the curvature (Cho et al., 2013; Vatanen et al., 2013; Ioffe & Szegedy, 2015). Centering can be viewed as an approximation to natural gradient where the Fisher matrix is approximated with a directed Gaussian graphical model (Grosse & Salakhutdinov, 2015). As discussed in Section 4.1, KFC is invariant to re-centering of activations, so it ought to automatically enjoy the optimization benefits of centering. However, batch normalization (Ioffe & Szegedy, 2015) includes some effects not automatically captured by KFC. First, the normalization is done separately for each mini-batch rather than averaged across mini-batches; this introduces stochasticity into the computations which may serve as a regularizer. Second, it discourages large covariate shifts in the pre-activations, which may help to avoid dead units. Since batch normalization is better regarded as a modification to the architecture than an optimization algorithm, it can be combined with KFC; we investigated this in our experiments.\nProjected Natural Gradient (PRONG; Desjardins et al., 2015) goes a step further than centering methods by fully whitening the activations in each layer. In the case of fully connected layers, the activations are transformed to have zero mean and unit covariance. For convolutional layers, they apply a linear transformation that whitens the activations across feature maps. While PRONG includes clever heuristics for updating the statistics, it\u2019s instructive to consider an idealized version of the method which has access to the exact statistics. We can interpret this idealized PRONG in our own framework as arising from following two additional approximations:\n\u2022 Spatially uncorrelated activations (SUA). The activations at any two distinct spatial locations are uncorrelated, i.e. Cov(aj,t, aj\u2032,t\u2032) = 0 for t 6= t\u2032. Also assuming SH, the correlations can then be written as Cov(aj,t, aj\u2032,t) = \u03a3(j, j \u2032).\n\u2022 White derivatives (WD). Pre-activation derivatives are uncorrelated and have spherical covariance, i.e. \u0393(i, i\u2032, \u03b4) \u221d 1i=i\u20321\u03b4=0. We can assume WLOG that the proportionality constant is 1, since any scalar factor can be absorbed into the learning rate.\nTheorem 4. Combining approximations IAD, SH, SUA, and WD results in the following approximation to the entries of the Fisher matrix:\nE [Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u03b2(\u03b4, \u03b4\u2032) \u2126\u0303(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1i=i\u2032 , (35)\nwhere 1 is the indicator function and \u2126\u0303(j, j\u2032, \u03b4) , \u03a3(j, j\u2032)1\u03b4=0 + M(j)M(j\u2032) is the uncentered autocovariance function. (\u03b2 is defined in Theorem 1. Formulas for the remaining entries are given in Appendix B.) If the \u03b2(\u03b4, \u03b4\u2032) term is dropped, the resulting approximate natural gradient descent update rule is equivalent to idealized PRONG, up to rescaling.\nAs we later discuss in Section 5.1, assumption WD appears to hold up well empirically, while SUA appears to lose a lot of information. Observe, for instance, that the input images are themselves treated as a layer of activations. Assumption SUA amounts to modeling each channel of an image as white noise, corresponding to a flat power spectrum. Images have a well-characterized 1/fp power spectrum with p \u2248 2 (Simoncelli & Olshausen, 2001), which implies that the curvature may be much larger in directions corresponding to low-frequency Fourier components than in directions corresponding to high-frequency components."}, {"heading": "5 Experiments", "text": "We have evaluated our method on two standard image recognition benchmark datasets: CIFAR-10 (Krizhevsky, 2009), and Street View Housing Numbers (SVHN; Netzer et al., 2011). Our aim is not to achieve state-of-the-art performance, but to evaluate KFC\u2019s ability to optimize previously published architectures. We first examine the probabilistic assumptions, and then present optimization results.\nFor CIFAR-10, we used the architecture from cuda-convnet4 which achieved 18% error in 20 minutes. This network consists of three convolution layers and a fully connected layer. (While cuda-convnet provides some better-performing architectures, we could not use these, since these included locally connected layers, which KFC can\u2019t handle.) For SVHN, we used the architecture of Srivastava (2013). This architecture consists of three convolutional layers followed by three fully connected layers, and uses dropout for regularization. Both of these architectures were carefully tuned for their respective tasks. Furthermore, the TDLCN CUDA kernels we used were carefully tuned at a low level to implement SGD updates efficiently for both of these architectures. Therefore, we believe our SGD baseline is quite strong."}, {"heading": "5.1 Evaluating the probabilistic modeling assumptions", "text": "In defining KFC, we combined three probabilistic modeling assumptions: independent activations and derivatives (IAD), spatial homogeneity (SH), and spatially uncorrelated derivatives (SUD). As discussed above, IAD is the same approximation made by standard K-FAC, and it was investigated in detail both theoretically and empirically by Martens & Grosse (2015). One implicitly\n4https://code.google.com/p/cuda-convnet/\nassumes SH when choosing to use a convolutional architecture. However, SUD is perhaps less intuitive. Why should we suppose the derivatives are spatially uncorrelated? Conversely, why not go a step further and assume the activations are spatially uncorrelated (as does PRONG; see Section 4.2) or even drop all of the correlations (thereby obtaining a much simpler diagonal approximation to the Fisher matrix)?\nWe investigated the autocorrelation functions for networks trained on CIFAR-10 and SVHN, each with 50 epochs of SGD. (These models were trained long enough to achieve good test error, but not long enough to overfit.) Derivatives were sampled from the model\u2019s distribution as described in Section 2.2. Figure 1(a) shows the autocorrelation functions of the pre-activation gradients for three (arbitrary) feature maps in all of the convolution layers of both networks. Figure 1(b) shows the correlations between derivatives for different feature maps in the same spatial position. Evidently, the derivatives are very weakly correlated, both spatially and cross-map, although there are some modest cross-map correlations in the first layers of both models, as well as modest spatial correlations in the top convolution layer of the CIFAR-10 network. This suggests that SUD is a good approximation for these networks.\nInterestingly, the lack of correlations between derivatives appears to be a result of max-pooling. Max-pooling has a well-known sparsifying effect on the derivatives, as any derivative is zero unless the corresponding activation achieves the maximum within its pooling group. Since neighboring locations are unlikely to simultaneously achieve the maximum, max-pooling weakens the spatial correlations. To test this hypothesis, we trained networks equivalent to those described above, except that the max-pooling layers were replaced with average pooling. The spatial autocorrelations and cross-map correlations are shown in Figure 1(c, d). Replacing max-pooling with average pooling dramatically strengthens both sets of correlations.\nIn contrast with the derivatives, the activations have very strong correlations, both spatially and cross-map, as shown in Figure 2. This suggests the spatially uncorrelated activations (SUA) as-\nsumption implicitly made by some algorithms could be problematic, despite appearing superficially analogous to SUD."}, {"heading": "5.2 Optimization performance", "text": "We evaluated KFC-pre in the context of optimizing deep convolutional networks. We compared against stochastic gradient descent (SGD) with momentum, which is widely considered a strong baseline for training conv nets. All architectural choices (e.g. sizes of layers) were kept consistent with the previously published configurations. Since the focus of this work is optimization rather than generalization, metaparameters were tuned with respect to training error. This protocol was favorable to the SGD baseline, as the learning rates which performed the best on training error also performed the best on test error.5 For both SGD and KFC-pre, we tuned the learning rates\n5For KFC-pre, we encountered a more significant tradeoff between training and test error, most notably in the choice of mini-batch size, so the presented results do not reflect our best runs on the test set. For instance, as\nfrom the set {0.3, 0.1, 0.03, . . . , 0.0003} separately for each experiment. For KFC-pre, we also chose several algorithmic parameters using the method of Appendix A.3, which considers only per-epoch running time and not final optimization performance.6\nFor both SGD and KFC-pre, we used an exponential moving average of the iterates (see Appendix A.1) with a timescale of 50,000 training examples (which corresponds to one epoch on CIFAR-10). This helped both SGD and KFC-pre substantially. All experiments for which wall clock time is reported were run on a single Nvidia GeForce GTX Titan Z GPU board.\nAs baselines, we also tried Adagrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2015), but none of these approaches outperformed carefully tuned SGD with momentum. This is consistent with the observations of Kingma & Ba (2015).\nFigure 3(a,b) shows the optimization performance on the CIFAR-10 dataset, in terms of wall clock time. Both KFC-pre and SGD reached approximately the previously published test error of 18% before they started overfitting. However, KFC-pre reached 19% test error in 3 minutes, compared with 9 minutes for SGD. The difference in training error was more significant: KFC-pre reaches a training error of 6% in 4 minutes, compared with 30 minutes for SGD. On SVHN, KFCpre reached the previously published test error of 2.78% in 120 minutes, while SGD did not reach it within 250 minutes. (As discussed above, test error comparisons should be taken with a grain of salt because algorithms were tuned based on training error; however, any biases introduced by our protocol would tend to favor the SGD baseline over KFC-pre.)\nBatch normalization (BN Ioffe & Szegedy, 2015) has recently had much success at training a variety of neural network architectures. It has been motivated both in terms of optimization benefits (because it reduces covariate shift) and regularization benefits (because it adds stochasticity to the updates). However, BN is best regarded not as an optimization algorithm, but as a modification to the network architecture, and it can be used in conjunction with algorithms other than SGD. We modified the original CIFAR-10 architecture to use batch normalization in each layer. Since the parameters of a batch normalized network would have a different scale from those of an ordinary network, we disabled the `2 regularization term so that both networks would be optimized to the same objective function. While our own (inefficient) implementation of batch normalization incurred substantial computational overhead, we believe an efficient implementation ought to have very little overhead; therefore, we simulated an efficient implementation by reusing the timing data from the non-batch-normalized networks. Learning rates were tuned separately for all four conditions (similarly to the rest of our experiments).\nTraining curves are shown in Figure 4. All of the methods achieved worse test error than the original network as a result of `2 regularization being eliminated. However, the BN networks reached a lower test error than the non-BN networks before they start overfitting, consistent with the stochastic regularization interpretation of BN.7 For both the BN and non-BN architectures, KFC-\nreported in Figure 3, the test error on CIFAR-10 leveled off at 18.5% after 5 minutes, after which the network started overfitting. When we reduced the mini-batch size from 512 to 128, the test error reached 17.5% after 5 minutes and 16% after 35 minutes. However, this run performed far worse on the training set. On the flip size, very large mini-batch sizes hurt generalization for both methods, as discussed in Section 5.3.\n6For SGD, we used a momentum parameter of 0.9 and mini-batches of size 128, which match the previously published configurations. For KFC-pre, we used a momentum parameter of 0.9, mini-batches of size 512, and a damping parameter \u03b3 = 10\u22123. In both cases, our informal explorations did not find other values which performed substantially better in terms of training error.\n7Interestingly, the BN networks were slower to optimize the training error than their non-BN counterparts. We speculate that this is because (1) the SGD baseline, being carefully tuned, didn\u2019t exhibit the pathologies that BN is meant to correct for (i.e. dead units and extreme covariate shift), and (2) the regularization effects of BN made it harder to overfit.\npre optimized both the training and test error and NLL considerably faster than SGD. Furthermore, it appeared not to lose the regularization benefit of BN. This suggests that KFC-pre and BN can be combined synergistically."}, {"heading": "5.3 Potential for distributed implementation", "text": "Much work has been devoted recently to highly parallel or distributed implementations of neural network optimization (e.g. Dean et al. (2012)). Synchronous SGD effectively allows one to use very large mini-batches efficiently, which helps optimization by reducing the variance in the stochastic gradient estimates. However, the per-update performace levels off to that of batch SGD once the variance is no longer significant and curvature effects come to dominate. Asynchronous SGD partially alleviates this issue by using new network parameters as soon as they become available, but needing to compute gradients with stale parameters limits the benefits of this approach.\nAs a proxy for how the algorithms are likely to perform in a highly distributed setting8, we measured the classification error as a function of the number of iterations (weight updates) for each algorithm. Both algorithms were run with large mini-batches of size 4096 (in place of 128 for SGD and 512 for KFC-pre). Figure 5 shows training curves for both algorithms on CIFAR-10\n8Each iteration of KFC-pre requires many of the same computations as SGD, most notably computing activations and gradients. There were two major sources of additional overhead: maintaining empirical averages of covariance statistics, and computing inverses or eigendecompositions of the Kronecker factors. These additional operations can almost certainly be performed asynchronously; in our own experiments, we only periodically performed these operations, and this did not cause a significant drop in performance. Therefore, we posit that each iteration of KFC-pre requires a comparable number of sequential operations to SGD for each weight update. This is in contrast to other methods which make good use of large mini-batches such as Hessian-free optimization (Martens, 2010), which requires many sequential iterations for each weight update. KFC-pre also adds little communication overhead, as the Kronecker factors need not be sent to the worker nodes which compute the gradients.\nand SVHN, using the same architectures as above.9 KFC-pre required far fewer weight updates to achieve good training and test error compared with SGD. For instance, on CIFAR-10, KFC-pre obtained a training error of 10% after 300 updates, compared with 6000 updates for SGD, a 20-fold improvement. Similar speedups were obtained on test error and on the SVHN dataset. These results suggest that a distributed implementation of KFC-pre has the potential to obtain large speedups over distributed SGD-based algorithms."}, {"heading": "Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M. Z., Ranzato, M., Senior,", "text": "A., Tucker, P., Yang, K., and Ng, A. Y. Large scale distributed deep networks. In Neural Information Processing Systems, 2012.\n9Both SGD and KFC-pre reached a slightly worse test error before they started overfitting, compared with the small-minibatch experiments of the previous section. This is because large mini-batches lose the regularization benefit of stochastic gradients. One would need to adjust the regularizer in order to get good generalization performance in this setting.\nDemmel, J. W. Applied Numerical Linear Algebra. SIAM, 1997.\nDesjardins, G., Simonyan, K., Pascanu, R., and Kavukcuoglu, K. Natural neural networks. arXiv:1507.00210, 2015.\nDuchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGrosse, Roger and Salakhutdinov, Ruslan. Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix. In Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\nHeskes, Tom. On \u201cnatural\u201d learning and pruning in multilayered perceptrons. Neural Computation, 12(4): 881\u2013901, 2000.\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9:1735\u20131780, 1997.\nIoffe, S. and Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.\nKingma, D. P. and Ba, J. L. Adam: a method for stochastic optimization. In International Conference on Learning Representations, 2015.\nKrizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In Neural Information Processing Systems, 2012."}, {"heading": "Le Roux, Nicolas, Manzagol, Pierre-antoine, and Bengio, Yoshua. Topmoumoute online natural gradient", "text": "algorithm. In Advances in Neural Information Processing Systems 20, pp. 849\u2013856. MIT Press, 2008."}, {"heading": "LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.", "text": "Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989.\nLeCun, Y., Bottou, L., Orr, G., and Mu\u0308ller, K. Efficient backprop. Neural networks: Tricks of the trade, pp. 546\u2013546, 1998.\nMartens, J. Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning (ICML), 2010.\nMartens, J. New insights and perspectives on the natural gradient method, 2014.\nMartens, J. and Grosse, R. Optimizing neural networks with Kronecker-factored approximate curvature. In International Conference on Machine Learning, 2015.\nMnih, V. CUDAMat: A CUDA-based matrix class for Python. Technical Report 004, University of Toronto, 2009.\nMore\u0301, J.J. The Levenberg-Marquardt algorithm: implementation and theory. Numerical analysis, pp. 105\u2013116, 1978.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. In Neural Information Processing Systems Deep Learning and Unsupervised Feature Learning Workshop, 2011.\nNocedal, Jorge and Wright, Stephen J. Numerical optimization. Springer, 2. ed. edition, 2006.\nOllivier, Y. Riemannian metrics for neural networks I: feedforward networks. Information and Inference, 4 (2):108\u2013153, 2015.\nPascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, 2013.\nPascanu, R., Dauphin, Y. N., Ganguli, S., and Bengio, Y. On the saddle point problem for non-convex optimization. arXiv:1405.4604, 2014.\nPolyak, B. T. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal of Control and Optimization, 30(4):838\u2013855, 1992.\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, Sanjeev. Parallel training of DNNs with natural gradient and parameter averaging. In International Conference on Learning Representations: Workshop track, 2015.\nRanzato, M. and Hinton, G. E. Modeling pixel means and covariances using factorized third-order Boltzmann machines. In Computer Vision and Pattern Recognition, 2010.\nRumelhart, D.E., Hinton, G.E., and Williams, R.J. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, 1986.\nSchraudolph, Nicol N. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14, 2002.\nSimoncelli, E. P. and Olshausen, B. A. Natural image statistics and neural representation. Annual Review of Neuroscience, 24:1193\u20131216, 2001.\nSohl-Dickstein, J., Poole, B., and Ganguli, S. Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods. In International Conference on Machine Learning, 2014.\nSrivastava, N. Improving neural networks with dropout. Master\u2019s thesis, University of Toronto, 2013.\nSrivastava, N. Toronto Deep Learning ConvNet. https://github.com/TorontoDeepLearning/convnet/, 2015.\nSutskever, I., Vinyals, O., and Le, Q. V. V. Sequence to sequence learning with neural networks. In Neural Information Processing Systems, 2014.\nSwersky, K., Chen, Bo, Marlin, B., and de Freitas, N. A tutorial on stochastic approximation algorithms for training restricted Boltzmann machines and deep belief nets. In Information Theory and Applications Workshop (ITA), 2010, pp. 1\u201310, Jan 2010."}, {"heading": "Tieleman, T. and Hinton, G. Lecture 6.5, RMSProp. In Coursera course Neural Networks for Machine", "text": "Learning, 2012.\nVatanen, Tommi, Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities. 2013.\nVinyals, O. and Povey, D. Krylov subspace descent for deep learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.\nYang, Z., Moczulski, M., Denil, M., de Freitas, N., Smola, A., Song, L., and Wang, Z. Deep fried convnets. arXiv:1412.7149, 2014.\nZeiler, Matthew D. ADADELTA: An adaptive learning rate method. 2013."}, {"heading": "A Optimization methods", "text": ""}, {"heading": "A.1 KFC as a preconditioner for SGD", "text": "The first optimization procedure we used in our experiments was a generic natural gradient descent approximation, where F\u0302(\u03b3) was used to approximate F. This procedure is like SGD with momentum, except that \u2207\u0302h is substituted for the Euclidean gradient. One can also view this as a preconditioned SGD method, where F\u0302(\u03b3) is used as the preconditioner. To distinguish this optimization procedure from the KFC approximation itself, we refer to it as KFC-pre. Our procedure is perhaps more closely analogous to earlier Kronecker product-based natural gradient approximations (Heskes, 2000; Povey et al., 2015) than to K-FAC itself.\nIn addition, we used a variant of gradient clipping (Pascanu et al., 2013) to avoid instability. In particular, we clipped the approximate natural gradient update v so that \u03bd , v>Fv < 0.3, where F is estimated using 1/4 of the training examples from the current mini-batch. One motivation for this heuristic is that \u03bd approximates the KL divergence of the predictive distributions before and after the update, and one wouldn\u2019t like the predictive distributions to change too rapidly. The value \u03bd can be computed using curvature-vector products (Schraudolph, 2002). The clipping was only triggered near the beginning of optimization, where the parameters (and hence also the curvature) tended to change rapidly.10 Therefore, one can likely eliminate this step by initializing from a model partially trained using SGD.\nTaking inspiration from Polyak averaging (Polyak & Juditsky, 1992; Swersky et al., 2010), we used an exponential moving average of the iterates. This helps to smooth out the variability caused by the mini-batch selection. The full optimization procedure is given in Algorithm 1."}, {"heading": "A.2 Kronecker-factored approximate curvature", "text": "The central idea of K-FAC is the combination of approximations to the Fisher matrix described in Section 2.2. While one could potentially perform standard natural gradient descent using the approximate natural gradient \u2207\u0302h, perhaps with a fixed learning rate and with fixed Tikhonov-style damping/reglarization, Martens & Grosse (2015) found that the most effective way to use \u2207\u0302h was within a robust 2nd-order optimization framework based on adaptively damped quadratic models, similar to the one employed in HF (Martens, 2010). In this section, we describe the K-FAC method in detail, while omitting certain aspects of the method which we do not use, such as the block tri-diagonal inverse approximation.\nK-FAC uses a quadratic model of the objective to dynamically choose the step size \u03b1 and momentum decay parameter \u00b5 at each step. This is done by taking v = \u03b1\u2207\u0302h+\u00b5vprev where vprev is the update computed at the previous iteration, and minimizing the following quadratic model of the objective (over the current mini-batch):\nM(\u03b8 + v) = h(\u03b8) +\u2207h>v + 1 2 v>(F + rI)v. (36)\nwhere we assume the h is the expected loss plus an `2-regularization term of the form r 2\u2016\u03b8\u2016 2. Since F behaves like a curvature matrix, this quadratic function is similar to the second-order\n10This may be counterintuitive, since SGD applied to neural nets tends to take small steps early in training, at least for commonly used initializations. For SGD, this happens because the initial parameters, and hence also the initial curvature, are relatively small in magnitude. Our method, which corrects for the curvature, takes larger steps early in training, when the error signal is the largest.\nAlgorithm 1 Using KFC as a preconditioner for SGD\nRequire: initial network parameters \u03b8(0)\nweight decay penalty \u03bb learning rate \u03b1 momentum parameter \u00b5 (suggested value: 0.9) parameter averaging timescale \u03c4 (suggested value: number of mini-batches in the dataset) damping parameter \u03b3 (suggested value: 10\u22123, but this may require tuning) statistics update period Ts (see Appendix A.3) inverse update period Tf (see Appendix A.3) clipping parameter C (suggested value: 0.3)\nk \u2190 0 p\u2190 0 \u03be \u2190 e\u22121/\u03c4 \u03b8\u0304 (0) \u2190 \u03b8(0) Estimate the factors {\u2126`}L\u22121`=0 and {\u0393`}L`=1 on the full dataset using Eqn. 32 Compute the inverses {[\u2126(\u03b3)` ]\u22121} L\u22121 `=0 and {[\u0393 (\u03b3) ` ] \u22121}L`=1 using Eqn. 28 while stopping criterion not met do k \u2190 k + 1 Select a new mini-batch\nif k \u2261 0 (mod Ts) then Update the factors {\u2126`}L\u22121`=0 and {\u0393`}L`=1 using Eqn. 33 end if if k \u2261 0 (mod Tf ) then\nCompute the inverses {[\u2126(\u03b3)` ]\u22121} L\u22121 `=0 and {[\u0393 (\u03b3) ` ] \u22121}L`=1 using Eqn. 28\nend if\nCompute \u2207h using backpropagation Compute \u2207\u0302h = [F\u0302(\u03b3)]\u22121\u2207h using Eqn. 29 v\u2190 \u2212\u03b1\u2207\u0302h\n{Clip the update if necessary} Estimate \u03bd = v>Fv + \u03bbv>v using a subset of the current mini-batch if \u03bd > C then\nv\u2190 v/ \u221a \u03bd/C\nend if\np(k) \u2190 \u00b5p(k\u22121) + v {Update momentum} \u03b8(k) \u2190 \u03b8(k\u22121) + p(k) {Update parameters} \u03b8\u0304\n(k) \u2190 \u03be\u03b8\u0304(k\u22121) + (1\u2212 \u03be)\u03b8(k) {Parameter averaging} end while return Averaged parameter vector \u03b8\u0304 (k)\nTaylor approximation to h. Note that here we use the exact F for the mini-batch, rather than the approximation F\u0302. Intuitively, one can think of v as being itself iteratively optimized at each step in order to better minimize M , or in other words, to more closely match the true natural gradient (which is the exact minimum of M). Interestingly, in full batch mode, this method is equivalent to performing preconditioned conjugate gradient in the vicinity of a local optimum (where F remains approximately constant).\nTo see how this minimization over \u03b1 and \u00b5 can be done efficiently, without computing the entire matrix F, consider the general problem of minimizing M on the subspace spanned by arbitrary vectors {v1, . . . ,vR}. (In our case, R = 2, v1 = \u2207\u0302h and v2 = vprev.) The coefficients \u03b1 can be found by solving the linear system C\u03b1 = \u2212d, where Cij = v>i Fvj and di = \u2207h>vi. To compute the matrix C, we compute each of the matrix-vector products Fvj using automatic differentiation (Schraudolph, 2002).\nBoth the approximate natural gradient \u2207\u0302h and the update v (generated as described above) arise as the minimum, or approximate minimum, of a corresponding quadratic model. In the case of v, this model is given by M and is designed to be a good local approximation to the objective h. Meanwhile, the quadratic model which is implicitly minimized when computing \u2207\u0302h is designed to approximate M (by approximating F with F\u0302).\nBecause these quadratic models are approximations, naively minimizing them over Rn can lead to poor results in both theory and practice. To help deal with this problem K-FAC employs an adaptive Tikhonov-style damping scheme applied to each of them (the details of which differ in either case).\nTo compensate for the inaccuracy of M as a model of h, K-FAC adds a Tikhonov regularization term \u03bb2 \u2016v\u2016\n2 to M which encourages the update to remain small in magnitude, and thus more likely to remain in the region where M is a reasonable approximation to h. This amounts to replacing r with r + \u03bb in Eqn. 36. Note that this technique is formally equivalent to performing constrained minimization of M within some spherical region around v = 0 (a \u201ctrust-region\u201d). See for example Nocedal & Wright (2006).\nK-FAC uses the well-known Levenberg-Marquardt technique (More\u0301, 1978) to automatically adapt the damping parameter \u03bb so that the damping is loosened or tightened depending on how accurately M(\u03b8 + v) predicts the true decrease in the objective function after each step. This accuracy is measured by the so-called \u201creduction ratio\u201d, which is given by\n\u03c1 = h(\u03b8)\u2212 h(\u03b8 + v) M(\u03b8)\u2212M(\u03b8 + v) , (37)\nand should be close to 1 when the quadratic approximation is reasonably accurate around the given value of \u03b8. The update rule for \u03bb is as follows:\n\u03bb\u2190  \u03bb \u00b7 \u03bb\u2212 if \u03c1 > 3/4\u03bb if 1/4 \u2264 \u03c1 \u2264 3/4 \u03bb \u00b7 \u03bb+ if \u03c1 < 1/4\n(38)\nwhere \u03bb+ and \u03bb\u2212 are constants such that \u03bb\u2212 < 1 < \u03bb+. To compensate for the inaccuracy of F\u0302, and encourage \u2207\u0302h to be smaller and more conservative, K-FAC similarly adds \u03b3I to F\u0302 before inverting it. As discussed in Section 2.2, this can be done approximately by adding multiples of I to each of the Kronecker factors \u03a8` and \u0393` of F\u0302` before inverting them. Alternatively, an exact solution can be obtained by expanding out the\neigendecomposition of each block F\u0302` of F\u0302, and using the following identity:[ F\u0302` + \u03b3I ]\u22121 = [ (Q\u03a8 \u2297Q\u0393) (D\u03a8 \u2297D\u0393) ( Q>\u03a8 \u2297Q>\u0393 ) + \u03b3I ]\u22121 (39)\n= [ (Q\u03a8 \u2297Q\u0393) (D\u03a8 \u2297D\u0393 + \u03b3I) ( Q>\u03a8 \u2297Q>\u0393 )]\u22121 (40)\n= (Q\u03a8 \u2297Q\u0393) (D\u03a8 \u2297D\u0393 + \u03b3I)\u22121 ( Q>\u03a8 \u2297Q>\u0393 ) , (41)\nwhere \u03a8` = Q\u03a8D\u03a8Q > \u03a8 and \u0393` = Q\u0393D\u0393Q > \u0393 are the orthogonal eigendecompositions of \u03a8` and \u0393` (which are symmetric PSD). These manipulations are based on well-known properties of the Kronecker product which can be found in, e.g., Demmel (1997, sec. 6.3.3). Matrix-vector products (F\u0302+\u03b3I)\u22121\u2207h can then be computed from the above identity using the following block-wise formulas:\nV1 = Q > \u0393 (\u2207W\u0304`h)Q\u03a8 (42) V2 = V1/(d\u0393d > \u03a8 + \u03b3) (43)\n(F\u0302` + \u03b3I) \u22121 vec(\u2207W\u0304`h) = vec ( Q\u0393V2Q > \u03a8 ) , (44)\nwhere d\u0393 and d\u03a8 are the diagonals of D\u0393 and D\u03a8 and the division and addition in Eqn. 43 are both elementwise.\nOne benefit of this damping strategy is that it automatically accounts for the curvature contributed by both the quadratic damping term \u03bb2 \u2016v\u2016 2 and the weight decay penalty r2\u2016\u03b8\u2016 2 if these\nare used. Heuristically, one could even set \u03b3 = \u221a \u03bb+ r, which can sometimes perform well. One should always choose \u03b3 at least this large. However, it may sometimes be advantageous to choose \u03b3 significantly larger, as F\u0302 might not be a good approximation to F, and the damping may help reduce the impact of directions erroneously estimated to have low curvature. For consistency with Martens & Grosse (2015), we adopt their method of automatically adapting \u03b3. In particular, each time we adapt \u03b3, we compute \u2207\u0302h for three different values \u03b3\u2212 < \u03b3 < \u03b3+. We choose whichever of the three values results in the lowest value of M(\u03b8 + v)."}, {"heading": "A.3 Efficient implementation", "text": "We based our implementation on the Toronto Deep Learning ConvNet (TDLCN) package (Srivastava, 2015), which is a Python wrapper around CUDA kernels. We needed to write a handful of additional kernels:\n\u2022 a kernel for computing \u2126\u0302` (Eqn. 32)\n\u2022 kernels which performed forward mode automatic differentiation for the max-pooling and response normalization layers\nMost of the other operations for KFC could be performed on the GPU using kernels provided by TDLCN. The only exception is computing the inverses {[\u2126(\u03b3)` ]\u22121} L\u22121 `=0 and {[\u0393 (\u03b3) ` ] \u22121}L`=1, which was done on the CPU. (The forward mode kernels are only used in update clipping; as mentioned above, one can likely eliminate this step in practice by initializing from a partially trained model.)\nKFC introduces several sources of overhead per iteration compared with SGD:\n\u2022 Updating the factors {\u2126`}L\u22121`=0 and {\u0393`}L`=1\n\u2022 Computing the inverses {[\u2126(\u03b3)` ]\u22121} L\u22121 `=0 and {[\u0393 (\u03b3) ` ] \u22121}L`=1\n\u2022 Computing the approximate natural gradient \u2207\u0302h = [F\u0302(\u03b3)]\u22121\u2207h\n\u2022 Estimating \u03bd = v>Fv + \u03bbv>v (which is used for gradient clipping)\nThe overhead from the first two could be reduced by only periodically recomputing the factors and inverses, rather than doing so after every mini-batch. The cost of estimating v>Fv can be reduced by using only a subset of the mini-batch. These shortcuts did not seem to hurt the per-epoch progress very much, as one can get away with using quite stale curvature information, and \u03bd is only used for clipping and therefore doesn\u2019t need to be very accurate. The cost of computing \u2207\u0302h is unavoidable, but because it doesn\u2019t grow with the size of the mini-batch, its per-epoch cost can be made smaller by using larger mini-batches. (As we discuss further in Section 5.3, KFC can work well with large mini-batches.) These shortcuts introduce several additional hyperparameters, but fortunately these are easy to tune: we simply chose them such that the per-epoch cost of KFC was less than twice that of SGD. This requires only running a profiler for a few epochs, rather than measuring overall optimization performance.\nObserve that the inverses {[\u2126(\u03b3)` ]\u22121} L\u22121 `=0 and {[\u0393 (\u03b3) ` ] \u22121}L`=1 are computed on the CPU, while all of the other heavy computation is GPU-bound. In principle, since KFC works fine with stale curvature information, the inverses could be computed asychronously while the algorithm is running, thereby making their cost almost free. We did not exploit this in our experiments, however."}, {"heading": "B Proofs of theorems", "text": ""}, {"heading": "B.1 Proofs for Section 3", "text": "Lemma 1. Under approximation IAD, E [Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4aj\u2032,t\u2032+\u03b4\u2032 ]E [Dsi,tDsi\u2032,t\u2032 ] (45)\nE [Dwi,j,\u03b4Dbi\u2032 ] = \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4]E [Dsi,tDsi\u2032,t\u2032 ] (46)\nE [DbiDbi\u2032 ] = |T |E [Dsi,tDsi\u2032,t\u2032 ] (47)\nProof. We prove the first equality; the rest are analogous.\nE[Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = E [(\u2211 t\u2208T aj,t+\u03b4Dsi,t )(\u2211 t\u2032\u2208T aj\u2032,t\u2032+\u03b4\u2032Dsi\u2032,t\u2032 )]\n(48)\n= E [\u2211 t\u2208T \u2211 t\u2032\u2208T aj,t+\u03b4Dsi,taj\u2032,t\u2032+\u03b4\u2032Dsi\u2032,t\u2032 ]\n(49)\n= \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4Dsi,taj\u2032,t\u2032+\u03b4\u2032Dsi\u2032,t\u2032 ] (50)\n= \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4aj\u2032,t\u2032+\u03b4\u2032 ]E [Dsi,tDsi\u2032,t\u2032 ] (51)\nAssumption IAD is used in the final line.\nTheorem 1. Combining approximations IAD, SH, and SUD yields the following factorization:\nE [Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u03b2(\u03b4, \u03b4\u2032) \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) \u0393(i, i\u2032, 0), E [Dwi,j,\u03b4Dbi\u2032 ] = \u03b2(\u03b4)M(j) \u0393(i, i\u2032, 0)\nE [DbiDbi\u2032 ] = |T |\u0393(i, i\u2032, 0) (52)\nwhere\n\u03b2(\u03b4) , (T1 \u2212 |\u03b41|) (T2 \u2212 |\u03b42|) \u03b2(\u03b4, \u03b4\u2032) , (T1 \u2212max(\u03b41, \u03b4\u20321, 0) + min(\u03b41, \u03b4\u20321, 0)) \u00b7 (T2 \u2212max(\u03b42, \u03b4\u20322, 0) + min(\u03b42, \u03b4\u20322, 0)) (53)"}, {"heading": "Proof.", "text": "E[Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4aj\u2032,t\u2032+\u03b4\u2032 ]E [Dsi,tDsi\u2032,t\u2032 ] (54)\n= \u2211 t\u2208T \u2211 t\u2032\u2208T \u2126(j, j\u2032, t\u2032 + \u03b4\u2032 \u2212 t\u2212 \u03b4)1 t+\u03b4\u2208T t\u2032+\u03b4\u2032\u2208T \u0393(i, i\u2032, t\u2032 \u2212 t) (55)\n= \u2211 t\u2208T \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1 t+\u03b4\u2208T t+\u03b4\u2032\u2208T \u0393(i, i\u2032, 0) (56) = |{t \u2208 T : t+ \u03b4 \u2208 T , t+ \u03b4\u2032 \u2208 T }| \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) \u0393(i, i\u2032, 0) (57) = \u03b2(\u03b4, \u03b4\u2032) \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) \u0393(i, i\u2032, 0) (58)\nLines 54, 55, and 56 use Lemma 1 and assumptions SH, and SUD, respectively. In Line 55, the indicator function (denoted 1) arises because the activations are defined to be zero outside the set of spatial locations. The remaining formulas can be derived analogously.\nTheorem 2. Under assumption SH, \u2126` = E [ JA`K>HJA`KH ] (59)\n\u0393` = 1 |T | E [ DS>` DS` ] (60)\nProof. In this proof, all activations and pre-activations are taken to be in layer `. The expected entries are given by:\nE [ JA`K>HJA`KH ] j|\u2206|+\u03b4, j\u2032|\u2206|+\u03b4 = E [\u2211 t\u2208T aj,t+\u03b4aj\u2032,t+\u03b4\u2032 ] (61)\n= \u2211 t\u2208T E [aj,t+\u03b4aj\u2032,t+\u03b4\u2032 ] (62)\n= \u2211 t\u2208T \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1 t+\u03b4\u2208T t+\u03b4\u2032\u2208T\n(63)\n= |{t \u2208 T : t+ \u03b4 \u2208 T , t+ \u03b4\u2032 \u2208 T }| \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) (64) = \u03b2(\u03b4, \u03b4\u2032) \u2126(j, j\u2032, \u03b4\u2032 \u2212 \u03b4) (65) = [\u2126`]j|\u2206|+\u03b4, j\u2032|\u2206|+\u03b4\u2032 (66)\nSH is used in Line 63. Similarly,\nE [ JA`K>HJA`KH ] 0, j|\u2206|+\u03b4 = E [\u2211 t\u2208T aj,t+\u03b4 ] (67)\n= \u03b2(\u03b4)M(j) (68)\n= [\u2126`]0, j|\u2206|+\u03b4 (69)[ JA`K>HJA`KH ] 0, 0 = |T | (70)\n= [\u2126`]0, 0 (71)\nE [ DS>` DS` ] i,i\u2032 = E [\u2211 t\u2208T Dsi,tDsi\u2032,t ] (72)\n= |T |\u0393(i, i\u2032, 0) (73) = |T | [\u0393`]i, i\u2032 (74)"}, {"heading": "B.2 Proofs for Section 4", "text": "Preliminaries and notation. In discussing invariances, it will be convenient to add homogeneous coordinates to various matrices:\n[A`]H , ( 1 A` ) (75)\n[S`]H , ( 1 S` ) (76)\n[W\u0304`]H , ( 1 b` W` ) (77)\nWe also define the activation function \u03c6 to ignore the homogeneous column, so that\n[A`]H = \u03c6([S`]H) = \u03c6(JA`\u22121K[W\u0304`]H). (78)\nUsing the homogeneous coordinate notation, we can write the effect of the affine transformations on the pre-activations and activations:\n[S\u2020`U` + 1c > ` ]H = [S \u2020 `]H [U`]H\n[A`V` + 1d > ` ]H = [A`]H [V`]H , (79)\nwhere\n[U`]H ,\n( 1 c>`\nU`\n) (80)\n[V`]H ,\n( 1 d>`\nV`\n) . (81)\nThe inverse transformations are represented as\n[U`] \u22121 H ,\n( 1 \u2212c>` U \u22121 `\nU\u22121`\n) (82)\n[V`] \u22121 H ,\n( 1 \u2212d>` V \u22121 `\nV\u22121`\n) . (83)\nWe can also determine the effect of the affine transformation on the expanded activations:\nJA`V` + 1d>` KH = JA`KHJV`KH , (84)\nwhere\nJV`KH , (\n1 d>` \u2297 1> V` \u2297 I\n) , (85)\nwith inverse\nJV`K\u22121H = ( 1 \u2212d>` V \u22121 ` \u2297 1>\nV\u22121` \u2297 I\n) . (86)\nNote that JV`KH is simply a suggestive notation, rather than an application of the expansion operator J\u00b7K.\nLemma 2. Let N , \u03b8, {\u03c6`}L`=0, and {\u03c6 \u2020 `}L`=0 be given as in Theorem 3. The network N \u2020 with activations functions {\u03c6\u2020`}L`=0 and parameters defined by\n[W\u0304\u2020` ]H , [U`] \u2212> H [W\u0304`]HJV`\u22121K\u2212>H , (87)\ncompute the same function as N .\nRemark. The definition of \u03c6\u2020` (Eqn. 34) can be written in homogeneous coordinates as\n[A\u2020`]H = \u03c6 \u2020 `([S \u2020 `]H) = \u03c6`([S \u2020 `]H [U`]H)[V`]H . (88)\nEqn. 87 can be expressed equivalently without homogeneous coordinates as\nW\u0304\u2020` , U \u2212> ` ( W\u0304` \u2212 c`e> ) JV`\u22121K\u2212>H , (89)\nwhere e = (1 0 \u00b7 \u00b7 \u00b7 0)>.\nProof. We will show inductively the following relationship between the activations in each layer of the two networks:\n[A\u2020`]H = [A`]H [V`]H . (90)\n(By our assumption that the top layer inputs are not transformed, i.e. [VL]H = I, this would imply that [A\u2020L]H = [AL]H , and hence that the networks compute the same function.) For the first layer, Eqn. 90 is true by definition. For the inductive step, assume Eqn. 90 holds for layer ` \u2212 1. From Eqn 84, this is equivalent to\nJA\u2020`\u22121KH = JA`\u22121KHJV`\u22121KH . (91)\nWe then derive the activations in the following layer:\n[A\u2020`]H = \u03c6 \u2020 ` ( [S\u2020`]H ) (92)\n= \u03c6` ( [S\u2020`]H [U`]H ) [V`]H (93)\n= \u03c6` ( JA\u2020`\u22121KH [W\u0304 \u2020 ` ] > H [U`]H ) [V`]H (94)\n= \u03c6` ( JA`\u22121KH JV`\u22121KH [W\u0304\u2020` ]>H [U`]H ) [V`]H (95)\n= \u03c6` ( JA`\u22121KH JV`\u22121KH JV`\u22121K\u22121H [W\u0304`]>H [U`]\u22121H [U`]H ) [V`]H (96)\n= \u03c6` ( JA`\u22121KH [W\u0304`]>H ) [V`]H (97)\n= [A`]H [V`]H (98)\nLines 94 and 98 are from Eqn. 78. This proves the inductive hypothesis for layer `, so we have shown that both networks compute the same function.\nLemma 3. Suppose the parameters are transformed according to Lemma 2, and the parameters are updated according to\n[W\u0304\u2020` ] (k+1) \u2190 [W\u0304\u2020` ] (k) \u2212 \u03b1P\u2020`(\u2207W\u0304\u2020`h)R \u2020 `, (99)\nfor matrices P` and R`. This is equivalent to applying the following update to the original network:\n[W\u0304`] (k+1) \u2190 [W\u0304`](k+1) \u2212 \u03b1P`(\u2207W\u0304`h)R`, (100)\nwith\nP` = U > ` P \u2020 `U` (101) R` = JV`\u22121KHR\u2020`JV`\u22121K>H . (102)\nProof. This is a special case of Lemma 5 from Martens & Grosse (2015).\nTheorem 3. Let N be a network with parameter vector \u03b8 and activation functions {\u03c6`}L`=0. Given activation functions {\u03c6\u2020`}L`=0 defined as in Eqn. 34, there exists a parameter vector \u03b8 \u2020 such that a network N \u2020 with parameters \u03b8\u2020 and activation functions {\u03c6\u2020`}L`=0 computes the same function as N . The KFC updates on N and N \u2020 are equivalent, in that the resulting networks compute the same function.\nProof. Lemma 2 gives the desired \u03b8\u2020. We now prove equivalence of the KFC updates. The Kro-\nnecker factors for N \u2020 are given by:\n\u2126\u2020` = E [ JA\u2020`K>HJA \u2020 `KH ]\n(103) = E [ JV`K>HJA`K>HJA`KHJV`KH ] (104)\n= JV`K>HE [ JA`K>HJA`KH ] JV`KH (105) = JV`K>H\u2126`JV`KH (106)\n\u0393\u2020` = 1 |T | E [ (DS\u2020`) >DS\u2020` ]\n(107)\n= 1 |T | E [ U`(DS\u2020`) >DS\u2020`U > ` ] (108)\n= 1\n|T | U`E\n[ (DS\u2020`) >DS\u2020` ] U>` (109)\n= U`\u0393`U > ` (110)\nThe approximate natural gradient update, ignoring momentum, clipping, and damping, is given by \u03b8(k+1) \u2190 \u03b8(k) \u2212 \u03b1F\u0302\u22121\u2207\u03b8h. For each layer of N \u2020,\n[W\u0304\u2020` ] (k+1) \u2190 [W\u0304\u2020` ] (k) \u2212 \u03b1(\u0393\u2020`) \u22121(\u2207W\u0304\u2020`h)(\u2126 \u2020 `\u22121) \u22121 (111)\nWe apply Lemma 3 with P\u2020` = (\u0393 \u2020 `) \u22121 and R\u2020` = (\u2126 \u2020 `\u22121) \u22121. This gives us\nP` = U > ` (\u0393 \u2020 `) \u22121U` (112)\n= \u0393\u22121` (113) R` = JV`\u22121KH(\u2126\u2020`\u22121)\u22121JV`\u22121K>H (114) = \u2126\u22121`\u22121, (115)\nwith the corresponding update\n[W\u0304`] (k+1) \u2190 [W\u0304`](k) \u2212 \u03b1\u0393\u22121` (\u2207W\u0304`h)\u2126 \u22121 `\u22121. (116)\nBut this is the same as the KFC update for the original network. Therefore, the two updates are equivalent, in that the resulting networks compute the same function.\nTheorem 4. Combining approximations IAD, SH, SUA, and WD results in the following approximation to the entries of the Fisher matrix:\nE [Dwi,j,\u03b4Dwi\u2032,j\u2032,\u03b4\u2032 ] = \u03b2(\u03b4, \u03b4\u2032) \u2126\u0303(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1i=i\u2032 (117) E [Dwi,j,\u03b4Dbi\u2032 ] = \u03b2(\u03b4)M(j)1i=i\u2032 (118)\nE [DbiDbi\u2032 ] = |T |1i=i\u2032 (119)\nwhere 1 is the indicator function and \u2126\u0303(j, j\u2032, \u03b4) = \u03a3(j, j\u2032)1\u03b4=0 + M(j)M(j \u2032) is the uncentered autocovariance function. (\u03b2 is defined in Theorem 1.) If the \u03b2 and |T | terms are dropped, the resulting approximate natural gradient descent update rule is equivalent to idealized PRONG, up to rescaling.\nProof. We first compute the second moments of the activations and derivatives, under assumptions SH, SUA, and WD:\nE [aj,taj\u2032,t\u2032 ] = Cov(aj,t, aj\u2032,t\u2032) + E[aj,t]E[aj\u2032,t\u2032 ] (120) = \u03a3(j, j\u2032)1\u03b4=0 +M(j)M(j \u2032) (121)\n, \u2126\u0303(j, j\u2032, \u03b4) (122)\nE [Dsi,tDsi\u2032,t\u2032 ] = 1i=i\u20321\u03b4=\u03b4\u2032 . (123)\nfor any t, t\u2032 \u2208 T . We now compute E [Dwi,j,\u03b4Dwi,j,\u03b4] = \u2211 t\u2208T \u2211 t\u2032\u2208T E [aj,t+\u03b4aj\u2032,t\u2032+\u03b4\u2032 ]E [Dsi,tDsi\u2032,t\u2032 ] (124)\n= \u2211 t\u2208T \u2211 t\u2032\u2208T \u2126\u0303(j, j\u2032, t\u2032 + \u03b4\u2032 \u2212 t\u2212 \u03b4)1 t+\u03b4\u2208T t\u2032+\u03b4\u2032\u2208T 1i=i\u20321t=t\u2032 (125)\n= \u2211 t\u2208T \u2126\u0303(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1 t+\u03b4\u2208T t+\u03b4\u2032\u2208T 1i=i\u2032 (126) = |{t \u2208 T : t+ \u03b4 \u2208 T , t+ \u03b4\u2032 \u2208 T }| \u2126\u0303(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1i=i\u2032 (127) = \u03b2(\u03b4, \u03b4\u2032) \u2126\u0303(j, j\u2032, \u03b4\u2032 \u2212 \u03b4)1i=i\u2032 (128)\nLine 124 is from Lemma 1. The other formulas are derived analogously. This can be written in matrix form as\nF\u0302 = \u2126\u0303\u2297 I (129)\n\u2126\u0303 ,\n( 1 \u00b5> \u2297 1> \u00b5\u2297 1 \u03a3\u2297 I + \u00b5\u00b5> \u2297 11> )\n(130)\nIt is convenient to compute block Cholesky decompositions:\n\u2126\u0303 =\n( 1\n\u00b5\u2297 1 B\u2297 I\n)( 1 \u00b5> \u2297 1>\nB> \u2297 I\n) (131)\n, LL> (132)\n\u2126\u0303 \u22121 = L\u2212>L\u22121 (133)\n=\n( 1 \u2212\u00b5>B\u2212> \u2297 1>\nB\u2212> \u2297 I\n)( 1\n\u2212B\u22121\u00b5\u2297 1 B\u22121 \u2297 I\n) , (134)\nwhere B is some square root matrix, i.e. BB> = \u03a3 (not necessarily lower triangular). Now consider PRONG. In the original algorithm, the network is periodically reparameterized such that the activations are white. In our idealized version of the algorithm, we assume this is done after every update. For convenience, we assume that the network is converted to the white parameterizaton immediately before computing the SGD update, and then converted back to its original parameterization immediately afterward. In other words, we apply an affine transformation (Eqn. 34) which whitens the activations:\nA\u2020` = \u03c6 \u2020 `(S \u2020 `) = ( \u03c6`(S \u2020 `)\u2212 1\u00b5 > ) B\u22121 (135)\n= \u03c6`(S \u2020 `)B \u22121 \u2212 1\u00b5>B\u22121, (136)\nwhere B is a square root matrix of \u03a3, as defined above. This is an instance of Eqn. 34 with U` = I, c` = 0, V` = B\n\u22121, and d` = \u2212B\u22121\u00b5. The transformed weights which compute the same function as the original network according to Lemma 2 are W\u0304\u2020` = W\u0304` JB\u22121K\u2212>H , where\nJB\u22121KH , (\n1 \u2212\u00b5>B\u2212> \u2297 1> B\u22121 \u2297 I\n) , (137)\nis defined according to Eqn. 85. But observe that JB\u22121KH = L\u2212>, where L is the Cholesky factor of \u2126\u0303 (Eqn. 134). Therefore, we have\nW\u0304\u2020` = W\u0304` L. (138)\nWe apply Lemma 3 with P\u2020` = I and R \u2020 ` = I. This gives us the update in the original coordinate\nsystem:\nW\u0304 (k+1) ` \u2190 W\u0304 (k) ` \u2212 \u03b1(\u2207W\u0304`h) L \u2212>L\u22121 (139)\n= W\u0304 (k) ` \u2212 \u03b1(\u2207W\u0304`h) \u2126\u0303\n\u22121 . (140)\nThis is equivalent to the approximate natural gradient update where the Fisher block is approximated as \u2126\u0303 \u2297 I. This is the same approximate Fisher block we derived given the assumptions of the theorem (Eqn. 129)."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural Computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "High performance convolutional neural networks for document", "author": ["S. Puri", "P. Simard"], "venue": "Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "K. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "K. et al\\.", "year": 2011}, {"title": "Enhanced gradient for training restricted Boltzmann machines", "author": ["T. Raiko", "A. Ilin"], "venue": "Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "K. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "K. et al\\.", "year": 2006}, {"title": "Applied Numerical Linear Algebra", "author": ["J.W. Demmel"], "venue": null, "citeRegEx": "Demmel,? \\Q1997\\E", "shortCiteRegEx": "Demmel", "year": 1997}, {"title": "Adaptive subgradient methods for online learning and stochastic", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2015}, {"title": "Scaling up natural gradient by sparsely factorizing the inverse", "author": ["Roger", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Roger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Roger et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "S. and Schmidhuber,? \\Q2000\\E", "shortCiteRegEx": "S. and Schmidhuber", "year": 2000}, {"title": "ImageNet classification with deep convolutional neural", "author": ["Toronto", "A. 2009. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Toronto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Toronto et al\\.", "year": 2009}, {"title": "Topmoumoute online natural gradient", "author": ["Le Roux", "Nicolas", "Manzagol", "Pierre-antoine", "Bengio", "Yoshua"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference", "citeRegEx": "Martens,? \\Q1998\\E", "shortCiteRegEx": "Martens", "year": 1998}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["R. Grosse"], "venue": "Machine Learning (ICML),", "citeRegEx": "J. and Grosse,? \\Q2010\\E", "shortCiteRegEx": "J. and Grosse", "year": 2010}, {"title": "Riemannian metrics for neural networks I: feedforward networks", "author": ["Y. Ollivier"], "venue": "Information and Inference,", "citeRegEx": "Ollivier,? \\Q2015\\E", "shortCiteRegEx": "Ollivier", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal of Control and Optimization,", "citeRegEx": "Polyak and Juditsky,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Parallel training of DNNs with natural gradient and parameter averaging", "author": ["Povey", "Daniel", "Zhang", "Xiaohui", "Khudanpur", "Sanjeev"], "venue": "In International Conference on Learning Representations: Workshop track,", "citeRegEx": "Povey et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2015}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M. Ranzato", "G.E. Hinton"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Ranzato and Hinton", "year": 2010}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Schraudolph", "Nicol N"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph and N.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph and N.", "year": 2002}, {"title": "Natural image statistics and neural representation", "author": ["E.P. Simoncelli", "B.A. Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Simoncelli and Olshausen,? \\Q2001\\E", "shortCiteRegEx": "Simoncelli and Olshausen", "year": 2001}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Srivastava,? \\Q2013\\E", "shortCiteRegEx": "Srivastava", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A tutorial on stochastic approximation algorithms for training restricted Boltzmann machines and deep belief nets", "author": ["K. Swersky", "Chen", "Bo", "B. Marlin", "N. de Freitas"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Lecture 6.5, RMSProp", "author": ["T. Tieleman", "G. Hinton"], "venue": "In Coursera course Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities", "author": ["Vatanen", "Tommi", "Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"], "venue": null, "citeRegEx": "Vatanen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["O. Vinyals", "D. Povey"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Vinyals and Povey,? \\Q2012\\E", "shortCiteRegEx": "Vinyals and Povey", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": null, "citeRegEx": "Zeiler and D.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2013}, {"title": "particular, each time we adapt \u03b3, we compute \u2207\u0302h for three different values \u03b3\u2212 < \u03b3 < \u03b3+. We choose whichever of the three values results in the lowest value of M(\u03b8 + v)", "author": ["Martens", "Grosse"], "venue": null, "citeRegEx": "Martens and Grosse,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "(See Martens (2014) for a review.", "startOffset": 5, "endOffset": 20}, {"referenceID": 22, "context": ", 2012) and recurrent neural networks (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014).", "startOffset": 38, "endOffset": 94}, {"referenceID": 20, "context": "It could also potentially be used as a pre-conditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014).", "startOffset": 90, "endOffset": 157}, {"referenceID": 17, "context": ") For each case, \u2207\u03b8h is usually computed using automatic-differentiation aka backpropagation (Rumelhart et al., 1986; LeCun et al., 1998), which can be thought of as comprising two steps: first computing the pre-activation derivatives \u2207s`h for each layer, and then computing \u2207W`h = (\u2207s`h)\u0101`\u22121.", "startOffset": 93, "endOffset": 137}, {"referenceID": 8, "context": "Martens, 2010; Vinyals & Povey, 2012). F is an n \u00d7 n matrix, where n is the number of parameters and can be in the tens of millions for modern deep architectures. Therefore, it is impractical to represent F explicitly in memory, let alone solve the linear system exactly. There are two general strategies one can take to find a good search direction. First, one can impose a structure on F enabling tractable inversion; for instance LeCun et al. (1998) approximates it as a diagonal matrix, TONGA (Le Roux et al.", "startOffset": 0, "endOffset": 453}, {"referenceID": 8, "context": "(1998) approximates it as a diagonal matrix, TONGA (Le Roux et al., 2008) uses a more flexible low-rank-within-block-diagonal structure, and factorized natural gradient (Grosse & Salakhutdinov, 2015) imposes a directed Gaussian graphical model structure. Another strategy is to approximately minimize the quadratic approximation to the objective using an iterative procedure such as conjugate gradient; this is the approach taken in Hessianfree optimization (Martens, 2010), a type of truncated Newton method (e.g. Nocedal & Wright, 2006). Conjugate gradient (CG) is defined in terms of matrix-vector products Fv, which can be computed efficiently and exactly using the method outlined by Schraudolph (2002). While iterative approaches can produce high quality search directions, they can be very expensive in practice, as each update may require tens or even hundreds of CG iterations to reach an acceptable quality, and each of these iterations is comparable in cost to an SGD update.", "startOffset": 55, "endOffset": 708}, {"referenceID": 9, "context": "Martens & Grosse (2015) pointed out that F\u0302` + (\u03bb+ \u03b3)I \u2248 ( \u03a8`\u22121 + \u03c0` \u221a \u03bb+ \u03b3 I ) \u2297 ( \u0393` + 1 \u03c0` \u221a \u03bb+ \u03b3 I ) .", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Mathematically, \u03c0` can be any positive scalar, but Martens & Grosse (2015) suggest the formula", "startOffset": 51, "endOffset": 75}, {"referenceID": 9, "context": "The algorithm as presented by Martens & Grosse (2015) has many additional elements which are orthogonal to the contributions of this paper.", "startOffset": 30, "endOffset": 54}, {"referenceID": 21, "context": "ConvNet (TDLCN) package (Srivastava, 2015), whose convolution kernels we use in our experiments. Like many modern implementations, this implementation follows the approach of Chellapilla et al. (2006), which reduces the convolution operations to large matrix-vector products in order to exploit memory locality and efficient parallel BLAS operators.", "startOffset": 25, "endOffset": 201}, {"referenceID": 9, "context": "1 Estimating the factors Since the true covariance statistics are unknown, we estimate them empirically by sampling from the model\u2019s distribution, similarly to Martens & Grosse (2015). To sample derivatives from the model\u2019s distribution, we select a mini-batch, sample the outputs from the model\u2019s predictive distribution, and backpropagate the derivatives.", "startOffset": 160, "endOffset": 184}, {"referenceID": 20, "context": "Alternatively, we could use it in the context of the full K-FAC algorithm, or as a preconditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014).", "startOffset": 133, "endOffset": 200}, {"referenceID": 9, "context": "Alternatively, we could use it in the context of the full K-FAC algorithm, or as a preconditioner for iterative second-order methods (Martens, 2010; Vinyals & Povey, 2012; Sohl-Dickstein et al., 2014). In our experiments, we explored two particular instantiations of KFC in optimization algorithms. First, in order to provide as direct a comparison as possible to standard SGD-based optimization, we used \u2207\u0302h in the context of a generic approximate natural gradient descent procedure; this procedure is like SGD, except that \u2207\u0302h is substituted for the Euclidean gradient. Additionally, we used momentum, update clipping, and parameter averaging \u2014 all standard techniques in the context of stochastic optimization. One can also view this as a preconditioned SGD method, where F\u0302 is used as the preconditioner. Therefore, we refer to this method in our experiments as KFC-pre (to distinguish it from the KFC approximation itself). This method is spelled out in detail in Appendix A.1. We also explored the use of F\u0302 in the context of K-FAC, which (in addition to the techniques of Section 2.2), includes methods for adaptively changing the learning rate, momentum, and damping parameters over the course of optimization. The full algorithm is given in Appendix A.2. Our aim was to measure how KFC can perform in the context of a sophisticated and well-tuned second-order optimization procedure. We found that the adaptation methods tended to choose stable values for the learning rate, momentum, and damping parameters, suggesting that these could be replaced with fixed values (as in KFC-pre). Since both methods performed similarly, we report results only for KFC-pre. We note that this finding stands in contrast with the autoencoder experiments of Martens & Grosse (2015), where the adapted parameters varied considerably over the course of optimization.", "startOffset": 134, "endOffset": 1774}, {"referenceID": 10, "context": "Ollivier (2015) showed that one such approximation was invariant to (invertible) affine transformations of individual activations.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Martens & Grosse (2015) showed that K-FAC is invariant to a broader class of reparameterizations: affine 2Our SGD baseline used momentum and parameter averaging as well.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Invariance to affine transformations also implies approximate invariance to smooth nonlinear transformations; see Martens (2014) for further discussion.", "startOffset": 114, "endOffset": 129}, {"referenceID": 25, "context": "Another class of approaches attempts to reparameterize a network such that its activations have zero mean and unit variance, with the goals of preventing covariate shift and improving the conditioning of the curvature (Cho et al., 2013; Vatanen et al., 2013; Ioffe & Szegedy, 2015).", "startOffset": 218, "endOffset": 281}, {"referenceID": 21, "context": ") For SVHN, we used the architecture of Srivastava (2013). This architecture consists of three convolutional layers followed by three fully connected layers, and uses dropout for regularization.", "startOffset": 40, "endOffset": 58}, {"referenceID": 9, "context": "As discussed above, IAD is the same approximation made by standard K-FAC, and it was investigated in detail both theoretically and empirically by Martens & Grosse (2015). One implicitly 4https://code.", "startOffset": 146, "endOffset": 170}, {"referenceID": 4, "context": "As baselines, we also tried Adagrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2015), but none of these approaches outperformed carefully tuned SGD with momentum. This is consistent with the observations of Kingma & Ba (2015). Figure 3(a,b) shows the optimization performance on the CIFAR-10 dataset, in terms of wall clock time.", "startOffset": 37, "endOffset": 263}], "year": 2017, "abstractText": "Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.", "creator": "LaTeX with hyperref package"}}}