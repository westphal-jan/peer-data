{"id": "1602.02282", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Ladder Variational Autoencoders", "abstract": "variational autoencoders are a powerful framework for unsupervised learning. however, previous work has been restricted rather to shallow models with one or two layers of inherently fully generic factorized stochastic controlled latent variables, limiting the flexibility of identifying the latent representation. we then propose three advances in robust training algorithms of variational autoencoders, for the first time allowing to train deep models ensembles of up to five stochastic temporal layers, ( about 1 ) using a structure similar to the ladder network as termed the inference model, ( 2 ) warm - up period to support stochastic units staying stationary active regardless in simple early training, alignment and ( 3 ) use of batch normalization. using these improvements we show state - of - the - art log - likelihood results for generative modeling on several benchmark datasets.", "histories": [["v1", "Sat, 6 Feb 2016 17:32:48 GMT  (4324kb,D)", "http://arxiv.org/abs/1602.02282v1", null], ["v2", "Tue, 24 May 2016 10:41:45 GMT  (5047kb,D)", "http://arxiv.org/abs/1602.02282v2", null], ["v3", "Fri, 27 May 2016 09:05:10 GMT  (5047kb,D)", "http://arxiv.org/abs/1602.02282v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["casper kaae s\u00f8nderby", "tapani raiko", "lars maal\u00f8e", "s\u00f8ren kaae s\u00f8nderby", "ole winther"], "accepted": true, "id": "1602.02282"}, "pdf": {"name": "1602.02282.pdf", "metadata": {"source": "META", "title": "How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks", "authors": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "emails": ["CASPERKAAE@GMAIL.COM", "TAPANI.RAIKO@AALTO.FI", "LARSMA@DTU.DK", "SKAAESONDERBY@GMAIL.COM", "OLWI@DTU.DK"], "sections": [{"heading": "1. Introduction", "text": "The recently introduced variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) provides a framework for deep generative models (DGM). DGMs have later been shown to be a powerful framework for semi-supervised learning (Kingma et al., 2014; Maaloee et al., 2016). Another line of research starting from denoising autoencoders introduced the Ladder network for unsupervised learning (Valpola, 2014) which have also been shown to perform very well in the semi-supervised setting (Rasmus et al., 2015).\nHere we study DGMs with several layers of latent vari-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nX\n!\"\nz\nz\nX\n!\"\nd !\"\nz\nd !\"\nz\nz\nz\n!\"\nX ~\na) b) c)\n1\n2\n1\n11\n2 2\n2 2\n2 2\n1 1\n1 1\n1 1\n2\nFigure 1. Inference (or encoder/recognition) and generative (or decoder) models. a) VAE inference model, b) Probabilistic ladder inference model and c) generative model. The z\u2019s are latent variables sampled from the approximate posterior distribution q with mean and variances parameterized using neural networks.\nables, each conditioned on the layer above, which allows highly flexible latent distributions. We study two different model parameterizations: the first is a simple extension of the VAE to multiple layers of latent variables and the second is parameterized in such a way that it can be regarded as a probabilistic variational variant of the Ladder network which, contrary to the VAE, allows interactions between a bottom up and top-down inference signal.\nPrevious work on DGMs have been restricted to shallow models with one or two layers of stochastic latent variables constraining the performance by the restrictive mean field approximation of the intractable posterior distribution. Using only gradient descent optimization we show that DGMs are only able to utilize the stochastic latent variables in the second layer to a limited degree and not at all in the third\nar X\niv :1\n60 2.\n02 28\n2v 1\n[ st\nat .M\nL ]\n6 F\nlayer or above. To alleviate these problems we propose (1) the probabilistic ladder network, (2) a warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization, for optimizing DGMs and show that the likelihood can increase for up to five layers of stochastic latent variables. These models, consisting of deep hierarchies of latent variables, are both highly expressive while maintaining the computational efficiency of fully factorized models. We first show that these models have competitive generative performance, measured in terms of test log likelihood, when compared to equally or more complicated methods for creating flexible variational distributions such as the Variational Gaussian Processes (Tran et al., 2015) Normalizing Flows (Rezende & Mohamed, 2015) or Importance Weighted Autoencoders (Burda et al., 2015). We find that batch normalization and warm-up always increase the generative performance, suggesting that these methods are broadly useful. We also show that the probabilistic ladder network performs as good or better than strong VAEs making it an interesting model for further studies. Secondly, we study the learned latent representations. We find that the methods proposed here are necessary for learning rich latent representations utilizing several layers of latent variables. A qualitative assessment of the latent representations further indicates that the multilayered DGMs capture high level structure in the datasets which is likely to be useful for semi-supervised learning.\nIn summary our contributions are:\n\u2022 A new parametrization of the VAE inspired by the Ladder network performing as well or better than the current best models.\n\u2022 A novel warm-up period in training increasing both the generative performance across several different datasets and the number of active stochastic latent variables.\n\u2022 We show that batch normalization is essential for training VAEs with several layers of stochastic latent variables."}, {"heading": "2. Methods", "text": "Variational autoencoders simultaneously train a generative model p\u03b8(x, z) = p\u03b8(x|z)p\u03b8(z) for data x using auxiliary latent variables z, and an inference model q\u03c6(z|x)1 by optimizing a variational lower bound to the likelihood p\u03b8(x) = \u222b p\u03b8(x, z)dz.\n1The inference models is also known as the recognition model or the encoder, and the generative model as the decoder.\nzd\nz\nz\na) b)\nn\nn\nnn\n\"Likelihood\"\nDeterministic\nbottom up pathway\nStochastic top down pathway\n\"Posterior\"\n\"Prior\"\n+ \"Copy\"\nTop down pathway\nthrough KL-divergences\nin generative model\nBottom up pathway in inference model\nIndirect top\ndown\ninformation through prior\nDirect flow of information z n\nGenerative\nmodel\n\"Copy\"\nFigure 2. Flow of information in the inference and generative models of a) probabilistic ladder network and b) VAE. The probabilistic ladder network allows direct integration (+ in figure, see Eq. (21) ) of bottom-up and top-down information in the inference model. In the VAE the top-down information is incorporated indirectly through the conditional priors in the generative model.\nThe generative model p\u03b8 is specified as follows:\np\u03b8(x|z1) = N ( x|\u00b5\u03b8(z1), \u03c32\u03b8(z1) ) or (1) P\u03b8(x|z1) = B (x|\u00b5\u03b8(z1)) (2)\nfor continuous-valued (Gaussian N ) or binary-valued (Bernoulli B) data, respectively. The latent variables z are split into L layers zi, i = 1 . . . L:\np\u03b8(zi|zi+1) = N ( zi|\u00b5\u03b8,i(zi+1), \u03c32\u03b8,i(zi+1) ) (3)\np\u03b8(zL) = N (zL|0, I) . (4)\nThe hierarchical specification allows the lower layers of the latent variables to be highly correlated but still maintain the computational efficiency of fully factorized models.\nEach layer in the inference model q\u03c6(z|x) is specified using a fully factorized Gaussian distribution:\nq\u03c6(z1|x) = N ( z1|\u00b5\u03c6,1(x), \u03c62\u03c6,1(x) ) (5)\nq\u03c6(zi|zi\u22121) = N ( zi|\u00b5\u03c6,i(zi\u22121), \u03c32\u03c6,i(zi\u22121) ) (6)\nfor i = 2 . . . L.\nFunctions \u00b5(\u00b7) and \u03c32(\u00b7) in both the generative and the inference models are implemented as:\nd(y) =MLP(y) (7) \u00b5(y) =Linear(d(y)) (8)\n\u03c32(y) =Softplus(Linear(d(y))) , (9)\nwhere MLP is a two layered multilayer perceptron network, Linear is a single linear layer, and Softplus applies log(1 + exp(\u00b7)) non linearity to each component of its argument vector. In our notation, each MLP(\u00b7) or Linear(\u00b7) gives a new mapping with its own parameters, so the deterministic variable d is used to mark that the MLP-part is shared between \u00b5 and \u03c32 whereas the last Linear layer is not shared.\nThe variational principle provides a tractable lower bound on the log likelihood which can be used as a training criterion L.\nlog p(x) \u2265 Eq\u03c6(z|x) [ log p\u03b8(x, z)\nq\u03c6(z|x)\n] = \u2212L(\u03b8, \u03c6;x) (10)\n= \u2212KL(q\u03c6(z|x)||p\u03b8(z)) + Eq\u03c6(z|x) [p\u03b8(x|z)] , (11)\nwhere KL is the Kullback-Leibler divergence.\nA strictly tighter bound on the likelihood may be obtained at the expense of a K-fold increase of samples by using the importance weighted bound (Burda et al., 2015):\nlog p(x) \u2265 Eq\u03c6(z(1)|x) . . . Eq\u03c6(z(K)|x)\n[ log K\u2211\nk=1\np\u03b8(x, z (k)) q\u03c6(z(k)|x)\n]\n\u2265 \u2212L(\u03b8, \u03c6;x) . (12)\nThe inference and generative parameters, \u03b8 and \u03c6, are jointly trained by optimizing Eq. (11) using stochastic gradient descent where we use the reparametrization trick for stochastic backpropagation through the Gaussian latent variables (Kingma & Welling, 2013; Rezende et al., 2014). All expectations are approximated using Monte Carlo sampling by drawing from the corresponding q distribution.\nPrevious work has been restricted to shallow models (L \u2264 2). We propose two improvements to VAE training and a new variational model structure allowing us to train deep models with up to at least L = 5 stochastic layers."}, {"heading": "2.1. Probabilistic Ladder Network", "text": "We propose a new inference model where we use the structure of the Ladder network (Valpola, 2014; Rasmus et al.,\n2015) as the inference model of a VAE, as shown in Figure 1. The generative model is the same as before.\nThe inference is constructed to first make a deterministic upward pass:\nd1 =MLP(x) (13) \u00b5d,i =Linear(di), i = 1 . . . L (14)\n\u03c32d,i =Softplus(Linear(di)), i = 1 . . . L (15)\ndi =MLP(\u00b5d,i\u22121), i = 2 . . . L (16)\nfollowed by a stochastic downward pass:\nq\u03c6(zL|x) =N ( \u00b5d,L, \u03c3 2 d,L ) (17)\nti =MLP(zi+1), i = 1 . . . L\u2212 1 (18) \u00b5t,i =Linear(ti) (19)\n\u03c32t,i =Softplus(Linear(ti)) (20)\nq\u03b8(zi|zi+1,x) =N ( \u00b5t,i\u03c3 \u22122 t,i + \u00b5d,i\u03c3 \u22122 d,i\n\u03c3\u22122t,i + \u03c3 \u22122 d,i\n, 1\n\u03c3\u22122t,i + \u03c3 \u22122 d,i\n) .\n(21)\nHere \u00b5d and \u03c32d carry the bottom-up information and \u03c3 2 t and \u00b5t carry top-down information. This parametrization has a probabilistic motivation by viewing \u00b5d and \u03c32d as the Gaussian likelihood that is combined with a Gaussian prior \u00b5t and \u03c32t from the top-down connections, together forming the approximate posterior distribution q\u03b8(zi|zi+1,x), see Figure 2 a).\nA line of motivation, already noted by Dayan et al. (1995), is that a purely bottom-up inference process as in i.e. VAEs\ndoes not correspond well with real perception, where iterative interaction between bottom-up and top-down signals produces the final activity of a unit2, see Figure 2. Notably it is difficult for the purely bottom-up inference networks to model the explaining away phenomenon, see van den Broeke (2016, Chapter 5) for a recent discussion on this phenomenon. The probabilistic ladder network provides a framework with the wanted interaction, while keeping complications manageable. A further extension could be to make the inference in k steps over an iterative inference procedure (Raiko et al., 2014)."}, {"heading": "2.2. Warm-up from deterministic to variational autoencoder", "text": "The variational training criterion in Eq. (11) contains the reconstruction term p\u03b8(x|z) and the variational regularization term. The variational regularization term causes some of the latent units to become inactive during training (MacKay, 2001) because the approximate posterior for unit k, q(zi,k| . . . ) is regularized towards its own prior p(zi,k| . . . ), a phenomenon also recognized in the VAE setting (Burda et al., 2015). This can be seen as a virtue of automatic relevance determination, but also as a problem when many units are pruned away early in training before they learned a useful representation. We observed that such units remain inactive for the rest of the training, presumably trapped in a local minima or saddle point at KL(qi,k|pi,k) \u2248 0, with the optimization algorithm unable to re-activate them.\nWe propose to alleviate the problem by initializing training using the reconstruction error only (corresponding to training a standard deterministic auto-encoder), and then gradually introducing the variational regularization term:\n\u2212L(\u03b8, \u03c6;x)T = (22) \u2212\u03b2KL(q\u03c6(z|x)||p\u03b8(z)) + Eq\u03c6(z|x) [p\u03b8(x|z)] ,\nwhere \u03b2 is increased linearly from 0 to 1 during the firstNt epochs of training. We denote this scheme warm-up (abbreviated WU in tables and graphs) because the objective goes from having a delta-function solution (corresponding to zero temperature) and then move towards the fully stochastic variational objective. A similar idea has previously been considered in Raiko et al. (2007, Section 6.2), however here used for Bayesian models trained with a coordinate descent algorithm."}, {"heading": "2.3. Batch Normalization", "text": "Batch normalization (Ioffe & Szegedy, 2015) is a recent innovation that improves convergence speed and stabilizes\n2The idea was dismissed at the time, since it could introduce substantial theoretical complications.\ntraining in deep neural networks by normalizing the outputs from each layer. We show that batch normalization (abbreviated BN in tables and graphs), applied to all layers except the output layers, is essential for learning deep hierarchies of latent variables for L > 2."}, {"heading": "3. Experiments", "text": "To test our models we use the standard benchmark datasets MNIST, OMNIGLOT (Lake et al., 2013) and NORB (LeCun et al., 2004). The largest models trained used a hierarchy of five layers of stochastic latent variables of sizes 64, 32, 16, 8 and 4, going from bottom to top. We implemented all mappings using two-layered MLP\u2019s. In all models the MLP\u2019s between x and z1 or d1 were of size 512. Subsequent layers were connected by MLP\u2019s of sizes 256, 128, 64 and 32 for all connections in both the VAE and probabilistic ladder network. Shallower models were created by removing latent variables from the top of the hierarchy. We sometimes refer to the five layer models as 64-32-16-8-4, the four layer models as 64-32-16-8 and so fourth. The models were trained end-to-end using the Adam (Kingma & Ba, 2014) optimizer with a mini-batch size of 256. The reported test log-likelihoods were approximated using Eq. (12) with 5000 importance weighted samples as in Burda et al. (2015). The models were implemented in the Theano (Bastien et al., 2012), Lasagne (Dieleman et al., 2015) and Parmesan3 frameworks.\nFor MNIST we used a sigmoid output layer to predict the mean of a Bernoulli observation model and leaky rectifiers (max(x, 0.1x)) as nonlinearities in the MLP\u2019s. The models were trained for 2000 epochs with a learning rate of 0.001 on the complete training set. Models using warm-up used Nt = 200. Similarly to Burda et al. (2015) we resample the binarized training values from the real-valued images using a Bernoulli distribution after each epoch which prevents the models from over-fitting. Some of the models were fine-tuned by continuing training for 2000 epochs while multiplying the learning rate with 0.75 after every 200 epochs and increase the number of Monte Carlo and\n3github.com/casperkaae/parmesan\nimportance weighted samples to 10 to reduce the variance in the approximation of the expectations in Eq. (10) and improve the inference model, respectively.\nModels trained on the OMNIGLOT dataset4, consisting of 28x28 binary images images were trained similar to above except that the number of training epochs was 1500.\nModels trained on the NORB dataset5, consisting of 32x32 grays-scale images with color-coding rescaled to [0, 1], used a Gaussian observation model with mean and variance predicted using a linear and a softplus output layer respectively. The settings were similar to the models above except that: hyperbolic tangent was used as nonlinearities in the MLP\u2019s, the learning rate was 0.002, Nt = 1000 and the number of training epochs were 4000."}, {"heading": "4. Results & Discussion", "text": "To asses the performance of the VAE and the probabilistic ladder networks with different numbers of stochastic latent variable layers and with or without batch normalization and warm-up we use the MNIST, OMNIGLOT and NORB datasets. Firstly we study the generative model loglikelihood for the different datasets and show that models using our contributions perform better than the vanilla VAE models6. Secondly we study the latent space representation and how batch normalization, warm-up and the probabilistic ladder network affect these compared to the vanilla VAE.\nGenerative log-likelihood performance\nIn Figure 3 we show the test set log-likelihood for a series of different models with varying number of stochas-\n4The OMNIGLOT data was partitioned and preprocessed as in Burda et al. (2015), https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT\n5The NORB dataset was downloaded in resized format from github.com/gwtaylor/convnet matlab\n6we use vanilla to refer to VAE models with out batch normalization and warm-up\ntic layers. The performance of the vanilla VAE model did not improve with more than two layers of stochastic latent variables. Contrary to this, models trained with batch normalization and warm-up consistently increase the model performance for additional layers of stochastic latent variables. As expected the improvement in performance is decreasing for each additional layer, but we emphasize that the improvements are consistent even for the addition of the top-most layers. The best performance is achieved using the probabilistic ladder network slightly outperforming the best VAE models trained with batch normalization and temperature. We emphasize that these VAE models already have a very strong performance showing that the probabilistic ladder network have competitive generative performance.\nThe models in Figure 3 were trained using fixed learning rate and one Monte Carlo (MC) and one importance weighted (IW) sample. To improve performance we finetuned the best performing five layer models by training these for a further 2000 epochs with annealed learning rate and increasing the number of MC and IW samples. In Table 1 we see that that by annealing the learning rate, drawing 10 MC and IW samples we can improve the loglikelihood of the VAE and probabilistic ladder network to \u221281.30 and \u221281.20 respectively. Comparing the results obtained here with current state-ofthe art results on permutation invariant MNIST, Burda et al. (2015) report a test-set performance of\u221282.90 using 50 IW samples and a two layer model with 100-50 latent units. Using a similar model Tran et al. (2015) achieves \u221281.90 by adding a variational Gaussian Process prior for each layer of latent variables, however here we note that our results are not directly comparable to these due to differences\nin the training procedure.\nAs shown above we found batch normalization (Ioffe & Szegedy, 2015) to be essential for achieving good performance for DGMs with L > 2. Figure 4 shows the MNIST training and test convergence for models with five layers of stochastic units. Comparing the VAE model with and without batch normalization we see a large increase in both trained and test performance. Adding warm-up does not increase the training performance however the models generalize better indicating a more robust latent representation. Lastly the probabilistic ladder network converges to slightly better values on both the test and training sets. We saw no signs of over-fitting for any of our models even though the hierarchical latent representations are highly expressive.\nTo test the models on more challenging data we used the OMNIGLOT dataset, consisting of characters from 50 different alphabets with 20 samples of each character. The Log-likelihood values, Table 3, shows similar trends as for MNIST with substantial increases in performance by adding batch-normalization and warm-up and again with the probabilistic ladder network performing slightly better than the other models. For both VAEs and probabilistic ladder networks the performance increases up to three layers of stochastic latent variables after which no gain is seen by adding more layers. The best Log-likelihood results obtained here, \u2212101.26, is higher than the best results from Burda et al. (2015) at \u2212103.38, which were obtained using more latent variables (100-50 vs 64-32-16) and 50 importance weighted samples for training.\nWe tested the models using a continuous Gaussian observation model on the NORB dataset consisting of gray-scale images of 5 different toy objects under different illuminations and observation angles. As seen in Table 3 we again find that bath normalization and warm-up increases performance. However here we do not see a gain in performance for L > 2 layers of latent stochastic variables. We found the Gaussian observation models to be harder to train requiring lower learning rates and hyperbolic tangent instead of leaky rectifiers as nonlinearities for stable training. The probabilistic ladder network were sometimes unstable during training explaining the high variance in the results. The harder optimization of the Gaussian observation model, also recognized in Oord et al. (2016), might explain the lower utilization of the topmost latent layers in these models.\nThe parametrization of the probabilistic ladder network allows for natural parameter sharing between top-down part \u00b5t,i(zi+1) and \u03c32t,i(zi+1) of the inference model q\u03c6, and the top-down mapping \u00b5\u03b8,i(zi+1) and \u03c32\u03b8,i(zi+1) of the generative model p\u03b8 in Equation (3). We did initial experiments with this setup but did not see improvements in performance. However we believe that parameter sharing between the inference and generative model is an interesting future research direction.\nAltogether for permutation invariant MNIST and OMNIGLOT we show state-of-the-art generative performance achieved using a deep hierarchy of latent variables consisting of fewer units (124 compared to 150) than current best methods. We show similar findings on the NORB dataset\nusing the more challenging continuous Gaussian observation model.\nLatent representations\nThe probabilistic generative models studied here automatically tune the model complexity to the data by reducing the effective dimension of the latent representation due to the regularization effect of the priors in Eq. (10). However, as previously identified (Raiko et al., 2007; Burda et al., 2015), the latent representation is often overly sparse with few stochastic latent variables propagating useful information.\nTo study this effect we calculated the KL-divergence between q(zi,k|zi\u22121,k) and p(zi|zi+1) for each stochastic latent variable k during training as seen in Figure 5. This term is zero if the inference model is independent of the data, i.e. q(zi,k|zi\u22121,k) = q(zi,k), and hence collapsed\nonto the prior carrying no information about the data. For the models without warm-up we find that the KLdivergence for each unit is stable during all training epochs with only very few new units activated during training. For the models trained with warm-up we initially see many active units which are then gradually pruned away as the variational regularization term is introduced. At the end of training warm-up results in more active units indicating a more distributed representation. This effect is quantified in Table 2 where we defined a stochastic latent unit zi,k to be active if KL(qi,k||pi,k) > 0.01. We find that batch normalization is essential for activating latent units above the second layer but find that it does not affect the number of active units in the first layer. Warm-up causes a large increase in the number of active stochastic latent units, especially in the lower layers. We can explain this observation from the structure of the VAE. For \u03b2 = 0, i.e. fully deterministic training, the stochastic latent layers above the first layer are effectively disconnected from the model since the variational regularization term is ignored. This causes all information to flow through the lowest stochastic layer z1 as illustrated in Figure 2 b). We further find that the probabilistic ladder network activates the most stochastic latent units. We speculate that this is due to the deterministic path\nfrom the input data to all latent distributions in the inference model.\nTo qualitatively study the latent representations, PCA plots of zi \u223c q(zi|zi\u22121) are seen in Figure 7. It is clear that the VAE model without batch normalization is completely collapsed on a standard normal prior above the second layer. The latent representations shows progressively more clustering according to class which is best seen in the topmost layer of the probabilistic ladder network. These findings indicate that hierarchical latent variable models produce structured high level latent representations that are likely useful for semi-supervised learning.\nThe hierarchical latent variable models used here allows highly flexible distributions of the lower layers conditioned on the layers above. We measure the divergence between these conditional distributions and the restrictive mean field approximation by calculating the KL-divergence between q(zi|zi\u22121) and a standard normal distribution for several models trained on MNIST, see Figure 6 a). As expected the lower layers have highly non (standard) Gaussian distributions when conditioned on the layers above. Interestingly the probabilistic ladder network seems to have more active intermediate layers than t,he VAE with batch normalization and warm-up. Again this might be explained by the deterministic upward pass easing flow of information to the intermediate and upper layers. We further note that the KL-divergence is approximately zero in the vanilla VAE model above the second layer confirming the inactivity of these layers. Figure 6 b) shows generative samples from the probabilistic ladder network created by injecting z\u2217 \u223c N (0, I) at different layers in the model. Sampling from the top-most layer, having a standard normal prior, produces nice looking samples whereas the samples get progressively worse when we inject z\u2217 in the lower layers supporting the finding above."}, {"heading": "5. Conclusion", "text": "We presented three methods for optimization of VAEs using deep hierarchies of latent stochastic variables. Using these models we demonstrated state-of-the-art generative performance for the MNIST and OMNIGLOT datasets and showed that a new parametrization, the probabilistic ladder network, performed as good or better than current models. Further we demonstrated that VAEs with up to five layer hierarchies of active stochastic units can be trained and that especially the new probabilistic ladder network were able to utilize many stochastic units. Future work includes extending these models to semi-supervised learning and studying more elaborate iterative inference schemes in the probabilistic ladder network."}, {"heading": "Acknowledgments", "text": "This research was supported by the Novo Nordisk Foundation, Danish Innovation Foundation and the NVIDIA Corporation with the donation of TITAN X and Tesla K40 GPUs."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "One-shot learning by inverting a compositional causal process", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan R", "Tenenbaum", "Josh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lake et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2013}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Improving semisupervised learning with auxiliary deep generative models", "author": ["Maaloee", "Lars P", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Maaloee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maaloee et al\\.", "year": 2016}, {"title": "Local minima, symmetry-breaking, and model pruning in variational free energy minimization", "author": ["MacKay", "David JC"], "venue": null, "citeRegEx": "MacKay and JC.,? \\Q2001\\E", "shortCiteRegEx": "MacKay and JC.", "year": 2001}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Building blocks for variational bayesian learning of latent variable models", "author": ["Raiko", "Tapani", "Valpola", "Harri", "Harva", "Markus", "Karhunen", "Juha"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raiko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2007}, {"title": "Iterative neural autoregressive distribution estimator nade-k", "author": ["Raiko", "Tapani", "Li", "Yao", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Semi-supervised learning with ladder networks", "author": ["Rasmus", "Antti", "Berglund", "Mathias", "Honkala", "Mikko", "Valpola", "Harri", "Raiko", "Tapani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational gaussian process", "author": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.06499,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "From neural pca to deep unsupervised learning", "author": ["Valpola", "Harri"], "venue": "arXiv preprint arXiv:1411.7783,", "citeRegEx": "Valpola and Harri.,? \\Q2014\\E", "shortCiteRegEx": "Valpola and Harri.", "year": 2014}, {"title": "What auto-encoders could learn from brains - generation as feedback in unsupervised deep learning and inference, 2016", "author": ["van den Broeke", "Gerben"], "venue": null, "citeRegEx": "Broeke and Gerben.,? \\Q2016\\E", "shortCiteRegEx": "Broeke and Gerben.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "The recently introduced variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) provides a framework for deep generative models (DGM).", "startOffset": 54, "endOffset": 100}, {"referenceID": 3, "context": "DGMs have later been shown to be a powerful framework for semi-supervised learning (Kingma et al., 2014; Maaloee et al., 2016).", "startOffset": 83, "endOffset": 126}, {"referenceID": 8, "context": "DGMs have later been shown to be a powerful framework for semi-supervised learning (Kingma et al., 2014; Maaloee et al., 2016).", "startOffset": 83, "endOffset": 126}, {"referenceID": 13, "context": "Another line of research starting from denoising autoencoders introduced the Ladder network for unsupervised learning (Valpola, 2014) which have also been shown to perform very well in the semi-supervised setting (Rasmus et al., 2015).", "startOffset": 213, "endOffset": 234}, {"referenceID": 16, "context": "We first show that these models have competitive generative performance, measured in terms of test log likelihood, when compared to equally or more complicated methods for creating flexible variational distributions such as the Variational Gaussian Processes (Tran et al., 2015) Normalizing Flows (Rezende & Mohamed, 2015) or Importance Weighted Autoencoders (Burda et al.", "startOffset": 259, "endOffset": 278}, {"referenceID": 1, "context": ", 2015) Normalizing Flows (Rezende & Mohamed, 2015) or Importance Weighted Autoencoders (Burda et al., 2015).", "startOffset": 88, "endOffset": 108}, {"referenceID": 1, "context": "A strictly tighter bound on the likelihood may be obtained at the expense of a K-fold increase of samples by using the importance weighted bound (Burda et al., 2015):", "startOffset": 145, "endOffset": 165}, {"referenceID": 15, "context": "(11) using stochastic gradient descent where we use the reparametrization trick for stochastic backpropagation through the Gaussian latent variables (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 149, "endOffset": 195}, {"referenceID": 12, "context": "A further extension could be to make the inference in k steps over an iterative inference procedure (Raiko et al., 2014).", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "), a phenomenon also recognized in the VAE setting (Burda et al., 2015).", "startOffset": 51, "endOffset": 71}, {"referenceID": 6, "context": "To test our models we use the standard benchmark datasets MNIST, OMNIGLOT (Lake et al., 2013) and NORB (LeCun et al.", "startOffset": 74, "endOffset": 93}, {"referenceID": 7, "context": ", 2013) and NORB (LeCun et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 0, "context": "The models were implemented in the Theano (Bastien et al., 2012), Lasagne (Dieleman et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 0, "context": "(12) with 5000 importance weighted samples as in Burda et al. (2015). The models were implemented in the Theano (Bastien et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "Similarly to Burda et al. (2015) we resample the binarized training values from the real-valued images using a Bernoulli distribution after each epoch which prevents the models from over-fitting.", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "The OMNIGLOT data was partitioned and preprocessed as in Burda et al. (2015), https://github.", "startOffset": 57, "endOffset": 77}, {"referenceID": 1, "context": "Comparing the results obtained here with current state-ofthe art results on permutation invariant MNIST, Burda et al. (2015) report a test-set performance of\u221282.", "startOffset": 105, "endOffset": 125}, {"referenceID": 1, "context": "Comparing the results obtained here with current state-ofthe art results on permutation invariant MNIST, Burda et al. (2015) report a test-set performance of\u221282.90 using 50 IW samples and a two layer model with 100-50 latent units. Using a similar model Tran et al. (2015) achieves \u221281.", "startOffset": 105, "endOffset": 273}, {"referenceID": 1, "context": "26, is higher than the best results from Burda et al. (2015) at \u2212103.", "startOffset": 41, "endOffset": 61}, {"referenceID": 1, "context": "26, is higher than the best results from Burda et al. (2015) at \u2212103.38, which were obtained using more latent variables (100-50 vs 64-32-16) and 50 importance weighted samples for training. We tested the models using a continuous Gaussian observation model on the NORB dataset consisting of gray-scale images of 5 different toy objects under different illuminations and observation angles. As seen in Table 3 we again find that bath normalization and warm-up increases performance. However here we do not see a gain in performance for L > 2 layers of latent stochastic variables. We found the Gaussian observation models to be harder to train requiring lower learning rates and hyperbolic tangent instead of leaky rectifiers as nonlinearities for stable training. The probabilistic ladder network were sometimes unstable during training explaining the high variance in the results. The harder optimization of the Gaussian observation model, also recognized in Oord et al. (2016), might explain the lower utilization of the topmost latent layers in these models.", "startOffset": 41, "endOffset": 980}, {"referenceID": 11, "context": "However, as previously identified (Raiko et al., 2007; Burda et al., 2015), the latent representation is often overly sparse with few stochastic latent variables propagating useful information.", "startOffset": 34, "endOffset": 74}, {"referenceID": 1, "context": "However, as previously identified (Raiko et al., 2007; Burda et al., 2015), the latent representation is often overly sparse with few stochastic latent variables propagating useful information.", "startOffset": 34, "endOffset": 74}], "year": 2016, "abstractText": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.", "creator": "LaTeX with hyperref package"}}}