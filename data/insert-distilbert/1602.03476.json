{"id": "1602.03476", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications", "abstract": "we conduct an axiomatic study of the problem of firstly estimating the strength of measuring a known causal relationship between a pair of variables. thereafter we propose that an estimate of causal gibbs strength should be based on the cumulative conditional distribution of the effect given exactly the cause ( and not on the driving distribution of the cause ), and study dependence measures on conditional distributions. shannon capacity, appropriately regularized, emerges as often a natural measure under these axioms. we examine the problem of calculating overall shannon capacity from the observed samples and collectively propose a novel fixed - $ k $ nearest neighbor estimator, and demonstrate its consistency. finally, we demonstrate an application to single - cell flow - cytometry, where the proposed estimators significantly theoretically reduce sample complexity.", "histories": [["v1", "Wed, 10 Feb 2016 18:27:04 GMT  (89kb,D)", "https://arxiv.org/abs/1602.03476v1", "41 pages, 3 figures"], ["v2", "Fri, 12 Feb 2016 19:32:09 GMT  (89kb,D)", "http://arxiv.org/abs/1602.03476v2", "41 pages, 3 figures, typos fixed and references added"], ["v3", "Thu, 2 Jun 2016 15:55:46 GMT  (295kb,D)", "http://arxiv.org/abs/1602.03476v3", "43 pages, 3 figures"]], "COMMENTS": "41 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["weihao gao", "sreeram kannan", "sewoong oh", "pramod viswanath"], "accepted": true, "id": "1602.03476"}, "pdf": {"name": "1602.03476.pdf", "metadata": {"source": "CRF", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications\u2217", "authors": ["Weihao Gao", "Sreeram Kannan", "Sewoong Oh", "Pramod Viswanath"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nThe axiomatic study of dependence measures on joint distributions between two random variables X and Y has a long history in statistics [Sha48, Re\u0301n59, Csi08]. In this paper, we study the relatively unexplored terrain of measures that depend only on the conditional distribution PY |X . We are motivated to study conditional dependence measures from a problem in causal strength estimation. Causal learning is a basic problem in many areas of scientific learning, where one wants to uncover the cause-effect relationship usually using interventions or sometimes directly from observational data [Pea09, RE15, MPJ+15].\nIn this paper, we are interested in an even simpler question: given a causal relationship, how does one measure the strength of the relationship. This problem arises in many contexts, for example, one may know causal genetic pathways but only a subset of these maybe active in a particular tissue or organ - therefore, deducing how much influence each causal link exerts becomes necessary.\nWe focus on a simple model: consider a pair of random variables (X,Y ) with known causal direction X \u2192 Y , and suppose that there are no confounders - we are interested in quantifying the causal influence X has on Y . We denote the causal influence quantity by C(X \u2192 Y ). There are two philosophically distinct ways to model the quantity: the first one is factual influence, i.e., how much influence does X exert on Y under the current probability of the cause X. The second possible way, which one can term as potential influence measures how much influence X can potentially exert on Y - without cognizance to the present distribution of the cause. For example, consider a (hypothetical) city which has very few smokers, but smoking inevitably leads to lung-cancer. In such a city, the factual influence of smoking on lung-cancer will be small but the potential influence is very high. Depending on the setting, one may prefer the former or the latter. In this paper, we are interested in the potential influence of a cause on its effect.\nWe want C(X \u2192 Y ) to be invariant to scaling and one-one transformations of the variables X,Y . This naturally suggests information theoretic metrics as plausible choices of C(X \u2192 Y ), starting with the mutual information I(X;Y ) = D(PXY ||PXPY ), at least in the case of factual influence. This measures the information through the channel from X \u2192 Y as given by the prior PX . Observe that this metric is \u2217Parts of this manuscript has appeared in the International Conference on Machine Learning (ICML), 2016. \u2020Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, email:wgao9@illinois.edu \u2021Department of Electrical Engineering, University of Washington, email:ksreeram@uw.edu \u00a7Department of Industrial and Enterprise Engineering, University of Illinois at Urbana-Champaign, email:swoh@illinois.edu \u00b6Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, email:pramodv@illinois.edu\nar X\niv :1\n60 2.\n03 47\n6v 3\n[ cs\n.I T\n] 2\nJ un\n2 01\n6\nsymmetric with respect to the directions X \u2192 Y and Y \u2192 X; this property is not always desirable. In fact, this measure is taken as a starting point to develop an axiomatic approach to studying causal strength on general graphs in [JBGW+13].\nIn a recent work [KSM+14], potential causal influence is posited as a relevant metric to spot \u201ctrends\u201d in gene pathways. In the particular application considered there, rare biological states of gene X in a given data may nevertheless correspond to important biological states (or become common under different biological conditions), and therefore it is important to have causal measures that are not sensitive to the cause distribution but only depend on the relationship between the cause and the effect. To quantify the potential influence of those rare X, the following approach is proposed. Replace the observed distribution PX by a uniform distribution UX and calculate the mutual information under the joint distribution UXPY |X . The resulting causal strength quantification is C(X \u2192 Y ) = D(UXPY |X ||PUPY ), where PY represents the distribution at the output of a channel PY |X with input given by UX . We call this quantification as Uniform Mutual Information (UMI) and pronounced \u201cyou-me\u201d. A key challenge is to compute this quantity from i.i.d. samples in a statistical efficient manner, especially when the channel output is continuous valued (and potentially in high dimensions). This is the first focus point of this paper.\nUMI is not invariant under bijective transformations (since a uniform distribution on X is different from a uniform distribution on X3) and is also sensitive to the estimated support size of X. Even more fundamentally, it is unclear why one would prefer the uniform prior to measure potential influence through the channel PY |X . Based on natural axioms of data processing and additivity, we motivate an alternative measure of causal strength: the largest amount of information that can be sent through the channel, namely the Shannon capacity. Formally C(X \u2192 Y ) = maxQX D(QXPY |X ||QXPY ), where PY represents the distribution at the output of a channel PY |X with input given by QX . We refer to such a quantification as Capacitated Mutual Information (CMI) and pronounced \u201csee-me\u201d. A key challenge is to compute this quantity from i.i.d. samples in a statistical efficient manner, especially when the channel output is continuous valued (and potentially in high dimensions). This is the second focus point of this paper. We make the following main contributions in this paper.\n\u2022 UMI Estimation: We construct a novel estimator to compute UMI from data sampled i.i.d. from a distribution PXY . The estimator brings together ideas from three disparate threads in statistical estimation theory: nearest-neighbor methods, a correlation boosting idea in the estimation of (standard) mutual information from samples [KSG04], and importance sampling. The estimator has only a single hyper parameter (the number of nearest-neighbors considered, set to 4 or 5 in practice), uses an offthe-shelf kernel density estimator of only PX , and has strong connections to the entropy estimator of [KL87]. Our main technical result is to show that the estimator is consistent (in probability) supposing that the Radon-Nikodym derivative dPUdPX is uniformly bounded over the support. In simulations, the estimator has very strong performance in terms of sample complexity (compared to a baseline of the partition-based estimator in [Mod89]).\n\u2022 CMI Estimation: We build upon the estimator derived for UMI and construct an optimization problem that mimics the optimization problem inherent in computing the capacity directly from the conditional probability distribution of the channel. Our main technical result is to show the consistency of this estimator, supposing that the Radon-Nikodym derivative\ndPQ dPX\nis uniformly bounded over the support, where PQ is the optimizing input to the channel. Simulation results show strong empirical performance, compared to a baseline of a partition-based method followed by discrete optimization.\n\u2022 Application to gene pathway influence: In [KSM+14], considered an important result in single-cell flow-cytometry data analysis, a causal strength metric (termed DREMI) is proposed for measuring the causal influence of a gene \u2013 this estimator is a specific way of implementing UMI along with a \u201cchannel amplification\u201d step, and DREMI was successfully used to spot gene-pathway trends. We show that our proposed CMI and UMI estimators also exhibit the same performance as DREMI when supplied with the full dataset, while at the same time, having significantly smaller sample complexity for the same performance.\n2 An Axiomatic Approach\nWe formally model an influence measure on conditional probability distributions, by postulating five natural axioms. Let X be drawn from an alphabet X , and Y from an alphabet Y. Let the probability distribution of Y given X be given as PY |X . Let P be a family of conditional distributions; usually we will consider the case when P is the set of all possible conditional distributions. Then the influence measure C(X \u2192 Y ) is a function of the conditional distribution to non-negative real numbers: C : P(Y|X )\u2192 R+, and we can write C(X \u2192 Y ) as C(PY |X). We postulate that the function C satisfies five axioms on P, and show that CMI satisfies all five axioms:\n0. Independence: The measure C(PY |X) = 0 if and only if PY=y|X=x depends only on y. 1. Data Processing: If X \u2192 Y \u2192 Z be a processing chain, i.e., PZ=z|X=x = \u2211 y\u2208Y PZ=z|Y=yPY=y|X=x,\nthen the natural data processing inequalities should hold: (a) C(PY |X) \u2265 C(PZ|X); and (b) C(PZ|Y ) \u2265 C(PZ|X).\n2. Additivity: For a parallel channel PY1,Y2|X1,X2 := PY1|X1PY2|X2 , we need\nC(PY1,Y2|X1,X2) = C(PY1|X1) + C(PY2|X2). (1)\n3. Monotonicity: A causal relationship is strong if many possible values of PY are achievable by varying the input probability distribution PX . Thus if we consider PY |X as a map from the probability simplex in X to the probability simplex in Y , the larger the range of this map, the stronger should be the causal strength.\n(a) C should only depend on the range of the map, Range(PY |X), the convex hull of the output distributions PY |X=x. (b) C should be a monotonic function of the range of the map. If PY |X and QY |X are such that, Range(PY |X) \u2286 Range(QY |X) then: C(PY |X) \u2264 C(QY |X).\n4. Maximum value: The maximum value over all possible conditional distributions for a particular output alphabet Y should be achieved exactly when the relationship is fully causal, i.e., each Y = y can be achieved by setting X = x for some x.\nWe begin our exploration of appropriate influence measures with the alphabets for X and Y being discrete. Let I(PXY ) := D(PXY ||PXPY ) denote the mutual information with respect to the joint distribution PXY . Since we are looking at potential influence measures, Shannon capacity, defined as the maximum over input probability distributions of the mutual information, is a natural choice:\nCMI(PY |X) := max PX I(PXPY |X). (2)\nOur first claim is the following:\nProposition 1. CMI satisfies all the axioms of causal influence.\nProof: The proof is fairly straightforward.\n\u2022 Clearly Axiom 0 holds, cf. Chapter 2 of [CT12].\n\u2022 Axiom 1: Suppose CMI(PZ|X) is achieved with P \u2217X . Consider the joint distribution P \u2217XPY |XPZ|Y . Utilizing the data-processing inequality for mutual information, we get\nCMI(PY |X) = max PX\nI(PXPY |X) \u2265 I(P \u2217XPY |X)\n\u2265 I(P \u2217XPZ|X) = CMI(PZ|X). (3)\nThus Axiom 1a is satisfied. Now consider Axiom 1b. With the same joint distribution, let P \u2217Y be the marginal of Y . Then we have,\nCMI(PZ|Y ) = max PY\nI(PY PZ|Y ) \u2265 I(P \u2217Y PZ|Y )\n\u2265 I(P \u2217XPZ|X) = CMI(PZ|X). (4)\n\u2022 Axiom 2: This is a standard result for Shannon capacity and we refer the interested reader to Chapter 7 of [CT12].\n\u2022 Axiom 3a: First we rewrite capacity equivalently as the information-centroid (see [CS04]):\nCMI(PY |X) := max PX min qY\nD(PY |X\u2016qY |PX)\n= min qY max PX\nD(PY |X\u2016qY |PX)\n= min qY max x\nD(PY |X=x\u2016qY ). (5)\nHere the conditional KL divergence D(X\u2016Y |Z) is defined in the usual way:\nD(X\u2016Y |Z) = \u2211 z PZ(z) \u2211 (x,y) PX|Z(x|z) log PX|Z(x|z) PY |Z(y|z) . (6)\nThis characterization allows us to make the observation that the capacity is a function only of the convex hull of the probability distributions PY |X=x. Given a conditional probability distribution PY |X , we augment the input alphabet to have one more input symbol x\u2032 such that PY |X=x\u2032 = \u2211 x \u03b1xPY |X=x is a convex combination of the other conditional distributions. We claim that the capacity of the new channel is unchanged: one direction is obvious, i.e., the new channel has capacity greater than or equal to the original channel, since adding a new symbol cannot decrease capacity. To show the other direction, we use (5) and observe that, due to the convexity of KL divergence in its arguments, we get,\nD(PY |X=x\u2032\u2016qY ) = D( \u2211 x \u03b1xPY |X=x\u2016qY )\n\u2264 \u2211 x \u03b1xD(PY |X=x\u2016qY ) \u2264 max x D(PY |X=x\u2016qY ).\nThus Shannon capacity is only a function of the convex hull of the range of the map PY |X , satisfying Axiom 3a. This function is monotonic directly from (5), thus satisfying Axiom 3b.\n\u2022 Axiom 4: For fixed output alphabet Y, it is clear that maxX ,PY |X CMI = log |Y|. Now suppose for some conditional distribution PY |X we have CMI(PY |X) = log |Y|. This implies that, with the optimizing input distribution, H(Y ) \u2212 H(Y |X) = log |Y|. This implies that H(Y ) = log |Y| and H(Y |X) = 0, thus Y is a deterministic function of the essential support of X and since H(Y ) = log |Y|, it implies that PY = UY , the uniform distribution and the deterministic function is onto.\nAxiomatic View of UMI : Now consider an alternative metric: Uniform Mutual Information (UMI) which is defined as the mutual information with uniform input distribution,\nUMI(PY |X) := I(UXPY |X), (7)\nwhere UX is the uniform distribution on X . This estimator is motivated by the recent work in [KSM+14]. We investigate how this estimator fares in terms of the proposed axioms.\n\u2022 UMI clearly satisfies Axiom 0. It also satisfies Axioms 1a. Data-processing inequality for mutual information on the joint distribution UXPY |XPZ|Y implies that I(UXPY |X) \u2265 I(UXPZ|X), which is the same as UMI(PY |X) \u2265 UMI(PZ|X). Thus I(UY PZ|Y ) \u2265 I(UXPZ|X).\n\u2022 UMI however does not satisfy Axiom 1b in general. However, if the transition matrices PY |X and PZ|Y are both doubly stochastic, then a straightforward calculation shows that UMI satisfies Axiom 1b too.\n\u2022 UMI satisfies Axiom 2 since the uniform distribution on X1, X2 naturally factors as UX1,X2 = UX1UX2 and we have UMI(PY1,Y2|X1,X2)\n= I(UX1,X2PY1,Y2|X1,X2) (8) = I(UX1UX2PY1|X1PY2|X2) (9) = UMI(PY1|X1) + UMI(PY2|X2). (10)\n\u2022 UMI does not satisfy Axiom 3a since multiple repeated values of PY |X=x does not alter the convex hull but alters the UMI value.\n\u2022 Interestingly, UMI does satisfy Axiom 4 for the same reason as CMI.\n2.1 Real-valued alphabets\nFor real-valued X, the Shannon mutual information is not finite without additional regularizations. This is also true of other measures of relation such as the Renyi correlation [Re\u0301n59], and in each case the measure is studied in the context of some form penalty term. Typically this is done via a cost constraint on the real-valued input parameters. In this context, one possibility is to consider the following norm-constrained optimization to ensure the causal effect is finite valued:\nCMI(PY |X , a) := max PX :E\u2016X\u201622\u2264a I(PXPY |X). (11)\nIn practice, a is chosen from the empirical moments of X from samples: a := 1N \u2211N i=1 \u2016Xi\u201622 for samples X1, . . . , XN . This regularization turns to be the so-called power constraint on the input, common in treatments of additive noise communication channels.\n3 Estimators\nAlthough the definition of UMI and CMI seamlessly applies to both discrete and continuous random variables, the estimation becomes relatively straightforward when both X and Y are discrete; the estimation of the conditional distribution PY |X and the computation of UMI and CMI can be separated in a straightforward manner. For this reason and also due to an application in genomic biology that we study, we focus on the more challenging regime Y is continuous. Due to certain subtleties in the estimation process, we provide separate estimators each customized for each case of discrete and continuous X , respectively.\n3.1 Uniform Mutual Information\nThe idea of applying UMI to infer the strength of conditional dependence was first proposed in [KSM+14]. Off-the-shelf 2-dimensional kernel density estimators (KDE) are used to first estimate the joint distribution PXY from given samples. Subsequently, the channel PY |X is computed directly from the joint distribution, and then UMI is computed via either numerical integration or sampling from UX and PY |X . This approach suffers from known drawbacks of KDE, such as sensitivity to the choice of the bandwidth and increased bias in higher dimensional X and Y . However, a more critical challenge in using KDE to estimate the joint distribution at all points (and not just at samples) is the overkill nature: we only need to compute a single functional (UMI) of the joint distribution, which could in principle be computed more efficiently directly from the samples. It is not at all clear how to directly estimate UMI.\nPerhaps surprisingly, we bring together ideas from three topics in statistical estimation to introduce novel estimators that are also provably convergent. Our estimator is based on (a) k-nearest neighbor estimators, e.g. [KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG+07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI. We explain each idea below.\nConsider a simpler task of computing the mutual information from samples; several approaches exist for this estimation: [Pan03, KSG04, WKV09, PPS10, SRHI10, PXS12, GSG14, GSG15, KKPW15]. Note that three applications of the entropy estimator, such as those from [BDGVdM97], gives an estimate of the mutual information, i.e. I\u0302(X;Y ) = H\u0302(X) + H\u0302(Y ) \u2212 H\u0302(X,Y ). Each entropy term can be computed using, for example, a KDE based approach, which suffers from the same challenges, as in UMI. Alternatively, to bypass estimating PXY at every point, the differential entropy estimation can be done via k nearest neighbor (kNN) methods (pioneering work in [KL87]). This KL entropy estimator provides the first step in designing the UMI estimator. However, taking the route of estimating the mutual information via estimating the three differential entropies (two marginals and one joint), it is entirely unclear how to estimate two of these quantities (differential entropy of Y and that of (U, Y )) directly from samples.\nPerhaps surprisingly, an innovative approach undertaken in [KSG04] to improve upon three applications of KL estimators provides a solution. The KSG estimator of [KSG04] is based on kNN distance \u03c1k,i defined as the distance to the k-th nearest neighbor from (Xi, Yi) in `\u221e distance, i.e. \u03c1k,i = max{\u2016Xjk \u2212Xi\u2016\u221e, \u2016Yjk \u2212 Yi\u2016\u221e} where (Xjk , Yjk) is the k-th nearest neighbor to (Xi, Yi). Precisely, the KSG estimator is I\u0302(X;Y ) =\n1\nN N\u2211 i=1 ( \u03c8(k) + \u03c8(N)\u2212 \u03c8(nx,i)\u2212 \u03c8(ny,i) ) , (12)\nwhere \u03c8(x) is the digamma function, \u03c8(x) = \u0393\u2032(x)/\u0393(x) (for large x, \u03c8(x) \u2248 log x\u2212 1/(2x)), and the kNN statistics nx,i and ny,i are defined as\nnx,i \u2261 \u2211 j 6=i I{\u2016Xj \u2212Xi\u2016\u221e < \u03c1k,i} , and (13)\nny,i \u2261 \u2211 j 6=i I{\u2016Yj \u2212 Yi\u2016\u221e < \u03c1k,i}. (14)\nNote that the number of nearest neighbors in X and Y are computed with respect to \u03c1k,i in the joint space (X,Y ). This innovative idea, not only gives a simple estimator, but also has an advantage of canceling correlations in three entropy estimates, giving an improved performance. However, despite its popularity in practice due to its simplicity, no convergence result has been known until very recently (when [GOV16] showed some consistency and rate of convergence properties).\nInspired by the innovative mutual information estimator in (12), we combine importance sampling techniques to adjust for the uniform prior for UMI, and propose a novel estimator. On top of the provable convergence, our estimator has only one hyper-parameter k (besides the choice of bandwidth hN for estimating the marginal distribution PX which is a significantly simpler task compared to estimating the joint), which is the number of nearest neighbors to consider; in practice k is set to a small integer such as 4 or 5. Continuous X . We propose a novel UMI estimator based on the Kraskov mutual information estimator. For a conditional probability density fY |X , we want to compute the uniform mutual information from N i.i.d. samples (X1, Y1), . . . , (XN , YN ) that are generated from fY |XfX for some prior on X. Our UMI estimator is based on k nearest neighbor (kNN) statistics. Given a choice of k \u2208 Z+ and N samples,\nU\u0302MI \u2261 1 N N\u2211 i=1 wi ( \u03c8(k) + log Ncdxcdy cdx+dy nx,i ny,i ) , (15)\nwhere X \u2286 Rdx , Y \u2286 Rdy , cd = \u03c0 d 2 /\u0393(d2 + 1) is the volume of d-dimensional unit ball, and wi is the self-normalized importance sampling estimate [CMMR12] of u(Xi)f(Xi) :\nwi \u2261 N/f\u0303(Xi)\u2211N\nj=1\n( 1/f\u0303(Xj) ) , (16) where f\u0303 : X \u2192 R is the estimate of fX(x). We use the standard kernel density estimator with a bandwidth hN :\nf\u0303(x) \u2261 1 NhdxN N\u2211 i=1 K (Xi \u2212 x hN ) . (17)\nWe define the kNN statistics nx,i and ny,i as follows. For each sample (Xi, Yi), calculate the Euclidean distance \u03c1k,i (as opposed to the `\u221e distance proposed by [KSG04]) to the k-th nearest neighbor. This determines the (random) number of samples within \u03c1k,i in X : first nx,i is defined as the same as in (13), but with Euclidean distance; second we have a weighted number of samples within \u03c1k,i in Y as\nny,i \u2261 \u2211 j 6=i wjI{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i}. (18)\nCompared to (12), we first exchange log function for the digamma functions of N , nx,i, and ny,i. This step (especially for nx,i, and ny,i) is crucial for proving convergence. We use ideas from importance sampling and introduce new variables wi\u2019s that capture the correction for the mismatch in the prior. The constants cdx , cdy , and cdx+dy correct for the volume measured in `2.\nDiscrete X . Similarly, for a discrete random variable X, the joint probability density function is denoted by f(x, y) = pX(x)fY |X(y|x). We propose a UMI estimator, and overload the same notation for this discrete case.\nU\u0302MI \u2261 1 N N\u2211 i=1 wXi ( \u03c8(k) + log N nXi nyi ) , (19)\nwhere nXi = |{j \u2208 [N ] : j 6= i,Xj = Xi}| is the number of samples j such that Xj = Xi, wXi is the self-normalizing estimate of 1/(|X |pX(Xi)) defined as\nwx \u2261 N\n|X |nx , (20)\nand ny,i is the weighted kNN statistics defined as follows. For each sample (Xi, Yi), let the distance to the k-th nearest neighbor be \u03c1k,i, where those samples that have the same X value as Xi is considered and the Euclidean distance is measured in Y. We define the weighted number of samples within \u03c1k,i in Y as\nny,i \u2261 \u2211 j 6=i wXj I{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i}. (21)\n3.2 Capacitated Mutual Information\nGiven standard estimators for mutual information and entropy, it is not at all clear how to directly estimate CMI where fX is changed to the (unknown) optimal input distribution. However, combining the mutual information estimator in (12) with importance sampling techniques, we design a novel estimator as a solution to an optimize over the space of the weights. Our estimator has only one hyper-parameter k, the number of nearest neighbors to consider. Continuous X . For a conditional distribution fY |X , we compute an estimate of CMI from i.i.d. samples (X1, Y1), . . . , (XN , YN ) generated from fY |XfX for some prior on X. We introduce a novel CMI estimator that is based on our UMI estimator. Given a choice of k \u2208 Z+ and N samples, the estimated CMI is the solution of the following constrained optimization:\nC\u0302MI = max w\u2208Ta,N\n1\nN N\u2211 i=1 wi ( \u03c8(k) + log( Ncdxcdy cdx+dynx,iny,i ) ) ,\nwhere dx, dy, nx,i, ny,i and cd are defined in the same as in (15). We optimize over w1, . . . , wN under the second moment constraint, i.e. Ta,N = {w \u2208 RN |wi \u2265 0,\u2200i \u2208 [N ], (1/N) \u2211N i=1 wi = 1, (1/N) \u2211N i=1 wi\u2016Xi\u20162 \u2264 a2}. Observe that no KDE of PX is needed for CMI estimation, making it particularly simple and robust. Discrete X . Similarly, we define the CMI estimate C\u0302MI as the solution of the following constrained optimization:\nC\u0302MI = max w\u2208T\u2206\n1\nN N\u2211 i=1 wXi ( \u03c8(k) + log( N nx,iny,i ) )\nwhere nx,i and ny,i are defined in (19). T\u2206 is the set of quantized version of an interval [C1, C2] with step size \u2206, i.e. T\u2206 = { w \u2208 {C1 + mi\u2206}|X | \u2223\u2223(1/N)\u2211|X |x=1 wx \u2208 [1 \u2212 |X |\u2206, 1 + |X |\u2206], and mi \u2208 {0, 1, . . . , d(C2 \u2212 C1)/C1e} for all i } . Such a quantization is crucial in proving consistence in Theorem 2.\n4 Convergence Guarantees\nWe show both the proposed UMI and CMI estimators are consistent under typical assumptions on the distribution. While consistency of estimators in the large sample limit is generally only a (basic) first step in understanding their properties, this is not so for fixed-k nearest neighbor based estimators. As far as we know, the only estimator based on fixed-k nearest neighbors that is known to be consistent is the entropy estimator of [KL87], and the convergence rate is only known for the univariate case [TVdM96] (and that too under significant assumptions on the univariate density). Our result below for the consistency of the UMI estimator for discrete alphabet marks another instance where consistency of fixed-k nearest neighbor based estimators is established.\nUniform Mutual Information: As our estimators use the off-the-shelf kernel density estimator of PX [DP84, SJ91] and also the ides from the nearest-neighbor methods [KL87], we make assumptions on the conditional density fY |X that are typical in these literature. One extra assumption we make for UMI is that the Radon-Nikodym derivative dPUdPX is uniformly bounded over the support. This is necessary for controlling the importance-sampling estimates of wi\u2019s. We refer to the Assumption 1 in the supplementary material for a precise description.\nTheorem 1. Under the Assumption 1 in the supplementary material, the UMI estimator converges to the true value in probability, i.e. for all \u03b5 > 0 and all \u03b4 > 0,\nlim N\u2192\u221e\nP ( \u2223\u2223U\u0302MI\u2212UMI(fY |X)\u2223\u2223 > \u03b5 ) = 0 , (22)\nif k > max{dy/dx, dx/dy} for continuous X and (logN)(1+\u03b4)dy < k < \u221a N/(5 logN) for discrete X.\nIn practice, we regularize the kNN distance \u03c1k,i in case it is much smaller than the expected distance of order N\u22121/(dx+dy). For continuous X , we require k to be larger than the ratio of the dimensions, which is a finite constant. For discrete X , however, the effective dimension of X is zero, which makes the ratio dy/dx unbounded. Hence, for concentration of measure to hold, we need k1/dy scaling at least logarithmically in the number of samples N .\nCapacitated Mutual Information: We make analogous assumptions which are described precisely in Assumption 2 in the supplementary material. The following theorem establishes consistency of our estimator when X is discrete and we quantize Y. Our analysis requires uniform convergence over all possible choices of the weights w, making the quantization step inevitable; improvements on this technical condition are natural future steps.\nTheorem 2. Under the Assumption 2 in the supplementary material, the CMI estimator converges in probability to the true value up to the resolution of the quantization, i.e. if k > (logN)(1+\u03b4)dy for some \u03b4 > 0, and k < \u221a N/(5 logN), for all \u03b5 > 0 and \u2206 > 0 and s(\u2206) = O(\u2206)\nlim N\u2192\u221e\nP ( \u2223\u2223C\u0302MI\u2212 CMI(fY |X)\u2223\u2223 > \u03b5+ s(\u2206) ) = 0.\n5 Numerical Experiments\n5.1 Gene Causal Strength from Single Cell Data\nWe briefly describe the setup of [KSM+14] to motivate our numerical experiments. Consider a simple genetic pathway: a cascade of genes having expression values X,Y, Z which interact linearly, i.e., X \u2192 Y \u2192 Z. A key question of interest in this case is how the signaling in the pathway varies in different conditions of intervention. Let T denote the time after the intervention (for example, after giving a certain drug). Then we may want to compare the strength of the causal relationship between two genes at different times after the intervention. In the experiments, usually samples are taken at very few time points, so T has very small cardinality (for example, before the drug, 10 minutes after the drug and 50 minutes after the drug), but at each given time point, many cells are interrogated so we get samples from the distribution\nPX,Y,Z;T=t = P (Y |X;T = t)P (Z|Y ;T = t). For each value of T = t, we observe Nt i.i.d. samples (Xi, Yi, Zi), for i = 1, 2, ..., Nt sampled from PX,Y,Z;T=t. These samples are obtained using a technique called single-cell mass flow cytometry, see [KSM+14] for details. We are interested in obtaining a causal measure C(X \u2192 Y ;T = t) = C(P (Y |X;T = t)) and another measure C(Y \u2192 Z;T = t) = C(P (Z|Y ;T = t)) for each time point t. This measure serves as a high level summary of how signaling proceeds in the cascade as a function of time, and lets one compare the strengths of a given causal relationship at different points after intervention.\nIf the drug indeed activates the causal pathway, one may expect the causal relationship to follow a certain trend, i.e., at earlier t, the strength of C(X \u2192 Y ;T = t) will be high and at a later value of t, the strength of C(Y \u2192 Z;T = t) will be high before the effect of the drug wears off, at which time we expect all the relationships to fall back to its low nominal value. Such an analysis is conducted in [KSM+14] where the causal strength function C is evaluated via the so-called DREMI estimator (essentially a version of UMI estimation with a \u201cchannel amplification\u201d step and careful choice of hyper parameters therein \u2013 no theoretical properties of this estimator were evaluated). In that paper, it is shown that, for two example pathways, DREMI recovers the correct trend, i.e., it correctly identifies the time at which each causal relationship is expected to peak as per prior biological knowledge. This demonstrates the utility of DREMI for causal strength inference in gene networks (see Figure 6 of [KSM+14]). The authors there also demonstrate that other metrics which depend on the whole joint distribution, such as mutual information, maximal information coefficient, and correlation do not capture the trend. As an aside, we note that a somewhat different set of \u201ctrend spotting\u201d estimators, primarily trying to find genes which demonstrate a monotonic trend over time from single-cell RNA-sequencing data, have been proposed very recently in [MJG15].\nIn this paper, we have studied influence measures axiomatically and proposed the UMI and CMI measures. It is natural to apply our estimators to each time point in the same setting as [KSM+14] \u2013 and look to understand two distinct issues in our experiments with the flow-cytometry data. The first is whether the proposed quantities of UMI and CMI are able to capture the same biological trend as DREMI was able to. The second question relates to the sample complexity: how does the ability to recover the trend vary as a function of the sample complexity? To study this, we subsample the original data from [KSM+14] multiple times (100 in the experiments) at each subsampling ratio and compute the fraction of times we recover the true biological trend. This is plotted in Figure 1. The figure demonstrates that when the whole dataset is made available, UMI and CMI are able to spot the trend correctly (just as DREMI does). When fewer samples are available, UMI uniformly dominates DREMI and, in turn, CMI uniformly dominates UMI in terms of capturing the biological trend as a function of number of samples available. We believe that this strong empirical evidence lends credence to our approach. For completeness, we note that the datasets represented in Figure 1 refer to regular T-cells (left figure) and T-cells exposed with an antigen (right figure), for which we expect different biological trends, but both of which are correctly captured by our metrics.\n5.2 Synthetic data\nWe demonstrate the accuracy of the proposed UMI and CMI estimators on synthetic experiments. We generate N samples from PXY where X is distributed as beta distribution Beta(1.5, 1.5) and Y = X + N , N \u223c N (0, \u03c32), independent of X. We present three results with varying \u03c32 \u2208 {0.09, 0.36, 1.0}. Figure\n2 shows the estimate of UMI, averaged over 100 instances. This is compared to the ground truth and the state-of-the-art partition based estimators from [Mod89]. The ground truth has been computed via simulations with 8192 samples from the desired distribution PY |XUX using Kraskov\u2019s mutual information estimator [KSG04]. For CMI, we use exactly the same distribution PXY as in UMI, but with varying \u03c32 \u2208 {0.36, 1.0, 2.25}, which is illustrated in Figure 3. Under the power constraint, the ground truth is given by 12 log(1 + \u03c32X \u03c32N ) = 12 log(1 + 1/16\u03c3 2). The proposed CMI estimator is compared against BlahutArimoto algorithm [Bla72, Ari72] for computing discrete channel capacity, applied to quantized data. Both figures illustrate that the proposed estimators significantly improves over the state-of-the-art partition based methods, in terms of sample complexity.\n6 Discussion\nIn this paper we have proposed novel information theoretic measures of potential influence of one variable on another, as well as provided novel estimators to compute the measures from i.i.d. samples. The technical innovation has been in proposing these estimators, by combining separate threads of ideas in statistics (including importance sampling and nearest-neighbor methods). The consistency proofs suggest that a similar analysis the very popular estimator of (traditional) mutual information in [KSG04] can be conducted successfully; such work has been recently conducted in [GOV16]. Several other issues in statistical estimation theory intersect with our current work and we discuss some of these topics below.\n(a) The main technical results of this paper have been weak consistency of the proposed estimators. Proving stronger consistency guarantees and rates of convergence would be natural improvements, albeit challenging ones. Rates of convergence in the nearest-neighbor methods are barely known in the literature even for traditional information theoretic quantities: for instance, [TVdM96] derives a \u221a N consistency for the single dimensional case of differential entropy estimation (under strong assumptions on the underlying\npdf), leaving higher dimensional scenarios open, and which recently have been successfully addressed in [GOV16].\n(b) There is a natural generalization of our estimators when the alphabet Y is high dimensional, using the kNN approach (just as in the differential entropy estimator of [KL87] or in the mutual information estimator of [KSG04]). However, very recent works [GSG14, GSG15, LP16] have shown that boundary biases common in high dimensional scenarios is much better handled using local parametric methods (as in [L+96, HJ96]). Adapting these approaches to the estimators for UMI and CMI is an interesting direction of future research.\n(c) We have considered both the case of discrete and (single dimensional) continuous alphabet X . The scenario of high dimensional X is significantly more challenging for CMI estimation: this is because of the (vastly) expanded space of distributions over which the optimization can be performed. Also challenging is to consider application specific regularization of the inputs in this scenario.\n(d) While the focus of this paper has been on quantifying potential causal influence, a related question involves testing the direction of causality for a pair of random variables. This is a widely studied topic with a long lineage [Pea09] but also of strong topical interest [JBGW+13, JSSS15, MPJ+15, SJSB15]. A natural inclination is to explore the efficacy of UMI and CMI measures to test for direction of causality \u2013 especially in the context of the benchmark data sets collected in [MPJ+15]. Our results are as follows: UMI has a 45% probability to predict the correct direction. CMI gives 53% probability. Directly comparing the marginal entropy H(X) and H(Y ) by the estimator in [KL87] also only provides 45% accuracy. While in [MPJ+15], different entropy estimators (with appropriate hyper parameter choices) were applied to get an accuracy up to 60%-70%. Further research is needed to shed conclusive light, although we point out that the benchmark data sets in [MPJ+15] have substantial confounding factors that make causal direction hard to measure in the first place.\n(e) The axiomatic derivation of potential causal influence naturally suggests CMI as an appropriate measure. We are also able to show that a more general quantity \u2013 the so-called Re\u0301nyi capacity \u2013 also meets the axioms. For any \u03bb > 0, \u03bb 6= 1, define Re\u0301nyi entropy as\nH\u03bb(P ) := 1\n\u03bb\u2212 1 logEP [(dP )\u03bb] (23)\nand Re\u0301nyi divergence as:\nD\u03bb(P\u2016Q) := 1\n\u03bb\u2212 1 logEQ\n[( dP\ndQ\n)\u03bb] . (24)\nNow define the asymmetric information measure [Csi95]:\nK\u03bb(PXPY |X) := inf QY\nD\u03bb(PXY \u2016PXQY ), (25)\nwhich converges to the traditional mutual information when \u03bb \u2192 1. Now we can define the Re\u0301nyi capacity for any parameter \u03bb as, for any fixed conditional distribution PY |X :\nCMI\u03bb := sup PX inf QY\nD\u03bb(PXY \u2016PXQY ), (26)\nObserve that as \u03bb\u2192 1, we have CMI\u03bb \u2192 CMI, the traditional Shannon capacity. We observe the following. Proposition 2. For any \u03bb > 0, \u03bb 6= 1 we have that CMI\u03bb satisfies the axioms in Section 2.\nThe proof is available in Appendix D. In the light of this result, it would be interesting to design estimators for the more general family of Re\u0301nyi capacity measures and confirm their performance on empirical tasks such as the ones studied in [KSM+14]. It would also be very interesting to understand the role of additional axioms that would lead to uniqueness of Shannon capacity (in the same spirit as entropy being uniquely characterized by somewhat similar axioms [Csi08]).\n(f) Finally, a comment on the optimization problem in CMI estimation: the optimization problem involving the wi\u2019s is not necessarily a concave program for a given sample realization, although this program converges to that of Shannon capacity computation involves maximizing mutual information, which is a concave function of the input probability distribution. Standard (stochastic) gradient decent is used in our experiments, and we did not face any disparity in convergent values over the set of synthetic experiments we conducted.\nAcknowledgements\nThis work is supported in part by ARO W911NF1410220, NSF SaTC award CNS-1527754, NSF CISE award CCF-1553452 and a University of Washington startup grant.\nReferences\n[Ari72] Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. Information Theory, IEEE Transactions on, 18(1):14\u201320, 1972.\n[BDGVdM97] Jan Beirlant, Edward J Dudewicz, La\u0301szlo\u0301 Gyo\u0308rfi, and Edward C Van der Meulen. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6(1):17\u201339, 1997.\n[Bla72] Richard E Blahut. Computation of channel capacity and rate-distortion functions. Information Theory, IEEE Transactions on, 18(4):460\u2013473, 1972.\n[CMMR12] Jean Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian P Robert. Adaptive multiple importance sampling. Scandinavian Journal of Statistics, 39(4):798\u2013812, 2012.\n[CS04] Imre Csisza\u0301r and Paul C Shields. Information theory and statistics: A tutorial. Now Publishers Inc, 2004.\n[Csi95] Imre Csisza\u0301r. Generalized cutoff rates and renyi\u2019s information measures. Information Theory, IEEE Transactions on, 41(1):26\u201334, 1995.\n[Csi08] Imre Csisza\u0301r. Axiomatic characterizations of information measures. Entropy, 10(3):261\u2013273, 2008.\n[CT12] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.\n[DP84] Luc Devroye and Clark S Penrod. The consistency of automatic kernel density estimates. The Annals of Statistics, pages 1231\u20131249, 1984.\n[GOV16] Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying fixed k-nearest neighbor information estimators. arXiv preprint arXiv:1604.03006, 2016.\n[GSG14] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information for strongly dependent variables. arXiv preprint arXiv:1411.2003, 2014.\n[GSG15] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Estimating mutual information by local gaussian approximation. arXiv preprint arXiv:1508.00536, 2015.\n[HJ96] Nils Lid Hjort and MC Jones. Locally parametric nonparametric density estimation. The Annals of Statistics, pages 1619\u20131647, 1996.\n[HV15] Siu-Wai Ho and Sergio Verdu\u0301. Convexity/concavity of renyi entropy and \u03b1-mutual information. In Information Theory (ISIT), 2015 IEEE International Symposium on, pages 745\u2013749. IEEE, 2015.\n[JBGW+13] Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Scho\u0308lkopf, et al. Quantifying causal influences. The Annals of Statistics, 41(5):2324\u20132358, 2013.\n[JSSS15] D. Janzing, B. Steudel, N. Shajarisales, and B. Scho\u0308lkopf. Justifying Information-Geometric Causal Inference, chapter 18, pages 253\u2013265. Springer International Publishing, 2015.\n[KBG+07] Shiraj Khan, Sharba Bandyopadhyay, Auroop R Ganguly, Sunil Saigal, David J Erickson III, Vladimir Protopopescu, and George Ostrouchov. Relative performance of mutual information estimation methods for quantifying the dependence among short and noisy data. Physical Review E, 76(2):026209, 2007.\n[KKPW15] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, and Larry Wasserman. Nonparametric von mises estimators for entropies, divergences and mutual informations. In Advances in Neural Information Processing Systems, pages 397\u2013405, 2015.\n[KL87] LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2):9\u201316, 1987.\n[KSG04] A. Kraskov, H. Sto\u0308gbauer, and P. Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.\n[KSM+14] Smita Krishnaswamy, Matthew H Spitzer, Michael Mingueneau, Sean C Bendall, Oren Litvin, Erica Stone, Dana Pe\u2019er, and Garry P Nolan. Conditional density-based analysis of t cell signaling in single-cell data. Science, 346(6213):1250689, 2014.\n[L+96] Clive R Loader et al. Local likelihood density estimation. The Annals of Statistics, 24(4):1602\u2013 1618, 1996.\n[LP16] Damiano Lombardi and Sanjay Pant. Nonparametric k-nearest-neighbor entropy estimator. Physical Review E, 93(1):013310, 2016.\n[MJG15] Jonas Mueller, Tommi Jaakkola, and David Gifford. Modeling trends in distributions. arXiv preprint arXiv:1511.04486, 2015.\n[Mod89] Rudy Moddemeijer. On estimation of entropy and mutual information of continuous distributions. Signal processing, 16(3):233\u2013248, 1989.\n[MPJ+15] J.M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Scho\u0308lkopf. Distinguishing cause from effect using observational data: methods and benchmarks. Journal of Machine Learning Research, 2015.\n[Owe13] Art B. Owen. Monte Carlo theory, methods and examples. 2013.\n[Pan03] Liam Paninski. Estimation of entropy and mutual information. Neural computation, 15(6):1191\u20131253, 2003.\n[Pea09] Judea Pearl. Causality. Cambridge university press, 2009.\n[PPS10] Da\u0301vid Pa\u0301l, Barnaba\u0301s Po\u0301czos, and Csaba Szepesva\u0301ri. Estimation of re\u0301nyi entropy and mutual information based on generalized nearest-neighbor graphs. In Advances in Neural Information Processing Systems, pages 1849\u20131857, 2010.\n[PV10] Yury Polyanskiy and Sergio Verdu\u0301. Arimoto channel coding converse and re\u0301nyi divergence. In Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 1327\u20131333. IEEE, 2010.\n[PXS12] Barnaba\u0301s Po\u0301czos, Liang Xiong, and Jeff Schneider. Nonparametric divergence estimation with applications to machine learning on distributions. arXiv preprint arXiv:1202.3758, 2012.\n[RE15] Robin J Richardson and Thomas S Evans. Non-parametric causal models. 2015.\n[Re\u0301n59] Alfre\u0301d Re\u0301nyi. On measures of dependence. Acta mathematica hungarica, 10(3-4):441\u2013451, 1959.\n[Sha48] C.E. Shannon. A mathematical theory of communication. Bell System Tech. J., 27:379423 and 623656, 1948.\n[SJ91] Simon J Sheather and Michael C Jones. A reliable data-based bandwidth selection method for kernel density estimation. Journal of the Royal Statistical Society. Series B (Methodological), pages 683\u2013690, 1991.\n[SJSB15] N. Shajarisales, D. Janzing, B. Scho\u0308lkopf, and M. Besserve. Telling cause from effect in deterministic linear dynamical systems. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, page 285?294. JMLR, 2015.\n[SMH+03] Harshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam Fedorowicz, and Eugene Demchuk. Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4):301\u2013321, 2003.\n[SRHI10] Kumar Sricharan, Raviv Raich, and Alfred O Hero III. Empirical estimation of entropy functionals with confidence. arXiv preprint arXiv:1012.4188, 2010.\n[TVdM96] Alexandre B Tsybakov and EC Van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support. Scandinavian Journal of Statistics, pages 75\u201383, 1996.\n[VEH14] Tim Van Erven and Peter Harremoe\u0308s. Re\u0301nyi divergence and kullback-leibler divergence. Information Theory, IEEE Transactions on, 60(7):3797\u20133820, 2014.\n[WKV09] Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu\u0301. Divergence estimation for multidimensional densities via-nearest-neighbor distances. Information Theory, IEEE Transactions on, 55(5):2392\u20132405, 2009.\nAppendix\nA Proof of the UMI estimator convergence in Theorem 1\nWe present the proof of the theorem for two separate UMI estimators: first for continuous X and next for discrete X. We first state the formal assumptions under which the theorem holds.\nAssumption 1. For continuous X , define fU (y) \u2261 \u222b x u(x)fY |X(y|x)dx, (27)\nfU (x \u2223\u2223y) \u2261 u(x)fY |X(y|x)\nfU (y) . (28)\nWe make the following assumptions: (a) \u222b fX,Y (x, y) ( log f(x, y) )2 dxdy <\u221e.\n(b) There exists a finite constant C such that the Hessian matrix of H(f) and H(fU ) exists and max{\u2016H(f)\u20162, \u2016H(fU )\u20162} < C almost everywhere.\n(c) There exists a positive constant C \u2032 such that the conditional pdfs satisfy fY |X(y \u2223\u2223x) < C \u2032 and fU (x\u2223\u2223y) <\nC \u2032 almost everywhere.\n(d) There exist positive constants C1 < C2 such that the marginal pdf satisfy, almost everywhere,\nC1 \u00b5(X ) < fX(x) < C2 \u00b5(X ) .\n(e) The bandwidth hN of kernel density estimator is chosen as hN = 1 2N \u22121/(2dx+3).\nFor discrete X , define\nfu(y) \u2261 \u2211 x\u2208X 1 |X | fY |X(y|x). (29)\nWe make the following assumptions: (a) \u222b fY |X(y|x) ( log fY |X(y|x) )2 dy <\u221e, for all x \u2208 X .\n(b) There exists a finite constant C such that the Hessian matrix of H(fY |X) exists and \u2016H(fY |X)\u20162 < C almost everywhere, for all x \u2208 X .\n(c) There exists a finite constant C \u2032 such that the conditional pdf fY |X(y \u2223\u2223x) < C \u2032 almost everywhere, for\nall x \u2208 X .\n(d) There exists finite constants C1 < C2 such that the prior pX(x) > C1/|X | and fX(x) < C2/|X | almost everywhere.\nA.1 The case of continuous X Given these assumptions, we define\ng(Xi, Yi) \u2261 \u03c8(k) + log(N) + log ( cdxcdy cdx+dy ) \u2212 ( log(nx,i) + log(ny,i) )\n(30)\nsuch that I\u0302Uk,N (X,Y ) = 1 N \u2211N i=1 wi g(Xi, Yi). Define each quantity with the true prior fX(x) as\nw\u2032i \u2261 u(Xi)\nfX(Xi) , (31) n\u2032y,i \u2261 \u2211 j 6=i w\u2032jI{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i} , (32)\ng\u2032(Xi, Yi) \u2261 \u03c8(k) + log(N) + log ( cdxcdy cdx+dy ) \u2212 ( log(nx,i) + log(n \u2032 y,i) ) . (33)\nWith UX equal to the uniform distribution on the support of X, we apply the triangle inequality to show that each term converges to zero in probability.\n\u2223\u2223I\u0302Uk,N (X,Y )\u2212 IU (fY |X)\u2223\u2223 = \u2223\u2223\u2223 1N N\u2211 i=1 wig(Xi, Yi)\u2212 \u222b\u222b u(x)fY |X(y|x) log fY |X(y|x)\u222b u(x\u2032)fY |X(y|x\u2032)dx\u2032 dxdy \u2223\u2223\u2223 \u2264 1\nN \u2223\u2223\u2223 N\u2211 i=1 ( wig(Xi, Yi)\u2212 w\u2032ig\u2032(Xi, Yi) )\u2223\u2223\u2223+ (34) \u2223\u2223\u2223 1 N N\u2211 i=1 w\u2032ig \u2032(Xi, Yi)\u2212 \u222b\u222b u(x)fY |X(y|x) log fY |X(y|x)\u222b u(x\u2032)fY |X(y|x\u2032)dx\u2032 dxdy \u2223\u2223\u2223.(35)\nThe first term (34) captures the error in the kernel density estimator and we have the following claim, whose proof is delegated to Appendix C.\nLemma 1. The term in Equation (34) converges to 0 as N \u2192\u221e in probability.\nThe second term in the error (35) comes from the sample noise in density estimation. Similar to the decomposition of mutual information, I(X;Y ) = H(X) + H(Y ) \u2212 H(X,Y ), we decompose our estimator into three terms:\n1\nN N\u2211 i=1 w\u2032ig \u2032(Xi, Yi) = H\u0302 U k,N (X) + H\u0302 U k,N (Y )\u2212 H\u0302Uk,N (X,Y )\u2212 N\u2211 i=1 w\u2032i N (2 log(N \u2212 1)\u2212 \u03c8(N)\u2212 log(N)) ,\nwhere\nH\u0302Uk,N (X,Y ) \u2261 N\u2211 i=1 w\u2032i N ( \u2212 \u03c8(k) + \u03c8(N) + log cdx+dy + (dx + dy) log \u03c1k,i ) , (36)\nH\u0302Uk,N (X) \u2261 N\u2211 i=1 w\u2032i N ( \u2212 log nx,i + log(N \u2212 1) + log cdx + dx log \u03c1k,i ) , (37)\nH\u0302Uk,N (Y ) \u2261 N\u2211 i=1 w\u2032i N ( \u2212 log ny,i + log(N \u2212 1) + log cdy + dy log \u03c1k,i ) . (38)\nNotice that \u2211N i=1 w\u2032i N (2 log(N \u22121)\u2212\u03c8(N)\u2212 log(N)) goes to 0 as N goes to infinity. The desired claim follows directly from the following two lemmas showing the convergence each entropy estimates to corresponding entropies under UMI.\nLemma 2. Under the hypotheses of Theorem 1, for all \u03b5 > 0\nlim N\u2192\u221e\nP ( \u2223\u2223\u2223H\u0302Uk,N (X,Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x) log fY |X(y|x)u(x)dxdy )\u2223\u2223\u2223 > \u03b5) = 0 . (39)\nLemma 3. Under the hypotheses of Theorem 1, for all \u03b5 > 0\nlim N\u2192\u221e\nP ( \u2223\u2223\u2223H\u0302Uk,N (X) + H\u0302Uk,N (Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x)( log fX(x) + fU (y))dxdy )\u2223\u2223\u2223 > \u03b5) = 0 , (40)\nwhere fU (y) = \u222b fY |X(y|x)u(x)dx.\nA crucial technical idea in proving these lemmas is the concept of importance sampling. For any function h(Xi, Yi), the importance sampling estimate of E[h] is given by\nh\u0303N = 1\nN N\u2211 i=1 w\u2032ih(Xi, Yi) , (41)\nwhere w\u2032i = Nu(Xi)/f(Xi). The following lemma gives the almost sure convergence of h\u0303n. Lemma 4 (Theorem 9.1 in [Owe13]). Assume E[h] = \u222b\u222b\nu(x)fY |X(y|x)h(x, y)dxdy exists, then\nP (\nlim n\u2192\u221e\nh\u0303n = E[h] ) = 1 .\nA.1.1 Proof of Lemma 10\nGiven Xi = x, denote aj = 1\nhdxN K(\nXj\u2212x hN ) such that f\u0303X(Xi) = 1 N \u2211N j=1 aj . For sufficiently small hN , the\nmean of aj is given by: E[aj ] = \u222b z\u2208RdX 1 hdxN K( z \u2212Xi hN )fX(z)dz\n= \u222b u\u2208RdX K(u)fX(x+ hNu)du\n= \u222b u\u2208RdX K(u) ( fX(x) + hNu T\u2207fX(x) + h2NuTH(fX)(x)u+ o(h2N ))du\n= fX(x) + h 2 NC \u222b u\u2208RdX \u2016u\u20162K(u)du+ o(h2N ) , (42)\nwhere we used the fact that the kernel is centered such that \u222b K(u)aTudu = 0. For sufficiently small hN , we obtain |E[aj ]\u2212 fX(x)| < hN . Therefore,\nP ( \u2223\u2223 f\u0303X(Xi)\u2212 fX(Xi) \u2223\u2223 > N\u22121/(2dx+3)\u2223\u2223\u2223Xi = x)\n\u2264 P ( \u2223\u2223 N\u2211\nj=1\naj \u2212NfX(Xi) \u2223\u2223 > N (2dx+2)/(2dx+3)\u2223\u2223\u2223Xi = x)\n\u2264 P ( \u2223\u2223 \u2211\nj 6=i\naj \u2212 (N \u2212 1)E[aj ] \u2223\u2223 > N (2dx+2)/(2dx+3) \u2212 \u2223\u2223ai \u2212NfX(x) + (N \u2212 1)E[aj ]\u2223\u2223\u2223\u2223\u2223Xi = x). (43)\nSince ai is bounded by A/h dx N , by choosing hN = 1 2N \u2212 12dx+3 , the right hand side is lower bounded by:\nN (2dx+2)/(2dx+3) \u2212 \u2223\u2223ai \u2212NfX(x) + (N \u2212 1)E[aj ]\u2223\u2223\n\u2265 N (2dx+2)/(2dx+3) \u2212 |ai \u2212 fX(x)| \u2212 (N \u2212 1)|E[aj ]\u2212 fX(x)| \u2265 N (2dx+2)/(2dx+3) \u2212A/hdxN \u2212NhN \u2265 1 3 N (2dx+2)/(2dx+3). (44)\nSince for j 6= i, a\u2032js are i.i.d and bounded by A/h dx N , by Hoeffding\u2019s inequality, we obtain\nP ( \u2223\u2223 \u2211\nj 6=i\naj \u2212 (N \u2212 1)E[aj ] \u2223\u2223 > N (2dx+2)/(2dx+3) \u2212 \u2223\u2223ai \u2212NfX(x) + (N \u2212 1)E[aj ]\u2223\u2223\u2223\u2223\u2223Xi = x)\n\u2264 P ( \u2223\u2223 \u2211\nj 6=i\naj \u2212 (N \u2212 1)E[aj ] \u2223\u2223 > 1\n3 N (2dx+2)/(2dx+3) \u2223\u2223\u2223Xi = x) \u2264 2 exp{\u22122 ( 13N (2dx+2)/(2dx+3))2\n(N \u2212 1) \u00b7A2h\u22122dxN }\n\u2264 2 exp{\u2212 2 9A2(N \u2212 1) (N (2dx+2)/(2dx+3)hdxN ) 2}\n\u2264 2 exp{\u2212N 1/(2dx+3)\n9A2 }. (45)\nSince this upper bound is independent of x, we can take expectation over x to obtain the desired claim.\nA.1.2 Proof of Lemma 2\nDefine\nf\u0302X,Y (Xi, Yi) = exp{\u03c8(k)\u2212 \u03c8(N)} cdx+dy\u03c1 dx+dy k,i , (46)\nso that\nH\u0302Uk,N (X,Y ) = \u2212 N\u2211 i=1 w\u2032i N log f\u0302X,Y (Xi, Yi) . (47)\nBy Theorem 8 of [SMH+03], we have\nlim N\u2192\u221e\nE [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)] = log fX,Y (x, y) . (48)\nNotice that w\u2032i log f\u0302(Xi, Yi) are identically distributed, therefore, by plugging in w \u2032 i = u(Xi)/fX(Xi), we have\nlim N\u2192\u221e\nEH\u0302Uk,N (X,Y )\n= \u2212 lim N\u2192\u221e E[w\u2032i log f\u0302X,Y (Xi, Yi)]\n= \u2212 lim N\u2192\u221e \u222b\u222b E [ u(Xi) fX(Xi) log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]fX,Y (x, y)dxdy\n= \u2212 lim N\u2192\u221e\n\u222b\u222b u(x)fY |X(y|x)E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]dxdy. (49) Now we want to show that\nlim N\u2192\u221e\n\u222b\u222b u(x)fY |X(y|x)E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]dxdy = \u222b\u222b u(x)fY |X(y|x) ( lim N\u2192\u221e E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)] )dxdy\n= \u222b\u222b u(x)fY |X(y|x) log fX,Y (x, y)dxdy , (50)\nwhich follows from the reverse Fatou\u2019s lemma and the fact that\nlim sup N\u2192\u221e \u222b\u222b \u2223\u2223 u(x) f(x) E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)] \u2223\u22232f(x, y)dxdy\n\u2264 C\u221221 lim sup N\u2192\u221e \u222b\u222b \u2223\u2223E[ log f\u0302X,Y (Xi, Yi)\u2223\u2223(Xi, Yi) = (x, y)] \u2223\u22232f(x, y)dxdy \u2264 C\u221221 \u222b\u222b lim sup N\u2192\u221e\n\u2223\u2223E[ log f\u0302X,Y (Xi, Yi)\u2223\u2223(Xi, Yi) = (x, y)] \u2223\u22232f(x, y)dxdy \u2264 C\u221221 \u222b\u222b lim sup N\u2192\u221e ( log fX,Y (x, y) )2 f(x, y)dxdy < +\u221e . (51)\nAs explained in the main result section, we regularize the kNN distance such that \u03c1 dx+dy k,i > Ck/N for some positive constant C. This ensures that log f\u0302X,Y (Xi, Yi) < C \u2032 almost surely. It follows that E[log f\u0302X,Y (Xi, Yi)|Xi = x, Yi = y] < C \u2032 and one can apply reverse Fatou\u2019s lemma. Similar interchange of limit has been used in [KL87, WKV09] without the regularization; in this context [PPS10] claims that this step is not justified (although no counterexample is pointed out). But in our case given the practical way the algorithm is implemented with the regularization, reverse Fatou\u2019s lemma is justified. Therefore,\nlim N\u2192\u221e\nEH\u0302Uk,N (X,Y ) = \u2212 \u222b\u222b u(x)fY |X(y|x) log fX,Y (x, y)dxdy. (52)\nMoreover, by Theorem11 of [SMH+03], we have:\nlim N\u2192\u221e\nVar[f\u0302X,Y (Xi, Yi)] = ( \u0393\u2032(k)\n\u0393(k) )\u2032Var[log fX,Y (x, y)] , (53)\nand for any j 6= i:\nlim N\u2192\u221e Cov[f\u0302X,Y (Xi, Yi), f\u0302X,Y (Xj , Yj)] = 0 . (54)\nBy w\u2032i \u2264 1/C1 for all i and the fact that f\u0302X,Y (Xi, Yi) are identically distributed, we have: Var [ H\u0302Uk,N (X,Y ) ] =\nN\u2211 i=1 (w\u2032i) 2 N2 Var[f\u0302X,Y (Xi, Yi)] + \u2211 j 6=i w\u2032iw \u2032 j N2 Cov[f\u0302X,Y (Xi, Yi), f\u0302X,Y (Xj , Yj)]\n\u2264 N\u2211 i=1 1 C21N 2 Var[f\u0302X,Y (Xi, Yi)] + \u2211 j 6=i 1 C21N 2 Cov[f\u0302X,Y (Xi, Yi), f\u0302X,Y (Xj , Yj)]\n= 1\nC21N ((\n\u0393\u2032(k) \u0393(k) )\u2032Var[log fX,Y (x, y)]) + 1\nC21N 2\n( N\n2\n) Cov[f\u0302X,Y (X1, Y1), f\u0302X,Y (X2, Y2)] . (55)\nTherefore,\nlim N\u2192\u221e\nVar [ H\u0302Uk,N (X,Y ) ] = 0 . (56)\nCombining (52) and (56), we get:\nlim N\u2192\u221e\nE [( H\u0302Uk,N (X,Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x) log fX,Y (x, y)dxdy ))2]\n= lim N\u2192\u221e\nVar [ H\u0302Uk,N (X,Y ) ] + lim N\u2192\u221e ( EH\u0302Uk,N (X,Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x) log fX,Y (x, y)dxdy ))2\n= 0. (57)\nTherefore, H\u0302Uk,N (X,Y ) converges to its mean in L 2, and hence in probability, i.e.,\nlim N\u2192\u221e\nP ( \u2223\u2223H\u0302Uk,N (X,Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x) log fX,Y (x, y)dxdy )\u2223\u2223 > \u03b5) = 0. (58)\nA.1.3 Proof of Lemma 3\nDefine\nf\u0302X(Xi) \u2261 nx,i\n(N \u2212 1)cdx\u03c1 dx k,i\n, (59)\nf\u0302U (Yi) \u2261 ny,i\n(N \u2212 1)cdy\u03c1 dy k,i\n, (60)\nsuch that\nH\u0302Uk,N (X) + H\u0302 U k,N (Y ) = \u2212 N\u2211 i=1 w\u2032i N ( log f\u0302X(Xi) + log f\u0302U (Yi) ) . (61)\nBy triangle inequality, we can write the formula in Lemma 3 as:\u2223\u2223H\u0302Uk,N (X) + H\u0302Uk,N (Y )\u2212 ( \u2212 \u222b\u222b u(x)fY |X(y|x)( log fX(x) + log fU (y))dxdy )\u2223\u2223 =\n\u2223\u2223 N\u2211 i=1 w\u2032i N ( log f\u0302X(Xi) + log f\u0302U (Yi) ) \u2212 \u222b\u222b u(x)fY |X(y|x) ( log fX(x) + log fU (y) ) dxdy \u2223\u2223 \u2264\n\u2223\u2223 N\u2211 i=1 w\u2032i N ( log fX(Xi) + log fU (Yi) ) \u2212 \u222b\u222b u(x)fY |X(y|x) ( log fX(x) + log fU (y) ) dxdy \u2223\u2223 (62) +\nN\u2211 i=1 w\u2032i N \u2223\u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223\u2223. (63) The first term (62) comes from sampling. Recall that w\u2032i = u(Xi)/fX(Xi). Since the random variables\nw\u2032i ( log fX(Xi) + log fU (Yi) ) are i.i.d., therefore, by the strong law of large numbers,\nN\u2211 i=1 w\u2032i N ( log fX(Xi) + log fU (Yi) ) \u2192 E ( u(x) fX(x) ( log fX(x) + log fU (y) ) ) (64)\nalmost surely. The mean is given by\nE ( u(x) fX(x) (log fX(x) + log fU (y)) ) =\n\u222b\u222b u(x)\nfX(x)\n( log fX(x) + log fU (y) ) f(x, y)dxdy\n= \u222b\u222b u(x)fY |X(y|x) ( log fX(x) + log fU (y) ) dxdy. (65)\nTherefore, (62) converges to 0 almost surely.\nThe second term (63) comes from density estimation. To simplfy the notations, let Zi = (Xi, Yi), z = (x, y) and f(z) = f(x, y). For any fixed \u03b5 > 0, by union bound, we obtain that\nP ( N\u2211 i=1 w\u2032i N \u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5 ) \u2264 P\n( N\u22c3 i=1 {\u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5/2} )+ P( N\u2211 i=1 w\u2032i > 2N).(66)\nThe second term converges to zero by Lemma 4. The first term is bounded by:\nP ( N\u22c3 i=1 {\u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5/2} ) (67) \u2264 N \u00b7 P\n( \u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5/2 ) = N\n\u222b P ( \u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223\u2223 > \u03b5/2\u2223\u2223Zi = z )\ufe38 \ufe37\ufe37 \ufe38\n\u2264I1(z)+I2(z)+I3(z)+I4(z)\nf(z)dz (68)\nwhere\nI1(z) = P ( \u03c1k,i > logN(Nf(x, y)cdx+dy ) \u2212 1dx+dy \u2223\u2223Zi = z ) (69)\nI2(z) = P ( \u03c1k,i < max { (logN)2(NfX(x)cdx) \u2212 1dx , (logN)2(NfU (y)cdy ) \u2212 1dy } \u2223\u2223Zi = z ) (70) I3(z) =\n\u222b r1 r=r2 P ( \u2223\u2223 log f\u0302X(Xi)\u2212 log fX(Xi)\u2223\u2223 > \u03b5/4\u2223\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr (71)\nI4(z) = \u222b r1 r=r2 P ( \u2223\u2223 log f\u0302U (Yi)\u2212 log fU (Yi)\u2223\u2223 > \u03b5/4\u2223\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr (72)\nwhere f\u03c1k,i(r) is the pdf of \u03c1k,i given Zi = z. Here r1 = logN(Nf(z)cdx+dy ) \u2212 1dx+dy and\nr2 = max { (logN)2(NfX(x)cdx) \u2212 1dx , (logN)2(NfU (y)cdy ) \u2212 1dy }. (73)\nI1(z) and I2(z) are the probability that the k-NN distance \u03c1k,i is large or small given Zi = z. I3(z) and I4(z) gives the probability that the estimator deviates from the true value, given that \u03c1k,i is medium. We will consider the four terms separately.\nI1(z): Let BZ(z, r) = {Z : \u2016Z \u2212 z\u2016 < r} be the (dx + dy)-dimensional ball centered at z with radius r. Since the Hessian matrix of H(f) exists and \u2016H(f)\u20162 < C almost everywhere, then for sufficiently small r, the probability mass within BZ(z, r) is given by\nP ( u \u2208 BZ(z, r) ) = \u222b \u2016u\u2212z\u2016\u2264r f(u)du\n= \u222b \u2016u\u2212z\u2016\u2264r f(z) + (u\u2212 z)T\u2207f(z) + (u\u2212 z)TH(f)(z)(u\u2212 z) + o(\u2016u\u2212 z\u20162)du\n\u2208 [ f(z)cdx+dyr dx+dy (1\u2212 Cr2)), f(z)cdx+dyrdx+dy (1 + Cr2)) ] . (74)\nThen for sufficiently large N , the probability mass within BZ(z, r1) is lower bounded by\np1 \u2261 P ( u \u2208 BZ(z, logN(Nf(z)cdx+dy ) \u2212 1dx+dy ) )\n\u2265 f(z)cdx+dy ( logN(Nf(z)cdx+dy ) \u2212 1dx+dy )dx+dy( 1\u2212 C(logN(Nf(z)cdx+dy ) \u2212 1dx+dy )2 ) \u2265 (logN) dx+dy\n2N . (75)\nI1(z) is the probability that at most k samples fall in BZ(z, r1), so it is upper bounded by\nI1(z) = P ( \u03c1k,i > logN(Nf(z)cdx+dy ) \u2212 1dx+dy \u2223\u2223Zi = z )\n= k\u22121\u2211 m=0 ( N \u2212 1 m ) pm1 (1\u2212 p1)N\u22121\u2212m\n\u2264 k\u22121\u2211 m=0 Nm(1\u2212 p1)N\u22121\u2212m \u2264 kNk\u22121(1\u2212 (logN) dx+dy\n2N )N\u2212k\u22121\n\u2264 kNk\u22121 exp{\u2212 (logN) dx+dy (N \u2212 k \u2212 1)\n2N }\n\u2264 kNk\u22121 exp{\u2212 (logN) dx+dy\n4 } (76)\nfor any dx, dy \u2265 1.\nI2: Let r2,1 \u2261 (logN)2(NfX(x)cdx) \u2212 1dx . Then for sufficiently large N , the probability mass within\nBZ(z, r2,1) is given by:\np2,1 \u2261 P ( u \u2208 BZ(z, (logN)2(NfX(x)cdx) \u2212 1dx ) )\n\u2264 f(z)cdx+dy ( (logN)2(NfX(x)cdx) \u2212 1dx )dx+dy( 1 + C(logN(NfX(x)cdx) \u2212 1dx )2 ) \u2264 2f(z)cdx+dy\n(f(x)cdx) dx+dy dx\n(logN)2(dx+dy)N\u2212 dx+dy dx\n\u2264 2fY |X(y|x) cdx+dy cdx (logN)2(dx+dy)N\u2212 dx+dy dx \u2264 2C \u2032 cdx+dy cdx (logN)2(dx+dy)N\u2212 dx+dy dx (77)\nwhere the last equation comes from the assumption that fY |X(y|x) < C \u2032. Similarly, let r2,2 = (logN)2(NfU (y)cdy ) \u2212 1dy , the probability of being in BZ(z, r2) is\np2,2 \u2261 P ( u \u2208 BZ(z, (logN)2(NfU (y)cdy ) \u2212 1dy ) )\n\u2264 f(z)cdx+dy ( (logN)2(NfU (y)cdx) \u2212 1dx )dx+dy( 1 + C(logN(NfU (y)cdx) \u2212 1dy )2 ) \u2264 2f(z)cdx+dy\n(fU (y)cdy ) dx+dy dy\n(logN)2(dx+dy)N \u2212 dx+dydy\n\u2264 2 f(z) fU (y) cdx+dy cdy (logN)2(dx+dy)N \u2212 dx+dydy \u2264 2C2 fU (x, y)\nfU (y) cdx+dy cdy (logN)2(dx+dy)N \u2212 dx+dydy\n\u2264 2C2C \u2032 cdx+dy cdy (logN)2(dx+dy)N \u2212 dx+dydy . (78)\nI2(z) is the probability that at least k samples lie in BZ(z,max{r2,1, r2,2}). It is upper bounded as\nfollows: I2(z) = P ( \u03c1k,i < max { (logN)2(NfX(x)cdx) \u2212 1dx , (logN)2(NfU (y)cdy ) \u2212 1dy } \u2223\u2223Zi = z ) =\nN\u22121\u2211 m=k ( N \u2212 1 m ) max{p2,1, p2,2}m(1\u2212 {p2,1, p2,2})N\u22121\u2212m\n\u2264 N\u22121\u2211 m=k Nm max{p2,1, p2,2}m\n\u2264 N\u22121\u2211 m=k ( 2C \u2032C2 cdx+dy min{cdx , cdy} (logN)2(dx+dy)N \u2212min{ dydx , dx dy } )m \u2264 (4C \u2032C2 cdx+dy\nmin{cdx , cdy} )k(logN)2k(dx+dy)N\n\u2212kmin{ dydx , dx dy }\n(79)\nfor sufficiently large N such that 2C \u2032 cdx+dy\nmin{cdx ,cdy} (logN)2(dx+dy)N\n\u2212min{ dx+dydx , dx+dy dy } < 1/2, the last inequal-\nity comes from sum of geometric series. This holds for any dx, dy \u2265 1 and k \u2265 1.\nI3: Given that Zi = z = (x, y) and \u03c1k,i = r. Recall that f\u0302X(Xi) = nx,i\n(N\u22121)cdx\u03c1 dx k,i\n, so we have\nP ( \u2223\u2223 log f\u0302X(Xi)\u2212 log fX(Xi)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( \u2223\u2223 log nx,i \u2212 log(N \u2212 1)\u2212 log cdx \u2212 dx log \u03c1k,i \u2212 log fX(x)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( \u2223\u2223 log nx,i \u2212 log(N \u2212 1)cdxrdxfX(x)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( nx,i > (N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2223\u2223\u03c1k,i = r, Zi = z ) + P ( nx,i < (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4\n\u2223\u2223\u03c1k,i = r, Zi = z ). (80) Given Zi = z and \u03c1k,i = r \u2208 [r2,1, r1], the probability distribution of nx,i is given in the following lemma:\nLemma 5. Given Zi = z = (x, y) and \u03c1k,i = r < rN for some deterministic sequence of rN such that limN\u2192\u221e rN = 0 and for any positive \u03b5 > 0, the number of neighbors nx,i \u2212 k is distributed as \u2211N\u22121 l=k+1 Ul, where Ul are i.i.d Bernoulli random variables with mean fX(x)cdxr dx(1\u2212\u03b5/8) \u2264 E[Ul] \u2264 fX(x)cdxrdx(1+\u03b5/8) for sufficiently large N .\nGiven lemma. 5, we obtain\nP ( nx,i > (N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2223\u2223\u2223\u03c1k,i = r, Zi = z ) = P\n( N\u22121\u2211 l=k+1 Ul > (N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2212 k )\n= P ( N\u22121\u2211 l=k+1 Ul \u2212 (N \u2212 k \u2212 1)E[Ul] > (N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Ul] ) , (81)\nand the right hand side in the probability is lower bounded by\n(N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2212 k \u2212 E[Ul] \u2265 (N \u2212 1)cdxrdxfX(x)e\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)fX(x)cdxrdx(1 + \u03b5/8) \u2265 (N \u2212 k \u2212 1)cdxrdxfX(x)(e\u03b5/4 \u2212 1\u2212 \u03b5/8)\u2212 k \u2265 (N \u2212 k \u2212 1)cdxrdxfX(x)\u03b5/16 (82)\nfor sufficiently large N such that (N \u2212k\u22121)cdxrdxfX(x)(e\u03b5/4\u22121\u2212 \u03b5/16) > k. Since Ul is bernoulli, we have\nE[U2l ] = E[Ul]. Now applying Bernstein\u2019s inequality, (81) is upper bounded by:\nP ( N\u22121\u2211 l=k+1 Ul \u2212 (N \u2212 k \u2212 1)E[Ul] > (N \u2212 1)cdxrdxfX(x)e\u03b5 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Ul] )\n\u2264 P ( N\u22121\u2211 l=k+1 Ul \u2212 (N \u2212 k \u2212 1)E[Ul] > (N \u2212 k \u2212 1)cdxrdxfX(x)\u03b5/16 ) \u2264 exp{\u2212 ((N \u2212 k \u2212 1)cdxr dxfX(x)\u03b5/16) 2\n2 (\n(N \u2212 k \u2212 1)E[U2l ] + 1 3 ((N \u2212 k \u2212 1)cdxrdxfX(x)\u03b5/16) )} \u2264 exp{\u2212 ((N \u2212 k \u2212 1)cdxr dxfX(x)\u03b5/16) 2\n2 ( (N \u2212 k \u2212 1)cdxrdxfX(x)(1 + \u03b5/8) + 13 ((N \u2212 k \u2212 1)cdxrdxfX(x)\u03b5/16) )}\n= exp{\u2212 \u03b5 2\n512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdxrdxfX(x)} . (83)\nSimilarly, the tail bound on the other direction is given by: P ( nx,i < (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2223\u2223\u2223\u03c1k,i = r, Zi = z ) = P\n( N\u22121\u2211 l=k+1 Ul < (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2212 k )\n= P ( N\u22121\u2211 l=k+1 Ul \u2212 (N \u2212 k \u2212 1)E[Ul] < (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Ul] ) , (84)\nwhere the right hand side is negative and upper bounded by:\n(N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2212 k \u2212 E[Ul] \u2264 (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)fX(x)cdxrdx(1\u2212 \u03b5/8) \u2264 (N \u2212 k \u2212 1)cdxrdxfX(x)(e\u2212\u03b5/4 \u2212 1 + \u03b5/8) \u2264 \u2212(N \u2212 k \u2212 1)cdxrdxfX(x)\u03b5/16 (85)\nfor small enough r such that cdxr dxfX(x)e \u2212\u03b5/4 \u2264 1 and small enough \u03b5 that e\u2212\u03b5/4 \u2212 1 + 3\u03b5/16 < 0. Similarly, (84) is upper bounded by:\nP ( N\u22121\u2211 l=k+1 Ul \u2212 (N \u2212 k \u2212 1)E[Ul] < (N \u2212 1)cdxrdxfX(x)e\u2212\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Ul] )\n\u2264 exp{\u2212 \u03b5 2\n512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdxrdxfX(x)}. (86)\nTherefore, I3(z) is upper bounded by:\nI3(z) = \u222b r1 r=r2 P ( \u2223\u2223 log f\u0302X(Xi)\u2212 log fX(Xi)\u2223\u2223 > \u03b5\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr\n\u2264 \u222b logN(Nf(z)cdx+dy )\u2212 1dx+dy r=(logN)2(NfX(x)cdx ) \u2212 1 dx P ( \u2223\u2223 log f\u0302X(Xi)\u2212 log fX(Xi)\u2223\u2223 > \u03b5\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr\n\u2264 \u222b logN(Nf(z)cdx+dy )\u2212 1dx+dy r=(logN)2(NfX(x)cdx ) \u2212 1 dx 2 exp{\u2212 \u03b5 2 512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdxrdxfX(x)}f\u03c1k,i(r)dr \u2264 2 exp{\u2212 \u03b5 2\n1024 NcdxfX(x)((logN)\n2(NfX(x)cdx) \u2212 1dx )dx}\n\u2264 2 exp{\u2212 \u03b5 2\n1024 (logN)2dx} (87)\nfor sufficiently large N such that (N \u2212 k \u2212 1)/(1 + 748\u03b5) > N/2.\nI4: Given that Zi = z = (x, y) and \u03c1k,i = r. Recall that f\u0302U (Yi) = ny,i\n(N\u22121)cdy r dy\n, then we have\nP ( \u2223\u2223 log f\u0302U (Yi)\u2212 log fU (Yi)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( \u2223\u2223 log ny,i \u2212 log(N \u2212 1)\u2212 log cdy \u2212 dy log \u03c1k,i \u2212 log fU (y)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( \u2223\u2223 log ny,i \u2212 log(N \u2212 1)cdyrdyfY (y)\u2223\u2223 > \u03b5/4\u2223\u2223\u03c1k,i = r, Zi = z )\n= P ( ny,i > (N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2223\u2223\u03c1k,i = r, Zi = z ) + P ( ny,i < (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4\n\u2223\u2223\u03c1k,i = r, Zi = z ). (88) Recall that\nny,i = \u2211 j 6=i w\u2032jI{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i} = \u2211 j 6=i u(Xi) fX(Xi) I{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i}. (89)\nWe write ny,i = m (1) y,i +m (2) y,i , where\nm (1) y,i = \u2211 j:\u2016Zj\u2212z\u2016<\u03c1k,i u(Xj) fX(Xj)\nm (2) y,i = \u2211 j:\u2016Zj\u2212z\u2016>\u03c1k,i u(Xj) fX(Xj) I{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i}. (90)\nSince C1/\u00b5(K) \u2264 fX(Xj) \u2264 C2/\u00b5(K), we have: k/C2 \u2264 m(1)y,i \u2264 k/C1. Given that Zi = z and \u03c1k,i = r \u2208 [r2,2, r1], the probability distribution of m (2) y,i is given by the following lemma:\nLemma 6. Given Zi = z = (x, y) and \u03c1k,i = r < rN for some deterministic sequence of rN such that limN\u2192\u221e rN = 0 and for a positive \u03b5 > 0, the distribution of m (2) y,i is distributed as \u2211N\u22121 l=k+1 Vl where Vl are i.i.d random variables with Vl \u2208 [0, 1/C1] and mean fU (y)cdyrdy (1 \u2212 \u03b5/8) \u2264 E[Vl] \u2264 fU (y)cdyrdy (1 + \u03b5/8), for sufficiently large N .\nGiven Lemma 6 and the fact that m (1) y,i \u2265 k/C2, we obtain\nP ( ny,i > (N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2223\u2223\u2223\u03c1k,i = r, Zi = z ) \u2264 P\n( N\u22121\u2211 l=k+1 Vl > (N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2212 k/C2 )\n= P ( N\u22121\u2211 l=k+1 Vl \u2212 (N \u2212 k \u2212 1)E[Vl] > (N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2212 k/C2 \u2212 (N \u2212 k \u2212 1)E[Vl] ) ; (91)\nhere the right hand side is lower bounded by\n(N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2212 k/C2 \u2212 E[Vl] \u2265 (N \u2212 1)cdyrdyfU (y)e\u03b5/4 \u2212 k/C2 \u2212 (N \u2212 k \u2212 1)cdyrdyfU (y)(1 + \u03b5/8) \u2265 (N \u2212 k \u2212 1)cdyrdyfU (y)(e\u03b5/4 \u2212 1\u2212 \u03b5/8)\u2212 k/C2 \u2265 (N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/16 (92)\nfor sufficiently large N such that (N \u2212k\u22121)cdyrdyfU (y)(e\u03b5/4\u22121\u2212\u03b5/16) > k/C2. Since Vl is upper bounded\nby 1/C1, so E[V 2l ] \u2264 E[Vl]/C1. Now applying Bernstein\u2019s inequality, (91) is upper bounded by:\nPr ( N\u22121\u2211 l=k+1 Vl \u2212 (N \u2212 k \u2212 1)E[Vl] > (N \u2212 1)cdyrdyfU (y)e\u03b5 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Vl] )\n\u2264 P ( N\u22121\u2211 l=k+1 Vl \u2212 (N \u2212 k \u2212 1)E[Vl] > (N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/16 ) \u2264 exp{\u2212 ((N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/16)2\n2 (\n(N \u2212 k \u2212 1)E[V 2l ] + 1 3C1 ((N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/8) )} \u2264 exp{\u2212 ((N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/16)2\n2 (\n(N \u2212 k \u2212 1)cdyrdyfU (y)(1 + \u03b5/8)/C1 + 13C1 ((N \u2212 k \u2212 1)cdyr dyfU (y)\u03b5/16) )} \u2264 exp{\u2212 C1\u03b5 2\n512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdyrdyfU (y)} . (93)\nSimilarly, since m (1) y,i < k/C1, the tail bound on the other way is given by:\nP ( ny,i < (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2223\u2223\u2223\u03c1k,i = r, Zi = z ) \u2264 P\n( N\u22121\u2211 l=k+1 Vl < (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2212 k/C1 )\n= P ( N\u22121\u2211 l=k+1 Vl \u2212 (N \u2212 k \u2212 1)E[Vl] < (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2212 k/C1 \u2212 (N \u2212 k \u2212 1)E[Vl] ) , (94)\nwhere the right hand side is negative and upper bounded by:\n(N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2212 k/C1 \u2212 E[Vl] \u2264 (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2212 k/C1 \u2212 (N \u2212 k \u2212 1)cdyrdyfU (y)(1\u2212 \u03b5/8) \u2264 (N \u2212 k \u2212 1)cdyrdyfU (y)(e\u2212\u03b5/4 \u2212 1 + \u03b5/8) \u2264 \u2212(N \u2212 k \u2212 1)cdyrdyfU (y)\u03b5/16 (95)\nfor small enough r such that cdyr dyfU (y)e \u2212\u03b5/4 \u2264 1/C1 and small enough \u03b5 that e\u2212\u03b5/4 \u2212 1 + 3\u03b5/16 < 0. Similarly, (94) is upper bounded by:\nP ( N\u22121\u2211 l=k+1 Vl \u2212 (N \u2212 k \u2212 1)E[Vl] < (N \u2212 1)cdyrdyfU (y)e\u2212\u03b5/4 \u2212 k \u2212 (N \u2212 k \u2212 1)E[Vl] )\n\u2264 exp{\u2212 C1\u03b5 2\n512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdyrdyfU (y)}. (96)\nTherefore, I4(z) is upper bounded by:\nI4(z) = \u222b r1 r=r2 P ( \u2223\u2223 log f\u0302U (Yi)\u2212 log fU (Yi)\u2223\u2223 > \u03b5\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr\n\u2264 \u222b logN(Nf(z)cdx+dy )\u2212 1dx+dy r=(logN)2(NfU (y)cdy ) \u2212 1 dy P ( \u2223\u2223 log f\u0302U (Yi)\u2212 log fU (Yi)\u2223\u2223 > \u03b5\u2223\u2223\u03c1k,i = r, Zi = z )f\u03c1k,i(r)dr\n\u2264 \u222b logN(Nf(z)cdx+dy )\u2212 1dx+dy r=(logN)2(NfU (y)cdy ) \u2212 1 dy exp{\u2212 C1\u03b5 2 512(1 + 7\u03b5/48) (N \u2212 k \u2212 1)cdyrdyfU (y)}f\u03c1k,i(r)dr \u2264 2 exp{\u2212C1\u03b5 2\n1024 NcdyfU (y)((logN)\n2(NfU (y)cdy ) \u2212 1dy )dy}\n\u2264 2 exp{\u2212C1\u03b5 2\n1024 (logN)2dy} (97)\nfor sufficiently large N such that (N \u2212 k \u2212 1)/(1 + 7\u03b5/48)) > N/2.\nNow combining (76), (79), (87) and (97), we obtain\nP ( N\u2211 i=1 w\u2032i N \u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5 ) \u2264 N \u222b\u222b (I1(z) + I2(z) + I3(z) + I4(z))f(z)dz\n\u2264 kNk exp{\u2212 (logN) dx+dy\n4 }+ (4C \u2032C2 cdx+dy min{cdx , cdy} )k(logN)2k(dx+dy)N 1\u2212kmin{ dydx , dy dy }\n+ 2N exp{\u2212 \u03b5 2 1024 (logN)2dx}+ 2N exp{\u2212C 2 1\u03b5 2 1024 (logN)2dy}. (98)\nIf k > max{dy/dx, dx/dy}, we have 1\u2212 kmin{dx+dydx , dx+dy dy } < 0. Then each of the four terms goes to 0 as N \u2192\u221e and we conclude:\nlim N\u2192\u221e P ( N\u2211 i=1 w\u2032i N \u2223\u2223( log f\u0302X(Xi) + log f\u0302U (Yi) )\u2212 ( log fX(Xi) + log fU (Yi) )\u2223\u2223 > \u03b5 ) = 0. (99) Therefore, by combining the convergence properties of error from kernel density estimation, error from selfnormalized importance sampling and error from density estimation, we obtain that I\u0302Uk,N (X,Y ) converges to IU (fY |X) in probability.\nA.1.4 Proof of Lemma 5\nGiven that Zi = z = (x, y) and \u03c1k,i = r, let {1, 2, . . . , i\u2212 1, i+ 1, . . . , N} = S \u222a {j} \u222a T be a partition of the indexes with \u2223\u2223S\u2223\u2223 = k \u2212 1 and \u2223\u2223T \u2223\u2223 = N \u2212 k \u2212 1. Then define an event AS,j,T associated to the partition as: AS,j,T = { \u2016Zs \u2212 z\u2016 < \u2016Zj \u2212 z\u2016,\u2200s \u2208 S, and \u2016Zt \u2212 z\u2016 > \u2016Zj \u2212 z\u2016,\u2200t \u2208 T } . (100)\nSince Zj \u2212 z are i.i.d. random variables, each event AS,j,T has identical probability. The number of such partitions is (N\u22121)!(N\u2212k\u22121)!(k\u22121)! , and thus P ( AS,j,T ) = (N\u2212k\u22121)!(k\u22121)!(N\u22121)! . So the cdf of nx,i is given by:\nP ( nx,i \u2264 k +m \u2223\u2223\u03c1k,i = r, Zi = z ) =\n\u2211 S,j,T P ( AS,j,T ) P ( nx,i \u2264 k +m \u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ) =\n(N \u2212 k \u2212 1)!(k \u2212 1)! (N \u2212 1)! \u2211 S,j,T P ( nx,i \u2264 k +m \u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ). (101) Now condition on event AS,j,T and \u03c1k,i = r, namely Zj is the k-nearest neighbor with distance r, S is the set of samples with distance smaller than r and T is the set of samples with distance greater than r. Recall that nx,i is the number of samples with \u2016Xj \u2212 x\u2016 < r. For any index s \u2208 S \u222a {j}, \u2016Xs \u2212 x\u2016 < r is satisfied. Therefore, nx,i \u2264 k + m means that there are no more than m samples in T with X-distance smaller than r. Let Ul = I{\u2016Xl \u2212 x\u2016 < r\n\u2223\u2223\u2016Zl \u2212 z\u2016 > r}.Therefore, P ( nx,i \u2264 k +m\n\u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ) = P\n( \u2211 t\u2208T I{\u2016Xt \u2212 x\u2016 < r} \u2264 m \u2223\u2223 \u2016Zs \u2212 z\u2016 < r,\u2200s \u2208 S, \u2016Zj \u2212 z\u2016 = r, \u2016Zt \u2212 z\u2016 > r,\u2200t \u2208 T,Zi = z )\n= P ( \u2211 t\u2208T I{\u2016Xt \u2212 x\u2016 < r} \u2264 m \u2223\u2223 \u2016Zt \u2212 z\u2016 > r,\u2200t \u2208 T )\n= P ( N\u22121\u2211 l=k+1 Ul \u2264 m ) . (102)\nWe can drop the conditions of Zs\u2019s for s 6\u2208 T since Zs and Xt are independent. Therefore, given that \u2016Zt \u2212 z\u2016 > r for all t \u2208 T , the variables I{\u2016Xt \u2212 x\u2016 < r} are i.i.d. and have the same distribution as Ul. Therefore, we have:\nP ( nx,i \u2264 k +m \u2223\u2223\u03c1k,i = r, Zi = z ) =\n(N \u2212 k \u2212 1)!(k \u2212 1)! (N \u2212 1)! \u2211 S,j,T P ( nx,i \u2264 k +m \u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ) =\n(N \u2212 k \u2212 1)!(k \u2212 1)! (N \u2212 1)! \u2211 S,j,T P ( N\u2211 l=k+1 Ul \u2264 m )\n= P ( N\u22121\u2211 l=k+1 Ul \u2264 m )\n(103)\nand so nx,i \u2212 k have the same distribution as \u2211N\u22121 l=k+1 Ul given Zi = z and \u03c1k,i = r. Here the mean of Ul is given by:\nE[Ul] = P ( \u2016Xl \u2212 x\u2016 < r \u2223\u2223 \u2016Zl \u2212 z\u2016 > r ) = P( \u2016Xl \u2212 x\u2016 < r, \u2016Zl \u2212 z\u2016 > r )P( \u2016Zl \u2212 z\u2016 > r ) = \u222b \u2016u\u2212x\u2016<r fX(u)du\u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016,r f(u, v)dudv\n1\u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016,r f(u, v)dudv\n. (104)\nSince \u2016H(fX)\u2016 \u2264 C almost everywhere, if r < rN and rN decays as N goes to infinity, for sufficiently large N , we have the following bound for E[Ul]:\nE[Ul] < \u222b \u2016u\u2212x\u2016<r fX(u)du\n= \u222b \u2016u\u2212x\u2016<r fX(x) + (u\u2212 x)\u2207fX(x) + (u\u2212 x)TH(fX)(x)(u\u2212 x) + o(\u2016u\u2212 x\u20162) < fX(x)cdxr dx(1 + Cr2) < fX(x)cdxr dx(1 + \u03b5/8) , (105)\nand\nE[Ul] > \u222b \u2016u\u2212x\u2016<r fX(u)du\u2212 \u222b \u2016(u,v)\u2212(x,y)\u2016,r f(u, v)dudv\n> fX(x)cdxr dx(1\u2212 Cr2)\u2212 f(x, y)cdx+dyrdx+dy (1 + Cr2) > fX(x)cdxr dx(1\u2212 \u03b5/8). (106)\nA.1.5 Proof of Lemma 6\nGiven that Zi = z = (x, y) and \u03c1k,i = r. Define AS,j,T as same as in Lemma 5. Let Vl = u(Xl)fX(Xl) I{\u2016Yl\u2212y\u2016 < r \u2223\u2223\u2016Zl \u2212 z\u2016 > r}. Condition on event AS,j,T , the cdf of m(2)y,i is given by:,\nP ( m\n(2) y,i \u2264 m \u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ) = P\n( \u2211 t\u2208T u(Xt) fX(Xt) I{\u2016Yt \u2212 y\u2016 < \u03c1k,i} \u2264 m \u2223\u2223 \u2016Zs \u2212 z\u2016 < r,\u2200s \u2208 S, \u2016Zj \u2212 z\u2016 = r, \u2016Zt \u2212 z\u2016 > r,\u2200t \u2208 T,Zi = z ) = P\n( \u2211 t\u2208T u(Xt) fX(Xt) I{\u2016Yt \u2212 y\u2016 < \u03c1k,i} \u2264 m \u2223\u2223 \u2016Zt \u2212 z\u2016 > r,\u2200t \u2208 T ) = P\n( N\u22121\u2211 l=k+1 Vl \u2264 m ) . (107)\nSimilarly we can drop the conditions of Zs\u2019s for s 6\u2208 T . Therefore, given that \u2016Zt \u2212 z\u2016 > r for all t \u2208 T , the variables u(Xt)fX(Xt) I{\u2016Yt \u2212 y\u2016 < r} are i.i.d. and have the same distribution as Vl. Therefore, we have:\nP ( m\n(2) y,i \u2264 m \u2223\u2223\u03c1k,i = r, Zi = z ) =\n(N \u2212 k \u2212 1)!(k \u2212 1)! (N \u2212 1)! \u2211 S,j,T P ( m (2) y,i \u2264 m \u2223\u2223AS,j,T , \u03c1k,i = r, Zi = z ) =\n(N \u2212 k \u2212 1)!(k \u2212 1)! (N \u2212 1)! \u2211 S,j,T P ( N\u22121\u2211 l=k+1 Vl \u2264 m )\n= P ( N\u22121\u2211 l=k+1 Vl \u2264 m )\n(108)\nsom (2) y,i have the same distribution as \u2211N\u22121 l=k+1 Vl given Zi = z and \u03c1k,i = r. Here Vl \u2264 supx u(x) fX(x) = 1/C1.\nThe mean of Vl is given by:\nE[Vl] = E [ u(Xl) fX(Xl) I{\u2016Yl \u2212 y\u2016 < r} \u2223\u2223 \u2016Zl \u2212 z\u2016 > r ]\n=\n\u222b\u222b \u2016v\u2212y\u2016<r u(u) fX(u) f(u, v)dudv \u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016<r u(u) fX(u) f(u, v)dudv\n1\u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016,r u(u) fX(u) f(u, v)dudv . (109)\nSince \u2016H(fU )\u2016 \u2264 C almost everywhere, if r < rN and rN decays as N goes to infinity, for sufficiently large N , we have the following bound for E[Vl]:\nE[Vl] < \u222b\u222b \u2016v\u2212y\u2016<r u(u) fX(u) f(u, v)dudv\n= \u222b \u2016v\u2212y\u2016<r fU (v)dy\n= \u222b \u2016v\u2212y\u2016<r fU (y) + (v \u2212 y)\u2207fU (y) + (v \u2212 y)TH(fU )(y)(v \u2212 y) + o(\u2016v \u2212 y\u20162) < fU (y)cdyr dy (1 + Cr2) < fU (y)cdyr dy (1 + \u03b5/8) (110)\nand\nE[Vl] > \u222b\u222b \u2016v\u2212y\u2016<r u(u) fX(u) f(u, v)dudv \u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016,r u(u) fX(u) f(u, v)dudv\n= \u222b \u2016v\u2212y\u2016<r fU (v)dy \u2212 \u222b\u222b \u2016(u,v)\u2212(x,y)\u2016,r fU (u, v)dudv > fU (y)cdyr dy (1\u2212 Cr2)\u2212 fU (x, y)cdx+dyrdx+dy (1 + Cr2) > fU (y)cdyr dy (1\u2212 \u03b5/8). (111)\nA.2 The case of discrete X Under Assumption 1, we prove a more general version of the theorem. Let (X1, Y1), . . . , (Xn, Yn) be i.i.d. samples drawn from some unknown prior pX(x) anb let qX(x) be some known distribution on X such that qX(x)/pX(x) \u2208 [C3, C4] for all x \u2208 X . Then define\nw(q)x \u2261 NqX(x)\nnx , (112)\nn (q) y,i \u2261 \u2211 j 6=i w (q) Xj I{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i}. (113)\nThe proposed estimator is:\nI\u0302 (q) k,N (X,Y ) \u2261\n1\nN N\u2211 i=1 w (q) Xi ( \u03c8(k) + log(N)\u2212 ( log(nXi) + log(n (q) y,i) ) ) . (114)\nWe claim that I\u0302 (q) k,N converges to the true value in probability, i.e.\nlim N\u2192\u221e\nP ( \u2223\u2223I\u0302(q)k,N (X,Y )\u2212 I(q)(fY |X)\u2223\u2223 > \u03b5 ) = 0 , (115)\nwhere\nI(q)(fY |X) \u2261 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x) fq(y) dy (116)\nand\nfq(y) \u2261 \u2211 x\u2032\u2208X qx(X)fY |X(y|x\u2032). (117)\nNotice that Theorem 1 is a special case when qX(x) is uniform. Define g(Xi, Yi) \u2261 \u03c8(k) + log(N)\u2212 ( log(nx,i) + log(n (q) y,i) )\n(118)\nsuch that I\u0302qk,N (X,Y ) = 1 N \u2211N i=1 w (q) Xi g(Xi, Yi). Define each quantity with the true prior pX(x) as\nw\u2032x \u2261 qX(x)\npX(x) , (119) n\u2032y,i \u2261 \u2211 j 6=i w\u2032Xj I{\u2016Yj \u2212 Yi\u2016 < \u03c1k,i} , (120)\ng\u2032(Xi, Yi) \u2261 \u03c8(k) + log(N)\u2212 ( log(nXi) + log(n \u2032 y,i) ) . (121)\nWe apply triangular inequality, and show that each term converges to zero in probability.\u2223\u2223I\u0302(q)k,N (X,Y )\u2212 I(q)(fY |X)\u2223\u2223 = \u2223\u2223\u2223 1 N N\u2211 i=1 w (q) Xi g(Xi, Yi)\u2212 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x)\u2211 x\u2032\u2208X qX(x)fY |X(y|x\u2032) dy \u2223\u2223\u2223\n\u2264 1 N \u2223\u2223\u2223 N\u2211 i=1 ( w (q) Xi g(Xi, Yi)\u2212 w\u2032Xig \u2032(Xi, Yi) ) \u2223\u2223\u2223 (122)\n+ \u2223\u2223\u2223 1 N N\u2211 i=1 w\u2032Xig \u2032(Xi, Yi)\u2212 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x)\u2211 x\u2032\u2208X qX(x)fY |X(y|x\u2032) dy \u2223\u2223\u2223. (123)\nThe first term (122) captures the error in estimating pX(x). Similar as in (188), the probability that it deviates from 0 is is upper bounded by:\nP ( 1 N \u2223\u2223 N\u2211 i=1 ( w (q) Xi g(Xi, Yi)\u2212 w\u2032Xig \u2032(Xi, Yi) ) \u2223\u2223 > \u03b5) \u2264 P( max x\u2208X |w(q)x \u2212 w\u2032x| > \u03b5/(3 logN) ) (124)\nfor sufficiently large N . Recall that wx = NqX(x)/nx, (124) is bounded by: P (\nmax x\u2208X |w(q)x \u2212 w\u2032x| > \u03b5/(3 logN) ) = P ( max x\u2208X \u2223\u2223\u2223NqX(x) nx \u2212 qX(x) pX(x)\n\u2223\u2223\u2223 > \u03b5/(3 logN)) = P ( \u2200x \u2208 X , nx 6\u2208 [\nNqX(x) qX(x) pX(x) + \u03b53 logN , NqX(x) qX(x) pX(x) \u03b5 3 logN\n] )\n\u2264 P ( \u2200x \u2208 X , nx 6\u2208 [NpX(x)(1\u2212\n\u03b5pX(x)\n6 logNqX(x) ), Npx(1 +\n\u03b5pX(x) 6 logNqX(x) )] )\n(125)\nfor sufficiently large N such that \u03b5px3 logNqX(x) < 1/3. Recall that for each x \u2208 X , nx = \u2211N i=1 I{Xi = x}. Therefore, nx is a binomial random variable with parameter (N, pX(x)). Therefore, by Hoeffding\u2019s inequality, for any x \u2208 X , we have:\nP ( |nx \u2212NpX(x)| \u2264 N\u03b5p2X(x)\n6 logNqX(x)\n) \u2264 2 exp{\u2212 1\n2N (\nN\u03b5p2X(x)\n6 logNqX(x) )2} \u2264 2 exp{\u2212 N\u03b5 2C21 72|X |2C24 (logN)2 } (126)\nwhere the last inequality comes from the assumption that pX(x) > C1/|X | and qX(x)/pX(x) < C4. Then by union bound, (125) is upper bounded by:\nP ( \u2200x \u2208 X , nx 6\u2208 [NpX(x)(1\u2212\n\u03b5pX(x)\n6 logNqX(x) ), Npx(1 +\n\u03b5pX(x) 6 logNqX(x) )] )\n\u2264 |X |max x\u2208X\nP ( |nx \u2212NpX(x)| \u2264 N\u03b5p2X(x)\n6 logNqX(x) ) \u2264 2|X | exp{\u2212 N\u03b5\n2C21 72|X |2C24 logN }. (127)\nCombining with (124), we know that (122) converges to 0 in probability. The second term in the error (123) comes from the sample noise in density estimation. we decompose our estimator into three terms:\n1\nN N\u2211 i=1 w\u2032Xig \u2032(Xi, Yi) = H\u0302 q k,N (Y )\u2212 H\u0302 q k,N (Y |X)\u2212 N\u2211 i=1 w\u2032Xi N (log(N \u2212 1)\u2212 logN + log(nXi)\u2212 \u03c8(nXi)) ,\nwhere\nH\u0302qk,N (Y |X) \u2261 N\u2211 i=1 w\u2032Xi N ( \u2212 \u03c8(k) + \u03c8(nXi) + log cdy + dy log \u03c1k,i ) , (128)\nH\u0302qk,N (Y ) \u2261 N\u2211 i=1 w\u2032Xi N ( \u2212 log ny,i + log(N \u2212 1) + log cdy + dy log \u03c1k,i ) . (129)\nNotice that \u2211N i=1 w\u2032Xi N (log(N \u2212 1) \u2212 logN + log(nXi) \u2212 \u03c8(nXi)) converges to 0 in probability as N goes to infinity. The desired claim follows directly from the following two lemmas showing the convergence each entropy estimates to corresponding conditional entropy Hq(Y |X) and entropy Hq(Y ). The desired claim immediately follows the two lemmas.\nLemma 7. Under the hypotheses of Theorem 1, for all \u03b5 > 0\nlim N\u2192\u221e\nP (\u2223\u2223\u2223H\u0302qk,N (Y |X)\u2212 ( \u2212\u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x)dy )\u2223\u2223\u2223 > \u03b5) = 0 . (130) Lemma 8. Under the hypotheses of Theorem 1, for all \u03b5 > 0\nlim N\u2192\u221e\nP (\u2223\u2223\u2223H\u0302qk,N (Y )\u2212 ( \u2212\u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fq(y) ) dy )\u2223\u2223\u2223 > \u03b5) = 0 , (131)\nwhere fq(y) = \u2211 x\u2208X qX(x)fY |X(y|x).\nA.2.1 Proof of Lemma 7\nDefine\nf\u0302Y |X(Yi|Xi) = exp{\u03c8(k)\u2212 \u03c8(nXi)}\ncdy\u03c1 dy k,i\n, (132)\nso that\nH\u0302qk,N (Y |X) = \u2212 N\u2211 i=1 w\u2032Xi N log f\u0302Y |X(Xi, Yi) , (133)\nNotice that f\u0302Y |X(Yi|Xi) is just the k-nearest neighbour density estimator for the conditional pdf fY |X(y|x). Therefore, by Theorem 8. [SMH+03], we have\nlim N\u2192\u221e\nE [ log f\u0302Y |X(Yi|Xi) \u2223\u2223(Xi, Yi) = (x, y)] = log fY |X(y|x) , (134)\nNotice that w\u2032Xi log f\u0302(Yi|Xi) are identically distributed, therefore, we have\nlim N\u2192\u221e\nEH\u0302Uk,N (X,Y )\n= \u2212 lim N\u2192\u221e E[w\u2032Xi log f\u0302X,Y (Xi, Yi)]\n= \u2212 lim N\u2192\u221e \u2211 x\u2208X qX(x) pX(x) pX(x) ( \u222b E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]fY |X(y|x)dy )\n= \u2212 lim N\u2192\u221e \u2211 x\u2208X qX(x) ( \u222b E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]fY |X(y|x)dy ) (135)\nUse the same technique in the proof of Lemma 2 and Equation (51), we can switch the order of limit and integration. Therefore,\nlim N\u2192\u221e \u2211 x\u2208X qX(x) ( \u222b E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]fY |X(y|x)dy )\n= \u2211 x\u2208X qX(x) ( \u222b lim N\u2192\u221e E [ log f\u0302X,Y (Xi, Yi) \u2223\u2223(Xi, Yi) = (x, y)]fY |X(y|x)dy )\n= \u2211 x\u2208X qX(x) \u222b logY |X(y|x)fY |X(y|x)dy , (136)\nTherefore,\nlim N\u2192\u221e EH\u0302qk,N (Y |X) = \u2212 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x)dy. (137)\nMoreover, by Theorem11. [SMH+03], we have:\nlim N\u2192\u221e\nVar[f\u0302Y |X(Yi|Xi)] = ( \u0393\u2032(k)\n\u0393(k) )\u2032Var[log fY |X(y|x)] <\u221e , (138)\nand for any j 6= i:\nlim N\u2192\u221e\nCov[f\u0302Y |X(Yi|Xi), f\u0302Y |X(Yj |Yi)] = 0. (139)\nSince w\u2032x \u2264 C4 for all x, similarly as in Lemma 2, we obtain\nlim N\u2192\u221e\nVar [ H\u0302qk,N (Y |X) ] = 0 , (140)\nCombining (137) and (140), we know H\u0302qk,N (Y |X) converges to its mean in L2, hence in probability, i.e.,\nlim N\u2192\u221e\nP ( \u2223\u2223H\u0302qk,N (X,Y )\u2212 ( \u2212\u2211\nx\u2208X qX(x)\n\u222b logY |X(y|x)fY |X(y|x)dy )\u2223\u2223 > \u03b5) = 0. (141)\nA.2.2 Proof of Lemma 8\nDefine\nf\u0302q(Yi) \u2261 ny,i\n(N \u2212 1)cdy\u03c1 dy k,i\n, (142)\nsuch that\nH\u0302qk,N (Y ) = \u2212 N\u2211 i=1 w\u2032Xi N log f\u0302q(Yi). (143)\nBy triangle inequality, we can write the formula in Lemma 8 as:\u2223\u2223H\u0302qk,N (Y )\u2212 ( \u2212\u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fq(y) ) dy )\u2223\u2223\n= \u2223\u2223 N\u2211 i=1 w\u2032Xi N log f\u0302q(Yi)\u2212 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fq(y) ) dy \u2223\u2223\n\u2264 \u2223\u2223 N\u2211 i=1 w\u2032Xi N log fq(Yi)\u2212 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fq(y) ) dy \u2223\u2223 (144)\n+ N\u2211 i=1 w\u2032i N \u2223\u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223\u2223. (145) The first term comes (144) from sampling. Recall that w\u2032Xi = qX(Xi)/pX(Xi). Therefore by strong law\nof large numbers,\nN\u2211 i=1 w\u2032Xi N log fq(Yi)\u2192 E ( qX(x) pX(x) log fq(y) )\n(146)\nalmost surely. The mean is given by E ( qX(x) pX(x) log fq(y) ) = \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fq(y)dy. (147)\nTherefore, (144) converges to 0 almost surely.\nThe second term (145) comes from density estimation. For any fixed \u03b5 > 0, by union bound, we obtain that\nP ( N\u2211 i=1 w\u2032Xi N \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5 ) \u2264 P\n( N\u22c3 i=1 {\u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223\u2223\u2223\u2223 > \u03b5/2} )+ P( N\u2211 i=1 w\u2032Xi > 2N). (148)\nThe second term converges to zero by the law of large numbers. The first term is bounded by:\nP ( N\u22c3 i=1 {\u2223\u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223\u2223\u2223\u2223 > \u03b5/2} ) (149) \u2264 N \u00b7 P\n( \u2223\u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223\u2223\u2223\u2223 > \u03b5/2 ) = N\n\u2211 x\u2208X pX(x) \u222b P ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223\u2223\u2223 > \u03b5/2\u2223\u2223\u2223(Xi, Yi) = (x, y) )\ufe38 \ufe37\ufe37 \ufe38\n\u2264I1(x,y)+I2(x,y)+I3(x,y)\nfY |X(y|x)dy (150)\nwhere\nI1(x, y) = P ( \u03c1k,i > (N 1/2pX(x)fY |X(y|x)cdy )\u22121/dy \u2223\u2223Xi = x, Yi = y ) (151)\nI2(x, y) = P ( \u03c1k,i < (logN) 1+\u03b4/2(Nfq(y)cdy ) \u22121/dy \u2223\u2223Xi = x, Yi = y ) (152) I3(x, y) =\n\u222b r1 r=r2 P ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5/2\u2223\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )f\u03c1k,i(r)dr (153)\nwhere f\u03c1k,i(r) is the pdf of \u03c1k,i given Xi and Yi. Here r1 = (N 1/2pX(x)fY |X(y|x)cdy ) \u2212 1dy and r2 = (logN)1+\u03b4/2(Nfq(y)cdy ) \u2212 1dy . We will consider the three terms separately.\nI1(z): Let B(x, y, r) = {(X,Y ) : \u2016Y \u2212 y\u2016 < r,X = x} be the dy-dimensional ball centered at y with radius r with same x. Since the Hessian matrix of H(fY |X) exists and \u2016H(fY |X)\u20162 < C almost everywhere for any x \u2208 X , then for sufficiently small r, the probability mass within B(x, y, r) is given by\nP ( (u, v) \u2208 B(x, y, r) )\n= pX(x) \u222b \u2016v\u2212y\u2016\u2264r fY |X(v)dv\n= pX(x) \u222b \u2016v\u2212y\u2016\u2264r fY |X(y) + (v \u2212 y)T\u2207fY |X(y) + (v \u2212 y)TH(fY |X)(y)(v \u2212 y) + o(\u2016v \u2212 y\u20162)du\n\u2208 [ pX(x)fY |X(y|x)cdyrdy (1\u2212 Cr2)), pX(x)fY |X(y|x)cdyrdy (1 + Cr2)) ] . (154)\nThen for sufficiently large N , the probability mass within B(x, y, r1) is lower bounded by p1 \u2261 P ( (u, v) \u2208 B(x, y, (N1/2pX(x)fY |X(y|x)cdy )\u22121/dy ) )\n\u2265 pX(x)fY |X(y|x)cdy ( (N1/2pX(x)fY |X(y|x)cdy )\u22121/dy )dy( 1\u2212 C((N1/2pX(x)fY |X(y|x)cdy ) \u2212 1dy )2 ) \u2265 1\n2 N\u22121/2. (155)\nI1(z) is the probability that at most k samples fall in BZ(z, r1), so it is upper bounded by I1(z) = P ( \u03c1k,i > r1 \u2223\u2223Zi = z ) =\nk\u22121\u2211 m=0 ( N \u2212 1 m ) pm1 (1\u2212 p1)N\u22121\u2212m\n\u2264 k\u22121\u2211 m=0 Nm(1\u2212 p1)N\u22121\u2212m \u2264 kNk\u22121(1\u2212 1 2 \u221a N )N\u2212k\u22121 \u2264 kNk\u22121 exp{\u2212N \u2212 k \u2212 1 2 \u221a N } (156)\nfor any dx, dy \u2265 1.\nI2: Let r2 = (logN) 1+\u03b4/2(Nfq(y)cdy ) \u22121/dy . Then for sufficiently large N , the probability mass within B(x, y, r2) is given by:\np2 \u2261 P ( u \u2208 B(x, y, (logN)1+\u03b4/2(Nfq(y)cdy )\u22121/dy ) ) \u2264 pX(x)fY |X(y|x)cdy ( (logN)1+\u03b4/2(Nfq(y)cdy ) \u22121/dy )dy( 1 + C((logN)1+\u03b4/2(Nfq(y)cdy ) \u22121/dy )2\n) \u2264\n2pX(x)fY |X(y|x) fq(y) (logN)(1+\u03b4/2)dyN\u22121\n\u2264 2pX(x)fY |X(y|x)\u2211 x\u2208X qX(x)fY |X(y|x) (logN)(1+\u03b4/2)dyN\u22121 \u2264 2 C3|X |N (logN)(1+\u03b4/2)dy (157)\nwhere the last equation comes from the assumption that qX(x)/pX(x) > C3. I2(z) is the probability that at least k samples lying in B(x, y, r2). Therefore, it is upper bounded by\nI2(z) = P ( \u03c1k,i < r2 \u2223\u2223Zi = z ) =\nN\u22121\u2211 m=k ( N \u2212 1 m ) pm2 (1\u2212 p2)N\u22121\u2212m\n\u2264 N\u22121\u2211 m=k Nmpm2 m!\n\u2264 N\u22121\u2211 m=k Nmpm2 (m/e)m\n\u2264 N\u22121\u2211 m=k ( Nep2 k )m\n\u2264 N\u22121\u2211 m=k ( 2e C3|X | (logN)(1+\u03b4/2)dy/k)m. (158)\nHere we use the fact that m! > (m/e)m for all m. Since k > (logN)(1+\u03b4)dy by assumption, (logN)(1+\u03b4/2)dy/k is decreasing as N increases. For sufficiently large N such that 2eC1|X | (logN) (1+\u03b4/2)dy/k < 1/2, we obtain:\nI2(z) \u2264 2( 2e\nC3|X | (logN)(1+\u03b4/2)dy/k)k\n\u2264 2( 2e C3|X | )(logN) (1+\u03b4)dy (logN)\u2212\u03b4(logN) (1+\u03b4)dy/2. (159)\nI3: Given that (Xi, Yi) = (x, y) and \u03c1k,i = r. Recall that f\u0302q(Yi) = ny,i\n(N\u22121)cdy r dy\n, then we have\nP ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )\n= P ( \u2223\u2223 log ny,i \u2212 log(N \u2212 1)\u2212 log cdy \u2212 dy log \u03c1k,i \u2212 log fq(y)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )\n= P ( \u2223\u2223 log ny,i \u2212 log(N \u2212 1)cdyrdyfq(y)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )\n= P ( ny,i > (N \u2212 1)cdyrdyfq(y)e\u03b5/2 \u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) ) + P ( ny,i < (N \u2212 1)cdyrdyfq(y)e\u2212\u03b5/2\n\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) ). (160) Following a similar technique as the analysis of I4 in proof of Lemma 3, we obtain\nP ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )\n\u2264 2 exp{\u2212 C3\u03b5 2\n128(1 + 7\u03b5/24) (N \u2212 k \u2212 1)cdyrdyfq(y)} (161)\nwhere C3 is the lower bound of qX(x)/pX(x). Therefore, I3(x, y) is upper bounded by:\nI3(x, y) = \u222b r1 r=r2 P ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )f\u03c1k,i(r)dr\n\u2264 \u222b (N1/2pX(x)fY |X(y|x)cdy )\u22121/dy r=(logN)1+\u03b4/2(Nfq(y)cdy ) \u22121/dy P ( \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5/2\u2223\u2223\u03c1k,i = r, (Xi, Yi) = (x, y) )f\u03c1k,i(r)dr\n\u2264 \u222b (N1/2pX(x)fY |X(y|x)cdy )\u22121/dy r=(logN)1+\u03b4/2(Nfq(y)cdy ) \u22121/dy 2 exp{\u2212 C3\u03b5 2 128(1 + 7\u03b5/24) (N \u2212 k \u2212 1)cdyrdyfq(y)}f\u03c1k,i(r)dr \u2264 2 exp{\u2212C3\u03b5 2\n256 Ncdyfq(y)((logN)\n1+\u03b4/2(Nfq(y)cdy ) \u2212 1dy )dy}\n\u2264 2 exp{\u2212C3\u03b5 2\n256 (logN)(1+\u03b4/2)dy} (162)\nfor sufficiently large N such that (N \u2212 k \u2212 1)/(1 + 7\u03b5/24)) > N/2.\nNow combine (156), (159), and (162), and we obtain\nP ( N\u2211 i=1 w\u2032i N \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5 ) \u2264 N\n\u2211 x\u2208X pX(x) \u222b (I1(x, y) + I2(x, y) + I3(x, y))dy\n\u2264 kNk exp{\u2212N \u2212 k \u2212 1 2 \u221a N }+ 2N exp{\u2212C3\u03b5 2 256 (logN)(1+\u03b4/2)dy} + 2N( 2e\nC3|X | )(logN)\n(1+\u03b4)dy (logN)\u2212\u03b4(logN) (1+\u03b4)dy/2. (163)\nOne can easily see that the first and second terms converges to 0 as N goes to infinity, given that k <\u221a N/(5 logN). To see that the last term converges to 0, we will show that the logarithm goes to \u2212\u221e as N goes to infinity, which is\nlog(N( 2e\nC3|X | )(logN)\n(1+\u03b4)dy (logN)\u2212\u03b4(logN) (1+\u03b4)dy/2)\n= logN + log( 2e\nC3|X | )(logN)(1+\u03b4)dy \u2212 logN\u03b4(logN)(1+\u03b4)dy/2\n= logN + log( 2e C3|X | )(logN)(1+\u03b4)dy \u2212 \u03b4 2 (logN)(1+\u03b4)dy+1. (164)\nThe negative term has the larger exponent, so the logarithm will goes to \u2212\u221e, and we have\nlim N\u2192\u221e P ( N\u2211 i=1 w\u2032Xi N \u2223\u2223 log f\u0302q(Yi)\u2212 log fq(Yi)\u2223\u2223 > \u03b5 ) = 0. (165) Therefore, by combining the convergence of error from sampling and error from density estimation, we obtain that I\u0302 (q) k,N (X,Y ) converges to I (q)(fY |X) in probability.\nB Proof of the CMI estimator convergence\nAssumption 2. We make the following assumptions: (a) \u222b fY |X(y|x) \u2223\u2223 log fY |X(y|x)\u2223\u2223dy <\u221e, for all x \u2208 X . (b) \u222b fY |X(y|x) ( log fY |X(y|x) )2 dy <\u221e, for all x \u2208 X .\n(c) There exists a finite constant C such that the Hessian matrix of H(fY |X) exists and \u2016H(fY |X)\u20162 < C almost everywhere, for all x \u2208 X .\n(d) There exists a finite constant C \u2032 such that the conditional pdf fY |X(y \u2223\u2223x) < C \u2032 almost everywhere, for\nall x \u2208 X .\n(e) There exists finite constants C1 < C3 < C4 < C2 such that the ratio of the optimal prior q \u2217 of the\nmaximizer in the definition of C(fY |X) and the true prior satisfies that q\u2217X(x)/pX(x) \u2208 [C3, C4] for every x \u2208 X .\n(e) There exists finite constants C5 < C6 such that pX(x) > C5/|X | and pX(x) < C6/|X |, for all x \u2208 X .\nDefine\nI(fY |X)(qX) \u2261 \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x)\u2211 x\u2032\u2208X qX(x \u2032)fY |X(y|x\u2032) dy (166)\nand\nI\u0302k,N (X,Y )(w) \u2261 1\nN N\u2211 i=1 wXi ( \u03c8(k) + log(N)\u2212 ( log(nXi) + log(ny,i) ) ) (167)\nsuch that C(fY |X) = maxqX\u2208Q I(fY |X)(qX) and C\u0302 \u2206 k,N (X,Y ) = maxw\u2208T\u2206 I\u0302k,N (X,Y )(w). First, consider the quantity:\nC\u2206(fY |X) \u2261 max qX\u2208T\u2206(Q) I(fY |X)(qX) (168)\nwhere the constraint set T\u2206(Q) is defined as: T\u2206(Q) = {qX \u2208 R|X | : [(qX(x)/pX(x))] \u2208 T\u2206 and \u2211 x\u2208X qX(x) \u2208 [1\u2212 |X |\u2206, 1 + |X |\u2206]} (169)\nwe rewrite the error term in Theorem 23 as\u2223\u2223C\u0302\u2206k,N (X,Y )\u2212 C(fY |X)\u2223\u2223 \u2264 \u2223\u2223C\u2206(fY |X)\u2212 C(fY |X)\u2223\u2223+ \u2223\u2223C\u0302\u2206k,N \u2212 C\u2206(fY |X)\u2223\u2223. (170) The first error comes from quantization. Let q\u2217 be the maximizer of C(fY |X). By assumption, q\u2217(x)/pX(x) \u2208 [C3, C4] \u2286 [C1, C2], for all x. Since T\u2206(Q) is a quantization of the simplex Q, so there exists a q0 \u2208 T\u2206(Q) such that |q0(x) \u2212 q\u2217(x)| < \u2206 \u00b7 pX(x) < \u2206 for all x \u2208 X . Now we will bound the difference of I(fY |X)(q0) and I(fY |X)(q \u2217) by the following lemma:\nLemma 9. Under the assumptions of Theorem 23, if q(x)/p(x) \u2208 [C1, C2] and q\u2032(x)/p(x) \u2208 [C1, C2] for all x \u2208 X , then \u2223\u2223 I(fY |X)(q)\u2212 I(fY |X)(q\u2032) \u2223\u2223 \u2264 Lmax\nx\u2208X |q(x)\u2212 q\u2032(x)| , (171)\nfor some positive constant L.\nThen we have:\nC(fY |X) = I(fY |X)(q \u2217)\n\u2264 I(fY |X)(q0) + Lmax x\u2208X |q0(x)\u2212 q\u2217(x)| \u2264 max q\u2208T\u2206(Q) I(fY |X)(q) + L\u2206 = C\u2206(fY |X) + L\u2206. (172)\nSimilarly, let q\u2217\u2217 be the maximizer of C\u2206(fY |X), we can also find a q1 \u2208 Q such that |q1(x)\u2212q\u2217\u2217(x)| < \u2206 for all x \u2208 X . Using Lemma 9 again, we will obtain C\u2206(fY |X) \u2264 C(fY |X) + L\u2206. Therefore, the first term in (170) is bounded by O(\u2206).\nNow consider the second term. Upper bound on the second term relies on the convergence of dicrete UMI estimation from Theorem . Recall that in the proof of Theorem 1, we have shown that under certain conditions,\nP ( \u2223\u2223I\u0302k,N (X,Y )(wq)\u2212 I(fY |X)(q)\u2223\u2223 > \u03b5/2 ) N\u2192\u221e\u2212\u2192 0 (173)\nfor any q with bounded qX/pX . Here (wq)x = q(x)/pX(x). Since the set T\u2206(Q) is finite, by union bound, we have:\nlim N\u2192\u221e\nP ( \u2200q \u2208 T\u2206(Q), \u2223\u2223I\u0302k,N (X,Y )(wq)\u2212 I(fY |X)(q)\u2223\u2223 \u2264 \u03b5/2 ) \u2265 1\u2212 |T\u2206(Q)| lim N\u2192\u221e P ( \u2223\u2223I\u0302k,N (X,Y )(wq)\u2212 I(fY |X)(q)\u2223\u2223 \u2264 \u03b5/2 ) = 1. (174)\nAlso, by the strong law of large numbers, we have that\nlim N\u2192\u221e\nP ( \u2200x \u2208 X , |px(X)\u2212 nX/N | < \u2206/C2|X | ) = 1. (175)\nWe claim that if the events inside the probability in (174) and (175) happen simultaneously, then \u2223\u2223C\u0302\u2206k,N \u2212\nC\u2206(fY |X) \u2223\u2223 < \u03b5+O(\u2206), which implies the desired claim.\nLet w\u2217 = arg maxw\u2208T\u2206 I\u0302k,N (X,Y )(w). Define q2(x) = w \u2217 xpX(x). Since [q2(x)/pX(x)] \u2208 T\u2206 for all x and\u2223\u2223 \u2211\nx\u2208X q2(x)\u2212 1 \u2223\u2223 = \u2223\u2223 \u2211 x\u2208X w\u2217x(pX(x)\u2212 nx/N) + (\u2206/2)|X | \u2223\u2223\n\u2264 |X | ((\u2206/2) + C2 max x\u2208X \u2223\u2223 pX(x)\u2212 nx/N \u2223\u2223) \u2264 (|X |/2 + 1)\u2206 . (176)\nTherefore, q2 \u2208 T\u2206(Q), so\nC\u0302\u2206k,N = I\u0302k,N (X,Y )(w\u2217) \u2264 I(fY |X)(q2) + \u03b5 \u2264 C\u2206(fY |X) + \u03b5. (177)\nOn the other hand, consider q\u2217\u2217 = arg maxqX\u2208T\u2206(Q) I(fY |X)(qX) again, and define (w0)x = q \u2217\u2217(x)/pX(x). We know that (w0) \u2208 T |X |\u2206 but not necessarily \u2211N i=1(w0)Xi = N . But we claim that the sum is closed to N as follows\n\u2223\u2223 N\u2211 i=1 (w0)Xi \u2212N \u2223\u2223 = \u2223\u2223 \u2211 x\u2208X nxq \u2217\u2217(x) pX(x) \u2212N |\n\u2264 N max x\u2208X { q\u2217\u2217(x) pX(x) \u2223\u2223 nx N \u2212 pX(x) \u2223\u2223 } \u2264 NC2 \u2206\nC2|X | < N\u2206 (178)\nso we can find a (w1) \u2208 T\u2206(W ) such that |(w1)x \u2212 (w0)x| \u2264 \u2206 for all x. Let q4(x) = (w1)xpX(x), similar as (176), we know that q4 \u2208 T\u2206(Q). Moreover, \u2223\u2223 q4(x) \u2212 q\u2217\u2217(x) \u2223\u2223 \u2264 pX(x)\u2223\u2223 (w1)x \u2212 (w0)x \u2223\u2223 \u2264 \u2206 for all x. Then we have\nC\u2206(fY |X) = I(fY |X)(q \u2217\u2217)\n\u2264 I(fY |X)(q4) + Lmax x\u2208X |q\u2217\u2217(x)\u2212 q4(x)|\n\u2264 I\u0302k,N (X,Y )(w1) + \u03b5+ L\u2206 = C\u0302\u2206k,N + \u03b5+ L\u2206. (179)\nTherefore, we have |C\u0302\u2206k,N \u2212 C(fY |X)| < \u03b5+O(\u2206), thus our proof is complete.\nB.1 Proof of Lemma 9\nWe will show that for any x \u2208 X , we have | \u2202\u2202qX(x)I(fY |X)(q)| \u2264 L/|X | for some L. Therefore,\u2223\u2223 I(fY |X)(q)\u2212 I(fY |X)(q\u2032) \u2223\u2223 \u2264 \u2211 x\u2208X | \u2202I(fY |X)(q) \u2202qX(x) | |qX(x)\u2212 q\u2032X(x)|\n\u2264 Lmax x\u2208X |qX(x)\u2212 q\u2032X(x)| (180)\nLet fq(y) = \u2211 x\u2208X qX(x)fY |X(y|x). Since qX(x) \u2208 [C1pX(x), C2pX(x)] \u2286 [C1C5/|X |, C2C6/|X |] we\nknow that\nfq(y) \u2208 [C1C5 min x\u2208X fY |X(y|x), C2C6 max x\u2208X fY |X(y|x)] (181)\nfor all x, y. Therefore, the absolute value of the gradient is bounded by\u2223\u2223\u2223 \u2202I(fY |X)(q) \u2202qX(x) \u2223\u2223\u2223 =\n\u2223\u2223\u2223 \u2202 \u2202qX(x) ( \u2211 x\u2208X qX(x) \u222b fY |X(y|x) log fY |X(y|x) fq(y) dy )\u2223\u2223\u2223\n= \u2223\u2223\u2223 \u222b fY |X(y|x) log fY |X(y|x) fq(y) dy \u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211\nx\u2032\u2208X qX(x\n\u2032)\n\u222b fY |X(y|x\u2032)fY |X(y|x) fq(y) dy \u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223max\ny log fY |X(y|x) fq(y) \u2223\u2223\u2223+ \u2223\u2223\u2223max y fY |X(y|x) fq(y) \u2223\u2223\u2223 \u2264 max{| logC1C5|, | logC2C6|}+ 1/(C1C5) (182)\nwhere L = |X |max{| logC1C5|, | logC2C6|}.\nC Proof of Lemma 1\nThe term in Equation (34) is upper bounded by:\n1\nN \u2223\u2223\u2223 N\u2211 i=1 ( wig(Xi, Yi)\u2212 w\u2032ig\u2032(Xi, Yi) ) \u2223\u2223\u2223 \u2264 1\nN N\u2211 i=1 \u2223\u2223wig(Xi, Yi)\u2212 w\u2032ig\u2032(Xi, Yi)\u2223\u2223 \u2264 1\nN N\u2211 i=1 ( |wi \u2212 w\u2032i| |g\u2032(Xi, Yi)|+ wi \u2223\u2223g(Xi, Yi)\u2212 g\u2032(Xi, Yi)\u2223\u2223) = 1\nN N\u2211 i=1 ( |wi \u2212 w\u2032i| |g\u2032(Xi, Yi)|+ wi \u2223\u2223 log(ny,i)\u2212 log(n\u2032y,i)\u2223\u2223) \u2264 1\nN N\u2211 i=1 ( |wi \u2212 w\u2032i| |g\u2032(Xi, Yi)|+ wi \u2223\u2223ny,i \u2212 n\u2032y,i\u2223\u2223( 12ny,i + 12n\u2032y,i ) )\n\u2264 1 N N\u2211 i=1 |wi \u2212 w\u2032i| |g\u2032(Xi, Yi)|+ N\u2211 i=1 wi N ( \u2211 j 6=i I{\u2016Yj \u2212 Yi| < \u03c1k,i}|w\u2032j \u2212 wj | 2 \u2211 j 6=i I{\u2016Yj \u2212 Yi| < \u03c1k,i}w\u2032j + \u2211 j 6=i I{\u2016Yj \u2212 Yi| < \u03c1k,i}|w\u2032j \u2212 wj | 2 \u2211 j 6=i I{\u2016Yj \u2212 Yi| < \u03c1k,i}wj ) \u2264 1\nN N\u2211 i=1 |wi \u2212 w\u2032i| |g\u2032(Xi, Yi)|+ N\u2211 i=1 wi N ( max1\u2264j\u2264N |w\u2032j \u2212 wj | 2 min1\u2264j\u2264N w\u2032j + max1\u2264j\u2264N |w\u2032j \u2212 wj | 2 min1\u2264j\u2264N w\u2032j ) \u2264 max\n1\u2264i\u2264N |wi \u2212 w\u2032i|\n( max\n1\u2264i\u2264N |g\u2032(Xi, Yi)|+\n1\n2 min1\u2264j\u2264N w\u2032j +\n1\n2 min1\u2264j\u2264N wj ) , (183)\nwhere the last inequality follows from the fact that \u2211N i=1 wi = N . We upper bound each term as follows.\nw\u2032j = u(Xi) fX(Xi) \u2265 1/\u00b5(K) C2/\u00b5(K) = 1/C2. (184)\nSimilarly we have w\u2032j \u2264 1/C1. This implies that n\u2032y,i = \u2211 j 6=i w \u2032 jI{\u2016Yi\u2212Yj\u2016 \u2264 \u03c1k,i} \u2265 k/C2. For finite k and sufficiently large N , we have:\ng\u2032(Xi, Yi) = \u03c8(k) + log(N) + log( cdxcdy cdx+dy\n)\u2212 ( log(nx,i) + log(ny,i) )\n\u2264 \u03c8(k) + log(N) + log( cdxcdy cdx+dy\n)\u2212 ( log(k) + log(k/C2) )\n\u2264 2 logN , (185)\nand similarly, using the fact that ny,i = \u2211 j 6=i w \u2032 jI{\u2016Yi \u2212 Yj\u2016 \u2264 \u03c1k,i} \u2264 N/C1,\ng\u2032(Xi, Yi) \u2265 \u03c8(k) + log(N) + log( cdxcdy cdx+dy\n)\u2212 ( log(N) + log(N/C1) )\n\u2265 \u22122 logN. (186)\nWe claim that for sufficiently large Nsuch that logN > max{C2\u03b5/3, 3C2/2}, if |wi \u2212 w\u2032i| < \u03b5/(3 logN) for all i, then (183) is upper bounded by \u03b5.\nmax 1\u2264i\u2264N\n|wi \u2212 w\u2032i| (\nmax 1\u2264i\u2264N\n|g\u2032(Xi, Yi)|+ 1\n2 min1\u2264j\u2264N w\u2032j +\n1\n2 min1\u2264j\u2264N wj )\n\u2264 \u03b5 3 logN\n( 2 logN +\nC2 2 + 1\n2/C2 \u2212 \u03b53 logN ) \u2264 \u03b5\n3 logN\n( 2 logN +\nC2 2\n+ C2 ) \u2264 \u03b5. (187)\nPutting these bounds together, we have, for any \u03b5 > 0 and sufficiently large N ,\nP ( 1 N \u2223\u2223 N\u2211 i=1 ( wig(Xi, Yi)\u2212 w\u2032ig\u2032(Xi, Yi) ) \u2223\u2223 > \u03b5) \u2264 P( max 1\u2264i\u2264N |wi \u2212 w\u2032i| > \u03b5/(3 logN) ) . (188)\nDefine\nw\u2032\u2032i = N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) ) (189) and applying the triangle inequality and union bound for (188), we have\nP (\nmax 1\u2264i\u2264N\n|wi \u2212 w\u2032i| > \u03b5\n3 logN ) \u2264 P ( max\n1\u2264i\u2264N |w\u2032i \u2212 w\u2032\u2032i |+ max 1\u2264i\u2264N |w\u2032\u2032i \u2212 wi| >\n\u03b5\n3 logN ) \u2264 P ( max\n1\u2264i\u2264N |w\u2032i \u2212 w\u2032\u2032i | >\n\u03b5\n6 logN\n) (190)\n+ P (\nmax 1\u2264i\u2264N\n|w\u2032\u2032i \u2212 wi| > \u03b5\n6 logN\n) . (191)\nFor (190), recall that w\u2032i = u(Xi)/fX(Xi). Since u(Xi)/fX(Xi) \u2208 [1/C2, 1/C1] for all i. Therefore\nP (\nmax 1\u2264i\u2264N\n|w\u2032i \u2212 w\u2032\u2032i | > \u03b5\n6 logN ) = P ( max\n1\u2264i\u2264N | u(Xi) fX(Xi) \u2212 N/fX(Xi)\u2211N j=1 ( 1/fX(Xj) ) | > \u03b5 6 logN ) = P ( ( max\n1\u2264i\u2264N\nu(Xi)\nfX(Xi) \u2223\u2223 1\u2212 N\u2211N j=1 ( u(Xj)/fX(Xj) ) \u2223\u2223 ) > \u03b5 6 logN ) \u2264 P\n( 1 C1 \u2223\u2223 N\u2211 j=1 u(Xj) fX(Xj) \u2212N \u2223\u2223 > \u03b5 6 logN N\u2211 j=1 u(Xj) fX(Xj) ) .\n\u2264 P ( \u2223\u2223 N\u2211\nj=1\nu(Xj)\nfX(Xj) \u2212N \u2223\u2223 > \u03b5 6 logN NC1 C2 ) . (192)\nNote that w\u2032j = u(Xj) fX(Xj) are i.i.d. random variables with E[w\u2032j ] = \u222b fX(x) u(x) fX(x)\ndx = 1 and w\u2032j \u2208 [1/C2, 1/C1]. Therefore, by Hoeffding\u2019s inequality, we obtain:\nP ( \u2223\u2223 N\u2211\nj=1\nu(Xj)\nfX(Xj) \u2212N \u2223\u2223 > \u03b5NC1 6 logNC2 )\n\u2264 2 exp \u2212 2 ( \u03b5NC1 6 logNC2 )2 N(1/C1 \u2212 1/C2)2  = 2 exp { \u2212 \u03b5\n2NC41 18(logN)2(C2 \u2212 C1)2\n} (193)\nwhich shows that the probability in (190) goes to 0 as N goes to infinity. For the probability in (191), recall that for any i,\n|w\u2032\u2032i \u2212 wi| = \u2223\u2223 N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) ) \u2212 N/f\u0303X(Xi)\u2211N j=1 ( 1/f\u0303X(Xj) ) \u2223\u2223. (194) We use the following lemma that shows an upper bound for the error of kernel density estimator.\nf\u0303X(Xi) = 1\n(N \u2212 1)hdxN \u2211 j 6=i K( Xj \u2212Xi hN ). (195)\nLemma 10. Assume that K(u) \u2264 A for all u, \u03baj(K) = \u222b Rdx \u2016u\u2016\njK(u)du < +\u221e for any positive integer j \u2265 1 and \u222b Rdx uK(u)du = 0. By choosing hN = 1 2N \u22121/(2dx+3), we have for a given i \u2208 {1, . . . , N},\nP ( \u2223\u2223 f\u0303X(Xi)\u2212 fX(Xi) \u2223\u2223 > N\u22121/(2dx+3)) \u2264 2 exp{\u2212N1/(2dx+3)\n16A2 }. (196)\nApplying the union bound we get that with probability at least 1\u2212 2N exp{\u2212N 1/(2dx+3)\n16A2 }, we have\n|f\u0303X(Xi)\u2212 fX(Xi)| < N\u22121/(2dx+3) , (197)\nfor all i. When this bound holds, we claim that the event inside of the probability in (191) holds for sufficiently large N . Together with (193), this proves the desired claim: for given \u03b5 > 0 and large enough N ,\n1\nN \u2223\u2223 N\u2211 i=1 ( wig(Xi, Yi)\u2212 w\u2032ig\u2032(Xi, Yi) ) \u2223\u2223 \u2264 \u03b5 (198)\nwith probability at least 1\u2212 2N exp{\u2212N 1/(2dx+3) 16A2 } \u2212 2 exp{\u2212 \u03b52NC41\n18(logN)2(C2\u2212C1)2 }. Now, we are left to show that (197) implies event inside of the probability in (191). Given (197), we\nhave\n| f\u0303X(Xi) fX(Xi)\n\u2212 1| < N \u22121/(2dx+3) fX(Xi) < \u00b5(K)N\u22121/(2dx+3) C1 (199)\nfor all i. Therefore, for sufficiently large N , wi \u2212 w\u2032i is lower bounded by\nwi \u2212 w\u2032\u2032i = N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) ) \u2212 N/f\u0303X(Xi)\u2211N j=1 ( 1/f\u0303X(Xj) ) \u2265 N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) )(1\u2212 1 + \u00b5(K)N\u22121/(2dx+3)C1 1\u2212 \u00b5(K)N \u22121/(2dx+3)\nC1 ) \u2265 N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) )(1\u2212 (1 + 3\u00b5(K)N\u22121/(2dx+3) C1 ) )\n(200)\n\u2265 \u22123C2\u00b5(K)N \u22121/(2dx+3)\nC21 , (201)\nwhere (200) follows from the fact that (1 + a)/(1 \u2212 a) \u2264 1 + 3a for a \u2208 [0, 1/3], and (201) follows from the fact that C1/\u00b5(K) \u2264 fX(x) \u2264 C2/\u00b5(K). Similarly, it is upper bounded by\nwi \u2212 w\u2032\u2032i = N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) ) \u2212 N/f\u0303X(Xi)\u2211N j=1 ( 1/f\u0303X(Xj) ) \u2264 N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) )(1\u2212 1\u2212 \u00b5(K)N\u22121/(2dx+3)C1 1 + \u00b5(K)N \u22121/(2dx+3)\nC1 ) \u2264 N/fX(Xi)\u2211N\nj=1\n( 1/fX(Xj) )(1\u2212 (1\u2212 3\u00b5(K)N\u22121/(2dx+3) C1 ) )\n\u2264 3C2\u00b5(K)N \u22121/(2dx+3)\nC21 . (202)\nHere (202) comes from the fact that (1 \u2212 a)/(1 + a) \u2265 1 \u2212 3a for all a \u2265 0. Therefore, |wi \u2212 w\u2032\u2032i | \u2264 3C2\u00b5(K)N\n\u22121/(2dx+3)/C21 . For a given \u03b5 > 0 and for sufficiently largeN such that \u03b5/(6 logN) \u2265 3C2\u00b5(K)N\u22121/(2dx+3)/C21 , we have\nP (\nmax 1\u2264i\u2264N\n|w\u2032\u2032i \u2212 wi| > \u03b5\n6 logN ) \u2264 P ( max\n1\u2264i\u2264N |wi \u2212 w\u2032\u2032i | >\n3C2\u00b5(K)N \u22121/(2dx+3)\nC21 ) \u2264 P ( \u2200i , |f\u0303X(Xi)\u2212 fX(Xi)| < N\u22121/(2dx+3) ) .\nTogether with (188) and (193), this proves the desired convergence of the first term (34).\nD Proof of Proposition 2\nThe proof steps are similar to that of Proposition 1, only requiring citations to properties of Re\u0301nyi divergence and asymmetric information.\n\u2022 Clearly Axiom 0 holds \u2013 it follows from a standard result that D\u03bb = 0 if and only if P = Q almost everywhere [Csi95].\n\u2022 Axiom 1: Suppose CMI\u03bb(PZ|X) is achieved with P \u2217X . Consider the joint distribution P \u2217XPY |XPZ|Y . Utilizing the data-processing inequality for asymmetric mutual information (cf. Equation (55) in [PV10]), we get\nCMI\u03bb(PY |X) = max PX\nK\u03bb(PXPY |X) \u2265 K\u03bb(P \u2217XPY |X)\n\u2265 K\u03bb(P \u2217XPZ|X) = CMI\u03bb(PZ|X). (203)\nThus Axiom 1a is satisfied. Now consider Axiom 1b. With the same joint distribution, let P \u2217Y be the marginal of Y . Then we have,\nCMI\u03bb(PZ|Y ) = max PY\nK\u03bb(PY PZ|Y ) \u2265 K\u03bb(P \u2217Y PZ|Y )\n\u2265 K\u03bb(P \u2217XPZ|X) = CMI\u03bb(PZ|X). (204)\n\u2022 Axiom 2: The asymmetric mutual information has the same additivity property as traditional mutual information, cf. Theorem 27 of [VEH14]. The corresponding additivity for CMI\u03bb now follows.\n\u2022 Axiom 3a: The information-centroid representation for CMI\u03bb states that (see [Csi95] or Equation (44) of [PV10]):\nCMI\u03bb(PY |X) = min QY max x\nD\u03bb(PY |X=x\u2016QY ). (205)\nThis characterization allows us to make the observation that CMI\u03bb is a function only of the convex hull of the probability distributions PY |X=x, just as earlier: given a conditional probability distribution PY |X , we augment the input alphabet to have one more input symbol x\n\u2032 such that PY |X=x\u2032 = \u2211 x \u03b1xPY |X=x is a convex combination of the other conditional distributions. We claim that the CMI\u03bb of the new channel is unchanged: one direction is obvious, i.e., the new channel has capacity greater than or equal to the original channel, since adding a new symbol cannot decrease capacity. To show the other direction, we use (5) and observe that, due to the quasi convexity of Re\u0301nyi divergence in its arguments (cf. Theorem 13 in [VEH14]), we get,\nD\u03bb(PY |X=x\u2032\u2016qY ) = D\u03bb( \u2211 x \u03b1xPY |X=x\u2016qY ) \u2264 max x D\u03bb(PY |X=x\u2016qY ).\nThus CMI\u03bb is only a function of the convex hull of the range of the map PY |X , satisfying Axiom 3a. This function is monotonic directly from (205), thus satisfying Axiom 3b.\n\u2022 Axiom 4: For fixed output alphabet Y, it is clear that maxX ,PY |X CMI\u03bb = log |Y| for each \u03bb. Now suppose for some conditional distribution PY |X we have CMI\u03bb(PY |X) = log |Y|. This implies that, with the optimizing input distribution, H\u03bb(Y ) \u2212 H\u03bb(Y |X) = log |Y|. This implies that H\u03bb(Y ) = log |Y| and H\u03bb(Y |X) = 0, thus Y is a deterministic function of the essential support of X and since H\u03bb(Y ) = log |Y|, the Schur concavity of Re\u0301nyi entropy (cf. Theorem 1 of [HV15]) implies that PY = UY , the uniform distribution and the deterministic function is onto."}], "references": [{"title": "IEEE Transactions on", "author": ["Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. Information Theory"], "venue": "18(1):14\u201320,", "citeRegEx": "Ari72", "shortCiteRegEx": null, "year": 1972}, {"title": "Nonparametric entropy estimation: An overview", "author": ["Jan Beirlant", "Edward J Dudewicz", "L\u00e1szl\u00f3 Gy\u00f6rfi", "Edward C Van der Meulen"], "venue": "International Journal of Mathematical and Statistical Sciences, 6(1):17\u201339,", "citeRegEx": "BDGVdM97", "shortCiteRegEx": null, "year": 1997}, {"title": "IEEE Transactions on", "author": ["Richard E Blahut. Computation of channel capacity", "rate-distortion functions. Information Theory"], "venue": "18(4):460\u2013473,", "citeRegEx": "Bla72", "shortCiteRegEx": null, "year": 1972}, {"title": "Scandinavian Journal of Statistics", "author": ["Jean Cornuet", "Jean-Michel Marin", "Antonietta Mira", "Christian P Robert. Adaptive multiple importance sampling"], "venue": "39(4):798\u2013812,", "citeRegEx": "CMMR12", "shortCiteRegEx": null, "year": 2012}, {"title": "Information theory and statistics: A tutorial", "author": ["Imre Csisz\u00e1r", "Paul C Shields"], "venue": "Now Publishers Inc,", "citeRegEx": "Csisz\u00e1r and Shields.,? \\Q2004\\E", "shortCiteRegEx": "Csisz\u00e1r and Shields.", "year": 2004}, {"title": "Generalized cutoff rates and renyi\u2019s information measures", "author": ["Imre Csisz\u00e1r"], "venue": "Information Theory, IEEE Transactions on, 41(1):26\u201334,", "citeRegEx": "Csi95", "shortCiteRegEx": null, "year": 1995}, {"title": "Entropy", "author": ["Imre Csisz\u00e1r. Axiomatic characterizations of information measures"], "venue": "10(3):261\u2013273,", "citeRegEx": "Csi08", "shortCiteRegEx": null, "year": 2008}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "CT12", "shortCiteRegEx": null, "year": 2012}, {"title": "The Annals of Statistics", "author": ["Luc Devroye", "Clark S Penrod. The consistency of automatic kernel density estimates"], "venue": "pages 1231\u20131249,", "citeRegEx": "DP84", "shortCiteRegEx": null, "year": 1984}, {"title": "Demystifying fixed k-nearest neighbor information estimators", "author": ["Weihao Gao", "Sewoong Oh", "Pramod Viswanath"], "venue": "arXiv preprint arXiv:1604.03006,", "citeRegEx": "GOV16", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of mutual information for strongly dependent variables", "author": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "venue": "arXiv preprint arXiv:1411.2003,", "citeRegEx": "GSG14", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating mutual information by local gaussian approximation", "author": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "venue": "arXiv preprint arXiv:1508.00536,", "citeRegEx": "GSG15", "shortCiteRegEx": null, "year": 2015}, {"title": "Locally parametric nonparametric density estimation", "author": ["Nils Lid Hjort", "MC Jones"], "venue": "The Annals of Statistics,", "citeRegEx": "Hjort and Jones.,? \\Q1996\\E", "shortCiteRegEx": "Hjort and Jones.", "year": 1996}, {"title": "Convexity/concavity of renyi entropy and \u03b1-mutual information", "author": ["Siu-Wai Ho", "Sergio Verd\u00fa"], "venue": "Information Theory (ISIT), 2015 IEEE International Symposium on, pages 745\u2013749. IEEE,", "citeRegEx": "HV15", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Dominik Janzing", "David Balduzzi", "Moritz Grosse-Wentrup", "Bernhard Sch\u00f6lkopf"], "venue": "Quantifying causal influences. The Annals of Statistics, 41(5):2324\u20132358,", "citeRegEx": "JBGW13", "shortCiteRegEx": null, "year": 2013}, {"title": "Justifying Information-Geometric Causal Inference", "author": ["D. Janzing", "B. Steudel", "N. Shajarisales", "B. Sch\u00f6lkopf"], "venue": "chapter 18, pages 253\u2013265. Springer International Publishing", "citeRegEx": "JSSS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Physical Review E", "author": ["Shiraj Khan", "Sharba Bandyopadhyay", "Auroop R Ganguly", "Sunil Saigal", "David J Erickson III", "Vladimir Protopopescu", "George Ostrouchov. Relative performance of mutual information estimation methods for quantifying the dependence among short", "noisy data"], "venue": "76(2):026209,", "citeRegEx": "KBG07", "shortCiteRegEx": null, "year": 2007}, {"title": "divergences and mutual informations", "author": ["Kirthevasan Kandasamy", "Akshay Krishnamurthy", "Barnabas Poczos", "Larry Wasserman. Nonparametric von mises estimators for entropies"], "venue": "Advances in Neural Information Processing Systems, pages 397\u2013405,", "citeRegEx": "KKPW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Problemy Peredachi Informatsii", "author": ["LF Kozachenko", "Nikolai N Leonenko. Sample estimate of the entropy of a random vector"], "venue": "23(2):9\u201316,", "citeRegEx": "KL87", "shortCiteRegEx": null, "year": 1987}, {"title": "Estimating mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Physical review E, 69(6):066138", "citeRegEx": "KSG04", "shortCiteRegEx": null, "year": 2004}, {"title": "Dana Pe\u2019er", "author": ["Smita Krishnaswamy", "Matthew H Spitzer", "Michael Mingueneau", "Sean C Bendall", "Oren Litvin", "Erica Stone"], "venue": "and Garry P Nolan. Conditional density-based analysis of t cell signaling in single-cell data. Science, 346(6213):1250689,", "citeRegEx": "KSM14", "shortCiteRegEx": null, "year": 2014}, {"title": "Local likelihood density estimation", "author": ["Clive R Loader"], "venue": "The Annals of Statistics,", "citeRegEx": "Loader,? \\Q1996\\E", "shortCiteRegEx": "Loader", "year": 1996}, {"title": "Nonparametric k-nearest-neighbor entropy estimator", "author": ["Damiano Lombardi", "Sanjay Pant"], "venue": "Physical Review E,", "citeRegEx": "Lombardi and Pant.,? \\Q2016\\E", "shortCiteRegEx": "Lombardi and Pant.", "year": 2016}, {"title": "Modeling trends in distributions", "author": ["Jonas Mueller", "Tommi Jaakkola", "David Gifford"], "venue": "arXiv preprint arXiv:1511.04486,", "citeRegEx": "MJG15", "shortCiteRegEx": null, "year": 2015}, {"title": "Signal processing", "author": ["Rudy Moddemeijer. On estimation of entropy", "mutual information of continuous distributions"], "venue": "16(3):233\u2013248,", "citeRegEx": "Mod89", "shortCiteRegEx": null, "year": 1989}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "author": ["J.M. Mooij", "J. Peters", "D. Janzing", "J. Zscheischler", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research", "citeRegEx": "MPJ15", "shortCiteRegEx": null, "year": 2015}, {"title": "Monte Carlo theory", "author": ["Art B. Owen"], "venue": "methods and examples.", "citeRegEx": "Owe13", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural computation", "author": ["Liam Paninski. Estimation of entropy", "mutual information"], "venue": "15(6):1191\u20131253,", "citeRegEx": "Pan03", "shortCiteRegEx": null, "year": 2003}, {"title": "Causality", "author": ["Judea Pearl"], "venue": "Cambridge university press,", "citeRegEx": "Pea09", "shortCiteRegEx": null, "year": 2009}, {"title": "In Advances in Neural Information Processing Systems", "author": ["D\u00e1vid P\u00e1l", "Barnab\u00e1s P\u00f3czos", "Csaba Szepesv\u00e1ri. Estimation of r\u00e9nyi entropy", "mutual information based on generalized nearest-neighbor graphs"], "venue": "pages 1849\u20131857,", "citeRegEx": "PPS10", "shortCiteRegEx": null, "year": 2010}, {"title": "and Computing (Allerton)", "author": ["Yury Polyanskiy", "Sergio Verd\u00fa. Arimoto channel coding converse", "r\u00e9nyi divergence. In Communication", "Control"], "venue": "2010 48th Annual Allerton Conference on, pages 1327\u20131333. IEEE,", "citeRegEx": "PV10", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric divergence estimation with applications to machine learning on distributions", "author": ["Barnab\u00e1s P\u00f3czos", "Liang Xiong", "Jeff Schneider"], "venue": "arXiv preprint arXiv:1202.3758,", "citeRegEx": "PXS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-parametric causal models", "author": ["Robin J Richardson", "Thomas S Evans"], "venue": null, "citeRegEx": "Richardson and Evans.,? \\Q2015\\E", "shortCiteRegEx": "Richardson and Evans.", "year": 2015}, {"title": "Acta mathematica hungarica", "author": ["Alfr\u00e9d R\u00e9nyi. On measures of dependence"], "venue": "10(3-4):441\u2013451,", "citeRegEx": "R\u00e9n59", "shortCiteRegEx": null, "year": 1959}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell System Tech. J., 27:379423 and 623656", "citeRegEx": "Sha48", "shortCiteRegEx": null, "year": 1948}, {"title": "A reliable data-based bandwidth selection method for kernel density estimation", "author": ["Simon J Sheather", "Michael C Jones"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Sheather and Jones.,? \\Q1991\\E", "shortCiteRegEx": "Sheather and Jones.", "year": 1991}, {"title": "Telling cause from effect in deterministic linear dynamical systems", "author": ["N. Shajarisales", "D. Janzing", "B. Sch\u00f6lkopf", "M. Besserve"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, page 285?294. JMLR", "citeRegEx": "SJSB15", "shortCiteRegEx": null, "year": 2015}, {"title": "American journal of mathematical and management sciences", "author": ["Harshinder Singh", "Neeraj Misra", "Vladimir Hnizdo", "Adam Fedorowicz", "Eugene Demchuk. Nearest neighbor estimates of entropy"], "venue": "23(3-4):301\u2013321,", "citeRegEx": "SMH03", "shortCiteRegEx": null, "year": 2003}, {"title": "Empirical estimation of entropy functionals with confidence", "author": ["Kumar Sricharan", "Raviv Raich", "Alfred O Hero III"], "venue": "arXiv preprint arXiv:1012.4188,", "citeRegEx": "SRHI10", "shortCiteRegEx": null, "year": 2010}, {"title": "Scandinavian Journal of Statistics", "author": ["Alexandre B Tsybakov", "EC Van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support"], "venue": "pages 75\u201383,", "citeRegEx": "TVdM96", "shortCiteRegEx": null, "year": 1996}, {"title": "IEEE Transactions on", "author": ["Tim Van Erven", "Peter Harremo\u00ebs. R\u00e9nyi divergence", "kullback-leibler divergence. Information Theory"], "venue": "60(7):3797\u20133820,", "citeRegEx": "VEH14", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on", "author": ["Qing Wang", "Sanjeev R Kulkarni", "Sergio Verd\u00fa. Divergence estimation for multidimensional densities via-nearest-neighbor distances. Information Theory"], "venue": "55(5):2392\u20132405,", "citeRegEx": "WKV09", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "In fact, this measure is taken as a starting point to develop an axiomatic approach to studying causal strength on general graphs in [JBGW13].", "startOffset": 133, "endOffset": 141}, {"referenceID": 20, "context": "In a recent work [KSM14], potential causal influence is posited as a relevant metric to spot \u201ctrends\u201d in gene pathways.", "startOffset": 17, "endOffset": 24}, {"referenceID": 19, "context": "The estimator brings together ideas from three disparate threads in statistical estimation theory: nearest-neighbor methods, a correlation boosting idea in the estimation of (standard) mutual information from samples [KSG04], and importance sampling.", "startOffset": 217, "endOffset": 224}, {"referenceID": 18, "context": "The estimator has only a single hyper parameter (the number of nearest-neighbors considered, set to 4 or 5 in practice), uses an offthe-shelf kernel density estimator of only PX , and has strong connections to the entropy estimator of [KL87].", "startOffset": 235, "endOffset": 241}, {"referenceID": 24, "context": "In simulations, the estimator has very strong performance in terms of sample complexity (compared to a baseline of the partition-based estimator in [Mod89]).", "startOffset": 148, "endOffset": 155}, {"referenceID": 20, "context": "\u2022 Application to gene pathway influence: In [KSM14], considered an important result in single-cell flow-cytometry data analysis, a causal strength metric (termed DREMI) is proposed for measuring the causal influence of a gene \u2013 this estimator is a specific way of implementing UMI along with a \u201cchannel amplification\u201d step, and DREMI was successfully used to spot gene-pathway trends.", "startOffset": 44, "endOffset": 51}, {"referenceID": 7, "context": "Chapter 2 of [CT12].", "startOffset": 13, "endOffset": 19}, {"referenceID": 7, "context": "\u2022 Axiom 2: This is a standard result for Shannon capacity and we refer the interested reader to Chapter 7 of [CT12].", "startOffset": 109, "endOffset": 115}, {"referenceID": 20, "context": "This estimator is motivated by the recent work in [KSM14].", "startOffset": 50, "endOffset": 57}, {"referenceID": 33, "context": "This is also true of other measures of relation such as the Renyi correlation [R\u00e9n59], and in each case the measure is studied in the context of some form penalty term.", "startOffset": 78, "endOffset": 85}, {"referenceID": 20, "context": "1 Uniform Mutual Information The idea of applying UMI to infer the strength of conditional dependence was first proposed in [KSM14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 64, "endOffset": 71}, {"referenceID": 16, "context": "[KL87]; (b) the correlation boosting idea of the estimator from [KSG04]\u2013which is widely adopted in practice [KBG07]; and (c) the importance sampling techniques to adjust for the uniform prior for UMI.", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "Note that three applications of the entropy estimator, such as those from [BDGVdM97], gives an estimate of the mutual information, i.", "startOffset": 74, "endOffset": 84}, {"referenceID": 18, "context": "Alternatively, to bypass estimating PXY at every point, the differential entropy estimation can be done via k nearest neighbor (kNN) methods (pioneering work in [KL87]).", "startOffset": 161, "endOffset": 167}, {"referenceID": 19, "context": "Perhaps surprisingly, an innovative approach undertaken in [KSG04] to improve upon three applications of KL estimators provides a solution.", "startOffset": 59, "endOffset": 66}, {"referenceID": 19, "context": "The KSG estimator of [KSG04] is based on kNN distance \u03c1k,i defined as the distance to the k-th nearest neighbor from (Xi, Yi) in `\u221e distance, i.", "startOffset": 21, "endOffset": 28}, {"referenceID": 9, "context": "However, despite its popularity in practice due to its simplicity, no convergence result has been known until very recently (when [GOV16] showed some consistency and rate of convergence properties).", "startOffset": 130, "endOffset": 137}, {"referenceID": 3, "context": "where X \u2286 Rx , Y \u2286 Ry , cd = \u03c0 d 2 /\u0393(d2 + 1) is the volume of d-dimensional unit ball, and wi is the self-normalized importance sampling estimate [CMMR12] of u(Xi) f(Xi) : wi \u2261 N/f\u0303(Xi) \u2211N j=1 ( 1/f\u0303(Xj) ) , (16)", "startOffset": 147, "endOffset": 155}, {"referenceID": 19, "context": "For each sample (Xi, Yi), calculate the Euclidean distance \u03c1k,i (as opposed to the `\u221e distance proposed by [KSG04]) to the k-th nearest neighbor.", "startOffset": 107, "endOffset": 114}, {"referenceID": 18, "context": "As far as we know, the only estimator based on fixed-k nearest neighbors that is known to be consistent is the entropy estimator of [KL87], and the convergence rate is only known for the univariate case [TVdM96] (and that too under significant assumptions on the univariate density).", "startOffset": 132, "endOffset": 138}, {"referenceID": 39, "context": "As far as we know, the only estimator based on fixed-k nearest neighbors that is known to be consistent is the entropy estimator of [KL87], and the convergence rate is only known for the univariate case [TVdM96] (and that too under significant assumptions on the univariate density).", "startOffset": 203, "endOffset": 211}, {"referenceID": 18, "context": "Uniform Mutual Information: As our estimators use the off-the-shelf kernel density estimator of PX [DP84, SJ91] and also the ides from the nearest-neighbor methods [KL87], we make assumptions on the conditional density fY |X that are typical in these literature.", "startOffset": 164, "endOffset": 170}, {"referenceID": 20, "context": "1 Gene Causal Strength from Single Cell Data We briefly describe the setup of [KSM14] to motivate our numerical experiments.", "startOffset": 78, "endOffset": 85}, {"referenceID": 20, "context": "Figure 1: CMI and UMI estimators significantly improve over DREMI in capturing the biological trend in flow-cytometry data: the figures above refer to the same setting as Figure 6 of [KSM14].", "startOffset": 183, "endOffset": 190}, {"referenceID": 20, "context": "These samples are obtained using a technique called single-cell mass flow cytometry, see [KSM14] for details.", "startOffset": 89, "endOffset": 96}, {"referenceID": 20, "context": "Such an analysis is conducted in [KSM14] where the causal strength function C is evaluated via the so-called DREMI estimator (essentially a version of UMI estimation with a \u201cchannel amplification\u201d step and careful choice of hyper parameters therein \u2013 no theoretical properties of this estimator were evaluated).", "startOffset": 33, "endOffset": 40}, {"referenceID": 20, "context": "This demonstrates the utility of DREMI for causal strength inference in gene networks (see Figure 6 of [KSM14]).", "startOffset": 103, "endOffset": 110}, {"referenceID": 23, "context": "As an aside, we note that a somewhat different set of \u201ctrend spotting\u201d estimators, primarily trying to find genes which demonstrate a monotonic trend over time from single-cell RNA-sequencing data, have been proposed very recently in [MJG15].", "startOffset": 234, "endOffset": 241}, {"referenceID": 20, "context": "It is natural to apply our estimators to each time point in the same setting as [KSM14] \u2013 and look to understand two distinct issues in our experiments with the flow-cytometry data.", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": "The second question relates to the sample complexity: how does the ability to recover the trend vary as a function of the sample complexity? To study this, we subsample the original data from [KSM14] multiple times (100 in the experiments) at each subsampling ratio and compute the fraction of times we recover the true biological trend.", "startOffset": 192, "endOffset": 199}, {"referenceID": 24, "context": "Figure 2: The proposed UMI estimator significantly outperforms partition based methods [Mod89] in sample complexity.", "startOffset": 87, "endOffset": 94}, {"referenceID": 24, "context": "This is compared to the ground truth and the state-of-the-art partition based estimators from [Mod89].", "startOffset": 94, "endOffset": 101}, {"referenceID": 19, "context": "The ground truth has been computed via simulations with 8192 samples from the desired distribution PY |XUX using Kraskov\u2019s mutual information estimator [KSG04].", "startOffset": 152, "endOffset": 159}, {"referenceID": 19, "context": "The consistency proofs suggest that a similar analysis the very popular estimator of (traditional) mutual information in [KSG04] can be conducted successfully; such work has been recently conducted in [GOV16].", "startOffset": 121, "endOffset": 128}, {"referenceID": 9, "context": "The consistency proofs suggest that a similar analysis the very popular estimator of (traditional) mutual information in [KSG04] can be conducted successfully; such work has been recently conducted in [GOV16].", "startOffset": 201, "endOffset": 208}, {"referenceID": 39, "context": "Rates of convergence in the nearest-neighbor methods are barely known in the literature even for traditional information theoretic quantities: for instance, [TVdM96] derives a \u221a N consistency for the single dimensional case of differential entropy estimation (under strong assumptions on the underlying", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "pdf), leaving higher dimensional scenarios open, and which recently have been successfully addressed in [GOV16].", "startOffset": 104, "endOffset": 111}, {"referenceID": 18, "context": "(b) There is a natural generalization of our estimators when the alphabet Y is high dimensional, using the kNN approach (just as in the differential entropy estimator of [KL87] or in the mutual information estimator of [KSG04]).", "startOffset": 170, "endOffset": 176}, {"referenceID": 19, "context": "(b) There is a natural generalization of our estimators when the alphabet Y is high dimensional, using the kNN approach (just as in the differential entropy estimator of [KL87] or in the mutual information estimator of [KSG04]).", "startOffset": 219, "endOffset": 226}, {"referenceID": 28, "context": "This is a widely studied topic with a long lineage [Pea09] but also of strong topical interest [JBGW13, JSSS15, MPJ15, SJSB15].", "startOffset": 51, "endOffset": 58}, {"referenceID": 25, "context": "A natural inclination is to explore the efficacy of UMI and CMI measures to test for direction of causality \u2013 especially in the context of the benchmark data sets collected in [MPJ15].", "startOffset": 176, "endOffset": 183}, {"referenceID": 18, "context": "Directly comparing the marginal entropy H(X) and H(Y ) by the estimator in [KL87] also only provides 45% accuracy.", "startOffset": 75, "endOffset": 81}, {"referenceID": 25, "context": "While in [MPJ15], different entropy estimators (with appropriate hyper parameter choices) were applied to get an accuracy up to 60%-70%.", "startOffset": 9, "endOffset": 16}, {"referenceID": 25, "context": "Further research is needed to shed conclusive light, although we point out that the benchmark data sets in [MPJ15] have substantial confounding factors that make causal direction hard to measure in the first place.", "startOffset": 107, "endOffset": 114}, {"referenceID": 5, "context": "Now define the asymmetric information measure [Csi95]: K\u03bb(PXPY |X) := inf QY D\u03bb(PXY \u2016PXQY ), (25) which converges to the traditional mutual information when \u03bb \u2192 1.", "startOffset": 46, "endOffset": 53}, {"referenceID": 20, "context": "In the light of this result, it would be interesting to design estimators for the more general family of R\u00e9nyi capacity measures and confirm their performance on empirical tasks such as the ones studied in [KSM14].", "startOffset": 206, "endOffset": 213}, {"referenceID": 6, "context": "It would also be very interesting to understand the role of additional axioms that would lead to uniqueness of Shannon capacity (in the same spirit as entropy being uniquely characterized by somewhat similar axioms [Csi08]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 0, "context": "References [Ari72] Suguru Arimoto.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "[BDGVdM97] Jan Beirlant, Edward J Dudewicz, L\u00e1szl\u00f3 Gy\u00f6rfi, and Edward C Van der Meulen.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[Bla72] Richard E Blahut.", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[CMMR12] Jean Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian P Robert.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[Csi95] Imre Csisz\u00e1r.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[Csi08] Imre Csisz\u00e1r.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[CT12] Thomas M Cover and Joy A Thomas.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[DP84] Luc Devroye and Clark S Penrod.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[GOV16] Weihao Gao, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[GSG14] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[GSG15] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[HV15] Siu-Wai Ho and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 6}, {"referenceID": 14, "context": "[JBGW13] Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Sch\u00f6lkopf, et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[JSSS15] D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[KBG07] Shiraj Khan, Sharba Bandyopadhyay, Auroop R Ganguly, Sunil Saigal, David J Erickson III, Vladimir Protopopescu, and George Ostrouchov.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[KKPW15] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, and Larry Wasserman.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[KL87] LF Kozachenko and Nikolai N Leonenko.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[KSG04] A.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[KSM14] Smita Krishnaswamy, Matthew H Spitzer, Michael Mingueneau, Sean C Bendall, Oren Litvin, Erica Stone, Dana Pe\u2019er, and Garry P Nolan.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[MJG15] Jonas Mueller, Tommi Jaakkola, and David Gifford.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[Mod89] Rudy Moddemeijer.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[MPJ15] J.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[Owe13] Art B.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "[Pan03] Liam Paninski.", "startOffset": 0, "endOffset": 7}, {"referenceID": 28, "context": "[Pea09] Judea Pearl.", "startOffset": 0, "endOffset": 7}, {"referenceID": 29, "context": "[PPS10] D\u00e1vid P\u00e1l, Barnab\u00e1s P\u00f3czos, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 7}, {"referenceID": 30, "context": "[PV10] Yury Polyanskiy and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 6}, {"referenceID": 31, "context": "[PXS12] Barnab\u00e1s P\u00f3czos, Liang Xiong, and Jeff Schneider.", "startOffset": 0, "endOffset": 7}, {"referenceID": 33, "context": "[R\u00e9n59] Alfr\u00e9d R\u00e9nyi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 34, "context": "[Sha48] C.", "startOffset": 0, "endOffset": 7}, {"referenceID": 36, "context": "[SJSB15] N.", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "[SMH03] Harshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam Fedorowicz, and Eugene Demchuk.", "startOffset": 0, "endOffset": 7}, {"referenceID": 38, "context": "[SRHI10] Kumar Sricharan, Raviv Raich, and Alfred O Hero III.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[TVdM96] Alexandre B Tsybakov and EC Van der Meulen.", "startOffset": 0, "endOffset": 8}, {"referenceID": 40, "context": "[VEH14] Tim Van Erven and Peter Harremo\u00ebs.", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[WKV09] Qing Wang, Sanjeev R Kulkarni, and Sergio Verd\u00fa.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "1 in [Owe13]).", "startOffset": 5, "endOffset": 12}, {"referenceID": 37, "context": "By Theorem 8 of [SMH03], we have", "startOffset": 16, "endOffset": 23}, {"referenceID": 29, "context": "Similar interchange of limit has been used in [KL87, WKV09] without the regularization; in this context [PPS10] claims that this step is not justified (although no counterexample is pointed out).", "startOffset": 104, "endOffset": 111}, {"referenceID": 37, "context": "(52) Moreover, by Theorem11 of [SMH03], we have:", "startOffset": 31, "endOffset": 38}, {"referenceID": 37, "context": "[SMH03], we have lim N\u2192\u221e E [ log f\u0302Y |X(Yi|Xi) \u2223\u2223(Xi, Yi) = (x, y)] = log fY |X(y|x) , (134)", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "[SMH03], we have:", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "\u2022 Clearly Axiom 0 holds \u2013 it follows from a standard result that D\u03bb = 0 if and only if P = Q almost everywhere [Csi95].", "startOffset": 111, "endOffset": 118}, {"referenceID": 30, "context": "Equation (55) in [PV10]), we get CMI\u03bb(PY |X) = max PX K\u03bb(PXPY |X) \u2265 K\u03bb(P \u2217 XPY |X) \u2265 K\u03bb(P \u2217 XPZ|X) = CMI\u03bb(PZ|X).", "startOffset": 17, "endOffset": 23}, {"referenceID": 40, "context": "Theorem 27 of [VEH14].", "startOffset": 14, "endOffset": 21}, {"referenceID": 5, "context": "\u2022 Axiom 3a: The information-centroid representation for CMI\u03bb states that (see [Csi95] or Equation (44) of [PV10]): CMI\u03bb(PY |X) = min QY max x D\u03bb(PY |X=x\u2016QY ).", "startOffset": 78, "endOffset": 85}, {"referenceID": 30, "context": "\u2022 Axiom 3a: The information-centroid representation for CMI\u03bb states that (see [Csi95] or Equation (44) of [PV10]): CMI\u03bb(PY |X) = min QY max x D\u03bb(PY |X=x\u2016QY ).", "startOffset": 106, "endOffset": 112}, {"referenceID": 40, "context": "Theorem 13 in [VEH14]), we get, D\u03bb(PY |X=x\u2032\u2016qY ) = D\u03bb( \u2211", "startOffset": 14, "endOffset": 21}, {"referenceID": 13, "context": "Theorem 1 of [HV15]) implies that PY = UY , the uniform distribution and the deterministic function is onto.", "startOffset": 13, "endOffset": 19}], "year": 2016, "abstractText": "We consider axiomatically the problem of estimating the strength of a conditional dependence relationship PY |X from a random variables X to a random variable Y . This has applications in determining the strength of a known causal relationship, where the strength depends only on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause). Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-k nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity.", "creator": "LaTeX with hyperref package"}}}