{"id": "1602.03609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Attentive Pooling Networks", "abstract": "and in this work, we propose attentive pooling ( ap ), a two - way attention mechanism tool for discriminative model training. in the context capable of pair - wise ranking or classification with neural networks, ap enables the pooling layer to be aware of varying the maximal current input pair, in a way that information from knowing the two input items can directly influence the computation outcomes of each other's representations. along done with such representations of the paired inputs, ap jointly learns a common similarity value measure over projected segments ( e. g. trigrams ) of the pair, and subsequently, ac derives the corresponding attention bias vector for each node input to completely guide the pooling. our structured two - way collaboration attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks ( lt cnns ) and recurrent neural networks ( rnns ) in our studies. the empirical results, from three very much different benchmark recognition tasks component of question answering / answer selection, demonstrate that our proposed models outperform a variety of strong baselines adequately and achieve state - of - the - art performance performance in all the benchmarks.", "histories": [["v1", "Thu, 11 Feb 2016 03:06:33 GMT  (917kb,D)", "http://arxiv.org/abs/1602.03609v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["cicero dos santos", "ming tan", "bing xiang", "bowen zhou"], "accepted": false, "id": "1602.03609"}, "pdf": {"name": "1602.03609.pdf", "metadata": {"source": "META", "title": "Attentive Pooling Networks", "authors": ["Cicero dos Santos", "Ming Tan", "Bing Xiang", "Bowen Zhou"], "emails": ["CICERONS@US.IBM.COM", "MINGTAN@US.IBM.COM", "BINGXIA@US.IBM.COM", "ZHOU@US.IBM.COM"], "sections": [{"heading": "1. Introduction", "text": "Neural networks (NN) with attention mechanisms have recently proven to be successful at different computer vision (CV) and natural language processing (NLP) tasks such as image captioning (Xu et al., 2015), machine translation (Bahdanau et al., 2015) and factoid question answering (Hermann et al., 2015). However, most recent work on neural attention models have focused on one-way attention mechanisms based on recurrent neural networks designed\n.\nfor generation tasks.\nAnother important family of machine learning tasks are centered around pair-wise ranking or classification, which have a broad set of applications, including but not limited to, question answering, entailment, paraphrasing and any other pair-wise matching problems. The current state-ofthe-art models usually include NN-based representation for the input pair, followed by a discriminative ranking or classification models. For example, a convolution (or a RNN) and a max-pooling is used to independently construct distributed vector representations of the input pair, followed by a large-margin training (Hu et al., 2014; Weston et al., 2014; Shen et al., 2014; dos Santos et al., 2015).\nThe key contribution of this work is that we propose Attentive Pooling (AP), a two-way attention mechanism, that significantly improves such discriminative models\u2019 performance on pair-wise ranking or classification, by enabling a joint learning of the representations of both inputs as well as their similarity measurement.\nSpecifically, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other\u2019s representations. The main idea in AP consists of learning a similarity measure over projected segments (e.g. trigrams) of the two items in the input pair, and using the similarity scores between the segments to compute attention vectors in both directions. Next, the attention vectors are used to perform pooling.\nThere are a few key benefits of our model.\n\u2022 Thanks to the two-way attention, our model projects the paired inputs, even though they may not be always semantically comparable for some applications (e.g., questions and answers in question answering), into a common representation space that they can be compared in a more plausible way.\n\u2022 Our model is effective in matching pairs of inputs with significant length variations.\nar X\niv :1\n60 2.\n03 60\n9v 1\n[ cs\n.C L\n] 1\n1 Fe\nb 20\n16\n\u2022 The two-way attention mechanism is independent of the underlying representation learning. For example, AP can be applied to both CNNs and RNNs, which is in contrast to the one-way attention used in the generation models mostly based on recurrent nets.\nIn this work, we perform an extensive number of experiments on applying attentive pooling CNNs (AP-CNN) and biLSTMs (AP-biLSTM) for the answer selection task. In this task, given a question q and an candidate answer pool P = {a1, a2, \u00b7 \u00b7 \u00b7 , ap} for this question, the goal is to search for and select the candidate answer a \u2208 P that correctly answers q. We perform experiments with three publicly available benchmark datasets, which vary in data scale, complexity and length ratios between question and answers: InsuranceQA, TREC-QA and WikiQA. For the three datasets, AP-CNN and AP-biLSTM respectively outperform the CNN and the biLSTM that do not use attention. Additionally, AP-CNN achieves state-of-the-art results for the three datasets.\nOur experimental results also demonstrate that attentive pooling makes the CNN more robust to large input texts. This is an important finding, since recent work have demonstrated that, in the context of semantically equivalent question retrieval, CNN based representations do not scale well with the size of the input text (dos Santos et al., 2015). Additionally, as AP-CNN does not rely only on the final vector representation to capture interactions between the input question and answer, it requires much less convolutional filters than the regular CNN. It means that APCNN-based representations are more compact, which can help to speed up the training process.\nAlthough we demonstrate experimental results for NLP tasks only, AP is a general method that can be also applied to different types of NNs that perform matching of two inputs. Therefore, we believe that AP can be useful for different applications, such as computer vision and bioinformatics.\nThis paper is organized as follows. In Section 2, we describe two NN architectures for answer selection that have been recently proposed in the literature. In Section 3, we detail the attentive pooling approach. In Section 4, we discuss some related work. Sections 5 and 6 detail our experimental setup and results, respectively. In Section 7 we present our final remarks."}, {"heading": "2. Neural Networks for Answer Selection", "text": "Different neural network architectures have been recently proposed to perform matching of semantically related text segments (Yu et al., 2014; Hu et al., 2014; dos Santos et al., 2015; Wang & Nyberg, 2015; Severyn & Moschitti, 2015; Tan et al., 2015). In this section we briefly review two NN\narchitectures that have previously been applied to the answer selection task: QA-CNN (Feng et al., 2015) and QAbiLSTM (Tan et al., 2015). Given a pair (q, a) consisting of a question q and a candidate answer a, both networks score the pair by first computing fixed-length independent continuous vector representations rq and ra, and then computing the cosine similarity between these two vectors.\nIn Figure 1 we present a joint illustration of these two neural networks. The first layer in both QA-CNN and QAbiLSTM transforms each input word w into a fixed-size real-valued word embedding rw \u2208 Rd. Word embeddings (WEs) are encoded by column vectors in an embedding matrix W 0 \u2208 Rd\u00d7|V |, where V is a fixed-sized vocabulary and d is the dimention of the word embeddings. Given the input pair (q, a), where the question q contains M tokens and the candidate answer a contains L tokens, the output of the first layer consists of two sequences of word embeddings qemb = {rw1 , ..., rwM } and aemb = {rw1 , ..., rwL}. Next, QA-CNN and QA-biLSTM use different approaches to process these sequences. While QA-CNN process both qemb and aemb using a convolution, QA-biLSTM uses a Bidirectional Long Short-Term Memory RNN (Hochreiter & Schmidhuber, 1997) to process these sequences."}, {"heading": "2.1. Convolution", "text": "Given the sequence qemb = {rw1 , ..., rwM }, let us define the matrix Zq = [z1, ..., zM ] as a matrix where each column contains a vector zm \u2208 Rdk that is the concatenation of a sequence of k word embeddings centralized in the mth word of the question. The output of the convolution with c filters over the question q is computed as follows:\nQ =W 1Zq + b1 (1)\nwhere each column m in Q \u2208 Rc\u00d7M contains features extracted in a context window around them-th word of q. The matrix W 1 and the vector b1 are parameters to be learned. The number of convolutional filters c, and the size of the word context window k are hyper-parameters to be chosen by the user.\nIn a similar manner, and using the same NN parametersW 1 and b1, we compute A \u2208 Rc\u00d7L, the output of the convolution over the candidate answer a.\nA =W 1Za + b1 (2)"}, {"heading": "2.2. Bidirectional LSTM (biLSTM)", "text": "Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modification. Given the sequence qemb = {rw1 , ..., rwM }, the hidden vector h(t) (with size H) at the time step t is updated as follows:\nit = \u03c3(Wir wt +Uih(t\u2212 1) + bi) (3) ft = \u03c3(Wfr wt +Ufh(t\u2212 1) + bf ) (4) ot = \u03c3(Wor wt +Uoh(t\u2212 1) + bo) (5)\nC\u0303t = tanh(Wmr wt +Umh(t\u2212 1) + bm) (6)\nCt = it \u2217 C\u0303t + ft \u2217 Ct\u22121 (7) ht = ot \u2217 tanh(Ct) (8)\nIn the LSTM architecture, there are three gates (input i, forget f and output o), and a cell memory vector c. \u03c3 is the sigmoid function. The input gate can determine how incoming vectors rwt alter the state of the memory cell. The output gate can allow the memory cell to have an effect on the outputs. Finally, the forget gate allows the cell to remember or forget its previous state. W \u2208 RH\u00d7d, U \u2208 RH\u00d7H and b \u2208 RH\u00d71 are the network parameters.\nSingle direction LSTMs suffer a weakness of not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions, and generate two independent sequences of LSTM output vectors. One processes the input sequence in the forward direction, while the other processes the input in the reverse direction.\nThe output at each time step is the concatenation of the two output vectors from both directions, ie. ht = \u2212\u2192 ht \u2016 \u2190\u2212 ht . We define c = 2\u00d7H for the notation consistency with the previous subsection. After computing the hidden state ht for each time step t, we generate the matrices Q \u2208 Rc\u00d7M and A \u2208 Rc\u00d7L, where the j-th column in Q (A) corresponds to j-th hidden state hj that is computed by the biLSTM when processing q (a). The same network parameters are used to process both questions and candidate answers."}, {"heading": "2.3. Scoring and Training Procedure", "text": "Given the matrices Q and A, we compute the vector representations rq \u2208 Rc and ra \u2208 Rc by applying a column-wise max-pooling over Q and A, followed by a non-linearity. Formally, the j-th elements of the vectors rq and ra are compute as follows:\n[rq]j = tanh ( max\n1<m<M [Qj,m]\n) (9)\n[ra]j = tanh ( max 1<l<L [Aj,l] ) (10)\nThe last layer in QA-CNN and QA-biLSTM scores the input pair (q,a) by computing the cosine similarity between the two representations:\ns(q, a) = rq.ra\n\u2016rq\u2016\u2016ra\u2016 (11)\nBoth networks are trained by minimizing a pairwise ranking loss function over the training set D. The input in each round is two pairs (q, a+) and (q, a\u2212), where a+ is a ground truth answer for q, and a\u2212 is an incorrect answer. As in (Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss:\nL = max{0,m\u2212 s\u03b8(q, a+) + s\u03b8(q, a\u2212)} (12)\nwhere m is constant margin, s\u03b8(q, a+) and s\u03b8(q, a\u2212) are scores generated by the network with parameter set \u03b8. During training, for each question we randomly sample 50 negative answers from the entire answer set, but only use the one with the highest score to update the model.\nWe use stochastic gradient descent (SGD) to minimize the loss function with respect to \u03b8. The backpropagation algorithm is used to compute the gradients of the network."}, {"heading": "3. Attentive Pooling Networks for Answer Selection", "text": "Attentive pooling is an approach that enables the pooling layer to be aware of the current input pair, in a way that information from the question q can directly influence\nthe computation of the answer representation ra, and vice versa. The main idea consists of learning a similarity measure over the projected segments in the input pairs, and uses the similarity scores between the segments to compute attention vectors. When AP is applied to CNN, which we call AP-CNN, the network learns the similarity measure over the convolved input sequences. When AP is applied to biLSTM, which we call AP-biLSTM, the network learns the similarity measure over the hidden states produced by the biLSTM when processing the two input sequences. We use a similarity measure that has a bilinear form but followed by a non-linearity.\nIn Fig. 2, we illustrate the application of AP over the output of the convolution or the biLSTM to construct the representations rq and ra. Consider the input pair (q, a) where the question has size M and the answer has size L1. After we compute the matrices Q \u2208 Rc\u00d7M and A \u2208 Rc\u00d7L, either by convolution or biLSTM, we compute the matrix G \u2208 RM\u00d7L as follows:\nG = tanh ( QTUA ) (13)\nwhere U \u2208 Rc\u00d7c is a matrix of parameters to be learned by the NN. When the convolution is used to compute Q and A, the matrix G contains the scores of a soft alignment between the convolved k-size context windows of q and a. When the biLSTM is used to compute Q and A, the matrix G contains the scores of a soft alignment between the hidden vectors of each token in q and a.\nNext, we apply column-wise and row-wise max-poolings over G to generate the vectors gq \u2208 RM and ga \u2208 RL, respectively. Formally, the j-th elements of the vectors gq and ga are computed as follows:\n[gq]j = max 1<m<M [Gj,m] (14)\n[ga]j = max 1<l<L [Gl,j ] (15)\nWe can interpret each element j of the vector ga as an importance score for the context around the j-th word in the candidate answer a with regard to the question q. Likewise, each element j of the vector gq can be interpreted as the importance score for the context around the j-th word in the question q with regard to the candidate answer a.\nNext, we apply the softmax function to the vectors gq and ga to create attention vectors \u03c3q and \u03c3a. For instance, the j-th element of the vector \u03c3q is computed as follows:\n[\u03c3q]j = e[g q ]j\u2211 1<l<M e[g q ]l\n(16)\n1In Fig. 2, q has a size of five and a has a size of seven.\nFinally, the representations rq and ra are computed as the dot product between the attention vectors \u03c3q and \u03c3a and the output of the convolution (or biLSTM) over q and a, respectively:\nrq = Q\u03c3q (17)\nra = A\u03c3a (18)\nLike in QA-CNN and QA-biLSTM, the final score is also computed using the cosine similarity between rq and ra. We use SGD to train AP-CNN and AP-biLSTM by minimizing the same pairwise loss function used in QA-CNN and QA-biLSTM."}, {"heading": "4. Related Work", "text": "Traditional work on answer selection have normally used feature engineering, linguistic tools, or external resources (Yih et al., 2013; Wang & Manning, 2010; Wang et al., 2007). Recently, deep learning (DL) approaches have been exploited for this task and achieved significant outperformance compared to traditional non-DL methods. For example, in (Yu et al., 2014; Feng et al., 2015; Severyn & Moschitti, 2015), the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on top of these representations. In Wang & Nyberg (2015), first a joint feature vectors is learned from a joint long short-term memory (LSTM) model connecting questions and answers, and then the task is converted into a learning-to-rank problem.\nAt the same time, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), caption generation (Xu et al., 2015) and factoid question answering (Hermann et al., 2015). Such models learn to focus their attention to specific parts of their input.\nSome recently-proposed approaches introduce attention mechanisms in the answer selection task. Tan et al. (2015) developed an attentive reader based on bidirectional long short-term memory, which emphasizes certain part of the answer according to the question embedding. Unlike (Tan et al., 2015), in which attention is imposed only on answer embedding generation, AP-CNN and AP-biLSTM consider the interdependence between questions and answers.\nIn the context of two-way attention, two very recent work are related to ours. Rockta\u0308schel et al. (2015), propose a two-way attention method that is inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding. Their approach, which is designed for RNNs only, differs in many aspects from the approach described in this work, which can be easily applied for CNNs and RNNs. Yin et al. (2015) present a two-way attention mechanism that is tailored to CNNs. Some of the main differ-\nences between their approach and this work are: (1) they use a simple Euclidean distance to compute the interdependence between the two input texts, while in this work we apply similarity metric learning, which has the potential to learn better ways to measure the interaction between segments of the input items; (2) the models in (Yin et al., 2015) compute the attention vector using sum-pooling over the alignment matrix and use the convolutional outputs updated by the attention as the input for another level of convolutional layer. In this work we use max-pooling over the alignment matrix plus softmax, in order to explicitly create an attention vector that is used to perform the pooling. Experimental results show that such difference yields substantial improvement of performance on WikiQA dataset."}, {"heading": "5. Experimental Setup", "text": ""}, {"heading": "5.1. Datasets", "text": "We apply AP-CNN, AP-biLSTM, QA-CNN and QAbiLSTM to three different answer selection datasets: Insur-\nanceQA, TREC-QA and WikiQA. These datasets contain text of different domains and have different caracteristics. Table 1 presents some statistics about the datasets, including the number of questions in each set, average length of questions (M) and answers (L), average number of candidate answers in the dev/test sets and the average ratio between the lengths of questions and their ground-truth answers.\nInsuranceQA2 is a recently released large-scale non-factoid QA dataset from the insurance domain. This dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between questions of the two test sets. For each question in dev/test sets, there is a set of 500 candidate answers, which include the ground-truth answers and randomly selected negative answers. More details can be found in (Feng et al., 2015).\nTREC-QA3 was created by Wang et al. (2007) based on\n2git clone https://github.com/shuzi/insuranceQA.git 3The data is obtained from (Yao et al., 2013)\nText REtrieval Conference (TREC) QA track (8-13) data. We follow the exact approach of train/dev/test questions selection in (Wang & Nyberg, 2015), in which all questions with only positive or negative answers are removed. Finally, we have 1162 training questions, 65 development questions and 68 test questions.\nWikiQA4 is an open domain question-answering dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 question/candidate pairs in train, 1,130 pairs in dev and 2,352 pairs in test. We adopt the standard setup of only considering questions that have correct answers for evaluation."}, {"heading": "5.2. Word Embeddings", "text": "In order to fairly compare our results with the ones in previous work, we use two different sets of pre-trained word embeddings. For the InsuranceQA dataset, we use the 100- dimensional vectors that were trained by Feng et al. (2015) using word2vec (Mikolov et al., 2013). Following Wang & Nyberg (2015), Tan et al. (2015) and Yin et al.(2015), for the TREC-QA and the WikiQA datasets we use the 300- dimensional vectors that were trained using word2vec and are publicly available on the website of this tool5."}, {"heading": "5.3. Neural Networks Setup", "text": "In Table 2, we show the selected hyperparameter values, which were tuned using the validation sets. We try to use as much as possible the same hyperparameters for all the three datasets. The size of the word embeddings is different due to the different pre-trained versions that we used for InsuranceQA and the other two datasets. We use a context window of size 3 for InsuranceQA, while we set this parameter to 4 for TREC-QA and WikiQA. Using the selected hyperparameters, the best results are normally achieved using between 15 and 25 training epochs. For AP-CNN, APbiLSTM and QA-LSTM, we also use a learning rate schedule that decreases the learning rate \u03bb according to the training epoch t. Following dos Santos & Zadrozny (2014), we\nhttp://cs.jhu.edu/\u02dcxuchen/packages/ jacana-qa-naacl2013-data-results.tar.bz2\n4The data is obtained from (Yang et al., 2015) 5https://code.google.com/p/word2vec/\nset the learning rate for epoch t, \u03bbt, using the equation: \u03bbt = \u03bb\nt .\nIn our experiments, the four NN architectures QA-CNN, AP-CNN, QA-biLSTM and AP-biLSTM are implemented using Theano (Bergstra et al., 2010)."}, {"heading": "6. Experimental Results", "text": ""}, {"heading": "6.1. InsuranceQA", "text": "In Table 3, we present the experimental results of the four NNs for the InsuranceQA dataset. The results are in terms of accuracy, which is equivalent to precision at top one. On the bottom part of this table, we can see that AP-CNN outperforms QA-CNN by a large margin in both test sets, as well as in the dev set. AP-biLSTM also outperforms the QA-biLSTM in all the three sets. AP-CNN and APbiLSTM have similar performance.\nOn the top part of Table 3 we present the results of two state-of-the-art systems for this dataset. In (Feng et al., 2015), the authors present a CNN architecture that is similar to QA-CNN, but that uses a different similarity metric instead of cosine similarity. In (Tan et al., 2015), the authors use a biLTSM architecture that employs unidirectional attention. Both AP-CNN and AP-biLSTM outperform the state-of-the-art systems.\nOne important characteristic of AP-CNN is that it requires less convolutional filters than QA-CNN. For the InsuranceQA dataset, AP-CNN uses 10x less filters (400) than QA-CNN (4000). Using 800 filters in AP-CNN produces very similar results as using 400. On the other hand, as also found in (Feng et al., 2015), QA-CNN requires at least 2000 filters to achieve more than 60% accuracy on Insur-\nanceQA. AP-CNN needs less filters because it does not rely only on the final vector representation to capture interactions between the input question and answer. As a result, although AP-CNN has a more complex architecture, its training time is two times faster than QA-CNN. Using a Tesla K20Xm, our Theano implementation of AP-CNN takes about 16 minutes to complete one epoch (training + inference over validation set) for InsuranceQA, which consists on processing 1.5 million text segments.\nIn figures 3 and 4, we plot the aggregated accuracy of AP-CNN and QA-CNN for answers up to a certain length for the Test1 and Test2 sets, respectively. We can see in both plots that the performance of both system is better for shorter answers. However, while the performance of QA-CNN continues to drop as larger answers are considered, the performance of AP-CNN seems to be stable after reaching a length of\u223c90 tokens. These results give support to our hypothesis that attentive pooling helps the CNN to become robust to larger input texts."}, {"heading": "6.2. TREC-QA", "text": "In Table 4, we present the experimental results of the four NNs for the TREC-QA dataset. The results are in terms of mean average precision (MAP) and mean reciprocal rank (MRR), which are the metric normally used in previous work with the same dataset. We use the official trec eval\nscorer to compute MAP and MRR. We can see in Table 4 that AP-CNN outperforms QA-CNN by a large margin in both metrics. AP-biLSTM outperforms the QA-biLSTM, but its performance is not as good as the of AP-CNN.\nOn the top part of Table 4 we present the results of three recent work that use TREC-QA as a benchmark. In (Wang & Nyberg, 2015), the authors present an LTSM architecture for answer selection. Their best result consists of a combination of LSTM and the BM25 algorithm. In (Severyn & Moschitti, 2015), the authors propose an NN architecture where the representations created by a convolutional layer are the input to similarity measure learning. Wang & Ittycheriah (2015) propose a word-alignment-based method that is suitable for the FAQ-based QA task. AP-CNN outperforms the state-of-the-art systems in both metrics, MAP and MRR."}, {"heading": "6.3. WikiQA", "text": "Table 5 shows the experimental results of the four NNs for the WikiQA dataset. Like in the other two datasets, AP-CNN outperforms QA-CNN, and AP-biLSTM outperforms the QA-biLSTM. The difference of performance between AP-CNN and QA-CNN is smaller than the one for the InsuranceQA dataset. We believe it is because the average size of the answers in WikiQA (25) is much smaller\nthan in InsuranceQA (95). It is expected that attentive pooling bring more impact to the datasets that have larger answer/question lengths.\nIn Table 5 we also present the results of two recent work that use WikiQA as a benchmark. Yang et al. (2015), present a bigram CNN model with average pooling. In (Yin et al., 2015), the authors propose an attention-based CNN. In order to make a fair comparison, in Table 5 we include Yin et al.\u2019s result that use word embeddings only6. AP-CNN outperforms these two systems in both metrics."}, {"heading": "6.4. Attentive Pooling Visualization", "text": "Figures 5 and 6 depict two heat maps of two test questions from InsuranceQA that were correctly answered by APCNN and whose answers are more than 100 words long. The stronger the color of a word in the question (answer), the larger the attention weight in \u03c3q (\u03c3a) of the trigram centered at that word. As we can see in the pictures, the attentive pooling mechanism is indeed putting more focus on the segments of the answer that have some interaction with the question, and vice-verse."}, {"heading": "7. Conclusions", "text": "We present attentive pooling, a two-way attention mechanism for discriminative model training. The main contributions of the paper are: (1) AP is more general than\n6Yin et al. (Yin et al., 2015) report 0.6921(MAP) and 0.7108(MRR) when using handcrafted features in addition to word embeddings.\nrecently proposed two-way attention mechanism because: (a) it learns how to compute interactions between the items in the input pair; and (b) it can be applied to both CNNs and RNNs; (2) we demonstrate that AP can be effectively used with CNNs and biLSTM in the context of the answer selection task, using three different benchmark datasets; (3) our experimental results demonstrate that AP helps the CNN to cope with large input texts; (4) we present new state-of-theart results for InsuranceQA and TREC-QA datasets. (5) for the WikiQA dataset our results are the best reported so far for methods that do not use handcrafted features."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Piero Molino for creating the script used to produce the text heat maps presented in this work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Cicero Nogueira", "Zadrozny", "Bianca"], "venue": "In Proceedings of the 31st International Conference on Machine Learning, JMLR: W&CP volume", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Feng", "Minwei", "Xiang", "Bing", "Glass", "Michael R", "Wang", "Lidan", "Zhou", "Bowen"], "venue": "arXiv preprint:1508.01585,", "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Jurgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Rockt\u00e4schel", "Tim", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Kocisk\u00fd", "Tom\u00e1s", "Blunsom", "Phil"], "venue": "CoRR, abs/1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Aliaksei", "Moschitti", "Alessandro"], "venue": "Proceedings of SIGIR,", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lstm-based deep learning models for nonfactoid answer selection", "author": ["Tan", "Ming", "dos Santos", "Cicero", "Xiang", "Bing", "Zhou", "Bowen"], "venue": "CoRR, abs/1511.04108,", "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question answering", "author": ["Wang", "Mengqiu", "Manning", "Christopher"], "venue": "The Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Wang", "Mengqiu", "Smith", "Noah", "Teruko", "Mitamura"], "venue": "The Proceedings of EMNLP-CoNLL,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Faq-based question answering via word alignment", "author": ["Wang", "Zhiguo", "Ittycheriah", "Abraham"], "venue": "arXiv preprint arXiv:1507.02628,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston", "Jason", "Chopra", "Sumit", "Adams", "Keith"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yang", "Yi", "Yih", "Wen-tau", "Meek", "Christopher"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Yao", "Xuchen", "Durme", "Benjamin", "Clark", "Peter"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih", "Wen-tau", "Chang", "Ming-Wei", "Meek", "Christopher", "Pastusiak", "Andrzej"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguist (ACL),", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "ABCNN: attention-based convolutional neural network for modeling sentence", "author": ["Yin", "Wenpeng", "Sch\u00fctze", "Hinrich", "Xiang", "Bing", "Zhou", "Bowen"], "venue": "pairs. CoRR,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Yu", "Lei", "Hermann", "Karl M", "Blunsom", "Phil", "Pulman", "Stephen"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": ", 2015), machine translation (Bahdanau et al., 2015) and factoid question answering (Hermann et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 4, "context": ", 2015) and factoid question answering (Hermann et al., 2015).", "startOffset": 39, "endOffset": 61}, {"referenceID": 14, "context": "For example, a convolution (or a RNN) and a max-pooling is used to independently construct distributed vector representations of the input pair, followed by a large-margin training (Hu et al., 2014; Weston et al., 2014; Shen et al., 2014; dos Santos et al., 2015).", "startOffset": 181, "endOffset": 263}, {"referenceID": 19, "context": "Different neural network architectures have been recently proposed to perform matching of semantically related text segments (Yu et al., 2014; Hu et al., 2014; dos Santos et al., 2015; Wang & Nyberg, 2015; Severyn & Moschitti, 2015; Tan et al., 2015).", "startOffset": 125, "endOffset": 250}, {"referenceID": 10, "context": "Different neural network architectures have been recently proposed to perform matching of semantically related text segments (Yu et al., 2014; Hu et al., 2014; dos Santos et al., 2015; Wang & Nyberg, 2015; Severyn & Moschitti, 2015; Tan et al., 2015).", "startOffset": 125, "endOffset": 250}, {"referenceID": 2, "context": "In this section we briefly review two NN architectures that have previously been applied to the answer selection task: QA-CNN (Feng et al., 2015) and QAbiLSTM (Tan et al.", "startOffset": 126, "endOffset": 145}, {"referenceID": 10, "context": ", 2015) and QAbiLSTM (Tan et al., 2015).", "startOffset": 21, "endOffset": 39}, {"referenceID": 3, "context": "Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modification.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": "As in (Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss:", "startOffset": 6, "endOffset": 44}, {"referenceID": 17, "context": "Traditional work on answer selection have normally used feature engineering, linguistic tools, or external resources (Yih et al., 2013; Wang & Manning, 2010; Wang et al., 2007).", "startOffset": 117, "endOffset": 176}, {"referenceID": 12, "context": "Traditional work on answer selection have normally used feature engineering, linguistic tools, or external resources (Yih et al., 2013; Wang & Manning, 2010; Wang et al., 2007).", "startOffset": 117, "endOffset": 176}, {"referenceID": 19, "context": "For example, in (Yu et al., 2014; Feng et al., 2015; Severyn & Moschitti, 2015), the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on top of these representations.", "startOffset": 16, "endOffset": 79}, {"referenceID": 2, "context": "For example, in (Yu et al., 2014; Feng et al., 2015; Severyn & Moschitti, 2015), the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on top of these representations.", "startOffset": 16, "endOffset": 79}, {"referenceID": 2, "context": ", 2014; Feng et al., 2015; Severyn & Moschitti, 2015), the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on top of these representations. In Wang & Nyberg (2015), first a joint feature vectors is learned from a joint long short-term memory (LSTM) model connecting questions and answers, and then the task is converted into a learning-to-rank problem.", "startOffset": 8, "endOffset": 236}, {"referenceID": 0, "context": "At the same time, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 131, "endOffset": 178}, {"referenceID": 9, "context": "At the same time, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 131, "endOffset": 178}, {"referenceID": 4, "context": ", 2015) and factoid question answering (Hermann et al., 2015).", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "Unlike (Tan et al., 2015), in which attention is imposed only on answer embedding generation, AP-CNN and AP-biLSTM consider the interdependence between questions and answers.", "startOffset": 7, "endOffset": 25}, {"referenceID": 10, "context": "Tan et al. (2015) developed an attentive reader based on bidirectional long short-term memory, which emphasizes certain part of the answer according to the question embedding.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Rockt\u00e4schel et al. (2015), propose a two-way attention method that is inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "Rockt\u00e4schel et al. (2015), propose a two-way attention method that is inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding. Their approach, which is designed for RNNs only, differs in many aspects from the approach described in this work, which can be easily applied for CNNs and RNNs. Yin et al. (2015) present a two-way attention mechanism that is tailored to CNNs.", "startOffset": 0, "endOffset": 342}, {"referenceID": 18, "context": "ences between their approach and this work are: (1) they use a simple Euclidean distance to compute the interdependence between the two input texts, while in this work we apply similarity metric learning, which has the potential to learn better ways to measure the interaction between segments of the input items; (2) the models in (Yin et al., 2015) compute the attention vector using sum-pooling over the alignment matrix and use the convolutional outputs updated by the attention as the input for another level of convolutional layer.", "startOffset": 332, "endOffset": 350}, {"referenceID": 2, "context": "More details can be found in (Feng et al., 2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 11, "context": "TREC-QA3 was created by Wang et al. (2007) based on", "startOffset": 24, "endOffset": 43}, {"referenceID": 16, "context": "git The data is obtained from (Yao et al., 2013)", "startOffset": 30, "endOffset": 48}, {"referenceID": 6, "context": "(2015) using word2vec (Mikolov et al., 2013).", "startOffset": 22, "endOffset": 44}, {"referenceID": 2, "context": "For the InsuranceQA dataset, we use the 100dimensional vectors that were trained by Feng et al. (2015) using word2vec (Mikolov et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 2, "context": "For the InsuranceQA dataset, we use the 100dimensional vectors that were trained by Feng et al. (2015) using word2vec (Mikolov et al., 2013). Following Wang & Nyberg (2015), Tan et al.", "startOffset": 84, "endOffset": 173}, {"referenceID": 2, "context": "For the InsuranceQA dataset, we use the 100dimensional vectors that were trained by Feng et al. (2015) using word2vec (Mikolov et al., 2013). Following Wang & Nyberg (2015), Tan et al. (2015) and Yin et al.", "startOffset": 84, "endOffset": 192}, {"referenceID": 2, "context": "For the InsuranceQA dataset, we use the 100dimensional vectors that were trained by Feng et al. (2015) using word2vec (Mikolov et al., 2013). Following Wang & Nyberg (2015), Tan et al. (2015) and Yin et al.(2015), for the TREC-QA and the WikiQA datasets we use the 300dimensional vectors that were trained using word2vec and are publicly available on the website of this tool5.", "startOffset": 84, "endOffset": 213}, {"referenceID": 15, "context": "bz2 The data is obtained from (Yang et al., 2015) https://code.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In (Feng et al., 2015), the authors present a CNN architecture that is similar to QA-CNN, but that uses a different similarity metric instead of cosine similarity.", "startOffset": 3, "endOffset": 22}, {"referenceID": 10, "context": "In (Tan et al., 2015), the authors use a biLTSM architecture that employs unidirectional attention.", "startOffset": 3, "endOffset": 21}, {"referenceID": 2, "context": "On the other hand, as also found in (Feng et al., 2015), QA-CNN requires at least 2000 filters to achieve more than 60% accuracy on Insur-", "startOffset": 36, "endOffset": 55}, {"referenceID": 18, "context": "In (Yin et al., 2015), the authors propose an attention-based CNN.", "startOffset": 3, "endOffset": 21}, {"referenceID": 15, "context": "Yang et al. (2015), present a bigram CNN model with average pooling.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "(Yin et al., 2015) report 0.", "startOffset": 0, "endOffset": 18}], "year": 2016, "abstractText": "In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other\u2019s representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.", "creator": "LaTeX with hyperref package"}}}