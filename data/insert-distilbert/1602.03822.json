{"id": "1602.03822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Neural Network Support Vector Detection via a Soft-Label, Hybrid K-Means Classifier", "abstract": "we use random geometric graphs instead to perfectly describe clusters of higher dimensional data points objects which should are bijectively mapped to a ( possibly ) lower dimensional space where an equivalent random cluster model is used to calculate the likelihood expected number of coherent modes to be originally found when separating the data of a weighted multi - modal data set into distinct clusters. furthermore, as a decreasing function of the expected number of modes and by the number of data points in the sample, an upper bound on a given distance measure is found such that data points have the greatest correlation if their mutual distances from a common center is less than or equal to the calculated bound. distant anomalies are exposed, which lie outside of the union of with all regularized clusters devoid of data points.", "histories": [["v1", "Thu, 11 Feb 2016 18:37:53 GMT  (93kb)", "https://arxiv.org/abs/1602.03822v1", "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn"], ["v2", "Fri, 12 Feb 2016 19:40:52 GMT  (93kb)", "http://arxiv.org/abs/1602.03822v2", "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn"], ["v3", "Wed, 24 Feb 2016 20:15:24 GMT  (93kb)", "http://arxiv.org/abs/1602.03822v3", "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn"], ["v4", "Sun, 10 Apr 2016 19:45:22 GMT  (93kb)", "http://arxiv.org/abs/1602.03822v4", "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn"], ["v5", "Tue, 19 Apr 2016 02:14:02 GMT  (93kb)", "http://arxiv.org/abs/1602.03822v5", "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn"]], "COMMENTS": "This paper is a combined replacement for the papers \"A Neural Network Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the Sharp Threshold Interval Length of Partially Connected Random Geometric Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all been withdrawn", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["robert a murphy"], "accepted": false, "id": "1602.03822"}, "pdf": {"name": "1602.03822.pdf", "metadata": {"source": "CRF", "title": "Neural Network Support Vector Detection via a Soft-Label, Hybrid K-Means Classifier", "authors": ["Robert A. Murphy"], "emails": ["robert.a.murphy@wustl.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n03 82\n2v 5\n[ cs\n.L G\n] 1\n9 A\npr 2\n01 6\nSimilar to finding a hyperplane which can be shifted along its normal to expose the maximal distance between binary classes, it is shown that the union of regularized clusters can be used to define a hyperplane which can be shifted by a certain amount to separate the data into binary classes and that the shifted hyperplane defines the activation function for a two-class discriminating neural network. Lastly, this neural network is used to detect the set of support vectors which determines the maximally-separating region between the binary classes.\nI. INTRODUCTION AND RELATED WORKS\nInherent in traditional support vector detection is the assumption that the data are separable into two distinct classes, with the data points in each class being labeled distinctly, while minimizing mislabeling instances. In the case of linear support vector detection, a single hyperplane is sought, whereby the single hyperplane can be optimally shifted along its normal vector and its reflection. The shifted hyperplane and its reflection across the original, single hyperplane defines an optimal intermediate region, along with two distinctly labeled classes.\nAssume the existence of a training set with known labels derived from a set of two distinct values. Since the weight vector for the single hyperplane defines its normal vector, then the problem reduces to finding a single weight vector such that its squared norm is minimized, subject to the constraint that the individually labeled training examples lie to one side of the original hyperplane, along with others sharing the same label. After which, by combining both the squared norm term and the equation for the original hyperplane, the constrained optimization problem is converted to an unconstrained optimization problem and solved using the method of Lagrange multipliers. The reader is referred to [1, pp. 311\u2212 314] for mathematical formalities on linear support vector machines.\nIn case the data are not linearly separable, the aforementioned algorithm will not work, as stated. Slack variables have to be introduced, which has the effect of reducing the area of the intermediate region, resulting in the constraint being modified. Furthermore, in the unconstrained problem, the slack\nvariables are taken into account by adding a penalty term as something on the order of the number of misclassifications encountered, where misclassifications take one of two forms. Either some data points are counted as unclassified, because they cannot be separated from the intermediate region, or they are misclassified, because they are incorrectly seen as training examples which belong to the class containing data points with a different label. The reader is referred to [1, pp. 315\u2212 318] for mathematical formalities on non-linear support vector machines.\nFor data sets which are extremely large, relative to its number of attributes, Mangasarian, et.al. showed in [55] that a linear support vector machine suffices when separating the data into two distinct sets with small error. Yet, in other kinds of problems, where the data are not linearly separable, or where a linear support vector machine gives rise to a significant number of misclassifications or an intermediate region with a high number of unclassified data points, the requirement for a nonlinear support vector machine can be too expensive, both in time complexity and memory usage [17].\nThe prevailing methods for dealing with the expense associated with solving the non-linear optimization problem are variations of the decomposition algorithm, [40], [42], [45], [63], whereby the data are projected to a lower dimensional subspace, whose basis vectors are chosen such that the projected data are linearly separable. By linear separability of the data, the unconstrained, non-linear problem can be solved in the subspace via a linear support vector machine, resulting in a sequence of solvable sub-problems in each projected subspace. Influenced by Keerthi, et.al [44], Chung, et.al. [17] propose the use of specific seed values for the set of Lagrange multipliers of the linear support vector machines in each subspace such that faster convergence to the solution of the non-linear optimization problem is achieved. Likewise, the chunking method of Vapnik [75] makes use of the fact that the rows and columns of the matrix in the quadratic constraint, which correspond to Lagrange multipliers with a zero value, can be removed, while retaining the same solution through each iteration of a sub-problem.\nMotivated to produce a method of solution which does not require iterative solutions of quadratic programs and does not require checking of certain conditions to ensure convergence to a solution, a method is provided such that the data are projected into a two dimensional space, whereby the projected data are either linearly separable or the data forms one contiguous class. When the data are separable, linear separation is achieved by forming a linear regression hyperplane through one of the classes and shifting it an appropriate amount in order to achieve separation of the classes. The associated shift\ncorresponds to the identification of the set of support vectors by measuring distances to the regression hyperplane from data points in the set which is in the complement of the set of data points used to form the regression hyperplane.\nSuppose infinitely many copies of a bounded structure are used to partition R2 and let B \u2282 R2 be a bounded subset containing finitely many copies of the bounded structure. Further suppose that structures in the partition are neighbors, if their respective boundaries have non-empty intersection and infinitely many of the bounded structures in the partition are individually occupied by exactly one point at the center of the structure, independently of all other structures. In [32] and [33], it is shown that, if the probability of neighboring structures each containing related points is greater than some critical value, then with probability 1, a path can be traced from any starting occupied bounded structure to any ending occupied bounded structure, with the path in between the start and the end consisting entirely of neighboring occupied structures. From this statement, the contrapositive statement is obtained such that, if the probability of neighboring structures each containing related points is less than or equal to the same critical value, then with probability 1, no such path exists for any two bounded structures. Hence, all points are either related to no other points or only related to finitely many points in neighboring bounded structures. It is this contrapositive condition that is of interest and great use during K-means classification, as classes are formed by groupings of interrelated data points.\nRarely does inter-related, real-world data conform to a predefined, rigid partition, as described above. As such, after removing the rigid partition of R2, suppose that the data are modeled by a node process which randomly generates points within B according to some predetermined probability distribution. Points in B are inter-related, if they are within a certain distance of one another or some common point, such as the average of a set of previously-grouped, inter-related points. In [57], it is shown that, with probability 1, there is a path of inter-related points between any two points in B, if the number of points relative to the area of B is beyond a certain critical number or, if the maximum distance between inter-related points is larger than a certain critical number.\nIn [60], it is proven in cor. (7.4.39) that an ordered set of data, which is assumed to be spatially uniformly distributed, will form clusters, i.e. classes, if the measured distances between data points (or some common data point in each class) are below a certain threshold, which is computed as a function of the number of data points sampled from the total population of data. Now, most datasets would be too computationally expensive to order. So, we can get around this limitation by making certain assumptions.\nWe can make the assumption that the node process generates points according to the normal distribution by making use of a theorem from [38] in probability called the Central Limit Theorem. In essence, this theorem states that any set of randomly distributed data with finite mean and variance will tend to be normally distributed as the sample size grows large. This accounts for the ubiquity of the normal distribution in nature and why it\u2019s safe to make an assumption of normality in most cases. As such, the node process is allowed to run until J = M2 points are generated, as represented by a sequence\nof independent, identically distributed random variables, X1, X2, ... , XJ , each with mean 0 and variance 1. An order statistic is applied to the these random variables so that Xk1 < Xk2 < ... < XkJ , where \u03c3(i) = ki for i \u2208 {1, 2, ..., J} is a permutation resulting in the ordering of {Xi}Ji=1. Note that, by default, {Xki} J i=1 is a sequence of dependent random variables since for each i \u2208 {2, 3, ..., J}, the random variable Xki depends upon Xkj for all j < i. Now, {Xki} J i=1 is a (less computationally expensive) ordering of a set of points generated by the node process. The edge space of the higher dimensional data is then embedded within the 2-dimensional plane in order to aid in the calculation of a threshold on the distance measure.\nTo make the assumption of uniformity of the ordered set of points and to perform clustering therein, it is first noted that the Beta distribution is the probability distribution of an order statistic of normally distributed random variables [54]. The Beta distribution shape parameters are then defined to be, \u03b1 = 1 = \u03b2, since the data points are injectively generated into exactly 1 structure of the uniformly partitioned, 2-dimensional, bounded region. Hence, the ordering of the sample can be assumed to be approximately uniform.\nFinally, with the defined partition, it is shown that under certain conditions, no approximation of probabilities in the continuum is required to prove the existence of a path of any order, as in [57]. Instead, the probability of a long range path in the continuum is equivalent to the probability of a long range path, over the same set of points, in the presence of the defined posterior partition when the maximum radial distance between connected points falls within a certain bounded interval. On this bounded interval, the probability of the existence of a long range path rises sharply when points connect at distances within the bounded interval. Lastly, the probability measure in question is found to be a unique random cluster measure which realizes a set of conditional probability measures. As such, the node process samples from the collection of conditional probability measures to form classes, when points connect at a distance less than or equal to the critical length.\nIn [13], Cai, et.al. investigate the problem of partial connectivity of randomly distributed points in a bounded region by making the assumption that, relative to the size of the bounded region, the number of points to be generated is relatively small. As such, a Poisson-distributed node process generates an independent set of points in the designated region. Copies of a hexagon of some fixed, immutable size, which is not dependent upon distances between generated points, are used to partition the bounded region. Points in the region are deemed to connect to form an open edge, if after the region is partitioned, the points lie within the same hexagon or neighboring hexagons, where hexagons are neighbors, if their respective boundaries have non-empty intersection. Otherwise, the edge between two points is closed. Likewise, they define the logical points at the centers of two neighboring hexagons to connect to form an open edge, if each neighboring hexagon independently contains at least one of the generated points. In [13], Cai, et.al. compute probabilities as a function of the density of hexagons which are occupied by at least one point. They showed that if the number of hexagons in the fixed partition is unbounded and the number of points generated within the continuum of the bounded region is below (or at) a critical threshold, then the\nprobability of a majority of the occupied hexagons (and points contained therein) in the bounded region being connected in a contiguous path will tend to zero. On the other hand, if the density of occupied hexagons is within a short interval around the critical threshold, then a connecting path of hexagons or points, from any start to any end, occurs with probability that rises sharply from some small positive value to a value close to 1 for densities that fall within the range of the short interval.\n[31, Thm. (1.1)] gives an estimate of the length of the short interval. If the area of the bounded region is assumed to be one, without loss of generality, then the estimate of the length of the short interval can never be any better than\n\u0398\n(\nlog1/4(n) \u221a\nlog (n) n\n)\n, where n is the number of points\ngenerated within the bounded region by the node process.\nThis work uses the distance notion of connectivity without the presence of a partition, the same as in [13]. However, it markedly differs in that the prototypical hexagon used in the defined posterior partition of the bounded region is allowed to change in size, if the node process is stopped and started again after a partition has already been defined. Since the prototypical hexagon is allowed to change in size so that the logical centers are closer to (or further away from) each other and point density is inversely proportional to the maximum connection length, therein lies an added advantage when calculating the probability of a connected path of hexagons or points. Moreover, if the prototypical hexagon always shrinks as the node process generates more points, then it should be expected that the critical threshold and the length of the short interval around the critical threshold are intertwined. It is shown that this is, in fact, the case.\nThe rest of this work is organized as follows. In section (II), we formulate the notion of connectivity of randomlygenerated data points in the continuum using random geometric graphs and prove certain continuity results of the probability measure of a class of graph properties. We use the continuity results to show the existence of a critical connectivity radius (equivalent density of data points) and prove that the length of the sharp threshold interval containing the critical radius is of a certain size, depending upon the number of data points and the length of the critical radius.\nIn section (III), we uniformly partition the bounded region into shapes of the same size and formulate the notion of connectivity of randomly-generated data points using the random cluster model. The continuity results of section (II) still hold true and are used to show the existence of a critical connectivity radius, with the associated sharp threshold interval length being of a certain size, depending upon the number of data points and the length of the critical radius. In addition, relationships between the graph properties and probabilities of the graph properties are proven, along with a result about the relationship between the critical radii. These results, as well as other results from the random cluster model, are used to give a practical estimate of the length of the sharp threshold interval and a lower bound estimate of the change in the probability of the class of graph properties. Finally, it is shown that under certain conditions which are best for K-means classification, the probabilities of the graph properties are equivalent and the critical radii are of the same length under both formulations.\nIn section (IV ), given a fixed, ordered sample from an unknown group of multi-modal normal distributions, we estimate the mean number of clusters to form in a typical K-means classification. Likewise, we also estimate the length of the sharp threshold interval as a function of the expected number of classes and an estimate of the size of the critical connectivity radius is given as a function of the number of data points in the sample and the expected number of classes to form.\nIn section (V ), using the setup and results from section (IV ), we define the regularized cluster and the set of anomalies, then prove that the regularized cluster and the set of anomalies are linearly separable and show that the linear regression hyperplane defined by the regularized cluster can be shifted by a certain distance along its normal vector to separate the regularized cluster from the set of anomalies.\nFinally, in section (V I), we show that the set of anomalies is a superset for the set of support vectors defining the shifted hyperplane which delineates the maximally separating region between the binary classes and that the resulting neural network detects all of the support vectors."}, {"heading": "II. RANDOM GEOMETRIC GRAPHS", "text": ""}, {"heading": "A. Definitions", "text": "Definition 1: A node process is a mapping X : R2 \u2192 R2 such that for any subset B \u2282 R2, there is an n \u2208 N and a subset Xn = {xk}1\u2264k\u2264n \u2282 B with X(B) = Xn.\nDefinition 2: Suppose B \u2282 R2 and X is a node process that randomly generates independent points Xn = {xk}1\u2264k\u2264n \u2282 B according to some probability distribution. Let d : R2 \u2192 R be a distance measure. Points x, y \u2208 Xn are said to be r-connected and form an r-open edge, if d(x, y) \u2264 r, for some fixed r > 0. Points x, y \u2208 Xn are r-disconnected and form an r-closed edge otherwise.\nDefinition 3: Let E be the set of edges between points in Xn. G(Xn; r) is defined to be the r-graph of the set of all r-open and r-closed edges from E between points in Xn.\nDefinition 4: Given points x, y \u2208 Xn and some fixed r > 0, denote the r-edge between x and y as < x, y >r. A subset of points C \u2286 Xn forms an r-connected cluster if and only if given any x, y \u2208 C, there exists r-open edges < x, a1 >r, < a1, a2 >r, ..., < ak\u22121, y >r \u2208 E connecting x to y, for points {a1, a2, ..., ak\u22121} \u2286 C.\nDefinition 5: Let A be a set of graphs of E and G(Xn; r) \u2208 A. A is said to be an increasing property if and only if for r\u2032 6= r such that G(Xn; r) \u2286 G(Xn; r\u2032), it is true that G(Xn; r\u2032) \u2208 A.\nDefinition 6: Let \u2126 be the set of values taken by the random variables Xn, with F being any \u03c3-algebra of subsets of \u2126 and P a probability measure on (\u2126,F). If A is a monotone (increasing) property and \u01eb \u2208 (0, 12 ), define\nr(n, \u01eb) = inf{r > 0 : P (G(Xn; r) \u2208 A) \u2265 \u01eb}\nand \u2206(n, \u01eb) = r(n, 1\u2212 \u01eb)\u2212 r(n, \u01eb).\nIf \u2206(n, \u01eb) = o(1), then A has a sharp threshold."}, {"heading": "B. An Important Result", "text": "Theorem 7: [31, Thm. (1.1)] For increasing properties A consisting of graphs of points Xn \u2282 R2,\n\u2206(n, \u01eb) = \u0398(rc log 1/4(n))\nwhere\nrc = O\n( \u221a\nlogn\nn\n)\n.\nThese writings will be concerned, at least in part, with estimating the length of this critical interval for a particular property A, using this framework."}, {"heading": "C. Procedure", "text": "Let B \u2282 R2 be a bounded region containing the origin 0\u0302 = (0, 0) and let X be a node process such that X(B) = Xn is a set of n points which are uniformly distributed spatially throughout B, where n is a Poisson random variable which takes a particular value (denoted as n) with density parameter \u03bb = \u03bb(n) indicating the expected number of points generated per unit area of B. For some fixed r > 0, points in Xn will connect if their mutual distance is within r. For fixed \u03c1 \u2208 (12 , 1), define A r [n,\u03c1] to be the set of all r-connected graphs of subsets of Xn containing at least 100\u03c1% of all generated points which contain 0\u0302.\nLet \u01eb > 0 be given and let r(n, \u03c1, \u01eb) be the least connectivity radius r > 0 such that P (Ar[n,\u03c1]) \u2265 \u01eb. It will be shown that P (Ar[n,\u03c1]) is an increasing function of the connection radius r. The aim is to estimate the length of the interval of connectivity radii such that the occurrence of Ar[n,\u03c1] increases in probability from a value of \u01eb to a value of 1\u2212\u01eb on the interval of radii. On this interval will be associated a particular radius such that the probability of the occurrence of Ar[n,\u03c1] is 1/2. On the left half of the interval, it is more likely that classes will form, with each containing less than half of all points so that no one class contains the majority of data points. No one class containing the majority of data points is important since, in this event, no one class will contain all data points with probability 1, which is guaranteed by [57] in the continuum and [33] in the partitioned continuum.\nAs an integral step in estimating the length of the interval of radii, continuity in r and \u03c1 of P (Ar[n,\u03c1]) will be shown. As such, by continuity in \u03c1, for small \u03b4 > 0, the probability of Ar[n,\u03c1] is close to the probabilities of A r [n,\u03c1+\u03b4] and Ar[n,\u03c1\u2212\u03b4]. Furthermore, by continuity and the increasing nature of P (Ar[n,\u03c1]) in r, there exists r0 = r0(n, \u03c1, \u01eb) such that P (Ar0[n,\u03c1]) = 1/2. This particular radius of connectivity demarcates the point, beyond which, the generated set of points will transition from a set of disjoint classes to one giant, interrelated class of points, almost surely. Furthermore, for \u01eb > 0, this radius of connectivity is the center of the estimated interval of radii, upon which, P (Ar[n,\u03c1]) increases from \u01eb to 1\u2212 \u01eb."}, {"heading": "D. Definitions", "text": "Definition 8: Given a fixed point, y \u2208 Xn, an r-connected component containing y is the subset of points < Cy >r \u2286 Xn\ncontaining y and every x \u2208 Xn\\{y} having a set of r-open edges connecting x to y.\nDefinition 9: Given an r-open edge, e = < x, y >r \u2208 G(Xn; r), an r-connected component containing e is the subset of points < Ce >r \u2286 Xn containing x and y together with every z \u2208 Xn\\{x, y} having a set of r-open edges connecting z to both x and y.\nDefinition 10: Let E be any \u03c3-algebra of subsets of E such that \u2205, E \u2208 E , any A \u2208 E implies Ac \u2208 E and all countable unions of subsets of E is again in E . Suppose {\u03b7k}k\u22651 is a sequence of random variables on E taking values in R. It will be said that \u03b7k converges weakly to a random variable \u03b7 : E \u2192 R (written \u03b7k \u21d2 \u03b7), if\nlim k\u2192\u221e Fk(x) = lim k\u2192\u221e P (\u03b7k \u2264 x)\n= P (\u03b7 \u2264 x)\n= F (x)\nfor all x \u2208 R."}, {"heading": "E. The Event", "text": "1) Bounded Number of Nodes: Let < C >r \u2286 Xn be an r-connected component containing 0\u0302 such that | < C >r | = N and define \u03c1n(C) = Nn . For \u03c1 \u2208 ( 1 2 , 1), define the graph property of all connected components containing at least 100\u03c1% of all available points by\nAr[n,\u03c1] = {< C >r \u2286 Xn : \u03c1n(C) \u2265 \u03c1}. (1)\nAs in [31], for \u01eb \u2208 (0, 12 ), define\nr(n, \u03c1, \u01eb) = inf{r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb} (2)\nto be the critical radius at which Ar[n,\u03c1] occurs with probability at least \u01eb and define\n\u2206(n, \u03c1, \u01eb) = r(n, \u03c1, 1 \u2212 \u01eb)\u2212 r(n, \u03c1, \u01eb) (3)\nto be the length of the continuum of radii upon which Ar[n,\u03c1] increases in probability of occurrence from \u01eb > 0 to 1\u2212\u01eb > 0.\n2) Unbounded Number of Nodes: In the case of n being unbounded, define the corresponding graph property to be\nAr = {< C >r \u2286 X\u221e : | < C >r | = \u221e}. (4)\nDefine\nr(\u01eb) = inf{r > 0 : P (Ar) \u2265 \u01eb} (5)\nto be the critical radius at which Ar occurs with probability at least \u01eb and define\n\u2206(\u01eb) = r(1 \u2212 \u01eb)\u2212 r(\u01eb) (6)\nto be the length of the continuum of radii upon which Ar increases in probability of occurrence from \u01eb > 0 to 1\u2212\u01eb > 0."}, {"heading": "F. Continuity Results", "text": "In order to prove the existence of r0 > 0 such that P (Ar0[n,\u03c1]) = 1/2, it will be shown that P (A r [n,\u03c1]) is a continuous function of r. By properties of probabilities measures, P (Ar[n,\u03c1]) \u2208 [0, 1] and by prop. (76), it is true that P (A r [n,\u03c1]) is non-decreasing as a function of increasing r > 0. By thm. (7), it is true that P (Ar[n,\u03c1]) increases from \u01eb > 0 to 1\u2212 \u01eb > 0 for fixed \u01eb \u2208 (0, 12 ). Then, by continuity, there exists r0 > 0 such that P (Ar0[n,\u03c1]) = 1/2. If I is any continuum of radii and P (AI[n,\u03c1]) is defined to be the set {P (A r [n,\u03c1]) : r \u2208 I}, then it is easily seen that r0 is in the interior of any compact interval of radii I\u01eb such that P (A I\u01eb [n,\u03c1]) = [\u01eb, 1\u2212 \u01eb]. Seeking a contradiction, suppose r0 is in the boundary of I\u01eb. Since I\u01eb is compact and P (Ar[n,\u03c1]) is continuous in r, then P (A r0 [n,\u03c1]) = \u01eb or P (Ar0[n,\u03c1]) = 1 \u2212 \u01eb. Therefore, P (A r0 [n,\u03c1]) = 1/2 implies \u01eb = 1/2. This is a contradiction since \u01eb \u2208 (0, 12 ). Thus, r0 is in the interior of I\u01eb. Q.E.D.\nNow, if it can be shown that r0 is independent of \u01eb, then r0 \u2208 I\u01eb for all \u01eb \u2208 (0, 12 ). Note that r0 \u2208 I = \u22c2\nk I\u01ebk for any sequence \u01ebk \u2192 1/2. Clearly I is compact so that r0 is in the interior of I . Therefore, either I is an interval or I = {r0}. Suppose I is an interval of radii. Since r0 is in the interior of I , then there exists r\u20320 < r0 \u2208 I . Now, since \u01ebk \u2192 1/2, then P (A\nr\u20320 [n,\u03c1]) = 1/2 and r \u2032 0 < r0 = inf{r > 0 : P (A r [n,\u03c1]) = 1 2}. This is a contradiction. Therefore, I = {r0} so that r0 is unique. Q.E.D.\nContinuity of P (Ar) in r is proven in [57] and can be used for proving continuity of P (Ar[n,\u03c1]) in r as follows. Let \u2202B denote the boundary of B and define ArB = {0\u0302 \u2194 \u2202B} to be the property that there is an r-connected cluster containing 0\u0302 and a point in \u2202B. By arguments in [57], continuity of P (Ar) in r is equivalent to continuity of P (ArB) in r for all bounded regions B containing 0\u0302. Clearly, P (ArB) = P (A r B \u2212 Ar[n,\u03c1]) + P (A r B \u2229 A r [n,\u03c1]) so that continuity of P (A r B) in r implies continuity of P (ArB \u2229 A r [n,\u03c1]) in r. Now, there exists r\u20320 > 0 such that P (A r B) = 1 for all r \u2265 r \u2032 0. Then, it follows that P (Ar[n,\u03c1]) = P (A r B \u2229A r [n,\u03c1]) is continuous when r \u2265 r\u20320. In particular, P (A r [n,\u03c1]) is continuous at r \u2032 0. So, there is \u03b4 > 0 such that P (Ar[n,\u03c1]) is continuous upon [r \u2032 0\u2212\u03b4, r \u2032 0+\u03b4]. Continuing this argument, continuity of P (Ar[n,\u03c1]) extends until r\u20320 \u2212 \u03b4 = 0 so that P (A r [n,\u03c1]) is continuous for all r \u2265 0. Q.E.D.\nTheorem 11: [57, Thm. (3.8)] Suppose {rk}k\u22651 is a sequence of radii such that 0 < rk \u2264 R for some R > 0 and {\u03b7k}k\u22651 is a sequence of random variables which take values rk with probability 1. If 0 < r \u2264 R and \u03b7 is a random variable taking the value r with probability 1 such that \u03b7k \u21d2 \u03b7 as k \u2192 \u221e. Then, P (A\u03b7k) \u2192 P (A\u03b7) as k \u2192 \u221e.\nProof: This is just a restatement of [57, Thm. (3.8)] for the special case of random variables \u03b7k and \u03b7 such that P (\u03b7k = rk) = 1 = P (\u03b7 = r) for all k \u2265 1.\nCorollary 12: (to Theorem 11) P (Ar[n,\u03c1]) is a continuous function of r.\nProof: Continuity of P (Ar) in r follows from thm. (11). Therefore, the result follows by the discussion preceding the\nstatement of thm. (11).\nTheorem 13: r = r(n, \u03c1, \u01eb) is a continuous function of \u01eb if and only if P (Ar[n,\u03c1]) is a continuous function of r.\nProof: Suppose r(n, \u03c1, \u01eb) is a continuous function of \u01eb and let {\u01ebk \u2208 (0, 12 )}k\u22651 be a sequence of positive real numbers such that \u01ebk \u2192 \u01eb0 as k \u2192 \u221e. Let {X(e)}e\u2208G(Xn;r) be a finite sequence of uniformly distributed random variables with values in [0, 1] and define a sequence of random variables {\u03b7k}k\u22651 by \u03b7k(e) = r(n, \u03c1, \u01ebk) \u2261 rk when X(e) < 1 and 0 otherwise. Clearly, \u03b7k = rk with probability 1 for all k \u2265 1. Likewise, define a random variable \u03b70 by \u03b70(e) = r(n, \u03c1, \u01eb0) \u2261 r0 when X(e) < 1 and 0 otherwise so that \u03b70 = r0 with probability 1. Since r(n, \u03c1, \u01eb) is continuous in \u01eb, then rk \u2192 r0 as k \u2192 \u221e so that \u03b7k \u21d2 \u03b70 as k \u2192 \u221e. Now, define R = 2 \u2217 max{d(x, y) : x, y \u2208 Xn}. By lemma (82), 0 < \u03b7k \u2264 R for all k \u2265 0. Therefore, P (A \u03b7k [n,\u03c1]) \u2192 P (A \u03b70 [n,\u03c1]) as k \u2192 \u221e by cor. (12) since rk \u2192 r0 as k \u2192 \u221e. Thus, P (Ar[n,\u03c1]) is a continuous function of r. Conversely, suppose P (Ar[n,\u03c1]) is a continuous function of r and let {\u01ebk \u2208 (0, 12 )}k\u22651 be any convergent sequence such that \u01ebk \u2192 \u01eb0. Define rk = r(n, \u03c1, \u01ebk) and r0 = r(n, \u03c1, \u01eb0). Given \u03be > 0, it is true that \u039e \u2261 {k \u2265 1 : |P (Ark[n,\u03c1]) \u2212 P (A r0 [n,\u03c1])| \u2265 \u03be} is a set of measure zero by the continuity assumption. Therefore, rk \u2192 r0 as k \u2192 \u221e by prop. (83). Thus, suppose that \u039e 6= \u2205. Then, \u039e is at most countable so that \u039e = \u2205 a.s. Hence, rk \u2192 r0 as k \u2192 \u221e by prop. (83) so that r(n, \u03c1, \u01eb) is a continuous function of \u01eb."}, {"heading": "G. Continuum Giant Component", "text": "Theorem 14: There exists r0 = r0(n, \u03c1, \u01eb) < \u221e, independent of \u01eb, such that\nP (Ar0[n,\u03c1]) = 1\n2 .\nProof: Let \u01eb \u2208 (0, 12 ) be given. Since A r [n,\u03c1] is an increasing property in r by prop. (73), thm. (7) applies. Thus, there exists an interval I\u01eb of length \u2206(n, \u03c1, \u01eb) such that P (Ar[n,\u03c1]) \u2208 [\u01eb, 1 \u2212 \u01eb] for r \u2208 I\u01eb. Since P (A r [n,\u03c1]) is a continuous function of r by cor. (12) and non-decreasing in r by prop. (76) and 12 \u2208 [\u01eb, 1\u2212 \u01eb], then there exists r0 \u2208 I\u01eb such that P (Ar0[n,\u03c1]) = 1/2. If R = 2 \u2217max{d(x, y) : x, y \u2208 Xn}, then by lemma (82), it is true that 0 < r0(n, \u03c1, \u01eb) \u2264 R < \u221e. It remains to be shown that r0 = r0(n, \u03c1, \u01eb), independent of \u01eb.\nRecall that \u03c1 \u2208 (12 , 1) and note that the maximum distance between any two connected points in B is inversely proportional to n. Then, the particular r0, which meets the requirements of thm. (14), is the exact radius, such that, it is equally probable that more than half of all points are connected contiguously, in which case, only one such cluster exists, with all other clusters being disjoint and sparsely connected throughout B, or all connected clusters disjointly contain half (or less than half) of all available points, in which case, more than one such cluster can exist. As such, r0 demarcates the radial connection length at which the property Ar[n,\u03c1] undergoes a phase transition such that the graph G(Xn; r) is likely to be sparsely connected and form disjoint connected classes of points [57, Thms. (3.3, 3.6)], almost surely, when r \u2208 [0, r0],\nwhile G(Xn; r) is more likely to be fully connected and form one connected class of points [57, Thms. (3.3, 3.6)], almost surely, when r \u2208 (r0, 1].\nLemma 15: r0 = r0(n, \u03c1, \u01eb) is independent of \u01eb.\nProof: Let \u01eb1, \u01eb2 \u2208 (0, 12 ) and suppose r0,1 = r0(n, \u03c1, \u01eb1), r0,2 = r0(n, \u03c1, \u01eb2) such that\nP (A r0,1 [n,\u03c1]) =\n1 2 = P (A r0,2 [n,\u03c1]). (7)\nIt has to be shown that r0,1 = r0,2. Let {\u01ebk}k\u22651 be a sequence such that \u01ebk = \u01eb1 for all k \u2265 1 and define r0,k = r0(n, \u03c1, \u01ebk). Then, for arbitrary \u03be > 0, it is true that\n\u039e \u2261 {k \u2265 1 : |P (A r0,k [n,\u03c1])\u2212 P (A r0,2 [n,\u03c1])| \u2265 \u03be} = \u2205 (8)\nsince r0,k = r0,1 for all k \u2265 1. Hence, by prop. (83), r0,k \u2192 r0,2 as k \u2192 \u221e. But, r0,k = r0,1 for all k \u2265 1 so that r0,1 = r0,2. Thus, r0 = r0(n, \u03c1), independent of \u01eb.\nRemark 16: As a result of thm. (15), r(\u01eb) is independent of \u01eb > 0, since r(n, \u03c1, \u01eb) \u2192 r(\u01eb) as E[n] \u2192 \u221e. As such, \u2206(\u01eb) = o(1) so that Ar has a sharp threshold, by definition (6).\nCorollary 17: The critical radius, associated with the property Ar, is unique.\nProof: r(\u01eb) is the limit of r(n, \u03c1, \u01eb) as E[n] \u2192 \u221e. As such, r0 is the constant limit of r0(n, \u03c1) as E[n] \u2192 \u221e.\nCorollary 18: Given r > 0, there exists a density of points \u03bb0 = \u03bb(n0) such that\nP (Ar[n0,\u03c1]) = 1\n2 .\nProof: By lemma (15), let n0 = n0(r, \u03c1) be the minimum of all positive (real) solutions to r = r0(n, \u03c1) for some fixed r > 0. The result follows.\nSince n is inversely proportional to connection distance r (requiring that n \u2208 [1,\u221e)), then the particular n0, which meets the requirements of cor. (18), is the exact number of points, such that, it is equally probable that more than half of all points are connected contiguously. In this case, only one such cluster exists, with all other clusters being disjoint and sparsely connected throughout B. Otherwise, all connected clusters disjointly contain half (or less than half) of all available points, in which case, more than one such cluster can exist. As such, n0 demarcates the number of points at which the property Ar[n,\u03c1] undergoes a phase transition such that the graph G(Xn; r) is likely to be sparsely connected and form disjoint connected classes of points [57, Thms. (3.3, 3.6)], almost surely, when n \u2208 [1, n0]. Alternatively, G(Xn; r) is more likely to be fully connected and form one connected class of points [57, Thms. (3.3, 3.6)], almost surely, when n \u2208 (n0,\u221e)."}, {"heading": "H. Continuum Sharp Threshold Interval Length", "text": "Given the particular radius guaranteed by thm. (14), then thm. (7) can be used to find an estimate of the length of the sharp threshold interval such that P (Ar[n,\u03c1]) increases sharply from some \u01eb \u2208 (0, 12 ) to 1\u2212 \u01eb. By lemma (15), it is true that\nr0 is independent of any particular \u01eb. Thus, the interval and its length must be fixed, given n and \u03c1 \u2208 (12 , 1).\nTheorem 19: \u2206(n, \u03c1) = \u0398(r0 log 1 4 n).\nProof: For \u03b4 \u2208 (0, 12 ), let \u01eb\u03b4 = 1 2 \u2212 \u03b4. By thm. (7) and\nthms. (14) and (15),\n\u2206(n, \u03c1) = lim \u03b4\u21920+ \u2206(n, \u03c1, \u01eb\u03b4)\n= lim \u03b4\u21920+\n\u0398(r(n, \u03c1, \u01eb\u03b4) log 1 4 n)\n= \u0398(r0 log 1 4 n).\nTheorem (19) gives an expected result, given thm. (7) above. However, in [13], a much more practical estimate of this length is obtained after the bounded region is partitioned by hexagons of a known size. If M2 is the number of these hexagons in the bounded region, then it is shown that a good estimate of the sharp threshold interval length is a polynomial in 1/M .\nTheorem 20: There is a constant c > 0, independent of M , such that for all \u01eb1 > 0 and every fixed small \u03b4 > 0\nP (Ar[n,\u03c1+\u03b4]) \u2264 ( 1\n2 + \u01eb1)M\n\u2212c(r0\u2212r) (9)\nfor all r \u2264 r0 and\nP (Ar[n,\u03c1\u2212\u03b4]) \u2265 1\u2212 ( 1\n2 + \u01eb1)M\n\u2212c(r\u2212r0) (10)\nfor all r \u2265 r0.\nTheorem 21: P (Ar[n,\u03c1]) is a continuous function of \u03c1.\nRemark 22: The proof of thm. (21) requires thm. (20) which will be proven later. For now, the result of thm. (21) is assumed. By thm. (21), for small \u03b4 > 0,\nP (Ar[n,\u03c1\u2212\u03b4]) \u2248 P (A r [n,\u03c1]) \u2248 P (A r [n,\u03c1+\u03b4]).\nTheorem (20) asserts that if r1 < r0 < r2 and for some \u01eb \u2208 (0, 12 ), it is true that P (A r1 [n,\u03c1]) = \u01eb and P (A r2 [n,\u03c1]) = 1\u2212 \u01eb, then r2\u2212r1 is an estimate of the sharp threshold interval length for the property, Ar[n,\u03c1]. Later, a similar result will be stated and proven which can be used in the proof of thm. (21)."}, {"heading": "III. HEXAGONAL PARTITION MODEL", "text": "It was seen in section (II-G) that r0 > 0 exists such that the probability is 1/2 for the occurrence of the property that at least half of all points connect in the bounded region, B. By thm. (7),\nrc = rc(n) = O\n( \u221a\nlogn\nn\n)\n\u2264 r0(n) = r0 (11)\nwhere rc defines the critical radius at which the continuum property occurs with arbitrarily small, positive probability.\nFor fixed r \u2208 (rc, r0), let hr be the largest hexagon that can be inscribed into a circle of radius r/4. Let Hr be a countably infinite collection of copies of hr such that\nR 2 =\n\u22c3\nhri,j\u2208Hr\nhri,j (12)\nand for hri,j , h r i\u2032,j\u2032 \u2208 Hr, it is true that h r i,j 6= h r i\u2032,j\u2032 whenever |i \u2212 i\u2032| + |j \u2212 j\u2032| 6= 0. Connectivity between x, y \u2208 Xn is then defined as x and y both lying in the same hexagon or neighboring hexagons.\nWith the bounded region B partitioned into hexagons contained within B \u2229 Hr, the analysis proceeds whereby the original problem of estimating the sharp threshold interval length in the continuum is now replaced by the problem of estimating the length in the hexagonal partition framework. As such, definitions of connectivity and the increasing property are defined in the new framework. Then, the continuity and existence results are shown to still hold in the new framework. Later, an analogue to thm. (20) is stated and proven."}, {"heading": "A. Definitions", "text": "Definition 23: A hexagonal partition of B is a finite collection of hexagons from Hr such that B is a union of all hexagons in the finite collection.\nDefinition 24: The Hamming distance between elements, hri,j , h r i\u2032,j\u2032 \u2208 Hr is defined to be the quantity\nh(hri,j , h r i\u2032,j\u2032) = |i\u2212 i \u2032|+ |j \u2212 j\u2032|.\nDefinition 25: Points x, y \u2208 Xn are Hr-connected and < x, y >Hr is an Hr-open edge, if there exists h r ix,jx , h r iy,jy \u2208 Hr such that x \u2208 hrix,jx and y \u2208 h r iy ,jy\nwhere h(hrix,jx , h r iy,jy\n) \u2264 2 with |ix \u2212 iy| \u2264 1 and |jx \u2212 jy| \u2264 1. Points in Xn are Hr-disconnected and form an Hr-closed edge otherwise.\nDefinition 26: Given a y \u2208 Xn, an Hr-connected component containing y is the subset of points < Cy >Hr \u2286 Xn containing y and every x \u2208 Xn\\{y} having an Hr-open set of edges connecting x to y.\nDefinition 27: Given an Hr-connected edge, e =< x, y >Hr , an Hr-connected component containing e is the subset of points < Ce >Hr \u2286 Xn containing x and y and every z \u2208 Xn\\{x, y} having an Hr-open set of edges connecting z to both x and y."}, {"heading": "B. The Increasing Property", "text": "1) Bounded Number of Nodes: Let < C >Hr \u2286 Xn be an Hr-connected component such that | < C >Hr | = N and let \u03c1n(C) = Nn be defined as in section (II-E1). For \u03c1 \u2208 (12 , 1), define the graph property of all connected components containing at least 100\u03c1% of all available points by\nAHr[n,\u03c1] = {< C >Hr \u2286 Xn : \u03c1n(C) \u2265 \u03c1}. (13)\nAs in [31], for \u01eb \u2208 (0, 12 ), define\nr\u2217(n, \u03c1, \u01eb) = inf{r > 0 : P (AHr[n,\u03c1]) \u2265 \u01eb} (14)\nto be the critical radius at which AHr[n,\u03c1] occurs with probability at least \u01eb and define\n\u2206\u2217(n, \u03c1, \u01eb) = r\u2217(n, \u03c1, 1\u2212 \u01eb)\u2212 r\u2217(n, \u03c1, \u01eb) (15)\nto be the length of the continuum of radii upon which AHr[n,\u03c1] increases in probability of occurrence from \u01eb > 0 to 1\u2212\u01eb > 0.\n2) Unbounded Number of Nodes: In the event that n is unbounded, define the corresponding graph property to be\nAHr = {< C >Hr \u2286 X\u221e : | < C >Hr | = \u221e}. (16)\nDefine r\u2217(\u01eb) = inf{r > 0 : P (AHr ) \u2265 \u01eb} (17)\nto be the critical radius at which AHr occurs with probability at least \u01eb and define\n\u2206\u2217(\u01eb) = r\u2217(1 \u2212 \u01eb)\u2212 r\u2217(\u01eb) (18)\nto be the length of the continuum of radii upon which AHr increases in probability of occurrence from \u01eb > 0 to 1\u2212\u01eb > 0."}, {"heading": "C. Continuity Results and Some Continuum Relationships", "text": "The continuity results of section (II-F) hold for the properties defined after the bounded region B is partitioned by copies of the hexagon hr, since connectivity is now characterized by points lying within distance r/2 (within neighboring hexagons). As such, the hexagonal partition connectivity model is only another way of viewing the continuum connectivity model. Then, by thm. (14), there exists r\u22170 = r \u2217 0(n, \u03c1) which satisfies the criteria of the theorem for the property AHr[n,\u03c1].\nDefinition 28: G(Xn;Hr) is defined to be the Hr-graph of all Hr-open and Hr-closed edges between points in Xn \u2282 B.\nIn addition to the continuity results under r-connectivity also holding under Hr-connectivity, the next lemma shows that the graph of the set of clusters formed under Hr-connectivity is a sub-graph of the set of clusters formed under r-connectivity.\nLemma 29: G(Xn;Hr) \u2286 G(Xn; r).\nProof: Suppose < x, y >Hr \u2208 G(Xn;Hr) is any Hrconnected edge. Without loss of generality, choose a coordinate system on R2 so that < x, y >Hr lies on a coordinate axis with 0\u0302 = (0, 0) defined such that d(x, 0\u0302) = d(x,y)2 = d(0\u0302, y). Since x, y \u2208 Xn \u2282 B and Hr is a partition of B, then there exists hrix,jx , h r iy,jy \u2208 Hr such that x \u2208 hrix,jx , y \u2208 h r iy,jy and h(hrix,jx , h r iy,jy\n) \u2264 max{|ix \u2212 iy|, |jx \u2212 jy|} \u2264 1. Each of hrix,jx and h r iy,jy are copies of h\nr and can be inscribed into copies of a circle of radius r4 . Therefore, d(x, y) = d(x, \u2202hrix,jx) + d(\u2202h r iy,jy , y) \u2264 r2 + r 2 = r so that x, y \u2208 Xn are r-connected. Thus, < x, y >Hr \u2208 G(Xn; r), which shows that G(Xn;Hr) \u2286 G(Xn; r).\nUsing lemma (29), the next results show that given a sample of size n > 1 and a connectivity radius r > 0, the probably of the event of one subset of connected data points containing 100\u03c1% of the n data points, for \u03c1 \u2208 (12 , 1) is (possibly) smaller under Hr-connectivity than under r-connectivity. In addition, a (possibly) larger radius of connectivity is required to achieve the same proportion of data points being connected into one cluster.\nLemma 30: P (AHr[n,\u03c1]) \u2264 P (A r [n,\u03c1]).\nProof: By lemma (29), it is true that AHr[n,\u03c1] \u2286 A r [n,\u03c1].\nLemma 31: r0 \u2264 r\u22170 .\nProof: Seeking a contradiction, suppose r0 > r\u22170 . Then,\n1 2 = P (Ar0[n,\u03c1]) (19)\n\u2265 P (A r\u22170 [n,\u03c1]) (20) \u2265 P (A Hr\u22170 [n,\u03c1]) (21)\n= 1\n2 (22)\nwhere equality (19) follows by thm. (14), ineq. (20) follows by properties of probability measures and by hypothesis, ineq. (21) follows by lemma (30) and equality (22) follows by thm. (14). It follows that P (Ar \u2217 0\n[n,\u03c1]) = 1/2. Therefore, r \u2217 0 \u2208 {r > 0 :\nP (Ar[n,\u03c1]) = 1 2} and r \u2217 0 < r0 = inf{r > 0 : P (A r [n,\u03c1]) = 1 2}. This is a contradiction. Thus, r0 \u2264 r\u22170 .\nLater, it will be shown that equality holds for both lemmas (30, 31) under special conditions which are perfect for Kmeans clustering."}, {"heading": "D. Hexagonal Sharp Threshold Interval Length", "text": "Given the particular radius guaranteed by thm. (14), then thm. (7) can be used to find an estimate of the length of the sharp threshold interval such that P (AHr[n,\u03c1]) increases sharply from some \u01eb \u2208 (0, 12 ) to 1\u2212 \u01eb. By lemma (15), it is true that r\u22170 is independent of any particular \u01eb. Thus, the interval and its length must be fixed given n and \u03c1 \u2208 (12 , 1).\nTheorem 32: \u2206\u2217(n, \u03c1) = \u0398(r\u22170 log 1 4 n).\nProof: For \u03b4 \u2208 (0, 12 ), let \u01eb\u03b4 = 1 2 \u2212 \u03b4. By thm. (7) and\nthms. (14) and (15),\n\u2206\u2217(n, \u03c1) = lim \u03b4\u21920+ \u2206\u2217(n, \u03c1, \u01eb\u03b4)\n= lim \u03b4\u21920+\n\u0398(r\u2217(n, \u03c1, \u01eb\u03b4) log 1 4 n)\n= \u0398(r\u22170 log 1 4 n).\nAs in thm. (19) above, thm. (32) gives an expected result, given thm. (7) above. Likewise, a similar result to [13, Thm. (3.3.1)] can be stated and later proven, as in the case of thm. (20). It is the result of thm. (33) that allows us to estimate the length of the sharp threshold interval in the presence of the hexagonal partition of B.\nTheorem 33: There is a constant c > 0, independent of M , such that for all \u01eb1 > 0 and every fixed small \u03b4 > 0\nP (AHr[n,\u03c1+\u03b4]) \u2264 ( 1\n2 + \u01eb1)M\n\u2212c(r\u22170\u2212r)\nfor all r \u2264 r\u22170 and\nP (AHr[n,\u03c1\u2212\u03b4]) \u2265 1\u2212 ( 1\n2 + \u01eb1)M\n\u2212c(r\u2212r\u22170) (23)\nfor all r \u2265 r\u22170 .\nLet M2 be the number of hexagons partitioning the region B and let HB(r) = Hr \u2229 B. Given < C >Hr \u2286 Xn, define HC = {hrB \u2208 HB(r) : h r B \u2229 < C >Hr 6= \u2205} to be\nthe connected cluster of hexagons such that each hexagon contains at least one point from the connected cluster of points, < C >Hr .\nLemma 34: E[ \u03c1n(C) ] = E[ |HC | ]\nM2 .\nProof: Let < C >Hr \u2286 Xn be an Hr-connected cluster and let KHC be a random variable taking as values the number of points in the region RHC defined by the hexagons in HC . Since the n points are uniformly distributed spatially and B is partitioned into M2 copies of the prototypical hexagon hr, then\nE[ KHC ] = n E[ area(RHC ) ]\narea(B)\n= n E[ |HC | ]\u00d7 area(hr)\nM2 \u00d7 area(hr)\n= n E[ |HC | ]\nM2 .\nBut, E[ KHC ] = E[ | < C >Hr | ]. Therefore,\nE[ | < C >Hr | ] = n E[ |HC | ]\nM2\nimplies\nE[ \u03c1n(C) ] = E[ |HC | ]\nM2 .\nDefine Dr[n,\u03c1] = {HC \u2286 HB(r) : E[ \u03c1n(C) ] \u2265 \u03c1}. With Dr[n,\u03c1] defined as such, the original problem of estimating the length of the sharp threshold for the property Ar[n,\u03c1] in the continuum is now recast as a site percolation problem on a hexagonal lattice. As will be defined later, a site in the lattice will be deemed open if the corresponding hexagon is occupied by at least one of the points from Xn and it will be deemed closed otherwise. Likewise, two sites are connected and belong to the same connected cluster if both sites are open and their hamming distance is less than or equal to one. Later, a torus on the lattice will be formed by defining a countable collection of permutations of the hexagons in the partition so that the length of the sharp threshold for the property Dr[n,\u03c1] can be approximated by the length for another property D\u0302r[n,\u03c1] on the torus. In this way, boundary connection issues for sites in the partition of B are mitigated and the length of the sharp threshold interval for the property D\u0302r[n,\u03c1] approximates the length for Dr[n,\u03c1], which approximates the length for A Hr [n,\u03c1], which finally approximates the length for Ar[n,\u03c1], the original property in the continuum.\nTheorem 35: There is a constant c > 0, independent of M , such that\nP (Dr[n,\u03c1]) \u2264 1\n2 M\u2212c(r\n\u2217 0\u2212r)\nfor all r \u2264 r\u22170 . Similarly, for some fixed small \u03b4 > 0 and for all \u01eb1 > 0, there is an M0(\u03b4, \u01eb1) such that for all M > M0(\u03b4, \u01eb1)\nP (Dr[n,\u03c1\u2212\u03b4]) \u2265 1\u2212 ( 1\n2 + \u01eb1)M\n\u2212c(r\u2212r\u22170)\nfor all r \u2265 r\u22170 .\nAn important part of the proof of thm. (35) relies upon the sharp threshold inequality results of [12] and [26]. In order to apply these results, connectivity in the hexagon lattice structure should be extended to the case of a torus, whereby any boundary connectivity issues are mitigated. As such, make HB(r) into a torus by identifying hi,j \u2208 HB(r) with an element hi\u2032,j\u2032 in a copy of HB(r), if i\u2032 = i mod M and j\u2032 = j mod M . For every k, l \u2208 Z, the mapping \u03c4k,l : hi,j \u2192 hi+k,j+l defines a shift translation. In this way, a subgroup of automorphisms \u03c4 = {\u03c4k,l : k, l \u2208 Z} with the transitivity property is formed. Thus, any hexagon hi,j can be shifted to any other hexagon hi\u2032,j\u2032 with the translation, \u03c4i\u2032\u2212i,j\u2032\u2212j . Now, hexagons in the 1st row (column) are allowed to be joined in a connected cluster with hexagons in the Mth row (column), provided that all hexagons in question are occupied.\nProposition 36: Define \u03c4(HB(r)) to be the torus created by translations of hexagons in HB(r) under the action of permutations in \u03c4 and define D\u0302r[n,\u03c1] = {HC \u2286 \u03c4(HB(r)) : E[ \u03c1n(C) ] \u2265 \u03c1}. Then, Dr[n,\u03c1] \u2282 D\u0302 r [n,\u03c1] and D r [n,\u03c1] 6= D\u0302 r [n,\u03c1].\nProof: Since D\u0302r[n,\u03c1] contains all of the connected hexagons from Dr[n,\u03c1] and any connections between the 1st and Mth rows (columns) while Dr[n,\u03c1] contains no connection between the 1st and Mth rows (columns), then the result follows.\nDefinition 37: To each hexagon in the partition of B, associate a site i \u2208 {1, 2, ...,M2} as the center of the hexagon. For sites i \u2208 {1, 2, ...,M2}, define si \u2208 {0, 1} to be the state on site i. A site i is said to be open if si = 1 and closed otherwise. There exists an edge e{i,j} between sites i, j \u2208 {1, 2, ...,M2} if and only if there exists a hexagon hri,j \u220b i, j or there exists neighboring hexagons h r i \u220b i and hrj \u220b j in the partition of B. Define e{i,j} to be open if and only if si = 1 = sj and closed otherwise.\nDefinition 38: The conditional influence of i on the property D\u0302r[n,\u03c1] is defined to be\nI(i) = P (D\u0302r[n,\u03c1] | si = 1)\u2212 P (D\u0302 r [n,\u03c1] | si = 0)\nand it is a measure of the change in the probability of D\u0302r[n,\u03c1] due to a state change from si = 0 to si = 1 at site, i.\nFor completeness, [13, Lemma (4.1.1)] is stated without proof, which gives an upper bound on the change in P (D\u0302r[n,\u03c1]) as a function of the point density \u03bb. Utilizing the chain rule for derivatives, a lower bound on the change in P (D\u0302r[n,\u03c1]) as a function of r is found and the resulting inequality relationship is used to estimate upper and lower bounds on P (D\u0302r[n,\u03c1]), which will approximate the inequality results of thm. (35).\nLemma 39: [13, Lemma (4.1.1)] There is a constant z > 0, independent of M and \u03bb, such that\nd\nd\u03bb P (D\u0302r[n,\u03c1]) \u2264 z \u2217(\u03bb)min{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])} logM\nwhere Ahr is the area of the prototypical hexagon hr and z\u2217(\u03bb) = \u2212zAhre\u2212Ah r\u03bb.\nLemma 40: There is a constant c > 0, independent of M and \u03bb, such that\nd\ndr P (D\u0302r[n,\u03c1]) \u2265 c \u2217(\u03bb)min{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])} logM\nwhere Ahr is the area of the prototypical hexagon hr and c\u2217(\u03bb) = c(\u03bb)Ahre\n\u2212Ahr\u03bb, with c(\u03bb) = \u2212cg(\u03bb) for some function g(\u03bb).\nProof: As in cor. (18), let n\u2217 be the inverse of r\u2217 and seeking a contradiction, suppose dr/d\u03bb = 0. Let \u01eb \u2208 (0, 12 ). By lemma (39), dP/d\u03bb exists. Now, the existence of dP/dr will be shown by proving a Lipschitz condition on the probability distribution P (D\u0302r[n,\u03c1]) as a function of r. Assume area(B) = 1. Without loss of generality, it can be assumed that r \u2208 [0, 1]. Without further loss of generality, let r\u22171 , r \u2217 2 \u2208 [0, 1] such that r\u22170 is the midpoint of [r \u2217 1 , r \u2217 2 ], i.e. r \u2217 0 = (r \u2217 2 \u2212 r \u2217 1)/2. Then, by thm. (32),\n|P (D\u0302 r\u22172 [n,\u03c1])\u2212 P (D\u0302 r\u22171 [n,\u03c1])| \u2264 1 = (\u2206 \u2217(n, \u03c1))\u22121|r\u22172 \u2212 r \u2217 1 |.\nTherefore, P (D\u0302r[n,\u03c1]) is Lipschitz continuous with respect to r. Hence, dP/dr exists. Now, since dP/d\u03bb, dP/dr and dr/d\u03bb all exist, then the Chain Rule for derivatives yields,\nd\nd\u03bb P (D\u0302r[n,\u03c1]) =\nd\ndr P (D\u0302r[n,\u03c1])\u00d7\ndr d\u03bb .\nNote that the existence of dP/dr requires that |dP/dr| < \u221e. Therefore, since dr/d\u03bb = 0, then\nd\nd\u03bb P (D\u0302r[n,\u03c1]) =\nd\ndr P (D\u0302r[n,\u03c1])\u00d7 0 = 0.\nAs a result, P (D\u0302r[n,\u03c1]) is constant as a function of \u03bb. So, suppose that 0 < n < n\u2217. Then, P (D\u0302r[n,\u03c1]) = 0, which implies that P (D\u0302r[n,\u03c1]) \u2261 0. This is a contradiction, since P (D\u0302 r [n,\u03c1]) is a probability distribution. Hence, dr/d\u03bb 6= 0. Now, by [33, Thm. (2.28)], there is a constant c > 0, independent of M and \u03bb, such that\nI(i) \u2265 cmin{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])}\nlogM\nM2 .\nUnder the action of \u03c4 , each hexagon in the bounded region B is translated to another hexagon in a copy of B. Therefore, D\u0302r[n,\u03c1] and P (D\u0302 r [n,\u03c1]) are invariant under the action of \u03c4 . Hence, I(i) = I(j) whenever, \u03c4(i) = j, where \u03c4(i) is defined to be the translation of the hexagon hri \u220b i to the hexagon h r j \u220b j in the copy of the partition of B. From [13], in the proof of thm. (39), the following identity holds, with p = p(\u03bb) = 1\u2212e\u2212Ahr\u03bb defined above,\nd\nd\u03bb P (D\u0302r[n,\u03c1]) =\nd\ndp P (D\u0302r[n,\u03c1])\u00d7\ndp\nd\u03bb\n= \u2212Ahre \u2212Ahr\u03bb\nM2 \u2211\ni=1\nI(i). (24)\nFor \u03b3 > 0, r > 0 and k > 0, any Hr-connected component in Xn containing at least \u03b3(n + k)/2 points will inherently contain an Hr-connected component of size at least \u03b3n/2. Hence, AHr[\u03b3(n+k),\u03c1] \u2286 A Hr [\u03b3n,\u03c1]. It follows that P (AHr[\u03b3(n+k),\u03c1]) \u2264 P (A Hr [\u03b3n,\u03c1]). Therefore, r \u2217(\u03b3n, \u03c1, \u01eb) \u2208 {r > 0 : P (AHr[\u03b3(n+k),\u03c1]) \u2265 \u01eb}, which implies r \u2217(\u03b3(n + k), \u03c1, \u01eb) \u2264 r\u2217(\u03b3n, \u03c1, \u01eb) for k > 0. Hence,\nr\u2217(\u03b3(n+ k), \u03c1, \u01eb)\u2212 r\u2217(\u03b3n, \u03c1, \u01eb) \u2264 0. (25)\nSince point density \u03bb is proportional to point count n for any bounded region B, then using ineq. (25) yields\ndr d\u03bb = lim k\u21920\nr\u2217(\u03b3(n+ k), \u03c1, \u01eb)\u2212 r\u2217(\u03b3n, \u03c1, \u01eb)\n\u03b3k \u2264 0,\nfor some \u03b3 > 0. Since dr/d\u03bb 6= 0, it follows that\ndr d\u03bb < 0.\nSince dr/d\u03bb exists, then |dr/d\u03bb| < \u221e. Thus, by substituting\nI(i) \u2265 cmin{P (D\u0302r[n,\u03c1]), 1 \u2212 P (D\u0302 r [n,\u03c1])}\nlogM\nM2\ninto (24), it follows that\nd\nd\u03bb P (D\u0302r[n,\u03c1]) = \u2212Ahre\n\u2212Ahr\u03bb M2 \u2211\ni=1\nI(i)\n\u2264 \u2212cAhre \u2212Ahr\u03bb \u00d7 M2 \u2211\ni=1\nmin{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])}\nlogM\nM2\n= \u2212cAhre \u2212Ahr\u03bb \u00d7 min{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])} logM.\nTherefore,\nd\nd\u03bb P (D\u0302r[n,\u03c1]) =\nd\ndr P (D\u0302r[n,\u03c1])\u00d7\ndr\nd\u03bb \u2264 \u2212cAhre \u2212Ahr\u03bb (26) \u00d7 min{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])} (27) \u00d7 logM (28)\nso that\nd\ndr P (D\u0302r[n,\u03c1]) \u2265 \u2212cAhre \u2212Ahr\u03bb\n(\ndr\nd\u03bb\n)\u22121\n(29)\n\u00d7 min{P (D\u0302r[n,\u03c1]), 1\u2212 P (D\u0302 r [n,\u03c1])} (30) \u00d7 logM. (31)\nDefining g(\u03bb) = (dr/d\u03bb)\u22121, the result follows.\nRemark 41: Let \u01eb > 0 be given. At the risk of ambiguity, denote n = E[n] and define \u03bb\u2217(n, \u03c1, \u01eb) = inf {\nn > 0 | P (D\u0302r[n,\u03c1]) \u2265 \u01eb } . Inequality (26) implies that\nP (D\u0302r[n,\u03c1]) is increasing as a function of decreasing node density \u03bb = \u03bb\u2217(n, \u03c1, \u01eb) such that the event {\nP (D\u0302r[n,\u03c1]) \u2265 \u01eb }\nfirst occurs. Likewise, since the maximum distance between connected points is inversely proportional to node density, then ineq. (29) implies that P (D\u0302r[n,\u03c1]) is decreasing as a function of increasing maximum distance r = r\u2217(n, \u03c1, \u01eb) between connected points such that the event {\nP (D\u0302r[n,\u03c1]) \u2265 \u01eb }\nfirst occurs.\nLemma 42: Let c > 0 be as in thm. (40). Then, there exists r\u22170 , independent of M , such that\nP (D\u0302r[n,\u03c1]) \u2264 1\n2 M\u2212c(r\n\u2217 0\u2212r)\nfor all r \u2264 r\u22170 and\nP (D\u0302r[n,\u03c1]) \u2265 1\u2212 1\n2 M\u2212c(r\u2212r\n\u2217 0)\nfor all r \u2265 r\u22170 .\nProof: Arguing as in the proof to thm. (14), there exists r\u22170 such that P (D\u0302 r\u22170 [n,\u03c1]) = 1/2. Arguing similarly to cor. (12), P (D\u0302r[n,\u03c1]) is continuous in r. Therefore, P (D\u0302 r [n,\u03c1]) \u2264 1 \u2212 P (D\u0302r[n,\u03c1]) for r \u2264 r \u2217 0 and P (D\u0302 r [n,\u03c1]) \u2265 1 \u2212 P (D\u0302 r [n,\u03c1]) for r \u2265 r\u22170 . Thus, the result of lemma (40) takes the form\nd\ndr P (D\u0302r[n,\u03c1]) \u2265 c \u2217(\u03bb)P (D\u0302r[n,\u03c1]) logM\nfor r \u2264 r\u22170 and\nd\ndr P (D\u0302r[n,\u03c1]) \u2265 c \u2217(\u03bb)(1 \u2212 P (D\u0302r[n,\u03c1])) logM\nfor r \u2265 r\u22170 . The last two inequalities can be written\nd\ndr logP (D\u0302r[n,\u03c1]) \u2265 c \u2217(\u03bb) logM\nfor r \u2264 r\u22170 and\nd\ndr log (1\u2212 P (D\u0302r[n,\u03c1])) \u2264 \u2212c \u2217(\u03bb) logM\nfor r \u2265 r\u22170 , respectively. Consider r \u2264 r \u2217 0 . Both sides of\nd\ndr logP (D\u0302r[n,\u03c1]) \u2265 c \u2217(\u03bb) logM\nare integrated in the direction of increasing point density since P (D\u0302r[n,\u03c1]) decreases as a function of point density \u03bb by the proof to lemma (40). It was also shown that dr/d\u03bb < 0, i.e. r is decreasing as a function of point density. Therefore, the integration limits for the interval [r, r\u22170 ] are from r \u2217 0 to r. Noting that the inequality is reversed for backward integration, the following is obtained for c > 0 and some K1(\u03bb) \u2265 0,\nlogP (D\u0302r[n,\u03c1]) \u2264 K1(\u03bb) logM c(r\u2212r\u22170)\nwhich can be rewritten as\nlogP (D\u0302r[n,\u03c1]) \u2264 K1(\u03bb) logM \u2212c(r\u22170\u2212r).\nThis implies\nP (D\u0302r[n,\u03c1]) \u2264 K2(\u03bb)M \u2212c(r\u22170\u2212r)\nfor some K2(\u03bb) \u2265 0. Therefore, using the initial condition P (D\u0302\nr\u22170 [n,\u03c1]) = 1/2 yields K2(\u03bb) = 1/2. Thus,\nP (D\u0302r[n,\u03c1]) \u2264 1\n2 M\u2212c(r\n\u2217 0\u2212r).\nNow, consider r \u2265 r\u22170 . Similary, both sides of\nd\ndr log (1\u2212 P (D\u0302r[n,\u03c1])) \u2264 \u2212c \u2217(\u03bb) logM\nare integrated in the direction of increasing connection radii on [r\u22170 , r] since P (D\u0302 r [n,\u03c1]) increases as a function of connection radii r by the proof to lemma (40). Therefore, the integration limits are from r\u22170 to r. The following is obtained for c > 0 and some K3(\u03bb) \u2265 0,\nlog (1\u2212 P (D\u0302r[n,\u03c1])) \u2264 \u2212K3(\u03bb) logM c(r\u2212r\u22170)\nwhich can be rewritten as\nlog (1\u2212 P (D\u0302r[n,\u03c1])) \u2264 \u2212K3(\u03bb) logM \u2212c(r\u22170\u2212r)\n= K3(\u03bb) logM \u2212c(r\u2212r\u22170).\nThis implies\n1\u2212 P (D\u0302r[n,\u03c1]) \u2264 K4(\u03bb)M \u2212c(r\u2212r\u22170)\nfor some K4(\u03bb) \u2265 0. Therefore, using the initial condition P (D\u0302\nr\u22170 [n,\u03c1]) = 1/2 yields K4(\u03bb) = 1/2. Hence,\nP (D\u0302r[n,\u03c1]) \u2265 1\u2212 1\n2 M\u2212c(r\u2212r\n\u2217 0).\nBy prop. (36), there are cases when Dr[n,\u03c1] \u2282 D\u0302 r [n,\u03c1], but\nDr[n,\u03c1] 6= D\u0302 r [n,\u03c1] so that the occurrence of D\u0302 r [n,\u03c1] does not imply the occurrence of Dr[n,\u03c1]. To exclude these possibilities, the arguments of [13] are followed whereby a slightly larger property Dr[n,\u03c1\u2212\u03b4] is considered for some small \u03b4 > 0 such that the occurrence of D\u0302r[n,\u03c1] implies the occurrence of D r [n,\u03c1\u2212\u03b4].\nAs in [13], let \u03c6(M) be any M -dependent integer such that \u03c6(M) \u2192 \u221e as M \u2192 \u221e and\n\u03c6(M) = o(c(r \u2212 r\u22170) logM).\nChoose a coordinate system so that B has its lower left corner at the origin. Define the top, bottom, left and right boundary strips of B as Hi, i = 1, 2, 3, 4 with sizes \u03c6(M)\u00d7M , \u03c6(M)\u00d7 M , M \u00d7 \u03c6(M) and M \u00d7 \u03c6(M) by\nH1 = {Hi,j : i = M \u2212 \u03c6(M) + 1, ...,M, j = 1, ...,M}\nH2 = {Hi,j : i = 1, ..., \u03c6(M), j = 1, ...,M}\nH3 = {Hi,j : i = 1, ...,M, j = 1, ..., \u03c6(M)}\nH4 = {Hi,j : i = 1, ...,M, j = M \u2212 \u03c6(M) + 1, ...,M}.\nLet Ei be the event that there is a connected path of occupied hexagons crossing the rectangle Hi using the longest straightline path.\nLemma 43: For i = 1, 2, 3, 4, there are constants ci > 0 such that for large M and r \u2265 r\u22170\nP (Ei) \u2265 1\u2212 e \u2212ci\u03c6(M).\nProof: As in [13], by the duality property, the occurrence of Ei, i = 1, 2, 3, 4 is equivalent to the non-occurrence of the event that there is a connected path of unoccupied hexagons crossing Hi, i = 1, 2, 3, 4 using the shortest straight-line path. The rest of the proof follows [13] with the edge probability as a function of point density p(\u03bb0) replaced by r\u2217(n, \u03c1, \u01eb) and the critical probability for the occurrence of an infinite cluster of occupied hexagons pc replaced by r\u22170 .\nProof: (Theorem 35) By prop. (36), Dr[n,\u03c1] \u2282 D\u0302 r [n,\u03c1] so\nthat P (Dr[n,\u03c1]) \u2264 P (D\u0302 r [n,\u03c1]). To estimate P (D r [n,\u03c1\u2212\u03b4]) for r > r0 and any given \u03b4 > 0, let E = E1 \u2229 E2 \u2229 E3 \u2229 E4 and\nconsider F = D\u0302r[n,\u03c1] \u2229 E. Since P (F ) = P (F \u2229 D r [n,\u03c1\u2212\u03b4]) +"}, {"heading": "P (F \u2212Dr[n,\u03c1\u2212\u03b4]), then", "text": "P (Dr[n,\u03c1\u2212\u03b4]) \u2265 P (F )\u2212 P (F \u2212D r [n,\u03c1\u2212\u03b4]).\nNoting that P (E1) = P (E2) and P (E3) = P (E4), then the FKG inequality of [32] yields\nP (F ) \u2265 P (D\u0302r[n,\u03c1])P 2(E1)P 2(E3).\nBy lemma (43), there exists b > 0 such that for all sufficiently large M ,\nP (F ) \u2265 1\u2212 1\n2 M\u2212c(r\u2212r\n\u2217 0) \u2212O(e\u2212b\u03c6(M)).\nUsing \u03c6(M) = o(c(r \u2212 r\u22170) logM), this implies that for any given \u01eb1 > 0 and all sufficiently large M depending upon \u01eb1,\nP (F ) \u2265 1\u2212\n(\n1 2 + \u01eb1\n)\nM\u2212c(r\u2212r \u2217 0).\nIt is now claimed that F \u2212Dr[n,\u03c1\u2212\u03b4] = \u2205, requiring that P (F \u2212 Dr[n,\u03c1\u2212\u03b4]) = 0 for all large M . Following [13], the occurrence of F implies that there is a connected path of hexagons which encloses the sub-lattice given by HB(r) \u2212 \u22c34 i=1 Hi. Because the points in Xn are uniformly distributed, then there is a connected cluster of hexagons within the original lattice totaling at least \u03c1M2 \u2212 (2M\u03c6(M) + 2\u03c6(M)(M \u2212 2\u03c6(M))) hexagons, where \u03c1M2 is a lower bound on the number of occupied hexagons in the largest connected cluster and 2M\u03c6(M) + 2\u03c6(M)(M \u2212 2\u03c6(M)) is the total number of hexagons in the strips, Hi, i = 1, 2, 3, 4. Let \u03b41 = (2M\u03c6(M)+ 2\u03c6(M)(M \u2212 2\u03c6(M)))/M2. It follows that F \u2282 Dr[n,\u03c1\u2212\u03b41], since F occurs in those hexagons of B that are not near the boundary of B by a simple translation \u03c4 of hexagons h \u2208\n\u22c34 i=1 Hi to hexagons h \u2208 HB(r)\u2212 \u22c34 i=1 Hi. Thus, if M is large enough so that \u03b41 < \u03b4, then F \u2282 Dr[n,\u03c1\u2212\u03b41] \u2282 D r [n,\u03c1\u2212\u03b4].\nProof: (Theorem 33) Consider r \u2264 r\u22170 . Since\nP (AHr[n,\u03c1+\u03b4]) = P (A Hr [n,\u03c1+\u03b4],D r [n,\u03c1]) + P (A Hr [n,\u03c1+\u03b4] \u2212D r [n,\u03c1])\nthen\nP (AHr[n,\u03c1+\u03b4]) \u2264 P (D r [n,\u03c1]) + P (A Hr [n,\u03c1+\u03b4] \u2212D r [n,\u03c1]).\nIt will be shown that P (AHr[n,\u03c1+\u03b4] \u2212 D r [n,\u03c1]) = o(M \u2212c(r\u22170\u2212r)). Let x be a configuration of states across hexagons in HB(r) and let C(x) = {C1, ..., CK} be the set of clusters in x. For i = 1, ...,K , let NCi be the number of points in the cluster, Ci. Then, {NCi | C(x), n} \u223c B(n, |HCi |\nM2 ). Suppose Ci0 \u2208 C(x) is any cluster such that \u03c1n(Ci0 ) \u2265 \u03c1 + \u03b4. Since the occurrence of the property (\nDr[n,\u03c1]\n)c\nimplies |HCi0 |\nM2 < \u03c1, then\nAHr[n,\u03c1+\u03b4] \u2212D r [n,\u03c1] \u2282\n{\n\u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4, |HCi0 |\nM2 < \u03c1\n}\n.\nBy arguments in [23] and [13], there is an \u03b1 = \u03b1(\u03c1, \u03b4) > 0 such that\nP\n(\n\u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4\n\u2223 \u2223 \u2223 \u2223 { |HCi0 |\nM2 < \u03c1\n}\n, C(x), n\n)\n\u2264 e\u2212\u03b1(\u03c1,\u03b4)n.\nIt follows that P (AHr[n,\u03c1+\u03b4] \u2212 D r [n,\u03c1]) \u2264 P ( {\u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4}, { |HCi0 | M2 < \u03c1 }) \u2264 P ( \u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4 \u2223 \u2223 \u2223 |HCi0 | M2 < \u03c1 ) \u00d7 P ( |HCi0 | M2 < \u03c1 ) \u2264 P ( \u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4 \u2223 \u2223 \u2223 |HCi0 | M2 < \u03c1 ) = E [ P ( \u03c1n(Ci0 ) \u2265 \u03c1+ \u03b4 \u2223 \u2223 \u2223 { |HCi0 | M2 < \u03c1 } , C(x), n )] \u2264 E[e\u2212\u03b1n] = exp {\u2212n(1\u2212 e\u2212\u03b1)}.\nNow, since n(1 \u2212 e\u2212\u03b1) > d logM implies exp {\u2212n(1\u2212 e\u2212\u03b1)} < M\u2212d, then for any d > 0 and every fixed \u03b4 > 0, it follows that P (AHr[n,\u03c1+\u03b4] \u2212D r [n,\u03c1]) decays to zero at a rate faster than M\u2212d for n large enough. The case of r \u2265 r\u22170 is proven with similar arguments.\nTheorem 44: P (AHr[n,\u03c1]) is a continuous function of \u03c1.\nProof: Let \u03c3 = 1\u2212 \u03c1 in eq. (13). Then, AHr[n,\u03c3] is an increasing property in \u03c3 for increasing \u03c1 \u2208 (12 , 1). Therefore, by [33, Thm. (2.48)], it is true that AHr[n,\u03c3] has a sharp threshold in \u03c3, and hence, in \u03c1. Thus, by [33, Ineq. (2.49)], P (AHr[n,\u03c1]) is differentiable in \u03c1, which implies that P (AHr[n,\u03c1]) is continuous as a function of \u03c1.\nRemark 45: By thm. (44), for small \u03b4 > 0,\nP (AHr[n,\u03c1\u2212\u03b4]) \u2248 P (A Hr [n,\u03c1]) \u2248 P (A Hr [n,\u03c1+\u03b4]).\nIn this light, thm. (33) asserts that if r\u22171 < r \u2217 0 < r \u2217 2 and for some \u01eb \u2208 (0, 12 ), it is true that P (A Hr\u2217 1 [n,\u03c1]) = \u01eb and P (A Hr\u2217 2\n[n,\u03c1]) = 1\u2212\u01eb, then r\u22172\u2212r \u2217 1 is an estimate of the sharp threshold interval length for the property, AHr[n,\u03c1].\nProof: (Theorem 20) Since P (Ar[n,\u03c1]) and P (A Hr [n,\u03c1]) are continuous functions of r, then by thm. (33) and lemma (31), for every r \u2208 [0, r0] there exists r\u2032 \u2264 r such that\nP (Ar \u2032 [n,\u03c1+\u03b4]) \u2264 P (A Hr [n,\u03c1+\u03b4]) (32)\n\u2264 ( 1\n2 + \u01eb1)M\n\u2212c(r\u22170\u2212r)\n\u2264 ( 1\n2 + \u01eb1)M\n\u2212c(r0\u2212r). (33)\nConsider r0 \u2208 [0, r0]. Then, continuity of P (Ar[n,\u03c1]) in r and the non-decreasing property of P (Ar[n,\u03c1]) in r implies ineq. (33) for all r \u2208 [0, r\u2032]. It is claimed that r\u2032 = r0. Seeking a contradiction if r\u2032 < r0, suppose P (Ar[n,\u03c1]) \u2264 (12 + \u01eb1)M\n\u2212c(r0\u2212r) for all r \u2208 [0, r\u2032] and P (Ar[n,\u03c1]) > (12 + \u01eb1)M \u2212c(r0\u2212r) for all r > r\u2032. By hypothesis, r0 > r\u2032 so that when r = r0, it follows that P (A r0 [n,\u03c1+\u03b4]) > 1/2. Now, since for any connected cluster < C >r such that \u03c1n(C) \u2265 \u03c1+\u03b4 for \u03b4 > 0, the statement \u03c1n(C) \u2265 \u03c1 is implied, then Ar[n,\u03c1+\u03b4] \u2286 A r [n,\u03c1] for all r \u2208 [0, r0]. Hence, r\n\u2032 < r0 leads to\nP (Ar0[n,\u03c1]) \u2265 lim sup \u03b4\u21920+ P (Ar0[n,\u03c1+\u03b4]) \u2265 P (A r0 [n,\u03c1+\u03b4]) >\n1 2 . (34)\nIn particular, ineq. (34) gives P (Ar0[n,\u03c1]) > 1/2. This is a contradiction since P (Ar0[n,\u03c1]) = 1/2 by thm. (14). It follows\nthat r\u2032 = r0 and\nP (Ar[n,\u03c1]) \u2264 ( 1\n2 + \u01eb1)M\n\u2212c(r0\u2212r)\nfor r \u2264 r0. A similar argument is used to prove\nP (Ar[n,\u03c1\u2212\u03b4]) \u2265 1\u2212 ( 1\n2 + \u01eb1)M\n\u2212c(r\u2212r0)\nfor r \u2265 r0.\nThe implication of the proof to thm. (20) is that P (Ar[n,\u03c1]) = P (A Hr [n,\u03c1]) for r \u2208 [0, r0]. By [33, Thm. (1.16)], the random cluster measure gives rise to a collection of conditional probability measures of connection events in the identified classes during K-means classification. Therefore, the node process X samples from each element of the collection.\nTheorem 46: P (Ar[n,\u03c1]) = P (A Hr [n,\u03c1]) for r \u2208 [0, r0].\nProof: By continuity in \u03c1 of P (AHr[n,\u03c1]) as given by thm. (44), it is true that\nlim \u03b4\u21920+\nP (AHr[n,\u03c1+\u03b4]) = P (A Hr [n,\u03c1]).\nSuppose \u03b41 > \u03b42 such that \u03c1 + \u03b41, \u03c1 + \u03b42 \u2208 (12 , 1) and let < C >r \u2208 Ar[n,\u03c1+\u03b41]. Then, \u03c1n(C) \u2265 \u03c1 + \u03b41 > \u03c1 + \u03b42 so that < C >r \u2208 Ar[n,\u03c1+\u03b42]. Hence, A r [n,\u03c1+\u03b41]\n\u2286 Ar[n,\u03c1+\u03b42]. By properties of probability measures, P (Ar[n,\u03c1]) is monotone non-decreasing as a function of decreasing \u03c1. By ineq. (32), it follows that for some fixed r \u2208 [0, r0], there exists r\u2032 \u2208 [0, r0] such that P (Ar \u2032\n[n,\u03c1+\u03b4]) \u2264 P (A Hr [n,\u03c1+\u03b4]) for all r \u2032\u2032 \u2208 [0, r\u2032] so that\nlim sup \u03b4\u21920+\nP (Ar \u2032\u2032\n[n,\u03c1+\u03b4]) \u2264 lim sup \u03b4\u21920+\nP (AHr[n,\u03c1+\u03b4]) = P (A Hr [n,\u03c1]). (35)\nFrom the proof of thm. (20), it was shown that r\u2032 = r0. Therefore, by continuity of P (AHr[n,\u03c1]) in r, ineq. (35) holds for all r \u2208 [0, r0], with r\u2032\u2032 replaced by r. The Monotone Convergence Theorem [67] applied to E[1Ar\n[n,\u03c1+\u03b4] ] and E[1Ar [n,\u03c1] ] guarantees\nthat P (Ar[n,\u03c1+\u03b4]) \u2192 P (A r [n,\u03c1]) as \u03b4 \u2192 0 +. Therefore, ineq. (35) becomes\nP (Ar[n,\u03c1]) = lim sup \u03b4\u21920+ P (Ar[n,\u03c1+\u03b4]) \u2264 P (A Hr [n,\u03c1]). (36)\nIn particular, P (Ar[n,\u03c1]) \u2264 P (A Hr [n,\u03c1]) so that with the result of lemma (30), namely P (AHr[n,\u03c1]) \u2264 P (A r [n,\u03c1]), the theorem follows.\nCorollary 47: P (Ar) = P (AHr ) for r \u2208 [0, r0].\nProof: By thm. (46), it is true that P (Ar[n,\u03c1]) = P (A Hr [n,\u03c1]) for all r \u2208 [0, r0] and all n \u2265 1. By prop. (77), it follows that P (AHr ) \u2264 P (AHr[n,\u03c1]) = P (A r [n,\u03c1]). In particular, P (A\nHr ) \u2264 P (Ar[n,\u03c1]). Without loss of generality, assume that area(B) = 1. From [13], differentiability of P (Ar[n,\u03c1]) in point density \u03bb = \u03bb(n) = E[n] implies continuity of P (Ar[n,\u03c1]) in \u03bb so that the following holds\nlim E[n]\u2192\u221e\nP (Ar[n,\u03c1]) = P (A r). (37)\nTherefore, P (AHr ) \u2264 P (Ar[n,\u03c1]) and eq. (37) implies P (AHr ) \u2264 P (Ar). Similarly, P (Ar) \u2264 P (AHr ) so that the corollary follows.\nCorollary 48: r0 = r\u22170 .\nProof: By thm. (46), it is true that 1/2 = P (Ar0[n,\u03c1]) =\nP (A Hr0 [n,\u03c1]). In particular, 1/2 = P (A Hr0 [n,\u03c1]). Since P (A Hr\u22170 [n,\u03c1]) = 1/2 = P (A Hr0 [n,\u03c1]), by the discussion preceding thm. (32) and by thm. (14), then the uniqueness of r\u22170 and r0 guarantees that r\u22170 = r0.\nProof: (Theorem 21) Follows directly from thms. (44) and (46).\nBy thm. (46) and cor. (48), the problem of estimating the probabilities and length of the sharp threshold interval in the continuum can be re-cast as problems of estimation in the presence of a hexagonal partition of the bounded region. As such, tools from percolation [32] and the random cluster model [33] can readily be employed. This fact will be of paramount importance in applications to K-means classification where a data set consisting of multi-dimensional points is partitioned into disjoint, connected subsets. As it is advantageous to not have one connected cluster containing at least 100\u03c1% of all points, since otherwise there may exist a single cluster containing almost all points by lemma (72), the connection radius for points in the continuum must be in the sub-critical range r \u2208 [0, r0] when classifying data into more than 2 classes. Since P (Ar[n,\u03c1]) = P (A Hr [n,\u03c1]) for r \u2208 [0, r0], disjoint clusters of points in the continuum are equivalent to disjoint clusters of occupied hexagons in the hexagonal partition of the bounded region containing all points. As such, multidimensional points in the continuum can be thought to belong to the same class if they are within a certain Euclidean distance of one another. As a result, the multi-dimensional points will have representatives belonging to occupied, connected hexagons in the 2-dimensional, bounded, partitioned region. All representatives in connected clusters of hexagons form the members of a class."}, {"heading": "IV. K -MEANS SHARP THRESHOLD AND CRITICAL RADIUS", "text": "For ease of computation, and at the risk of ambiguity, suppose n = M2. The idea is to partition B into M2 hexagons and find K = N2 contiguous clusters of hexagons such that each of the clusters are mutually disjoint. Into one and only one hexagon of a given cluster will each data point be mapped to form a point in the connected cluster. As such, the connected clusters of hexagons will be the K = N2 classes containing a representative point associated to one and only one data point.\nTheorem 49: Assume that there are M2 points and N2 classifications for the points. The minimum number of hexagons required to partition the unit square into N2 disjoint regions such that M2 is the sum total of all hexagons in the disjoint regions is given by\nS(M,N) = M2 + 2M(N \u2212 1)2.\nProof: Since M2 >> N2 by hypothesis, then the total number of hexagons required to partition B into disjoint regions of contiguous hexagons is O(M2). Label the disjoint regions A1, A2, ..., AN2 and let k be any integer such that 1 \u2264 k \u2264 N2. Since the total number of hexagons partitioning B is O(M2), then the number of hexagons in Ak is proportional to\nM2. Likewise, the total number of hexagons in boundary(Ak) is proportional to area(Ak). Since area(Ak) is proportional to M2, then the number of hexagons in boundary(Ak) is proportional to M2. Note that each Ak shares a portion of its separating boundary with each of its neighboring clusters of hexagons. Let Aj be a neighboring cluster of Ak such that j 6= k and 1 \u2264 j \u2264 N2. Since this portion of the separating boundary is proportional to both area(Ak) and area(Aj), then it is proportional to a common area of size area(Akj). Repeating this same logic for all integers k and j such that 1 \u2264 k \u2264 N2 and 1 \u2264 j \u2264 N2, the total number of hexagons in the entire separating boundaries is proportional to a common area of size area(A). Since minimizing the total number of hexagons in B is tantamount to minimizing the area(A), then making an application of the law of large numbers, each of the N2 disjoint clusters of connected hexagons is the same size and must be a square sub-region of B containing M2/N2 hexagons. The minimum number of hexagons that are required to enclose N2 sub-regions of B containing M2/N2 hexagons is exactly 2M(N \u2212 1)2. Therefore, the minimum number of hexagons required to partition B into N2 disjoint regions such that M2 is the sum total of all hexagons in the disjoint regions is given by\nS(M,N) = M2 + 2M(N \u2212 1)2. (38)\nThe idea is to use the result of the theorem to calculate, as a function of M and N = N(M), the exact size of a prototypical hexagon which will be used to partition B into hexagons of equal size. As K = N2 is fixed as the number of classes of data points, M2 is fixed for the initial calculation of S(M,N) and the subsequent classification of the first M2 data points. In [32], it is stated and proven that there is a critical probability of connection between hexagons containing a point of a network such that it is no longer possible to have disjoint clusters of points when this critical probability of connection is exceeded. Hence, all points will be connected into one cluster, which is not what we intend to model, in this case. Since the size of B is fixed, then to decrease the probability of connection while maintaining K = N2 disjoint contiguous clusters of points, the size of each hexagon must decrease while increasing the number of hexagons in the boundaries of the disjoint regions. In this way, the ratio of the total number of occupied hexagons to the total number of hexagons will be less than this critical probability of connection. Note that we used uniformity of the points throughout B so that the approximate number of points in a cluster of hexagons is proportional to the ratio of the number of hexagons in the cluster divided by the number of hexagons in the entire region, B. Also, note that the minimum number of hexagons required for separation is given by thm. (49), so that the common radius of the circle that can circumscribe any one of these hexagons is of size\nR(M,N) = 1\n2 \u221a S(M,N) , (39)\nthereby necessarily indicating that\nB(M,N) = 2 \u2217R(M,N)\nis the diameter of the circumscribing circle. R(M,N) is decreasing for increasing M and N as a direct result of eqs. (38) and (39).\nLemma 50: R(M,N) is decreasing for increasing M and N .\nProof: By eq. (38), S(M,N) is increasing for increasing M and N . Consequently, by eq. (39), R(M,N) is decreasing for increasing M and N .\nTheorem 51: Suppose that the node process X generates infinitely many points in R2. An infinite connected cluster exists across hexagons in R2 with probability 1 if and only if the probability that any two points connect exceeds pc, where pc is the critical probability of connection. Otherwise, all connected clusters are disjoint with probability 1.\nTheorem (51) is a restatement of [33, Thm. (1.11)]. A direct result of thm. (51) is that, given any bounded region B, all points generated within B are almost surely connected into one cluster. Therefore, in order to not exceed the critical probability of connection, which means maintaining the N2 classes of M2 data points, the radial length of each hexagon\u2019s circumscribing circle must be less than or equal to R(M,N). By [32, Thm. (1.11)], the clusters will be disjoint with probability 1. Hence, the following corollary to thm. (49) follows from these statements and lemma (53).\nCorollary 52: Let hr be a hexagon of size such that it can be inscribed into a circle of radius r = r(M,N) > 0 where\n0 < r \u2264 R(M,N).\nIf B is partitioned into copies of hr, then with probability 1, N2 is the mean number of disjoint clusters of contiguous hexagons in the region B that are occupied by the M2 points.\nWith r0 given by cor. (52), the size of the prototypical hexagon can be calculated for repartitioning B. Furthermore, cor. (52) guarantees that the classes will remain distinct, with probability 1, through each new classification. By cor. (52), the expected value of the number of classes to form can be calculated.\nLemma 53: For M2 uniformly distributed data points in B and for any \u03c1 \u2208 (0, pc], with pc = 1\u2212 2 sin (\u03c0/18),\nM2\nS(M,N) =\nM2\nM2 + 2M(N \u2212 1)2 = \u03c1 (40)\ndetermines the expected number K = N2 of disjoint classes to form such that M2 is the total of all occupied hexagons across all classes.\nProof: At the risk of ambiguity, let N2 denote both the random variable and the expectation of the random variable which takes the number of formed classes as its value. Because B is partitioned by hexagons, it is shown in [33, Chapter 3] that pc = 1\u22122 sin (\u03c0/18). By uniformity, the mean number of data points in each class is M2/N2. By thm. (51), each class will be disjoint and each hexagon in B will be as large as possible if B is partitioned into S(M,N) hexagons of equal size. Also, by thm. (51), the probability of any of the M2 hexagons being populated with a data point has to be less than or equal to pc in order that the expected classes form with probability 1, resulting in eq. (40). For any \u03c1 \u2208 (0, pc], K = N2 is found by solving eq. (40) to obtain K = N2 as the least integer which is not less than the integer part of a non-negative solution to eq. (40), for fixed, positive M2.\nLemma 54: For fixed \u03c1 \u2208 (12 , 1) and r > 0 there exists \u03b4 = \u03b4(\u03c1) \u2208 (0, 12 ), such that\n{\n| < C >Hr |\nS(M,N) <\n1\n2\n}\n= (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\nupto sets of P -measure zero.\nProof: By definition, (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\n= {\n|<C>Hr | S(M,N) < \u03c1\u2212 \u03b4\n}\n. Take \u03b4 = \u03c1\u2212 12 .\nBy choosing \u03b4 as in lemma (54), continuity in r > 0\nand the non-decreasing property of P (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)\nfor\nincreasing r > 0 granted by cor. (12) and prop. (76), respectively, then by ineq. (10), it follows that\nR(M,N) < r\u22170 = r \u2217 0(M,N)\nfor the property (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\n, since\nP (( A HR(M,N) [S(M,N),\u03c1\u2212\u03b4]\n)c)\n= 1\n> 1\n2\n= P\n(\n(\nA Hr\u22170 [S(M,N),\u03c1\u2212\u03b4]\n)c )\nand the probability of (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\nis non-decreasing for decreasing r \u2264 r\u22170 , a reversal.\nLet \u01eb \u2208 (0, 12 ) be given and let r \u2217 1 > 0 and r \u2217 2 > 0, guaran-\nteed by cor. (12), be such that P\n(\n(\nA Hr\u2217 1\n[S(M,N),\u03c1\u2212\u03b4]\n)c )\n= 1\u2212\u01eb\nand P\n(\n(\nA Hr\u22172 [S(M,N),\u03c1\u2212\u03b4]\n)c )\n= \u01eb, respectively. Then, again by\ncor. (12), it follows that\nR(M,N) < r\u22171 < r \u2217 0 = r \u2217 0(M,N) < r \u2217 2 .\nBy symmetry, it follows that\nR(M,N) < r\u22171 < r \u2217 0 = r \u2217 0(M,N) < r \u2217 2 < 2r \u2217 0 \u2212 R(M,N). (41)\nNote that by cor. (52) and by symmetry,\nP ((\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c)\n= 0\nwhen r \u2265 2r\u22170 \u2212 R(M,N). Therefore, if ( AHr[S(M,N),\u03c1\u2212\u03b4]\n)c\noccurs with probability 0, then the property { M2\nS(M,N) < 1 2\n}\noccurs with probability 0. Otherwise, (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\nwould\noccur with positive probability, since { M2\nS(M,N) < 1 2\n}\n\u2286 {\n|<C>Hr | S(M,N) < 1 2\n} = (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c\n, upto sets of P -\nmeasure zero, by lemma (54). Hence, { M2\nS(M,N) \u2265 1 2\n}\noccurs with probability 1. As a result,\nM2\nM2 + 2M(N \u2212 1)2 \u2265\n1 2 (42)\nwith probability 1. Therefore, with probability 1 for M , it follows that N is a solution to M2+2M(N\u22121)2\u22122M2 = 0.\nLemma 55: If r \u2265 1/(2N), then\nP ((\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)c)\n= 0.\nProof: Without loss of generality, suppose area(B) = 1 and further suppose that B is divided into squares with sides of length 2r = 1/N . By hypothesis, B contains M2 data points and it is to be divided into N2 regions. Clearly then, there are no boundary hexagons separating each of the N2 regions since the sides of B have length 2rN = 1 which gives B an area of 1. Let each square be inscribed by a circle of radius r, which itself is inscribed by a hexagon. By hypothesis, each of the N2 regions in B contains at least one of the M2 data points. Hence, each of the N2 (occupied) regions is connected in a cluster to every other region in B so that P (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n) = 1. Since P (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)\n= 1 for\nr = 1/(2N), then P (\nAHr[S(M,N),\u03c1\u2212\u03b4]\n)\n= 1 for r \u2265 1/(2N)\nby prop. (76).\nAs a result of lemma (55) and by using ineq. (41), a conservative estimate for r\u22170 is given by a solution to\n2r\u22170 \u2212R(M,N) \u2265 1\n2N (43)\nthat maximizes 1/(2N) as a function of M . The value of N satisfies ineq. (42) and a maximal solution is found when equality holds. As such, for \u01eb \u2208 (0, 12 ), since (r \u2217 1 , r \u2217 2) \u2282 ( R(M,N), 2r\u22170 \u2212R(M,N) ), then by ineq. (43),\nr\u22172 \u2212 r \u2217 1 \u2248 2r \u2217 0 \u2212 2R(M,N)\n= 1\n2N \u2212R(M,N) (44)\nis an estimate of the length of the sharp threshold interval r\u22172 \u2212 r \u2217 1 about r \u2217 0 .\nUsing the value of r\u22170 given by eq. (43) and by using the estimate for the length of the sharp threshold interval about r\u22170 given by eq. (44), an estimate for the value of r \u2217 1 can be obtained. Thus, when r \u2264 r\u22171 , the property ( AHr[S(M,N),\u03c1\u2212\u03b4] )c occurs with probability at least 1 \u2212 \u01eb and falls sharply to a probability of occurrence of \u01eb as r \u2192 r\u22172 .\nBy cor. (48) and thm. (46), the left half of the sharp threshold interval about r0 is given by [r\u22171 , r0]. Using lemma (30), there exists r2 \u2264 r\u22172 such that [r0, r2] is the right half of the sharp threshold interval for \u01eb > 0 given. Thus, when r \u2264 r\u22171 , the property ( Ar[S(M,N),\u03c1\u2212\u03b4] )c\noccurs with probability at least 1 \u2212 \u01eb and falls sharply to a probability of occurrence of (no greater than) \u01eb as r \u2192 r\u22172 . As such, the sharp threshold interval for classifying M2 data points into N2 classes, in the mean continuum case, is of length (no greater than) r\u22172 \u2212 r \u2217 1 .\nTheorem 56: Let \u2206\u2217(M,N) denote the sharp threshold interval length for the event of classifying M2 random data points into N2 classes. Then,\n\u2206\u2217(M,N) = O(N\u22121).\nProof: Follows directly from eq. (44), eq. (39) and thm. (49)."}, {"heading": "V. THE NEURAL NETWORK", "text": "Let T 2 be the total number of rows in the data set from which the M2 samples are taken. From lemma (50), recall that R(M,N) is decreasing for increasing M . As such, once the actual number of classes to form, N20 , is known, then given M2 << T 2, for classifications of the ordered data set, it follows that R(M,N0) constitutes the actual upper bound on the distance that any class member is allowed to differ from the center of its respective class. As such, any anomalies, which by definition lie outside of any regularized class, will be at a distance greater than R(M,N0) from the regression hyperplane formed from the union of the regularized classes. This distance is measured as the length of the projection of an anomaly onto the normal vector of the regression hyperplane. By normality, the center of the union of regularized classes is the maximum likelihood estimate (MLE) of the union, which defines the best linear estimate, given the data. Thus, the regression hyperplane necessarily passes through the center of the union formed after the K-means clustering process.\nSince the initial classification of the ordered set was done with R(M,N) as the upper bound, then class members are candidate anomalies when they fall outside the new upper bound R(M,N0) in distance from the regression hyperplane. Furthermore, the same restriction is applied to the class centers, whereby, given that the regression hyperplane is the linear average of the data points used in its definition, then anomalous classes, those sparsely populated clusters of points which lie outside of the union of the regularized classes, are those with centers that deviate from the regression hyperplane by more than a distance of R(M,N0)."}, {"heading": "A. Definitions", "text": "Definition 57: (Macro Anomaly Detection) Let N20 be the actual number of classes to form after classification of all data points has completed. Suppose the regression hyperplane H\u03b8[M,N0] is given by ( w\u03b8[M,N0] )t z = \u03b8, for some real vector w\u03b8[M,N0] and some constant \u03b8 \u2208 R. Let h \u2208 {1, ..., N 2 0} and H\u03b8[M,N0] denote the regression hyperplane formed from the union of the regularized classes. Then, a class Ch is anomalous, if x(h) is the center of Ch and d(x(h), H\u03b8[M,N0]) \u2265 R(M,N0). Otherwise, Ch is NON-anomalous.\nDefinition 58: (Micro Anomaly Detection) Given h \u2208 {1, ..., N20}, a data point x \u2208 Ch is anomalous, if d(x,H\u03b8[M,N0]) \u2265 R(M,N0) and NON-anomalous otherwise."}, {"heading": "B. Anomaly Segregation and The Decision Boundary", "text": "The combined definitions of macro and micro anomaly detection given in defs. (57) and (58) simply states that the nonanomalous data should all be tightly wrapped in the interior of hyperspheres of diameter no more than 2 \u2217R(M,N0), with each of the weighted centers being of distance no more than R(M,N0) from the regression hyperplane, once classification has ceased.\n1) Regularity Characterized:\nLemma 59: Given h \u2208 {1, ..., N20}, a class Ch is NONanomalous if and only if d(x(h), H\u03b8[M,N0]) < R(M,N0).\nFurthermore, if Ch is NON-anomalous, then d(x, y) < 2 \u2217 R(M,N0) for all x, y \u2208 Ch.\nProof: The first part follows from the definition of an anomalous class given in section (57). For the second part, the triangle inequality and the above definitions of anomalous class and anomalous data point in sections (57) and (58), respectively, are used to obtain\nd(x, y) \u2264 d(x,H\u03b8[M,N0]) + d(H \u03b8 [M,N0] , y)\n< R(M,N0) +R(M,N0)\n= 2 \u2217R(M,N0).\n2) An Anomaly Segregation Theorem:\nTheorem 60: Given h \u2208 {1, ..., N20}, if class Ch is anomalous, then there exists at least one anomalous data point, x \u2208 Ch.\nProof: Let H\u03b8[M,N0] be defined as before and define\nX (h) [M,N0] = {x \u2208 Ch | d(x,H\u03b8[M,N0]) \u2265 R(M,N0)}. Seeking a contradiction, suppose X(h)[M,N0] = \u2205. Then, for every x \u2208 Ch, it is true that d(x,H\u03b8[M,N0]) < R(M,N0). Therefore, the contradiction is obtained and the theorem is proven, if it can be shown that there exists x \u2208 Ch such that d(x(h), H\u03b8[M,N0]) \u2264 d(x,H\u03b8[M,N0]), which implies d(x(h), H \u03b8 [M,N0]\n) < R(M,N0), the sought contradiction to class Ch being anomalous. Thus, if |Ch| = 1, then x(h) = x \u2208 Ch. Otherwise, suppose that |Ch| > 1 and assume that d(x(h), H\u03b8[M,N0]) > d(x,H \u03b8 [M,N0] ) for all x \u2208 Ch. Let vh be the vector normal to H\u03b8[M,N0] which passes through x(h) and let vth denote its transpose. If x\u0302 \u2208 Ch is such that d(x\u0302, H\u03b8[M,N0]) \u2265 d(x,H \u03b8 [M,N0]\n) for all x \u2208 Ch, then\nd(x(h), H \u03b8 [M,N0]\n) = |vthx(h)|\n\u2016vh\u2016\n= |vth\n\u2211 x\u2208Ch x\n|Ch| |\n\u2016vh\u2016\n= \u2016vh\u2016 \u22121\n| \u2211 x\u2208Ch vthx|\n|Ch| (45)\nand\nd(x\u0302, H\u03b8[M,N0]) = |vthx\u0302|\n\u2016vh\u2016\n= \u2016vh\u2016 \u22121\n\u2211\nx\u2208Ch |vthx\u0302|\n|Ch| (46)\nso that eq. (46), together with d(x(h), H\u03b8[M,N0]) > d(x,H\u03b8[M,N0]) and the triangle inequality applied to eq. (45) implies d(x,H\u03b8[M,N0]) = \u2016vh\u2016 \u22121|vthx| > \u2016vh\u2016\u22121|vthx\u0302| = d(x\u0302, H \u03b8 [M,N0]\n) for all x \u2208 Ch. This contradicts d(x\u0302, H\u03b8[M,N0]) \u2265 d(x,H \u03b8 [M,N0]\n) for all x \u2208 Ch. Thus, d(x(h), H\u03b8[M,N0]) \u2264 d(x,H \u03b8 [M,N0]\n) for at least one x \u2208 Ch, which is the originally-sought contradiction.\nTheorem (60) provides a means for segregating all anomalous data points from designated anomalous classes, leaving only classes consisting of non-anomalous data points. For each\nh \u2208 {1, ..., N20}, let X (h) [M,N0] = {x \u2208 Ch | d(x,H\u03b8[M,N0]) \u2265 R(M,N0)}, as in the proof of thm. (60). Then, (\nX (h) [M,N0]\n)c\n=\n{x \u2208 Ch | d(x,H \u03b8 [M,N0] ) < R(M,N0)} is a class of nonanomalous data points for each h \u2208 {1, ..., N20}. Define\nX[M,N0] =\nN20 \u22c3\nh=1\nX (h) [M,N0] .\nDefinition 61: \u2126 = Xc[M,N0] \u22c3 X[M,N0] is pointwise linearly separable, if there exists x \u2208 \u2126 and a subset Ax \u2282 \u2126 such that wtxy \u2264 \u03b8 for all y \u2208 Ax and w t xy > \u03b8 for all y \u2208 \u2126\\Ax, where wx \u2208 RL, L \u2265 2 and \u03b8 \u2208 R.\nTheorem 62: If X[M,N0] 6= \u2205, then \u2126 is pointwise linearly separable into X[M,N0] and X c [M,N0]\nfor some x \u2208 X[M,N0] \u2282 \u2126.\nProof: For some specific h \u2208 {1, ..., N20} to be chosen later, suppose x \u2208 X(h)[M,N0] \u2282 X[M,N0]. The idea is to shift H\u03b8[M,N0] by a certain length along the vector normal to H \u03b8 [M,N0] which passes through x(h). Thus, without loss of generality, suppose x lies to one side of H\u03b8[M,N0] so that ( w\u03b8[M,N0] )t x \u2264 \u03b8 is a hyper half-plane. Define y\u0302 \u2208 Xc[M,N0] to be a vector such that d(y\u0302, H\u03b8[M,N0]) \u2265 d(y,H \u03b8 [M,N0]\n) for all y \u2208 Xc[M,N0], where d(y,H\u03b8[M,N0]) is the length of the projection of y onto the vector normal to H\u03b8[M,N0], and is given by\nd(y,H\u03b8[M,N0]) = \u2016w \u03b8 [M,N0]\n\u2016\u22121 (\nw\u03b8[M,N0]\n)t\ny.\nDefine\nd\u03b8[M,N0] = minx\u2208X[M,N0]\n(\nd(x,H\u03b8[M,N0])\u2212 d(y\u0302, H \u03b8 [M,N0] )\n)\n. (47)\nNow, h \u2208 {1, ..., N20} can be chosen such that x \u2208 X (h) [M,N0] is a vector which satisfies eq. (47). For \u03b3 \u2208 (0, 1), let w \u03b8x\u03b3 [M,N0] be a real vector such that (\nw \u03b8x\u03b3 [M,N0]\n)t\nz = \u03b8\u2212\u03b8x\u03b3 is the hyperplane\nH \u03b8x\u03b3 [M,N0] = H\u03b8[M,N0] +\n(\n\u03b8x\u03b3 \u00d7 w\u03b8[M,N0]\n\u2016w\u03b8[M,N0]\u2016\n)\n, (48)\nwhere\n\u03b8x\u03b3 = d(x,H \u03b8 [M,N0] )\u2212 \u03b3d\u03b8[M,N0], (49)\nwith the right side of eq. (48) being a vector sum for each vector in H\u03b8[M,N0]. Thus, by copying and shifting the regression hyperplane H\u03b8[M,N0] along the direction of its normal vector in order to obtain H \u03b8x\u03b3 [M,N0]\nand by considering the reflection of the shifted hyperplane across the regression hyperplane, it now follows that\n( w \u03b8x\u03b3 [M,N0]\n)t\ny \u2264 (\u03b8 + \u03b8x\u03b3), (50)\nfor all y \u2208 X[M,N0] such that ( w\u03b8[M,N0]\n)t\ny \u2264 \u03b8 and\n( w \u03b8x\u03b3 [M,N0]\n)t\ny > (\u03b8 + \u03b8x\u03b3) (51)\nfor all y \u2208 X[M,N0] such that ( w\u03b8[M,N0]\n)t\ny > \u03b8, with\nthe opposite of inequalities (50) and (51) otherwise, for y \u2208 Xc[M,N0]. Given M 2, T 2, N20 , \u03b3 \u2208 (0, 1) and some fixed \u03b8 \u2208 R, take wx = w \u03b8x\u03b3 [M,N0]\nfor x \u2208 X[M,N0] which satisfies eq. (47).\nTheorem (62) provides the means for identifying the decision boundary to be used when determining if certain data points are anomalous, with ineq. (50) or (51) defining the shifted regression hyperplane.\n3) The Neural Network Anomaly Detector: By thm. (62), with the anomalous data points segregated and collected into the set X[M,N0], it\u2019s now possible to store the anomaly detector offline as the set of synaptic weights of a two-class discriminating neural network, which can be designed as a perceptron with a single input layer used to compute the synaptic weights w\n\u03b8x\u03b3 [M,N0] associated with copying and shifting the\nregression hyperplane H\u03b8[M,N0], given by ( w\u03b8[M,N0]\n)t\nz = \u03b8,\nin the direction of the normal vector to H\u03b8[M,N0], for some x \u2208 X (h) [M,N0] and h \u2208 {1, ..., N20}.\nTheorem 63: (Neural Network Anomaly Detector) Suppose X[M,N0] 6= \u2205 and let x \u2208 X[M,N0] satisfy eq. (47), with w \u03b8x\u03b3 [M,N0] defined by\nw \u03b8x\u03b3 [M,N0] = w\u03b8[M,N0] +\n(\n\u03b8x\u03b3 \u00d7 w\u03b8[M,N0]\n\u2016w\u03b8[M,N0]\u2016\n)\n, (52)\nfor some chosen \u03b3 \u2208 (0, 1). For all newly sampled data points y \u2208 \u2126, define \u03c6 \u03b8x\u03b3 [M,N0] : \u2126 \u2192 R as\n\u03c6 \u03b8x\u03b3 [M,N0] (y) = ( w \u03b8x\u03b3 [M,N0]\n)t\ny \u2212 (\u03b8 + \u03b8x\u03b3). (53)\nThen, the activation function \u03c6 \u03b8x\u03b3 [M,N0] , along with the synaptic weight vector w \u03b8x\u03b3 [M,N0]\n, defines a two-class discriminating neural network such that y \u2208 \u2126 is anomalous if for some \u03b8\u0302x\u03b3 \u2208 R, the reflection of \u03c6 \u03b8x\u03b3 [M,N0] across w\u03b8[M,N0] = \u03b8, given by \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] , satisfies \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] (y) > 0 whenever ( w\u03b8[M,N0] )t y \u2264 \u03b8 or if \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] (y) \u2264 0 whenever ( w\u03b8[M,N0] )t\ny > \u03b8. Otherwise, y \u2208 \u2126 is non-anomalous.\nProof: The synaptic weight vector w \u03b8x\u03b3 [M,N0] given in eq.\n(52) follows since H \u03b8x\u03b3 [M,N0]\ngiven in eq. (48) is uniquely determined by shifting the vector w\u03b8[M,N0], normal to H \u03b8 [M,N0] at the origin, in the direction which is determined by ineq. (50). Without loss of generality, suppose \u03b8 = 0 and further suppose (\nw\u03b8[M,N0]\n)t\ny \u2264 \u03b8. The neural network anomaly detector,\ngiven by the activation function \u03c6 \u03b8x\u03b3 [M,N0] in eq. (53), gives\n\u03c6 \u03b8x\u03b3 [M,N0] (y) = ( w \u03b8x\u03b3 [M,N0]\n)t\ny \u2212 (\u03b8 + \u03b8x\u03b3)\n= (\nw\u03b8[M,N0]\n)t\ny\n+\n\n  \u03b8x\u03b3 \u00d7\n(\nw\u03b8[M,N0]\n)t\ny\n\u2016w\u03b8[M,N0]\u2016\n\n  \u2212 (\u03b8 + \u03b8x\u03b3 ) (54)\n= (\nw\u03b8[M,N0]\n)t\ny \u2212 \u03b8\n+\n\n  \u03b8x\u03b3 \u00d7\n(\nw\u03b8[M,N0]\n)t\ny\n\u2016w\u03b8[M,N0]\u2016\n\n  \u2212 \u03b8x\u03b3 (55)\n\u2264\n\n  \u03b8x\u03b3 \u00d7\n(\nw\u03b8[M,N0]\n)t\ny\n\u2016w\u03b8[M,N0]\u2016\n\n  \u2212 \u03b8x\u03b3 (56)\n\u2264 0, (57)\nwhere ineq. (56) follows by the assumption that (\nw\u03b8[M,N0]\n)t\ny \u2264 \u03b8 and ineq. (57) follows since \u03b8x\u03b3 \u2265 0\nand since, by assumption, (\nw\u03b8[M,N0]\n)t\ny \u2264 \u03b8 = 0. It\nfollows that \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] (y) > 0. Similarly, \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] (y) \u2264 0 whenever (\nw\u03b8[M,N0]\n)t\ny > \u03b8. All other cases result in y being non-anomalous.\nCorollary 64: \u03b8\u0302x\u03b3 = \u2212\u03b8 x \u03b3 and \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0] = \u03c6 \u2212\u03b8x\u03b3 [M,N0] .\nProof: Follows directly from thm. (63) along with eqs. (52) and (53).\nRemark 65: The shifted regression hyperplane,\n\u03c6 \u03b8x\u03b3 [M,N0] (y) = 0, and its reflection across ( w\u03b8[M,N0]\n)t\ny = \u03b8\ngiven by \u03c6\u0302 \u03b8\u0302x\u03b3 [M,N0]\n(y) = 0, combine to segregate anomalous data points from non-anomalous data points."}, {"heading": "VI. THE SUPPORT VECTOR DETECTOR", "text": "The contrapositive of thm. (62) requires that the absence of linear separability in the projected space results in the set of anomalies being empty by necessity. Hence, \u2126 = Xc[M,N0] so that all of the data belongs to one class. Furthermore, by calculating the distance from points in X[M,N0] to the regression hyperplane, thms. (62) and (63) provide the means for identifying the class boundary to be used when determining if certain data points are anomalous, with ineq. (50) or (51) defining the shifted regression hyperplane. By identifying the set of data points in X[M,N0] which minimize the distance to each section of the regression hyperplane, the set of support vectors defining half of the bound of the intermediate region is found. Similarly, by identifying the set of data points in Xc[M,N0] which minimize the distance to the half of the bound which was previously found and by maximizing the distance to each section of the regression hyperplane, the set of support vectors defining the other half of the bound of the intermediate region is found.\nTheorem 66: If X[M,N0] 6= \u2205, then X[M,N0] contains the set of all support vectors which bound half of the intermediate region.\nProof: Iteratively, apply thm. (62) (at most) N20 times to find each x \u2208 X[M,N0] which satisfies eq. (47) by using uniformity to successively remove class Ch, where h \u2208 {1, ..., N20} gives rise to d(x,H \u03b8 [M,N0] ) = d\u03b8[M,N0], while also removing all y \u2208 X[M,N0] such that d(y,H \u03b8x\u03b3 [M,N0]\n) is minimized, for some fixed \u03b3 \u2208 (0, 1). The resulting set remains uniformly distributed in the projected space. Repeat this process (at most) N20 times to find a set V[M,N0]. Since each x \u2208 V[M,N0] \u2282 X[M,N0] satisfies eq. (47), then by construction, V[M,N0] contains the set of support vectors which bound half of the intermediate region.\nCorollary 67: Xc[M,N0] contains the set of all support vectors which bound the other half of the intermediate region.\nProof: Let Y[M,N0] = X c [M,N0] and apply thm. (66).\nRemark 68: By necessity, each of the support vectors contained in Xc[M,N0] are those data points closest in distance to the boundary of Ch, h \u2208 {1, ..., N20} when projected onto the vector normal to H \u03b8x\u03b3 [M,N0]\nwhich passes through x(h), while the support vectors contained within X[M,N0] are found by minimizing the distance to H \u03b8x\u03b3 [M,N0]\nfrom the set of anomalies which are exposed as a result of R(M,N) shrinking to R(M,N0).\nDefinition 69: Suppose x \u2208 X[M,N0] satisfies eq. (47). The equivalency class of x (denoted [x]) is the set of all vectors y \u2208 Xn such that either y satisfies eq. (47) or\nd(y,H \u03b8x\u03b3 [M,N0] ) \u2264 d\u03b8[M,N0].\nTheorem 70: (Support Vector Characterization) If X[M,N0] 6= \u2205 and x \u2208 X[M,N0] satisfies eq. (47), then [x] 6= \u2205 and [x] defines the complete set of support vectors for the hyperplanes of maximally-separating distance between X[M,N0] and X c [M,N0] .\nProof: Follows directly from thms. (62, 63, 66), cor. (64) and def. (69)."}, {"heading": "VII. CONCLUSIONS", "text": "It was shown that by mapping (possibly) higher dimensional data into a partitioned 2-dimensional space, a critical radius of connectivity could be found such that when radii are less than the critical value, then clusters of data points form. Furthermore, with the critical value defined as a function of the number of data points and the expected number of classes to form, the union of sets of regularized data points could be linearly segregated away from all other data points and that the normal vector to the shifted linear hyperplane defines weights of a 2-class discriminating neural network, which determines the support vectors of the formed binary classes. As such, the process of separating regularized data from all other (anomalous) data amounts to finding a critical radius, which is estimated in section (IV ). After formation of the regularized classes by using the critical radius as the upper bound on the distance measure, we are able to formulate three estimates of the distribution of the regularized data.\nThe regression hyperplane of the union amounts to a global, stationary estimate of the non-stationary mixture. Furthermore, by formulating a regression hyperplane for each individual class of the mixture distribution, we can estimate the nonstationary distribution of the union as a sequence of piecewise linear (stationary) estimates, one for each class. Lastly, a piecewise, non-linear (non-stationary) estimate of the mixture is determined by the mode of each class.\nFinally, by finding the x \u2208 X[M,N0] which provides the necessary shift for some \u03b3 \u2208 (0, 1), the task of segregating the union of regularized classes from the set of anomalies is completed, without the need of numerical techniques for estimation of a synaptic weight vector, since the original regression hyperplane H\u03b8[M,N0] is an analytic least squares solution, as determined by the union of the regularized classes. As the neural network detects the complete set of support vectors providing for a maximal-distance region between the union of regularized classes and the set of anomalies, no techniques such as Lagrange multipliers are required in order to determine the set of support vectors.\nAPPENDIX"}, {"heading": "A. Graph", "text": "Proposition 71: If r < r\u2032, then G(Xn; r) \u2286 G(Xn; r\u2032).\nProof: Suppose r < r\u2032. If < x, y >r \u2208 G(Xn; r), then d(x, y) \u2264 r < r\u2032 so that < x, y >r \u2208 G(Xn; r\u2032). Hence, G(Xn; r) \u2286 G(Xn; r\u2032).\nB. Increasing Property\nLemma 72: |Ar[n,\u03c1]| \u2264 1.\nProof: If Ar[n,\u03c1] = \u2205, then there is nothing to prove. Thus, suppose that Ar[n,\u03c1] occurs and < C >r \u2208 A r [n,\u03c1]. Since \u03c1n(C) \u2265 \u03c1 > 1/2, then all other connected components are of order strictly less than half of all points. Therefore, |Ar[n,\u03c1]| = 1.\nProposition 73: Ar[n,\u03c1] is an increasing property in r.\nProof: Suppose < C >r \u2208 Ar[n,\u03c1] and fix arbitrary r \u2032 > r. Then, d(x, y) \u2264 r < r\u2032 for all x, y \u2208 < C >r. Thus, < C >r \u2286 < C >r\u2032 implies N = | < C >r | \u2264 | < C >r\u2032 |. Hence, < C >r \u2208 Ar[n,\u03c1] implies < C >r\u2032 \u2208 A r [n,\u03c1]. Since r\n\u2032 > r is arbitrary, then Ar[n,\u03c1] is an increasing property in r.\nProposition 74: Ar[n,\u03c1] is a decreasing property in n.\nProof: Suppose < C >r \u2208 Ar[n,\u03c1]. If n \u2032 < n, then | < C >r |/n\u2032 > | < C >r |/n \u2265 \u03c1 so that < C >r\u2208 Ar[n\u2032,\u03c1]. Hence, Ar[n,\u03c1] \u2286 A r [n\u2032,\u03c1]. Since n\n\u2032 < n, then Ar[n,\u03c1] is decreasing in n."}, {"heading": "C. Probability Measure", "text": "Proposition 75: The property Ar[n,\u03c1] is P -measurable.\nProof: For x, y \u2208 Xn and S \u2286 Xn, define the state on < x, y >r to be 1 if and only if < x, y >r \u2208 G(S; r) and \u22121 otherwise. Then, S mutually determines an element \u03c9S \u2208 \u2126 = {\u22121, 1}Xn so that S is P -measureable. Since Ar[n,\u03c1] is\nthe property that there exists \u03c9S \u2208 \u2126 mutually determined by S \u2286 Xn such that (maxy\u2208S | < Cy >r |)/n \u2265 \u03c1, then Ar[n,\u03c1] is P -measureable.\nProposition 76: P (Ar[n,\u03c1]) is a non-decreasing function of r.\nProof: Suppose r1 \u2264 r2. Since Ar[n,\u03c1] is an increasing property in r by prop. (73), then Ar1[n,\u03c1] \u2286 A r2 [n,\u03c1] so that P (Ar1[n,\u03c1]) \u2264 P (A r2 [n,\u03c1]) by properties of probability measures. Thus, P (Ar[n,\u03c1]) is non-decreasing in r.\nProposition 77: P (Ar[n,\u03c1]) is a non-increasing function of n.\nProof: Suppose n\u2032 < n. Since Ar[n,\u03c1] is a decreasing property in n by prop. (74), then Ar[n,\u03c1] \u2286 A r [n\u2032,\u03c1] so that P (Ar[n,\u03c1]) \u2264 P (A r [n\u2032,\u03c1]) by properties of probability measures. Thus, P (Ar[n,\u03c1]) is non-increasing in n."}, {"heading": "D. Connection Radius", "text": "Proposition 78: r(n, \u03c1, \u01eb) is a non-decreasing function of \u01eb.\nProof: Suppose \u01eb1, \u01eb2 \u2208 (0, 12 ) such that \u01eb1 \u2264 \u01eb2. Define r1 = r(n, \u03c1, \u01eb1) and r2 = r(n, \u03c1, \u01eb2) and suppose r1 > r2. Since P (Ar[n,\u03c1]) is non-decreasing in r by prop. (76), then P (Ar1[n,\u03c1]) \u2265 P (A r2 [n,\u03c1]) \u2265 \u01eb2 \u2265 \u01eb1. Hence, r2 \u2208 {r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb1} and r2 < r1 = inf{r > 0 : P (A r [n,\u03c1]) \u2265 \u01eb1}. Contradiction. Thus, r1 \u2264 r2 so that r(n, \u03c1, \u01eb) is nondecreasing in \u01eb.\nLemma 79: If R = 2 \u2217 max{d(x, y) : x, y \u2208 Xn}, then Xn = {x \u2208 Xn : d(x, y) \u2264 R} for all fixed y \u2208 Xn.\nProof: Clearly, {x \u2208 Xn : d(x, y) \u2264 R} \u2286 Xn. Conversely, fix any y \u2208 Xn. For every x \u2208 Xn, it is true that d(x, y) \u2264 2 \u2217 max{d(x, y) : x, y \u2208 Xn} = R. Hence, Xn \u2286 {x \u2208 Xn : d(x, y) \u2264 R} for all fixed y \u2208 Xn. Thus, Xn = {x \u2208 Xn : d(x, y) \u2264 R} for all fixed y \u2208 Xn.\nCorollary 80: If R = 2 \u2217max{d(x, y) : x, y \u2208 Xn}, then < Cy >R \u2208 AR[n,\u03c1] for all y \u2208 Xn and n \u2265 1.\nProof: Fix an arbitrary y \u2208 Xn. By lemma (79), if < Cy >R = {x \u2208 Xn : d(x, y) \u2264 R}, then < Cy >R = Xn so that | < Cy >R | = |Xn| = n. Therefore, since y \u2208 Xn is arbitrary, then < Cy >R \u2208 AR[n,\u03c1] for all y \u2208 Xn and n \u2265 1.\nCorollary 81: If R = 2 \u2217max{d(x, y) : x, y \u2208 Xn}, then P (AR[n,\u03c1]) = 1 for all n \u2265 1.\nProof: By lemma (79) and cor. (80), it is true that Xn \u2208 AR[n,\u03c1] for all n \u2265 1 and \u03c1 \u2208 ( 1 2 , 1). Thus, A R [n,\u03c1] 6= \u2205 for all n \u2265 1 and \u03c1 \u2208 (12 , 1). Hence, P (A R [n,\u03c1]) = 1 for all n \u2265 1.\nLemma 82: If R = 2 \u2217 max{d(x, y) : x, y \u2208 Xn}, then 0 < r(n, \u03c1, \u01eb) \u2264 R for all \u01eb \u2208 (0, 12 ).\nProof: By lemma (79), it is true that Xn = {x \u2208 Xn : d(x, y) \u2264 R} for all fixed y \u2208 Xn. Therefore, P (AR[n,\u03c1]) = 1 \u2265 \u01eb for all \u01eb \u2208 (0, 12 ). Suppose that \u01eb0 \u2208 (0, 1 2 ) exists such that r0 = r(n, \u03c1, \u01eb0) > R. Thus, AR[n,\u03c1] \u2286 A r0 [n,\u03c1] so that\n1 = P (AR[n,\u03c1]) \u2264 P (A r0 [n,\u03c1])\nsince P (Ar[n,\u03c1]) is non-increasing in n by prop. (80), nondecreasing in r by prop. (76) and by properties of probability measures. Hence, P (Ar0[n,\u03c1]) = 1. But, then R \u2208 {r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb0} and R < r0 = inf{r > 0 : P (A r [n,\u03c1]) \u2265 \u01eb0}. Contradiction. Thus, 0 < r0 \u2264 R. Therefore, 0 < r(n, \u03c1, \u01eb) \u2264 R for all \u01eb \u2208 (0, 12 ).\nProposition 83: Suppose {\u01ebk \u2208 (0, 12 )}k\u22651 is any convergent sequence such that \u01ebk \u2192 \u01eb0. Define rk = r(n, \u03c1, \u01ebk) and r0 = r(n, \u03c1, \u01eb0). For arbitrary \u03be > 0, if {k \u2265 1 : |P (Ark[n,\u03c1]) \u2212 P (A r0 [n,\u03c1])| \u2265 \u03be} is a set of measure zero, then rk \u2192 r0 as k \u2192 \u221e.\nProof: If \u03be > 0 is arbitrary and {k \u2265 1 : |P (Ark[n,\u03c1]) \u2212 P (Ar0[n,\u03c1])| \u2265 \u03be} is a set of measure zero, then\nP (Ark[n,\u03c1]) = P (A r0 [n,\u03c1]) \u2265 \u01eb0\nfor all k \u2265 1. Hence, rk \u2208 {r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb0} for all k \u2265 1. Thus,\nlim k\u2192\u221e rk = lim k\u2192\u221e r(n, \u03c1, \u01ebk)\n= lim k\u2192\u221e\ninf{r > 0 : P (Ar[n,\u03c1]) \u2265 \u01ebk} (58)\n= inf{r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb0} (59)\n= r(n, \u03c1, \u01eb0)\n= r0\nwhere eq. (58) and eq. (59) follow since rk \u2208 {r > 0 : P (Ar[n,\u03c1]) \u2265 \u01ebk} \u22c2\n{r > 0 : P (Ar[n,\u03c1]) \u2265 \u01eb0} for all k \u2265 1 and \u01ebk \u2192 \u01eb0 as k \u2192 \u221e."}], "references": [{"title": "Introduction to Machine Learning, Second Edition", "author": ["E. Alpaydin"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Deploying Wireless Sensors to Achieve Both Coverage and Connectivity", "author": ["X. Bai", "S. Kumar", "D. Xuan", "Z. Yun", "T. Lai"], "venue": "ACM MobiHoc\u201906,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Preserving Coverage for Wireless Sensor Networks of Nodes with Various Sensing Ranges", "author": ["H. Bai", "X. Chen", "X. Guan"], "venue": "Proceedings of 2006 IEEE International Conference on Networking, Sensing and Control, ICNSC 2006,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Connectivity of Random k-Nearest Neighbour Graphs", "author": ["A. Balister", "B. Bollobas", "A. Sarkar", "M. Walters"], "venue": "Advances in Applied Probability,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Reliable Density Estimates for Coverage and Connectivity in Thin Strips of Finite Length", "author": ["A. Balister", "B. Bollobas", "A. Sarkar", "S. Kumar"], "venue": "ACM MobiCom\u201907,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Structural Redundancy and Multiplicity in Corporate Networks, International Network for Social Network Analysis (INSNA), Volume 30, Issue 2, pp", "author": ["R. Barnes", "T. Burkett"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Stochastic Geometry: Selected Topics", "author": ["V. Benes", "J. Rataj"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "On the Minimum Node Degree and Connectivity of a Wireless Multihop Network", "author": ["C. Bettstetter"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Genetic Algorithm Based Node Placement Methodology for Wireless Sensor Networks", "author": ["A.P. Bhondekar", "R. Vig", "M.L. Singla", "C. Ghanshyam", "P. Kapur"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Statistical Performance of Support Vector Machines", "author": ["G. Blanchard", "O. Bousquet", "P. Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Random Walks", "author": ["B. Bollobas"], "venue": "Proceedings of Symposia in Applied Mathematics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "The Influence of Variables in Product Spaces\u201d, Israel", "author": ["J. Bourgain", "J. Kahn", "G. Kalai", "Y. Katznelson", "N. Linial"], "venue": "Journal of Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}, {"title": "Critical Sensor Density for Partial Connectivity in Large Area Wireless Sensor Networks", "author": ["Haiyan Cai", "Xiaohua Jia", "Sha Mo"], "venue": "Proceedings IEEE InfoCom\u201910,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Lower Bounds for Embedding into Distributions over Excluded Minor Graph Families", "author": ["D.E. Carroll", "A. Goel"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Anomaly Detection: A Survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Sensor Placement for Maximizing Per Unit Cost in Wireless Sensor Networks", "author": ["Y. Chen", "C. Chuah", "Q. Zhao"], "venue": "Proc. IEEE Military Communications Conf.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Decomposition Methods for Linear Support Vector Machines, Acoustics", "author": ["K.M. Chung", "W.C. Cao", "C.L. Sun", "C.J. Lin"], "venue": "Speech, and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A Simple Proof of the O(sqrt(n) log3/4(n)) Upright Matching Bound", "author": ["Coffman Jr.", "E.G", "Shor P.W"], "venue": "SIAM J. Disc Math,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Generalized Poisson Distributions Properties and Applications, Marcel Dekker", "author": ["P.C. Consul"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Some Results for Two Dimensional Random Walks", "author": ["E. Csaki"], "venue": "Advances in Combinatorial Methods and Applications to Probability and Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "A Particle Visualization Framework for Clustering and Anomaly Detection", "author": ["I. Davidson", "M. Ward"], "venue": "ACM KDD Workshop on Visual Data Mining,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Maximum Likelihood for Incomplete Data via the EM Algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Seris B (Methodological), Volume 39,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1977}, {"title": "The TEX Probability : Theory and Examples., Duxbury", "author": ["R. Durrett"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Improvements on Bottleneck Matching and Related Problems Using Geometry", "author": ["A. Efrat", "A. Itai", "M. Katz"], "venue": "Proceedings of the 12th Annual Symposium on Computational Geometry,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Continuum Percolation with Unreliable and Spread-Out Connections", "author": ["M. Franceschetti", "L. Booth", "M. Cook", "R. Meester", "J. Bruck"], "venue": "Journal of Statistical Physics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Every Monotone Graph Property has a Sharp Threshold", "author": ["E. Friedgut", "G. Kalai"], "venue": "Proceedings of the American Mathematical Society,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-6,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1984}, {"title": "Continuum Percolation of Wireless Ad Hoc Communication Networks", "author": ["I. Glauche", "W. Krause", "R. Sollacher", "M. Greiner"], "venue": "Physica A,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Monotone Properties of Random Geometric Graphs have Sharp Thresholds", "author": ["A. Goel", "S. Rai", "B. Krishnamachari"], "venue": "The Annals of Applied Probability,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "The Random-Cluster Model, Springer- Verlag", "author": ["Grimmett", "Geoffrey"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Random Fields on a Network: Modeling, Statistics and Applications, Springer", "author": ["Guyon", "Xavier"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Deploying Directional Wireless Sensors with Guranteed Coverage and Connectivity", "author": ["X. Han", "X. Cao", "Y. Zhang", "E. Lloyd", "C. Shen"], "venue": "IEEE InfoCom", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Neural Networks, A Comprehensive Foundation, Macmillan College Publishing Company, Inc", "author": ["S. Haykin"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Introduction to Mathematical Statistics", "author": ["R.V. Hogg", "J.W. McKean", "A.T. Craig"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Transmission Range Control in Multi-hop Packet Radio Networks", "author": ["T. Hou", "V. Li"], "venue": "IEEE Trans. on Communications,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1986}, {"title": "A Simple Decomposition Method for Support Vector Machines", "author": ["C.W. Hsu", "C.J. Lin"], "venue": "Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Making Large-Scale Support Vector Machine Learning Practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods: Support Vector Machines,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1998}, {"title": "Weyl\u2019s Equidistribution Theorem, Resonance, pp", "author": ["A. Kar"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}, {"title": "Asymptotic Support Vector Machines with Gaussian Kernel", "author": ["S.S. Keerthi", "C.J. Lin"], "venue": "Neural Computation, Volume 15, Issue", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}, {"title": "Improvements to Platt\u2019s SMO Algorithm for SVM Classifier Design", "author": ["S.S. Keerthi", "S.K. Shevade", "C. Battacharyya", "K.R.K. Murthy"], "venue": "Neural Computation,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2001}, {"title": "Complex Dynamics of Autonomous Communication Networks and the Intelligent Communication Paradigm", "author": ["K.P. Kirilyuk"], "venue": "Report at the International Workshop on Autonomic Communication,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2004}, {"title": "Optimal Transmission Radii for Packet Radio Networks or Why Six is a Magic Number", "author": ["L. Kleinrock", "J. Silvester"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1978}, {"title": "Introductory Real Analysis, Dover Publications", "author": ["A.N. Kolmogorov", "S.V. Fomin"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1970}, {"title": "Covering the Plane with Congruent Copies of a Convex Body", "author": ["W. Kuperberg"], "venue": "Bulletin of the London Mathematical Society,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1989}, {"title": "Information-Theoretic Anomaly Detection, Proceedings", "author": ["W. Lee", "D. Xiang"], "venue": "IEEE Symposium on Security and Privacy, pp", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2001}, {"title": "Tight Bounds for Minimax Grid Matching with Applications to the Average Case Analysis of Algorithms", "author": ["T. Leighton", "P. Shor"], "venue": "Combinatorica, Vol. 9,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1989}, {"title": "On the Convergence of the Decomposition Method for Support Vector Machines", "author": ["C.J. Lin"], "venue": "IEEE Transactions on Neural Networks, Volume 12, Issue", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "On Optimal Communication Cost for Gathering Correlated Data through Wireless Sensor Networks", "author": ["J. Liu", "M. Adler", "D. Towsley", "Z. Yun", "C. Zhang"], "venue": "ACM MobiCom\u201906,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "A Generalization of the Skew-Normal Distribution: The Beta Skew-Normal, Communications in Statistics - Theory and Methods, Volume", "author": ["V. Mameli", "M. Musio"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Active Set Support Vector Machine Classification", "author": ["O.L. Mangasarian", "D.R. Musicant"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2000}, {"title": "Design and Implementation of an Anomaly Detection System: An Empirical Approach", "author": ["G. Maselli", "L. Deri", "S. Suin"], "venue": "Proceedings of Terana Networking Conference, Zagreb Croatia,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "Continuum Percolation", "author": ["Ronald Meester", "Rahul Roy"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1996}, {"title": "Neural Networks for Control", "author": ["W.T. Miller", "R.S. Sutton", "P.J. Werbos"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1990}, {"title": "Elementary Geometry from an Advanced Standpoint, Addison Wesley", "author": ["E. Moise"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1990}, {"title": "Partial Connectivity in Wireless Sensor Networks with Applications, UMI Proquest", "author": ["Murphy", "Robert"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Estimating the Mean Number of K-Means Clusters to Form, arXiv", "author": ["R. Murphy"], "venue": "ID 1503.03488,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "Improved Training Algorithm for Support Vector Machines", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "Proc. IEEE NNSP", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1997}, {"title": "Fast Training of Support Vector Machines Using Sequential Minimal Optimization, Advances in Kernel Methods: Support", "author": ["J.C. Platt"], "venue": "Vector Machines,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1998}, {"title": "The Spectrum of a Random Geometric Graph is Concentrated", "author": ["A. Rai"], "venue": "http://arxiv.org/PS cache/math/pdf/0408/0408103.pdf,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2004}, {"title": "Local Anomaly Detection", "author": ["V. Saligrama", "M. Zhao"], "venue": "Journal of Machine Learning Research, W&CP,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2012}, {"title": "The Critical Transmitting Range for Connectivity in Sparse Wireless Ad-Hoc Networks", "author": ["P. Santi", "D. Blough"], "venue": "IEEE Trans. on Mobile Computing,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2003}, {"title": "Handbook of Analysis and Its Foundations", "author": ["E. Schechter"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1996}, {"title": "Minimax Grid Matching and Empirical Measures", "author": ["P.W. Shor", "J.E. Yukich"], "venue": "The Annals of Probability,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1991}, {"title": "Conditional Anomaly Detection", "author": ["X. Song", "M. Wu", "C. Jermaine", "S. Ranka"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2007}, {"title": "A Classification Framework for Anomaly Detection", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2005}, {"title": "Optimal Transmission Ranges for Randomly Distributed Packet Radio Terminals", "author": ["H. Takagi", "L. Kleinrock"], "venue": "IEEE Trans. on Communications,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1984}, {"title": "Global Convergence of Decomposition Learning Methods for Support Vector Machines", "author": ["N. Takahashi", "T. Nishi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2006}, {"title": "Global Convergence of SMO Algorithm for Support Vector Regression", "author": ["N. Takahashi", "G. Jun", "T. Nishi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2008}, {"title": "Estimation of Dependencies Based Upon Empirical Data, Springer-Verlag", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1982}, {"title": "Asymptotic Critical Transmission Radius and Critical Neighbor Number for k-Connectivity in Wireless Ad- Hoc Networks", "author": ["P. Wan", "C. Yi"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2004}, {"title": "Integrated Coverage and Connectivity Configuration in Wireless Sensor Networks", "author": ["X. Wang", "X. Guoliang", "Y. Zhang", "C. Lu", "R. Pless", "C. Gill"], "venue": "SenSys \u201903,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2003}, {"title": "Integrated Coverage and Connectivity Configuration for Energy Conservation in Sensor Networks", "author": ["G. Xing", "X. Wang", "Y. Zhang", "C. Lu", "R. Pless", "C. Gill"], "venue": "ACM Trans. on Sensor Networks,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2005}, {"title": "On the \u03b8-Coverage and Connectivity of Large Random Networks", "author": ["F. Xue", "P.R. Kumar"], "venue": "Joint Special Issue of the IEEE Trans. on Information Theory and the IEEE/ACM Trans. on Networking on \u201dNetworking and Information Theory\u201d,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2006}, {"title": "Communication Models for Algorithm Design Automation in Wireless Sensor Networks", "author": ["Y. Yu", "B. Hong", "V.K. Prasanna"], "venue": "Proceedings of the IEEE International Parallel and Distributed Processing Symposium,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2005}, {"title": "Maintaining Sensing Coverage and Connectivity in Large Sensor Networks", "author": ["H. Zhang", "J. Hou"], "venue": "Ad Hoc & Sensor Wireless Networks,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2005}, {"title": "Continuum Limits of Markov Chains with Application to Network Modeling", "author": ["Y. Zhang", "E.K.P. Chong", "J. Hannig", "D. Estep"], "venue": "Proceedings of the 49th IEEE Conference on Decision and Control,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2010}], "referenceMentions": [{"referenceID": 49, "context": "showed in [55] that a linear support vector machine suffices when separating the data into two distinct sets with small error.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Yet, in other kinds of problems, where the data are not linearly separable, or where a linear support vector machine gives rise to a significant number of misclassifications or an intermediate region with a high number of unclassified data points, the requirement for a nonlinear support vector machine can be too expensive, both in time complexity and memory usage [17].", "startOffset": 366, "endOffset": 370}, {"referenceID": 35, "context": "The prevailing methods for dealing with the expense associated with solving the non-linear optimization problem are variations of the decomposition algorithm, [40], [42], [45], [63], whereby the data are projected to a lower dimensional subspace, whose basis vectors are chosen such that the projected data are linearly separable.", "startOffset": 159, "endOffset": 163}, {"referenceID": 36, "context": "The prevailing methods for dealing with the expense associated with solving the non-linear optimization problem are variations of the decomposition algorithm, [40], [42], [45], [63], whereby the data are projected to a lower dimensional subspace, whose basis vectors are chosen such that the projected data are linearly separable.", "startOffset": 165, "endOffset": 169}, {"referenceID": 39, "context": "The prevailing methods for dealing with the expense associated with solving the non-linear optimization problem are variations of the decomposition algorithm, [40], [42], [45], [63], whereby the data are projected to a lower dimensional subspace, whose basis vectors are chosen such that the projected data are linearly separable.", "startOffset": 171, "endOffset": 175}, {"referenceID": 57, "context": "The prevailing methods for dealing with the expense associated with solving the non-linear optimization problem are variations of the decomposition algorithm, [40], [42], [45], [63], whereby the data are projected to a lower dimensional subspace, whose basis vectors are chosen such that the projected data are linearly separable.", "startOffset": 177, "endOffset": 181}, {"referenceID": 38, "context": "al [44], Chung, et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "[17] propose the use of specific seed values for the set of Lagrange multipliers of the linear support vector machines in each subspace such that faster convergence to the solution of the non-linear optimization problem is achieved.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "Likewise, the chunking method of Vapnik [75] makes use of the fact that the rows and columns of the matrix in the quadratic constraint, which correspond to Lagrange multipliers with a zero value, can be removed, while retaining the same solution through each iteration of a sub-problem.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "In [32] and [33], it is shown that, if the probability of neighboring structures each containing related points is greater than some critical value, then with probability 1, a path can be traced from any starting occupied bounded structure to any ending occupied bounded structure, with the path in between the start and the end consisting entirely of neighboring occupied structures.", "startOffset": 12, "endOffset": 16}, {"referenceID": 51, "context": "In [57], it is shown that, with probability 1, there is a path of inter-related points between any two points in B, if the number of points relative to the area of B is beyond a certain critical number or, if the maximum distance between inter-related points is larger than a certain critical number.", "startOffset": 3, "endOffset": 7}, {"referenceID": 54, "context": "In [60], it is proven in cor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "We can make the assumption that the node process generates points according to the normal distribution by making use of a theorem from [38] in probability called the Central Limit Theorem.", "startOffset": 135, "endOffset": 139}, {"referenceID": 48, "context": "To make the assumption of uniformity of the ordered set of points and to perform clustering therein, it is first noted that the Beta distribution is the probability distribution of an order statistic of normally distributed random variables [54].", "startOffset": 241, "endOffset": 245}, {"referenceID": 51, "context": "Finally, with the defined partition, it is shown that under certain conditions, no approximation of probabilities in the continuum is required to prove the existence of a path of any order, as in [57].", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "In [13], Cai, et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [13], Cai, et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "This work uses the distance notion of connectivity without the presence of a partition, the same as in [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 51, "context": "No one class containing the majority of data points is important since, in this event, no one class will contain all data points with probability 1, which is guaranteed by [57] in the continuum and [33] in the partitioned continuum.", "startOffset": 172, "endOffset": 176}, {"referenceID": 29, "context": "No one class containing the majority of data points is important since, in this event, no one class will contain all data points with probability 1, which is guaranteed by [57] in the continuum and [33] in the partitioned continuum.", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "As in [31], for \u01eb \u2208 (0, 12 ), define", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "By properties of probabilities measures, P (A[n,\u03c1]) \u2208 [0, 1] and by prop.", "startOffset": 54, "endOffset": 60}, {"referenceID": 51, "context": "Continuity of P (A) in r is proven in [57] and can be used for proving continuity of P (A[n,\u03c1]) in r as follows.", "startOffset": 38, "endOffset": 42}, {"referenceID": 51, "context": "By arguments in [57], continuity of P (A) in r is equivalent to continuity of P (AB) in r for all bounded regions B containing 0\u0302.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Let {X(e)}e\u2208G(Xn;r) be a finite sequence of uniformly distributed random variables with values in [0, 1] and define a sequence of random variables {\u03b7k}k\u22651 by \u03b7k(e) = r(n, \u03c1, \u01ebk) \u2261 rk when X(e) < 1 and 0 otherwise.", "startOffset": 98, "endOffset": 104}, {"referenceID": 12, "context": "However, in [13], a much more practical estimate of this length is obtained after the bounded region is partitioned by hexagons of a known size.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "As in [31], for \u01eb \u2208 (0, 12 ), define r(n, \u03c1, \u01eb) = inf{r > 0 : P (Ar [n,\u03c1]) \u2265 \u01eb} (14) to be the critical radius at which Ar [n,\u03c1] occurs with probability at least \u01eb and define", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "(35) relies upon the sharp threshold inequality results of [12] and [26].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "(35) relies upon the sharp threshold inequality results of [12] and [26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "Without loss of generality, it can be assumed that r \u2208 [0, 1].", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "Without further loss of generality, let r 1 , r \u2217 2 \u2208 [0, 1] such that r 0 is the midpoint of [r \u2217 1 , r \u2217 2 ], i.", "startOffset": 54, "endOffset": 60}, {"referenceID": 12, "context": "From [13], in the proof of thm.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "To exclude these possibilities, the arguments of [13] are followed whereby a slightly larger property D [n,\u03c1\u2212\u03b4] is considered for some small \u03b4 > 0 such that the occurrence of D\u0302 [n,\u03c1] implies the occurrence of D r [n,\u03c1\u2212\u03b4].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "As in [13], let \u03c6(M) be any M -dependent integer such that \u03c6(M) \u2192 \u221e as M \u2192 \u221e and", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "Proof: As in [13], by the duality property, the occurrence of Ei, i = 1, 2, 3, 4 is equivalent to the non-occurrence of the event that there is a connected path of unoccupied hexagons crossing Hi, i = 1, 2, 3, 4 using the shortest straight-line path.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "The rest of the proof follows [13] with the edge probability as a function of point density p(\u03bb0) replaced by r(n, \u03c1, \u01eb) and the critical probability for the occurrence of an infinite cluster of occupied hexagons pc replaced by r 0 .", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Following [13], the occurrence of F implies that there is a connected path of hexagons which encloses the sub-lattice given by HB(r) \u2212 \u22c34 i=1 Hi.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "By arguments in [23] and [13], there is an \u03b1 = \u03b1(\u03c1, \u03b4) > 0 such that", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "By arguments in [23] and [13], there is an \u03b1 = \u03b1(\u03c1, \u03b4) > 0 such that", "startOffset": 25, "endOffset": 29}, {"referenceID": 61, "context": "The Monotone Convergence Theorem [67] applied to E[1Ar [n,\u03c1+\u03b4] ] and E[1Ar [n,\u03c1] ] guarantees that P (A[n,\u03c1+\u03b4]) \u2192 P (A r [n,\u03c1]) as \u03b4 \u2192 0 .", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "From [13], differentiability of P (A[n,\u03c1]) in point density \u03bb = \u03bb(n) = E[n] implies continuity of P (A[n,\u03c1]) in \u03bb so that the following holds", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "As such, tools from percolation [32] and the random cluster model [33] can readily be employed.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "We use random geometric graphs to describe clusters of higher dimensional data points which are bijectively mapped to a (possibly) lower dimensional space where an equivalent random cluster model is used to calculate the expected number of modes to be found when separating the data of a multimodal data set into distinct clusters. Furthermore, as a function of the expected number of modes and the number of data points in the sample, an upper bound on a given distance measure is found such that data points have the greatest correlation if their mutual distances from a common center is less than or equal to the calculated bound. Anomalies are exposed, which lie outside of the union of all regularized clusters of data points. Similar to finding a hyperplane which can be shifted along its normal to expose the maximal distance between binary classes, it is shown that the union of regularized clusters can be used to define a hyperplane which can be shifted by a certain amount to separate the data into binary classes and that the shifted hyperplane defines the activation function for a two-class discriminating neural network. Lastly, this neural network is used to detect the set of support vectors which determines the maximally-separating region between the binary classes.", "creator": "LaTeX with hyperref package"}}}