{"id": "1602.03903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Wavelet-Based Semantic Features for Hyperspectral Signature Discrimination", "abstract": "hyperspectral signature classification is a quantitative analysis approach for hyperspectral imagery processes which performs detection and classification of the constituent materials at the pixel level in the overall scene. the classification procedure can be operated directly on hyperspectral data or performed by using some mathematical features extracted from the corresponding hyperspectral signatures containing information like the signature's energy or shape. in this paper, we describe : a technique that applies non - homogeneous hidden markov chain ( nhmc ) models to hyperspectral signature classification. the so basic idea is to use statistical models ( such as nhmc ) algorithms to characterize wavelet coefficients measurements which capture the spectrum semantics ( i. e., structural information ) contained at multiple levels. experimental results show that the modelling approach based on nhmc models can outperform several existing approaches relevant in classification tasks.", "histories": [["v1", "Thu, 11 Feb 2016 21:25:36 GMT  (683kb,D)", "https://arxiv.org/abs/1602.03903v1", "23 pages, 8 figures, preprint, first published online on April 28 2015"], ["v2", "Fri, 8 Apr 2016 19:50:27 GMT  (862kb,D)", "http://arxiv.org/abs/1602.03903v2", "21 pages, 8 figures, 4 tables, preprint, revised April 8 2016"]], "COMMENTS": "23 pages, 8 figures, preprint, first published online on April 28 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["siwei feng", "yuki itoh", "mario parente", "marco f duarte"], "accepted": false, "id": "1602.03903"}, "pdf": {"name": "1602.03903.pdf", "metadata": {"source": "CRF", "title": "Wavelet-Based Semantic Features for Hyperspectral Signature Discrimination", "authors": ["Siwei Feng", "Yuki Itoh", "Mario Parente", "Marco F. Duarte"], "emails": ["yitoh}@engin.umass.edu", "mduarte}@ecs.umass.edu."], "sections": [{"heading": null, "text": "Hyperspectral signature classification is a quantitative analysis approach for hyperspectral imagery which performs detection and classification of the constituent materials at the pixel level in the scene. The classification procedure can be operated directly on hyperspectral data or performed by using some features extracted from the corresponding hyperspectral signatures containing information like the signature\u2019s energy or shape. In this paper, we describe a technique that applies non-homogeneous hidden Markov chain (NHMC) models to hyperspectral signature classification. The basic idea is to use statistical models (such as NHMC) to characterize wavelet coefficients which capture the spectrum semantics (i.e., structural information) at multiple levels. Experimental results show that the approach based on NHMC models can outperform existing approaches relevant in classification tasks.\nIndex Terms\nClassification, Hyperspectral Signatures, Semantics, Wavelet, Hidden Markov Model\nI. INTRODUCTION\nHyperspectral remote sensors collect reflected image data simultaneously in hundreds of narrow, adjacent spectral bands that make it possible to derive a continuous spectrum curve for each image cell. Such hyperspectral reflectance curves provide insight into the on-ground (or near ground) constituent materials in a single remotely sensed pixel.\nThe identification of ground materials from hyperspectral images often requires comparing the reflectance spectra of the image pixels, extracted endmembers, or ground cover exemplars to a training library of spectra obtained in the laboratory from well characterized samples. There is a rich literature on hyperspectral image classification (see [5] for a recent survey); however, classification methods emphasizing matching to a spectral library and material identification have received less attention [6], [7], [8]. On the one hand, many methods rely on nearest neighbor\nThe authors are with the Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, 01003, USA. E-mail: {siwei, yitoh}@engin.umass.edu and {mparente, mduarte}@ecs.umass.edu. Portions of this work have been presented at the IEEE Workshop on Hyperspectral Image and Signal Proc.: Evolution in Remote Sensing (WHISPERS) [1], the Allerton Conference on Communication, Control, and Computing [2] the IEEE Int. Conf. Image Processing (ICIP) [3], and the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) [4].\nThis work was supported by the National Science Foundation under grant number IIS-1319585.\nar X\niv :1\n60 2.\n03 90\n3v 2\n[ cs\n.C V\n] 8\nA pr\n2 01\n2 classification schemes based on one of many possible spectral similarity measures to match the observed test spectra with training library spectra. On the other hand, practitioners have designed feature extraction schemes that capture relevant information, in conjunction with appropriate similarity metrics, in order to discriminate between different materials.\nClassification methods based on spectral similarity measures can provide researchers with simple implementation and relatively small computational requirements; however, there is a tradeoff with the amount of storage required for the training spectra as well as with the uneven performance of nearest neighbor methods. For example, in some cases taking the whole spectrum into consideration brings a large amount of redundant information to practitioners, while the role of relevant structural features is weakened.\nPractitioners recognize several structural features in the spectral curves of each material as \u201cdiagnostic\u201d or characteristic of its chemical makeup, such as the position and shape of absorption bands. Several approaches like the Tetracorder [9] have been proposed to encode such characteristics. However, such techniques require the construction of ad-hoc rules to characterize instances of each material while new rules must be created when spectral species which were not previously analyzed are added. Parente et al. [10] proposed an approach using parametric models to represent the absorption features. However, it still requires the construction of specific rules to match observations to a training library.\nIn this paper, we consider the formulation of an information extraction process from hyperspectral signatures via the use of mathematical models for hyperspectral signals. Our goal is to encode the signature\u2019s scientifically meaningful structural features into numerical features, which are referred to as semantic features, without adhoc rules for the spectra of any material type. Our proposed method provides automated extraction of semantic information from the hyperspectral signature, in contrast with the aforementioned diagnostic characteristics designed by hand by expert practitioners. Furthermore, no new rules should need to be constructed when mineral species which were not analyzed before are added.\nMathematical signal models have been used to represent reflectance spectra. More specifically, models leveraging wavelet decompositions are of particular interest because they enable the representation of structural features at different scales. The wavelet transform is a popular tool in many signal processing applications due to the capability of wavelet coefficients to characterize signal discontinuities at different scales and offsets. As mentioned above, the semantic information utilized by researchers is heavily related to the shape of reflectance spectra, which is succinctly represented in the magnitudes of its wavelet coefficients. A coefficient with large magnitude generally indicates a rapid change in its support while a small wavelet coefficient generally implies a smooth region. Existing wavelet approaches are limited to filtering techniques but do not extract features [6], [7].\nIn this paper, we apply hidden Markov models (HMMs) to the wavelet coefficients derived from the observed hyperspectral signals so that the correlations between wavelet coefficients at adjacent scales can be captured by the models. The HMMs allow us to identify significant vs. nonsignificant portions of the hyperspectral signatures with respect to the database used for training. The applications of HMMs for this purpose is inspired by the hidden Markov tree (HMT) model proposed in [11]. As for the wavelet transform, we use an undecimated wavelet transform\n3 (UWT) in order to obtain maximum flexibility on the set of scales and offsets (spectral bands or wavelengths1) considered.\nOur model for a spectrum encompassing N spectral bands takes the form of a collection of N non-homogeneous hidden Markov chains (NHMCs), each corresponding to a particular spectral band. Such a model provides a map from each signal spectrum to a binary space that encodes the structural features at different scales and wavelengths, effectively representing the semantic features that allow for the discrimination of spectra. To the best of our knowledge, the application of statistical wavelet models to the automatic selection of semantically meaningful features in hyperspectral signatures has not been proposed previously.\nThis paper is organized as follows. Section II introduces the mathematical background behind our hyperspectral signature classification system and reviews relevant existing approaches for the hyperspectral classification task. Section III provides an overview of the proposed feature extraction method, including details about the choice of mother wavelet, statistical model training, and label computing; we also show examples of the semantic information in hyperspectral signatures captured by the proposed features. Section IV describes our experimental test setup as well as the corresponding results. Some conclusions are provided in Section V. Finally, proofs of our theoretical results are presented in the appendix."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "In this section, we begin by discussing several existing spectral matching approaches. Then, we review the theoretical background for our proposed hyperspectral signature classification system, including wavelet analysis, hidden Markov chain models, and the Viterbi algorithm."}, {"heading": "A. Spectral Matching Measures", "text": "A direct comparison of spectral similarity measures taken on the observed hyperspectral signals is the easiest and the most direct way to do spectral matching. Generally speaking, spectral similarity measures can be combined with nearest neighbor classifiers. In this paper we use four commonly used spectral similarity measures. To present these measures, we use ri = (ri1, ri2, ..., riN )T and rj = (rj1, rj2, ..., rjN )T to denote the reflectance or radiance signatures of two hyperspectral image pixel vectors\n1) Spectral Angle Measure: The spectral angle measure (SAM) [12] between two reflectance spectra is defined\nas\nSAM(ri, rj) = cos \u22121 ( \u3008ri, rj\u3009\u221a ||ri||22||rj ||22 ) .\nA smaller spectral angle indicates larger similarity between the spectra.\n2) Euclidean Distance Measure: The Euclidean distance measure (ED) [13] between two reflectance spectra is defined as ED(ri, rj) = ||ri\u2212rj ||2. As with SAM, smaller ED implies larger similarity between two vectors. The ED measure takes the intensity of two reflectance spectra into account, while the former is invariant to intensity.\n1We use these three equivalent terms interchangeably in the sequel.\n4 3) Spectral Correlation Measure: The spectral correlation measure (SCM) [14] between two reflectance spectra\nis defined as\nSCM(ri, rj) = \u2211N k=1(rik \u2212 r\u0304i)(rjk \u2212 r\u0304j)\u221a\u2211N\nk=1(rik \u2212 r\u0304i)2 \u2211N k=1(rjk \u2212 r\u0304j)2 .\nwhere r\u0304i is the mean of the values of all the elements in a reflectance spectrum vector ri. The SCM can take both positive or negative values; larger positive values are indicative of similarity between spectra.\n4) Spectral Information Divergence Measure: The spectral information divergence measure (SID) [15] between two reflectance spectra is defined as SID(ri, rj) = D(ri||rj) + D(rj ||ri), where D(ri||rj) is regarded as the relative entropy (or Kullback-Leibler divergence) of rj with respect to ri, which is defined as\nD(ri||rj) = \u2212 N\u2211 k=1 pik(log pjk \u2212 log pik).\nHere pik = rik/ \u2211N k=1 rik corresponds to a normalized version of the spectrum ri at the k th spectral band, which is interpreted in the relative entropy formulation as a probability distribution."}, {"heading": "B. Wavelet Analysis", "text": "The wavelet transform of a signal provides a multiscale analysis of a signal\u2019s content which effectively encodes the locations and scales at which the signal structure is present in a compact fashion [16]. To date, several hyperspectral classification methods based on wavelet transform have been proposed. Most of these classification approaches (e.g. [8], [17], [18]) employ a dyadic/decimated wavelet transform (DWT) as the preprocessing step. Compared with UWT, the DWT provides a more concise representation because it minimizes the amount of redundancy in the coefficients. However, the tradeoff for such redundancy is that UWT provides maximum flexibility on the choice of scales and offsets used in the multiscale analysis, which is desired because it allows for a simple characterization of the spectrum structure at each individual spectral band.\nA one-dimensional real-valued UWT of an N -sample signal x \u2208 RN is composed of wavelet coefficients ws, each labeled by a scale l \u2208 1, ..., L and offset n \u2208 1, ..., N , where L 6 N . The coefficients are defined using inner products as wl,n = \u3008x, \u03c6l,n\u3009, where \u03c6l,n \u2208 RN denotes a sampled version of the mother wavelet function \u03c6 dilated to scale l and translated to offset n:\n\u03c6l,n(\u03bb) = 1\u221a l \u03c6 ( \u03bb\u2212 n l ) .\nTo improve the interpretability of the notation, we will change our notation for scales in the sequel from l = 1, 2, . . . , L to s = L,L\u2212 1, . . . , 1 (i.e., we reverse the ordering of the scales). With this change, small values of s correspond to coarse scales while large values of s correspond to fine scales. All the coefficients can be organized into a two-dimensional matrix W of size L\u00d7N , where rows represent scales and columns represent wavelengths. In this case, each coefficient ws,n, where s < L, has a child coefficient ws+1,n at scale s + 1. Similarly, each coefficient ws,n at scale s > 1 has one parent ws\u22121,n at scale s \u2212 1. Such a structure in the wavelet coefficients enables the representation of fluctuations in a spectral signature by chains of large coefficients appearing within the columns of the wavelet coefficient matrix W .\n5"}, {"heading": "C. Advantages of Haar Wavelet", "text": "The Haar wavelet is the simplest possible compact wavelet which has the properties of square-like shape and discontinuity. These properties makes the Haar wavelet sensitive to a larger range of fluctuations than other mother wavelets and provides it with a lower discriminative power. Thus, the Haar wavelet enables the detection of both slow-varying fluctuations and sudden changes in a signal [16], while not particularly sensitive to small discontinuities (i.e., noise) on a signal, in effect averaging them out over the wavelet support.\nConsider the example in Fig. 1, where the figure at the top represents an example hyperspectral signature, while the figures in the middle and at the bottom show the undecimated wavelet coefficient matrix of the spectrum under the Haar and Daubechies-4 wavelets, respectively. The middle figure in Fig. 1 shows the capability of Haar wavelets to capture both rapid changes and gently sloping fluctuations in the sample reflectance spectrum. Similarly, the bottom figure shows that the Daubechies-4 wavelet is sensitive to compact and drastic discontinuities (i.e., higher order fluctuations that are often due to noise). Thus, the Daubechies-4 wavelet does not provide a good match to semantic information extraction for this example reflectance spectrum. Intuitively, these issues will also be present for other higher-order wavelets, which provide good analytical matches to functions with fast, high-order fluctuations.\nIn general, wavelet representations of spectral absorption bands are less emphasized under Haar wavelet than under other higher order wavelets. However, this drawback can be alleviated using discretization, which will be described in the next subsection."}, {"heading": "D. Statistical Modeling of Wavelet Coefficients", "text": "Crouse et al. [11] proposed the use of hidden Markov models (HMM) to capture the statistics of DWT coefficients. In that paper, the dyadic nature of DWT coefficients gives rise to a hidden Markov tree (HMT) model that characterizes the clustering and persistence properties of wavelet coefficients. The statistical model is constructed based on the wavelet representation of spectra in a training library.\n6 The statistical model is motivated by the compression property of the DWT, which states that the wavelet transform of a piecewise smooth signal generally features a small number of large coefficients and a large number of small coefficients. This property motivates the use of a zero-mean Gaussian mixture model (GMM) with two Gaussian components to capture the compression property, where one Gaussian component with a high-variance characterizes the small number of \u201clarge\u201d coefficients (labeled with a state L), while a second Gaussian component with a low-variance characterizes the large number of \u201csmall\u201d wavelet coefficients (labeled with a state S). The state Ss \u2208 {S,L} of a wavelet coefficient1 is said to be hidden because its value is not explicitly observed. The likelihoods of the two Gaussian components pSs(L) = p(Ss = L) and pSs(S) = p(Ss = S) should meet the condition that pSs(L) + pSs(S) = 1. The conditional probability of a particular wavelet coefficient ws given the value of the state Ss can be written as p(ws|Ss = i) = N (0, \u03c32i,s), where i = {S,L}, and the distribution of the same wavelet coefficient can be written as p(ws) = pSs(L)N (0, \u03c32L,s) + pSs(S)N (0, \u03c32S,s).\nIn cases where a UWT is used, the persistence property of wavelet coefficients [19], [20] (which implies the high probability of a chain of wavelet coefficients to be consistently small or large across adjacent scales) can be accurately modeled by a non-homogeneous hidden Markov chain (NHMC) that links the states of wavelet coefficients in the same offset. This means the state Ss of a coefficient ws is only affected by the state Ss\u22121 of its parent (if it exists) and by the value of its coefficient ws. The Markov chain is completely determined by the likelihoods for the first state and the set of state transition matrices for the different parent-child label pairs (Ss\u22121, Ss) for s > 1:\nAs = pS\u2192S,s pL\u2192S,s pS\u2192L,s pL\u2192L,s  , (1) where pi\u2192j,s := P (Ss = j|Ss\u22121 = i) for i, j \u2208 {L,S}. The training process of an HMM is based on the expectation maximization (EM) algorithm which generates a set of HMM parameters \u03b8 = {pS1(S), pS1(L), {As}Ls=2, {\u03c3S,s, \u03c3L,s}Ls=1} including the probabilities for the first hidden states, the state transition matrices, and Gaussian variances for each of the states. We define the L \u00d7 N matrix S containing the collection of state values for all scales and spectral bands. The iterative parts of the algorithm can be briefly described as follows:\n1) E step: Perform maximum likelihood estimation of the state labels using a forward-backward algorithm [21]:\nSl = arg max S p(S|W,\u03b8l);\nthis joint conditional probability mass function (PMF) will be used in the M step.\n2) M step: Update model parameters to maximize the expected value of the joint likelihood of the wavelet\ncoefficients and state estimates [11]:\n\u03b8l+1 = arg min \u03b8 ES [ln f(W,S|\u03b8l)|W,\u03b8l].\n1Since the same model is used for each chain of coefficients {S1,n, . . . , SL,n}, n = 1, . . . , N , we remove the index n from the subscript for simplicity in this sequel whenever possible.\n7 3) Set l = l + 1. If converged, then stop; otherwise, repeat."}, {"heading": "E. Wavelet-based Spectral Matching", "text": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23]. In this paper, we review two approaches that are particularly close in scope to our proposed method, which will be used for comparison in our numerical experiments. Since our focus in this paper is on hyperspectral classification for individual pixels, we limit our comparison to methods that rely exclusively on the spectral of a given pixel or on features obtained from the pixel\u2019s spectra. More specifically, we do not compare to other methods that use additional information (e.g. spatial information for a HSI) or that assume prior knowledge of the location of semantic information, which is usually obtained from an expert practitioner.\nFirst, Rivard et al. [6] propose a method based on the wavelet decomposition of the spectral data. The obtained wavelet coefficients are separated into two categories: low-scale components of power (LCP) capturing mineral spectral features (corresponding to the first fine scales), and high-scale components of power (HCP) containing the overall continuum (corresponding to coarser scales). The coefficients for the LCP spectrum, which capture detailed structural features, are summed across scales at each spectral band. This process can conceptually be described as a filtering approach, since the division into LCP and HCP effectively acts as a high-pass filter that preserves only the fine-scale detailed portion of the spectrum.\nA second wavelet-based classification approach is proposed in [7]. This second approach applies an UWT on the entire database. The set of wavelet coefficients for each separate wavelength is considered as a separate feature vector. Linear discriminant analysis (LDA) is performed on each one of these vectors for dimensionality reduction purposes. The outputs are grouped into C classes, corresponding to the elements of interest, to train either a single multivariate Gaussian distribution or a GMM for each of the classes, where a classification label or score is obtained for each wavelength. Finally, decision fusion is performed among the wavelengths to obtain a single classification label for the spectrum. It is implicitly expected by this method that the number of training samples for each one of the classes is sufficiently large so that the class-specific Gaussian (mixture) models can be accurately constructed."}, {"heading": "III. NHMC-BASED FEATURE EXTRACTION AND CLASSIFICATION", "text": "In this section, we introduce a feature extraction scheme for hyperspectral signatures that exploits a Markov model for the signature\u2019s wavelet coefficients. A wavelet analysis is used in an UWT to capture information on the fluctuations of the spectra. The state labels extracted from the Markov model represent the semantic information relevant for hyperspectral signal processing."}, {"heading": "A. Multi-State Hidden Markov Chain Model", "text": "In our system, we choose to use the NHMC model described in Section II-D applied to the UWT via the Haar wavelet. We select the Haar wavelet due to its special shape, which allows for the magnitude of the wavelet\n8 coefficients to be proportional to the slope of the spectra across the wavelet\u2019s support. Furthermore, the signs of these coefficients are indicative of the slope orientation (increasing or decreasing for negative and positive, respectively).\nIn contrast to the prior work of [11], we design our NHMC to feature k-state GMMs for the wavelet coefficients. We increase the number of states from 2 to k > 2 because a two-state zero-mean GMM provides an overly coarse distinction between sharper absorption bands (fluctuations) and flatter regions in a hyperspectral signature, which are usually assigned large and small state labels, respectively. In our cases of interest, spectrum classification requires a labeling granularity for the signature fluctuations that is finer than that achieved by binary labels.\nWe associate each wavelet coefficient ws with an unobserved hidden state Ss \u2208 {0, 1, ..., k\u22121}, where the states have prior probabilities pi,s := p(Ss = i) for i = 0, 1, ..., k \u2212 1. Here the state i = 0 represents smooth regions of the spectral signature, in a fashion similar to the small (S) state for binary GMMs, while i = 1, . . . , k\u2212 1 represent a more finely grained set of states for spectral signature fluctuations, similarly to the large (L) state in binary\nGMMs. All the weights should meet the condition \u2211k\u22121 i=0 pi,s = 1. Each state is characterized by a zero-mean Gaussian distribution for the wavelet coefficient with variance \u03c32i,s. The value of Ss determines which of the k components of the mixture model is used to generate the probability distribution for the wavelet coefficient ws:\np(ws|Ss = i) = N (0, \u03c32i,s). We can then infer that p(ws) = \u2211k\u22121 i=0 pi,sp(ws|Ss = i). In analogy with the binary GMM case, we can also define a k \u00d7 k transition probability matrix\nAs =  p0\u21920,s p1\u21920,s \u00b7 \u00b7 \u00b7 pk\u22121\u21920,s p0\u21921,s p1\u21921,s \u00b7 \u00b7 \u00b7 pk\u22121\u21921,s ... ... . . . ...\np0\u2192k\u22121,s p1\u2192k\u22121,s \u00b7 \u00b7 \u00b7 pk\u22121\u2192k\u22121,s\n ,\nwhere pi\u2192j,s = p(Ss = j|Ss\u22121 = i). Note that the probabilities in the diagonal of As are expected to be larger than those in the off-diagonal elements due to the persistence property of wavelet transforms. Note also that all state probabilities pi,s for s > 1 can be derived from the matrices {As}Ls=2 and {pi,1}k\u22121i=0 .\nThe training of the k-GMM NHMC is also performed via an EM algorithm. Because of the overlap between wavelet functions at a fixed scale and neighboring offsets, adjacent coefficients may have correlations in relative magnitudes [24]. However, for computational reasons, in this paper we only consider the parent-child relationship of the wavelet coefficients in the same offset. Namely, we train an NHMC separately on each of the N wavelengths sampled by the hyperspectral acquisition device. The set of NHMC parameters \u03b8n of a certain spectral band n include the probabilities for the first hidden states {pi,1,n}k\u22121i=0 , the state transition matrices {As,n}Ls=2, and the Gaussian variances {\u03c320,s,n, \u03c321,s,n, . . . , \u03c32k\u22121,s,n}Ls=1. In the sequel, we remove from the parameters \u03b8 the dependence on the wavelength index n whenever possible."}, {"heading": "B. Label Computation", "text": "Given the model parameters \u03b8, the state label values {Ss}Ls=1 for a given observation are obtained using a Viterbi algorithm [21], [11]. For a particular wavelet coefficient ws, a k-dimensional conditional probability vector\n9 is defined with elements being the conditional PMF of the wavelet coefficient\np(ws|Ss = i) = 1\u221a\n2\u03c0\u03c32s exp\n( \u2212ws 2\n2\u03c32s ) under each possible state value i = 0, . . . , k \u2212 1. A variable \u03b4i,s is defined as the \u201cbest score\u201d that ends in a particular state i at scale s from its previous state, while the variable \u03c8i,s is the most likely state at a particular scale s\u2212 1 to have children s with state i. The definitions of the two variables are\n\u03c8i,1 = 0, (2)\n\u03b4i,1 = pi,1 \u00b7 p(w1|S1 = i), (3)\n\u03c8i,s = arg max j=0,...,k\u22121 (\u03b4j,s\u22121pj\u2192i,s), (4)\n\u03b4i,s = \u03b4\u03c8i,s,s\u22121p\u03c8i,s\u2192i,s \u00b7 p(ws|Ss = i), (5)\nfor i = 1, . . . , k \u2212 1 and s = 2, . . . , L. The algorithm also returns the likelihood p(W |\u03b8) of a wavelet coefficient matrix W under the model \u03b8 as a byproduct. We propose the use of the state label array S as classification features for the original hyperspectral signal x. It is easy to identify the presence of such features simply by inspecting the labels obtained from the NHMC."}, {"heading": "C. Additional Modifications to NHMC", "text": "As mentioned above, because of the shape of the Haar wavelet function, the signs of Haar wavelet coefficients of a reflectance spectrum capture whether the slopes increase or decrease as a function of wavelength. This characteristic of Haar wavelet coefficients can be utilized to design state labels that capture the slope orientations of the corresponding reflectance spectra. Thus, we make a simple modification by adding the sign of a Haar wavelet coefficient to its counterpart in the corresponding state label matrix. Fig. 2 shows the effect of adding signs to state label matrices. The top two figures represent the reflectance spectrum of a sample material and the corresponding Haar wavelet coefficient matrix, while the bottom two show the corresponding state label matrices with and without being added wavelet coefficient signs, respectively. The figure shows that the fluctuations in the region 0.6\u22120.8 \u00b5m are predominantly not detected by state labels. Furthermore, one can see many narrow chains of \u201clarge\u201d state labels starting at 1.7 \u00b5m. Increasing the number of GMM state enables a finer-scale quantization of spectral signature fluctuations, which is somewhat analogous to increasing the quantization resolution for our wavelet analysis. This is quite important when the Haar wavelet is used due to its sensitivity to a large range of fluctuation orders, which implies a relatively low discriminative power when compared with higher-order wavelet transforms.\nUnfortunately, a large number of GMM states might also have negative influence on classification results. The GMM state of a particular wavelet coefficient ws,n is determined by the coefficient\u2019s magnitude with respect to those for the rest of the NHMC training spectra, the state label of its parent Ss\u22121,n, and the transition probability matrix As,n. In practice, this dependence causes different maps between coefficient value ranges and GMM states across scales and offsets (s, n). This variance often makes it difficult to assess the semantic information in the label array\n10\nof a spectral signature. In practice, this variance may sometimes affect the interpretability of features obtained from GMM labels. Furthermore, the likelihood of such variability in the value-to-state mappings could increase when we use multi-state GMM. Additionally, such variance may have a particularly negative influence on classification schemes based on NN classifiers that act on GMM state label vectors. Thus, we desire a modification to the model that features the simplicity of a binary-state GMM (to preclude mismatch in coefficient-to-state mappings across wavelengths and states) and the spectral fluctuation characterization capability of a multi-state GMM (providing finer fluctuation characterization than a binary-state GMM).\nWe propose a solution that combines the advantages of a binary-state GMM and a k-state GMM, where k > 2. Our modified wavelet coefficient statistical model consists of a binary-state NHMC with a \u201csmall\u201d state (0) modeled by a standard zero-mean Gaussian distribution and a \u201clarge\u201d state (1) modeled by a mixture of k-1 Gaussian distributions. Note that we use numbers here instead of letters for the state labels to distinguish between the 2-state GMM NHMC and the 2-state MOG NHMC. We denote this modified model mixture of Gaussians (MOG) NHMC in the sequel. As desired, this modified model maintains the discriminability between smooth regions and absorption bands in spectral signatures, while providing classification features (binary labels, in this case) that decrease the likelihood of the variability stated above.\nIn order to obtain a MOG NHMC model, the first step is to train a k-state GMM NHMC model that yields state labels Ss \u2208 {0, . . . , k\u22121}. After that, all the states are quantized into two states so that we can get a MOG NHMC that yields state labels Zs \u2208 {0, 1} with probabilities qi,s = P (Zs = i), i = 0, 1. One can show that the change of\n11\nmodels lead to the following mapping for labels:\nZ(S) =  0 if S = 0,1 if S 6= 0. (6) Similar to (1), we can define a transition probability matrix\nBs = q0\u21920,s q1\u21920,s q0\u21921,s q1\u21921,s  for the MOG NHMC, where qi\u2192j,s := P (Zs = j|Zs\u22121 = i) for i, j \u2208 {0, 1} and s = 1, . . . , L. We have the following pair of intuitive results, whose proves are presented in Appendices A and B.\nLemma 1. Denote the vector of state probabilities for a wavelet coefficient ws under the k-state GMM NHMC as P s = (p0,s, p1,s, ..., pk\u22121,s)T . The corresponding vector of probabilities for the MOG NHMC states Qs can be written as follows:\nQs = (q0,s, q1,s) T = ( p0,s, k\u22121\u2211 i=1 pi,s )T = (p0,s, 1\u2212 p0,s)T .\nLemma 2. The elements of the MOG NHMC transition matrix Bs can be written in terms of the elements of the GMM NHMC transition matrix As as follows:\nq0\u21920,s = p0\u21920,s, (7)\nq1\u21920,s = \u2211k\u22121 i=1 pi\u21920,spi,s\u22121\u2211k\u22121\ni=1 pi,s\u22121 , (8)\nq0\u21921,s = k\u22121\u2211 j=1 p0\u2192j,s, (9)\nq1\u21921,s =\n\u2211k\u22121 i=1 pi,s\u22121 \u2211k\u22121 j=1 pi\u2192j,s\u2211k\u22121\ni=1 pi,s\u22121 . (10)\nHere i and j represent state labels ranging from 1 to k \u2212 1.\nBelow is an example of the transform of a state probability vector and transition probability matrix, respectively,\nwhere the original number of state is 4:\n(0.422, 0.3696, 0.1042, 0.1042)T \u2192 (0.422, 0.578)T , 1 0.0001 0 0 0 0.9999 0 0 0 0 0.5 0.4999\n0 0 0.5 0.5001\n\u2192 1 0 0 1  .\n12\nCorrespondingly, we also make small modifications to the label computation scheme from Section III-B. For the\nMOG NHMC, equations (2\u20135) become\n\u03c8i,1 = 0,\n\u03b4i,1 = qi,1 \u00b7 p(w1|Z1 = i),\n\u03c8i,s = arg max j=0,1 (\u03b4j,s\u22121qj\u2192i,s),\n\u03b4i,s = \u03b4\u03c8i,s,s\u22121q\u03c8i,s\u2192i,s \u00b7 p(ws|Zs = i),\nrespectively, for i = 0, 1 and s = 2, . . . , L. The required conditional probabilities involving Zs can be written as given in the following lemma.\nLemma 3. The state-conditional probabilities for the MOG NHMC can be given in terms of the state-conditional probabilities for the GMM NHMC as follows:\np(ws|Zs = 0) = p(ws|Ss = 0), p(ws|Zs = 1) = \u2211k\u22121 i=1 pi,sp(ws|Ss = i)\u2211k\u22121\ni=1 pi,s ,\nwhere i denotes a state label ranging from 1 to k \u2212 1.\nWe provide an example comparison between labels obtained from the GMM NHMC and the MOG NHMC in\nFig. 3.\n13\nD. Illustration of Extracted Semantic Information\nThe state label arrays obtained from the NHMC model characterize four important semantic features of the corresponding hyperspectral signatures: (i) the orientations of the signature slope, which is reflected in the state label values; (ii) the extent of the signature slope, which is reflected in the duration of corresponding state label values through different wavelengths; (iii) the intensity of the signature slope, which is reflected on the depth of the corresponding state label values through the scales (when GMMs are used); and (iv) the locations of the absorption bands, which are reflected in the locations at which the labels switch from +1 to \u22121. In order to showcase the semantic information captured by our designed features, we illustrate these four types of semantic features in several example reflectance spectra. For convenience of illustration, we only use state label arrays based on MOG due to its binary property, which only reflects the orientation of slopes regardless of the intensities. To begin, we calculate the mean of each column in a state label array and then transform it to an integer by using round. In this way, we obtain what we call a state label mean vector of the same length as the corresponding reflectance spectrum whose possible element values are \u00b11 and 0. Figure 4 shows four example reflectance spectra with the corresponding extracted semantic information based on an NHMC using an MOG with 2 states as well as the corresponding state label arrays. We plot the reflectance spectral curve by using three different colors to encode the value of the state label mean vector: green, red, and blue portions represent wavelengths for which the state label mean vector elements are 0, +1, and \u22121, respectively. Finally, we calculate all the middle points between the end of a 1\u2019s series and the beginning of a \u22121\u2019s series, and mark those points on the plotted reflectance spectra to find the locations of absorption bands. As expected, spectral curves in Fig. 4 have blue increasing slopes, red decreasing slopes, and green flat regions.\n14"}, {"heading": "E. Classification System Overview", "text": "We provide an overview of the NHMC-based hyperspectral classification system in Fig. 5. The system consists of two modules: an NHMC model training module and a classification module. While the figure assumes a binary-state Gaussian mixture model (GMM) in the NHMC, as described in Section II-D, one can easily formulate a k-ary GMM state variant, k = 2, 3, . . ., as described in Section III. The training stage uses a training library of spectra containing samples from the classes of interest to train the NHMC model, which is then used to compute state estimates for each of the training spectra using a Viterbi Algorithm. The state arrays obtained from the NHMC model will then be used as classification features coupled with a classification scheme, e.g., nearest-neighbor (NN) or support vector machine (SVM) classification. The testing module considers a spectrum under test and computes the state estimates under the trained NHMC model using the parameters obtained during training. The module then applies the classification scheme being tested, returning the class label of the selected training spectrum."}, {"heading": "IV. CLASSIFICATION EXPERIMENTS AND RESULT ANALYSIS", "text": "In this section, we present multiple experimental results that assess the performance of the proposed features in hyperspectral signature classification. We also study the effect of NHMC parameter selections on the classification performance from the corresponding extracted features.\n15"}, {"heading": "A. Study Data and Performance Evaluation", "text": "The dataset used in this paper is a part of the RELAB spectral database with 26 mineral reflectance spectrum classes. Since the spectra in the original database have different wavelength ranges, we only use the spectral region from 0.35 \u00b5m to 2.6 \u00b5m (if applicable) which contains almost all of the visible and near-infrared region of the electromagnetic spectrum. We only use the spectra with spectral resolution being 5 nm to eliminate the differences in spectral resolution in different sources. A different number of samples is present in each mineral class. Thus, in order to ensure the same weight of each class in the training process, we use the Hapke mixing model [25] to generate additional mixtures of existing spectra in a given class until all classes have the same number of samples. We do this to prevent different mineral types from having different contributions to the model obtained and influencing the final classification accuracy. The final dataset contains 1690 reflectance spectra with each class including 65 reflectance spectra. Additionally, in order to eliminate the influences caused by illumination conditions, we perform normalization on the whole database by dividing each reflectance spectrum by its maximum value.\nWe compare different NHMC models (both GMM and MOG with different number of mixed Gaussian components and with/without assigning Haar wavelet coefficient signs to state labels). We first randomly separate the dataset into a training library (including 1352 samples with each class containing 52 reflectance spectra) and a test set (including 338 samples with each class containing 13 reflectance spectra). In order to evaluate the performance of different NHMC-based features, we train these NHMC-based features on the training library. Then we use the Viterbi algorithm to obtain the corresponding state labels for both the training library and test set and use both linear and non-linear classifiers (nearest neighbor (NN) classifier, support vector machine (SVM) classifier) on the test set to evaluate the classification accuracy of different models.\nUnfortunately, the resulting dataset features a significant separation between the different classes, and so it is difficult to differentiate the performances of the different proposed methods, which are very high. In order to discriminate among the methods, we introduced mixing into the database as an attempt to increase the variability among reflectance spectra in each given class. Our mixing methodology is designed to resemble the image blurring process common in hyperspectral imaging. First, we randomly order the reflectance spectra in the database into a 3-D array (a so-called datacube) with two spatial dimensions and one spectral dimension. We then perform identical spatial blurring on each wavelength using a 3\u00d73-pixel Gaussian smoothing operator. Finally, we build a new library from the blurred pixels\u2019 spectra while retaining the original labels. By performing this image-based blurring, each spectrum in the resulting database exhibits a mixture of structural features from spectra in multiple classes, which provides a more challenging spectrum classification setup. We vary the Gaussian blurring kernel variance among a range of values to adjust the amount of mixing performed: the dominant material percentage (DMP) of the original pixel in the corresponding blurred pixel is obtained as the normalized weight of the central element in a Gaussian smoothing operator. In our experiment, we vary the DMP from 70% to 100% with a step of 5%.\n16"}, {"heading": "B. Feature Comparison", "text": "For this study, classification performance is evaluated by using NN and SVM classification accuracies. For the NN classifier, three distance metrics are employed: `1 distance, Euclidean (`2) distance, and cosine similarity measure. For the SVM classifier, we use radial basis function (RBF) as the kernel and perform a grid search for the corresponding parameter values (cost and Gaussian variance) that provide best performance for each NHMC model. Both the NHMC model (if applicable) and the classifier (NN or SVM) are trained using the aforementioned training set, and the performance is measured on the aforementioned test set.\nFigure 6 shows the classification rates for different NHMC models under different dominant material percentages using the aforementioned NN and SVM classifiers. Additionally, the figure also includes the classification accuracies of the related approaches described in Section II-E. In the figure, different classification features are identified as follows: \u201cRivard\u201d denotes the approach proposed in [6];2 \u201cWavelet Coefficient\u201d denotes the classification scheme of using wavelet coefficients as classification features; \u201cSpectral Similarity\u201d denotes spectral similarity matching classification scheme (i.e., the spectra themselves are the input to each NN classifier); \u201cGMM\u201d denotes an NHMC featuring Gaussian mixture models; \u201cMOG\u201d denotes an NHMC featuring mixtures of Gaussians; and \u201cGMM+Sign\u201d and \u201cMOG+Sign\u201d denotes the previous two approaches where Haar wavelet coefficient signs being added to state labels. Our NHMC tests involve NHMC models containing different numbers of mixed Gaussian components; Fig. 6 shows the highest performance among all tested values for the number of mixed Gaussian components, and Tables I-IV list the best-performing values for each DMP.\nWe highlight some features of the obtained results:\n\u2022 In most cases, the use of signs in the NHMC features improves performance with respect to their original\ncounterparts.\n\u2022 In the NN classifiers, GMM performs better than MOG for lower DMPs, which are more challenging settings,\nwhile MOG with additional signs outperforms GMM for DMPs closer to 100%. Nonetheless, in most cases\n2Note that \u201cRivard\u201d only appears in the bottom left figure of Fig. 6 because it is defined specifically in terms of a NN classifier with cosine\ndistance [6].\n17\n18\nWe also attempted to implement the approach proposed in [7]. However, because of the lack of sufficient data for individual classes, we obtained several ill-conditioned covariance matrices when constructing multivariate GMMs. Thus, we do not include the comparison with this approach in this paper."}, {"heading": "C. NHMC Parameters", "text": "Next, we evaluate the effect of the number of states included in the NHMC model on the performance of the tested classifiers. We set the DMP to 85% for concreteness, and evaluate the classification performance of all proposed NHMC features with NN and SVM classifiers as a function of the number of states, which varies between 2 and 10 for GMM and between 3 and 10 for MOG. Fig. 7 shows the variation tendency of classification accuracy with increasing number of mixed Gaussian components using different classifiers and similarity metrics.\nFrom these four figures, we see that MOG with additional wavelet coefficient signs provides relatively consistent performance compared with other NHMC-based models. Additionally, in terms of classification accuracy, the two model configurations using MOG provide two performance extremes: by adding wavelet coefficient signs we obtain\n19\nthe highest classification performance, while MOG without signs provides the lowest one. As mentioned earlier, MOG combines the simplicity of a binary-state GMM and the spectral fluctuation characterization capability of a multistate GMM. In that case, if we do not consider the signs of the wavelet coefficient, spectra that have approximately matching locations for their fluctuations while exhibiting differing magnitudes and orientations will be matched to similar MOG label vectors. The reason is that a binary-state GMM form could assign the same state labels to several fluctuations of different levels and orientations. However, if Haar wavelet coefficient signs are added, the state labels better reflect the spectral fluctuation orientation information."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed the design of a feature extraction scheme for hyperspectral signatures that preserves the semantic information used by practitioners in signature discrimination (i.e., location of distinguishing fluctuations and discontinuities). Our approach is automated thanks to the use of statistical models for wavelet transform coefficients, which succinctly capture the location and magnitude of fluctuations in the spectra observed. Furthermore, the\n20\nstatistical model also enables a segmentation of the spectra into informative and non-informative portions. The success of statistical modeling is mostly dependent on the availability of a large-scale database for training containing representative examples of the spectra that are observed by the sensing system.\nWe also tested the quality of the preservation of semantic information in our proposed features by using a simple example hyperspectral classification system based on nearest neighbor search. We also compared our feature extraction method with three existing feature extraction approaches for classification; the first approach is spectral matching, which performs classification directly on the hyperspectral signature; the second approach performs classification directly on wavelet coefficients, and the third approach computes features as the sum of wavelet coefficients of certain scales. We showed that the performance of our proposed features meets or exceeds that of baselines relying on spectral matching and wavelet coefficient representations, in particular for high DMP.\nWhile the performance of each method we tested decreases as the DMP is reduced, the reduction is stronger for the MOG and GMM methods in comparison with some of their counterparts (in particular, to the case where NN with cosine similarity is applied directly on the spectra). We believe that this effect is due to the additional difficulty of modeling signal classes of increased variability (as the DMP decreases) using the extracted binary features. Nonetheless, we note that even with this handicap the performance of the best combinations of NHMC features and NN classifiers exceeds the performance of the comparison baseline methods when the DMP is sufficiently large. Furthermore, we believe that the size of the datasets we use here, while much larger than that of our previous results [1], [2], [3], may still be insufficient to fully exploit the power of the statistical models leveraged here. Thus, further work will focus on expanding the size of database and investigating additional modifications to the feature extraction scheme and the underlying statistical models. As an example, NHMC models based on nonzeromean GMM are an attractive alternative to be pursued in the future, as in certain cases the histogram of wavelet coefficients cannot be accurately modeled by zero-mean Gaussian mixture models."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Mr. Ping Fung for providing an efficient implementation of our NHMC training code that largely\ndecreased the time of running experiments."}, {"heading": "APPENDIX A", "text": "PROOF OF LEMMA 1\nBy denoting ps,S\u2192Za\u2192b = p(Zs = b|Ss = a) and using law of total probability, we can get p(Zs = b) =\u2211 a p s,S\u2192Z a\u2192b p(Ss = a). From the Z(S) map in eq. (6), we can infer that p s,S\u2192Z 0\u21920 = 1 and p s,S\u2192Z i\u21921 = 1. Therefore, it is easy to derive the conclusion in Lemma 1."}, {"heading": "APPENDIX B", "text": "PROOF OF LEMMA 2\nThe relationship between the original state labels Ss\u22121, Ss and the combined state labels Zs\u22121, Zs can be characterized by a directed graphical model shown in Fig. 8. By considering all possible transitions from Zs\u22121 to\n21\nZs through the state transitions Ss\u22121 to Ss and the map above, and denoting\nps,Z\u2192Sa\u2192b = p(Ss = b|Zs = a),\nwe appeal to the law of total probability to write\nqb\u2192a,s = k\u22121\u2211 x=0 k\u22121\u2211 y=0 ps,S\u2192Zx\u2192a py\u2192x,sp s\u22121,Z\u2192S b\u2192y . (11)\nFrom the Z(S) map in equation (6), we can also infer that ps,S\u2192Z0\u21921 = 0, p s,S\u2192Z i\u21920 = 0, p s\u22121,Z\u2192S 0\u21920 = 1, p s\u22121,Z\u2192S 0\u2192i = 0, ps\u22121,Z\u2192S1\u21920 = 0, and\nps\u22121,Z\u2192S1\u2192i = p(Ss\u22121 = i)\u2211k\u22121 j=1 p(Ss\u22121 = j) ,\nwhere i = 1, ..., k\u22121. After combining the equalities above with (11) for a, b \u2208 {0, 1}, we can get the four elements in new matrices expressed in (7\u2212 10), proving the lemma."}], "references": [{"title": "A new semantic wavelet-based spectral representation", "author": ["M. Parente", "M.F. Duarte"], "venue": "IEEE Workshop on Hyperspectral Image and Signal Proc.: Evolution in Remote Sensing (WHISPERS), Gainesville, FL, June 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-homogeneous hidden Markov chain models for wavelet-based hyperspectral image", "author": ["M.F. Duarte", "M. Parente"], "venue": "Allerton Conf. Communication, Control, and Computing (Allerton), Monticello, IL, Oct. 2013, pp. 154\u2013159.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Tailoring non-homogeneous Markov chain wavelet models for hyperspectral signature classification", "author": ["S. Feng", "Y. Itoh", "M. Parente", "M.F. Duarte"], "venue": "IEEE Int. Conf. Image Processing (ICIP), Paris, France, Oct. 2014, pp. 5073\u20135077.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Universality of wavelet-based non-homogeneous hidden Markov chain model features for hyperspectral signatures", "author": ["S. Feng", "M. Duarte", "M. Parente"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), 2015, pp. 19\u201327.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in hyperspectral image classification: Earth monitoring with statistical learning methods", "author": ["G. Camps-Valls", "D. Tuia", "L. Bruzzone", "J. Atli Benediktsson"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 1, pp. 45\u201354, Jan. 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous wavelets for the improved use of spectral libraries and hyperspectral data", "author": ["B. Rivard", "J. Feng", "A. Gallie", "A. Sanchez-Azofeifa"], "venue": "Remote Sensing of Environment, vol. 112, no. 6, pp. 2850\u20132862, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Information fusion in the redundant-wavelet-transform domain for noise-robust hyperspectral classification", "author": ["S. Prasad", "W. Li", "J.E. Fowler", "L.M. Bruce"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 50, no. 9, pp. 33 473\u20133486, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavelet domain statistical hyperspectral soil texture classification", "author": ["X. Zhang", "N.H. Younan", "C.G.O. Hara"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 43, no. 3, pp. 615\u2013618, Mar. 2005.  22", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Imaging spectroscopy: Search and planetary remote sensing with the USGS Tetracorder and expert systems", "author": ["R. Clark", "G.A. Swayze", "K. Livo", "S. Sutley", "J. Dalton", "R. Mc-Dougal", "C. Gent"], "venue": "Journal of Geophysical Research: Planets, vol. 108, no. E12, Dec. 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Decomposition of mineral absorption bands using nonlinear least squares curve fittening: application to martian meteorites and CRISM data", "author": ["M. Parente", "H.D. Makarewicz", "J.L. Bishop"], "venue": "Planetary and Space Science, vol. 59, no. 5\u20136, pp. 423\u2013442, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelet-based statistical signal processing using hidden Markov models", "author": ["M.S. Crouse", "R.D. Nowak", "R.G. Baraniuk"], "venue": "IEEE Trans. Signal Processing, vol. 46, no. 4, pp. 886\u2013902, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "The Spectral Image Processing System (SIPS) \u2013 Interactive visulization and analysis of imaging spectrometer data", "author": ["F.A. Kruse", "A.B. Lefkoff", "J.W. Boardman", "K.B. Heidebrecht", "A.T. Shapiro", "P.J. Barloon", "F.H. Goetz"], "venue": "Remote Sensing of Environment, vol. 44, no. 2\u20133, pp. 145\u2013163, May 1993.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "The spectral similarity scale and its application to the classification of hyperspectral remote sensing data", "author": ["J.N. Sweet"], "venue": "IEEE Workshop on Advances in Techniques for Analysis of Remotely Sensed Data, pp. 92\u201399, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "CCSM: Cross correlogram spectral matching", "author": ["F. van der Meer", "W. Bakker"], "venue": "International Journal of Remote Sensing, vol. 18, no. 3, pp. 1197\u20131201, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "An information-theoretic approach to spectral variability, similarity, and discrmination for hyperspectral image analysis", "author": ["C.I. Chang"], "venue": "IEEE Trans. Information Theory, vol. 46, no. 5, pp. 1927\u20131932, May 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1927}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Hyperspectral imaging with wavelet transform for classification of colon tissue biopsy samples", "author": ["K. Masood"], "venue": "Proc. SPIE 7073, Applications of Digital Image Processing XXXI, ser. Proc. SPIE, vol. 7073, San Diego, CA, Aug. 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiclassifiers and decision fusion in the wavelet domain for exploitation of hyperspectral data", "author": ["T. West", "S. Prasad", "L.M. Bruce"], "venue": "IEEE Int. Sym. Geoscience and Remote Sensing (IGARSS), 2007, pp. 4850\u20134853.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Singularity detection and processing with wavelets", "author": ["S. Mallat", "W. Hwang"], "venue": "IEEE Trans. Information Theory, vol. 38, no. 2, pp. 617\u2013643, Mar. 1992.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "Characterization of signals from multiscale edges", "author": ["S. Mallat", "S. Zhong"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 14, no. 7, pp. 710\u2013732, Jul. 1992.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Hyperspectral image classification based on structured sparse logistic regression and three-dimensional wavelet texture features", "author": ["Y. Qian", "M. Ye", "J. Zhou"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 51, no. 4, pp. 2276\u20132291, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Three-dimensional gabor wavelets for pixel-based hyperspectral imagery classification", "author": ["L. Shen", "S. Jia"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 49, no. 12, pp. 5039\u20135046, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "An investigation of wavelet-based image coding using an engropy-constrained quantization framework", "author": ["M.T. Orchard", "K. Ramchandran"], "venue": "Data Compression Conf. (DCC), Snowbird, UT, Mar. 1994, pp. 341\u2013350.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Theory of reflectance and emittance spectroscopy", "author": ["B. Hapke"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "There is a rich literature on hyperspectral image classification (see [5] for a recent survey); however, classification methods emphasizing matching to a spectral library and material identification have received less attention [6], [7], [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "There is a rich literature on hyperspectral image classification (see [5] for a recent survey); however, classification methods emphasizing matching to a spectral library and material identification have received less attention [6], [7], [8].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "There is a rich literature on hyperspectral image classification (see [5] for a recent survey); however, classification methods emphasizing matching to a spectral library and material identification have received less attention [6], [7], [8].", "startOffset": 233, "endOffset": 236}, {"referenceID": 7, "context": "There is a rich literature on hyperspectral image classification (see [5] for a recent survey); however, classification methods emphasizing matching to a spectral library and material identification have received less attention [6], [7], [8].", "startOffset": 238, "endOffset": 241}, {"referenceID": 0, "context": ": Evolution in Remote Sensing (WHISPERS) [1], the Allerton Conference on Communication, Control, and Computing [2] the IEEE Int.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": ": Evolution in Remote Sensing (WHISPERS) [1], the Allerton Conference on Communication, Control, and Computing [2] the IEEE Int.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Image Processing (ICIP) [3], and the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) [4].", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Image Processing (ICIP) [3], and the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "Several approaches like the Tetracorder [9] have been proposed to encode such characteristics.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "[10] proposed an approach using parametric models to represent the absorption features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Existing wavelet approaches are limited to filtering techniques but do not extract features [6], [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Existing wavelet approaches are limited to filtering techniques but do not extract features [6], [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "The applications of HMMs for this purpose is inspired by the hidden Markov tree (HMT) model proposed in [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": ", rjN ) to denote the reflectance or radiance signatures of two hyperspectral image pixel vectors 1) Spectral Angle Measure: The spectral angle measure (SAM) [12] between two reflectance spectra is defined as SAM(ri, rj) = cos \u22121 ( \u3008ri, rj\u3009 \u221a ||ri||2||rj ||2 ) .", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "2) Euclidean Distance Measure: The Euclidean distance measure (ED) [13] between two reflectance spectra is defined as ED(ri, rj) = ||ri\u2212rj ||2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "3) Spectral Correlation Measure: The spectral correlation measure (SCM) [14] between two reflectance spectra is defined as SCM(ri, rj) = \u2211N k=1(rik \u2212 r\u0304i)(rjk \u2212 r\u0304j) \u221a\u2211N k=1(rik \u2212 r\u0304i) \u2211N k=1(rjk \u2212 r\u0304j) .", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "4) Spectral Information Divergence Measure: The spectral information divergence measure (SID) [15] between two reflectance spectra is defined as SID(ri, rj) = D(ri||rj) + D(rj ||ri), where D(ri||rj) is regarded as the relative entropy (or Kullback-Leibler divergence) of rj with respect to ri, which is defined as D(ri||rj) = \u2212 N \u2211", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Wavelet Analysis The wavelet transform of a signal provides a multiscale analysis of a signal\u2019s content which effectively encodes the locations and scales at which the signal structure is present in a compact fashion [16].", "startOffset": 217, "endOffset": 221}, {"referenceID": 7, "context": "[8], [17], [18]) employ a dyadic/decimated wavelet transform (DWT) as the preprocessing step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[8], [17], [18]) employ a dyadic/decimated wavelet transform (DWT) as the preprocessing step.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "[8], [17], [18]) employ a dyadic/decimated wavelet transform (DWT) as the preprocessing step.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "Thus, the Haar wavelet enables the detection of both slow-varying fluctuations and sudden changes in a signal [16], while not particularly sensitive to small discontinuities (i.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "[11] proposed the use of hidden Markov models (HMM) to capture the statistics of DWT coefficients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In cases where a UWT is used, the persistence property of wavelet coefficients [19], [20] (which implies the high probability of a chain of wavelet coefficients to be consistently small or large across adjacent scales) can be accurately modeled by a non-homogeneous hidden Markov chain (NHMC) that links the states of wavelet coefficients in the same offset.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "In cases where a UWT is used, the persistence property of wavelet coefficients [19], [20] (which implies the high probability of a chain of wavelet coefficients to be consistently small or large across adjacent scales) can be accurately modeled by a non-homogeneous hidden Markov chain (NHMC) that links the states of wavelet coefficients in the same offset.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "The iterative parts of the algorithm can be briefly described as follows: 1) E step: Perform maximum likelihood estimation of the state labels using a forward-backward algorithm [21]:", "startOffset": 178, "endOffset": 182}, {"referenceID": 10, "context": "2) M step: Update model parameters to maximize the expected value of the joint likelihood of the wavelet coefficients and state estimates [11]:", "startOffset": 138, "endOffset": 142}, {"referenceID": 5, "context": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23].", "startOffset": 152, "endOffset": 155}, {"referenceID": 21, "context": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "Many hyperspectral signature classification approaches have been proposed in the literature, with a subset of them involving wavelet analysis [6], [7], [8], [22], [23].", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "[6] propose a method based on the wavelet decomposition of the spectral data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A second wavelet-based classification approach is proposed in [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "In contrast to the prior work of [11], we design our NHMC to feature k-state GMMs for the wavelet coefficients.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "Because of the overlap between wavelet functions at a fixed scale and neighboring offsets, adjacent coefficients may have correlations in relative magnitudes [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "Given the model parameters \u03b8, the state label values {Ss}s=1 for a given observation are obtained using a Viterbi algorithm [21], [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Given the model parameters \u03b8, the state label values {Ss}s=1 for a given observation are obtained using a Viterbi algorithm [21], [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "Thus, in order to ensure the same weight of each class in the training process, we use the Hapke mixing model [25] to generate additional mixtures of existing spectra in a given class until all classes have the same number of samples.", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "In the figure, different classification features are identified as follows: \u201cRivard\u201d denotes the approach proposed in [6];2 \u201cWavelet Coefficient\u201d denotes the classification scheme of using wavelet coefficients as classification features; \u201cSpectral Similarity\u201d denotes spectral similarity matching classification scheme (i.", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "6 because it is defined specifically in terms of a NN classifier with cosine distance [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "We also attempted to implement the approach proposed in [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "Furthermore, we believe that the size of the datasets we use here, while much larger than that of our previous results [1], [2], [3], may still be insufficient to fully exploit the power of the statistical models leveraged here.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "Furthermore, we believe that the size of the datasets we use here, while much larger than that of our previous results [1], [2], [3], may still be insufficient to fully exploit the power of the statistical models leveraged here.", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Furthermore, we believe that the size of the datasets we use here, while much larger than that of our previous results [1], [2], [3], may still be insufficient to fully exploit the power of the statistical models leveraged here.", "startOffset": 129, "endOffset": 132}], "year": 2016, "abstractText": "Hyperspectral signature classification is a quantitative analysis approach for hyperspectral imagery which performs detection and classification of the constituent materials at the pixel level in the scene. The classification procedure can be operated directly on hyperspectral data or performed by using some features extracted from the corresponding hyperspectral signatures containing information like the signature\u2019s energy or shape. In this paper, we describe a technique that applies non-homogeneous hidden Markov chain (NHMC) models to hyperspectral signature classification. The basic idea is to use statistical models (such as NHMC) to characterize wavelet coefficients which capture the spectrum semantics (i.e., structural information) at multiple levels. Experimental results show that the approach based on NHMC models can outperform existing approaches relevant in classification tasks.", "creator": "LaTeX with hyperref package"}}}