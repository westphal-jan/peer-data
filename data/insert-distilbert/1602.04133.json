{"id": "1602.04133", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation", "abstract": "deep gaussian processes ( dgps ) are multi - layer hierarchical generalisations of gaussian processes ( gps ) and are formally equivalent to actual neural networks with multiple, infinitely wide horizontal hidden layers. dgps engines are nonparametric finite probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. this paper develops a new approximate bayesian learning scheme that enables dgps to be applied to a range of medium to large scale regression problems for the first time. the new method uses an approximate expectation propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. we evaluate the new method for non - ordinary linear regression on eleven real - world datasets, showing that it always outperforms gp regression and is almost already always better than state - of - the - art deterministic and precision sampling - based approximate inference design methods for bayesian neural networks. as a by - product, this work provides a comprehensive analysis of six approximate elementary bayesian methods for numerical training neural networks.", "histories": [["v1", "Fri, 12 Feb 2016 17:32:39 GMT  (486kb)", "http://arxiv.org/abs/1602.04133v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["thang d bui", "daniel hern\u00e1ndez-lobato", "jos\u00e9 miguel hern\u00e1ndez-lobato", "yingzhen li", "richard e turner"], "accepted": true, "id": "1602.04133"}, "pdf": {"name": "1602.04133.pdf", "metadata": {"source": "CRF", "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation", "authors": ["Thang D. Bui"], "emails": ["tdb40@cam.ac.uk", "daniel.hernandez@uam.es", "yl494@cam.ac.uk", "jmhl@seas.harvard.edu", "ret26@cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n04 13\n3v 1"}, {"heading": "1 Introduction", "text": "Gaussian Processes (GPs) are powerful nonparametric distributions over continuous functions that can be used for both supervised and unsupervised learning problems (Rasmussen & Williams, 2005). In this article, we study a multi-layer hierarchical generalisation of GPs or deep Gaussian Processes (DGPs) (Damianou & Lawrence, 2013) for supervised learning tasks. A GP is equivalent to an infinitely wide neural network with single hidden layer and similarly a DGP is a multi-layer neural network with multiple infinitely wide hidden layers (Neal, 1995). The mapping between layers in this type of network is parameterised by a GP, and, as a result, DGPs retain useful properties of GPs such as nonparametric modelling power and well-calibrated predictive uncertainty estimates. In addition, DGPs employ a hierarchical structure of GP mappings and therefore are arguably more flexible, have\na greater capacity to generalise, and are able to provide better predictive performance (Damianou, 2015). This family of models is attractive as it can also potentially discover layers of increasingly abstract data representations, in much the same way as their deep parametric counterparts, but it can also handle and propagate uncertainty in the hierarchy.\nThe addition of non-linear hidden layers can also potentially overcome practical limitations of shallow GPs. First, modelling real-world complex datasets often requires rich, hand-designed covariance functions. DGPs can perform input warping or dimensionality compression or expansion, and automatically learn to construct a kernel that works well for the data at hand. As a result, learning in this model provides a flexible form of Bayesian kernel design. Second, the functional mapping from inputs to outputs specified by a DGP is non-Gaussian which is a more general and flexible modelling choice. Third, DGPs can repair damage done by sparse approximations to the representational power of each GP layer. For example, pseudo datapoint based approximation methods for DGPs trade model complexity for a lower computational complexity ofO(NLM2) whereN is the number of datapoints, L is the number of layers, and M is the number of pseudo datapoints. This complexity scales quadratically in M whereas the dependence on the number of layers L is only linear. Therefore, it can be cheaper to increase the representation power of the model by adding extra layers rather than by adding more pseudo datapoints.\nThe focus of this paper is Bayesian learning of DGPs, which involves inferring the posterior over the layer mappings and hyperparameter optimisation via the marginal likelihood. Unfortunately, exact Bayesian learning in this model is analytically intractable and as such approximate inference is needed. Current proposals in the literature do not scale well and have not been compared to alternative deep Bayesian models. We will first review the model and past work in Section 2, and then make the following contributions:\n\u2022 We propose a new approximate inference scheme for DGPs for regression, using a sparse GP approximation, a novel approximate Expectation Propagation scheme and the probabilistic backpropagation algorithm, resulting in a computationally efficient, scalable and easy to implement algorithm (Sections 3, 4 and 5).\n\u2022 We demonstrate the validity of our method in supervised learning tasks on various medium to large scale datasets and show that the proposed method is always better than GP regression and is almost always better than state-of-the-art approximate inference techniques for multi-layer neural networks (Section 8)."}, {"heading": "2 Deep Gaussian processes", "text": "We first review DGPs and existing literature on approximate inference and learning for DGPs. Suppose we have a training set comprising of N D-dimensional input and observation pairs (xn, yn). For ease of presentation, the outputs are assumed to be real-valued scalars, but other types of data can be easily accommodated1 . The probabilistic representation of a DGP comprising of L layers can be written as follows,\np(fl|\u03b8l) = GP(fl;0,Kl), l = 1, \u00b7 \u00b7 \u00b7 , L p(hl|fl,hl\u22121, \u03c32l ) = \u220f\nn\nN (hl,n; fl(hl\u22121,n), \u03c32l ), h1,n = xn\n1We also discuss how to handle non-Gaussian likelihoods in the supplementary material.\np(y|fL,hL\u22121, \u03c32L) = \u220f\nn\nN (yn; fL(hL\u22121,n), \u03c32L)\nwhere hidden layers2 are denoted hl,n and the functions in each layer, fl. More formally, we place a zero mean GP prior over the mapping fl, that is, given the inputs to fl any finite set of function values are distributed under the prior according to a multivariate Gaussian p(fl) = N (f ;0,Kff ). Note that these function values and consequently the hidden variables are not marginally normally distributed, as the inputs are random variables. When L = 1, the model described above collapses back to GP regression or classification. When the inputs {xn} are unknown and random, the model becomes a DGP latent variable model, which has been studied in Damianou & Lawrence (2013).\nAn example of DGPs when L = 2 and dim(h1) = 2 is shown in Figure 1. We use this network with the proposed approximation and training algorithm to fit a value function of the mountain car problem (Sutton & Barto, 1998) from a small number of noisy evaluations. This function is particularly difficult for models such as GP regression with a standard exponentiated quadratic kernel due to a steep value function cliff, but is well handled by a DGP with only two GP layers. Interestingly the functions in the first layer are fairly simple and learn to cover or explain different parts of the input space.\n2Hidden variables in the intermediate layers can and will generally have multiple dimensions but we have omitted this here to lighten the notation.\nWe are interested in inferring the posterior distribution over the latent function mappings and the intermediate hidden variables, as well as obtaining a marginal likelihood estimate for hyperparameter tuning and model comparison. Due to the nonlinearity in the hierarchy, these quantities are analytically intractable. As such, approximate inference is required. The simplest approach is to obtain the maximum a posteriori estimate of the hidden variables (Lawrence & Moore, 2007). However, this procedure is prone to over-fitting and does not provide uncertainty estimates. An alternative existing approach is based on a variational-free-energy method proposed by Damianou & Lawrence (2013), extending the seminal work on variational sparse GPs by Titsias (2009). In this scheme, a variational approximation over both latent functions and hidden variables is chosen such that a free energy is both computationally and analytically tractable. Critically, as a variational distribution over the hidden variables is used in this approach, in addition to one over the inducing outputs, the number of variational parameters increases linearly with the number of training datapoints which hinders the use of this method for large scale datasets. Furthermore, initialisation for this scheme is a known issue, even for a modest number of datapoints (Turner & Sahani, 2011). An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models.\nA special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; La\u0301zaro-Gredilla, 2012). In La\u0301zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood. However, the performance of warped GPs is often similar to a standard GP, most likely due to the narrow bottleneck in the hidden layer.\nOur work differs substantially from the above and introduces an alternative approximate inference scheme for DGPs based on three approximations. First, in order to sidestep the cubic computational cost of GPs we leverage a well-known pseudo point sparse approximation (Snelson & Ghahramani, 2006; Quin\u0303onero-Candela & Rasmussen, 2005). Second, an approximation to the Expectation Propagation (EP) energy function (Seeger, 2007), a marginal likelihood estimate, is optimised directly to find an approximate posterior over the inducing outputs. Third, the optimisation demands analytically intractable moments that are approximated by nesting Assumed Density Filtering (Herna\u0301ndez-Lobato & Adams, 2015). The proposed algorithm is not restricted to the warped GP case and is applicable to non-Gaussian observation models.\nThe complexity of our method is similar to that of the variational approach proposed in Damianou & Lawrence (2013), O(NLM2), but is much less memory intensive, O(LM2) vs. O(NL+ LM2). These costs are competitive to those of the nested variational approach in Hensman & Lawrence (2014)."}, {"heading": "3 The Fully Independent Training Conditional approximation", "text": "The computational complexity of full GP models scales cubically with the number of training instances, making it intractable in practice. Sparse approximation techniques are therefore often necessary. They can be coarsely put into two classes: ones that explicitly sparsify and create a semiparametric representation that approximates the original model, and ones that retain the original nonparametric properties and perform sparse approximation to the exact posterior. The method used here, Fully Independent Training Conditional (FITC), falls into the first category. The FITC approximation is formed by considering a small set of M function values u in the infinite dimensional vector f and assuming conditional independence between the remaining values given the set u (Snelson & Ghahramani, 2006; Quin\u0303onero-Candela & Rasmussen, 2005). This set is often called inducing outputs or pseudo targets and their input locations z can be chosen by optimising the approximate marginal likelihood. The resulting model can be written as follows,\np(ul|\u03b8l) = N (ul;0,Kul\u22121,ul\u22121), l = 1, \u00b7 \u00b7 \u00b7 , L p(hl|ul,hl\u22121, \u03c32l ) = \u220f\nn\nN (hl,n;Cn,lul,Rn,l),\np(y|uL,HL\u22121, \u03c32L) = \u220f\nn\nN (yn;Cn,LuL,Rn,L).\nwhere Cn,l = Khl,n,ulK \u22121 ul,ul and Rn,l = Khl,n,hl,n \u2212Khl,n,ulK\u22121ul,ulKul,hl,n + \u03c32l I. Note that the function outputs index the covariance matrices, for example Khl,n,ul denotes the covariance between hl,n and ul, and takes hl\u22121,n and zl as inputs respectively. This is important when propagating uncertainty through the network. The FITC approximation creates a semi-parametric model, but one which is cleverly structured so that the induced non-stationary noise captures the uncertainty introduced from the sparsification. The computational complexity of inference and hyperparameter tuning in this approximate model is O(NM2) which means M needs to be smaller than N to provide any computational gain (i.e. the approximation should be sparse). The quality of the approximation largely depends on the number of inducing outputs M and the complexity of the underlying function, i.e. if the function\u2019s characteristic lengthscale is small, M needs to be large and vice versa. As M tends to N and z = X, i.e. the inducing inputs and training inputs are shared, the approximate model reverts back to the original GP model. The graphical model is shown in Figure 2 [left]."}, {"heading": "4 Approximate Bayesian inference via EP", "text": "Having specified a probabilistic model for data using a deep sparse Gaussian processes we now consider inference for the inducing outputs u = {ul}Ll=1 and learning of the model parameters \u03b1 = {zl, \u03b8l}Ll=1. The posterior distribution over the inducing outputs can be written as p(u|X,y) \u221d p(u) \u220f\nn p(yn|u,Xn). This quantity can then be used for output prediction given a test input, p(y\u2217|x\u2217,X,y) = \u222b\ndu p(u|X,y) p(y\u2217|u,x\u2217). The model hyperparameters can be tuned by maximising the marginal likelihood p(y|\u03b1) = \u222b\ndudh p(u,h) p(y|u,h, \u03b1). However, both the posterior of u and the marginal likelihood are not analytically tractable when there is more than one GP layer in the model. As such, approximate inference is needed; here we make use of the EP energy function with a tied factor constraint similar to that proposed in the Stochastic Expectation Propagation (SEP) algorithm (Li et al., 2015) to produce a scalable, convergent approximate inference method."}, {"heading": "4.1 EP, Stochastic EP and the EP approximate energy", "text": "In EP (Minka, 2001), the approximate posterior is assumed to be q(u) \u221d p(u)\u220fn t\u0303n(u) where {t\u0303n(u)}Nn=1 are the approximate data factors. Each factor approximately captures the contribution of datapoint nmakes to the posterior and, in this work, they take an unnormalised Gaussian form. The factors can be found by running an iterative procedure which often requires several passes through the training set for convergence3 . The EP algorithm also provides an approximation to the marginal likelihood,\nlog p(y|\u03b1) \u2248 F(\u03b1) = \u03c6(\u03b8)\u2212 \u03c6(\u03b8prior) + N \u2211\nn=1\nlog Z\u0303n\nwhere log Z\u0303n = logZn + \u03c6(\u03b8\\n)\u2212 \u03c6(\u03b8),\nwhere \u03b8, \u03b8\\n and \u03b8prior are the natural parameters of q(u), the cavity q\\n(u) [q\\n(u) \u221d q(u)/t\u0303n(u)] and p(u) respectively, \u03c6(\u03b8) is the log normaliser of a Gaussian distribution with natural parameters \u03b8, and logZn = log \u222b\ndu q\\n(u) p(yn|u,Xn) (Seeger, 2007). Unfortunately, EP is not guaranteed to converge, but if it does, the fixed points lie at the stationary points of the EP energy, which is given by \u2212F(\u03b1). Furthermore, EP requires the approximate factors to be stored in memory, which has a cost of O(NLM2) in this application as we need to store the mean and the covariance matrix for each factor.\n3We summarise the EP steps in the supplementary material."}, {"heading": "4.2 Direct EP energy minimisation with a tied factor constraint", "text": "In order to reduce the expensive memory footprint of EP, the data-factors are tied. That is the posterior p(u|X,y) is approximated by q(u) \u221d p(u)g(u)N , where the factor g(u) could be thought of as an average data factor that captures the average effect of a likelihood term on the posterior. Approximations of this form were recently used in the SEP algorithm (Li et al., 2015) and although seemingly limited, in practice were found to perform almost as well as full EP while significantly reducing EP\u2019s memory requirement, from O(NLM2) to O(LM2) in our case.\nThe original SEP work devised modified versions of the EP updates appropriate for the new form of the approximate posterior. Originally we applied this method to DGPs (details of this approach including hyperparameter optimisation are included in the supplementary material). However, an alternative approach was found to have superior performance, which is to optimise the EP energy function directly (for both the approximating factors and the hyperparameters). Normally, optimisation of the EP energy requires a double-loop algorithm, which is computationally inefficient, however the use of tied factors simplifies the approximate marginal likelihood and allows direct optimisation. The energy becomes,\nF(\u03b1) = \u03c6(\u03b8)\u2212 \u03c6(\u03b8prior) + N \u2211\nn=1\n[ logZn + \u03c6(\u03b8\\1)\u2212 \u03c6(\u03b8) ]\n= (1\u2212N)\u03c6(\u03b8) +N\u03c6(\u03b8\\1)\u2212 \u03c6(\u03b8prior) + N \u2211\nn=1\nlogZn\nsince the cavity distribution q\\n(u) \u221d q(u)/t\u0303n(u) = q(u)/g(u) = q\\1(u) is the same for all training points. This elegantly removes the need for a double-loop algorithm, since we can posit a form for the approximate posterior and optimise the above approximate marginal likelihood directly. However, it is important to note that, in general, optimising this objective will not give the same solution as optimising the full EP energy. The new energy produces an approximation formed by averaging the moments of q\\1(u) p(yn|u,xn) over datapoints, whereas EP averages natural parameters, which is arguably more sensible but less tractable.\nIn detail, we assume the tied factor takes a Gaussian form with natural parameters \u03b81. As a result, the approximate posterior and the cavity are also Gaussian with natural parameters \u03b8 = \u03b8prior +N\u03b81 and \u03b8\\1 = \u03b8prior + (N \u2212 1)\u03b81 respectively. This means that we can compute the first three terms in the energy function exactly. However, it remains to compute logZn = log \u222b\ndu q\\1(u) p(yn|u,xn) which we will discuss next."}, {"heading": "5 Probabilistic backpropagation for deep Gaussian processes", "text": "Computing logZn in the objective function above is analytically intractable for L \u2265 1 since the likelihood given the inducing outputs u is nonlinear and the propagation of the Gaussian cavity through each layer results in a complex distribution. However, for certain choices of covariance functions {Kl}Ll=1, it is possible to use an efficient and accurate approximation which propagates a Gaussian through the first layer of the network and projects this non-Gaussian distribution back to a moment matched Gaussian before propagating through the next layer and repeating the same steps. This scheme is an algorithmic identical to Assumed Density Filtering and a central part of the probabilistic backpropagation algorithm that has been applied to standard neural networks (Herna\u0301ndez-Lobato & Adams, 2015).\nThe aim is to compute logZ and its gradients with respect to the parameters such as \u03b81 or the hyperparameters of the model4. By reintroducing the hidden variables in the middle layers, we perform a Gaussian approximation to Z in a sequential fashion, as illustrated in Figure 2 [right]. We take a two layer case as a running example:\nZ = \u222b du p(y|x,u) q\\1(u)\n=\n\u222b dh1 du2 p(y|h1,u2) q\\1(u2) \u222b du1 p(h1|x,u1) q\\1(u1)\nOne key difference between our approach and the variational free energy method of Damianou & Lawrence (2013) is that our algorithm does not retain an explicit approximate distribution over the hidden variables. Instead, we approximately integrate them out when computing logZ as follows.\nFirst, we can exactly marginalise out the inducing outputs for each GP layer, leading to Z = \u222b\ndh1 q(y|h1) q(h1) where q(h1) = N (h1;m1, v1), q(y|h1) = N (y|h1;m2|h1 , v2|h1) and\nm1 = Kh1,u1K \u22121 u1,u1 m \\1 1 ,\nv1 = \u03c3 2 1 +Kh1,h1 \u2212Kh1,u1K\u22121u1,u1Ku1,h1 +Kh1,u1K\u22121u1,u1V \\1 1 K \u22121 u1,u1 Ku1,h1 ,\nm2|h1 = Kh2,u2K \u22121 u2,u2 m \\1 2 ,\nv2|h1 = \u03c3 2 2 +Kh2,h2 \u2212Kh2,u2K\u22121u2,u2Ku2,h2 +Kh2,u2K\u22121u2,u2V \\1 1 K \u22121 u2,u2 Ku2,h2 .\nFollowing (Girard et al., 2003; Barber & Schottky, 1998; Deisenroth & Mohamed, 2012), we can use the law of iterated conditionals to approximate the difficult integral in the equation above by a Gaussian Z \u2248 N (y|m2, v2) where the mean and variance take the following form,\nm2 = Eq(h1)[m2|h1 ]\nv2 = Eq(h1)[v2|h1 ] + varq(h1)[m2|h1 ]\nwhich results in\nm2 = Eq(h1)[Kh2,u2 ]A\nv2 = \u03c3 2 2 +Eq(h1)[Kh2,h2 ] + tr\n( BEq(h1)[Ku2,h2Kh2,u2 ] ) \u2212m22\nwhere A = K\u22121u2,u2m \\1 2 and B = K \u22121 u2,u2 (V \\1 2 +m \\1 2 m \\1,T 2 )K \u22121 u2,u2 \u2212K\u22121u2,u2 . The equations above require the expectations of the kernel matrix under a Gaussian distribution over the inputs, which are analytically tractable for widely used kernels such as exponentiated quadratic, linear or a more general class of spectral mixture kernels (Titsias & Lawrence, 2010; Wilson & Adams, 2013). In addition, the approximation above can be improved for networks that have multidimensional intermediate variables, by using a Gaussian with a non-diagonal covariance matrix. We discuss this in the supplementary material.\nAs the mean and variance of the Gaussian approximation in each intermediate layer can be computed analytically, their gradients with respect to the mean and variance of the input distribution, as well as the parameters of the current layers are also available. Since we require the gradients of the approximation to logZ , we need to store these results in the forward propagation step, compute\n4We ignore the data index here to lighten the notation\nthe approximate logZ and its gradients at the output layer and use the chain rule in the backward step to differentiate through the ADF procedure. This procedure is reminiscent of the backpropagation algorithm in standard parametric neural networks, hence the name probabilistic backpropagation (Herna\u0301ndez-Lobato & Adams, 2015)."}, {"heading": "6 Stochastic optimisation for scalable training", "text": "The propagation and moment-matching as described above costs O(LM2) and needs to be repeated for all datapoints in the training set in batch mode, resulting in an overall complexity of O(NLM2). Fortunately, the last term of the objective in Section 4.2 is a sum of independent terms, i.e. its computation can be distributed, resulting in a substantial decrease in computational cost. Furthermore, the objective is suitable for stochastic optimisation. In particular, an unbiased noisy estimate of the objective and its gradients can be obtained using a minibatch of training datapoints,\nF \u2248 \u2212(N \u2212 1)\u03c6(\u03b8) +N\u03c6(\u03b8\\1)\u2212 \u03c6(\u03b8prior) + N\n|B|\n|B| \u2211\nb=1\nlogZb,\nwhere |B| denotes the minibatch size."}, {"heading": "7 Approximate predictive distribution", "text": "Given the approximate posterior and a new test input x\u2217, we wish to make a prediction about the test output y\u2217. That is to find p(y\u2217|x\u2217,X,Y) \u2248 \u222b\ndu p(y\u2217|x\u2217,u) q(u|X,Y). This predictive distribution is not analytically tractable, but fortunately, we can approximate it by a Gaussian in a similar fashion to the method described in Section 5. That is, a single forward pass is performed, in which each layer takes in a Gaussian distribution over the input, incorporates the approximate posterior of the inducing outputs and approximates the output distribution by a Gaussian. An alternative to obtain the prediction is to forward sample from the model, but we do not use this approach in the experiments."}, {"heading": "8 Experiments", "text": "We implement and compare the proposed approximation scheme to state-of-the-art methods for Bayesian neural networks. We first detail our implementation in Section 8.1 and then discuss the experimental results in Sections 8.2 and 8.3."}, {"heading": "8.1 Experimental details", "text": "In all the experiments reported here, we use Adam with the default learning rate (Kingma & Ba, 2015) for optimising our objective function. We use an exponentiated quadratic kernel with ARD lengthscales for each layer. The hyperparameters and pseudo point locations are different between functions in each layer. The lengthscales and inducing inputs of the first GP layer are sensibly initialised based on the median distance between datapoints in the input space and the k-means cluster centers respectively. We use long lengthscales and initial inducing inputs between [\u22121, 1] for the higher layers to force them to start with an identity mapping. We parameterise the natural parameters of the average factor and initialise them with small random values. We evaluate the predictive performance on the test set using two popular metrics: root mean squared error (RMSE) and mean log likelihood (MLL)."}, {"heading": "8.2 Regression on UCI datasets", "text": "We validate the proposed approach for training DGPs in regression experiments using several datasets from the UCI repository. In particular, we use the ten datasets and train/test splits used in Herna\u0301ndezLobato & Adams (2015) and Gal & Ghahramani (2015): 1 split for the year dataset [N \u2248 500000,D = 90], 5 splits for the protein dataset [N \u2248 46000,D = 9], and 20 for the others.\nWe compare our method (FITC-DGP) against sparse GP regression using FITC (FITC-GP) and Bayesian neural network (BNN) regression using several state-of-the-art deterministic and samplingbased approximate inference techniques. As baselines, we include the results for BNNs reported in Herna\u0301ndez-Lobato & Adams (2015), BNN-VI(G)-1 and BNN-PBP-1, and in Gal & Ghahramani (2015), BNN-Dropout-1. The results reported for these methods are for networks with one hidden layer of 50 units (100 units for protein and year). Specifically, BNN-VI(G) uses a mean-field Gaussian approximation for the weights in the network, and obtains the stochastic estimates of the bound and its gradient using a Monte Carlo approach (Graves, 2011). BNN-PBP employs Assumed Density Filtering and the probabilistic backpropagation algorithm to obtain a Gaussian approximation for the weights (Herna\u0301ndez-Lobato & Adams, 2015). BNN-Dropout is a recently proposed technique that employs dropout during training as well as at prediction time, that is to average over several predictions, each made by the entire network with a random proportion of the weights set to zero (Gal & Ghahramani, 2015). We implement other methods as follows,\n\u2022 DGP: we evaluate three different architectures of DGPs, each with two GP layers and one hidden layer of one, two and three dimensions respectively (DGP-1, DGP-2 and DGP-3). We include the results for two settings of the number of inducing outputs, M = 50 and M = 100 respectively. Note that for the bigger datasets protein and year, we use M = 100 and M = 200 but do not annotate this in Figure 3. We choose these settings to ensure the run time for our method is smaller or comparable to that of other methods for BNNs.\n\u2022 GP: we use the same number of pseudo datapoints as in DGP (GP 50 and GP 100).\n\u2022 BNN-VI(KW): this method, similar to Graves (2011), employs a mean-field Gaussian variational approximation but evaluates the variational free energy using the reparameterisation trick proposed in Kingma & Welling (2014). We use a diagonal Gaussian prior for the weights and fix the prior variance to 1. The noise variance of the Gaussian noise model is optimised together with the means and variances of the variational approximation using the variational free energy. We test two different network architectures with the rectified linear activation function, and one and two hidden layers, each of 50 units (100 for the two big datasets), denoted by VI(KW)-1 and VI(KW)-2 respectively.\n\u2022 BNN-SGLD: we reuse the same networks with one and two hidden layers as with VI(KW) and approximately sample from the posterior over the weights using Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011). We place a diagonal Gaussian prior over the weights, and parameterise the observation noise variance as \u03c32 = log(1 + exp(\u03ba)), a broad Gaussian prior over \u03ba and sample \u03ba using the same SGLD procedure. Two step sizes, one for the weights and one for \u03ba, were manually tuned for each dataset. We use Autograd for the implementation of BNN-SGLD and BNN-VI(KW) (github.com/HIPS/autograd).\n\u2022 BNN-HMC: We run Hybrid Monte Carlo (HMC) (Neal, 1993) using the MCMCstuff toolbox (Vanhatalo & Vehtari, 2006) for networks with one hidden layer. We place a Gaussian prior over the network weights and a broad inverse Gamma hyper-prior for the prior variance. We also assume an inverse Gamma prior over the observation noise variance. The number of leapfrog steps and step size are first tuned using Bayesian optimisation using the pybo package (github.com/mwhoffman/pybo). Note that this procedure takes a long time (e.g. 3 days for protein) and the year dataset is too large to be handled in this way.\nFigure 3 shows the average test log likelihood (MLL) for a subset of methods with their standard errors. We exclude methods that perform consistently poorly to improve readability. Full results and many more comparisons are included in the supplementary material. We also evaluate the average rank of the MLL performance of all methods across the datasets and their train/test splits and include the results in Figure 4. This figure is generated using the comparison scheme provided by Dems\u030car (2006), and shows statistical differences in the performance of the methods. More precisely, if the gap between the average ranks of any two methods is above the critical distance (shown on the top right), the two methods\u2019 performances are statistically significantly different. Methods that are not\nsignificantly different from each other are linked by a solid line. The rank result shows that DGPs with our inference scheme are the best performing methods overall. Specifically, the DGP-3-100 architecture obtains the best performance on 6 out of 10 datasets and are competitive on the remaining four datasets. The performance of other DGP variants follow closely with the exception for DGP-1 which is a standard warped GP, the network with one dimensional hidden layer. DGP-1 performs poorly compared to GP regression, but is still competitive with several methods for BNNs. The results also strongly indicate that the predictive performance is almost always improved by adding extra hidden layers or extra hidden dimensions or extra inducing outputs.\nThe best non-GP method is BNN-VI(KW)-2 which obtains the best performance on three datasets. However, this method performs poorly on 6 out of 7 remaining datasets, pushing down the corresponding average rank. Despite this, VI(KW) is the best method among all deterministic approximations for BNNs with one or two hidden layers. Overall, the VI approach without the reparameterisation trick of Graves, Dropout and PBP perform poorly in comparison and give inaccurate predictive uncertainty.\nSampling based methods such as SGLD and HMC obtain good predictive performance overall, but often require more tuning compared to other methods. In particular, HMC appears superior on one dataset, and competitively close to DGP\u2019s performance on three other datasets; however, this method does not scale to large datasets.\nThe results for the RMSE metric follow the same trends with DGP-2 and DGP-3 performing as well or better compared to other methods. Interestingly, BNN-SGLD, despite being ranked relatively low according to the MLL metric, often provides good RMSE results. Full results are included in the supplementary material."}, {"heading": "8.3 Predicting the efficiency of organic photovoltaic molecules", "text": "Having demonstrated the performance of our inference scheme for DGPs, we carry out an additional regression experiment on a challenging dataset. We obtain a subset of 60,000 organic molecules and their power conversion efficiency from the Harvard Clean Energy Project (HCEP) (available at http://www.molecularspace.org) (Hachmann et al., 2011). We use 50,000 molecules for training and 10,000 for testing. The molecules are represented using 512-dimensional binary feature vectors, which were generated using the RDKit package, based on the molecular structures in the canonical SMILES format and a bond radius of 2. The power conversion efficiency of these molecules was estimated using density functional theory, determining whether a molecule could be potentially used as solar cell. The overall aim of the HCEP is to find organic molecules that are as efficient as their silicon counterparts. Our aim here is to show DGPs are effective predictive models that provide good uncertainty estimates, which can be used for tasks such as Bayesian Optimisation.\nWe test the method on two DGPs with one hidden layer of 2 and 5 dimensions, denoted by DGP-2 and DGP-5 respectively and each GP is sparsified using 200 inducing outputs. We compare these against two FITC-GP architectures with 200 and 400 pseudo datapoints respectively. We also repeat the experiment using a Bayesian neural network with two hidden layers, each of 400 hidden units. We use the variational approach with the reparameterisation trick of Kingma & Welling (2014) to perform inference in this model. The noise variance was fixed to 0.16 based on a suggestion in Pyzer-Knapp et al. (2015). Figure 5 shows the predictive performance by five architectures. The DGP with a five dimensional hidden layer significantly outperforms others in terms of test MLL, including the shallow structure with considerably more pseudo datapoints. This result demonstrates the efficacy of DGPs in providing good predictive uncertainty estimates, even when the kernel used is a simple exponentiated quadratic kernel and the input features are binary. Surprisingly, VI(KW), although performing poorly\nas measured by the MLL, makes good predictions for the mean."}, {"heading": "9 Summary", "text": "This paper has introduced a new and powerful deterministic approximation scheme for DGPs based upon an approximate EP algorithm and the FITC approximation to sidestep the computational and analytical intractability. A novel extension of the probabilistic backpropagation algorithm was developed to address a difficult marginalisation problem in the approximate EP algorithm used. The new method was evaluated on eleven datasets and compared against a number of state-of-the-art algorithms for Bayesian neural networks. The results show that the new method for training DGPs is superior on 7 out of 11 datasets considered, and performs comparably on the remainder, demonstrating that DGPs are a competitive alternative to multi-layer Bayesian neural networks for supervised learning tasks.\nThe proposed method, in principle, can be applied to classification and unsupervised learning. However, initial work on classification using DGPs, as included in the supplementary, does not show a substantial gain over a GP. This issue is potentially related to the diagonal Gaussian approximation currently used for the hidden layers from the second layer onwards. A non-diagonal approximation is feasible but more expensive. This can be easily addressed because the computation of our training method can be distributed on GPUs for example, making it even more scalable. We will investigate both problems in future work."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Nilesh Tripuraneni, Alex Matthews, Jes Frellsen and Carl Rasmussen for insightful comments and discussion. TDB thanks Google for funding his European Doctoral Fellowship. JMHL acknowledges support from the Rafael del Pino Foundation. DHL and JMHL acknowledge support from Plan Nacional I+D+i, Grant TIN2013-42351-P, and from CAM, Grant S2013/ICE-2845 CASI-CAM-CM. YL thanks the Schlumberger Foundation for her Faculty for the Future PhD fellowship. RET thanks EPSRC grants EP/G050821/1 and EP/L000776/1."}, {"heading": "A Extra experimental results", "text": "A.1 Regression\nDue to the page limitation of the main text, we include here several figures and tables showing the full experimental results and analyses from the regression experiments on 10 UCI datasets. Note that the results for DGPs reported here could be improved further by increasing the number of pseudo datapoints. We choose 50 and 100 pseudo datapoints (or 100 and 200 for the big datasets) so that the training time and prediction time are comparable across all methods. Next we show the full results for the implemented methods and the their average rank across all train/test splits.\n\u2022 Figures 6 and 7 show the full MLL results for all methods and all datasets. Part of these results have been included in the main text. These figures show that DGPs with our approximation scheme is superior as measured by the MLL metric, obtaining the top spot in the average ranking table.\n\u2022 Figures 8 and 9 show the full RMSE results for all methods. Surprisingly, though not doing well on the MLL metric, i.e. providing inaccurate predictive uncertainty, BNN-SGLD with one and two layers are very good at predicting the mean of the test set. DGPs, on average, rival or perform better than this approximate sampling scheme and other methods.\n\u2022 Figures 10 and 11 show the subset of the MLL results above, for GP architectures, and their average ranking. This again demonstrate that DGPs are more flexible than GPs, hence always obtain better predictive performance. The only exception is the network with a one dimensional hidden layer or a warped GP which performs poorly relative to other architectures.\n\u2022 Similarly, Figures 12 and 13 show evidence that increasing the number of layers and hidden dimensions helps improving the accuracy of the predictions.\n\u2022 We include a similar analysis for approximate inference methods for BNNs in Figures 14, 15, 16 and 17. This set of results demonstrates that VI(KW) and SGLD with two hidden layers provide good performance on the test sets, outperforming other methods in shallower architectures. HMC with one hidden layer performs well overall, but its running time is much larger compared to other methods. Other deterministic approximations [VI(G), PBP and Dropout] perform poorly overall.\nTables 3 and 4 show the average test log-likelihood and error respectively for all datasets. The best deterministic method for each dataset is bolded, the best method overall (deterministic and sampling) is underlined and emphasised in italic. The average ranks of the methods across the 10 datasets are also included.\nA.2 Binary and multiclass classification\nWe test our approximate inference scheme for DGPs with non-Gaussian noise models. However, as shown in Tables 1 and 2, DGPs often obtain a marginal gain over GPs, as compared to some substantial improvement in the regression experiments above. We speculate that this is due to our\ncurrent initialisation strategy and our diagonal Gaussian approximation at last layer for multiclass classification. We will follow this up in future work.\n1 2 3 4 5 6 7 8\nMLL Average Rank\nDGP\u22121 50 GP 50 DGP\u22121 100 GP 100 DGP\u22123 100 DGP\u22122 100 DGP\u22123 50 DGP\u22122 50\nCD\nFigure 11: The average rank based on the test MLL for GP/DGP models across the datasets and their train/test splits, generated based on Dems\u030car (2006). See the main text for more details.\n1 2 3 4 5 6 7 8\nRMSE Average Rank\nDGP\u22121 50 DGP\u22121 100 GP 50 GP 100 DGP\u22123 100 DGP\u22122 100 DGP\u22123 50 DGP\u22122 50\nCD\nFigure 13: The average rank based on the test RMSE for GP/DGP models across the datasets and their train/test splits, generated based on Dems\u030car (2006). See the main text for more details.\nVI(KW)\u22122\nCD\n23"}, {"heading": "B EP and SEP", "text": "In this section, we summarise the EP and SEP iterative procedures. The EP algorithm is often mistaken to be optimising KL(p(u|X,y)||q(u)); however, this objective function is intractable. Instead, EP updates one approximate factor at a time by the following procedure: 1. remove the factor t\u0303n(u) to form the leave-one-out or cavity distribution q\\n(u) \u221d q(u)/t\u0303n(u), 2. minimise KL(q\\n(u)p(yn|u,Xn)||q(u)), resulting in a new approximate factor t\u0303newn (u) which can be 3. combined with the cavity to form the new approximate posterior. This procedure is iteratively performed for each datapoint, and often requires several passes through the training set for convergence. One disadvantage of the EP algorithm is the need to store the approximate factors in memory, which costs O(NM2).\nTo sidestep this expensive memory requirement, the SEP algorithm proposes tying the approximate data factors, that is to make some or all factors the same. The simplest case is q(u) \u221d p(u)g(u)N where g(u) is the average data factor. The SEP algorithm, similar to EP, involves iteratively finding the new approximate factor gnew(u), as follows: 1. remove the factor g\u0303(u) to form the leave-one-out or cavity distribution q\\1(u) \u221d q(u)/g\u0303(u), 2. minimise KL(q\\1(u)p(yn|u,Xn)||q(u)), resulting in a new approximate factor g\u0303new(u) which can be 3. combined with the cavity to form the new approximate posterior, and in addition to EP, 4. perform an explicit update to the average factor g(u): g(u)\u2190 g1\u2212\u03b2(u)g\u03b2new(u), where \u03b2 is a small learning rate."}, {"heading": "C EP/SEP moment matching step", "text": "We have proposed using the EP approximate marginal likelihood for direct optimisation of the approximate posterior over the pseudo datapoints and the hyperparameters. An alternative is to run SEP/EP to obtain the approximate posterior, and once this is done, obtain the approximate marginal likelihood for hyperparameter tuning and repeat.\nAs we use Gaussian EP/SEP, the deletion, the update step and the explicit update step in the case of SEP are straightforward. The moment matching step is equivalent to the following updates to the mean and covariance of the approximate posterior:\nm = m\\1 +V\\1 d logZ dm\\1 V = V\\1 \u2212V\\1 [\nd logZ dm\\1 ( d logZ dm\\1 ) \u22ba \u2212 2d logZ dV\\1 ] V\\1,\nwhere q\\1(u) = N (u;m\\1,V\\1) is the cavity distribution, obtained by the deletion step. The inference scheme therefore reduces to evaluating the normalising constant Z and its gradient. Fortunately, we can approximately compute logZ and its gradients using the probabilistic propagation algorithm, in exactly the same way as discussed in the main text.\nD Computing the gradients of logZ Let ml and vl be the mean and variance of the output Gaussian at the l-th layer in the forward propagation step, as we have shown in the main text,\nml = \u03c8l,1Al (1)\nvl = \u03c3 2 l + \u03c8l,0 + tr (Bl\u03c8l,2)\u2212m2l (2)\nwhere\n\u03c8l,0 = Eq(h1)[Khl,hl ] (3) \u03c8l,1 = Eq(hl\u22121)[Khl,ul ] (4) \u03c8l,1 = Eq(hl\u22121)[Kul,hlKhl,ul ] (5)\nAl = K \u22121 ul,ul m \\1 l\n(6)\nBl = K \u22121 ul,ul (V \\1 l +m \\1 l m \\1,T l )K \u22121 ul,ul \u2212K\u22121 ul,ul\n(7)\nIn the forward propagation step, we need to compute the gradients of ml and vl w.r.t. \u03b1l, the parameters of the model and ml\u22121 and vl\u22121, the mean and variance of the distribution over the input. Let \u03b2l = {\u03b1l,ml\u22121, vl\u22121} As Al and Bl are shared between datapoints, one trick to reduce the computation required for each datapoint is to compute the gradients w.r.t. A and B first, then combine them at the end of each minibatch. If we assume that Al and Bl are fixed, the gradients of ml and vl are as follows\ndml d\u03b2l = d\u03c8l,1 d\u03b2l Al (8)\ndvl d\u03b2l = d\u03c32l d\u03b2l + d\u03c8l,0 d\u03b2l + tr\n(\nBl d\u03c8l,2 d\u03b2l\n)\n\u2212 2ml dml d\u03b2l\n(9)\ndml dAl = \u03c8\u22bal,1 (10) dml dBl = 0 (11) dvl dAl = \u22122ml dml dAl (12) dvl dBl = \u03c8\u22bal,2 (13)\nAt the end of the forward step, we can obtain Z = q(y) = N (y;mL, vL), leading to,\nlogZ = \u22121 2 log(2\u03c0vL)\u2212 1 2 (y \u2212mL)2 vL\n(14)\nd logZ dmL = y \u2212mL vL\n(15)\nd logZ dvL = \u2212 1 2vL + 1 2 (y \u2212mL)2 v2L . (16)\nWe are now ready to perform the backpropagation step, that is we compute the gradients of logZ w.r.t. parameters at a layer \u03b1l using the chain rule,\nd logZ d\u03b1l = d logZ dml dml d\u03b1l + d logZ dvl dvl d\u03b1l . (17)\nSimilarly, we can compute the gradients w.r.t. the mean and variance of the input distribution, ml\u22121 and vl\u22121, and Al and Bl."}, {"heading": "E Computing the gradients of the approximate marginal likelihood", "text": "The approximate marginal likelihood as discussed in the main text is as follows,\nF = \u2212(N \u2212 1)\u03c6(\u03b8) +N\u03c6(\u03b8\\1)\u2212 \u03c6(\u03b8prior) + N \u2211\nn=1\nlogZn (18)\nwhere \u03b8, \u03b8\\1 and \u03b8prior are the natural parameters of q(u), q\\1(u) and p(u) respectively, \u03c6(\u03b8) is the log normaliser or log partition function of a Gaussian distribution with natural parameters \u03b8 or mean m and covariance V,\n\u03c6(\u03b8) = 1 2 log |V|+ 1 2 m\u22baV\u22121m, (19)\n\u03b1 is the model hyperameters that we need to tune, and logZn = log \u222b q\\n(u)p(yn|u,Xn)du. Consider the gradient of this objective function w.r.t. one parameter \u03b1i,\ndF d\u03b1i = \u2212(N \u2212 1)d\u03c6(\u03b8) d\u03b1i +N d\u03c6(\u03b8\\1) d\u03b1i\n\u2212 d\u03c6(\u03b8prior) d\u03b1i +\nN \u2211\nn=1\nd logZn d\u03b1i\n= \u2212(N \u2212 1)d\u03c6(\u03b8) d\u03b8 d\u03b8 d\u03b1i +N\nd\u03c6(\u03b8\\1) d\u03b8\\1 d\u03b8\\1 d\u03b1i\n\u2212 d\u03c6(\u03b8prior) d\u03b8prior d\u03b8prior d\u03b1i\n+ N \u2211\nn=1\nd logZn d\u03b1i\n= \u2212(N \u2212 1)\u03b7\u22ba d\u03b8 d\u03b1i +N\u03b7\\1,\u22ba d\u03b8\\1 d\u03b1i\n\u2212 \u03b7\u22baprior d\u03b8prior d\u03b1i\n+ N \u2211\nn=1\nd logZn d\u03b1i\nwhere \u03b7, \u03b7\\1 and \u03b7prior are the expected sufficient statistics under the q(u), q \\1(u) and p(u) respectively. Specifically, for Gaussian approximate EP as discussed in the main paper, the natural parameters are as follows,\nq(u) : \u03b8 = \u03b8prior +N\u03b8g\nq\\1(u) : \u03b8\\1 = \u03b8prior + (N \u2212 1)\u03b8g p(u) : \u03b8prior\nleading to\ndF d\u03b1i = [ \u2212(N \u2212 1)\u03b7\u22ba +N\u03b7\\1,\u22ba \u2212 \u03b7\u22baprior ] d\u03b8prior d\u03b1i\n+N(N \u2212 1) [ \u2212\u03b7\u22ba + \u03b7\\1,\u22ba ] d\u03b8g d\u03b1i\n+ N \u2211\nn=1\nd logZn d\u03b1i"}, {"heading": "F Dealing with non-Gaussian likelihoods", "text": "In this section, we discuss how to compute the log of Z = \u222b du q\\1(u) p(y|u,x) when we have a non-Gaussian likelihood p(y|u,x). For example, if the observations are binary, we can use the probit likelihood, that is p(y|fL, hL\u22121) = \u03c6(yfL) where \u03c6 is the Gaussian cdf. We now need to compute,\nZ = \u222b q\\1(u)p(y|u,x)du\n=\n\u222b\nq\\1(u)p(fL|hL\u22121,uL)p(y|fL)dudhL\u22121dfL\n\u2248 \u222b N (fL;mf , vf)p(y|fL)dfL\nwhere we can find q(fL) = N (fL;mf , vf) using the forward pass of the probabilistic backpropagation. The final integral above can be computed exactly, leading to,\nZ \u2248 \u03c6 ( ymf\u221a vf + 1 )\nIf we have a different likelihood and there is no simple approximation available as above, we can evaluate Z by Monte Carlo averaging, that is to draw samples from q(fL), evaluate the likelihood, then sum and normalise accordingly. However, as we are interested in logZ and its gradients, the objective and gradients obtained by Monte Carlo will be slightly biased. This bias is, however, can be significantly reduced by using more samples.\nG Improving the Gaussian approximation\nIn this section, we discuss how to obtain a non-diagonal Gaussian approximation for the hidden variables from the second layer and above, when computing logZ . Consider a DGP with two GP layer, a one dimensional hidden layer and two dimensional observations y = [y1, y2]. Following the derivation in the main text, we can exactly marginalise out the inducing outputs for each GP layer:\nZ = \u222b dh1q(y|h1)q(h1) (20)\nwhere q(h1) = N (h1;m1, v1) and\nq(y|h1) = N (y|h1;my|h1 ,Vy|h1)\n= N ( y|h1; [ my1|h1 my2|h1 ] , [ vy1|h1 0 0 vy2|h1 ])\nsince we assume that there are two independent GPs in the second layer, and the distribution above is a conditional given the input to the second layer, h1. Importantly, we need to integrate out h1 in eqn. (20). As such, the resulting distribution over y become a complicated distribution in which y1 and y2 are strongly correlated. Consequently, any approximation that breaks this dependency could be poor. We aim to approximate this distribution by a non-diagonal Gaussian with the same moments, that is in words, the approximating Gaussian will have the mean being the expected mean, and the new covariance being the expected covariance plus the covariance of the mean,\nmy = Eq(h1)[my|h1 ] (21)\nVy = Eq(h1)[Vy|h1 ] + covarq(h1)[my|h1 ] (22)\nSubstitute the mean and covariance of the conditional q(y|h1) into the above expressions gives us,\nmy =\n[\nEq(h1)[my1|h1 ] Eq(h1)[my2|h1 ]\n]\n(23)\nand\nVy =\n[\nEq(h1)[vy1|h1 ] 0\n0 Eq(h1)[vy2|h1 ]\n]\n+\n[\nEq(h1)[m 2 y1|h1 ] Eq(h1)[my1|h1my2|h1 ]\nEq(h1)[my1|h1my2|h1 ] Eq(h1)[m 2 y2|h1 ]\n]\n\u2212mym\u22bay (24)\nNote that the diagonal elements of Vy are identical to the expression for the variance in the main text for the single dimensional case."}], "references": [{"title": "Radial basis functions: a Bayesian treatment", "author": ["D. Barber", "B. Schottky"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Barber and Schottky,? \\Q1998\\E", "shortCiteRegEx": "Barber and Schottky", "year": 1998}, {"title": "Variational auto-encoded deep Gaussian processes", "author": ["Dai", "Zhenwen", "Damianou", "Andreas", "Gonz\u00e1lez", "Javier", "Lawrence", "Neil"], "venue": "arXiv preprint arXiv:1511.06455,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Deep Gaussian processes and variational propagation of uncertainty", "author": ["Damianou", "Andreas"], "venue": "PhD thesis,", "citeRegEx": "Damianou and Andreas.,? \\Q2015\\E", "shortCiteRegEx": "Damianou and Andreas.", "year": 2015}, {"title": "Deep Gaussian processes", "author": ["Damianou", "Andreas C", "Lawrence", "Neil D"], "venue": "In 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Damianou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2013}, {"title": "Expectation propagation in Gaussian process dynamical systems", "author": ["Deisenroth", "Marc", "Mohamed", "Shakir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Deisenroth et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2012}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Dem\u0161ar", "Janez"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar and Janez.,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar and Janez.", "year": 2006}, {"title": "Avoiding pathologies in very deep networks", "author": ["Duvenaud", "David", "Rippel", "Oren", "Adams", "Ryan P", "Ghahramani", "Zoubin"], "venue": "In 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Duvenaud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2014}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Gaussian process priors with uncertain inputs \u2014 application to multiple-step ahead time series forecasting", "author": ["Girard", "Agathe", "Rasmussen", "Carl Edward", "Qui\u00f1onero-Candela", "Joaquin", "Murray-Smith", "Roderick"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Girard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Girard et al\\.", "year": 2003}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community", "author": ["Hachmann", "Johannes", "Olivares-Amaya", "Roberto", "Atahan-Evrenk", "Sule", "Amador-Bedolla", "Carlos", "S\u00e1nchez-Carrera", "Roel S", "Gold-Parker", "Aryeh", "Vogt", "Leslie", "Brockway", "Anna M", "Aspuru-Guzik", "Al\u00e1n"], "venue": "grid. The Journal of Physical Chemistry Letters,", "citeRegEx": "Hachmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hachmann et al\\.", "year": 2011}, {"title": "Nested variational compression in deep Gaussian processes", "author": ["Hensman", "James", "Lawrence", "Neil D"], "venue": "arXiv preprint arXiv:1412.1370,", "citeRegEx": "Hensman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan P"], "venue": "In 32nd International Conference on Machine Learning,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In 3rd International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Stochastic gradient VB and the variational auto-encoder", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In The International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["Lawrence", "Neil D", "Moore", "Andrew J"], "venue": "In 24th International Conference on Machine Learning,", "citeRegEx": "Lawrence et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2007}, {"title": "Bayesian warped Gaussian processes", "author": ["L\u00e1zaro-Gredilla", "Miguel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "L\u00e1zaro.Gredilla and Miguel.,? \\Q2012\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla and Miguel.", "year": 2012}, {"title": "Stochastic expectation propagation", "author": ["Li", "Yingzhen", "Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Turner", "Richard E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Minka", "Thomas P"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Bayesian learning via stochastic dynamics", "author": ["Neal", "Radford M"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neal and M.,? \\Q1993\\E", "shortCiteRegEx": "Neal and M.", "year": 1993}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "Learning from the Harvard clean energy project: The use of neural networks to accelerate materials discovery", "author": ["Pyzer-Knapp", "Edward O", "Li", "Kewei", "Aspuru-Guzik", "Alan"], "venue": "Advanced Functional Materials,", "citeRegEx": "Pyzer.Knapp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pyzer.Knapp et al\\.", "year": 2015}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["Qui\u00f1onero-Candela", "Joaquin", "Rasmussen", "Carl Edward"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2005}, {"title": "Expectation propagation for exponential families", "author": ["Seeger", "Matthias"], "venue": "Technical report, Department of EECS, University of California at Berkeley,", "citeRegEx": "Seeger and Matthias.,? \\Q2007\\E", "shortCiteRegEx": "Seeger and Matthias.", "year": 2007}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2006}, {"title": "Warped Gaussian processes", "author": ["Snelson", "Edward", "Rasmussen", "Carl Edward", "Ghahramani", "Zoubin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Snelson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snelson et al\\.", "year": 2004}, {"title": "Introduction to Reinforcement Learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Titsias", "Michalis K"], "venue": "In 12th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias and K.,? \\Q2009\\E", "shortCiteRegEx": "Titsias and K.", "year": 2009}, {"title": "Bayesian Gaussian process latent variable model", "author": ["Titsias", "Michalis K", "Lawrence", "Neil D"], "venue": "In 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Two problems with variational expectation maximisation for time-series models", "author": ["R.E. Turner", "M. Sahani"], "venue": "Bayesian Time series models,", "citeRegEx": "Turner and Sahani,? \\Q2011\\E", "shortCiteRegEx": "Turner and Sahani", "year": 2011}, {"title": "MCMC methods for MLP-network and Gaussian process and stuff\u2013 a documentation for Matlab toolbox MCMCstuff", "author": ["Vanhatalo", "Jarno", "Vehtari", "Aki"], "venue": null, "citeRegEx": "Vanhatalo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vanhatalo et al\\.", "year": 2006}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In 28th International Conference on Machine Learning,", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Gaussian process kernels for pattern discovery and extrapolation", "author": ["Wilson", "Andrew", "Adams", "Ryan"], "venue": "In 30th International Conference on Machine Learning,", "citeRegEx": "Wilson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 26, "context": "A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012).", "startOffset": 100, "endOffset": 145}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al.", "startOffset": 132, "endOffset": 150}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013).", "startOffset": 132, "endOffset": 207}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013).", "startOffset": 132, "endOffset": 249}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al.", "startOffset": 132, "endOffset": 430}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models.", "startOffset": 132, "endOffset": 477}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models.", "startOffset": 132, "endOffset": 507}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 828}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 890}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions.", "startOffset": 132, "endOffset": 921}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood.", "startOffset": 132, "endOffset": 1126}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood. However, the performance of warped GPs is often similar to a standard GP, most likely due to the narrow bottleneck in the hidden layer. Our work differs substantially from the above and introduces an alternative approximate inference scheme for DGPs based on three approximations. First, in order to sidestep the cubic computational cost of GPs we leverage a well-known pseudo point sparse approximation (Snelson & Ghahramani, 2006; Qui\u00f1onero-Candela & Rasmussen, 2005). Second, an approximation to the Expectation Propagation (EP) energy function (Seeger, 2007), a marginal likelihood estimate, is optimised directly to find an approximate posterior over the inducing outputs. Third, the optimisation demands analytically intractable moments that are approximated by nesting Assumed Density Filtering (Hern\u00e1ndez-Lobato & Adams, 2015). The proposed algorithm is not restricted to the warped GP case and is applicable to non-Gaussian observation models. The complexity of our method is similar to that of the variational approach proposed in Damianou & Lawrence (2013), O(NLM2), but is much less memory intensive, O(LM2) vs.", "startOffset": 132, "endOffset": 2277}, {"referenceID": 1, "context": "An extension of Damianou & Lawrence (2013) that has skip links from the inputs to every hidden layer in the network was proposed in Dai et al. (2015), based on suggestions provided in Duvenaud et al. (2014). Recent work by Hensman & Lawrence (2014) introduces a nested variational scheme that only requires a variational distribution over the inducing outputs, removing the parameter scaling problem of Damianou & Lawrence (2013). However, both approaches of Dai et al. (2015) and Hensman & Lawrence (2014) have not been fully evaluated on benchmark supervised learning tasks or on medium to large scale datasets, nor compared to alternative deep models. A special case of DGPs whenL = 2 and the sole hidden layer h1 is only one dimensional is warped GPs (Snelson et al., 2004; L\u00e1zaro-Gredilla, 2012). In L\u00e1zaro-Gredilla (2012) a variational approach, in a similar spirit to Titsias (2009) and Damianou & Lawrence (2013), was used to jointly learn the latent functions. In contrast, the latent function in the second layer is assumed to be deterministic and parameterised by a small set of parameters in Snelson et al. (2004), which can be learnt by maximising the analytically tractable marginal likelihood. However, the performance of warped GPs is often similar to a standard GP, most likely due to the narrow bottleneck in the hidden layer. Our work differs substantially from the above and introduces an alternative approximate inference scheme for DGPs based on three approximations. First, in order to sidestep the cubic computational cost of GPs we leverage a well-known pseudo point sparse approximation (Snelson & Ghahramani, 2006; Qui\u00f1onero-Candela & Rasmussen, 2005). Second, an approximation to the Expectation Propagation (EP) energy function (Seeger, 2007), a marginal likelihood estimate, is optimised directly to find an approximate posterior over the inducing outputs. Third, the optimisation demands analytically intractable moments that are approximated by nesting Assumed Density Filtering (Hern\u00e1ndez-Lobato & Adams, 2015). The proposed algorithm is not restricted to the warped GP case and is applicable to non-Gaussian observation models. The complexity of our method is similar to that of the variational approach proposed in Damianou & Lawrence (2013), O(NLM2), but is much less memory intensive, O(LM2) vs. O(NL+ LM2). These costs are competitive to those of the nested variational approach in Hensman & Lawrence (2014).", "startOffset": 132, "endOffset": 2446}, {"referenceID": 17, "context": "As such, approximate inference is needed; here we make use of the EP energy function with a tied factor constraint similar to that proposed in the Stochastic Expectation Propagation (SEP) algorithm (Li et al., 2015) to produce a scalable, convergent approximate inference method.", "startOffset": 198, "endOffset": 215}, {"referenceID": 17, "context": "Approximations of this form were recently used in the SEP algorithm (Li et al., 2015) and although seemingly limited, in practice were found to perform almost as well as full EP while significantly reducing EP\u2019s memory requirement, from O(NLM2) to O(LM2) in our case.", "startOffset": 68, "endOffset": 85}, {"referenceID": 8, "context": "Following (Girard et al., 2003; Barber & Schottky, 1998; Deisenroth & Mohamed, 2012), we can use the law of iterated conditionals to approximate the difficult integral in the equation above by a Gaussian Z \u2248 N (y|m2, v2) where the mean and variance take the following form, m2 = Eq(h1)[m2|h1 ] v2 = Eq(h1)[v2|h1 ] + varq(h1)[m2|h1 ] which results in", "startOffset": 10, "endOffset": 84}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011).", "startOffset": 5, "endOffset": 28}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011). We use 50,000 molecules for training and 10,000 for testing. The molecules are represented using 512-dimensional binary feature vectors, which were generated using the RDKit package, based on the molecular structures in the canonical SMILES format and a bond radius of 2. The power conversion efficiency of these molecules was estimated using density functional theory, determining whether a molecule could be potentially used as solar cell. The overall aim of the HCEP is to find organic molecules that are as efficient as their silicon counterparts. Our aim here is to show DGPs are effective predictive models that provide good uncertainty estimates, which can be used for tasks such as Bayesian Optimisation. We test the method on two DGPs with one hidden layer of 2 and 5 dimensions, denoted by DGP-2 and DGP-5 respectively and each GP is sparsified using 200 inducing outputs. We compare these against two FITC-GP architectures with 200 and 400 pseudo datapoints respectively. We also repeat the experiment using a Bayesian neural network with two hidden layers, each of 400 hidden units. We use the variational approach with the reparameterisation trick of Kingma & Welling (2014) to perform inference in this model.", "startOffset": 6, "endOffset": 1218}, {"referenceID": 10, "context": "org) (Hachmann et al., 2011). We use 50,000 molecules for training and 10,000 for testing. The molecules are represented using 512-dimensional binary feature vectors, which were generated using the RDKit package, based on the molecular structures in the canonical SMILES format and a bond radius of 2. The power conversion efficiency of these molecules was estimated using density functional theory, determining whether a molecule could be potentially used as solar cell. The overall aim of the HCEP is to find organic molecules that are as efficient as their silicon counterparts. Our aim here is to show DGPs are effective predictive models that provide good uncertainty estimates, which can be used for tasks such as Bayesian Optimisation. We test the method on two DGPs with one hidden layer of 2 and 5 dimensions, denoted by DGP-2 and DGP-5 respectively and each GP is sparsified using 200 inducing outputs. We compare these against two FITC-GP architectures with 200 and 400 pseudo datapoints respectively. We also repeat the experiment using a Bayesian neural network with two hidden layers, each of 400 hidden units. We use the variational approach with the reparameterisation trick of Kingma & Welling (2014) to perform inference in this model. The noise variance was fixed to 0.16 based on a suggestion in Pyzer-Knapp et al. (2015). Figure 5 shows the predictive performance by five architectures.", "startOffset": 6, "endOffset": 1342}], "year": 2016, "abstractText": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "creator": "LaTeX with hyperref package"}}}