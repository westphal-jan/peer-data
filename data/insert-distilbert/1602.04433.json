{"id": "1602.04433", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Unsupervised Domain Adaptation with Residual Transfer Networks", "abstract": "currently the recent success of deep descent neural networks relies on massive amounts of labeled data. for a target task oriented where labeled data translation is unavailable, domain adaptation can transfer toward a learner from a different source domain. in this paper,, we propose a distinct new approach orthogonal to domain adaptation in deep networks that can simultaneously together learn completely adaptive classifiers and transferable features from freshly labeled data in the source domain and unlabeled data encoded in the target domain. we relax a shared - classifier assumption made by previous computational methods and independently assume that the source classifier and target classifier differ by a residual function. we help enable classifier adaptation by plugging uniformly several layers into the deep network to explicitly learn the residual function with reference to the target classifier. we embed features of multiple layers into reproducing kernel class hilbert spaces ( rkhss ) and match feature distributions essential for feature adaptation. the adaptation behaviors can be achieved in most feed - forward regression models worldwide by extending them with uniquely new residual layers and loss functions, which can be trained efficiently using standard back - propagation. empirical evidence exhibits that though the approach outperforms state of art methods on standard domain adaptation datasets.", "histories": [["v1", "Sun, 14 Feb 2016 09:47:30 GMT  (242kb)", "http://arxiv.org/abs/1602.04433v1", null], ["v2", "Thu, 16 Feb 2017 07:56:49 GMT  (339kb,D)", "http://arxiv.org/abs/1602.04433v2", "30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingsheng long", "han zhu", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1602.04433"}, "pdf": {"name": "1602.04433.pdf", "metadata": {"source": "META", "title": "Unsupervised Domain Adaptation with Residual Transfer Networks", "authors": ["Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "emails": ["MINGSHENG@TSINGHUA.EDU.CN", "JIMWANG@TSINGHUA.EDU.CN", "JORDAN@BERKELEY.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n04 43\n3v 1\n[ cs\n.L G\n] 1\n4 Fe\nb 20"}, {"heading": "1. Introduction", "text": "Deep neural networks have significantly improved the state of the art for a wide variety of machine-learning problems and applications. Unfortunately, these impressive gains in performance come only when massive amounts of labeled data are available for supervised training. Since manual labeling of sufficient training data for diverse application domains on-the-fly should be prohibitive, for problems short\nPreliminary work. Copyright 2016 by the author(s).\nof labeled data, there is strong incentive to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled data from a different but related source domain that is big enough for training large-scale deep networks. However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task. A typical example is to apply an object recognition model trained on manually tagged images to test images under substantial variations in pose, occlusion, or illumination (Torralba & Efros, 2011).\nLearning a discriminative model in the presence of a shift between training and test distributions is known as domain adaptation, a special case of transfer learning (Pan & Yang, 2010). A rich line of approaches to domain adaptation have been proposed in the context of both shallow learning and deep learning, which bridge the source and target domains by learning domain-invariant feature representations without using target labels, such that the classifier learned from the source domain can also be applied to the target domain. In particular, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation (Donahue et al., 2014; Yosinski et al., 2014), by disentangling explanatory factors of variations underlying data samples, and grouping deep features hierarchically in accordance with their relatedness to invariant factors. The latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning to extract domain-invariant representations (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).\nThe previous papers on deep domain adaptation worked under the assumption that the source classifier can be directly transferred to the target domain upon the learned domaininvariant feature representations. This assumption is rather strong as in practical applications, it is often infeasible to check whether the source classifier and target classifier can be shared or not. Hence, we focus in this paper on a more general (and safe) domain adaptation scenario in which the\nsource classifier and target classifier differ by a small perturbation function. The goal of this paper is to simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain by embedding the adaptations of both classifiers and features in an end-to-end deep architecture.\nThis work is primarily motivated by He et al. (2015), the winner of the ImageNet ILSVRC 2015 challenge. We thus propose a novel Residual Transfer Network (RTN) approach to domain adaptation in deep networks which can simultaneously learn adaptive classifiers and transferable features, both are complementary to each other and can be combined. We relax the shared-classifier assumption made by previous methods and assume that the source and target classifiers differ by a small residual function. Thus we enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier. In this way, the source classifier and target classifier can be bridged tightly in the back-propagation process. The target classifier is made to better fit the target structures by exploiting the low-density separation criterion. We embed features of multiple layers into reproducing kernel Hilbert spaces (RKHSs) and match feature distributions for feature adaptation. We will show that the adaptation behaviors can be achieved in most feedforward models by extending them with new residual layers and loss functions, which can be trained efficiently using standard back-propagation. Extensive empirical evidence suggests that the proposed RTN approach outperforms state of art methods on standard domain adaptation benchmarks.\nThe contributions of this paper are summarized as follows. (1) We propose a new residual transfer network for domain adaptation, where both classifiers and features are adapted. (2) We explore a deep residual learning framework for classifier adaptation, which does not require target labeled data. Our approach is generic since it can be used to add domain adaptation to almost all existing feed-forward architectures."}, {"heading": "2. Related Work", "text": "This work is related to domain adaptation, a special case of transfer learning (Pan & Yang, 2010), which builds models that can bridge different domains or tasks, explicitly taking the cross-domain discrepancy into account. Transfer learning is to mitigate the burden of manual labeling for machine learning (Pan et al., 2011; Duan et al., 2012a; Zhang et al., 2013; Li et al., 2014; Wang & Schneider, 2014), computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Duan et al., 2012b; Hoffman et al., 2014) and natural language processing (Collobert et al., 2011). It is a consensus that the domain discrepancy in probability distributions of different domains should be formally reduced, either by shallow\nlearning or deep learning methods. Deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples (Bengio et al., 2013) and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks (Yosinski et al., 2014). Thus deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Oquab et al., 2013; Hoffman et al., 2014), multimodal and multi-task learning (Collobert et al., 2011; Ngiam et al., 2011; Gupta et al., 2015), where significant performance gains have been witnessed relative to prior shallow transfer learning methods.\nHowever, recent advances show that deep neural networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy (Glorot et al., 2011; Tzeng et al., 2014). Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks (Ben-David et al., 2007; Mansour et al., 2009; Ben-David et al., 2010). Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015). They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched (Tzeng et al., 2014; Long et al., 2015), or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm (Ganin & Lempitsky, 2015; Tzeng et al., 2015). While performance was significantly improved, these state of the art methods may be restricted by the assumption that under the learned domain-invariant feature representations, the source classifier can be directly transferred to the target domain. In particular, this assumption may not hold when the source classifier and target classifier cannot be shared. As theoretically studied in (Ben-David et al., 2010), when the combined error of the ideal joint hypothesis is large, then there is no single classifier that performs well on both source and target domains, so we cannot find a good target classifier by directly transferring from the source domain.\nThis work is primarily motivated by He et al. (2015), the winner of the ImageNet ILSVRC 2015 challenge. They present a residual learning framework to ease the training of very deep networks (up to 152 layers), termed residual nets. The residual nets explicitly reformulate the layers as learning residual functions F(x) with reference to the layer inputs x, instead of directly learning the unreferenced functions H(x) = F(x) + x as in traditional plain nets. The method focuses on standard deep learning in which training data and test data are drawn from identical dis-\ntributions, hence it cannot be directly applied to domain adaptation. In this paper, we propose to bridge the source classifier fS(x) and target classifier fT (x) by the residual layers such that the cross-domain classifier mismatch can be explicitly modeled by the residual functions F(x) in an end-to-end deep learning architecture. Note that the idea of adapting source classifier to target domain by adding a perturbation function has been studied by (Yang et al., 2007; Duan et al., 2012b). However, these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study. Another crucial distinction is that their perturbation function is defined in the input space x, while the input to our residual function is the target classifier fT (x), which can better capture the mismatch between the source and target classifiers. Finally, their source classifiers should be pre-learned in a separated step, and the prediction phase will be inefficient using nonlinear kernel functions."}, {"heading": "3. Residual Transfer Networks", "text": "In unsupervised domain adaptation, we are provided with a source domain Ds = {(xsi , y s i )} ns i=1 of ns labeled examples and a target domain Dt = {xti} nt i=1 of nt unlabeled examples. The source domain and target domain are sampled from probability distributions p and q respectively, and note p 6= q. The goal of this paper is to craft a deep neural network that enables learning of transfer classifiers y = fs (x) and y = ft (x) to bridge the source-target discrepancy, such that the target risk Rt (ft) = Pr(x,y)\u223cq [ft (x) 6= y] is minimized by leveraging the source domain supervision.\nThe challenge of unsupervised domain adaptation arises in that the target domain has no labeled data, while the source classifier fs trained on source domain Ds cannot be directly applied to the target domain Dt due to the distribution discrepancy p(x, y) 6= q(x, y). The distribution discrepancy may give rise to mismatches in both features and classifiers, i.e. p(x) 6= q(x) and fs(x) 6= ft(x). Both mismatches should be fixed by the adaptation of features and classifiers to enable effective domain adaptation. Classifier adaptation is more difficult than feature adaptation because the target domain is fully unlabeled. Note that current state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015) generally assume shared classifier on their adaptive deep features. This paper assumes fs 6= ft and presents an end-to-end deep learning architecture for joint adaptation of classifiers and features."}, {"heading": "3.1. Convolutional Neural Network", "text": "We will extend from the breakthrough AlexNet architecture (Krizhevsky et al., 2012) that comprises five convolutional layers (conv1\u2013conv5) and three fully connected layers\n(fc6\u2013fc8), where conv1\u2013fc7 are feature layers and fc8 is the classifier layer. Specifically, each fully connected layer \u2113 will learn a nonlinear mapping h\u2113i = a \u2113(W\u2113h\u2113\u22121i + b \u2113), where h\u2113i is the \u2113 th-layer hidden representation of example xi, W\u2113 and b\u2113 are the \u2113th-layer weight and bias parameters, and a\u2113 is the \u2113th-layer activation function, taken as rectifier units (ReLU) a\u2113(x) = max(0,x) for the feature layers or softmax units a\u2113 (x) = ex/\n\u2211|x| j=1 e xj for classifier layer. Denote by F = {W\u2113,b\u2113}l\u2113=1 the set of network parameters (l layers in total). The empirical error E (Ds; fs) of source-classifier fs on source data Ds = {(xsi , y s i )} ns i=1 is\nmin fs\u2208F\nE (Ds; fs) = 1\nns\nns \u2211\ni=1\nL (fs (x s i ) , y s i ), (1)\nwhere fs (xsi ) is the conditional probability that CNN assigns point xsi to label y s i , L(\u00b7, \u00b7) is the cross-entropy loss function L(fs(xsi ), y s i ) = \u2212 \u2211c j=1 1 {y s i = j} log f s j (x s i ),\nc is the number of classes, and f sj (x s i ) = e\nhs,l ij / \u2211 j\u2032 e hs,l ij\u2032\nis the softmax function defined on the lth-layer activation h s,l i that computes the probability of predicting point x s i to class j. Based on the quantification study of feature transferability (Yosinski et al., 2014), the convolutional layers conv1\u2013conv5 can learn generic features transferable across domains (Yosinski et al., 2014). Hence, when adapting the pre-trained AlexNet model from the source domain to the target domain, we opt to fine-tune conv1\u2013conv5 such that the efficacy of feature co-adaptation can be preserved. As we perform residual transfer and distribution matching only for fully connected layers and pooling layers, we will not elaborate the computational details of convolutional layers."}, {"heading": "3.2. Classifier Adaptation", "text": "Although the source classifier and target classifier are different, fs(x) 6= ft(x), they should be related to ensure the feasibility of domain adaptation. Hence it is reasonable to assume that fs(x) and ft(x) differ only by a small perturbation function \u2206f . Previous works (Yang et al., 2007; Duan et al., 2012b) assume that ft(x) = fs(x) + \u2206f(x), where the perturbation \u2206f(x) is a function of input feature x. Unfortunately, these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study. We argue that the difficulty arises because no constraint is imposed to the perturbation function \u2206f(x), therefore it is independent on the source labeled data and source classifier fs(x) so that it must be learned from target labeled data. How to bridge fs(x) and ft(x) in a tight way such that the perturbation \u2206f can be constrained and learned effectively is a critical challenge for unsupervised domain adaptation.\nWe are motivated by the deep residual learning framework presented by He et al. (2015) to win the ImageNet ILSVRC\n2015 challenge. Consider fitting H(x) as an original mapping by a few stacked layers (convolutional layers or fully connected layers) in Figure 2, where x denotes the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e. H(x) \u2212 x. Thus rather than expect stacked layers to approximate H(x), one explicitly lets these layers approximate a residual function F(x) , H(x)\u2212x, with the original function becoming F(x)+x. The operation F(x)+x is performed by a shortcut connection and an element-wise addition, while the residual function is parameterized as F(x; {W\u2113}) by residual layers within each building block. Although both forms are able to asymptotically approximate the desired functions, the ease of learning is different. In reality, it is unlikely that identity mappings are optimal, but it should be easier to find the perturbations with reference to an identity mapping, than to learn the function as new. The residual learning is the key contributor to the successful training of very deep networks (He et al., 2015).\nAs elaborated above, the deep residual learning framework bridges the inputs and outputs of the residual layers by the shortcut connection (identify mapping) such that H(x) = F(x) + x, which eases the learning of residual function F(x) (similar to the perturbation function across the source\nand target classifiers). Based on this observation, we extend the CNN architecture (Figure 1) by plugging in the residual block (Figure 2). We formulate the residual block to bridge the source classifier fS(x) and target classifier fT (x) by x , fT (x), H(x) , fS(x), and F(x) , \u2206f(fT (x)). Note that fS(x) is the outputs of the source-classifier layer fc10 and fT (x) is the outputs of the target-classifier layer fc8, both before softmax activation \u03c3(\u00b7), hence fs (x) , \u03c3 (fS (x)) , ft , \u03c3 (fT (x)). We can bridge the source and target classifiers (before activation) by the residual block as\nfS (x) = fT (x) + \u2206f (fT (x)) , (2)\nwhere we have used the unactivated classifiers fS and fT to ensure the final classifier fs and ft will output well-defined probabilities as the softmax classifier in deep neural networks. Both residual layers fc9\u2013fc10 are fully connected with c\u00d7c units, where c is the number of classes. We set the source classifier fS as the outputs of the residual block to guarantee that it can be trained from the source-labeled data by the deep residual learning framework. In other words, if we set fT as the outputs of the residual block, then we will be unable to learn it successfully as we do not have target labeled data and standard back-propagation will not work. The deep residual learning (He et al., 2015) ensures to output valid classifiers |\u2206f (fT (x))| \u226a |fT (x)| \u2248 |fS (x)|. Note that, the residual learning framework will make the perturbation function \u2206f (fT (x)) dependent on both the target classifier fT (x) (the functional dependency) as well as the source classifier fS(x) (thanks to back-propagation).\nAlthough we cast the classifier adaptation into the residual learning framework, while the residual learning framework guarantees that the target classifier ft cannot deviate much from the source classifier fs, we still cannot guarantee that ft will fit the target-specific structures well. To address this problem, we further exploit the entropy minimization principle (Grandvalet & Bengio, 2004) for classifier adaptation, which favors the low-density separation between classes by minimizing the conditional-entropy E (Dt; ft) of the class probability p(yti = j|x t i; ft) = f t j (x t i) on target\ndata Dt as\nmin ft\u2208F\nE (Dt; ft) = 1\nnt\nnt \u2211\ni=1\nH ( ft ( x t i )) , (3)\nwhere ft (xti) is the conditional probability that CNN assigns unlabeled-pointxti to pseudo-label y t i , and H(\u00b7) is the conditional-entropy loss function defined as H (ft (xti)) = \u2212 \u2211c\nj=1 f t j (x t i) log f t j (x t i), c is the number of classes, and\nf tj (x t i) is the probability of predicting point x t i to class j. By minimizing the entropy penalty (3), the target classifier ft is made directly accessible to target-unlabeled data and amend itself to pass through the target low-density regions."}, {"heading": "3.3. Feature Adaptation", "text": "As classifier adaptation does not necessarily undo the mismatch in the feature distributions, we further perform feature adaptation to make domain adaptation more effective. The literature has revealed that the deep features learned by CNNs can disentangle the explanatory factors of variations underlying data distributions and facilitate knowledge transfer (Oquab et al., 2013; Bengio et al., 2013). But deep features can only reduce, but not remove, the cross-domain distribution discrepancy (Yosinski et al., 2014; Tzeng et al., 2014), which thus motivates the state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015). As feature adaptation has been addressed quite well, we adopt the deep adaptation network (DAN) (Long et al., 2015) to fine-tune CNN on labeled examples and require the feature distributions of the source and target to become similar under feature representations of fully connected layers L = {fc6, fc7, fc8}, which is realized by minimizing a multi-layer MK-MMD penalty as\nmin fs,ft\u2208F\nE (Ds,Dt; fs, ft, k) = \u2211\n\u2113\u2208L\nM2k ( D\u2113s,D \u2113 t ) , (4)\nwhere D\u2113s , {h s,\u2113 i } and D \u2113 t , {h t,\u2113 i } are \u2113 th-layer hidden representations of the source and target points respectively, M2k ( D\u2113s,D \u2113 t )\nis the MK-MMD between source and target evaluated using the \u2113th-layer hidden representations, and L is the indices of layers where MK-MMD penalty is active. Let Hk be the reproducing kernel Hilbert space (RKHS) with kernel k. The multi-kernel maximum mean discrepancy (MK-MMD) between p and q is (Gretton et al., 2012b)\nM2k (p, q) , \u2225 \u2225Ep [\u03c6 (x s)]\u2212 Eq [ \u03c6 ( x t )]\u2225 \u2225 2\nHk , (5)\nwhere \u03c6(\u00b7) is the nonlinear feature mapping. An important property is p = q iff M2k (p, q) = 0 (Gretton et al., 2012a). Characteristic kernel k (x,x\u2032) = \u3008\u03c6 (x) , \u03c6 (x\u2032)\u3009 is defined as the convex combination of m PSD kernels {ku}, K ,\n{k = \u2211m u=1 \u03b2uku : \u2211m\nu=1 \u03b2u = 1, \u03b2u > 0, \u2200u} where the kernel coefficients {\u03b2u} can be simply set to 1/m or can be learned by multiple kernel learning (Long et al., 2015)."}, {"heading": "3.4. Residual Transfer Network", "text": "Based on the aforementioned analysis, to enable effective unsupervised domain adaptation, we propose the residual transfer network (RTN) to be an integration of deep feature learning (1), classifier adaptation (2)\u2013(3), and feature adaptation (4) in an end-to-end deep learning framework as\nmin fS=fT+\u2206f(fT )\n1\nns\nns \u2211\ni=1\nL (fs (x s i ) , y s i )\n+ \u03b3\nnt\nnt \u2211\ni=1\nH ( ft ( x t i ))\n+ \u03bb \u2211\n\u2113\u2208L\nM2k ( D\u2113s,D \u2113 t ) ,\n(6)\nwhere \u03b3 and \u03bb are the tradeoff parameters for the entropy penalty (3) and multi-layer MK-MMD penalty (4) respectively. The proposed RTN model (6) is able to learn both adaptive classifiers and transferable features. As classifier adaptation proposed in this paper and feature adaptation studied in (Long et al., 2015; Ganin & Lempitsky, 2015) are tailored to adapt different layers of the deep networks, they are well complementing to each other to establish better performance and can be combined wherever necessary.\nAs training deep CNNs requires a large amount of labeled data that is prohibitive for many domain adaptation applications, we start with the AlexNet model pre-trained on ImageNet 2012 data and fine-tune it as (Long et al., 2015). The training of RTN mainly follows standard backpropagation, including the residual layers for classifier adaptation (He et al., 2015). But the optimization of MKMMD penalty (4) requires carefully-designed algorithm as detailed in (Long et al., 2015) for linear-time training with back-propagation.\nDiscussion: The proposed idea of residual transfer is quite general and can be readily extended in several scenarios. In heterogeneous domain adaptation where the feature representations are different across domains (Li et al., 2014), the residual transfer can be applied to the feature layers (e.g. fc6\u2013fc7) of the deep network to bridge the source and target representations. (2) In multi-task learning (Chu et al., 2015) that solves multiple potentially correlated tasks together to improve performance of all these tasks by sharing statistic strength, the residual transfer can also be extended to bridge the gap between specific task and the shared task. We will leave these possible extensions to the future work."}, {"heading": "4. Experiments", "text": "We evaluate the proposed residual transfer network against state of the art transfer learning and deep learning methods on unsupervised domain adaptation benchmarks. Datasets, codes, and configurations will be made available publicly."}, {"heading": "4.1. Setup", "text": "Office-31 (Saenko et al., 2010) is a standard benchmark for domain adaptation, comprising 4,652 images distributed in 31 classes collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images taken by web camera and digital SLR camera in an office environment with different photographical settings, respectively. We evaluate all methods across three transfer tasks A \u2192 W, D \u2192 W and W \u2192 D, which are commonly adopted by previous deep transfer learning methods (Donahue et al., 2014; Tzeng et al., 2014; Ganin & Lempitsky, 2015), and across another three transfer tasks A \u2192 D, D \u2192 A and W \u2192 A as presented in (Long et al., 2015; Tzeng et al., 2015).\nOffice-Caltech (Gong et al., 2012) is built by selecting the 10 common categories shared by the Office-31 and Caltech256 (C) (Griffin et al., 2007) datasets, and is widely used by previous methods (Long et al., 2013; Sun et al., 2016). We consider all domain combinations of this dataset, and build 12 transfer tasks: A \u2192 W, D \u2192 W, W \u2192 D, A \u2192 D, D \u2192 A, W \u2192 A, A \u2192 C, W \u2192 C, D \u2192 C, C \u2192 A, C \u2192 W, and C \u2192 D. While Office-31 has more categories and is more difficult for domain adaptation algorithms, OfficeCaltech provides more transfer tasks to enable an unbiased look at the dataset bias issue (Torralba & Efros, 2011). We adopt DeCAF7 (Donahue et al., 2014) features for shallow transfer methods and original images for deep transfer methods.\nWe compare with state of the art transfer and deep learning methods: Transfer Component Analysis (TCA) (Pan et al., 2011), Geodesic Flow Kernel (GFK) (Gong et al., 2012), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Deep Adaptation Network (DAN) (Long et al., 2015), and Reverse Gradient (RevGrad) (Ganin & Lempitsky, 2015). TCA is a conventional transfer learning method based on MMD-regularized Kernel PCA. GFK is a widely-adopted manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge the source and target. DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD. DAN learns more transferable features by embedding deep features of multiple task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally by multi-kernel MMD. RevGrad boosts domain adaptation by making source and\ntarget indistinguishable for a discriminative domain classifier through adversarial training (Goodfellow et al., 2014). To go deeper with the efficacy of residual transfer with entropy minimization, we evaluate several variants of RTN: (1) RTN-res, by removing the residual layers from RTN; (2) RTN-ent, by removing the entropy penalty from RTN; (3) RTN-mmd, by removing the MMD penalty from RTN. Note that RTN-mmd is no longer built upon DAN, and this experiment will suggest that residual transfer of classifiers and MMD adaptation of features are well complementary.\nWe follow standard evaluation protocols for unsupervised domain adaptation (Saenko et al., 2010; Long et al., 2015). For the Office-31 dataset, we adopt the sampling protocol (Saenko et al., 2010) that randomly samples the source domain with 20 labeled examples per category for Amazon (A) and 8 labeled examples per category for Webcam (W) and DSLR (D). For the Office-Caltech dataset, we adopt the full-sampling protocol (Long et al., 2015) that uses all labeled source examples and all unlabeled target examples. All unlabeled examples in the target domain are used for learning transfer classifiers. We compare the average classification accuracy of each method on five random experiments, and report the standard error of the classification accuracies by different experiments of the same transfer task.\nFor all methods, we either follow the procedures for model selection explained in their respective papers, or conduct cross-validation on labeled source data if parameter selection strategies are not explained. For MMD-based methods (TCA, DDC, DAN, and RTN), we use Gaussian kernel with bandwidth b set to median pairwise squared distances on training data, i.e. median heuristic (Gretton et al., 2012b). We follow (Long et al., 2015) and use multi-kernel MMD for DAN and RTN, and a family of m Gaussian kernels by varying bandwidth bu \u2208 [2\u22128b, 28b] with a multiplicative step-size of 21/2. We conduct cross-validation on labeled source data to select parameters of RTN (Long et al., 2015). We implement all deep methods based on the Caffe (Jia et al., 2014) deep-learning framework, and fine-tune from Caffe-trained models of AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Russakovsky et al., 2014). For RTN, We fine-tune convolutional layers conv1\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8 and residual layers fc9\u2013fc10, all through standard back-propagation. Since the classifier and residual layers are trained from scratch, we set their learning rate to be 10 times that of the lower layers. We use mini-batch SGD with momentum of 0.9 and the learning rate annealing strategy implemented in Caffe, and cross-validate base learning rate between 10\u22125 and 10\u22122 by a multiplicative step-size 101/2. To suppress noisy predictions (due to random initialization) that bias the entropy penalty at early stages of the training procedure, we set parameter \u03b3 = 0 within 1000 mini-batch iterations and set it to the cross-\nvalidated value afterwards."}, {"heading": "4.2. Results and Discussion", "text": "The classification accuracy results of unsupervised domain adaptation on the six transfer tasks generated from Office-31 are shown in Table 1, and the results on the twelve transfer tasks generated from Office-Caltech are shown in Table 2. Note that the results of RevGrad under the sampling protocol (Saenko et al., 2010) are directly reported from (Sun et al., 2016), as the original paper only provided results under the full-sampling protocol (Ganin & Lempitsky, 2015). The RTN model based on AlexNet (Figure 1) outperforms all comparison methods on most transfer tasks. In particular, RTN substantially improves the classification accuracy on hard transfer tasks, e.g. A \u2192 W and A \u2192 D, where the source and target are substantially different, and achieves comparable classification accuracy on easy transfer tasks, D \u2192 W and W \u2192 D, where source and target are similar (Saenko et al., 2010). The encouraging results suggest that RTN is able to learn more adaptive classifiers as well as more transferable features for effective domain adaptation.\nFrom the results, we can make insightful observations. (1) Standard deep-learning methods (AlexNet) perform comparably with traditional shallow transfer-learning methods with deep DeCAF7 features as input (TCA and GFK). The only difference between these two sets of methods is that AlexNet can take the advantage of supervised fine-tuning on the source-labeled data, while TCA and GFK can take\nbenefits of their domain adaptation procedures. This result confirms the current practice that supervised fine-tuning is important for transferring source classifier to target domain (Oquab et al., 2013), and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy (Yosinski et al., 2014). This reveals that the two worlds of deep learning and domain adaptation are not compatible with each other in the two-step pipeline, which motivates carefully-designed deep adaptation architectures to unify them. (2) Deep-transfer learning methods that reduce the domain discrepancy by domain-adaptive deep networks (DDC, DAN and RevGrad) substantially outperform standard deep learning methods (AlexNet) and traditional shallow transfer-learning methods with deep features as the input (TCA and GFK). This confirms that incorporating domain-adaptation modules to deep networks can improve domain adaptation performance. By adapting source-target distributions in multiple task-specific layers using optimal multi-kernel two-sample matching, DAN performs the best in general among the prior deep-transfer learning methods. (3) The proposed residual transfer network (RTN) method performs the best and sets a new state of the art record on these datasets. Different from all the previous deep-transfer learning methods that only adapt the feature layers of deep neural networks to learn more transferable features, RTN further adapts the classifier layers to bridge the source and target classifiers in an end-to-end residual learning framework, which can correct the classifier mismatch effectively.\nTo go deeper into the modules of RTN, we show the results\nof RTN variants: RTN-res (no residual transfer), RTN-ent (no entropy penalty), and RTN-mmd (no MMD penalty). The results in Tables 1 and 2 well validate our motivation. (1) RTN-res achieves much better results than RevGrad and DAN, but substantially underperforms RTN. This highlights the importance of the residual transfer of classifier layers for learning more adaptive classifiers. This is critical as in practical applications, there is no guarantee that the source classifier and target classifier can be safely shared. (2) RTN-ent also achieves much better results than DAN and RevGrad, but substantially underperforms RTN. This highlights the importance of entropy minimization for lowdensity separation, which exploits the cluster structure of target-unlabeled data such that the target-classifier can be better adapted to the target data. Otherwise, the residual function may tend to learn a useless zero mapping such that the source and target classifiers are nearly identical (He et al., 2015). (3) RTN-mmd performs comparably with the best baseline DAN, but substantially underperforms RTN. This evidence suggests the residual transfer of classifiers devised in this paper is as effective as the MMD adaptation of features (Long et al., 2015). Since these two methods are tailored to adapt different layers of the deep networks, they are well complementing each other to establish better performance as witnessed by the best performance of RTN."}, {"heading": "4.3. Empirical Analysis", "text": "Visualization of Predictions: We illustrate the adaptivity of classifiers by visualizing in Figures 3(a)\u20133(d) the t-SNE embeddings (Donahue et al., 2014) of the predictions made by the classifiers of DAN and RTN for transfer task A \u2192 W, respectively. We can make the following observations. (1) The predictions made by DAN in Figure 3(a)\u20133(b) show that the target categories are not well discriminated by the source classifier, which implies that target data is not compatible with the source classifier. Hence the source and target classifiers should not be assumed to be identical, which, whatsoever, has been a common assumption made by all prior deep domain adaptation methods (Tzeng et al., 2014;\nLong et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015). (2) The predictions made by RTN in Figures 3(c)\u2013 3(d) show that the target categories are discriminated much better by the target classifier, which suggests that residual transfer of classifiers proposed in this paper is a reasonable extension to the previous deep domain adaptation methods. RTN learns more adaptive classifiers as well as more transferable features to enable effective deep domain adaptation.\nAnalysis of Layer Responses: We illustrate in Figure 4(a) the average magnitudes and standard deviations of the layer responses (He et al., 2015), which are the outputs of fT (x) (fc8 layer), \u2206f(fT (x)) (fc10 layer), and fS(x) (sum operator) before other nonlinearity (ReLU/addition/softmax), respectively. For the residual transfer network, this analysis exposes the response strength of the residual functions. The results reveal that the residual functions \u2206f(fT (x)) have generally much smaller responses than the shortcut functions fT (x). These results support our original motivation that the residual functions might be generally closer to zero than the non-residual functions, since they characterize the small gap between the source classifier and target classifier. The small residual function can be learned more effectively through the residual learning framework (He et al., 2015).\nParameter Sensitivity: Besides the MMD penalty parameter \u03bb as DAN (Long et al., 2015), the RTN model involves another entropy penalty parameter \u03b3. We perform sensitivity analysis for it on transfer tasks A \u2192 W (31 classes) and C \u2192 W (10 classes) by varying the parameter of interest in {0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1.0}. The results are shown in Figures 4(b), with the best results of the baseline shown as dashed lines. We observe that the accuracy of RTN first increases and then decreases as \u03b3 varies and demonstrates a desirable bell-shaped curve. This justifies our motivation of jointly learning deep features and adapting classifiers using residual layers and entropy penalty in deep nets, as a good trade-off between them can promote transfer performance."}, {"heading": "5. Conclusion", "text": "This paper presented a novel approach to unsupervised domain adaptation of deep networks, which enables simultaneous learning of adaptive classifiers and transferable features. Similar to many prior domain adaptation techniques, the feature adaptation is achieved through matching the distributions of features across domains. However, unlike all previous techniques, the proposed approach also supports classifier adaptation, which is accomplished through a new residual transfer module to bridge the source classifier and target classifier. This new ingredient makes the approach a good complement to existing techniques. The approach can be trained by standard back-propagation, which is scalable and can be implemented by any deep learning package. We will constitute further evaluations on larger-scale tasks and semi-supervised domain adaptation settings as future work."}], "references": [{"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "In NIPS,", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Multi-task recurrent neural network for immediacy prediction", "author": ["X. Chu", "W. Ouyang", "W. Yang", "X. Wang"], "venue": "In ICCV, pp", "citeRegEx": "Chu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2015}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "TPAMI, 34(3):465\u2013479,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W. Tsang", "J. Luo"], "venue": null, "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "In ICML,", "citeRegEx": "Ganin and Lempitsky,? \\Q2015\\E", "shortCiteRegEx": "Ganin and Lempitsky", "year": 2015}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In ICCV. IEEE,", "citeRegEx": "Gopalan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "In NIPS, pp", "citeRegEx": "Grandvalet and Bengio,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet and Bengio", "year": 2004}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "JMLR, 13:723\u2013773,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "Griffin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 2007}, {"title": "Cross modal distillation for supervision transfer", "author": ["S. Gupta", "J. Hoffman", "J. Malik"], "venue": "arXiv preprint arXiv:1507.00448,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In MM,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation", "author": ["W. Li", "L. Duan", "D. Xu", "I.W. Tsang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "In ICCV,", "citeRegEx": "Long et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Long et al\\.", "year": 2013}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In COLT,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": null, "citeRegEx": "Oquab et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2013}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "TNNLS,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Return of frustratingly easy domain adaptation", "author": ["B. Sun", "J. Feng", "K. Saenko"], "venue": "In AAAI,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "In CVPR,", "citeRegEx": "Torralba and Efros,? \\Q2011\\E", "shortCiteRegEx": "Torralba and Efros", "year": 2011}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": null, "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "In ICCV,", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Flexible transfer learning under support and model shift", "author": ["X. Wang", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "Wang and Schneider,? \\Q2014\\E", "shortCiteRegEx": "Wang and Schneider", "year": 2014}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "In MM,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Domain adaptation under target and conditional shift", "author": ["K. Zhang", "B. Sch\u00f6lkopf", "K. Muandet", "Z. Wang"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "In particular, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation (Donahue et al., 2014; Yosinski et al., 2014), by disentangling explanatory factors of variations underlying data samples, and grouping deep features hierarchically in accordance with their relatedness to invariant factors.", "startOffset": 126, "endOffset": 171}, {"referenceID": 29, "context": "The latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning to extract domain-invariant representations (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 155, "endOffset": 239}, {"referenceID": 21, "context": "The latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning to extract domain-invariant representations (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 155, "endOffset": 239}, {"referenceID": 30, "context": "The latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning to extract domain-invariant representations (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 155, "endOffset": 239}, {"referenceID": 15, "context": "This work is primarily motivated by He et al. (2015), the winner of the ImageNet ILSVRC 2015 challenge.", "startOffset": 36, "endOffset": 53}, {"referenceID": 25, "context": "Transfer learning is to mitigate the burden of manual labeling for machine learning (Pan et al., 2011; Duan et al., 2012a; Zhang et al., 2013; Li et al., 2014; Wang & Schneider, 2014), computer vision (Saenko et al.", "startOffset": 84, "endOffset": 183}, {"referenceID": 34, "context": "Transfer learning is to mitigate the burden of manual labeling for machine learning (Pan et al., 2011; Duan et al., 2012a; Zhang et al., 2013; Li et al., 2014; Wang & Schneider, 2014), computer vision (Saenko et al.", "startOffset": 84, "endOffset": 183}, {"referenceID": 19, "context": "Transfer learning is to mitigate the burden of manual labeling for machine learning (Pan et al., 2011; Duan et al., 2012a; Zhang et al., 2013; Li et al., 2014; Wang & Schneider, 2014), computer vision (Saenko et al.", "startOffset": 84, "endOffset": 183}, {"referenceID": 26, "context": ", 2014; Wang & Schneider, 2014), computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Duan et al., 2012b; Hoffman et al., 2014) and natural language processing (Collobert et al.", "startOffset": 49, "endOffset": 153}, {"referenceID": 9, "context": ", 2014; Wang & Schneider, 2014), computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Duan et al., 2012b; Hoffman et al., 2014) and natural language processing (Collobert et al.", "startOffset": 49, "endOffset": 153}, {"referenceID": 7, "context": ", 2014; Wang & Schneider, 2014), computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Duan et al., 2012b; Hoffman et al., 2014) and natural language processing (Collobert et al.", "startOffset": 49, "endOffset": 153}, {"referenceID": 16, "context": ", 2014; Wang & Schneider, 2014), computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Duan et al., 2012b; Hoffman et al., 2014) and natural language processing (Collobert et al.", "startOffset": 49, "endOffset": 153}, {"referenceID": 1, "context": "Deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples (Bengio et al., 2013) and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks (Yosinski et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 33, "context": ", 2013) and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks (Yosinski et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 6, "context": "Thus deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Oquab et al., 2013; Hoffman et al., 2014), multimodal and multi-task learning (Collobert et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 24, "context": "Thus deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Oquab et al., 2013; Hoffman et al., 2014), multimodal and multi-task learning (Collobert et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 16, "context": "Thus deep neural networks have been explored for domain adaptation (Glorot et al., 2011; Oquab et al., 2013; Hoffman et al., 2014), multimodal and multi-task learning (Collobert et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 23, "context": ", 2014), multimodal and multi-task learning (Collobert et al., 2011; Ngiam et al., 2011; Gupta et al., 2015), where significant performance gains have been witnessed relative to prior shallow transfer learning methods.", "startOffset": 44, "endOffset": 108}, {"referenceID": 14, "context": ", 2014), multimodal and multi-task learning (Collobert et al., 2011; Ngiam et al., 2011; Gupta et al., 2015), where significant performance gains have been witnessed relative to prior shallow transfer learning methods.", "startOffset": 44, "endOffset": 108}, {"referenceID": 6, "context": "However, recent advances show that deep neural networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy (Glorot et al., 2011; Tzeng et al., 2014).", "startOffset": 166, "endOffset": 207}, {"referenceID": 29, "context": "However, recent advances show that deep neural networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy (Glorot et al., 2011; Tzeng et al., 2014).", "startOffset": 166, "endOffset": 207}, {"referenceID": 0, "context": "Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks (Ben-David et al., 2007; Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 137, "endOffset": 207}, {"referenceID": 22, "context": "Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks (Ben-David et al., 2007; Mansour et al., 2009; Ben-David et al., 2010).", "startOffset": 137, "endOffset": 207}, {"referenceID": 29, "context": "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 149, "endOffset": 233}, {"referenceID": 21, "context": "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 149, "endOffset": 233}, {"referenceID": 30, "context": "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 149, "endOffset": 233}, {"referenceID": 29, "context": "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched (Tzeng et al., 2014; Long et al., 2015), or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm (Ganin & Lempitsky, 2015; Tzeng et al.", "startOffset": 182, "endOffset": 221}, {"referenceID": 21, "context": "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched (Tzeng et al., 2014; Long et al., 2015), or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm (Ganin & Lempitsky, 2015; Tzeng et al.", "startOffset": 182, "endOffset": 221}, {"referenceID": 30, "context": ", 2015), or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm (Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 192, "endOffset": 237}, {"referenceID": 15, "context": "This work is primarily motivated by He et al. (2015), the winner of the ImageNet ILSVRC 2015 challenge.", "startOffset": 36, "endOffset": 53}, {"referenceID": 32, "context": "Note that the idea of adapting source classifier to target domain by adding a perturbation function has been studied by (Yang et al., 2007; Duan et al., 2012b).", "startOffset": 120, "endOffset": 159}, {"referenceID": 21, "context": "Note that current state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015) generally assume shared classifier on their adaptive deep features.", "startOffset": 67, "endOffset": 131}, {"referenceID": 30, "context": "Note that current state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015) generally assume shared classifier on their adaptive deep features.", "startOffset": 67, "endOffset": 131}, {"referenceID": 18, "context": "We will extend from the breakthrough AlexNet architecture (Krizhevsky et al., 2012) that comprises five convolutional layers (conv1\u2013conv5) and three fully connected layers (fc6\u2013fc8), where conv1\u2013fc7 are feature layers and fc8 is the classifier layer.", "startOffset": 58, "endOffset": 83}, {"referenceID": 33, "context": "Based on the quantification study of feature transferability (Yosinski et al., 2014), the convolutional layers conv1\u2013conv5 can learn generic features transferable across domains (Yosinski et al.", "startOffset": 61, "endOffset": 84}, {"referenceID": 33, "context": ", 2014), the convolutional layers conv1\u2013conv5 can learn generic features transferable across domains (Yosinski et al., 2014).", "startOffset": 101, "endOffset": 124}, {"referenceID": 32, "context": "Previous works (Yang et al., 2007; Duan et al., 2012b) assume that ft(x) = fs(x) + \u2206f(x), where the perturbation \u2206f(x) is a function of input feature x.", "startOffset": 15, "endOffset": 54}, {"referenceID": 15, "context": "We are motivated by the deep residual learning framework presented by He et al. (2015) to win the ImageNet ILSVRC", "startOffset": 70, "endOffset": 87}, {"referenceID": 21, "context": "structures, hence they are not safely transferable and should be adapted with MK-MMD minimization (Long et al., 2015); (2) supervised", "startOffset": 98, "endOffset": 117}, {"referenceID": 15, "context": "A building block for residual learning (He et al., 2015).", "startOffset": 39, "endOffset": 56}, {"referenceID": 15, "context": "The residual learning is the key contributor to the successful training of very deep networks (He et al., 2015).", "startOffset": 94, "endOffset": 111}, {"referenceID": 15, "context": "The deep residual learning (He et al., 2015) ensures to output valid classifiers |\u2206f (fT (x))| \u226a |fT (x)| \u2248 |fS (x)|.", "startOffset": 27, "endOffset": 44}, {"referenceID": 24, "context": "The literature has revealed that the deep features learned by CNNs can disentangle the explanatory factors of variations underlying data distributions and facilitate knowledge transfer (Oquab et al., 2013; Bengio et al., 2013).", "startOffset": 185, "endOffset": 226}, {"referenceID": 1, "context": "The literature has revealed that the deep features learned by CNNs can disentangle the explanatory factors of variations underlying data distributions and facilitate knowledge transfer (Oquab et al., 2013; Bengio et al., 2013).", "startOffset": 185, "endOffset": 226}, {"referenceID": 33, "context": "But deep features can only reduce, but not remove, the cross-domain distribution discrepancy (Yosinski et al., 2014; Tzeng et al., 2014), which thus motivates the state of the art deep feature adaptation methods (Long et al.", "startOffset": 93, "endOffset": 136}, {"referenceID": 29, "context": "But deep features can only reduce, but not remove, the cross-domain distribution discrepancy (Yosinski et al., 2014; Tzeng et al., 2014), which thus motivates the state of the art deep feature adaptation methods (Long et al.", "startOffset": 93, "endOffset": 136}, {"referenceID": 21, "context": ", 2014), which thus motivates the state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 83, "endOffset": 147}, {"referenceID": 30, "context": ", 2014), which thus motivates the state of the art deep feature adaptation methods (Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 83, "endOffset": 147}, {"referenceID": 21, "context": "As feature adaptation has been addressed quite well, we adopt the deep adaptation network (DAN) (Long et al., 2015) to fine-tune CNN on labeled examples and require the feature distributions of the source and target to become similar under feature representations of fully connected layers L = {fc6, fc7, fc8}, which is realized by minimizing a multi-layer MK-MMD penalty as", "startOffset": 96, "endOffset": 115}, {"referenceID": 21, "context": "Characteristic kernel k (x,x) = \u3008\u03c6 (x) , \u03c6 (x)\u3009 is defined as the convex combination of m PSD kernels {ku}, K , {k = \u2211m u=1 \u03b2uku : \u2211m u=1 \u03b2u = 1, \u03b2u > 0, \u2200u} where the kernel coefficients {\u03b2u} can be simply set to 1/m or can be learned by multiple kernel learning (Long et al., 2015).", "startOffset": 264, "endOffset": 283}, {"referenceID": 21, "context": "As classifier adaptation proposed in this paper and feature adaptation studied in (Long et al., 2015; Ganin & Lempitsky, 2015) are tailored to adapt different layers of the deep networks, they are well complementing to each other to establish better performance and can be combined wherever necessary.", "startOffset": 82, "endOffset": 126}, {"referenceID": 21, "context": "As training deep CNNs requires a large amount of labeled data that is prohibitive for many domain adaptation applications, we start with the AlexNet model pre-trained on ImageNet 2012 data and fine-tune it as (Long et al., 2015).", "startOffset": 209, "endOffset": 228}, {"referenceID": 15, "context": "The training of RTN mainly follows standard backpropagation, including the residual layers for classifier adaptation (He et al., 2015).", "startOffset": 117, "endOffset": 134}, {"referenceID": 21, "context": "But the optimization of MKMMD penalty (4) requires carefully-designed algorithm as detailed in (Long et al., 2015) for linear-time training with back-propagation.", "startOffset": 95, "endOffset": 114}, {"referenceID": 19, "context": "In heterogeneous domain adaptation where the feature representations are different across domains (Li et al., 2014), the residual transfer can be applied to the feature layers (e.", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "(2) In multi-task learning (Chu et al., 2015) that solves multiple potentially correlated tasks together to improve performance of all these tasks by sharing statistic strength, the residual transfer can also be extended to bridge the gap between specific task and the shared task.", "startOffset": 27, "endOffset": 45}, {"referenceID": 26, "context": "Office-31 (Saenko et al., 2010) is a standard benchmark for domain adaptation, comprising 4,652 images distributed in 31 classes collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.", "startOffset": 10, "endOffset": 31}, {"referenceID": 29, "context": "We evaluate all methods across three transfer tasks A \u2192 W, D \u2192 W and W \u2192 D, which are commonly adopted by previous deep transfer learning methods (Donahue et al., 2014; Tzeng et al., 2014; Ganin & Lempitsky, 2015), and across another three transfer tasks A \u2192 D, D \u2192 A and W \u2192 A as presented in (Long et al.", "startOffset": 146, "endOffset": 213}, {"referenceID": 21, "context": ", 2014; Ganin & Lempitsky, 2015), and across another three transfer tasks A \u2192 D, D \u2192 A and W \u2192 A as presented in (Long et al., 2015; Tzeng et al., 2015).", "startOffset": 113, "endOffset": 152}, {"referenceID": 30, "context": ", 2014; Ganin & Lempitsky, 2015), and across another three transfer tasks A \u2192 D, D \u2192 A and W \u2192 A as presented in (Long et al., 2015; Tzeng et al., 2015).", "startOffset": 113, "endOffset": 152}, {"referenceID": 7, "context": "Office-Caltech (Gong et al., 2012) is built by selecting the 10 common categories shared by the Office-31 and Caltech256 (C) (Griffin et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 13, "context": ", 2012) is built by selecting the 10 common categories shared by the Office-31 and Caltech256 (C) (Griffin et al., 2007) datasets, and is widely used by previous methods (Long et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 20, "context": ", 2007) datasets, and is widely used by previous methods (Long et al., 2013; Sun et al., 2016).", "startOffset": 57, "endOffset": 94}, {"referenceID": 27, "context": ", 2007) datasets, and is widely used by previous methods (Long et al., 2013; Sun et al., 2016).", "startOffset": 57, "endOffset": 94}, {"referenceID": 25, "context": "We compare with state of the art transfer and deep learning methods: Transfer Component Analysis (TCA) (Pan et al., 2011), Geodesic Flow Kernel (GFK) (Gong et al.", "startOffset": 103, "endOffset": 121}, {"referenceID": 7, "context": ", 2011), Geodesic Flow Kernel (GFK) (Gong et al., 2012), Deep Domain Confusion (DDC) (Tzeng et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 29, "context": ", 2012), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Deep Adaptation Network (DAN) (Long et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 21, "context": ", 2014), Deep Adaptation Network (DAN) (Long et al., 2015), and Reverse Gradient (RevGrad) (Ganin & Lempitsky, 2015).", "startOffset": 39, "endOffset": 58}, {"referenceID": 8, "context": "RevGrad boosts domain adaptation by making source and target indistinguishable for a discriminative domain classifier through adversarial training (Goodfellow et al., 2014).", "startOffset": 147, "endOffset": 172}, {"referenceID": 26, "context": "We follow standard evaluation protocols for unsupervised domain adaptation (Saenko et al., 2010; Long et al., 2015).", "startOffset": 75, "endOffset": 115}, {"referenceID": 21, "context": "We follow standard evaluation protocols for unsupervised domain adaptation (Saenko et al., 2010; Long et al., 2015).", "startOffset": 75, "endOffset": 115}, {"referenceID": 26, "context": "For the Office-31 dataset, we adopt the sampling protocol (Saenko et al., 2010) that randomly samples the source domain with 20 labeled examples per category for Amazon (A) and 8 labeled examples per category for Webcam (W) and DSLR (D).", "startOffset": 58, "endOffset": 79}, {"referenceID": 21, "context": "For the Office-Caltech dataset, we adopt the full-sampling protocol (Long et al., 2015) that uses all labeled source examples and all unlabeled target examples.", "startOffset": 68, "endOffset": 87}, {"referenceID": 21, "context": "We follow (Long et al., 2015) and use multi-kernel MMD for DAN and RTN, and a family of m Gaussian kernels by varying bandwidth bu \u2208 [2b, 2b] with a multiplicative step-size of 2.", "startOffset": 10, "endOffset": 29}, {"referenceID": 21, "context": "We conduct cross-validation on labeled source data to select parameters of RTN (Long et al., 2015).", "startOffset": 79, "endOffset": 98}, {"referenceID": 17, "context": "We implement all deep methods based on the Caffe (Jia et al., 2014) deep-learning framework, and fine-tune from Caffe-trained models of AlexNet (Krizhevsky et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 18, "context": ", 2014) deep-learning framework, and fine-tune from Caffe-trained models of AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Russakovsky et al.", "startOffset": 84, "endOffset": 109}, {"referenceID": 26, "context": "Accuracy on Office-31 dataset under sampling protocol (Saenko et al., 2010) for unsupervised adaptation.", "startOffset": 54, "endOffset": 75}, {"referenceID": 25, "context": "Method A \u2192 W D \u2192 W W \u2192 D A \u2192 D D \u2192 A W \u2192 A Avg TCA (Pan et al., 2011) 59.", "startOffset": 51, "endOffset": 69}, {"referenceID": 7, "context": "8 GFK (Gong et al., 2012) 58.", "startOffset": 6, "endOffset": 25}, {"referenceID": 18, "context": "7 AlexNet (Krizhevsky et al., 2012) 60.", "startOffset": 10, "endOffset": 35}, {"referenceID": 29, "context": "7 DDC (Tzeng et al., 2014) 62.", "startOffset": 6, "endOffset": 26}, {"referenceID": 21, "context": "3 DAN (Long et al., 2015) 66.", "startOffset": 6, "endOffset": 25}, {"referenceID": 21, "context": "Accuracy on Office-Caltech dataset under full protocol (Long et al., 2015) for unsupervised adaptation.", "startOffset": 55, "endOffset": 74}, {"referenceID": 25, "context": "Method A\u2192W D\u2192W W\u2192D A\u2192D D\u2192A W\u2192A A\u2192C W\u2192C D\u2192C C\u2192A C\u2192W C\u2192D Avg TCA (Pan et al., 2011) 84.", "startOffset": 63, "endOffset": 81}, {"referenceID": 7, "context": "0 GFK (Gong et al., 2012) 89.", "startOffset": 6, "endOffset": 25}, {"referenceID": 18, "context": "5 AlexNet (Krizhevsky et al., 2012) 83.", "startOffset": 10, "endOffset": 35}, {"referenceID": 29, "context": "3 DDC (Tzeng et al., 2014) 86.", "startOffset": 6, "endOffset": 26}, {"referenceID": 21, "context": "2 DAN (Long et al., 2015) 93.", "startOffset": 6, "endOffset": 25}, {"referenceID": 26, "context": "Note that the results of RevGrad under the sampling protocol (Saenko et al., 2010) are directly reported from (Sun et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 27, "context": ", 2010) are directly reported from (Sun et al., 2016), as the original paper only provided results under the full-sampling protocol (Ganin & Lempitsky, 2015).", "startOffset": 35, "endOffset": 53}, {"referenceID": 26, "context": "A \u2192 W and A \u2192 D, where the source and target are substantially different, and achieves comparable classification accuracy on easy transfer tasks, D \u2192 W and W \u2192 D, where source and target are similar (Saenko et al., 2010).", "startOffset": 199, "endOffset": 220}, {"referenceID": 24, "context": "This result confirms the current practice that supervised fine-tuning is important for transferring source classifier to target domain (Oquab et al., 2013), and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy (Yosinski et al.", "startOffset": 135, "endOffset": 155}, {"referenceID": 33, "context": ", 2013), and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy (Yosinski et al., 2014).", "startOffset": 176, "endOffset": 199}, {"referenceID": 15, "context": "Otherwise, the residual function may tend to learn a useless zero mapping such that the source and target classifiers are nearly identical (He et al., 2015).", "startOffset": 139, "endOffset": 156}, {"referenceID": 21, "context": "This evidence suggests the residual transfer of classifiers devised in this paper is as effective as the MMD adaptation of features (Long et al., 2015).", "startOffset": 132, "endOffset": 151}, {"referenceID": 29, "context": "Hence the source and target classifiers should not be assumed to be identical, which, whatsoever, has been a common assumption made by all prior deep domain adaptation methods (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 176, "endOffset": 260}, {"referenceID": 21, "context": "Hence the source and target classifiers should not be assumed to be identical, which, whatsoever, has been a common assumption made by all prior deep domain adaptation methods (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 176, "endOffset": 260}, {"referenceID": 30, "context": "Hence the source and target classifiers should not be assumed to be identical, which, whatsoever, has been a common assumption made by all prior deep domain adaptation methods (Tzeng et al., 2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng et al., 2015).", "startOffset": 176, "endOffset": 260}, {"referenceID": 15, "context": "Analysis of Layer Responses: We illustrate in Figure 4(a) the average magnitudes and standard deviations of the layer responses (He et al., 2015), which are the outputs of fT (x) (fc8 layer), \u2206f(fT (x)) (fc10 layer), and fS(x) (sum operator) before other nonlinearity (ReLU/addition/softmax), respectively.", "startOffset": 128, "endOffset": 145}, {"referenceID": 15, "context": "The small residual function can be learned more effectively through the residual learning framework (He et al., 2015).", "startOffset": 100, "endOffset": 117}, {"referenceID": 21, "context": "Parameter Sensitivity: Besides the MMD penalty parameter \u03bb as DAN (Long et al., 2015), the RTN model involves another entropy penalty parameter \u03b3.", "startOffset": 66, "endOffset": 85}], "year": 2016, "abstractText": "The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier. We embed features of multiple layers into reproducing kernel Hilbert spaces (RKHSs) and match feature distributions for feature adaptation. The adaptation behaviors can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently using standard back-propagation. Empirical evidence exhibits that the approach outperforms state of art methods on standard domain adaptation datasets.", "creator": "LaTeX with hyperref package"}}}